{
    "paper_title": "Instruction-Guided Autoregressive Neural Network Parameter Generation",
    "authors": [
        "Soro Bedionita",
        "Bruno Andreis",
        "Song Chong",
        "Sung Ju Hwang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Learning to generate neural network parameters conditioned on task descriptions and architecture specifications is pivotal for advancing model adaptability and transfer learning. Existing methods especially those based on diffusion models suffer from limited scalability to large architectures, rigidity in handling varying network depths, and disjointed parameter generation that undermines inter-layer coherence. In this work, we propose IGPG (Instruction Guided Parameter Generation), an autoregressive framework that unifies parameter synthesis across diverse tasks and architectures. IGPG leverages a VQ-VAE and an autoregressive model to generate neural network parameters, conditioned on task instructions, dataset, and architecture details. By autoregressively generating neural network weights' tokens, IGPG ensures inter-layer coherence and enables efficient adaptation across models and datasets. Operating at the token level, IGPG effectively captures complex parameter distributions aggregated from a broad spectrum of pretrained models. Extensive experiments on multiple vision datasets demonstrate that IGPG consolidates diverse pretrained models into a single, flexible generative framework. The synthesized parameters achieve competitive or superior performance relative to state-of-the-art methods, especially in terms of scalability and efficiency when applied to large architectures. These results underscore ICPG potential as a powerful tool for pretrained weight retrieval, model selection, and rapid task-specific fine-tuning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 2 1 0 2 0 . 4 0 5 2 : r Accepted at the ICLR Workshop on Neural Network Weights as New Data Modality INSTRUCTION-GUIDED AUTOREGRESSIVE NEURAL NETWORK PARAMETER GENERATION Soro Bedionita1, Bruno Andreis1, Song Chong1, Sung Ju Hwang1,2 1KAIST AI, 2DeepAuto.ai, South Korea {sorobedio,andries,songchong, sungju.hwang}@kaist.ac.kr"
        },
        {
            "title": "ABSTRACT",
            "content": "Learning to generate neural network parameters conditioned on task descriptions and architecture specifications is pivotal for advancing model adaptability and transfer learning. Existing methodsespecially those based on diffusion modelssuffer from limited scalability to large architectures, rigidity in handling varying network depths, and disjointed parameter generation that undermines interlayer coherence. In this work, we propose IGPG (Instruction-Guided Parameter Generation), an autoregressive framework that unifies parameter synthesis across diverse tasks and architectures. IGPG leverages VQ-VAE and an autoregressive model to generate neural network parameters, conditioned on task instructions, dataset, and architecture details. By autoregressively generating neural network weights tokens, IGPG ensures inter-layer coherence and enables efficient adaptation across models and datasets. Operating at the token level, IGPG effectively captures complex parameter distributions aggregated from broad spectrum of pretrained models. Extensive experiments on multiple vision datasets demonstrate that IGPG consolidates diverse pretrained models into single, flexible generative framework. The synthesized parameters achieve competitive or superior performance relative to state-of-the-art methods, especially in terms of scalability and efficiency when applied to large architectures. These results underscore IGPGs potential as powerful tool for pretrained weight retrieval, model selection, and rapid task-specific fine-tuning."
        },
        {
            "title": "INTRODUCTION",
            "content": "Deep neural networks have driven breakthroughs across domainsfrom image recognition (Russakovsky et al., 2015; He et al., 2016) to natural language processingleading to vast repositories of pretrained models (Schurholt et al., 2022c) available via platforms like Hugging Face1 and libraries such as TIMM (Wightman, 2019). Despite their success, adapting these models to new tasks or datasets is challenging. It often requires manual intervention, extensive fine-tuning, and careful model selection. Prior work in transfer learning, meta-learning, and knowledge distillation (Gou et al., 2021; Yang et al., 2021; Elsken et al., 2019) has predominantly focused on individual models, often overlooking the cross-task insights embedded in large-scale model collections. More recent efforts in hyperrepresentation learning (Schurholt et al., 2021; Schurholt et al., 2022b;a; Wang et al., 2024) have sought to learn distributions over network weights to enhance initialization. However, these methods are typically unconditional and limited to single-task scenarios, neglecting the potential benefits of incorporating pretrained dataset embeddings during training. While few studies have explored task-specific parameter generation (Soro et al., 2024), there remains significant gap in developing unified and flexible solution. Furthermore, when applied to large architectures, these approaches tend to generate weight chunks without considering the relationships within each sampled layer, thereby limiting performance and increasing sampling time. Equal contribution. 1https://huggingface.co/ 1 Accepted at the ICLR Workshop on Neural Network Weights as New Data Modality 2025 Figure 1: Our approach integrates VQ-VAE autoencoder (ED) with transformer prior. First, the VQ-VAE encodes vectorized network parameters (see Section 2.2), and then the transformer is trained on the resulting codebook (see Section 3). Additionally, promptsincluding data, task, or architecture detailsare processed using multimodal or language modeling techniques (see Section 3), with an example training simplified prompt template provided in Remark 1. To address these challenges, we introduce Instruction-Guided Parameter Generation (IGPG), novel framework that integrates Vector Quantized Variational Autoencoders (VQ-VAE) with autoregressive modeling to generate neural network parameters conditioned on both task and architecture. IGPG jointly encodes three key elements: Task Representations: Using dataset embeddings or natural language instructions to capture target task semantics; Architecture Specifications: Explicitly representing network designs to enable cross-architecture parameter generation; and Inter-Layer Dependencies: Employing autoregressive modeling to preserve coherence across layers. This joint conditioning enables IGPG to efficiently synthesize coherent, task-optimized parameters, reducing reliance on extensive fine-tuning. Our contributions are summarized as follows: 1. Task-Conditioned Generation: We propose mechanism for directly generating network parameters from natural language or dataset descriptors, offering intuitive task control. 2. Architecture-Agnostic Framework: Our method generates parameters across diverse architectures, leveraging knowledge from multiple pretrained models. 3. Autoregressive Coherence: By modeling layer-wise dependencies, IGPG ensures internally consistent parameters that accelerate convergence and enhance transfer performance. Extensive experiments demonstrate that IGPG compresses and transfers the collective knowledge of diverse pretrained models into single generative framework, achieving competitive or superior performance on unseen tasks and scaling effectively to larger architectures (see Figure 1). 2 INSTRUCTION-GUIDED PARAMETERS GENERATION 2.1 PRELIMINARY We introduce Instruction-Guided Parameter Generation (IGPG), framework that learns the distribution of pretrained models to generate new weights on demand (see Figure 1). By capturing key feature patterns of high-performing networks, IGPG can generate specialized weights for both existing and novel tasks or dataset, reducing extensive retraining and accelerating model deployment in diverse computer vision scenarios. Our method begins with set of pretrained models {θi}N description {Di}N i=1 and their corresponding datasets or task i=1. We construct model zoo by vectorizing each networks parameters in one of Accepted at the ICLR Workshop on Neural Network Weights as New Data Modality 2025 two ways. In the layer-wise setting, each layers weights (including biases) are flattened into single vector, yielding per-layer parameter samples from across all pretrained networks. Alternatively, in the architecture-wise setting, the flattened layer weights of each model are sequentially concatenated to form single global parameter vector per model, preserving the original layer order. Both approaches produce uniform parameter representations that IGPG uses to learn generative mapping, enabling the generation of dataset/taskand architecture-specific weights for efficient adaptation. We formalize our setup by defining as the space of possible datasets or tasks, as the space of neural architectures, and Θ as the parameter space. Our generative mapping operates in two phases: during training, : Θ Θ; during inference, : Θ. Thus, given dataset Di and architecture Ai, the trained produces tailored initialization ˆθi = H(Di, Ai). To enforce autoregressive parameter generation and capture layer-wise dependencies, we build IGPG based on VQGAN structure combined with transformer autoregressive prior. This design ensures coherent parameter generation by modeling dependencies between layers while leveraging the strengths of both architectures."
        },
        {
            "title": "2.2 NEURAL NETWORK PARAMETERS ENCODING WITH VQVAE",
            "content": "We encode neural network parameters using Gumbel Vector Quantized Variational Autoencoder (VQVAE) (van den Oord et al., 2017) to generate discrete representations suitable for autoregressive modeling. For parameter vector Θ RD, we employ fixed-size chunking with chunk size K, padding Θ to length = and splitting it into = chunks for efficient processing. The VQVAE architecture consists of an encoder E, decoder D, and quantization module with codebook = {e1, ..., em}. For input parameters θ, the encoder produces latent representations = E(θ), which are quantized using Gumbel-Softmax sampling: zq = (cid:88) j=1 yjej, yj = exp((log πj + gj)/τ ) i=1 exp((log πi + gi)/τ ) (cid:80)m (1) where πj are encoder logits, gj are Gumbel noise samples, and τ is the temperature parameter. The decoder reconstructs the input as ˆθ = D(zq). The model is optimized by minimizing: = (θ ˆθ)2 2 (cid:125) (cid:123)(cid:122) reconstruction (cid:124) +γ sg[zq]2 2 (cid:125) (cid:123)(cid:122) commitment (cid:124) +β sg[z] zq2 2 (cid:125) (cid:123)(cid:122) codebook (cid:124) (2) where masks padding values, sg[] denotes stop-gradient, and {β, γ} are balancing coefficients. This Gumbel-VQVAE formulation enables stochastic, differentiable quantization while preparing parameter vectors for subsequent autoregressive modeling using transformer architectures. 2.3 AUTOREGRESSIVE MODELING OF ENCODED PARAMETERS We design an autoregressive framework that conditions parameter generation on both dataset content and network architecture. For labeled dataset D, we sample balanced subset (e.g., five images per class) and embed each image using CLIP (Radford et al., 2021). Mean-pooling these embeddings yields the dataset-level vector eD. To encode network architecture A, we convert its specifications into standardized textual description desc(A) and process it with LLaMA-3-Instruct (Dubey et al., 2024), producing the architecture-level embedding eA. In the transformers forward pass, we concatenate eD, eA, and the VQVAE codebook embeddings to form unified representation for conditioning the autoregressive prior (e.g., GPT-2). Training Process: Following VQGAN (Esser et al., 2020), we employ transformer-based prior (mini-GPT (Radford et al., 2019)) for conditional sampling. For each pretrained model, its encoded tokens are gathered into single sequence representing the network. Our VQVAE is structured so that during training the autoregressive model can generate the full codebook in one pass based on next tokens prediction procedure where the context length is the length of the larger sequence vector. We train the GPT-based transformer to model the sequence likelihood and minimize the corresponding 3 Accepted at the ICLR Workshop on Neural Network Weights as New Data Modality 2025 loss in single formulation: Lprior = Esp(s,eA,eD) [log p(s eA, eD)] , where p(s eA, eD) = p(si s<i, eA, eD). (3) (cid:89) Equation equation 3 encapsulates our training objective, aligning generated parameter tokens with both dataset and architectural embeddings for coherent and efficient parameter synthesis."
        },
        {
            "title": "3 AUTOREGRESSIVE PARAMETER GENERATION",
            "content": "K We consider target architecture whose parameters are given by vector θA RL. To represent θA with manageable token sequence, we split it into = (cid:6) (cid:7) chunks, each of size K. learned VQ-VAE tokenizer maps each chunk from RK to sequence of tokens from discrete codebook V, i.e. : RK l. Consequently, the entire parameter vector θA can be expressed via token sequence of length kl. VQ-VAE decoder recovers real-valued parameter chunks from these tokens, : RK, and flattening operator reassembles the decoded chunks into the full parameter vector θA. Given maximum token-sequence length Nmax observed in training, we distinguish two modes of parameter generation. In the simpler scenario where kl Nmax, single-stage procedure suffices. Specifically, an autoregressive model takes as input an architecture embedding eA and dataset/task embedding eD collectively denoted by (eA, eD) and outputs sequence kl. Splitting into segments s1, . . . , sk, each of length l, and decoding them with reconstructs the parameter chunks. Finally, flattening these chunks with produces θA. For larger architectures, where kl > Nmax, we adopt chunk-wise autoregressive generation. Here, the model cannot generate all kl tokens at once without exceeding its maximum context size. Instead, we first generate an initial sequence s(1) Nmax via G(eA, eD). We then iteratively generate additional token blocks s(2), . . . , s(J), each conditioned on (eA, eD) and context window from the previously . Concatenating all blocks yields sfull NmaxJ , which we generated block, where = truncate to the first kl tokens if necessary. Finally, we split sfull into segments of length and decode each via to form the chunks in RK. Flattening these chunks with produces the full parameter vector θA. Thus, by leveraging chunked VQ-VAE representation and limiting each generation step to Nmax tokens, we enable parameter generation for arbitrarily large architectures. Whenever kl Nmax, single-step generation suffices; otherwise, we compose multiple chunks autoregressively. This design efficiently scales the generation process while maintaining the models capacity to represent high-dimensional parameter vectors. More details are provided in Algorithm 1. (cid:108) kl Nmax (cid:109)"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP Implementation Details. All experiments are performed on NVIDIA RTX V100 GPU. We train IGPG with AdamW and linear learning rate schedule, starting from 1 104. Datasets and Model Collection. We evaluate IGPG on broad suite of pretrained models gathered from public repositories, covering diverse architectures and datasets. This setup enables thorough examination of IGPGs performance across different model scales, and data settings. The instructions used to guide the weights generation consist of text description of the architecture combined with dataset embeddings. Evaluation Protocol We evaluate IGPG through three primary experiments: 1. Comparison with existing methods on the pretrained model zoo from Schurholt et al. (2022a) 2. Generalization assessment across diverse ResNet architectures 3. Parameter generation efficiency evaluation on architectures with varying parameter counts Baselines We compare IGPG against three state-of-the-art approaches: 4 Accepted at the ICLR Workshop on Neural Network Weights as New Data Modality 2025 Hyper- (Schurholt representations et al., 2022a)(SKDE: weights generation method that uses kernel density estimator(KDE) as prior. SANE (Schurholt et al., 2024): An improved version of (SKDE that uses weight tokenization with KDE prior. D2NWG (Soro et al., 2024): diffusion-based approach to neural network weight generation. 4.2 BENCHMARKING ON TINY MODEL ZOO Table 1: Comparison of weight initialization methods trained on pretrained from epochs 2125. We compare: (1) training from scratch (tr. fr. scratch), (2) SKDE30 (Schurholt et al., 2022b), (3) SANE with KDE30, (4) subsampled SANESUB (aligned with IGPG), and (5) D2NWG (Soro et al., 2024). Ep. Method MNIST SVHN CIFAR-10 STL 0 1 10 /% 10 /% 10 /% 10 /% 68.66.7 54.55.9 tr. fr. scratch SKDE30 SAN EKDE30 84.80.8 70.71.4 56.30.5 39.20.8 86.70.8 72.31.6 57.90.2 43.51.0 SAN ESU 80.520.82 66.60.7 58.800.1 44.500.1 D2NWG 83.200.01 67.100.4 58.30.1 44.410.1 IGPG n/a n/a 20.61.6 19.40.6 37.21.4 21.31.6 83.71.3 69.91.6 tr. fr. scratch SKDE30 SAN EKDE30 85.50.8 71.31.4 58.20.2 43.50.7 87.50.6 73.31.4 59.10.3 44.31.0 SAN ESU 87.80.4 73.61.3 59.20.3 44.80.2 D2NWG 86.100.1 74.00.3 58.70.3 44.90.1 IGPG n/a n/a 5 n/a n/a 36.75.2 23.54.7 48.51.0 31.64.2 92.40.7 57.312.4 tr. fr. scratch SKDE30 SAN EKDE30 87.50.7 72.21.2 58.80.4 45.20.6 89.00.4 73.61.5 59.60.3 45.30.9 SAN ESU 92.50.9 74.00.1 60.30.1 45.40.1 D2NWG 91.70.8 74.50.5 60.30.1 45.70.1 IGPG We evaluate IGPG on the Tiny Model Zoo dataset (Schurholt et al., 2022c), which comprises compact CNNs trained on MNIST, SVHN, CIFAR-10, and STL-10. Specifically, we use 2-layer CNN (2,464 parameters) for MNIST and SVHN, and 3-layer CNN (10,853 parameters) for CIFAR-10 and STL-10. Following prior work, we draw pretrained weights from epochs 2125 of 50epoch training schedule, with datasets split into training (70%), validation (15%), and test (15%). Unlike methods requiring separate models per dataset (Schurholt et al., 2022a; Schurholt et al., 2024), IGPG learns single generator that robustly handles all architectures and tasks. tr. fr. scratch SKDE30 SAN EKDE30 92.00.3 74.70.8 60.20.6 48.40.5 92.30.4 75.11.0 61.20.1 48.00.4 SAN ESU 96.20.3 75.70.5 64.11.0 48.70.5 D2NWG 94.50.1 76.90.1 63.90.0 49.060.2 IGPG 83.32.6 66.78.5 57.20.8 44.01.0 93.00.7 74.21.4 91.12.6 70.78.8 61.50.7 47.40.9 50 tr. fr. scratch n/a n/a 25 Task. We evaluate IGPGs ability to generate neural network parameters that remain effective under both fine-tuning and transfer learning scenarios. In particular, we seek to confirm that IGPGs synthesized weights are readily adaptable for fine-tuning scenarios. Results and Analysis. Table 1 shows that IGPG outperforms the sequential parameter generation method of Schurholt et al. (2024), while matching the rapid convergence characteristic of state-of-theart diffusion-based approaches. Crucially, IGPG preserves both zero-shot accuracy and fine-tuning performance when compared to previous works (Schurholt et al., 2024; Schurholt et al., 2022a). This highlights IGPGs capacity to generate robust initial weights that can be efficiently adapted, thereby accommodating multiple architectures and datasets within single unified framework. 4.3 TRANSFER LEARNING AND FINE-TUNING ON UNSEEN DATASETS In this section, we train single model on 30 diverse datasets from Meta-Album (Ullah et al., 2022), which span broad range of class distributions. We then sample parameters for CIFAR-10 and Oxford Pets, thus evaluating how well model pretrained on heterogeneous datasets adapts to unseen tasks. The training and target datasets are disjoint to ensure fair assessment. Because we fix the architecture (a MobileNetV3 subnet from OFA) and pretrained with images of size 224, we only encode dataset information rather than architectural details since the arhitecture is the same. Table 6 lists the training datasets. As shown in Figure 2, our sampled models begin at performance level on par with random initialization but achieve over 50% relative improvement 5 Accepted at the ICLR Workshop on Neural Network Weights as New Data Modality 2025 Figure 2: Transfer learning evaluation on novel datasets: CIFAR100, CIFAR10, Aircraft30, and PETS10 compared to random initialization. within one epoch. This underscores the advantage of leveraging broad pretraining data for faster adaptation: although zero-shot performance may start near baseline, it quickly surpasses random initialization, reflecting the effectiveness of our method in generating meaningful parameters. 4.4 CROSS-ARCHITECTURE BENCHMARKING We evaluated our instruction-guided parameter generation on CIFAR-10 using 125 randomly sampled ResNet-56 variants spanning 200k700k parameters, with block configurations from [4,4,4] to [8,8,8]. Each model was trained for 100 epochs, and we collected the last five epochs weights to form our training set. We set the maximum token-sequence length to 768 and trained VQ-VAE to encode these parameters. Next, we employed GPT-2 model, conditioned by an instruction template (preprocessed via LLaMA3-1-8B-Instruct), to generate the codebook tokens. We also experimented with fine-tuning larger language models on these codebooks, observing that while minor token mismatches (e.g. non-integers) can occur, the approach remains feasible. We then tested five ResNet architectures, including two in-distribution variants (directly sampled) and three out-of-distribution (ResNet-20, ResNet-56, and ResNet-110). Figure 3 compares IGPG against random initialization and CIFAR-100-pretrained weights. On in-distribution architectures, IGPG achieves accuracy on par with pretrained baselines. For out-of-distribution networks, our method attains up to 64% accuracy on ResNet-20 and 46% on both ResNet-56 and ResNet-110, outperforming the other baselines. These results highlight IGPGs strong cross-architecture generalization without requiring additional fine-tuning, underscoring the potential of instruction-guided parameter generation in handling unseen network configurations. 4.5 HANDLING DIVERSE PRETRAINED MODELS FROM VARIED DATASETS We further demonstrate IGPGs versatility by encoding broad set of architectures pretrained on datasets with varying numbers of classes (CIFAR-10 vs. CIFAR-100). We gather 19 publicly available models from GitHub2 spanning ResNet, ShuffleNet, MobileNet, and others. These architectures range from 0.27M to 27M parameters (over 100 difference), as shown in Figure 5. Experimental Setup. We train VQ-VAE with chunk size of 2694256 parameters, each encoded into 64 tokens. For the largest models (roughly 10 chunks), this translates into maximum sequence 2https://github.com/chenyaofo/pytorch-cifar-models Accepted at the ICLR Workshop on Neural Network Weights as New Data Modality 2025 Figure 3: Performance evaluation with seen and unseen ResNet architectures on CIFAR-10 against models pretrained on CIFAR-100 and Random Initialization. (a) CIFAR10 (b) CIFAR100 Figure 4: Comparison of IGPGs conditional sampled weight based initialization versus pretrained models across diverse architectures on CIFAR10 and CIFAR100 length of 640 tokens for our autoregressive model. We condition on both architecture descriptions and dataset encodings (via CLIP image encoder using five images per class). Results and Significance. Figure 4 reports near-perfect Pearson correlations for both CIFAR-10 (0.9999) and CIFAR-100 (0.9991), suggesting that our generated parameters track closely with original pretrained weights. The regression lines for each dataset align closely with = x, indicating comparable performance between IGPG-generated weights and their pretrained counterparts. These findings highlight IGPGs capacity to learn from diverse architecturedataset pairs and produce parameters that faithfully approximate original performance, pointing to its potential for guiding model selection and fast adaptation under datasetor instruction-based constraints. 4.6 EXTENSION TO DIVERSE LORA PARAMETERS GENERATION To demonstrate that IGPG can learn distributions of diverse LoRA modules and improve downstream performance, we evaluated it on six standard computer vision benchmarks: Oxford-IIIT Pets, Stanford Cars, CIFAR-10, EuroSAT, the Describable Textures Dataset (DTD), and FGVC Aircraft. We began by fine-tuning Vision Transformer (ViT-Base) with LoRA modules, following the procedure of Gao et al. (2024). For each dataset, we retained the top five performing models and learned their parameter distributions. We then used specialized version of IGPG to explore these distributions and generate novel LoRA parameters. 7 Accepted at the ICLR Workshop on Neural Network Weights as New Data Modality 2025 Table 2: Performance evaluation on LoRA weights generation conditioned on the dataset Model Method # Trainable Parameters OxfordPets StanfordCars CIFAR10 DTD EuroSAT FGVC Average LP FF LoRA FourierFT(Gao et al., 2024) - IGPG - 90.280.43 85.8M 93.140.40 581K 93.190.36 72K 93.210.26 72K 92.840.30 25.760.28 96.410.02 69.770.67 88.720.13 17.440.43 64.73 79.781.15 98.920.05 77.681.21 99.050.09 54.841.23 83.90 45.380.41 98.780.05 74.950.40 98.440.15 25.160.16 72.65 46.110.24 98.580.07 75.090.37 98.290.04 27.510.64 73.13 57.670.52 98.450.95 88.740.08 98.700.23 36.630.11 78. Table 3: Learning distribution of combined ViT-Small (CIFAR-10, CIFAR-100, CINIC-10, SVHN, Tiny-ImageNet) and MobileNetV3-Small (ImageNet). We report model size (Parameters), Pretrained accuracy, VQVAE Reconstruction accuracy, and the best among five samples (Best Sampled Weights). Dataset Architecture Parameters Pretrained (%) VQVAE Reconstruction (%) IGPG(%) ViT-Small CIFAR-10 ViT-Small CIFAR-100 ViT-Small CINIC-10 ViT-Small SVHN Tiny-ImageNet ViT-Small ImageNet MobileNetV3-Small 2697610 2714980 2697610 2697610 2771144 2542856 93.67 72.29 83.23 97.74 53.44 67.67 93.67 72.28 83.23 97.74 53.44 67.66 93.70 72.30 83.24 97.76 53.45 67. Table 2 shows that our generated LoRA parameters deliver up to 10% improvement compared to the baseline pretrained model (Gao et al., 2024), highlighting the efficacy of our distribution-learning approach in uncovering higher-performing LoRA configurations within the existing parameter space. 4.7 LEARNING DISTRIBUTION OF MODELS PRETRAINED ON DIVERSE DATASETS To demonstrate that IGPG can learn distribution of multiple model pretrained on both large, medium and small scale dataset while maintaining higher performance, we collect pretrained weights of ViT and Mobilenetv3 small pretrained on various datasets including ImageNet-1k(see Table 3) to train our method. Experiment Setup and Findings. We train VQ-VAE on combined weights from diverse datasets, and the transformer on its codebook using 5 samples per class and architecture descriptions as instructions, then evaluate both reconstruction and autoregressive sampling in-distribution. Table 3 reports the results as follows: Pretrained (%): Accuracy of the original pretrained weights. VQVAE Reconstruction (%): Accuracy after encoding and decoding the pretrained weights via VQVAE, showing the fidelity of our compressive representation. Best Sampled Weights (%) with IGPG: Accuracy of the best sample among five randomly drawn sequences from our trained model, conditioned on the respective dataset and architecture. For ViT-Small across CIFAR-10, CIFAR100, CINIC-10, SVHN, and Tiny-ImageNet, we observe that both VQ-VAE reconstruction and IGPG sampled weights attain accuracy nearly indistinguishable from the original pretrained models. Moreover, the best samples often match or slightly exceed the baseline. On MobileNetV3-Small, sampled weights remain competitive with the pretrained baseline. These results confirm that IGPG preserves performance while offering the flexibility to sample diverse parameter configurations. Additional results are presented in the Appendix and Ablation in Appendix D."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduced IGPG, an instruction-guided framework for neural network parameter generation that combines VQVAE with autoregressive modeling. IGPG generates network parameters conditioned on task descriptions and architectural specifications. Experimental results demonstrate that IGPG achieves performance comparable to that of pretrained models while converging faster than random initialization. Furthermore, our approach effectively compresses large pretrained datasets and generalizes across diverse architectures, thereby advancing the fields of neural architecture search and transfer learning. 8 Accepted at the ICLR Workshop on Neural Network Weights as New Data Modality"
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was supported by Institute for Information & communications Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT) (RS-2019-II190075, Artificial Intelligence Graduate School Program(KAIST)) and (No.RS-2022-II220713, Meta-learning Applicable to Real-world Problems), by Samsung Research Funding Center of Samsung Electronics (No. IO201210-08006-01), Institute of Information & communications Technology Planning & Evaluation (IITP) under Open RAN Education and Training Program (IITP-2024-RS-2024-00429088) grant funded by the Korea government(MSIT) and, by National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2023-00256259)"
        },
        {
            "title": "REFERENCES",
            "content": "Justin Zhao Timothy Wang Wael Abid Geoffrey Angus Arnav Garg Jeffery Kinnison Piero Molino Travis Addair and Devvret Rishi. 310 fine-tuned llms that rival gpt-4, technical report, April 2024. URL https://predibase.com/blog/ lora-land-fine-tuned-open-source-llms-that-outperform-gpt-4. Lora land: Lior Deutsch. Generating neural networks with neural networks. ArXiv, abs/1801.01952, 2018. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei 9 Accepted at the ICLR Workshop on Neural Network Weights as New Data Modality Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzman, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, IrinaElena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vıtor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: survey. ArXiv, abs/1808.05377, 2019. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis, 2020. Ziqi Gao, Qichao Wang, Aochuan Chen, Zijing Liu, Bingzhe Wu, Liang Chen, and Jia Li. Parameterefficient fine-tuning with discrete fourier transform, 2024. Jianping Gou, Baosheng Yu, Stephen J. Maybank, and Dacheng Tao. Knowledge distillation: survey. International Journal of Computer Vision, 129(6):17891819, mar 2021. 10 Accepted at the ICLR Workshop on Neural Network Weights as New Data Modality 2025 Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770778, 2016. Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. Lorahub: Efficient cross-task generalization via dynamic lora composition, 2023. Boris Knyazev, Michal Drozdzal, Graham W. Taylor, and Adriana Romero-Soriano. Parameter prediction for unseen deep architectures. ArXiv, abs/2110.13100, 2021. William S. Peebles, Ilija Radosavovic, Tim Brooks, Alexei A. Efros, and Jitendra Malik. Learning to learn with generative models of neural network checkpoints. ArXiv, abs/2209.12892, 2022. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. Neale Ratzlaff and Li Fuxin. HyperGAN: generative model for diverse, performant neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 53615369. PMLR, 0915 Jun 2019. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115 (3):211252, 2015. doi: 10.1007/s11263-015-0816-y. Konstantin Schurholt, Boris Knyazev, Xavier Giro Nieto, and Damian Borth. Hyper-representations as generative models: Sampling unseen neural network weights. In Advances in Neural Information Processing Systems, 2022a. Konstantin Schurholt, Boris Knyazev, Xavier Giro Nieto, and Damian Borth. Hyper-representation for pre-training and transfer learning. In First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML 2022, 2022b. Konstantin Schurholt, Diyar Taskiran, Boris Knyazev, Xavier Giro Nieto, and Damian Borth. Model zoos: dataset of diverse populations of neural network models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022c. Konstantin Schurholt, Michael W. Mahoney, and Damian Borth. Towards scalable and versatile weight space learning. In Proceedings of the 41st International Conference on Machine Learning (ICML), 2024. Konstantin Schurholt, Dimche Kostadinov, and Damian Borth. Self-supervised representation learning on neural network weights for model characteristic prediction. In Advances in Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia, 2021. Yang Shu, Zhi Kou, Zhangjie Cao, Jianmin Wang, and Mingsheng Long. Zoo-tuning: Adaptive transfer from zoo of models. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 96269637. PMLR, 1824 Jul 2021. Bedionita Soro, Bruno Andreis, Hayeon Lee, Song Chong, Frank Hutter, and Sung Ju Hwang. Diffusion-based neural network weights generation, 2024. Kenneth O. Stanley and Risto Miikkulainen. Evolving neural networks through augmenting topologies. Evolutionary Computation, 10:99127, 2002. Zihao Tang, Zheqi Lv, Shengyu Zhang, Fei Wu, and Kun Kuang. Modelgpt: Unleashing llms capabilities for tailored model generation, 2024. Accepted at the ICLR Workshop on Neural Network Weights as New Data Modality 2025 Ihsan Ullah, Dustin Carrion, Sergio Escalera, Isabelle Guyon, Mike Huisman, Felix Mohr, Jan van Rijn, Haozhe Sun, Joaquin Vanschoren, and Phan Anh Vu. Meta-album: Multi-domain meta-dataset for few-shot image classification. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://meta-album. github.io/. Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NIPS, 2017. Kai Wang, Zhaopan Xu, Yukun Zhou, Zelin Zang, Trevor Darrell, Zhuang Liu, and Yang You. Neural network diffusion, 2024. Ross Wightman. Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019. Jian Yang, Gang Xiao, Yulong Shen, Wei Jiang, Xinyu Hu, Ying Zhang, and Jinghui Peng. survey of knowledge enhanced pre-trained models. ArXiv, abs/2110.00269, 2021. Chris Zhang, Mengye Ren, and Raquel Urtasun. Graph hypernetworks for neural architecture search. ArXiv, abs/1810.05749, 2019. Ziyu Zhao, Leilei Gan, Guoyin Wang, Yuwei Hu, Tao Shen, Hongxia Yang, Kun Kuang, and Fei Wu. Retrieval-augmented mixture of lora experts for uploadable machine learning, 2024. Andrey Zhmoginov, Mark Sandler, and Max Vladymyrov. Hypertransformer: Model generation for supervised and semi-supervised few-shot learning. ArXiv, abs/2201.04182, 2022. 12 Accepted at the ICLR Workshop on Neural Network Weights as New Data Modality 2025 Limitations: key limitation of our method is its reliance on training with large, diverse set of pretrained modelsa comprehensive public repository of such models does not yet exist. However, this challenge is increasingly mitigated by the growing availability of models from repositories like Hugging Face and by efficient fine-tuning techniques such as LoRA."
        },
        {
            "title": "A OVERVIEW",
            "content": "A.0.1 VECTORIZING NEURAL NETWORK PARAMETERS To enable our generative mapping function to learn from diverse pretrained models, we introduce standardized parameter vectorization scheme that transforms weights and biases into uniform vector representation. For network with layers, fully connected layers are vectorized by reshaping the weight matrix θ(l) Rdl1dl into vec(θ(l)) Rdl1dl and appending the bias b(l) Rdl , yielding dl1dl + dl elements. Similarly, convolutional layers with kernel θ(l) Rkhkwcincout are flattened to vec(θ(l)) Rkhkwcincout and concatenated with the bias b(l) Rcout to produce khkwcincout + cout elements. We consider two aggregation strategies: an architecture-wise vectorization that concatenates (cid:2)vec(θ(l)) b(l)(cid:3), and layer-wise encoding that preserves all layers into single vector Θ = (cid:76)L each layers representation. l=1 A.1 MODEL OVERVIEW Our VQVAE model is modified implementation based on the VQGAN codebase. Table 4 summarizes the architecture details, while Table 5 provides an overview of the generative model along with its parameter counts. We optimize the model using Adam with learning rate of 1 104 and employ cyclical temperature schedule for the Gumbel-Softmax, annealing the temperature from 1 to 1 104. During both training and inference, when image dataset encoding is unnecessary, the model conditions solely on the architecture; otherwise, it leverages both architectural and dataset embeddings. We also illustrate an example template 5 for experimental results in Figure 3, where only architecture embeddings are used since all architectures were pretrained on the same dataset. Index 0 1 2 3"
        },
        {
            "title": "Name\nencoder\ndecoder\nloss\nquantize\nquant conv\npost quant conv",
            "content": "Type Encoder Decoder VQLoss GumbelQuantize Conv2d Conv2d Parameters 197 198 0 132 4.2 4.2 Table 4: Pretrained weights configuration for the VQVAE model. The table details the layer-wise parameters of the VQVAE model, highlighting its encoder, decoder, and quantization components optimized for downstream tasks. Index 0 1"
        },
        {
            "title": "Type\nGumbelVQNoDisc\nCondStage\nIdentity\nGPT",
            "content": "Parameters 48.5 524 0 27.8 Table 5: Layer-wise configuration of the model, including the first-stage model, conditional stage model, and transformer components. The table specifies the type and parameter count of each component. This table shows the structure of the model used for the experiments in Section 4.2 Remark 1. Instruction: You are codebook generator. Input: Generate the full codebook of length 512 for ResNet with: num blocks = [6, 7, 6]. Output:[995, 66, 750, 391, 809, 830,...]. 1 Accepted at the ICLR Workshop on Neural Network Weights as New Data Modality 2025 Algorithm 1 Autoregressive Parameter Generation Require: Architecture encoding eA, dataset encoding eD, parameter length L, chunk size Ensure: Architecture parameters θA RL 1: L/K 2: if kl Nmax then G(eA, eD) 3: for 1 to do 4: si s(i1)l:il 5: θi D(si) 6: 7: 8: else 9: 10: end for Number of chunks Single-pass generation Split into chunks Decode chunks Initial generation s(1) G(eA, eD) for 2 to kl/Nmax do s(j) G(eA, eD, s(j1) ctx ) end for sfull concat(s(1), . . . , s(j))1:kl for 1 to do 11: 12: 13: 14: 15: 16: 17: 18: end if 19: θA F([θ1; . . . ; θk]) 20: return θA end for si (sfull)(i1)l:il θi D(si) Flatten chunks"
        },
        {
            "title": "B ADDITIONAL RESULTS AND TABLES",
            "content": "Domain Original Dataset Name Number of Classes Large Animals Small Animals Plants Plant Diseases Microscopy Remote Sensing Vehicles Manufacturing Human Actions OCR Birds Dogs Animals with Attributes Plankton Insects 2 Insects Flowers PlantNet Fungi Plant Village Medicinal Leaf PlantDoc Bacteria PanNuke Subcellular Human Protein RESISC RSICB RSD Cars Airplanes Boats Textures Textures DTD Textures ALOT Sports Stanford 40 Actions MPII Human Pose Omniprint-MD-mix Omniprint-MD-5-bis Omniprint-MD-6 315 120 102 102 117 102 25 25 38 26 27 33 19 21 45 45 43 196 21 64 47 250 73 40 29 706 706 703 Table 6: Diverse pretrained datasets grouped by domain, showcasing their variety in classes for cross-dataset adaptation tasks. In this section, we provide comprehensive report of the experimental results such as those presented in section 4.5. The primary objective of these results is to demonstrate the ability to encode diverse Accepted at the ICLR Workshop on Neural Network Weights as New Data Modality 2025 Figure 5: Parameters distribution of diverse architectures pretrained on CIFAR-10 and CIFAR100 all jointly encoded by IGPG architectures across variety of datasets while maintaining retrieval performance comparable to the pretrained models. Table 9 showcases representative results for different architectures, along with their corresponding parameter counts. The conditioning has been applied to both architectural configurations and datasets. Additionally, we include example configuration samples used for instruction encoding in Table 7. Table 7: Examples of architecture configurations for various neural network families. Architecture Name Configuration vgg11 bn mobilenetv2 x0 5 repvgg a0 shufflenetv2 x0 5 resnet architecture: A, layers: [64, M, 128, M, 256, 256, M, 512, 512, M, 512, 512, M], batch norm: true width multiplier: 0.5, final layer channels: 640 width multiplier: [0.75, 0.75, 0.75, 2.5] stages repeats: [4, 8, 4], stages out channels: [24, 48, 96, 192, 1024] layers: [3, 3, 3]"
        },
        {
            "title": "C RELATED WORK",
            "content": "Neural Network Parameter Generation Parameter generation for neural networks has evolved along two main trajectories: hypernetwork-based generation and generative hyper-representation learning. Hypernetwork approaches generate weights from scratch (Stanley & Miikkulainen, 2002; Ratzlaff & Fuxin, 2019; Deutsch, 2018) , with recent advances in graph-hypernetworks (Zhang et al., 2019; Knyazev et al., 2021) and transformer architectures (Zhmoginov et al., 2022). However, these methods primarily serve as initialization techniques requiring subsequent optimization. In contrast, generative hyper-representation learning focuses on modeling distributions of pretrained weights. Recent work has explored adaptive weight transfer (Shu et al., 2021), learning distribution of pretrained weights (Schurholt et al., 2022b), and diffusion-based generation (Peebles et al., 2022) which demonstrated promising results. Our current work extend these work to autoregressive generation preserving inter-layer relation in the network. Applications and Implications Weight generation, particularly from pretrained distributions, offers several key advantages in modern deep learning such as enabling efficient model compression and 3 Accepted at the ICLR Workshop on Neural Network Weights as New Data Modality 2025 Table 8: Performance comparison of conditional sampling with chunk-based and chunk-free approach. C-10 and C-100 refer to respectively CIFAR-10 and CIFAR-100 datasets MODELS #PARAMS TOP-1 TOP-5 TOP-1 TOP-5 TOPTOP5 DATASETS PRETRAINED CHUNK-FREE CHUNK-BASED RESNET-56 RESNET-56 MOBILENETV2 X0 860026 865876 834324 94.37 72.63 70.88 99.83 91.94 91.72 94.400.01 72.450.01 71.140.01 99.820.00 91.960.01 92.480.04 94.190.01 72.41 0.2 70.93 0. 99.87 0.00 91.80 0.03 92.55 0.02 C-10 C-100 C-100 serving for large language models by enabling the generation of task-specific adaptations (Huang et al., 2023; Addair & Rishi, 2024) or act as parameters retrieval-based systems similar to Zhao et al. (2024). Second, it enhances transfer learning by enabling task-adaptive sampling (Tang et al., 2024), streamlining model adaptation."
        },
        {
            "title": "D ABLATION STUDY",
            "content": "Full Model vs Layer-Wise Sampling We compare two parameter generation approaches: full model-wise encoding and layer-wise encoding. In layer-wise encoding, we assemble parameters from each layer into separate training datasets, applying chunking and padding per layer. While both methods perform well on in-distribution parameters, layer-wise encoding shows superior generalization to novel architectures, suggesting better adaptability and robustness. Impact of Chunking Strategy We evaluate the effect of chunking network weights versus using complete weight vectors. Using pretrained weights from PyTorchHub3, we assess ResNet56 and MobileNetV2 on CIFAR-10 and CIFAR-100. Results in Table 8 show that while chunking offers no significant advantage for medium-sized architectures, it becomes crucial for larger models where chunk-free approaches struggle to maintain performance. LLM-based Parameter Generation We investigate whether instruction-tuned LLMs can generate neural network parameters directly. Using LLaMA-3.2-1B-Instruct with LoRA fine-tuning and GPT-2 trained on sequence-to-sequence codebook generation, we find mixed results. While LLaMA-3.2-1B accurately generates initial tokens, it struggles with longer sequences. Similarly, GPT-2 with top-k sampling (k=1) successfully matches pretrained codebooks for small parameter sets but degrades significantly beyond 1024 dimensions. These results indicate that LLMs can generate VQVAE codebook parameters for small models, but scalability remains significant challenge. D.1 NEURAL NETWORK CODEBOOK GENERATION USING CHAT MODELS We conducted several attempts to generate neural network codebooks using large language models (LLMs) optimized for chat-based interactions. Our investigation with LLaMA-3.2-1B revealed significant challenges in adapting chat models to generate neural network parameters without pretraining on the specific codebook data. For instance, training GPT-2-small in sequence-to-sequence fashion with the codebook, followed by instruction tuning, enabled the model to successfully generate correct codebook in 96.6 However, simply applying LoRA tuning to an instruction-tuned LLaMA model resulted in the generation of short and inconsistent codebooks, often with mixed or incomplete outputs. These findings highlight the limitations of current chat models in directly producing neural network parameters without substantial fine-tuning or pretraining on task-specific data. In future work, we aim to explore this problem more comprehensively, focusing on improving parameter generation capabilities with chat models. Most of our experiments to date utilized GPT-2 variants, and we plan to expand this investigation with other LLM architectures. 3https://github.com/chenyaofo/pytorch-cifar-models 4 Accepted at the ICLR Workshop on Neural Network Weights as New Data Modality 2025 Table 9: Performance of weights generation from diverses models pretrained on CIFAR-10 and CIFAR-100 datasets CIFARCIFAR-100 IGPG Pretrained Params (M) IGPG Pretrained Params (M)"
        },
        {
            "title": "Model",
            "content": "MobileNetV2 (0.5) MobileNetV2 (0.75) MobileNetV2 (1.0) MobileNetV2 (1.4) Average (MobileNetV2) RepVGG-A0 RepVGG-A1 RepVGG-A2 Average (RepVGG) ResNet-20 ResNet-32 ResNet-44 ResNet-56 Average (ResNet) ShuffleNetV2 (0.5) ShuffleNetV2 (1.0) ShuffleNetV2 (1.5) ShuffleNetV2 (2.0) Average (ShuffleNetV2) VGG11-BN VGG13-BN VGG16-BN VGG19-BN Average (VGG) 93.16 94.16 93.98 94.26 93. 94.44 94.93 95.36 94.91 92.47 93.47 93.92 94.56 93.61 90.61 92.75 93.58 94.02 92.74 92.85 94.01 94.19 94.09 93.79 Global Average (All Models) 93. 0.70 1.37 2.24 4.33 2.16 7.84 12.82 26.82 15.83 0.27 0.47 0.66 0.86 0.57 0.35 1.26 2.49 5.37 2.37 9.76 9.94 15.25 20.57 13.88 6. 71.05 74.15 74.41 75.97 73.90 75.32 76.51 77.52 76.45 68.92 69.98 71.39 72.43 70.68 67.90 72.55 74.22 75.39 72.52 70.76 74.40 73.98 73.82 73.24 73. 71.14 74.06 74.30 76.29 73.95 75.29 76.45 77.50 76.41 68.84 70.15 71.64 72.60 70.81 67.80 72.59 74.22 75.49 72.53 70.78 74.62 74.00 73.87 73.32 73. 0.82 1.48 2.35 4.50 2.29 7.96 12.94 26.94 15.95 0.28 0.47 0.67 0.86 0.57 0.44 1.36 2.58 5.55 2.48 9.80 9.99 15.30 20.61 13.93 7. 93.12 94.08 94.05 94.22 93.87 94.47 94.93 95.27 94.89 92.59 93.53 94.01 94.37 93.63 90.65 93.30 93.57 93.99 92.88 92.78 94.00 94.15 93.91 93.71 93."
        }
    ],
    "affiliations": [
        "DeepAuto.ai, South Korea",
        "KAIST AI"
    ]
}