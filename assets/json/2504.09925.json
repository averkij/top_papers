{
    "paper_title": "FUSION: Fully Integration of Vision-Language Representations for Deep Cross-Modal Understanding",
    "authors": [
        "Zheng Liu",
        "Mengjie Liu",
        "Jingzhou Chen",
        "Jingwei Xu",
        "Bin Cui",
        "Conghui He",
        "Wentao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce FUSION, a family of multimodal large language models (MLLMs) with a fully vision-language alignment and integration paradigm. Unlike existing methods that primarily rely on late-stage modality interaction during LLM decoding, our approach achieves deep, dynamic integration throughout the entire processing pipeline. To this end, we propose Text-Guided Unified Vision Encoding, incorporating textual information in vision encoding to achieve pixel-level integration. We further design Context-Aware Recursive Alignment Decoding that recursively aggregates visual features conditioned on textual context during decoding, enabling fine-grained, question-level semantic integration. To guide feature mapping and mitigate modality discrepancies, we develop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a Synthesized Language-Driven Question-Answer (QA) dataset through a new data synthesis method, prioritizing high-quality QA pairs to optimize text-guided feature integration. Building on these foundations, we train FUSION at two scales-3B, 8B-and demonstrate that our full-modality integration approach significantly outperforms existing methods with only 630 vision tokens. Notably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most benchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited to 300 vision tokens. Our ablation studies show that FUSION outperforms LLaVA-NeXT on over half of the benchmarks under same configuration without dynamic resolution, highlighting the effectiveness of our approach. We release our code, model weights, and dataset. https://github.com/starriver030515/FUSION"
        },
        {
            "title": "Start",
            "content": "FUSION: Fully Integration of Vision-Language Representations for Deep Cross-Modal Understanding Zheng Liu1,2*, Mengjie Liu1,2, Jingzhou Chen1,2, Jingwei Xu3, Bin Cui1, Conghui He2, Wentao Zhang1 1Peking University, 2Shanghai AI Laboratory, 3Nanjing University 5 2 0 2 4 1 ] . [ 1 5 2 9 9 0 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce FUSION, family of multimodal large language models (MLLMs) with fully vision-language alignment and integration paradigm. Unlike existing methods that primarily rely on late-stage modality interaction during LLM decoding, our approach achieves deep, dynamic integration throughout the entire processing pipeline. To this end, we propose Text-Guided Unified Vision Encoding, incorporating textual information in vision encoding to achieve pixel-level integration. We further design ContextAware Recursive Alignment Decoding that recursively aggregates visual features conditioned on textual context during decoding, enabling fine-grained, question-level semantic integration. To guide feature mapping and mitigate modality discrepancies, we develop Dual-Supervised Semantic Mapping Loss. Additionally, we construct Synthesized Language-Driven Question-Answer (QA) dataset through new data synthesis method, prioritizing highquality QA pairs to optimize text-guided feature integration. Building on these foundations, we train FUSION at two scales3B, 8Band demonstrate that our full-modality integration approach significantly outperforms existing methods with only 630 vision tokens. Notably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most benchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited to 300 vision tokens. Our ablation studies show that FUSION outperforms LLaVA-NeXT on over half of the benchmarks under same configuration without dynamic resolution, highlighting the effectiveness of our approach. We release our code, model weights, and dataset. https://github.com/starriver030515/FUSION 1. Introduction Research has long shown that human visual perception is not passive process like camera capturing reality but rather an active, interpretative mechanism shaped by cognitive factors such as language and environmental context. critical component of this mechanism is the dynamic in- *Email: lz030515123@gmail.com Corresponding Author 1 Figure 1. Performance comparison of FUSION with leading MLLM models across 18 benchmark dimensions. With only 630 vision tokens, our model (FUSION-X) significantly outperforms Cambrian-1 and Florence-VL, achieving overall parity with LLaVA-OneVision, while maintaining minimal performance gap with top-tier models such as InternVL2 and Qwen2VL. Furthermore, even when the number of vision tokens is reduced to 300, our model (FUSION-L) preserves 95% of its original performance, remaining on par with Florence-VL. teraction between vision and language. In [22], the authors demonstrated that hearing the name of target object before searching for it significantly improved both the speed and accuracy of object detection. This finding underscores the deep integration of vision and language, suggesting that linguistic input facilitating more effective visual processing by directing attention and helping the brain prioritize relevant features. Despite the fundamental role of vision-language interaction in human perception, current MLLMs often fail to capture this relationship effectively. In traditional MLLMs[9, 43], visual information is processed independently through Figure 2. Visualization of modality alignment and integration. At pixel-level, we compute attention maps between image regions and question-relevant keywords within the vision encoder. At space-level, we measure the cosine similarity between vision tokens projected into the LLM embedding space and corresponding keywords. At question-level, we visualize attention maps from question keywords to vision tokens during LLM decoding. The results indicate that our model achieves consistent and progressively enhanced cross-modal alignment throughout the processing pipeline. isolated vision encoders. These models typically treat vision features as static information, fusing them with language only at later LLM decoding stage. This decoupled approach results in lack of centralized encoding of vision features, leading to limited useful information available for textual interaction. Recent advancements[4, 12, 37, 42, 44, 60, 64] have sought to improve visual comprehension capability through techniques such as dynamic resolution encoding or multiple vision encoders, which enhances image details and provides richer representations. However, these improvements predominantly focus on enhancing visual encoding itself, overlooking the deeper, bidirectional integration necessary for effective vision-language fusion. Furthermore, relying solely on vision encoders or more image tokens introduces redundancy and inefficiencies, increasing computational costs while failing to establish truly interactive fusion between modalities. This challenge serves as the core motivation for our work: achieving deep, adaptive, and dynamic interaction and integration between vision and language throughout the entire multimodal processing pipeline. Previous studies have explored varied integration strategies, such as text-based query integration[14], text-oriented vision encoding[20], and entity-enhanced modality alignment[75]. Nevertheless, these models lack systematic, comprehensive framework for aligning and integrating visual and textual modalities. key unresolved issue is embedding misalignment, where inherent discrepancies between vision and text embeddings hinder seamless integration. Moreover, current models lack high-quality training data specifically designed to guide the alignment and integration process. As result, these methods fall short of achieving SOTA performance. In this paper, we push the boundaries of vision-language fusion by proposing sophisticated framework that realizes deeper, more interactive integration of visual and textual modalities. Unlike prior approaches that treat vision and language as separate entities, our approach consistently integrates textual guidance throughout the visual encoding phase. We introduce supervised guidance in modality space mapping and dynamically aggregate vision features based on textual context during LLM decoding. Additionally, we introduce new data synthesis method that generates QA pairs from text, guiding the model toward more efficient feature integration. Our method enables more cognitively inspired multimodal learning paradigm. Overall, our contributions are as follows: Text-Guided Unified Vision Encoding: We project text embeddings into the visual feature space and jointly perform attention with visual representations to enhance pixel-level integration. Context-Aware Recursive Alignment Decoding: We introduce interaction layers that recursively aggregate visual features based on textual context during LLM decoding, enabling fine-grained, question-level alignment and integration. Dual-Supervised Semantic Mapping Loss: We propose novel bidirectional reconstruction loss, ensuring robust consistency and mitigating semantic discrepancies between modalities. Synthesized Language-Driven QA Dataset: We deFigure 3. Illustration of our Text-Guided Unified Vision Encoding and Dual-Supervised Semantic Mapping Loss. Given an input image, the corresponding question is first projected into the vision feature space and processed jointly with the image. The extracted visual features are then mapped into the text space and fed into LLM. To ensure the reliability of the mapping MLP, we reconstruct the text and image tokens by reusing the encoded tokens and projecting them back into their original feature spaces, then compute the similarity between the reconstructed and raw tokens to encourage structural alignment between the two spaces. velop new method that prioritizes constructing highquality QA pairs while leveraging generative models to produce images. We synthesize large-scale, diverse QA dataset to optimize text-guided integration. Building on our exploration, we present new family of MLLMs that achieve leading performance across diverse benchmarks. To foster further research, we release our code, model weights, and data. We hope that our work will empower the research community and accelerate advancements in MLLMs. 2. Preliminaries and Related Work Multimodal LLMs Multimodal Large Language Models (MLLMs) [9, 43, 44] are designed to process and integrate information from multiple modalities, primarily combining visual and textual data. Typically, these models are structured with three main components: vision encoder, projection module, and large language model (LLM). Images are first divided into patches and processed through vision encoders to extract visual representations. These visual features are then mapped into textual embedding space via projector before being combined with textual data during the language models decoding phase. Recent advances [4, 37, 39, 40, 57, 84] have explored dynamic or arbitrary resolution techniques, significantly enhancing the precision and adaptability of visual encoding. Meanwhile, alternative approaches [42, 60, 64] have employed multiple vision encoders to capture more comprehensive set of visual features. Despite these innovations, most current methodologies primarily emphasize visual encoding quality, often neglecting deeper and more interactive vision-language dynamics, thus hindering the full potential of multimodal learning. Modality Alignment and Integration To enhance multimodal integration, some studies have investigated improved methods at various stages within MLLMs. For instance, [14] integrates linguistic guidance by embedding textual queries within the QFormer module, facilitating languagedriven visual feature projection. [75] enhances modality alignment by incorporating contrastive loss in the projector module during pretraining to better associate visual features with entity representations. More recently, [8] introduced generative vision encoder that leverages multiple textual prompts to integrate various feature types, achieving SOTA performance. But fundamental challenge remainsthe inherent difference between visual and textual feature spaces makes direct feature alignment difficult, often leading to inconsistencies in multimodal representations. Instruction Tuning Data High-quality instruction tuning data is crucial factor in the training and generalization capabilities of MLLMs. Existing works[37, 40, 60, 64] have constructed training recipes with visual question answering (VQA) benchmarks. Other efforts[7, 66, 71] lever3 Figure 4. Illustration of our Context-Aware Recursive Alignment Decoding. For each set of question tokens (highlighted in yellow), we prepend set of context-aware latent tokens (highlighted in green). Additional interaction layers are introduced between decoding layers, where vision tokens interact with both latent tokens and question tokens at question-level granularity (e.g., Group 1, Group 2, . . . ). aged GPT-4V to generate high-quality QA pairs from images. These datasets enhance multimodal understanding but are still predominantly vision-centered, where QA pairs are constructed primarily based on image content. Yet, the inherent limitations of image content restrict their capacity to support complex, instruction-following tasks across diverse domains. promising alternative [48] attempts to address this limitation by synthesizing high-quality captions first and employing diffusion models to generate corresponding images. Despite its promise, this approach is still constrained to the pretraining stage, and the dataset size remains relatively small, limiting its effectiveness in largescale multimodal training. 3. Method In this section, we present our multimodal alignment and integration strategies as well as language-driven dataset construction method. In 3.1, we introduces Text-Guided Unified Vision Encoding for controlled vision-text integration. In 3.2, we proposes Context-Aware Recursive Alignment Decoding for adaptive alignment. In 3.3, we defines DualSupervised Semantic Mapping Loss for cross-modal consistency. In 3.4, we outlines our Synthesized Language-Driven QA Dataset framework for high-quality data generation. 3.1. Text-Guided Unified Vision Encoding Inspired by [20], we incorporate textual information into the vision encoder to enhance multimodal representation learning. However, we adopt specialized fusion strategy to refine high-level visual features while preserving the integrity of low-level visual information. As shown in Figure 3, given an image and textual questions , we first extract visual embeddings Vimg and textual embeddings Etxt. Then we obtain Text in ViSpace Vtxt by mapping the textual representation into vision feature space. Vimg = VisionEmbed(I) Etxt = LLMEmbed(T ), Vtxt = MLPt2v(Etxt) (1) (2) The mapped textual embeddings Vtxt and visual embeddings Vimg are then jointly processed through stack of encoding layers, enabling mutual refinement of multimodal features. Formally, each encoding layer updates these embeddings as follows: (V img, txt) = EncoderLayer(V i1 = 1, . . . , (3) where represents the total number of encoder layers. Initial embeddings are set as 0 img = Vimg and 0 img , i1 txt = Vtxt. txt ), To ensure balanced interaction between textual and visual features, we restrict textual influence at lower layers by masking the textual-to-visual attention in the first half of the encoder. After encoding, we aggregate visual and textual 4 embeddings separately into early and late-stage representations to capture both coarse and fine-grained multimodal features: img ="
        },
        {
            "title": "2\nN",
            "content": "N 2(cid:88) i=1 img, img = txt ="
        },
        {
            "title": "2\nN",
            "content": "N 2(cid:88) i=1 txt, txt ="
        },
        {
            "title": "2\nN",
            "content": "N (cid:88) i= 2 +1 (cid:88) i= 2 +"
        },
        {
            "title": "V i\ntxt",
            "content": "(5) state representation HP R1D at position , which aggregates context information up to the current position, is extracted and concatenated with each latent token IC[i, j], forming context-aware query representations:"
        },
        {
            "title": "V i\nimg",
            "content": "(4) IQ[i, j] = MLP (Concat(HP , IC[i, j])) R1D (9) Subsequently, each latent token IC[i, j] is updated through localized attention as follows: IC[i, j] = softmax (cid:18) Q[i, j]K[i, j]T (cid:19) [i, j] R1D (10)"
        },
        {
            "title": "The final encoded visual and textual representations are",
            "content": "obtained via channel-wise concatenation: where: img = Concat(V img, img; dim=channel) txt = Concat(V txt, txt; dim=channel) (6) (7) Ultimately, the visual representation img is mapped into the LLM embedding space, yielding Image in TeSpace Eimg, which facilitates direct interaction with textual features. Eimg = MLPv2t(V img) (8) 3.2. Context-Aware Recursive Alignment Decoding Unlike conventional approaches that position vision tokens solely at the beginning of the input sequence, we introduce additional context-aware latent tokens IC that are specifically dedicated to each question. As shown in Figure 4, we introduce interaction layers between the LLMs decoding layers. Within each interaction layer, latent tokens IC recursively refine visual-textual alignment based on the evolving textual context, thus ensuring question-level, fine-grained multimodal interaction. To enhance computational efficiency and enable effective fusion of multimodal features, we employ localized windowed attention mechanism. Specifically, we define grid of latent tokens IC RnnD, where denotes the spatial resolution of the token grid and represents the embedding dimension. predefined window size is used to limit attention operations within manageable regions. To minimize biases from the text-guided encoding and to maintain the efficacy of the window attention, we construct an auxiliary visual representation IA, which is achieved by partitioning the original image into four sub-images, encoding each without textual conditioning, and concatenating them. IA is later utilized as keys and values. To ensure compatibility with both the predefined window size and latent tokens grid, we interpolate IA to resolution of (n w, w) and finally get IA R(nw)(nw)D Before decoding, we initialize IC by interpolating IA onto the (n, n) grid. During decoding, given textual question , we determine the position corresponding to the end of this question within the token sequence. The hidden 5 Q[i, j] = IQ[i, j], Q[i, j] R1D, K[i, j] = Patch(IA, i, j, w), K[i, j] Rw2D, [i, j] Rw2D [i, j] = Patch(IA, i, j, w), Here, Q, K, represent learned projection matrices, and the Patch function extracts localized regions from IA defined as: Patch(IA, i, j, w) = IA[iw : (i+1)w, ; jw : (j+1)w] Rw2D (11) During the decoding process, we consistently leverage the textual context to interact back with the vision features, thereby updating IC. Through this recursive and contextaware alignment strategy, we dynamically enhance integration between visual and textual modalities. 3.3. Dual-Supervised Semantic Mapping Loss In the previous sections, visual and textual features are frequently mapped and interact. To better guide feature mapping, we introduce Dual-Supervised Semantic Mapping Loss based on two complementary transformations: MLPv2t for vision-to-text transformations and MLPt2v for text-to-vision transformations. For MLPv2t, it is crucial to ensure that the mapped visual features align closely with the feature space of LLM. We shift our focus to txt, which, as discussed earlier, represents textual tokens processed by the vision encoder in the visual feature space. Consequently, after mapping txt through LPv2t, its rebuilt textual representation should closely resemble the LLM-based textual representation Etxt. To quantify the alignment quality of MLPv2t, we compute the cosine similarity-based loss between these two feature representations: Lv2t = 1 Etxt MLPv2t(V txt) Etxt MLPv2t(V txt) (12) Analogous to Lv2t, we use Eimg to evaluate the effectiveness of LPt2v. An optimal LPt2v should transform Figure 5. Overview of our Text-Centered QA Dataset framework. Our approach shifts the focus from visual content to textual richness by leveraging high-quality captions, enriching them with LLMs, and using them as the foundation for both image generation via diffusion models and diverse QA pair construction. Eimg into rebuilt image representation that closely resembles the vision feature Vimg in the visual space. Thus, we again employ cosine similarity loss to assess the quality of LPt2v: of textual information. Unlike conventional methods, our framework prioritizes detailed textual descriptions, making text the central element guiding both image generation and QA pair creation. Lt2v = 1 MLPt2v(Eimg) Vimg MLPt2v(Eimg) Vimg (13) In addition to the supervised feature space mapping loss, we incorporate fundamental cross-entropy loss. LCE = (cid:88) i= log (TiT<i, ) (14)"
        },
        {
            "title": "The complete training objective combines these losses",
            "content": "with balancing parameter : Ltotal = LCE + λ(Lv2t + Lt2v). (15) The illustration of our reconstructed loss is shown in Figure 3. This dual-supervised approach explicitly reinforces semantic correspondence between the visual and textual representations, significantly boosting the effectiveness and accuracy of multimodal integration. 3.4. Synthesized Language-Driven QA Dataset We propose novel language-driven approach to constructing QA datasets, fundamentally shifting the traditional emphasis from visual content to the richness and diversity As shown in Figure 5, our process begins with carefully selecting high-quality captions from an extensive caption pool. These initial captions are then enriched with LLMs, producing detailed and nuanced textual descriptions that capture various visual and contextual attributes. These enriched descriptions serve as prompts for diffusion model, which generates images closely aligned with the provided textual context. Simultaneously, we utilize the enriched descriptions as input content, leveraging the LLM again to construct diverse QA pairs, ensuring broad coverage across multiple-choice questions, multi-turn dialogues, reasoning-based tasks. This step distinguishes our method from traditional visual-first or MLLM-based approaches. Directly leveraging comprehensive textual data significantly enhances the models understanding and representation of visual contexts. Generative models inherently introduce challenges, including ambiguity, missing and inconsistencies. To address these issues, we implement rigorous, multi-stage filtering process, with clearly defined criteria detailed in Appendix D.2. Examples of our prompts and QA pairs are provided in Appendix D.1. 6 By placing textual content at the heart of the dataset construction process, our approach offers scalable, adaptable, and robust method for synthesizing diverse and high-quality QA datasets. This language-driven framework thus provides fresh perspective on multimodal data creation, highlighting the essential role of textual richness in enhancing model comprehension and performance. 4. Experiments"
        },
        {
            "title": "Backbone",
            "content": "LLM: We select two strong language models, Phi-3.5mini-instruct[3] and LLaMA3.1-8B-instruct[3], due to their robust language modeling capabilities. Vision Encoder: We utilize SigLIP-SO400M-Patch14384[79] and the most advanced SigLIP2-GiantOPT-Patch16-384[65]. By leveraging the powerful SigLIP2-Giant-OPT-Patch16-384, we achieve superior performance with fewer tokens. Projector: LPt2v and LPv2t are both implemented as two-layer MLPs. Training Strategies We introduce three-stage training framework, distinct from traditional two-stage paradigms, ensuring comprehensive alignment and integration between visual and linguistic modalities. Stage1: Foundational Semantic Alignment: We pretrain the vision encoder using extensive image-caption datasets to establish precise semantic alignment between visual and textual representations. Stage1.5: Contextual Multimodal Fusion: In contrast to Stage 1, this intermediate stage incorporates various types of QA data along with image-caption pairs. This phase is designed to enhance the models adaptability in aligning vision and language representations across broad spectrum of scenarios. Stage2: Visual Instruction Tuning: At this stage, we expose the model to various visual tasks, enabling it to answer downstream vision-related questions effectively. Data In Stage 1, we utilize large-scale collection of image-caption pairs, such as PixelProse[62], LLaVA 558k[43], and ShareGPT4V Captions[9]. Additionally, we use our pipeline produce 1.5M image-caption pairs across various scenarios. In Stage 1.5, the primary objective is to enhance question diversity. We generate 1M distinct QA samples spanning reasoning, conversational interactions, and multiple-choice questions. We also use publicly available datasets such as PixmoAskModelAnything[15], Q-Instruct[71] and LVIS-Instruct[66]. Additionally, we borrow some domain-specific data, including Math, OCR, and Science from Stage 2 to further enrich the dataset. In Stage 2, the focus shifts towards vision-centric instruction tuning. We utilize part of domain-specific data from Stage 1.5. We incorporate datasets specifically designed for downstream visual tasks, such as LLaVA 665K[43], MMathCot[54], and Cambrian-7M[64], while also generating approximately 1M additional task-oriented samples. Details of dataset composition are provided in Appendix A. Implementation Details For the Text-Guided Unified Vision Encoding component, we constrain the maximum supported text length to 256 tokens, with any excess truncated to ensure computational efficiency. In the Context-Aware Recursive Alignment Decoding module, we employ fixed attention window size of 3 to strike balance between performance and efficiency. Furthermore, we adopt dynamic token mechanism that enables the model to adapt to varying lengths of visual token sequences. We randomly sample the number of latent tokens from the set {4, 16, 64, 144, 256} for each batch. In Appendix B.4, we show that this strategy can improve training stability while maintaining representational effectiveness. For the Dual-Supervised Semantic Mapping Loss, the balancing coefficient of λ is set to 0.1. For the main results, we use total of 9M image-caption pairs in Stage 1, and 12M instruction data (5M in Stage 1.5 and 7M in Stage 2). In each stage, we unfreeze all components to ensure comprehensive optimization and seamless integration. In Appendix B.5, we observe that unfreezing all parameters in each stage accelerates model convergence and enhances performance. For all models, the global batch size during the pretraining stage is 256, with cosine decay learning rate with maximun value 1e-4. In Stage 1.5, the global batch size is 128 and the learning rate is 2e-5. In Stage 2, we maintain global batch size of 128 with learning rate of 1e-5. Evaluation. Follow [8], We evaluate the performance of different MLLM models on 21 benchmarks with four different categories: General multimodal benchmarks: MMBench (EN and CN)[46], VisWiz[24], POPE[17], MM-Vet[76], MME Perception[18], MME Cognition[18], SeedBench[36], HallusionBench[23], LLaVA in the Wild[43] and MMStar[10]. OCR & Chart benchmark: TextVQA[61], OCRBench[47], ChartQA[55] and DocVQA[56]. Knowledge based benchmark: AI2D[26], MathVista[52], MMMU[77] and ScienceQA[51]. Vision Centric benchmark: MME-RealWorld[85], MMVP[82], RealworldQA[73] and CV-Bench[64]. 7 Table 1. Model comparison on 22 Benchmarks. We provide model variants with different Vision Encoders: SigLIP-SO400M-Patch14-384 (FUSION) and SigLIP2-Giant-OPT-Patch16-384 (FUSION-X). FUSION-L is built upon FUSION-X by employing interpolation on image tokens during inference. FUSION-X achieves superior performance with fewer tokens. Model General Benchmarks (a) Results on general multimodal benchmarks. Method Qwen2.5VL 3B InternVL2 4B DeepSeek-VL2-Tiny MM1.5 3B Phi 3.5-Vision Florence-VL 3B FUSION 3B (ours) FUSION-X 3B (ours) FUSION-L 3B (ours) Qwen2VL 7B InternVL2 8B LLaVA-OneVision 8B MM1.5 7B Cambrain 8B Florence-VL 8B Eagle 8B FUSION 8B (ours) FUSION-X 8B (ours) FUSION-L 8B (ours) . s # - - - - - 576 780 620 - - - - 576 1024 780 620 M 79.1 78. 74.6 - 75.5 71.6 79.5 80. 77.6 83.0 81.7 81.7 - 75. 76.2 75.9 80.5 82.0 M 78. 73.9 72.1 - 64.2 60.8 71. 74.8 70.8 80.5 81.2 78.0 - 67.9 69.5 - 74.9 76.2 W E - M <=4B Model Comparison - - - - 58.2 59. 64.6 66.1 65.3 85.9 84.6 - 88.1 82.2 88.3 88.9 88.7 88. 61.4 50.5 52.5 41.0 46.5 51. 57.2 60.3 56.7 >=7B Model Comparison - - - - - 59.1 - 59. 62.9 88.4 86.9 87.2 88.6 87. 89.9 - 89.3 88.8 62.0 54. 58.8 42.2 48.0 56.3 - 60. 60.0 M 1592.4 1532.8 1548.3 1478. 1473.4 1498.7 1595.9 1582.1 1573.7 1639. 1639.7 1626.0 1514.9 1547.1 1560.0 1559. 1592.3 1607.5 M 607.5 531.8 357. 319.6 412.1 403.9 416.5 440.0 406. 637.1 575.3 483.0 346.4 - 381. - 396.1 337.2 m - S 74. 73.2 72.3 72.4 69.9 70.6 74. 75.3 74.1 76.0 75.4 74.8 73. 74.7 74.9 76.3 77.2 78.2 80. 308 (b) Results on Vision centric, Knowledge based, and OCR & Chart benchmarks. 1601.7 338.9 73.6 59.9 88. 57.3 75.9 a - - - 73.0 68.8 71.1 84.7 85.2 77. - - 86.9 74.2 71.0 74. - 86.9 88.0 82.1 S 56. 53.9 45.9 - 49.0 44.9 52. 50.9 47.7 60.7 61.5 60.9 - 50.0 50.0 - 52.4 52.7 49. l 46.6 42.4 39.6 - 53.3 58.1 51.4 51.9 48.7 50. 45.2 47.5 - 48.7 57.3 - 52.6 51.4 46.7 Model Vision centric Knowledge based OCR & Chart Method # Vis tok. Qwen2.5VL 3B InternVL2 4B DeepSeek-VL2-Tiny MM1.5 3B Phi 3.5 Vision Florence-VL 3B FUSION 3B (ours) FUSION-X 3B (ours) FUSION-L 3B (ours) Qwen2VL 7B InternVL2 8B LLaVA-OneVision 7B MM1.5 7B Cambrian 8B Florence-VL 8B Eagle 8B FUSION 8B (ours) FUSION-X 8B (ours) FUSION-L 8B (ours) - - - - - 576 780 308 - - - - 576 1024 780 620 308 - M 53.1 52.1 - - - - 41.5 41.7 39.5 57.4 53.5 57. - - - - 46.0 44. 42.3 R 65.5 60.5 64.2 56. 53.5 60.4 65.1 63.7 61.8 70. 64.4 65.5 62.5 64.2 64.2 66. 65.2 66.1 65.1 i a 61. 58.5 53.6 44.4 - 52.2 54. 54.9 48.6 58.2 58.3 56.1 47. 49.0 55.5 52.7 56.6 59.4 55. n C - M 2 <= 4B Model Comparison - - - - 69.3 70.2 76. 78.3 76.2 - - - - 67.7 64.7 76.0 78.1 77.0 81. 79.0 71.6 65.7 77.4 73.8 78. 79.2 77.3 >= 7B Model Comparison 83.0 83.6 81. 72.2 73.0 74.2 76.1 80.4 81. 79.2 - - - - 72. 73.4 - 78.7 79.2 78.2 - - - - 51.3 73.3 71. 78.7 79.7 76.7 8 M 51. 48.3 40.7 37.1 43.3 41.8 44. 44.2 43.4 54.1 52.6 47.7 41. 42.7 43.7 43.8 43.1 42.2 41. S 79.3 96.0 - 85.8 89. 84.6 87.1 87.3 85.6 85.5 96. 96.6 89.6 80.4 85.9 84.3 89. 90.3 88.3 t - 74.7 80. 76.5 61.1 69.1 71.8 73.9 71. 84.3 77.4 78.5 76.5 71.7 74. 77.1 77.3 74.7 72.8 e O 82. 78.4 80.5 65.7 59.8 63.0 60. 63.7 56.9 86.6 79.4 69.7 63. 62.4 63.4 62.6 63.8 66.6 59. t C 84.0 81.5 81.0 74. 72.0 70.7 75.7 75.8 67.7 83. 83.3 78.8 88.1 73.3 74.7 80. 80.3 79.8 73.0 c 93.9 89. 86.9 87.5 75.9 - 70.9 71. 63.5 94.5 91.6 87.5 78.2 77. - 86.6 78.6 77.8 66.0 Table 2. Performance analysis of model components. We evaluate the contribution of different components, including Text-Guided Unified Vision Encoding(TUNE), Context-Aware Recursive Alignment Decoding(CARD), and Dual-supervised Semantic Mapping(DSM) U D C LLaVA-NeXT M 74.4 72.8 73.2 73.7 74.0 73. 74.9 M 70.4 69.2 70.4 70. 71.6 71.4 72.2 z 52.0 59.8 60.2 61.1 60.7 58. 61.2 P 87.8 86.9 87.4 87. 87.6 87.6 87.9 - M 43. 41.3 43.1 43.4 45.0 45.6 44. M 1552.0 1529.6 1589.8 1538.3 1506. 1568.6 1558.3 M 331.4 323.7 340. 351.2 330.4 350.4 361.8 m e 74. 72.4 72.0 72.7 72.3 72.3 73. V 81.4 77.0 80.5 81.1 83. 78.9 82.7 e 71.2 66.2 66. 67.4 66.8 67.0 69.7 M 66. 66.5 68.0 68.0 69.0 68.3 70. 2 71.2 70.2 70.6 71.0 71. 71.2 71.7 M 41.1 37.8 39. 39.8 38.6 40.0 39.2 77. 74.9 75.8 76.0 77.1 76.2 78. V T 69.8 63.0 62.2 62.8 63. 63.0 63.6 r 69.7 54. 54.1 55.9 55.6 54.9 55.5 Table 3. Impact of synthetic data on performance. We gradually incorporate our generated data into different stage, showing the improvement in performance on various tasks. 5 . 1 t 2 t 1 t B 77.2 baseline +1.5M +1.0M 78.0 +0.9M 80. 77.5 M 71.4 71.6 72.1 74. W 66.6 65.7 66. 66.1 P 88.1 88.2 88.9 88. - M 57.4 58.2 59.4 60. M 1544.9 1567.6 1567.2 1582.1 M 379.4 386.1 408.5 440.0 m e 73. 73.6 74.1 75.3 a 84.4 83. 81.9 85.2 e 74.8 74.6 86. 78.3 M 74.6 74.5 74.2 78. 2 77.2 77.4 78.8 79.2 M 42.5 42.6 43.9 44.2 84. 85.8 85.7 87.3 t 73.4 73. 73.5 73.9 r 74.8 75. 74.9 75.8 Baselines. We evaluate our approach against wide range of open-source baseline models. For the 3B model size, we choose InternVL2-4B[69], Qwen2.5VL-3B[4], DeepSeekVL2-Tiny[72], Florence-VL 3B[8], Phi3.5-Vision 3B[1] and MM1.5 3B[81]. the 8B model size, we For choose InternVL2-8B[69], Qwen2VL-7B[68], Eagle-8B [60], Florence-VL 8B[8], Cambrian-8B[64] and LLaVAOneVision-7B[37]. Result. As shown in Table 1, our model demonstrates leading performance across the majority of benchmarks. It significantly outperforms Cambrian-1. When compared to Florence-VL under comparable data scale, our model achieves substantial performance gain. Remarkably, our 3B model surpasses both Cambrian-1 8B and Florence-VL 8B. Furthermore, thanks to our fully integrated fusion and alignment mechanisms, our model requires only 620 tokens to achieve performance that is competitive with or In nearly half of superior to the current SOTA models. the benchmarks, our model outperforms InternVL2, which leverages high-resolution inputs with substantially larger number of vision tokens and significantly more training data. Additionally, in certain metrics, our models performance approaches that of Qwen2.5VL. Moreover, our 3B model achieves the best result in MMBench, underscoring the effectiveness of our model architecture and the advantages conferred by our synthetic data strategy. We provide comparative analysis with additional models in Figure 1. Additionally, detailed visualization of our models alignment mechanisms is presented in Figure 2, illustrating how cross-modal integration is progressively refined throughout the pipeline. 5. Ablation Study In this section, we first compare our model to the LLaVANeXT architecture to validate the effectiveness of each component within our framework. We present an additional ablation study that demonstrates the significant impact of latent token on model performance. Subsequently, we examine the contribution of synthetic data to modality alignment and downstream task performance. Lastly, to further highlight the superiority of our models fullmodality integration, we reduce the number of tokens and analyze the models performance under increasingly constrained conditions. Unless otherwise specified, the model used in this section is FUSION-X with its respective configuration. 5.1. Component-wise Analysis of Modality Fusion We select LLaMA3-8B as the language model and CLIP as the vision encoder, with Open LLaVA-NeXT as the base9 (a) Model performance under varying numbers of latent vision tokens. (b) Model performance under varying numbers of global vision tokens. Figure 6. Comparative analysis of performance with different latent and global vision token configurations. line for comparison. Both models are trained on identical datasets comprising 558K pretraining samples and 1M Open LLaVA-NeXT finetuning dataset. Experimental results in Table 2 indicate that each component of our model significantly contributes to overall performance enhancement. Specifically, Text-Guided Unified Vision Encoding and Dual-supervised Semantic Mapping notably improve general multimodal tasks. We observe that the integration of Dual-supervised Semantic Mapping with TextGuided Unified Vision Encoding results in significant performance boost, highlighting the importance of modality alignment and the effectiveness of our Dual-supervised Semantic Mapping design. Furthermore, Context-Aware Recursive Alignment Decoding substantially boosts OCR accuracy and effectively reduces hallucinations. When integrated collectively, these components enable our model to outperform Open LLaVA-NeXT across more than half of benchmarks, without relying on dynamic resolution techniques. 5.2. Impact of Latent Token on Performance To further underscore the pivotal role of latent tokens in our modality alignment strategy, we evaluate our 3B models performance under varying latent token configurations. As depicted in Figure 6a, even little increase in latent token count leads to substantial improvements, particularly in OCR & Chart and Vision-Centric tasks. This indicates that latent tokens, while fewer in number, play crucial role in boosting multi-modal understanding and alignment with minimal resource overhead. Further case studies and detailed analyses on the role of latent tokens are provided in the Appendix C.2. 5.3. Influence of Synthetic Data on Modality Fusion To assess the value of synthetic data, we conduct experiments on our 3B model by removing generated samples utilized in each stage. Our results in Table 3 confirm that synthetic data significantly facilitate modality alignment and enhance performance in targeted downstream tasks. Specif10 ically, incorporating synthetic data during Stage 2 training markedly improves performance on tasks such as MMBench, SQA, CVBench and MMVP. 5.4. Robustness Under Reduced Token Constraints To further showcase the effectiveness of our comprehensive modality alignment strategy, we test the models resilience by progressively reducing the number of image tokens through interpolation. Experimental outcomes in Figure 6b demonstrate that our model maintains superior performance compared to Florence-VL, even when the token count is reduced to 300(we have named this version of the model FUSION-L). This robust performance under constrained conditions underscores the efficiency and adaptability of our modality fusion approach. 6. Conclusion In this paper, we present FUSION, family of MLLMs that rethinks how vision and language should be integrated. Rather than treating vision as static input fused only at the decoding stage, our model enables interaction at every phasestarting from vision encoding guided by text, to decoding that recursively aligns with visual features. To address semantic mismatch between modalities, we propose dual-supervised loss that enforces consistency through bidirectional reconstruction. Beyond architecture, we also introduce scalable data generation strategy that synthesizes diverse and informative QA pairs, providing strong supervision for vision-language alignment. Our experiments show that FUSION achieves superior performance with far fewer vision tokens than existing models, highlighting both its efficiency and accuracy. Even with reduced input, the model matches or surpasses larger baselines, proving the value of deeper modality interaction."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sebastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio Cesar Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: highly capable language model locally on your phone, 2024. 9 [2] Manoj Acharya, Kushal Kafle, and Christopher Kanan. TalIn AAAI, lyqa: Answering complex counting questions. 2019. 1 [3] AI@Meta. Llama 3 model card. 2024. 7 [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. 2, 3, 9 [5] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marcal Rusinol, Ernest Valveny, CV Jawahar, and DimosIn thenis Karatzas. Scene text visual question answering. ICCV, 2019. 1 [6] Ali Furkan Biten, Ron Litman, Yusheng Xie, Srikar Appalaraju, and Manmatha. Latr: Layout-aware transformer for scene-text vqa. In CVPR, 2022. [7] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language model. arXiv preprint arXiv:2402.11684, 2024. 3, 1 [8] Jiuhai Chen, Jianwei Yang, Haiping Wu, Dianqi Li, Jianfeng Gao, Tianyi Zhou, and Bin Xiao. Florence-vl: Enhancing vision-language models with generative vision encoder and depth-breadth fusion, 2024. 3, 7, 9 [9] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 1, 3, 7 [10] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large vision-language models?, 2024. 7 [11] Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, et al. Finqa: dataset of numerical reasoning over financial data. In EMNLP, 2021. [12] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Internvl: Scaling up vision foundation models and Dai. aligning for generic visual-linguistic tasks, 2024. 2 [13] Zhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia, Jiaqi Guo, Yan Gao, Shi Han, Jian-Guang Lou, and Dongmei Zhang. Hitab: hierarchical table dataset for question answering and natural language generation. In ACL, 2022. 1 [14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning. In NeurIPS, 2024. 2, 3 [15] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris CallisonBurch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models, 2024. 7, 1 [16] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1119811201, 2024. 7 [17] Zhiwen Fan, Panwang Pan, Peihao Wang, Yifan Jiang, Dejia Xu, Hanwen Jiang, and Zhangyang Wang. Pope: 6-dof 11 promptable pose estimation of any object, in any scene, with one reference, 2023. 7 [18] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. 7 [19] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. 2024. 10 [20] Roy Ganz, Yair Kittenplon, Aviad Aberdam, Elad Ben Avraham, Oren Nuriel, Shai Mazor, and Ron Litman. Question aware vision transformer for multimodal reasoning, 2024. 2, 4, 7 [21] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. [22] Lupyan Gary."
        },
        {
            "title": "Linguistically modulated perception and\nFrontiers in",
            "content": "cognition: The label-feedback hypothesis. Psychology, 2012. 1 [23] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large visionlanguage models, 2024. 7 [24] Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In CVPR, 2018. 7, 1 [25] Xuehai He, Yichen Zhang, Luntian Mou, Eric P. Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical visual question answering. CoRR, abs/2003.10286, 2020. 1 [26] Tuomo Hiippala, Malihe Alikhani, Jonas Haverinen, Timo Kalliokoski, Evanfiya Logacheva, Serafina Orekhova, Aino Tuomainen, Matthew Stone, and John Bateman. Ai2drst: multimodal corpus of 1000 primary school science diagrams. Language Resources and Evaluation, 55:661688, 2021. 7 [27] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, 2017. 1 [28] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In CVPR, 2018. 1 [29] Shankar Kantharaj, Rixie Tiffany Ko Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, and Shafiq Joty. Chart-to-text: large-scale benchmark for chart summarization. In ACL, 2022. 1 [30] Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu, Xi Chen, and Radu Soricut. Geomverse: systematic evaluation of large models for geometric reasoning. 2023. 1 [31] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In ECCV, 2016. 1 [32] Geewook Kim, Teakgyu Hong, Moonbin Yim, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Donut: Document understanding transformer without ocr. In ECCV, 2022. 1 [33] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 1 [34] LAION. laion/gpt4v-dataset, 2023. 1 [35] Hugo Laurencon, Andres Marafioti, Victor Sanh, and Leo Building and better understanding visionTronchon. language models: insights and future directions., 2024. 1 [36] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension, 2023. [37] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer, 2024. 2, 3, 9, 7 [38] Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. Multimodal arxiv: dataset for improving scientific comprehension of large vision-language models. arXiv preprint arXiv:2403.00231, 2024. 1 [39] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. 3 [40] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models, 2024. 3 [41] Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and Teknium. Openorca: An open dataset of gpt augmented flan reasoning traces. https : / / https : / / huggingface . co / Open - Orca/OpenOrca, 2023. 1 [42] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, and Yu Qiao. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models, 2023. 2, [43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 1, 3, 7 [44] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 2, 3 [45] Shuo Liu, Kaining Ying, Hao Zhang, Yue Yang, Yuqi Lin, Tianle Zhang, Chuanhao Li, Yu Qiao, Ping Luo, Wenqi Shao, and Kaipeng Zhang. Convbench: multi-turn conversation evaluation benchmark with hierarchical capability for large vision-language models, 2024. 7 [46] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 7 Eagle: Exploring the design space for multimodal llms with mixture of encoders, 2025. 2, 3, [47] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12), 2024. 7 [48] Zheng Liu, Hao Liang, Bozhou Li, Tianyi Bai, Wentao Xiong, Chong Chen, Conghui He, Wentao Zhang, and Bin Cui. Synthvlm: High-efficiency and high-quality synthetic data for vision language models, 2025. 4, 9 [49] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: new benchmark for abstract diagram understanding and visual language reasoning. In NeurIPS, 2021. 1 [50] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In NeurIPS, 2022. 1 [51] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering, 2022. 7 [52] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. ICLR, 2023. 7 [53] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, SongChun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In ICLR, 2023. 1 [54] Ruilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu, Xinzhe Ni, Zicheng Lin, Jin Zeng, and Yujiu Yang. Ursa: Understanding and verifying chain-of-thought reasoning in multimodal mathematics, 2025. 7, [55] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In ACL, 2022. 7, 1 [56] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In WACV, 2021. 7, 1 [57] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024. 3 [58] Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. In ACL, 2015. 1 [59] Jordi Pont-Tuset, Jasper R. R. Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with localized narratives. In ECCV, 2020. 1 [60] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, Yilin Zhao, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, Bryan Catanzaro, Andrew Tao, Jan Kautz, Zhiding Yu, and Guilin Liu. [61] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read, 2019. 7 [62] Vasu Singla, Kaiyu Yue, Sukriti Paul, Reza Shirkavand, Mayuka Jayawardhana, Alireza Ganjdanesh, Heng Huang, Abhinav Bhatele, Gowthami Somepalli, and Tom Goldstein. From pixels to prose: large dataset of dense image captions, 2024. 7, 1 [63] Benny Tang, Angie Boggust, and Arvind Satyanarayan. Vistext: benchmark for semantically rich chart captioning. arXiv preprint arXiv:2307.05356, 2023. 1 [64] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Ziteng Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms, 2024. 2, 3, 7, 9, 1 [65] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Henaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual visionlanguage encoders with improved semantic understanding, localization, and dense features, 2025. 7 [66] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning, 2023. 3, 7, 1 [67] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804, 2024. 1 [68] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. [69] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, and Jifeng Dai. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization, 2024. 9 [70] Chris Wendler. wendlerc/renderedtext, 2023. 1 [71] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Kaixin Xu, Chunyi Li, Jingwen Hou, Guangtao Zhai, Geng Xue, Wenxiu Sun, Qiong Yan, and Weisi Lin. Q-instruct: Improving low-level visual abilities for multi-modality foundation models, 2023. 3, 7, 1 [72] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, and Chong Ruan. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding, 2024. 9 13 [86] Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. 2017. 1 [87] Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. Tat-qa: question answering benchmark on hybrid of tabular and textual content in finance. In ACL, 2021. 1 [88] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li FeiFei. Visual7w: Grounded question answering in images. In CVPR, 2016. [73] x.ai. Grok 1.5v: The next generation of ai. https://x. ai/blog/grok-1.5v, 2023. Accessed: 2024-07-26. 7 [74] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, and Dacheng Tao. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search, 2024. 1 [75] Yuanyang Yin, Yaqi Zhao, Yajie Zhang, Ke Lin, Jiahao Wang, Xin Tao, Pengfei Wan, Di Zhang, Baoqun Yin, and Wentao Zhang. Sea: Supervised embedding alignment for token-level visual-textual integration in mllms, 2024. 2, 3 [76] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities, 2024. 7 [77] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. [78] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. In ICLR, 2024. 1 [79] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. 7 [80] Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and SongChun Zhu. Raven: dataset for relational and analogical visual reasoning. In CVPR, 2019. 1 [81] Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, Sam Dodge, Keen You, Zhen Yang, Aleksei Timofeev, Mingze Xu, Hong-You Chen, JeanPhilippe Fauconnier, Zhengfeng Lai, Haoxuan You, Zirui Wang, Afshin Dehghan, Peter Grasch, and Yinfei Yang. Mm1.5: Methods, analysis & insights from multimodal llm fine-tuning, 2024. 9 [82] He Zhang, Shenghao Ren, Haolei Yuan, Jianhui Zhao, Fan Li, Shuangpeng Sun, Zhenghao Liang, Tao Yu, Qiu Shen, and Xun Cao. Mmvp: multimodal mocap dataset with vision and pressure sensors, 2024. 7 [83] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint arXiv:2306.17107, 2023. [84] Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, Liang Wang, and Rong Jin. Beyond llava-hd: Diving into high-resolution large multimodal models, 2024. 3 [85] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans?, 2025. 7 14 FUSION: Fully Integration of Vision-Language Representations for Deep Cross-Modal Understanding"
        },
        {
            "title": "Appendix",
            "content": "Short Captions (21.3%) SynthCount (300.0 K) SynthSpatial (300.0 K) SynthText (400.0 K) SynthColor (250.0 K) SynthScene (250.0 K) LLaVA-Pretrain[43] (558.0 K) Long Captions (69.8%) ShareCaptioner [9] (1246.0 K) PixelProse [62] (5500.0 K) Math (8.9%) URSA Alignment [54] (860.0 K) Synth QA (12.3%) WikiSQL [86] (74.0 K) SynthMultiChoice (450.0 K) WTQ [58] (38.0 K) SynthConvLong (150.0 K) SynthConvShort (250.0 K) OCR (14.1%) DocVQA [56] (100.0 K) PixmoDoc [15] (251.0 K) SynthDog [32] (300.0 K) Science (10.0%) MulBurry [74] (100.0 K) MMathCot [54] (200.0 K) DataEngine [64] (160.0 K) SynthContrastShort (70 K) SynthTextQA (150 K) General QA (20.1%) LVISInstruct4V [66] (200.0 K) Q-Instruct [71] (400.0 K) GPT4Rewrote [64] (137.0 K) PixmoAskAny [15] (70.0 K) PixmoCapQA [15] (160.0 K) Synth QA (21.5%) Long Captions (29.7%) SynthConvLong (200.0 K) SynthConvShort (200.0 K) SynthMultiChoice (200.0 K) Language (4.3%) SynthReasoning (100.0 K) SynthContrastLong (70.0 K) MathInstruct [78] (100.0 K) ALLaVA-Cap [7] (700.0 K) PixmoCap [15] (640.0 K) OpenOrca [41] (100.0 K) MulBerry [74] (270.0 K) MMathCot [54] (640.0 K) ScienceQA [50] (6.0 K) PathVQA [25] (32.0 K) DataEngine [64] (160.0 K) GeoQA [21] (170.0 K) Math Vision [67] (3.0 K) IconQA [49] (27.0 K) Chart2Text [29] (25.0 K) TabMWP [53] (23.0 K) LLAVAR [83] (20.0 K) ST-VQA [5] (17.0 K) RenderedText [70] (10.0 K) RAVEN [80] (42.0 K) VisText [63] (10.0 K) FinQA [11] (5.2 K) InfoVQA [6] (2.0 K) TAT-QA [87] (2.0 K) HiTab [13] (2.5 K) GeomVerse [30] (9.0 K) Long Captions (5.8%) General QA (27.4%) Science (19.2%) ALLaVA-QA [7] (700.0 K) LLaVA [43] (665.0 K) GPT4V [34] (8.0 K) ShareGPT4V [9] (90.0 K) VizWiz [24] (20.0 K) Visual7w [88] (14.0 K) LNQA [59] (400.0 K) Counting (3.6%) CLEVR [27] (150.0 K) TallyQA [2] (100.0 K) Language (2.9%) ALLaVA-Cap [7] (250.0 K) OpenOrca [41] (200.0 K) PixmoCap [15] (150.0 K) OCR (28.9%) DocMatix [35] (920.0 K) PixmoDoc [15] (251.0 K) DVQA [28] (120.0 K) DocVQA [56] (39.0 K) ChartQA [55] (28.0 K) AI2D [31] (15.0 K) ArxivQA [38] (100.0 K) ScreenQA (79.0 K) SynthDog [32] (200.0 K) Figure 7. FUSION-10M and FUSION-12M. We collected total of 10M samples for pretraining (Stage 1) and 12M samples for fine-tuning (Stage 1.5 and Stage 2). The circle illustrates the data distribution, with the right side of the circle showing all the utilized data and their respective proportions. A. Data Collection The data we used is summarized in Figure 7. In Stage 1, we extract CC12M data from PixelProse[62], consisting of 5.5M samples, and use the entire dataset from ShareCaptioner, which includes 1.2M samples. In addition, we utilize the math dataset URSA-Alignment[54] to train the vision encoder for adaptation to different types. For our generated data, we synthesize across various scenarios, including text descriptions, color attributes, scene understanding, spatial relation, and object counting. In Stage 1.5, we collect wide variety of data. We utilize most of the data from Pixmo[15], including QA, Caption, and OCR. Additionally, we introduce some data related to mathematics and reasoning from Stage2. For our data generation pipeline, we synthesize 7 categories QA data. Prompts in this phase prioritize variability in order to improve the models generalization capabilities. In Stage 2, we primarily use LLaVA 665k[43] and 1 most of the data from Cambrian-1 7M[64]. Furthermore, we introduce additional mathematical datasets, including MMathCoT[54] and MulBerry[74]. We also integrate large amount of OCR data, selecting all single and doubleimage data from DocMatix[35], totaling 920k samples. For our data generation pipeline, we synthesize 3 categories QA data. Prompts in this phase emphasizes task-based instructions to enhance the models ability to follow instructions. In both Stage 1.5 and Stage 2, we incorporate pure text data to maintain the models text capabilities while also enabling the LLM to adapt to our decoding format. Among all the data, we totally generate 3.3M samples using our framework, with 1.5M used for pretraining and 1.8M for supervised fine-tuning. The prompts and selection process for all the data are provided in the Appendix D. All images are generated by FLUX[33], with the guidance step set to 60 and the image resolution set to 1024x1024. Table 4. Performance analysis of model components. We evaluate the contribution of different components, including Text-Guided Unified Vision Encoding(TUNE), Context-Aware Recursive Alignment Decoding(CARD), and Dual-supervised Semantic Mapping(DSM) T D LLaVA-NeXT LLaVA-NeXT B 68.5 68.2 68.9 68.6 69. 68.8 70.0 74.4 72.8 73.2 73. 74.0 73.8 74.9 M 60.7 59. 60.7 60.0 61.6 60.4 62.1 70. 69.2 70.4 70.5 71.6 71.4 72. W 59.7 58.8 59. 59.1 60.2 59.5 59.9 52.0 59. 60.2 61.1 60.7 58.9 61.2 P 87.1 86.9 87.1 87.3 87.2 87. 87.2 87.8 86.9 87.4 87.5 87. 87.6 87.9 - M 40.6 35. 36.7 37.4 38.2 37.9 38.9 43. 41.3 43.1 43.4 45.0 45.6 44. M M a e Vicuna-7B + Clip 1540.0 1526. 1548.9 1543.1 1575.4 1550.3 1564.1 319. 298.4 317.3 309.4 347.9 322.1 326. 70.6 68.5 68.9 68.7 69.4 69. 69.8 LLaMA3-8B + Clip 1552.0 1529.6 1589.8 1538. 1506.1 1568.6 1558.3 331.4 323.7 340. 351.2 330.4 350.4 361.8 74.4 72. 72.0 72.7 72.3 72.3 73.0 a 79.0 73.1 75.7 75.2 77.8 74. 76.4 81.4 77.0 80.5 81.1 83. 78.9 82.7 e 64.9 63.1 63. 64.8 64.4 65.0 65.8 71.2 66. 66.7 67.4 66.8 67.0 69.7 M 62.3 64.1 64.3 64.8 65.2 64. 65.7 66.0 66.5 68.0 68.0 69. 68.3 70.0 2 64.5 63.7 64. 64.9 65.1 64.4 64.9 71.2 70. 70.6 71.0 71.4 71.2 71.7 M 39.0 37.0 36.9 37.1 37.8 36. 36.8 41.1 37.8 39.2 39.8 38. 40.0 39.2 70.7 69.1 69. 70.4 70.5 70.9 71.4 77.3 74. 75.8 76.0 77.1 76.2 78.3 t T 67.2 58.7 59.4 60.9 59.8 60. 61.2 69.8 63.0 62.2 62.8 63. 63.0 63.6 r 64.2 52. 52.0 52.6 51.9 52.4 52.2 69. 54.0 54.1 55.9 55.6 54.9 55. Table 5. Impact of the Interaction Layer. We investigate the effect of the Interaction Layer on model performance, specifically in scenarios where Latent Tokens are utilized. y o a n k n L M 69. 69.1 70.0 74.0 74.2 74.9 M 61.6 62.3 62.1 71.6 71.5 72. O 87.2 87.0 87.2 87.6 87. 87.9 - M 38.2 38.7 38. 45.0 43.7 44.8 M M m e Vicuna-7B + Clip 347.9 311.4 326.4 69.4 69. 69.8 LLaMA3-8B + Clip 330.4 331.8 361.8 72. 72.6 73.0 1575.4 1537.6 1564.1 1506. 1567.8 1558.3 a 77.8 77.2 76. 83.9 79.4 82.7 e 64.4 64. 65.8 66.8 68.8 69.7 M 65. 64.6 65.7 69.0 68.0 70.0 2 65.1 65.2 64.9 71.4 72.2 71. M 37.8 37.4 36.8 38.6 38. 39.2 70.5 69.9 71.4 77. 77.4 78.3 t 59.8 60.7 61. 63.3 63.6 63.6 r 51. 52.1 52.2 55.6 55.8 55.5 W 60.2 58.8 59.9 60. 59.4 61.2 B. Additional Ablation Studies In this section, we conduct additional ablation experiments to further validate the effectiveness and generalization ability of our proposed model. We also perform deeper investigations into our design choices. Unless otherwise specified, we evaluated two configurations throughout the following experiments: Vicuna 7B + Clip and LLaMA3 8B + Clip. B.1. Extended Component-wise Analysis We conducted additional experiments in Vicuna 7B configuration to further supplement the component-wise analysis presented in the main paper. The comprehensive results are 2 summarized in Table 4. The findings indicate that ContextAware Recursive Alignment Decoding plays crucial role in OCR-related tasks, achieving superior performance with only minimal number of latent tokens introduced. Moreover, our experiments reveal that the combined use of TextGuided Unified Vision Encoding and Dual-Supervised Semantic Mapping leads to significant performance gains, highlighting the effectiveness of modality alignment. The results consistently demonstrate that our model outperforms LLaVA-NeXT by larger margin under this configuration. Table 6. performance. Specifically, we experiment with window sizes of 2, 3, and 4. Impact of localized window size. We investigate the effect of varying window sizes within the Interaction Layer on model S n 3 4 2 3 4 M 69.5 70.0 69.8 74.0 74.9 74. B 61.2 62.1 61.7 71.5 72. 71.7 z 59.0 59. 59.3 59.8 61.2 59.4 P 86. 87.2 87.3 87.4 87.9 87.0 - V 38.1 38.9 39.2 46.2 44. 44.5 M M m e V Vicuna-7B + Clip 322.6 326.4 319.2 69.9 69.8 70. 78.1 76.4 77.6 LLaMA3-8B + Clip 323.9 361. 337.9 72.1 73.0 72.2 81.9 82. 79.2 1544.4 1564.1 1557.9 1563.2 1558. 1565.7 e 64.7 65.8 65.3 66. 69.7 66.2 M 64.9 65.7 63. 69.7 70.0 67.3 2 64.2 64. 64.5 71.2 71.7 70.2 M 37. 36.8 36.6 38.8 39.2 38.7 S 70.7 71.4 71.1 76.2 78.3 76. V T 60.5 61.2 60.9 62.1 63. 62.6 r 51.6 52.2 51. 53.3 55.5 54.2 Table 7. Impact of the Number of Latent Tokens. We examine how different configurations of Latent Tokens affect model performance. Our experiments include both fixed-length settings (64, 144, 256) and dynamic-length configurations (ranging from 4 to 256, and 16 to 576 tokens). M 69. 69.5 68.9 70.0 68.9 73.8 74. 73.2 74.9 73.5 M 61.5 61. 60.8 62.1 61.4 70.3 71.1 71. 72.2 71.1 z 60. 60.3 58.7 59.9 57.6 61.4 59. 60.7 61.2 60.6 P 86.6 86. 87.0 87.2 87.0 87.5 87.7 87. 87.9 87.3 - M 38.8 38. 39.2 38.9 38.1 45.6 45.5 46. 44.8 43.8 M M m e Vicuna-7B + Clip 1548.2 1557.1 1546.9 1564.1 1543.2 322. 335.8 320.4 326.4 319.6 69.6 70. 68.9 69.8 69.3 LLaMA3-8B + Clip 1535.9 1579. 1560.4 1558.3 1544.3 356.8 351.4 348. 361.8 320.7 72.7 72.6 72.4 73. 72.2 a 78.4 73.8 74.2 76. 70.5 79.1 81.8 77.2 82.7 82. n C 66.3 65.4 64.7 65.8 64. 68.8 69.0 68.5 69.7 68.1 M 64.8 65.6 64.5 65.7 62.8 69. 69.3 69.4 70.0 68.0 2 65. 63.7 64.2 64.9 64.3 71.1 70. 71.0 71.7 70.1 M 36.9 37. 37.4 36.8 35.9 41.0 41.5 40. 39.2 38.0 70.5 70.2 69. 71.4 68.8 77.1 77.5 76.9 78. 75.5 t 59.9 60.7 61.5 61. 59.2 62.9 62.6 64.2 63.6 62. t C 51.1 52.0 52.4 52. 51.7 54.0 54.3 56.7 55.5 54. Latent Token 64 144 256 4,16,64,144,256 16,64,144,256. 64 144 256 4,16,64,144,256 16,64,144,256.576 B.2. Effectiveness of Context-Aware Decoding B.3. Effect of Localized Window Size In the above experiments, we demonstrated the effectiveness of Context-Aware Recursive Alignment Decoding for OCR tasks. However, natural question arises: does the performance improvement stem merely from the additional image tokens introduced by this decoding strategy? To isolate this factor, we conduct an ablation study in which we still introduce Latent Tokens but remove the Interaction Layers. In this setting, the Latent Tokens serve solely as holistic representations of the image tokens, without engaging in contextual interaction. As shown in Table 5, introducing Latent Tokens alone yields no significant performance improvement compared to the baseline , with only marginal gain observed on OCR tasks. In contrast, when the Interaction Layers are included, the model achieves substantial overall performance boost, underscoring their critical role in effective context-aware decoding. In the main experimentss, we fix the localized window size to 3 as trade-off between performance and computational efficiency. Here, we vary the window size to further investigate its impact. As shown in Table 6, increasing the window size does not lead to significant performance improvements, while it introduces noticeable increase in computational cost. Conversely, when the window size is too small, interpolation is required to match the dimensions between the windowed features and the latent tokens. This interpolation leads to substantial compression of global visual information, resulting in decline in performance. B.4. Effect of the Number of Latent Tokens In the main results, we set the number of latent tokens to {4, 16, 64, 144, 256}, randomly sampling from this set for each training batch. We find that this random selection stratTable 8. Effectiveness of Our Training Strategy. We validate the effectiveness of our training strategy by comparing different parameterfreezing schemes. Specifically, in Stage 1, when parameters are not fully unfrozen, only the projector and interaction layer are updated. In Stages 1.5 and 2, partial unfreezing involves training the projector, interaction layer, and the LLM, while keeping encoder fixed. 5 . 1 t 1 t M 2 t 70.4 70.0 69.3 68.9 68.7 M 62.6 62.4 62.8 62.7 62.8 W 51.9 51.4 51.2 51. 51.1 P 85.6 85.9 85.1 85. 85.3 - M 41.1 40.2 39. 37.7 35.9 M 1396.8 1379.1 1365. 1354.2 1368.6 M 272.4 307.9 288. 276.4 275.1 a 72.7 70.6 71. 71.5 69.1 e 63.2 62.5 61. 61.6 60.7 M 66.8 66.9 67. 66.7 66.5 2 69.5 69.2 68. 68.7 68.4 M 38.9 38.1 39. 38.7 38.9 73.4 72.6 72. 71.6 71.2 t 63.9 63.3 62. 62.1 61.9 r 62.2 61. 61.6 61.9 61.6 Table 9. Comprehensive evaluation of FUSION models with varying total visual token counts (180-630) across General, Vision-Centric, Knowledge-Based, and OCR & Chart benchmarks. Model General Benchmarks (a) Results on general multimodal benchmarks. . s # 630 300 180 630 300 180 576 576 576 576 Method FUSION-X 3B FUSION-X 3B FUSION-X 3B FUSION-X 8B FUSION-X 8B FUSION-X 8B Cambrian-1 3B Cambrian-1 8B Florence-VL 3B Florence-VL 8B Model B C z i O - M M M a - S 80.3 77.6 75.6 82.0 80.0 76.7 74.6 75.9 71.6 76.2 75.3 74.1 72.1 78.2 75.9 73.6 72.6 74.7 70.6 74.9 (b) Results on Vision centric, Knowledge based, and OCR & Chart benchmarks. 1582.1 1573.7 1485.3 1607.5 1601.7 1561.6 1497.0 1547.1 1498.7 1560.0 440.0 406.8 375.0 337.2 338.9 302.9 355.8 372.6 403.9 381.1 60.3 56.7 48.5 60.0 57.3 47.2 44.1 48.0 51.0 56.3 74.8 70.8 68.6 76.2 73.6 69.7 64.7 67.9 60.8 69. 88.7 88.3 87.0 88.8 88.5 88.4 87.7 87.4 88.3 89.9 66.1 65.3 63.8 62.9 59.9 58.5 58.5 60.2 59.1 59.1 a 85.2 77.6 74.1 88.0 82.1 77.8 68.1 71.0 71.1 74.2 S 50.9 47.7 45.6 52.7 49.3 46.4 47.3 50.0 44.9 50. l 51.9 48.7 45.5 51.4 46.7 45.5 46.6 48.7 58.1 57.3 . s # 630 300 180 630 300 180 576 576 576 - 41.7 39.5 37.7 44.7 42.3 41.2 38.4 41.2 40.6 43.2 Method FUSION-X 3B FUSION-X 3B FUSION-X 3B FUSION-X 8B FUSION-X 8B FUSION-X 8B Cambrian-1 3B Cambrian-1 8B Florence-VL 3B Florence-VL 8B Vision Centric l h B 78.3 76.2 75.3 79.2 78.2 77.1 68.6 72.2 70.2 73. e 63.7 61.8 61.2 66.1 65.1 63.2 59.0 64.2 60.4 64.2 Knowledge Based OCR & Chart M 78.1 77.0 73.7 79.7 76.7 76.3 40.0 51.3 64.7 73. 2 79.2 77.3 74.6 81.4 79.6 76.5 73.9 73.0 73.8 74.2 i a 54.9 48.6 44.2 59.4 55.2 51.6 48.4 49.0 52.2 55.5 M 44.2 43.4 42.7 42.2 41.8 40.6 42.7 42.7 41.8 43.7 87.3 85.6 83.7 90.3 88.3 86.7 79.2 80.4 84.6 85.9 t 73.9 71.4 67.8 74.7 72.8 71.6 68.7 71.7 69.1 74.2 e O 63.7 56.9 47.3 66.6 59.5 54.2 54.6 62.4 63.0 63.4 r 75.8 67.7 56.1 79.8 73.0 63.2 66.8 73.3 70.7 74.7 c 71.1 63.5 47.5 77.8 66.0 54.4 66.1 77.8 70.8 75. egy balances the strengths of different token quantities and accelerates model convergence. As shown in Table 7, we evaluate both fixed numbers of latent tokens and dynamic sampling from different ranges. Our results show that increasing the number of latent tokens leads to notable improvements in OCR tasks, which is consistent with our ear4 Table 10. Comprehensive evaluation of FUSION models with varying latent visual token counts (0-256) across General, Vision-Centric, Knowledge-Based, and OCR & Chart benchmarks. Model General Benchmarks (a) Results on general multimodal benchmarks. . s n L # 0 4 16 64 144 256 - - - - Method FUSION-X 3B FUSION-X 3B FUSION-X 3B FUSION-X 3B FUSION-X 3B FUSION-X 3B Cambrian-1 3B Cambrian-1 8B Florence-VL 3B Florence-VL 8B B C z i O - M M M a - S 79.9 80.3 80.4 80.4 80.4 80.0 74.6 75.9 71.6 76.2 74.1 74.7 74.3 74.5 74.6 74.4 72.6 74.7 70.6 74.9 (b) Results on Vision centric, Knowledge based, and OCR & Chart benchmarks. 1563.4 1577.9 1584.3 1585.7 1600.4 1595.3 1497.0 1547.1 1498.7 1560.0 422.1 429.8 431.5 436.1 450.2 440.5 355.8 372.6 403.9 381.1 58.5 60.3 59.5 59.7 61.3 59.0 44.1 48.0 51.0 56.3 59.6 66.0 66.3 64.8 65.9 65.4 58.5 60.2 59.1 59. 87.2 88.2 88.5 88.7 88.8 88.9 87.7 87.4 88.3 89.9 74.0 74.3 74.8 75.0 74.7 74.2 64.7 67.9 60.8 69.5 a 81.4 86.2 83.4 88.0 85.2 85.2 68.1 71.0 71.1 74.2 S 50.9 50.5 49.6 50.0 49.5 48.1 47.3 50.0 44.9 50.0 a 51.0 51.4 51.6 51.4 51.5 51.5 46.6 48.7 58.1 57.3 Model Vision Centric Knowledge Based OCR & Chart . s n L # 0 4 16 64 144 256 - - - - d W R 61.3 62.1 62.5 62.8 63.4 63.8 59.0 64.2 60.4 64.2 - 41.2 41.4 41.5 41.4 41.8 41.6 38.4 41.2 40.6 43.2 Method FUSION-X 3B FUSION-X 3B FUSION-X 3B FUSION-X 3B FUSION-X 3B FUSION-X 3B Cambrian-1 3B Cambrian-1 8B Florence-VL 3B Florence-VL 8B n C 77.6 78.3 78.2 78.2 78.4 78.2 68.6 72.2 70.2 73.4 M 78.0 78.7 78.7 79.3 80.7 80.0 40.0 51.3 64.7 73.3 2 78.4 78.6 79.0 79.0 78.9 78.4 73.9 73.0 73.8 74. s t 52.6 53.3 53.2 53.8 54.6 54.2 48.4 49.0 52.2 55.5 M 44.7 43.7 43.9 43.7 43.3 44.2 42.7 42.7 41.8 43.7 S 86.1 87.1 86.8 87.0 86.8 86.8 79.2 80.4 84.6 85.9 t 68.9 72.2 73.2 74.6 75.0 75.1 68.7 71.7 69.1 74.2 e O 58.4 61.8 61.6 64.0 64.7 64.5 54.6 62.4 63.0 63.4 t C 69.2 73.3 74.8 76.3 76.5 77.1 66.8 73.3 70.7 74.7 c 62.5 66.1 69.4 73.6 75.9 77.4 66.1 77.8 70.8 75.1 lier findings. Conversely, reducing the number of latent tokens yields slight performance gains on general and visioncentric benchmarks. Furthermore, when extending the sampling range to {16, 64, 144, 256, 576}, the model does not exhibit significant performance improvements. We hypothesize that, with 576 latent tokens, the use of localized attention requires interpolating global information to much higher resolution, which may reduce information density and consequently limit performance gains. B.5. Effectiveness of Training Strategy In our main experiments, we unfreeze all parameters in every training stage. To assess the significance of this strategy, we conducted ablation studies by selectively freezing certain components of the model during different training phases. To reduce computational cost, we adopt the Phi-3.5mini-instruct+SigLip2 configuration and train each variant for 8000 steps in each stage. As shown in Table 8, fully unfreezing all parameters in each stage consistently results in superior performance. We attribute this to the fact that our framework departs from the conventional usage patterns of both the vision encoder and the language model. As result, the model must continually adapt to new processing paradigm, which requires end-to-end optimization across all components. B.6. Supplementary Results We provide detailed metrics and corresponding evaluation results for the models presented in Figure 6a and Figure 6b of the main paper. For the models shown in Figure 1, 5 Table 11. Comprehensive Benchmark Results Across All Models. We present detailed comparison of benchmark metrics for all evaluated models, enabling thorough assessment of their performance across various evaluation tasks. I e e 74.1 67.9 74. 73.6 76.3 74.8 59.0 60.9 66. 69.7 VITA-1.5-8B Molmo-1B Molmo-7B Eagle-X5-7B Eagle-X5-8B Eagle-X5-13B InternVL2.5-1B InternVL2.5-2B InternVL2.5-4B InternVL2.5-8B LLaVA-OneVision(SI)-0.5B 62. LLaVA-OneVision(SI)-7B VILA1.5-3B VILA1.5-8B VILA1.5-13B Cambrian-8B Cambrian-13B Florence-VL-3B Florence-VL-8B Qwen2-VL-2B Qwen2-VL-7B Qwen2.5-VL-3B Qwen2.5-VL-7B InternVL2-1B InternVL2-2B InternVL2-4B InternVL2-8B Phi-3.5-Vision-4B Phi-4.0-5B LLaMA3.2-11B LLaVA-v1.5-7B LLaVA-v1.5-13B LLaVA-Next-Vicuna-7B LLaVA-Next-Vicuna-13B MM1.5-1B MM1.5-3B MM1.5-7B FUSION-X 3B(Ours) FUSION-X 8B(Ours) FUSION-L 3B(Ours) FUSION-L 8B(Ours) 75.4 68.0 65.0 72.7 73.3 73. 70.6 74.9 72.4 76.0 74.0 77. 65.2 70.9 73.2 75.4 69.7 73. 72.7 65.8 68.2 69.6 71.4 70. 72.4 73.4 75.3 78.2 74.1 75. T h B 79.8 64.6 73. 70.1 75.9 75.7 70.7 74.7 81. 84.6 55.8 80.5 64.5 62.1 74. 74.6 73.2 71.6 76.2 74.6 82. 79.1 83.2 65.2 73.4 78.5 82. 76.0 83.4 68.0 66.5 69.2 69. 70.0 65.2 68.1 72.4 80.3 82. 77.6 80.0 S c M M 78.6 2328 46.6 1470 75.2 1853 62.5 1866 70.6 1922 70.4 66.3 1950 71.9 2138 79.3 2338 82.6 2344 50.2 1392 78.0 56.2 1647 60.4 1698 67.7 1718 67.9 1802 65.0 1876 60.8 69.5 1941 73.8 1899 80.6 2276 78.1 2157 82.8 2312 61.0 71.2 1864 73.9 2064 80.9 2215 66.1 1838 77.7 1961 65.8 46.4 1808 58.2 1780 62.3 1769 68.5 1745 63.1 1611 65.4 69.5 1861 M 52.7 37.8 41.5 33. 53.3 52.6 47.2 62.6 61.5 62. 32.2 50.6 38.8 41.9 45.0 48. 48.9 51.0 56.3 51.5 61.8 61. 69.7 31.5 39.7 50.9 54.3 43. 51.9 57.6 32.9 35.6 40.2 44. 37.4 41.0 42.2 74.8 76.2 70. 73.6 2011 60.3 1955 60.0 1980 56.7 1938 57.3 U M 52.6 33.9 49.1 37.6 43.8 41. 41.2 43.2 51.8 56.2 36.2 46. 34.2 37.4 41.1 41.8 41.6 41. 43.7 42.2 53.7 53.1 58.0 36. 36.3 48.3 51.2 44.6 56.0 48. 35.7 37.0 37.6 37.3 35.8 37. 41.8 44.2 42.2 43.4 41.8 e a 72.8 66.7 72.4 59.5 72.0 70. 60.2 62.7 84.3 87.3 61.7 77. 65.5 71.7 73.5 71.0 76.1 71. 74.2 50.5 70.1 77.0 91.0 52. 60.0 68.6 73.3 63.7 66.5 85. 61.8 66.1 72.7 73.9 71.6 73. 74.2 85.2 88.9 77.6 82.1 2 79.2 86.4 91.0 73.6 76.1 74. 69.0 74.9 81.4 84.6 53.5 82. 57.9 58.8 69.9 74.6 73.6 73. 74.2 74.7 83.0 81.4 84.3 87. 94.1 96.3 97.1 88.9 97.3 77. 55.5 61.1 67.0 72.2 82.1 85. 89.6 79.2 81.4 77.3 79.2 T c c 95.8 87.5 58.1 71.2 84.3 82. 93.1 96.0 97.6 98.4 67.5 86. 69.2 73.2 79.1 81.0 79.3 84. 85.9 78.7 85.5 81.4 89.0 87. 94.1 96.3 97.1 88.9 97.3 83. 69.2 72.6 70.3 73.7 82.1 85. 89.6 87.3 90.3 85.6 88.3 l W e 66.9 60.4 71.1 63.8 66. 64.8 57.5 60.1 64.3 70.1 46. 69.5 53.2 53.2 53.3 60.0 58. 60.4 64.2 60.7 68.5 65.5 68. 50.2 57.4 60.5 64.2 53.6 64. 63.3 54.8 55.3 57.8 57.6 53. 56.9 62.5 63.7 66.1 61.8 65. V t 70.3 78.8 81.7 70.5 77. 75.5 72.0 74.3 76.8 79.1 65. 78.5 67.2 68.5 65.0 72.6 72. 69.1 74.2 79.7 84.3 79.3 84. 70.9 73.4 74.7 77.4 61.1 76. 75.3 45.5 48.9 63.8 66.9 72. 75.6 76.5 73.9 74.7 71.4 72. E t C 84.1 78.0 84.1 66. 80.1 77.6 75.9 79.2 84.0 84. 60.5 80.9 50.7 52.7 59.5 72. 73.8 70.7 74.7 73.5 83.0 84. 84.0 67.8 71.7 80.7 84.3 72. 83.0 80.0 17.8 18.2 54.3 61. 67.2 74.2 78.6 75.8 79.8 67. 73.0 T C 92.2 77.1 92.2 80. 86.6 85.9 84.8 88.7 91.6 93. 70.0 87.5 39.2 40.6 58.6 77. 76.8 75.9 78.8 90.1 94.5 93. 95.7 81.7 86.9 89.2 91.6 75. 93.2 87.5 40.6 50.1 57.7 65. 81.0 87.7 88.1 71.1 77.8 63. 66.0 P 88.9 86.6 89.0 89. 89.8 89.8 89.9 90.6 90.9 90. 86.8 87.2 86.8 83.3 85.0 86. 86.8 88.3 89.9 87.3 88.4 87. 85.9 84.9 85.2 84.6 84.2 82. 85.9 88.1 86.1 88.4 87.5 87. 84.2 85.6 86.3 88.7 88.8 88. 88.5 e O 74.1 54.7 65.6 55. 62.6 61.9 77.4 80.2 82.0 82. 56.5 69.7 43.7 43.8 46.0 61. 61.0 63.0 63.4 79.7 84.3 82. 88.8 75.5 78.1 78.4 79.4 59. 84.4 75.3 31.8 33.7 53.2 53. 60.5 65.7 63.5 s 60. 43.1 56.1 41.7 53.0 53.7 51. 54.3 58.7 63.2 37.5 56.7 40. 39.7 44.2 50.7 47.1 44.9 50. 47.5 60.7 56.3 64.1 45.6 49. 53.9 61.5 47.5 58.9 49.8 33. 34.3 37.6 40.4 30.2 35.6 42. 50.9 63.7 52.7 66.6 47.7 54.9 49.3 59.5 e i l 44. 38.0 46.4 35.4 50.0 47.8 39. 42.3 46.6 49.0 31.7 47.5 31. 35.3 39.3 30.6 39.4 58.1 57. 42.4 50.4 46.3 51.9 34.3 38. 42.4 45.0 40.5 40.5 40.3 27. 24.5 27.6 31.8 30.2 35.1 40. s t 66.2 37.6 48.7 38. 52.7 54.4 47.1 51.1 60.8 64. 32.7 58.6 31.8 37.4 42.3 48. 47.7 52.2 55.5 48.0 61.6 62. 68.1 40.4 47.0 58.5 58.3 43. 65.8 47.7 25.5 27.7 32.5 35. 37.2 44.4 47.2 54.9 51.9 59.4 51.4 48.6 48. 55.2 46.7 6 Figure 8. Comparison of Modality Alignment with traditional MLLMs. We conduct comparative analysis of modality alignment across three different levels using FUSION, LLaVA, and LLaVA-NeXT. To ensure fair evaluation, we adopt consistent visualization and augmentation strategies across all models. we primarily relied on the open leaderboard1 provided in [16], supplemented by results reported in individual papers as well as our own reproduction experiments. The results corresponding are presented in Table 11 and Table 9, Table 10, respectively. C. Comparison and Deeper Exploration of"
        },
        {
            "title": "Model Architectures",
            "content": "C.1. Compare with Traditional MLLMs In this section, we provide comparative analysis between our model and traditional MLLMs using additional heatmap visualizations. However, due to the unavailability of public weights and code for [20], we are unable to include pixellevel alignment visualizations for that model. the"
        },
        {
            "title": "For",
            "content": "space-level evaluation of and questionlevel alignment, we adopt LLaVA[43] and LLaVAOneVision[37] as our baselines. As shown in Figure 8, our model demonstrates significantly stronger alignment in both aspects. Specifically, the absence of Dual-Supervised Semantic Mapping Loss in LLaVA and LLaVA-OneVision prevents these models from achieving meaningful semantic alignment, resulting in poor performance in space-level alignment. In terms of question-level alignment, our model consistently focuses more accurately on semantically relevant regions in response to the query. These results highlight 1Available at: https : / / huggingface . co / spaces / opencompass/open_vlm_leaderboard 7 the effectiveness of our overall architecture and confirm the benefits introduced by our alignment strategies. C.2. Beyond Single Questions key limitation of Text-Guided Unified Vision Encoding In real-world lies in its handling of multi-turn dialogue. scenarios, it is impossible to anticipate future user questions in advance. As result, image encoding is typically performed based only on the initial question or the current question at hand, which can lead to representational bias in the visual features. Our proposed Context-Aware Recursive Alignment Decoding offers suboptimal yet effective solution to this challenge. As described in the main text, we initialize the questionaware latent tokens using conventional image encoding without any text guidance. This initialization helps mitigate the encoding bias introduced by question-specific image representations. Furthermore, we dynamically update the latent tokens based on each incoming question. This design acts as compensation mechanism for any prior representational drift introduced by previous questions. To further evaluate our models ability to support multi-turn dialogue, we adopt ConvBench[45], benchmark specifically designed to measure multi-turn dialogue capabilities in vision-language models. Using our pretrained model, we assess its performance in this setting. Results are reported in Table 12. While the use of Text-Guided Unified Vision Encoding alone may result in slight drop in performance for multi-turn tasks compared to simple encoder, the inTable 12. Comparison of Performance for LVLMs on ConvBench with Direct Grading method. S1, S2, S3, and S0 correspond to the average scores of the first turn, second turn, third turn, and overall, respectively. ˆS2, ˆS3, and ˆS0 represent the scores for the second turn, third turn, and overall when the first answer is given as premise. indicates the score when the first two answers are given as premise. R2 is defined as (S1 + S2 + S3)/3, indicating the mean performance over three turns. Meanwhile, R1 is computed as (R2 + S0)/2, representing the models overall score. TUNE CARD R1 S1 S2 S3 S0 Ablation ˆS2( ˆS2 S2) ˆS3( ˆS3 S3) ˆS0( ˆS0 S0) S3( S3 ˆS3) S0( S0 ˆS0) 5.31 5.38 5.45 5.59 5.11 5. 6.81(+1.22) 6.61(+1.50) 7.00(+1.76) 6.43(-0.18) 7.82(+0.82) 5. 5.35 5.36 5.63 5.07 5.11 6.87(+1.24) 6.55(+1.48) 6.98(+1.87) 6.31(-0.24) 7.87(+0.89) 5.19 5. 5.42 5.54 4.96 5.09 6.61(+1.07) 6.47(+1.51) 6.92(+1.83) 6.39(-0.08) 7.76(+0.84) Model R1 S1 S2 S3 S0 General ˆS2( ˆS2 S2) ˆS3( ˆS3 S3) ˆS0( ˆS0 S0) S3( S3 ˆS3) S0( S0 ˆS0) GPT-4V 7.09 7. 7.30 7.48 7.12 6.88 8.23(+0.75) 8.00(+0.88) 8.25(+1.37) 7.34(-0.66) 8.18(-0.07) Claude 6.54 6. 6.53 7.04 6.68 6.32 7.48(+0.44) 7.06(+0.38) 7.55(+1.23) 7.18(+0.12) 8.13(+0.58) FUSION-X-3B 6.03 6. 6.35 6.28 5.66 5.97 7.28(+1.00) 6.81(+1.15) 7.46(+1.49) 6.49(-0.32) 8.15(+0.69) ShareGPT4V-7B 5.83 5. 6.02 6.14 5.80 5.67 7.19(+1.05) 6.77(+0.97) 7.31(+1.64) 6.93(+0.16) 8.19(+0.88) XComposer2 5.82 5. 5.98 6.17 5.78 5.66 7.35(+1.18) 7.04(+1.26) 7.66(+2.00) 7.00(-0.04) 8.20(+0.54) FUSION-X-8B 5.70 5. 6.12 6.08 5.27 5.58 7.09(+1.01) 6.45(+1.18) 7.22(+1.64) 6.64(+0.19) 8.01(+0.79) InternVL-Chat-V1-5 5.60 5. 6.11 5.93 5.25 5.43 6.34(+0.41) 5.60(+0.35) 6.50(+1.07) 6.32(+0.72) 7.79(+1.29) Qwen-VL-Chat 5.54 5. 5.96 5.78 5.22 5.43 7.04(+1.26) 6.53(+1.31) 7.26(+1.83) 6.57(+0.04) 8.00(+0.74) InternVL-Chat-V1-2 5.49 5. 5.80 5.88 5.39 5.29 6.66(+0.78) 6.12(+0.73) 6.75(+1.46) 6.31(+0.19) 7.70(+0.95) LLaVA-V1.5-7B 5.16 5. 4.95 5.59 5.34 5.03 7.28(+1.69) 6.68(+1.34) 7.28(+2.25) 6.72(+0.04) 7.97(+0.69) mPLUG-Owl2 5.04 5. 4.98 5.38 5.14 4.91 6.77(+1.39) 6.64(+1.50) 7.22(+2.31) 5.93(-0.71) 7.62(+0.40) LLaVA-V1.5-13B 4.94 5. 5.03 5.41 4.99 4.74 7.43(+2.02) 7.13(+2.14) 7.70(+2.95) 6.14(-0.99) 7.60(-0.10) ShareGPT4V-13B 4.85 5. 5.16 5.06 4.86 4.67 7.42(+2.36) 7.17(+2.31) 7.65(+2.98) 6.24(-0.93) 7.65(+0.00) troduction of CARD leads to significant improvement in overall dialogue capabilities. In fact, our model surpasses the performance of static encoding-based baselines. Additionally, Figure 9 showcases qualitative examples of multi-turn interactions conducted by FUSION-X 3B, demonstrating its ability to engage in complex, multi-turn tasks such as captioning, reasoning, and creative generation. This illustrates that our model moves beyond functioning as simple answer machine, and instead exhibits robust, context-aware dialogue capabilities. D. Synthesized Language-Driven Framework"
        },
        {
            "title": "Process",
            "content": "D.1. Prompt Design and Data Generation We present the data generation prompts used in both the Pretraining and Finetuning stages. For the Pretraining data, we generated five distinct types of data: SynthColor, SynthCount, SynthSpatial, SynthText, and SynthScene. Each of these datasets was generated using single prompt, and the details of these prompts are provided in Table 14, Table 15, Table 16, Table 17, Table 18. In the Finetuning phase, specifically during Stage 1.5, we generated seven categories of data, including SynthMultiChoice, SynthConvLong, SynthConvShort, SynthReasoning, SynthTextQA, SynthContrastLong, and SynthContrastShort. Among these, SynthMultiChoice, SynthConvLong, SynthConvShort, and SynthReasoning were first created based on Captions to generate descriptions. These descriptions were then used to generate the corresponding QA pairs. The details of these prompts are provided in Table 24, Table 20, Table 22, Table 19. more complex procedure was employed for SynthTextQA. Given the complexity of the OCR task and the limitations posed by caption elements, we first generated SynthText from the caption. Then, SynthText was used to generate Descriptions, and finally, the 8 Figure 9. Illustration of our models multiturn dialogue QA pairs were produced. This approach ensures that the generated descriptions contain text elements, resulting in higher-quality QA pairs. The prompts are provided in Table 28, 29. For the SynthContrastLong and SynthContrastShort categories, the challenge was to highlight the detailed contrast between similar images. To address this, we carefully designed prompt capable of generating two descriptions, each corresponding to an image, along with QA pairs that emphasize the differences between these two descriptions. The design of these prompts are presented in Table 27, 26. The primary focus of the prompts in this phase was to prioritize variability in QA pairs, thereby enhancing the models generalization across diverse multimodal contexts. In Stage 2, we generated SynthMultiChoice, SynthConvLong, and SynthConvShort, using similar process to Stage 1.5. The details of these prompts are presented in Table 25, 21, 23.The design of the prompts in this stage focused on enhancing the models ability to follow specific instructions for various tasks."
        },
        {
            "title": "We present specific examples and forms of the different",
            "content": "data types in Figures 10, 11, 12,13,14,15,16,17,18,19. D.2. QA Pairs Data Filtering Generative models inherently introduce challenges, including ambiguity, missing information, and inconsisten cies in generated content. To address these issues, we implement rigorous, multi-stage filtering process. The pretraining data filtering process we adopt follows the approach outlined in [48]. We primary focus on the selection of finetuning data. Our filtering procedure consists of four main stages, each designed to ensure the quality and relevance of the data used in subsequent training steps. 9 Table 13. Metric and Prompt used for Caption Filtering"
        },
        {
            "title": "Caption Filtering",
            "content": "## Rule-Based Metrics Alphanumeric Filter: Tokenization: false, Min ratio: 0.60 Character Repetition Filter: Rep length: 10, Max ratio: 0.09373663 Flagged Words Filter: Language: en, Tokenization: false, Max ratio: 0.0 Perplexity Filter: Language: en, Max perplexity: 5500.0 Special Characters Filter: Min ratio: 0.16534802, Max ratio: 0. Word Repetition Filter: Language: en, Tokenization: false, Rep length: 10, Max ratio: 0.03085751 Image-Text Matching Filter: HF BLIP: Salesforce/blip-itm-base-coco, Min score: 0.8, Max score: 1.0, Horizontal flip: false, Vertical flip: false, Reduce mode: avg, Any or all: any, Mem required: 1500MB Image-Text Similarity Filter: HF CLIP: openai/clip-vit-base-patch32, Min score: 0.28 ## Prompt Assume you are an expert in the field of AI image generation. Your goal is to select high-descriptive prompts that will enable the successful generation of images. will provide you with specific descriptive prompt, and your task is to evaluate it thoroughly. Consider the prompts level of detail, its logical coherence, and the clarity with which it describes the desired image. It is essential to assess whether the prompt contains sufficient information to guide the diffusion model effectively, ensuring that it can produce an image that meets expectations. You should only respond with Yes or No. D.2.1 Caption Filtering D.2.2 Description Filtering The first step focuses on selecting high-quality captions for generating the QA pairs. The captions are sourced from the Datacomp[19] dataset, which contains significant amount of low-quality or irrelevant data. Prior to generating descriptions, we conduct filtering process to remove lowquality captions, such as those containing watermarks, advertisements, or non-English text. Initially, rule-based filtering methods are applied, including: The second step focuses on selecting high-quality descriptions for generating QA pairs and images. The 10 million captions selected in Step 1 are input into LLaMA3.1 70B to generate corresponding descriptions. similar filtering approach is employed for descriptions as in Step 1, with the key difference being the exclusion of image-text matching calculations. After filtering, approximately 12 million descriptions are retained. Removal of character-level and word-level repetitions, D.2."
        },
        {
            "title": "Image Generation Filtering",
            "content": "Removal of captions with high perplexity values, Ensuring strong image-text alignment between the caption and its corresponding original image. This step ensures that the selected captions are better aligned with real-world images and more easily extensible. Subsequently, we use LLM to score these captions, ensuring their semantic correctness and syntactic richness. The scoring process emphasizes the diversity of the elements in the captions. The prompt, along with the specific rule-based methods, is outlined in Table 13. For the filtering process, we use Data-Juicer for rule-based filtering and LLaMA3.1 70B for scoring. After this step, approximately 20 million captions are retained. This step aims to ensure the quality of the generated images and their alignment with the corresponding descriptions. Let represent the generated image, and represent the corresponding description. We first calculate the ClipScore between the description and the generated image to measure the alignment. To assess the image quality, we utilize the Structural Similarity Index (SSIM) metric. As described in the main paper, will be resized and processed by Siglip. Additionally, will be divided into four non-overlapping subimages, each processed individually through Siglip. The SSIM is computed both between the resized image and its original version, as well as between each sub-image and its corresponding resized sub-image."
        },
        {
            "title": "Let the height and width of the image be denoted as H",
            "content": "10 the ClipScore between the statement and the corresponding image. The final score for QA pair is the average of the ClipScores for each declarative sentence within the QA pair: Final Score ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 ClipScore(QAi, I) where is the number of questions in the QA pair. The QA correctness is evaluated using the LLaVA-OneVision 72B, while the transformation to declarative sentences is carried out with LLaMA3.1-70B. After filtering, we retain approximately 4 million QA pairs. and , respectively, and let the crop size used in Siglip be S. The first step is to calculate the SSIM between the original image and the resized image: SSIMw = SSIM (I, Resize (Resize(I, (S, S)), (H, ))) For the four sub-images, we first partition the original image into four non-overlapping regions. Specifically, each subimage Isub(i,j) corresponds to section of the image, where and {1, 2} indicate the row and column of the subimage. The extraction of the (i, j)-th sub-image is defined as: Isub(i,j) = (cid:20) 2 (i 1) : 2 i, 2 (j 1) : (cid:21) 2 The SSIM between each sub-image and its resized version is computed as: SSIMs(i, j) = SSIM (cid:0)Isub(i,j), (cid:18)"
        },
        {
            "title": "Resize",
            "content": "Resize(Isub(i,j), (S, S)), (cid:19)(cid:19)(cid:19) (cid:18) 2 , 2 Next, we compute the overall SSIM score for the entire image by combining the SSIM of the full image (SSIMw) and the SSIM values for each of the four sub-images (SSIMs(i, j)): SSIMa = w1 SSIMw + 2 (cid:88) 2 (cid:88) i=1 j=1 w2 SSIMs(i, j) where w1 is the weight for the SSIM of the full image and w2 is the weight for each sub-images SSIM. These weights are chosen to appropriately balance the contribution of the full image and sub-images to the overall image quality score. Finally, the total score for the image is computed by combining the ClipScore with the overall SSIM score SSIMa, using weighting factor λ: Total Score = ClipScore(I, D) + λ SSIMa In our implementation, w1 is set to 1 and w2 is set to 0.25. λ is set to 0.5 to banlance the importance of the SSIM score and ClipScore. After this filtering step, we retain approximately 6 million images. D.2.4 QA Pair Filtering The final step involves ensuring the quality and relevance of the generated QA pairs. We begin by using MLLM to verify the correctness of the answers within each QA pair, filtering out all incorrect QA pairs. Subsequently, we convert each QA pair into declarative sentence and compute 11 Table 14. Prompt used for SynthColor"
        },
        {
            "title": "SynthColor",
            "content": "## Role Assume you are an AI visual assistant. ## Goal will provide you with brief description of an image. Your primary task is to accurately construct this image in your mind. Based on your understanding and the basic image, you need to adjust the colors of objects to enhance the expressiveness and visual information of the image. If the original description is vague or insufficient, creatively adjust or add color elements to improve clarity and impact. ## Rule To maintain harmony: The number of color types should not exceed three The overall description should be concise Colors should be diverse, avoiding repetition After modifying, you need to provide new description that specifies the color of each significant object. Ensure that the new description is: Logically consistent Overall harmonious Focused on visual information Minimizing irrelevant content No more than 12 words ## Output Format Present the description content directly, without any extraneous information. Table 15. Prompt used for SynthCount"
        },
        {
            "title": "SynthCount",
            "content": "## Role Assume you are an AI visual assistant. ## Goal will provide you with brief description of an image. Your primary task is to accurately construct this image in your mind. Based on your understanding and the basic image, you need to modify the quantity of objects to enhance the expressiveness and the visual information of the image. If the original description is vague or insufficient, creatively adjust or add elements to improve clarity and impact. ## Rule To maintain harmony: The quantity of each object type should not exceed three The total number of objects should not exceed six After modifying, you need to provide new description that quantifies each significant object type. Ensure that the new description is: Logically consistent Overall harmonious Focused on visual information Minimizing irrelevant content No more than 12 words ## Output Format Present the description content directly. Here are some possible formats: 1. Three balloons float above table where two children play board games. 2. Two dark mountains loom above the water with three people at the shoreline. 12 Table 16. Prompt used for SynthSpaital"
        },
        {
            "title": "SynthSpaital",
            "content": "## Role Assume you are an AI visual assistant. ## Goal will provide you with brief description of an image. Your primary task is to accurately construct this image in your mind, focusing particularly on the spatial arrangement and relative positions of objects within it. Based on your understanding, you need to modify the positions of objects to highlight their interactions and improve spatial coherence. If the original description lacks detail on positioning, creatively adjust or add elements to clarify spatial relationships and enhance visual impact. ## Rule Ensure that the arrangement is logical and maintains aesthetic harmony. After modifying, you need to provide new description that quantifies each significant object type. Ensure that the new description is: Logically consistent Overall harmonious Focused on visual information Minimizing irrelevant content No more than 12 words ## Output Format Present the description content directly. Here are some possible formats: 1. Bicycle leaned against the left side of park bench under tree. 2. Bed against the right wall, with luggage beside it, near curtained windows. 3. Woman sits inside dumpster, back against the left wall, surrounded by trash. Table 17. Prompt used for SynthText"
        },
        {
            "title": "SynthText",
            "content": "## Role Assume you are an AI visual assistant. ## Goal will provide you with brief description of an image. Your primary task is to accurately construct this image in your mind. Based on your understanding and the basic image, you need to add textual content to enhance the expressiveness and the visual information of the image, and these texts should be closely related to the theme and visual elements of the image. ## Rule The textual information can include: Words and numbers on signs Billboards Posters Digital displays Graffiti Slogans Traffic directions Road signs Historical markers Famous quotes Additionally, you can add other types of text as needed to ensure the diversity and richness of the text. You can also create visual content appropriately if the given description is not suitable for adding text. After adding text, you need to provide new description that includes the text you added. Remember: Your response must contain text You must specify the type for each text The description should be logically consistent Focus on visual information No more than 12 words ## Output Format Present the description content directly. Here are some possible formats: 1. vibrant mural displays Dream Big! with colorful butterflies and stars. 2. street sign reads Main St with red arrow pointing right. Table 18. Prompt used for SynthScene"
        },
        {
            "title": "SynthScene",
            "content": "## Role Assume you are an AI visual assistant. ## Goal will provide you with brief description of an image. Your primary task is to accurately construct this image in your mind. Based on your understanding and the basic image, you need to adjust or enhance the scene elements to improve the expressiveness and visual information of the image. ## Rule If the original description is vague or insufficient, creatively adjust or add scene elements such as: Landmarks Iconic buildings Natural landscapes Urban settings Specific environments Historical sites Famous monuments Picturesque vistas Distinctive locations Remember: Each caption must contain only one scene element Scene elements should be diverse (avoid repetition like multiple Eiffel Towers) Description should be logically consistent and harmonious Focus on visual information, minimize irrelevant content Response must be no more than 12 words ## Output Format Present the description content directly, without any extraneous information. Table 19. Prompt used for SynthReasoning"
        },
        {
            "title": "Reasoning",
            "content": "## Role Suppose you are an AI visual assistant, viewing detailed description of an image. ## Goal The task is to use the provided description of an image, create some plausible questions about the image, and provide the answers in detail. You can create complex questions beyond describing the scene. Make the question challenging by not including the visual content details in the question so that the user needs to reason about that first. To answer such questions, you should require first understanding the visual content, then based on the background knowledge, think through the solution step by step. Either explain why the things are happening that way, or provide guides and help to users request. ## Rule Please give the Q&A content directly without any extraneous words and separate questions and answers with and A. Ensure that the answer to each question is no less than 100 words. Additionaly, you should control the QA pairs not exceeding 6 pairs to preserve visual clarity. 14 Table 20. Prompt used for Stage1.5 SynthConvLong Stage1.5-SynthConvLong ## Role Suppose you are an AI visual assistant, viewing detailed description of an image. ## Goal Design conversation where user asks wide range of questions about an image, and the AI visual assistant responds based on what it sees. The questions should cover broad spectrum of aspects related to the image, ensuring variety and inclusivity of different types of inquiry. The aim is to explore the image from multiple angles and include different kinds of questions to create diverse, dynamic conversation. ## Rule The questions should explore the following aspects: Object types: What objects are visible in the image? What are their characteristics? Counting objects: How many of certain type of object or entity can be seen in the image? Object actions: Are objects being used or manipulated in any way? Object locations: Where are specific objects located relative to one another in the image? Relative positions between objects: How are objects positioned in relation to one another in the scene? Background Elements: What is the background environment like? What elements or features are visible in the backdrop? Lighting & Color: What kind of lighting or color schemes dominate the image? How does the lighting affect the mood or feel of the scene? Textures and Details: Are there any significant textures or small details in the image that stand out? Ensure that only questions with clear, confident answers are included: 1. The content referenced in the question is clearly visible in the image, and the answer can be confidently provided. 2. It is clear from the image that the content is not present. Whats more, the conversation should span these different dimensions, including variations in the types of questions to ensure the dialogue explores rich variety of observations about the image. Ensure that the conversation is logically consistent, focuses on describing the visual information, and avoids unnecessary distractions. Keep the number of QA pairs to no more than 8 to maintain clarity. ## Example Output Format Q: How many people are visible in the image? A: there are two people visible, one sitting at desk and the other standing near the window. Q: What are the people doing in the image? A: The person sitting at the desk appears to be typing on laptop, while the other person is holding cup, likely standing near the coffee machine. Q: What is in the background of the image? A: In the background, theres large window with some greenery visible outside, suggesting its sunny day. Q: How is the room lit? A: The room is brightly lit by natural sunlight streaming in through the window, with soft shadows from the furniture. Q: Is there any art or decoration on the walls? A: Yes, there is large modern painting hanging on the wall above the desk. 15 Table 21. Prompt used for Stage2 SynthConvLong Stage2-SynthConvLong ## Role Suppose you are an AI visual assistant, viewing detailed description of an image. ## Goal Design conversation where user asks questions about an image, and the AI visual assistant responds with high precision and specificity, based solely on what is clearly visible in the image. Each answer should be directly aligned with the visual content, ensuring that it is 100% accurate and reflects the scene as it appears. The questions should be carefully framed to avoid any speculative or unclear answers. ## Rule Key aspects to focus on: Object types: Identify objects and details based on their distinct visual attributes, ensuring the description matches what is observable in the image. Counting objects: Provide exact numbers of objects or entities visible, with no guesswork. Object actions: Only describe actions that can be clearly observed in the image, with no assumptions. Object locations: Describe precise spatial relationships between objects based on their positioning in the image. Relative positions between objects: Indicate how objects are arranged in relation to one another, using clear visual evidence from the image. Ensure that only questions with clear, confident answers are included: 1. The content referenced in the question is clearly visible in the image, and the answer can be confidently provided. 2. It is clear from the image that the content is not present. Whats more, avoid asking questions that cannot be confidently answered from the image. The conversation can include more complex questions, such as asking about the background knowledge of the objects in the image or discussing events happening in the image. However, make sure these questions are based on clear visual evidence from the image and not uncertain speculation. Ensure that the conversation is logically consistent, focuses on describing the visual information, and avoids unnecessary distractions. Keep the number of QA pairs to no more than 8 to maintain clarity. ## Example Output Format Q: How many lamps are visible in the image? A: here are two lamps, both on either side of the room, near the walls. Q: What color is the couch? A: The couch is deep brown color, with smooth leather texture. Q: Where is the desk located in the image? A: The desk is located against the far wall of the room, with chair placed in front of it. Q: What is the shape of the rug? A: The rug is rectangular, positioned underneath the coffee table. Q: Is there TV in the room? A: No, there is no television visible in the room. Table 22. Prompt used for Stage1.5 SynthConvShort Stage1.5-SynthConvShort ## Role Suppose you are an AI visual assistant, viewing detailed description of an image. ## Goal Design conversation where user asks questions about an image, and the AI visual assistant responds with brief, concise, and tothe-point answers. The focus is on providing short responses that directly match what is visible in the image, with no elaboration. The AI should answer clearly and simply, avoiding long explanations or unnecessary details. The questions should cover broad spectrum of aspects related to the image, ensuring variety and inclusivity of different types of inquiry. The aim is to explore the image from multiple angles and include different kinds of questions to create diverse, dynamic conversation. ## Rule The questions should explore the following: Object types: What objects are visible in the image? What are their characteristics? Counting objects: How many of certain type of object or entity can be seen in the image? Object actions: Are objects being used or manipulated in any way? Object locations: Where are specific objects located relative to one another in the image? Relative positions between objects: How are objects positioned in relation to one another in the scene? Background Elements: What is the background environment like? What elements or features are visible in the backdrop? Lighting & Color: What kind of lighting or color schemes dominate the image? How does the lighting affect the mood or feel of the scene? Textures and Details: Are there any significant textures or small details in the image that stand out? Ensure that only questions with clear, confident answers are included: 1. The content referenced in the question is clearly visible in the image, and the answer can be confidently provided. 2. It is clear from the image that the content is not present. Whats more, the conversation should span these different dimensions, including variations in the types of questions to ensure the dialogue explores rich variety of observations about the image. Ensure that the conversation is logically consistent, focuses on describing the visual information, and avoids unnecessary distractions. Keep the number of QA pairs to no more than 8 to maintain clarity. ## Example Output Format Q: What furniture is in the room? A: Sofa and table Q: What is placed on the table? A: Coffee cup Q: What is the color of the carpet? A: Grey Q: Where is the clock located? A: On the wall 17 Table 23. Prompt used for Stage2 SynthConvShort Stage2-SynthConvShort ## Role Suppose you are an AI visual assistant, viewing detailed description of an image. ## Goal Design conversation where user asks questions about an image, and the AI visual assistant responds with high precision and specificity, based solely on what is clearly visible in the image. The answers should be as brief as possible, ensuring that they directly reflect the visible elements in the image. Each answer should be directly aligned with the visual content, ensuring that it is 100% accurate and reflects the scene as it appears. The questions should be carefully framed to avoid any speculative or unclear answers. ## Rule Key aspects to focus on: Object types: Identify objects and details based on their distinct visual attributes, ensuring the description matches what is observable in the image. Counting objects: Provide exact numbers of objects or entities visible, with no guesswork. Object actions: Only describe actions that can be clearly observed in the image, with no assumptions. Object locations: Describe precise spatial relationships between objects based on their positioning in the image. Relative positions between objects: Indicate how objects are arranged in relation to one another, using clear visual evidence from the image. Ensure that only questions with clear, confident answers are included: 1. The content referenced in the question is clearly visible in the image, and the answer can be confidently provided. 2. It is clear from the image that the content is not present. Whats more, avoid asking questions that cannot be confidently answered from the image. The conversation can include more complex questions, such as asking about the background knowledge of the objects in the image or discussing events happening in the image. However, make sure these questions are based on clear visual evidence from the image and not uncertain speculation. Ensure that the conversation is logically consistent, focuses on describing the visual information, and avoids unnecessary distractions. Keep the number of QA pairs to no more than 8 to maintain clarity. ## Example Output Format Q: Is the laptop open or closed? A: Open Q: How many chairs are around the table? A: Four Q: Is there dog visible in the image? A: No Q: What is the person doing? A: Reading 18 Table 24. Prompt used for Stage1.5 SynthMultiChoice Stage1.5-SynthMultiChoice ## Role Imagine you are an AI visual assistant tasked with analyzing detailed description of an image. ## Goal You are guiding user through understanding the image by asking multiple-choice and yes/no questions. Each question should help the user explore the image from variety of perspectives. The goal is to generate dynamic conversation that covers as many aspects of the visual content as possible, ensuring diverse and varied inquiries. ## Rule The types of questions should involve range of topics, including but not limited to: Object types: What types of objects can be seen in the image? What are their characteristics? Counting objects: How many instances of particular object or entity are visible? Object actions: Are any objects being manipulated, used, or interacted with? Object locations: Where are specific objects or entities located in the image? Relative positions between objects: How are the objects positioned in relation to one another in the scene? Background Elements: What does the background consist of? Are there any environmental features or structures visible? Lighting & Color: What types of lighting are visible? How does the lighting affect the visual atmosphere of the scene? Textures and Details: Are there any notable textures, intricate details, or small visual elements in the image? Ensure that only questions with clear, confident answers are included: 1. The content referenced in the question is clearly visible in the image, and the answer can be confidently provided. 2. It is clear from the image that the content is not present. Ensure that each question brings out different angle of the image, covering wide range of observations and keeping the conversation engaging and informative. The questions should vary in scope, from general descriptions to more specific details, allowing the user to explore various facets of the image. Ensure that the conversation is logically consistent, focuses on describing the visual information, and avoids unnecessary distractions. Keep the number of QA pairs to no more than 8 to maintain clarity. ## Example Output Format What is the object located in the center of the image? A. vase B. sculpture C. pillow D. mirror A: Q: Is the room illuminated by sunlight? A: Yes Q: What type of floor covering is visible? A. Carpet B. Wood C. Tiles D. Concrete A: Q: What color is the chair in the corner of the room? A. Red B. Black .Green .Yellow A: Q: Is the person holding something in their hand? A: NO Q: Where is the large plant in the room positioned? A. Next to the window B. Near the door C. Beside the desk D. In the corner of the room A: 19 Table 25. Prompt used for Stage2 SynthMultiChoice Stage2-SynthMultiChoice ## Role Suppose you are an AI visual assistant, viewing detailed description of an image. ## Goal Design conversation where user asks questions about an image, and the AI visual assistant responds with high precision and specificity, based solely on what is clearly visible in the image. The focus is on high-quality matching where each answer must be directly tied to the observable content in the image. The questions should ensure that the answers are clear, precise, and grounded in visible evidence, and should avoid any speculative responses. ## Rule Key aspects to focus on: Object types: Identify objects and details based on their distinct visual attributes, ensuring the description matches what is observable in the image. Counting objects: Provide exact numbers of objects or entities visible, with no guesswork. Object actions: Only describe actions that can be clearly observed in the image, with no assumptions. Object locations: Describe precise spatial relationships between objects based on their positioning in the image. Relative positions between objects: Indicate how objects are arranged in relation to one another, using clear visual evidence from the image. Ensure that only questions with clear, confident answers are included: 1. The content referenced in the question is clearly visible in the image, and the answer can be confidently provided. 2. It is clear from the image that the content is not present. Whats more, each question must avoid ambiguity and the answers are firmly grounded in visual evidence. The conversation can include more complex questions, such as asking about the background knowledge of the objects in the image or discussing events happening in the image. However, make sure these questions are based on clear visual evidence from the image and not uncertain speculation. Ensure that the conversation is logically consistent, focuses on describing the visual information, and avoids unnecessary distractions. Keep the number of QA pairs to no more than 8 to maintain clarity. ## Example Output Format Q: How many people are visible in the image? A. One B. Two C. Three D. None A: Q: Is there door visible in the image? A: Yes Q: What is the color of the table in the image? A. Black B. Brown C. White D. Blue A: Q: What is the object on the desk? A. phone B. laptop C. coffee cup D. lamp A: Q: What is the object beside the couch? A. small table B. lamp C. plant D. book A: Q: Are there any curtains visible? A: No Table 26. Prompt used for SynthContrastShort"
        },
        {
            "title": "SynthContrastShort",
            "content": "## Role Suppose you are an AI visual assistant, viewing detailed description of an image. ## Goal Your primary task is to accurately construct the basic visual of the image in your mind. Based on this basic visual, you need to create two descriptions of images. Both descriptions should stem from the description provided but must differ distinctly in aspects such as color, quantity, type of objects, or their placement and so on. Then you need to create set of questions and answers that highlight the differences between the two images. ## Rule Instructions for Image Descriptions: Image1 Description: Provide detailed depiction of image1, focusing on visual elements such as the types of objects, their quantities, actions, locations, and their relative positions to one another. Image2 Description: Develop contrasting depiction of image2 by altering key visual elements. Changes can include similiar objects, different color scheme, different locations, varying quantities or types of objects, or new arrangement. Instructions for Questions and Answers: Create set of questions and answers that effectively highlight the differences between the two images. For clarity, in QA Section, refer to them as the left (Image1) and right (Image2) images. The questions should inquire about specific differences in visual aspects such as color, quantity, or arrangement of objects. Answer the questions using single word or phrase. ## Example Output Format Output Format: Image1: The image depicts [Detailed visual description of the image1], ensuring the description is no less than 120 words and no more than 180 words. Image2: The image describes [Detailed visual description of the image2], ensuring it clearly differs from Image 1, the description is no less than 120 words and no more than 180 words. QA Section Example: (You should focus more on the visual differences between the two images) Q: What objects are present in both the left and right images? A: Trees and house Q: What objects are in the left image but not in the right image? A: sedan car Q: What objects are in the right image but not in the left image? A: bicycle Q: How do the colors of the objects differ between the two images? A: Left image has green trees, right image has palm trees in blue Q: Does the left image have more trees than the right image? A: No Q: Is the quantity of objects different between the two images? A: Yes Q: Do both images feature pedestrians? A: Yes You can design other questions to explore additional aspects, such as the context of the objects, their arrangement, any textual elements, or the mood conveyed by the colors and lighting in each image. Additionally, you should control the QA pairs not exceeding 7 pairs to preserve visual clarity. Please present the descriptions and QAs directly, without any extraneous information. 21 Table 27. Prompt used for SynthContrastLong"
        },
        {
            "title": "SynthContrastLong",
            "content": "## Role Suppose you are an AI visual assistant, viewing detailed description of an image. ## Goal Your primary task is to accurately construct the basic visual of the image in your mind. Based on this basic visual, you need to create two descriptions of images. Both descriptions should stem from the description provided but must differ distinctly in aspects such as color, quantity, type of objects, or their placement and so on. Then you need to create set of questions and answers that highlight the differences between the two images. ## Rule Instructions for Image Descriptions: Image1 Description: Provide detailed depiction of image1, focusing on visual elements such as the types of objects, their quantities, actions, locations, and their relative positions to one another. Image2 Description: Develop contrasting depiction of image2 by altering key visual elements. Changes can include similiar objects, different color scheme, different locations, varying quantities or types of objects, or new arrangement. Instructions for Questions and Answers: Create set of questions and answers that effectively highlight the differences between the two images. For clarity, in QA Section, refer to them as the left (Image1) and right (Image2) images. The questions should inquire about specific differences in visual aspects such as color, quantity, or arrangement of objects. The answers should provide clear and concise explanations, directly referencing the visual disparities between the left and right images. ## Example Output Format Output Format: Image1: The image depicts [Detailed visual description of the image1], ensuring the description is no less than 120 words and no more than 180 words. Image2: The image describes [Detailed visual description of the image2], ensuring it clearly differs from Image 1, the description is no less than 120 words and no more than 180 words. QA Section Example: Q: What are the differences in the objects found in the left and right images? A: The left image contains [specific objects and features in Image1], while the right image includes [specific objects and features in Image2]. Q: What is the color difference between the objects in the left and right images? A: The left image features [description of colors in Image1], whereas the right image has [description of colors in Image2]. Q: How does the quantity of objects differ between the two images? A: In the left image, there are [number and types of objects in Image1], compared to [number and types of objects in Image2] in the right image. You can design other questions to explore additional aspects, such as the context of the objects, their arrangement, any textual elements, or the mood conveyed by the colors and lighting in each image. Additionally, you should control the QA pairs not exceeding 7 pairs to preserve visual clarity. Please present the descriptions and QAs directly, without any extraneous information. 22 Table 28. Prompt used for SynthTextQA-Step1 SynthTextQA-Step1 ## Role Suppose you are an AI visual assistant, viewing detailed description of an image. ## Goal Your primary task is to accurately construct the image in your mind. Based on your understanding and construction of the basic image, you need to further expand and enrich the content of the image. ## Rule You need to add textual content to enhance the expressiveness and the visual information of the image, ensuring that the text becomes the main subject of the image and is closely related to the theme and visual elements of the image. The textual information can include: Words and numbers on signs Billboards Posters Digital displays Graffiti Slogans Traffic directions Road signs Historical markers Famous quotes To maintain harmony: The image contains no more than three textual elements. The total word count shuold not exceed twelve words. After adding text, you need to provide new image description that includes the text you added. Ensure that the new description is logically consistent, overall harmonious, and focuses on describing the visual information in the image, minimizing the interference of irrelevant content. Your description must be thorough and specific enough to serve as visual guide for an image generation model, enabling it to accurately reproduce the imagined scene. ## Example Output Format Please ensure your response is no less than 110 words and no more than 150 words, and present the description content directly, without any extraneous information. Here are some possible formats: 1. pink heart button displays Love Yourself in cursive, bold font, against pastel-colored background. On nearby wall, motivational poster titled Self-Love Journey in modern, sans-serif font, features stylized illustration of person embracing their reflection. Below the poster, small sticker on laptop reads You Are Enough in playful, handwritten font, adding touch of whimsy to the scene. The surrounding environment is minimalistic, with few scattered flowers and faint, gradient glow, emphasizing the uplifting and empowering message of the image. 2. serene and vibrant painting titled Sweet Dreams dominates the wall, featuring an array of colorful flowers and puffy white clouds that evoke sense of peacefulness. The title Sweet Dreams is written in elegant, cursive script at the top of the painting, while small plaque at the bottom reads Artist: Lily Rose in simple, yet refined font. tiny inscription on the frame states Dream Big in delicate letters, further emphasizing the whimsical and soothing ambiance of the artwork. 3. beautiful, rustic wooden sign with the words Welcome to New Hope Church in elegant, golden letters hangs above the entrance of quaint, countryside church. On the church door, smaller sign reads Sunday Service 10am in simple, yet inviting font, providing essential information to visitors. In the foreground, stone path is lined with vibrant flowers and directional signpost that states Peace Garden with an arrow pointing towards serene garden area to the side, further emphasizing the churchs peaceful atmosphere and inviting all to explore its tranquil grounds. 23 Table 29. Prompt used for SynthTextQA-Step2 SynthTextQA-Step ## Role Suppose you are an AI visual assistant provided with description of an image containing textual elements. ## Goal Your task is to design conversation between yourself and person asking about these textual elements. ## Rule 1. Conversation Format: Present the conversation directly, separating questions and answers with and A. Limit the conversation to no more than 5 QA pairs to maintain clarity. 2. Tone and Style: Answer in the tone of visual AI assistant observing and describing the image. Ensure answers are based solely on the visible content of the image. 3. Content to Include: All textual elements mentioned in the image description must be covered in the conversation. 4. Question Design: Diversify the way questions are asked, especially when they are of the same type. Answer the question using single word or phrase. For questions focusing on specific texts, vary the format: Half of the time, answer only with the text. The other half, describe the text. Ensure you ask about each textual element in the description. Examples: Q: Whats written on the wooden sign? A: Welcome Home Q: Could you describe the text on the taxis side panel? A: NYC Taxi Q: What inscription is on the traditional Japanese lantern? A: Konnichiwa 5. Optional Questions: You may include additional questions exploring aspects like the location or font style of the text, but its not mandatory. You can also design other questions to explore additional aspects. But also answer the question using single word or phrase. 6. Avoiding Repetition: For similar questions, avoid repeating the same phrasing. Design questions that explore different aspects to keep the conversation engaging. 24 Figure 10. Illustration of our SynthConvShort Figure 11. Illustration of our SynthConvLong 25 Figure 12. Illustration of our SynthContrastShort Figure 13. Illustration of our SynthContrastLong 26 Figure 14. Illustration of our SynthMultiChoice Figure 15. Illustration of our SynthColor 27 Figure 16. Illustration of our SynthCount Figure 17. Illustration of our SynthScene 28 Figure 18. Illustration of our SynthSpatial Figure 19. Illustration of our SynthText"
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Peking University",
        "Shanghai AI Laboratory"
    ]
}