{
    "paper_title": "BrushEdit: All-In-One Image Inpainting and Editing",
    "authors": [
        "Yaowei Li",
        "Yuxuan Bian",
        "Xuan Ju",
        "Zhaoyang Zhang",
        "Ying Shan",
        "Yuexian Zou",
        "Qiang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Image editing has advanced significantly with the development of diffusion models using both inversion-based and instruction-based methods. However, current inversion-based approaches struggle with big modifications (e.g., adding or removing objects) due to the structured nature of inversion noise, which hinders substantial changes. Meanwhile, instruction-based methods often constrain users to black-box operations, limiting direct interaction for specifying editing regions and intensity. To address these limitations, we propose BrushEdit, a novel inpainting-based instruction-guided image editing paradigm, which leverages multimodal large language models (MLLMs) and image inpainting models to enable autonomous, user-friendly, and interactive free-form instruction editing. Specifically, we devise a system enabling free-form instruction editing by integrating MLLMs and a dual-branch image inpainting model in an agent-cooperative framework to perform editing category classification, main object identification, mask acquisition, and editing area inpainting. Extensive experiments show that our framework effectively combines MLLMs and inpainting models, achieving superior performance across seven metrics including mask region preservation and editing effect coherence."
        },
        {
            "title": "Start",
            "content": "BrushEdit: All-In-One Image Inpainting and Editing 1 Yaowei Li1 Junhao Zhuang4 Yuxuan Bian3 Ying Shan2 Xuan Ju3 Zhaoyang Zhang2 Yuexian Zou1 Qiang Xu3 1Peking University 2ARC Lab, Tencent PCG 3The Chinese University of Hong Kong 4Tsinghua University Equal Contribution Project Lead Project Page: https://liyaowei-stu.github.io/project/BrushEdit Corresponding Author 4 2 0 2 6 1 ] . [ 2 6 1 3 0 1 . 2 1 4 2 : r BrushEdit is cutting-edge interactive image editing framework that combines language models and inpainting techniques for seamless edits. Leveraging pre-trained multimodal language models and BrushNets dual-branch architecture, users can achieve diverse edits such as adding objects, removing elements, or making structural changes with free-form masks. AbstractImage editing has advanced significantly with the development of diffusion models using both inversion-based and instruction-based methods. However, current inversion-based approaches struggle with big modifications (e.g., adding or removing objects) due to the structured nature of inversion noise, which hinders substantial changes. Meanwhile, instruction-based methods often constrain users to black-box operations, limiting direct interaction for specifying editing regions and intensity. To address these limitations, we propose BrushEdit, novel inpainting-based instruction-guided image editing paradigm, which leverages multimodal large language models (MLLMs) and image inpainting models to enable autonomous, user-friendly, and interactive free-form instruction editing. Specifically, we devise system enabling free-form instruction editing by integrating MLLMs and dual-branch image inpainting model in an agentcooperative framework to perform editing category classification, main object identification, mask acquisition, and editing area inpainting. Extensive experiments show that our framework effectively combines MLLMs and inpainting models, achieving superior performance across seven metrics including mask region preservation and editing effect coherence. Index TermsImage Editing, Image Inpainting, Multimodal"
        },
        {
            "title": "Large Language Model",
            "content": "I. INTRODUCTION HE rapid advancement of diffusion models has significantly propelled text-guided image generation [1][4], delivering exceptional quality [5], diversity [6], and alignment with textual guidance [7]. However, in image editing taskswhere target image is generated based on source image and editing instructionssuch progress remains limited due to the difficulty of collecting large amounts of paired data. To perform image editing based on diffusion generation models, previous methods primarily focus on two strategies: (1) Inversion-based Editing: This approach leverages the structural information of noised latent derived from inversion to preserve content in non-edited regions, while manipulating in edited regions to achieve the desired modithe latent 2 Fig. 1. BrushEdit can achieve all-in-one inpainting for arbitrary mask shapes without requiring separate model training for each mask type. This flexibility in handling arbitrary shapes also enhances user-driven editing, as user-provided masks often combine segmentation-based structural details with random mask noise. By supporting arbitrary mask shapes, BrushEdit avoids the artifacts introduced by the random-mask version of BrushNet-Ran and the edge inconsistencies caused by the segmentation-mask version BrushNet-Segs strong reliance on boundary shapes. fications [8][11]. Although this method effectively maintains the overall image structure, it is often time-consuming due to multiple diffusion sampling processes. Additionally, the implicit inverse condition significantly limits editability, making large-scale edits (e.g., background replacement) and structural changes (e.g., adding or removing objects) challenging [12]. Furthermore, these methods typically require users to provide precise and high-quality source and target captions to leverage the conditional generation models priors for preserving backgrounds and altering foregrounds. However, in practical scenarios, users often prefer to achieve target area modifications with simple editing instructions. (2) Instruction-based Editing: This strategy involves collecting paired source image-instruction-target image data and finetuning diffusion models for editing tasks [13][16]. Due to the difficulty of obtaining manually edited paired data, training datasets are often generated using multimodal large language models (MLLMs) and inversion-based image editing methods (e.g., Prompt-to-Prompt [8] and Masactrl [9]). However, the low success rate and unstable quality of these training-free methods [11] result in noisy and unreliable datasets, leading to suboptimal performance of the trained models. Additionally, these methods often use black-box editing process, preventing users from interactively controlling and refining edits [17]. Given these limitations, we pose the question: Can we develop another editing paradigm that overcomes the challenges of inference efficiency, scalable data curation, editability, and controllability? The remarkable image-text understanding capabilities of Multimodal Large Language Models (MLLMs) [18][21], combined with the outstanding background preservation and text-aligned foreground generation abilities of Image Inpainting models [22], [23], inspires us to propose BrushEdit. BrushEdit is an agent-based, free-form, interactive framework for inpainting-driven image editing with instruction guidance that highlights the untapped potential in combining language understanding and image generation capabilities to enable free-form, high-quality interactive natural language-based instruction image editing. This framework requires users to input only natural language editing instructions and supports efficient, arbitrary-round interactive editing, allowing for adjustments in editing types and intensity. Our approach consists of four main steps: (i) Editing category classification: determine the type of editing required. (ii) Identification of the primary editing object: Identify the main object to be edited. (iii) Acquisition of the editing mask and target Caption: Generate the editing mask and corresponding target caption. (iv) Image inpainting: Perform the actual image editing. Steps (i) to (iii) utilize pre-trained MLLMs [20], [21] and detection models [24] to ascertain the editing type, target object, editing masks, and target caption. Step (iv) involves image editing using the dual-branch inpainting model BrushNet, as detailed in our previous conference paper. This model inpaints the target areas based on the target caption and editing masks, leveraging the generative potential and background preservation capabilities of inpainting models. This framework enables steps (i) to (iii) to extract and summarize instructional information via MLLMs, providing clear intermediate interactive guidance for subsequent diffusion models. Meanwhile, step (iv) maximizes the inpainting models ability to preserve the background and generate foreground content as instructed. Meanwhile, users can interactively modify intermediate control information (e.g., editing mask or the caption of the edited image) during steps (i) to (iv) and iteratively execute these steps as many times as needed until satisfactory editing result is achieved. The result is user-friendly, free-form, multi-turn interactive instruction editing system. Moreover, we found that BrushNets original strategy of training separately on segmentation-based masks and random masks greatly limits its practical applicability. This is because these masks differ significantly from user-drawn masks, resulting in suboptimal performance. User-drawn masks often resemble segmentation masks in terms of object edge shapes but also contain noise and irregularities similar to random masks. To overcome this limitation, we refined, merged, and expanded the original BrushData. This allowed us to train an all-in-one inpainting model capable of handling arbitrary mask shapes, thereby facilitating versatile image editing and inpainting, as illustrated in Fig. 1. We present comprehensive evaluation of BrushEditthrough both qualitative and quantitative analyses. We demonstrate that our system significantly enhances image editing quality and efficiency compared to existing methods. It excels particularly in aligning with edit instructions and maintaining background fidelity, thereby validating the effectiveness of our unified inpainting-driven, instruction-guided editing paradigm. In summary, we extend our conference version [22] by introducing several novel contributions: 1) We introduce BrushEdit, an advanced iteration of the previous BrushNet model. BrushEdit extends the capabilities of controllable image generation by pioneering an inpainting-based image editing approach. This unified model supports instruction-guided image editing and inpainting, offering user-friendly, free-form, multi-turn interactive editing experience. 2) By integrating with existing pre-trained multimodal large language models and vision understanding models, BrushEdit significantly improves language comprehension and controllable image generation without necessitating additional training process. 3) We expand BrushNet into versatile image inpainting framework that can accommodate arbitrary mask shapes. This eliminates the need for separate models for different types of mask configurations and enhances its adaptability to real-world user masks. II. RELATED WORK A. Image Editing Image editing involves modifying object shapes, colors, poses, materials, and adding or removing objects [34]. Recent advancements in diffusion models [1], [2] have notably improved visual generation tasks, outperforming GAN-based models [35][37] in image editing. To enable controlled and guided editing, various methods leverage modalities like text instructions [6], [13], [14], masks [15], [23], [38], layouts [8], [9], [39], segmentation maps [40], [41], and point-dragging interfaces [42], [43]. However, these methods often struggle with large structural edits due to noisy latent inversions overwhelming structural information or rely on scarce highquality source image-target image-editing instruction pairs. Additionally, they usually require users to operate in blackbox manner, demanding precise inputs like masks, text, or 3 TABLE COMPARISON OF BrushEdit WITH PREVIOUS IMAGE EDITING/INPAINTING METHODS. NOTE THAT WE ONLY LIST COMMONLY USED TEXT-GUIDED DIFFUSION METHODS IN THIS TABLE. Editing Model Plug-and-Play Flexible-Scale Multi-turn Interactive Instruction Editing Prompt2Prompt [8] MasaCtrl [9] MagicQuill [17] InstructPix2Pix [13] GenArtist [25] BrushEdit Inpainting Model Plug-and-Play Flexible-Scale Content-Aware Shape-Aware Blended Diffusion [26], [27] SmartBrush [28] SD Inpainting [5] PowerPaint [29] HD-Painter [30] ReplaceAnything [31] Imagen [32] ControlNet-Inpainting [33] BrushEdit layouts, limiting their usability for content creators. These challenges impede the development of free-form, interactive natural language editing system. Many Multi-modal Large Language Model (MLLM)-based methods leverage advanced vision and language understanding capabilities for image editing [15][17], [25], [44]. MGIE refines instruction-based editing by generating more detailed and expressive prompts. SmartEdit enhances the comprehension and reasoning of complex instructions. FlexEdit integrates MLLMs to process image content, masks, and textual inputs. GenArtist employs an MLLM agent to decompose complex tasks, guide tool selection, and systematically execute image editing, generation, and self-correction with iterative verification. However, these methods often involve costly MLLM fine-tuning, are limited to single-turn black-box editing, or face both challenges. The recent MagicQuill [17] enables fine-grained control over shape and color at the regional level using scribbles and colors, leveraging fine-tuned MLLM to infer editing options from user input. While it provides precise interactive control, it requires labor-intensive strokes to define regions training costs to fine-tune MLLMs. and incurs significant In contrast, our method relies solely on natural language instructions (e.g., remove the rose from the dogs mouth or convert the dumplings on the plate to sushi) and integrates MLLMs, detection models, and our dual-branch inpainting mode in training-free, agent-cooperative framework. And our framework also supports multi-round refinement, users can iteratively adjust the generated editing mask and target image caption to achieve multi-round interaction. As summarized in Tab. I, our BrushEdit overcomes the limitations of current editing methods through an instruction-based, multiturn interactive, and plug-and-play design, enabling flexible preservation of unmasked regions and establishing itself as versatile editing solution. B. Image Inpainting Image inpainting remains key challenge in computer vision, focusing on reconstructing masked regions with realistic and coherent content [45], [46]. Traditional methods [47], [48] and early Variational Auto-Encoder (VAE) [49], [50] or Generative Adversarial Network (GAN) [35][37] approaches 4 Fig. 2. Model overview. Our model outputs an inpainted image given the mask and masked image input. Firstly, we downsample the mask to accommodate the size of the latent, and input the masked image to the VAE encoder to align the distribution of latent space. Then, noisy latent, masked image latent, and downsampled mask are concatenated as the input of BrushEdit. The feature extracted from BrushEdit is added to pretrained UNet layer by layer after zero convolution block [33]. After denoising, the generated image and masked image are blended with blurred mask. often depend on hand-crafted features, results. leading to limited III. PRELIMINARIES AND MOTIVATION Recently, diffusion-based models [26][28], [51][53] have gained traction for their superior generation quality, precise control, and diverse outputs [1], [5], [54]. Early diffusion approaches for text-guided inpainting [26], [27], [51], [53], [55][57], such as Blended Latent Diffusion, modify denoising by sampling masked regions using pre-trained models while preserving unmasked areas from input images. Despite their popularity in tools like Diffusers [58], these methods perform well on simple tasks but falter with complex masks, content, or prompts, often yielding inconsistent outputs due to limited contextual understanding of mask boundaries and surrounding regions. To overcome these shortcomings, recent works [5], [28], [29], [31], [32], [59][61] have fine-tuned base models for enhanced contentand shape-awareness. For example, SmartBrush [28] integrates object-mask predictions for better sampling, while Stable Diffusion Inpainting [5] processes masks, masked images, and noisy latents through the UNet architecture for optimized inpainting. Moreover, HD-Painter [30] and PowerPaint [29] improve these models for higher quality and multi-task functionality. In this section, we will first introduce diffusion models in Sec. III-A. Then, Sec. III-B would review previous inpainting techniques based on sampling strategy modification and special training. Finally, the motivation is outlined in Section III-D. A. Diffusion Models Diffusion models include forward process that adds Gaussian noise ϵ to convert clean sample z0 to noise sample zT , and backward process that iteratively performs denoising from zT to z0, where ϵ (0, 1), and represents the total number of timesteps. The forward process can be formulated as follows: zt = αtz0 + 1 αtϵ (1) zt is the noised feature at step with [1, ], and α is hyper-parameter. In the backward process, given input noise zT sampled from random Gaussian distribution, learnable network ϵθ estimates noise at each step conditioned on C. After progressively refining iterations, z0 is derived as the output sample: However, many methods struggle to generalize inpainting capabilities to arbitrary pre-trained models. One prominent effort is fine-tuning ControlNet [33] on inpainting pairs, but its design remains limited in perceptual understanding, leading to suboptimal results. As summarized in Tab. I, our BrushEdit addressed these issues with content-aware, shapeaware, and plug-and-play design, allowing flexible preservation of unmasked regions. Building on this, BrushEdit unifies training across random and segmentation masks, enabling single model to handle arbitrary masks seamlessly, advancing its role as versatile inpainting solution. zt1 = αt1 αt zt + αt1 (cid:32)(cid:115) 1 αt1 1 (cid:114) 1 αt (cid:33) 1 ϵθ (zt, t, C) (2) The training of diffusion models revolves around optimizing the denoiser network ϵθ to conduct denoising with condition C, guided by the objective: min θ Ez0,ϵN (0,I),tU (1,T ) ϵ ϵθ (zt, t, C) (3) B. Image Inpainting Models Previous image inpainting approaches can be broadly categorized into Sampling Strategy Modification and Dedicated Inpainting Models. Sampling Strategy Modification. These methods perform inpainting by iteratively blending masked images with generated content. representative example is Blended Latent Diffusion (BLD) [27], inpainting technique in the default popular diffusion-based libraries (e.g., Diffusers [58]). Given binary mask and masked image xmasked , BLD extracts the latent representation zmasked using VAE. The mask is resized to mresized to match the latent dimensions. During inpainting, Gaussian noise is added to zmasked over steps, 0 producing zmasked , where [1, ]. The denoising starts from zmasked , with each sampling step (eq. 2) followed by: 0 0 zt1 zt1 (cid:0)1 mresized(cid:1) + zmasked t1 mresized (4) Despite its simplicity, Sampling Strategy Modification often struggles to preserve unmasked regions and align generated content. These shortcomings stem from: (1) inaccuracies introduced by resizing the mask, which hinder proper blending of noisy latents, and (2) the diffusion models limited contextual understanding of mask boundaries and unmasked regions. Dedicated Inpainting Models. To enhance performance, these methods fine-tune base models by adding the mask and masked image as additional UNet input channels, creating architectures specialized for inpainting. While they surpass BLD in generation quality, they face several challenges: (1) They merge noisy latents, masked image latents, and masks at the UNets initial convolution layer, where text embeddings globally affect all features, making it difficult for deeper layers to focus on masked image details. (2) Simultaneously handling conditional inputs and generation tasks increases the UNets computational load. (3) Extensive fine-tuning is required for different diffusion backbones, leading to high computational costs and limited adaptability to custom diffusion models. C. Image Editing Models Recent image editing methods can fall into two types: a) Inversion Methods: These approaches [8][10], [26], [62], [63] achieve editing by manipulating the latents obthey generate edit-friendly tained through inversion. First, noisy latents using various inversion techniques, followed by three paradigms for preserving background regions while modifying target areas: (1) Attention Integration: They [8], [9], [42], [64][67] fuse attention maps linking text and image between the source and editing diffusion branches. (2) Target Embedding: They [62], [63], [68][74] manage to embed the editing information from the target branch and integrate it into the source diffusion branch. (3) Latent Integration: These methods [10], [26], [27], [42], [75][77] try to directly inject editing instructions via noisy latent features from the target diffusion branch into the source diffusion branch. Although these methods are computationally efficient and achieve competitive zero-shot or few-shot performance, they are often limited in the diversity of supported edits (e.g., typically restricted to 5 object interaction or attribute modification) due to simplistic generation controls. Additionally, the structural prominence in inversion latents often leads to failures when handling significant structural changes, such as object addition/removal or background replacement. b) End-to-end Methods: These methods [13], [78][80] train end-to-end diffusion models for image editing, leveraging various ground-truth or pseudo-paired editing datasets. They support broader range of edits and avoid the significant speed drawbacks of inversion methods, completing edits in single forward pass. However, their performance is often constrained by the limited availability of ground-truth editing pairs, necessitating pseudo-pair generation via inversion methods, which hinders their upper-bound performance. Furthermore, these end-to-end models lack support for interactive, multi-round editing, preventing content creators from iterative refining or enhancing edits, thus reducing their practicality. D. Motivation Based on the analysis in Section III-B, more effective inpainting architecture could incorporate an additional branch dedicated to processing masked images, enabling the backbone to recognize mask boundaries and the corresponding background without requiring modifications or retraining. Similarly, as discussed in Section III-C, there is an urgent need for free-form, interactive natural language instruction editing model. Leveraging the exceptional multimodal understanding of MLLMs, such model can efficiently identify the editing type, target objects, and regions to edit, as well as generate annotations for the desired output. With the support of image inpainting models, precise edits within the target masked regions can then be achieved. Moreover, this process can be iteratively refined, allowing users to create transparently and iteratively. IV. METHOD An overview of BrushEdit is shown in Fig. 2. Our framework integrates MLLMs with dual-branch image inpainting model via agent collaboration, enabling free-form, multiturn interactive instruction editing. Specifically, pre-trained MLLM, acting as the Editing Instructor, interprets user instructions to identify editing types, locate target objects, retrieve detection results for the editing region, and generate textual descriptions of the edited image. Guided by this information, the inpainting model, serving as the Editing Conductor, fills the masked region based on the target text caption. This iterative process allows users to modify or refine intermediate control inputs at any stage, supporting flexible and interactive instruction-based editing. A. Editing Instructor In BrushEdit, we use an MLLM as an editing instructor to interpret users free-form editing instructions, categorize them into predefined types (addition, removal, local edit, background edit), identify target objects, and utilize pre-trained detection model to find the relevant editing mask. Finally, the edited image caption is generated. In the next stage, this information is packaged and sent to the editing system to complete the task using an image inpainting approach. The formal process is as follows: Given the editing instruction TIns and source image Isrc, we first use pretrained MLLM ϕM LLM to identify the users editing type and the corresponding target object The MLLM then calls pre-trained detection model ϕD to search for the target object mask Md based on O. After obtaining the mask, the MLLM combines K, O, and Isrc to generate the final edited image caption. The source image Isrc, target mask Md, and the caption are then passed to the next stage, the Editing Conductor, for image-inpainting-based editing. B. Editing Conductor Our Editing Conductor, built on our previous BrushNet, employs mixed fine-tuning strategy using both random and segmentation masks. This approach enables the inpainting model to handle diverse mask-based inpainting tasks without being restricted by mask types, achieving comparable or superior performance. Specifically, we inject masked image features into pre-trained diffusion network (e.g., Stable Diffusion 1.5) through an additional control branch. These features include the noisy latent for enhancing semantic coherence by providing information on the current generation process, the masked image latent extracted via VAE to guide semantic consistency between the prompt foreground and the ground truth background, and the mask downsampled via cubic interpolation to explicitly indicate the position and boundaries of the foreground filling region. To retain masked image features, BrushEdit employs duplicate of the pre-trained diffusion model with all attention layers removed. The pre-trained convolutional weights serve as robust prior for extracting masked image features, while excluding cross-attention layers ensures the branch focuses solely on pure background information. BrushEdit features are integrated into the frozen diffusion model layer-by-layer, enabling hierarchical, dense per-pixel control. Following ControlNet [33], zero convolution layers are used to link the frozen model with the trainable BrushEdit, mitigating noise during early training stages. The feature insertion operation is defined in Eq. 5: θ (cid:0)(cid:2)zt, zmasked ϵθ (zt, t, C)i = ϵθ (zt, t, C)i + (cid:0)ϵBrushN et , mresized(cid:3) , t(cid:1) (5) , where ϵθ (zt, t, C)i represents the feature of the i-th layer in the network ϵθ, where [1, n], and denotes the total number of layers. The same notation is applied to ϵBrushN et . The network ϵBrushN et processes the concatenated noisy latent zt, masked image latent zmasked , and downsampled mask 0 mresized, where concatenation is represented by []. refers to the zero convolution operation, and is the preservation scale that adjusts the influence of BrushEdit on the pretrained diffusion model. θ θ Previous studies have highlighted that downsampling during latent blending can introduce inaccuracies, and the VAE 6 encoding-decoding process has inherent limitations that impair full image reconstruction. To ensure consistent reconstruction of unmasked regions, prior methods have explored various strategies. Some approaches [29], [31] rely on copy-and-paste techniques to directly transfer unmasked regions, but these often result in outputs lacking semantic coherence. Latent blending methods inspired by BLD [5], [27] also struggle to retain desired information in unmasked areas effectively. In this work, we propose simple pixel-space approach that applies mask blurring before copy-and-paste using the blurred mask. Although this may slightly affect accuracy near the mask boundary, the error is nearly imperceptible and significantly improves boundary coherence. The architecture of BrushEdit is inherently designed for seamless plug-and-play integration with various pretrained diffusion models, enabling flexible preservation control. Specifically, the flexible capabilities of BrushEdit include: (1) Plugand-Play Integration: As BrushEdit does not modify the pretrained diffusion models weights, it can be effortlessly integrated with any community fine-tuned models, facilitating easy adoption and experimentation. (2) Preservation Scale Adjustment: The preservation scale of the unmasked region can be controlled by incorporating BrushEdit features into the frozen diffusion model with weight w, which adjusts the influence of BrushEdit on the level of preservation. (3) Blurring and Blending Customization: The preservation scale can be further refined by adjusting the blurring scale and applying blending operations as needed. These features provide finegrained and flexible control over the editing process. V. EXPERIMENTS A. Evaluation Benchmark and Metrics a) Benchmark: To comprehensively evaluate the performance of BrushEdit, we conducted experiments on both image editing and image inpainting benchmarks: Image Editing. We used PIE-Bench [11] (Prompt-based Image Editing Benchmark) to evaluate BrushEditand all baselines on image editing tasks. PIE-Bench consists of 700 images spanning 10 editing types, evenly distributed between natural and artificial scenes (e.g., paintings) across four categories: animal, human, indoor, and outdoor. Each image includes five annotations: source image prompt, target image prompt, editing instruction, main editing body, and editing mask. Image Inpainting. Extending our prior conference work, we replaced traditional benchmarks [81][86] with BrushBenchfor segmentation-based masks and EditBench for random brush masks. These benchmarks span real and generated images across human bodies, animals, and indoor and outdoor scenes. EditBench includes 240 images with an equal mix of natural and generated content, each annotated with mask and caption. BrushBench, shown in Fig. 3, contains 600 images with human-annotated masks and captions, evenly distributed across natural and artificial scenes (e.g., paintings) and covering various categories such as humans, animals, and indoor/outdoor environments. (cid:1) 7 Fig. 3. Benchmark overview. and II separately show natural and artificial images, masks, and caption of BrushBench. (a) to (d) show images of humans, animals, indoor scenarios, and outdoor scenarios. Each group of images shows the original image, inside-inpainting mask, and outside-inpainting mask, with an image caption on the top. III show image, mask, and caption from EditBench [32], with (e) for generated images and (f) for natural images. The images are randomly selected from both benchmarks. We refined the task into two scenarios for segmentation-based mask inpainting: inside-inpainting and outside-inpainting, enabling detailed performance evaluation across distinct image regions. Notably, BrushEdit surpasses BrushNet by leveraging unified high-quality inpainting masked images for training, enabling it to handle all mask types. This establishes BrushEditas unified model capable of performing all inpainting and editing benchmark tasks, whereas BrushNet required separate fine-tuning for each mask type. b) Dataset: Building upon the BrushData proposed in our previous conference version, we integrate two subsets of segmentation masks and random masks, and further extend the data from the Laion-Aesthetic [87] dataset, resulting in BrushData-v2. key difference is that we select images with clean backgrounds and pair them randomly with either segmentation or random masks, effectively creating pairs that simulate deletion-based editing, significantly enhancing our frameworks removal capability in image editing. The data expansion process is as follows: We use Grounded-SAM [88] to annotate open-world masks, then filter them based on confidence scores to retain only those with higher confidence. We also consider mask size and continuity during the filtering. focusing on unedited/uninpainted region preservation and edited/inpainted region text alignment. Additionally, we conducted extensive user studies to validate the superior performance of BrushEdit in edit instruction alignment and background fidelity. c) Metrics: We evaluate five metrics, Background Fidelity. We adopt standard metrics, including Peak Signal-to-Noise Ratio (PSNR) [89], Learned Perceptual Image Patch Similarity (LPIPS) [90], Mean Squared Error (MSE) [91], and Structural Similarity Index Measure (SSIM) [92], to evaluate the consistency between the unmasked regions of the generated and original images. Text Alignment. We use CLIP Similarity (CLIP Sim) [93] to assess text-image consistency by projecting both into the shared embedding space of the CLIP model [94] and measuring the similarity of their representations. B. Implementation Details We evaluate various inpainting methods under consistent i.e., using NVIDIA Tesla setting unless stated otherwise, V100 GPUs and their open-source code with Stable Diffusion v1.5 as the base model, 50 steps, and guidance scale of 7.5. Each method utilizes its recommended hyper-parameters across all images to ensure fairness. BrushEdit and all ablation models are trained for 430k steps on 8 NVIDIA Tesla V100 GPUs, requiring approximately 3 days. Notably, for all image editing (PnPBench) and image inpainting (BrushBench and EditBench) tasks, BrushEditachieves unified image editing and inpainting using single model trained on BrushData-v2. In contrast, our previous BrushNet required separate training and testing for different mask types. Additional details are available in the provided code. C. Quantitative Comparison (Image Editing) Tab. II and Tab. III compare the quantitative image editing performance on PnPBench [11]. We evaluate the editing results of previous inversion-based methods, including four inversion techniquesDDIM Inversion [2], Null-Text Inversion [95], Negative-Prompt Inversion [96], and StyleDiffusion [97]as Fig. 4. Comparison of previous editing methods and BrushEdit on natural and synthetic images, covering image editing operations such as removing objects (I), adding objects (II), modifying attributes (III), and swapping objects (IV). well as four editing methods: Prompt-to-Prompt [8], MasaCtrl [9], pix2pix-zero [65], and Plug-and-Play [66]. The results in Tab. II confirm the superiority of BrushEdit in preserving unedited regions and ensuring accurate text alignment in edited areas. While inversion-based methods, such as DDIM Inversion (DDIM) [2] and PnP Inversion (PnP) [11], can achieve high-quality background preservation, they are inherently limited by reconstruction errors that affect background retention. In contrast, BrushEdit separately models unedited background information through dedicated branch, while the main network generates the edited region based on the text prompt. Combined with predefined user masks and blending operations, it ensures near-lossless background preservation and semantically coherent edits. More importantly, our method preserves high-fidelity background information without being affected by the irretrievable structural noise from inversion-based methods. It allows operations, such as adding or removing objects, that are typically impossible with inversion-based editing. Furthermore, since no inversion is required, BrushEdit only needs single forward pass to perform the editing operation. As shown in Tab. III , the editing time of BrushEdit is significantly short, greatly improving the efficiency of image editing. D. Qualitative Comparison (Image Editing) The qualitative comparison with previous image editing methods is shown in Fig. 4. We present results on both artificial and natural images across various editing tasks, including deleting objects (I), adding objects (II), modifying objects (III), and swapping objects (IV). BrushEdit consistently achieves superior coherence between the edited and unedited regions, excelling in adherence to editing instructions, smoothness at the editing mask boundaries, and overall content consistency. Notably, Fig. 4 and II involve tasks such as deleting flower or laptop, and adding collar or earring. TABLE II COMPARISON OF BrushEdit WITH VARIOUS EDITING METHODS IN PNPBENCH. FOR EDITING METHODS PROMPT-TO-PROMPT (P2P) [8], MASACTRL [9], PIX2PIX-ZERO (P2P-ZERO) [9], AND PLUG-AND-PLAY (PNP) [66], WE EVALUATE TWO INVERSION TECHNIQUES, DDIM INVERSION (DDIM) [2] AND PNP INVERSION (PNP) [11], TO ESTABLISH STRONGER BASELINES. RED STANDS FOR THE BEST RESULT, BLUE STANDS FOR THE SECOND BEST RESULT. Inverse Editing PSNR LPIPS103 MSE104 SSIM102 CLIP Similariy 71.14 84.76 79.67 81.33 74.67 77.05 79.05 79.68 97.08 22.44 22.10 21.16 21.35 20.54 21.05 22.55 22.62 22.44 208.80 54.55 106.62 87.94 172.22 138.98 113.46 106.06 17.22 DDIM P2P 17.87 PnP P2P 27.22 DDIM MasaCtrl 22.17 PnP MasaCtrl 22.64 DDIM P2P-Zero 20.44 PnP P2P-Zero 21.53 DDIM PnP 22.28 PnP PnP 22.46 32. 219.88 32.86 86.97 81.09 144.12 127.32 83.64 80.45 8.43 TABLE III COMPARISON OF INFERENCE TIME BETWEEN OUR INPAINTING-BASED BrushEdit AND OTHER INVERSION-BASED METHODS, INCLUDING NEGATIVE-PROMPT INVERSION (NP), EDIT FRIENDLY INVERSION (EF), AIDI [98], EDICT, NULL-TEXT INVERSION (NT), AND STYLE DIFFUSION ADDED WITH PROMPT-TO-PROMPT. BrushEdit ACHIEVES BETTER EDITING RESULTS WITH FAR LESS INFERENCE TIME THAN ALL INVERSION-BASED METHODS. BrushEdit Methods Inference Time (s) BrushEdit NP EF AIDI EDICT NT Style Diffusion 3.57 18.22 19.10 35.41 35.48 148. 382.98 While previous methods failed to deliver satisfactory results due to persistent structural artifacts caused by inversion noise, BrushEdit successfully performs the intended operations and produces seamless edits that blend harmoniously with the background, owing to its dual-branch decoupled inpaintingbased editing paradigm. E. Quantitative Comparison (Image Inpainting) Tab. IV and Tab. present the quantitative comparison on BrushBench and EditBench [32]. We evaluate the inpainting results of the sampling strategy modification method Blended 9 TABLE IV QUANTITATIVE COMPARISONS BETWEEN BrushEdit AND OTHER DIFFUSION-BASED INPAINTING MODELS IN BrushBench: BLENDED LATENT DIFFUSION (BLD) [27], STABLE DIFFUSION INPAINTING (SDI) [5], HD-PAINTER (HDP) [30], POWERPAINT (PP) [29], CONTROLNET-INPAINTING (CNI) [33], AND OUR PREVIOUS SEGMENTATION-BASED BRUSHNET-SEG [22]. THE TABLE SHOWS METRICS ON BACKGROUND FIDELITY AND TEXT ALIGNMENT (TEXT ALIGN) FOR BOTH INSIDEAND OUTSIDE-INPAINTING. ALL MODELS USE STABLE DIFFUSION V1.5 AS THE BASE MODEL. RED INDICATES THE BEST RESULT, WHILE BLUE INDICATES THE SECOND-BEST RESULT. Inside-inpainting Outside-inpainting Metrics Models Masked Background Fidelity Text Align PSNR MSE103 LPIPS103 SSIM103 CLIP Sim Metrics Models Masked Background Fidelity Text Align PSNR MSE103 LPIPS103 SSIM103 CLIP Sim BLD (1) SDI (2) HDP (3) PP (4) CNI (5) CNI* (5) BrushNet-Seg* BrushEdit* 21.33 21.52 22.61 21.43 12.39 22.73 31.94 31.98 9.76 13.87 9.95 32.73 78.78 24.58 0.80 0.79 * with blending operation 49.26 48.39 43.50 48.43 243.62 43.49 18.67 18.92 74.58 89.07 89.03 86.39 65.25 91.53 96.55 96. 26.15 26.17 26.37 26.48 26.47 26.22 26.39 26.24 BLD (1) SDI (2) HDP (3) PP (4) CNI (5) CNI* (5) BrushNet-Seg* BrushEdit* 15.85 18.04 18.03 18.04 11.91 17.50 27.82 27.65 35.86 19.87 22.99 31.78 83.03 37.72 2.25 2.30 21.40 15.13 15.22 15.13 58.16 19.95 4.63 4.90 77.40 91.42 90.48 90.11 66.80 94.87 98.95 98. 26.73 27.21 26.96 26.72 27.29 26.92 27.22 27.29 Latent Diffusion [27], dedicated inpainting models Stable Diffusion Inpainting [5], HD-Painter [30], PowerPaint [29], the plug-and-play method ControlNet [33] trained on inpainting data, and our previous BrushNet1. Results confirm BrushEdits superiority in preserving uninpainted regions and ensuring accurate text alignment in inpainted areas. Blended Latent Diffusion [27] performs the worst, primarily due to incoherent transitions between masked and unmasked regions, stemming from its disregard for mask boundaries and blending-induced latent space losses. HDPainter [30] and PowerPaint [29], both based on Stable Diffusion Inpainting [5], achieve similar results to their base model for inside-inpainting tasks. However, their performance deteriorates sharply in outside-inpainting, as they are designed exclusively for inside-inpainting. ControlNet [33], explicitly trained for inpainting, shares the most comparable experimental setup with ours. Nonetheless, its design mismatch with the inpainting task hampers its ability to maintain masked region fidelity and text alignment, requiring integration with Blended Latent Diffusion [27] for reasonable results. Even with this combination, it falls short of specialized inpainting models and BrushEdit. The performance on EditBench aligns closely with that on BrushBench, both demonstrating BrushEdits superior results. This suggests that our method performs consistently well across various inpainting tasks, including segmentation, random, inside, and outside inpainting masks. noting to BrushNet, BrushEdit now surpasses BrushNet in both segmentationmask-based and random-mask-based benchmarks with single model, achieving more general and robust all-inone inpainting. This improvement is largely attributed to our unified mask types and the richer data distribution in BrushData-v2. is worth compared that, It F. Qualitative Comparison (Image Inpainting) The qualitative comparison with previous image inpainting methods is shown in Fig. 5. We evaluate results on both artificial and natural images across diverse inpainting tasks, 1BrushNet fine-tunes separate models for different mask types, while BrushEdit uses unified model and achieves state-of-the-art performance on both segmentation-based BrushBench and random-mask-based EditBench. TABLE QUANTITATIVE COMPARISONS AMONG BrushEdit AND OTHER DIFFUSION-BASED INPAINTING MODELS, RANDOM-MASK-BASED BRUSHNET-RAN IN EDITBENCH. DETAILED EXPLANATION OF COMPARED METHODS AND METRICS CAN BE FOUND IN THE CAPTION OF TAB. IV. RED STANDS FOR THE BEST RESULT, BLUE STANDS FOR THE SECOND BEST RESULT. Metrics Models Masked Background Fidelity Text Align PSNR MSE103 LPIPS103 SSIM103 CLIP Sim BLD [27] SDI [5] HDP [30] PP [29] CNI [33] CNI* [33] BrushNet-Ran* BrushEdit* 20.89 23.25 23.07 23.34 12.71 22.61 33.66 32.97 10.93 6.94 6.70 20.12 69.42 35.93 0.63 0.70 * with blending operation 31.90 24.30 24.32 24.12 159.71 26.14 10.12 7.24 85.09 90.13 92.56 91.49 79.16 94.05 98.13 98.60 28.62 28.00 28.34 27.80 28.16 27.74 28.87 29. including random mask inpainting and segmentation mask inpainting. BrushEdit consistently achieves superior coherence between the generated and unmasked regions in terms of both content and color (I, II). Notably, in Fig. 5 II (left), the task involves generating both cat and goldfish. While all prior methods fail to recognize the existing goldfish in the masked image and instead generate an additional fish, BrushEdit accurately integrates background context, enabled by its dualbranch decoupling design. Furthermore, BrushEdit outperforms our previous BrushNet in overall inpainting performance without fine-tuning for specific mask types, achieving comparable or even better results on both random and segmentationbased masks. G. Flexible Control Ability Fig. 6 and Fig. 7 demonstrate the flexible control offered by BrushEdit in two key areas: base diffusion model selection and scale adjustment. This flexibility extends beyond inpainting to image editing, as it is achieved by altering the backbone networks generative prior and branch information injection strength. In Fig. 6, we show how BrushEdit can be combined with various community-finetuned diffusion models, enabling users to choose the model that best aligns with their specific editing or inpainting needs. This greatly enhances the prac10 Fig. 5. Performance comparisons of BrushEdit and previous image inpainting methods across various inpainting tasks: (I) Random Mask Inpainting (II) Segmentation Mask Inpainting. Each group of results contains 7 inpainting methods: (b) Blended Latent Diffusion (BLD) [27], (c) Stable Diffusion Inpainting (SDI) [5], (d) HD-Painter (HDP) [30], (e) PowerPaint (PP) [29], (f) ControlNet-Inpainting (CNI) [33], (g) Our Previous BrushNet and (h) Ours. Fig. 6. Integrating BrushEdit to community fine-tuned diffusion models. We use five popular community diffusion models fine-tuned from stable diffusion v1.5: DreamShaper (DS) [99], epiCRealism (ER) [100], Henmix Real (HR) [101], MeinaMix (MM) [102], and Realistic Vision (RV) [103]. MM is specifically designed for anime images. tical value of BrushEdit. Fig. 7 illustrates the control over BrushEdits scale parameter, which allows users to adjust the extent of unmasked region protection during editing or inpainting, offering fine-grained control for precise and customizable results. H. Ablation Study We conducted ablation studies to examine the impact of different model designs on image inpainting tasks. Since BrushEdit is based on an image inpainting model, the editing task is achieved through inference-only by chaining MLLMs, BrushEdit, and an image detection model as agents. The inpainting capability directly reflects our models training outcome. Tab. VI compares the dual-branch and single-branch designs, while Tab. VII highlights the ablation study on the additional branch architecture. The ablation studies, performed on BrushBench, average the performance for both inside-inpainting and outside-inpainting. TABLE VI ABLATION ON DUAL-BRANCH DESIGN. STABLE DIFFUSION INPAINTING (SDI) USE SINGLE-BRANCH DESIGN, WHERE THE ENTIRE UNET IS FINE-TUNED. WE CONDUCTED AN ABLATION ANALYSIS BY TRAINING DUAL-BRANCH MODEL WITH TWO VARIATIONS: ONE WITH THE BASE UNET FINE-TUNED, AND ANOTHER WITH THE BASE UNET FORZENED. RESULTS DEMONSTRATE THE SUPERIOR PERFORMANCE ACHIEVED BY ADOPTING THE DUAL-BRANCH DESIGN. RED IS THE BEST RESULT. Metrics Image Quality Masked Region Preservation Text Align Model IR10 HPS102 AS PSNR MSE102 LPIPS103 CLIP Sim SDI w/o fine-tune w/ fine-tune 11.00 11.59 11.63 27.53 27.71 27.73 6.53 6.59 6.60 19.78 19.86 20.13 16.87 16.09 15.84 31.76 31.68 31. 26.69 26.91 26.93 The results in Tab. VI show that the dual-branch design significantly outperforms the single-branch design. Moreover, finetuning the base diffusion model in the dual-branch setup yields superior results compared to freezing it. However, fine-tuning may limit flexibility and control over the model. Considering the trade-off between performance and flexibility, we chose to 11 Fig. 7. Flexible control scale of BrushEdit. (a) shows the given masked image, (b)-(h) show adding control scale from 1.0 to 0.2. Results show gradually diminishing controllable ability from precise to rough control. adopt the frozen dual-branch design for our model. Tab. VII explains the reasoning behind key design choices: (1) using VAE encoder instead of randomly initialized convolution layers for processing masked images, (2) incorporating the full UNet feature layer-by-layer into the pre-trained UNet, and (3) removing text cross-attention in BrushEditto prevent masked image features from being influenced by text. TABLE VII ABLATION ON MODEL ARCHITECTURE. WE ABLATE ON THE FOLLOWING COMPONENTS: THE IMAGE ENCODER (ENC), SELECTED FROM RANDOM INITIALIZED CONVOLUTION (CONV) AND VAE; THE INCLUSION OF MASK IN INPUT (MASK), CHOSEN FROM ADDING (W/) AND NOT ADDING (W/O); THE PRESENCE OF CROSS-ATTENTION LAYERS (ATTN), CHOSEN FROM ADDING (W/) AND NOT ADDING (W/O); THE TYPE OF UNET FEATURE ADDITION (UNET), SELECTED FROM ADDING THE FULL UNET FEATURE (FULL), ADDING HALF OF THE UNET FEATURE (HALF), AND ADDING THE FEATURE LIKE CONTROLNET (CN); AND FINALLY, THE BLENDING OPERATION (BLEND), CHOSEN FROM NOT ADDING (W/O), DIRECT PASTING (PASTE), AND BLURRED BLENDING (BLUR). RED IS THE BEST RESULT. Metrics Image Quality Masked Region Preservation Text Align Enc Mask Attn UNet Blend IR10 HPS102 AS PSNR MSE102 LPIPS103 CLIP Sim w/o full Conv w/ w/o VAE w/o w/o w/o full w/o full w/ VAE w/ w/ CN w/o Conv w/ VAE w/ w/ CN w/o VAE w/ w/o CN w/o half w/o VAE w/ w/o w/o full VAE w/ w/o paste full VAE w/ w/o 11.05 11.55 11.25 9.58 10.53 11.42 11.47 11.59 11.72 26.23 27.70 27.62 26.85 27.42 27.69 27.70 27.71 27.93 6.55 14.89 6.57 17.96 6.56 18.69 6.47 12.15 6.59 18.28 6.58 18.49 6.57 19.01 6.59 19.86 6.58 - VAE w/ w/o full blur 11.76 27.94 6.58 29.88 37.23 26.38 19.44 80.91 24.36 24.09 23.77 16.09 - 1. 64.54 49.33 34.28 150.89 41.63 36.33 33.57 31.68 - 11.65 26.76 26.87 26.63 26.88 26.89 26.86 26.87 26.91 26.80 26.81 VI. DISCUSSION"
        },
        {
            "title": "This",
            "content": "paper introduces a) Conclusion.: novel Inpainting-based Instruction-guided Image Editing paradigm (IIIE), which combines (LLMs) and plug-and-play, all-in-one image inpainting models to enable autonomous, user-friendly, and interactive free-form results instruction editing. Quantitative large language models and qualitative on PnPBench, our proposed benchmark, BrushBench, and EditBench demonstrate the superior performance of BrushEdit in terms of masked background preservation and image-text alignment in image editing and inpainting tasks. and b) Limitations Future Work.: However, BrushEdit has some limitations: (1) The quality and content generated by our model heavily depend on the selected base model. (2) Even with BrushEdit, poor generation results still occur when the mask has an irregular shape or when the provided text does not align well with the masked image. In future work, we aim to address these challenges. c) Negative Social Impact.: Image inpainting models offer exciting opportunities for content creation but also present potential risks to individuals and society. Their reliance on internet-collected training data may amplify social biases, and there is specific risk of generating misleading content by manipulating human images with offensive elements. To mitigate these concerns, responsible use and the establishment of ethical guidelines are essential, which will also be focus in our future model releases."
        },
        {
            "title": "REFERENCES",
            "content": "[1] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, Advances in Neural Information Processing Systems (NIPS), vol. 33, pp. 68406851, 2020. [2] J. Song, C. Meng, and S. Ermon, Denoising diffusion implicit models, arXiv preprint arXiv:2010.02502, 2020. [3] X. Ju, A. Zeng, C. Zhao, J. Wang, L. Zhang, and Q. Xu, Humansd: native skeleton-guided diffusion model for human image generation, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 15 98815 998. [4] X. Liu, J. Ren, A. Siarohin, I. Skorokhodov, Y. Li, D. Lin, X. Liu, Z. Liu, and S. Tulyakov, Hyperhuman: Hyper-realistic human generation with latent structural diffusion, arXiv preprint arXiv:2310.08579, 2023. [5] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, High-resolution image synthesis with latent diffusion models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022, pp. 10 68410 695. [6] X. Dai, J. Hou, C.-Y. Ma, S. Tsai, J. Wang, R. Wang, P. Zhang, S. Vandenhende, X. Wang, A. Dubey et al., Emu: Enhancing image generation models using photogenic needles in haystack, arXiv preprint arXiv:2309.15807, 2023. [7] Y. Li, X. Liu, A. Kag, J. Hu, Y. Idelbayev, D. Sagar, Y. Wang, S. Tulyakov, and J. Ren, Textcraftor: Your text encoder can be image quality controller, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 79857995. [8] A. Hertz, R. Mokady, J. Tenenbaum, K. Aberman, Y. Pritch, and D. Cohen-or, Prompt-to-prompt image editing with cross-attention control, in International Conference on Learning Representations (ICLR), 2023. [9] M. Cao, X. Wang, Z. Qi, Y. Shan, X. Qie, and Y. Zheng, Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing, arXiv preprint arXiv:2304.08465, 2023. [10] C. Meng, Y. He, Y. Song, J. Song, J. Wu, J.-Y. Zhu, and S. Ermon, SDEdit: Guided image synthesis and editing with stochastic differential equations, in International Conference on Learning Representations (ICLR), 2022. [11] X. Ju, A. Zeng, Y. Bian, S. Liu, and Q. Xu, Pnp inversion: Boosting diffusion-based editing with 3 lines of code, International Conference on Learning Representations (ICLR), 2024. [12] S. Xu, Y. Huang, J. Pan, Z. Ma, and J. Chai, Inversion-free image editing with natural language, arXiv preprint arXiv:2312.04965, 2023. [13] T. Brooks, A. Holynski, and A. A. Efros, Instructpix2pix: Learning to follow image editing instructions, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 18 39218 402. [14] Z. Geng, B. Yang, T. Hang, C. Li, S. Gu, T. Zhang, J. Bao, Z. Zhang, H. Li, H. Hu et al., Instructdiffusion: generalist modeling interface for vision tasks, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 12 70912 720. [15] Y. Huang, L. Xie, X. Wang, Z. Yuan, X. Cun, Y. Ge, J. Zhou, C. Dong, R. Huang, R. Zhang et al., Smartedit: Exploring complex instructionbased image editing with multimodal large language models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 83628371. [16] T.-J. Fu, W. Hu, X. Du, W. Y. Wang, Y. Yang, and Z. Gan, Guiding instruction-based image editing via multimodal large language models, arXiv preprint arXiv:2309.17102, 2023. [17] Z. Liu, Y. Yu, H. Ouyang, Q. Wang, K. L. Cheng, W. Wang, Z. Liu, Q. Chen, and Y. Shen, Magicquill: An intelligent interactive image editing system, 2024. [18] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu, B. Li, P. Luo, T. Lu, Y. Qiao, and J. Dai, Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks, arXiv preprint arXiv:2312.14238, 2023. [19] Z. Chen, W. Wang, H. Tian, S. Ye, Z. Gao, E. Cui, W. Tong, K. Hu, J. Luo, Z. Ma et al., How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites, arXiv preprint arXiv:2404.16821, 2024. [20] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., Gpt-4 technical report, arXiv preprint arXiv:2303.08774, 2023. [21] A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang et al., Qwen2 technical report, arXiv preprint arXiv:2407.10671, 2024. [22] X. Ju, X. Liu, X. Wang, Y. Bian, Y. Shan, and Q. Xu, Brushnet: plug-and-play image inpainting model with decomposed dual-branch diffusion, 2024. [23] J. Zhuang, Y. Zeng, W. Liu, C. Yuan, and K. Chen, task is worth one word: Learning with task prompts for high-quality versatile image inpainting, arXiv preprint arXiv:2312.03594, 2023. [24] T. Ren, S. Liu, A. Zeng, J. Lin, K. Li, H. Cao, J. Chen, X. Huang, Y. Chen, F. Yan et al., Grounded sam: Assembling open-world models for diverse visual tasks, arXiv preprint arXiv:2401.14159, 2024. [25] Z. Wang, A. Li, Z. Li, and X. Liu, Genartist: Multimodal llm as an agent for unified image generation and editing, arXiv preprint arXiv:2407.05600, 2024. [26] O. Avrahami, D. Lischinski, and O. Fried, Blended diffusion for textdriven editing of natural images, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 18 20818 218. [27] O. Avrahami, O. Fried, and D. Lischinski, Blended latent diffusion, ACM transactions on graphics (TOG), vol. 42, no. 4, pp. 111, 2023. [28] S. Xie, Z. Zhang, Z. Lin, T. Hinz, and K. Zhang, Smartbrush: Text and shape guided object inpainting with diffusion model, in Proceedings of 12 the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 22 42822 437. [29] J. Zhuang, Y. Zeng, W. Liu, C. Yuan, and K. Chen, task is worth one word: Learning with task prompts for high-quality versatile image inpainting, arXiv preprint arXiv:2312.03594, 2023. [30] H. Manukyan, A. Sargsyan, B. Atanyan, Z. Wang, S. Navasardyan, and H. Shi, Hd-painter: High-resolution and prompt-faithful textguided image inpainting with diffusion models, arXiv preprint arXiv:2312.14091, 2023. [31] C. Binghui, L. Chao, Z. Chongyang, X. Wangmeng, G. Yifeng, and X. Xuansong, Replaceanything as you want: Ultra-high quality content [Online]. Available: https: //aigcdesigngroup.github.io/replace-anything/ replacement, 2023. [32] S. Wang, C. Saharia, C. Montgomery, J. Pont-Tuset, S. Noy, S. Pellegrini, Y. Onoe, S. Laszlo, D. J. Fleet, R. Soricut et al., Imagen editor and editbench: Advancing and evaluating text-guided image inpainting, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 18 35918 369. [33] L. Zhang, A. Rao, and M. Agrawala, Adding conditional control to text-to-image diffusion models, in IEEE/CVF International Conference on Computer Vision (ICCV), 2023. [34] Y. Huang, J. Huang, Y. Liu, M. Yan, J. Lv, J. Liu, W. Xiong, H. Zhang, S. Chen, and L. Cao, Diffusion model-based image editing: survey, arXiv preprint arXiv:2402.17525, 2024. [35] H. Liu, Z. Wan, W. Huang, Y. Song, X. Han, and J. Liao, Pd-GAN: Probabilistic diverse GAN for image inpainting, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 93719381. [36] H. Zheng, Z. Lin, J. Lu, S. Cohen, E. Shechtman, C. Barnes, J. Zhang, N. Xu, S. Amirghodsi, and J. Luo, Image inpainting with cascaded modulation GAN and object-aware training, in European Conference on Computer Vision (ECCV). Springer, 2022, pp. 277296. [37] S. Zhao, J. Cui, Y. Sheng, Y. Dong, X. Liang, E. I. Chang, and Y. Xu, Large scale image completion via co-modulated generative adversarial networks, arXiv preprint arXiv:2103.10428, 2021. [38] J. Singh, J. Zhang, Q. Liu, C. Smith, Z. Lin, and L. Zheng, Smartmask: Context aware high-fidelity mask generation for fine-grained object insertion and layout control, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 64976506. [39] D. Epstein, A. Jabri, B. Poole, A. Efros, and A. Holynski, Diffusion self-guidance for controllable image generation, Advances in Neural Information Processing Systems, vol. 36, pp. 16 22216 239, 2023. [40] N. Matsunaga, M. Ishii, A. Hayakawa, K. Suzuki, and T. Narihira, Fine-grained image editing by pixel-wise guidance using diffusion models, arXiv preprint arXiv:2212.02024, 2022. [41] Y. Yang, H. Peng, Y. Shen, Y. Yang, H. Hu, L. Qiu, H. Koike et al., instructions for exemplarImagebrush: Learning visual based image manipulation, Advances in Neural Information Processing Systems, vol. 36, 2024. in-context [42] Y. Shi, C. Xue, J. Pan, W. Zhang, V. Y. Tan, and S. Bai, Dragdiffusion: Harnessing diffusion models for interactive point-based image editing, arXiv preprint arXiv:2306.14435, 2023. [43] X. Pan, A. Tewari, T. Leimkuhler, L. Liu, A. Meka, and C. Theobalt, Drag your gan: Interactive point-based manipulation on the generative image manifold, in ACM SIGGRAPH 2023 Conference Proceedings, 2023, pp. 111. [44] T.-T. Nguyen, D.-A. Nguyen, A. Tran, and C. Pham, Flexedit: Flexible and controllable diffusion-based object-centric image editing, arXiv preprint arXiv:2403.18605, 2024. [45] W. Quan, J. Chen, Y. Liu, D.-M. Yan, and P. Wonka, Deep learningbased image and video inpainting: survey, International Journal of Computer Vision (IJCV), pp. 134, 2024. [46] Z. Xu, X. Zhang, W. Chen, M. Yao, J. Liu, T. Xu, and Z. Wang, review of image inpainting methods based on deep learning, Applied Sciences, vol. 13, no. 20, p. 11189, 2023. [47] M. Bertalmio, G. Sapiro, V. Caselles, and C. Ballester, Image inpainting, in International Conference and Exhibition on Computer Graphics and Interactive Techniques (SIGGRAPH), 2000, pp. 417424. [48] A. Criminisi, P. Perez, and K. Toyama, Region filling and object removal by exemplar-based image inpainting, IEEE Transactions on Image Processing, vol. 13, no. 9, pp. 12001212, 2004. [49] C. Zheng, T.-J. Cham, and J. Cai, Pluralistic image completion, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 14381447. [50] J. Peng, D. Liu, S. Xu, and H. Li, Generating diverse structure for image inpainting with hierarchical vq-vae, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 10 77510 784. [51] A. Lugmayr, M. Danelljan, A. Romero, F. Yu, R. Timofte, and L. Van Gool, RePaint: Inpainting using denoising diffusion probabilistic models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 11 46111 471. [52] A. Razzhigaev, A. Shakhmatov, A. Maltseva, V. Arkhipkin, I. Pavlov, I. Ryabov, A. Kuts, A. Panchenko, A. Kuznetsov, and D. Dimitrov, Kandinsky: an improved text-to-image synthesis with image prior and latent diffusion, arXiv preprint arXiv:2310.03502, 2023. [53] A. Liu, M. Niepert, and G. V. d. Broeck, Image inpainting via tractable steering of diffusion models, arXiv preprint arXiv:2401.03349, 2023. [54] J. Ho and T. Salimans, Classifier-free diffusion guidance, arXiv preprint arXiv:2207.12598, 2022. [55] G. Zhang, J. Ji, Y. Zhang, M. Yu, T. Jaakkola, and S. Chang, Towards coherent image inpainting using denoising diffusion implicit models, 2023. [56] C. Corneanu, R. Gadde, and A. M. Martinez, Latentpaint: Image inpainting in latent space with diffusion models, in IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2024, pp. 43344343. [57] S. Yang, L. Zhang, L. Ma, Y. Liu, J. Fu, and Y. He, Magicremover: Tuning-free text-guided image inpainting with diffusion models, arXiv preprint arXiv:2310.02848, 2023. [58] P. von Platen, S. Patil, A. Lozhkov, P. Cuenca, N. Lambert, K. Rasul, M. Davaadorj, and T. Wolf, Diffusers: State-of-the-art diffusion models, https://github.com/huggingface/diffusers, 2022. [59] S. Xie, Y. Zhao, Z. Xiao, K. C. Chan, Y. Li, Y. Xu, K. Zhang, and T. Hou, Dreaminpainter: Text-guided subject-driven image inpainting with diffusion models, arXiv preprint arXiv:2312.03771, 2023. [60] T. Yu, R. Feng, R. Feng, J. Liu, X. Jin, W. Zeng, and Z. Chen, Inpaint anything: Segment anything meets image inpainting, arXiv preprint arXiv:2304.06790, 2023. [61] S. Yang, X. Chen, and J. Liao, Uni-paint: unified framework for multimodal image inpainting with pretrained diffusion model, in ACM International Conference on Multimedia (MM), 2023, pp. 31903199. [62] B. Kawar, S. Zada, O. Lang, O. Tov, H. Chang, T. Dekel, I. Mosseri, and M. Irani, Imagic: Text-based real image editing with diffusion models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 60076017. [63] D. Valevski, M. Kalman, Y. Matias, and Y. Leviathan, Unitune: Textdriven image editing by fine tuning an image generation model on single image, arXiv preprint arXiv:2210.09477, 2022. [64] L. Han, S. Wen, Q. Chen, Z. Zhang, K. Song, M. Ren, R. Gao, Y. Chen, D. L. 0003, Q. Zhangli et al., Improving tuning-free real image editing with proximal guidance. CoRR, 2023. [65] G. Parmar, K. Kumar Singh, R. Zhang, Y. Li, J. Lu, and J.-Y. Zhu, Zero-shot image-to-image translation, in Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH), 2023, pp. 111. [66] N. Tumanyan, M. Geyer, S. Bagon, and T. Dekel, Plug-and-play diffusion features for text-driven image-to-image translation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 19211930. [67] Y. Zhang, J. Xing, E. Lo, and J. Jia, Real-world image variation by aligning diffusion inversion chain, arXiv preprint arXiv:2305.18729, 2023. [68] B. Cheng, Z. Liu, Y. Peng, and Y. Lin, General image-toimage guidance, arXiv preprint image translation with one-shot arXiv:2307.14352, 2023. [69] Q. Wu, Y. Liu, H. Zhao, A. Kale, T. Bui, T. Yu, Z. Lin, Y. Zhang, and S. Chang, Uncovering the disentanglement capability in text-toimage diffusion models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 1900 1910. [70] M. Brack, F. Friedrich, D. Hintersdorf, L. Struppek, P. Schramowski, and K. Kersting, Sega: Instructing diffusion using semantic dimensions, arXiv preprint arXiv:2301.12247, 2023. [71] L. Tsaban and A. Passos, LEDITS: Real image editing with ddpm inversion and semantic guidance, arXiv preprint arXiv:2307.00522, 2023. [72] W. Dong, S. Xue, X. Duan, and S. Han, Prompt tuning inversion for text-driven image editing using diffusion models, arXiv preprint arXiv:2305.04441, 2023. [73] C. H. Wu and F. De la Torre, Unifying diffusion models latent space, with applications to cyclediffusion and guidance, arXiv preprint arXiv:2210.05559, 2022. 13 [74] , latent space of stochastic diffusion models for zero-shot image editing and guidance, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 73787387. [75] G. Couairon, J. Verbeek, H. Schwenk, and M. Cord, Diffedit: Diffusion-based semantic image editing with mask guidance, in International Conference on Learning Representations (ICLR), 2023. [76] Z. Zhang, L. Han, A. Ghosh, D. N. Metaxas, and J. Ren, Sine: Single image editing with text-to-image diffusion models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 60276037. [77] K. Joseph, P. Udhayanan, T. Shukla, A. Agarwal, S. Karanam, K. Goswami, and B. V. Srinivasan, Iterative multi-granular image editing using diffusion models, arXiv preprint arXiv:2309.00613, 2023. [78] G. Kim, T. Kwon, and J. C. Ye, Diffusionclip: Text-guided diffusion models for robust image manipulation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 24262435. [79] A. Q. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mcgrew, I. Sutskever, and M. Chen, Glide: Towards photorealistic image generation and editing with text-guided diffusion models, in International Conference on Machine Learning (ICML). PMLR, 2022, pp. 16 78416 804. [80] Z. Geng, B. Yang, T. Hang, C. Li, S. Gu, T. Zhang, J. Bao, Z. Zhang, H. Hu, D. Chen et al., Instructdiffusion: generalist modeling interface for vision tasks, arXiv preprint arXiv:2309.03895, 2023. [81] Z. Liu, P. Luo, X. Wang, and X. Tang, Deep learning face attributes in the wild, in IEEE/CVF International Conference on Computer Vision (ICCV), December 2015. [82] H. Huang, R. He, Z. Sun, T. Tan et al., Introvae: Introspective variational autoencoders for photographic image synthesis, Advances in Neural Information Processing Systems (NIPS), vol. 31, 2018. [83] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, Imagenet: the large-scale hierarchical IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). image database, in Proceedings of Ieee, 2009, pp. 248255. [84] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick, Microsoft coco: Common objects in context, in European Conference on Computer Vision (ECCV). Springer, 2014, pp. 740755. [85] A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. PontTuset, S. Kamali, S. Popov, M. Malloci, A. Kolesnikov et al., The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale, International Journal of Computer Vision (IJCV), vol. 128, no. 7, pp. 19561981, 2020. [86] F. Yu, A. Seff, Y. Zhang, S. Song, T. Funkhouser, and J. Xiao, Lsun: Construction of large-scale image dataset using deep learning with humans in the loop, arXiv preprint arXiv:1506.03365, 2015. [87] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman et al., Laion-5b: An open large-scale dataset for training next generation image-text models, Advances in Neural Information Processing Systems (NIPS), vol. 35, pp. 25 27825 294, 2022. [88] T. Ren, S. Liu, A. Zeng, J. Lin, K. Li, H. Cao, J. Chen, X. Huang, Y. Chen, F. Yan et al., Grounded sam: Assembling open-world models for diverse visual tasks, arXiv preprint arXiv:2401.14159, 2024. [89] Wikipedia contributors, Peak signal-to-noise ratio Wikipedia, [Online; accessed 4-March-2024]. the free encyclopedia, 2024, [Online]. Available: https://en.wikipedia.org/w/index.php?title=Peak signal-to-noise ratio&oldid=1210897995 [90] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, The unreasonable effectiveness of deep features as perceptual metric, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 586595. [91] Wikipedia contributors, Mean squared error Wikipedia, the free encyclopedia, 2024, [Online]. Available: https://en.wikipedia.org/w/index.php?title=Mean squared error&oldid=1207422018 [Online; accessed 4-March-2024]. [92] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, Image quality assessment: from error visibility to structural similarity, IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600612, 2004. [93] C. Wu, L. Huang, Q. Zhang, B. Li, L. Ji, F. Yang, G. Sapiro, and N. Duan, GODIVA: Generating open-domain videos from natural descriptions, arXiv preprint arXiv:2104.14806, 2021. [94] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, 14 Learning transferable visual models from natural language supervision, in International Conference on Machine Learning (ICML). PMLR, 2021, pp. 87488763. [95] R. Mokady, A. Hertz, K. Aberman, Y. Pritch, and D. Cohen-Or, Nulltext inversion for editing real images using guided diffusion models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 60386047. [96] D. Miyake, A. Iohara, Y. Saito, and T. Tanaka, Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models, arXiv preprint arXiv:2305.16807, 2023. [97] S. Li, J. van de Weijer, T. Hu, F. S. Khan, Q. Hou, Y. Wang, and J. Yang, Stylediffusion: Prompt-embedding inversion for text-based editing, arXiv preprint arXiv:2303.15649, 2023. [98] Z. Pan, R. Gherardi, X. Xie, and S. Huang, Effective real image editing with accelerated iterative diffusion inversion, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 15 91215 921. [99] Lykon, Dreamshaper, 2022. [Online]. Available: https://civitai.com/ models/4384?modelVersionId=128713 [100] epinikion, epicrealism, 2023. [Online]. Available: https://civitai.com/ models/25694?modelVersionId=143906 [101] heni29833, Henmixreal, 2024. [Online]. Available: https://civitai. com/models/20282?modelVersionId= [102] Meina, Meinamix, 2023. [Online]. Available: https://civitai.com/ models/7240?modelVersionId=119057 [103] SG161222, Realisticvision, 2023. [Online]. Available: https://civitai. com/models/4201?modelVersionId="
        }
    ],
    "affiliations": [
        "ARC Lab, Tencent PCG",
        "Peking University",
        "The Chinese University of Hong Kong",
        "Tsinghua University"
    ]
}