{
    "paper_title": "Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models",
    "authors": [
        "Jiaqi Wang",
        "Kevin Qinghong Lin",
        "James Cheng",
        "Mike Zheng Shou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning (RL) has proven to be an effective post-training strategy for enhancing reasoning in vision-language models (VLMs). Group Relative Policy Optimization (GRPO) is a recent prominent method that encourages models to generate complete reasoning traces before answering, leading to increased token usage and computational cost. Inspired by the human-like thinking process-where people skip reasoning for easy questions but think carefully when needed-we explore how to enable VLMs to first decide when reasoning is necessary. To realize this, we propose TON, a two-stage training strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective 'thought dropout' operation, where reasoning traces are randomly replaced with empty thoughts. This introduces a think-or-not format that serves as a cold start for selective reasoning; (ii) a GRPO stage that enables the model to freely explore when to think or not, while maximizing task-aware outcome rewards. Experimental results show that TON can reduce the completion length by up to 90% compared to vanilla GRPO, without sacrificing performance or even improving it. Further evaluations across diverse vision-language tasks-covering a range of reasoning difficulties under both 3B and 7B models-consistently reveal that the model progressively learns to bypass unnecessary reasoning steps as training advances. These findings shed light on the path toward human-like reasoning patterns in reinforcement learning approaches. Our code is available at https://github.com/kokolerk/TON."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 4 5 8 6 1 . 5 0 5 2 : r Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models Jiaqi Wang1 Kevin Qinghong Lin2 James Cheng2 Mike Zheng Shou2(cid:66) 1The Chinese University of Hong Kong 2Show Lab, National University of Singapore"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning (RL) has proven to be an effective post-training strategy for enhancing reasoning in visionlanguage models (VLMs). Group Relative Policy Optimization (GRPO) is recent prominent method that encourages models to generate complete reasoning traces before answering, leading to increased token usage and computational cost. Inspired by the human-like thinking processwhere people skip reasoning for easy questions but think carefully when neededwe explore how to enable VLMs to first decide when reasoning is necessary. To realize this, we propose TON, two-stage training strategy: (i) supervised finetuning (SFT) stage with simple yet effective thought dropout operation, where reasoning traces are randomly replaced with empty thoughts. This introduces think-or-not format that serves as cold start for selective reasoning; (ii) GRPO stage that enables the model to freely explore when to think or not, while maximizing task-aware outcome rewards. Experimental results show that TON can reduce the completion length by up to 90% compared to vanilla GRPO, without sacrificing performance or even improving it. Further evaluations across diverse vision-language taskscovering range of reasoning difficulties under both 3B and 7B modelsconsistently reveal that the model progressively learns to bypass unnecessary reasoning steps as training advances. These findings shed light on the path toward human-like reasoning patterns in reinforcement learning approaches. Our code is available at https://github.com/kokolerk/TON."
        },
        {
            "title": "Introduction",
            "content": "To think or not to think, that is the question.\" Reinforcement learning (RL) has recently emerged as dominant post-supervised fine-tuning (SFT) strategy in vision-language models (VLMs) [1, 2, 3, 4]. Methods like GRPO [5] have shown promising results in enhancing reasoning capabilities through KL-divergence losses based on rule-driven rewards. However, these approaches often lead to unnecessarily long and redundant reasoning processes due to their reliance on full-length generative trajectories [6, 7, 8]. To address this inefficiency, some works attempt to shorten reasoning chains with rule-based reward penalties [9, 10, 11] during the pre-training phase or introduce external control mechanisms, such as in very recent Qwen3 [12]. Nonetheless, more natural and scalable solution is to enable the model to decide when to thinkmirroring how humans modulate cognitive effort in response to task difficulty. In this work, we begin by presenting empirical evidence that thinking is not always necessary. In AITZ [13], we observe that 51% of questions can be answered correctly even when the entire reasoning trace is omitted, resulting in significant savings in thought tokens. This finding underscores the potential of selective reasoning strategies to improve efficiency without sacrificing accuracy. Equal contribution. (cid:66) Corresponding authors. Preprint. Figure 1: Illustrating the think or not think trade-off. Left: For simple queries, explicit reasoning is unnecessarymodels like GRPO that always \"think\" incur redundant computation. Right: For more complex geometric problems, step-by-step reasoning is essential to arrive at the correct answer. Our proposed TON framework learns to adaptively invoke reasoning only when neededskipping it for easy cases while engaging in deeper inference for harder tasks. Secondly, by exploring simple prompting strategy allowing the model to skip reasoning steps for easier queries we observe that even math-enhanced VLMs struggle to adaptively omit redundant thought generation. Instead, they tend to default to conservative approach, producing full reasoning traces regardless of task difficulty. This suggests that the ability to think or not is not solely determined by reasoning capacity, but should instead be treated as distinct skillone that should be explicitly activated through format-following in supervised fine-tuning (SFT) stage. Motivated by the above observations, we introduce TON (i.e., Think-or-Not), two-stage training framework featuring simple yet effective thought dropout approach. This method explicitly replace reasoning traces with minimal nn delimiter and employs SFT to train the model that reasoning can be skippedthereby enabling the possibility of bypassing reasoning. subsequent GRPO stage further refines this selective-reasoning policy via self-exploration, rewarding answers without introducing extra regularization. As illustrated in Figure 1, vanilla GRPO consistently generates reasoning sequences regardless of task difficulty. In contrast, our method, TON, adaptively allocates reasoning based on the complexity of the task. For simple tasks (left), TON can bypass unnecessary reasoning and directly provide the answer, reducing 90% token usage. For more hard problems (right), it still engages in detailed, step-by-step reasoning to arrive at the correct solution. Built on top of TON, we using the Qwen-2.5-VL series and conduct extensive evaluations across range of vision-language tasksspanning counting (CLEVR [14], SuperCLEVR [15]), mobile agent navigation (AITZ [13]), and mathematical reasoning (GeoQA [16])which collectively cover spectrum of reasoning levels and diverse task settings. Overall, we find that TON achieves substantial reductions in completion length without compromising performancecutting 87% of tokens on CLEVR and 65% on GeoQA. Notably, on the multi-step navigation task AITZ, TON reduces the average task-level output length from 3.6K to 0.9K tokens. Moreover, we observe that omitting reasoning traces can even improve performance: on GeoQA, TON outperforms the vanilla GRPO baseline by up to 17% in accuracy, demonstrating free-lunch effect where shorter reasoning outperforms or matches longer trajectories. Comprehensive ablation studies further reveal that the skip-thought ratio increases progressively with reward improvements during training, suggesting the model learns to selectively bypass unnecessary reasoning steps in an adaptive manner."
        },
        {
            "title": "2 Related Works",
            "content": "Reinforcement Learning for VisionLanguage Models. Most VLMs start with SFT on large collections of instruction data to acquire broad foundational knowledge [17, 18, 13, 19]. To further improve performance, recent work has adopted post-training paradigm that leverages human feedback [20, 21, 22]. RL from human feedback (RLHF) fits reward model on preference annotations and refines the policy via Proximal Policy Optimization (PPO) [23, 24, 21, 25]. Direct Preference Optimization (DPO) [26] streamlines this workflow by recasting policy updates as binary classification task, aligning model outputs distributions with human preferences without reward module. Beyond these methods, Group Relative Policy Optimization (GRPO) [5] blends offline and online learning: it samples groups of thinking process, uses Answer verification (such as Math verifier) as reward feedback, and computes relative advantages within each group. By avoiding value function, 2 GRPO provide an elegant solution by promoting diverse reasoning paths and improved answer quality. Despite series of GRPO follow-up works [27, 28, 9], all of these approaches assume that every question demands full thinkingleading to lengthy decoding. In contrast, our work focuses on when to think instead of how to think: we introduce selective reasoning policy that learns to skip unnecessary think phases, boosting inference efficiency without sacrificing accuracy. Thinking in Language Models. From early Chain-of-Thought [29, 30, 31] prompting to recent reasoning-intensive reinforcement learning approaches [5, 22, 32, 33], reasoning has emerged as core dimension in the development of language models. Most existing work emphasizes how to enhance reasoning capabilities, often resulting in increasingly lengthy and complex thought processes [7, 34, 10] while relatively few studies address the efficiency of reasoning. For instance, [35] proposes long2short strategy to compress decoding length, [36] encourages models to output dont know to terminate unproductive reasoning, and [37] introduces token-budget-aware reasoning policy. While these approaches offer promising insights into controlling reasoning length, we argue for more foundational perspective: rather than deciding how to reason once the process has started, models should first determine whether reasoning is necessary at all. Simple questions may be answered directly without any explicit reasoning, while complex questions may require maintaining full reasoning trajectory [8, 6, 9]. In this work, we explore the selective reasoning paradigm within VLMs by introducing simple yet effective method thought-dropout. We validate its effectiveness on tasks such as Counting, Math, and further extend it to more practical agentic settings."
        },
        {
            "title": "3 Preliminary",
            "content": "Task Definition. We formalize the vision-language reasoning environment as Markov Decision Process (MDP) defined by tuple (V, Q, , π, r), covering wide range of vision-language tasks. Here, denotes the visual context (e.g., an image). is language-based query or question posed about the visual input. The model, governed by policy π, takes the input pair (V, Q) and generates predicted answer S. The environment provides scalar reward function r() based on the models response O. correct prediction, e.g., matches the ground truth answer , yields positive reward, while an incorrect one yields zero. The objective in this environment is to learn an adaptive policy πθ, parameterized by θ, that maximizes the expected reward, enabling the model to reason selectively and efficiently across diverse input settings. Reward Function. The reward function r() can be either model-based [23, 25] or rule-based, as recently demonstrated in [5, 22], which is typically categorized into two types: format rewards rf and outcome rewards ro. While the outcome rewards are usually carefully designed based on different tasks or requests in previous works [5, 1, 2, 9], the format reward rf , is always shared in the same. Given the response O, it should follow the required HTML tag format <think>T <think><answer>S<answer>, where is the reasoning process (i.e., thought) and is the predicted answer. This formulation requires the model to think before deriving the answer and makes it easy to parse both the reasoning process and the final outcome (e.g., via regular expression)."
        },
        {
            "title": "4 TON: Selective Reasoning via Policy Optimization",
            "content": "Observation. In practice, humans do not require explicit reasoning for all tasksmany can be completed intuitively. Similarly, models can often produce correct answers to simple questions without explicit thinking. As illustrated in figure 2, the percentages of correct and incorrect samples under different setups with and without the thinking process in inference (see Appendix for overall performance). We find that 52.1% of answers remained correct without think, and 14.5% were even correct only without itimplying that explicit thinking is not always necessary. straightforward idea is to prompt the model to decide whether to think or not (we prompt the model to skip thinking in the simple questions in Sec. 5.4). However, as shown in our experiments (Figure 5d and Appendix G.7), 3 Figure 2: Accuracy comparison of with v.s. without thinking during SFT using Qwen-2.5-VL-3B on the AITZ task. the model still tends to generate the full reasoning process without any no-think try. This suggests that the ability to decide whether to think is not solely governed by reasoning capability, but should instead be treated as separate skillone that must be explicitly trained through format-following during the supervised fine-tuning (SFT) stage. These observations motivate us to activate this ability early in the SFT stage and develop TON, which enables selective reasoning by automatically switching between think and non-think modes. 4.1 First SFT stage: Thought Dropout In the initial stage, the model is typically fine-tuned on think-answer formatted data, where the think contains high-quality reasoning traces to serve as cold start. To extend this predefined reasoning ability to selective reasoning, we view think vs. non-think as part of the output format itself by dropping the think component during training. However, it is difficult to determine which samples should be skipped, as different models exhibit varying reasoning capabilities. Therefore, we begin with random dropout and allow the model to learn to decide for itself during the second RL stage (Sec.4.2). To this end, we propose Thought Dropout that randomly injecting empty thought segments, requiring only minor code changes: Algorithm 1 Pseudo-code for thought_dropout def thought_dropout(thought, dropout_prob): if random.random() < dropout_prob: thought = \"nn\" return thought This approach injects both the answer format and the skip-thought format as prior knowledge before the second RL stage. Where do Thoughts come from? Given policy operating in an environment (V, Q, , π, r), key challenge is how to curate high-quality cold-start thought data without relying on external models, such as closed-source APIs. naïve approach is to run multiple inference passes and retain only successful cases based on answer matchingbut we find this to be less effective. To address the scarcity of high-quality thought data, we instead adopt reverse thinking strategy: leveraging the base model π itself to self-generate rich corpus of thought sequences. Specifically, given the visual context V, textual query Q, and ground-truth answer , we prompt the policy πθ to deduce the corresponding intermediate thought as follows: πθ (cid:0)V, Q, (cid:1) (1) Specially, we generate intermediate thoughts with the following prompts:"
        },
        {
            "title": "Prompt for Reverse Thinking",
            "content": "Based on the following question and image, generate thought process to explain how to derive the answer from the inputs. Image: {Image} Question: {Question} Answer: {Answer} Do not output the answer, only generate the reasoning process. Formulate your outputs using concise language. In this way, we curate sufficient thought data without relying on external models. These serve as our cold-start training corpus, enabling us to apply the Thought Dropout strategy during SFT to activate the models ability to bypass thoughts. 4.2 Second RL stage: Group Relative Policy Optimization Although SFT teaches the skip-thought format, it still leaves central question unresolved: when should thoughts be skipped or retained? Ideally, the model should learn to explore this decision on its own. To this end, we adopt reinforcement learning via GRPO to enhance the models ability to explore this decision as part of its reasoning process. 4 Figure 3: Illustration of the responses from GRPO and TON. q1 is the question and {o1, , o5} are the generated responses containing thoughts (circle) and answers (triangle). TON can sample from the empty think Tnn, thus enhancing the response diversity over the vanilla GRPO. Given an image and text query Q, GRPO samples candidate responses with variations {o1, o2, . . . , oN } from the policy πθ and evaluates each response oi using reward function r(), which measures the quality of the candidate in the context of the given question. To determine the relative quality of these responses, GRPO normalizes the rewards by computing their mean and standard deviation and subsequently derives the advantage as: Ai = r(oi) mean{r(o1), r(o2), . . . , r(oN )} std{r(o1), r(o2), . . . , r(oN )} (2) where Ai represents the advantage of the candidate response oi relative to other sampled responses. GRPO encourages the model to generate responses with higher advantages within the group by updating the policy πθ using the following objective: JGRP O(θ) = E[{oi}N i=1 πθold (v, q)] 1 (cid:88) i=1 {min[αi Ai, βi Ai] βDKL[πθπref ]} (3) αi = πθ(oiv, q) πθold (oiv, q) , βi = clip (cid:18) πθ(oiv, q) πθold(oiv, q) (cid:19) , 1 + ϵ, 1 ϵ . (4) How does TON impact GRPO? As illustrated in Fig. 3, our TON allows the model to choose emptythink Tnn during the inference step, thus resulting in significant variation in the distribution between the non-think (oi Tnn) and think responses (oi ) by TON compared to both think ones (oi ) generated by vanilla GRPO. Unlike previous works like DAPO [27] emphasize on advantage distribution Ai by dynamic sampling in the sparse reward space, our TON shifts the focus to the latent distribution space of responses (πθ(oiv, q)), thus enhancing the diversity of the terms α and β in Eq. 4. How to design Rewards? To support GRPO training across diverse settings, it is crucial to carefully examine reward design choices. We consider two main types of matching: (i) Discrete Matching. For tasks with deterministic, categorical or numerical outputse.g., classification, counting, or math problemswe use binary value reward rd(s, g) = 1(s = g): if the predicted answer matches the ground-truth g, we assign rd = 1; otherwise, rd = 0. (ii) Continous Matching. For tasks producing continuous outputse.g., spatial coordinates in UI navigation or object groundingwe allow tolerance region. Given predicted point = [x, y] and ground-truth box = [x1, y1, x2, y2], we define: rc(p, b) = (cid:26)1, lies inside b, 0, otherwise. If only ground-truth point is available, we use distance threshold θ: rc(p, p) = (cid:26)1, p2 θ, 0, otherwise. In practice, we sum the applicable components to form an outcome reward: ro = rd + rc. This simple yet flexible scheme can cover classification, numeric reasoning, and grounding. See Appendix for details on adapting these rewards alongside the format reward to individual downstream tasks. 5 Table 2: Performance comparison between TON and vanilla GRPO. Acc. is the accuracy on the test set. Time is the RL training time. Length is the average competition length at the end of training. Counting (CLEVR) Math (GeoQA) Acc () Time () Len. () Acc () Time () Len. () Acc () Time () Len. () Qwen-2.5-VL-3B Qwen-2.5-VL-3B Qwen-2.5-VL-7B 64.0 Baseline w. GRPO 93.5 98.5 w. TON - 1h 44m 57m 306 227 36 37 51 Gain +5.0 47m 87% + - 2h 50m 2h 4m 46m 924 272 96 38 44 61 65% + - 3h 4m 2h 34m 29m 939 1069 112 90%"
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we conduct experiments on various benchmarks to evaluate our approach. Mainly, we design the experiments to study the following key questions: Q1: Compared to vanilla GRPO, how does TON impact performance and efficiency? Q2: Is there correlation between TONs skipping behavior and the strength of reasoning ability (e.g., different model sizes or single model under different iterations)? Q3: Do we really need SFT with thought dropout? Can we rely solely on prompt following if the base model is strong enough? 5.1 Benchmarks and Settings To evaluate the effectiveness and generalization ability of our approach on the below settings: Type OOD Benchmark Difficulty Answer Thought len. Table 1: Summary of benchmark used in our evaluation. Benchmarks. We evaluate TON on three vision-language benchmarks, including the general benchmark CLEVR [14] (3D object counting), agent benchmark AITZ [13] (mobile navigation), and the math benchmark GeoQA [16] (middle school math questions) as illustrated in Table 1, spanning spectrum of reasoning levels from simple to complex. To benchmark the models Out-of-Distribution (OOD) performance, we also evaluate on Super-CLEVR [15] to supplement the CLEVR. AITZ comprises four test domains: we train on the {General} and test on the remaining OOD domains: {Web shopping, Google apps, Install}. We remove the choices in GeoQA and ask the model to generate the answer, enhancing the reasoning complexity. AITZ includes action thought annotations, which we utilize directly, while applying our reverse thinking to generate thoughts for SFT on CLEVR and GeoQA. More benchmark details refer to Appendix E. Integrate Easy Integrate Easy Hard Number GUI Agent Medium Action (x) GUI Agent Medium Action (x) CLEVR [14] Super-CLEVR [15] GeoQA [16] AITZ [13] AITZ (OOD) 586 1652 283 283 Counting Counting Math Training details. We conduct our experiments using Qwen-2.5-VL-Instruct-3B/7B [38] as the base model. All experiments are conducted utilizing 8 NVIDIA H20 GPUs. We train 100 steps for both CLEVR and AITZ, and 300 epochs for GeoQA, given its higher reasoning difficulty level. See setup details in Appendix F. We leverage vLLM [39] to accelerate GRPO training. We add the SFT stage before GRPO as the baseline on the agent task with the same setting as TON because we observe that directly applying GRPO would cause the 0 coordinate reward during the training process, considering its complex output format. For simplicity, we set the dropout probabilities to 50% and examine the impact of different dropout ratios selected from {20%, 50%, 80%} in Sec 5.3. For evaluation, we test all the datasets under the greedy strategy. In CLEVR and GeoQA tasks, where answers are numerical, we measure accuracy by comparing the predicted number to the ground truth. In the AITZ task, where answers are structured as JSON-formatted actions, we report step-level and task-level metrics, including type accuracy (correct action type) and exact accuracy (correct action type and click coordinates) following [17]. 6 (a) Training rewards (b) Completion length (c) Skip think ratio (d) Output len. with think Figure 4: Training metrics comparison between TON and GRPO on GeoQA. (a) Training rewards, (b) Completion length over training steps, (c) Ratio of non-think outputs to total samples at each step for TON, and (d) Average completion length of think outputs across training. Table 3: Out-of-domain (OOD) performance comparison between our method TON and GRPO on the the AITZ multi-step mobile navigation. Type is the action type accuracy and Exact requires both the type and value to be correct exactly. Avg. is the average accuracy of all domains. Task-level thoughts is the average output lengths on all OOD domains. Step-level accuracy is reported. Think? IID General Qwen-2.5-VL-3B Ours w. SFT w. SFT w. GPRO w. TON Gain Qwen-2.5-VL-7B w. GRPO w. TON Gain Ours type 0.01 0.39 0.67 0.74 0.74 +0.0 0.28 0.64 0.74 +0.1 exact 0 0.11 0.12 0.6 0.6 +0.0 0.14 0.22 0.54 +0.32 Google apps exact type OOD Web Install type exact type 0.01 0.44 0.53 0.72 0.74 +0.02 0.26 0.73 0.62 -0.11 0 0.12 0.17 0.57 0.56 -0. 0.1 0.32 0.23 -0.09 0.01 0.54 0.56 0.7 0.72 +0.02 0.33 0.6 0.68 +0.08 0 0.19 0.13 0.5 0.5 +0.0 0.13 0.15 0.47 +0.32 0.01 0.47 0.58 0.81 0.78 -0. 0.39 0.62 0.73 +0.09 Avg Task-level type exact Thoughts len. 0.01 0.46 0.58 0.74 0.75 +0.01 0.31 0.65 0.69 +0.04 0 0.15 0.14 0.59 0.59 +0.0 2132 742 3572 3664 922 2742 0.13 0.23 0.45 +0.22 2364 3304 3272 exact 0 0.17 0.14 0.65 0.64 -0.01 0.16 0.23 0.55 +0.32 5.2 Q1: Performance and Efficiency Comparison between TON and GRPO In Table 2, we present TON on the CLEVR and GeoQA benchmarks under both 3B and 7B settings, with the performance, time consumption, and the average completion length at the RL stage. We find that TON effectively reduces the average of the completion length by up to 90% while achieving comparable even superior performance compared to GRPO with maximum of 17 Acc. gains. This imply that skipping unnecessary reasoning can lead to better performance. The reduction of the completion length decreases the decoding time when generating samples, thus simultaneously shortening the training time. Figure 4a & 4b show the reward and completion length curves where TON remains the rewards on par with vanilla GRPO while the completion length reduces significantly. Appendix G.2 & G.1 shows the entire metrics during training. Multi-step Navigation and OOD Testing. In Table 3, we evaluate TONs performance on AITZ multi-step mobile navigation, we also assessed its generalization capabilities on OOD test sets using greedy decoding strategy. Table 3 summarizes the step-level type match accuracy and exact match accuracy for both IID (general) and OOD (Google Apps, web shopping, and install) domains on AITZ, with detailed training visualization in Appendix G.3. Overall, TON demonstrates comparable OOD generalization performance to GRPO, while significantly reducing the task-level output length from 3K to 0.9K (70% token saving). This highlights the strong potential of TON to substantially reduce completion length without compromising performance. See Appendix G.4 for the OOD performance on other benchmarks. 5.3 Q2: Skip Thought Ratio Analysis Beyond the performance change and completion length reduction achieved by TON, we further investigated the evolution of the skip ratio in Thought dropout during the training step. Figure 4c illustrates the percentage of skip ratio in the generated samples at each step on GeoQA. We observed an increasing trend in the skip ratio during the training process with the increase in training reward. similar trend is observed across three benchmarks in Figure 13 in the Appendix G.6. This phenomenon suggests that the model progressively internalizes the reasoning processlearning 7 (a) Completion length (b) Skip think ratio (c) Training rewards (d) Prompting v.s. SFT Figure 5: Further Analysis of TON on the AITZ benchmark. (a)(b)(c) is the average completion length, skip thought ratios, and the reward under different dropout probabilities. (d) Prompting (hybrid) does not reduce the completion length, while TON using SFT can effectively reduce it. to skip explicit thoughts while still producing accurate answers. Moreover, Figure 4d illustrates the length of these outputs generated with think. TON maintain comparable lengths to the vanilla GRPO, indicating that the TON model can choose not to think but remains diligent when deeper reasoning is necessary. Thought dropout ratio ablation. We experiment with the impact of different thought dropout ratios of 20%, 50%, and 80% during the SFT stage. Figure 5a & 5b show the completion lengths and the skip ratio during the training process on AITZ. Figure 5c shows close reward curve of these three variants. Refer more metrics on Appendix G.5. Although the dropout ratios differ, TON consistently exhibits an increasing skip ratio as training progresses. Notably, the 20% setting shows rapid increase in skip rate, while the higher 80% setting remains relatively stable throughout training. This motivates us to start with lower dropout probability for further investigation. TON can then be dynamically optimized according to reward signalsdecreasing the dropout ratio when performance is high and increasing it when performance drops. 5.4 Q3: Emprical Verfication of SFT Significance in TON In addition to incorporating the skip-think format during the SFT stage as in TON, we explored simpler alternative: modifying the prompt to encourage the model to automatically omit reasoning steps, enabling direct GRPO training without the need for separate SFT stage. The hybrid-thought prompt is defined as follows: Prompt for Hybrid Thinking conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant evaluates whether the question is simple enough to answer directly. If simple, the output is formatted as <think>nn<think><answer>answer here<answer>. If the question is difficult, the assistant needs to first think then answering the question. The output is formatted as <think> reasoning process here <think><answer> answer here <answer>. The assistant is encouraged to use <think>nn<think> while maintaining accuracy. Figure 5d shows the completion length of GRPO using the hybrid prompt, vanilla GRPO (with full-think prompt), and TON throughout the training process on AITZ. Appendix G.7 presents similar trends across three benchmarks, revealing only minor differences in completion length between the hybrid prompt and vanilla GRPO. Moreover, we observe only 2 skip cases in GeoQA and none in AITZ among all samples generated by GRPO during both training and inference. We attribute this to the models tendency to play it safe by generating long and detailed reasoning, consistent with its ingrained behavioral patterns learned during pre-training or SFT. Since the model does not produce skip-thought outputs, applying additional reward to these outputs has no effect, resulting in zero contribution throughout training. These findings highlight the necessity of our SFT stage with thought dropout (Sec. 4.1) to establish the desired format-following behavior. 8 Figure 6: Comparison between GRPO and TON on Agent setting AITZ [13]. TON adaptively skips unnecessary think steps during multi-step mobile navigation, achieving greater decoding efficiency compared to GRPO while maintaining task accuracy (saving 60% tokens in this case). Table 4: Illustration between Thinking and Non-Thinking modes from CLEVR [14]. TON demonstrates selective activation of reasoningengaging thought only when neededwhereas GRPO generates reasoning traces for both cases indiscriminately. The full outputs are in Table 10. Counting examples from CLEVR [14] Question (GT) How many items are there in the image? <answer>3</answer> How many items are there in the image? <answer>8</answer> GRPOs outputs <think> The image shows three objects ... 1. Green Cylinder: Transparent (metallic) 2. Teal Sphere: Transparent (metallic) </think> <answer>3</answer> <think> - count six distinct objects in the picture: large green matte cylinder, blue matte cylinder, ... - also note the size and color variety of each object as described by the visual cues. </think> <answer>10</answer> TONs outputs <think>nn</think> <answer>3</answer> <think> Okay, lets check this out. The user provided image with .... First, need to count ... 1. \"A small purple rubber sphere...\"... 3. \"A small gray rubber cube...\" Third. ... Wait, thats eight distinct entries... </think> <answer> 8 </answer> 5.5 Qualitative Examples Figure 6 compares GRPO and TON on the AITZ benchmark for multi-step mobile navigation. While GRPO generates verbose reasoning at every step, TON adaptively skips unnecessary thinking, reducing token usage by 60% without sacrificing task accuracy. This demonstrates TONs efficiency in handling real-world, long-horizon procedural agent tasks. Table 4 further illustrates TONs ability to selectively activate reasoning. Unlike GRPO, which consistently generates detailed thought traces, TON omits reasoning for simple questions that can be answered at glance, while producing accurate and focused reasoning for complex scenarios involving visual occlusion."
        },
        {
            "title": "6 Conclusion",
            "content": "We present TON, simple yet effective two-stage training framework that enables vision-language models to learn when to reasonintroducing selective reasoning as controllable and trainable behavior. By combining thought dropout during supervised fine-tuning with reward-guided refinement via GRPO, TON significantly reduces completion length (up to 90%) without sacrificingand in some cases improvingperformance across diverse reasoning tasks. Our findings challenge the assumption that full reasoning traces are always beneficial and pave the way for more efficient, human-like reasoning strategies in both multimodal intelligence and reinforcement learning."
        },
        {
            "title": "7 Acknowledge",
            "content": "We sincerely thank Dongchi Huang for his invaluable guidance on the code and for providing essential computational resources. We also appreciate Binghui Xies insightful discussion on topic selection and idea suggestions. Additionally, we are grateful to Qiguang Chen and Yuxuan Wan for their thoughtful and constructive feedback on this paper. Finally, we extend our gratitude to Xiaojun Guo and Qixun Wang for their valuable advice on visual reasoning and the GRPO series methods."
        },
        {
            "title": "References",
            "content": "[1] Zilin Huang, Zihao Sheng, Yansong Qu, Junwei You, and Sikai Chen. Vlm-rl: unified vision language models and reinforcement learning framework for safe autonomous driving. arXiv preprint arXiv:2412.15544, 2024. [2] Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning, 2025. [3] Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1 : generalist r1-style vision-language action model for gui agents, 2025. [4] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models, 2025. [5] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [6] Jinyan Su, Jennifer Healey, Preslav Nakov, and Claire Cardie. Between underthinking and overthinking: An empirical study of reasoning length and correctness in llms, 2025. [7] Qiguang Chen, Libo Qin, Jiaqi Wang, Jinxuan Zhou, and Wanxiang Che. Unlocking the capabilities of thought: reasoning boundary framework to quantify and optimize chain-ofthought, 2024. [8] Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, and Yisen Wang. When more is less: Understanding chain-of-thought length in llms, 2025. [9] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do not think that much for 2+3=? on the overthinking of o1-like llms, 2024. [10] Haotian Luo, Haiying He, Yibo Wang, Jinluan Yang, Rui Liu, Naiqiang Tan, Xiaochun Cao, Dacheng Tao, and Li Shen. Adar1: From long-cot to hybrid-cot via bi-level adaptive reasoning optimization, 2025. [11] Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, and Shiguo Lian. Dast: Difficulty-adaptive slow-thinking for large reasoning models, 2025. [12] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, 10 Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [13] Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. Android in the zoo: Chain-of-action-thought for gui agents. arXiv preprint arXiv:2403.02713, 2024. [14] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary In Proceedings of the IEEE conference on computer vision and pattern visual reasoning. recognition, pages 29012910, 2017. [15] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan Yuille. Super-clevr: virtual benchmark to diagnose domain robustness in visual reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1496314973, 2023. [16] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. arXiv preprint arXiv:2105.14517, 2021. [17] Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent. arXiv preprint arXiv:2411.17465, 2024. [18] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. [19] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. arXiv preprint arXiv:2312.08914, 2023. [20] OpenAI. Gpt-4 technical report, 2023. [21] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [22] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan 11 Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [23] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [24] OpenAI. Gpt-4v, 2023. [25] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020. [26] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. [27] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [28] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. [29] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [30] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [31] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023. [32] Hao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning. arXiv preprint arXiv:2406.11896, 2024. [33] Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, et al. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning. arXiv preprint arXiv:2411.02337, 2024. [34] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023. [35] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [36] Zirui Zhao, Hanze Dong, Amrita Saha, Caiming Xiong, and Doyen Sahoo. Automatic curriculum expert iteration for reliable llm reasoning. arXiv preprint arXiv:2410.07627, 2024. [37] Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. Token-budget-aware llm reasoning, 2025. [38] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [39] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [40] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android in the wild: large-scale dataset for android device control. arXiv preprint arXiv:2307.10088, 2023. 12 [41] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024."
        },
        {
            "title": "Contents",
            "content": "A Motivation Experiments Rewards for Downstream Tasks Limitations Broader Impact Dataset Setup Experiments G.1 TON on MathGeoQA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.2 TON on CountingCLEVR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.3 TON on Mobile AgentAITZ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.4 OOD Performance of TON on CLEVR . . . . . . . . . . . . . . . . . . . . . . . . G.5 Different Thought Dropout Probabilities . . . . . . . . . . . . . . . . . . . . . . . G.6 Skip-thought Ratio on Different benchmarks . . . . . . . . . . . . . . . . . . . . . G.7 Prompt v.s. SFT on different benchmarks . . . . . . . . . . . . . . . . . . . . . . G.8 Visualization Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.9 Prompt for AITZ task . G.10 Reward for length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 15 15 15 16 17 17 18 19 20 20 21 22"
        },
        {
            "title": "A Motivation Experiments",
            "content": "Table 5 presents the performance of the agent VQA with and without the think source during the SFT stage, as well as with and without the think format in the GRPO reward function. Using the think source results in higher performance but longer output lengths, while excluding it leads to lower performance with shorter outputs. Table 5: Qwen2.5-VL-3B on the agent dataset (Android-in-the-zoo) with/ without think before the answer by SFT and vanilla GRPO. acc is the test accuracy and len is the output length of step-level."
        },
        {
            "title": "GRPO",
            "content": "acc 0.67 0.39 len 181 29 acc 0.74 0. len 88 29 think wo think"
        },
        {
            "title": "B Rewards for Downstream Tasks",
            "content": "General VQA: = rf + rd, where rf is the format reward and rf = 1 if the response follows the think answer format, otherwise 0, rd is the concrete reward that rd = 1 if the predicted answer is equal to the ground truth number. Agent VQA: = rf + rd + rc, where rf is the format reward and rf = 1 if the response follows the think answer format, otherwise 0, rd is the concrete reward that rd = 1 if the predicted action type is equal to the ground truth action type,e.g.,click, press_home, rc is the continues reward for the predicted coordinates when the action type is click. In this paper, we use the normalized coordinates ranging from 0-1 and set θ = 0.14 following [18]. Math VQA: = rf + rd, where rf is the format reward and rf = 1 if the response follows the think answer format, otherwise 0, rd is the concrete reward that rd = 1 if the predicted answer is equal to the ground truth number."
        },
        {
            "title": "C Limitations",
            "content": "Due to computational resources, our current work focuses on smaller-sized visual-language models like 3B and 7B, the proposed method has not been evaluated on even larger models (e.g.,235B). We implement TON on the open-domain VLMs; however, without access to the source code of proprietary VLMs like GPT-4o, the proposed method has not been implemented on them."
        },
        {
            "title": "D Broader Impact",
            "content": "In this paper, we propose simple yet effective method TON, to cooperate SFT and RL stages by thought dropout. We improve the vanilla GRPOs performance by sampling minor code changes to teach the model to reason during the RL exploration stage selectively. This enables deeper understanding of RL in VLMs, inspiring flexible injection of prior knowledge into the SFT stage instead of manually creating rule-based rewards. For social impact, this work has certain impact on the RL research in the VLM and LLM."
        },
        {
            "title": "E Dataset",
            "content": "General VQA. The CLEVR dataset [14] is designed to generate complex multi-step questions based on synthetic images, assessing models true reasoning ability. It is diagnostic dataset that includes 100,000 rendered images and approximately one million automatically generated questions, of which 853,000 are unique. The dataset features challenging questions involving counting, comparison, logical reasoning, and memory storage, while the images depict simple 3D shapes. In contrast to the original CLEVR dataset, Super-CLEVR [15] introduces more complex visual components and 15 offers better control over the factors contributing to domain shift. For our experiments, we select subset of 1,000 datasets that contain only counting problems for training. We evaluate the models performance on test sets by selecting 200 samples from CLEVR that were not seen in the training set, as well as 200 counting problems from the out-of-distribution Super-CLEVR dataset. Math VQA. GeoQA [16] is large-scale geometric question answering dataset that contains 4,998 geometric problems collected from real math exams in Chinese middle school. Each problem is accompanied by annotated programs illustrating the solution process. While this dataset features multiple-choice questions, we increase the difficulty in this paper by removing the answer choices and requiring the model to generate the answers directly. We select subset of 1k problems that involve computing angles and side lengths for training and test the model on this training set. GUI Agent. AITZ [13] is dataset designed for the graph user interface (GUI) navigation task derived from the large-scale mobile benchmark Android-in-the-wild (AITW [40]). It features unique annotation called chain-of-action thought (CoAT), establishing connection between perceptionspecifically, the understanding of screen layouts and UI elementsand cognition, which involves action decision-making. The AITZ dataset includes 2,504 operational trajectories that encompass 18.6K real-world intentions. Additionally, it is categorized into five subsets based on application domains: General, Install, GoogleApps, Single, and WebShopping. We train the model using the General domain with dataset of randomly selected 1k examples and evaluate its performance on the corresponding test sets, as well as on other out-of-distribution domains."
        },
        {
            "title": "F Setup",
            "content": "We use Llamafactory [41] for the SFT stage with full parameters, and the training time is no longer than 15 minutes for both Qwen2.5-VL-3B/7B models. We set θ = 0.14 following [18]. We use vLLM [39] and the zero1_no_optimizer GRPO settings to optimize further: Table 6: Training Parameters for the first SFT of TON Parameter cutoff_len per_device_train_batch_size gradient_accumulation_steps learning_rate lr_scheduler_type warmup_ratio epoch Value 2048 8 1 1.0e-5 cosine 0.1 2 Table 7: Training Parameters for the second GRPO stage of TON in general/agent Parameter max_prompt_length max_completion_length per_device_train_batch_size gradient_accumulation_steps learning_rate lr_scheduler_type bf16 β gradient_checkpointing attn_implementation min_pixels max_pixels temperature num_generations step Value 4096 2048 1 2 1e-6 constant true 0.04 true flash_attention_2 3136 501760 1.0 8 100 16 Table 8: Training Parameters for the second GRPO stage of TON in math Parameter max_prompt_length max_completion_length per_device_train_batch_size gradient_accumulation_steps learning_rate lr_scheduler_type bf16 β attn_implementation min_pixels max_pixels temperature num_generations step Value 4096 2048 1 2 1e-6 constant true 0.04 flash_attention_2 3136 501760 1.0"
        },
        {
            "title": "G Experiments",
            "content": "G.1 TON on MathGeoQA Figure 7 & 8 illustrate the progression of various variables throughout the training process. Figure 7: TON and GRPO visualization during the training process on Qwen2.5-VL-3B on GeoQA. 17 Figure 8: TON and GRPO visualization during the training process on Qwen2.5-VL-7B on GeoQA. G.2 TON on CountingCLEVR Figure 9 illustrates the progression of various variables throughout the training process. Figure 9: TON and GRPO visualization during the training process on Qwen2.5-VL-3B on CLEVR. G.3 TON on Mobile AgentAITZ Figure 10 & 11 illustrate the progression of various variables throughout the training process. 18 Figure 10: TON and GRPO visualization during the training process on Qwen2.5-VL-3B on AITZ. Figure 11: TON and GRPO visualization during the training process on Qwen2.5-VL-7B on AITZ. G.4 OOD Performance of TON on CLEVR Table 9 compares the IID and OOD performance of TON and vanilla GRPO. TON demonstrates superior performance in both IID and, particularly, OOD scenarios under easy reasoning tasks, helping to avoid overfitting to the training set of vanilla GRPO. 19 Table 9: Qwen2.5-VL-3B on the IID domain CLEVR and OOD domain Super-CLEVR. CLEVR Super-CLEVR think ours base SFT"
        },
        {
            "title": "GRPO\nTON",
            "content": "acc 64 88.5 93.5 98.5 acc 57.3 13.17 51.9 62. G.5 Different Thought Dropout Probabilities Figure 12 illustrates the progression of various variables throughout the training process under different dropout probabilities. Figure 12: GRPO visualization during the training process on Qwen2.5-VL-3B on AITZ under dropout probabilities 20%, 50%, 80%. G.6 Skip-thought Ratio on Different benchmarks Figure 13 illustrates the skip-thought ratios under TON throughout the training process under different VQA benchmarks. (a) General VQA (b) Agentic VQA (c) Math VQA Figure 13: Skip Ratio of the output thinking during our TON training on three benchmarks. 20 G.7 Prompt v.s. SFT on different benchmarks Figure 14 & 15 & 16 illustrate the progression of various variables throughout the training process between injecting the skip-thought during the prompt and the SFT stage. Figure 14: hybrid prompt v.s. SFT visualization during the training process on Qwen2.5-VL-3B on clevr. Figure 15: hybrid prompt v.s. SFT visualization during the training process on Qwen2.5-VL-3B on AITZ. 21 Figure 16: hybrid prompt v.s. SFT visualization during the training process on Qwen2.5-VL-3B on GeoQA. G.8 Visualization Examples Figure 17: Examples of TON on math VQA and GUI agent VQA settings. 22 Counting examples from CLEVR [14] Question GRPOs outputs How many items are there in the image? <think>The image shows three objects: large green cylinder from 1919, large teal sphere, and large golden sphere. Both the teal and golden spheres appear to be in translucent state, indicating that they are metallic. The green cylinder is plain and opaque, without any reflective properties. 1. Green Cylinder: Transparent (metallic) 2. Teal Sphere: Transparent (metallic) 3. Golden Sphere: Transparent (metallic) The question seems to ask for the count of items, and considering that all three objects are distinguishable from each other as separate items in the image, the count is three. </think> <answer>3</answer> How many items are there in the image? <think> - count six distinct objects in the picture: large green matte cylinder, blue matte cylinder, large green reflective sphere, large blue matte cube, small blue matte cube, light blue rubber object, shiny silver cube, large reflective blue object, silver or dark green reflective sphere, shiny purple metallic cylinder, and blue matte sphere. - observe the reflective surface properties of the objects around the central large green sphere. - also note the size and color variety of each object as described by the visual cues. </think> <answer>10</answer> TONs outputs <think>nn</think> <answer>3</answer> <think>Okay, lets check this out. The user provided image with several objects and their properties. Each entry starts with \"A\" followed by attributes like size, color, material, shape, rotation, 3D coordinates, and pixel coordinates. First, need to count how many items there are. Let me list each entry one by one. 1. \"A small purple rubber sphere...\" Thats the first item. 2. \"A large yellow rubber cube...\" Second item. 3. \"A small gray rubber cube...\" Third. 4. \"A large green rubber cylinder...\" Fourth. 5. \"A small blue metal cube...\" Fifth. 6. \"A small green metal sphere...\" Sixth. 7. \"A large brown metal cube...\" Seventh. 8. \"A small green rubber cylinder...\" Eighth. Wait, thats eight distinct entries. Each one has unique combinations of attributes. cant see any duplicates. Each has different values for color, size, material, rotation, or coordinates, so theyre separate. So yes, there are eight items in total. </think> <answer> 8 </answer> Table 10: Counting example from CLEVR [14]. Although the question is simple, the two conditioned images differ significantly in difficulty: the left image is clearly easier, while the right involves greater complexity due to object occlusion. TON demonstrates selective activation of reasoningengaging thought only when neededwhereas GRPO generates reasoning traces for both cases indiscriminately. 23 G.9 Prompt for AITZ task"
        },
        {
            "title": "AITZ System Prompt",
            "content": "You are an assistant trained to navigate the mobile phone. Given task instruction, screen observation, and an action history sequence, output the next action and wait for the next observation. Here is the action space: 1. CLICK: Click on an element, value is not applicable and the position [x,y] is required. 2. TYPE: Type string into an element, value is string to type and the position is not applicable. 3. SCROLL UP: Scroll up for the screen. 4. SCROLL DOWN: Scroll down for the screen. 5. SCROLL LEFT: Scroll left for the screen. 6. SCROLL RIGHT: Scroll right for the screen. 7. PRESS BACK: Press for returning to the previous step, value and position are not applicable. 8. PRESS HOME: Press for returning to the home screen, value and position are not applicable. 9. PRESS ENTER: Press for submitting the input content, value and position are not applicable. 10. STATUS TASK COMPLETE: Indicate the task is completed, value and position are not applicable. Format the action as dictionary with the following keys: {action: ACTION TYPE, value: element, position: [x,y]} If value or position is not applicable, set it as None. Position represents the relative coordinates on the screenshot and should be scaled to range of 0-1. Think Prompt Please first thinks about the reasoning process in the mind and then provides the user with the action. The reasoning process and answer are enclosed within <think> </think> and <action> </action> tags, respectively, i.e., <think> reasoning process here </think><action> action here </action> Overall Prompt {AITZ System Prompt} {Think Prompt} Task: {Task} <image> Action history: {Action history} **Next action:** 24 G.10 Reward for length We give the reward rl = 1 for the model if it outputs <think>nn<think>, and otherwise 0. We find that the length reward remains 0 at the first 100 steps, the whole training process visualization is as follows: Figure 18: Length penalty rewards and completion length on AITZ datasets on Qwen2.5-VL-3B."
        }
    ],
    "affiliations": [
        "Show Lab, National University of Singapore",
        "The Chinese University of Hong Kong"
    ]
}