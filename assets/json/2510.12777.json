{
    "paper_title": "What If : Understanding Motion Through Sparse Interactions",
    "authors": [
        "Stefan Andreas Baumann",
        "Nick Stracke",
        "Timy Phan",
        "Bj√∂rn Ommer"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding the dynamics of a physical scene involves reasoning about the diverse ways it can potentially change, especially as a result of local interactions. We present the Flow Poke Transformer (FPT), a novel framework for directly predicting the distribution of local motion, conditioned on sparse interactions termed \"pokes\". Unlike traditional methods that typically only enable dense sampling of a single realization of scene dynamics, FPT provides an interpretable directly accessible representation of multi-modal scene motion, its dependency on physical interactions and the inherent uncertainties of scene dynamics. We also evaluate our model on several downstream tasks to enable comparisons with prior methods and highlight the flexibility of our approach. On dense face motion generation, our generic pre-trained model surpasses specialized baselines. FPT can be fine-tuned in strongly out-of-distribution tasks such as synthetic datasets to enable significant improvements over in-domain methods in articulated object motion estimation. Additionally, predicting explicit motion distributions directly enables our method to achieve competitive performance on tasks like moving part segmentation from pokes which further demonstrates the versatility of our FPT. Code and models are publicly available at https://compvis.github.io/flow-poke-transformer."
        },
        {
            "title": "Start",
            "content": "What If : Understanding Motion Through Sparse Interactions Stefan Andreas Baumann* Nick Stracke* Timy Phan* Bjorn Ommer CompVis @ LMU Munich Munich Center for Machine Learning (MCML) 5 2 0 2 4 1 ] . [ 1 7 7 7 2 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Understanding the dynamics of physical scene involves reasoning about the diverse ways it can potentially change, especially as result of local interactions. We present the Flow Poke Transformer (FPT), novel framework for directly predicting the distribution of local motion, conditioned on sparse interactions termed pokes. Unlike traditional methods that typically only enable dense sampling of single realization of scene dynamics, FPT provides an interpretable directly accessible representation of multi-modal scene motion, its dependency on physical interactions and the inherent uncertainties of scene dynamics. We also evaluate our model on several downstream tasks to enable comparisons with prior methods and highlight the flexibility of our approach. On dense face motion generation, our generic pre-trained model surpasses specialized baselines. FPT can be fine-tuned in strongly out-of-distribution tasks such as synthetic datasets to enable significant improvements over in-domain methods in articulated object motion estimation. Additionally, predicting explicit motion distributions directly enables our method to achieve competitive performance on tasks like moving part segmentation from pokes which further demonstrates the versatility of our FPT. Code and models are publicly available at compvis.github.io/flow-poke-transformer. 1. Introduction key feat of human visual intelligence is motion understanding, our ability to understand and predict the various ways the world around us could potentially change at given point in time (see Fig. 1). Our cortex is not creating mental video, focusing on how the colors of individual pixels change. Rather, we are constantly making predictions about the various ways individual objects or parts thereof could potentially move and deform [30]. We do not perceive the future as an unambiguously deterministic sequence of events but as vast space of possibilities. *Equal Contribution. Figure 1. What If: Our Flow Poke Transformer directly models the uncertainty of the world by predicting distributions of how objects () may move conditioned on some input movements (pokes, ). We see that whether the hand (below paw) or the paw (above hand) moves downwards directly influences the others movement. Left: the paw pushing the hand down, will force the hand downwards, resulting in unimodal distribution. Right: the hand moving down results in two modes, the paw following along or staying put. It is natural to focus selectively on parts of scene, infer how they might evolve, and reason about the underlying physical properties and interactions that drive change. This selective, probabilistic, and multimodal reasoning is rooted in the perceived inherent stochastic nature of the world. It is governed by the stochastic physical properties of complex systems and further compounded by the presence of agents with complex, inaccessible internal state, lead by free will or other, from the outside often unapproachable causes. This inherent uncertainty makes dense, deterministic predictions of future motion both impractical and ill-posed to represent real-world dynamics. The prediction of pixel1 perfect and long-term sequences [28] requires models to commit to one trajectory and, in so doing, ignore the rich multimodality of real-world outcomes. At best, such prediction biases the scenario towards single plausible future; at worst, it produces photorealistic frames that show limited understanding of physical processes, interactions, and constraints. In many situations, like autonomous agent systems, robotics, and automated planning, the ability to predict and process multiple possible outcomes of given situation is more valuable than the naive assumption that events play out according to single trajectory. To address these issues, we propose framework for representing the distribution of possible motions of parts of scene. To control the degree of uncertainty, human observer can interact with or perturb scene with local pokes, by nudging an object or applying force. By repeating similar interactions, the multimodal nature of potential outcomes can be observed. Similarly, we allow conditioning the motion distribution on such sparse, localized pokes. Compared to traditional dense approaches, our method operates at higher level of abstraction, predicting localized distributions of motion rather than committing to single outcome. This approach aligns more closely with real-world dynamics where uncertainty and multimodality are intrinsic and actionable insights often emerge from reasoning about sparse, local changes rather than exhaustive dense predictions. These include aspects like the inherent interpretability of explicitly predicted distributions, such as identifying modes and quantifying uncertainty directly. By avoiding dense (video) prediction, our model reframes motion prediction as problem of capturing potential dynamics, directly predicting motion distributions. For instance, poke applied to an unstable stack of blocks might cause it to topple in multiple ways, remain stable, or shift slightly without collapsing. We capture this variability, avoiding the pitfalls of dense video models that have to commit to one specific sample in the set of potential outcomes. This also addresses the impracticality of dense or long-term predictions, where compounding uncertainty renders dense outputs increasingly arbitrary. Applications of our proposed model include (sparse) interactive simulation, where pokes guide scene exploration and the multimodal distributions of possible motions are directly captured, moving part segmentation, but also classic sampling of dense motion predictions. As opposed to optical flow estimation and tracking, where the future is given via future frame, from which motion is estimated, we predict what future motion might be from only single frame. Overall, we present step toward more efficient, flexible, and detailed understanding of scene dynamics. We focus on the vast distribution of what could happenoptionally conditioned on sparse interactions instead of rendering specific futures. Our framework is not only efficient and scalable, but also conceptually aligned with the inherent uncertainties our human perception and reasoning is facing when dealing with our changing environment. Our main contributions are as follows: Multimodal Distribution Prediction: we directly predict full distributions of potential motion instead of just enabling sampling from them, providing increased flexibility in applications over previous approaches, such as directly estimating uncertainties. Sparse Kinematics Modeling: our method reasons about sparse, local motion distributions across the scene. This balances efficiency with expressive power by focusing computational resources where they matter most. Generalizability: our method can learn generic motion understanding from unstructured web videos, generalizing effectively to diverse, open-world data. Efficiency: our approach of sparsely modeling interactions enables sparse predictions with our method in 25ms and throughputs of more than 160k parallel predictions per second on single modern GPU which is promising for real-time applications. 2. Related Work Estimation of plausible motion for given image or scene has been approached in various ways over the years. What makes this task particularly challenging is that it requires the model to have physical understanding of how objects move in general, how they can be manipulated, and how they relate to each other. Many approaches directly predict video from still images which makes it harder to access and leverage the underlying motion understanding of the model. Other approaches first predict dense flow map which they use to later warp the images. However, more complex scenes can have multiple instantiations of realistic motion depending on the given conditioning, which we aim to model directly. In the following, we review various methods which have been studied in the literature. Motion-based Editing field that has recently gained attention is image editing using diffusion models by providing set of pokes that indicate how specific parts of the image should move. [31] takes GAN [13] generated image and warps it using motion supervision based on user-provided pokes. InstantDrag [36] on the other hand first predicts dense optical flow using GAN and uses that as conditioning for diffusion model to generate the final warped image. Similar approaches have been used in video editing [5, 22, 45] that extend base models with ControlNets [54] or LoRAs [15] to condition the model on the desired motion. The goal is then to move entire objects according to specific poke by selecting the object with bounding box or entity representation [46]. Unlike our method, this general 2 direction neglects an understanding of physical and realistic motion in exchange for precise adherence to the poke guidance and realistic inpainting of occluded regions. Motion Generation Various works learn to hallucinate motion for static images [2, 11, 24, 33, 35, 43]. MoVideo [24] and Motion-I2V [35] use diffusion models to predict dense flow sequences given start frame and use them to synthesize videos. Motion-I2V specifically allows conditioning on sparse movement information using motion drags, similar to pokes, which is an improvement upon DragNUWA [50] that directly synthesizes dense RGB video from drags. The latter makes the actual motion prediction substantially less accessible because it needs to be estimated with an additional model like RAFT [40] or COTR [17], property also shared by other methods [19, 20]. [43] predicts discrete bins of optical flow for static images with classification loss. This enables them to model multiple flow fields for single image. Im2Flow [11] predicts single realization of continuous optical flow for an image and combines that with the image to boost action classification performance. Learning how objects move and behave together can also be used as general pretext task to build physical scene understanding. [2] introduced the concept of pokes as sparse motion conditioning to indicate how the poked object should move and directly synthesize dense RGB videos on limiteddomain datasets. This is an unspecified problem as movement information is only available for small number of poked pixels and the model needs to learn how the remainder of the scene moves. DragAPart [20] and the follow-up work PuppetMaster [19] focus on modeling the movement of individual parts of objects for closed-domain, synthetic dataset (part-level motion). While these works focus on building more fine-grained physical understanding, they directly predict the result of the poke(s) in RGB space. This makes the underlying motion representation harder to access and requires e.g. optical flow estimation between frames. Additionally, they do not provide any uncertainty estimation in the form of an underlying motion distribution, but simply render single possible sample of the result space. Other approaches focus on directly predicting specific physical representation of motion. Generative Image Dynamics [23] learns oscillatory dynamics as commonly found in nature using Fourier-based motion representations. PhysDreamer [56] and PhysGaussian [49] extend the work of [23] from 2D to 3D scenes. While these approaches work well in their limited domains, they lack the flexibility to model the vast, often non-oscillatory motion space of the real world and are thus limited in the amount of general motion understanding they can obtain though training. Generative Models in Computer Vision Generative models have recently become cornerstone in computer vision, as they allow modeling tasks through full conditional distributions p(yx) instead of reducing predictions to singlepoint estimates, such as the expectation E[yx] often used in discriminative models. Major paradigms include GANs [13], diffusion models [14, 37], and autoregressive (AR) models [42]. GANs and diffusion models enable sampling from the modeled distribution but provide limited direct insight into its structure. Diffusion models, in particular, have demonstrated scalability to general data distributions and billions of parameters [32], whereas GANs are typically constrained to closed-set distributions. AR models, extensively applied in NLP [42] and increasingly adopted for vision tasks [3, 9, 51], directly model probability mass functions (PMFs) for discrete distributions and scale well to hundreds of billions of parameters [8]. However, the discrete nature of PMFs limits their applicability to real-valued problems, which are prevalent in vision tasks. Recent advances such as GIVT [41] and [21] have extended AR transformers to continuous-valued outputs while retaining the scalability of AR transformer models [10]. Specifically, [21] employ diffusion model to sample from autoregressively predicted feature vectors, while GIVT directly parameterizes distributions as Gaussian Mixture Models (GMMs) with diagonal covariances, enabling direct access to the probability density function (PDF) for downstream applications. Our implementation builds upon the latter, while extending it to non-diagonal covariances to enable accurate modeling of motion distributions. 3. Method 3.1. Problem Setting Given an image I, we aim to model the movement of all visible points in the image and their interdependencies. To this end, our goal is to model the conditional distribution p(f(q)P, I) of the movement f() R2 of arbitrary query points R2 in the image conditioned on set of Np pokes = {(pi, f(pi))}Np i=1, each specifying the movement f(pi) at locations pi R2. Explicit conditioning on movement information given at specific points is crucial to enable the exploration of interactions and controlling movement prediction in the scene. Here, the movement f() of point describes its change in position from the current time to future time + t, also referred to as forward flow. 3.2. Flow Poke Transformer To model the movement distribution pŒ∏(f(q)P, I), we use transformer-based architecture, denoted as pŒ∏. Transformers [42] are especially well-suited for this task, as they are well-capable of working with sparse sequences due to token interactions only being implemented via the attention mechanism. We show high-level overview in Fig. 2. We view each poke (pi, f(pi)) and each query point qj as 3 Figure 2. High-level Model Architecture Overview. Given an image I, set of given pokes (visualized as arrows ), and query positions (), our model directly predicts an explicit distribution of the movement at each query position. The flow poke transformer cross-attends to features from jointly trained image encoder to incorporate visual information. Crucially, our architecture represents movement at individual points (enabling sparse & off-grid motion processing) and directly predicts continuous, multimodal output distributions. individual tokens. Each pokes movement f(p) is encoded at the input using Fourier embedding, while query tokens are set to learned embedding. Positional encoding is implemented using relative positional embeddings [38], allowing positions to be set with arbitrary precision without needing to conform to any grid. This is important, as it enables training the model with high-quality but sparse and off-grid flow obtained via optical tracking. During self-attention, queries only attend to themselves and pokes, not to other queries. This enables evaluating the distribution pŒ∏(f(q)P, I) for multiple queries qj in parallel, which is crucial for efficient dense flow predictions. The image is encoded separately using vision transformer, resulting in set of spatial encoded image tokens E(I). The poke and query tokens then cross-attend to the image tokens, with spatial information again encoded using relative positional embeddings [38]. To obtain the movement distribution pŒ∏(f(q)P, I), projection head at the transformers output directly predicts Gaussian Mixture Model (GMM), enabling real-valued distributional predictions following GIVT [41]. The distribution being directly accessible in this manner enables range of additional capabilities, such as directly capturing multi-modal distributions in single forward pass or enabling the fine-grained quantification of uncertainty. Unlike [41], we parametrize each component using full covariance matrix Œ£(n) R22 instead of purely diagonal one, greatly increasing the predictions degrees of freedom. The positive semi-definiteness of the covariance matrix is ensured by the model predicting lower triangular matrix L(n) R22 with positive diagonal (by soft-clipping to lower threshold), from which the covariance matrix is computed as Œ£(n) = L(n)(L(n)). Overall, this results in the predicted -component GMM pŒ∏ = (cid:80)N n=1 œÄ(n) (¬µ(n), Œ£(n)), (1) with component mixture coefficients œÄ(n) and means ¬µ(n). specific challenge with learning motion understanding from open-world web videos is that camera movement can dominate the overall motion distribution of frame. Only training on videos with static cameras is not viable, as it would limit potential training data too much. We address this by replacing the typical normalization layers in the transformer with adaptive normalization layers [16], using which we condition the model on whether the camera is static, which we detect by whether significant fraction of the scenes content is static. This allows us to learn motion prediction on general videos. Training Objective We directly train our model to minimize the negative log-likelihood (NLL) of ground truth flow f(q) of the random query point q, conditioned on random set of flow pokes L(f(q), P, I; Œ∏) = log pŒ∏(f(q)P, I) n=1œÄ(n)N (f(q)¬µ(n) Œ∏ (P, I), Œ£(n) (cid:16)(cid:80)N = log (cid:17) Œ∏ (P, I)) . (2) Specifically, we compute the loss for image conditioned on random sets of pokes (i) of length (0) = 0, . . . , (Np) = Np. Predicting the flow distribution at Nq different random query positions per set of pokes. To enable efficient training, we introduce variation of teacher forcing [39], typically used to train autoregressive transformers [42]. We select the set of random pokes such that (0) (1) . . . (Np). We then use causal attention mask on the poke tokens and let the queries for each set of pokes individually attend to all the pokes in their respective set. We call the resulting attention pattern query-causal attention (see Fig. for visualizations). As opposed to training with independent sets of pokes and full self-attention for all sets of pokes and queries, this reduces the computational complexity from O(N 2 ) to O(N 2 +Np Nq) for the same number of trained predictions. 2 4 Since Np can be large during training, this substantially improves performance and enables efficient training. 3.3. Downstream Applications Our methods primary goal is to enable efficient and interpretable modeling of multimodal movement distributions of different parts in scenes. We achieve this by modeling the conditional distribution of the movement of query points in the image given the movement of any number of pokes to condition on. As our model directly makes probability density functions for each querys movement accessible and captures its multi-modality (c.f., Fig. 3), the distribution of potential movements can be directly interpreted. Besides modeling this relation of movement of different parts in scene, this also enables other direct and indirect downstream tasks and applications, which we describe in this section. Dense Motion Prediction The conditional distribution our method learns to model can also be used to predict dense grid of queries Q. This prediction can be done both purely conditioned on the reference image or given reference pokes P. This motion can be obtained by predicting the pointwise flow distributions in parallel or via autoregressive sampling. To sample from the joint distribution p(f(Q)P, I) that models the precise interactions between all points in the image, we employ autoregressive sampling. Iteratively, we predict the flow distribution for random query qi Q, sample flow instance from the conditional distribution f(qi) pŒ∏(f(qi)P, I), and add the query to the set of pokes {qi}. This results in individual coherent samples Fsample pŒ∏(f(Q)P, I) from the distribution of possible dense flows. We show qualitative examples of sampled dense flow in Fig. 6. For parallel sampling, we compute the mean dense flow Fmean = [f(Q)P, I; Œ∏] in pointwise manner for all queries qi in parallel. This provides efficient, high-quality flow predictions (see, e.g., appendix Fig. F), but also results in mode averaging. Segmenting Moving Parts Segmenting parts that move together is task introduced in [20] and is useful for various applications such as predicting affordances. Given poke (p, f(p)), the aim is to segment the image into parts that would move in response to it. Unlike [20], we do not need to rely on involved methods for extracting and comparing internal feature activations of our model for this task. Instead, our model enables direct quantification of the effect movement f(p) of point has on another point by measuring the relative entropy between the conditional and unconditional distribution, i.e., how much conditioning on changes the movement distribution of q. This is done using the Kullback-Leibler (KL) divergence DKL(pŒ∏(f(q)(p, f(p)), I) pŒ∏(f(q)I)). (3) Specifically, if the movements of and are independent, the conditional distribution p(f(q)(p, f(p)), I) is equal to the marginal distribution p(f(q)I)), and thus, the KL divergence in Eq. (3) is zero. Otherwise, it quantifies the change in movement distribution, and, thus, the motion interdependencies of different parts of the scene. We efficiently approximate the KL divergence using the matched bound approximation [12]. This can then be computed over all points in the image in parallel and directly quantifies the effect the given movement of has on each point q. 4. Experiments 4.1. Dataset and Implementation Details For general pretraining, we train on random 3.8M video clip subset of WebVid [1]. The wide variety of concepts present in WebVid enables our model to learn general representation for motion instead of being limited to specific domain, such as face-only videos. We train our model with flow from optical tracks using CoTracker3 [18] for random 48-frame interval from each clip using uniform 482 grid from the respective start frames. The image encoder and the poke transformer are ViTBase transformers [7] for total parameter count of 220M. We use RoPE [4, 38] both for self-attention between flow tokens and for cross-attention to image tokens. We initialize the image encoder with DINOv2-R [6, 29] to make training more efficient but keep the weights unlocked. Jointly training the full model is essential, as DINOv2 does not have good instance segmentation capabilities (see Sec. for additional details), which are essential for our task. We pass images to the vision encoder at resolution of 4482 to obtain 322 grid of visual embedding tokens. The model is trained in bfloat16 precision for 800k steps using AdamW [26] with learning rate of 5e-5, at global batch size of 32 images, which is increased to 128 after 250k steps. Per image, we sample sets of random pokes of sizes 0, 1, . . . , 128, and compute losses on Nq = 15 random query points per set of pokes. This results in global batch size of 61,440 (245,760) queries. Overall, training this model took 7 days on 2 Nvidia H200s. We also train second model on dataset of 5M open-set video clips we collected, with optical tracks obtained using TAPNext [57]. Here, we simplify the training setup to use batch size of 128 across the whole training, and add cosine decay [25] for the learning rate after an initial warmup. further optimized training setup allows us to train this model to 1M steps in 24h on 8 Nvidia H200s. We provide additional details and ablations in appendix Secs. and B, and explore the potential for extension to 3D motion in Sec. C. Without inference optimizations such as quantization or 8-bit inference, single conditional movement distribution prediction for query in an image can be obtained in less than 25ms of delay on single H200. This makes our model 5 motion generations in Fig. 6. They successfully show diverse but realistic global scene motion. Figure 4. Predicted Mode Analysis. Values are computed at resolution of 642. (a) Diversity of predicted modes is high, with mode variation covering large fraction of poke magnitude. (b) One mode typically has substantially higher confidence than others, which increases with given poke count. The mode closest to the ground truth consistently has higher-than-average confidence. (c) More confident modes are more accurate as measured by PCK. FPT predicts meaningful multimodal motion distributions. One important property that differentiates FPT from common motion modeling approaches is that it directly predicts the multimodality of possible future motions. For these multimodal predictions to be valuable, they should cover the diverse modes of possible motion and have meaningful predicted confidences œÄ(n). We analyze these properties in Fig. 4. Generally, we find the modes to be highly diverse (Fig. 4a, cf. Figs. 3 and E), with them covering substantially different movements. As expected, the multimodal predictions reduce to primarily unimodal predictions when enough conditioning information is available to reduce the stochastic uncertainty of the future and discern one clear correct mode (Fig. 4b). Importantly, the confidence of the mode closest to the ground truth motion is consistently substantially higher than the average. This indicates that the models confidence predictions are meaningful. Similarly, analyzing the modes accuracy (Fig. 4c) shows that the model assigns higher confidences to modes more likely to be correct. Still, secondary and tertiary predicted modes are also meaningful, as indicated by the accuracy of the mode closest to the ground truth exceeding that of the most confident one. FPTs predicted distributions accurately model uncertainty. We evaluate the predictive quality of our models predicted uncertainty w.r.t. true prediction error in Fig. 5. Specifically, we investigate the relation between the predicted distributions standard deviation Std[f(q)P, I; Œ∏] and the motion estimation error as measured with the endpoint error (EPE). We find that the predicted motions error strongly correlates to the predicted uncertainty. This capability is independent of the approach to derive the single motion prediction from the predicted distribution. Sampling from the predicted distribution (Pearson œÅ = 0.66), using its mean (œÅ = 0.64), or using the most confident mode (œÅ = 0.62) all lead to high predictive accuracy of the true prediction error compared to the ground truth. Figure 3. Multimodal Motion Distribution Prediction. We condition on one or multiple pokes () and then query the motion distribution of specific points (). Our models predictions capture the multi-modal nature of motion and exhibit understanding of interactions, such as only lifting the cup by its handle not necessarily causing the whole cup to move upwards, while grabbing it at stable points does. It also demonstrates prior understanding from scenes, such as car in an intersection being more likely to move forwards than backward and cars in traffic likely moving together. applicable for real-time applications. Throughput (with parallel predictions) is about 160k predictions per second per image, thanks to our query-causal attention implementation. We generally evaluate predicted motion at resolution of 642 unless specified otherwise. We primarily rely on endpoint error EPE = ÀÜf(q) fGT(q)2, which measures the difference between the true motion fGT(q) and the predicted motion ÀÜf(q). Additionally, we also compute the percentage of correct keypoints PCK = E[ÀÜf(q) fGT(q)2 < Œ±], with Œ± = 1px unless specified otherwise. 4.2. Evaluation of FPTs Key Abilities FPTs Ability to Predict Movement Distributions We observe our models predicted distributions pŒ∏(f(q)P, I) given an image of scene and conditioned on sparse set of pokes in Fig. 3 (see appendix Fig. for additional samples). Qualitatively, our model exhibits an understanding of physical phenomena and interactions, predicting realistic movement distributions for the given pokes in the context of the respective scenes. Most importantly, it captures the multimodality of potential movements in different circumstances and their variability/uncertainty. Our approach is trained in an open-world setting, being not limited to individual object categories, but, nevertheless, captures fine-grained details of specific objects potential motion. One can also draw samples from the joint distribution of movement of the whole scene pŒ∏(f(Q)P, I) by autoregressive sampling. We show examples of such unconditional 6 Method Trained On InstantDrag [36] Motion-I2V [35] Generic (WebVid-10M, Zero-Shot) Generic (WebVid-3.8M, Zero-Shot) Generic (Open Set-5M, Zero-Shot) Faces Ours 1 Poke 2 Pokes 5 Pokes 10 Pokes 100 Pokes EPE PCK LPIPS EPE PCK LPIPS EPE PCK LPIPS EPE PCK LPIPS EPE PCK LPIPS 9.24 0.193 0.18 29.08 0.029 0.35 7.64 0.150 0.16 7.99 0.150 0.17 9.12 0.196 0.17 27.40 0.031 0.34 6.87 0.154 0.15 7.02 0.154 0.16 8.82 0.197 0.17 24.22 0.030 0.32 5.32 0.167 0.13 5.50 0.158 0.14 8.39 0.198 0.16 20.90 0.048 0.30 4.20 0.183 0.12 4.17 0.182 0. 7.29 0.212 0.15 n/a n/a n/a 2.51 0.264 0.10 2.44 0.285 0.10 Table 1. Face Motion Generation Evaluation. We evaluate the accuracy of predicted motion on TalkingHead-1KH [44] given starting frame and one or more pokes (partially) defining the head movement. Our method performs substantially better in zero-shot comparison to Motion-I2V, which was also trained in generic setting. Compared to InstantDrag, which was trained for specifically this setting, our method achieves substantially better endpoint error (EPE) but slightly worse PCK for low poke counts, highlighting our models capability to perform competitively with purpose-trained methods while being generic. It can also make more efficient use of the available information, achieving greater accuracy gains from additional pokes compared to other methods. When using the predicted motions to warp the source image, our method consistently outperforms others. Ours InstantDrag [36] Input with Pokes Flow Warped Flow Warped Figure 5. Uncertainty Calibration. We find that the motion prediction error measured by EPE strongly correlates with the predicted uncertainty (Pearson œÅ = 0.64). This relationship holds for low & high numbers of given pokes. Input Image Unconditional Motion Samples Figure 6. Unconditional AR Motion Sampling. We show samples of generated flow without prior motion conditioning on pokes. Our model can generate wide variety of realistic motions. 4.3. Comparisons on Motion Prediction To enable quantitative comparisons of our models motion understanding with existing methods, we evaluate on predicting dense flow from sparse flow pokes, given starting frame. We compare against baseline methods in the setting they have been trained on to enable fair comparisons. To evaluate against DragAPart [20] and PuppetMaster [19], which only generate images/videos based on drags, we extract flow from the source to the generated image using RAFT [40]. Face Motion Estimation We compare against InstantDrag [36], which was trained on CelebV-Text [52] in their evaluation setting poke-conditioned motion prediction on aligned faces on TalkingHead-1KH [44]. The qualitative 7 Figure 7. Fine-grained Face Motion Control. We show finegrained zero-shot poking results on faces and compare against InstantDrag [36], which was trained for this task. We further visualize the predicted motion as warps using I-Ds face warping model. results (see Fig. 7) show that our model tends to predict more accurate and localized motion. This can also be observed when visualizing the motion by warping the image. For quantitative evaluations, we extract chunks of length 0.8s (following [36]) and use CoTracker3 [18] at grid size of 1282 to obtain the target motion from the start to the end frame. Then, we condition on {1, 2, 5, 10, 100} pokes P, where the first poke is chosen to be the one with the largest flow magnitude, and the others are sampled randomly, and compare the dense predicted motion to the target downsampled by factor of two. We also compare quantitatively with [35], which was trained generically. We compare favorably to both methods in EPE independent of the number of given pokes, indicating further that our predicted flow is more precise (Tab. 1). The PCK is slightly worse than the non-generically trained InstantDrag for low poke counts but catches up with more conditioning. When using the respective methods generated motion to perform image warping using InstantDrags warping stage, our method consistently outperforms both others, as measured by the LPIPS [55] distance to the ground truth images. Articulated Objects DragAPart [20] and PuppetMaster [19] were explicitly trained on part-level object motion on synthetic dataset, Drag-A-Move [20] (DAM). To enable comparisons with them, we evaluate on the test set of DAM, which contains synthetic images of furniture with Method Trained On (a) Motion Est. (b) Moving Part Segm. EPE PCK mIoU Input with Poke Moving Part Segmentation DragAPart Ours Input with Poke Moving Part Segmentation DragAPart Ours Motion-I2V [35] DragAPart [20] PuppetMaster [19] Ours Ours (fine-tuned) Generic (Zero-Shot) Objects (DAM) Objects (DAM + OAHQ) Generic (WV, Zero-Shot) Generic DAM 33.27 9.69 9.62 12.74 3. 0.043 0.514 0.472 0.191 0.834 0.073 0.273 0.112 0.287 0.572 taken from original publication, our evaluation yields 0.228. Table 2. Articulated Object Motion Estimation. We compare motion (flow) estimation (a) and moving part segmentation performance (b) on Drag-A-Move [20] (DAM). On zero-shot motion estimation, our model substantially outperforms the other zeroshot method M-I2V, while not being much worse than specifically trained methods. When adapted, our method significantly outperforms previous approaches. In moving part segmentation, even our generic model outperforms other, in-domain models. one or more pokes that define the movement of one or more articulated parts of that object. It also provides ground truth dense flow at resolution of 5122, which we compare against. As DAM is significantly dissimilar from our train set, we evaluate our model both in zero-shot and finetuned setting. For the fine-tuned setting, we fine-tune our model on DAM for 30k steps at batch size of 128 with an exponentially decaying learning rate of 5e-7 that halves every 10k steps. Our quantitative evaluation (see Tab. 2, for additional qualitative samples see appendix Fig. D) shows that, in zero-shot setting, our models predicted motion is substantially more accurate than Motion-I2V [35] but less accurate than the specifically trained DragAPart and PuppetMaster. This demonstrates that our model generalizes to out-of-distribution data comparatively well, but, expectedly, falls short of models explicitly trained for this OOD domain. Once fine-tuned, our model outperforms even the purposemade DragAPart and PuppetMaster by wide margin. We also show qualitative examples in Fig. D. These find MotionI2V to hallucinate substantial motion independently from the given pokes and struggle with matching the precise flow direction and magnitude for the given pokes. The extracted flow from DragAPart also seems to face struggles in complex multi-poke situations, which our model handles better. 4.4. Segmenting Moving Parts We perform moving part segmentation with our method by thresholding the KL divergence between the pointwise unconditional motion distribution and the pointwise motion distribution conditioned on specific poke (Eq. (3)). We show qualitative results in Fig. 8 and compare quantitatively against other methods, similarly thresholding the flow magnitude in Tab. 2b. Here, we find that our method, especially when finetuned in-domain, outperforms DragAPart [20], which introduced this benchmark, and the other methods by wide margin. large part of this gain can be attributed to our divergence-based score, which leverages FPTs unique property of directly predicting distributions. Without it, our method achieves an mIoU of 0.415 still (a) We directly replicate Fig. 7 from DragAPart [20] with our method. Our method provides spatially continuous predictions and makes fewer critical mistakes like segmenting the furniture body with the drawer (top right). Input Prediction Input Prediction Input Prediction (b) Open-set moving part dependency visualization. The degree to which the movement of each part is influenced by the poke () is visualized as heatmap, where brighter color means higher degree of influence. Figure 8. Segmenting Moving Parts. We show qualitative results for moving part segmentation, as introduced in [20], both on the Drag-A-Move dataset (a) and in generic, open-set setting (b). Input Prediction Input Prediction Figure 9. Common Failure Cases. Our model, generically pretrained on primarily realistic videos, does not generalize well to cartoons, causing parts of the background to be moved together with objects. Additionally, our model sometimes, but not consistently, jointly predicts the movement of shadows together with objects, which can be problematic for downstream use cases. outperforming the previous state-of-the-art by wide margin, but less so. Our method is also robust to threshold choice halving/doubling it leads to mIoUs of 0.54/0.52 respectively. 5. Conclusion We introduce the Flow Poke Transformer (FPT), novel framework for motion understanding that captures real-world dynamics multi-modal and stochastic nature through interpretable distributions of local motion, conditioned on targeted interactions (pokes). Contrary to previous motion prediction approaches, FPT directly models the probabilistic distribution of possible outcomes, providing insights into the effects of physical interactions and inherent uncertainties. Our evaluations demonstrate FPTs versatility across different domains and its generalization capabilities. Despite being designed and trained for sparse, general-purpose motion understanding, it also offers competitive performance in 8 established tasks such as dense motion generation on faces or articulated objects. Importantly, while valuable for comparison, these evaluations do not fully reflect our methods primary strength its ability to provide directly interpretable and useable predictions of motion distributions in interactive environments, bridging the gap between physical plausibility and efficiency. Furthermore, capabilities such as moving part segmentation directly emerge from our methods design. Overall, these results show our methods strength as versatile, interpretable, general-purpose motion model. We envision this work as foundation for more probabilistic, generalizable, and actionable approaches to motion understanding, paving the way for deeper insights into complex physical dynamics and future advancements in handling ambiguous and extreme out-of-distribution scenarios  (Fig. 9)  ."
        },
        {
            "title": "Acknowledgment",
            "content": "We would like to thank Mahdi M. Kalayeh for helpful discussions and feedback. We would further like to thank Jannik Wiese, Kim-Louis Barwig, Enrico Shippole, Ming Gui, Thomas Ressler-Antal, Olga Grebenkova, Paul Hofman, Owen Vincent, and the anonymous reviewers. This work was supported in part by research grant from Netflix. The authors thank Netflix for its support. This project was also supported by the Federal Ministry for Economic Affairs and Energy within the project NXT GEN AI METHODS - Generative Methoden fur Perzeption, Pradiktion und Planung, the project GeniusRobot (01IS24083) funded by the Federal Ministry of Research, Technology and Space (BMFTR), the bidt project KLIMA-MEMES, and the Horizon Europe project ELLIOT (GA No. 101214398). The authors gratefully acknowledge the Gauss Center for Supercomputing for providing compute through the NIC on JUWELS/JUPITER at JSC and the HPC resources supplied by the NHR @FAU Erlangen."
        },
        {
            "title": "References",
            "content": "[1] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-toend retrieval. In IEEE International Conference on Computer Vision, 2021. 5, 12 [2] Andreas Blattmann, Timo Milbich, Michael Dorkenwald, and Bjorn Ommer. ipoke: Poking still image for controlled stochastic video synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14707 14717, 2021. 3 [3] Chenjie Cao, Yuxin Hong, Xiang Li, Chengrong Wang, Chengming Xu, Yanwei Fu, and Xiangyang Xue. The image local autoregressive transformer. Advances in Neural Information Processing Systems, 34:1843318445, 2021. 3 [4] Katherine Crowson, Stefan Andreas Baumann, Alex Birch, Tanishq Mathew Abraham, Daniel Kaplan, and Enrico Shippole. Scalable high-resolution pixel-space image synthesis with hourglass diffusion transformers. In Proceedings of the 41st International Conference on Machine Learning, pages 95509575. PMLR, 2024. 5, 12 [5] Zuozhuo Dai, Zhenghao Zhang, Yao Yao, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Fine-grained open domain image animation with motion guidance. arXiv preprint arXiv:2311.12886, 2023. [6] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers, 2023. 5 [7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. 5 [8] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 3 [9] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. 2021 ieee. In CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3 [10] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. 3 [11] Ruohan Gao, Bo Xiong, and Kristen Grauman. Im2flow: Motion hallucination from static images for action recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 59375947, 2018. 3 [12] Goldberger and Greenspan. An efficient image similarity measure based on approximations of kl-divergence between two gaussian mixtures. In Proceedings Ninth IEEE International conference on computer vision, pages 487493. IEEE, 2003. [13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2014. 2, 3 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [15] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2 [16] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE international conference on computer vision, pages 15011510, 2017. 4, 12 [17] Wei Jiang, Eduard Trulls, Jan Hosang, Andrea Tagliasacchi, and Kwang Moo Yi. Cotr: Correspondence transformer for matching across images. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6207 6217, 2021. 3 [18] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. CoTracker3: Simpler and better point tracking by pseudolabelling real videos. 2024. 5, 7, 12 [19] Ruining Li, Chuanxia Zheng, Christian Rupprecht, and Andrea Vedaldi. Puppet-master: Scaling interactive video generation as motion prior for part-level dynamics. arXiv preprint arXiv:2408.04631, 2024. 3, 7, 8, 14 [20] Ruining Li, Chuanxia Zheng, Christian Rupprecht, and Andrea Vedaldi. Dragapart: Learning part-level motion prior for articulated objects. In Computer Vision ECCV 2024, pages 165183, Cham, 2025. Springer Nature Switzerland. 3, 5, 7, 8, 14 [21] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. NeurIPS 2024, 2024. 3 [22] Yaowei Li, Xintao Wang, Zhaoyang Zhang, Zhouxia Wang, Ziyang Yuan, Liangbin Xie, Yuexian Zou, and Ying Shan. Image conductor: Precision control for interactive video synthesis. arXiv preprint arXiv:2406.15339, 2024. [23] Zhengqi Li, Richard Tucker, Noah Snavely, and Aleksander Holynski. Generative image dynamics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2414224153, 2024. 3 [24] Jingyun Liang, Yuchen Fan, Kai Zhang, Radu Timofte, Luc Van Gool, and Rakesh Ranjan. Movideo: Motion-aware video generation with diffusion models. In European Conference on Computer Vision, pages 00000000, 2024. 3 [25] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. 5 [26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 5, 12 [27] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-tovideo generation. arXiv preprint arXiv:2407.02371, 2024. 14 [28] OpenAI. Sora, 2024. 2 [29] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 5, [30] Stephen E. Palmer. Vision Science: Photons to Phenomenology. The MIT Press, 1999. 1 [31] Xingang Pan, Ayush Tewari, Thomas Leimkuhler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. Drag your gan: Interactive point-based manipulation on the generative image manifold. In ACM SIGGRAPH 2023 Conference Proceedings, pages 111, 2023. 2 [32] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. 3 [33] Pol Rosello. Predicting future optical flow from static video frames. Retrieved on: Jul, 18:2, 2016. 3 [34] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. 12 [35] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. SIGGRAPH 2024, 2024. 3, 7, 8, 13, 14 [36] Joonghyuk Shin, Daehyeon Choi, and Jaesik Park. Instantdrag: Improving interactivity in drag-based image editing. arXiv preprint arXiv:2409.08857, 2024. 2, 7, 13 [37] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. [38] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: enhanced transformer with rotary position embedding. corr abs/2104.09864 (2021). arXiv preprint arXiv:2104.09864, 2021. 4, 5, 12 [39] Ilya Sutskever, Oriol Vinyals, and Quoc Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2014. 4 [40] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. 3, 7 [41] Michael Tschannen, Cian Eastwood, and Fabian Mentzer. In EuGivt: Generative infinite-vocabulary transformers. ropean Conference on Computer Vision, pages 292309. Springer, 2024. 3, 4, 13 [42] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 3, [43] Jacob Walker, Abhinav Gupta, and Martial Hebert. Dense optical flow prediction from static image. In Proceedings of the IEEE international conference on computer vision, pages 24432451, 2015. 3 [44] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot free-view neural talking-head synthesis for video conferencing. In CVPR, 2021. 7 [45] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 2 [46] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting 10 Gao, and Di Zhang. Draganything: Motion control for anything using entity representation. In European Conference on Computer Vision, pages 331348. Springer, 2025. [47] Yuxi Xiao, Jianyuan Wang, Nan Xue, Nikita Karaev, Iurii Makarov, Bingyi Kang, Xin Zhu, Hujun Bao, Yujun Shen, and Xiaowei Zhou. Spatialtrackerv2: 3d point tracking made easy. In ICCV, 2025. 13 [48] Yuxi Xiao et al. Spatialtracker: Tracking any 2d pixels in 3d space. In CVPR, 2024. 13 [49] Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, and Chenfanfu Jiang. Physgaussian: PhysicsIn Prointegrated 3d gaussians for generative dynamics. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 43894398, 2024. 3 [50] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. 3 [51] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. Transactions on Machine Learning Research, 2022. Featured Certification. 3 [52] Jianhui Yu, Hao Zhu, Liming Jiang, Chen Change Loy, Weidong Cai, and Wayne Wu. Celebv-text: large-scale facial text-video dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1480514814, 2023. [53] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. 12 [54] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 2 [55] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 7 [56] Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, and William Freeman. Physdreamer: Physics-based interaction with 3d objects via video generation. In European Conference on Computer Vision, pages 388406. Springer, 2025. 3 [57] Artem Zholus, Carl Doersch, Yi Yang, Skanda Koppula, Viorica Patraucean, Xu Owen He, Ignacio Rocco, Mehdi SM Sajjadi, Sarath Chandar, and Ross Goroshin. Tapnext: Tracking any point (tap) as next token prediction. arXiv preprint arXiv:2504.05579, 2025. 5, 12 A. Implementation Details The hyperparameters for the base model used for all evaluation and qualitative examples are reported in Tab. B. We train the WebVid model for total of 800k steps with learning rate of 5.0 105 using the AdamW [26] optimizer and linear warmup of 5000 steps. The first 250k steps are trained with batch size of 32; for the remainder, we set the batch size to 128. For our second model, keep the batch size constant and significantly increase warmup time. We also add cosine LR decay and increase training time to 1M steps. Training time differences are due to an improved trainer setup from the first to the second model. As described in Sec. 4.1, we randomly sample flow pokes and their corresponding positions from the given trajectories. We enforce that all flow values are in [1, 1] by applying tanh mapping and obtain sinusoidal embedding for the and components of the flow. We then find the corresponding image features using the pokes positions and concatenate them with the flow features and local image features extracted by the image feature extractor E(I). Finally, we project them using mapping network. These embeddings are then combined with query tokens. The query tokens represent the locations in the image for which we want to predict flow distribution and are realized by learnable token and the corresponding positional encoding. The flow pokes and query tokens are fed to the transformer, which is 12 blocks deep and has width of 768. The self-attention uses our query-causal attention mask as introduced in Sec. 3.2. We visualize it in Fig. A. In the cross-attention, the pokes and queries attend to the image features, enabling them to learn global understanding of the scene. In both attention mechanisms, we use 2D Axial RoPE [4, 38] to model the spatial relationships of tokens. The FNNs expand the internal feature dimension by factor of three, use SwiGLU [34] as an activation function, and are conditioned on whether or not the camera is static using AdaRMSNorms. On the output, we observe that directly using the output of the transformer for GMM parameter prediction sometimes produces unreasonable distributions and thus bad performance. Therefore, we use simple MLP to project the transformers output, alleviating that problem. For further details, we refer to our reference implementation1, which contains extensive further comments in context. Whether the camera is static or not is detected using simple heuristic: we consider it to be static if significant fraction of the scenes content is static. This information can be directly derived from the training tracks. Specifically, we find that considering camera static once 40% of the frame move by at most 3px (at our training resolution of 4482) works well on our training data. For the second training, we use 1% of the frame side length as the threshold instead. 1https://github.com/CompVis/flow-poke-transformer Figure A. Query-Causal Attention Pattern Visualization. We show the resulting attention patterns for our query-causal attention for different numbers of queries per poke count. We put poke tokens first, followed by query tokens. (a) In the simplest setting, with one query per set of pokes, there is one query token per set of pokes, with each query token attending to one more poke token than its predecessor. (b) For Nq > 1, the poke attention does not change, but there are multiple query tokens per poke set. Here, even query tokens for the same poke set do not attend to each other to enable parallel evaluation during inference. Parameter Dataset Number of clips Batch size Optimizer Peak learning rate Learning rate schedule Betas Warm-up steps Total Steps Precision Total Parameters GPUs Training Time Tracker Tracker position seeding Flow scale Image size Mixtures Covariance Given pokes Query factor Value WebVid [1] Subset 3.8M Open-Set Videos 5M 32128 AdamW [26] 5.0 105 constant (0.9, 0.99) 5k 800k bfloat16 230M 2 Nvidia H200 7 days CoTracker3 [18] 48 48 grid [1, 1] 448 448 4 Full 128 128 AdamW [26] 5.0 105 cosine decay to 108 (0.9, 0.99) 100k 1M bfloat16 230M 8 Nvidia H200 1 day TAPNext [57] 1024 random [1, 1] 448 448 4 Full 128 15 Depth SA width CA width Normalization FFN expand factor Activation Positional encoding Static scene conditioning Adaptive Norm [16] 12 768 768 RMSNorm [53] 3 SwiGLU [34] 2D Axial RoPE [4, 38] 12 768 768 RMSNorm [53] 3 SwiGLU [34] 2D Axial RoPE [4, 38] Adaptive Norm [16] Table B. Hyperparameters for our main model across both datasets. Ablation models use the same parameters as the first one, but only train for 250k steps."
        },
        {
            "title": "Method",
            "content": "InstantDrag [36] Motion-I2V [35] Ours (full training)"
        },
        {
            "title": "Trained On",
            "content": "EPE@1 EPE@2 EPE@5 EPE@10 EPE@100 Faces Generic (Zero-Shot) Generic (Zero-Shot) 9.24 29.08 7.64 9.12 27.40 6.87 8.82 24.22 5.32 8.39 20.90 4. Vision Feature Extractor Initialization Jointly Trained Pre-Trained (Ours) Frozen Pre-Trained Trained from Scratch GMM Component Count 1 Component 2 Components 4 Components (Ours) 8 Components 16 Components Generic (Zero-Shot) Generic (Zero-Shot) Generic (Zero-Shot) Generic (Zero-Shot) Generic (Zero-Shot) Generic (Zero-Shot) Generic (Zero-Shot) Generic (Zero-Shot) GMM Covariance Parametrization Full Covariance, 4 Components (Ours) Generic (Zero-Shot) Generic (Zero-Shot) Diagonal, 4 Components 8.08 8.30 8. 7.60 8.98 8.08 8.23 8.41 8.08 8.13 6.96 7.22 7.51 6.87 7.87 6.96 7.19 7.26 6.96 7.09 5.38 5.44 6. 5.42 5.83 5.38 5.57 5.44 5.38 5.40 3.99 3.78 4.73 4.18 4.08 3.99 4.01 3.90 3.99 3.98 7.29 n/a 2. 2.33 2.22 2.57 2.59 2.34 2.33 2.29 2.34 2.33 2.24 Table A. Extension of Tab. 1 including our ablations. The experiment is identical to the original one. The ablation models have been trained for 250k steps compared to 800k for the full training. EPE@N refers to the endpoint error given pokes. B. Ablations Input Prediction (Frozen Vis. Enc.) Prediction (Jointly Trained) We show an extended version of Tab. 1 that includes quantitative results for hyperparameter ablations, which were used to motivate the choices mentioned in the main paper, in Tab. A. All comparison models follow the original training recipe but are only trained until 250k steps due to compute constraints. First, we ablate whether to initialize the vision encoder with pre-trained weights and whether to continue training them. We find that initializing with pre-trained weights gives performance boost in this setting. We hypothesize that, for very long trainings, both versions might end up performing similarly well. As noted in Sec. 4.1, freezing the feature extractor when initializing with DINOv2 [29] empirically results in model with reduced instance segmentation capabilities compared to the unlocked version. We show qualitative example of this in Fig. B. This behavior can also be observed in the quantitative evaluations, where, for low poke counts, the jointly trained model performs better than the one with frozen feature extractor. At high poke counts, instance segmentation capabilities likely become less relevant, as the movement of all instances is likely already given explicitly via the conditioning. When ablating the number of GMM components, we find that adding too many components reduces the models performance when conditioned on low numbers of given pokes. Only predicting single component results in better quantitative performance at low given poke counts, but, obviously prevents the model from predicting multimodal distributions (see, e.g., Figs. 1, 3 and E), omitting central property of our model. Parametrizing the covariance matrices as pure diagonal matrix as done in GIVT [41] results in slightly reduced Figure B. Jointly training the vision encoder is important. We train model with frozen pretrained vision encoder. The model struggles with instance-specificity, predicting the same movement for the womans hand as it does for the mans hand. When jointly training the vision encoder with the flow poke transformer, the mans hands movement does not directly influence the womans hand. performance on average. Qualitatively, it also prevents angled distributions that our model successfully uses to express directional uncertainty (see, e.g., Fig. 1). C. Extension to 3D Motion In this paper, we evaluated the Flow Poke Transformer in the two-dimensional setting, meaning that the model only reasons in the image plane. However, the architecture itself is not limited to this setting and can trivially be extended to higher dimensions if desired. We show qualitative motion prediction results from such version in Fig. C, where the model also successfully predicts reasonable out-of-plane motion in full 3D. This model was obtained by continued training from 2D FPT checkpoint with 3D trackers obtained using SpatialTrackerV2 [47, 48] on subset of Open13 Input Prediction Fxy Prediction Fz Figure C. 3D Motion Estimation. We show unconditional 3D motion estimation samples from an FPT variant fine-tuned on 3D track data. The in-plane motion prediction Fxy resembles that of 2D FPT model, while this version can also successfully predict plausible out-of-plane motion Fz. Vid1M [27]. The model is capable of predicting movement towards and away from the camera without any pokes, as shown by the near-static background and the clearly segmented motion of the animals in all three dimensions. When combining the predicted flow in Z-direction with depth estimation of the input image, it is possible to tell which parts in the image will be occluded in the future. D. Additional Qualitative Examples For additional context for our quantitative results in Tab. 2, we show visualizations of some samples from that experiment in Fig. D. We also show additional qualitative examples for and pointwise motion predictions in Fig. and samples for dense motion estimation in Fig. F. Input Motion-I2V DragAPart PuppetMaster Ours Ground Truth Figure D. Qualitative Results for Articulated Object Motion Estimation. We compare on Drag-A-Move [20] with MotionI2V [35], DragAPart [20], and PuppetMaster [19]. Our model is qualitatively more capable of capturing complex conditioning with multiple different pokes than DAP and PM in this setup. MotionI2V often fails to accurately follow the conditioning locally. 14 Input Image Overlayed Distribution Zoom Distribution Input Image Overlayed Distribution Zoom Distribution (a) When the head of the giraffe is moving down, we get different flow distributions depending on how close the query is to the head. Since the head can also move down without the neck following, we get distributions with more emphasis on no movement when the query is further away from the head (first example). When the query gets really close to the head (second example), the likelihood of movement at the query also increases which can be seen in the stronger bottom mode. (b) The model accounts both for the possibility of the tower falling over with the bricks movement and with it staying stationary. The likelihood of the tower falling over depends on the velocity with which the brick is removed. (c) Depending on which hand moves, the cup is predicted to be either stationary or potentially moving together with the hand holding it. Note that the case of the cup not moving with the hand holding it is very improbable, as visualized by the arrow pointing to that mode having substantially less opacity. (d) Depending on the height of the position queried on the tree, the magnitude of the predicted movement changes, reflecting typical intuition as to how tree moves. (e) The model is capable of understanding the effect of rotational movements. Figure E. Visualization of flow distribution for different pokes on the same image. The overlayed distribution visualizes the potential movement in the overall images, with the opacity of arrows denoting how likely each mode is (the more likely mode, the less transparent the arrow). Input Image Flow Prediction Input Image Flow Prediction Input Image Flow Prediction Figure F. Qualitative samples visualizing motion predictions inferred from single image and (optionally) pokes."
        }
    ],
    "affiliations": [
        "CompVis @ LMU Munich",
        "Munich Center for Machine Learning (MCML)"
    ]
}