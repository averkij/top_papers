{
    "paper_title": "MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse",
    "authors": [
        "Zhenyu Pan",
        "Han Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present MetaSpatial, the first reinforcement learning (RL)-based framework designed to enhance 3D spatial reasoning in vision-language models (VLMs), enabling real-time 3D scene generation without the need for hard-coded optimizations. MetaSpatial addresses two core challenges: (i) the lack of internalized 3D spatial reasoning in VLMs, which limits their ability to generate realistic layouts, and (ii) the inefficiency of traditional supervised fine-tuning (SFT) for layout generation tasks, as perfect ground truth annotations are unavailable. Our key innovation is a multi-turn RL-based optimization mechanism that integrates physics-aware constraints and rendered image evaluations, ensuring generated 3D layouts are coherent, physically plausible, and aesthetically consistent. Methodologically, MetaSpatial introduces an adaptive, iterative reasoning process, where the VLM refines spatial arrangements over multiple turns by analyzing rendered outputs, improving scene coherence progressively. Empirical evaluations demonstrate that MetaSpatial significantly enhances the spatial consistency and formatting stability of various scale models. Post-training, object placements are more realistic, aligned, and functionally coherent, validating the effectiveness of RL for 3D spatial reasoning in metaverse, AR/VR, digital twins, and game development applications. Our code, data, and training pipeline are publicly available at https://github.com/PzySeere/MetaSpatial."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 0 7 4 8 1 . 3 0 5 2 : r MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse Zhenyu Pan Department of Computer Science Northwestern University zhenyupan@u.northwestern.edu Han Liu Department of Computer Science Northwestern University hanliu@northwestern.edu"
        },
        {
            "title": "Abstract",
            "content": "We present MetaSpatial, the first reinforcement learning (RL)-based framework designed to enhance 3D spatial reasoning in vision-language models (VLMs), enabling real-time 3D scene generation without the need for hard-coded optimizations. MetaSpatial addresses two core challenges: (i) the lack of internalized 3D spatial reasoning in VLMs, which limits their ability to generate realistic layouts, and (ii) the inefficiency of traditional supervised fine-tuning (SFT) for layout generation tasks, as perfect ground truth annotations are unavailable. Our key innovation is multi-turn RL-based optimization mechanism that integrates physics-aware constraints and rendered image evaluations, ensuring generated 3D layouts are coherent, physically plausible, and aesthetically consistent. Methodologically, MetaSpatial introduces an adaptive, iterative reasoning process, where the VLM refines spatial arrangements over multiple turns by analyzing rendered outputs, improving scene coherence progressively. Empirical evaluations demonstrate that MetaSpatial significantly enhances the spatial consistency and formatting stability of various scale models. Post-training, object placements are more realistic, aligned, and functionally coherent, validating the effectiveness of RL for 3D spatial reasoning in metaverse, AR/VR, digital twins, and game development applications. Our code, data, and training pipeline are publicly available at https://github.com/PzySeere/MetaSpatial."
        },
        {
            "title": "Introduction",
            "content": "This work introduces MetaSpatial, the first reinforcement learning (RL)-based framework designed to enhance the 3D spatial reasoning capabilities of vision-language models (VLMs) for 3D scene generation. It primarily addresses two key challenges in existing methods: (1) the lack of internalized 3D spatial reasoning in VLMs, which limits their ability to generate realistic and coherent scene layouts, and (2) the inherent limitations of supervised fine-tuning (SFT) for this task, as there is no single \"perfect\" layout. Since spatial arrangements can vary widely based on context and intent, SFT struggles to capture the full distribution of plausible layouts, restricting the models adaptability and generalization. In contrast, MetaSpatial leverages RL to overcome these limitations, enabling models to learn from rewards rather than fixed annotations, refining spatial structures dynamically. Existing approaches often struggle with physical plausibility, coherence, and structural consistency. To address these challenges, some methods adopt multi-agent/round refinement, where large language models (LLMs) iteratively refine layouts through reasoning and search [6, 22]. However, they are time-consuming and prone to deadlocks, where iterations fail to converge. Other approaches leverage VLMs by providing asset and room images, utilizing their multimodal reasoning capabilities to improve spatial arrangement. Yet, they still suffer from inconsistencies, often requiring extensive post-processing such as differentiable optimization in LayoutVLM [21]. While SFT is proposed to reduce post-processing overhead [21], its applicability is limited in spatial reasoning tasks. Scene Preprint. Under review. Figure 1: Overview of MetaSpatial framework. Given room images, user preferences, and object status, the model generates JSON-formatted object layout with precise (x, y, z) coordinates and reasoning process. It evaluates the layout using three reward signals: Format Detection, Physical Detection, and Rendering-based Evaluation. The RL updates are based on multiple multi-turn refinement trajectories, optimizing grouped policy via Group Relative Policy Optimization (GRPO) to enable the VLM to learn deeper spatial reasoning from diverse refinement outcomes. generation lacks definitive ground truththere is no single \"correct\" layout but rather diverse distribution of valid spatial configurations. Since SFT relies on explicit annotations, it struggles to teach models generalizable and adaptable reasoning abilities. In contrast, RL is inherently well-suited for this task, as it enables models to learn from evaluative feedback rather than static labels, optimizing for spatial plausibility based on physics constraints and high-level layout principles. By replacing rigid supervision with adaptive learning, RL allows models to develop more flexible and efficient spatial reasoning capabilities without post-processing, ensuring coherent and realistic 3D scenes. To that end, we propose MetaSpatial, an RL-based framework that enables VLMs to develop adaptive and generalizable spatial reasoning for 3D scene generation. Unlike existing methods that rely on iterative refinement, post-processing, or SFT, MetaSpatial directly optimizes spatial structures through reinforcement learning, learning from implicit feedback rather than rigid annotations. This allows for more coherent, efficient, and realistic scene synthesis without the need for extensive corrections. Our experiments demonstrate that MetaSpatial improves spatial plausibility and layout quality, establishing RL as promising approach for advancing multimodal 3D spatial reasoning and enabling scalable, real-time 3D scene generation. In detail, as shown in Figure 1, our framework follows structured pipeline to enhance VLM-based 3D scene generation through reinforcement learning. Given an input room image, user preferences, and object status, the VLM rollout first generates reasoning trace alongside JSON-formatted layout, specifying object positions with precise (x, y, z) coordinates. This predicted layout is then evaluated through three key validation mechanisms to provide meaningful reward signals for RL optimization. (1) Format Detection ensures that the generated object layout is structurally sound by verifying whether the number of predicted objects matches the expected count, whether object IDs are consistent, and whether all objects contain valid spatial attributes. (2) Physical Detection converts the JSON layout into scene graph representation, enabling the system to assess spatial constraints, object collisions, and violations of physical rules, penalizing unrealistic layouts accordingly. (3) Rendering-based Evaluation takes the predicted object coordinates to construct 3D-rendered scene, which is then scored by GPT-4o based on multiple criteria, including physical plausibility, aesthetic coherence, and alignment with user preferences. Crucially, our framework employs multi-turn refinement, where each iteration refines the layout based on previous evaluations, forming trajectory of improvements. Rather than updating RL based on single-step refinements, we adopt Group Relative Policy Optimization (GRPO) to optimize the model using group of refinement trajectories, ensuring deeper understanding of spatial relationships and scene composition. This structured approach 2 allows the model to iteratively learn generalizable and adaptable spatial reasoning, enabling highquality and physically consistent 3D scene generation without relying on excessive post-processing. In summary, our work makes the following key contributions: We introduce MetaSpatial, the first RL-based framework for enhancing 3D spatial reasoning in VLMs, enabling coherent 3D scene generation without extensive post-processing. We propose multi-turn refinement strategy optimized using GRPO, where the model improves scene layouts through structured reasoning and spatial feedback, leveraging multiple refinement trajectories to develop more generalizable and adaptable spatial understanding. We design three-level evaluation mechanism, incorporating format detection, physical detection, and rendering-based assessment, providing adaptive reward signals for RL. Our extensive experiments demonstrate the effectiveness of our approach and show the improvements in spatial coherence, physical plausibility, and scene quality."
        },
        {
            "title": "2.1 Problem Formulation",
            "content": "We consider the task of 3D scene layout generation, where the goal is to place set of given objects within specified room according to spatial constraints and user preferences. Formally, given an input consisting of room image I, list of object candidates = {o1, o2, . . . , on} (each annotated with category, size, and material), and optional user instructions , the model is required to generate 3D layout where each object is assigned precise position (x, y, z) in the room. This task is inherently ill-posed: for the same input, multiple valid layouts can satisfy both physical and semantic constraints. As such, traditional supervised fine-tuning (SFT) that relies on fixed ground-truth annotations is inadequateit fails to capture the diversity and adaptability required for realistic spatial reasoning. Instead, we model layout generation as policy learning problem, where vision-language model πθ is optimized to generate semantically meaningful and physically consistent layouts through interaction with spatial feedback environment. 2.2 Overview of the MetaSpatial Framework As illustrated in Figure 1, MetaSpatial is reinforcement learning-based framework that enhances 3D spatial reasoning in vision-language models. The core idea is to enable VLMs to generate layouts not only through initial inference, but also by learning from multi-turn refinement trajectories guided by structured reward signals. The framework takes as input room image, user preferences, and object metadata, and produces both reasoning trace and JSON-formatted layout prediction containing precise object coordinates (x, y, z) . The layout is then evaluated using three types of reward signals: (1) Format Detection, which checks structural validity of the output layout; (2) Physical Detection, which detects spatial violations such as collisions and out-of-bound placements; and (3) Rendering-based Reward, which scores the realism and user alignment of the rendered scene using language model evaluator. Unlike standard RL frameworks that learn from one-step feedback, MetaSpatial performs multi-turn refinement, producing trajectory of layout updates per sample. These refinement trajectories are then grouped and optimized via Group Relative Policy Optimization (GRPO), allowing the model to learn adaptive and generalizable spatial reasoning strategies without relying on rigid annotations. 2.3 Layout Generation with Vision-Language Models At the core of our framework is VLM πθ responsible for generating structured 3D layout given the input context. Specifically, the model receives visual prompt composed of: (1) rendered room image that provides global spatial context; (2) list of object specifications describing the target objects names, categories, sizes, and styles; and (3) optional user preferences in natural language (e.g., \"place the dining table in the center of the room\"). The VLM processes this multimodal input and outputs two components: (1) reasoning trace that reflects the models spatial thinking process in natural language, outlining the logic behind each 3 object placement; and (2) layout JSON that encodes the final predicted positions for each object: = {(oi, xi, yi, zi)}n i=1. This layout is used as the candidate solution to be evaluated and refined by our reinforcement learning pipeline. We implement the layout generation using Qwen-VL models, which are capable of multimodal text generation conditioned on both images and textual prompts. 2.4 Multi-Turn Refinement Trajectory Generation Unlike standard single-step layout generation, MetaSpatial introduces multi-turn refinement strategy to enable iterative improvement of scene quality. At each refinement step t, the model observes the current scene state Lt and generates an updated layout Lt+1 by re-evaluating object placements based on prior reasoning and feedback. This process forms trajectory τ = {L0, L1, ..., LT }, where L0 is the initial layout and subsequent layouts aim to incrementally improve spatial plausibility and user alignment. Crucially, each step involves re-generating both reasoning trace and layout proposal conditioned on the previous layout and its corresponding reward signals. These trajectories serve two purposes: (1) They expose the model to diverse layout revisions, promoting structural adaptability and robust spatial understanding; and (2) They allow reward comparison not just across samples, but within refinement paths, which is essential for GRPO strategy introduced later. This refinement-based approach aligns with how humans iteratively adjust object arrangements in space, and provides the necessary supervision signal for layout optimization in the absence of absolute ground truth. 2.5 Reward Design To guide reinforcement learning without relying on explicit annotations, MetaSpatial adopts hybrid reward scheme consisting of three complementary components. Each component captures distinct aspect of layout quality and collectively enables fine-grained spatial reasoning. Given predicted layout Lt at step t, we define the total reward R(Lt) as weighted sum: R(Lt) = λ1Rformat + λ2Rphysics + λ3Rrender (1) (1) Format Reward. This component assesses whether the generated output adheres to the expected structural format, including both syntactic and semantic validity. Instead of assigning binary score, we use graded reward function that evaluates the following aspects: Tag Structure Check: The model output must follow the expected pattern: reasoning trace enclosed in <think> tags and corresponding layout enclosed in <answer> tags. Outputs failing to match this structure receive reward of 0. JSON Parsing Check: If the layout section cannot be parsed into valid JSON object, partial reward of 0.1 is given. Object Count Consistency: The number of objects in the predicted layout must match the number in the ground-truth scene specification (excluding assistant metadata). If there is mismatch, reduced reward of 0.5 is assigned. Full Match: If all checks pass, full reward of 1.0 is granted. Formally, the format reward Rformat {0, 0.1, 0.5, 1.0} is determined by parsing the model output and applying sequence of rule-based validators on both structure and content. This design allows the model to receive meaningful gradients even for partially correct outputs, facilitating more stable learning during early training stages. (2) Physics Reward. To ensure that the generated layout adheres to fundamental physical constraints, we simulate spatial arrangements by converting the predicted JSON layout into scene graph and performing rule-based physical checks. Two major types of violations are penalized: Collision Ratio: Measures the proportion of objects that intersect or overlap in space, computed via bounding box intersection in 3D. Constraint Violation Ratio: Measures how often objects are placed outside of allowable spatial bounds, such as floating in midair or extending beyond room boundaries. 4 The physics reward is computed as: Rphysics = α CollisionRatio β ConstraintRatio (2) where α and β are weight factors (set to 0.2 by default). This component promotes physically valid layouts and discourages unrealistic object placements. (3) Rendering-based Reward. To assess the overall realism, functionality, and aesthetic coherence of generated scene, we adopt rendering-based evaluation strategy using GPT-4o as vision-language judge. Specifically, the predicted layout is rendered into 3D scene image using Blender, and the image is sent to GPT-4o along with the users textual preferences. The model is prompted to rate the layout across five human-aligned criteria: (1) Realism and 3D Geometric Consistency; (2) Functionality and Activity-based Alignment; (3) Layout and Furniture Appropriateness; (4) Color Scheme and Material Choices; and (5) Overall Aesthetic and Atmosphere. Each category is graded on scale from 1 to 10, and the final reward is computed as the normalized sum: Rrender = 1 5 (cid:88) i="
        },
        {
            "title": "Gradei",
            "content": "(3) This high-level reward signal captures subjective qualities that are hard to model directly, such as stylistic alignment and visual appeal, and serves as proxy for human feedback in training. 2.6 Learning with Group Relative Policy Optimization (GRPO) To optimize spatial reasoning in the absence of ground-truth labels, we adopt Group Relative Policy Optimization (GRPO), reinforcement learning algorithm that leverages our grouped refinements and relative reward comparisons to update the policy. GRPO differs from traditional policy gradient or PPO by eliminating the need for value function and relying instead on group-level reward baselines. In our setting, for each input scene x, the model generates refinement trajectories {τ1, . . . , τG}, each containing sequence of layout versions τi = {Li,0, . . . , Li,T }. Each layout Li,t is associated with composite reward R(Li,t) computed from three evaluation components: format validity, physical plausibility, and aesthetic quality. Unlike prior work that aggregates final rewards, we compute discounted cumulative reward: ˆAi,t = γt R(Li,t), where γ (0, 1) is decay factor that prioritizes early layout quality. We then optimize the VLM policy πθ using the following GRPO objective: (4) JGRPO(θ) = ExD (cid:34) 1 (cid:88) i= 1 + 1 (cid:88) t=0 min (cid:18) πθ(Li,t x) πold(Li,t x) (cid:19) , clip() (cid:35) ˆAi,t β DKL[πθπref], (5) where πold is the behavior policy used to generate rollouts, πref is the frozen reference policy, and DKL is the KL divergence term to prevent over-exploration. This formulation provides three benefits: (1) It encourages the model to produce high-quality layouts in earlier refinement steps via decay-weighted rewards. (2) It eliminates the need for ground-truth labels, relying entirely on constraint-driven, evaluative feedback; and (3) It enables stable and group-wise updates that generalize well to diverse scene configurations."
        },
        {
            "title": "3 Experiments",
            "content": "Here, we evaluate MetaSpatials effectiveness in improving spatial reasoning and 3D layout generation capabilities of VLMs through reinforcement learning. We present experimental setups, quantitative metrics, qualitative comparisons, and ablation analyses to demonstrate the benefits of MetaSpatial. 3.1 Experimental Setup We conduct experiments using Qwen2.5-VL 3B and 7B as our base VLMs. All models are trained on curated dataset of indoor 3D scenes, where each scene includes room image, list of objects 5 Table 1: Performance comparison across models with and without RL. RL leads to consistent improvements in formatting accuracy, physical feasibility, and perceptual scene quality. Model Format GPT-4o Score Collision Constraint Overall Qwen 3B Qwen 3B + MetaSpatial Qwen 7B Qwen 7B + MetaSpatial 0.12 0.49 0.85 0.98 0.03 0.18 0.35 0.58 79.0% 68.5% 38.2% 13.7% 100% 100% 95.5% 76.2% -0.27 -0.09 0.51 0.89 with specifications, and user preference description. Instead of relying on ground-truth layouts, MetaSpatial is trained purely through interaction and feedback using our custom reward function. Our total reward is computed as weighted combination of four components, directly reflecting the priorities in our learning objective: 1 50 Rrender + 0.5 Rformat 0.2 CollisionRatio 0.2 ConstraintVioRatio, R(Lt) = (6) Rrender: The aggregated GPT-4o score, obtained by evaluating rendered scene images across five criteria (Realism, Functionality, Layout, Color Scheme, and Aesthetic), each scored from 1 to 10 and normalized by 50. Rformat: structured format reward with values in {0, 0.1, 0.5, 1.0}, based on whether the model output is correctly structured, parsable, and matches the expected number of objects. CollisionRatio: The percentage of objects that overlap with others in the 3D layout, penalizing physically implausible scenes. ConstraintVioRatio: The proportion of objects violating spatial constraints, such as exceeding room boundaries or being improperly placed. This reward composition guides RL by promoting physically valid and aesthetically pleasing layouts while penalizing malformed or unrealistic ones. We apply multi-turn refinement and GRPO to update the model with trajectory-aware gradients. All experiments are conducted using 4 H100 GPUs. 3.2 Quantitative Results We report results across three main metrics: (1) format correctness (i.e., structurally valid JSON outputs), (2) physical feasibility (collision and constraint violation ratios), and (3) GPT-4o-assessed layout quality. Table 1 presents the performance of Qwen-VL models (3B and 7B) with and without MetaSpatial across three evaluation dimensions: format correctness, physical feasibility (measured by collision and constraint violation ratios), and perceptual scene quality assessed by GPT-4o. We observe that MetaSpatial significantly improves all metrics. For format correctness, MetaSpatial enables models to better conform to structured output expectations, with accuracy rising from 0.12 to 0.49 in the 3B model and from 0.85 to 0.98 in the 7B model. In terms of physical feasibility, RL training reduces the collision rate by 10.5% for the 3B model and 24.5% for the 7B model, while also lowering the constraint violation ratio, especially in the 7B setting. Importantly, the GPT-4obased perceptual scoresused as proxy for overall layout realism, coherence, and alignment with user preferenceshow notable gains: from 0.03 to 0.18 for Qwen-VL 3B, and from 0.35 to 0.58 for Qwen-VL 7B. These improvements are reflected in the final composite scores as well, where Qwen-VL 7B with RL achieves 0.89 compared to 0.51 without RL. Overall, the results demonstrate that MetaSpatial significantly enhances the spatial reasoning and generation quality of VLMs, and that larger models benefit more from multi-turn refinement and structured reward feedback. 3.3 Qualitative Results Figure 2 illustrates qualitative comparisons between scenes generated before and after RL training. Prior to reinforcement learning, object placements are often misaligned, physically implausible, and visually cluttered, with issues such as floating or overlapping items. After applying MetaSpatial, layouts become significantly more structured and realisticobjects are better aligned, grounded, and arranged in contextually appropriate positions. These improvements confirm that RL enables VLMs to internalize spatial constraints and generate more coherent, functional 3D scenes, demonstrating its value for real-world applications like AR/VR, metaverse design, and game development. 6 Figure 2: Before vs After: It highlights the efficacy of MetaSpatial in improving 3D spatial reasoning. 3.4 Ablation Study We further analyze the contribution of each reward component by training models with partial reward configurations. Results in Table 2 show that removing any reward component leads to degraded performance, especially when the rendering-based reward is omitted. These results verify that all three components contribute to robust and reliable spatial reasoning. Table 2: Ablation study of reward components on Qwen2.5-VL 7B. Reward Setting Format GPT-4o Score Collision Constraint Full Reward (Ours) w/o Rendering (λ3 = 0) w/o Physics (λ2 = 0) w/o Format (λ1 = 0) 0.98 0.96 0.97 0.72 0.58 0.45 0.40 0.41 13.7% 17.5% 35.0% 16.3% 76.2% 80.5% 89.6% 84.8% Trajectory vs. Single-Step Optimization We compare MetaSpatials full multi-turn refinement strategy with one-step RL baseline, Proximal Policy Optimization (PPO), as well as GRPO variants using different refinement depths. As shown in Table 3, multi-turn refinement consistently improves layout quality across all evaluation metrics. Compared to single-step optimization, our approach leads to better format accuracy, lower collision and constraint violation rates, and higher GPT-4o scores, indicating more coherent, physically plausible, and perceptually satisfying scenes. Moreover, we observe that increasing the number of refinement steps further boosts performance. This suggests that iterative spatial reasoning allows the model to make more informed layout adjustments, gradually improving scene structure and alignment with user preferences. These findings validate the effectiveness of GRPO in leveraging grouped trajectories for learning long-horizon spatial policies and reinforce the importance of multi-turn refinement in layout generation tasks."
        },
        {
            "title": "4 Related Work",
            "content": "Our work builds upon recent progress in 3D scene generation, vision-language modeling, and reinforcement learning for structured reasoning. We review three lines of related research: (1) paradigms for generating 3D scenes either through generative modeling or layout-based synthesis; (2) methods for enhancing spatial reasoning of VLMs; and (3) the emerging role of RL in enhancing the reasoning capabilities of foundation models. These perspectives contextualize the motivation and novelty of our proposed MetaSpatial framework together. 4.1 3D Scene Generation Paradigms In recent years, two primary directions have emerged in 3D scene generation. The first direction leverages generative models to create 3D representations such as meshes [12, 20]. However, the generated scenes often lack the granularity and fidelity required for downstream embodied applications, where high-quality and individually controllable objects are essential [5]. With the advancement of large foundation models [1518], the second direction focuses on generating intermediate representationsnamely, scene layouts-by retrieving objects from large-scale asset repositories [3, 19]. LayoutGPT is among the first to utilize LLMs as visual planners, generating layouts conditioned on text descriptions [6]. However, its performance is limited by the general-purpose pretraining strategy of LLMs. I-Design further introduces multi-agent framework, where team of LLMs represents different roles in the design process [3]. To enhance physical plausibility, LayoutDreamer integrates 7 Table 3: Comparison of single-step RL and our multi-turn refinement strategy with GRPO. Method Format GPT-4o Score Collision Constraint One-step RL (PPO) Multi-turn RL (GRPO) w/o = 1 Multi-turn RL (GRPO) w/o = 3 Multi-turn RL (GRPO) w/o = 5 0.97 0.96 0.96 0.98 0.44 0.5 0.54 0.58 26.6% 21.3% 16.0% 13.7% 83.0% 81.2% 79.5% 76.2% 3D Gaussian Splatting to optimize the generated layout [26]. Meanwhile, LayoutVLM leverages the visual understanding capabilities of VLMs to produce enhanced representations from visually marked images, followed by differentiable optimization process [21]. This work also demonstrates that fine-tuning VLMs with existing layout data can improve generation quality. Despite these advances, two key challenges remain: (1) the lack of internalized 3D spatial reasoning in VLMs, which limits their abilities to ensure physical plausibility without costly post-processing; and (2) the inefficient of SFT, which limits generation diversity due to the absence of perfect ground truth annotations. These challenges motivate us to explore new paradigm for layout generation. 4.2 Spatial Reasoning with VLMs VLMs show impressive capabilities in tasks involving visual understanding and natural language generation, such as image captioning, visual question answering, and referring expression comprehension [25]. However, their capacity for structured spatial reasoningparticularly in 3D environmentsremains underexplored. Prior works such as BLIP-2 [10] and Flamingo [1] exhibit limited understanding of spatial relations beyond 2D grounding or image-text alignment. Recent efforts attempt to address this by providing VLMs with more structured or spatially annotated inputs such as SpatialVLM [4] and AutoSpatial [9]. Yet, such approaches still depend heavily on external supervision to enforce spatial constraints. However, 3D spatial reasoning inherently lacks perfect annotations and single ground truthmultiple valid layouts can exist for the same input, each satisfying different contextual or functional constraints. As result, current supervised approaches struggle to capture the diversity and adaptability required for realistic scene generation. In contrast, MetaSpatial addresses this limitation by allowing VLMs to learn spatial reasoning through interactive feedback and constraint-driven exploration, moving beyond annotation-dependent paradigms. 4.3 RL for Enhancing Reasoning in Foundation Models RL recently re-emerges as promising strategy for improving the reasoning capabilities of large foundation models [7]. Instead of relying on traditional SFT with fixed ground truth, RL allows models to learn from evaluative feedback, which is especially useful in tasks lacking single correct answer or exhibiting structural ambiguity [13]. In language models, RL has been widely used in the form of Reinforcement Learning from Human Feedback (RLHF) to align model outputs with human preferences [2]. Such approaches are fundamental to the success of instruction-following models such as ChatGPT [24]. In addition, rule-based reinforcement fine-tuningsuch as OpenAIs o1 [8] and DeepSeek-R1 [7]has demonstrated strong performance in mathematical reasoning [14], code generation [11], and multi-step logic tasks [23]. These methods show that verifiable rewardssuch as symbolic correctness or execution-based signalscan serve as effective supervision substitutes. While RL shows strong potential in language tasks, it remains largely unexplored in multimodal or spatial contexts. Unlike these domains, 3D layout generation lacks definitive ground truthmultiple valid solutions may exist for the same inputmaking supervised fine-tuning insufficient to cover the full solution space. To address this, we propose the first rule-driven RL framework for vision-language models in 3D environments, incorporating multi-turn refinement and optimization to enable adaptive, constraint-aware spatial reasoning without reliance on rigid annotations."
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "In this work, we propose MetaSpatial, the first RL-based framework for enhancing 3D spatial reasoning in VLMs. By leveraging novel three-level reward design and multi-turn refinement strategy optimized via GRPO, MetaSpatial enables VLMs to generate physically plausible, structurally coherent, and aesthetically satisfying 3D layouts without requiring hard-coded post-processing or 8 exhaustive annotations. Our experiments demonstrate significant improvements in layout quality, model adaptability, and generation diversity over traditional supervised approaches. For future work, we plan to explore more lightweight rendering and evaluation pipelines to reduce computational overhead and extend MetaSpatial to support open-world object retrieval and more complex multiroom scenes. We also aim to investigate the generalizability of our spatial reasoning paradigm to other domains such as robotic planning, AR/VR design, and embodied AI."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [2] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [3] Ata Çelen, Guo Han, Konrad Schindler, Luc Van Gool, Iro Armeni, Anton Obukhov, and Xi Wang. I-design: Personalized llm interior designer. arXiv preprint arXiv:2404.02838, 2024. [4] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. [5] Chuan Fang, Yuan Dong, Kunming Luo, Xiaotao Hu, Rakesh Shrestha, and Ping Tan. Ctrlroom: controllable text-to-3d room meshes generation with layout constraints. arXiv preprint arXiv:2310.03602, 2023. [6] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual planning and generation with large language models. Advances in Neural Information Processing Systems, 36:1822518250, 2023. [7] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [8] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [9] Yangzhe Kong, Daeun Song, Jing Liang, Dinesh Manocha, Ziyu Yao, and Xuesu Xiao. Autospatial: Visual-language reasoning for social robot navigation through efficient spatial reasoning learning. arXiv preprint arXiv:2503.07557, 2025. [10] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [11] Jiawei Liu and Lingming Zhang. Code-r1: Reproducing r1 for code with reliable rewards. github, 2025. [12] Yunze Man, Shuhong Zheng, Zhipeng Bao, Martial Hebert, Liangyan Gui, and Yu-Xiong Wang. Lexicon3d: Probing visual foundation models for complex 3d scene understanding. Advances in Neural Information Processing Systems, 37:7681976847, 2024. [13] Gianluca Mondillo, Simone Colosimo, Alessandra Perrotta, Vittoria Frattolillo, and Mariapia Masino. Comparative evaluation of advanced ai reasoning models in pediatric clinical decision support: Chatgpt o1 vs. deepseek-r1. medRxiv, pages 202501, 2025. [14] Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. Tinyzero. https://github.com/Jiayi-Pan/TinyZero, 2025. Accessed: 2025-01-24. [15] Zhenyu Pan, Rongyu Cao, Yongchang Cao, Yingwei Ma, Binhua Li, Fei Huang, Han Liu, and Yongbin Li. Codev-bench: How do llms understand developer-centric code completion? arXiv preprint arXiv:2410.01353, 2024. [16] Zhenyu Pan, Haozheng Luo, Manling Li, and Han Liu. Chain-of-action: Faithful and multimodal question answering through large language models. arXiv preprint arXiv:2403.17359, 2024. [17] Zhenyu Pan, Haozheng Luo, Manling Li, and Han Liu. Conv-coa: Improving open-domain question answering in large language models via conversational chain-of-action. arXiv preprint arXiv:2405.17822, 2024. [18] Zhenyu Pan, Xuefeng Song, Yunkun Wang, Rongyu Cao, Binhua Li, Yongbin Li, and Han Liu. Do code llms understand design patterns? arXiv preprint arXiv:2501.04835, 2025. [19] Ohad Rahamim, Hilit Segev, Idan Achituve, Yuval Atzmon, Yoni Kasten, and Gal Chechik. Lay-a-scene: Personalized 3d object arrangement using text-to-image priors. arXiv preprint arXiv:2406.00687, 2024. [20] Jonas Schult, Sam Tsai, Lukas Höllein, Bichen Wu, Jialiang Wang, Chih-Yao Ma, Kunpeng Li, Xiaofang Wang, Felix Wimbauer, Zijian He, et al. Controlroom3d: Room generation using semantic proxy rooms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62016210, 2024. [21] Fan-Yun Sun, Weiyu Liu, Siyi Gu, Dylan Lim, Goutam Bhat, Federico Tombari, Manling Li, Nick Haber, and Jiajun Wu. Layoutvlm: Differentiable optimization of 3d layout via vision-language models. arXiv preprint arXiv:2412.02193, 2024. [22] Wei-Feng Tung and Soe-Tysr Yuan. idesign: an intelligent design framework for service innovation. In 2007 40th Annual Hawaii International Conference on System Sciences (HICSS07), pages 6464. IEEE, 2007. [23] Zihan Wang*, Kangrui Wang*, Qineng Wang*, Pingyue Zhang*, Linjie Li*, Zhengyuan Yang, Kefan Yu, Minh Nhat Nguyen, Monica Lam, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, and Manling Li. Training agents by reinforcing reasoning, 2025. [24] Tianyu Wu, Shizhu He, Jingping Liu, Siqi Sun, Kang Liu, Qing-Long Han, and Yang Tang. brief overview of chatgpt: The history, status quo and potential future development. IEEE/CAA Journal of Automatica Sinica, 10(5):11221136, 2023. [25] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [26] Yang Zhou, Zongjin He, Qixuan Li, and Chao Wang. Layoutdreamer: Physics-guided layout for text-to-3d compositional scene generation. arXiv preprint arXiv:2502.01949, 2025."
        }
    ],
    "affiliations": [
        "Department of Computer Science Northwestern University"
    ]
}