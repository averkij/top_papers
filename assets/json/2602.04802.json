{
    "paper_title": "VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?",
    "authors": [
        "Qing'an Liu",
        "Juntong Feng",
        "Yuhao Wang",
        "Xinzhe Han",
        "Yujie Cheng",
        "Yue Zhu",
        "Haiwen Diao",
        "Yunzhi Zhuge",
        "Huchuan Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench."
        },
        {
            "title": "Start",
            "content": "VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? Qingan Liu * 1 Juntong Feng * 1 Yuhao Wang * 1 Xinzhe Han 1 Yujie Cheng 1 Yue Zhu 1 Haiwen Diao 2 Yunzhi Zhuge 1 Huchuan Lu 1 6 2 0 2 4 ] . [ 1 2 0 8 4 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "VisionLanguage Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at VISTA-Bench. 1. Introduction Recent visionlanguage models (VLMs) (Bai et al., 2025a; Wang et al., 2025; Diao et al., 2024; 2025b;a) extend large language models (LLMs) (Yang et al., 2025; Grattafiori et al., 2024; Cai et al., 2024) to diverse tasks by treating visual tokens and text queries as standard inputs. Accordingly, existing benchmarks predominantly assume pure-text 1School of Artificial Intelligence, Dalian University of Technology, Dalian, China 2S-Lab, Nanyang Technological University, Singapore. Correspondence to: Haiwen Diao <haiwen.diao@ntu.edu.sg>, Yunzhi Zhuge <zgyz@dlut.edu.cn>. Preprint. February 5, 2026. queries. However, real-world scenarios also embed language directly within visual content in Figure 1(a), raising fundamental question: Do visionlanguage models truly understand visualized text as well as pure text? Recent explorations such as DeepSeek-OCR (Wei et al., 2025) and Glyph (Cheng et al., 2025) point to growing text-as-pixels paradigm, in which text is rendered into image to reduce token overhead in long contexts and to establish unified perceptual interface across modalities. Crucially, replacing pure-text queries with visualized text fundamentally alters the input pathway in Figure 1(b), forcing all information to be processed through unified modality. These developments expose an interesting and underexplored challenge: whether foundation models can preserve semantic fidelity and cross-modal alignment when language is represented as pixels, especially in terms of visuallanguage understanding and unified model architecture design in the future. Notably, existing VLM evaluation protocols remain largely text-centric, spanning both standard language reasoning benchmarks (Hendrycks et al., 2020) and widely used multimodal benchmarks (Liu et al., 2024b; Yue et al., 2024; Li et al., 2024a). This blind spot overlooks the perceptual challenge of reading language from pixels, leaving it unclear whether model behavior remains stable when language isnt conveyed symbolically. Fortunately, some concurrent works start to expose this gap. VTCBench (Zhao et al., 2025) reports significant performance degradation when text-only tasks are converted to visualized text. Impressively, Gemini-3-Pro (Google, 2025) exhibits tiny modality gap, highlighting the potential for modality-equivalent representations. However, existing analyses are largely confined to unimodal settings. It therefore remains unclear whether similar phenomena persist in multimodal tasks from perception to reasoning and how they manifest in such scenarios. To address this question, we uncover pronounced modality gap between pure-text and visualized-text inputs through preliminary experiments. We then introduce VISTA-Bench, benchmark of 1,500 carefully filtered samples designed for rigorous comparison under strictly matched pure-text and visualized-text conditions. VISTA-Bench is built through three-stage pipeline that curates high-quality questions, VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? Figure 1. (a) Humans integrate visual context with embedded text, whereas standard VLM evaluation provides language as discrete tokens. (b) Presenting language as visualized text can induce behavioral deviations from pure-text inputs, revealing modality gap. renders text into diverse visual layouts, and enforces fidelity via VLM-based verification. It contains multiple-choice questions and organizes them under hierarchical capability taxonomy spanning perception, reasoning, and knowledge. Evaluating over 20 state-of-the-art VLMs, we uncover pervasive modality gap that is consistently amplified by perceptually challenging renderings. Our analysis attributes this gap primarily to limited perceptual robustness, while additional visual grounding offers only partial mitigation. Notably, MiMo-VL-7B-RL (MiMo, 2025) stands out as rare exception, exhibiting markedly stronger robustness under visualized-text inputs. Overall, VISTA-Bench provides principled testbed for diagnosing modality gaps under realistic text-as-pixels inputs and underscores the need for more robust unified representations in future VLMs. Our main contributions are summarized as follows: Exposing modality non-equivalence phenomenon. We uncover comprehensive analyses across multimodality perception, reasoning, and knowledge tasks when text is presented as pixels rather than symbols. Defining systematic visualized-text benchmark. VISTA-Bench is rigorously controlled and easy-todeploy benchmark, filling blind spot in evaluating the gap between pure-text and visualized-text inputs. Providing text-as-pixels diagnosis and outlook. Large-scale analyses identify limited perceptual robustness as the core driver of the modality gap, and VISTA-Bench elevates text-as-pixels to foundational challenge for unified visionlanguage representations. 2. Related Work 2.1. Multimodal Understanding Benchmarks The rapid advancement of Vision-Language Models (VLMs) has led to significant breakthroughs in areas such as finegrained OCR, text retrieval and text-image understanding. To comprehensive evaluate these capabilities, variety of benchmarks have been proposed. Early efforts, including DocVQA (Mathew et al., 2021) and OCRBench (Liu et al., 2024c), primarily concentrate on localized text recognition and document-level OCR tasks. Subsequent benchmarks have expanded their scope to encompass higher-level visual cognition, reasoning and the integration of external knowledge (Liu et al., 2024b; Yue et al., 2024; Li et al., 2024a). Despite this progress, most existing evaluation paradigms predominantly focus on pure-text. Following the established philosophies of comprehensive benchmarks, we introduce VISTA-Bench, systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains based on visualized text input. VISTA-bench serves as necessary supplement that reveals the modality gap between pure and visualized text and provides critical entry point for research on unified multimodal representations. 2.2. Emerging Paradigms in Visualized Text Recent advancements, such as DeepSeek-OCR (Wei et al., 2025) and Glyph (Cheng et al., 2025), have revitalized the text-as-pixel paradigm, demonstrating that visualized text can significantly enhance text compression and inference efficiency while maintaining accuracy (Xing et al., 2025; Li et al., 2025). concurrent work, VTCBench (Zhao et al., 2025), renders text-only tasks into images, revealing that 2 VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? the above settings, we can fully explore the modality gap introduced by visualized text across both task paradigms. 3.2. Preliminary experiments To investigate the modality gap between text and visualized text, we conduct preliminary experiments. Here are three key findings we summarized from the experiments. Finding 1: Replacing pure text with visualized text reveals persistent modality gap. Visualized text consistently underperforms pure text. As shown in Figure 2, in unimodal tasks, the drop is largest, with Qwen3-VL-8BInstruct accuracy falling from 75.99% to 68.46% on MMLU. Multimodal tasks show smaller declines, indicating that image input partially compensates for visualized text. Overall, Qwen3-VL-8B-Instruct outperforms InternVL-3.5-8B. Finding 2: Visualized text processing is governed by both perceptual robustness and instruction sensitivity. Appropriate settings improve accuracy, while poor choices can sharply degrade it. From the visualized text and prompt, we explore three key factors that directly affect the models processing of visualized text. Details are as follows. Font Size. Text questions are rendered in Arial at 9pt, 16pt, 32pt, 48pt, and 64pt to examine how font size affects the modality gap. As shown in Figure 3 top row, font size substantially influences performance across models and tasks: very small fonts yield the lowest accuracy due to poor legibility, with larger impact in the unimodal setting where all semantics must be recovered from pixels. Increasing the font size to 3248pt consistently improves accuracy, with the largest gains on MMLU. At 64pt, performance saturates or slightly declines because larger text induces more line wrapping and reduces the effective context per image. Overall, even under the best font sizes, visualized text remains worse than pure text, confirming persistent modality gap. Font Style. Building on the font size analysis, we fix the rendering size at 16 pt to examine the effect of font style on the modality gap. We compare sans-serif font, Arial; two serif fonts, Times New Roman and Cambria; and handwritten-style font, Brush Script MT. Across both models and tasks, standard fonts yield similar performance, with only minor variations relative to the Arial baseline. The handwritten-style font consistently degrades accuracy across all benchmarks, with the largest impact on MMLU, where Qwen3-VL-8B-Instruct drops from 68.5% (Arial) to 64.5% (Brush) in Figure 3, and InternVL-3.5-8B exhibits similar trend. Overall, the gap is minimal with standard fonts but substantial with handwritten-style fonts. Prompt Design. We compare five prompt variants to assess prompt sensitivity, details are provided in Appendix Figure 9. Moderate-length prompts with semantic guidance slightly improve accuracy and reduce the modality gap. Figure 2. Comparison between Text and Visualized Text Inputs. most models suffer from significant modality gap, whereas Gemini (Google, 2025) exhibits much higher robustness to such visualized transitions, indicating strong potential for unified visual-textual representations. However, this benchmark is largely confined to single-modality tasks and falls short of covering the diverse modality and broad capability required in real-world applications. In the multimodal domain, several efforts merge images and questions into single visual input (Li et al., 2023; An et al., 2025a). However, this all-in-one design offers limited flexibility, especially for text-dense images, where overlaying queries can introduce visual interference. In contrast, VISTA-Bench adopts decoupled formulation that integrates multimodal and unimodal tasks while separating original images from questionrendered images. This design enables more comprehensive benchmark and allows precise, efficient evaluation of the modality gap between pure-text and visualized-text inputs. 3. VISTA-Bench We introduce VISTA-Bench, the first benchmark that uses visualized text to systematically evaluate the visual understanding capabilities of VLMs. In this section, we first present the task settings and preliminary experiments to reveal three key findings. Based on these insights, we detail the construction of VISTA-Bench in three steps. 3.1. Task Settings We study the impact of replacing pure text with visualized text under both unimodal and multimodal tasks. Each question is evaluated in two forms: pure text and visualized text and processed by the vision encoder. For the unimodal task, we use MMLU (Hendrycks et al., 2020). For the multimodal task, we evaluate three benchmarks, including MMBench en (Liu et al., 2024b), Seed-Bench (Li et al., 2024a), and MMMU (Yue et al., 2024). Experiments are conducted on two VLMs, Qwen3-VL-8B-Instruct (Bai et al., 2025a) and InternVL-3.5-8B (Wang et al., 2025). Unless otherwise specified, visualized text is rendered in Arial at 16pt, with fixed width of 800 pixels and adaptive height. With 3 VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? Figure 3. Perceptual factor impact. Top: Font Size (9, 16, 32, 48, 64). Bottom: Font Style (Arial, Cambria, Roman, Brush). Conversely, very short or highly structured prompts, such as Chain-of-Thought, can hurt performance. CoT is especially harmful for InternVL-3.5-8B, often producing irrelevant output instead of an answer. Overall, Qwen3-VL-8B-Instruct is stable across prompts, while InternVL-3.5-8B shows larger fluctuations, indicating model-dependent sensitivity. Finding 3: The text recognition capability of VLMs is closely related to the modality gap. Models with stronger text recognition abilities show smaller modality gap. In the experiments above, we observe that Qwen3-VL-8BInstruct has smaller modality gap on visualized text, while InternVL-3.5-8B shows larger gap. We attribute this difference to stronger visual text recognition. On OCRrelated benchmarks, Qwen3-VL-8B-Instruct scores 96.1 on DocVQA test (Mathew et al., 2021) and 896 on OCRBench (Liu et al., 2024c), whereas InternVL-3.5-8B scores 92.3 and 832. These results suggest that better OCR ability helps Qwen3-VL-8B-Instruct interpret visualized text more accurately, which in turn reduces the modality gap. Building on these findings, we observe that visualized-text inputs induce consistent modality gap across models and tasks, whose magnitude depends on recognition robustness and is modulated by rendering and prompting choices. This motivates the construction of dedicated benchmark to systematically evaluate VLMs under visualized-text conditions. 3.3. VISTA-Bench Construction We propose VISTA-Bench, benchmark designed to evaluate the capability of models to process visualized text. As shown in Figure 4, construction of VISTA-Bench consists of three steps: (i) Data Construction, (ii) Rendering Pipeline, and (iii) VLM as Filter Judge. Below, we provide details of the construction and brief introduction to the benchmark. Step 1: Data Construction. We assemble data pool by collecting various samples across distinct categories from existing benchmarks (Liu et al., 2024b; Yue et al., 2024; Li et al., 2024a; Hendrycks et al., 2020). To guaranty diversity, our initial data extraction was guided by the underlying categories present in the data pool. Subsequently, the extracted samples were subjected to rigorous manual review to ensure their accuracy. Exclusively correct samples populate the filtered dataset prior to the rendering process. Step 2: Rendering Pipeline. Since the filtered dataset preserves structured code and LaTeX formulas, naive rendering without specialized handling can produce unreadable text and visual artifacts. To mitigate this, we propose LaTeXbased rendering pipeline. During preprocessing, we apply dedicated treatment to text, code, and formulas to preserve semantic accuracy. Width anchoring and font mapping in the generation stage maintain visual harmony and font diversity. In post-processing, we perform fidelity-preserving rasterization, content localization, and adaptive cropping to obtain the precisely extracted visualized text regions. Step 3: VLM as Filter Judge. To ensure rendering fidelity, we employ Qwen3-VL-32B as the filter judge. Guided by system instructions, we prioritize the rendering accuracy of text, code, and formulas as primary verification verticals. three-tier hierarchical alignment status is employed to categorize rendering quality, where samples scoring below 2 undergo manual review to drive the pipeline refinement. Through this procedure, we finally establish VISTA-Bench, supported by sophisticated rendering pipeline. 4 VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? Figure 4. Overview of the construction. First, we extract filtered dataset from existing data rely on diversity and accuracy. Second, we transform text into visualized text through the rendering pipeline. We then validate the precision of visualized text depends on VLM and continuously refine the pipeline. Through this process, we finally establish VISTA-Bench, supported by sophisticated rendering pipeline. VISTA-Bench comprises 1,500 instances, predominantly featuring Multiple Choice Questions alongside selective set of open-ended queries. Each instance provides dual data formats: visualized text and their corresponding text counterparts. As illustrated in Figure 5, we establish hierarchical evaluation framework to systematically assess the capabilities of VLMs in processing visualized text. Our benchmark is structured into three levels, comprising 4 primary tasks, 10 sub-tasks and 25 fine-grained dimensions. The design motivations for the four primary tasks are detailed below: Multimodal Perception. This task contains 300 instances and evaluates perceptual grounding under visualized-text inputs. It covers three sub-dimensions: (i) Global Perception for holistic scene understanding, (ii) Instance Perception for locating or identifying specific entities and (iii) Attribute Perception for recognizing fine-grained properties. Together, it measures how well models extract and ground information when textual cues are presented as dense pixels. Multimodal Reasoning. This task contains 300 instances and evaluates reasoning over visualized-text inputs with accompanying visual context. It spans three sub-dimensions: (i) Logical Reasoning for multi-step inference, (ii) Spatial & Relation Understanding for relative positions and relations, and (iii) Cross-Instance Reasoning for aggregating evidence across multiple visual elements. Together, it measures whether VLMs can effectively execute non-trivial reasoning when key linguistic cues are rendered as pixels. Multimodal Knowledge. This task includes 400 knowledge intensive instances and evaluates knowledge application when questions are presented as visualized text with Figure 5. Ability dimensions in VISTA-Bench. VISTA-Bench includes two main levels of dimensions based on Inherent Modality Dependence and Cognitive Dimension, with 10 distinct abilities. supporting visual evidence. We organize queries into two domains: (i) STEM & Health, covering scientific and medical knowledge and (ii) Social-Humanities & Management, covering cultural and organizational knowledge. It measures whether models can retrieve and apply domain knowledge when key linguistic cues must be read from pixels. Unimodal Knowledge. This task includes 500 instances and isolates knowledge retrieval from visual scene understanding by removing external images. Each instance is formed by rendering pure-text knowledge prompt into visualized text, so all evidence must be read from pixels. We VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? Table 1. Comparison of different VLMs on our benchmark. Results are reported under Visualized Text (VT) and Text inputs for each metric. Text cells are shaded in light gray. The best result per column is bolded and the second best is underlined. The Gap column denotes the overall performance drop when switching from Text to Visualized Text. All metrics are reported as percentages (%). Model Vision-Language Models (2B) DeepSeek-VL2-Tiny (Wu et al., 2024) Qwen3-VL-2B-Instruct (Bai et al., 2025a) Ovis2-2B (Lu et al., 2024) NEO-2B-SFT (Diao et al., 2025a) Qwen2.5-VL-3B-Instruct (Bai et al., 2025b) InternVL3.5-2B (Wang et al., 2025) SAIL-VL2-2B (Yin et al., 2025) Ovis2.5-2B (Lu et al., 2025) Vision-Language Models (8B) LLaVA-1.5-7B (Liu et al., 2024a) LLaVA-OneVision-7B (Li et al., 2024b) Qwen2.5-VL-7B-Instruct (Bai et al., 2025b) MiniCPM-V-4 5 (Yu et al., 2025) Qwen3-VL-8B-Instruct (Bai et al., 2025a) Ovis2-8B (Lu et al., 2024) InternVL3.5-8B (Wang et al., 2025) MiMo-VL-7B-RL (MiMo, 2025) NEO-9B-SFT (Diao et al., 2025a) LLaVA-OneVision-1.5-8B (An et al., 2025b) MiMo-VL-7B-SFT (MiMo, 2025) SAIL-VL2-8B (Yin et al., 2025) Ovis2.5-9B (Lu et al., 2025) GLM-4.1V-9B-Thinking (Hong et al., 2025) Vision-Language Models (30B-A3B) 69.0 Kimi-VL-A3B-Thinking (Team et al., 2025) Qwen3-VL-30B-A3B-Instruct (Bai et al., 2025a) 64.3 64.3 InternVL3.5-30B-A3B (Wang et al., 2025) Multimodal Multimodal Multimodal Unimodal Perception Reasoning Knowledge Knowledge VT Text VT Text VT Text VT Text VT Overall Gap 44.3 51.3 58.3 40.0 65.0 56.0 65.3 66.3 33.0 40.3 65.7 64.3 65.3 66.7 61.3 70.3 32.7 62.7 68.3 68.7 68.3 70.7 64.0 69.0 66.7 68.3 67.7 66.3 69.7 69.7 58.7 66.0 65.3 71.6 67.3 71.0 64.3 69.0 69.0 68.7 69.3 70.0 69.0 71.3 71.0 71.0 70. 31.3 32.7 39.7 31.3 43.3 39.3 47.3 51.7 27.3 27.0 52.7 45.7 49.0 47.7 45.7 58.3 29.0 46.3 61.3 54.3 56.3 58.7 49.7 51.0 50.3 43.7 49.7 52.3 49.3 54.3 50.3 57.7 58.0 44.3 56.3 53.0 60.3 49.3 60.0 52.3 61.3 58.0 59.0 62.7 60.7 65.3 61.7 61.7 60.0 61. 27.5 17.5 27.0 25.3 32.5 30.5 32.2 28.5 26.5 20.0 27.0 31.5 37.5 29.0 35.3 40.5 24.8 33.5 40.5 37.8 38.8 50.0 30.8 33.0 40.8 28.8 24.0 31.2 37.3 35.8 39.8 39.0 39.5 27.3 35.3 35.3 36.0 44.5 39.8 44.5 38.3 40.5 42.5 41.0 44.8 52.0 52.5 33.5 44.0 50. 27.6 37.2 36.0 29.4 54.8 44.8 43.6 51.8 24.0 27.0 62.4 50.4 57.8 50.8 57.6 69.0 28.6 57.4 70.6 58.0 66.0 73.8 52.6 54.2 61.8 41.8 52.0 50.2 53.4 56.6 57.0 54.8 60.0 48.8 58.6 62.0 55.0 68.2 65.4 71.2 68.8 69.2 67.8 72.0 71.0 73.8 75.8 66.4 71.0 75. 31.7 33.9 38.8 30.8 48.6 42.1 45.7 48.5 27.1 27.8 51.7 47.2 52.1 47.5 50.0 59.5 28.5 49.9 60.3 54.0 57.3 63.8 49.4 49.9 54.4 Text 43.1 47.5 48.9 51.3 52.8 52.9 54.1 56.1 44.1 53.4 53.7 54.3 57.9 58.6 58.9 59.2 59.3 59.5 61.3 61.7 65.3 65. 57.6 61.6 64.9 -11.4 -13.6 -10.1 -20.5 -4.2 -10.8 -8.4 -7.6 -17.0 -25.6 -2.0 -7.1 -5.8 -11.1 -8.9 +0.3 -30.8 -9.6 -1.0 -7.7 -8.0 -2.1 -8.2 -11.7 -10.5 cover two domains: (i) Natural & Life Sciences and (ii) Social & Applied Sciences. This setup diagnoses whether performance is primarily limited by decoding dense visualizedtext signals rather than by the availability of visual context. 4. Experiments 4.1. Setup Models and Evaluation. We evaluate broad suite of open-source vision-language models spanning three scale regimes: (i) small models around 2-3B parameters, (ii) midsized models around 7-9B parameters and (iii) MoE-based 30B-A3B models. Our suite covers major open families, including InternVL3.5 (Wang et al., 2025), Qwen2.5/3VL (Bai et al., 2025b;a), LLaVA (1.5, OneVision and OneVision-1.5) (Liu et al., 2024a; Li et al., 2024b; An et al., 2025b), MiMo-VL (MiMo, 2025), GLM-4.1V (Hong et al., 2025), NEO (Diao et al., 2025a), DeepSeek-VL2 (Wu et al., 2024), Ovis2/2.5 (Lu et al., 2024; 2025), SAIL-VL2 (Yin et al., 2025), Kimi-VL (Team et al., 2025), and MiniCPMV 4.5 (Yu et al., 2025). All evaluations are performed using VLMEvalKit (Duan et al., 2024) with the default decoding settings released by each official repository, ensuring consistent comparisons. Experiments are conducted in BF16 on NVIDIA A800 GPUs, ensuring comparable evaluation. 4.2. Main Results The modality gap is pervasive across current VLMs. Table 1 shows consistent accuracy declines for almost all evaluated models when shifting from pure-text to visualizedtext questions. The degradation can be substantial: NEO9B-SFT decreases by 30.8 points overall, and LLaVAOneVision-7B falls by 25.6 points, whereas stronger modern models show much smaller declines, such as GLM-4.1V9B-Thinking with 2.1-point drop and Qwen2.5-VL-7BVISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? Figure 6. Qualitative attention visualization analysis of models with disparate OCR capabilities under various rendering configurations. Instruct with 2.0-point drop. Overall, these results indicate that high pure-text performance does not reliably translate to robust visualized-text understanding. Within multimodal tasks, the modality gap is most pronounced for reasoning and knowledge, while perception is relatively more stable. Table 1 shows that strong models can remain close on multimodal perception under visualized text, yet drop substantially on reasoning despite strong pure-text reasoning performance, indicating that imperfect text perception can be amplified throughout multi-step inference. For Ovis2.5-9B, multimodal reasoning decreases by 9.0% and multimodal knowledge decreases by 13.2%, while perception remains near parity. similar trend appears for Kimi-VL-A3B, whose reasoning drops by 12.0% even with high perception. In contrast, the knowledge gap is less dominant for some models where pure-text performance is low. LLaVA-1.5-7B illustrates this case, with knowledge remaining similarly low at 27.3% versus 26.5%, while its perception and reasoning scores drop substantially by 25.7% and 17.0%, respectively. Finally, certain models exhibit large degradations across all three multimodal sub-tasks, such as NEO-9B-SFT with reductions of 36.3% (perception), 29.0% (reasoning), and 15.7% (knowledge), which is consistent with limited training exposure to text-as-pixels inputs and weaker robustness to rendering variations. Unimodal knowledge is stress test of whether models can read text from pixels and use it for answering. Across models, the modality gap is sharply polarized, spanning from -0.4 to 40.6 points, showing strong pure-text performance does not automatically translate to robust visualizedtext understanding. representative failure is high puretext accuracy paired with low visualized-text accuracy, as NEO-9B suffers 40.6% degradation. In contrast, small set of models remains near-parity, including GLM-4.1V9B with 2.0% drop and MiMo-VL-7B-SFT with 1.4% drop, indicating that unimodal text-as-pixels understanding is achievable. Within the LLaVA family, robustness changes markedly across iterations: LLaVA-OneVision-1.5-8B narrows the drop to 10.4%, whereas LLaVA-OneVision-7B collapses by 31.6%, aligning with the former introducing stronger text-centric visual modeling and more targeted post-training for text-rich perception. Overall, these results highlight unimodal knowledge as the setting where the textperception bottleneck is directly exposed and where model differences in visualized-text robustness become visible. 4.3. Fine-grained Analysis Perceptual robustness is the primary bottleneck. To locate the source of the modality gap, we analyze cases where model answers correctly under pure text but fails under visualized text, which isolates perceptual errors from semantic deficits. As shown in Figure 6, the blue area represents rendering-related errors, while the gray area represents rendering-agnostic errors. Based on results from four representative models (GLM-4.1V-9B-Thinking, NEO-9B-SFT, InternVL3.5-2B, and Qwen3-VL-30B-A3B), we make two (i) consistent observations that support this conclusion. Rendering-driven failures dominate. Across models, most errors are associated with challenging renderings such as small fonts and handwritten styles, indicating that models often have the required knowledge, as reflected by their pure-text success, but fail to access it once the same content must be read from pixels. (ii) Stronger models become increasingly perception-limited. Models with smaller overall modality gaps show higher share of rendering-related errors, suggesting that as semantic reasoning improves, the remaining failures concentrate on fine-grained visual text perception rather than on reasoning itself. Models are sensitive to rendering complexity. We take GLM-4.1V-9B-Thinking as representative model and examine its performance under different font sizes and font styles in Figure 7. Two observations stand out. (i) Perceptu7 VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? Figure 7. Sensitivity to font size and style on VISTA-Bench. Figure 8. Modality Gap Across Multimodal and Unimodal Tasks. ally difficult renderings remain clear challenge. Even for this strong model, atypical presentations such as 9pt small text and Brush-style handwriting substantially enlarge the modality gap to 6.8 and 8.4, compared to an overall gap of 2.1. This indicates that reduced legibility and rendering artifacts can still disrupt reliable evidence extraction from pixels. (ii) Clean renderings can remove the gap and sometimes even outperform pure text. Under large font sizes or standard font styles, the model becomes nearly gap-free, and in some settings the visualized-text accuracy slightly exceeds the pure-text accuracy. This suggests that when visualized text is rendered cleanly, the image input can act as stable interface that supports precise grounding and reduces ambiguity in the downstream decision. Multimodal context attenuates the modality gap. We define the modality gap as the accuracy difference between pure-text and visualized-text inputs and report two aggregates: the Multimodal Gap computed over the combined set of perception, reasoning, and knowledge tasks and the Unimodal Gap measured on the knowledge task without additional images. As shown in Figure 8, the blue dots represent the Unimodal Gap, while the orange dots represent the Multimodal Gap, these two metrics diverge substantially: the average Unimodal Gap reaches 15.3%, whereas the Multimodal Gap is lower at 10.2%. plausible explanation is that multimodal inputs provide additional contextual visual evidence that grounds interpretation and constrains plausible answers, whereas unimodal inputs rely entirely on text recovered from pixels, making errors harder to correct and more likely to be filled by language priors alone. 5. Limitations and Disscusion We construct systematic benchmark to evaluate capabilities from multimodal perception, reasoning, to unimodal understanding within the visualized text input. We acknowledge several limitations that also suggest future directions: (i) More top-tier VLMs. Our study mainly focuses on publicly accessible and reproducible VLMs. Due to resource and accessibility constraints, some closed-source frontier VLMs, such as GPT-5 (Singh et al., 2025) and Gemini3pro (Google, 2025), are currently outside the scope of this work and will be added in the near future. (ii) Evaluating LLMs via agentic auxiliary. Beyond VLMs, LLMs can also be assessed under complementary agentic pipeline, where external tools convert visualized text and visual information into textual inputs. (iii) Evaluating generation models for visualized-text understanding. Beyond discriminative VLMs, generative models may offer an alternative pathway for visualized-text understanding. To explore this direction, we derive small evaluation set from VISTA-Bench and evaluate Qwen-Image-Edit (Wu et al., 2025) (Appendix Sec. D.2). We manually inspect the generated outputs for correctness. Note that unified multimodal models (UMMs) may process visualized text through direct image generation. Such generative evaluation paradigm provides complementary perspective on cross-modal representation learning and warrants further investigation. 6. Conclusion We launch VISTA-Bench, an interesting and systematic benchmark spanning multimodal perception, reasoning, knowledge, and unimodal understanding, using decoupled visualized-text inputs rather than pure-text queries. Through extensive evaluation of over 20 representative VLMs, we uncover substantial modality gap: models that perform strongly on pure text often degrade markedly when identical semantic content is presented as visualized text, with the gap widening as perceptual difficulty increases. Besides, our experiments reveal that this phenomenon is pervasive across both unimodal and multimodal understanding scenarios, consistently leading to perceptual failures across modalities. In summary, VISTA-Bench establishes principled evaluation and elevates text-as-pixels to crucial challenge, motivating the evolution of textual and visual tokenization toward more unified and robust language representations. 8 VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?"
        },
        {
            "title": "References",
            "content": "An, J., Jiang, L., Luo, J., Wu, W., and Huang, L. Voqa: Visual-only question answering. arXiv preprint arXiv:2505.14227, 2025a. An, X., Xie, Y., Yang, K., Zhang, W., Zhao, X., Cheng, Z., Wang, Y., Xu, S., Chen, C., Zhu, D., et al. Llavaonevision-1.5: Fully open framework for democratized multimodal training. arXiv preprint arXiv:2509.23661, 2025b. Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., Li, M., Li, M., Li, K., Lin, Z., Lin, J., Liu, X., Liu, J., Liu, C., Liu, Y., Liu, D., Liu, S., Lu, D., Luo, R., Lv, C., Men, R., Meng, L., Ren, X., Ren, X., Song, S., Sun, Y., Tang, J., Tu, J., Wan, J., Wang, P., Wang, P., Wang, Q., Wang, Y., Xie, T., Xu, Y., Xu, H., Xu, J., Yang, Z., Yang, M., Yang, J., Yang, A., Yu, B., Zhang, F., Zhang, H., Zhang, X., Zheng, B., Zhong, H., Zhou, J., Zhou, F., Zhou, J., Zhu, Y., and Zhu, K. Qwen3-vl technical report, 2025a. URL https://arxiv.org/abs/2511.21631. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. Cai, Z., Cao, M., Chen, H., Chen, K., Chen, K., Chen, X., Chen, X., Chen, Z., Chen, Z., Chu, P., et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024. Cheng, J., Liu, Y., Zhang, X., Fei, Y., Hong, W., Lyu, R., Wang, W., Su, Z., Gu, X., Liu, X., et al. Glyph: Scaling context windows via visual-text compression. arXiv preprint arXiv:2510.17800, 2025. Diao, H., Cui, Y., Li, X., Wang, Y., Lu, H., and Wang, X. Unveiling encoder-free vision-language models. Advances in Neural Information Processing Systems, 37: 5254552567, 2024. Diao, H., Li, M., Wu, S., Dai, L., Wang, X., Deng, H., Lu, L., Lin, D., and Liu, Z. From pixels to wordstowards native vision-language primitives at scale. arXiv preprint arXiv:2510.14979, 2025a. Diao, H., Li, X., Cui, Y., Wang, Y., Deng, H., Pan, T., Wang, W., Lu, H., and Wang, X. Evev2: Improved baselines for encoder-free vision-language models. arXiv preprint arXiv:2502.06788, 2025b. Duan, H., Yang, J., Qiao, Y., Fang, X., Chen, L., Liu, Y., Dong, X., Zang, Y., Zhang, P., Wang, J., et al. Vlmevalkit: An open-source toolkit for evaluating large In Proceedings of the 32nd multi-modality models. 9 ACM international conference on multimedia, pp. 11198 11201, 2024."
        },
        {
            "title": "Gemini",
            "content": "pro model 2025. https://storage.googleapis. Google. URL com/deepmind-media/Model-Cards/ Gemini-3-Pro-Model-Card.pdf. cal Report. Technicard, Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Hong, W., Yu, W., Gu, X., Wang, G., Gan, G., Tang, H., Cheng, J., Qi, J., Ji, J., Pan, L., et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv preprint arXiv:2507.01006, 2025. Li, B., Ge, Y., Ge, Y., Wang, G., Wang, R., Zhang, R., and Shan, Y. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1329913308, 2024a. Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Zhang, P., Li, Y., Liu, Z., et al. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024b. Li, X., Lu, Y., Gan, Z., Gao, J., Wang, W. Y., and Choi, Y. Text as images: Can multimodal large language models arXiv preprint follow printed instructions in pixels? arXiv:2311.17647, 2023. Li, Y., Lan, Z., and Zhou, J. Text or pixels? it takes half: On the token efficiency of visual text inputs in multimodal llms. arXiv preprint arXiv:2510.18279, 2025. Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines In Proceedings of the with visual instruction tuning. IEEE/CVF conference on computer vision and pattern recognition, pp. 2629626306, 2024a. Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pp. 216233. Springer, 2024b. Liu, Y., Li, Z., Huang, M., Yang, B., Yu, W., Li, C., Yin, X.-C., Liu, C.-L., Jin, L., and Bai, X. Ocrbench: on the VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024c. Lu, S., Li, Y., Chen, Q.-G., Xu, Z., Luo, W., Zhang, K., and Ye, H.-J. Ovis: Structural embedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797, 2024. Lu, S., Li, Y., Xia, Y., Hu, Y., Zhao, S., Ma, Y., Wei, Z., Li, Y., Duan, L., Zhao, J., et al. Ovis2. 5 technical report. arXiv preprint arXiv:2508.11737, 2025. Yu, T., Wang, Z., Wang, C., Huang, F., Ma, W., He, Z., Cai, T., Chen, W., Huang, Y., Zhao, Y., et al. Minicpm-v 4.5: Cooking efficient mllms via architecture, data, and training recipe. arXiv preprint arXiv:2509.18154, 2025. Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Mathew, M., Karatzas, D., and Jawahar, C. Docvqa: In Proceedings dataset for vqa on document images. of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. Zhao, H., Wang, M., Zhu, F., Liu, W., Ni, B., Zeng, F., Meng, G., and Zhang, Z. Vtcbench: Can vision-language models understand long context with vision-text compression? arXiv preprint arXiv:2512.15649, 2025. MiMo. Mimo-vl technical report, 2025. URL https: //arxiv.org/abs/2506.03569. Singh, A., Fry, A., Perelman, A., Tart, A., Ganesh, A., El-Kishky, A., McLaughlin, A., Low, A., Ostrow, A., Ananthram, A., et al. Openai gpt-5 system card. arXiv preprint arXiv:2601.03267, 2025. Team, K., Du, A., Yin, B., Xing, B., Qu, B., Wang, B., Chen, C., Zhang, C., Du, C., Wei, C., et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. Wang, W., Gao, Z., Gu, L., Pu, H., Cui, L., Wei, X., Liu, Z., Jing, L., Ye, S., Shao, J., et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. Wei, H., Sun, Y., and Li, Y. Deepseek-ocr: Contexts optical compression. arXiv preprint arXiv:2510.18234, 2025. Wu, C., Li, J., Zhou, J., Lin, J., Gao, K., Yan, K., Yin, S.-m., Bai, S., Xu, X., Chen, Y., et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. Wu, Z., Chen, X., Pan, Z., Liu, X., Liu, W., Dai, D., Gao, H., Ma, Y., Wu, C., Wang, B., et al. Deepseek-vl2: Mixtureof-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. Xing, L., Wang, A. J., Yan, R., Qu, H., Li, Z., and Tang, J. See the text: From tokenization to visual reading. arXiv preprint arXiv:2510.18840, 2025. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yin, W., Ye, Y., Shu, F., Liao, Y., Kang, Z., Dong, H., Yu, H., Yang, D., Wang, J., Wang, H., et al. Sail-vl2 technical report. arXiv preprint arXiv:2509.14033, 2025. 10 VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?"
        },
        {
            "title": "Appendix",
            "content": "A. Benchmark Details A.1. Complete Category Taxonomy The VISTA-Bench dataset is organized into four primary categories, each targeting distinct capabilities under visualized text. Table 2 provides the hierarchical distribution of samples across categories and sub-domains. Below we describe each category in detail, including instance counts, evaluation dimensions and sub-domain composition. Unimodal Knowledge. This category contains 500 instances (33.33% of the dataset) designed to isolate the models ability to retrieve and apply knowledge from visualized text alone, without reliance on any accompanying images. Instances are generated by transforming textual knowledge bases into visualized text while preserving semantic content. Evaluation spans two main domains: Natural & Life Sciences (276 instances) covering Physics, Mathematics & Statistics (93), Chemistry, Computing & Geography (91) and Biology & Medical Sciences (92); and Social & Applied Sciences (224 instances) including History, Politics & Law (77), Logic, Ethics & Philosophy (62) and Economics, Business & Society (85). This category specifically tests whether performance bottlenecks arise from challenges in decoding high-density visualized text. Multimodal Knowledge. Featuring 400 instances (26.67% of the dataset), this category evaluates how effectively models integrate visual evidence with factual and domain-specific knowledge when processing visualized text alongside the corresponding problem image. The instances are grouped into two primary domains: STEM & Health (228 instances), which include Physical Sciences & Engineering (88), Chemistry & Materials (30) and Medical & Life Sciences (110); and Social-Humanities & Management (172 instances), encompassing Humanities & Arts (47), Economics & Management (79) and Social Sciences (46). This category assesses the models ability to retrieve and apply knowledge in context-rich multimodal settings, measuring how visual grounding affects domain-specific reasoning. Multimodal Perception. This category consists of 300 instances (20.00% of the dataset) designed to probe the models perceptual grounding capabilities under visualized text and problem images. The evaluation is organized into three dimensions: Global Perception (89 instances), assessing scene understanding and coarse-grained context (Coarse Perception 48, Scene Understanding 41); Instance Perception (103 instances), targeting the identification, localization and instancelevel text understanding (Fine-grained Perception 54, Instance Identity 28, Instance Location 20, Text Understanding 1); and Attribute Perception (108 instances), evaluating sensitivity to object or textual attributes (Attribute Reasoning 45, Instance Attributes 63). This setup allows detailed analysis of how well models can capture fine-grained textual and visual information under dense, multimodal inputs. Multimodal Reasoning. Comprising 300 instances (20.00% of the dataset), this category investigates the models reasoning capabilities under visualized text paired with problem images. Evaluation is divided into three dimensions: Logical Reasoning (68 instances), including Logic Reasoning (60) and Visual Reasoning (8); Spatial & Relation (69 instances), covering Relation Reasoning (38), Spatial Relation (28) and Instance Interaction (3); and Cross-Instance (163 instances), focusing on aggregation and association across multiple visual elements (Fine-grained Perception 60, Instances Counting 103). This category measures the models capacity to execute multi-step, pixel-grounded reasoning that integrates information across visualized text and imagery. A.2. Visualized-Text Question Statistics To characterize the visualized-text representations in VISTA-Bench, we provide detailed analysis of the question images across all 1,500 instances. Each question is rendered into high-resolution visualized-text image, capturing the full textual content in pixel-based format. Table 4 summarizes the basic statistics of these images, including height, width and aspect ratio. All images are standardized to fixed width of 800 pixels, while heights vary from 88 to 7,683 pixels, with mean height of 351.9 pixels and median of 187 pixels. The resulting aspect ratios (width divided by height) span from 0.1 to 9.1, reflecting the diversity in text length, layout and formatting across the dataset. Table 3 provides finer-grained distribution analysis. The majority of visualized-text question images (82.9%) have heights below 500 pixels, with only small fraction extending beyond 2,000 pixels. The aspect ratio distribution indicates strong horizontal orientation: over half of the images (52.9%) have width-to-height ratio of 4.0 or greater, whereas vertical or nearly square layouts are rare (2.0% and 4.5%, respectively). These statistics highlight the variability in the visualized-text question layouts in VISTA-Bench, including wide range of VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? heights, aspect ratios and text densities. This diversity is critical for evaluating VLMs ability to decode high-density textual information when presented as visualized pixels, mirroring realistic document and exam-style question layouts. A.3. Examples and Case Studies To provide concrete understanding of VISTA-Bench, we present representative cases spanning different task categories, rendering configurations and modality settings. Samples are shown in Figure 12 through Figure 15 at the end. Table 2. Hierarchical Distribution of Dataset Categories: Comparison of Multimodal and Unimodal Streams Category/Sub-category Count Percentage(%) Category/Sub-category Count Percentage(%) Multimodal Knowledge STEM & Health Phys. Sci. & Eng. Chem. & Materials Med. & Life Sci. Social-Hum. & Manag. Humanities & Arts Econ. & Manag. Social Sciences Multimodal Perception Attribute Perception Attr. Reasoning Inst. Attributes Instance Perception Fine-grained (Inst.) Inst. Identity Inst. Location Text Understanding Global Perception Coarse Perception Scene Underst. 400 228 88 30 110 172 47 79 46 300 108 45 63 103 54 28 20 1 89 48 41 26.67 15.20 5.87 2.00 7.33 11.47 3.13 5.27 3.07 20.00 7.20 3.00 4.20 6.87 3.60 1.87 1.33 0.07 5.93 3.20 2.73 Unimodal Knowledge Natural & Life Sciences Phys, Math & Stat Chem, Comp & Geo Bio & Med Sci Social & Applied Sci. Hist, Pol & Law Logic, Eth & Phil Econ, Bus & Soc Multimodal Reasoning Cross-Instance Fine-grained (Cross) Inst. Counting Spatial & Relation Rel. Reasoning Spatial Relation Inst. Interaction Logical Reasoning Logic Reasoning Visual Reasoning 500 276 93 91 92 224 77 62 85 300 163 60 103 69 38 28 3 68 60 8 33.33 18.40 6.20 6.07 6.13 14.93 5.13 4.13 5. 20.00 10.87 4.00 6.87 4.60 2.53 1.87 0.20 4.53 4.00 0.53 Table 3. Geometric Distribution Analysis of Dataset Image Properties Height Distribution (Pixels) Count Percentage(%) Aspect Ratio (W/H) Count Percentage(%) [0, 500) [500, 1000) [1000, 2000) [2000, 3000) [3000, +) 1244 193 39 15 9 82.9 12.9 2.6 1.0 0.6 < 0.5 (Vertical) [0.5, 1.0) [1.0, 2.0) [2.0, 4.0) 4.0 (Horizontal) 30 67 268 341 794 2.0 4.5 17.9 22.7 52. Table 4. Summary Statistics of Image Dimensions (N = 1500) Dimension Height (px) Width (px) Aspect Ratio (W/H) Min 88.0 800.0 0.1 Max Mean Median Std 7683.0 800.0 9.1 351.9 800.0 3.8 187.0 800.0 4.3 462.1 0.0 1. 12 VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? B. Rendering Pipeline B.1. Pipeline Detailed To ensure high-fidelity and diverse visual representations of textual knowledge, VISTA-Bench adopts rigorous rendering pipeline that transforms discrete linguistic tokens into pixel-based visual inputs. The pipeline is designed to preserve typographic fidelity while simulating the visual characteristics of real-world documents. Text Preprocessing. To achieve semantic accuracy, the pipeline employs multi-layered preprocessing workflow that reconciles raw data with LaTeX constraints through three integrated transformations. This begins with Text Normalization, which standardizes typographic elements like smart quotes and em-dashes into ASCII equivalents to eliminate rendering artifacts and ensure stable input stream. Simultaneously, Code Isolation establishes syntax-shielding barrier, distinguishing descriptive prose from mathematical environments to protect reserved characterssuch as &, % and while strictly preserving the integrity of pre-existing valid instructions. Finally, Formula Synthesis performs symbolic reconstruction, identifying logical operators and Greek letters within the text to dynamically transcode them into high-fidelity macros, while standardizing specialized contexts like temperature units and degree symbols into publication-grade mathematical notation. LaTeX-based Generation. To achieve Visual Harmony, the pipeline utilizes LaTeX as the core engine, implementing Width Anchoring mechanism that mathematically calibrates the target pixel width into physical LaTeX points. By precisely aligning the rendering DPI with the standard typographic point scale, the pipeline locks the horizontal span at 800 pixels, ensuring consistent line-wrapping behavior and geometric stability across all generated samples. Complementing this, Font Mapping dynamically resolves style names into specific OpenType font families, such as Arial or Cambria, while varying font sizes from 9pt to 48pt. This approach allows the pipeline to reflect the typographic diversity of real-world documents while maintaining strictly standardized visual framework. Image Conversion and Post-processing. The final stage of the pipeline executes Precise Extraction, vision-based refinement process that transforms vector documents into visualized-text image assets. This begins with Fidelity Rasterization, which utilizes high-DPI sampling to convert the PDF into pixel-based format, effectively eliminating aliasing and ensuring that the sharpness of fine mathematical symbols is preserved. Subsequently, the system performs Content Localization by translating the image into grayscale matrix and executing pixel-level scan to pinpoint the exact vertical boundaries of the rendered text. Finally, Adaptive Cropping utilizes these detected coordinates to strip away redundant whitespace while automatically applying protective margin to prevent the clipping of character descenders or complex mathematical structures. This rigorous post-processing ensures that every output image is both spatially efficient and semantically complete, providing clean visual signal for model evaluation. B.2. Rendering Configuration To ensure reproducibility, we disclose our optimized rendering configurations. Table 5 presents the Font Mapping distribution, which ensures typographic diversity across 1,500 samples by varying font families and sizes. As shown in Table 6, we identified Width Anchoring at 800 pixels and DPI of 72.27 as the most stable configuration, guaranteeing that digital font sizes precisely align with physical typographic standards. These parameters, combined with strategic margins, prevent character truncation at large font sizes to preserve the Semantic Integrity of the visualized text. Table 5. Rendering font mapping font Arial Times New Roman Cambria Brush Script MT Sum 9(pt) 16(pt) 32(pt) 48(pt) Sum 136 102 91 56 385 134 97 63 47 106 63 72 54 295 186 133 84 76 479 562 395 310 233 1500 Table 6. Other rendering configurations Config Width DPI Left and Right Margin Top and Bottom Margin value 800 72.27 40 13 VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? B.3. VLM Settings To guarantee the rendering precision of VISTA-Bench, we implement VLM-as-Judge protocol that utilizes Rendering Quality Auditor to verify pixel-level fidelity through rigorous auditing process. The System Prompt directs the VLM to evaluate Text Fidelity, Code Integrity and Formula Precision on strict 02 scale, focusing solely on rendering quality rather than semantic correctness. Complementing this, the User Prompt instructs the judge to compare the rendered image against the reference text and provide score with an optional explanation for any detected issues. Under our Acceptance and Refinement Policy, only Flawless instances are admitted into the final dataset, while scores of 1 or 0 trigger an iterative loop where typographic and layout parameters are adjusted until the required rendering standard is achieved. System Prompt (Rendering Quality Auditor).: You are highly precise AI Rendering Quality Auditor specializing in document digitization and visual text verification. Your task is to evaluate the alignment between rendered visualized-text image and its corresponding ground-truth text. You must focus strictly on rendering quality rather than semantic correctness. Specifically, you should evaluate the following aspects: 1. Text Fidelity: whether all characters, words and spacing in the image accurately match the reference text. 2. Code Integrity: whether code snippets preserve symbols, indentation and formatting without omissions or distortions. 3. Formula Precision: whether mathematical expressions, operators and structures are rendered faithfully. Based on the overall alignment, output single alignment score according to the following scale: - 2 (Flawless): Perfect alignment with no observable rendering artifacts. - 1 (Acceptable): Minor imperfections that do not affect readability or interpretation. - 0 (Misaligned): Clear rendering errors that impair correctness or readability. Do not perform reasoning beyond rendering verification. Do not infer missing content. Do not assess semantic correctness. User Prompt (Rendering Verification).: The following input consists of: - visualized-text image generated by rendering pipeline. - The corresponding ground-truth text content. Please compare the image with the reference text and evaluate their alignment according to the specified criteria. Return: - single integer score from 2, 1, 0 indicating the alignment status. - (Optional) brief explanation describing any detected rendering issues, if applicable. Acceptance and Refinement Policy.: Instances receiving an alignment score of 2 (Flawless) are directly accepted into VISTA-Bench. Instances receiving scores of 1 (Acceptable) or 0 (Misaligned) are routed to rendering refinement process, where typographic or layout parameters are adjusted. The refined outputs are re-evaluated using the same VLM-as-Judge protocol until acceptable rendering quality is achieved. Remark. This VLM-as-Judge protocol follows the same principle as prior LLM-as-Judge paradigms, while extending it to the visual modality by explicitly verifying pixel-level rendering fidelity rather than semantic correctness. B.4. Rendering Cases We present the evaluations generated by the VLM-as-Filter-Judge across various rendering outcomes, alongside targeted refinement directions for the pipeline based on these assessments. These cases illustrate how the judges feedback facilitates the iterative optimization of rendering parameters to resolve systematic errors and achieve the required visual fidelity. 14 VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? C. Evaluation Protocol C.1. Prompts Used in Evaluation We defines set of standardized prompts to evaluate VLMs under different levels of instruction granularity. Each prompt is designed to encourage direct answer extraction from visualized-text questions while reducing irrelevant or verbose output. Figure 9. Impact of Prompt Design. Prompt: 10-words, 20-words, 50-words, image understanding and CoT. Prompts vary in length and reasoning emphasis, enabling an analysis of how instruction detail and presentation style impact model behavior, as illustrated below. All prompts are displayed in uniform visual format using the promptbox. 10-words prompt. minimal, highly concise instruction designed to provide only the essential guidance. Models are prompted to read the question and options and respond with the single letter corresponding to the correct answer. This prompt evaluates whether the model can correctly interpret visualized-text questions with minimal context. 20-words prompt. slightly longer prompt that explicitly references the visual context of the question. This format encourages the model to carefully consider the visualized question while still producing concise single-letter response. 50-words prompt. An expanded prompt that emphasizes careful comprehension of each option and the question context. This format tests whether providing more detailed textual guidance improves reasoning and answer accuracy when interpreting visualized-text questions. Image-understand prompt. detailed prompt focusing on full comprehension of the visualized-text question along with any accompanying problem image. It guides the model to interpret context and relationships before selecting an answer, without providing explanations, thereby testing multimodal understanding and attentional alignment. CoT prompt. Designed to induce step-by-step internal reasoning, this prompt instructs the model to consider the visualized-text information sequentially but explicitly discard the reasoning process in the final output. Only the single-letter answer is required. This evaluates whether structured reasoning can improve accuracy without contaminating the response. 10-words prompt: Read the question and options, then answer with only the single letter (e.g., A, B, C, D). 20-words prompt (Original): Read the question and options shown in the image(s). Answer with only the single letter of the correct option (e.g., A, B, C, D). 50-words prompt: Please carefully read the question and options presented in the image(s). Ensure you understand the meaning of each option. Based on the question, choose the most appropriate answer and respond with only the letter of the correct option (e.g., A, B, C, or D). Do not include any additional text or explanations in your response. Image understand prompt: Please take moment to carefully read the question and all available options shown in the image(s). Ensure you fully understand the context and meaning behind each option. After considering the question and options, choose the most appropriate answer. Respond only with the letter corresponding to the correct option (e.g., A, B, C, or D). Do not include any explanations or comments in your response. 15 VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? CoT prompt: Analyze the image to identify the question text and the available options. Think step-by-step to deduce the correct answer based on the visual information provided. However, discard your reasoning process in the final output. Your final response must consist of nothing but the single letter of the correct option (e.g., A). Do not explain why and do not use punctuation. C.2. Model Configurations We evaluate broad suite of open-source VLMs implemented in VLMEvalKit. Unless otherwise specified, we follow the default model wrappers and decoding settings released by each official repository as integrated in VLMEvalKit and run all models in BF16 on NVIDIA A800 GPUs. InternVL series. We evaluate InternVL3 and InternVL3.5 across three scales: InternVL3-2B, InternVL3-8B, InternVL3.52B, InternVL3.5-8B and the MoE model InternVL3.5-30B-A3B. All InternVL models are instantiated via the InternVLChat wrapper (version V2.0) with their corresponding official checkpoints and default generation settings (deterministic decoding by default, with maximum of 4096 new tokens unless overridden by the dataset). The wrapper enforces global image budget of 64 patches and dynamically sets the per-image patch budget based on the number of images; the default maximum number of patches is 6. We make two lightweight wrapper-level modifications for evaluation: (i) in prompt construction, we allow the wrapper to fall back to question image path when image path is absent, so that visualized-text question images are correctly loaded in our MM MCQ setting; and (ii) we handle the text-only edge case (e.g., MMLU) by setting the patch budget when the number of images is zero, avoiding invalid computation and ensuring consistent behavior across unimodal and multimodal evaluations. Qwen-VL series. We evaluate Qwen2.5-VL and Qwen3-VL using their corresponding VLMEvalKit wrappers. For Qwen2.5-VL, we include Qwen2.5-VL-3B-Instruct and Qwen2.5-VL-7B-Instruct. We follow the standard configuration with custom prompts disabled and explicit vision-token constraints, using min pixels = 1280 28 28 and max pixels = 16384 28 28. For Qwen3-VL, we include Qwen3-VL-2B-Instruct, Qwen3-VL-8B-Instruct and Qwen3-VL-30B-A3BInstruct, instantiated under the Instruct setting with custom prompts disabled. We use the default generation configuration in our setup (temperature = 0.7, top-p = 0.8, top-k = 20, repetition penalty = 1.0, presence penalty = 1.5 and max new tokens = 16384). All models are evaluated with the same protocol; when vLLM is enabled by the wrapper, the number of image inputs per prompt is capped at 24. LLaVA series. We evaluate three representative LLaVA variants: LLaVA-1.5-7B, LLaVA-OneVision-7B and LLaVAOneVision-1.5-8B. For LLaVA-1.5-7B, we do not rely on the original LLaVA codebase; instead, we use the official HuggingFace Transformers implementation (processor + LlavaForConditionalGeneration) to ensure unified and reproducible inference pipeline. We follow deterministic decoding (no sampling) with maximum generation length of 2048 tokens (temperature = 0, single-beam). For LLaVA-OneVision, we use the VLMEvalKit wrapper based on the official LLaVA-OneVision implementation, keeping its default prompt template and decoding configuration. Since LLaVAOneVision-1.5 is not natively supported in VLMEvalKit, we implement lightweight wrapper following the evaluation procedure released by lmms-lab for OneVision-1.5, including the same chat-template construction and image-token alignment behavior in the processor. This implementation is compatibility layer and does not modify model weights; therefore, it is not expected to introduce material difference in the reported results beyond standard evaluation variance. MiMo-VL series. We evaluate MiMo-VL-7B-SFT and MiMo-VL-7B-RL. Both models are instantiated via the same Qwen2VLChat wrapper and configuration as Qwen2.5-VL (custom prompts disabled, min pixels = 1280 28 28, max pixels = 16384 28 28), ensuring consistent evaluation settings. Ovis series. We evaluate Ovis2-2B, Ovis2-8B, Ovis2.5-2B and Ovis2.5-9B using the official Ovis wrappers in VLMEvalKit. For Ovis2, we use the standard Transformers backend provided by the wrapper (AutoModelForCausalLM with trust remote code=True) and keep deterministic decoding with the default maximum generation length (1024 new tokens) and the models built-in preprocessing (preprocess inputs) under fixed multimodal context length budget. The wrapper also sets the image partitioning strategy according to the dataset modality and the number of images to control memory usage. For Ovis2.5, the original implementation is designed to run with vLLM backend; however, we did not have an appropriate vLLM deployment for the released checkpoints in our environment. We therefore implement Transformers-based inference path as drop-in replacement, while preserving the core evaluation logic used by the vLLM version: (i) we keep the same prompt construction (including the fixed suffix for extracting the final answer), (ii) we use the 16 VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? model-provided preprocess inputs interface with the same min pixels/max pixels settings (448448 for OCR-style inputs and 10241024 to 17921792 otherwise) and (iii) we follow the same thinking enablement rules exposed by the wrapper for the relevant benchmark subsets. Since both backends ultimately invoke the same model weights and preprocessing pipeline with deterministic decoding, this change is intended to be compatibility layer and is not expected to materially alter the reported results beyond standard inference variance. SAIL-VL2 series. We evaluate SAIL-VL2-2B and SAIL-VL2-8B via the SailVL wrapper. The wrapper uses dynamic image preprocessing with fixed vision input size (448) and patch-based tiling strategy, where the maximum number of tiles is controlled by per-dataset image budget (up to 10 by default and increased to 12/18/24 for text-dense benchmarks such as OCRBench and HRBench). The total patch budget is capped at 64 to avoid OOM. We follow deterministic decoding with the wrapper-default maximum generation length (1024 new tokens). In our setting, SAIL-VL2 is configured with MSAC enabled for single-image inputs (use msac=True) to improve coverage of text-dense renderings. For compatibility with our visualized-text question format, we apply the same wrapper-level I/O handling as in InternVLChat, ensuring that question-image fields are correctly loaded and that text-only edge cases are handled consistently. NEO series. We evaluate NEO-2B-SFT and NEO-9B-SFT via the NEOChat wrapper. NEO uses native patch-based image tokenizer with patch size 16 and downsample ratio of 0.5, together with explicit pixel-range constraints (min pixels = 1280 32 32, max pixels = 4096 32 32) to control the visual token budget. The wrapper adopts deterministic decoding by default with maximum generation length of 4096 new tokens and adjusts the per-example image budget using global cap of 64 and dataset-specific maxima (6 by default, increased to 12/18/24 for text-dense benchmarks such as OCRBench and HRBench). For OCRBench, the wrapper further relaxes the minimum pixel threshold to 256256 to better accommodate small text regions. We apply the same wrapper-level I/O handling described for InternVLChat to ensure that visualized-text question-image fields are correctly loaded and that text-only edge cases are handled consistently. Other models. We additionally evaluate DeepSeek-VL2-Tiny, GLM-4.1V-9B-Thinking, MiniCPM-V-4.5 and Kimi-VLA3B-Thinking-2506 using their corresponding VLMEvalKit wrappers and default settings. For Kimi-VL-A3B-Thinking2506, we follow the wrapper configuration with temperature 0.8 and maximum generation length of 32,768 tokens. Remarks. For fair comparison, we do not hand-tune prompts or decoding per model beyond wrapper defaults and all models are evaluated under the same protocol. When wrapper exposes explicit image-token constraints (e.g., minimum/- maximum pixels), we keep the default values to match the intended usage of the official implementation. C.3. Answer Extraction and Post-processing We follow the VLMEvalKit evaluation pipeline for prediction aggregation and answer parsing. For each instance, the model output is stored as raw string and then post-processed for scoring. Multiple-choice answer parsing. For MCQ-style tasks, we infer the final option letter using robust parser. We first normalize the model output by uppercasing and removing common punctuation (e.g., parentheses and brackets), then tokenize by whitespace. If exactly one option letter from the candidate set (typically {A, B, C, D}) appears, we accept it as the prediction only when it occurs near the end of the output (within the last five tokens), which reduces false matches from intermediate reasoning. If no option letter is detected, we apply fallback that matches the option text content: when the output contains exactly one candidate option string (case-insensitive) and the output length is not excessively longer than the total option text length, we map it back to the corresponding letter. For ordering-style MCQ, we additionally support sequence extraction by matching contiguous four-letter pattern (ABCD) or step-wise descriptions (first/second/third/fourth). Refusals and invalid outputs. If the output contains explicit refusal or failure patterns, such as API failure messages or Cannot determine the answer-style responses, we mark it as invalid (mapped to sentinel label) and count it as incorrect. Outputs that do not yield valid option letter after the parsing steps above are also treated as invalid. Result aggregation. We run inference with distributed workers and aggregate per-rank outputs into single prediction file indexed by sample id. When enabled, we optionally split reasoning traces from final answers by extracting text after the closing </think> tag, while keeping the final prediction string for evaluation. 17 VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? D. Additional Experiments D.1. Rendering Factor Ablations We extend the fine-grained rendering sensitivity study from GLM-4.1V-9B-Thinking to eight additional representative models, as shown in Figure 11. For each model, we evaluate performance under Pure Text and Visualized Text while varying font size (9pt, 16pt, 32pt, 48pt) and font style (Arial, Cambria, Roman, Brush) and report the modality gap as AccText AccVT. Two consistent patterns emerge. First, perceptually difficult renderings, especially very small font sizes and Brush-style text, lead to markedly larger gaps across most models, indicating persistent vulnerability to reduced legibility and rendering artifacts. Second, cleaner renderings substantially reduce the gap for many models and small subset even becomes nearly gap-free or shows slight gains under Visualized Text; however, several models still exhibit large residual gaps under standard settings, suggesting limitations beyond perceptual readability. D.2. Qwen-Image-Edit Evaluation To examine whether generative multimodal models can be evaluated under the visualized-text paradigm, we conduct preliminary study with Qwen-Image on 200-instance subset of VISTA-Bench. Specifically, we randomly sample 50 instances from each of the four tasks (multimodal tasks and unimodal task), resulting in balanced evaluation set. In this setting, both the question and options are rendered as visualized text and, together with the problem image, are provided as unified visual input. The model is instructed to write its final answer directly into designated region of the output image. Due to the open-ended and image-based nature of the generation process, all outputs are assessed via human evaluation. For each generated result, annotators verify (i) generation validity: whether readable question/answer is produced in the specified region and (ii) answer correctness: whether the final answer is correct. This protocol enables reliable judgment of both layout-controlled generation and task performance, which is not robustly captured by automatic string matching. Across 200 evaluated instances, 149 yield valid generations in which the model produces readable question/option text in the designated region and outputs final answer (see Figure 10 for representative example), while 50 are labeled as no question due to failures to generate readable question or answer; one additional sample shows minor encoding artifacts and is excluded. Considering all 200 samples, Qwen-Image-Edit achieves an overall accuracy of 22.5% and the accuracy remains similar at 22.15% when restricted to the 149 valid samples. The small difference suggests that end-to-end performance is limited not only by generation stability under constrained layouts, but also by the difficulty of correctly extracting semantics and reasoning from pixel-level text even when the visualized text is successfully produced. Overall, these results indicate that generative multimodal models such as Qwen-Image can naturally interface with VISTABench and be evaluated in an end-to-end visualized-text setting. However, their performance is jointly constrained by generation robustness and downstream visualized-text understanding. More broadly, this experiment suggests that visualized text introduces challenges beyond conventional OCR or text-conditioned generation, further motivating VISTA-Bench as meaningful testbed for studying vision-centric language understanding in next-generation multimodal systems. Figure 10. successful Qwen-Image-Edit case under the visualized-text setting. The model correctly generates readable visualized text in the designated region and produces the correct final answer. VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? Figure 11. Rendering sensitivity study on eight additional representative models. 19 VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? Figure 12. Visualized examples for Multimodal Perception task. Top: Attribute Perception (Times New Roman, 9pt). Middle: Global Perception (Brush Script MT, 32pt). Bottom: Instance Perception (Times New Roman, 16pt). VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? Figure 13. Visualized examples for Multimodal Reasoning task. Top: Logical Reasoning (Arial, 16pt). Middle: Spatial & Relation (Cambria, 32pt). Bottom: Cross-Instance (Cambria, 48pt). 21 VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? Figure 14. Visualized examples for Multimodal Knowledge task. Top: STEM & Health (Arial, 32pt). Middle: Social-Humanities & Management (Cambria, 9pt). Bottom: STEM & Health (Brush Script MT, 9pt). VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? Figure 15. Visualized examples for Unimodal Knowledge task. Top: Applied Sciences & Social (Times New Roman, 48pt). Bottom: Natural & Life Sciences (Brush Script MT, 32pt). 23 VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? Figure 16. Mathematical formula rendering error. Config: Arial, 16pt VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? Figure 17. Code structure rendering error.Config: Arial, 9pt 25 VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? Figure 18. Handwritten font rendering error.Config: Brush, 32pt VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? Figure 19. Top: Rendering correct example.Config: Arial, 32pt. Bottom: Rendering correct example.Config: Cambria, 16pt"
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University, Singapore",
        "School of Artificial Intelligence, Dalian University of Technology, Dalian, China"
    ]
}