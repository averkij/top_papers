{
    "paper_title": "AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios",
    "authors": [
        "Kaiyuan Chen",
        "Qimin Wu",
        "Taiyu Hou",
        "Tianhao Tang",
        "Xueyu Hu",
        "Yuchen Hou",
        "Bikun Li",
        "Chengming Qian",
        "Guoyin Wang",
        "Haolin Chen",
        "Haotong Tian",
        "Haoye Zhang",
        "Haoyu Bian",
        "Hongbing Pan",
        "Hongkang Zhang",
        "Hongyi Zhou",
        "Jiaqi Cai",
        "Jiewu Rao",
        "Jiyuan Ren",
        "Keduan Huang",
        "Lucia Zhu Huang",
        "Mingyu Yuan",
        "Naixu Guo",
        "Qicheng Tang",
        "Qinyan Zhang",
        "Shuai Chen",
        "Siheng Chen",
        "Ting Ting Li",
        "Xiaoxing Guo",
        "Yaocheng Zuo",
        "Yaoqi Guo",
        "Yinan Wang",
        "Yinzhou Yu",
        "Yize Wang",
        "Yuan Jiang",
        "Yuan Tian",
        "Yuanshuo Zhang",
        "Yuxuan Liu",
        "Yvette Yan Zeng",
        "Zenyu Shan",
        "Zihan Yin",
        "Xiaobo Hu",
        "Yang Liu",
        "Yixin Ren",
        "Yuan Gong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of a broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete a diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution, which assesses adherence to explicit and complex workflows; Latent Instruction, which requires agents to infer implicit instructions from attachments; and Iterative Refinement, which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities, enabling AI application teams to develop cutting-edge Agent products."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 3 ] . [ 2 3 1 6 0 2 . 1 0 6 2 : r AgentIF-OneDay: Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios xbench.org The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution, which assesses adherence to explicit and complex workflows; Latent Instruction, which requires agents to infer implicit instructions from attachments; and Iterative Refinement, which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities, enabling AI application teams to develop cutting-edge Agent products. Code: github.com/xbench-ai Data: huggingface.co/xbench-ai Correspondence: team@xbench.org"
        },
        {
            "title": "1 Introduction",
            "content": "AI agents driven by Large Language Models (LLMs) have recently exhibited remarkable progress in autonomous reasoning, planning, and tool usage. Although agents have achieved success in handling increasingly complex tasks within vertical domains like vibe coding (Jimenez et al., 2023; Zan et al., 2025), deep research (Wei et al., 2025; Chen et al., 2025) and complex problem solving (Phan et al., 2025), these advancements are not yet fully palpable to ordinary users. We posit that current evaluations lack the necessary diversity to reflect real-world scenarios across work, life, and learning. Addressing this gap, we propose AgentIF-OneDay, framework to assess the efficacy of AI agents in assisting general users with daily tasks through language instructions. AgentIF-OneDay demands the delivery, analysis, and modification of comprehensive work outputs, structured around three key dimensions: * Equal contribution. 1 1. Open Workflow Execution: These tasks assess the agents ability to execute automated processes precisely and exhaustively when the user provides clear and detailed operational procedure. This tests the agents robustness in long-context processing, avoidance of \"instruction forgetting,\" and suppression of hallucinations. 2. Latent Instruction Inference: This dimension requires the agent to autonomously induce and infer implicit, unstated general rules or constraints from task attachments and apply them to new tasks. 3. Iterative Refinement: These tasks simulate multi-turn, progressive collaborative process between the user and the agent. The agent must perform precise content updates and adjustments based on supplementary or corrective instructions from the user upon existing outputs, measuring the efficacy of its state maintenance and human-machine collaboration. We control the distribution of topics in the evaluation set to balance alignment with authentic user needs and maximization of task diversity. The former includes common demands such as daily office software usage, life planning, and research, helping us understand the correlation between agent capabilities and real-world value. The latter primarily increases the difficulty of \"hack\" evaluation metrics through out-of-domain tasks, including game strategy research and the automation of scientific research workflows. Beyond manually annotated tasks, we designed File-centered Automated Agentic Pipeline for task generation. We utilize the ChatGPT agent to collect information-dense attachments with high potential for question generation. Using human-annotated tasks as seeds, we extract and augment workflow frameworks. Finally, we synthesize the attachments and workflows to produce AgentIF-OneDay evaluation items that are logically coherent and deeply correlated with the attachment information. Furthermore, this methodology is extensible to broader agentic data synthesis applications. We employ Instance-level rubrics for evaluation and have enhanced the consistency between LLM-based Verifiers and human scoring through pipeline design. For instance, we implemented visual parsing for PPT and HTML files and utilized Vision-Language Models (VLMs) as verifiers. Additionally, for rubrics requiring real-time verification, the model defaults to Search Mode. The utilization of superior multimodal foundation models has significantly improved the accuracy of the discriminator. When employing Gemini-3-Pro as the judge model, the consistency between AgentIF-OneDay scoring points and human scoring reaches 80.6%. AgentIF-OneDay comprises 104 tasks and covers 767 scoring points. Through systematic testing and analysis of mainstream AI agent products on this benchmark, we aim to reveal the strengths and bottlenecks of current agent products and agentic models, providing empirically valuable references for the future optimization of AI agents. Simultaneously, the high-quality, finely annotated instruction data accumulated by AgentIF-OneDay holds significant potential as training data for reinforcement learning."
        },
        {
            "title": "2 Related Work",
            "content": "Instruction Following Benchmarks. Instruction following benchmarks are crucial for evaluating the ability of large language models to execute tasks based on specific instructions. For instance, IFEval (Zhou et al., 2023) assesses models capability using set of verifiable instructions to ensure accuracy. Another notable benchmark, FOFO (Xia et al., 2024), classifies tasks into various domains and imposes specific format requirements, such as the Prescription Format Standard, to test models precision. To further push the boundaries of evaluation, ComplextBench (Wen et al., 2024) generates instructions through compositional approach, which is specifically designed to enhance the assessment of models understanding of more complex commands. Agent Benchmarks. Agent benchmarks are designed to evaluate the ability of AI agents to perform complex, real-world tasks that often require combination of skills (Liu et al., 2023; Hu et al., 2024). GAIA (Mialon et al., 2023), for example, assesses an agents reasoning and tool-using capabilities by having it complete variety of real-world tasks. Similarly, VisualWebArena (Koh et al., 2024) focuses on an agents ability to execute tasks specifically within web environment, which is common and important domain. Additionally, tau-Bench (Yao et al., 2024) provides another dedicated framework for evaluating models capacity for effective tool usage. 2 Evaluation on Real-World Economic Tasks. Recent research has shifted focus from general abilities to evaluating AIs potential for economic labor automation. Handa et al. (2025) analyzed millions of user conversations, revealing that AI usage is predominantly concentrated in software development and writing tasks, with 57% of interactions focusing on augmenting human capabilities rather than full automation. Similarly, Chatterji et al. (2025) examined consumer usage patterns of ChatGPT, finding that while non-work interactions are growing, the model provides significant economic value as decision-support tool for knowledge-intensive tasks. To rigorously assess professional capabilities, Patwardhan et al. (2025) introduced GDPval, benchmark evaluating AI on long-horizon, multi-modal tasks across 44 occupations. Complementing this, Mazeika et al. (2025) proposed the Remote Labor Index (RLI) based on real-world freelance projects, finding that current frontier agents achieve only 2.5% automation rate on these complex, end-to-end workflows. Focusing on the growth trajectory of these capabilities, Kwa et al. (2025) introduced the 50%-task-completion time horizon metric, observing that this capability horizon doubles approximately every seven months, projecting that AI could automate month-long software tasks within five years. Reinforcement for Instruction Following. Reinforcement learning has emerged as powerful paradigm for training models to follow complex instructions by leveraging feedback signals. VerIF (Peng et al., 2025) uses both soft and hard verifiable signals to provide direct rewards for correctly executed instructions. Ren et al. (2025) employs self-supervised RL to tackle multi-constraint instructions. This technique works by decomposing complex instructions into simpler ones with an incrementally increasing number of constraints, which in turn generates preference pairs for instruction following."
        },
        {
            "title": "3 AgentIF-OneDay",
            "content": "Current evaluations of instruction following predominantly focus on individual models, chatbots, or the isolated agentic capabilities of foundation models. Conversely, assessments targeting complete agent systems tend to be confined to vertical domains. However, as general-purpose agent products undergo rapid iteration, both their capability boundaries and user expectations are evolving dynamically. To address this, we introduce AgentIF-OneDay, benchmark characterized by the following core features: 1. End-to-End Task Completion Assessment: Each task involves not only multimodal input/output (text and images) but also includes attachments that mirror real-world requirements. We impose no constraints on the architecture of the evaluated system; instead, the primary criterion for success is whether the delivered outcome strictly adheres to the instructions. 2. Alignment with Real-World General Agent Usage and Long-Tail Coverage: AgentIF-OneDay is domain-agnostic. We collected authentic requirements from broad spectrum of general agent users, encompassing daily office routines, personal hobbies, and specialized professional tasks. This approach ensures that evaluation scores effectively align with user value, while the inclusion of long-tail tasks mitigates the risk of overfitting or gaming the benchmark. Furthermore, capabilities such as image editing and in-depth researchnow common in chatbot applicationsare included if they constitute the task deliverable. Crucially, we do not prescribe the method of solution; for instance, image editing may be accomplished via software manipulation rather than solely relying on text-to-image generation models. 3. Instance-Level Rubrics with Workflow-Based Judging: Evaluating AgentIF-OneDay requires the judge to comprehend diverse file formats and utilize tools for factual verification. The advancement of multimodal foundation models has significantly enhanced the efficacy of automated judges. We anticipate that Workflow Judges and Judge Agents will become increasingly prevalent standard practices in future evaluations."
        },
        {
            "title": "3.1 Task Categories",
            "content": "We categorize instructions into three types based on how users interact with general AI agents: Open Workflow Execution, Latent Instruction Inference, and Iterative Refinement. We introduce each type in sequence and present example questions in Figure 1. Open Workflow Execution evaluates an agents ability to automate tasks precisely and exhaustively when 3 Figure 1 AgentIF-OneDay categorizes tasks into three types based on how users interact with agent products. presented with explicit, detailed operational procedures. The core competency lies in managing long-context information and strictly adhering to instruction sequences, requiring the agent to overcome instruction forgetting or hallucinations to faithfully reconstruct user-defined steps. For instance, in the NeurIPS 2025 Travel Planning task, the user dictates rigorous five-step workflow rather than simple query. The agent must first verify the San Diego Convention Center via the official website and cross-reference with third-party sources. Subsequently, after confirming dates and submission deadlines, it must check for the detailed schedule before finally generating economical and fast travel options from New York based on this verified data. This process demands not only information retrieval but also faithful execution of the verify-then-plan logical chain, ensuring every output is grounded in the precise execution of preceding steps. Distinct from explicit instruction following, Latent Instruction Inference (or context-based conditional reasoning) focuses on an agents ability to mine implicit logic from provided materials and generalize it to new tasks. In real-world scenarios, users often provide reference documents or case studies rather than explicit formulas, requiring the agent to comprehend and apply latent constraints. For example, in the iPhone 17 Pro Max Purchase Decision task, the user provides only their current device model (iPhone 13 Pro Max) and carrier status, requesting the scheme with the lowest total cost. The agent cannot derive the answer directly from the prompt; it must analyze the attached iPhone_Plan.pdf to deduce complex pricing logicintegrating the base device price, trade-in values for specific models, and varying carrier plan costs. The agent must accurately identify the trade-in value of the users current device from the table and perform calculations based on the plan duration. This task rigorously tests the agents depth of understanding of unstructured information and its reasoning capabilities under implicit constraints. Iterative Refinement simulates the incremental modification scenarios common in human-machine collaboration, assessing the agents ability to refine and optimize existing outputs based on specific constraints and feedback. Rather than generating content from scratch, the agent must perform operations such as adjustment, recalculation, and layout optimization while maintaining the current state, which demands strong state consistency management. In the example task shown, the user provides floor plan file (venue_layout.svg) and constraints file (venue_constraints.xlsx). The agent faces complex optimization challenge: it must update the venue layout to satisfy all hard constraints listed in the Excel filesuch as visibility, proximity, separation, size preference, and vacancy requirementswhile keeping the design readable and walkable. Success 4 Figure 2 Evaluation pipeline in AgentIF-OneDay. depends on the agents ability to process multi-modal inputs (SVG and Excel), accurately interpret specific constraints, and seamlessly integrate these rules into the existing layout to deliver valid and functional final design."
        },
        {
            "title": "3.2 Evaluation\nAgentIF-OneDay employs an instance-level, rubric-based scoring mechanism that assigns specific verification\nchecks to the conditions and tasks within each problem. This system is characterized by three key properties:",
            "content": "(1) Binary Scoring, where each rubric item is strictly evaluated as satisfied or not to ensure objectivity and mitigate LLM judging bias; (2) Distinction between Bonus and Penalty Items, separating the assessment of capability (bonus items for key requirements) from error rates (penalty items for critical mistakes), reflecting the distinct tolerance levels for system capabilities versus failures; (3) File-Content Alignment, enabling the direct evaluation of agent-generated files alongside textual output. The final scoring aggregates these components. Let be the total number of problems. For the i-th problem, let S+ points and as Smax and normalizing against the maximum: represent the sum of satisfied bonus the sum of triggered penalty points. The maximum achievable score for the problem is denoted . The normalized score for problem i, denoted as si, is calculated by clamping the net score at zero The final AgentIF-OneDay score is the mean of these normalized scores across all problems: si = ) max(0, S+ Smax Scorefinal = 1 (cid:88) i=1 si (1) (2) As shown in Fig 2, we adopt an LLM-as-judge approach, leveraging the rubrics provided in each task for scoring. For each rubric, the LLM only determines whether it is satisfied, and the final score is computed by summing or subtracting the points associated with the corresponding rubrics and performing normalization. Since some tasks rely on facts that may change over time, we utilize web search for factual verification. Specifically, for each task, the LLM decomposes it into facts that need verification, which are then grounded with Google Search 1. For tasks that require writing HTML code, we render the HTML to provide clear visual perception, thus avoiding the need for LLMs to directly read complex HTML code to perform scoring. Prompts for evaluation are provided in the Appendix A.2. 5 Figure 3 Statistical overview of AgentIF-OneDay: (a) task category distribution, (b) rubric score polarity, (c) domain distribution, (d) file extension frequency, (e) attachment count per task, and (f) rubric categories."
        },
        {
            "title": "3.3 AgentIF-OneDay Statistics\nFigure 3 presents a comprehensive statistical overview of the AgentIF-OneDay benchmark. As shown in the\ntask category distribution (Figure 3(a)), the workflow is dominated by Open Workflow Execution (53.8%),\nfollowed by Latent Instruction Inference (25.0%) and Iterative Refinement (21.2%). The domain distribution\n(Figure 3(c)) illustrates that the tasks are primarily centered around Work scenarios (59.6%), with significant\nrepresentation in Study (23.1%) and Life (17.3%) domains, ensuring a balance between professional, academic,\nand personal user needs.",
            "content": "Reflecting the complexity of the tasks, Figure 3(e) shows the distribution of attachments per question. While plurality of tasks involve 0 Files (42 tasks) or 1 File (39 tasks), notable subset requires handling multiple attachments, with some tasks involving up to 10 files, testing multi-file reasoning capabilities. As detailed in Figure 3(d), the dataset encompasses wide variety of file extensions: .pdf (31) and .png (19) are the most frequent, but the inclusion of diverse formats such as code files (.html, .py), structured data (.xlsx, .csv), and media (.jpg, .srt) rigorously tests the agents multimodal tool utilization. To ensure granular evaluation, the benchmark utilizes robust rubric system. Figure 3(b) shows the rubric score distribution, where the majority of criteria are Positive (72.8%), focusing on successful task completion, while 27.2% are Negative, likely testing for constraints and safety adherence. The categorical breakdown in Figure 3(f) highlights the specific areas of evaluation: Content is the most heavily weighted category with 385 points, followed by Form (244 points) and Execution (138 points), ensuring holistic assessment of the agents output quality and procedural accuracy. 1https://ai.google.dev/gemini-api/docs/google-search 6 Figure 4 Workflow of synthesizing instances in AgentIF-OneDay."
        },
        {
            "title": "4.1 Collection",
            "content": "We developed detailed annotation guideline for collecting instances from human labelers. Each labeler was asked to submit original questions within their verified area of expertise, following detailed guidelines emphasizing that questions should be: (1) Difficult: not answerable by non-experts, (2) Objective: with single, unambiguous correct answer that domain experts would agree upon, and (3) Search resistant: with correct answers not readily retrievable via standard search engines. Submitted questions underwent multi-stage review pipeline: (1) Initial screening: Editorial review for formatting, clarity, and adherence to guidelines; (2) Expert validation: Review by at least one additional domain expert (distinct from the original author) who attempted to answer the question and provided feedback on its quality; (3) Revision: Author revisions based on feedback from the validators to improve clarity or appropriate difficulty; (4) Final review: quality check by an independent editorial team before inclusion. Questions were rejected at any stage if they were found to be ambiguous, too easy (i.e., answerable via simple web search), or lacking clear, correct answer."
        },
        {
            "title": "4.2 Data Synthesis and Rewriting",
            "content": "We designed an automatic synthetic data generation pipeline to expand the dataset by leveraging high-quality, human-authored seed tasks, but reduce cost, as shown in Fig. 4. This process automatically generates new problems and corresponding evaluation criteria that share the same workflow as the seed tasks but are instantiated in different application scenarios. The synthesized problems, attachments, and rubrics undergo rigorous human review and refinement process. The final synthetic tasks are evaluated using the same scoring methodology as the human-authored tasks. We used Gemini-2.5-Pro during the synthesizing. The pipeline operates as multi-stage process anchored by the extracted workflow driven by diverse humanauthored seed tasks. This workflow acts as the structural backbone, which eventually transforms single human-authored task into diverse set of synthetic tasks combined with attachments. The detailed prompts for each module are provided in Appendix A.1. 7 Step 1: Workflow Extraction. The process begins by analyzing high-quality human seed task to isolate the underlying logic. LLMs are instructed to analyze the seed task to extract the logical sequence of operations required for completion, including the steps, required inputs, expected outputs, and dependencies between steps. This abstract workflow serves as the structural backbone for all subsequent generations. Step 2: Attachment Searching. To ensure that the synthetic tasks are grounded in realistic contexts, the pipeline procures and analyzes domain-specific supplementary materials. Based on the functional requirements of the extracted workflow, diverse search queries (e.g., detailed financial report dashboard or medical invoice template) are generated to retrieve relevant documents and images from the web with Search mode in ChatGPT 2. The retrieved attachments are then analyzed by ChatGPT, which summarizess the content, identifies key data points, and determines the suitability of the files for specific task types, ensuring that the visual data align with the intended logic. Step 3: Query Generation. Using the fixed workflow template and the analyzed attachments, the pipeline generates specific task instructions. New task instructions are generated to preserve the original logical workflow while introducing new content, domains, and contexts. Crucially, specialized prompts are employed to guide the LLM in producing tasks that align with the specific characteristics of the three core categories, ensuring that the synthetic data preserves the cognitive demands of the original human-designed categories. Step 4: Rubrics Generation. Rubrics are formulated by strictly following human annotation guidelines to construct Bonus Criteria and Penalty Criteria. The rubric adheres to the principles of one-score-per-point and independent verification. Step 5: Filtering and Rewriting. To ensure the automatically generated tasks meet the high standards of AgentIF-OneDay, we implement strict filtering and manual refinement process. We specifically filter for complex tasks with more than 3 steps and apply the following optimization criteria: (1) Answer Verifiability: The task must yield clear, measurable answer (e.g., specific numerical value, generated file, or defined output string). Vague requirements (e.g., Analyze market trends) are discarded or concretized (e.g., Calculate the year-over-year growth rate); (2) Spatio-Temporal Independence: Instructions relying on relative time or location (e.g., last week, nearby) are replaced with fixed data ranges, specific timestamps, or absolute locations (e.g., Analyze stock changes from JanJun 2024) to ensure the task remains valid over time; (3) Data Source Validity: All referenced URLs, documents, or data sources are verified for existence and relevance. Tasks relying on non-existent resources are either fixed with reliable alternatives or discarded; (4) Security and Ethics: Instructions requiring sensitive operationssuch as account registration, logging in, or accepting cookie policiesare removed to prevent security risks and platform dependencies; (5) Rubric Alignment: The generated rubrics are manually reviewed to ensure the distinction between Bonus and Penalty items is clear. Evaluation emphasizes final, verifiable artifacts, while intermediate actions (e.g., successfully visiting website) are not scored."
        },
        {
            "title": "5.1 Setup",
            "content": "We selected four advanced and popular agents for testing to comprehensively evaluate their performance in complex tasks and diverse scenarios. All agent platforms were accessed and configured in December 2025. These agents represent leading generative AI technology and Agent design paradigms currently available on the market: (1) ChatGPT-Agent3, (2) Genspark4, (3) Manus5, and (4) Minimax-Agent6. We selected Gemini-3-Pro-preview as the judge model, with max_tokens set to 100,000 and temperature set to 0.1. 2All web content was accessed in November 2025. 3https://chatgpt.com/ 4https://www.genspark.ai/ 5Manus 1.5, https://manus.im/ 6Pro mode, https://agent.minimaxi.com/"
        },
        {
            "title": "5.2 Results",
            "content": "Table 1 presents the main results of our evaluation across the four selected agents. The performance is gauged by the overall success rate across all tasks. The top performing agent is Manus, achieving an overall score of 0.645. Genspark and ChatGPT-Agent follow closely, with scores of 0.635 and 0.626, respectively, forming cluster of high-performing agents. Minimax-Agent lags behind with 0.562. Performance Parity Between Agents Developed by Foundation Model Companies Natively and API-Driven Application Companies The results demonstrate crucial performance parity: agents built purely upon leading LLM APIs (relying on prompt engineering and external tooling) achieve success rates comparable to those of custom Agent RL-based systems. This equivalence indicates that the baseline agentic competence has become commodity, effectively integrated into modern foundational models. Future competitive edge will likely shift from building core agentic skills to optimizing products for specific user needs and leveraging user data to refine the overall agent experience. Table 1 Agent Performance Summary across Key Metrics. Agent Manus Genspark ChatGPT-Agent Minimax-Agent Overall Score 0.645 0.635 0.626 0.562 Avg. Latency (s) 500.0 484.1 665.1 1416.2 Rubric Attachment Inst. Following Factuality Logic/ Func. Neg. Const. w/ Att. Score w/o Att. Score 0.762 0.766 0.739 0.709 0.731 0.663 0.687 0.623 0.693 0.720 0.673 0. 0.529 0.824 0.824 0.588 0.646 0.691 0.666 0.603 0.644 0.551 0.566 0.502 Instruction Following Latent Iter. Inst. Ref. Open Work. 0.646 0.681 0.689 0.717 0.610 0.719 0.613 0.510 0.661 0.577 0.606 0."
        },
        {
            "title": "5.3 Analysis\nWhat are the specific strengths of different agents? In terms of task domains, ChatGPT stands out as the\npremier productivity tool, Manus as the ultimate life assistant, and Genspark as the ideal learning partner,\nshown in Table 2.",
            "content": "These three products pursue different iteration paths. The ChatGPT-Agent prioritizes \"GDPval\" focusing on the user experience within professional work scenarios. In contrast, Manus and Genspark place greater emphasis on user feedback. These distinct evaluation approaches have resulted in different product strengths and weaknesses. We believe that an excellent general-purpose Agent should balance wide variety of tasks rather than favoring one specific area. Table 2 Agent Performance Ranking in Work, Life, and Study Work Agent Score ChatGPT-Agent 72.18 Life Agent Manus Score 73. Study Agent Genspark Genspark 71.86 ChatGPT-Agent 69. Manus Score 71.19 64.41 Manus 70. Genspark 67.85 ChatGPT-Agent 59.29 1st 2nd 3rd Regarding capability dimensions, Genspark performs best in implicit instruction inference, Manus excels in open workflow execution, and the Minimax-Agent possesses the best iterative editing capabilities, shown on the right side of Table 1. The variation in performance across capability dimensions likely stems from differences in Agent frameworks. Implicit condition inference is currently the weakest capability across agents generally. Some tasks require the Agent to automatically identify formatting rules from attachmentssuch as extracting header/footer structures or citation styles from PPT templateand apply them to new content generation. We observed 9 that even the best-performing systems struggle to be completely accurate in these tasks. They either replicate the format correctly but with insufficient coverage, or they understand the content well but fail to maintain structural consistency. How do performance metrics vary across different quality rubrics and attachment conditions? Analyzing the breakdown by rubric reveals distinct strengths. Genspark excels at Instruction Following (0.766) and handling Negative Constraints (0.824), tying with ChatGPT-Agent in the latter. Manus achieves the highest Factuality score (0.731). Intriguingly, Minimax-Agent exhibits the highest performance in Logic/Functionality (0.755), suggesting strong reasoning core despite its lower overall score. Regarding attachment handling, Genspark scores highest with attachments (0.691). Notably, Manus maintains virtually identical performance with (0.646) and without (0.644) attachments, demonstrating exceptional robustness to changes in input modality. What is the efficiency profile of the tested agents? Efficiency, measured by Average Latency (s), presents clear trade-off with overall quality. Genspark (484.1 s) and Manus (500.0 s) offer strong balance between speed and quality. In contrast, Minimax-Agent is the slowest agent, requiring an average of 1416.2 s, significantly slower than all other agents. This high latency potentially correlates with its specialized strength in Logic/Functionality, suggesting its reasoning process may be more computationally intensive. Consistency Between Human and LLM Judges. We constructed an evaluation set comprising 28 problems and total of 171 scoring criteria, covering three problem types and various attachment formats. Human annotations were performed on model outputs within the evaluation set, and multiple LLMs were tested for automated scoring. The performance of different LLM judges in aligning with human annotations is summarized in Table 3. Table 3 LLM Judge Accuracy Against Human Annotations. Judge Model Gemini-3-Pro-preview Gemini-2.5-Pro GPT-5.1 Consistency 80.1% 73.9% 63.8% As shown in Table 3, Gemini-3-Pro-preview achieved the highest agreement with human judges at 80.1%, while GPT-5.1 showed an accuracy of 63.8%. The discrepancies observed between different LLM scores are primarily attributable to issues such as hallucination during scoring and inconsistent instruction following. Furthermore, the differences between LLM and human judgments often stem from varying interpretations of abstract concepts, including \"conciseness,\" \"relative completeness,\" and \"design sense.\" Case Study. We analyze sample problems on latent instruction inference in Figure 5. The task (a) requires the agent to research articles published by DeepMind in Nature within specified timeframe and organize them into PowerPoint presentation with specific format. The format of the PPT is not explicitly described in the text prompt; instead, the agent must understand the format from an attached PPT file. For example, features include each articles introduction slide having an accompanying image and citation marker located in the bottom left corner. The task requires the agent to mine these intrinsic features from the file and implicitly adhere to them when generating new slides. It can be observed that the ChatGPT agent followed the page format relatively well, but the number of articles listed was insufficient. Genspark, on the other hand, encountered issues with the PPT format; it missed the citation marker for the second article and included irrelevant information on the third and fourth pages. In the cross-platform shopping task (b), the user requirement for background similar to the screenshot compels the agent to perform high-level semantic image understanding and style transfer alongside standard information retrieval. While the ChatGPT agent successfully processes the explicit scalar constraint of price under six hundred dollars, it demonstrates limitations in multimodal reasoning by disregarding the visual context of the golf course image and misses technical specifications such as shaft stiffness. In contrast, 10 (a) Making research slide (b) Cross-platform shopping Figure 5 Case study of latent instruction inference tasks. Genspark displays superior cross-modal integration by accurately inferring implicit visual directives to generate context-aware user interfaces while simultaneously satisfying rigorous data filtering requirements, highlighting significant performance disparity in handling complex bimodal prompts. Successfully completing this task in full requires the agent to sustain research effort over longer duration while maintaining adherence to hidden instructions, which remains challenge for current agents."
        },
        {
            "title": "6.1 Challenges in Data Collection\nConstructing the AgentIF-OneDay presents unique data scaling challenges due to the inherent complexity of\nthe tasks:",
            "content": "High Annotation Cost: Our pilot study indicates that designing single valid task requires an average of three hours per annotator. Limited Individual Scalability: Task formulation relies heavily on personal life scenarios and imagination. Annotators quickly exhaust familiar contexts, making continuous high-quality generation difficult. Consequently, scaling requires broad population of annotators rather than small group of experts. Verification Heterogeneity: Unlike vertical domains where background knowledge is shared among experts, daily life scenarios possess highly diverse contexts. It is challenging to recruit experts capable of verifying tasks across such wide spectrum of topics. To partially address these bottlenecks, we implemented synthetic generation pipeline. By leveraging models to brainstorm diverse topics for subsequent human refinement, we effectively reduce dependency on individual annotator creativity and specific background knowledge."
        },
        {
            "title": "6.2 From OneDay to OneWeek",
            "content": "Extending agent capabilities to longer time horizons represents the next technical frontier. The design philosophy presented in this paper is transferable from daily to weekly scenarios. While recent benchmarks have explored long-horizon tasks (e.g., Game playing Pleines et al. (2025), Vendingbench Backlund & Petersson (2025), and Paper Bench Starace et al. (2025)), they often focus on specific domains. Although domain-specific training can improve performance on these metrics, it does not guarantee 11 generalization to other tasks of equivalent duration. To accurately assess progress toward Artificial General Intelligence without relying on extensive domain-specific customization, it remains essential to construct comprehensive OneWeek benchmarks derived from broad spectrum of work, learning, and daily life activities."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we introduced AgentIF-OneDay, task-level instruction-following benchmark designed to evaluate general AI agents under realistic, daily-use conditions. Through carefully constructed tasks spanning open workflow execution, latent instruction inference, and iterative refinement, our study highlights that effective agent behavior hinges not merely on raw reasoning ability, but on sustained instruction adherence, filecentered understanding, and stateful collaboration across time. Empirical results across multiple leading agent systems reveal notable convergence in baseline agentic competence between API-driven and RLbased agents, suggesting that agent-level capabilities are becoming increasingly standardized within modern foundation models. At the same time, the observed performance gaps across task categories and rubric dimensions underscore persistent challenges in implicit constraint inference and long-horizon consistency. Beyond benchmarking, AgentIF-OneDay provides scalable methodology for generating verifiable, workflowgrounded agent tasks, offering both diagnostic tool for current systems and high-quality data source for future agent training. We hope this benchmark will serve as foundation for evaluating and advancing general-purpose agents toward more reliable, user-aligned assistance in everyday scenarios."
        },
        {
            "title": "Core Contributors",
            "content": "Kaiyuan Chen, Qimin Wu, Taiyu Hou, Tianhao Tang, Xueyu Hu, Yuchen Hou (These authors contributed equally to this work)"
        },
        {
            "title": "Contributors",
            "content": "Bikun Li, Chengming Qian, Guoyin Wang, Haolin Chen, Haotong Tian, Haoye Zhang, Haoyu Bian, Hongbing Pan, Hongkang Zhang, Hongyi Zhou, Jiaqi Cai, Jiewu Rao, Jiyuan Ren, Keduan Huang, Lucia Zhu Huang, Mingyu Yuan, Naixu Guo, Qicheng Tang, Qinyan Zhang, Shuai Chen, Siheng Chen, Ting Ting Li, Xiaoxing Guo, Yaocheng Zuo, Yaoqi Guo, Yinan Wang, Yinzhou Yu, Yize Wang, Yuan Jiang, Yuan Tian, Yuanshuo Zhang, Yuxuan Liu, Yvette Yan Zeng, Zenyu Shan, Zihan Yin"
        },
        {
            "title": "Supervision",
            "content": "Xiaobo Hu, Yang Liu, Yixin Ren, Yuan Gong Contributors are listed alphabetically by their first name."
        },
        {
            "title": "References",
            "content": "Axel Backlund and Lukas Petersson. Vending-bench: benchmark for long-term coherence of autonomous agents. arXiv preprint arXiv:2502.15840, 2025. Aaron Chatterji, Thomas Cunningham, David Deming, Zoe Hitzig, Christopher Ong, Carl Yan Shan, and Kevin Wadman. How people use chatgpt. Technical report, National Bureau of Economic Research, 2025. Kaiyuan Chen, Yixin Ren, Yang Liu, Xiaobo Hu, Haotong Tian, Tianbao Xie, Fangfu Liu, Haoye Zhang, Hongzhang Liu, Yuan Gong, et al. xbench: Tracking agents productivity scaling with profession-aligned real-world evaluations. arXiv preprint arXiv:2506.13651, 2025. Kunal Handa, Alex Tamkin, Miles McCain, Saffron Huang, Esin Durmus, Sarah Heck, Jared Mueller, Jerry Hong, Stuart Ritchie, Tim Belonax, Kevin K. Troy, Dario Amodei, Jared Kaplan, Jack Clark, and Deep Ganguli. Which economic tasks are performed with ai? evidence from millions of claude conversations, 2025. URL https: //arxiv.org/abs/2503.04761. Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Qianli Ma, Guoyin Wang, Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, et al. Infiagent-dabench: Evaluating agents on data analysis tasks. arXiv preprint arXiv:2401.05507, 2024. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks, 2024. URL https://arxiv.org/abs/2401.13649. Thomas Kwa, Ben West, Joel Becker, Amy Deng, Katharyn Garcia, Max Hasin, Sami Jawhar, Megan Kinniment, Nate Rush, Sydney Von Arx, Ryan Bloom, Thomas Broadley, Haoxing Du, Brian Goodrich, Nikola Jurkovic, Luke Harold Miles, Seraphina Nix, Tao Lin, Neev Parikh, David Rein, Lucas Jun Koba Sato, Hjalmar Wijk, Daniel M. Ziegler, Elizabeth Barnes, and Lawrence Chan. Measuring ai ability to complete long tasks, 2025. URL https://arxiv.org/abs/2503.14499. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023. Mantas Mazeika, Alice Gatti, Cristina Menghini, Udari Madhushani Sehwag, Shivam Singhal, Yury Orlovskiy, Steven Basart, Manasi Sharma, Denis Peskoff, Elaine Lau, Jaehyuk Lim, Lachlan Carroll, Alice Blair, Vinaya Sivakumar, Sumana Basu, Brad Kenstler, Yuntao Ma, Julian Michael, Xiaoke Li, Oliver Ingebretsen, Aditya Mehta, Jean Mottola, John Teichmann, Kevin Yu, Zaina Shaik, Adam Khoja, Richard Ren, Jason Hausenloy, Long Phan, Ye Htet, 13 Ankit Aich, Tahseen Rabbani, Vivswan Shah, Andriy Novykov, Felix Binder, Kirill Chugunov, Luis Ramirez, Matias Geralnik, Hernán Mesura, Dean Lee, Ed-Yeremai Hernandez Cardona, Annette Diamond, Summer Yue, Alexandr Wang, Bing Liu, Ernesto Hernandez, and Dan Hendrycks. Remote labor index: Measuring ai automation of remote work, 2025. URL https://arxiv.org/abs/2510.26787. Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants, 2023. URL https://arxiv.org/abs/2311.12983. Tejal Patwardhan, Rachel Dias, Elizabeth Proehl, Grace Kim, Michele Wang, Olivia Watkins, Simón Posada Fishman, Marwan Aljubeh, Phoebe Thacker, Laurance Fauconnet, Natalie S. Kim, Patrick Chao, Samuel Miserendino, Gildas Chabot, David Li, Michael Sharman, Alexandra Barr, Amelia Glaese, and Jerry Tworek. Gdpval: Evaluating ai model performance on real-world economically valuable tasks, 2025. URL https://arxiv.org/abs/2510.04374. Hao Peng, Yunjia Qi, Xiaozhi Wang, Bin Xu, Lei Hou, and Juanzi Li. Verif: Verification engineering for reinforcement learning in instruction following, 2025. URL https://arxiv.org/abs/2506.09942. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. Marco Pleines, Daniel Addis, David Rubinstein, Frank Zimmer, Mike Preuss, and Peter Whidden. Pokemon red via reinforcement learning. arXiv preprint arXiv:2502.19920, 2025. Qingyu Ren, Qianyu He, Bowei Zhang, Jie Zeng, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, and Fei Yu. Beyond the trade-off: Self-supervised reinforcement learning for reasoning models instruction following, 2025. URL https://arxiv.org/abs/2508.02150. Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, et al. Paperbench: Evaluating ais ability to replicate ai research. arXiv preprint arXiv:2504.01848, 2025. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. Bosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao Huang, Jinfeng Zhou, Wenchuang Li, Binxin Hu, Wendy Gao, Jiaxin Xu, Yiming Liu, Jie Tang, Hongning Wang, and Minlie Huang. Benchmarking complex instruction-following with multiple constraints composition, 2024. URL https://arxiv.org/abs/2407.03978. Congying Xia, Chen Xing, Jiangshu Du, Xinyi Yang, Yihao Feng, Ran Xu, Wenpeng Yin, and Caiming Xiong. Fofo: benchmark to evaluate llms format-following capability, 2024. URL https://arxiv.org/abs/2402.18667. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ -bench: benchmark for tool-agent-user interaction in real-world domains, 2024. URL https://arxiv.org/abs/2406.12045. Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, et al. Multi-swe-bench: multilingual benchmark for issue resolving. arXiv preprint arXiv:2504.02605, 2025. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models, 2023. URL https://arxiv.org/abs/2311.07911."
        },
        {
            "title": "A Prompts",
            "content": "A.1 Synthetic Data Generation Prompt for Workflow Extraction (Step 1) You are an expert at extracting structured information from unstructured text. Given the following human-made question, extract the workflow steps and represent them in structured JSON format. Human Question: {task} Analyze this question and break it down into logical workflow steps that would be needed to answer it completely. Output format: return as **JSON**: { \"task\": \"{task}\", \"task_type\": \"{task_type}\", \"workflow\": [ { \"step_id\": 1, \"description\": \"Description of step 1\", \"input\": \"Input details for step 1\", \"output\": \"Expected output for step 1\", \"dependencies\": [] }, { \"step_id\": 2, \"description\": \"Description of step 2\", \"input\": \"Input details for step 2\", \"output\": \"Expected output for step 2\", \"dependencies\": [1] } ] } Instructions: - Break down the task into logical, sequential steps - Each step should have clear input/output requirements - Include dependencies between steps where applicable - Focus on actionable, concrete steps - Ensure the workflow completely addresses the original question Table 4 Prompt for Workflow Extraction. A.2 LLM as Judge 15 Prompt for Attachment Searching (Step 2) You are an expert at generating Google Image search queries to find images that can be used as attachments for creating similar workflow-based questions across different scenarios. Workflow Steps: {workflow} Your goal is to find images that: 1. Contain enough visual information to create questions that follow similar workflow pattern 2. Are from realistic, practical scenarios (business documents, real interfaces, actual products, etc.) 3. Can be applied to different contexts while maintaining the same workflow structure 4. Have clear, readable details that an AI agent would need to analyze Generate {num_queries} diverse and specific Google Image search queries that would find: - Real-world examples with rich details (text, numbers, structures) - Documents, interfaces, or visuals from different industries/domains - Images that require similar analysis steps but in varied contexts Guidelines for queries: - Be specific about the type of document/interface/visual needed - Include keywords that suggest detail-rich content (\"detailed\", \"with text\", \"example\", \" template\", \"filled\") - Target professional or educational materials that contain substantive information - Avoid generic or abstract queries - Focus on finding images that contain actionable information Examples of good queries: - \"detailed invoice template with line items filled out\" - \"restaurant menu with prices and descriptions\" - \"project gantt chart with tasks and deadlines\" - \"ecommerce product page with specifications screenshot\" - \"financial report dashboard with charts and metrics\" - \"medical prescription form filled out example\" - \"architectural floor plan with measurements and labels\" Return exactly {num_queries} queries as JSON list. Ensure queries are diverse across different domains but maintain similar complexity levels. Queries: Table 5 Prompt for Attachment Searching. 16 Prompt for Query Generation (Step 3) You are an expert at generating diverse, high-quality questions based on workflow templates and question attachments. Given workflow structure, generate new questions that follow the same logical pattern but with different content/context using attachments. Original Workflow: {workflow} Available Attachments: {available_attachments} Attachments analysis: {attachment_analysis} {task_type_description} Generate {num_questions} new questions that follow this same workflow pattern. Each question should: 1. Follow the same logical workflow pattern 2. Have different content/domain/context than the original 3. Utilize attachments appropriately when provided 4. Have verifiable answers 5. Be realistic and meaningful 6. Matches the cognitive requirements of {task_type} 7. Be diverse in domain (tech, business, science, daily life, etc.) Return as JSON: {{ \"original_task\": \"{original_task}\", \"workflow_pattern\": {workflow}, \"generated_questions\": [ {{ \"question_id\": 1, \"task_type\": \"{task_type}\", \"question\": \"New question following the workflow pattern\", \"domain\": \"Domain/context of this question\", \"attachments_used\": [\"List of attachment used\"] }} ] }} Table 6 Prompt for Query Generation. 17 Prompt for Rubrics Generation (Step 4) Generate comprehensive evaluation criteria for the given task and workflow. Task: {task} Workflow: {workflow} Create detailed evaluation rubric with bonus and penalty criteria following these principles: **Scoring Guidelines:** * **Bonus Criteria (+points)**: Represent completion of core assessment points of the task, such as: - Each node in the logical chain is bonus item - Hidden conditions in \"In context learning\" are bonus items - Successfully achieving key task objectives * **Penalty Criteria (-points)**: Include two categories: - Failure to meet basic requirements (e.g., incorrect output filename, wrong file type, etc.) - Making unnecessary and harmful modifications (e.g., changing content that should not be modified) Return as JSON: {{ \"evaluation_rubric\": {{ \"task_id\": \"{task_id}\", \"bonus_criteria\": [ {{ \"description\": \"Specific core achievement to reward (logical chain node or key objective) \", \"points\": 1, \"target_output\": \"expected_output.txt or null\", \"step_id\": 1, \"verification\": \"How to verify this criterion\", \"category\": \"core_objective\" // or \"logical_chain_node\" or \"context_learning_condition\" }} ], \"penalty_criteria\": [ {{ \"description\": \"Basic requirement failure or harmful modification\", \"points\": -1, \"target_output\": \"incorrect_output.txt or null\", \"step_id\": 2, \"verification\": \"How to detect this error\", \"category\": \"basic_requirement_failure\" // or \"harmful_modification\" }} ], \"max_possible_score\": 10, \"min_possible_score\": -5 }} }} Requirements: 1. Each workflow step should have at least one evaluation criterion 2. Bonus criteria should focus on core task objectives and logical progression 3. Penalty criteria should clearly distinguish between basic requirement failures and harmful modifications 4. Points should be 1 or -1 primarily, occasionally 2 for critical core objectives 5. Criteria must be objectively verifiable 6. Include file format and naming convention checks in penalty criteria where applicable 7. Ensure penalty criteria specifically target unnecessary changes to protected content Table 7 Prompt for Rubrics Generation."
        }
    ],
    "affiliations": []
}