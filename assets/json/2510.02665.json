{
    "paper_title": "Self-Improvement in Multimodal Large Language Models: A Survey",
    "authors": [
        "Shijian Deng",
        "Kai Wang",
        "Tianyu Yang",
        "Harsh Singh",
        "Yapeng Tian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in self-improvement for Large Language Models (LLMs) have efficiently enhanced model capabilities without significantly increasing costs, particularly in terms of human effort. While this area is still relatively young, its extension to the multimodal domain holds immense potential for leveraging diverse data sources and developing more general self-improving models. This survey is the first to provide a comprehensive overview of self-improvement in Multimodal LLMs (MLLMs). We provide a structured overview of the current literature and discuss methods from three perspectives: 1) data collection, 2) data organization, and 3) model optimization, to facilitate the further development of self-improvement in MLLMs. We also include commonly used evaluations and downstream applications. Finally, we conclude by outlining open challenges and future research directions."
        },
        {
            "title": "Start",
            "content": "Self-Improvement in Multimodal Large Language Models: Survey Shijian Deng1 Kai Wang2 Tianyu Yang3 Harsh Singh4 Yapeng Tian1 1The University of Texas at Dallas 3University of Notre Dame {shijian.deng,yapeng.tian}@utdallas.edu 4Mohamed bin Zayed University of Artificial Intelligence 2University of Toronto kaikai.wang@mail.utoronto.ca tyang4@nd.edu harsh.singh@mbzuai.ac.ae 5 2 0 2 3 ] . [ 1 5 6 6 2 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in self-improvement for Large Language Models (LLMs) have efficiently enhanced model capabilities without significantly increasing costs, particularly in terms of human effort. While this area is still relatively young, its extension to the multimodal domain holds immense potential for leveraging diverse data sources and developing more general self-improving models. This survey is the first to provide comprehensive overview of self-improvement in Multimodal LLMs (MLLMs). We provide structured overview of the current literature and discuss methods from three perspectives: 1) data collection, 2) data organization, and 3) model optimization, to facilitate the further development of self-improvement in MLLMs. We also include commonly used evaluations and downstream applications. Finally, we conclude by outlining open challenges and future research directions."
        },
        {
            "title": "Introduction",
            "content": "Self-improvement aims to enable models to collect and organize data required to build better generation of themselves, which offers path to overcome the costly scaling issues and potential performance ceilings of static training paradigms. In Multi-Modal Large Language Models (MLLMs), self-improvement seeks to use MLLMs themselves to obtain their own training data, resulting in improved MLLMs. Recent research (Favero et al., 2024; Deng et al., 2024b; Amirloo et al., 2024) show that this approach can significantly reduce hallucinations and improve performance on general tasks with relatively low cost. Significant progress has been made in this direction. Some current studies (Zhou et al., 2024a) partially leverage self-improvement by combining it with external tools or peer models, while others (Yu et al., 2024b) explore approaches that rely solely on Figure 1: An illustration of self-improvement in Multimodal Large Language Models. The process involves selecting seed MLLM to generate new data, organizing it into dataset (which can optionally guide further data collection), and finally obtaining an improved model through training. This process can be iterated to achieve recursive self-improvement. single model to handle all processes, toward full self-improvement. Although previous work (Tao et al., 2024) has summarized the self-improvement in text-only LLMs and other surveys study the general scope of MLLMs (Yin et al., 2024; Zhang et al., 2024a) or specific issues such as hallucinations (Bai et al., 2024), there is no comprehensive survey that focuses on these self-improvement methods for MLLMs. To fill this gap, we dedicate this paper to providing comprehensive review of this area and identifying the challenges that need to be addressed. Compared to self-improvement in LLMs (Huang et al., 2022; Tao et al., 2024), self-improvement in MLLMs faces unique challenges, such as the inclusion of multiple modalities. This can introduce modality alignment problems, which are known to cause issues like hallucination in MLLMs (Li et al., 2023b). Additionally, MLLMs often cannot generate all the training data they need on their own, as most current models (Liu et al., 2024a; Bai et al., 2023) are unable to generate images directly. Despite these challenges, there is growing interest in leveraging self-improvement in MLLMs to build models more effectively and efficiently. Promising results have already been achieved in this area. This paper aims to summarize previous works, compare methods, and provide clearer guidance for future research directions in this field. In this survey, we follow the structure outlined below: First, we provide an overview of the field. Next, we introduce the most commonly used seed models that serve as starting points for selfimprovement. For the detailed methodology, we divide the discussion into three parts as shown in Figure 2: data collection, data organization, and model optimization. We list current approaches and discuss their differences. We also collect evaluation methods commonly used to measure performance gains from self-improvement, compiling benchmark results for comprehensive comparison. Additionally, we discuss downstream applications, to highlight the real-world impact of this paradigm. Finally, we identify the challenges in this field, which also represent potential future directions, and conclude the survey. With this work, we aim to establish clearer pathway for developing the next generation of MLLMs with better self-improvement mechanisms, moving beyond random exploration with biases. We hope to attract more researchers to explore this promising direction."
        },
        {
            "title": "2 Overview",
            "content": "In this section, we first formally define selfimprovement in multimodal large language models (MLLMs) in the context of this paper, and then compare it to similar concepts that have been used in MLLMs research. Afterwards, we summarize representative works in this domain to provide general overview of the existing methods. 2.1 Definition There are many similar terms to Self-Improvement, such as Self-Evolution, Self-Training, SelfConsistency, Self-Correction, Self-Reflection, and Self-Refinement, which have also been mentioned in previous MLLM research. There is trend where the boundaries between these concepts are becoming blurred, and they may become more interchangeable in the future, depending heavily on the context. However, we clearly distinIn this paper, we define guish two paradigms. Figure 2: An overview of three steps for selfimprovement in MLLMs. Each step can involve different methods based on requirements. For the full taxonomy please check Figure 3. self-improvement shown in Figure 1 as updating the model from m0 to m1, as opposed to selfrefinement, which involves updating responses in context from r0 to r1. Formally, we express these concepts as follows: Self-Improvement (Model Update through Training): m1 = I(m0, D), where I() denotes the self-improvement operator that upgrades the entire model by training on self-curated multimodal dataset D. Self-Refinement (Response Update in Context): r1 = R(r0, c), where R() represents the self-refinement operator that refines the initial response r0 based on the context c, which can be seen as type of test-time scaling (or inferencetime self-improvement (Dong et al., 2024)). It is worth noting that some refined responses may have the potential to be incorporated into training data and thus contribute to further self-improvement.1 typical self-improvement process in MLLMs involves three modules: data collection, data organization, and model optimization as demonstrated in Figure 2, which follows the structure of general model-building process but focuses on automating the model development process using models rather than relying heavily on human intervention. While these commonly used modules are widely involved in the self-improvement of MLLMs, it is important to note that their life cycle does not necessarily conclude once an improved model is obtained. The iterative loop can persist, using the newly improved model as the seed for the next stage of selfimprovement as demonstrated in Figure 1. This life cycle can be highly dynamic, particularly in 1Here, we do not consider storing newly acquired skills during inference in memory as an analogy for parameter tuning. online settings, where data collection is directly influenced by the optimization design. This design may incorporate or encourage the model to explore more diverse or constrained data generation in subsequent rounds. We conceptualize self-improvement in MLLMs as spectrum of methods aiming to reduce human workload and maximize automation in improving model performance. Some methods target full autonomy, while others are limited to guided or assisted self-improvement, as long as they do not fully rely on human effort. Most papers in our survey do not leverage stronger external models. However, external models can be treated as tools that the seed model calls or uses. Under this formulation, we believe such approaches fall within the spectrum of self-improvement, albeit at the less independent end due to their reliance on external tools. To illustrate this, we add Table 1 comparing different levels of self-improvement in MLLMs, detailing what they automate and their limitations, allowing all discussed methods to fit organically within this spectrum. 2.2 Related and Representative Works Improvement without human supervision in MLLMs encompasses various strategies aimed at enhancing model performance through internal mechanisms. These approaches can be broadly categorized into Self-Refinement, Peer-Improvement, Self-Improvement for image LLMs, and extensions to Video LLMs and agents. 2.2.1 Self-Refinement and Peer-Improvement Early methods like Woodpecker (Yin et al., 2023) and VCD (Leng et al., 2024) focus on reducing hallucinations within generated content through training-free techniques. Due to the significant gap between proprietary models and early openweight models, LLaVA (Liu et al., 2024a) and HADPO (Zhao et al., 2023) leverage GPT-4 to help build or refine multimodal capabilities, avoiding human supervision from scratch. 2.2.2 Self-Improvement in Image Large Language Models Self-improvement strategies aim to enhance model abilities fundamentally by modifying model weights and reducing dependency on external models. Recent methods include on-the-fly enhancement of instruction-tuning data VIGC (Wang et al., 2024a), shifting from answering generation to self-questioning SQ-LLaVA (Sun et al., 2025a), and synergy-driven cycles that interleave describing and locating objects SC-Tune (Yue et al., 2024b). Others reduce hallucinations by converting training-free interventions into trainable ones M3ID (Favero et al., 2024), enabling interpretability in decision-making without extra annotations like LLaVA-ASD (Deng et al., 2024a), leveraging data augmentation to construct preference pairs like SeVa (Zhu et al., 2024), and applying step-wise self-rewarding CSR (Zhou et al., 2024b). Some approaches rely on internal checks, such as visual metrics for preference tuning SIMA (Wang et al., 2024b) or using the models own encoder for finegrained alignment FiSAO (Cui et al., 2024). 2.2.3 Extensions to Video i-SRT (Ahn et al., 2024a) applies self-improvement in video large language models, addressing the issue of self-generated preferences that are linguistically plausible but not grounded in the visual content of the associated video. Video-STaR (Zohar et al., 2024) adapts the STaR approach for the video domain, enabling the use of any labeled video dataset (such as Kinetics-700) for video instruction tuning. 2.2.4 Multimodal Agents When augmenting MLLMs as agents and allowing them to act or even interact with each other, selfimprovement enhances model performance across various tasks, including learning through self-play in image identification (Konyushkova et al., 2025) or improving decision-making in games such as Blackjack and ALFWorld (Zhai et al., 2025)."
        },
        {
            "title": "3 Seed Models",
            "content": "A seed model does not need to be exceptionally strong, but it must clear small set of capability floors that the self-improvement loop relies on. If these floors are missing, the model tends to generate low-quality data and the loop either stalls or collapses (Hu et al., 2025). Capability floors. Some skills are costly to \"retrofit\" purely from self-improvement and therefore should be present in the seed: Basic visual grounding Robust text-in-the-wild handling Temporal aggregation for video Coherent reasoning traces (for reflection) Level Primary actor Typical technique / example L0 No self-improvement L1 Human-guided improvement L2 Peer improvement L3 Hybrid self-improvement L4 Conditional self-improvement L5 High self-improvement Humans do all data collection and curation Model generates responses, while humans choose preferred data External models (e.g. GPT-4-V) supply data; minimal direct human effort Model collects its own data, but queries external augmentations or verifiers Target model runs its own data loop except images are from existing datasets Model generates and curates both images and text without external data sources Flamingo (Alayrac et al., 2022) RLHF-V (Yu et al., 2024a) LLaVA (Liu et al., 2024a) Hybrid approaches, CSR (Zhou et al., 2024a) RLAIF-V (Yu et al., 2024b) with self-reward SUDER (Hong et al., 2025), UniRL (Mao et al., 2025) Table 1: Levels of multimodal self-improvement. Common choices. Several commonly used MLLMs have been adopted as seed models in selfimprovement research: LLaVA (Liu et al., 2024a): As one of the earliest popular MLLMs, LLaVA has been widely used in MLLM self-improvement research due to its representativeness. The most commonly used versions are LLaVA-1.5 (7B and 13B). Some works, such as STIC and BDHS, utilize LLaVA-1.6. Qwen-VL (Bai et al., 2023): Built on top of Qwen-LM, this model uses three-stage training pipeline: Pretraining, Multi-task Pretraining, and Supervised Fine-tuning, to optimize its performance. InstructBLIP (Dai et al., 2023): InstructBLIP introduces an instruction-aware Query Transformer that extracts informative features tailored to given instructions. It is trained on 13 datasets converted into an instructiontuning format. MiniGPT4 (Zhu et al., 2023): An early opensource effort to replicate the capabilities of GPT-4, MiniGPT4 aligns frozen visual encoder with frozen advanced LLM (Vicuna) using single projection layer. Video-LLaVA (Lin et al., 2023): It is commonly used as seed model in video models. As its name implies, Video-LLaVA is similar to LLaVA but also fine-tuned on video datasets. It is designed for both image and video comprehension tasks. Beyond these commonly used seed models, some works train their own seed models from scratch using pretrained LLM to maintain more control over the entire process and address specific needs. conventional machine learning approaches, data collection typically relies on extensive human labor. This labor-intensive process, while effective, can be both time-consuming and costly, and is often limited by the availability and scalability of human resources. In the context of self-improvement for MLLMs, shift towards autonomous data collection is both desirable and increasingly feasible, thereby reducing the dependency on human intervention. This approach not only enhances efficiency but also enables continuous and scalable learning. We compare advantages and disadvantages of these methods in Table 2. 4.1 Random Sampling The most straightforward method for autonomous data generation is random sampling (Zhao et al., 2023), where the model generates data by sampling from its existing knowledge base without specific guidance. Although random sampling is simple to implement and can produce diverse set of data, it has notable inefficiencies such as the generation of redundant or irrelevant data, which can waste computational resources and time. 4.2 Guided Data Generation To address the inefficiencies of random sampling, guided data generation techniques have been developed (Cheng et al., 2024). These methods employ predefined pipelines with carefully designed prompts to steer the model towards generating desired and high-quality responses. One prominent technique is Chain-of-Thought (CoT), which encourages the model to generate intermediate reasoning steps before producing final answer. In order to further improve sample efficiency, some approaches adopt search-based methods such as beam search and Monte Carlo Tree Search (MCTS) and its variants (Yao et al., 2024)."
        },
        {
            "title": "4 Data Collection",
            "content": "4.3 Negative Samples Effective data collection is crucial for enabling MLLMs to acquire and refine specific abilities. In Negative samples are essential for refining the models ability to distinguish between correct and Method Benefits Drawbacks Random Sampling (Zhao et al., 2023; Yu et al., 2024b) Easy to use; works for any MLLM Prompt-Guided Generation (Wang et al., 2024a; Fang et al.) Chain of Thought (Zhai et al., 2025; Zohar et al., 2024) Highly controllable; can generate almost any type of response Can generate long responses for reasoning tasks Input Injection (Zhou et al., 2024a; Zhu et al., 2024) Can generate negative examples Sourcing from Multiple MLLMs (Li et al., 2023a; Xiong et al., 2024) Ensures diversity in generated outputs May not be efficient; difficult to obtain samples with desired features Requires significant human effort; difficult to scale Sometimes produces redundant or irrelevant reasoning steps Minor distortions may sometimes produce better examples than undistorted ones Requires additional effort to manage different models Table 2: Comparison of data collection methods. incorrect responses, thereby enhancing its overall accuracy and reliability. Various strategies have been explored to generate negative samples autonomously. Poorly Designed Prompts (Deng et al., 2024b): Crafting ambiguous or misleading prompts can lead the model to generate suboptimal or incorrect responses. Distorted Images (Zhou et al., 2024a): Introducing visual distortions or noise into images challenges the models visual comprehension capabilities. Attention Masking (Amirloo et al., 2024): Manipulating the attention mechanism during the decoding process can result in responses that focus on irrelevant parts of the input. Additionally, the generation of negative samples can be finely controlled by altering the decoding path (Deng et al., 2025b), which produces responses that are less grounded in the visual context to the desired level, serving as effective negative examples for training. Some methods utilize peer models for data generation (distillation), but implementing the same pipeline with the seed model itself may theoretically produce similar effects."
        },
        {
            "title": "5 Data Organization",
            "content": "The data collected by MLLMs may not be directly suitable for feeding back into the models without further processing. To ensure the efficacy of selfimprovement, thorough verification and processing step is essential before leveraging the newly obtained data. The quality of this organization process is paramount, as it directly determines the robustness and reliability of the self-improvement mechanism in MLLMs. 5.1 Verification Methods The verification process can be critical step during data organization and is usually implemented using either predefined rules or sophisticated models. Each method has its own advantages and limitations, which are discussed below. We also compare these methods in Table 3. 5.1.1 Rule-Based Verification Rule-based organization involves applying predefined criteria to assess the quality and correctness of the generated data. This approach is straightforward and computationally efficient but may lack flexibility in handling diverse data scenarios. Majority Voting (Ensembling or Consensus): The simplest approach compares multiple generated responses and selects the one with the highest frequency. While easy to implement, it may not always yield the best quality data, as the most frequent response might still contain inaccuracies or lack diversity. Ground Truth Alignment (He et al., 2024a): For datasets with established ground truths, the verification can involve cross-referencing the models output with the correct answers. For instance, in terms of the tasks requiring bounding boxes, an Intersection over Union (IoU) threshold can determine the acceptance of generated content (Yue et al., 2024b). If the IoU score exceeds the predefined threshold, the content is deemed acceptable; otherwise, it can be discarded or flagged for further review. Alternatively, IoU can also be used as reward function during RL training (Liu et al., 2025a). 5.1.2 Model-Based Verification Model-based organization leverages additional models to assess the quality of generated data. This method can provide more nuanced evaluations and modifications but may introduce additional computational overhead. Peer Model Evaluation (Hernandez et al., 2025): Utilizing separate peer models to judge the quality of outputs can reduce bias and improve the reliability of the verification process. These models can provide independent evaluations, enhancing the overall robustness of data verification. Self-Critic Mechanism (Wang et al., 2024b): The MLLM itself can generate the rewards that evaluate the correctness and relevance of the data at various levels-token (Cui et al., 2024), sentence (Zhou et al., 2024b), or output. This allows for more detailed assessments compared to ruleMethod Benefits Drawbacks Pre-assigned Labels (Zhou et al., 2024a) No extra effort required after data collection Cannot handle complex cases Rule-Based Organization (Yue et al., 2024b) Highly explainable Not robust enough for novel samples Self-Evaluation (Ahn et al., 2024b) No additional reliance on external tools Can suffer from model bias or hallucinations Judgment by External Verifiers (Sun et al., 2024a) Well-defined verifiers are highly robust Some verifiers may incur significantly higher costs Feedback from Environment (Zhai et al., 2025) Robust and requires minimal additional effort Many cases may be difficult to implement Table 3: Comparison of data verification methods. based methods. 5.1.3 Verification from the Environment MLLMs acting as agents that interact with their environment can also leverage environmental feedback for verification. The environment can be either the real world (Guo et al., 2025a; Chen et al., 2025d) or simulated environments, such as games (Zhai et al., 2025; Konyushkova et al., 2025). 5.2 Dataset Arrangement The new data can be further post-processed by, for example, removing low-quality responses, fixing answers, normalizing formats, or storing the data by category. Depending on the goal, the final dataset can be created by: Filtering/Rejecting unwanted data (Liu et al., 2024c); Editing/Refining outputs or rationales (e.g., through generator-corrector workflows and reflective self-correction), sometimes involving more complex procedures such as Topicaware overwriting, where errors are corrected within semantic clusters (Wang et al., 2024a; Zhang et al., 2024b; He et al., 2024a,b); Archiving/Scheduling hard examples for future rounds of curriculum learning as the model gets stronger (Han et al., 2025). The resulting dataset can be formatted for SFT or DPO, or it can be used to train reward/judge model to guide future RL training or for other use cases such as evaluation (Xiong et al., 2024). this data into curated set, which can be sent back to the original model to generate higher quality or more diverse data before being used for training. This process improves data quality and reduces noise over successive iterations (Luo et al., 2024a; Chen et al., 2025c). Recursive Improvement (model-centric): The loop also supports upgrading the model itself, so that the next round of data is produced and organized by stronger model. This enables the coevolution of data and model capability (Tan et al., 2024; Liu et al., 2024c; Deng et al., 2025c; Chen et al., 2025c)."
        },
        {
            "title": "6 Model Optimization",
            "content": "After obtaining the organized dataset, the next step is to update the parameters of the seed model. Several training methods have been employed in selfimprovement for MLLMs, including supervised fine-tuning, reinforcement learning, and direct preference optimization. As discussed in the paper DeepSeekMath (Shao et al., 2024), all these methods are actually connected. We compare advantages and disadvantages of these methods in Table 4. Method Benefits Drawbacks SFT (Wang et al., 2024a; Luo et al., 2024a; Xiong et al., 2024) PPO (Yue et al., 2024b; Zhai et al., 2025) GRPO (Chen et al., 2025b) RFT (Liu et al., 2024c) DPO (Li et al., 2023a; Ouali et al., 2025; Luo et al., 2024b) Highly efficient when using existing highquality datasets classic online RL method, easy to deploy than efficient More PPO since no value model is needed Can be used in an offline manner Can leverage both positive and negative samples high-cost Requires human effort or strong models The reward model may be difficult to obtain Involves trade-off between efficiency and the number of groups All negative samples are discarded May experience distribution shift issues after extensive training 5.3 Collection-Organization Loop Table 4: Comparison of model optimization methods. The data collection-organization pipeline is not necessarily unidirectional. Data organized in early rounds can influence data collection in later rounds, creating loop that iteratively enhances the dataset and, eventually, the models performance. Iterative Evolution (data-centric): In each round, the current model generates data. An organization step then verifies, filters, or transforms 6.1 Supervised Fine-tuning Instruction tuning, or supervised fine-tuning (SFT), has become widely adopted post-training method to enable LLMs and MLLMs to follow instructions and solve broader range of general tasks. In SFT, the model is trained to minimize the discrepancy between its predictions and the ground truth responses provided in the dataset. Formally, given dataset = {(xi, yi)}N i=1, where xi represents the input and yi the corresponding target output, the objective is to minimize the cross-entropy loss: (cid:88) Ti(cid:88) yi,t log p(yi,txi, yi,<t; θ) (1) LSFT = 1 i= t=1 This loss function encourages the model to generate outputs that closely match the ground truth. In the context of self-improvement for MLLMs, it enables the new model to better align with the desired improvement goals in generated output. 6.2 Reinforcement Learning Reinforcement learning (RL) methods have been used to improve MLLMs without human demonstration data, particularly for preference alignment and reasoning tasks. It aims to generate outputs that receive high rewards. The objective is then expressed as: LRL(θ) = E(x,y)Dπθ r(x, y) (2) Methods such as Proximal Policy Optimization (PPO) have been initially employed in RLHF for MLLMs (Sun et al., 2023). More recently, GRPO (Shao et al., 2024) has emerged as an efficient alternative to PPO for training MLLMs (Chen et al., 2025b), as it does not require value model. They can also be used in Reinforcement Learning from Verifiable Rewards (RLVR). 6.3 Direct Preference Optimization Direct Preference Optimization (DPO) (Rafailov et al., 2024) is reinforcement-learning-free offline alternative for preference learning, which has become the de facto standard in preference optimization for MLLMs. Unlike SFT and Rejection Fine-Tuning (RFT), which can only leverage positive data, it can also take advantage of negative data. It formulates the optimization problem as follows: Given pair of outputs (y+, y) where y+ is preferred over y, the objective is to maximize the likelihood of preferred outputs while minimizing the likelihood of dispreferred outputs. The DPO loss can be expressed as: LDPO(θ) = 1 N (cid:88) i=1 (cid:2)log σ(s(y+ ) s(y ))(cid:3) (3) This objective encourages the model to assign higher scores to preferred outputs compared to dispreferred ones. 6.4 Other Enhanced Variants Some works adjust the classic method by adding additional components (Xiao et al., 2024), such as penalty terms for specific designs. For example, incorporating regularization terms can help maintain model stability and prevent overfitting to the preference data. 6.5 Alternative Ways of Using Negative Samples It is worth noting that preference learning is not the only way to utilize negative data samples. Combining negative samples with self-reflection and correction using CoT approach can further enhance model performance (Cheng et al., 2024). This involves generating detailed reasoning steps that allow the model to identify and correct its own errors, thereby improving the quality of the outputs. 6.6 Curriculum Multi-stage training with different optimization methods has become common practice in MLLM training. Some research shows that certain training stages may hurt the model (Zhou et al., 2025), while other studies find that certain performance gains can be more easily obtained by combining different stages of optimization (Huang et al., 2025b). Research has shown that even one training stage, such as RL, can benefit from being further divided into different substages (Deng et al., 2025a)."
        },
        {
            "title": "7 Dataset and Evaluation",
            "content": "Although some datasets created with the help of MLLMs can be used to further improve them, no benchmark has been specifically designed for selfimprovement in MLLMs. Typically, researchers use existing MLLM benchmarks and report performance gains compared to the seed model and other SOTA models. Some attempts, such as LLMEvolve (You et al., 2024), aim to build new type of benchmark; however, this particular benchmark operates in non-parametric setting. 7.1 Dataset Some datasets have been developed to facilitate the self-improvement of MLLMs. VLFeedback (Li et al., 2024b) is the first large-scale, AI-annotated vision-language feedback dataset, containing over 82K multimodal instructions and comprehensive, model-generated rationales. The DeepPerception Dataset (Ma et al., 2025) aims to enhance the cognitive visual perception capabilities of MLLMs for knowledge-intensive visual grounding (KVG); it comprises high-quality, knowledge-aligned training samples generated through an automated data synthesis pipeline. The OmniAlign-V-DPO Dataset (Zhao et al., 2025b) leverages answers from the OmniAlign-V SFT dataset as positive examples. To create the preference pairs necessary for DPO, negative samples are generated using another MLLM, LLaVANext baseline, through rejection sampling. The VisionPrefer Dataset (Wu et al., 2024) is another high-quality, fine-grained preference dataset created to train reward model for aligning text-to-image generative models. It aggregates feedback from AI annotators, specifically utilizing GPT-4Vs capabilities to evaluate generated images based on defined criteria. The LLaVA-Critic dataset (Xiong et al., 2024), comprising 113,000 evaluation instruction samples across 46,000 images, was generated using GPT-assisted pipeline, with GPT-4o providing judgment scores and reasons for evaluating MLLM responses. The community has also contributed additional datasets, including those used in RLAIF-V (Yu et al., 2024b) and Open-R1-Multimodal (EvolvingLMMs-Lab, 2025). These datasets created with MLLMs have demonstrated their utility in improving various MLLMs. We summarize these datasets in Table 5 to highlight their differences. 7.2 Benchmarks Evaluating the self-improvement of MLLMs can leverage current popular MLLM benchmarks. These benchmarks can be broadly categorized as follows: 7.2.1 General Knowledge Benchmarks in this category assess the models ability to understand and reason across multiple disciplines using multimodal inputs. Notable benchmarks include MMMU (Yue et al., 2024c) and MMStar (Chen et al., 2024a), which focus on comprehensive multimodal understanding across various academic and professional domains. 7.2.2 Reasoning These benchmarks evaluate higher-order cognitive abilities and commonsense reasoning within multimodal contexts. Examples such as Mathvista (Lu et al., 2023) and VCR (Zellers et al., 2019) are designed to test mathematical reasoning and commonsense understanding through visual inputs. 7.2.3 Hallucination Detecting and mitigating hallucinations in generated content is crucial for reliable MLLMs. Benchmarks like CHAIR (Rohrbach et al., 2018), POPE (Li et al., 2023b), and AMBER (Wang et al., 2023) provide metrics and evaluation frameworks to assess the accuracy and relevance of model outputs against visual inputs. 7.2.4 Medical Medical benchmarks focus on the models capability to understand and reason with medical images and related queries. Datasets such as VQARAD (Lau et al., 2018), SLAKE (Liu et al., 2021), and PathVQA (He et al., 2020) are designed to evaluate the models proficiency in medical image analysis and question-answering tasks. 7.2.5 Video QA Assessing MLLMs understanding of dynamic visual content is addressed by video-based benchmarks. Notable datasets include MSVD-QA (Xu et al., 2017), MSRVTT-QA (Xu et al., 2017), TGIF-QA (Jang et al., 2017), and ActivityNetQA (Yu et al., 2019), which provide questionanswer pairs based on video clips to test temporal and contextual reasoning. Judging Abilities 7.2.6 Evaluating the models capability to act as judge involves assessing various aspects. Benchmarks like VL-RewardBench (Li et al., 2025), MJBench (Chen et al., 2024b) are designed to measure these attributes, ensuring that the models evaluations are reliable and consistent. Meanwhile, AutoBench-V (Bao et al., 2024) attempts to enable the MLLM itself to propose and construct new benchmarks. 7.3 Meta-Analysis Across Benchmarks Using the compiled results, we observed the following robust patterns: Method-Task Match. Rule-/verification-based RL (e.g., with step-wise or outcome checks) drives the largest absolute gains on verifiable tasks (visual math, programmatic reasoning, constrained captioning), while preference/AI-feedback data most reliably lowers hallucination metrics (e.g., POPE/AMBER) and improves general helpfulness/faithfulness. Type of Dataset Examples How the dataset is created with MLLMs How the dataset can be used for MLLMs Instruction tuning dataset DeepPerception (Stage-1) strong peer is used to generate CoT reasoning data (with ground truth provided in the input). To train MLLMs with SFT to initialize cognitive-perceptual synergy for the target domain. VLFeedback OmniAlign-V Preference dataset LLaVA-Critic Instructions from many previous datasets; responses generated by pool of LVLMs; preferences assigned by GPT-4V. To train MLLMs with DPO to improve helpfulness and harmlessness. Images from previous datasets; questions generated by GPT4o (given prompts); responses are generated and refined by MLLMs; negative samples are generated by the LLaVANext baseline and filtered by an LLM judge. Pointwise data uses GPT-4o to provide judgment scores (input includes instructions, responses from previous datasets, and GPT-4o response as reference); pairwise data uses previous preference datasets with further judges justification augmented by GPT-4o. To use SFT or DPO to align MLLMs with human preferences without decreasing general abilities. To train an MLLM judge with SFT to assign scores or rankings based on the prompts criteria and to justify its judgments. VisionPrefer GPT-4V is used to generate three types of feedback for generated images. To train reward model to score generated images. RLVR dataset DeepPerception (Stage-2) Images and prompts are reused from an existing dataset (while response text is sampled on the fly during training). For RL with an IoU-based reward and format reward to further improve performance on Knowledge-Intensive Visual Grounding. Table 5: Comparison of datasets created with MLLMs. Seed Strength Matters. Relative improvement seed typically shrinks as seed models get stronger; however, strong seeds show more stable gains across benchmarks. For identical pipelines (e.g., STIC-style), better seeds consistently yield higher end performance. Cross-Benchmark Inconsistency. Methods that boost compositional reasoning can regress on perception-heavy tasks (fine-grained recognition, OCR, attribute binding), and vice versa. Pairwise rank correlations between benchmarks are often modest; gains on one suite do not guarantee gains on others. Persistent Bottlenecks. We observe recurring difficulty in (i) fine-grained spatial reasoning (counting under occlusion, relative positions), (ii) multi-image/multi-hop consistency, (iii) longhorizon video temporal grounding, (iv) diagram/chart/plan understanding, and (v) robustness under noisy OCR or layout-heavy documents. Hallucination recurs in open-world scenes unless visual evidence is tightly verified. Judge/Reward Leakage. When the same or closely related judges curate and evaluate (e.g., GPT-4V-like feedback used both for data construction and testing), scores inflate. Separation of curation and evaluation signals is critical for credible claims. Efficiency Analysis. We discuss the efficiency of self-improvement methods in MLLMs from computational cost perspective, considering factors like memory use during each stage and the data generation scale. First, regarding data sampling: random sampling often has the highest cost since it normally has high rejection rate. Prompt-guided generation helps address this issue by giving more guidance, thereby reducing the search space of possible responses. Using negative samples further enables the usage of all generated data; even samples considered low-scoring can be used as negative samples, thus avoiding waste. For verification methods, the rule-based method generally has the lowest cost, since checking whether generated content satisfies rules is typically straightforward. Model-based verification can handle very complex scenarios but has higher cost. Verifying the outcome in the real environment can have the highest cost due to simulation complexity but may yield the highest feedback quality, especially for the most difficult verifications."
        },
        {
            "title": "8 Conclusion",
            "content": "In this paper, we presented comprehensive and structured survey of self-improvement in multimodal large language models (MLLMs). We defined the concept of self-improvement as used in this survey and clarified its differences from other related concepts. We discussed and compared representative works in this domain, highlighting their similarities and differences from three perspectives: 1) data collection, 2) data organization, and 3) model optimization. Further, we summarized commonly used evaluations and applications. Finally, we identified current challenges and potential opportunities for future research. We hope this survey serves as valuable guide for researchers interested in exploring and developing new self-improvement methods for MLLMs."
        },
        {
            "title": "Limitations",
            "content": "Due to space limitations, this paper primarily focuses on macro-level description and analysis of self-improvement within the current scope of MLLMs. Given the rapid evolution of the field, some of the most recent developments and new directions may not be included. Since we focus on the MLLM domain, we did not review work that involves only LLMs or agents; however, some methods may potentially be adapted to MLLMs as well. Despite these limitations, we believe this work, as the first survey in the area of self-improvement in MLLMs, provides valuable overview of current research."
        },
        {
            "title": "References",
            "content": "Daechul Ahn, Yura Choi, San Kim, Youngjae Yu, Dongyeop Kang, and Jonghyun Choi. 2024a. i-srt: Aligning large multimodal models for videos by iterative self-retrospective judgment. arXiv preprint arXiv:2406.11280. Daechul Ahn, Yura Choi, Youngjae Yu, Dongyeop Kang, and Jonghyun Choi. 2024b. Tuning large multimodal models for videos using reinforcement learning from ai feedback. arXiv preprint arXiv:2402.03746. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, and 1 others. 2022. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:23716 23736. Massih-Reza Amini, Vasilii Feofanov, Loic Pauletto, Lies Hadjadj, Emilie Devijver, and Yury Maximov. 2024. Self-training: survey. Neurocomputing, page 128904. Elmira Amirloo, Jean-Philippe Fauconnier, Christoph Roesmann, Christian Kerl, Rinu Boney, Yusu Qian, Zirui Wang, Afshin Dehghan, Yinfei Yang, Zhe Gan, and 1 others. 2024. Understanding alignment in multimodal llms: comprehensive study. arXiv preprint arXiv:2407.02477. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966. Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou. 2024. Hallucination of multimodal large language models: survey. arXiv preprint arXiv:2404.18930. Han Bao, Yue Huang, Yanbo Wang, Jiayi Ye, Xiangqi Wang, Xiuying Chen, Mohamed Elhoseiny, and Xiangliang Zhang. 2024. Autobench-v: Can large visionarXiv language models benchmark themselves? preprint arXiv:2410.21259. André Bauer, Simon Trapp, Michael Stenger, Robert Leppich, Samuel Kounev, Mark Leznik, Kyle Chard, and Ian Foster. 2024. Comprehensive exploration of synthetic data generation: survey. arXiv preprint arXiv:2401.02524. Kejia Chen, Jiawen Zhang, Jiacong Hu, Jiazhen Yang, Jian Lou, Zunlei Feng, and Mingli Song. 2025a. Shape: Self-improved visual preference alignment by iteratively generating holistic winner. arXiv preprint arXiv:2503.04858. Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. 2025b. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/R1-V. Accessed: 2025-02-02. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and 1 others. 2024a. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330. Xiuwei Chen, Wentao Hu, Hanhui Li, Jun Zhou, Zisheng Chen, Meng Cao, Yihan Zeng, Kui Zhang, Yu-Jie Yuan, Jianhua Han, and 1 others. 2025c. C2-evo: Co-evolving multimodal data and model arXiv preprint for self-improving reasoning. arXiv:2507.16518. Yuhui Chen, Shuai Tian, Shugao Liu, Yingting Zhou, Haoran Li, and Dongbin Zhao. 2025d. Conrft: reinforced fine-tuning method for vla models via consistency policy. arXiv preprint arXiv:2502.05450. Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou, Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Qinglan Huang, and 1 others. 2024b. Mj-bench: Is your multimodal reward model really good judge for text-to-image generation? arXiv preprint arXiv:2407.04842. Kanzhi Cheng, Yantao Li, Fangzhi Xu, Jianbing Zhang, Hao Zhou, and Yang Liu. 2024. Vision-language models can self-improve reasoning via reflection. arXiv preprint arXiv:2411.00855. Chenhang Cui, An Zhang, Yiyang Zhou, Zhaorun Chen, Gelei Deng, Huaxiu Yao, and Tat-Seng Chua. 2024. Fine-grained verifiers: Preference modeling as nexttoken prediction in vision-language alignment. arXiv preprint arXiv:2410.14148. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. Instructblip: Towards general-purpose vision-language models with instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems. Huilin Deng, Ding Zou, Rui Ma, Hongchen Luo, Yang Cao, and Yu Kang. 2025a. Boosting the generalization and reasoning of vision language models with curriculum reinforcement learning. arXiv preprint arXiv:2503.07065. Kaiyue Feng, Yilun Zhao, Yixin Liu, Tianyu Yang, Chen Zhao, John Sous, and Arman Cohan. 2025b. Physics: Benchmarking foundation models on universityarXiv preprint level physics problem solving. arXiv:2503.21821. Shijian Deng, Erin Kosloski, Siddhi Patel, Zeke Barnett, Yiyang Nan, Alexander Kaplan, Sisira Aarukapalli, William Doan, Matthew Wang, Harsh Singh, and 1 others. 2024a. Hear me, see me, understand me: Audio-visual autism behavior recognition. IEEE Transactions on Multimedia. Shijian Deng, Wentian Zhao, Yu-Jhe Li, Kun Wan, Daniel Miranda, Ajinkya Kale, and Yapeng Tian. 2025b. Efficient self-improvement in multimodal large language models: model-level judge-free approach. In Second Conference on Language Modeling. Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. 2025c. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement. arXiv preprint arXiv:2503.17352. Yihe Deng, Pan Lu, Fan Yin, Ziniu Hu, Sheng Shen, James Zou, Kai-Wei Chang, and Wei Wang. 2024b. Enhancing large vision language models with selftraining on image comprehension. arXiv preprint arXiv:2405.19716. Xiangjue Dong, Maria Teleki, and James Caverlee. 2024. survey on llm inference-time selfimprovement. arXiv preprint arXiv:2412.14352. EvolvingLMMs-Lab. 2025. open-r1-multimodal: fork to add multimodal model training to openhttps://github.com/EvolvingLMMs-Lab/ r1. open-r1-multimodal. Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei Liu, Zihao Li, and 1 others. 2025. comprehensive survey of self-evolving ai agents: new paradigm bridging foundation models and lifelong agentic systems. arXiv preprint arXiv:2508.07407. Yunhao Fang, Ligeng Zhu, Yao Lu, Yan Wang, Pavlo Molchanov, Jan Kautz, Jang Hyun Cho, Marco Pavone, Song Han, and Hongxu Yin. Vilaˆ 2: Vlm augmented vlm with self-improvement. Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, and Stefano Soatto. 2024. Multi-modal hallucination control by visual information grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1430314312. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. 2025a. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776. Steven Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. 2021. survey of data augmentation approaches for nlp. arXiv preprint arXiv:2105.03075. Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, and 1 others. 2025. survey of self-evolving agents: On path to artificial super intelligence. arXiv preprint arXiv:2507.21046. Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, and 1 others. 2023. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370. Seyed Kamyar Seyed Ghasemipour, Ayzaan Wahid, Jonathan Tompson, Pannag Sanketi, and Igor Mordatch. 2025. Self-improving embodied foundation models. arXiv preprint arXiv:2509.15155. Jie Gui, Tuo Chen, Jing Zhang, Qiong Cao, Zhenan Sun, Hao Luo, and Dacheng Tao. 2024. survey on selfsupervised learning: Algorithms, applications, and future trends. IEEE Transactions on Pattern Analysis and Machine Intelligence. Yanjiang Guo, Jianke Zhang, Xiaoyu Chen, Xiang Ji, Yen-Jen Wang, Yucheng Hu, and Jianyu Chen. 2025a. Improving vision-language-action model with online reinforcement learning. arXiv preprint arXiv:2501.16664. Zilu Guo, Hongbin Lin, Zhihao Yuan, Chaoda Zheng, Pengshuo Qiu, Dongzhi Jiang, Renrui Zhang, ChunMei Feng, and Zhen Li. 2025b. Pisa: selfaugmented data engine and training strategy for 3d understanding with large models. arXiv preprint arXiv:2503.10529. Yujin Han, Hao Chen, Andi Han, Zhiheng Wang, Xinyu Lin, Yingya Zhang, Shiwei Zhang, and Difan Zou. 2025. Self-contradiction as self-improvement: Mitigating the generation-understanding gap in mllms. arXiv preprint arXiv:2507.16663. Jiayi He, Hehai Lin, Qingyun Wang, Yi Fung, and Heng Ji. 2024a. Self-correction is more than refinement: learning framework for visual and language reasoning tasks. arXiv preprint arXiv:2410.04055. Lehan He, Zeren Chen, Zhelun Shi, Tianyu Yu, Jing Shao, and Lu Sheng. 2024b. topic-level selfcorrectional approach to mitigate hallucinations in mllms. arXiv preprint arXiv:2411.17265. Tao He, Hao Li, Jingchang Chen, Runxuan Liu, Yixin Cao, Lizi Liao, Zihao Zheng, Zheng Chu, Jiafeng Liang, Ming Liu, and 1 others. 2025. Breaking the reasoning barrier survey on llm complex reasoning In Findings of through the lens of self-evolution. the Association for Computational Linguistics: ACL 2025, pages 73777417. Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. 2020. Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286. Jefferson Hernandez, Jing Shi, Simon Jenni, Vicente Ordonez, and Kushal Kafle. 2025. Improving large vision and language models by learning from panel of peers. arXiv preprint arXiv:2509.01610. Jixiang Hong, Yiran Zhang, Guanzhong Wang, Yi Liu, Ji-Rong Wen, and Rui Yan. 2025. Reinforcing multimodal understanding and generation with dual selfrewards. arXiv preprint arXiv:2506.07963. Zizhao Hu, Mohammad Rostami, and Jesse Thomason. 2025. Multi-modal synthetic data training and model collapse: Insights from vlms and diffusion models. arXiv preprint arXiv:2505.08803. Jiaxin Huang, Runnan Chen, Ziwen Li, Zhengqing Gao, Xiao He, Yandong Guo, Mingming Gong, and Tongliang Liu. 2025a. Mllm-for3d: Adapting multimodal large language model for 3d reasoning segmentation. arXiv preprint arXiv:2503.18135. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve. arXiv preprint arXiv:2210.11610. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. 2025b. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749. Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. 2017. Tgif-qa: Toward spatiotemporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 27582766. Weiyang Jin, Baihan Yang, Huan-ang Gao, Jingwei Zhao, Kangliang Chen, and Hao Zhao. Spa: Enhancing 3d multimodal llms with mask-based streamlining preference alignment. Ksenia Konyushkova, Christos Kaplanis, Serkan Cabi, and Misha Denil. 2025. Vision-language model dialog games for self-improvement. arXiv preprint arXiv:2502.02740. Jason Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. 2018. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):110. 2024. Mitigating object hallucinations in large visionlanguage models through visual contrastive decoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1387213882. Boyu Li, Haobin Jiang, Ziluo Ding, Xinrun Xu, Haoran Li, Dongbin Zhao, and Zongqing Lu. 2024a. Selu: Self-learning embodied mllms in unknown environments. arXiv preprint arXiv:2410.03303. Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, and 1 others. 2025. Vlrewardbench: challenging benchmark for visionlanguage generative reward models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2465724668. Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong. 2023a. Silkie: Preference distillation for large visual language models. arXiv preprint arXiv:2312.10665. Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong, and Qi Liu. 2024b. Vlfeedback: large-scale ai feedback dataset for large vision-language models alignment. arXiv preprint arXiv:2410.09421. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023b. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355. Zhenwen Liang, Kehan Guo, Gang Liu, Taicheng Guo, Yujun Zhou, Tianyu Yang, Jiajun Jiao, Renjie Pi, Jipeng Zhang, and Xiangliang Zhang. 2024. SceMQA: scientific college entrance level multimodal question answering benchmark. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 109119, Bangkok, Thailand. Association for Computational Linguistics. Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. 2023. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122. Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. 2021. Slake: semantically-labeled knowledge-enhanced dataset for medical visual quesIn 2021 IEEE 18th International tion answering. Symposium on Biomedical Imaging (ISBI), pages 16501654. IEEE. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024a. Visual instruction tuning. Advances in neural information processing systems, 36. Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Sannyuya Liu, Jintian Feng, Zongkai Yang, Yawei Luo, Qian Wan, Xiaoxuan Shen, and Jianwen Sun. 2024b. Comet:cone of experience enhanced large multimodal model for mathematical problem generation. Science China Information Sciences, 67(12):12. Wei Liu, Junlong Li, Xiwen Zhang, Fan Zhou, Yu Cheng, and Junxian He. 2024c. Diving into selfevolving training for multimodal reasoning. arXiv preprint arXiv:2412.17451. Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. 2025a. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785. Ziyu Liu, Yuhang Zang, Yushan Zou, Zijian Liang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. 2025b. Visual agentic reinforcement fine-tuning. arXiv preprint arXiv:2505.14246. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. 2023. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521. Run Luo, Haonan Zhang, Longze Chen, Ting-En Lin, Xiong Liu, Yuchuan Wu, Min Yang, Minzheng Wang, Pengpeng Zeng, Lianli Gao, and 1 others. 2024a. Mmevol: Empowering multimodal large language models with evol-instruct. arXiv preprint arXiv:2409.05840. Tiange Luo, Ang Cao, Gunhee Lee, Justin Johnson, and Honglak Lee. 2024b. Probing visual language priors in vlms. arXiv preprint arXiv:2501.00569. Xinyu Ma, Ziyang Ding, Zhicong Luo, Chi Chen, Zonghao Guo, Derek Wong, Xiaoyi Feng, and Maosong Sun. 2025. Deepperception: Advancing r1-like cognitive visual perception in mllms for knowledge-intensive visual grounding. arXiv preprint arXiv:2503.12797. Weijia Mao, Zhenheng Yang, and Mike Zheng Shou. 2025. Unirl: Self-improving unified multimodal models via supervised and reinforcement learning. arXiv preprint arXiv:2505.23380. Yassine Ouali, Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos. 2025. Clip-dpo: Visionlanguage models as source of preference for fixing hallucinations in lvlms. In European Conference on Computer Vision, pages 395413. Springer. Yi Peng, Xiaokun Wang, Yichen Wei, Jiangbo Pei, Weijie Qiu, Ai Jian, Yunzhuo Hao, Jiachun Pan, Tianyidan Xie, Li Ge, and 1 others. 2025. Skywork r1v: pioneering multimodal reasoning with chain-of-thought. arXiv preprint arXiv:2504.05599. Chau Pham, Hoang Phan, David Doermann, and Yunjie Tian. 2024. Personalized large vision-language models. arXiv preprint arXiv:2412.17610. Renjie Pi, Jianshu Zhang, Tianyang Han, Jipeng Zhang, Rui Pan, and Tong Zhang. 2024. Personalized visual instruction tuning. arXiv preprint arXiv:2410.07113. Leigang Qu, Haochuan Li, Wenjie Wang, Xiang Liu, Juncheng Li, Liqiang Nie, and Tat-Seng Chua. 2024. Silmm: Self-improving large multimodal models for compositional text-to-image generation. arXiv preprint arXiv:2412.05818. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36. Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. 2018. Object hallucination in image captioning. arXiv preprint arXiv:1809.02156. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasonarXiv preprint ing in open language models. arXiv:2402.03300. Guohao Sun, Can Qin, Huazhu Fu, Linwei Wang, and Zhiqiang Tao. 2024a. Stllava-med: Selftraining large language and vision assistant for arXiv preprint medical question-answering. arXiv:2406.19973. Guohao Sun, Can Qin, Jiamian Wang, Zeyuan Chen, Ran Xu, and Zhiqiang Tao. 2024b. Sq-llava: Selfquestioning for large vision-language assistant. In European Conference on Computer Vision, pages 156172. Springer. Guohao Sun, Can Qin, Jiamian Wang, Zeyuan Chen, Ran Xu, and Zhiqiang Tao. 2025a. Sq-llava: Selfquestioning for large vision-language assistant. In European Conference on Computer Vision, pages 156172. Springer. Haoyuan Sun, Jiaqi Wu, Bo Xia, Yifu Luo, Yifei Zhao, Kai Qin, Xufei Lv, Tiantian Zhang, Yongzhe Chang, and Xueqian Wang. 2025b. Reinforcement fine-tuning powers reasoning capability of multimodal large language models. arXiv preprint arXiv:2505.18536. Zeyi Sun, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Tong Wu, Dahua Lin, and Jiaqi Wang. 2025c. Seagent: Self-evolving computer use agent with autonomous learning from experience. arXiv preprint arXiv:2508.04700. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, LiangYan Gui, Yu-Xiong Wang, Yiming Yang, and 1 others. 2023. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525. Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, and Ziwei Liu. 2025a. Mmsearchr1: Incentivizing lmms to search. arXiv preprint arXiv:2506.20670. Weijia Wu, Chen Gao, Joya Chen, Kevin Qinghong Lin, Qingwei Meng, Yiming Zhang, Yuke Qiu, Hong Zhou, and Mike Zheng Shou. 2025b. Reinforcement learning in vision: survey. arXiv preprint arXiv:2508.08189. Wentao Tan, Qiong Cao, Yibing Zhan, Chao Xue, and Changxing Ding. 2024. Beyond human data: Aligning multimodal large language models by iterative self-evolution. arXiv preprint arXiv:2412.15650. Xun Wu, Shaohan Huang, and Furu Wei. 2024. Multimodal large language model is human-aligned annotator for text-to-image generation. arXiv preprint arXiv:2404.15100. Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang, Dacheng Tao, and Jingren Zhou. 2024. survey on self-evolution of large language models. arXiv preprint arXiv:2404.14387. Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, and 1 others. 2024a. Vigc: Visual instruction generation and correction. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 53095317. Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. 2023. An llm-free multi-dimensional benchmark for mllms hallucination evaluation. arXiv preprint arXiv:2311.07397. Qiuchen Wang, Ruixue Ding, Yu Zeng, Zehui Chen, Lin Chen, Shihang Wang, Pengjun Xie, Fei Huang, and Feng Zhao. 2025a. Vrag-rl: Empower visionperception-based rag for visually rich information understanding via iterative reasoning with reinforcement learning. arXiv preprint arXiv:2505.22019. Shaowen Wang, Xinyuan Chen, and Yao Xu. 2025b. Self-improvement for audio large language arXiv preprint model using unlabeled speech. arXiv:2507.20169. Wanfu Wang, Qipeng Huang, Guangquan Xue, Xiaobo Liang, and Juntao Li. 2025c. Learning active perception via self-evolving preference optimization for gui grounding. arXiv preprint arXiv:2509.04243. Xiyao Wang, Jiuhai Chen, Zhaoyang Wang, Yuhang Zhou, Yiyang Zhou, Huaxiu Yao, Tianyi Zhou, Tom Goldstein, Parminder Bhatia, Furong Huang, and 1 others. 2024b. Enhancing visual-language modality alignment in large vision language models via selfimprovement. arXiv preprint arXiv:2405.15973. Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei, Jianhao Zhang, Yunzhuo Hao, Xuchen Song, and 1 others. 2025. Skywork r1v2: Multimodal hybrid reinforcement learning for reasoning. arXiv preprint arXiv:2504.16656. Wenyi Xiao, Ziwei Huang, Leilei Gan, Wanggui He, Haoyuan Li, Zhelun Yu, Fangxun Shu, Hao Jiang, and Linchao Zhu. 2024. Detecting and mitigating hallucination in large vision language models via fine-grained ai feedback. arXiv preprint arXiv:2404.14233. Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. 2024. Llava-critic: Learning to evaluate multimodal models. arXiv preprint arXiv:2410.02712. Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. 2017. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 16451653. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, and 1 others. 2025. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. 2025. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. arXiv preprint arXiv:2504.08066. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, and 1 others. 2025. R1onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615. Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, and 1 others. 2024. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2024. survey on multimodal large language models. National Science Review, page nwae403. Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. 2023. Woodpecker: Hallucination correction for multimodal large language models. arXiv preprint arXiv:2310.16045. Jiaxuan You, Mingjie Liu, Shrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. 2024. Llm-evolve: Evaluation for llms evolving capability on benchmarks. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1693716942. Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, and 1 others. 2024a. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13807 13816. Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, and 1 others. 2024b. Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220. Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. 2019. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 91279134. Junpeng Yue, Xinru Xu, Börje Karlsson, and Zongqing Lu. 2024a. Mllm as retriever: Interactively learning multimodal retrieval for embodied agents. arXiv preprint arXiv:2410.03450. Tongtian Yue, Jie Cheng, Longteng Guo, Xingyuan Dai, Zijia Zhao, Xingjian He, Gang Xiong, Yisheng Lv, and Jing Liu. 2024b. Sc-tune: Unleashing selfconsistent referential comprehension in large vision language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1307313083. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, and 1 others. 2024c. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9556 9567. Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From recognition to cognition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67206731. Simon Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Peter Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, and 1 others. 2025. Fine-tuning large visionlanguage models as decision-making agents via reinforcement learning. Advances in Neural Information Processing Systems, 37:110935110971. Yufei Zhan, Yousong Zhu, Shurong Zheng, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. 2025. Vision-r1: Evolving human-free alignment in large vision-language models via vision-guided reinforcement learning. arXiv preprint arXiv:2503.18013. Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan Su, Chenhui Chu, and Dong Yu. 2024a. Mmllms: Recent advances in multimodal large language models. arXiv preprint arXiv:2401.13601. Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. 2025. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937. Jinrui Zhang, Teng Wang, Haigang Zhang, Ping Lu, and Feng Zheng. 2024b. Reflective instruction tuning: Mitigating hallucinations in large vision-language models. arXiv preprint arXiv:2407.11422. Mengxi Zhang, Wenhao Wu, Yu Lu, Yuxin Song, Kang Rong, Huanjin Yao, Jianbo Zhao, Fanglong Liu, Yifan Sun, Haocheng Feng, and 1 others. 2024c. Automated multi-level preference for mllms. arXiv preprint arXiv:2405.11165. Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Ziyu Guo, Shicheng Li, Yichi Zhang, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, and 1 others. 2024d. Mavis: Mathematical visual instruction tuning with an automatic data engine. arXiv preprint arXiv:2407.08739. Jiaxing Zhao, Xihan Wei, and Liefeng Bo. 2025a. R1omni: Explainable omni-multimodal emotion recognition with reinforcement learning. arXiv preprint arXiv:2503.05379. Xiangyu Zhao, Shengyuan Ding, Zicheng Zhang, Haian Huang, Maosong Cao, Weiyun Wang, Jiaqi Wang, Xinyu Fang, Wenhai Wang, Guangtao Zhai, and 1 others. 2025b. Omnialign-v: Towards enhanced alignment of mllms with human preference. arXiv preprint arXiv:2502.18411. Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. 2023. Beyond hallucinations: Enhancing lvlms through hallucinationaware direct preference optimization. arXiv preprint arXiv:2311.16839. Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. 2025. R1zeros\" aha moment\" in visual reasoning on 2b non-sft model. arXiv preprint arXiv:2503.05132. Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. 2024a. Aligning modalities in vision large language models via preference finetuning. arXiv preprint arXiv:2402.11411. Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. 2024b. Calibrated self-rewarding vision language models. arXiv preprint arXiv:2405.14622. Yiyang Zhou, Zhaoyang Wang, Tianle Wang, Shangyu Xing, Peng Xia, Bo Li, Kaiyuan Zheng, Zijian Zhang, Zhaorun Chen, Wenhao Zheng, and 1 others. Anyprefer: An automatic framework for preference data synthesis. In Neurips Safe Generative AI Workshop 2024. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592. Ke Zhu, Liang Zhao, Zheng Ge, and Xiangyu Zhang. 2024. Self-supervised visual preference alignment. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 291300. Orr Zohar, Xiaohan Wang, Yonatan Bitton, Idan Szpektor, and Serena Yeung-Levy. 2024. Video-star: Selftraining enables video instruction tuning with any supervision. arXiv preprint arXiv:2407.06189."
        },
        {
            "title": "A Full Taxonomy",
            "content": "For space reasons, the main paper provides only an overview of our taxonomy of self-improvement in MLLMs. This appendix presents the complete hierarchy covering Data Collection (4), Data Organization (5), and Model Optimization (6), and annotates each branch with representative works. See Figure 3 for the full diagram."
        },
        {
            "title": "B Applications",
            "content": "Self-improvement can be particularly useful for applications that lack sufficient related instruction data. Models can autonomously generate the required data and conduct self-improvement to acquire new skills for downstream tasks. B.1 Math & Science Tasks in fields like math and many other sciences require advanced reasoning sometimes including multimodal reasoning to address. However, the underlying reasoning data is not abundant, since humans seldom write down all the details of their reasoning steps, let alone reasoning that occurs via unconscious pathways. Self-improvement frameworks combined with peer-improvement have enabled MLLMs to autonomously generate and refine multimodal reasoning content, significantly reducing reliance on human-annotated data. For example, methods like MAVIS (Zhang et al., 2024d) and COMET (Liu et al., 2024b) enhance mathematical reasoning by generating problems and visual explanations through structured prompts and alignment techniques. Similarly, frameworks like G-LLaVA (Gao et al., 2023) integrate geometryspecific tasks with generated datasets, achieving state-of-the-art performance on benchmarks like ScienceQA (Lu et al., 2022), SceMQA (Liang et al., 2024) and PHYSICS (Feng et al., 2025b). B.2 Control Self-improvement in MLLMs can be applied to real-world applications such as control. Recent work (Zhou et al.) proposes an automatic framework for preference data synthesis and employs an MLLM with an image segmentation model as tool, judged by GPT-4o, to improve object segmentation and trajectory generation. The proposed method achieved 15.50% improvement in four visuo-motor control tasks. Random Sampling (4.1) VLM-RLAIF(Ahn et al., 2024b), RLAIF-V(Yu et al., 2024b), i-SRT(Ahn et al., 2024a), AnyPrefer(Zhou et al.), FiSAO(Cui et al., 2024), TPO(He et al., 2024b) 1. Data Collection (4) Guided Data Generation (4.2) VIGC(Wang et al., 2024a), SQ-LLaVA(Sun et al., 2024b), SC-Tune(Yue et al., 2024b), Video-STaR(Zohar et al., 2024), VILA2(Fang et al.), SCL(He et al., 2024a), R3V(Cheng et al., 2024) Negative Samples (4.3) POVID(Zhou et al., 2024a), M3ID(Favero et al., 2024), SeVa(Zhu et al., 2024), STIC(Deng et al., 2024b), BDHS(Amirloo et al., 2024), ESI(Deng et al., 2025b), SENA(Tan et al., 2024), Image-DPO(Luo et al., 2024b) L i m r - S 2. Data Organization (5) 3. Model Optimization (6) Verification Methods (5.1) Dataset Arrangement (5.2) CollectionOrganization Loop (5.3) Supervised Fine-tuning (6.1) Reinforcement Learning (6.2) Direct Preference Optimization (6.3) Rule-Based Verification (5.1.1) Model-Based Verification (5.1.2) Verification from the Environment (5.1.3) SC-Tune(Yue et al., 2024b), BDHS(Amirloo et al., 2024), Video-STaR(Zohar et al., 2024), SCL(He et al., 2024a) VLM-RLAIF(Ahn et al., 2024b), SIMA(Wang et al., 2024b), RLAIF-V(Yu et al., 2024b), i-SRT(Ahn et al., 2024a), STLLaVA-Med(Sun et al., 2024a), LLaVA-Critic(Xiong et al., 2024), R3V(Cheng et al., 2024) RL4VLM(Zhai et al., 2025), Visual - ARFT(Liu et al., 2025b), MMSearch - R1(Wu et al., 2025a), VRAG - RL(Wang et al., 2025a), SEAgent(Sun et al., 2025c), LASER(Wang et al., 2025c), iRe - VLA(Guo et al., 2025a), ConRFT(Chen et al., 2025d) Filtering & Rejecting M-STaR (Liu et al., 2024c) Editing & Refining VIGC(Wang et al., 2024a), RIT(Zhang et al., 2024b), SCL(He et al., 2024a), TPO(He et al., 2024b) Archiving & Scheduling Co - Improvement(Han et al., 2025) Judge Yielding LLaVA - Critic(Xiong et al., 2024) MMEvol(Luo et al., 2024a), Beyond Human Data(Tan et al., 2024), - STaR(Liu et al., 2024c), OpenVLThinker(Deng et al., 2025c), UniRL(Mao et al., 2025), SUDER(Hong et al., 2025), C2 - Evo(Chen et al., 2025c) VIGC(Wang et al., 2024a), SQ-LLaVA(Sun et al., 2024b), VideoSTaR(Zohar et al., 2024), VILA2(Fang et al.), MMEvol(Luo et al., 2024a), LLaVA-Critic(Xiong et al., 2024), PVIT(Pi et al., 2024), R3V(Cheng et al., 2024), M-STaR(Liu et al., 2024c), Mulberry7B(Yao et al., 2024), VLM Dialog Games(Konyushkova et al., 2025) SC-Tune(Yue et al., 2024b), RL4VLM(Zhai et al., 2025), MSTaR(Liu et al., 2024c), iRe-VLA(Guo et al., 2025a), ConRFT(Chen et al., 2025d), Visual-RFT(Liu et al., 2025a), VisualThinkerR1-Zero(Zhou et al., 2025), R1-Omni(Zhao et al., 2025a), Vision-R1(Huang et al., 2025b), Curr-ReFT(Deng et al., 2025a), R1-Onevision(Yang et al., 2025), R1-VL(Zhang et al., 2025), Vision-R1(Zhan et al., 2025), Video-R1(Feng et al., 2025a), Skywork R1V(Peng et al., 2025), Skywork R1V2(Wei et al., 2025) POVID(Zhou et al., 2024a), M3ID(Favero et al., 2024), SeVa(Zhu et al., 2024), HSA-DPO(Xiao et al., 2024), CSR(Zhou et al., 2024b), SIMA(Wang et al., 2024b), RLAIF-V(Yu et al., 2024b), AMP(Zhang et al., 2024c), STIC(Deng et al., 2024b), i-SRT(Ahn et al., 2024a), BDHS(Amirloo et al., 2024), CLIP-DPO(Ouali et al., 2025), AnyPrefer(Zhou et al.), SCL(He et al., 2024a), FiSAO(Cui et al., 2024), ESI(Deng et al., 2025b), TPO(He et al., 2024b), SENA(Tan et al., 2024), Image-DPO(Luo et al., 2024b), SHAPE(Chen et al., 2025a) Figure 3: The taxonomy of three steps for self-improvement in MLLMs. Each step can involve different methods based on requirements. B.3 Healthcare Exciting advancements, such as STLLaVAMed (Sun et al., 2024b), have introduced the SelfTraining Large Language and Vision Assistant for medical applications. This innovative approach focuses on training policy model (an MLLM) to auto-generate medical visual instruction data, improving data efficiency through Direct Preference Optimization (DPO). Notably, more robust and larger model (e.g., GPT-4o) serves as biomedical expert, guiding the DPO fine-tuning process on the auto-generated data to effectively align the policy model with human preferences. This method achieves impressive zero-shot performance on three major medical VQA benchmarks: VQARAD, SLAKE, and PathVQA, while using only 9% of the available medical data. Additionally, LLaVAASD (Deng et al., 2024a) has explored using selfimprovement approaches to enable MLLMs not only to assist in screening but also to provide explanations for their decision-making processes. This advancement offers more explainable AI-assisted screening approach, enhancing transparency and user trust. B.4 Personalization With self-improvement approaches, users can easily personalize MLLMs (Pi et al., 2024; Pham et al., 2024) using automated pipelines to construct datasets and train models for their own use, requiring minimal additional effort. B.5 3D and Embodied Intelligence Recent advances in self-improvement for MLLMs also benefit areas such as 3D and embodied intelligence. notable example is the MLLM-For3D framework (Huang et al., 2025a), which introduces method for achieving 3D reasoning segmentation without the need for explicitly labeled 3D training data. This framework leverages pre-trained 2D MLLMs to generate multi-view pseudo segmentation masks along with corresponding text embeddings. These 2D masks are then projected into 3D space and aligned with the text embeddings, effectively transferring the 2D models understanding to the 3D realm. Similarly, PiSA-Engine (Point-SelfAugmented-Engine) (Guo et al., 2025b) has been introduced as novel approach for generating instruction point-language datasets enriched with 3D spatial semantics. Streamlining Preference Alignment (Jin et al.), post-training stage designed for MLLMs equipped with 3D encoders, enhances the ability of MLLMs to understand and reason about 3D spatial relationships, which is fundamental for their effective application in 3D environments. Self-improvement offers powerful paradigm for enabling MLLM agents to improve their performance in embodied tasks through interaction with their environment. An example is SELU (SelfLearning in Unknown Environments) (Li et al., 2024a), which allows MLLMs to improve their capabilities in embodied tasks without relying on explicit external human or environmental feedback. SELU adopts an actor-critic framework consisting of two MLLM components: the critic MLLM is responsible for evaluating the outcomes of the actors actions and for improving its understanding of the environment. Simultaneously, the actor MLLM is improved based on the self-feedback provided by the critic. MART (MLLM As ReTriever) (Yue et al., 2024a) is another example that enhances the performance of embodied agents by utilizing interaction data to fine-tune an MLLM retriever based on preference learning."
        },
        {
            "title": "C Challenges and Opportunities",
            "content": "Self-improvement in MLLMs presents unique challenges and opportunities compared to text-only LLMs. We expand on these below: C.1 Uniqueness of Multi-Modality Many tasks and objectives in MLLMs fundamentally differ from those in LLMs. While LLMs primarily focus on maximizing the likelihood of text sequences, MLLMs must handle objectives incorporating spatial and temporal understanding. For instance, tasks involving images or videos require objectives beyond sequential prediction: Spatial Understanding (e.g., Object Detection): Requires predicting bounding boxes = {bk} and classes = {ck}. The objective might take the form: (cid:88) (cid:0)Lcls(ckI; θ) + λLreg(bkI; θ)(cid:1) Lspatial = where Lcls is classification loss and Lreg is bounding box regression loss. Temporal Understanding (e.g., Video Action Recognition): Requires understanding sequences of frames = (f1, . . . , fm) to predict an action a. The objective could be: Ltemporal = log (aV ; θ) Cross-modal alignment and distillation without high-quality data (Liu et al., 2024a) might introduce multimodal hallucination. While text-only LLMs can hallucinate facts, MLLMs can hallucinate content inconsistent with an input image or other modality. C.2 Better Seed Models and Emerging Modalities Current self-improvement in MLLMs primarily operates on limited set of modalities, typically Mcurrent = {Text, Image, Video}. The action space for self-correction or data generation is often confined to textual outputs. However, significant potential lies in emerging modalities like Audio (A) (Wang et al., 2025b), 3D data (D), and Embodied Actions (Act) (Ghasemipour et al., 2025), extending the modality set to Memerging = Mcurrent {A, D, Act, . . . }. Expanding to these domains, particularly embodied AI, drastically increases the complexity and dimensionality of the action space. Selfimprovement must transition from generating primarily discrete textual actions Atext to generating sequences of potentially continuous or highdimensional actions at Aembodied required for interaction within an environment E. The optimization objective shifts towards maximizing expected return in sequential decision-making tasks: max πθ Eτ πθ (cid:35) γtR(st, at) (cid:34) (cid:88) t=0 where τ = (s0, a0, s1, a1, . . . ) is trajectory generated by policy πθ in environment E, st is the state (often multimodal), at Aembodied, is the reward function, and γ is the discount factor. Works like (Zhai et al., 2025; Guo et al., 2025a; Chen et al., 2025d) are beginning to explore self-improvement in these expanded action and modality spaces. C.3 Omni I/O limitation in MLLM self-improvement is the restricted input/output pipeline. Current models often follow mappings like : (Min, Tprompt) Tout, where Min might be or . Generating the non-textual input data (e.g., images I) often requires external datasets or separate generative models (Luo et al., 2024b). This also limits MLLMs capabilities of self-verification and correction without extra models while forced to do so may compounding hallucinations. True \"Omni I/O\" capability implies model Momni that can handle arbitrary combinations of modalities as both input and output. Let be the set of all relevant modalities. The mapping becomes: Momni : {mi}Nin i=1 {m j}Nout j=1 : I, Momni where each mi and M. For selfimprovement, this means the model should ideally be able to generate its own training data across modalities, such as Momni : , Momni : (I, A) (T, ), etc., potentially in an interleaved manner. Recent advances like native image generation in GPT-4o/Gemini and open-source efforts like Qwen2.5-Omni (Xu et al., 2025) suggest potential towards this goal, where self-improvement could enhance generation and understanding across text, vision, and audio within single loop. Some work (Qu et al., 2024; Zhao et al., 2025a) has begun to unify these areas. C.4 Biases and Robust Verification After obtaining initial generated data, further verification and organization of this raw data are necessary, as we formulated these as the next steps for conducting self-improvement after collecting data. However, even with these controls, there is still no guarantee that bias and incorrectness can be eliminated. This is significant challenge and an unsolved problem in self-improvement, as the bias may accumulate and potentially stop further recursive improvement, which presents good opportunity for future research. The feasibility of self-improvement is intrinsically linked to the ability to reliably verify the quality or correctness of the models outputs. This echoes the computational complexity concept related to vs NP: generating optimal outputs might be hard, but verifying them should ideally be tractable. We can formalize this with verification function (x, y), where is the input and is the MLLMs output (which could be multimodal). (x, y) returns score or binary judgment (correct/incorrect, high/low quality). Self-improvement often relies on optimizing parameters θ based on this verification: max θ E(x,y)P (x,yθ)[V (x, y)] or using implicitly as reward signal in reinforcement learning. The core principle is: Effective self-improvement is contingent upon the existence of an efficient and reliable verification mechanism . If the complexity of verification, Complexity(V ), is low (e.g., polynomial time), then iterative improvement guided by becomes practical. As the real world is inherently multimodal, MLLMs could potentially leverage environmental feedback or cross-modal consistency checks as powerful verification signals (Ahn et al., 2024b), potentially making more robust compared to text-only domains. C.5 Generalization Current self-improvement pipelines often focus on specific tasks τ (e.g., reducing hallucinations, improving reasoning on benchmarks) and may exhibit diminishing returns after finite number of iterations: θi+1 = Improve(θi, Di, τ ), = 0, . . . , 1 where Di is the data used/generated at iteration i. Performance might plateau, i.e., (θk, τ ) (θk+1, τ ). major future direction is developing general MLLM self-improvement framework capable of recursive enhancement across universal set of tasks Tuniv without plateauing. The idealized goal is process: Mi+1 = SelfImprove(Mi, Tuniv, WorldKnowledge) such that the models capabilities C(Mi) monotonically increase across Tuniv as : τ Tuniv, lim (Mi, τ ) = OptimalPerformance(τ ) This requires mechanisms that not only refine parameters but potentially adapt the models architecture, learning algorithms, and knowledge representation recursively, moving beyond narrow, task-specific improvement loops towards universal, open-ended capability growth. C.6 Scalability Although we have collected many models and frameworks in this survey, we found that many of these methods are normally conducted on very small scale. Therefore, the performance gain is not as significant as in many other model developments that simply scale things up. It would be more practical and impactful for the real world deployment if the approaches had satisfactory scalability which would address the data shortage problem and therefore allow the model development to be further scaled up. C.7 Higher Autonomy Although current self-improvement MLLM frameworks can reduce the human workload from data generation and verification perspective, human effort is still required in many other areas, such as proposing ideas, developing codebases, conducting experiments, and demonstrating or evaluating the final outcome. To overcome this bottleneck and achieve fully autonomous self-improvement requires higher autonomy, such as R&D automation (Lu et al., 2024; Yamada et al., 2025). Meanwhile, these R&D skills could themselves be further boosted by the improved base MLLMs, for instance, through better multimodal understanding of the environment. This mutually beneficial self-improvement paradigm can increase effectiveness by removing bottlenecks, eliminating blind spots, and raising the upper bound."
        },
        {
            "title": "D Related Surveys",
            "content": "There are several surveys on multimodal large language models (MLLMs) (Yin et al., 2024) and self-improvement/evolution in LLMs (Tao et al., 2024; He et al., 2025). However, to the best of our knowledge, no existing survey specifically addresses self-improvement in MLLMs. To fill this gap, we have collected related papers and systematically constructed this survey. More recent works adjacent to our scope include (i) surveys on reinforcement learning for MLLMs (Sun et al., 2025b; Wu et al., 2025b), which is specific domain of self-improvement, and (ii) surveys on self-evolving agents that focus on agents rather than MLLMs (Gao et al., 2025; Fang et al., 2025). Other surveys focus on topics such as selfsupervised learning (Gui et al., 2024), selftraining (Amini et al., 2024), synthetic data (Bauer et al., 2024), or data augmentation (Feng et al., 2021), which are loosely connected at high level. Our survey is the first to focus specifically on self-improvement in MLLMs, collecting broad range of methods for automating MLLM improvement with less human effort. Concretely, we structure the field into three-stage pipeline: data collection, data organization, and model optimization to analyze different techniques used in each module. We also formulate unified levels of autonomy for self-improvement in MLLMs to guide future development toward more effective self-improvement methodologies."
        }
    ],
    "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence",
        "The University of Texas at Dallas",
        "University of Notre Dame",
        "University of Toronto"
    ]
}