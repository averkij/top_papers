{
    "paper_title": "SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer",
    "authors": [
        "Tongcheng Fang",
        "Hanling Zhang",
        "Ruiqi Xie",
        "Zhuo Han",
        "Xin Tao",
        "Tianchen Zhao",
        "Pengfei Wan",
        "Wenbo Ding",
        "Wanli Ouyang",
        "Xuefei Ning",
        "Yu Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Transformers have recently demonstrated remarkable performance in video generation. However, the long input sequences result in high computational latency due to the quadratic complexity of full attention. Various sparse attention mechanisms have been proposed. Training-free sparse attention is constrained by limited sparsity and thus offers modest acceleration, whereas training-based methods can reach much higher sparsity but demand substantial data and computation for training. In this work, we propose SALAD, introducing a lightweight linear attention branch in parallel with the sparse attention. By incorporating an input-dependent gating mechanism to finely balance the two branches, our method attains 90% sparsity and 1.72x inference speedup, while maintaining generation quality comparable to the full attention baseline. Moreover, our finetuning process is highly efficient, requiring only 2,000 video samples and 1,600 training steps with a batch size of 8."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 2 ] . [ 1 5 1 5 6 1 . 1 0 6 2 : r SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer Tongcheng Fang1,2,, Hanling Zhang3, Ruiqi Xie1 Zhuo Han1 Xin Tao2 Tianchen Zhao1 Pengfei Wan2 Wenbo Ding1 Wanli Ouyang3 Xuefei Ning1 Yu Wang1 1Tsinghua University 2Kling Team, Kuaishou Technology 3The Chinese University of Hong Kong"
        },
        {
            "title": "Abstract",
            "content": "Diffusion Transformers have recently demonstrated remarkable performance in video generation. However, the long input sequences result in high computational latency due to the quadratic complexity of full attention. Various sparse attention mechanisms have been proposed. Training-free sparse attention is constrained by limited sparsity and thus offers modest acceleration, whereas training-based methods can reach much higher sparsity but demand substantial data and computation for training. In this work, we propose SALAD, introducing lightweight linear attention branch in parallel with the sparse attention. By incorporating an input-dependent gating mechanism to finely balance the two branches, our method attains 90% sparsity and 1.72 inference speedup, while maintaining generation quality comparable to the full attention baseline. Moreover, our finetuning process is highly efficient, requiring only 2,000 video samples and 1,600 training steps with batch size of 8. 1. Introduction Transformers face efficiency challenges in modeling long sequences due to the quadratic complexity of the attention mechanism [19]. Especially for the video diffusion transformer [20, 29], there is challenge when generating highresolution and long-duration videos. To reduce the inference cost of the video generation, various sparse attention mechanisms have been proposed [23, 24, 28, 33, 35, 37, 40]. Leveraging the inherent locality and sparsity of the attention 1 Co-first authors. 2 This work was conducted during the authors internship at Kling Team, Kuaishou Technology. 3 Corresponding authors: Yu Wang (yu-wang@mail.tsinghua.edu.cn), Xuefei Ning (foxdoraame@gmail.com). Figure 1. Comparison of Our Method and Other Sparse Attention Mechanisms. Score versus speedup, with point size representing densitysmaller points indicate lower computational density. The Summation Score is computed as summation of VBench metrics (Subject Consistency, Background Consistency, Imaging Quality, and Text Consistency). This reflects overall qualityefficiency trade-off. Models compared include our approach, SVG2 [28], PARO [40], ST-SWA [33], and ST-SWA + LoRA [7]. map, these methods restrict each query to attend subset of keys and values, thereby reducing the computation. Higher sparsity leads to lower computational cost in sparse attention. However, training-free sparse attention can only achieve limited sparsity [24, 28, 33, 35], such as 40% 60% under sequence length of 30k. Training-based approaches can achieve remarkable sparsity of 80% to 95%, yet they incur substantial training overhead in both data preparation and computation. For instance, VMoBA [23] requires approximately 182 GPU hours to train their sparse attention model on the large-scale Koala-36M dataset [22]. Fine-tuning with Low-Rank Adaptation (LoRA) [7] provides parameter-efficient approach for recovering perfor1 mance after compression of large models [6, 36]. In Transformers, LoRA typically adapts the attention projection matrices (Wq, Wk, Wv, Wo) while keeping all other parameters frozen. But when applied to ultra-sparse attention models and fine-tuned with limited training budget, achieving performance comparable to the dense counterpart becomes challenging. As shown in Fig. 1, the sparse attention model fine-tuned with LoRA still exhibits degradation relative to the full-attention baseline, even under moderate sparsity level of 77.4%. The evaluation score is computed as the sum of selected VBench [8] metrics, including Subject Consistency, Background Consistency, Imaging Quality, and Text Consistency (Overall Consistency). Fig. 2 presents the results generated by the sparse attention model (specifically using sliding-window mask), along with those from the full-attention and LoRA-tuned variants. While LoRA fine-tuning substantially improves the generative quality of the sparse attention model, the videos still exhibit artifacts inherently tied to the sparse pattern. For example, although the text prompt describes dog, the LoRA-tuned model initially generates two dogs within the same frame, which gradually merge into one as the video progresses. Such textual and temporal inconsistencies highlight the limited effectiveness of tuning that relies solely on LoRA. Intuitively, these artifacts arise from the intrinsic limitations of sparse attention, which restricts cross-token interactions and leads to information loss. For example, sliding-window attention constrains each token to attend only to its local neighbors within single layer. Even with multiple stacked layers, the receptive field cannot effectively expand to cover the entire sequence [25], thereby limiting the models ability to capture long-range dependencies. This incomplete context aggregation manifests as the text and temporal inconsistencies observed in Fig. 2. In principle, LoRA can compensate for the loss of crosstoken interactions through its multi-layer stacked computations. During fine-tuning, token representations in each attention block are refined, enabling the current layer to incorporate information from distant tokens. Through layerby-layer propagation, deeper layers can thus access broader contextual information, theoretically restoring long-range interactions. Consequently, LoRA fine-tuning holds the potential to recover performance comparable to full attention. However, our experiments show that achieving substantial recovery remains difficult under limited training budgets. To compensate for the information loss in sparse attention, we introduce linear attention branch. This branch enables global token mixing, offering the potential to restore critical cross-token interactions that are neglected under ultra-high sparsity attention. Moreover, its O(N ) computational complexity incurs minimal overhead during inference, allowing the model to retain most of the efficiency gains provided by high-sparsity attention. Figure 2. Comparison of Full Attention Model, Sparse Attention Model and Sparse Attention Model with LoRA Tuning. However, we empirically find that although the linear attention branch can help supplement the lost information, it struggles to model the whole long sequences. Thus, the sparse attention should dominate the output while the linear attention branch should be set to an auxiliary role. To achieve practical sparse-linear attention , we propose SALAD (High-Sparsity Attention paralleling with Linear Attention for Diffusion Transformer), an efficient attention module to help achieve high-sparsity attention via linear attention branch. We first introduce linear attention branch paralleling with the sparse attention. And then we design an Input-Dependent Scalar Gate to finely control the impact of linear attention layer by layer. Our contribution can be summarized as follows: 1. We propose SALAD, an efficient attention architecture designed to enhance the performance of ultra-sparse attention through parallel linear attention branch. Most parameters of the linear branch are shared with the sparse attention module, introducing only about 4.99% additional parameters relative to the pretrained model. 2. We observe that in hybrid sparse-linear attention, it is essential to regulate the influence of the linear attention branch. To this end, we design an input-dependent scalar gate that precisely controls its contribution, effectively improving overall model performance. 3. With just 2k open-source video samples and 1.6k training steps, SALAD achieves 90% sparsity and 1.72 inference speedup, while maintaining generation quality comparable to the dense model. 2. Related Work 2.1. Video Diffusion Models Diffusion Transformer [17] has demonstrated its advantage in video generation. Earlier approaches such as Latte [16] decouple video sequences into spatial and temporal tokens and apply attention to each separately, which limits the models expressive capacity. More recent works [1, 20, 29] 2 instead process the entire video as unified token sequence and perform full attention across all tokens. While this strategy significantly improves generation quality, it also introduces substantial memory and computational overhead due to the extended sequence length. 2.2. Efficient Video Diffusion Transformer To address the computational challenges in video diffusion transformers, various efficient video generation strategies have been proposed. Quantization [3, 10, 39] reduces memory usage and latency in video diffusion models by lowering weight and activation precision. Caching mechanisms [5, 9, 12, 14] can reuse intermediate features across denoising timesteps, alleviating redundant computations. Timestep distillation [41] speeds up video generation by reducing the number of required sampling steps. Token merging compresses sequence length by combining similar tokens, mitigating memory and compute overheads for long videos [32]. Additionally, efficient attention methods have been explored lot to reduce the computation caused by the long video token sequence. Sparse attention reduces the costs by limiting cross-token interactions [24, 28, 31, 38, 40]. Linear attention mechanism is also adopted. It transforms the quadratic complexity of attention to linear complexity, effectively reducing the burden of modeling the long video token sequence [2, 21]. 2.3. LoRA for Model Compression Recovery LoRA [7] is an efficient fine-tuning technique that freezes the pre-trained model weights and injects trainable low-rank decomposition matrices. Beyond downstream task adaptation, LoRA has also been widely adopted for model compression recovery. For instance, Ma et al. [15] combine LoRA with Large Language Models pruning to mitigate performance degradation with minimal fine-tuning overhead. QLoRA [6] introduces quantized LoRA tuning for 4-bit model performance recovery, while LongLoRA [4] extends LoRA to normalization, embedding and FeedForward Network (FFN) layers for more effective longcontext adaptation under local attention. In this work, we find that LoRA fails to fully recover the performance of ultra-sparse (90%) Video Diffusion Transformers and propose SALAD as more effective tuning approach. 3. Preliminary 3.1. Sparse Attention Given queries (Q), keys (K), and values (V ) RN d, the full attention mechanism can be formalized as follows: Attention(Q, K, ) = softmax (cid:19) (cid:18) QK dk V. (1) The computation of the attention matrix QK incurs O(N 2d) memory and computation complexity, which is prohibitive for large . To reduce the high computational cost, sparse attention limits the interactions to only selected group of token pairs. This is formally done by incorporating sparsity mask {, 0}nn into the attention operation: (cid:18) QK + SparseAttention(Q, K, ) = softmax V. (cid:19) (2) Positions marked with are effectively excluded from the softmax operation. Sparse attention mechanisms can be categorized into static and dynamic approaches, depending on how the sparse mask is obtained. Static sparse attention predefined the sparse mask prior to inference, often through heuristic or data-driven calibration strategies [31, 33, 40] that exploit prior knowledge about attention patterns. In contrast, dynamic approaches determine the sparse mask on-the-fly during inference, allowing the sparsity pattern to adapt to the input data via learned or rule-based decision functions [24, 28]. Besides, the sparse mask can take different structural forms. In this work, we explore both static and dynamic sparse attention mechanisms, specifically the static slidingwindow attention (SWA) and the dynamic Top-k attention. Sliding-window attention (SWA) exploits the inductive bias of locality within the input sequence [31, 38]. Its sparse mask can be efficiently derived through calibration on small subset of data. Motivated by [24], we further apply spatial-temporal reordering on this design to better handle video tokens, termed ST-SWA. Details of the ST-SWA used in our experiments are provided in the Appendix A.1. Top-K attention dynamically constructs irregular sparse masks by retaining the most informative querykey pairs. For each query, only the keys with the highest importance scores are preserved, allowing the model to focus computation on salient interactions. In this work, we use Top-K sparse attention implementation of VMoBA [23]. In the meantime, Top-P attention adopts similar principle but retains variable number of keys whose cumulative importance exceeds threshold [13, 30, 34]. 3.2. Linear Attention Linear attention reduces the computational complexity from O(N 2d) in standard attention to O(N d2) by replacing the softmax operation with kernel function ϕ() applied to and K. This allows the attention weights to be approximated as ϕ(Q)ϕ(K), enabling associative reordering. Specifically, one can first compute = ϕ(K)V and = ϕ(K)1, and then obtain the output as = ϕ(Q)H ϕ(Q)Z . common choice for ϕ() is ReLU function [2, 26]. The naive relu-based linear attention can be formulated as: 3 Figure 3. Overview of SALAD Attention Module. Oi = (cid:88) j=1 ReLU(Qi)ReLU(Kj)Vj j=1 ReLU(Qi)ReLU(Kj) (cid:80)N (cid:16)(cid:80)N = ReLU(Qi) j=1 ReLU(Kj)Vj ReLU(Qi) (cid:16)(cid:80)N j=1 ReLU(Kj) (cid:17) (cid:17) . (3) 4. Methods 4.1. Overview The architecture of SALAD is presented in Fig.3. SALAD introduces parallel linear attention branch to sparse attention, with the output of the linear attention added to the sparse attention output. The linear attention branch shares query (Q), key (K) and value (V) with sparse attention. The attention operation means the interaction of Q, and V. The linear attention is followed by projection layer. Following the prior design [34], we attempt to leverage it to balance the distribution of the two types of attention output. SALAD can be generally applied to various sparse attention mechanisms, including both static and dynamic variants. In our experiments, we adopt modified slidingwindow attention [31] and self-implemented top-k sparse attention [23], as described in Sec. 3.1. These two approaches are the static and dynamic sparse attention, respectively. For the linear attention branch, we use ReLU-based linear attention formulation. For the linear attention branch, we observe that carefully regulating its output range is critical to overall performance. To achieve adaptive control over this branch, we introduce Algorithm 1: SALAD Attention Computation Input: X: Input hidden states to the attention block Output: O: Output of the attention block = XW Q; = XW K; = XW ; Os = SparseAttn(Q, K, V) // Corresponds to Equation 2 Ol = LinearAttn(Q, K, V) // Corresponds to Equation 3 (cid:80)n = σ(WGX + bG) // Rn1 = 1 i=1 Gi; = Os + Proj(Ol) // Proj is linear layer; is the scalar GATE = OW O; // is linear projection an input-dependent scalar gate. Specifically, the input hidden states are projected to scalar that modulates the output of the linear attention before it is fused with the sparse attention branch. This design enables the model to dynamically adjust the contribution of the linear branch based on the input. The complete computation of the SALAD attention is presented in Algorithm 1. Next, Sec. 4.2 will provides an overview of the linear attention branch and the overall training pipeline of SALAD. Sec. 4.3 gives more details of GATE designing for precisely controlling the influence of linear attention. 4 Figure 4. The Rank of Sparse Attention and Linear Attention. 4.2. Linear Attention Branch for Sparse Attention For the linear attention branch, we adopt the naive ReLUbased linear attention formulation mentioned in Sec. 3.2. However, the original linear attention is not designed for video sequence modeling. Consequently, to handle the spatial-temporal relationship in video sequence, we integrate the 3D Rotary Position Embeddings (3D RoPE) [18] to linear attention branch. As Fig.3 presented, we apply the 3D RoPE directly to Query and Key, following what full attention dose [20]. SALAD introduces few additional parameters. Specifically, projection layer is used after the linear attention branch, following the design of [34]. The layer plays role in balancing the distribution of the two types of attention outputs. And to obtain the gate, we add linear layer to SALAD, which performs channel mixing on the features. The weights Wq, Wk, Wv, and Wo are initialized from the pretrained model and fine-tuned using LoRA. For the projection layer (proj) in the linear attention branch, we apply zero-initialization to enable training to start from the vanilla sparse attention model. Both the projection layer weights and the input-dependent gate weights are fully trainable during fine-tuning. 4.3. Input Dependent Scalar Gate Prior studies [34] employ single projection linear layer to integrate the outputs from the sparse and linear attention branches. Nevertheless, we empirically find that it is not sufficient to balance the output distribution of the branches. Fig. 4 shows that the output rank of the linear attention branch is substantially lower than that of sparse attention, indicating clear capacity imbalance. In the sparselinear attention design, the sparse branch carries the primary sequence modeling load, while the linear branch functions as an auxiliary path. Since the linear branch mainly compensates for information missed by highly sparse attention, we speculate that properly regulating the contribution of the linear branch, rather than allowing it to dominate or vanish, Figure 5. Effect of Scaling the Linear Attention Branch. As λ decreases, we observe improvements in background consistency, imaging quality, and text consistency. This suggests that the contribution of the linear attention branch to the final output must be carefully constrained. Relying solely on static projection layer to regulate its influence appears insufficient, highlighting the need for more dynamic control over the branch fusion ratio. is important for ensuring its effective complementarity to sparse attention. However, we observe that introducing hyperparameter λ to modulate the linear attention branch can yield additional performance gains. This hyperparameter is incorporated as follows: = Os + λ Proj(Ol) (4) As illustrated in Fig. 5, although λ is set to 1.0 during training, applying moderately reduced scaling factor during inference improves performance in several aspects. But excessively small values of λ will lead to performance degradation. Intuitively, the role of the branch might vary across different layers and inputs. However, the projection layers hold static weight, and therefore cannot adapt to different input conditions. This motivates us to design an InputDependent Scalar Gate for linear attention branch. Fig.3 and Alg. 1 illustrate the gating operation.The input sequence of the branch is passed through linear layer followed by nonlinear activation. After that, we leverage token averaging to get scalar gate and the linear attention output will be multiplied by the gate. Since the gate with scale smaller than one achieves better performance, we adopt the sigmoid function as the non-linear function. 5. Experiments 5.1. Setup Base Model & Datasets. We test our method and along with other baseline methods, using the Wan2.1-1.3B[20] 5 Method Original (Full Attention) Training-free Spatial-Temporal SVG2 PAROAtten Tuning-based SLA Top-K w. LoRA Top-K w. SALAD Spatial-Temp. w. LoRA Spatial-Temp. w. SALAD Table 1. Quality and efficiency result of SALAD and baselines. SC () BC () 96.17 95. IQ () TC () VisionReward () 65.93 25.31 0.109 80.14 90.75 93.67 83.77 96.84 97.15 94.96 96.54 93.96 94.31 94. 96.26 96.10 96.27 95.29 96.37 42.07 63.07 64.88 25.82 66.03 64.76 65.27 66.09 7.98 25.08 25.01 3.60 24.02 25.83 25.39 25.55 -0.200 0.031 0. -0.205 0.058 0.107 0.072 0.092 Sparsity () 0% Speedup () 63% 45% 56% 90% 80% 90% 63% 90% 1.52 1.48 1.20 1.42 0.95 1.04 1.52 1.72 Figure 6. Generated Video Example of SALAD and other baselines. model with generation configuration of 480p resolution and 77 frames. For our method and training-based baseline methods, we use 2000 videos from Mixkit, an open-source video dataset released with Opensora plan [11], for model fine-tuning with 1600 steps. Baselines. We compare our method with various efficient video generation approaches. For training-free baselines, we include two static sparse attention methods: spatio-temporal sliding window attention and PAROAttention [40]as well as one adaptive sparse attention method, SVG2 [28]. The spatio-temporal local attention is extended from DiTFastAttnV2, incorporating the token reordering kernel from SVG. Following common practice, all trainingfree methods retain full attention for the denoising steps close to the noise to preserve generation quality. For tuningbased baselines, we evaluate ST-SWA tuned with LoRA, SLA [34] and Top-K sparse attention implemented with VMoBA [23] mentioned in Sec. 3.1. Both methods are fine-tuned under the same computational budget and follow their original training configurations to ensure fair comparison. For end-to-end speedup evaluation, we adopt the official open-source kernels released by these methods. Evaluation Metrics. We evaluate the generated video quality using the VBench [8] and Vision Reward [27] met6 rics. From VBench, we report Subject Consistency (SC), Background Consistency (BC), Image Quality (IQ), and Text Consistency (TC; also reported as Overall Consistency in other works). We follow the official VBench settings and use its extended prompt set to generate videos. Comparisons with baseline video quality and the initialization study are conducted on the full set of VBench prompts. Besides, parameter-efficiency comparisons with LoRA and other ablation studies are performed on subset of VBench prompts. For efficiency evaluation, we report Sparsity (the proportion of computations reduced by sparse attention) and end-to-end speedup. All speedup measurements are carried out on single GPU with batch size of 1. 5.2. Main Results Comparison with Sparse Attention Methods. We compare SALAD with state-of-the-art sparse attention methods, as summarized in Tab. 1. When applied to the spatialtemporal model, SALAD consistently outperforms prior methods across all four evaluation metrics, achieving 1.77 speedup. Existing post-training sparse attention approaches with roughly 50% sparsity suffer from noticeable quality degradation and limited speedup. Although SLA [34] effectively reduces training cost, it still requires 20,000 video samples and 2,000 training steps with batch size of 64 to maintain generation quality, which leads to performance degradation under our training budget. LoRA tuning recovers image quality and text consistency, but subject and background consistency remain below the dense baseline at 63% sparsity. In contrast, SALAD achieves 90% sparsity while maintaining video quality comparable to, and even surpassing the dense model, achieving 96.01 in subject consistency and 25.48 in text consistency. Comparison with LoRA. We compared SALAD with various LoRA configurations under the same sparsity. As shown in Tab. 7, increasing the number of LoRA parameters during model tuning does not consistently improve video quality, with the best performance achieved at rank 128. SALAD outperforms almost all the LoRA variants in Subject Consistency, Background Consistency, and Image Quality, while maintaining comparable Text Consistency, using fewer trainable parameters than LoRA at rank 256. This suggests that the observed gains in video quality arise from the methods intrinsic design rather than an increase in parameter count. 5.3. Ablation Study Architecture Design Tab. 3 presents an ablation study on how each design component affects overall video quality. Adding the projection alone improves subject consistency and image quality but reduces background and text consistency. Incorporating the gate alongside the projection enhances performance across all metrics, indicating that the Figure 7. Generated Video Samples of LoRA, SALAD, and SALAD with linear branch dropped during inference. projection alone is insufficient to balance branch outputs. Compared with the Non-shared weight variant, our design achieves even higher scores on all metrics while introducing substantially fewer parameters, demonstrating that the combination of shared architecture, projection, and gating not only optimizes performance but also provides favorable trade-off between accuracy and parameter efficiency. Initalization. We experiment SALAD with different initialization settings. Zero initialization continuously has lower training loss compared with random initialization. After finetuning, the model with zero initialization projection module demonstrates higher subject, background, and text consistency as shown in Tab. 4. Gate Design. To further investigate how the gate facilitates performance recovery, we examine two variants. The Constant Scale variant replaces the gate with fixed scalar λ = 0.5, while the Detached Gate variant is designed to analyze whether backpropagating gradients to the input positively contribute to performance. In this variant, such gradients are blocked, while the gate parameters remain trainable. As shown in Tab. 9, the Constant Scale variant improves video quality, indicating that adjusting the contribution of the linear attention during both training and inference enhances recovery. However, it performs worse than the input-dependent gate, suggesting that adaptive gating more effectively regulates the auxiliary role of the linear attention branch. The Detached Gate variant also achieves higher video quality, implying that the improvement arises primarily from dynamic scaling of the linear attention rather than from gradient flow to the input. 5.4. Additional Observations Effect of Branch To investigate the effect of the branch, we dropped the branch in inference after using SALAD (denote as SALAD-drop) and compared the video generated with SALAD and the one using LoRA. Fig. 7 shows that, without the branch, some prompt objects (e.g., fishing boats, top) are missing, and subject duplication occurs (e.g., two dogs, bottom) in the LoRA-tuned video, suggesting that LoRA tuning cannot fully recover the long dependency 7 Table 2. Performance and trainable parameters comparison between SALAD and LoRA. Method Original (Full Attention) LoRA (r = 128) LoRA (r = 256) LoRA (r = 512) SALAD Text Subject Background Trainable Image Consistency Consistency Quality Consistency Parameters 70.17 67.80 63.75 66.39 69. - 94M 189M 377M 165M 28.95 26.87 25.25 26.19 25.56 96.76 96.73 96.25 94.31 96.83 94.19 96.77 90.54 91.30 97.21 Table 3. Performance comparison across different architecture designs. Here, denotes the number of channels of the input hidden states to the attention block, and denotes the number of channels used for the query/key/value. In our deployment, we set = H. For the parameter count of each projection matrix, Q,K,V,O is D, the projection layer for the linear attention branch introduces parameters, and the weight of linear layer in GATE operation has 1 parameters. Architecture Design SC () BC () IQ () TC () Added Parameters Nonshared Shared Shared + Proj Shared + Proj + Gate 96.02 95.82 95.94 97. 96.07 66.87 24.92 4H 96.88 95.22 96.83 68.50 68.72 69. 23.35 23.14 25.56 0 + Table 4. Performance comparison across initialization settings. Initialization SC () BC () IQ () TC () Random Zero 96.16 96.54 95.51 96.37 65.67 66.09 25.70 25.55 Table 5. Performance comparison across gate design. Gate Design SC () BC () IQ () TC () w/o Gate w/ Gate Constant Scale Detached 95.94 97.21 97.45 97.00 95.22 96.83 96.55 96. 68.72 69.41 64.96 72.90 23.14 25.56 24.23 25.89 loss due to the high sparsity. The generated videos of SALAD-drop have better video-text consistency. However, there is loss of details (e.g., the boat in video sample 1 (upper) and the dogs tail in video sample 2 (bottom)). With the linear attention branch, the videos show more details and better frame continuity. This demonstrates that adding linear branch allows the sparse attention component to focus on basic video generation performance restoration, while the linear branch adds additional token interactions to handle detail supplementation and coherence restoration function. Attention Map Analysis We visualize attention maps from selected heads of sparse and linear attention maps in Fig. 8. In both examples, linear attention weight expands more exFigure 8. Examples of linear and sparse attention maps at block 8, timestep 20, head 2 and 4. Linear attention attends to long distant tokens, adding global information. tensively across longer token distances an observation evident from the lighter, more distributed regions in its maps. The extended range of linear attention allows it to capture long-range dependencies that are largely neglected by sparse attention, enhancing its capacity for global context integration. Post-training Branch Dropping We find that the linear attention branch on different attention layers benefits the performance differently, and some linear branches can be dropped while maintaining performance. As shown in the Appendix A.3, dropping 20% of branches with larger values achieves performance comparable to not dropping while providing an additional 5% acceleration. These results suggest that more fine-grained branch-dropping strategies could further improve the efficiency of SALAD, which we leave for future work. 6. Conclusion We propose SALAD, an efficient sparse-linear attention architecture. To compensate for cross-token information loss 8 caused by ultra-sparse attention, we introduce parallel linear attention branch with 3D RoPE. We observe that the linear branch primarily captures critical but sparse information, while sparse attention handles the majority of the sequence. To regulate this balance, we design an inputdependent scalar gate that dynamically controls the linear branch. SALAD effectively restores information lost in sparse attention, achieving 90% sparsity, 1.72 speedup, and generation quality comparable to full attention. Importantly, it requires only 2k video samples and 1.6k training steps, demonstrating its tuning efficiency. 7. Acknowledge This work was supported by National Natural Science Foundation of China (No. 62506197, 62325405, 62104128, U19B2019, U21B2031, 61832007, 62204164), Tsinghua EE Xilinx AI Research Fund, Beijing National Research Center for Information Science and Technology (BNRist) and Kuaishou Technology."
        },
        {
            "title": "References",
            "content": "[1] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1(8):1, 2024. [2] Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, et al. Sana-video: Efficient video generation with block linear diffusion transformer. arXiv preprint arXiv:2509.24695, 2025. [3] Lei Chen, Yuan Meng, Chen Tang, Xinzhu Ma, Jingyan Jiang, Xin Wang, Zhi Wang, and Wenwu Zhu. Q-dit: Accurate post-training quantization for diffusion transformers. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2830628315, 2025. [4] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. In The International Conference on Learning Representations, 2024. [5] Hanshuai Cui, Zhiqing Tang, Zhifei Xu, Zhi Yao, Wenyi Zeng, and Weijia Jia. Bwcache: Accelerating video diffusion transformers through block-wise caching. arXiv preprint arXiv:2509.13789, 2025. [6] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in neural information processing systems, 36: 1008810115, 2023. [7] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [8] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [9] Kumara Kahatapitiya, Haozhe Liu, Sen He, Ding Liu, Menglin Jia, Chenyang Zhang, Michael Ryoo, and Tian Xie. Adaptive caching for faster video generation with diffusion transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1524015252, 2025. [10] Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, and Song Han. Svdquant: Absorbing outliers by lowrank components for 4-bit diffusion models. arXiv preprint arXiv:2411.05007, 2024. [11] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. [12] Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. Timestep embedding tells: Its time to cache for video diffusion model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 73537363, 2025. [13] Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, et al. Moba: Mixture of block attention for longcontext llms. arXiv preprint arXiv:2502.13189, 2025. [14] Zhengyao Lv, Chenyang Si, Junhao Song, Zhenyu Yang, Yu Qiao, Ziwei Liu, and Kwan-Yee Wong. Fastercache: Training-free video diffusion model acceleration with high quality. arXiv preprint arXiv:2410.19355, 2024. [15] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems, 36:21702 21720, 2023. [16] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. [17] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. [18] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [20] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, 9 Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [21] Hongjie Wang, Chih-Yao Ma, Yen-Cheng Liu, Ji Hou, Tao Xu, Jialiang Wang, Felix Juefei-Xu, Yaqiao Luo, Peizhao Zhang, Tingbo Hou, et al. Lingen: Towards high-resolution minute-length text-to-video generation with linear compuIn Proceedings of the Computer Vitational complexity. sion and Pattern Recognition Conference, pages 25782588, 2025. [22] Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, et al. Koala-36m: large-scale video dataset improving consistency between fine-grained conditions and video content. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 84288437, 2025. [23] Jianzong Wu, Liang Hou, Haotian Yang, Xin Tao, Ye Tian, Pengfei Wan, Di Zhang, and Yunhai Tong. Vmoba: Mixtureof-block attention for video diffusion models. arXiv preprint arXiv:2506.23858, 2025. [24] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. [25] Guangxuan Xiao. Why stacking sliding windows cant see very far. https://guangxuanx.com/blog/ stacking-swa.html, 2025. [26] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synarXiv preprint thesis with linear diffusion transformers. arXiv:2410.10629, 2024. [27] Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, et al. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation. arXiv preprint arXiv:2412.21059, 2024. [28] Shuo Yang, Haocheng Xi, Yilong Zhao, Muyang Li, Jintao Zhang, Han Cai, Yujun Lin, Xiuyu Li, Chenfeng Xu, Kelly Peng, et al. Sparse videogen2: Accelerate video generation with sparse attention via semantic-aware permutation. arXiv preprint arXiv:2505.18875, 2025. [29] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations, 2025. [30] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Yuxing Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. In Proceedings of the 63rd Annual Meeting of the Associa10 tion for Computational Linguistics (Volume 1: Long Papers), pages 2307823097, 2025. [31] Zhihang Yuan, Hanling Zhang, Lu Pu, Xuefei Ning, Linfeng Zhang, Tianchen Zhao, Shengen Yan, Guohao Dai, and Yu Wang. Ditfastattn: Attention compression for diffusion transformer models. Advances in Neural Information Processing Systems, 37:11961219, 2024. [32] Zhihang Yuan, Rui Xie, Yuzhang Shang, Hanling Zhang, Siyuan Wang, Shengen Yan, Guohao Dai, and Yu Wang. Dlfr-gen: Diffusion-based video generation with dynamic latent frame rate. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1641016419, 2025. [33] Hanling Zhang, Rundong Su, Zhihang Yuan, Pengtao Chen, Mingzhu Shen, Yibo Fan, Shengen Yan, Guohao Dai, and Yu Wang. Ditfastattnv2: Head-wise attention compression In Proceedings for multi-modality diffusion transformers. of the IEEE/CVF International Conference on Computer Vision, pages 1639916409, 2025. [34] Jintao Zhang, Haoxu Wang, Kai Jiang, Shuo Yang, Kaiwen Zheng, Haocheng Xi, Ziteng Wang, Hongzhou Zhu, Min Zhao, Ion Stoica, et al. Sla: Beyond sparsity in diffusion transformers via fine-tunable sparse-linear attention. arXiv preprint arXiv:2509.24006, 2025. [35] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference. In International Conference on Machine Learning (ICML), 2025. [36] Mingyang Zhang, Hao Chen, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, and Bohan Zhuang. Loraprune: Pruning meets low-rank parameter-efficient fine-tuning. 2023. [37] Peiyuan Zhang, Yongqi Chen, Haofeng Huang, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, and Hao Zhang. Vsa: Faster video diffusion with trainable sparse attention. arXiv preprint arXiv:2505.13389, 2025. [38] Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhengzhong Liu, and Hao Zhang. Fast video generation with sliding tile attention. arXiv preprint arXiv:2502.04507, 2025. [39] Tianchen Zhao, Tongcheng Fang, Haofeng Huang, Enshu Liu, Rui Wan, Widyadewi Soedarmadji, Shiyao Li, Zinan Lin, Guohao Dai, Shengen Yan, Huazhong Yang, et al. Viditq: Efficient and accurate quantization of diffusion transformers for image and video generation. In International Conference on Learning Representations, 2025. [40] Tianchen Zhao, Ke Hong, Xinhao Yang, Xuefeng Xiao, Huixia Li, Feng Ling, Ruiqi Xie, Siqi Chen, Hongyu Zhu, Yichong Zhang, et al. Paroattention: Pattern-aware reordering for efficient sparse and quantized attention in visual generation models. arXiv preprint arXiv:2506.16054, 2025. [41] Kaiwen Zheng, Yuji Wang, Qianli Ma, Huayu Chen, Jintao Zhang, Yogesh Balaji, Jianfei Chen, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Large scale diffusion distillation arXiv via score-regularized continuous-time consistency. preprint arXiv:2510.08431, 2025. A. Appendix A.1. Sparse Attention Implementation Details A.1.1. Spatial-Temporal Sliding Window Attention As mentioned in the Sec. 3.1, we now provide more details about the Spatial-Temporal Sliding Window Attention (STSWA). Previous studies [24] have shown that attention heads in Video Diffusion Transformers exhibit two characteristic sparse patterns: spatial local attention, where token primarily attends to tokens within the same frame, and temporal attention, where token focuses on tokens at the same spatial location across neighboring frames. In these models, 3D video latents of size [Hh, Wh, Fh] are flattened into 1D token sequence. common flattening strategy treats the temporal dimension as the slowest-varying index, ensuring that spatially adjacent tokens within each frame remain consecutive. In contrast, tokens at the same spatial location across frames are separated by Hh Wh positions, causing high temporal attention scores to form multi-diagonal pattern in the attention map. As result, spatial locality can be effectively captured using local sliding-window attention, whereas temporal locality cannot be modeled well under this default ordering. To address this, for attention heads that exhibit temporal locality, we apply token permutation that groups tokens by spatial location, making temporal neighbors consecutive in the flattened sequence. This permutation realigns temporal locality with the diagonal structure of the attention map. After this, sliding-window attention pass with an appropriately chosen window size can directly and effectively capture local temporal dependencies. In our implementation of spatial-temporal attention, we adopt the headwise permutation kernel from SVG [24] and the headwise sliding-window attention kernel from DiTFastAttnV2 [33]. For configuring the window size of each attention head, we follow the procedure of DiTFastAttnV2 and use eight VBench video prompts as the profiling dataset. We then greedily select the smallest window size whose induced error does not exceed pre-defined threshold. The error threshold δ (maximum RSE after applying the sparse attention) is set to 2.0. A.1.2. Top-K Block Sparse Attention Instead of applying fixing pattern, Top-K, or the extension Top-P, dynamically select the key and value tokens that are more likely to have high attention scores. The calculation of Top-K block sparse attention includes three steps. Step 1: The input tokens is first partitioned in to blocks and the mean of each block is calculated. Step 2: The similarity of query tokens and the mean of key blocks is calculated and the top blocks are selected. Step 3: each query token only performs attention with the selected key blocks. For the Top-K sparse attention implementation, we adopt the Top-K mode from VMoBAs implementation. In this work, we use k=8 for LoRA tuning and k=4 for SALAD tuning. A.2. Training Details A.2.1. Training Settings The training details is shown in Tab. 6. We use subset of Mixkit Dataset, with HeightWidthFrames equals to 480 832 77. For training, we using 4 GPUs, setting the total batch size to 8 and learning rate to 1e-4. Table 6. Implementation of Salad Tuning optimizer learning rate training steps batch size number of training samples Total tuning GPU hours AdamW 1e-4 1600 8 2000 20.6 A.2.2. Training Cost SALAD requires only 2,000 training videos and approximately 20.6 GPU hours with batch size of 8 and achieves effective high-sparsity adaptation in Video Diffusion Transformers. This constitutes substantially lower computational footprint compared with methods that pretrain sparseattention architectures from scratch. For reference, VSA [37] is pretrained on 80,000 videos using 32 NVIDIA H100 GPUs, while VMoBA [23] leverages the Koala-36M dataset (36 million video clips) and 104 NVIDIA H800 GPUs under the token sequence length of 33K. SLA [34], recent linear-attention-based approach for high-sparsity adaptation, is tuned on 20,000 self-collected videosten times more than SALAD. SLA also requires batch size of 64 for tuning. Under our experimental setting, SLA fails to recover the performance of the original dense model as reported in Tab. 1, whereas SALAD achieves comparable or superior results at fraction of the resource cost. These findings underscore SALADs tuning efficiency for sparse adaptation in large-scale video diffusion models. A.3. Details of Post-training Branch Dropping In Sec. 5.4 we mention that some linear branches can be dropped while maintaining performance. We now provide more details about it. We first analyze the distribution of gate values across denoising timesteps. As illustrated in Fig. 9, the gate value distributions remain consistent across layers and timesteps. In particular, the 20th, 40th, 60th, and 80th percentiles vary 1 Table 7. Performance and trainable parameters comparison between SALAD and LoRA. Method Original (Full Attention) Spatial-Temporal Sliding Window Attention LoRA (r = 64) LoRA (r = 128) LoRA (r = 256) LoRA (r = 512) SALAD TopK Sparse Attention (k=4) LoRA (r = 128) LoRA (r = 256) LoRA (r = 512) SALAD Subject Background Trainable Image Consistency Consistency Quality Consistency Parameters 70.17 96. 28.95 94.19 Text - 96.89 96.77 90.54 91.30 97.21 95.37 98.35 96.52 98. 96.18 96.73 96.25 94.31 96.83 96.72 96.88 96.52 95.93 65.65 67.80 63.75 66.39 69.41 67.30 69.99 69.56 70.33 27.60 26.87 25.25 26.19 25.56 27.33 26.88 24.27 27. 47M 94M 189M 377M 165M 94M 189M 377M 165M Figure 9. Quintile Trends of Gate Values across Time-steps. The distributions of gate values remain remarkably stable throughout the diffusion process. This temporal consistency motivates our timestep-agnostic thresholding strategy for branch selection. smoothly over time. Since the gate value reflects, to some extent, the relative importance of each branch, we can set threshold to determine when the linear-attention branch can be safely dropped. This observation motivates simple yet effective thresholding strategy that uses the average percentile as the threshold, avoiding the need for per-timestep calibration. Using the above thresholds, we conducted series of experiments to investigate the impact of dropping branches according to their gate values. Specifically, we progressively drop linear attention branches in ascending order of gate magnitude. Fig. 10 reports the performance results on VBench subset. Notably, dropping approximately 20% of the branches led to the optimal video generation metrics. Increasing the proportion of dropped branches beyond this point caused decline in quality, indicating that excessive pruning compromises the models expressive capacity. To further figure out the best dropping plan, we performed an ablation study by dropping branches from differFigure 10. Progressive Branch Dropping Analysis. IQ (Image Quality) scores across different branch reservation strategies. The shaded region shows the effective dropping range that maintains baseline quality with around 20% branch dropped. ent gate intervals. Tab. 8 summarizes these results. Removing the branches corresponding to the largest 20% of gate values consistently yielded superior metrics, especially for subject consistency, compared to dropping branches from other intervals or random selection, which may be partially attributed to the low-rank nature of these high-gate branches. By selectively dropping these branches, we not only preserve video quality but also achieve additional computational acceleration. A.4. Details of Non-Shared Sparse-Linear Attention As discussed in Sec. 5.3 and Tab. 3, the nonshared-weight sparselinear attention architecture maintains two independent parameter sets for the sparse and linear attention branches, resulting in lower parameter efficiency compared with the shared-weight design. Although it introduces adTable 8. Qualitative results with different drop strategies Drop Strategy Original 0-20% 20%-40% 40%-60% 60%-80% 80%-100% Random (20%) SC () BC () 96.83 97.21 95.60 96.88 96.30 96.93 96.56 96.75 96.94 96.84 97.44 95.78 96.27 96.89 IQ () TC () 25.56 69.41 25.14 69.37 23.56 67.11 24.33 67.97 24.90 68.25 69.76 24.66 24.40 66.79 ditional parameters, the nonshared-weight variant can also be adapted to the SALAD formulation. Fig. 15 illustrates the naive sparselinear attention (S-L Attn) and the SALAD attention under both shared-weight and nonshared-weight settings. Table 9. Performance comparison across shared and nonshared weight SALAD. Architecture SC () BC () IQ () TC () Shared S-L Attn Shared SALAD NonShared S-L Attn NonShared SALAD 95.82 97.21 96.02 97.29 96.88 96.83 96.07 96. 68.50 69.41 66.87 68.38 23.35 25.56 24.92 27.05 Tab. 3 primarily compares the shared weight SALAD attention against the naive non-shared weight sparse-linear attention to demonstrate the parameter efficiency of our design. To further examine the impact of our approach, we additionally evaluate the non-shared weight SALAD architecture. For the experiments in Tab. 15, we follow the same training configuration for both shared and non-shared variants and evaluate all models on the same VBench subset. Though has more weights, the performance of naive nonshared sparse-linear attention does not significantly increase when compared with the shared sparse-linear attention variant. After incorporating the input-dependent gate and the zero-initialization strategy, its performance on all the metrics improves substantially. Meanwhile, the shared-weight and non-shared-weight versions of SALAD achieve comparable overall performance. The nonnon-shared-weight variant yields slightly higher SC and TC, whereas the shared-weight model performs better on BC and IQ. These results highlight the parameter efficiency of the shared-weight formulation. To avoid additional memory and latency overhead, we therefore adopt the shared-weight SALAD architecture as our final design. Figure 11. Gate design. The computation of GATE. Figure 12. Comparison of Branch and Sparse Attention Outputs. The top panel demonstrates the value range of the Sparse Attention Outputs and Linear Attention Branch Outputs. The blue shaded region denotes the sparse attention outputs, while the red shaded region highlights the gated branch outputs, which provide fine-grained complementary information without disrupting the original structural patterns. The bottom panel further visualizes the gate values, typically ranging between 0.1 and 0.4, indicating how the gating mechanism adaptively modulates the contribution of linear attention across different blocks. A.5. Observation on Input-Dependent Scalar Gate As shown in Sec. A.3, the learned gate values range from 0 to 1, which constrains the contribution of the linear branch to fine-grained interval, thereby balancing the overall distribution. As illustrated in Fig. 12, the magnitudes of the 3 A.6. Ablation on the Non-Linear Function in the"
        },
        {
            "title": "GATE",
            "content": "In this section, we investigate the choice of the non-linear function used in the GATE. As illustrated in the main paper and the Fig. 11, we introduce non-linearity to increase the expressive capacity of the GATE, and we adopt the sigmoid function since gating range constrained to [0, 1] empirically improves the performance of SALAD. Tab. 10 reports the ablation results across different non-linear functions. Sigmoid yields the best performance, while ReLU and Tanh underperform. This suggests that restricting the gate values to [0, 1] is sufficient and beneficial, which is also consistent with our visualization results. Table 10. Comparison of Non-Linear Functions in the GATE Non-Linear Function Tanh ReLU Sigmoid SC () BC () 96.25 96.89 96.10 96.77 96.83 97.21 IQ () TC () 26.97 66.20 28.26 67.12 69.41 25.56 A.7. More Qualitative Results To further assess the visual quality and temporal consistency achieved by SALAD, we present additional qualitative comparisons against several representative baselines in this section. We evaluate SALAD with other baselines on Wan 2.1-1.3B. As illustrated in Figure 16 and Figure 17, SALAD consistently preserves high-fidelity appearance details while generating temporally coherent motion, often matching or surpassing the visual quality produced by dense-attention counterparts. Additional video examples are provided in the supplementary materials for more comprehensive evaluation. original Sparse Attention Output and the Linear Attention Branch Output differ substantially while the gate provides fine-grained modulation, complementing the information from the sparse attention while preserving its original structure. To further investigate the role of the gate during inference, we conducted controlled study by fixing its value between 0 and 1.5 on model trained with gate of 1. As shown in Fig. 13, varying the gate reveals distinct failure modes. Setting gate=0 forces the model to rely solely on the LoRA branch, resulting in severe color distortions (e.g., the dogs fur appears unnaturally green or red), indicating Inthat LoRA alone cannot recover global interactions. creasing the gate to 0.7 partially activates linear attention, which improves overall sharpness but introduces semantic errors, such as duplicating the dog (e.g., two dogs appear in the scene instead of one). At gate=1, the output aligns with the prompt semantics, yet local spatial inconsistencies persist, such as mismatched headbody placement of the dog (e.g., disproportionate size of the dogs head and body). Pushing the gate to 1.5 over-amplifies linear attention, causing generation collapse, producing blurred frames with noise. These observations suggest that the gate must be carefully moderated. balanced linear attention contribution enriches sparse attention with complementary global cues while preserving the structural integrity of the generated content. By applying an input-dependent gate, such balance of linear attention and sparse attention can be automatically learned from training. similar pattern arises during training. As illustrated in Fig. 14, constraining the gate to fixed value throughout optimization substantially impairs model quality. With gate=0 (LoRA only), the model fails to capture semantic information and spatial correlation, as generates 2 dogs instead of 1. Setting gate=0.5 introduces semantic errors too, such as duplicated tails or misaligned subjects, indicating that fixed gates cannot adapt to varying contextual complexity across training samples. In contrast, the input-dependent gate dynamically modulates the contribution of the linear branch relative to sparse attention, enhancing global coherence while preserving fine-grained structural details (e.g., smoothly moving tails and consistent object placement). This content-aware, adaptive modulation is crucial for achieving both visual fidelity and stable training behavior. These observations underscore the necessity of an inputdependent, dynamic gating mechanism to balance contributions from the sparse and linear branches, ensuring both structural fidelity and fine-grained detail preservation in both training and inference. Figure 13. Video example of the effect of Gate choices when inference. The model is trained with gate value of 1 and evaluated under inference with different gate settings. Using gate=0 leads to severe color distortion, while gate=0.7 incorrectly introduces an additional dog. Although gate=1 aligns with the prompt semantics, it produces spatially inconsistent dog headbody structures. With gate=1.5, the model fails to generate meaningful outputs. In contrast, input-dependent gate values yield high-quality and semantically aligned results. Figure 14. Effect of Gate Choices During Training. We compare models trained with fixed gate values to those trained with inputdependent gates. Fixed-gate models exhibit distinct semantic errors, such as gate=0 producing two dogs and gate=0.5 generating duplicate tails. Even gate=1 shows spatial inconsistencies. In contrast, the input-dependent gate model produces natural videos with coherent structures and fine-grained details (e.g., wagging tail). 5 Figure 15. Shared-Weight and Non-Shared-Weight Architecture of Naive Sparse-Linear attention and SALAD Attention. 6 Figure 16. Generated Video Example of SALAD and other baselines. 7 Figure 17. Generated Video Example of SALAD and other baselines."
        }
    ],
    "affiliations": [
        "Kling Team, Kuaishou Technology",
        "The Chinese University of Hong Kong",
        "Tsinghua University"
    ]
}