{
    "paper_title": "VisPlay: Self-Evolving Vision-Language Models from Images",
    "authors": [
        "Yicheng He",
        "Chengsong Huang",
        "Zongxia Li",
        "Jiaxin Huang",
        "Yonghui Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 2 1 6 6 5 1 . 1 1 5 2 : r VisPlay: Self-Evolving Vision-Language Models from Images Yicheng He1* Chengsong Huang2* Zongxia Li3* Jiaxin Huang2 Yonghui Yang4 1University of Illinois Urbana-Champaign 3University of Maryland yh84@uiuc.edu 4National University of Singapore zli12321@umd.edu chengsong@wustl.edu 2Washington University in St. Louis"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) provides principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often depend on human-annotated labels or task-specific heuristics to define verifiable rewardsboth costly and limited in scalability. We introduce VisPlay, self-evolving RL framework that enables VLMs to autonomously improve their reasoning capabilities from massive unlabeled image data. Starting from single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and Multimodal Reasoner that generates silver responses. These roles are jointly trained using Group Relative Policy Optimization (GRPO), which uses diversity and difficulty rewards to balance the difficulty of generated questions with the quality of silver answers. VisPlay scales efficiently across two model families. Trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks including MM-Vet and MMMU, and establishes scalable path toward self-evolving multimodal intelligence. Our project page is available at https://bruno686. github.io/VisPlay/. 1. Introduction Self-evolving mechanisms [8, 32] represent promising frontier for advancing artificial intelligence. The training of state-of-the-art (SoTA) models has traditionally relied on large volumes of expert curated tasks and labels. However, the reliance on human annotation is not only costly, laborintensive, and difficult to scale, but also presents funda- *Core Contribution. Figure 1. Illustration of the average accuracy improvement (averaged over seven datasets) through successive evolutions (Evo 1 to Evo 5) on Qwen2.5-VL-3B-Instruct, compared to baseline trained on Vision-47K with GRPO, demonstrating the effectiveness of our VisPlay . mental bottleneck to advancing intelligence toward capabilities that could surpass itself without human signal guidance [45]. Self-evolution offers compelling alternative by equipping models with the capacity to independently generate, refine, and learn from their own experiences such as through self-play or synthetic data generation. Motivated by these advantages, the research community has increasingly explored self-evolution, most notably in the context of Large Language Models (LLMs). line of works have demonstrated how LLMs can autonomously enhance their complex reasoning and coding faculties, often by generating their own tasks or data [16, 26, 51]. However, the self-evolution paradigm remains largely underexplored for Vision-Language Models (VLMs) [2, 22, 35]. Unlike LLMs, which rely solely on text, developing self-evolving VLMs poses additional challenges due to their dependence on the visual modality. In world where human annotation is costly and time-consuming, yet vast amounts of visual data are freely available online, self-evolving VLMs present promising direction to continual improvement without human signals and directly from the abundant visual content on the internet [3, 33]. In this paper, we introduce VisPlay, self-evolving RL framework that enables VLMs to autonomously improve their reasoning capabilities using only raw, unannotated images. The framework utilizes single base VLM that alternates between two roles: the Image-Conditioned Questioner, which generates diverse and challenging questions conditioned on an input image, and the Multimodal Reasoner, which produces silver responses based on both the image and the generated question. Both roles are jointly optimized using Group Relative Policy Optimization (GRPO) [34], where designed rewards encourage balance between question difficulty and answer quality without requiring external supervision. The Image-Conditioned Questioner learns to generate challenging yet answerable questions grounded in visual inputs, while the Multimodal Reasoner learns to produce accurate, detailed, and grounded responses. This self-evolving framework enables the VLM to progressively improve its visual reasoning abilities through iterative co-improvement of the Questioner and the Reasoner as Figure 1. We apply our self-evolving RL framework to train three state-of-the-art (SoTA) VLMs and observe consistent performance gains across diverse visual reasoning benchmarks. Our main contributions are: We propose VisPlay, self-evolving RL framework for Vision-Language models. We apply VisPlay to three strong modelsQwen2.5VL-3B, Qwen2.5-VL-7B [35], and MiMo-VL-7B [43]. We run extensive evaluations over three major domainsGeneral Visual Understanding, Visual Mathematics, and Hallucination Detection. All models show consistent gains in accuracy after several iterations. We run extensive ablation studies to further validate the contribution of Image-Conditioned Questioner and the Multimodal Reasoner component to further show how VisPlay progressively strengthens multimodal reasoning across vision-language tasks. 2. Method 2.1. Preliminary Learning with Verifiable Rewards Reinforcement (RLVR) [20] is paradigm for training VLMs in domains where the correctness of model outputs can be verified. rule-based verifier : {0, 1} assigns binary reward to each generation xi: ri = v(xi) = (cid:40) 1, 0, otherwise. if xi satisfies correctness rule, (1) Such verifiable rewards are effective in tasks like mathematical reasoning, multiple choice, and code generation [52], where correctness can be objectively evaluated. GRPO [34] provides practical RL algorithm without value function by using relative rewards among multiple samples from the same prompt. Given prompt p, policy πθold produces complete responses {x1, . . . , xG} with corresponding rewards {r1, . . . , rG}. Rewards are normalized within the group to compute response-level advantages: ˆAi = ri mean(r1, . . . , rG) std(r1, . . . , rG) + εnorm where εnorm is small constant for stability. , (2) The policy is then optimized using clipped surrogate objective, regularized by KL term to constrain policy drift: LGRPO(θ) = (cid:32) (cid:88) i=1 min"
        },
        {
            "title": "1\nG\n(cid:18) πθ(xi)\nπθold(xi)",
            "content": "clip πθ(xi) πθold(xi) ˆAi, , 1ϵ, 1+ϵ (cid:33) (cid:19) ˆAi (3) + β KL(πθ πθold) . GRPO operationalizes RLVR principles to improve reasoning and generation quality in VLMs by rewarding responses with positive relative advantages while limiting policy deviation 2.2. Pipeline Overview We introduce VisPlay, self-play reinforcement learning framework designed to evolve VLMs without humanannotated data. As illustrated in Figure 2, the framework operates as closed-loop system involving two agents evolved from the same base model: an Image-Conditioned Questioner and Multimodal Reasoner. The process begins with the Questioner taking an image as input to generate visual query. Subsequently, the Reasoner receives both the image and the generated query to produce response. Both the Questioner and the Reasoner are initialized from shared pretrained backbone. The two agents co-evolve through iterative interactions: the Questioner is trained to generate more challenging questions, while the reasoner is trained to solve more and more challenging questions. The complete process is described in Algorithm 1. 2.3. Image-Conditioned Questioner Training The Questioner is an autoregressive policy denoted by Qθ. Conditioned on an input image I, it samples group of questions {xi}G i=1 Qθ(I), which are evaluated to produce scalar rewards {ri}G i=1. These rewards are used to compute group-normalized advantages and to update Qθ with GRPO objective. We next define reward components that constitute each ri. Figure 2. An illustration of our VisPlay framework, depicting the co-evolution of the Image-Conditioned Questioner and Multimodal Reasoner. Top: During the Questioner training stage, the Image-Conditioned Questioner is optimized via GRPO to produce challenge questions. The reward stems from the uncertainty of the frozen Multimodal Reasoner, computed by the consistency of its multiple generated answers. Bottom: In the Reasoner training stage, the Multimodal Reasoner is trained via GRPO on curated set of challenging questions from the now-frozen Image-Conditioned Questioner, leveraging pseudo-labels from its own majority voting. Pseudo-Label Generation. Since self-evolving VLMs learn without relying on labeled data, ground-truth answers for the Questioners generated questions are unavailable. Therefore, we introduce method to approximate the corresponding answers. Given an image and the generated question x, we introduce Reasoner Sϕ that samples responses {yj}m j=1.1 We define the empirical frequency of 1{yj = y}, candidate answer as ˆp(yx, I) = 1 and derive the pseudo-label via majority voting [15]: = arg maxy ˆp(yx, I). We then define the confidence score for this pseudo-label as: (cid:80)m j= conf(x, I) = ˆp(yx, I). (4) Intuitively, conf(x, I) measures the Reasoners certainty about the pseudo-label: high values indicate stable and consistent predictions, whereas values near 0.5 reflect strong uncertainty. We therefore treat the degree of uncertainty (i.e., how close conf(x, I) is to 0.5) as proxy for the model-perceived difficulty of the generated question. Uncertainty Reward. The confidence score quantifies the Reasoners uncertainty, which we use as proxy for 1The reasoner is evolved from the same base model as the Questioner. See Section 2.4 for details. the model-perceived difficulty of the generated question. To encourage questions that probe the Reasoners limits, we compute the reward based on the confidence score = conf(x, I). We define the uncertainty reward to penalize deviations from the point of maximum uncertainty: runc(x, I) = 1 (cid:12) (cid:12)2c 1(cid:12) (cid:12). (5) This formulation yields maximal reward of 1 when = 0.5 and decreases linearly to 0 as the reasoners response distribution becomes deterministic (i.e., 1). Diversity Regularization. To prevent the model from collapsing [5, 6, 9, 28, 54] into generating repetitive questions for given image I, we introduce redundancy penalty within its generated group XI . We cluster these generated questions based on pairwise similarity (BLEU score) to identify duplicates. For question xi belonging to cluster (I) XI , the regularization term is: rdiv(xi, I) = λ (I) , (6) where (I) denotes the cluster of similar questions for image I, and is the total number of generated questions for that image. Format Constraint. We enforce hard filter to ensure structural validity. Specifically, we require the generated question to be strictly enclosed within <question> tags. Any output failing to meet this format requirement is assigned zero reward. We denote this validity indicator as: 1valid(x) = (cid:40) 1, 0, if is wrapped in <question> tags, otherwise. (7) Final Questioner Reward. For each generated question xi conditioned on image I, we integrate the uncertainty signal and diversity regularization into unified scalar objective: ri = 1valid(xi) ReLU (cid:16) runc(xi, I) rdiv(xi, I) (cid:17) . (8) This composite reward incentivizes the Questioner to generate challenging yet non-redundant questions while strictly filtering out malformed outputs. The ReLU function stabilizes GRPO updates by preventing spurious negative values from skewing the reward normalization across the group. 2.4. Multimodal Reasoner Training The training of the Multimodal Reasoner Sϕ builds upon the advancements of the Image-Conditioned Questioner. In each iteration, the Image-Conditioned Questioner functions produce challenging samples that serve as training targets. The Multimodal Reasoner then learns from these automatically curated samples, improving its visual reasoning ability without any external supervision. Curated Dataset Construction. Following the update of the Image-Conditioned Questioner, we generate diverse pool of candidate questions {xi}N i=1 per image by samI). For each xi, we obtain repling xi Qθ( sponse samples from the current Multimodal Reasoner and compute the pseudo-label yi and confidence score ci = conf(xi, I). To focus on training samples that offer high information gain, we enforce an informative filter that retains pairs (xi, yi) with moderate confidence: τlow ci τhigh, (9) where τlow and τhigh are thresholds set to 0.25 and 0.75, respectively. This criterion effectively discards trivial samples where the model is already certain (ci > 0.75) as well as highly unstable or noisy generations (ci < 0.25). The final curated training set is formed by collecting all retained pairs across images, up to budgeted size, to optimize the Multimodal Reasoner via GRPO. Algorithm 1: VisPlay: Vision-Language Models Input: Initial models Qθ, Sϕ; Image dataset I; Self-Evolving RL for Group size G; Reasoner samples m; Dataset budget ; Thresholds τlow = 0.25, τhigh = 0.75. Output: Evolved models Qθ and Sϕ. 1 for each self-play iteration do for each image batch do 3 4 5 6 7 8 10 11 12 13 14 15 16 17 18 19 21 22 23 24 25 26 i=1 Qθ( I); j=1 Sϕ( I, xi); Sample question group {xi}G for each question xi do Sample answers {yj}m Compute confidence ci conf(xi, I) via majority vote (Eq. 4); Compute uncertainty reward runc 1 2ci 1; Compute diversity penalty rdiv via clustering (Eq. 6); Final reward: ri 1valid(xi) ReLU(cid:0)runc rdiv (cid:1); end Update Qθ via GRPO using rewards {ri}G i=1; end Initialize curated dataset ; for each image do Generate candidate questions via Qθ ; for each candidate xk do Obtain pseudo-label yk and confidence ck from Sϕ; if τlow ck τhigh then Add (I, xk, yk) to S; end end end for each minibatch (I, x, y) do Sample answers {yj}G j=1 Sϕ( I, x); Compute binary rewards rj 1(yj = y); Update Sϕ via GRPO using rewards {rj}G j=1; end 27 28 end Per-Sample Verifiable Reward. For question xi with pseudo-label yi, the Multimodal Reasoner generates group of candidate answers {yj}G j=1. Each sampled answer receives the binary reward rj = (cid:40) 1, if yj = yi, 0, otherwise. (10) Table 1. Comprehensive results on visual reasoning benchmarks. Each base model is evaluated against two settings: VisPlay ( challenger) baseline, in which the Reasoner is trained on questions produced by an untrained Challenger, and our iterative VisPlay framework. The highest performance reached during training for each model is emphasized in bold. We take accuracy as the metric. General Visual Understanding Visual Math Hallucination Methods MMMU MM RealWorld VisNum Math MATH -Vision -Vet Bench Verse QA Qwen2.5-VL-3B-Instruct challenger) Base Model VisPlay ( VisPlay (Iter 1) VisPlay (Iter 2) VisPlay (Iter 3) Qwen2.5-VL-7B-Instruct challenger) Base Model VisPlay ( VisPlay (Iter 1) VisPlay (Iter 2) VisPlay (Iter 3) MiMo-VL-7B-SFT Base Model VisPlay ( VisPlay (Iter 1) VisPlay (Iter 2) VisPlay (Iter 3) challenger) 19.95 23.34 29.40 33.37 37.11 23.10 35.24 28.94 27.07 38. 30.22 27.54 25.67 34.07 28.24 36.24 43.58 48.62 44.50 38.07 44.95 45.87 46.33 42.66 46.33 59.17 58.72 56.42 55.96 56.88 49.28 57.78 67.06 65.62 71.90 57.52 69.67 62.61 60.92 69. 78.17 63.27 69.54 78.69 71.50 27.08 29.33 30.01 29.64 39.15 32.57 32.41 28.65 27.08 32.57 44.80 49.66 51.59 51.18 52.69 26.14 33.50 29.67 32.36 35.15 33.78 35.13 33.88 36.32 39. 41.80 38.78 40.20 41.65 46.02 20.23 23.39 22.57 24.67 29.97 24.05 26.22 26.91 25.00 31.15 25.33 24.80 25.13 28.45 29.44 Hallusion Bench 32.81 64.88 91.80 94.95 90. 66.88 78.13 80.34 67.72 92.32 87.17 57.83 87.59 86.65 74.55 Avg. 30.61 33.77 44.16 44.87 47.27 40.41 38.33 44.53 40.97 48.61 43.56 39.63 43.16 45.58 45. These rewards are group-normalized to produce advantages ˆAj as in Eq. 2 (with the Reasoners rewards), and Sϕ is updated by minimizing LGRPO(ϕ) as in Eq. 3. 3. Experiments 3.1. Benchmarks and Evaluation Protocol We use existing image datasets from Vision-47K [12, 25], which contains 47K web images collected from diverse domains, e.g. including charts, medical images, textbooks, and driving simulations.2 We train exams, three backbone models using VisPlayQwen2.5-VL-3BInstruct, Qwen2.5-VL-7B-Instruct, and Mimo-7B-SFT.3 We run evaluation across three multimodal domains [25]. General Visual Understanding. We measure performance on four established benchmarks. MM-Vet [47] provides unified LLM-based score across recognition, OCR, and visual math tasks. MMMU [48] evaluates cross-modal reasoning and subject knowledge through 11.5K college-level, four-choice questions spanning six academic disciplines. RealWorldQA [41] contains roughly 700 real-world images paired with spatially grounded questions. VisNumBench [40] focuses on vi2We only use the images without the questions and answers. Details of the dataset breakdowns are in supplement materials. 3The detailed training configurations are provided in the supplementary material. sual number sense, covering around 1.9K questions involving numerical attributes and estimation tasks. Multimodal Mathematical Reasoning. MathVerse [50] consists of 2.6K diagram-centric questions spanning geometry and functions, provided in multiple visualtext formats. MATH-Vision [37] includes around 3K competition-level problems across 16 subjects and five difficulty tiers. Visual Hallucination Detection. HallusionBench [14] is used to analyze model errors, distinguishing between language-only hallucinations and visual-illusion errors, with simple yes/no evaluation format. 3.2. Main Results We use LLM-as-a-judge to assess the correctness of the answers to ensure more robust evaluation [13, 23]. We present the outputs of the Multimodal Reasoner and analyze its reasoning ability progression in Table 1. We summarize the main findings of our experimental results below. VisPlay consistently improves overall performance across different models. All models trained with VisPlay consistently surpass both their corresponding base models and the Base Challenger over successive training iterations. Qwen2.5-VL-3B shows remarkable improvement, with the average score increasing from 30.61 at baseline to 44.16 after the first iteration and reaching 47.27 at the third. Qwen2.5-VL-7B and MiMo-VL-7B Figure 3. Changes in question difficulty (orange, left axis) and problem-solving accuracy (blue, right axis) during Image-Conditioned Questioner and Multimodal Reasoner training across three VLMs. Table 2. Analysis of model performance and data quality. The shaded column indicates the estimated accuracy of the self-generated pseudo-labels for each question set, as determined using ChatGLM-Flash. Performance of Evaluated Model (vs. Ground Truth) Base Model Reasoner (Iter 1) Reasoner (Iter 2) Reasoner (Iter 3) Pseudo-Label Acc. DIter 1 DIter 2 DIter 3 39.0 37.5 36.0 44.0 42.5 40.0 45.5 44.0 41.5 49.0 47.5 45.0 72.0 65.0 61. follow similar upward trends, improving from 40.41 to 48.61 and 43.56 to 45.69, respectively. These results demonstrate the robust generalization ability and scalability of the proposed self-evolving framework across different models and model sizes. Performance gains across diverse task types. VisPlay shows improvements across general visual understanding tasks, visual reasoning or math benchmarks, and are more robust to hallucination. For Qwen2.5-VL-3B, the Hallucination score rises from 32.81 to 94.95 by the second iteration, showing substantial enhancement in factual grounding. Similar patterns are observed in other modelsreasoning benchmarks consistently improve without compromising accuracy on general understanding tasksdemonstrating that VisPlay effectively strengthens both task-specific reasoning and cross-domain multimodal generalization. Iterative co-evolution between the Questioner and Reasoner drives improvement. Performance trajectories across iterations highlight the co-evolution between the Questioner and Reasoner. As the Questioner generates more diverse and challenging queries, the Reasonertrained with GRPO using high-quality silver supervisionlearns to handle increasingly complex reasoning steps. This iterative loop allows both components to reinforce each other, leading to continual improvement in reasoning quality, generalization, and robustness. The results indicate that the co-evolutionary design of VisPlay provides scalable path toward self-improving multimodal intelligence. 3.3. Performance Comparison with Human-"
        },
        {
            "title": "Annotated Data",
            "content": "We conduct performance comparison between models trained with VisPlay and those trained using human-curated imagequestionanswer pairs from the Vision-47K dataset under standard GRPO for one epoch, as shown in Table 3 for Qwen2.5-VL-3B and 7B. Although this experiment is not an ablation study in the strict sense, it provides clear view of how our fully automated training pipeline performs relative to conventional supervised training. Overall, we observe that models trained with VisPlay achieve competitive average accuracy compared with those trained on real, human-written data. While performance on several task categories differs slightly, the general trend indicates that the self-evolving process can produce training signals of sufficient quality to improve base VLMs capabilities. These findings suggest that even in settings where human annotations are costly, limited, or unavailable, our framework can still serve as an effective and scalable alternative, enabling VLMs to develop stronger generalization abilities without depending on manual supervision. 3.4. Co-Evolution Dynamics of Two Roles The Evolution of Question Difficulty and Solution Accuracy. To analyze the co-evolution dynamics of the two roles, we examine the changes in question difficulty (orange, left axis) and problem-solving accuracy (blue, right axis) across three VLMs during the first training iteration (Figure 3). Question difficulty is operationalized as the Reasoners model-perceived difficulty, derived from the Table 3. Performance comparison between VisPlay and standard GRPO training with human-labeled data. Although VisPlay relies entirely on self-generated supervision, it achieves competitive overall accuracy and significantly reduces hallucination. This demonstrates that our self-evolving framework can meaningfully enhance VLM performance even in the absence of human-annotated datasets."
        },
        {
            "title": "MMMU",
            "content": "Qwen2.5-VL-3B-Instruct Standard GRPO VisPlay (Iter 3) 40.3 37.1 Qwen2.5-VL-7B-Instruct Standard GRPO VisPlay (Iter 3) 39.8 38. MM RealWorld VisNum Math MATH -Vision -Vet"
        },
        {
            "title": "Verse",
            "content": "QA 49.5 38.1 51.8 46.3 63.0 71.9 66.6 69.7 36.7 39. 43 32.6 42.8 35.2 53.2 39.1 29.9 30.0 33.8 31."
        },
        {
            "title": "Hallusion\nBench",
            "content": "67.4 90.5 66.6 92.3 Avg. 47.1 47.3 50.7 48.6 Table 4. Examples of challenging questions generated by the self-evolving Vision-Language model across three training iterations. The questions progressively increase in complexity, illustrating the growth in difficulty of the Questioners outputs over iterations. Images on the left and right correspond to the visual context for each question. The questions are raised by Qwen2.5-VL-3B-Instruct. Challenging Examples from Self-Evolving Trained Vision-Language Model Question (Iter 1) Approximately how many lung fields are visible in the X-ray image? Which skeletal structure most likely belongs to bird with hollow bones? Question (Iter 2) On thoracic x-ray, the right lobe of the lung is more spread out compared to the left lobe. If the right lobe is given score of 1 and the left lobe is given score of 0, what is the difference in scores between the right and left lung lobes? On which figure does the long neck of the dinosaur have the greatest horizontal angle with the vertical axis? Question (Iter 3) On which rib is the line approximately 2.5 cm above the images midpoint? Which skeletal structure is most likely to have evolved secondary to flying abilities and which is less likely to have this trait? confidence score defined in Eq. 4 in Section 2.3. Across all models, consistent co-evolution pattern emerges. The Image-Conditioned Questioners difficulty curves exhibit general upward trend, with initial increments followed by sustained growth, indicating the roles ability to progressively formulate more challenging visual questions. Concurrently, the Multimodal Reasoners accuracy curves, despite minor fluctuations, show complementary upward trajectory. This means that as question difficulty rises, the Reasoner adapts and enhances its problem-solving capability, with both metrics reinforcing each others improvement over iterations. Such mutual reinforcement validates VisPlays core mechanism, where the two interacting roles drive scalable self-evolution in multimodal reasoning. The Evolution of Capabilities and Data Accuracy. Building on the interaction between question difficulty and solution accuracy, we further analyze how the Reasoners capabilities and the quality of pseudo-labeled data evolve across iterations. For each training iteration, the Questioner generates questions for the same 200 images, and Reasoners from each iteration attempt to answer them. As shown in Table 2, the Reasoners accuracy steadily improves across iterations (e.g., from 44.0 to 49.0 on first-iteration questions), while the estimated accuracy of pseudo-labels slightly declines (from 72 to 61), reflecting increasing question difficulty. These trends highlight the co-evolution of model reasoning ability and data complexity during self-improving training. 3.5. Case Study on Question Difficulty Evolution Table 4 presents example questions generated by the selfevolving Vision-Language Models across three training iterations. Iteration 1 questions focus on direct observation, such as counting or identifying objects. Iteration 2 introduces relational and comparative reasoning, requiring the model to assess differences or evaluate spatial angles. Iteration 3 further increases complexity with multi-step reasoning and inference, including precise localization and causal relationships. This progression demonstrates systematic increase in question difficulty, providing increasingly challenging training signals that encourage the model to adapt and improve its reasoning capabilities. Such design ensures that both the Questioner and Reasoner co-evolve, progressively enhancing the overall performance of the system. 4. Related Work Post-Training for Vision-Language Models Recent research in post-training of vision-language models (VLMs) has shifted from supervised fine-tuning (SFT) toward reinforcement learning (RL) paradigms, driven by the increasingly strong capabilities of pre-trained VLMs. Earlier works such as LLaVA [17, 27] primarily rely on SFT to align language model backbone with visual information via projection layer, enabling multimodal instructionfollowing and visual reasoning. However, as base model quality improves, RL-based post-training has emerged as more powerful alternative. In particular, R1-style training [10, 18, 53] has gained attention for its ability to enhance reasoning and visual understanding without explicit supervision. key insight behind this success is that RL becomes effective only when the base model is sufficiently capable to self-explore reasoning trajectories [1, 7, 21, 22]. Despite these advances, most existing RL-based VLM approaches still depend on annotated multimodal datasets, which are costly and difficult to scale [11, 12, 24, 31, 42, 49]. To reduce reliance on human supervision, several recent works explore VLM self-play paradigms in games. Vision-Zero [38] and Game-RL [36] train VLMs with simulated game data to improve their general reasoning ability. Nonetheless, these methods often continue to depend on external models or tools for training data generation. While large language models (LLMs) have demonstrated selfevolving learning dynamics without any human or modellabeled data, extending such paradigms to VLMs remains more challenging due to the additional visual modality. Self-Evolving In Large Language Models Recent work has focused on enabling large language models (LLMs) to self-evolve their reasoning capabilities with minimal to zero human supervision. Various approaches have been proposed to achieve this. General unsupervised self-training frameworks like Genius [44] and Deep Self-Evolving Reasoning [29] aim for advanced reasoning. common theme is data-free training or starting from zero data, as explored in R-Zero [16], Absolute zero [51], and Language self-play [19]. Other methods adapt self-play for specific goals or environments. For example, SPICE [26] improves reasoning in corpus environments, Search Self-play [30] pushes capabilities via search, and SPELL [46] focuses on evolving long-context models. Additionally, some methods utilize interactions between multiple agents to collectively bootstrap reasoning abilities, as seen in Socratic-Zero [39] and Multi-Agent Evolve [4]. 5. Limitation While our work introduces scalable, self-evolving framework, we acknowledge two primary limitations that suggest directions for future research. First, due to computational constraints, our experiments were limited to the Qwen2.5VL and MiMo-VL families. The scalability and effectiveness of VisPlay on significantly larger VLMs (e.g., 10B parameters) is still an important open question. Second, our framework lacks definitive verification method for the self-generated data. While our GRPO policy indirectly optimizes for quality, developing more robust, automated methods to verify data faithfulness and prevent error accumulation is key area for future investigation. 6. Conclusion We present VisPlay, self-evolving RL framework that enables vision-language models to autonomously improve from unlabeled images. By decomposing VLM into an Image-Conditioned Questioner and Multimodal Reasoner and optimizing them via GRPO, our method balances challenge and accuracy without human supervision. Experiments show consistent gains in reasoning, compositional generalization, and hallucination reduction across multiple benchmarks. VisPlay demonstrates that scalable, self-improving multimodal intelligence is achievable. By iteratively generating and learning from its own experiences, model can refine its capabilities beyond humanlabeled data. This framework opens avenues for richer multimodal interactions and cross-domain adaptation, pointing toward intelligence systems that can continually evolve autonomously. Our results suggest promising path toward truly autonomous vision-language systems that improve themselves over time."
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. 8 [2] Florian Bordes, Richard Yuanzhe Pang, Anurag Ajay, Alexander C. Li, Adrien Bardes, Suzanne Petryk, Oscar Manas, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman, Mark Ibrahim, Melissa Hall, Yunyang Xiong, Jonathan Lebensold, Candace Ross, Srihari Jayakumar, Chuan Guo, Diane Bouchacourt, Haider Al-Tahan, Karthik Padthe, Vasu Sharma, Hu Xu, Xiaoqing Ellen Tan, Megan Richards, Samuel Lavoie, Pietro Astolfi, Reyhane Askari Hemmat, Jun Chen, Kushal Tirumala, Rim Assouel, Mazda Moayeri, Arjang Talattof, Kamalika Chaudhuri, Zechun Liu, Xilun Chen, Quentin Garrido, Karen Ullrich, Aishwarya Agrawal, Kate Saenko, Asli Celikyilmaz, and Vikas Chandra. An introduction to vision-language modeling, 2024. 1 [3] Xiuwei Chen, Wentao Hu, Hanhui Li, Jun Zhou, Zisheng Chen, Meng Cao, Yihan Zeng, Kui Zhang, Yu-Jie Yuan, Jianhua Han, Hang Xu, and Xiaodan Liang. C2-evo: Coevolving multimodal data and model for self-improving reasoning, 2025. 1 [4] Yixing Chen, Yiding Wang, Siqi Zhu, Haofei Yu, Tao Feng, Muhan Zhang, Mostofa Patwary, and Jiaxuan You. Multiagent evolve: Llm self-improve through co-evolution. arXiv preprint arXiv:2510.23595, 2025. 8 [5] Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, and Guang Shi. Pass@ training for adaptively balancing exploration and exploitation of large reasoning models. arXiv preprint arXiv:2508.10751, 2025. 3 [6] Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. 3 [7] Xiangxiang Chu, Jianlin Su, Bo Zhang, and Chunhua Shen. Visionllama: unified llama backbone for vision tasks, 2024. [8] Jeff Clune. Ai-gas: Ai-generating algorithms, an alternate paradigm for producing general artificial intelligence. arXiv preprint arXiv:1905.10985, 2019. 1 [9] Runpeng Dai, Linfeng Song, Haolin Liu, Zhenwen Liang, Dian Yu, Haitao Mi, Zhaopeng Tu, Rui Liu, Tong Zheng, Hongtu Zhu, et al. Cde: Curiosity-driven exploration for efficient reinforcement learning in large language models. arXiv preprint arXiv:2509.09675, 2025. 3 [10] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. 8 [11] Kaixuan Fan, Kaituo Feng, Haoming Lyu, Dongzhan Zhou, and Xiangyu Yue. Sophiavl-r1: Reinforcing mllms reasoning with thinking reward, 2025. 8 [12] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms, 2025. 5, 8, 1 [13] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao, Lionel Ni, and Jian Guo. survey on llm-as-ajudge, 2025. [14] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large visionlanguage models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1437514385, 2024. 5, 1 [15] Chengsong Huang, Langlin Huang, Jixuan Leng, Jiacheng Liu, and Jiaxin Huang. Efficient test-time scaling via selfcalibration. arXiv preprint arXiv:2503.00031, 2025. 3 [16] Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, and Dong Yu. R-zero: Self-evolving reasoning llm from zero data. 2025. 1, 8 [17] Mengzhao Jia, Zhihan Zhang, Wenhao Yu, Fangkai Jiao, and Meng Jiang. Describe-then-reason: Improving multimodal mathematical reasoning through visual comprehension training, 2024. 8 [18] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Searchr1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. 8 [19] Jakub Grudzien Kuba, Mengting Gu, Qi Ma, Yuandong Tian, and Vijai Mohan. Language self-play for data-free training. arXiv preprint arXiv:2509.07414, 2025. 8 [20] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model posttraining. arXiv preprint arXiv:2411.15124, 2024. 2 [21] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, 2022. 8 [22] Zongxia Li, Xiyang Wu, Hongyang Du, Fuxiao Liu, Huy Nghiem, and Guangyao Shi. survey of state of the art large vision language models: Benchmark evaluations and challenges. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR) Workshops, pages 1587 1606, 2025. 1, 8 [23] Zongxia Li, Xiyang Wu, Ishani Mondal, Alexa Siu, Jordan Lee Boyd-Graber, and Ani Nenkova. LLM-as-a-judge failures at automating the identification of poor quality outputs in free-form texts. In Proceedings of The 5th New Frontiers in Summarization Workshop, pages 116, Hybrid, 2025. Association for Computational Linguistics. 5 [24] Zongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Tianyi Zhou, Dinesh Manocha, and Jordan Lee Boyd-Graber. Videohallu: Evaluating and mitigating multi-modal hallucinations on synthetic video understanding, 2025. 8 [25] Zongxia Li, Wenhao Yu, Chengsong Huang, Rui Liu, Zhenwen Liang, Fuxiao Liu, Jingxi Che, Dian Yu, Jordan Boyd-Graber, Haitao Mi, et al. Self-rewarding visionlanguage model via reasoning decomposition. arXiv preprint arXiv:2508.19652, 2025. 5, 1 [26] Bo Liu, Chuanyang Jin, Seungone Kim, Weizhe Yuan, Wenting Zhao, Ilia Kulikov, Xian Li, Sainbayar Sukhbaatar, Jack Lanchantin, and Jason Weston. Spice: Self-play in corpus environments improves reasoning. arXiv preprint arXiv:2510.24684, 2025. 1, [27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 8 [28] Rui Liu, Dian Yu, Tong Zheng, Runpeng Dai, Zongxia Li, Wenhao Yu, Zhenwen Liang, Linfeng Song, Haitao Mi, Pratap Tokekar, et al. Vogue: Guiding exploration with visual uncertainty improves multimodal reasoning. arXiv preprint arXiv:2510.01444, 2025. 3 [29] Zihan Liu, Shun Zheng, Xumeng Wen, Yang Wang, Jiang Bian, and Mao Yang. Deep self-evolving reasoning. arXiv preprint arXiv:2510.17498, 2025. 8 [30] Hongliang Lu, Yuhang Wen, Pengyu Cheng, Ruijin Ding, Haotian Xu, Jiaqi Guo, Chutian Wang, Haonan Chen, Xiaoxi Jiang, and Guanjun Jiang. Search self-play: Pushing the frontier of agent capability without supervision. arXiv preprint arXiv:2510.18821, 2025. 8 [31] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl, 2025. [32] Jurgen Schmidhuber. Godel machines: Fully self-referential optimal universal self-improvers. In Artificial general intelligence, pages 199226. Springer, 2007. 1 [33] Atharva Sehgal, Patrick Yuan, Ziniu Hu, Yisong Yue, Jennifer J. Sun, and Swarat Chaudhuri. Self-evolving visual concept library using vision-language critics, 2025. 1 [34] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2 [35] Qwen Team. Qwen2.5-vl, 2025. 1, 2 [36] Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, Chaoran Tao, Zhiyuan Guo, Jizhou Yu, Tianhao Cheng, Zhiheng Xi, Changhao Jiang, Zhangyue Yin, Yining Zheng, Weifeng Ge, Guanhua Chen, Tao Gui, Xipeng Qiu, Qi Zhang, and Xuanjing Huang. Game-rl: Synthesizing multimodal verifiable game data to boost vlms general reasoning, 2025. 8 [37] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset, 2024. 5, 1 [38] Qinsi Wang, Bo Liu, Tianyi Zhou, Jing Shi, Yueqian Lin, Yiran Chen, Hai Helen Li, Kun Wan, and Wentian Zhao. Vision-zero: Scalable vlm self-improvement via strategic gamified self-play, 2025. 8 [39] Shaobo Wang, Zhengbo Jiao, Zifan Zhang, Yilang Peng, Xu Ze, Boyu Yang, Wei Wang, Hu Wei, and Linfeng Zhang. Socratic-zero: Bootstrapping reasoning via data-free agent co-evolution. arXiv preprint arXiv:2509.24726, 2025. 8 [40] Tengjin Weng, Jingyi Wang, Wenhao Jiang, and Zhong Evaluating number sense of Visnumbench: Ming. Haitao Mi, and Dong Yu. Evolving language models without labels: Majority drives selection, novelty promotes variation. arXiv preprint arXiv:2509.15194, 2025. 3 multimodal arXiv:2503.14939, 2025. 5, 1 large language models. arXiv preprint [41] xAI. Realworldqa: Real-world spatial understanding benchmark. https://x.ai/blog/grok1.5vandrealworldqa, 2024. CC BY-ND 4.0 license. Benchmark dataset released with Grok-1.5 Vision. 5, 1 [42] Jiaer Xia, Yuhang Zang, Peng Gao, Yixuan Li, and Kaiyang Zhou. Visionary-r1: Mitigating shortcuts in visual reasoning with reinforcement learning, 2025. 8 [43] LLM-Core-Team Xiaomi. Mimo-vl technical report, 2025. 2 [44] Fangzhi Xu, Hang Yan, Chang Ma, Haiteng Zhao, Qiushi Sun, Kanzhi Cheng, Junxian He, Jun Liu, and Zhiyong Wu. Genius: generalizable and purely unsupervised selftraining framework for advanced reasoning. arXiv preprint arXiv:2504.08672, 2025. 8 [45] Yuqing Yang, Yan Ma, and Pengfei Liu. Weak-to-strong reasoning. arXiv preprint arXiv:2407.13647, 2024. 1 [46] Ziyi Yang, Weizhou Shen, Ruijun Chen, Chenliang Li, Fanqi Wan, Ming Yan, Xiaojun Quan, and Fei Huang. Spell: Selfplay reinforcement learning for evolving long-context language models. arXiv preprint arXiv:2509.23863, 2025. 8 [47] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 5, 1 [48] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 5, 1 [49] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via stepwise group relative policy optimization, 2025. 8 [50] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, and Hongsheng Li. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?, 2024. 5, [51] Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335, 2025. 1, 8 [52] Yulai Zhao, Haolin Liu, Dian Yu, Sunyuan Kung, Meijia Chen, Haitao Mi, and Dong Yu. One token to fool llm-as-ajudge. arXiv preprint arXiv:2507.08794, 2025. 2 [53] Tong Zheng, Hongming Zhang, Wenhao Yu, Xiaoyang Wang, Runpeng Dai, Rui Liu, Huiwen Bao, Chengsong Huang, Heng Huang, and Dong Yu. Parallel-r1: Towards parallel thinking via reinforcement learning. arXiv preprint arXiv:2509.07980, 2025. 8 [54] Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang, VisPlay: Self-Evolving Vision-Language Models from Images"
        },
        {
            "title": "Supplementary Material",
            "content": "A.1 Detailed Training Dataset and Benchmarks Training Dataset We use the image data from Vision47K dataset [12, 25], which contains 47,000 web-sourced images covering wide variety of domains. The dataset includes charts, medical images, educational exams, textbook illustrations, and driving simulation frames. For our purposes, we exclusively use the images themselves, omitting any associated questions and answers. The dataset consists of approximately 10K charts, 8K medical images, 12K educational images (from exams and textbooks), 7K driving scenes, and 10K miscellaneous images from various domains. All images were standardized to resolution of 224224 pixels for model training. Backbone Models and Training We trained three backbone models using VisPlay: Qwen2.5-VL-3B-Instruct4: 3 billion parameters, finetuned with multimodal instruction data to enhance reasoning over visual-text tasks. Qwen2.5-VL-7B-Instruct5: 7 billion parameters, trained under the same protocol with extended batch sizes and longer training schedules to improve complex reasoning and generalization. Mimo-VL-7B-SFT6: 7 billion parameters, optimized with supervised fine-tuning on multimodal datasets for better alignment with human instructions. General Visual Understanding Four established benchmarks are used: MM-Vet [47]: Evaluates recognition, OCR, and visual math abilities using unified LLM-based scoring metric. The dataset contains over 5,000 test samples with detailed scoring for each subtask. MMMU [48]: Cross-modal reasoning benchmark with 11.5K college-level multiple-choice questions spanning six academic disciplines. Each question is image-based and designed to test subject knowledge and reasoning ability. RealWorldQA [41]: Contains approximately 700 realworld images paired with spatially grounded questions. 4https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct 5https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct 6https://huggingface.co/XiaomiMiMo/MiMo-VL-7B-SFT Evaluation emphasizes spatial reasoning and contextual understanding. VisNumBench [40]: Focused on visual number sense, includes roughly 1.9K questions involving numerical attributes, comparisons, and estimations. Multimodal Mathematical Reasoning Two specialized benchmarks: MathVerse [50]: 2.6K diagram-centric questions covering geometry, functions, and algebra, provided in multiple visual-text formats. MATH-Vision [37]: Approximately 3K competitionlevel problems across 16 subjects and five difficulty tiers. Focuses on integrating visual information into advanced mathematical reasoning. Visual Hallucination Detection HallusionBench [14] is used to identify model errors caused by either languageonly hallucinations or visual illusions. Evaluation is conducted in simple yes/no format, enabling precise measurement of hallucination rates and error types. A.2 Training Configuration Image-based Questioner Configuration. The Questioner is trained using visionlanguage model with maximum context window of 8192 tokens. The training set consists of 47K multimodal samples (Vision-SR1-47K), and evaluation is performed on the MMStar benchmark. Each sample uses problem, answer, and images as the prompt, label, and image fields, respectively. During rollouts, the Questioner generates 8 candidate questions per input. For the 3B model, we train for 20 steps, and for the 7B model, we train for 10 steps. In this setup, four GPUs run the vLLM service to provide reward signals, while the other four GPUs are used for training. The model parameters are loaded from specified Questioner checkpoint, and all checkpoints are saved under the designated experiment directory. Validation before training is disabled to maximize efficiency during early-stage learning. Multimodal Reasoner Configuration. The Solver is trained using chain-of-thought reinforcement learning. Its output length is capped at 4096 tokens, and prompts are constructed using dedicated Jinja template to enforce consistent reasoning format. Training uses the self-play Image-Conditioned Questioner Prompt Template You are an intelligent Question Generator. Your task is to create question based on the given image. Requirements (must follow exactly): 1. Analyze the image carefully and understand all details. 2. Generate exactly one question that is directly related to the image. 3. Choose the question type from only one of the following: - multiple choice (Yes/No or four options labeled A, B, C, D; only one correct answer) - numerical (requires specific numeric answer) - regression (requires predicting continuous value, such as measurement, quantity, or coordinate) 4. The question must require analysis or reasoning, not just description. 5. Output must be strictly in format < question > < /question >, with nothing else: Strict rules: - Do not use any other labels, punctuation, or formatting. - Do not add commentary, explanations, or extra text. Example of correct output: < question > How many clubs are there < /question > Multimodal Reasoner Prompt Template Please reason step by step carefully based on the question: + content + and the image. After completing your reasoning, you MUST output the final, clean, and concise answer strictly inside + boxed{} + . The final answer MUST appear inside boxed{}, and nowhere else. If there is no boxed answer, your response is considered incorrect. LLM-as-Judge Prompt Template You are an answer evaluation assistant. Your task is to judge whether two answers are substantially equivalent. When evaluating, you should ignore superficial differences such as format, spaces, punctuation, case, etc., and focus on whether they are consistent in core content, logical meaning and information expression. The judgment criteria should be lenient and inclusive, as long as the expressed meaning is basically the same, it is considered equivalent. dataset produced by the Questioner, while evaluation again uses MMStar. To ensure stable learning under long sequences, we adopt conservative micro-batch size of 1 for both updates and experience rollouts. The rollout engine supports up to 20K batched tokens per forward pass. The number of training steps for the Solver is set to be the same as for the Questioner. A.3 Prompt Templates The following three prompt templates define the core interaction structure used in our self-evolving Vision-Language Model. In this setup, multiple specialized rolessuch as question generator, multimodal reasoner, and an evaluatorare orchestrated to form an autonomous learning loop. Each template specifies precise behavioral constraints that allow the model to generate tasks, solve them with step-bystep reasoning, and assess answer consistency. Together, these components establish controlled self-play environment that enables the model to iteratively refine its reasoning capabilities without relying on external supervision."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "University of Illinois Urbana-Champaign",
        "University of Maryland",
        "Washington University in St. Louis"
    ]
}