{
    "paper_title": "EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series Forecasting",
    "authors": [
        "Sachith Abeywickrama",
        "Emadeldeen Eldele",
        "Min Wu",
        "Xiaoli Li",
        "Chau Yuen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Transformer-based models have significantly advanced time series forecasting, with patch-based input strategies offering efficiency and improved long-horizon modeling. Yet, existing approaches rely on temporally-agnostic patch construction, where arbitrary starting positions and fixed lengths fracture temporal coherence by splitting natural transitions across boundaries. This naive segmentation often disrupts short-term dependencies and weakens representation learning. In response, we propose EntroPE (Entropy-Guided Dynamic Patch Encoder), a novel, temporally informed framework that dynamically detects transition points via conditional entropy and dynamically places patch boundaries. This preserves temporal structure while retaining the computational benefits of patching. EntroPE consists of two key modules, namely an Entropy-based Dynamic Patcher (EDP) that applies information-theoretic criteria to locate natural temporal shifts and determine patch boundaries, and an Adaptive Patch Encoder (APE) that employs pooling and cross-attention to capture intra-patch dependencies and produce fixed-size latent representations. These embeddings are then processed by a global transformer to model inter-patch dynamics. Experiments across long-term forecasting benchmarks demonstrate that EntroPE improves both accuracy and efficiency, establishing entropy-guided dynamic patching as a promising new paradigm for time series modeling. Code is available at: https://github.com/Sachithx/EntroPE."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 7 5 1 6 2 . 9 0 5 2 : r Preprint. Under review. ENTROPE: ENTROPY-GUIDED DYNAMIC PATCH ENCODER FOR TIME SERIES FORECASTING Sachith Abeywickrama1, 2 Emadeldeen Eldele3, 2 Min Wu2 Xiaoli Li2, 4 Chau Yuen1 1School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore. 2Institute for Infocomm Research, A*STAR, Singapore. 3Department of Computer Science, Khalifa University, UAE. 4Information Systems Technology and Design, Singapore University of Technology and Design, Singapore. e240203@e.ntu.edu.sg, {emad0002, chau.yuen}@ntu.edu.sg, wumin@i2r.a-star.edu.sg, xiaoli li@sutd.edu.sg"
        },
        {
            "title": "ABSTRACT",
            "content": "Transformer-based models have significantly advanced time series forecasting, with patch-based input strategies offering efficiency and improved long-horizon modeling. Yet, existing approaches rely on temporally-agnostic patch construction, where arbitrary starting positions and fixed lengths fracture temporal coherence by splitting natural transitions across boundaries. This naive segmentation often disrupts short-term dependencies and weakens representation learning. In response, we propose EntroPE (Entropy-Guided Dynamic Patch Encoder), novel, temporally informed framework that dynamically detects transition points via conditional entropy and dynamically places patch boundaries. This preserves temporal structure while retaining the computational benefits of patching. EntroPE consists of two key modules, namely an Entropy-based Dynamic Patcher (EDP) that applies information-theoretic criteria to locate natural temporal shifts and determine patch boundaries, and an Adaptive Patch Encoder (APE) that employs pooling and cross-attention to capture intra-patch dependencies and produce fixed-size latent representations. These embeddings are then processed by global transformer to model inter-patch dynamics. Experiments across long-term forecasting benchmarks demonstrate that EntroPE improves both accuracy and efficiency, establishing entropy-guided dynamic patching as promising new paradigm for time series modeling. Code is available at https://github.com/Sachithx/EntroPE."
        },
        {
            "title": "INTRODUCTION",
            "content": "Time series analysis is fundamental to numerous critical applications, including electricity load forecasting (Gasparin et al., 2022), financial market prediction (Fischer & Krauss, 2018), healthcare monitoring (Reis & Mandl, 2003), and environmental surveillance (Smith, 1989). However, accurately modeling time series data remains challenging due to inherent noise, complex temporal dependencies, and irregular patterns spanning multiple time horizons (Lim & Zohren, 2021; Torres et al., 2021). The transformer architecture (Vaswani et al., 2017) has revolutionized sequence modeling across domains through self-attention mechanisms, leading to significant advances in time series forecasting. Despite their promise in learning long-term temporal relationships, time series transformers remain suboptimal due to unique complexities such as non-stationarity, seasonality, varying temporal granularities (Wen et al., 2023; Zeng et al., 2023). Recent advances in time series transformers include Informer (Zhou et al., 2021), Autoformer (Wu et al., 2021), and FEDformer (Zhou et al., 2022), which address long-sequence modeling challenges through sparse attention mechanisms, decomposition techniques, and improved positional encodings while operating on point-wise inputs. Triformer (Cirstea et al., 2022) and PatchTST (Nie et al., 2023) introduced patch-based input representation, dividing raw sequences into fixed-length patches and achieving substantial improvements in computational efficiency and long-term fore1 Preprint. Under review. casting performance. This demonstrated the critical impact of input tokenization strategies on transformer performance in time series. However, subsequent approaches have predominantly adopted similar temporally-agnostic patching schemes, which may not fully capture the temporal coherence and statistical properties inherent in time series data. Such temporally-agnostic patching introduces critical technical challenges. Fixed-length patching creates inconsistent input representations between training and inference phases. During training, time series samples from different time periods result in patches that begin at various temporal positions relative to underlying patterns (e.g., patch might start mid-trend in one sample but at trend beginning in another). However, during inference, patches are extracted from predetermined positions in the input sequence, creating systematic distribution shift where the model encounters patch configurations it rarely or never saw during training. Further, predetermined patch boundaries arbitrarily segment coherent temporal structures without regard for underlying dynamics (see Fig. 1(a)). For example, gradual trend change or seasonal transition may be split across multiple patches, breaking the natural temporal dependencies that are crucial for accurate pattern recognition and forecasting. These challenges directly impact model performance through two mechanisms. Fragmented temporal patterns lead to incomplete intra-patch representations, where semantically related time points are separated across different patches and processed independently. Additionally, the traininference mismatch reduces the models ability to generalize, as it must extrapolate to patch configurations with different statistical properties than those encountered during training. This is particularly detrimental for capturing rapid transitions and fine-grained temporal dynamics that require consistent temporal context. Unlike prior work that treats patching as static preprocessing step, we propose EntroPE, an Entropy-Guided Dynamic Patch Encoder, which integrates dynamic boundary detection as core architectural component. Our approach employs entropy-based criteria to identify natural temporal transition points, ensuring patch boundaries align with the underlying temporal structure rather than arbitrary positions (see Figure 1(b)). We then employ adaptive encoding mechanisms to process these variable-length patches while preserving intra-patch dependencies. This temporally-informed approach addresses both the train-inference mismatch and boundary fragmentation issues inherent in temporally-agnostic patching methods. (a) Temporally-agnostic, and (b) Figure 1: Temporally-informed (proposed) patching with calculated entropies for ETTh1 sample. The main contributions of this paper are summarized as follows: We introduce the first information-theoretic approach to patching for time series forecasting, where conditional entropy is used to place boundaries that respect temporal causality and predictive difficulty. We design an adaptive encoder that converts variable-length patches into fixed representations via pooling and cross-attention, enabling efficient batch computation. We demonstrate consistent gains across diverse benchmarks, showing that entropy-guided dynamic patching is practical and generalizable direction for time series transformers."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Time Series Transformers and Long-Term Forecasting. The transformer architecture has fundamentally transformed sequence modeling across domains, with time series forecasting being no exception. Early adaptations like Informer (Zhou et al., 2021) addressed the quadratic complexity of 2 Preprint. Under review. self-attention in long sequences through ProbSparse attention mechanisms, while Autoformer (Wu et al., 2021) and FEDformer (Zhou et al., 2022) introduced decomposition-based approaches and frequency domain modeling to capture complex temporal dependencies. These foundational works established transformers as viable architectures for time series tasks but operated on point-wise inputs, leading to computational challenges and limited ability to capture local temporal patterns efficiently. The fundamental challenge these approaches faced was the trade-off between sequence length and computational tractability. Point-wise processing, while preserving fine-grained temporal information, resulted in prohibitively long input sequences for practical applications, particularly in long-term forecasting scenarios where historical contexts spanning hundreds or thousands of time steps are crucial for accurate predictions. Patch-Based Input Strategies. Inspired by vision transformers (Dosovitskiy et al., 2021), patchbased input strategies have been widely adopted in time series forecasting. Triformer (Cirstea et al., 2022) and PatchTST (Nie et al., 2023) segment time series into fixed-length patches that serve as input tokens. This design achieves quadratic reduction in sequence length while preserving local semantic information through channel-independent processing, demonstrating that tokenization strategies can significantly improve both computational efficiency and forecasting accuracy. Building on this foundation, Crossformer (Zhang & Yan, 2023) introduced hierarchical mechanisms tailored for modeling both intra-patch and inter-patch dependencies. CARD (Xue et al., 2024) extended patching to multivariate forecasting, while patch-independence approaches (Lee et al., 2024) revealed superior representations in self-supervised settings which further highlights the importance of patch-based input strategies for time series modeling. Beyond Transformer architectures, xPatch (Stitsyuk & Choi, 2025) combined dual-flow MLPCNN designs with exponential decomposition, whereas Pathformer (Chen et al., 2024) and PatchMLP (Tang & Zhang, 2025) leveraged hierarchical processing to capture multi-scale relationships. However, fixed-length patching suffers from important limitations. Uniform segmentation can fragment coherent patterns such as seasonal transitions or trend shifts. During training, windows are sampled by shifting step by step so all possible boundaries are eventually seen, whereas inference fixes single segmentation that may not align with natural transitions, weakening representation learning and harming generalization. In response, MSPatch (Cao et al., 2025) introduces multi-scale patch segmentation to capture both shortand long-term dependencies, mitigating the fragmentation of coherent temporal patterns. HDMixer (Huang et al., 2024) proposes LengthExtendable Patcher (LEP) module that adjusts patch lengths via bi-linear interpolation to enrich boundary information, though it remains constrained by predetermined patch counts and may introduce interpolation-induced uncertainty. Beyond patch resizing, foundation models have explored alternative input constructions such as MOIRAI (Woo et al., 2024) leverages frequency-based patch sizing to preserve periodic structures, while Chronos (Ansari et al., 2024) bypasses patching entirely by treating individual time points as tokens, albeit at substantially higher computational cost. While recent works have advanced patch construction strategies, significant gaps remain. Existing adaptive methods often overlook causal structure and temporal coherence. This prevents them from capturing optimal boundaries guided by temporal uncertainty and predictive difficulty. Given the highly dynamic nature of time series across domains, principled and fully adaptive approach is required. Moreover, efficient training capabilities are required, particularly due to the variable number of patches in each sample. In contrast, dynamic patching schemes in other domains, such as natural language processing, have shown superior performance and efficiency. For instance, the Byte Latent Transformer (Pagnoni et al., 2025) employes byte level inputs instead of traditional word tokenization, and dynamically decide patches as part of the architeture. Motivated by these advances, our work introduces entropy-guided dynamic boundary detection that respects temporal causality and predictive uncertainty, coupled with adaptive encoding mechanisms specifically designed to preserve temporal dependencies within variable-length patches."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "3.1 PROBLEM FORMULATION Given multivariate time series = [x1, x2, . . . , xC] RCL with channels and look-back length L, the goal is to predict ˆY RCT over forecast horizon . Following the channel3 Preprint. Under review. Figure 2: Comprehensive architecture of EntroPE. The model processes input through: (A) Entropy-Based Dynamic Patcher - small causal transformer calculates entropy at each time point to identify boundaries where predictive uncertainty is high; (B) Adaptive Patch Encoder - Cross-attention layers aggregate intra-patch dependencies into fixed-size global embeddings; (C) Fusion Decoder - Cross-attention combines global patch context with local encoder hidden states for accurate forecasting. independence principle from PatchTST (Nie et al., 2023), we deliberately disregard cross-channel dependencies and apply shared function fϕ : RL RT (with shared parameters ϕ) to each channel independently: ˆyc = fϕ(xc), = 1, 2, . . . , C. This design choice isolates the temporal modeling capabilities from cross-variate relationships, allowing us to focus purely on the effectiveness of our dynamic patching mechanism within each univariate series. The multivariate prediction is constructed by stacking channel-wise results: ˆY = [ˆy1, ˆy2, . . . , ˆyC] RCT 3.2 FORWARD PROCESS The forward process of EntroPE (see Fig. 2) transforms raw multivariate time series into forecasts through sequence of interconnected components. The raw input multivariate time series RBCL, where B, C, and represent batch size, number of channels (variables), and sequence length respectively, is first processed following the channel independence principle, converting it to ci R(BC)L for independent processing of each variable. As the next step, to mitigate the statistical difference between training and testing distributions, we integrate the Reversible Instance Normalization layer (Kim et al., 2021) which outputs orm. R(BC)L. Following the Chronos tokenization approach (Ansari et al., 2024) (Appendix A.6), we discretize the continuous input sequence into uniform bins based on fixed-size vocabulary , producing tok. The tokenized sequence tok is then processed by the EDP (explained in Sec. 3.3), which generates patch mask RBCL indicating optimal patch boundary locations. Using this patch mask, the original continuous input ci is segmented into variable-length patches. These patches are fed into the APE (explained in Sec. 3.4), which generates patch embeddings while aggregating intra-patch dependencies. Preprint. Under review. The encoder produces two key outputs, namely, (1) patch embeddings in RBC ˆpdp , which serve as input embeddings for the global transformer, and (2) encoder hidden states enc RBCLdt that capture temporal information at the original sequence resolution for subsequent decoding head. Here, ˆp is not fixed due to the dynamic nature of the patching, dt and dp are time point-wise embedding and patch embedding dimensions, respectively. The patch embeddings in are processed by the Global Transformer, which learns inter-patch long-term dependencies and outputs enriched patch embeddings out RBC ˆpdp . Finally, the Fusion Decoder (FD) combines the global transformer outputs (P out) with the encoder hidden states (H enc) through cross-attention, flattens the enriched representations, and projects them through linear head and Instance De-Normalization layer to generate predictions for the future time steps, producing the final forecast ˆY RBCT ."
        },
        {
            "title": "3.3 ENTROPY-BASED DYNAMIC PATCHER (EDP)",
            "content": "As shown in Fig. 2(A), our dynamic patch boundary detection leverages lightweight causal transformer to identify temporal transitions based on predictive uncertainty. The process operates in two independent phases, namely model pre-training for temporal pattern learning and entropy-guided patch boundary detection with frozen weights. Entropy Model Pre-training. Drawing inspiration from the decoder-only transformer architecture and cross-entropy training demonstrated in Chronos, we adopt the GPT-2 paradigm (Radford et al., 2019) to train compact transformer for next-token prediction on quantized time series tokens. Our lightweight architecture comprises 8 embedding dimensions (shared input and head), 2 transformer layers, 4 attention heads, totaling 3,648 learnable parameters. The model performs autoregressive prediction at time {t2, t3, . . . , tL} from input sequence at time {t1, t2, . . . , tL1} using standard cross-entropy loss. This streamlined design efficiently captures temporal dynamics while maintaining computational tractability for entropy estimation. Crucially, our objective is not to achieve superior forecasting performance, but rather to align the causal transformer for measuring predictive uncertainty across the discrete vocabulary. This architectural approach enables identification of natural transition points in raw time series sequences, where predictive uncertainty that quantified through entropy over the output next-token distribution reaches local maxima. Entropy-Guided Patch Boundary Detection. Upon training convergence, we freeze model weights and compute entropy for each time position. For position t, we calculate the Shannon entropy of the next-token prediction distribution: H(xt) = (cid:88) vV pθ(xt+1 = vxt) log pθ(xt+1 = vxt) (1) Following Pagnoni et al. (2025), we implement dual-threshold boundary detection combining global and relative uncertainty measures. patch boundary is placed at position when three conditions are met simultaneously: H(xt) > θ H(xt) H(xt1) > γ (global entropy threshold) (relative entropy increase) (2) (3) This mechanism ensures patches begin only at positions with both high absolute uncertainty and significant uncertainty increases, avoiding fragmentation from minor entropy fluctuations while capturing meaningful temporal transitions. EDP outputs patch mask , which indicates the nonoverlapping patch segmentation for further processing. 3.4 ADAPTIVE PATCH ENCODER (APE) The EDP produces variable-length patches where both patch count ˆp and individual lengths vary across samples (patch mask ). As shown in Fig. 2(B), the APE converts these heterogeneous patches into fixed-size representations suitable for transformer processing while preserving temporal information. When handling variable patches, we ensure batch processing compatibility by padding sequences to the maximum patch count within each batch and applying attention masks to handle variable patch lengths during cross-attention operations. 5 Preprint. Under review. Architecture Design. Our encoder employs two-stage approach. First initial dimensionality reduction via pooling (from time-point embeddings to initial patch embeddings), followed by iterative cross-attention refinement. This design is inspired by the Perceiver architecture (Jaegle et al., 2021) and Byte Latent Transformer (Pagnoni et al., 2025), but specialized for temporal patch aggregation. Initial Patch Embedding. We first obtain the time-point embedding h0 for the entire sequence with length through an Embedding Layer RV dt over the same Vocabulary and Tokenizer used in the Entropy Model. For each variable-length patch pj, we generate initial patch representations via max pooling: P0,j = MaxPool(Ef (pj)), where Ef represents the time-point embedding function. Max pooling captures salient temporal features while providing translation invariance within patches. Cross-Attention Refinement. We refine patch representations using cross-attention, where patch embeddings serve as queries and time-point embeddings act as keys and values. For layer n: Pn = Pn1 + Wo (cid:18) softmax (cid:18) QK dk (cid:19) (cid:19) , with attention components: Qj = Wq(Pn1,j) Ki = Wk(hn1,i), (patch embeddings as queries), Vi = Wv(hn1,i) (time-point embeddings as keys/values), (4) (5) (6) where Wq, Wk, Wv, and Wo are learnable projection matrices corresponding to queries, keys, For -layer architectures with > 1, values, and output time-point embeddings are iteratively refined through stacked transformer layers as hn = APETransformer(hn1), with cross-attention mechanisms operating between consecutive layers. Here, hn is the time point embedding sequence at layer n. transformations, respectively. To ensure that self-attention within each APETransformer operates exclusively within patch boundaries, we employ tailored attention mask that accommodates the variable patch lengths throughout the sequence. This masking strategy prevents information leakage across patch boundaries while preserving intra-patch temporal dependencies. The encoder outputs fixed-size patch embeddings in = PN RBC ˆpdp for global transformer processing and preserves encoder hidden states enc = hN RBCLdt for temporal information in the decoding stage. 3.5 GLOBAL TRANSFORMER & FUSION DECODER The final components of the EntroPE process are the fixed-size patch embeddings to learn long-term dependencies and generate forecasts. Global Transformer. The patch embeddings in from the adaptive encoder are processed through standard transformer architecture to capture long-range temporal relationships between patches. Since intra-patch dependencies are already modeled by the encoder, the global transformer focuses exclusively on inter-patch interactions. We employ bidirectional attention (non-causal) to enable comprehensive context modeling across the entire sequence: out = GlobalTransformer(P in). Fusion Decoder (FD). key challenge in dynamic patching is that the number of patches ˆp varies across samples, making direct forecasting from flattened patch representations problematic. To address this, we employ cross-attention mechanism that fuses global patch context with fine-grained temporal information preserved from the encoder (see Fig. 2(C)). The decoder reverses the attention configuration used in the adaptive encoder. Here, encoder hidden states enc serve as queries, while global patch embeddings act as keys and values: Qi = Wq(H enc Kj = Wk(P out ), ) (time-point queries), Vj = Wv(P out ) (patch keys/values) The cross-attention operation enriches time-point representations with global context: dec = enc + Wo (cid:18) softmax (cid:18) QK dk (cid:19) (cid:19) . 6 (7) (8) (9) Preprint. Under review. This mechanism enables knowledge transfer from high-level patch representations back to detailed time-point embeddings, ensuring preservation of both local temporal patterns and global contextual dependencies. Optionally, similar to APE, the iterative cross-attention refinement can be configured through alternating transformer layers and cross-attention layers, creating structured information exchange between different representational levels. Forecasting Head. The enriched representations dec are projected to generate future preˆY = dictions. We apply flattening and linear projection with Instance De-Normalization: InstanceDeNorm (cid:0)Wproj(Flatten(H dec))(cid:1), where ˆY RBCT represents the forecasted values for the next time steps, maintaining the original multivariate structure. The model is trained using Mean Squared Error (MSE) loss LM SE(Y, ˆY ) between predicted and ground truth future values."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Datasets. We evaluate EntroPE on seven widely-used long-term multivariate forecasting benchmarks: ETT family (ETTh1, ETTh2, ETTm1, ETTm2), Weather, Electricity, and Exchange Rate datasets, covering diverse domains and temporal characteristics (detailed descriptions in Appendix Tab. 4). Baselines. We benchmark against 14 state-of-the-art deep forecasting models from 20212025, organized by architecture. (1) Transformer-based models: iTransformer (Liu et al., 2024b), PatchTST (Nie et al., 2023), FEDformer (Zhou et al., 2022), Autoformer (Wu et al., 2021); (2) CNN-based model: TimesNet (Wu et al., 2023); (3) MLP-based models: TimeMixer (Wang et al., 2024), HDMixer (Huang et al., 2024), DLinear (Zeng et al., 2023); (4) Kernel-attention model: TimeKAN (Huang et al., 2025b); (5) Foundation and lightweight models: Time-FFM (Liu et al., 2024a), TimeBase (Huang et al., 2025a); (6) LLM-aligned and cross-modal models: LangTime (Niu et al., 2025), CALF (Liu et al., 2025); (7) Filter-based model: FilterTS (Wang et al., 2025). (Further details on baselines are discussed in Appendix A.1.) Experimental Settings. Following standard practices in time series forecasting literature, we evaluate performance using Mean Squared Error (MSE) and Mean Absolute Error (MAE) metrics across multiple prediction horizons = {96, 192, 336, 720} and fixed look-back window = 96. Adopting the same experiemnt setup as Huang et al. (2025b), baseline results up to 2024 are taken from TimeKAN, while other methods are re-evaluated using their official implementations. EntroPE experiemnts are conductd in 5 random seeds and we report the best results. (see Appendix A.3.) 4.1 MAIN RESULTS Table 1 summarizes average performance across all prediction horizons. EntroPE matches or surpasses, in many cases, recent variable-mixing, Transformer-based, CNN-based, MLP-based, foundation, lightweight, LLM-aligned, filter-based, and KAN-based models under the standard 96-step look-back with horizons up to 720. It retains PatchTSTs overall architecture but introduces two key innovations namely, (i) entropy-driven dynamic patching and (ii) adaptive embeddings. Relative to PatchTST, EntroPE achieves notable accuracy gains of approximately 20% on ETTh1, 15% on Electricity, and about 10% on average, while also reducing token count and improving efficiency by leveraging channel independence and non-overlapping patches. Beyond PatchTST, EntroPE consistently outperforms HDMixers length-extendable patching and improves upon TimeBase, though the latter remains highly efficient. On the Electricity dataset, iTransformer continues to offer the strongest performance-efficiency tradeoff given its channel-wise self-attention across 321 and 862 variables, respectively. CALF and LangTime also remain competitive, though their large model sizes (CALF has 18 million, LangTime has 500 million while EntroPE has 0.1 million range parameters) impose substantial costs. Across other benchmarks, EntroPE delivers robust improvements, underscoring the value of entropy-guided temporal segmentation for multivariate forecasting. 4.2 ABLATION STUDIES Model component analysis. To systematically evaluate each components contribution, we conduct comprehensive ablation studies across four datasets (ETTh1, ETTh2, ETTm1, Weather) using prediction horizons of 336 and 720 time steps, measuring performance via MSE. We progressively 7 Preprint. Under review. Table 1: Multivariate time series forecasting results on benchmark datasets. Results are averaged across prediction horizons = {96, 192, 336, 720} with fixed input length = 96 for all datasets. Best results are highlighted in red and second-best in blue. dash (-) indicates that the configuration was not found in the original implementation. (Full table: Appendix A.4) Models Autoformer [2021] FEDformer [2022] DLinear [2023] TimesNet [2023] PatchTST [2023] Time-FFM [2024] HDMixer [2024] iTransformer [2024] TimeMixer [2024] TimeBase [2025] LangTime [2025] TimeKAN [2025] FilterTS [2025] CALF [2025] EntroPE [2025] ETTh1 ETTm2 MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE Electricity Exchange Weather ETTm ETTh2 0.496 0.498 0.461 0.495 0.516 0.442 0.448 0.454 0.459 0.463 0.437 0.418 0.440 0.441 0.416 0.487 0.484 0.457 0.450 0.484 0.434 0.437 0.447 0.444 0.429 0.425 0.427 0.432 0.435 0.425 0.450 0.437 0.563 0.414 0.391 0.382 0.384 0.383 0.390 0.409 0.375 0.391 0.375 0.372 0.366 0.459 0.449 0.519 0.427 0.411 0.406 0.407 0.407 0.409 0.425 0.392 0.410 0.399 0.395 0.387 0.588 0.448 0.404 0.400 0.406 0.399 0.396 0.407 0.382 0.431 0.397 0.380 0.386 0.396 0. 0.517 0.452 0.408 0.406 0.407 0.402 0.402 0.410 0.397 0.420 0.392 0.398 0.397 0.391 0.391 0.327 0.305 0.354 0.291 0.290 0.286 0.286 0.288 0.279 0.290 0.284 0.285 0.279 0.280 0.286 0.371 0.349 0.402 0.333 0.334 0.332 0.331 0.332 0.324 0.332 0.321 0.331 0.323 0.321 0.335 0.338 0.309 0.265 0.251 0.265 0.270 0.253 0.258 0.245 0.252 0.252 0.244 0.253 0.250 0.242 0.382 0.360 0.315 0.294 0.285 0.288 0.285 0.278 0.276 0.279 0.273 0.273 0.280 0.274 0.273 0.227 0.214 0.225 0.193 0.216 0.216 0.205 0.178 0.182 0.227 0.201 0.197 0.184 0.177 0. 0.338 0.327 0.319 0.304 0.318 0.299 0.295 0.270 0.272 0.296 0.285 0.286 0.275 0.266 0.271 0.613 0.519 0.354 0.416 0.367 0.338 - 0.360 0.378 - 0.361 - 0.355 - 0.331 0.539 0.429 0.414 0.443 0.404 0.391 - 0.403 0.410 - 0.400 - 0.400 - 0.386 remove components from our full EntroPE architecture to create four configurations: (1) EntroPE (Full): Dynamic patching + Adaptive encoder + Fusion decoder; (2) EntroPE - Dynamic: Static patching + Adaptive encoder + Fusion decoder; (3) EntroPE - (Dynamic Patching + Adaptive Encoder): Static patching + Max pooling + Fusion decoder; (4) EntroPE - (Dynamic Patching + Adaptive Encoder + Fusion Decoder): Static patching + Max pooling + Flattened output. The full EntroPE architecture achieves the best performance across all datasets and horizons, while systematic component removal leads to progressive performance degradation  (Table 2)  . The EntroPE - Dynamic Patching configuration yields second-best results, demonstrating the significant contribution of dynamic boundary detection. Further removing the adaptive encoder (EntroPE - (Dynamic Patching + Adaptive Encoder)) shows additional degradation, validating our cross-attentionbased encoder design. The worst performance occurs when all three components are removed, producing only static patching with basic pooling and flattened output. This systematic performance decline confirms that each architectural choice addresses specific modeling challenges in time series forecasting. Table 2: Ablation study showing component importance and dynamic vs. static patching comparison. Best results are highlighted in red and second-best in blue. Dataset Full -EDP -EDP -APE -EDP -APE -FD Baselines Dynamic Static(1) Static(8) Static(16) Pool + FD Pool + Flat PatchTST iTrans. ETTh1 336 720 ETTh2 336 720 ETTm1 336 720 Weather 336 0.429 0.439 0.355 0.397 0.393 0.445 0.258 0.341 0.425 0.460 0.428 0. 0.401 0.451 0.261 0.343 0.441 0.469 0.435 0.470 0.409 0.459 0.265 0. 0.444 0.477 0.439 0.463 0.415 0.452 0.270 0.359 0.438 0.461 0.439 0. 0.402 0.457 0.262 0.347 0.519 0.527 0.462 0.456 0.427 0.483 0.262 0. 0.501 0.503 0.427 0.436 0.421 0.462 0.284 0.356 0.487 0.500 0.428 0. 0.426 0.491 0.278 0.358 Dynamic vs. Static Patching. In addition to the component ablations, we compare EntroPE with dynamic patching against three static (fixed-length, non-overlapping) patching schemes. Experiments are conducted on ETTh1, ETTm1, and Weather datasets with fixed input length of 96 and forecasting horizons of 336 and 720. Dynamic patching achieves the lowest MSE in most settings (Table 2, while single-token input (patch length of 1) remains competitive, notably without leveraging the Dynamic Patching scheme. However, the computational cost of this setting is substantially higher, as the self-attention mechanism must attend to every time step individually. Figure 3 illustrates the efficiency advantage of dynamic patching over static alternatives under different configurations. Threshold Sensitivity. We further investigate the effect of the entropy threshold on EntroPEs performance using the ETTh1 and Weather datasets, which represent small and medium-scale bench8 Preprint. Under review. Figure 3: Ablation study for Dynamic and Static(patch length) patching with single epoch duration. marks. As shown in Figure 4, the MSE values for the 96336 forecasting setting remain stable across relative threshold values γ in the range 0.15-0.55, indicating robustness to threshold selection. At the same time, training time and patch construction reveal that the threshold effectively acts as control knob for computational complexity: lower thresholds yield more patches (higher cost), while higher thresholds reduce the number of patches and speed up (31% in Weather and 25% in ETTh1) training, with minimal variation in predictive accuracy. Figure 4: Ablation study for threshold sensitivity with epoch duration. 4.3 EFFICIENCY ANALYSIS As shown in Figures 3 and 4, our model adapts to different patching schemes, directly affecting computational efficiency. In the dynamic setting, the model maintains superior predictive performance while operating within an optimal computation budget. We evaluate efficiency on the ETTm1 dataset (9696) by comparing recent models in terms of Multiply-Accumulate Operations (MACs), Mean Squared Error (MSE), and the number of trainable parameters. Figure 5 demonstrates that our model achieves more favorable performanceefficiency trade-off, with bubble size proportional to the parameter count. Notably, while TimeMixer and TimeKAN occupy nearby region in the trade-off space, our model attains superior position, especially with its transformer-based architecture. Figure 5: EntroPE efficiency analysis."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduced EntroPE, temporally informed dynamic patch encoding framework that extends time series transformer architectures through entropy-guided boundary detection. Instead of partitioning sequences at arbitrary intervals, EntroPE strategically places patch boundaries at natural temporal transitions, preserving temporal coherence while supporting efficient variable-length patch 9 Preprint. Under review. processing. This design provides principled way to incorporate information-theoretic signals into patching, yielding improved forecasting accuracy alongside practical computational benefits. Beyond its empirical performance, EntroPE highlights the importance of respecting the intrinsic structure of time series, opening new research directions in adaptive sequence modeling and contextaware temporal representation learning."
        },
        {
            "title": "REFERENCES",
            "content": "Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Syndar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor, Jasper Zschiegner, Danielle C. Maddix, Michael W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-Schneider, and Yuyang Wang. Chronos: Learning the language of time series. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=gerNCVqqtR. Yizhi Cao, Zijian Tian, Wenjie Guo, and Xinggao Liu. Mspatch: multi-scale patch mixing framework for multivariate time series forecasting. Expert Syst. Appl., 273:126849, 2025. URL https://doi.org/10.1016/j.eswa.2025.126849. Peng Chen, Yingying Zhang, Yunyao Cheng, Yang Shu, Yihang Wang, Qingsong Wen, Bin Yang, and Chenjuan Guo. Pathformer: Multi-scale transformers with adaptive pathways for time series forecasting. In International Conference on Learning Representations (ICLR), 2024. Razvan-Gabriel Cirstea, Chenjuan Guo, Bin Yang, Tung Kieu, Xuanyi Dong, and Shirui Pan. Triformer: Triangular, variable-specific attentions for long sequence multivariate time series foreIn Lud De Raedt (ed.), Proceedings of the Thirty-First International Joint Confercasting. ence on Artificial Intelligence, IJCAI-22, pp. 19942001. International Joint Conferences on doi: 10.24963/ijcai.2022/277. URL https: Artificial Intelligence Organization, 7 2022. //doi.org/10.24963/ijcai.2022/277. Main Track. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https: //openreview.net/forum?id=YicbFdNTTy. Thomas Fischer and Christopher Krauss. Deep learning with long short-term memory networks for financial market predictions. European journal of operational research, 270(2):654669, 2018. Alberto Gasparin, Slobodan Lukovic, and Cesare Alippi. Deep learning for time series forecasting: The electric load case. CAAI Transactions on Intelligence Technology, 7(1):125, 2022. Qihe Huang, Lei Shen, Ruixin Zhang, Jiahuan Cheng, Shouhong Ding, Zhengyang Zhou, and Yang Wang. Hdmixer: Hierarchical dependency with extendable patch for multivariate time series In Proceedings of the AAAI conference on artificial intelligence, volume 38, pp. forecasting. 1260812616, 2024. Qihe Huang, Zhengyang Zhou, Kuo Yang, Zhongchao Yi, Xu Wang, and Yang Wang. Timebase: The power of minimalism in efficient long-term time series forecasting. In Forty-second International Conference on Machine Learning, 2025a. Songtao Huang, Zhen Zhao, Can Li, and LEI BAI. TimeKAN: KAN-based frequency decomposition learning architecture for long-term time series forecasting. In The Thirteenth International Conference on Learning Representations, 2025b. URL https://openreview.net/ forum?id=wTLc79YNbh. Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International conference on machine learning, pp. 46514664. PMLR, 2021. Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift. In International conference on learning representations, 2021. 10 Preprint. Under review. Seunghan Lee, Taeyoung Park, and Kibok Lee. Learning to embed time series patches indepenIn The Twelfth International Conference on Learning Representations, 2024. URL dently. https://openreview.net/forum?id=WS7GuBDFa2. Bryan Lim and Stefan Zohren. Time-series forecasting with deep learning: survey. Philosophical Transactions of the Royal Society A, 379(2194):20200209, 2021. Peiyuan Liu, Hang Guo, Tao Dai, Naiqi Li, Jigang Bao, Xudong Ren, Yong Jiang, and Shu-Tao Xia. Calf: Aligning llms for time series forecasting via cross-modal fine-tuning. Proceedings of the AAAI Conference on Artificial Intelligence, 39(18):1891518923, Apr. 2025. doi: 10. 1609/aaai.v39i18.34082. URL https://ojs.aaai.org/index.php/AAAI/article/ view/34082. Qingxiang Liu, Xu Liu, Chenghao Liu, Qingsong Wen, and Yuxuan Liang. Time-ffm: Towards lm-empowered federated foundation model for time series forecasting. Advances in Neural Information Processing Systems, 37:9451294538, 2024a. Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview. net/forum?id=JePfAI8fah. Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. time series is worth 64 words: Long-term forecasting with transformers. In International Conference on Learning Representations, 2023. Wenzhe Niu, Zongxia Xie, Yanru Sun, Wei He, Man Xu, and Chao Hao. Langtime: languageIn Fortyguided unified model for time series forecasting with proximal policy optimization. second International Conference on Machine Learning, 2025. URL https://openreview. net/forum?id=VfoKOD65Zq. Artidoro Pagnoni, Ramakanth Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman, and Srini Iyer. Byte latent transformer: Patches scale better than tokens. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 92389258, Vienna, Austria, 2025. Association for Computational Linguistics. doi: 10.18653/v1/2025.acl-long.453. URL https://aclanthology.org/2025. acl-long.453/. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Ben Reis and Kenneth Mandl. Time series modeling for syndromic surveillance. BMC medical informatics and decision making, 3(1):2, 2003. Richard Smith. Extreme value analysis of environmental time series: an application to trend detection in ground-level ozone. Statistical Science, pp. 367377, 1989. Artyom Stitsyuk and Jaesik Choi. xpatch: Dual-stream time series forecasting with exponential seasonal-trend decomposition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2060120609, 2025. Peiwang Tang and Weitai Zhang. Unlocking the power of patch: Patch-based mlp for long-term time series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 1264012648, 2025. Jose Torres, Dalil Hadjout, Abderrazak Sebaa, Francisco MartınezAlvarez, and Alicia Troncoso. Deep learning for time series forecasting: survey. Big data, 9(1):321, 2021. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 11 Preprint. Under review. Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Zhang, and JUN ZHOU. Timemixer: Decomposable multiscale mixing for time series forecasting. In International Conference on Learning Representations (ICLR), 2024. Yulong Wang, Yushuo Liu, Xiaoyi Duan, and Kai Wang. Filterts: Comprehensive frequency filtering for multivariate time series forecasting. Proceedings of the AAAI Conference on Artificial Intelligence, 39(20):2137521383, Apr. 2025. doi: 10.1609/aaai.v39i20.35438. URL https://ojs.aaai.org/index.php/AAAI/article/view/35438. Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun. In International Joint Conference on Artificial IntelliTransformers in time series: survey. gence(IJCAI), 2023. Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, and Doyen Sahoo. Unified training of universal time series forecasting transformers. 2024. Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in neural information processing systems, 34:2241922430, 2021. Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In International Conference on Learning Representations, 2023. Wang Xue, Tian Zhou, QingSong Wen, Jinyang Gao, Bolin Ding, and Rong Jin. Card: Channel In International Conference on aligned robust blend transformer for time series forecasting. Learning Representations (ICLR), 2024. Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series In Proceedings of the AAAI conference on artificial intelligence, volume 37, pp. forecasting? 1112111128, 2023. Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency In The eleventh international conference on learning for multivariate time series forecasting. representations, 2023. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pp. 1110611115, 2021. Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In International conference on machine learning, pp. 2726827286. PMLR, 2022. 12 Preprint. Under review."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 BASELINE DETAILS Table 3: Overview of baseline models (20212025) grouped by architecture. We highlight their key design paradigms for deep time series forecasting. Model Category Key Features / Innovations iTransformer (Liu et al., 2024b) PatchTST (Nie et al., 2023) FEDformer (Zhou et al., 2022) Autoformer (Wu et al., 2021) Transformer Transformer Transformer (freq.-decomp.) Transformer (decomp.-autocorr.) Inverted attention across feature and time dimensions. Patch-based embedding with channel-independence. Fourier/Wavelet enhanced decomposition. Progressive decomposition with auto-correlation. TimesNet (Wu et al., 2023) CNN Temporal 2D variation modeling with convolutional layers. TimeMixer (Wang et al., 2024) MLP MLP HDMixer (Huang et al., 2024) Linear/MLP DLinear (Zeng et al., 2023) Multiscale mixing, separation of trend and seasonal signals. Hierarchical dependency with extendable patches. Channel-independent trend/seasonal decomposition. TimeKAN (Huang et al., 2025b) Kernel Attn. Network Kernelized attention distinct from Transformer/MLP. Time-FFM (Liu et al., 2024a) TimeBase (Huang et al., 2025a) Foundation Lightweight Pre-trained foundation model for time series. Resource-efficient, practical forecasting baseline. LangTime (Niu et al., 2025) CALF (Liu et al., 2025) LLM-aligned / Cross-modal LLM-aligned / Cross-modal Language-guided forecasting with RL optimization. Cross-modal match, feature regularization, output consistency. FilterTS (Wang et al., 2025) Filter-based Static global + dynamic cross-variable frequency filtering. A.2 DATASET DETAILS Table 4: Summary of datasets used for long-term forecasting evaluation. Dataset sizes represent the number of temporal observations in each partition (Training, Validation, Test). Dataset Dim Dataset Size (Train, Val, Test) Frequency Domain Information ETTh1 ETTh2 ETTm1 ETTm2 Weather Electricity (ECL) Exchange Rate 7 7 7 7 21 321 8 (8545, 2881, 2881) (8545, 2881, 2881) (34465, 11521, 11521) (34465, 11521, 11521) (36792, 5271, 10540) (18317, 2633, 5261) (5120, 665, 1422) Hourly Hourly 15min 15min 10min Hourly Daily Energy Infrastructure Energy Infrastructure Energy Infrastructure Energy Infrastructure Meteorological Monitoring Electricity Consumption Foreign Exchange Market This study employs comprehensive collection of benchmark forecasting datasets to rigorously evaluate model performance across diverse temporal domains and application scenarios. The selected datasets represent critical real-world forecasting challenges spanning energy management, meteorological prediction, and financial markets. Electricity Transformer Temperature (ETT) Datasets: The ETT collection comprises four distinct datasets (ETTh1, ETTh2, ETTm1, and ETTm2) that monitor temperature variations in electricity transformers alongside corresponding load measurements. These datasets facilitate the prediction of future temperature profiles and electrical loads based on historical patterns, which is essential for transformer maintenance and grid stability. The collection offers varied temporal granularities: ETTh1 and ETTh2 provide hourly measurements, while ETTm1 and ETTm2 capture data at 15-minute intervals, enabling comprehensive evaluation across different forecasting horizons and temporal resolutions. Weather: The weather dataset incorporates comprehensive meteorological measurements recorded at 10-minute intervals from dedicated weather station. This dataset supports forecasting of various atmospheric phenomena, providing crucial information for agricultural planning, transportation safety, and general societal planning activities. The multivariate nature of weather data makes it particularly challenging for forecasting models, as it requires capturing complex interdependencies among atmospheric variables. 13 Preprint. Under review. Electricity (ECL): This dataset encompasses hourly electricity consumption records from 321 individual clients, providing comprehensive insights into consumption patterns and enabling accurate demand forecasting. The dataset is particularly valuable for optimizing power generation scheduling and distribution network management, representing critical application domain for time series forecasting in energy systems. Exchange Rate: This dataset contains daily exchange rates of the US dollar against eight different currencies, spanning from 1990 to 2016. It represents financial time series forecasting challenges with inherent volatility and complex market dynamics. Table 4 presents the detailed characteristics of all datasets employed in this study. The collection spans multiple temporal frequencies (10-minute, 15-minute, hourly, and daily intervals) and varies significantly in dimensionality, from 7-dimensional ETT datasets to the 321-dimensional ECL dataset. Dataset sizes are reported in the format (Training, Validation, Test) to clearly delineate the data partitioning strategy employed across all experiments. A."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "We summarize the hyperparameter configurations used for EntroPE across all benchmark datasets in Table 6. The table reports dataset-specific settings alongside common defaults, covering patching, embedding, optimization, and training parameters to ensure reproducibility and fair comparison with prior work. All experiments are conducted with 5 different random seeds, and reported results correspond to the best performance metrics across these runs. Experiments were performed on multiple GPU platforms, including NVIDIA RTX 4090, RTX A5000, NVIDIA GeForce RTX 4090 D, and NVIDIA RTX 6000 Ada-16Q. Entropy Model Pre-Training. We adopt the exact configuration described in the EDP section 3.3. The model hyperparameters are summarized in Table 5. We pre-train this lightweight model with early stopping, where training is terminated once the validation loss fails to improve by more than 7%. The same setting is applied across all datasets, except for the Electricity dataset where we use batch size of 32 (batch size of 128 is used for all others). Table 5: Entropy model configuration used for pre-training. Hyperparameter Number of layers (n layer) Number of heads (n head) Embedding dimension (n embd) Dropout Bias Vocabulary size (vocab size) Block size (block size) Value 2 4 8 0.1 False 256 96 A.4 EXTENDED RESULTS A.4.1 FULL RESULTS (LONG-TERM FORECASTING). INPUT LENGTH 96. We adopted the training criteria established by TimeKAN for fair comparison across all baseline methods. The results for AutoFormer, FEDformer, TimesNet, PatchTST, Time-FFM, iTransformer, and TimeMixer were directly extracted from the TimeKAN paper to ensure consistency in experimental conditions. For HDMixer, TimeBase, CALF, and FilterTS, we executed the original implementations following identical training criteria. Specifically for FilterTS, we addressed the challenge of multiple configuration options by running experiments across all different settings proposed by the authors for each forecasting horizon. We then selected the unified setting that achieved the lowest Mean Squared Error (MSE) and Mean Absolute Error (MAE) performance (detailed results are provided in Section A.4.2). LangTime baseline results were obtained directly by running the authors Preprint. Under review. Table 6: Hyperparameter settings for EntroPE across datasets. Hyperparameter ETTh1 ETTh2 ETTm1 ETTm2 Weather Electricity Exchange Rate dim heads layers max patch length batch size 8 2 1 64 8 2 2 24 16 2 1 24 32 4 1 24 32 learning rate 0. 0.01 0.01 0.001 dropout monotonicity threshold (θ) 0.05 1 3 threshold add (γ) 0.25 train epochs patience 20 7 0.1 1 3. 0.2 20 7 0.1 1 3. 0.3 20 7 0.1 1 0.25 0.35 20 7 20 16 2 2 24 128 0. 0.2 1 3 32 4 24 32 8 2 1 32 0.01 0.01 0.1 1 3. 0.3 20 7 0.2 1 3. 0.3 20 7 implementation to maintain consistency with the their reported performance. This methodology ensures that all comparative results are obtained under equivalent experimental conditions, providing fair and rigorous evaluation framework for our proposed approach. See Tab 7 and Tab. 8 for full results. A.4.2 FILTERTS PERFORMANCE COMPARISON ACROSS CONFIGURATIONS The original FilterTS results (9) were reported using different hyperparameter settings for different forecast lengths, which complicates direct comparison with other baselines. To ensure fairness, we systematically re-evaluate all provided configurations (Config 1-4) across all forecast horizons and report the configuration that achieves the best performance in terms of MSE and MAE. A.5 ENTROPY QUALITATIVE ANALYSIS Entropy Visualization: Figure 6 demonstrates how entropy-driven boundaries align with temporal transitions, creating patches that preserve intra-patch coherence while capturing natural temporal structure. This validates our core hypothesis that content-aware segmentation outperforms arbitrary fixed-length approaches. In figure 6, Panel (a) - Original Time Series: Shows the normalized time series data from the ETTh1 dataset (Sample 0), exhibiting typical temporal patterns with periodic fluctuations and trend changes. The series demonstrates varying levels of predictability across different time segments, ranging from smooth transitions to sharp discontinuities. Panel (b) - Tokenized Sequence: Displays the discrete token representation of the continuous time series after quantization using the MeanScaleUniformBins tokenizer. Token values range approximately from 50 to 180, capturing the underlying temporal dynamics while enabling discrete sequence modeling. The step-wise pattern reflects the quantization process that maps continuous values to discrete vocabulary tokens. Panel (c) - Token-wise Entropy with Threshold Regions: Presents the entropy values computed by the pre-trained entropy model for each token position. High entropy regions (red shading) indicate positions where the next token is highly uncertain, suggesting natural breakpoints for dynamic patching. Low entropy regions (green shading) represent predictable sequences that can be grouped into coherent patches. The entropy fluctuates between approximately 3.2-4.4 nats, with clear temporal patterns corresponding to the underlying time series structure. 15 Preprint. Under review. Table 7: Forecasting performance comparison (MSE and MAE) across baselines and our proposed EntroPE model. Look-back window set to 96 for all cases, forecast window = {96, 192, 336, 720} Dataset EntroPE TimeKAN HDMixer TimeBase CALF FilterTS LangTime TimeMixer Time-FFM MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTm1 ETTm2 ETTh1 ETTh2 Weather Electricity Exchange 96 0.317 0.354 0.327 0.365 0.340 0.369 0.373 0.388 0.319 0.348 0.323 0.362 0.319 0.348 0.317 0.356 0.336 0.369 192 0.360 0.379 0.354 0.380 0.379 0.385 0.411 0.409 0.375 0.376 0.364 0.382 0.368 0.375 0.367 0.384 0.378 0.389 336 0.388 0.399 0.386 0.403 0.398 0.409 0.436 0.421 0.410 0.399 0.396 0.404 0.413 0.402 0.391 0.406 0.411 0.410 720 0.446 0.433 0.452 0.442 0.467 0.443 0.503 0.461 0.478 0.439 0.462 0.441 0.487 0.439 0.454 0.441 0.469 0.441 Avg. 0.378 0.391 0.380 0.398 0.396 0.402 0.431 0.420 0.396 0.391 0.386 0.397 0.397 0.392 0.382 0.397 0.399 0.402 96 0.180 0.265 0.175 0.258 0.183 0.266 0.188 0.271 0.174 0.252 0.174 0.256 0.188 0.258 0.175 0.257 0.181 0.267 192 0.241 0.311 0.241 0.301 0.246 0.306 0.251 0.309 0.241 0.297 0.240 0.299 0.245 0.297 0.240 0.302 0.247 0.308 336 0.313 0.352 0.308 0.349 0.307 0.347 0.311 0.346 0.304 0.338 0.301 0.339 0.301 0.336 0.303 0.343 0.309 0.347 720 0.411 0.413 0.417 0.414 0.408 0.404 0.411 0.401 0.402 0.395 0.399 0.399 0.402 0.393 0.392 0.396 0.406 0.404 Avg. 0.286 0.335 0.285 0.331 0.286 0.331 0.290 0.332 0.280 0.321 0.279 0.323 0.284 0.321 0.279 0.324 0.286 0.332 0.375 0.399 0.367 0.395 0.387 0.401 0.399 0.392 0.377 0.395 0.377 0.391 0.391 0.388 0.385 0.402 0.385 0.400 96 192 0.423 0.425 0.416 0.424 0.441 0.428 0.455 0.423 0.429 0.427 0.431 0.423 0.429 0.419 0.443 0.430 0.439 0.430 336 0.429 0.432 0.446 0.436 0.452 0.433 0.501 0.443 0.475 0.449 0.479 0.448 0.462 0.440 0.512 0.470 0.480 0.449 720 0.439 0.454 0.441 0.453 0.513 0.485 0.498 0.458 0.482 0.470 0.471 0.466 0.458 0.445 0.497 0.476 0.462 0.456 Avg. 0.416 0.425 0.418 0.427 0.448 0.437 0.463 0.429 0.441 0.435 0.440 0.432 0.437 0.425 0.459 0.444 0.442 0.434 96 0.281 0.336 0.290 0.339 0.289 0.336 0.338 0.376 0.288 0.336 0.293 0.344 0.299 0.336 0.289 0.342 0.301 0.351 192 0.371 0.393 0.386 0.399 0.386 0.397 0.402 0.405 0.368 0.386 0.374 0.389 0.374 0.382 0.378 0.397 0.378 0.397 336 0.392 0.394 0.441 0.447 0.438 0.442 0.437 0.440 0.415 0.423 0.411 0.423 0.410 0.418 0.432 0.434 0.422 0.431 720 0.421 0.427 0.450 0.458 0.421 0.454 0.460 0.477 0.417 0.435 0.423 0.441 0.418 0.426 0.464 0.464 0.427 0.444 Avg. 0.366 0.387 0.391 0.410 0.384 0.407 0.409 0.425 0.372 0.395 0.375 0.399 0.375 0.392 0.390 0.409 0.382 0.406 96 0.164 0.211 0.163 0.209 0.175 0.223 0.170 0.215 0.164 0.205 0.161 0.208 0.178 0.204 0.163 0.209 0.191 0.230 192 0.210 0.252 0.208 0.251 0.226 0.265 0.216 0.256 0.213 0.252 0.226 0.264 0.211 0.249 0.211 0.254 0.236 0.267 336 0.256 0.290 0.263 0.290 0.264 0.301 0.272 0.297 0.270 0.292 0.279 0.304 0.269 0.292 0.263 0.293 0.289 0.303 720 0.339 0.342 0.341 0.341 0.348 0.349 0.351 0.348 0.352 0.346 0.345 0.344 0.351 0.347 0.344 0.348 0.362 0.350 Avg. 0.242 0.273 0.244 0.273 0.253 0.285 0.252 0.279 0.250 0.274 0.253 0.280 0.252 0.273 0.245 0.276 0.270 0. 96 0.163 0.252 0.174 0.266 0.180 0.271 0.212 0.279 0.145 0.239 0.153 0.247 0.181 0.266 0.153 0.245 0.198 0.282 192 0.177 0.268 0.182 0.273 0.184 0.275 0.209 0.281 0.162 0.253 0.168 0.260 0.185 0.273 0.166 0.257 0.199 0.285 336 0.194 0.284 0.197 0.286 0.207 0.299 0.222 0.295 0.177 0.268 0.187 0.278 0.198 0.281 0.185 0.275 0.212 0.298 720 0.235 0.321 0.236 0.320 0.249 0.335 0.264 0.327 0.222 0.304 0.227 0.313 0.241 0.320 0.224 0.312 0.253 0.330 Avg. 0.182 0.271 0.197 0.286 0.205 0.295 0.227 0.296 0.177 0.266 0.184 0.275 0.201 0.285 0.182 0.272 0.216 0.299 96 0.083 0.140 192 0.177 0.235 336 0.299 0.371 720 0.761 0.869 Avg. 0.331 0.386 0.084 0.201 0.089 0.201 0.087 0.205 0.081 0.201 0.173 0.295 0.175 0.298 0.179 0.300 0.168 0.293 0.316 0.407 0.329 0.409 0.333 0.417 0.299 0.396 0.827 0.685 0.852 0.690 0.912 0.719 0.805 0.674 0.355 0.400 0.361 0.400 0.378 0.410 0.338 0.391 Panel (d) - Time Series with Entropy-based Coloring: Overlays the original time series with color-coding based on entropy values. Yellow points indicate high entropy (high uncertainty), while dark blue/brown points represent low entropy (high predictability). This visualization reveals the relationship between time series patterns and predictive uncertainty, where rapid transitions and trend changes typically correspond to higher entropy values. A.6 TIME SERIES QUANTIZATION Real-valued time series data cannot be directly processed by entropy-based boundary detection methods that rely on discrete probability distributions. Following Ansari et al. (2024), we employ quantization to convert continuous values into discrete tokens while preserving temporal structure. Given scaled time series x1:C+H = [x1, . . . , xC, . . . , xC+H ], we define quantization function : {1, 2, . . . , B} that maps real values to discrete bins. The quantization and dequantization functions are defined as: q(x) = if < b1, if b1 < b2, 1 2 ... if bB1 < , and d(j) = cj (10) Unlike traditional approaches that rely on dequantization for prediction, our method uses quantization purely for entropy-based boundary detection while employing linear projection on continuous representations for forecasting. 16 Preprint. Under review. (a) Sample 1 (b) Sample 2 (c) Sample 3 Figure 6: Comprehensive entropy analysis for dynamic patching on ETT and Electricity data. Subfigures show how entropy-driven patches align with temporal transitions. 17 Preprint. Under review. A.7 ENTROPY CALCULATION We begin with the standard Shannon entropy for discrete random variable with possible values {x1, x2, . . . , xn}: H(X) = (cid:88) i=1 p(xi) log p(xi) (11) For sequential data, we are interested in the entropy of variable Xi+1 conditioned on the previous observations Xi = {X1, X2, . . . , Xi}. This leads to the conditional entropy: H(Xi+1Xi) = (cid:88) vV p(Xi+1 = vXi) log p(Xi+1 = vXi) (12) In practice, we estimate these conditional probabilities using parameterized model θ, giving us: H(xi) = (cid:88) vV pθ(xi+1 = vxi) log pθ(xi+1 = vxi) (13) Table 8: Forecasting performance comparison (MSE and MAE) across baselines and our proposed EntroPE model. The look-back window is fixed at = 96, with forecast horizons = {96, 192, 336, 720}. Baseline results up to 2024 are extracted from Huang et al. (2025b). Dataset EntroPE iTrans. PatchTST TimesNet DLinear FEDformer Autoformer MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTm ETTm2 ETTh1 ETTh2 Weather Electricity Exchange 96 0.317 0.354 0.334 0.368 0.352 0.374 0.338 0.375 0.346 0.374 0.379 0.419 0.505 0.475 192 0.360 0.379 0.377 0.391 0.390 0.393 0.374 0.387 0.382 0.391 0.426 0.441 0.553 0.496 336 0.388 0.399 0.426 0.420 0.421 0.414 0.410 0.411 0.415 0.415 0.445 0.459 0.621 0.537 720 0.446 0.433 0.491 0.459 0.462 0.449 0.478 0.450 0.473 0.451 0.543 0.490 0.671 0.561 Avg. 0.378 0.391 0.407 0.410 0.406 0.407 0.400 0.406 0.404 0.408 0.448 0.452 0.588 0.517 96 0.180 0.265 0.180 0.264 0.183 0.270 0.187 0.267 0.193 0.293 0.203 0.287 0.255 0.339 192 0.241 0.311 0.250 0.309 0.255 0.314 0.249 0.309 0.284 0.361 0.269 0.328 0.281 0.340 336 0.313 0.352 0.311 0.348 0.309 0.347 0.321 0.351 0.382 0.429 0.325 0.366 0.339 0.372 720 0.411 0.413 0.412 0.407 0.412 0.404 0.408 0.403 0.558 0.525 0.421 0.415 0.433 0.432 Avg. 0.286 0.335 0.288 0.332 0.290 0.334 0.291 0.333 0.354 0.402 0.305 0.349 0.327 0.371 96 0.375 0.399 0.386 0.405 0.460 0.447 0.384 0.402 0.397 0.412 0.395 0.424 0.449 0.459 192 0.423 0.425 0.441 0.436 0.512 0.477 0.436 0.429 0.446 0.441 0.469 0.470 0.500 0.482 336 0.429 0.432 0.487 0.458 0.546 0.496 0.638 0.469 0.489 0.467 0.490 0.477 0.521 0.496 720 0.439 0.454 0.503 0.491 0.544 0.517 0.521 0.500 0.513 0.510 0.598 0.544 0.514 0.512 Avg. 0.416 0.425 0.454 0.447 0.516 0.484 0.495 0.450 0.461 0.457 0.498 0.484 0.496 0.487 96 0.281 0.336 0.297 0.349 0.308 0.355 0.340 0.374 0.340 0.394 0.358 0.397 0.346 0.388 192 0.371 0.393 0.380 0.400 0.393 0.405 0.402 0.414 0.482 0.479 0.429 0.439 0.456 0.452 336 0.392 0.394 0.428 0.432 0.427 0.436 0.452 0.452 0.591 0.541 0.496 0.487 0.482 0.486 720 0.421 0.427 0.427 0.445 0.436 0.450 0.462 0.468 0.839 0.661 0.463 0.474 0.515 0.511 Avg. 0.366 0.387 0.383 0.407 0.391 0.411 0.414 0.427 0.563 0.519 0.437 0.449 0.450 0.459 96 0.164 0.211 0.174 0.214 0.186 0.227 0.172 0.220 0.195 0.252 0.217 0.296 0.266 0.336 192 0.210 0.252 0.221 0.254 0.234 0.265 0.219 0.261 0.237 0.295 0.276 0.336 0.307 0.367 336 0.256 0.290 0.278 0.296 0.284 0.301 0.246 0.337 0.282 0.331 0.339 0.380 0.359 0.395 720 0.339 0.342 0.358 0.347 0.356 0.349 0.365 0.359 0.345 0.382 0.403 0.428 0.419 0.428 Avg. 0.242 0.273 0.258 0.278 0.265 0.285 0.251 0.294 0.265 0.315 0.309 0.360 0.338 0.382 0.163 0.252 0.148 0.240 0.190 0.296 0.168 0.272 0.210 0.302 0.193 0.308 0.201 0.317 96 192 0.177 0.268 0.162 0.253 0.199 0.304 0.184 0.322 0.210 0.305 0.201 0.315 0.222 0.334 336 0.194 0.284 0.178 0.269 0.217 0.319 0.198 0.300 0.223 0.319 0.214 0.329 0.231 0.443 720 0.235 0.321 0.225 0.317 0.258 0.352 0.220 0.320 0.258 0.350 0.246 0.355 0.254 0.361 Avg. 0.182 0.271 0.178 0.270 0.216 0.318 0.193 0.304 0.225 0.319 0.214 0.327 0.227 0. 0.083 0.140 0.086 0.206 0.088 0.205 0.107 0.234 0.088 0.218 0.148 0.278 0.197 0.323 96 192 0.177 0.235 0.177 0.299 0.176 0.299 0.226 0.344 0.176 0.315 0.271 0.315 0.300 0.369 336 0.299 0.371 0.331 0.417 0.301 0.397 0.367 0.448 0.313 0.427 0.460 0.427 0.509 0.524 720 0.761 0.869 0.847 0.691 0.901 0.714 0.964 0.746 0.839 0.695 1.195 0.695 1.447 0.941 Avg. 0.331 0.386 0.360 0.403 0.367 0.404 0.416 0.443 0.354 0.414 0.519 0.429 0.613 0.539 18 Preprint. Under review. Table 9: Comprehensive evaluation of FilterTS across multiple hyperparameter configurations. We assess all available configurations (Config 1-4) for each forecast horizon and report the bestperforming results based on MSE and MAE. Missing values (-) denote configurations not originally reported for certain horizons in the official implementation. Dataset T Config 1 Config 2 Config 3 Config 4 MSE MAE MSE MAE MSE MAE MSE MAE ETTm1 ETTm2 ETTh1 ETTh2 Weather Electricity Exchange Rate 96 96 96 96 Avg. 96 96 96 96 Avg. 96 96 96 96 Avg. 96 96 96 96 Avg. 96 96 96 96 Avg. 96 96 96 96 Avg. 96 96 96 96 Avg. 96 0.321 0.360 0.323 0.362 0.330 0.366 192 0.363 0.382 0.364 0.382 0.366 0.383 336 0.397 0.405 0.396 0.404 0.398 0.403 720 0.478 0.447 0.462 0.441 0.462 0.438 0.390 0.399 0.386 0.397 0.389 0.398 96 0.174 0.256 0.174 0.257 0.174 0.256 192 0.239 0.300 0.238 0.299 0.240 0.299 336 0.299 0.338 0.300 0.340 0.301 0.339 720 0.405 0.399 0.406 0.399 0.399 0.399 0.279 0.323 0.280 0.324 0.279 0.323 96 0.375 0.390 0.381 0.396 0.377 0.391 192 0.424 0.421 0.431 0.423 0.431 0.423 336 0.479 0.451 0.465 0.442 0.479 0.448 720 0.480 0.471 0.495 0.481 0.471 0.466 0.440 0.433 0.443 0.436 0.440 0.432 96 0.289 0.338 0.293 0.344 0.291 0.341 192 0.374 0.390 0.374 0.389 0.394 0.400 336 0.416 0.426 0.411 0.423 0.433 0.431 720 0.421 0.440 0.423 0.441 0.431 0.442 0.375 0.399 0.375 0.399 0.387 0. 96 0.161 0.208 0.180 0.228 0.182 0.228 0.161 0.208 192 0.226 0.264 0.210 0.252 0.213 0.255 0.226 0.264 336 0.279 0.304 0.286 0.307 0.263 0.292 0.279 0.304 720 0.345 0.344 0.349 0.347 0.351 0.348 0.345 0.344 0.253 0.280 0.256 0.284 0.252 0.281 0.253 0.280 96 0.151 0.245 0.153 0.247 192 0.164 0.256 0.168 0.260 336 0.181 0.274 0.187 0.278 720 0.241 0.326 0.227 0.313 0.184 0.275 0.184 0.275 96 0.085 0.201 0.083 0.200 0.084 0.201 0.082 0.198 192 0.180 0.300 0.171 0.294 0.173 0.295 0.172 0.294 336 0.342 0.420 0.323 0.411 0.316 0.407 0.322 0.410 720 0.878 0.699 0.916 0.713 0.827 0.685 0.828 0.686 0.371 0.405 0.373 0.405 0.350 0.397 0.351 0.397 where: H(xi) represents the conditional entropy at position pθ(xi+1 = vxi) is the models predicted probability distribution over the vocabulary xi = {x1, x2, . . . , xi} denotes all observations up to position θ represents the learnable parameters of the predictive model This conditional entropy quantifies the uncertainty in predicting the next value given the historical context, with higher values indicating less predictable (more informative) regions of the time series. A.8 CROSS-ENTROPY LOSS LCE = 1 (cid:88) i=1 log (ti+1t1, . . . , ti; θ) (14) where θ represents the model parameters. 19 Preprint. Under review. A.9 MEAN SQUARED ERROR (MSE) LM SE ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (yi ˆyi)2 A.10 ENTROPE MODEL ALGORITHMS Algorithm 1 EntroPE Forward Pass Require: Raw time series RBCL, forecast horizon Ensure: Predictions ˆY RBCT 1: ci reshape(X, (B C, L)) 2: norm, params RevIN forward(X ci) 3: tok tokenize(X norm) 4: B, EDP(X tok, θ) 5: in, enc APE(X norm, B, ) 6: out GlobalTransformer(P in) 7: dec FusionDecoder(P out, enc) 8: ˆY norm Wproj(flatten(H dec)) 9: ˆY norm reshape( ˆY norm, (B C, )) 10: ˆY RevIN inverse( ˆY norm, params) 11: ˆY reshape( ˆY , (B, C, )) 12: return ˆY context tok[:, 1 : t] logits entropy modelθ(context) pθ(xt+1xt) softmax(logits) H(xt) (cid:80) Algorithm 2 Entropy-Based Dynamic Patcher (EDP) Require: Tokenized sequence tok R(BC)L, pre-trained entropy model θ Ensure: Patch boundaries B, patch mask R(BC)L 1: Initialize , 0 2: Phase 1: Entropy Calculation 3: for = 1 to 1 do 4: 5: 6: 7: 8: end for 9: Phase 2: Boundary Detection 10: for = 2 to 1 do 11: 12: 13: 14: end for 15: Add first and last positions as boundaries 16: {1} {L} 17: [:, 1] 1; [:, L] 1 18: return B, if H(xt) > θ and H(xt) H(xt1) > γ then {t}; [:, t] 1 vV pθ(v) log pθ(v) end if A.11 USE OF LARGE LANGUAGE MODELS Parts of the text in this paper were refined with the assistance of large language model (ChatGPT, GPT-5), used exclusively for language polishing and improving clarity of exposition. All ideas, methodology, experiments, and analyses were conceived and executed solely by the authors. 20 Preprint. Under review. Time-point embeddings patchi ci[:, B[i] : B[i + 1]] Append patchi to patches Algorithm 3 Adaptive Patch Encoder (APE) Require: Continuous sequence ci R(BC)L, patch boundaries B, vocab size Ensure: Patch embeddings in, hidden states enc 1: EmbeddingLayer(V, dt) 2: h0 E(tokenize(X ci)) 3: Extract variable-length patches 4: patches {} 5: for = 1 to 1 do 6: 7: 8: end for 9: Initial patch embedding via max pooling 10: for each patch pj do 11: 12: 13: end for 14: Cross-attention refinement (N layers) 15: hcurr h0, Pcurr P0 16: for = 1 to do 17: 18: 19: emb E(tokenize(pj)) P0[:, j, :] MaxPool(emb) 20: hn TransformerEncodern(hcurr) Wq(Pcurr) Wk(hn), Wv(hn) out softmax Pn Pcurr + Wo(out) Update hcurr, Pcurr (cid:16) QKT dk 21: 22: 23: end for 24: return in PN , enc hN (cid:17) V"
        }
    ],
    "affiliations": [
        "Department of Computer Science, Khalifa University, UAE",
        "Information Systems Technology and Design, Singapore University of Technology and Design, Singapore",
        "Institute for Infocomm Research, A*STAR, Singapore",
        "School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore"
    ]
}