{
    "paper_title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study",
    "authors": [
        "Yuqi Zhu",
        "Yi Zhong",
        "Jintian Zhang",
        "Ziheng Zhang",
        "Shuofei Qiao",
        "Yujie Luo",
        "Lun Du",
        "Da Zheng",
        "Huajun Chen",
        "Ningyu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate models across three dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 4 9 7 9 1 . 6 0 5 2 : r Why Do Open-Source LLMs Struggle with Data Analysis? Systematic Empirical Study Yuqi Zhu, Yi Zhong, Jintian Zhang, Ziheng Zhang, Shuofei Qiao, Yujie Luo, Lun Du, Da Zheng, Huajun Chen, Ningyu Zhang* Zhejiang University Ant Group Independent Researcher Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph {zhuyuqi,zhangningyu}@zju.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) hold promise in automating data analysis tasks, yet opensource models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating seed dataset of diverse, realistic scenarios, we evaluate models across three dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates greater impact than diversity in achieving optimal performance. We leverage these insights to develop data synthesis methodology, demonstrating significant improvements in open-source LLMs analytical reasoning capabilities. Figure 1: Core capabilities involved in data analysis tasks. We break down the process into three key components: data understanding, coding and planning."
        },
        {
            "title": "Introdution",
            "content": "Data analysis is complex and multifaceted task that lies at the core of many real-world applications (Donoho, 2017; Inala et al., 2024a). It involves series of interdependent steps, including understanding user queries, exploring and interpreting structured data, formulating hypotheses, and generating insights. As shown in Figure 1, unlike traditional tasks, data analysis requires tight integration of natural language understanding, logical reasoning, code drafting, and result interpretationall within an interactive and goal-driven setting. Large Language Models (LLMs) have emerged as promising tools to automate data analysis tasks, especially given their strong capabilities in reasoning, planning, and code generation. To enhance their performance on these complex tasks, researchers have explored strategies such as prompt- * Corresponding Author. ing techniques, fine-tuning, and multi-agent frameworks (Guo et al., 2024; Trirat et al., 2024; Hu et al., 2024; Hong et al., 2024). Additionally, domainspecific datasets like DSBench (Jing et al., 2024) and BLADE (Gu et al., 2024) have been developed to adapt models to these unique challenges. Despite the promise, current state-of-the-art performance is often driven by large-scale, highly advanced models (e.g., GPT-4 (Hurst et al., 2024), DeepSeek-R1 (DeepSeek-AI et al., 2025)), which can limit accessibility and pose challenges for advancing research. Open-source small models, while promising, continue to lag in capability, particularly for reasoning-intensive applications. This raises key research question: How can we effectively enhance open-source LLMs for complex, reasoning-intensive data analysis tasks? Inspired by prior work on fine-tuning with high-quality synthetic data (Muennighoff et al., 2025; Ye et al., 2025; Ahmad et al., 2025), we explore whether similar strategies can be leveraged to improve opensource LLMs in the data analysis domain. their analysis before generating final reports with their findings. To answer the above question, we conduct systematic exploration of the essential capabilities required for data analysis and propose methods to develop these capabilities in open-source LLMs. Specifically, we use Qwen models as the foundational baseline and evaluate their performance across three critical dimensions: Data Comprehension, Code generation, and Strategic Planning. Leveraging curated seed dataset that encompasses diverse data analysis scenarios, we conduct targeted experiments and ablation studies to analyze the factors influencing model generalization and performance on complex analytical tasks. Our analysis reveals three key findings: (1) The models planning ability emerges as more critical determinant of success than its capabilities in data understanding and code generation. This underscores the importance of strategic foresight and structured reasoning in navigating complex analytical scenarios. (2) Appropriate interaction turns, combined with the complexity of the data and suitable reasoning descriptions, can enhance the models reasoning capacity. However, performance gains vary across different data analysis tasks, suggesting that task-specific characteristics play crucial role in shaping the models effectiveness. (3) High-quality training data proves to be more critical than data diversity for achieving optimal performance in data analysis tasks. This emphasizes the necessity of curating datasets with precise and comprehensive annotations to ensure reliable and robust model outcomes. To further substantiate these insights, we synthesize new training data informed by the identified strategies and subsequently retrain the model, thereby validating the effectiveness of our approach."
        },
        {
            "title": "2 Background of Data Analysis Agents",
            "content": "Data analysis tasks aim to derive actionable insights from data through systematic exploration and analysis (Inala et al., 2024b; Sun et al., 2024). In typical workflow, analysts begin with specific questions about dataset, then proceed through multiple analytical steps. These steps include data preprocessing and cleaning, hypothesis exploration, data transformations, and report generation. The process is inherently interactive - analysts work with structured tabular data, develop analysis code, interpret intermediate results, and iteratively refine To formally characterize the data analysis process in the context of LLM agents, we define parameterized analysis function fθ that maps the input components to the analytical outputs: fθ : (D, Q, ) (S, R) (1) where represents the structured data, specifies the analytical objective or query, and is the library of available analysis tools. fθ models the behavior of an LLM-based agent that performs analysis by generating sequence of intermediate analysis states = {st}, ultimately producing final report to summarize the results."
        },
        {
            "title": "3 Dataset and Experiments",
            "content": "To investigate the capabilities of different models in the domain of data analysis, we first collect dataset tailored to this field and introduce the experimental setup in this section."
        },
        {
            "title": "3.1 Data Collection and Curation",
            "content": "Initial Collection. To construct our training dataset, we gather samples from DAEval (Hu et al., 2024), which provides real-world CSV files from GitHub repositories, DSBench (Jing et al., 2024) containing analysis tasks from ModelOff competitions, and TableBench (Wu et al., 2025) featuring complex tabular tasks across multiple domains. We further include structured data from WTQ (Pasupat and Liang, 2015) and FetaQA (Nan et al., 2022). To investigate the impact of different chain-of-thought reasoning strategies on data analysis tasks, we employ DeepSeek-R1 to generate synthetic datasets tailored for this study. All training data are collected to ensure no overlap with our evaluation benchmarks, enabling reliable out-of-distribution (OOD) assessment. Utilizing correctness-based filtering, we curated 6,443 distinct samples, encompassing diverse array of analytical challenges (refer to Appendix for detailed statistics). Filtering Criteria. To ensure data quality, we apply two-stage filtering process. First, we automatically remove invalid samples based on several criteria: low-quality code implementations, such as those failing to utilize provided files or producing code with no meaningful return values; samples containing compilation errors; and entries that do not conform to format requirements. Following the automated filtering, we perform manual verification through sampling to further refine the dataset. This process results in final collection of 5,613 high-quality samples for subsequent experiments. 3.2 Experiment Setup. We evaluate series of open-source model variants, including Qwen2.5-7B-Instruct, Qwen2.57B-Coder, Qwen2.5-7B-Instruct-1M, and R1Distill-Qwen-7B, alongside well-established opensource models like GPT-4o, DeepSeek-v31, and DeepSeek-R1. Following prior work (Hu et al., 2024; Zeng et al., 2025), we adopt ReAct (Yao et al., 2023) to iteratively alternate between planning, code generation, and execution for multi-turn data analysis. Model training is conducted based on Qwen2.5-Instruct, utilizing LoRA (Hu et al., 2022) for efficient fine-tuning (Details on hyperparameters are provided in Appendix A). Evaluation. To assess models performance on data analysis tasks, we employ two comprehensive benchmarks: DiscoveryBench (Majumder et al., 2024) and QRData (Liu et al., 2024). DiscoveryBench covers 264 tasks across six domains (e.g., sociology, engineering), and we focus on its real-world test set comprising 239 tasks. QRData, specifically designed for statistical and causal analysis, consists of 411 questions paired with data sheets sourced from textbooks, materials, online learning and academic papers. Following prior work (Zeng et al., 2025), we adopt accuracy as our evaluation metric. Since both model outputs and ground truth are expressed in natural language, we employ GPT-4o-mini2 for agreement evaluation."
        },
        {
            "title": "4 Core Capabilities for Data Analysis",
            "content": "Data analysis tasks present unique challenge, requiring the integration of multiple capabilities. The process encompasses three essential skills that determine model performance: data comprehension, which addresses the ability to understand and utilize structured data effectively; code generation , which focuses on writing correct and efficient analytical solutions through programming; and strategic planning , which involves understanding problems and breaking them down into manageable steps. These three aspects - Data comprehension (4.1), Code generation (4.2) and Strategic planning (4.3) - form the foundation of our 1We use gpt-4o-2024-08-06 and deepseek-v3-0324. 2gpt-4o-mini-2024-07-18 Model QRData DiscoveryBench Multi-Turn Setting Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-7B-Coder Qwen2.5-7B-Instruct-1M R1-Distill-Qwen-7B GPT-4o DeepSeek-v3 Deepseek-R1 39.71 53.53 57.18 36.50 39.17 30.41 59.85 65.21 63.26 14.64 24.27 27. 13.60 15.48 7.95 28.03 36.82 37.66 Table 1: Performance comparison across models in multi-turn settings. investigation into data requirements for effective analytical capabilities. 4.1 Data Comprehension To investigate whether data comprehension serves as critical factor in enabling effective data analysis, we design series of experiments to evaluate the models ability to reason over structured information. Specifically, the experiments focus on two key aspects: (1) whether explicitly providing structured context, such as tabular data, enhances the models reasoning accuracy, and (2) how the model performs when faced with increased complexity, including scenarios involving multiple sources of structured information, some of which may be irrelevant or distractive. Tabular Information. To evaluate the models ability to understand tabular data, we provide both the table content and the associated reasoning path as input. This setup allows us to examine whether explicitly including table information enhances the models reasoning performance. As shown in Table 2, adding table information helps the models in simpler tasks like QRData, showing slight improvements in performance. However, the gains are limited, indicating that the models already handle much of the reasoning without explicit table inputs. For the more complex DiscoveryBench, while the 7B model benefits from the inclusion of table information, the 14B model exhibits slight drop in accuracy. This decline can be attributed to the increased input length introduced by the table input, which leads the 14B model to generate longer output sequences. The expanded output length appears to negatively impact the models reasoning performance in this specific case. Model QRData DiscoveryBench Model QRData DiscoveryBench w/o Info Info w/o Info Info Qwen2.5-7B Qwen2.5-14B 6.57 15. 7.54 15.82 0.42 0.42 1.26 0.00 Table 2: Accuracy comparison of 7B and 14B models on QRData and DiscoveryBench (w/o and w/ table information). Model QRData DiscoveryBench w/o Extra w/ Extra w/o Extra w/ Extra Qwen2.5-7B Qwen2.5-14B 37.96 52.55 34.55 52.07 5.44 10.88 4.18 12.13 Table 3: Accuracy comparison of 7B and 14B models on QRData and DiscoveryBench (w/o and w/ extra data files). Data Complexity. To further evaluate the robustness of the model under increasing data complexity, we introduce additional tables as redundant information, including distractors unrelated to the task. This setup requires the model to reason over multiple tables, some of which are irrelevant, simulating scenarios with heightened complexity. Models that effectively filter out irrelevant data while focusing on task-relevant information are considered to exhibit stronger data understanding capabilities. For fair evaluation, we remove task-specific background descriptions from the input, forcing the model to rely solely on the tabular data for reasoning. As shown in Table 3, the inclusion of redundant data increases input complexity but does not lead to significant decline in overall performance. The 7B model exhibits modest decline in accuracy on both benchmarks, suggesting it is more sensitive to increased input noise. In contrast, the 14B model demonstrates stronger robustness, maintaining stable performance across both datasets. Discussion and Implications. Data information exerts limited influence on models, particularly larger ones (e.g., 14B), which maintain stable performance even as input complexity increases. This suggests that the ability to process structured data is not the primary limitation for larger models; instead, their performance may be constrained by other underlying bottlenecks. Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct 34.64% 29.94% 20.73% Qwen2.5-7B-Coder 43.30% Qwen2.5-7B-Instruct-1M 31.44% 48.37% R1-Distill-Qwen-7B 54.25% 40.64% 31.73% 50.98% 55.00% 60.00% Table 4: Average code error rate of different models. Code Performance. Our evaluation here examines whether code-specialized pretraining improves task performance and explores the trade-offs between code generation accuracy and model efficiency in analytical tasks. Table 1 provides an overview of the overall model perfomance, while Table 4 summarizes the average error rates observed across two datasets. Our analysis reveals several key findings: (1) Limited impact of code specialization: Qwen2.57B-Coder does not demonstrate clear advantage over general-purpose models. This suggests that code specialization alone may not directly translate to better performance in analytical tasks, possibly due to limitations in instruction-following or reasoning generalization. (2) Hallucination in distilled models: R1-Distill-Qwen-7B, despite being distilled from long chain of thought model, performs poorly, often hallucinating file interpretations rather than generating executable code. (3) Efficiency in task execution: When comparing Qwen2.5-7B-Instruct-1M and Qwen2.5-7BInstruct under matched output length constraints, both models exhibit comparable coding capabilities; however, the latter demonstrated superior planning efficiency by completing tasks in fewer interaction rounds. To further substantiate these findings, we manually sample 354 erroneous responses generated by the and categorized the errors using GPT-4o-mini. The categorization is based on the gap between the incorrect responses and the corresponding correct trajectories. As shown in Figure 2, only small fraction of the errors are attributed to deficiencies in code generation capabilities. Instead, the majority of errors are linked to shortcomings in the models planning and reasoning abilities."
        },
        {
            "title": "4.2 Code Capability",
            "content": "To explore the relationship between code generation capabilities and data analysis performance, we conduct series of experiments using models that vary in scale and specialization. Discussion and Implications. The ability to generate code alone is insufficient to guarantee strong performance in data analysis tasks. Instead, models capacity to effectively plan and seamlessly integrate code generation with reasoning Figure 2: The distribution of error type. emerges as more decisive factor. 4.3 Strategic Planning Data analysis is inherently planning-intensive process that demands careful coordination of data access, transformation, and reasoning steps. Building on prior experiments, we further investigate how the strategic reasoning capabilities of LLMs influence data analysis. Specifically, we evaluate their performance along four key dimensions: Interaction Turns, Reasoning Length, Task Complexity and Problem Diversity. 4.3."
        },
        {
            "title": "Interaction Turns",
            "content": "To assess the impact of dialogue turn strategies on model performance, we categorized interactions into three primary turn lengths: Short (2-3 turns), Medium (4-5 turns), and Long (6+ turns). Additionally, we included Mixed strategy that combines varying turn lengths to reflect more dynamic interaction scenarios. Trend across Different Model Sizes. Figure 3 reports the performance of Qwen2.5-7B and Qwen2.5-14B under different dialogue strategies. To ensure fair comparison, both models were fine-tuned using the same dataset size across all strategies, which is 1020 here. The results reveal consistent trends across the two models: mediumlength interactions generally achieve relatively better performance across both datasets, suggesting their effectiveness for tasks requiring moderate reasoning depth. In contrast, short and long interaction strategies yield slightly lower yet relatively stable results. However, the mixed strategy consistently demonstrates inferior performance, potentially due to the variation in turn lengths, which may hinder the models ability to capture consistent interaction patterns during training. Then given the alignment in performance trends between the two models, we selected the 7B model for subsequent experiments. This decision balances computational efficiency Figure 3: Impact of dialogue turn strategies across different Qwen model scales and training methods. Turn Category # Sample QRData DiscoveryBench All Short Medium Long Medium + Short Medium + Long 5613 1034 3559 1020 4593 4579 48. 47.68 49.15 47.94 47.45 46.96 15.00 23.85 18.83 18.41 21.34 21.76 Table 5: Effectiveness analysis on different turn lengths. with analytical rigor, enabling deeper exploration of dialogue strategies. Further Investigation. To better understand the impact of turn length, we examine the performance of various interaction strategies using the collected dataset. The results are presented in Table 5, from which we derive the following observations: (i) Turn length preferences differ across tasks. The results reveal that Medium-length turns achieve relatively higher performance on QRData, indicating their suitability for tasks requiring moderate reasoning depth. On DiscoveryBench, which features longer and more complex inputs, Short turns surprisingly outperform other strategies, potentially due to their ability to focus on concise and straightforward reasoning. (ii) Quality Over Quantity. Our experiments reveal that increasing the amount of training data does not necessarily lead to better performance, even when using the same interaction turn strategies. In fact, medium-length turns trained on smaller subset consistently outperform models trained on the full dataset. This finding highlights the importance of data quality and task relevance, suggesting that the effectiveness of fine-tuning is shaped by factors beyond data quantity alone. (iii) balanced selection strategy is necessary. Mixed strategies, such as Medium + Short and Medium + Long, exhibit balanced but slightly lower performance. These observations suggest that different interaction styles may offer complementary strengths, but leveraging them effectively likely requires carefully designed curriculum or selection mechanism rather than uniform mixing. To ensure consistency and facilitate direct comparisons in subsequent experiments, we adopt the Medium turn strategy as the baseline for all followup evaluations. 4.3.2 Reasoning Length To investigate whether longer reasoning chains from stronger models improve planning and task success, we augment training samples with intermediate <think> segments generated by DeepSeekR1. These segments aim to capture explicit intermediate reasoning steps that may scaffold better decision-making during multistep analysis. We evaluate three settings: (1) Original reasoning, which uses the original training data without modification. These samples include reasoning content, but the reasoning is typically shorter and less explicit compared to the augmented settings; (2) Full reasoning, which replaces the original reasoning with full <think> traces for intermediate turns (excluding the first and final turns); and (3) Summarized reasoning, which substitutes the original reasoning with concise summaries (generated by an LLM) of the full traces. The reasoning is inserted only in the middle turns to avoid hallucinations during initial grounding and to maintain brevity in the final answer generation. Experiments are conducted under varying per-turn token budgets (1024, 2048, and 4096) to simulate different interaction constraints. Results are reported in Figure 4 from which we get the following findings: (i) Longer are not always better. While longer reasoning chains might seem beneficial, the Full setting consistently underperforms the Original across most configurations, particularly in DiscoveryBench. The sharp performance decline at the 4096-token level suggests that overly verbose and unfiltered reasoning may overwhelm the model. In contrast, the Summarized reasoning setting, which provides more detailed reasoning than the Original while avoiding the verbosity of the Full, consistently matches or surpasses baseline accuracy. By striking balance between informativeness and brevity, summarized reasoning preserves essential steps and enhances clarity, offering more effective Figure 4: Performance comparison of QRData and DiscoveryBench across different settings and token limits. Difficulty QRData DiscoveryBench Easy Medium Hard Medium + Hard 42.58 51.34 48.18 51.34 20.50 18.83 19.50 23.01 Table 6: Analysis of Complexity characteristics using optimal turn settings (% accuracy). Each difficulty level contains 733 samples. approach to robust decision-making. These findings indicate that longer reasoning chains are not inherently advantageous, and appropriately detailed yet concise reasoning may lead to better outcomes. (ii) Token budgets exhibit diminishing returns. Increasing the per-turn token budget can improve performance by enabling better integration of reasoning and task-relevant content, as seen in datasets like QRData. However, these gains are often limited, and in some cases, such as with DiscoveryBench, larger token budget may even reduce performance by amplifying noise or irrelevant information. This highlights that more tokens are not always better; the effectiveness of token budgets depends on the specific task and dataset. Concise and high-quality reasoning is often more effective than simply increasing the token budget."
        },
        {
            "title": "4.3.3 Task Complexity",
            "content": "To evaluate the impact of data difficulty on model reasoning capabilities, we classify each example based on the performance of models with varying capacities. Specifically, task is labeled easy if it can be correctly solved by Qwen2.5-7B, medium if only Qwen2.5-14B can solve it, and hard if it requires DeepSeek-R1 to provide the correct answer. We train the model using data of varying difficulty levels, with the results summarized in Table 6. Analysis of the results reveals clear trend: the models overall performance improves as the diffiDiversity QRData DiscoveryBench Original Distribution Balanced Sampling 46.72 45.00 20.92 21. Table 7: Analysis of Diversity characteristics using optimal turn settings (% accuracy). The number of samples is 2220. examples under two settings: (1) natural distribution that reflects the original domain frequencies, and (2) balanced distribution that down-samples dominant domains while retaining all examples from underrepresented ones. Both settings preserve the overall difficulty distribution of the dataset. Results in Table 7 show minimal performance differences between the two settings, indicating that domain diversity alone does not significantly influence model performance in this context. These findings suggest that the effectiveness of model is not solely determined by the diversity of problems it encounters during training. Rather, the diversity and richness of reasoning strategiessuch as the depth of reasoning processes and the complexity of logical stepsappear to play more influential role in shaping models performance. What Makes Data Effective ? The efficacy of large language models in data analysis is fundamentally shaped by the quality of the data rather than its sheer diversity. Highquality, well-structured tasks that emphasize nuanced complexity and transparent reasoning pathways are instrumental in refining the models analytical capabilities."
        },
        {
            "title": "Performance",
            "content": "While the previous section examined how different aspects of strategic planning influence model performance, we now ask practical question: how can these insights be leveraged to guide data collection and reuse in order to improve data analysis capabilities?"
        },
        {
            "title": "5.1 Strategy-Guided Data Synthesis",
            "content": "We design our data synthesis process in three systematically organized stages to construct refined dataset that supports reasoning in data analysis. 1) Prompt-Based Answer Generation. We begin by leveraging prompt-based generation techniques to produce multiple candidate answers for each Figure 5: Analysis of Difficulty. (a) Average number of response rounds of the model. (b) Average output token length of the model. culty of the training data increases. To further investigate the impact of task complexity on the models behavior, we analyze the models dialogue turns and average response length, as illustrated in Figure 5. The findings show that: As the difficulty of training data increases, the models responses shift from relying on interactions with more turns to obtain feedback to providing more comprehensive answers in fewer turns. This also implies an increase in the workload per step for the model and improvement in model capabilities. From the results in Table 6, the comprehensive effects of medium and hard data are better, especially on QRData, which is relatively simple. Notably, when medium-level and hard-level data are combined during training, the model achieves the lowest average number of interaction turns and the shortest average response length, indicating that this mixed-difficulty strategy cultivates diverse problem-solving approaches and maximizes answer efficiency. However, on the more complex DiscoveryBench dataset, models trained on simple data performed better, likely due to the datasets need for more dialogue turnsa behavior aligned with models trained on simpler tasks."
        },
        {
            "title": "4.3.4 Problem Diversity",
            "content": "We further investigate whether adjusting the diversity of question improves model performance. To this end, we annotate the dataset with semantic category labels using GPT-4o-mini and retain ten major domains after manual consolidation. To quantify diversity, we employed three-stage procedure to classify each question into distinct domains. The detailed methods and descriptions of the categories can be found in Appendix C. From the complete medium turn dataset, we select 2,220 Model QRData DiscoveryBench API Models GPT-4o DeepSeek-v3 Deepseek-R 7B Models 59.85 65.21 63.26 Qwen2.5-Instruct Ours 39.71 53.77 14B Models Qwen2.5-Instruct Ours 53.53 58.15 28.03 36.82 37.66 14.64 22.59 24.27 36.82 Table 8: Comparison of Model Performances Across API and Different Sizes. input query. This step ensures diverse pool of responses, capturing range of reasoning patterns and perspectives. 2) Targeted Instance Selection. Next, we prioritize medium-length dialogues and examples of medium to high difficulty, as these have been shown to facilitate more stable and effective learning. Through this filtering process, we select instances that strike balance between complexity and informativeness. 3) Reasoning-Driven Data Enrichment. Finally, we enrich each selected instance with concise reasoning summary. This step enhances abstraction and generalization by explicitly capturing the underlying reasoning process, allowing models to better learn transferable insights. By following this three-stage synthesis process with format standardization, we construct dataset comprising 2.8k instances. The resulting dataset serves as the foundation for Supervised Fine-Tuning (SFT) to optimize model performance. detailed overview of the synthesis process and the training parameters is provided in the Appendix D."
        },
        {
            "title": "5.2 Evaluation Results",
            "content": "Table 8 presents the evaluation results of our approach, where we fine-tune 7B and 14B models on the curated dataset. Notably, the fine-tuned 7B model demonstrates substantial performance improvements compared to its baseline, while the fine-tuned 14B model achieves results that are comparable to or surpass those of GPT-4o. These results suggest that even simple, insight-driven adjustments to training data can yield substantial improvements in models performance on complex analytical tasks. However, the performance gains appear to diminish as model scale increases, suggesting potential saturation point. One possible explanation is that our filtering strategy is constructed using the 7B model, making the resulting training distribution better aligned with its inductive biases. While this alignment benefits smaller architectures, it may be less effective for larger models with more diverse representational capacities. And key limitation of our approach lies in the dataset itself. While the curated dataset provides value, it falls short in addressing the diversity and complexity required for more challenging tasks, making high-quality data persistent bottleneck. To overcome this, future efforts should focus on expanding the dataset to include richer, more representative samples from real-world applications. Such improvements would not only better capture the variability and nuances of real-world scenarios but also enable further refinement of filtering strategies, enhancing scalability and generalizability across different model sizes and domains."
        },
        {
            "title": "6 Related Work",
            "content": "LLM Agents. To adapt LLMs to complex reasoning tasks, recent work has explored three main approaches: prompt engineering, supervised finetuning (SFT), and reinforcement learning (RL). Prompt-based methods (Zhao et al., 2024; Yao et al., 2023; Wang et al., 2023) improve reasoning performance by reformulating inputs to better elicit the models latent capabilities. SFT-based methods (Chen et al., 2023; Zeng et al., 2024; Yin et al., 2024; Qiao et al., 2024; Song et al., 2024) provide task adaptation through labeled data, and have been applied in settings such as S1 (Muennighoff et al., 2025), which introduces \"budget forcing\" to control reasoning steps, and LIMO (Ye et al., 2025), which demonstrates that complex mathematical abilities can emerge from carefully curated examples. RLbased approaches further align model behavior with human preferences and problem-solving strategies. Reinforcement learning further enhances reasoning capabilities by optimizing multi-step decisionmaking. For example, DeepSeek-R1 (DeepSeekAI et al., 2025) leverages Group Relative Policy Optimization (GRPO) to train LLMs on complex reasoning tasks. RAGEN (Wang et al., 2025) introduces modular RL framework that supports stable multi-turn learning via the StarPo architecIn addition, ReSearch (Chen et al., 2025) ture. integrates RL with retrieval-augmented generation (RAG), treating search operations as part of the reasoning process and learning when and how to retrieve external information. LLM agents for Data Analysis. Data science is an interdisciplinary field that focuses on extracting valuable insights from various data sources, with data analysis serving as crucial component. Recently, researchers have proposed several specialized LLMs to enhance automated data analysis capabilities. Data Copilot (Zhang et al., 2023) is code-centric data analysis agent that efficiently handles massive data processing and visualization through pre-designed interfaces, specifically tailored to automate financial data analysis tasks while reducing errors and improving efficiency. Data Interpreter (Hong et al., 2024) takes different path by using hierarchical dependency graphs to represent workflows, enabling automatic task breakdown and code improvements. AutoKaggle (Li et al., 2024) coordinates multiple specialized agents (Planner, Developer, and Reviewer) to handle endto-end data analysis tasks, while keeping room for human input when needed. To evaluate the ability of these agents, researchers have developed several comprehensive benchmarks, such as InfiAgentDABench (Hu et al., 2024), DSBench (Jing et al., 2024), DA-code (Huang et al., 2024), and DataSciBench (Zhang et al., 2025) for assessing the effectiveness of LLM-based data analysis solutions."
        },
        {
            "title": "7 Conclusion and Future Work",
            "content": "In this work, we conduct detailed investigation into the data efficiency challenges faced by opensource LLMs in data analysis tasks. To this end, we collect targeted dataset tailored to data analysis scenarios and explore the question: What strategies and resources are most effective for enhancing open-source LLMs on data analysis tasks? By examining model performance from the perspective of different capability dimensions, we identify key factors that influence effectiveness across varying interaction patterns. Our findings offer insights into how training strategies and multi-turn data design can be better aligned with the reasoning demands of data analysis tasks. In the future, we plan to construct large-scale synthetic dataset for data analysis and incorporate reinforcement learning to optimize data quality and further enhance model performance."
        },
        {
            "title": "Limitations",
            "content": "While our experiments provide valuable insights into the performance of data efficiency in the context of data analysis, several limitations should be acknowledged. Our work does not incorporate experiments on broader range of foundation models, such as LLaMA (Dubey et al., 2024) and other emerging alternatives, which may exhibit distinct characteristics and behaviors. Second, due to GPU limitations, we were unable to extend our experiments to larger-scale datasets that could better reflect the diversity and complexity of real-world data analysis scenarios. Third, our evaluation is restricted to models of specific parameter sizes, leaving the behavior of significantly larger models (e.g., 72B) or smaller models(e.g., 2B) unexplored. Finally, the generalizability of our findings to other tasks, domains and application contexts remains an open question and warrants further investigation."
        },
        {
            "title": "References",
            "content": "Wasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha Jain, Jocelyn Huang, Vahid Noroozi, and Boris Ginsburg. 2025. Opencodereasoning: Advancing data distillation for competitive coding. CoRR, abs/2504.01943. Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. 2023. Fireact: Toward language agent fine-tuning. CoRR, abs/2310.05915. Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. 2025. Research: Learning to reason with search for llms via reinforcement learning. CoRR, abs/2503.19470. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948. David Donoho. 2017. 50 years of data science. Journal of Computational and Graphical Statistics, 26(4):745766. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. 2024. The llama 3 herd of models. CoRR, abs/2407.21783. Ken Gu, Ruoxi Shang, Ruien Jiang, Keying Kuang, Richard-John Lin, Donghe Lyu, Yue Mao, Youran Pan, Teng Wu, Jiaqian Yu, Yikun Zhang, Tianmai M. Zhang, Lanyi Zhu, Mike A. Merrill, Jeffrey Heer, and Tim Althoff. 2024. BLADE: benchmarking language model agents for data-driven science. In Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 1393613971. Association for Computational Linguistics. Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, and Jun Wang. 2024. DS-agent: Automated data science by empowering large language models with case-based reasoning. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 1681316848. PMLR. Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Li Zhang, Lingyao Zhang, Min Yang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang, Xiangtao Lu, Xiawu Zheng, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zongze Xu, and Chenglin Wu. 2024. Data interpreter: An llm agent for data science. Preprint, arXiv:2402.18679. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Qianli Ma, Guoyin Wang, Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, Yao Cheng, Jianbo Yuan, Jiwei Li, Kun Kuang, Yang Yang, Hongxia Yang, and Fei Wu. 2024. Infiagent-dabench: Evaluating agents on data analysis tasks. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Yiming Huang, Jianwen Luo, Yan Yu, Yitong Zhang, Fangyu Lei, Yifan Wei, Shizhu He, Lifu Huang, Xiao Liu, Jun Zhao, and Kang Liu. 2024. Da-code: Agent data science code generation benchmark for large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 1348713521. Association for Computational Linguistics. Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, and et al. 2024. Gpt-4o system card. Preprint, arXiv:2410.21276. Jeevana Priya Inala, Chenglong Wang, Steven Mark Drucker, Gonzalo Ramos, Victor Dibia, Nathalie Riche, Dave Brown, Dan Marshall, and Jianfeng Gao. 2024a. Data analysis in the era of generative AI. CoRR, abs/2409.18475. Jeevana Priya Inala, Chenglong Wang, Steven Mark Drucker, Gonzalo Ramos, Victor Dibia, Nathalie Riche, Dave Brown, Dan Marshall, and Jianfeng Gao. 2024b. Data analysis in the era of generative AI. CoRR, abs/2409.18475. Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, and Dong Yu. 2024. Dsbench: How far are data science agents to becoming data science experts? Preprint, arXiv:2409.07703. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz, Germany, October 23-26, 2023, pages 611 626. ACM. Ziming Li, Qianbo Zang, David Ma, Jiawei Guo, Tuney Zheng, Minghao Liu, Xinyao Niu, Yue Wang, Jian Yang, Jiaheng Liu, Wanjun Zhong, Wangchunshu Zhou, Wenhao Huang, and Ge Zhang. 2024. Autokaggle: multi-agent framework for autonomous data science competitions. CoRR, abs/2410.20424. Xiao Liu, Zirui Wu, Xueqing Wu, Pan Lu, Kai-Wei Chang, and Yansong Feng. 2024. Are llms capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 92159235. Association for Computational Linguistics. Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Bhavana Dalvi Mishra, Abhijeetsingh Meena, Aryan Prakhar, Tirth Vora, Tushar Khot, Ashish Sabharwal, and Peter Clark. 2024. Discoverybench: Towards data-driven discovery with large language models. CoRR, abs/2407.01725. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel J. Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. CoRR, abs/2501.19393. Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryscinski, Hailey Schoelkopf, Riley Kong, Xiangru Tang, Mutethia Mutuma, Ben Rosand, Isabel Trindade, Renusree Bandaru, Jacob Cunningham, Caiming Xiong, and Dragomir R. Radev. 2022. Fetaqa: Freeform table question answering. Trans. Assoc. Comput. Linguistics, 10:3549. Panupong Pasupat and Percy Liang. 2015. Compositional semantic parsing on semi-structured tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pages 1470 1480. The Association for Computer Linguistics. Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. 2024. Autoact: Automatic agent learning from scratch for QA via self-planning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 30033021. Association for Computational Linguistics. Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. 2024. Trial and error: Exploration-based trajectory optimization for LLM agents. CoRR, abs/2403.02502. Maojun Sun, Ruijian Han, Binyan Jiang, Houduo Qi, Defeng Sun, Yancheng Yuan, and Jian Huang. 2024. survey on large language model-based agents for statistics and data science. CoRR, abs/2412.14222. Patara Trirat, Wonyong Jeong, and Sung Ju Hwang. 2024. Automl-agent: multi-agent LLM framework for full-pipeline automl. CoRR, abs/2410.02958. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Planand-solve prompting: Improving zero-shot chainof-thought reasoning by large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 26092634. Association for Computational Linguistics. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Monica Lam, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, and Manling Li. 2025. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. Preprint, arXiv:2504.20073. Xianjie Wu, Jian Yang, Linzheng Chai, Ge Zhang, Jiaheng Liu, Xeron Du, Di Liang, Daixin Shu, Xianfu Cheng, Tianzhen Sun, Tongliang Li, Zhoujun Li, and Guanglin Niu. 2025. Tablebench: comprehensive and complex benchmark for table question answering. In AAAI-25, Sponsored by the Association for the Advancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pages 2549725506. AAAI Press. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. LIMO: less is more for reasoning. CoRR, abs/2502.03387. Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Raghavi Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin. 2024. Agent lumos: Unified and modular training for open-source language agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1238012403. Association for Computational Linguistics. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. 2024. Agenttuning: Enabling generalized agent abilities for llms. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 30533077. Association for Computational Linguistics. Qiuhai Zeng, Claire Jin, Xinyue Wang, Yuhan Zheng, and Qunhua Li. 2025. An analyst-inspector framework for evaluating reproducibility of llms in data science. CoRR, abs/2502.16395. Dan Zhang, Sining Zhoubian, Min Cai, Fengzu Li, Lekang Yang, Wei Wang, Tianjiao Dong, Ziniu Hu, Jie Tang, and Yisong Yue. 2025. Datascibench: An LLM agent benchmark for data science. CoRR, abs/2502.13897. Wenqi Zhang, Yongliang Shen, Weiming Lu, and Yueting Zhuang. 2023. Data-copilot: Bridging billions of data and humans with autonomous workflow. arXiv preprint arXiv:2306.07209. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. 2024. Expel: LLM In Thirty-Eighth agents are experiential learners. AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 1963219642. AAAI Press. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine-tuning In Proceedings of the of 100+ language models. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand. Association for Computational Linguistics. Experimental Setup. Our models are fine-tuned using the LLaMA Factory framework (Zheng et al., 2024). Experiments are conducted on 4 NVIDIA A800 80G GPUs, leveraging the DeepSpeed ZeRO-3 optimization for efficient distributed training. We adopt perdevice batch size of 1, with gradient accumulation steps set to 4. Training is performed using bfloat16 precision, and warmup ratio of 0.02. We employ cosine learning rate scheduler with an initial learning rate of 1 105. All our fine-tuned models are evaluated using vLLM (Kwon et al., 2023) with the temperature set to 0. Datasets Collection. We synthesized series of trajectories based on existing data. Notably, we exclude portions of DSBench data that involve image inputs to maintain consistency and ensure the focus of our analysis. Detailed statistics of the generated data can be found in Table 9. Source # Question # Sample DAEval DSBench TableBench WTQ FetaQA Total 257 466 886 4,344 2,000 7,899 228 319 612 3,779 1,505 6,443 Table 9: Statistics of the generated data size."
        },
        {
            "title": "C Problem Diversity",
            "content": "To quantify diversity, we employed three-stage procedure to classify each question into distinct domains. In the first stage, we asked the model to classify small sample of questions without any constraints; Based on its raw outputs, we manually merged semantically similar concepts and split those that were overly broad, yielding preliminary set of reference categories. In the second stage, we iteratively refined this taxonomy: the model classified each question according to the current reference set but was permitted to propose new categories for outliers, which we then reviewed and either incorporated into or merged with the existing categories. This cycle continued until no further category labels emerged. In the third stage, we constrained the model to assign every question exclusively to one of the finalized categories. Applying this pipeline to our full dataset yielded 10 distinct domains. Hyperparameter 7B 14B batch size batch size per device gradient accumulation steps learning rate epochs warmup ratio bf16 32 1 8 5.0 106 3 0.1 true 32 1 4 1.0 105 3 0.1 true Table 10: Detailed training hyperparameters."
        },
        {
            "title": "D Data Synthesis Process and Training",
            "content": "Details. The data synthesis pipeline refines responses generated by LLMs to ensure high-quality outputs, as detailed in Algorithm 1. Additionally, we provide some of the prompts we use in our experiments, as shown in Figures 78, which serve as reference. For model fine-tuning, the 7B model was trained using 4 NVIDIA A800 80G GPUs, while the 14B Here we present the specific descriptions of each category. Data Profiling: Systematically examining dataset characteristics (e.g., completeness, distributions, anomalies) to understand its fundamental structure. Data Retrieval: Extracting specific data subsets from storage systems using query-based methods. Data Aggregation: Combining data from multiple sources into unified formats for analytical purposes. Causal Analysis: Identifying cause-effect relationships between variables through controlled experiments or counterfactual reasoning. Exploratory Analysis: Preliminary investigation using visualization and summary statistics to formulate hypotheses. Predictive Analysis: Building models to forecast future outcomes based on historical patterns. Inferential Analysis: Drawing population-level conclusions from sample data via statistical hypothesis testing. Variance Analysis: Quantifying and attributing variability in data to specific influencing factors. Pattern Description: Detecting and formally characterizing recurring structures/relationships in data. Data Visualization: Creating graphical representations to communicate complex information efficiently. Figure 6: Data Analysis Task Categories. model utilized 8 NVIDIA A800 80G GPUs. The training configurations are summarized in Table 10. You are an expert data analyst tasked with solving complex analytical challenges. Your approach requires careful data investigation, rigorous statistical analysis, thorough validation, evidence-based decision making, and clear documentation. **Response Structure**: 1. Begin each step with \"## Thought: \" followed by **a thoughtful narrative explanation** of your reasoning and approach, using natural language to walk through your thought process. - Clearly explain your current thinking and analytical approach - Include why you chose this approach and what alternatives you considered - Explain how this step connects to your overall analysis strategy - Present your thinking as fluid, well-structured narrative 2. Follow this explanation with the corresponding Python code in '''python ''' tags: - Include code ONLY when it contributes to solving the task - Every code block MUST include print() statements with clear labels - Ensure code is focused and uses accurate syntax 3. Await \"## Observation:\" sections to review outputs before continuing analysis. 4. Conclude your response with \"## Final Answer:\", which should: - Include brief summary of the analysis process - Provide the final, precise solution backed by evidence Format example: ## Thought: [Description] ## Code: ''' python [code if needed] ''' ## Thought: [Description] ## Code: ''' python [code if needed] ''' ... ## Final Answer: [Your final answer] Figure 7: Prompt for Data Generation. Algorithm 1 Data Generation Pipeline Require: Question set Q: collection of natural language queries to be processed. File set F: set of files providing supplementary information. Ensure: Final responses R: structured set of high-quality responses, where each entry consists of formatted response with embedded reasoning summary and its corresponding code segments. 1: Initialize an empty output set 2: for each question do 3: Rq GenerateResponses(q, F) supplementary files Generate initial candidate responses using question and 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: Rq FilterCorrectResponses(Rq) if Rq = then continue end if for each response Rq do rclean ApplyRulesAndFormat(r) if IsLowComplexity(rclean) or IsMediumTurn(rclean) then Apply formatting rules to clean the response continue end if reasonchain ExtractReasonChain(rclean) reasonsummary Summarize(reasonchain) rfinal InsertBeforeCode(rclean, reasonsummary) Add rfinal to end for 17: 18: end for 19: return Return the set of finalized responses Please reconstruct the given reasoning content according to the following two steps: 1. Extract the main process and key steps from the reasoning content 2. Reconstruct the complete reasoning process based on these key steps <Requirements> Express the reconstructed reasoning process in more concise way, requiring: - Retain all effective reasoning information - Ensure logical coherence and consistency - Maintain the same meaning as the original reasoning content - Remove redundant and repetitive content - Present the entire reasoning process with clear structure in few short sentences - Keep the tone of the original thought process </Requirements> <Output_format> ## Reconstruction: [Only Final Reconstruction Results] </Output_format> Here is the reasoning content: {reasoning_content} Figure 8: Prompt for Summarization."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Zhejiang University",
        "Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph"
    ]
}