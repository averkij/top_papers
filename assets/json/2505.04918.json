{
    "paper_title": "Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction",
    "authors": [
        "Jiaqi Zheng",
        "Qing Ling",
        "Yerong Feng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although deep learning models have demonstrated remarkable potential in weather prediction, most of them overlook either the \\textbf{physics} of the underlying weather evolution or the \\textbf{topology} of the Earth's surface. In light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted And Topology-informed deep learning model for weather prediction. PASSAT attributes the weather evolution to two key factors: (i) the advection process that can be characterized by the advection equation and the Navier-Stokes equation; (ii) the Earth-atmosphere interaction that is difficult to both model and calculate. PASSAT also takes the topology of the Earth's surface into consideration, other than simply treating it as a plane. With these considerations, PASSAT numerically solves the advection equation and the Navier-Stokes equation on the spherical manifold, utilizes a spherical graph neural network to capture the Earth-atmosphere interaction, and generates the initial velocity fields that are critical to solving the advection equation from the same spherical graph neural network. In the $5.625^\\circ$-resolution ERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based weather prediction models and the operational numerical weather prediction model IFS T42. Code and checkpoint are available at https://github.com/Yumenomae/PASSAT_5p625."
        },
        {
            "title": "Start",
            "content": "Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction Jiaqi Zheng1, Qing Ling1, and Yerong Feng2 1Sun Yat-Sen University, 2Shenzhen Institute of Meteorological Innovation 2025-5-9 Although deep learning models have demonstrated remarkable potential in weather prediction, most of them overlook either the physics of the underlying weather evolution or the topology of the Earths surface. In light of these disadvantages, we develop PASSAT, novel Physics-ASSisted And Topology-informed deep learning model for weather prediction. PASSAT attributes the weather evolution to two key factors: (i) the advection process that can be characterized by the advection equation and the Navier-Stokes equation; (ii) the Earth-atmosphere interaction that is difficult to both model and calculate. PASSAT also takes the topology of the Earths surface into consideration, other than simply treating it as plane. With these considerations, PASSAT numerically solves the advection equation and the Navier-Stokes equation on the spherical manifold, utilizes spherical graph neural network to capture the Earth-atmosphere interaction, and generates the initial velocity fields that are critical to solving the advection equation from the same spherical graph neural network. In the 5.625-resolution ERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based weather prediction models and the operational numerical weather prediction model IFS T42. Code and checkpoint are available at https://github.com/Yumenomae/PASSAT_5p625. 1. Introduction Weather prediction is of paramount importance to social security and economic development, and has attracted extensive research efforts since the ancient time. Among the modern weather prediction methods, numerical weather prediction (NWP) is built upon differential equations that govern the weather evolution [Ran+07; BTB15]. These differential equations attribute the weather evolution to the advection process and the Earth-atmosphere interaction [Roo87; SMP90], as shown in Figure 1. The advection process is the evolution of weather variables (described by the advection equation) driven by the evolution of their velocity fields (described by the Navier-Stokes equation) [Tem84]. The Earth-atmosphere interaction encompasses other complex physical processes in the atmosphere, such as radiation, clouds, and subgrid turbulent motions. One particular challenge in NWP is that the Earth-atmosphere interaction is difficult to model and calculate, forming bottleneck of improving the accuracy of NWP [Hou+17; Koc+24]. Besides, the accuracy of NWP does not improve with the increasing amount of historical observations. On the other hand, data-driven methods that predict the weather based on the historical observations, especially deep learning models, have become very popular in recent years [Bou+24]. With the aid of high-quality and ever-accumulating data, state-of-the-art deep learning models have demonstrated great potentials and been integrated into the modern weather prediction systems [Kur+23; Bi+23; Lam+23]. In addition, deep learning-based models are able to remarkably shorten the time consumption in the prediction stage [Kur+23]. However, these models disregard either the physics of the weather evolution or the topology of the Earths surface. Thus, their predictions are often unreliable due to the lack of the physical constraints or suffer from the distortions caused by the topological structure [Sch+21; Xu+24a]. 5 2 0 2 8 ] . [ 1 8 1 9 4 0 . 5 0 5 2 : r Author email(s): zhengjq23@mail2.sysu.edu.cn, lingqing556@mail.sysu.edu.cn, yerong_feng@gd121.cn Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction Figure 1: Attributions of the weather evolution. 1.1. Enhancing Deep Learning with Physics Combining with the differential equations that characterize the weather evolution can enhance the precisions, efficiency and robustness of deep learning models, because the differential equations provide valuable prior knowledge [Xia+22]. Some works incorporate differential equations into losses during training deep learning models [Daw+21]. Nevertheless, tuning weights for the differential equations and computing stochastic gradients of the losses bring new challenges. Some other works use deep learning models to correct NWP models [Kwa+23; Xu+24b; Koc+24]. Though having high accuracies, these approaches are computationally demanding since they need to both solve large system of differential equations and train end-to-end neural networks. The closest to ours are [Zha+23; VHG24], in which neural networks are trained with the aid of differential equations. However, they both overlook the Navier-Stokes equation that drives the evolution of the velocity fields. Despite that these physics-assisted deep learning models are harder to train and slower in inference compared to the end-to-end deep learning methods, they significantly enhance the robustness of predictions and demonstrate remarkable potentials [Che+18]. 1.2. Taking Topology of Earth into Consideration The historical observations used during training most deep learning-based weather prediction models are often on planar latitude-longitude grids, other than on the spherical surface of the Earth. However, neglecting the topology of the Earths surface introduces remarkable distortions, as shown in Figure 2 [Coh+18; Mai+23]. For example, the points that are close to the poles turn to be denser on the spherical manifold than on the planar latitude-longitude grid. notable consequence is that one weather pattern appears differently on the sphere and the plane, such that capturing the weather pattern on the plan suffers from distortions. These distortions also affect the patches and convolution kernels, negatively impacting the deep learning models based on convolutional neural networks or transformers [CCG18]. In addition, the velocity fields defined on the planar are significantly distorted when increasing the latitude towards the poles, which will bring biases to the deep learning models that learn the velocity fields [Zha+23; VHG24]. 2 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction Figure 2: Distortions due to planar projection. (a) The spherical and planner representations of the global weather. (b) The same weather patterns on the sphere are distorted on the plane. (c) The convolutions on the sphere are distorted on the plane. 1.3. Contributions In this paper, we propose PASSAT, novel Physics-ASSisted And Topology-informed deep learning model for weather prediction. PASSAT attributes the weather evolution to the analytical advection process and the complex Earth-atmosphere interaction. In the advection process, the evolution of weather variables is driven by the evolution of their velocity fields, and the two are respectively described by the advection equation and the Navier-Stokes equation. PASSAT also takes the topology of the Earths surface into consideration. Therefore, PASSAT: (i) trains spherical graph neural network to estimate the Earth-atmosphere interaction; (ii) generates the initial velocity fields with the same spherical graph neural network; (iii) numerically solves the advection equation on the spherical manifold; (iv) updates the velocity fields through numerically solving the Navier-Stokes equation on the spherical manifold. Our contributions are as follows. PASSAT seamlessly integrates the historical observations, the physics of the weather evolution and the topology of the Earths surface, yielding novel physics-assisted and topology-informed deep learning model for weather prediction. Compared to the black-box deep learning models, PASSAT takes advantages of the physical constraints, characterized by the advection equation and the Navier-Stokes equation, and thus remarkably improves the quality of medium-term prediction. Compared to the NWP models, PASSAT avoids modeling and calculating the complex Earth-atmosphere interaction. PASSAT is also able to utilize the historical observations to improve the prediction accuracy. PASSAT solves the differential equations and trains the graph neural network on the spherical manifold other than on the planar latitude-longitude grid, and thus effectively avoids the distortions brought by the latter. We conduct experiments on the 5.625 ERA5 data set, demonstrating the competitive performance of PASSAT compared to the state-of-the-art deep learning models and the NWP model IFS T42. 2. Related Works Numerical weather prediction (NWP). NWP is fundamental physics-based method for weather prediction [Sch18], utilizing the underlying differential equations to predict how the weather should evolve over the time. For example, the operational Integrated Forecast System (IFS) consists of several NWP models with different spatial resolutions [Bou+24]. Despite of its widespread applications, modeling and calculating 3 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction the complex Earth-atmosphere interaction are challenging. In addition, solving the differential equations is sensitive to the initial conditions, and also computationally demanding [Koc+24]. Deep learning-based weather prediction. Different from NWP, deep learning models learn from the historical observations to predict the weather. Though time-consuming during training, deep learning models are rapid during prediction as they do not involve solving the differential equations. State-of-the-art deep learning-based weather prediction models include FourCastNet [Kur+23], SFNO [Bon+23], GraphCast [Lam+23], Graph-EFM [Kei22], AIFS [Lan+24], Pangu [Bi+23], Fengwu [Che+23a], Fuxi [Che+23b], Stormer [Ngu+24], etc. FourCastNet and SFNO are based on the Fourier neural operator [Li+21], GraphCast, Graph-EFM and AIFS are based on the graph neural network [Wu+21], whereas Pangu, Fengwu, Fuxi, and Stormer utilize the vision [Dos+21] and swin [Liu+21] transformers. Among these models, SFNO, GraphCast, Graph-EFM, and AIFS take the Earths topology into consideration. Nevertheless, all of them disregard the underlying physics information. Deep learning-based, physics-assisted weather prediction. Integrating the differential equations with deep learning models significantly improves the precisions, efficiency and robustness of the latter. Notable recent works along this line include ClimODE [VHG24] and NowcastNet [Zha+23]. Different to PASSAT, ClimODE characterizes the evolution of the weather variables with the continuity equation, other than the advection equation. On the other hand, ClimODE updates the velocity fields with neural network, other than the Navier-Stokes equation. NowcastNet focuses on regional precipitation nowcasting, while PASSAT focuses on global, multi-variable and medium-term weather prediction. Besides, PASSAT solves the differential equations and trains its graph neural network on spherical manifold, other than on the planar latitude-longitude grid used by ClimODE and NowcastNet, effectively avoiding the distortions. 3. Methods Considering the attributions of the weather evolution demonstrated in Figure 1, we accordingly build physics-assisted and topology-informed deep learning model for weather prediction, abbreviated as PASSAT. Given any initial time, PASSAT: (i) generates the initial velocity fields of the weather variables with the velocity branch of spherical graph neural network; and then autoregressively (ii) predicts the effects of the Earth-atmosphere interaction with the interaction branch of the spherical graph neural network; (iii) numerically solves the advection equation on the spherical manifold; (iv) numerically updates the velocity fields through solving the Navier-Stokes equation on the spherical manifold, aided by the initial velocity fields provided by (i). In the following, we discuss how PASSAT captures the evolution of the weather variables and their velocity fields, via integrating the two differential equations and the spherical graph neural network (see also Figure 3). We disregard the impact of vertical actions and focus on analyzing the advection and Navier-Stokes equations on the spherical manifold. All the analyses and approaches presented below can be readily extended to scenarios where the vertical actions are taken into account. We begin by introducing the spherical manifold in Section 3.1 and describing the evolution of the weather variables in Section 3.2. Then, we present the advection equation on the spherical manifold, the spherical graph neural network and the Navier-Stokes equation on the spherical manifold in Section 3.3, Section 3.4 and Section 3.5, respectively. We also introduce the time integration scheme in Section 3.6. Finally, we summarize in Section 3.7. 3.1. Spherical Manifold The historical observations used during training most deep learning-based weather prediction models are often on planar latitude-longitude grids, other than on the spherical surface of the Earth. Ignoring this topology information leads to remarkable distortions in both the neural networks and the differential equations [Coh+18; Mai+23]. In order to avoid such distortions, we project the weather variables from planar latitude-longitude grid onto the Earths surface. We assume the Earths surface to be an ideal unit sphere, with the radius of 1 4 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction Figure 3: Overview of PASSAT. unit length (6371km). We denote the unit sphere = {s R3 s2 = 1} as the Earths surface. Any spatial coordinate on the unit sphere corresponds to point (ğœ‘, ğœƒ) within the planar latitude-longitude grid, where ğœƒ is the latitude and ğœ‘ is the longitude. Thus, we use and s(ğœ‘, ğœƒ) interchangeably. Given any spatial coordinate s, eğœ‘(s) R3 and eğœƒ(s) R3 are two orthogonal unit vectors originated from and along the parallel and meridian directions, respectively. We denote as the spatial gradient on the unit sphere and as the inner product. 3.2. Evolution of Weather Variables Weather prediction depends on understanding the evolution of weather variables that we are interested in. Given any weather variable ğ‘¢, its evolution is characterized as follows. The weather variable ğ‘¢ is viewed as differentiable, real-valued function ğ‘¢ : ğ‘‡ R, within which ğ‘‡ is the time set and is the Earths surface. According to Figure 1, the evolution of weather variable ğ‘¢ is attributed to the advection process and the Earth-atmosphere interaction. To be specific, for any (ğ‘¡, s) ğ‘‡ S, we have: ğ‘¢(ğ‘¡ + ğ›¿ğ‘¡, + ğ›¿s) ğ‘¢(ğ‘¡, s) + ğ‘¡+ğ›¿ğ‘¡ ğ‘¡ â„(ğœ, + ğ›¿s)ğ‘‘ğœ, (1) where ğ›¿ğ‘¡ > 0 is small lead time, ğ›¿s = ğ›¿ğ‘¡ v(ğ‘¡, s), is the velocity fields of ğ‘¢, and â„ is the tendency of ğ‘¢ due 5 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction to the Earth-atmosphere interaction. Taking the first-order Taylors approximation of (1) yields: ğ‘¢ ğ‘¡ (ğ‘¡, s) v(ğ‘¡, s) sğ‘¢(ğ‘¡, s) + â„(ğ‘¡, s) = ( ğ‘¢ ğ‘¡ (ğ‘¡, s))advection + ( ğ‘¢ ğ‘¡ (ğ‘¡, s))interaction. (2) According to (2), the total tendency of weather variable ğ‘¢ can be decomposed into two part: (i) sğ‘¢, the tendency due to the advection process and (ii) â„, the tendency due to the Earth-atmosphere interaction. Once the total tendency ğ‘¢ ğ‘¡ (ğ‘¡, s) is known, we can predict the value of ğ‘¢ at any future time ğ‘¡ + ğ‘¡ via using proper numerical methods to solve: ğ‘¢(ğ‘¡ + ğ‘¡, s) = ğ‘¢(ğ‘¡, s) + ğ‘¡+Î”ğ‘¡ ğ‘¡ ğ‘¢ ğ‘¡ (ğœ, s)ğ‘‘ğœ. (3) In PASSAT, we use Eulers method for this time integration. Therefore, the key of weather prediction is to compute the tendencies of the advection process and the Earth-atmosphere interaction. Though the tendency of the advection process can be numerically estimated by solving the advection equation on the spherical manifold, the tendency of the Earth-atmosphere interaction is difficult to model and calculate so that we resort to spherical graph neural network. We introduces them one by one in the following. 3.3. Advection Equation on Spherical Manifold The advection process is the evolution of the weather variables driven by their velocity fields. Given any weather variable ğ‘¢, its velocity fields : ğ‘‡ R3 are differentiable functions of time and spatial coordinate. As we disregard vertical actions, the velocity fields can be express by v(ğ‘¡, s) = ğ‘£ğœƒ(ğ‘¡, s)eğœƒ(s) + ğ‘£ğœ‘(ğ‘¡, s)eğœ‘(s), where ğ‘£ğœƒ and ğ‘£ğœ‘ are the velocities of ğ‘¢ along the meridian and parallel directions, respectively. With particular note, at any initial time ğ‘¡ and spatial coordinate s, ğ‘¢(ğ‘¡, s) is known but v(ğ‘¡, s) is to be calculated. As discussed in Section 3.2, the tendency of ğ‘¢ due to the advection process is given by solving the advection equation [Cha22], as: ( ğ‘¢ ğ‘¡ (ğ‘¡, s))advection+ v(ğ‘¡, s) sğ‘¢(ğ‘¡, s) advective derivative = 0. (4) Once the advective derivative is known, the tendency of ğ‘¢ due to the advection process is known too. On the spherical manifold and the planar latitude-longitude grid, the advective derivative has different forms, and the latter brings distortions in weather prediction, as discussed in the following. Given spatial coordinate = s(ğœ‘, ğœƒ) S, on the spherical manifold, the advective derivative is in the form of [LTW92]: v(ğ‘¡, s) sğ‘¢(ğ‘¡, s) = ğ‘£ğœƒ(ğ‘¡, s) ğ‘¢ ğœƒ For ğ‘£ğœƒ(ğ‘¡, s) and ğ‘£ğœ‘(ğ‘¡, s), PASSAT will estimate their initial values utilizing the velocity branch of spherical graph neural network, and calculate their future values through solving the Navier-Stokes equation. The differentials ğ‘¢ ğœ‘ (ğ‘¡, s) can be estimated using the difference quotients of ğ‘¢ on the planar latitudelongitude grid. ğœƒ (ğ‘¡, s) and ğ‘¢ ğ‘£ğœ‘(ğ‘¡, s) cos ğœƒ (ğ‘¡, s) + ğ‘¢ ğœ‘ (ğ‘¡, s). (5) In contrast, on the planar latitude-longitude grid, the advective derivative is in the form of: v(ğ‘¡, s) sğ‘¢(ğ‘¡, s) = ğ‘£ ğœƒ(ğ‘¡, s) ğ‘¢ ğœƒ (ğ‘¡, s) + ğ‘£ ğœ‘(ğ‘¡, s) ğ‘¢ ğœ‘ (ğ‘¡, s), (6) ğœƒ(ğ‘¡, s) and ğ‘£ where ğ‘£ latitude-longitude planar grid, not on the spherical manifold. We have ğ‘£ ğœ‘(ğ‘¡, s) are respectively the velocities along the meridian and parallel directions, but on the ğœ‘(ğ‘¡, s) = ğ‘£ğœ‘(ğ‘¡,s) . ğœƒ(ğ‘¡, s) = ğ‘£ğœƒ(ğ‘¡, s) and ğ‘£ cos ğœƒ 6 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction Figure 4: Overview of PASSATs graph neural network. TOP: The backbone model and two branches. BOTTOM: The basic block. ğœ‘(ğ‘¡, s) with neural networks. However, we can observe that fixing the value of ğ‘£ğœ‘(ğ‘¡, s), ğ‘£ ClimODE and NowcastNet both calculate the the advective derivative according to (6), through estimating ğœ‘(ğ‘¡, s) is ğœƒ(ğ‘¡, s) and ğ‘£ ğ‘£ not spatial-invariant it is large when is close to the poles and small when is close to the equator. Such distortions will affect the pattern recognition of the neural networks. In contrast, PASSAT takes advantages of the spherical manifold, and thus avoids the distortions. 3.4. Spherical Graph Neural Network ğ‘¡ (ğ‘¡, s))advection, the tendency of ğ‘¢ due to the advection process, we need to As discussed above, to calculate ( ğ‘¢ estimate the initial velocity fields of v(ğ‘¡, s). On the other hand, we also need to estimate ( ğ‘¢ ğ‘¡ (ğ‘¡, s))interaction, the tendency of ğ‘¢ due to the Earth-atmosphere interaction. We train spherical graph neural network to estimate these values. The spherical graph neural network consists of backbone model along with two branches: the interaction branch that estimates ( ğ‘¢ ğ‘¡ (ğ‘¡, s))interaction and the velocity branch that estimates the initial velocity fields of v(ğ‘¡, s); see Figure 4. Their basic block has two components: node-edge connection and node-node aggregation. The node-edge connection component enables efficient message passing between nodes and edges [Zho+20], while the node-node aggregation component performs graph convolution among nodes [KW16]. The spherical graph neural network incorporates the topology information from the spherical manifold, and thus avoids the distortions caused by the planar latitude-longitude grid. For more details, please refer to the supplementary material. 7 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction 3.5. Navier-Stokes Equation on Spherical Manifold To calculate the future ( ğ‘¢ ğ‘¡ (ğ‘¡, s))advection, we still need to estimate the future velocity fields of v(ğ‘¡, s), for which we resort to the Navier-Stokes equation [LTW92]. On the spherical manifold, the velocity fields of each weather variable v(ğ‘¡, s) = ğ‘£ğœƒ(ğ‘¡, s)eğœƒ(s) + ğ‘£ğœ‘(ğ‘¡, s)eğœ‘(s) satisfy: 1 ğ‘ ğœŒ ğœƒ pressure gradient force ğœ‡ cos2 ğœƒ viscous friction ğœ‘ tan ğœƒ curvature Coriolis force + 2ğœ”ğ‘£ğœ‘ sin ğœƒ ğ‘£ğœ‘ cos ğœƒ advection ğ‘£ğœƒ ğœƒ ğ‘£ğœƒ ğ‘¡ ğ‘£ğœƒ ğœ‘ + (ğ‘£ğœƒ + ğ‘£2 = 0, ğ‘£ğœƒ + + + (7) ) ğ‘£ğœ‘ ğ‘¡ + (ğ‘£ğœƒ ğ‘£ğœ‘ ğœƒ + ğ‘£ğœ‘ cos ğœƒ ğ‘£ğœ‘ ğœ‘ ) advection ğ‘£ğœ‘ğ‘£ğœƒ tan ğœƒ curvature + 1 ğœŒ cos ğœƒ pressure gradient force ğ‘ ğœ‘ 2ğœ”ğ‘£ğœƒ sin ğœƒ Coriolis force + ğœ‡ cos2 ğœƒ viscous friction ğ‘£ğœ‘ = 0, (8) We omit the pair (ğ‘¡, s) for notational simplicity. In the Navier-Stokes equation, ğœŒ(ğ‘¡, s) is the atmospheric density, ğœ” = 0.2618 (radian/hour) is the Earths rotation speed, ğ‘(ğ‘¡, s) is the atmospheric pressure, and ğœ‡ is constant related to the Reynolds constant. For computational efficiency, we simplify the Navier-Stokes equation by retaining only the viscous friction in the Laplacian. The Navier-Stokes equation governs the evolution of both ğ‘£ğœƒ(ğ‘¡, s) and ğ‘£ğœ‘(ğ‘¡, s). After calculating ğ‘£ğœƒ(ğ‘¡,s) ğ‘£ğœ‘(ğ‘¡,s) ğ‘¡ as in (3). and from the Navier-Stokes equation, we apply numerical methods to predict ğ‘£ğœƒ(ğ‘¡ + ğ‘¡, s) and ğ‘£ğœ‘(ğ‘¡ + ğ‘¡, s) ğ‘¡ 3.6. Time Integration Scheme ğ‘¡ (ğ‘¡, s))interaction (from the spherical graph neural network) and Up to now, at time ğ‘¡, we have known ( ğ‘¢ ğ‘¡ (ğ‘¡, s))advection (from the advection equation) with the aid of v(ğ‘¡, s) (from both the spherical graph neural ( ğ‘¢ network and the Navier-Stokes equation). Therefore, we can the predict the value of ğ‘¢ at future time ğ‘¡ + ğ‘¡ according to (3). However, the numerical methods to solve (3) are sensitive to the integration step size. As we will see, in our weather prediction, ğ‘¡ ranges from 6 to 144 hours, at the temporal resolution of 6 hours. Hence, choosing proper integration step size is critical to medium-term or long-term prediction. With the above consideration, in PASSAT, we set the integration step size as 0.2 hours. However, such small integration step size requires the interaction branch of PASSAT to frequently estimate ( ğ‘¢ ğ‘¡ (ğ‘¡, s))interaction see (2) and (3) resulting in excessive back propagation during the training. To address the issue of memory-intensive training, we estimate ( ğ‘¢ ğ‘¡ (ğ‘¡, s))interaction only once every hour and keep it unchanged within the next hour. 3.7. Summary of PASSAT We summarize PASSAT in Algorithm 1. For simplicity, we omit the spatial coordinate s, using ğ‘¢ğ‘¡ to denote any weather variable at time ğ‘¡ and vğ‘¡ to denote its velocity fields. We also use uğ‘¡ to denote all weather variables at time ğ‘¡. We use ğ‘“vel and ğ‘“int to denote the velocity and interaction branches of the spherical graph neural network, respectively. We use iğ‘¡ to denote the estimated effect of the Earth-atmosphere interaction for time ğ‘¡ and spatial coordinate s. Within the Navier-Stokes equation, 1 are unknown. We ğœŒ replace them with the gradients of geopotential ğ‘§ğ‘¡ at the 500hPa pressure level, converting the units from m2second2 to (6731km)2hour2. To ensure stability, the initial velocity fields estimated from the velocity branch, with the unit of (6731km)/hour, are projected onto [0.005, 0.005]. and 1 ğœŒ ğ‘ğ‘¡ ğœƒ ğ‘ğ‘¡ ğœ‘ 4. Experiments Data & Tasks. We conduct the experiments on the European Centre for Medium-Range Weather Forecasts Reanalysis V5 (ERA5) 5.625-resolution data set, spanning from 1979 to 2018 and provided by WeatherBench 8 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction Algorithm 1: PASSAT: Predicting any weather variable ğ‘¢ for ğœ = ğ‘¡ + 0.2, ğ‘¡ + 0.4, , ğ‘¡ + ğ‘¡ at time ğ‘¡ Input: ğ‘¢ğ‘¡ Output: ğ‘¢ğœ and vğœ = ğ‘£ğœ ğœƒ eğœƒ + ğ‘£ğœ ğœ‘eğœ‘ for ğ‘§ = ğ‘¡, ğ‘¡ + 1, , ğ‘¡ + ğ‘¡ 1 do if ğ‘§ = ğ‘¡ then Initial velocity fields: vğ‘¡ = ğ‘“vel(uğ‘¡) end Earth-atmosphere interaction: iğ‘§ = ğ‘“int(uğ‘§) for ğœ = ğ‘§, ğ‘§ + 0.2, ğ‘§ + 0.4, ğ‘§ + 0.6, ğ‘§ + 0.8 do Compute tendencies of ğ‘¢, ğ‘£ğœƒ, ğ‘£ğœ‘ ğ‘¢ğœ ğ‘¢ğœ ğœ = ğ‘£ğœ ğœƒ ğ‘£ğœ ğ‘£ğœ ğœ = ğ‘£ğœ ğœƒ ğœƒ ğœƒ ğ‘£ğœ ğ‘£ğœ ğœ = ğ‘£ğœ ğœ‘ ğœ‘ ğœƒ Update ğ‘¢ğœ , ğ‘£ğœ ğ‘£ğœ ğœ‘ cos ğœƒ ğ‘£ğœ ğœ‘ cos ğœƒ ğ‘£ğœ ğœ‘ cos ğœƒ ğœƒ , ğ‘£ğœ ğœ‘ ğ‘¢ğœ ğœ‘ + iğ‘§ ğ‘£ğœ ğœ‘)2 tan ğœƒ ğ‘§ğœ ğœ‘ (ğ‘£ğœ ğœƒ ğ‘£ğœ ğœƒ tan ğœƒ 1 ğœ‘ğ‘£ğœ ğœ‘ + ğ‘£ğœ ğœ‘ cos ğœƒ ğœƒ ğœƒ ğœƒ ğ‘¢ğœ +0.2 = ğ‘¢ğœ + 0.2 ğ‘¢ğœ ğœ ğœƒ + 0.2 ğ‘£ğœ ğ‘£ğœ +0.2 ğœƒ ğœƒ ğœ ğ‘£ğœ ğ‘£ğœ +0.2 ğœ‘ ğœ‘ + 0.2 ğœ‘ ğœ = ğ‘£ğœ = ğ‘£ğœ end end ğœƒ 2ğœ”ğ‘£ğœ ğ‘§ğœ ğœ‘ + 2ğœ”ğ‘£ğœ ğœ‘ sin ğœƒ ğœ‡ ğ‘£ğœ ğœƒ cos2 ğœƒ ğœƒ sin ğœƒ ğœ‡ ğ‘£ğœ ğœ‘ cos2 ğœƒ [Her+20; Ras+20]. The data samples from 1979 to 2015 are used in the training set, 2016 in the validation set, as well as 2017 and 2018 in the test set. The interested weather variables are temperature at 2m height (t2m), temperature at 850hPa pressure level (t850), geopotential at 500hPa pressure level (z500), component of wind at 10m height (u10), and component of wind at 10m height (v10). We use PASSAT and the baseline models to predict these weather variables, at temporal resolution of 6 hours (6am, 12am, 6pm, and 12pm of each day) and lasting for 24 steps (144 hours). Performance metrics include root mean square error (RMSE) and anomaly correlation coefficient (ACC). Baseline deep learning models. We compare PASSAT with the following baseline deep learning models: (i) ClimODE [VHG24]; (ii) FourCastNet [Kur+23]; (iii) Pangu [Bi+23]; (iv) GraphCast [Lam+23]; (v) SFNO [Bon+23]. For fair comparisons, we unify the number of parameters of all models to the same magnitude (around 1.15 million) and train these baseline deep learning models from scratch according to their open-source codes and NVIDIAs Modulus. We do not compare with NowcastNet, Graph-EFM, AIFS, Stormer, Fengwu, and Fuxi. Among them, NowcastNet integrates the differential equations with the deep learning model, Graph-EFM and AIFS both take the Earths topology into consideration, while Stormer, Fengwu and Fuxi do not utilize physics and topology information. However, NowcastNet exclusively focuses on regional precipitation nowcasting, while PASSAT focuses on global, multi-variable and medium-term weather prediction. Graph-EFM and AIFS adopt hierarchical grid-mesh graph structures, similar to GraphCast. Stormer, Fengwu and Fuxi utilize attentionbased structures similar to Pangu, and their focus is on improving long-term predictions via enhancing the training and inference strategies. Baseline NWP models. PASSAT is also compared with the following operational NWP models: IFS T42 and IFS T63 [Ras+20]. IFS T42 and IFS T63 are the Integrated Forecast System (IFS) model run at two different resolutions, 2.8 and 1.9 respectively. We can observe that they are both finer than the 5.625 resolution of PASSAT, at the cost of being computationally demanding in solving large systems of differential equations. https://github.com/NVIDIA/modulus/tree/main/modulus/models Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction Figure 5: Comparison between PASSAT and other models. The x-axis represents the lead time in hours. Smaller RMSE and larger ACC values indicate better performance. Note that some results of IFS T42 exceed the bounds. Results. As demonstrated in Figure 5, PASSAT outperforms the other deep learning models in all weather variables across different lead times. The closest with PASSAT is GraphCast, which takes the topology of the Earths surface into consideration. However, GraphCast ignores the physics of the weather evolution, and thus has to use more complex graph structure than PASSAT (twice in terms of the number of nodes and three times in terms of the number of edges). ClimODE, despite of its physics-assisted structure, does not perform well. This phenomenon could be attributed to the following reasons: (i) ClimODE characterizes the evolution of the weather variables with the continuity equation, without considering the Earth-atmosphere interaction; (ii) ClimODE updates the velocity fields with neural network, other than the Navier-Stokes equation; (iii) ClimODE ignores the topology information, and thus suffers from the distortions. In contrast, PASSAT benefits from both the physics information and the topology information, allowing it to achieve remarkably better performance. Figure 6: Comparison between PASSAT and the three variants. The x-axis represents the lead time in hours. Smaller RMSE and larger ACC values indicate better performance. The RMSEs and ACCs of IFS T42 and IFS T63 are from [Ras+20], only including t2m, t850 and z500 for the lead times of 72 and 120 hours. Observe that PASSAT outperforms IFS T42, pure physical model solved at finer resolution (2.8). Improving the resolution of the physical model from 2.8 to 1.9, IFS T63 surpasses PASSAT and the other deep learning models, nevertheless at the cost of high computational complexity. Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction 5. Ablation Studies We conduct ablation studies to evaluate the effectiveness of the physics and topology information used in PASSAT. We compare with three models: (i) PASSAT (without topology), which constructs the graph neural network on the latitude-longitude planar grid, other than on the spherical manifold; (ii) PASSAT (without physics), which uses the spherical graph neural network to predict the weather in an end-to-end manner; (iii) PASSAT (without topology and physics), which uses the planar graph neural network to predict the weather in an end-to-end manner. The results are depicted in Figure 6. PASSAT significantly outperforms the three variants across all weather variables, highlighting the importance of incorporating both physics and topology information. An interesting observation from our ablation studies is that different variables benefit from different sources of information. For t2m, the performance gains of respectively using the topology information and the physics information are almost the same. On the other hand, the rest variables of t850, z500, u10, and v10 benefit more from the topology information than from the physics information. We do not evaluate the individual effects of the advection equation and the Naiver-Stokes equation, the two parts of the physics information. First, without the advection equation, it is unnecessary to update the velocity fields with the Naiver-Stokes equation. Second, estimating the future velocity fields with the spherical graph neural network, other than the Naiver-Stokes equation, shall require frequent calls of the velocity branch, leading to excessive back propagation and thus unaffordable memory consumption during training. 6. Conclusions and Future Works In this paper, we propose PASSAT, novel physics-assisted and topology-informed deep learning model for weather prediction. PASSAT seamlessly integrates the advection equation and the Navier-Stokes equation that govern the evolution of the weather variables and their velocity fields, with graph neural network that estimates the complex Earth-atmosphere interaction and the initial velocity fields. PASSAT also takes the topology of the Earths surface into consideration, during solving the equations and training the graph neural network. In the 5.625-resolution ERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based weather prediction models and the operational numerical weather prediction model IFS T42. Our ablation studies demonstrate that both the physics information and the topology information are essential to the performance gain. As future works, we will extend PASSAT in the following aspects: (i) Enhance PASSAT by incorporating more weather variables; (ii) Refine PASSAT through training over data set with finer resolution; (iii) Incorporate new time integration scheme that is more efficient than Eulers method, during both training and prediction. We expect that PASSAT is able to motivate more research efforts in combining physics, topology and historical observations for weather prediction. 7. Ethical Statement There are no ethical issues. 11 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction"
        },
        {
            "title": "References",
            "content": "[Bi+23] [Bon+23] [Bou+24] [BTB15] [CCG18] [Cha22] [Che+18] Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Accurate medium-range global weather forecasting with 3D neural networks. In: Nature 619.7970 (2023), pp. 533538. Boris Bonev, Thorsten Kurth, Christian Hundt, Jaideep Pathak, Maximilian Baust, Karthik Kashinath, and Anima Anandkumar. Spherical Fourier Neural Operators: Learning Stable Dynamics on the Sphere. In: arXiv preprint arXiv:2306.03838 (2023). Zied Ben BouallÃ¨gue, Mariana C. A. Clare, Linus Magnusson, Estibaliz GascÃ³n, Michael MaierGerber, Martin JanouÅ¡ek, Mark Rodwell, Florian Pinault, Jesper S. Dramsch, Simon T. K. Lang, Baudouin Raoult, Florence Rabier, Matthieu Chevallier, Irina Sandu, Peter Dueben, Matthew Chantry, and Florian Pappenberger. The Rise of Data-Driven Weather Forecasting: First Statistical Assessment of Machine LearningBased Weather Forecasts in an Operational-Like Context. In: Bulletin of the American Meteorological Society 105.6 (2024), pp. 864883. Peter Bauer, Alan Thorpe, and Gilbert Brunet. The quiet revolution of numerical weather prediction. In: Nature 525.7567 (2015), pp. 4755. Benjamin Coors, Alexandru Paul Condurache, and Andreas Geiger. Spherenet: Learning spherical representations for detection and classification in omnidirectional images. In: European Conference on Computer Vision (2018). Anantharaman Chandrasekar. Linear Advection Equation. In: Numerical Methods for Atmospheric and Oceanic Sciences. Cambridge University Press, 2022. Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. In: Advances in Neural Information Processing Systems (2018). [Che+23a] Kang Chen, Tao Han, Junchao Gong, Lei Bai, Fenghua Ling, Jing-Jia Luo, Xi Chen, Leiming Ma, Tianning Zhang, Rui Su, et al. Fengwu: Pushing the skillful global medium-range weather forecast beyond 10 days lead. In: arXiv preprint arXiv:2304.02948 (2023). [Che+23b] Lei Chen, Xiaohui Zhong, Feng Zhang, Yuan Cheng, Yinghui Xu, Yuan Qi, and Hao Li. FuXi: cascade machine learning forecasting system for 15-day global weather forecast. In: Climate and Atmospheric Science 6.1 (2023), p. 190. Taco S. Cohen, Mario Geiger, Jonas KÃ¶hler, and Max Welling. Spherical CNNs. In: International Conference on Learning Representations (2018). [Coh+18] [Dos+21] [Daw+21] Arka Daw, Anuj Karpatne, William Watkins, Jordan Read, and Vipin Kumar. Physics-guided Neural Networks (PGNN): An Application in Lake Temperature Modeling. In: arXiv preprint arXiv:1710.11431 (2021). Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In: arXiv preprint arXiv:2010.11929 (2021). Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, AndrÃ¡s HorÃ¡nyi, JoaquÃ­n MuÃ±ozSabater, Julien Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, et al. The ERA5 reanalysis. In: Quarterly Journal of the Royal Meteorological Society 146.730 (2020), pp. 1999 2049. [Her+20] [Hou+17] FrÃ©dÃ©ric Hourdin, Thorsten Mauritsen, Andrew Gettelman, Jean-Christophe Golaz, Venkatramani Balaji, Qingyun Duan, Doris Folini, Duoying Ji, Daniel Klocke, Yun Qian, Florian Rauser, Catherine Rio, Lorenzo Tomassini, Masahiro Watanabe, and Daniel Williamson. The Art and Science of Climate Model Tuning. In: Bulletin of the American Meteorological Society 98.3 (2017), pp. 589602. Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction [Kei22] Ryan Keisler. Forecasting Global Weather with Graph Neural Networks. In: arXiv preprint arXiv:2202.07575 (2022). [Kur+23] [Koc+24] Dmitrii Kochkov, Janni Yuval, Ian Langmore, Peter Norgaard, Jamie Smith, Griffin Mooers, Milan KlÃ¶wer, James Lottes, Stephan Rasp, Peter DÃ¼ben, Sam Hatfield, Peter Battaglia, Alvaro Sanchez-Gonzalez, Matthew Willson, Michael P. Brenner, and Stephan Hoyer. Neural general circulation models for weather and climate. In: Nature 632.8027 (2024), pp. 10601066. Thorsten Kurth, Shashank Subramanian, Peter Harrington, Jaideep Pathak, Morteza Mardani, David Hall, Andrea Miele, Karthik Kashinath, and Anima Anandkumar. FourCastNet: Accelerating Global High-Resolution Weather Forecasting Using Adaptive Fourier Neural Operators. In: The Platform for Advanced Scientific Computing Conference (2023). Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In: arXiv preprint arXiv:1609.02907 (2016). [KW16] [Lan+24] [Kwa+23] Anna Kwa, Spencer K. Clark, Brian Henn, Noah D. Brenowitz, Jeremy McGibbon, Oliver Watt-Meyer, W. Andre Perkins, Lucas Harris, and Christopher S. Bretherton. Machine-Learned Climate Model Corrections From Global Storm-Resolving Model: Performance Across the Annual Cycle. In: Journal of Advances in Modeling Earth Systems 15.5 (2023). [Lam+23] Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, Alexander Merose, Stephan Hoyer, George Holland, Oriol Vinyals, Jacklynn Stott, Alexander Pritzel, Shakir Mohamed, and Peter Battaglia. Learning skillful medium-range global weather forecasting. In: Science 382.6677 (2023), pp. 14161421. Simon Lang, Mihai Alexe, Matthew Chantry, Jesper Dramsch, Florian Pinault, Baudouin Raoult, Mariana C. A. Clare, Christian Lessig, Michael Maier-Gerber, Linus Magnusson, Zied Ben BouallÃ¨gue, Ana Prieto Nemesio, Peter D. Dueben, Andrew Brown, Florian Pappenberger, and Florence Rabier. AIFS ECMWFs data-driven forecasting system. In: arXiv preprint arXiv:2406.01465 (2024). Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier Neural Operator for Parametric Partial Differential Equations. In: arXiv preprint arXiv:2010.08895 (2021). Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. In: arXiv preprint arXiv:2103.14030 (2021). Jacques-Louis Lions, Roger Temam, and Shouhong Wang. New formulations of the primitive equations of atmosphere and applications. In: Nonlinearity 5.2 (1992), p. 237. [LTW92] [Liu+21] [Li+21] [Mai+23] Gengchen Mai, Yao Xuan, Wenyun Zuo, Yutong He, Jiaming Song, Stefano Ermon, Krzysztof Janowicz, and Ni Lao. Sphere2Vec: general-purpose location representation learning over spherical surface for large-scale geospatial predictions. In: ISPRS Journal of Photogrammetry and Remote Sensing 202 (2023), pp. 439462. Tung Nguyen, Rohan Shah, Hritik Bansal, Troy Arcomano, Romit Maulik, Veerabhadra Kotamarthi, Ian Foster, Sandeep Madireddy, and Aditya Grover. Scaling transformer neural networks for skillful and reliable medium-range weather forecasting. In: arXiv preprint arXiv:2312.03876 (2024). [Ngu+24] [Ran+07] David Randall, Richard Wood, Sandrine Bony, Robert Colman, Thierry Fichefet, John Fyfe, Vladimir Kattsov, Andrew Pitman, Jagadish Shukla, Jayaraman Srinivasan, et al. Climate Change 2007: The Physical Science Basis. In: Cambridge University Press, 2007. [Ras+20] Stephan Rasp, Peter Dueben, Sebastian Scher, Jonathan Weyn, Soukayna Mouatadid, and Nils Thuerey. WeatherBench: benchmark data set for data-driven weather forecasting. In: Journal of Advances in Modeling Earth Systems 12.11 (2020). 13 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction [Roo87] [Sch18] Richard Rood. Numerical advection algorithms and their role in atmospheric transport and chemistry models. In: Reviews of Geophysics 25.1 (1987), pp. 71100. Sebastian Scher. Toward Data-Driven Weather and Climate Forecasting: Approximating Simple General Circulation Model With Deep Learning. In: Geophysical Research Letters 45.22 (2018), pp. 12,61612,622. [SMP90] [Sch+21] Martin Schultz, Clara Betancourt, Bing Gong, Felix Kleinert, Michael Langguth, Lukas Hubert Leufen, Amirpasha Mozaffari, and Scarlet Stadtler. Can deep learning beat numerical weather prediction? In: Philosophical Transactions of the Royal Society 379.2194 (2021), p. 20200097. Stuart D. Smith, Robin D. Muench, and Carol H. Pease. Polynyas and leads: An overview of physical processes and environment. In: Journal of Geophysical Research: Oceans 95.6 (1990), pp. 94619479. eprint: https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/ JC095iC06p09461. Roger Temam. NavierStokes equations: theory and numerical analysis. American Mathematical Society, 1984. [Tem84] [VHG24] [Wu+21] [Xia+22] Yogesh Verma, Markus Heinonen, and Vikas Garg. ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs. In: International Conference on Learning Representations (2024). Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. Comprehensive Survey on Graph Neural Networks. In: IEEE Transactions on Neural Networks and Learning Systems 32.1 (2021), pp. 424. Zixue Xiang, Wei Peng, Xu Liu, and Wen Yao. Self-adaptive loss balanced physics-informed neural networks. In: Neurocomputing 496 (2022), pp. 1134. [Xu+24a] Hongxiong Xu, Yang Zhao, Dajun Zhao, Yihong Duan, and Xiangde Xu. Improvement of disastrous extreme precipitation forecasting in North China by Pangu-weather AI-driven regional WRF model. In: Environmental Research Letters 19.5 (2024), p. 054051. [Xu+24b] Wanghan Xu, Fenghua Ling, Wenlong Zhang, Tao Han, Hao Chen, Wanli Ouyang, and Lei Bai. Generalizing Weather Forecast to Fine-grained Temporal Scales via Physics-AI Hybrid Modeling. In: arXiv preprint arXiv:2405.13796 (2024). Yuchen Zhang, Mingsheng Long, Kaiyuan Chen, Lanxiang Xing, Ronghua Jin, Michael Jordan, and Jianmin Wang. Skilful nowcasting of extreme precipitation with NowcastNet. In: Nature 619.7970 (2023), pp. 526532. [Zha+23] [Zho+20] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: review of methods and applications. In: AI Open 1 (2020), pp. 5781. 14 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction A. Data A.1. Data Set We adopt the European Centre for Medium-Range Weather Forecasts Reanalysis V5 (ERA5) 5.625-resolution data set from 1979 to 2018, provided by WeatherBench. Table 1 summarizes the weather variables in the data set. For more details, readers are referred to ERA5. Table 1: Weather variables in the data set. Long name geopotential temperature 2m_temperature 10m_u_component_of_wind 10m_v_component_of_wind land_binary_mask t850 t2m u10 v10 lsm Short name Description Unit Levels Proportional to the height of pressure level m2s2 500hPa Temperature Temperature at 2m height above surface Wind in longitude-direction at 10m height Wind in latitude-direction at 10m height Land-sea binary mask ms1 ms1 (0/1) 850hPa - - - - - orography orography Height of surface A.2. Data Preprocessing Normalization. As the weather variables have diverse magnitudes, we use their means and standard deviations in 2006 to normalize the entire data set. Mapping the latitude-longitude grid to the sphere. In PASSAT, we need to project latitude-longitude point (ğœƒ, ğœ‘) onto the unit sphere, as: s(ğœƒ, ğœ‘) = cos ğœƒ cos ğœ‘ cos ğœƒ sin ğœ‘ sin ğœƒ S. (9) B. PASSAT B.1. Structure In Table 2, we illustrate the structures of PASSAT and its variants. For PASSAT without physics, we no longer need the velocity branch, and are able to merge the interaction branch with the backbone. Table 2: The structure of PASSAT and its variants. The number in the brackets is the output dimension. PASSAT w/o physics w/o topology w/o topology&physics Num Nodes Num Edges 2048 9152 Input Embedding 1 MLP (48) Backbone 2 Basic Block (48) Velocity Branch Physics Branch Parameters 1 Basic Block (48) [2 Basic Block (48), 1 Basic Block (24)] 1.15 (M) 2048 9152 2048 4000 4000 1 MLP (45) [2 Basic Block (45), 1 Basic Block (90)] - - 1.15 (M) 1 MLP (48) 2 Basic Block (48) 1 Basic Block (48) [2 Basic Block (48), 1 Basic Block (24)] 1.15(M) 1 MLP (45) [2 Basic Block (45), 1 Basic Block (90)] - - 1.15(M) 1 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction Graph. The spherical graph neural network of PASSAT is denoted as (ğ’© , â„°, ğ´), where ğ’© is the node set, â„° is the edge set and ğ´ is the adjacency matrix. Each node of PASSAT is represented by spatial coordinate. Corresponding to the 5.625-resolution data set, there are 2048 nodes, represented as: (n0, n1, , n63, , n2047) = (s(ğœ‘0, ğœƒ0), s(ğœ‘1, ğœƒ0), , s(ğœ‘63, ğœƒ0), , s(ğœ‘63, ğœƒ31)). The (ğ‘–, ğ‘—)-th element of the original adjacency matrix ğ´ is given by the Haversine formula and Gaussian kernel: (ğ´)ğ‘–ğ‘— = exp(200 Haversine(nğ‘–, nğ‘—)2). To improve the computational efficiency, we prune ğ´ by setting its elements to zero if they are below given threshold. In the pruned adjacency matrix, the sparsest row has 5 non-zeros. We also re-normalize ğ´ to avoid exploding/vanishing gradients, as: (10) ğ´ ğ· 1 where ğ· is diagonal degree matrix, with (ğ·)ğ‘–ğ‘– = We will use ğ´ for feature extraction. In addition, we define the edge set â„° by ğ´. If the (ğ‘–, ğ‘—)-th element of ğ´ is non-zero, we say that nodes nğ‘– and nğ‘— are neighbors, and (nğ‘–, nğ‘—) â„°. Initial States of Nodes and Edges. We utilize to stack all weather variables. The states of all weather variables at initial time determine the initial states of nodes and edges. For node nğ‘–, its initial state is: 2 ğ´ğ· 1 2 , ğ‘—(ğ´)ğ‘–ğ‘—. (11) â„node ğ‘– = u0(nğ‘–), (12) where u0(nğ‘–) denote the weather variables of spatial coordinate nğ‘– at step 0, respectively. For edge (nğ‘–, nğ‘—) connecting nodes nğ‘– and nğ‘—, its initial state is: edge ğ‘–ğ‘— = (ğœƒğ‘– ğœƒğ‘—, ğœ‘ğ‘– ğœ‘ğ‘—, Haversine(nğ‘–, nğ‘—)). â„ (13) Basic Blocks in PASSAT. The basic block of PASSAT consists of node-edge connection block and node-node aggregation block. In node-edge connection, the node to edge message passing (blue arrow) involves concatenating the hidden states of the two nodes nğ‘– and nğ‘— connected by each edge (nğ‘–, nğ‘—), written as: edge ğ‘–ğ‘— edge â„ ğ‘–ğ‘— (â„ , â„node ğ‘— , â„node ğ‘– (14) ), edge ğ‘–ğ‘— is the hidden state of edge (nğ‘–, nğ‘—), while â„node are the hidden states of nodes nğ‘– and where â„ nğ‘—, respectively. The edge to node message passing (orange arrow) is to concatenate the sum of the hidden states of edges that each node nğ‘– connects, written as: and â„node ğ‘— ğ‘– ğ‘– (â„node â„node ğ‘– , â„ edge ğ‘–ğ‘— ). ğ‘—:(nğ‘–,nğ‘— )â„° (15) The node-node aggregation uses the adjacency matrix ğ´ to aggregate the hidden states of all nodes (green arrow), and concatenate the results to update the hidden states of all nodes, written as: ğ‘– (â„node â„node ğ‘– , (ğ´)ğ‘–ğ‘—â„node ğ‘— ). ğ‘— (16) C. Performance Metrics Root mean square error (RMSE). Given weather variable ğ‘¢, we suppose that the initial time is 0 and that the lead time is ğœ . The RMSE is defined as: 1 Sd ğ‘s(ğ‘¢(ğœ, s) Ë‡ğ‘¢(ğœ, s))2. RMSE(ğœ ) = (17) sSd 2 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction Therein, Sd is the set of discrete spatial coordinates, while ğ‘¢(ğœ, s) and Ë‡ğ‘¢(ğœ, s) are the prediction and observation of ğ‘¢ at lead time ğœ and spatial coordinate s. The weight ğ‘(s) is defined as: ğ‘(s) = cos ğœƒ 1 Sd sSd cos ğœƒ . (18) The reported RMSE is the average over all initial times. Anomaly correlation coefficient (ACC). Given weather variable ğ‘¢, we suppose that the initial time is 0 and that the lead time is ğœ . The ACC is defined as: ACC(ğœ ) = Therein, we define: ğ‘sClim(ğ‘¢(ğœ, s))Clim(Ë‡ğ‘¢(ğœ, s)) sSd ğ‘sClim(ğ‘¢(ğœ, s))2 sSd ğ‘sClim(Ë‡ğ‘¢(ğœ, s))2 sSd Clim(ğ‘¢(ğœ, s)) = ğ‘¢(ğœ, s) ğ¶(s) 1 Sd sSd (ğ‘¢(ğœ, s) ğ¶(s)), . (19) (20) where ğ¶(s) is the climatological mean of weather variable ğ‘¢ at spatial coordinate s, computed using the ERA5 data set of 2006. The reported ACC is the average over all initial times. D. Training Details D.1. Loss Functions Given weather variable ğ‘¢ and at any initial time denoted by 0, the loss function of ClimODE, GraphCast, Pangu, FourCastNet, and SFNO is given by: â„’basic = 1 ğ‘‡ğ‘ ğœ 1:ğ‘‡ğ‘ 1 Sd sSd (ğ‘¢(ğœ, s) Ë‡ğ‘¢(ğœ, s))2. (21) Therein, Sd is the set of discrete spatial coordinates, while ğ‘¢(ğœ, s) and Ë‡ğ‘¢(ğœ, s) are the prediction and observation of ğ‘¢ at lead time ğœ and spatial coordinate s. We use ğ‘‡ğ‘ to denote the number of autoregressive steps. Then, â„’basic is averaged over all weather variables and all initial times. The predictions of PASSAT also involve the initial motion fields, whose values must be controlled. Therefore, we introduce several penalty terms to the loss function. Given motion field ğ‘£ and at any initial time denoted by 0, we define: â„’velocity = â„’ velocity + â„’2 velocity + â„’3 velocity, where â„’1 velocity = ğœ†1 2Sd sSd [(ğ‘£ğœƒ(0, s))2 + (ğ‘£ğœ‘(0, s))2], â„’2 velocity = â„’3 velocity = ğœ†2 2Sd [( ğœƒ sSd ğœ†3 2Sd [( ğœ‘ sSd ğ‘£ğœƒ(0, s))2 + ( ğ‘£ğœƒ(0, s))2 + ( ğœƒ ğœ‘ ğ‘£ğœ‘(0, s))2], ğ‘£ğœ‘(0, s))2]. (22) (23) (24) (25) Therein, ğœ†1 = 10, ğœ†2 = 1 and ğœ†3 = 1 are constants to penalize the initial motion field and its smoothness. Then, â„’velocity is averaged over all velocity fields and all initial times. In summary, the loss function of PASSAT is given by â„’ = â„’basic + â„’velocity. Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction D.2. Training Strategies We train PASSAT and the baseline deep learning models from scratch in an autoregressive manner; that is, we treat the current predictions as observations and feed them back to the models to generate future predictions. We train each model to predict the weather variables for the future 6, 12, 18 and 24 hours. We use the AdamW optimizer with parameters ğ›½1 = 0.9 and ğ›½2 = 0.999, and set the weight decay as 0.05. Gradient clipping is also employed, with maximum norm value of 1. We use PyTorch for training, validation and prediction, on four GeForece RTX 2080. The batch size is 2 per GPU (8 in total). we adjust the learning rate with the Cosine-LR-Scheduler, and set the maximum and minimum learning rates to 1e-3 and 3e-7, respectively. The number of epochs is 50. E. Baseline Deep Learning Models We consider the following baseline deep learning models: GraphCast, Pangu, ClimODE, FourCastNet, and SFNO. For fair comparisons, we unify the number of parameters to the same magnitude (around 1.15 million). We show their modified structures in Tables 37. Table 3: The modification of GraphCast. Original Modified (this paper) Num Nodes Num Edges Mesh Level Embedding Dimension Processer Layers 1079202 5061126 6 512 4610 29732 4 151 4 Parameters 36.7 (M) 1.15 (M) Table 4: The modification of Pangu. Embedding Dimension Num Layers Num Heads Patch Sizes Window Sizes Parameters Original Modified (this paper) 24 [2(192), 6(384), 6(384), 2(192)] [2(24), 6(48), 6(48), 2(24)] [6, 12, 12, 6] [2, 4, 4, 2] (2, 4, 4) (2, 6, 12) 64 (M) (1, 1) (4, 8) 1.20 (M) F. Comparisons with Baseline Models We compare PASSAT with the other models in Tables 8 and 9 in terms of RMSE and ACC. PASSAT is the best among all deep learning models. Graphcast and FourCastNet are the closest two. IFS T63 outperforms IFS T42 and all deep learning models, but is computationally expensive. 4 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction Table 5: The modification of ClimODE. Num Layers (Velocity) Num Layers (Noise)"
        },
        {
            "title": "Original",
            "content": "Modified (this paper) [5(128), 3(64), 2(10)] [5(64), 3(32), 2(10))] [3(128), 2(64), 2(10)] [4(96), 3(72), 2(5)] 2.8 (M) 1.33 (M) Table 6: The modification of FourCastNet. Embedding Dimension Num Layers Num Blocks Pacth Size Parameters Original Modified (this paper) 768 12 16 (16, 16) 59.1 (M) 172 4 4 (2, 2) 1.17 (M) Table 7: The modification of SFNO. Original Modified (this paper) Embedding Dimension Scaler factor Rank Parameters 1024 3.0 128 65 1.0 107 (M) 1.15 (M) 5 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction Table 8: Comparisons between PASSAT and the other models in terms of RMSEs, over the test set. For each lead time and each weather variable, the best model RMSE is in bold and the second best model RMSE is underlined. Lead Time (h) PASSAT GraphCast ClimODE Pangu FourCastNet SFNO IFS IFS T63 Physics-assisted Topology-informed Parameters (M) t2m t850 u10 v10 - - - 24 48 72 96 120 24 48 72 96 120 144 24 48 72 96 120 144 24 48 72 96 120 144 24 48 72 96 120 144 Yes Yes 1.15 1.25 1.54 1.85 2.16 2.44 2.69 1.29 1.66 2.12 2.59 3.03 3.39 171 293 420 543 651 738 1.54 2.22 2.90 3.46 3.89 4.19 1.57 2.26 2.95 3.55 4.01 4. No Yes 1.15 1.21 1.52 1.84 2.17 2.48 2.75 1.29 1.69 2.17 2.67 3.12 3.49 180 307 438 565 675 1.59 2.29 2.97 3.54 3.97 4.27 1.62 2.32 3.03 3.64 4.10 4.44 Yes No 1.33 1.59 2.23 NAN NAN NAN NAN 1.46 2.26 NAN NAN NAN NAN 209 452 NAN NAN NAN NAN 1.77 2.73 NAN NAN NAN NAN 1.80 2.73 NAN NAN NAN NAN No No 1.20 1.31 1.63 1.94 2.26 2.55 2.80 1.40 1.83 2.31 2.79 3.21 3.55 196 337 477 605 712 797 1.68 2.41 3.08 3.61 4.01 4.29 1.71 2.44 3.14 3.70 4.13 4. No No 1.17 1.29 1.58 1.87 2.16 2.44 2.68 1.39 1.78 2.24 2.71 3.12 3.47 188 313 442 566 674 1.68 2.38 3.05 3.60 4.00 4.29 1.72 2.42 3.12 3.70 4.14 4.44 No Yes 1.15 1.43 1.75 2.02 2.30 2.55 2. 1.51 1.91 2.37 2.84 3.25 3.59 205 342 478 607 718 807 1.83 2.50 3.18 3.73 4.13 4.41 1.86 2.53 3.24 3.83 4.27 4.57 - - - - - 3.21 - 3.69 - - - 3.09 - 3.83 - - - 489 - 743 - - - - - - - - - - - - - - - - - - 2.04 - 2.44 - - - 1.85 - 2.52 - - - 268 - 463 - - - - - - - - - - - - - G. Ablation Studies We also compare PASSAT with its variants in Tables 10 and 11, in terms of RMSE and ACC. It demonstrates that both the physics information and the topology information are critical to the performance gain. H. Visualizations We present several visualization examples of the predictions generated by PASSAT for t2m (Figure 7), t850 (Figure 8), z500 (Figure 9), u10 (Figure 10), and v10 (Figure 11). 6 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction Table 9: Comparisons between PASSAT and the other models in terms of ACCs, over the test set. For each lead time and each weather variable, the best model ACC is in bold and the second best model ACC is underlined. Physics-assisted Topology-informed Parameters (M) t2m t850 z500 u10 v10 Lead Time (h) PASSAT GraphCast ClimODE Pangu FourCastNet SFNO IFS T42 IFS - - - 24 48 72 96 120 144 24 48 72 96 120 144 24 48 72 96 120 24 48 72 96 120 144 24 48 72 96 120 144 Yes Yes 1.15 0.970 0.955 0.935 0.911 0.886 0. 0.966 0.943 0.906 0.859 0.806 0.756 0.986 0.958 0.912 0.849 0.780 0.713 0.929 0.848 0.733 0.610 0.501 0.414 0.926 0.842 0.722 0.589 0.468 0.372 No Yes 1.15 0.972 0.956 0.937 0.912 0.886 0.861 0.966 0.941 0.902 0.851 0.796 0.745 0.984 0.954 0.904 0.838 0.766 0.698 0.924 0.838 0.718 0.593 0.485 0.399 0.921 0.833 0.708 0.572 0.452 0. Yes No 1.33 0.950 0.910 0.510 0.020 NAN NAN 0.960 0.900 0.450 0.030 NAN NAN 0.980 0.910 0.470 NAN NAN NAN 0.910 0.760 0.300 NAN NAN NAN 0.900 0.770 0.320 0.010 NAN NAN No No 1.20 0.968 0.950 0.929 0.903 0.877 0. 0.960 0.931 0.888 0.835 0.780 0.732 0.981 0.944 0.885 0.813 0.737 0.668 0.914 0.818 0.692 0.565 0.457 0.373 0.912 0.814 0.680 0.542 0.420 0.324 No No 1.17 0.969 0.953 0.934 0.911 0.888 0.865 0.960 0.934 0.895 0.845 0.792 0.743 0.983 0.951 0.901 0.835 0.763 0.695 0.914 0.824 0.702 0.578 0.472 0.389 0.911 0.818 0.689 0.553 0.435 0. No Yes 1.15 0.962 0.942 0.922 0.899 0.875 0.853 0.953 0.924 0.881 0.828 0.773 0.724 0.979 0.942 0.883 0.809 0.731 0. 0.898 0.802 0.672 0.541 0.434 0.349 0.895 0.798 0.662 0.519 0.399 0.307 - - - - - 0.870 - 0.830 - - - 0.860 - 0.780 - - - 0.900 - 0.780 - - - - - - - - - - - - - - - - - - 0.940 - 0.920 - - - 0.940 - 0.900 - - - 0.970 - 0.910 - - - - - - - - - - - - - 7 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction Table 10: Ablation studies in terms of RMSE, over the test set. For each lead time and each weather variable, the best model RMSE is in bold and the second best model RMSE is underlined. Lead Time (h) PASSAT w/o physics w/o topology w/o topology&physics Physics-assisted Topology-informed Parameters (M) t2m t850 z500 u10 v10 - - - 24 48 72 96 120 144 24 48 72 96 120 144 24 48 72 96 120 144 24 48 72 96 120 24 48 72 96 120 144 Yes Yes 1.15 1.25 1.54 1.85 2.16 2.44 2.69 1.29 1.66 2.12 2.59 3.03 3. 171 293 420 543 651 738 1.54 2.22 2.90 3.46 3.89 4.19 1.57 2.26 2.95 3.55 4.01 4.34 No Yes 1. 1.33 1.65 1.97 2.30 2.58 2.83 1.33 1.76 2.25 2.75 3.18 3.54 197 331 462 585 690 776 1.61 2.34 3.03 3.60 4.01 4.30 1.64 2.37 3.09 3.69 4.13 4.45 Yes No 1.15 1.28 1.62 1.97 2.30 2.58 2.82 1.35 1.82 2.37 2.87 3.28 3.60 199 357 502 626 723 796 1.63 2.42 3.12 3.65 4.02 4. 1.69 2.50 3.22 3.79 4.18 4.44 No No 1.15 1.35 1.71 2.07 2.40 2.68 2.91 1.39 1.91 2.47 2.97 3.37 3. 225 387 531 649 740 807 1.70 2.51 3.21 3.74 4.10 4.34 1.75 2.58 3.31 3.86 4.23 4.47 8 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction Table 11: Ablation studies in terms of ACC, over the test set. For each lead time and each weather variable, the best model ACC is in bold and the second best model ACC is underlined. Lead Time (h) PASSAT w/o physics w/o topology w/o topology&physics Physics-assisted Topology-informed Parameters (M) t2m t850 u10 v10 - - - 24 48 72 96 120 24 48 72 96 120 144 24 48 72 96 120 144 24 48 72 96 120 144 24 48 72 96 120 144 Yes Yes 1.15 0.970 0.955 0.935 0.911 0.886 0.862 0.966 0.943 0.906 0.859 0.806 0.756 0.986 0.958 0.912 0.849 0.780 0.713 0.929 0.848 0.733 0.610 0.501 0.414 0.926 0.842 0.722 0.589 0.468 0. No Yes 1.15 0.967 0.949 0.927 0.901 0.874 0.849 0.964 0.936 0.894 0.842 0.786 0.734 0.981 0.946 0.893 0.825 0.752 0. 0.922 0.830 0.705 0.577 0.468 0.382 0.920 0.825 0.693 0.555 0.435 0.338 Yes No 1.15 0.969 0.950 0.926 0.899 0.872 0. 0.962 0.930 0.881 0.823 0.767 0.718 0.980 0.936 0.871 0.796 0.724 0.661 0.920 0.817 0.685 0.557 0.450 0.369 0.915 0.804 0.661 0.516 0.398 0.310 No No 1.15 0.966 0.945 0.919 0.890 0.863 0.838 0.960 0.924 0.870 0.810 0.754 0.705 0.975 0.925 0.855 0.778 0.705 0.644 0.913 0.802 0.662 0.532 0.427 0.350 0.907 0.790 0.638 0.493 0.378 0. 9 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction Figure 7: Prediction visualization of t2m. 10 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction Figure 8: Prediction visualization of t. 11 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction Figure 9: Prediction visualization of z. 12 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction Figure 10: Prediction visualization of u10. 13 Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction Figure 11: Prediction visualization of v10."
        }
    ],
    "affiliations": [
        "Shenzhen Institute of Meteorological Innovation",
        "Sun Yat-Sen University"
    ]
}