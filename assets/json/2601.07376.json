{
    "paper_title": "OpenTinker: Separating Concerns in Agentic Reinforcement Learning",
    "authors": [
        "Siqi Zhu",
        "Jiaxuan You"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce OpenTinker, an infrastructure for reinforcement learning (RL) of large language model (LLM) agents built around a separation of concerns across algorithm design, execution, and agent-environment interaction. Rather than relying on monolithic, end-to-end RL pipelines, OpenTinker decomposes agentic learning systems into lightweight, composable components with clearly defined abstraction boundaries. Users specify agents, environments, and interaction protocols, while inference and training are delegated to a managed execution runtime. OpenTinker introduces a centralized scheduler for managing training and inference workloads, including LoRA-based and full-parameter RL, supervised fine-tuning, and inference, over shared resources. We further discuss design principles for extending OpenTinker to multi-agent training. Finally, we present a set of RL use cases that demonstrate the effectiveness of the framework in practical agentic learning scenarios."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 1 ] . [ 1 6 7 3 7 0 . 1 0 6 2 : r OpenTinker: Separating Concerns in Agentic Reinforcement Learning Siqi Zhu, Jiaxuan You University of Illinois Urbana-Champaign"
        },
        {
            "title": "Abstract",
            "content": "We introduce OpenTinker, an infrastructure for reinforcement learning (RL) of large language model (LLM) agents built around separation of concerns across algorithm design, execution, and agentenvironment interaction. Rather than relying on monolithic, end-to-end RL pipelines, OpenTinker decomposes agentic learning systems into lightweight, composable components with clearly defined abstraction boundaries. Users specify agents, environments, and interaction protocols, while inference and training are delegated to managed execution runtime. OpenTinker introduces centralized scheduler for managing training and inference workloads, including LoRAbased and full-parameter RL, supervised fine-tuning, and inference, over shared resources. We further discuss design principles for extending OpenTinker to multi-agent training. Finally, we present set of RL use cases that demonstrate the effectiveness of the framework in practical agentic learning scenarios. Project Page: https://github.com/open-tinker/OpenTinker"
        },
        {
            "title": "Introduction",
            "content": "As LLMs evolve into agents that perform multi-step reasoning and interact with external tools, RL has become key mechanism for improving agent behavior beyond supervised fine-tuning. In practice, however, applying RL to agentic workflows poses substantial systems challenges, driven by long-horizon rollouts, heterogeneous execution patterns, and the need to coordinate inference and training at scale. Recent RL systems for LLMs have made substantial progress in improving scalability and efficiency. OpenRLHF [2] emphasizes practical distributed training with high-throughput rollout engines and engineeringoriented optimizations for PPO-style RLHF. HybridFlow [5] formulates RLHF as dataflow DAG and introduces hybrid control model to express complex multi-stage RL pipelines. AReaL [1] further disaggregates rollout generation and model optimization into asynchronous pipelines across separate GPU clusters, significantly improving hardware utilization. More recently, Agent-Lightning [4] focuses on agentic workloads by separating agent runtimes from training backends to enable minimal-intrusion integration with existing RL frameworks. While these systems excel at scaling RL training workloads, they offer limited support for decoupling agent programming from execution and typically assume repeated deployment and configuration of training infrastructure, which complicates reuse across environments and raises the barrier for users without dedicated resources. From systems perspective, two gaps remain. First, agent environments and interaction protocols are rarely treated as reusable, first-class abstractions. Second, there is limited support for decoupling agent programming from execution, despite growing demand for RL workflows that resemble cloud-based services rather than monolithic research pipelines. Commercial systems such as Tinker [3] demonstrate the practicality and value of this direction. By abstracting away infrastructure concerns and providing managed execution environment, Tinker lowers the friction of deploying RL for agents in real-world settings. However, as proprietary system, its design choices and abstractions are not publicly accessible, limiting reproducibility and extensibility for the research community. 1 Figure 1 OpenTinker architecture. The Scheduler orchestrates resource allocation, while Task Servers execute the training/inference jobs. The Client is lightweight interface for definition and monitoring. In this paper, we present OpenTinker, an open-source framework for RL of LLM agents that enforces clear separation of concerns across agentenvironment specification, RL algorithms, and execution and resource management. Users define agents, environments, and interaction protocols purely at the programming level, while rollout generation, training, and scheduling are handled by managed runtime. This design enables agent environments to be reused across algorithms and execution backends. Furthermore, to demonstrate the generality of this abstraction, we introduce an agent protocol coordinator within the environment, which manages interaction order and synchronization among multiple agents during RL. This design allows OpenTinker to naturally support multi-agent training within the same execution framework. Overall, OpenTinker provides an open and extensible foundation for RL-based training of LLM agents. By exposing agent environments and interaction protocols as first-class abstractions and decoupling agent programming from execution, it bridges the gap between commercial RL-as-a-service platforms and researchoriented RL systems, while laying the groundwork for distributed agentic RL system."
        },
        {
            "title": "2 Approach",
            "content": "The OpenTinker architecture (Figure 1) is designed as Reinforcement Learning as Service (RLaaS) system that supports distributed, multi-tenant serving of agentic workloads. It enables multiple users and agents to concurrently execute RL, fine-tuning, and inference tasks over shared cluster resources. The system is built around modular and extensible architecture that naturally generalizes to multi-agent training settings, allowing coordinated learning and interaction among multiple agents with minimal changes to the execution runtime."
        },
        {
            "title": "2.1 Overall Design\nOpenTinker adopts a modular, client–scheduler–server architecture that cleanly separates user-facing\nworkflow specification from resource management and execution backends. This design enables flexible\ncomposition of training and inference pipelines while maintaining efficient utilization of shared cluster\nresources. The system consists of four core components: Client, Scheduler, Server, and Environment.",
            "content": "Client. The client module serves as the primary user-facing interface of OpenTinker. It provides (1) programming abstraction for defining user-specific environments and agent workflows, including training (e.g., RL, SFT) and inference pipelines, and (2) set of APIs for submitting tasks, streaming data, and managing checkpoints via the scheduler. The client programming interface is illustrated in Figure 2. To ensure robust lifecycle management, the client is equipped with built-in context manager that automatically notifies the server when task terminates, either actively (e.g., upon successful completion) or passively (e.g., due to errors or interruptions). This mechanism guarantees timely resource cleanup and prevents orphaned processes in long-running or distributed executions. Scheduler. The scheduler is implemented as @ray.remote module and acts as the central control plane of the system. It is responsible for (1) managing and allocating GPU resources across heterogeneous workloads, and (2) receiving, launching, and terminating tasks initiated by clients, including RL, supervised fine-tuning, and inference jobs. 2 class Environment: def reset(self): return initial_state def step(self, action): class InferenceClient: def __init__(self, env, infer_server): self.env = env self.infer_server = infer_server return state, score, done, info def inference(self): class RLClient: def __init__(self, env, rl_server): self.env = env self.rl_server = rl_server def train_step(self): pass def validation(self): pass def fit(self): while not converged: self.train_step() self.validation() s, done = self.env.reset(), False while not done: = self.infer_server(s) s, _, done, _ = self.env.step(a) def evaluate(self): pass class Scheduler: def submit_job(self, job): pass def launch_server(self, config): pass Figure 2 Programming API with environment, clients, and scheduler. Upon receiving task request, the scheduler first inspects the current availability of GPU resources. Once sufficient resources are available, it launches the corresponding job using Ray primitives and returns the tasks communication endpoints and execution metadata to the client, enabling monitoring and fine-grained control. When task completes or is terminated, the scheduler deterministically cleans up all associated processes and Ray actors, ensuring isolation across concurrent jobs. Server. The server component specifies how different classes of tasks are instantiated and executed. It encapsulates the training and inference backends, task-specific initialization logic, and post-processing functions for data received from clients. Concretely, the server exposes execution interfaces such as train_step, validation, and generate, which are invoked by the scheduler during task execution. In addition, the server manages checkpointing, including saving, loading, and versioning of model states. By standardizing task execution semantics across RL, SFT, and inference, the server enables unified orchestration while allowing backend implementations to evolve independently. Environment. The environment module defines the interactive setting in which agents operate. An environment may run locally on the client or be deployed as standalone cloud service. It specifies the underlying game or task dynamics, interprets agent actions, and executes the corresponding game logic. After each interaction, the environment returns both reward signal and the current game state to the agent. To reduce interaction latency when agents are executed in batch or at scale, the environment is designed to be highly parallel. This parallelism manifests at two levels: (1) the environment server processes incoming and outgoing requests concurrently, and (2) the internal execution of multiple game instances is parallelized. This design is critical for sustaining high-throughput agentenvironment interaction in large-scale RL and agentic workflows."
        },
        {
            "title": "2.2 Multi-Turn Agentic Training",
            "content": "The Training Server executes multi-turn agentenvironment interactions using finite state machine (FSM) that defines unified execution semantics for both training and inference. The FSM explicitly specifies how context is constructed, how agent actions are generated, and how environment feedback is incorporated across turns, together with precise rules for token masking. The FSM consists of four high-level states: PENDING (Context Construction). The environment constructs the input context for the current turn, including system instructions, dialogue history, and previously observed environment outputs. Tokens introduced in this state are treated as conditioning context and are excluded from loss computation. GENERATING (Action Generation). The agent model generates tokens autoregressively to form an action. Only tokens produced in this state are marked as trainable and contribute to the training objective. Generation continues until predefined end-of-sequence tokens are reached. 3 Figure 3 Distributed Multi-Agent Training. An agent protocol coordinator synchronizes multi-agent interactions between training/validation environments and agent instances running on remote Task Server. The Task Server hosts multiple resource pools, each dedicated to specific agent tasks including trajectory rollout , forward inference (fwd) , and parameter updates. INTERACTING (Environment Step). The generated action text is passed to the environment through the step() interface. The environment returns an observation, which is appended to the context for the next turn. Observation tokens are masked from the loss and serve solely as input for subsequent action generation. TERMINATED. When terminal condition is met, the interaction episode ends and the full trajectory, including contexts, actions, and rewards, is finalized. For RL, rewards are associated with the corresponding action tokens and used for policy optimization. The same FSM rules are reused during inference. In this case, the system follows identical state transitions and context construction logic, while loss computation and gradient updates are disabled. As result, agent workflows defined for multi-turn training can be executed at inference time without modifying prompt templates, control flow, or environment interfaces. Training and inference therefore share single execution model, differing only in whether gradients are applied."
        },
        {
            "title": "2.3 Multi-Agent Training\nOpenTinker supports multi-agent reinforcement learning (MARL) through a coordinator-centric design\n(Figure 3) in which each agent maintains an independent policy and optimization pipeline. Model parameters\nand gradients are not shared across agents. Coordination emerges exclusively from agent interactions within a\nshared environment, where a coordinator is implemented as an environment-level component that enforces\ninteraction protocols.",
            "content": "The multi-agent environment consists of training environment and validation environment. The training environment requires explicit interaction among multiple agents. The validation environment may either be lightweight simulated environment that does not require multi-agent coordination, or distinct environment used to evaluate generalization. The multi-agent environment interacts with independently executed training servers deployed on the cluster through an Agent Protocol Coordinator. The coordinator functions as control-plane component that governs agent interaction protocols and orchestrates execution and synchronization across distributed training processes. In the two-agent example shown in Figure 4, Agent 1 and Agent 2 execute in fixed turn-based order. This behavior is enforced by the Agent Protocol Coordinator through phase-aware scheduling and synchronization. Phase-Level Synchronization. The coordinator inserts global barriers at the boundaries of the rollout and update phases. These barriers ensure that all agents complete the current phase before any agent transitions to the next phase or turn, establishing consistent synchronization points across distributed workers. Intra-Phase Scheduling. Within the rollout phase, an internal barrier controls fine-grained execution. It precisely regulates agent start and stop times as well as turn-taking order, enabling deterministic sequential or interleaved execution patterns. Figure 4 Synchronization mechanism of the Agent Protocol Coordinator. Global barriers synchronize the start and end of the rollout and update phases across turns. Within these phases, internal barriers manage the execution timing and order of individual agents. State Management. Each agent is explicitly tracked in either running or pending state. The coordinator transitions agents between states according to the interaction protocol, preventing race conditions and enforcing correct execution ordering during multi-agent training."
        },
        {
            "title": "3.1 Supported Scenarios and Use Cases\nWe outline the representative RL scenarios that are supported by OpenTinker, providing a high-level view\nof the system’s experimental coverage. As summarized in Table 1, the supported settings span single-turn\nand multi-turn interactions, language-only and vision–language agents, as well as both single-agent and\nmulti-agent environments. They further include both offline dataset-driven training and online simulated\ninteraction, together with heterogeneous reward definitions. These scenarios serve as a structural reference for\nthe subsequent experimental evaluation.",
            "content": "Table 1 Representative supported scenarios in OpenTinker. Only short descriptions are included in the table; see the text for details. Scenario Type Environment Data Source Reward Signal single-turn llm single-turn llm lora single-turn vlm multi-turn llm multi-turn vlm two-agent llm math math geometry 3k gomoku geometry 3k with tool call two-agent gomoku correctness correctness correctness huggingface dataset huggingface dataset huggingface dataset simulated game states win/loss huggingface dataset simulated game states win/loss correctness"
        },
        {
            "title": "3.2 Functional Validation\nWe validate the functional correctness of OpenTinker as a RL execution system. The objective is to verify\nthat reward signals are correctly propagated and that policy optimization proceeds in a stable manner under\nthe task specifications introduced in the previous section. Across all experiments, we track validation-time\nmetrics that are computed independently from training rollouts. A consistent upward trend in these metrics\nserves as evidence that environment interaction, reward computation, and policy updates are correctly\ncoordinated by the system.",
            "content": "Reward Propagation and Optimization Dynamics. Figure 5 reports the evolution of validation mean scores over training. In all cases, policies exhibit non-degenerate learning behavior, characterized by steady improvement rather than reward collapse or oscillation. This confirms that reward signals are properly aligned with agent actions and that gradients derived from these rewards are correctly applied during optimization. 5 Figure 5 Functional validation of RL execution in OpenTinker. Each plot shows the evolution of validation metrics over training steps. Consistent improvement across all settings confirms correct reward propagation, trajectory handling, and policy optimization. Multi-Agent Execution Correctness. We further validate system correctness in two-agent gomoku game with independent agent policies interacting through shared environment. Since the evaluated scenario constitutes zero-sum game, validation rewards for different agents initially improve simultaneously and subsequently exhibit competitive dynamics, where gains by one agent are offset by losses of the other. Such opposing trends are expected under correct zero-sum interaction and indicate that rewards are consistently attributed to the acting agent at each turn. Overall, the observed reward trajectories confirms correct turn-based execution in the multi-agent pipeline. Taken together, these results confirm that OpenTinker correctly implements end-to-end RL execution. Both single-agent and multi-agent training proceed as expected under the defined task interfaces."
        },
        {
            "title": "4 Conclusion",
            "content": "We present OpenTinker, an open-source framework for agentic RL that realizes RL as service through modular architecture. The framework decouples environment and interaction logic from training infrastructure, and provides unified interface for multi-tenant agentic task serving. These design principles enable scalable development of complex agentic workflows, including multi-agent training with independent policies and multi-turn tool-augmented reasoning. Looking ahead, we plan to extend OpenTinker toward larger-scale and more efficient deployments. Future work includes support for multi-node cluster orchestration, separation between server-side training and inference engines, and batch-level scheduling mechanisms for LoRA-based RL. Together, these directions aim to further strengthen OpenTinker as general-purpose infrastructure for scalable, programmable agentic RL."
        },
        {
            "title": "References",
            "content": "[1] Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, and Yi Wu. Areal: large-scale asynchronous reinforcement learning system for language reasoning, 2025. URL https://arxiv.org/abs/2505.24298. [2] Jian Hu, Xibin Wu, Wei Shen, Jason Klein Liu, Zilin Zhu, Weixun Wang, Songlin Jiang, Haoran Wang, Hao Chen, Bin Chen, Weikai Fang, Xianyu, Yu Cao, Haotian Xu, and Yiming Liu. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework, 2025. URL https://arxiv.org/abs/2405.11143. [3] Thinking Machines Lab. Tinker, 2025. URL https://thinkingmachines.ai/tinker/. [4] Xufang Luo, Yuge Zhang, Zhiyuan He, Zilong Wang, Siyun Zhao, Dongsheng Li, Luna K. Qiu, and Yuqing Yang. Agent lightning: Train any ai agents with reinforcement learning, 2025. URL https://arxiv.org/abs/2508.03680. [5] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, page 12791297. ACM, March 2025. doi: 10.1145/3689031.3696075. URL http://dx.doi.org/10.1145/3689031.3696075."
        }
    ],
    "affiliations": [
        "University of Illinois Urbana-Champaign"
    ]
}