{
    "paper_title": "Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions",
    "authors": [
        "Jun Li",
        "Che Liu",
        "Wenjia Bai",
        "Rossella Arcucci",
        "Cosmin I. Bercea",
        "Julia A. Schnabel"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual Language Models (VLMs) have demonstrated impressive capabilities in visual grounding tasks. However, their effectiveness in the medical domain, particularly for abnormality detection and localization within medical images, remains underexplored. A major challenge is the complex and abstract nature of medical terminology, which makes it difficult to directly associate pathological anomaly terms with their corresponding visual features. In this work, we introduce a novel approach to enhance VLM performance in medical abnormality detection and localization by leveraging decomposed medical knowledge. Instead of directly prompting models to recognize specific abnormalities, we focus on breaking down medical concepts into fundamental attributes and common visual patterns. This strategy promotes a stronger alignment between textual descriptions and visual features, improving both the recognition and localization of abnormalities in medical images.We evaluate our method on the 0.23B Florence-2 base model and demonstrate that it achieves comparable performance in abnormality grounding to significantly larger 7B LLaVA-based medical VLMs, despite being trained on only 1.5% of the data used for such models. Experimental results also demonstrate the effectiveness of our approach in both known and previously unseen abnormalities, suggesting its strong generalization capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 8 7 2 3 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Enhancing Abnormality Grounding for Vision\nLanguage Models with Knowledge Descriptions",
            "content": "Jun Li1,2, Che Liu4, Wenjia Bai4, Rossella Arcucci4, Cosmin I. Bercea*1,3((cid:66)), and Julia A.Schnabel*1,2,3,5((cid:66)) 1 Technical University of Munich, Germany 2 Munich Center for Machine Learning, Germany 3 Helmholtz AI and Helmholtz Munich, Germany 4 Imperial College London, UK 5 Kings College London, UK {june.li,cosmin.bercea,julia.schnabel}@tum.de Abstract. Visual Language Models (VLMs) have demonstrated impressive capabilities in visual grounding tasks. However, their effectiveness in the medical domain, particularly for abnormality detection and localization within medical images, remains underexplored. major challenge is the complex and abstract nature of medical terminology, which makes it difficult to directly associate pathological anomaly terms with their corresponding visual features. In this work, we introduce novel approach to enhance VLM performance in medical abnormality detection and localization by leveraging decomposed medical knowledge. Instead of directly prompting models to recognize specific abnormalities, we focus on breaking down medical concepts into fundamental attributes and common visual patterns. This strategy promotes stronger alignment between textual descriptions and visual features, improving both the recognition and localization of abnormalities in medical images.We evaluate our method on the 0.23B Florence-2 base model and demonstrate that it achieves comparable performance in abnormality grounding to significantly larger 7B LLaVA-based medical VLMs, despite being trained on only 1.5% of the data used for such models. Experimental results also demonstrate the effectiveness of our approach in both known and previously unseen abnormalities, suggesting its strong generalization capabilities. The code and model are available here. Keywords: Visual Grounding Large Language Models Multimodality"
        },
        {
            "title": "Introduction",
            "content": "Vision-Language Models (VLMs) [14] have achieved remarkable success in variety of visual understanding tasks, such as image captioning [5], visual question Shared senior authors. Link: https://lijunrio.github.io/AG-KD/ 2 Jun Li et al. Fig. 1: Overview of our approach. We train 0.23B model on just 16,087 samples (1.5% of the data) and achieve similar or better results than the 7B RadVLM, pre-trained on 1 million samples, by using text descriptions that highlight key visual features of abnormalities. answering [6], and visual grounding [7]. By jointly modeling both visual and textual representations, these models excel at associating image content with natural language descriptions, enabling them to be highly adaptable across diverse domains. Recent advancements have extended VLMs to the medical imaging domain, where models such as RadVLM [8], MAIRA-2 [9], and ChexAgent [10] have demonstrated significant potential in tasks like radiology report generation, question answering, and abnormality grounding. These models use large-scale paired X-rays image-text datasets [1114], allowing them to perform different medical tasks, thus providing valuable support to radiologists in diagnosis. Despite these advancements, abnormality grounding remains critical yet underexplored task in medical image analysis. Unlike report generation [1517] and question answering [1820], which has been extensively studied within the context of VLMs, abnormality grounding requires not only the ability to understand textual queries but also the precise localization of corresponding abnormalities within images. Existing medical VLMs, though powerful, are typically large-scale models that require substantial computational resources and extensive pretraining on diverse datasets. While these models demonstrate strong general performance, their general-purpose design may limit their ability to effectively address specialized tasks such as abnormality grounding on medical images. Moreover, significant challenge in abnormality grounding stems from the complexity of medical terminology and its weak alignment with visual features. In general visual grounding tasks, objects like cat or dog possess well-defined and easily recognizable features, facilitating the formation of direct visual-language associations. However, medical abnormalities present more nuanced challenges. Terms like lung opacity or interstitial lung disease refer to combinations of textural, morphological, and contextual features, none of which map to single, welldefined visual counterpart. Instead, these terms describe subtle, heterogeneous manifestations that can vary based on clinical context and imaging modalities. To address these challenges, we introduce novel approach for abnormality grounding by incorporating decomposed knowledge descriptions tied to visual Enhance Abnormality grounding for VLMs with Knowledge Descriptions 3 Fig. 2: Overview of our method. (A) shows the pipeline for obtaining decomposed knowledge descriptions, (B) presents the model architecture and training process for the abnormality grounding task. features as shown in Figure 1. Specifically, we leverage textual descriptions capturing key visual attributes of medical abnormalities, including shape, density, and locationessential for accurate abnormality interpretation in medical images. By explicitly encoding this domain-specific knowledge, we enhance the models ability to associate complex medical terms with their corresponding visual features. Our method achieves superior performance compared to state-ofthe-art approaches [8,9], despite utilizing much smaller model framework with fewer parameters [2]. Additionally, our approach demonstrates that knowledgeenhanced prompts can considerably boost performance in zero-shot settings, even for previously unseen abnormalities. Our key contributions are as follows: We propose knowledge-enhanced approach for abnormality grounding in VLM training, by using fine-grained, attribute-based textual descriptions to improve visual grounding performance. We show that small-scale VLM (0.23B parameters) can match the abnormality grounding performance of large-scale models (7B parameters) using only 1.5% of the training data. We show that our approach considerably improves zero-shot generalization, enabling the model to better detect unseen abnormalities, which is essential in low-data scenarios."
        },
        {
            "title": "2 Methods",
            "content": "Problem Setup. Unlike traditional object detection methods [21] that regress bounding box coordinates, VLMs frame detection as an autoregressive sequence generation task. Given an image RHW , where and denote its dimensions, and text prompt describing the target object, the VLM generates set of bounding boxes = {L1, L2, . . . , LM }. Each bounding box Li = {ℓi } consists of four discrete tokens representing its top-left and x0 bottom-right corners. Coordinates are normalized to [0, 1000] as ℓx = xpixel 1000, , ℓi x1 , ℓi y1 , ℓi y0 4 Jun Li et al. ℓy = ypixel 1000 and quantized into fixed vocabulary, as described in [2]. The bounding box count equals the number of detected instances. This formulation enables detection to be naturally integrated into language modeling. Decomposed Knowledge Description We improve VLM performance in detecting and localizing medical abnormalities by breaking down complex medical concepts into key visual attributes, such as shape, location, and density, as shown in Figure 2A. We first retrieve the medical definitions of each abnormality [22], which often lack explicit references to their visual manifestations in medical images. For example, the medical definition of lung opacity is: Any abnormal focal or generalized opacity or opacities in lung fields (including but not limited to consolidation, cavity, fibrosis, nodule, mass, calcification, interstitial thickening). While informative, such definitions contain extraneous details that do not emphasize the core visual characteristics of abnormalities. To address this, we define set of visual attributes commonly used in medical imaging, including {shape, location, density, color}, which are crucial for characterizing an abnormalitys visual appearance. Note that although pixel intensity is more accurate term in medical imaging, we retain color to ensure compatibility with generalist visionlanguage models that may not recognize the relationship between tissue density and intensity values. With both the definitions and visual attributes at hand, we design prompting strategy to instruct the language model [23] to generate decomposed knowledge descriptions focusing on the visual aspects of each abnormality. Specifically, we construct an input template that integrates medical definitions with defined visual features. The prompt follows this structure: Here is the medical definition of [abnormality name]: \"[medicaldefinition].\" Based on this definition and focusing on the following visual attributes (e.g., shape, location, density, color), provide brief description of the abnormality. By prompting the large language model with this query, we obtain descriptions such as: An area of increased density in the lung fields, typically appearing as white or grayish patch. for each abnormality term. Architecture. As shown in Figure 2B, we use the Florence-2 base [2] as the backbone, which integrates visual encoder and multi-modal encoderdecoder. The visual encoder, based on DaViT [24], processes an input image RHW 3 to produce flattened visual token embeddings RN D, where is the number of visual tokens and is their dimensionality. Simultaneously, knowledge-decomposed prompts are tokenized into text embeddings. Both the visual and text embeddings are passed through the multi-modal transformer encoder-decoder [25] to generate the final answer. The model generates output through auto-regressive decoding, applying cross-entropy loss to all discrete localization tokens. Finally, the loss is defined as: = (cid:88) i=1 log p(yi{V, }) Enhance Abnormality grounding for VLMs with Knowledge Descriptions 5 where yi is the target localization token at position i, and p(yi{V, }) is the predicted probability distribution over the vocabulary, conditioned on both the visual and textual embedding. We fine-tune the entire model in an end-to-end manner using our decomposed textual knowledge prompts, which break down complex medical concepts into key visual attributes, guiding the model to focus on the core visual characteristics of abnormalities."
        },
        {
            "title": "3 Experiments",
            "content": "Dataset. We trained our method on the VinDr-CXR dataset [22], large-scale chest X-ray dataset with bounding boxes annotated by radiologists for various abnormalities. To ensure annotation consistency, we applied weighted box fusion [26] to merge overlapping bounding boxes and converted them into localization tokens. Since our task focuses on abnormality grounding, we retained only images with at least one annotated abnormality, resulting in 18,195 imageabnormality pairs, with 16,087 for training and 2,108 for test. To assess the zero-shot generalization capabilities of our method, we conducted experiments on the PadChest-GR dataset [14]. We focused on two zero-shot scenarios: generalization to new dataset and detection of previously unseen diseases. Following the predefined data split, we selected the test set and converted its bounding box annotations into text-box pairs, resulting in 1,285 imagebounding box pairs. To distinguish between these scenarios, we further divided the test set into two subsets. The first subset, PadChest-known (641 pairs), contains six diseases that overlap with the VinDr-CXR dataset. The second subset, PadChest-unknown (644 pairs), includes diseases not present in VinDr-CXR, enabling us to assess the models performance in detecting previously unseen abnormalities. Benchmark Baselines. We consider two recent state-of-the-art medical VLMs, MAIRA-2 [9] and RadVLM [8], for comparison, both of which are significantly larger (with 3056 times more parameters) than our model and trained on extensive multi-source datasets. Table 1 shows that MAIRA-2 is 13B parameter model trained on 501,825 training samples from combination of MIMICCXR [11], PadChest [13], and USMix [27]. RadVLM, 7B parameter model, is trained on an even broader set, including MIMIC-CXR, CheXpert-Plus [28], CheXpert [29], Vindr-CXR, MS-CXR [30], and PadChest-GR [14], with total of 1,022,742 image-instruction pairs. Our proposed model is trained solely on Vindr-CXR with 16,086 training samples, and has only 0.23B parameters. Experimental setup. Our method leverages Florence-2-base [2] as the backbone, 0.23B parameter architecture. We fine-tune the model using its pretrained weights with the Adam optimizer [31], setting the learning rate to 5106 and weight decay to 0.01. Training is conducted with batch size of 16 and an input resolution of 512 512 on two NVIDIA A6000 GPUs. For comparison, we use the publicly available pre-trained weights of MAIRA-2 and RadVLM and evaluate them on the same test set. In our ablation studies, we first establish 6 Jun Li et al. Table 1: Comparison on the VinDr-CXR and PadChest-GR datasets. Our method achieves competitive results on both datasets, with the best performance on VinDr-CXR and competitive results in zero-shot setting on PadChestGR. Best and second-best performances are coloured Green and Yellow . Rloc, Rshape, Rcls, and Rtotal are different aspects of the RoDeO metrics. Methods marked with indicate the dataset had not been seen during training. Test Set Method Params Train. Samp. mAP95 50 mAP50 mAP75 Rloc Rshape Rcls Rtotal MAIRA-2 VinDr-CXR RadVLM 13B 7B 501,825 1,022,742 1.22 8. 4.94 21.93 0.32 5.14 Ours 0.23B 16,087 10.81 25.5 7.45 25.65 17. 80.13 24.08 60.15 56.92 39.47 41. 81.36 80.92 53.98 54.38 MAIRA-2 PadCh. Know. RadVLM 13B 7B 501,825 1,022,742 Ours 0.23B 16,087 8.36 19.17 5.81 33. 29.68 81.92 37.14 2.53 2.68 10. 11.07 0.81 0.56 58.61 57.34 29. 79.64 46.16 32.48 83.00 49.13 baseline model that uses only abnormality labels, without knowledge-enhanced prompts. The final model, in contrast, incorporates these prompts during training, which provide additional visual context and improve abnormality grounding. Evaluation metrics. We evaluate all models using standard abnormality detection metrics [32], including mean average precision (mAP) at various Intersection over Union (IoU) thresholds: mAP50, mAP50:95, and mAP75. Besides, we use the Robust Detection Outcome (RoDeO) [33], metric for pathology detection that evaluates classification, shape, and localization for bounding box quality."
        },
        {
            "title": "4.1 Comparison with SOTA",
            "content": "Fewer Parameters, Competitive Performance. Table 1 compares the performance of our method with existing state-of-the-art models on the VinDrCXR and PadChest-Known datasets. Despite having only 0.23B parameters, our model consistently outperforms much larger counterparts across multiple key evaluation metrics on the VinDr-CXR dataset. Specifically, our method achieves the highest mAP50 of 25.5%, surpassing the second-best model by 3.57 points. It also attains mAP75 of 7.45%, outperforming the second-best model by 2.31 points. For mAP50:95, our model reaches 10.81%, exceeding the closest competitor by 2.63 points. Additionally, our method achieves the highest overall RoDeO score of 54.38%. These results demonstrate that, despite having significantly fewer parameters, our knowledge-enhanced model achieves competitive, and in some cases, superior performance compared to much larger models trained on extensive multi-source datasets. Notably, we observe that MAIRA-2 underperforms significantly compared to RadVLM, which can be attributed to MAIRA-2 not being trained on the VinDr-CXR dataset, thus operating in zero-shot setting. In Figure 3, we also evaluate our methods performance across individual Enhance Abnormality grounding for VLMs with Knowledge Descriptions 7 Fig. 3: Performance for each disease class, with the y-axis representing the RoDeo total metric. Our method achieves first place in 14 out of 21 diseases from the VinDr-CXR dataset and 3 out of 6 known diseases from the PadChest-GR dataset. The best performances are highlighted in the callout. diseases. Our model ranks first in 14 out of 21 diseases on VinDr-CXR, highlighting its ability to effectively detect abnormalities across wide range of conditions. Comparable Zero-shot Performance. We evaluate our method on the PadChest-Known dataset, which contains 6 diseases overlapping with the VindrCXR dataset, in zero-shot setting. Table 1 shows that our method achieves the best performance in RoDeO shape matching, classification, and overall scores, with values of 32.48%, 83.00%, and 49.13%, respectively. Besides, our model ranks second in mAP50:95 with score of 2.68% and in mAP50 with 11.07%. Our method, though not trained on PadChest, shows competitive performance against MAIRA-2 and RadVLM. We also evaluate the performance for each disease, where our model ranks first in 3 out of 6 classes, demonstrating its good generalization capability, as shown in the right part of the Figure 3."
        },
        {
            "title": "4.2 Ablation Study",
            "content": "Knowledge-enhanced prompts boost VLMs performance. Table 2 presents the results of an ablation study comparing the baseline to the knowledgeenhanced method on the Vindr-CXR and PadChest-GR datasets. Our proposed method demonstrates substantial improvements across all evaluation metrics on the Vindr-CXR test set, achieving mAP50 of 25.5% (vs. 13.26%), mAP75 of 7.45% (vs. 2.76%), and an overall RoDeO score of 54.38% (vs. 45.22%). These results highlight that integrating disease-specific visual knowledge enhances the models ability to detect abnormalities effectively, outpacing the baseline by large margin. We also evaluate both methods performance on the PadChestGR Known dataset. While mAP75 shows slight decrease compared to the baseline, all other metrics exhibit considerable improvement, indicating that the 8 Jun Li et al. Table 2: Ablation study on the effect of knowledge descriptions. Base refers to the Florence-2 model [2], while Ours incorporates knowledge descriptions (KD). We evaluate in-distribution performance on VinDr-CXR and assess zero- ) and shot generalization to an unseen dataset (PadChest-Known, marked by to both an unseen dataset and previously unseen disease classes (PadChestUnknown, marked by ). Best performances are highlighted in green . Test Set Method Vindr-CXR Base + KD (Ours) Pad. Know. Base Pad. Unkn. + KD (Ours) Base + KD (Ours) mAP95 50 mAP50 mAP75 2.76 7.45 13.26 25.50 4.92 10.81 1.92 2. 0.37 0.95 8.34 11.07 1.48 3.05 0.65 0.56 0.03 0.29 Rloc 44.26 56. 48.11 57.34 38.14 44.71 Rshape 34.36 41.41 31.19 32.48 20.90 22.93 Rcls 78.19 80. 81.25 83.00 78.69 86.12 Rtotal 45.22 54.38 44.59 49.13 32.05 33.72 knowledge integration allows the model to generalize better to different datasets. Knowledge-enhanced prompts improve detection of unknown findings. In this section, we further evaluate the performance of our proposed method on the PadChest-Unknown dataset, which contains 18 diseases not present in the Vindr-CXR dataset. Table 2 shows that the knowledge-enhanced method outperforms the baseline across all metrics. Specifically, mAP50 increases from 1.48% to 3.05%, and mAP75 improves from 0.03% to 0.29%. These results suggest that integrating disease-specific knowledge enhances the models ability to transfer knowledge to unknown diseases, i.e., not encountered during training, thereby improving zero-shot performance."
        },
        {
            "title": "5 Discussion and Conclusion",
            "content": "We introduce novel knowledge-enhanced approach to VLMs for abnormality grounding. By integrating fine-grained, decomposed disease-specific visual descriptions, our method demonstrates that smaller, task-specific models can achieve performance comparable to much larger VLMs trained on extensive datasets. This approach substantially improves zero-shot generalization, particularly for previously unseen datasets, making VLMs more effective in low-data settings. Our results further demonstrate that knowledge-enhanced prompts not only boost in-domain performance but also help the model generalize to new diseases, highlighting their potential for real-world medical applications where labeled data is scarce. While our results demonstrate the effectiveness of our knowledge-enhanced prompts method, several avenues for future research remain. First, expanding the knowledge base to include broader range of diseases, along with the integration of multimodal data sources, could further enhance the models generalization capability. Additionally, integrating decomposed, knowledge-enhanced prompts Enhance Abnormality grounding for VLMs with Knowledge Descriptions 9 with larger, more complex VLMs could push performance boundaries. Larger models trained on more extensive datasets have the potential to benefit from the specialized disease-specific knowledge embedded in the prompts, improving their performance. Finally, exploring dynamic prompt adjustment for each disease could further optimize model performance. Recent studies have shown that prompt engineering, which focuses on emphasizing the most relevant cues for each disease, can significantly enhance VLMs performance."
        },
        {
            "title": "References",
            "content": "1. OpenAI: ChatGPT can now see, hear, and speak. https://openai.com/index/ chatgpt-can-now-see-hear-and-speak/ (2024), accessed: 2024-11-26 2. Xiao, B., Wu, H., Xu, W., et al.: Florence-2: Advancing unified representation for variety of vision tasks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 48184829 (2024) 3. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966 (2023) 4. Lu, H., Liu, W., Zhang, B., Wang, B., Dong, K., Liu, B., Sun, J., Ren, T., Li, Z., Yang, H., et al.: DeepSeek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525 (2024) 5. Stefanini, M., Cornia, M., et al.: From show to tell: survey on deep learning-based image captioning. IEEE transactions on pattern analysis and machine intelligence 45(1), 539559 (2022) 6. Lin, Z., Zhang, D., et al.: Medical visual question answering: survey. Artificial Intelligence in Medicine 143, 102611 (2023) 7. Xiao, L., Yang, X., Lan, X., et al.: Towards visual grounding: survey. arXiv preprint arXiv:2412.20206 (2024) 8. Deperrois, N., Matsuo, H., Ruipérez-Campillo, S., Vandenhirtz, M., Laguna, S., Ryser, A., Fujimoto, K., Nishio, M., Sutter, T.M., Vogt, J.E., et al.: RadVLM: multitask conversational vision-language model for radiology. arXiv preprint arXiv:2502.03333 (2025) 9. Bannur, S., Bouzid, K., Castro, D.C., Schwaighofer, A., Thieme, A., Bond-Taylor, S., Ilse, M., Pérez-García, F., Salvatelli, V., Sharma, H., et al.: Maira-2: Grounded radiology report generation. arXiv preprint arXiv:2406.04449 (2024) 10. Chen, Z., Varma, et al.: Chexagent: Towards foundation model for chest X-ray interpretation. arXiv preprint arXiv:2401.12208 (2024) 11. Johnson, A.E., Pollard, T.J., Greenbaum, N.R., Lungren, M.P., Deng, C.y., Peng, Y., Lu, Z., Mark, R.G., Berkowitz, S.J., Horng, S.: MIMIC-CXR-JPG, large publicly available database of labeled chest radiographs. arXiv preprint arXiv:1901.07042 (2019) 12. Wu, J.T., Agu, N.N., Lourentzou, I., Sharma, A., Paguio, J.A., Yao, J.S., Dee, E.C., Mitchell, W., Kashyap, S., Giovannini, A., et al.: Chest imagenome dataset for clinical reasoning. arXiv preprint arXiv:2108.00316 (2021) 13. Bustos, A., Pertusa, A., Salinas, J.M., De La Iglesia-Vaya, M.: Padchest: large chest x-ray image dataset with multi-label annotated reports. Medical Image Analysis 66, 101797 (2020) Jun Li et al. 14. Castro, D.C., Bustos, A., Bannur, S., Hyland, S.L., Bouzid, K., Wetscherek, M.T., Sánchez-Valverde, M.D., Jaques-Pérez, L., Pérez-Rodríguez, L., Takeda, K., et al.: Padchest-gr: bilingual chest X-ray dataset for grounded radiology report generation. arXiv preprint arXiv:2411.05085 (2024) 15. Li, J., Su, T., Zhao, B., Lv, F., Wang, Q., Navab, N., Hu, Y., Jiang, Z.: Ultrasound report generation with cross-modality feature alignment via unsupervised guidance. arXiv preprint arXiv:2406.00644 (2024) 16. Tanida, T., Müller, P., Kaissis, G., Rueckert, D.: Interactive and explainable regionguided radiology report generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 74337442 (2023) 17. Li, J., Li, S., Hu, Y., Tao, H.: self-guided framework for radiology report generation. In: International Conference on Medical Image Computing and ComputerAssisted Intervention. pp. 588598. Springer (2022) 18. Singhal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn, E., Amin, M., Hou, L., Clark, K., Pfohl, S.R., Cole-Lewis, H., et al.: Toward expert-level medical question answering with large language models. Nature Medicine pp. 18 (2025) 19. Li, J., Kim, S.H., Müller, P., Felsner, L., Rueckert, D., Wiestler, B., Schnabel, J.A., Bercea, C.I.: Language models meet anomaly detection for better interpretability and generalizability. In: Medical Image Computing and Computer Assisted InterventionMICCAI 2024 Workshops. Lecture Notes in Computer Science, vol. 15401, pp. 111. Springer Nature Switzerland AG (2025) 20. Zhang, X., Wu, C., Zhao, Z., et al.: Pmc-VQA: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415 (2023) 21. Jiang, P., Ergu, D., et al.: review of yolo algorithm developments. Procedia computer science 199, 10661073 (2022) 22. Nguyen, H.Q., Lam, K., Le, L.T., Pham, H.H., Tran, D.Q., Nguyen, D.B., Le, D.D., Pham, C.M., Tong, H.T., Dinh, D.H., et al.: Vindr-cxr: An open dataset of chest X-rays with radiologists annotations. Scientific Data 9(1), 429 (2022) 23. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: GPT-4 technical report. arXiv preprint arXiv:2303.08774 (2023) 24. Ding, M., Xiao, B., Codella, N., et al.: Davit: Dual attention vision transformers. In: European Conference on Computer Vision. pp. 7492. Springer (2022) 25. Waswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L., Polosukhin, I.: Attention is all you need. In: NIPS (2017) 26. Müller, P., Kaissis, G., Rueckert, D.: Chex: Interactive localization and region description in chest X-ray. In: European Conference on Computer Vision. pp. 92 111. Springer (2024) 27. Demner-Fushman, D., Kohli, M.D., Rosenman, M.B., Shooshan, S.E., Rodriguez, L., Antani, S., Thoma, G.R., McDonald, C.J.: Preparing collection of radiology examinations for distribution and retrieval. Journal of the American Medical Informatics Association 23(2), 304310 (2016) 28. Chambon, P., Delbrouck, J.B., Sounack, T., Huang, S.C., Chen, Z., Varma, M., Truong, S.Q., Chuong, C.T., Langlotz, C.P.: CheXpert Plus: Augmenting large chest X-ray dataset with text radiology reports, patient demographics and additional image formats. arXiv preprint arXiv:2405.19538 (2024) 29. Irvin, J., Rajpurkar, P., Ko, M., Yu, Y., Ciurea-Ilcus, S., Chute, C., Marklund, H., Haghgoo, B., Ball, R., Shpanskaya, K., et al.: Chexpert: large chest radiograph dataset with uncertainty labels and expert comparison. In: Proceedings of the AAAI conference on artificial intelligence. vol. 33, pp. 590597 (2019) Enhance Abnormality grounding for VLMs with Knowledge Descriptions 11 30. Boecking, B., Usuyama, N., Bannur, S., Castro, D.C., Schwaighofer, A., Hyland, S., Wetscherek, M., Naumann, T., Nori, A., Alvarez-Valle, J., et al.: Making the most of text semantics to improve biomedical visionlanguage processing. In: European Conference on Computer Vision. pp. 121. Springer (2022) 31. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International Conference on Learning Representations (2018) 32. Padilla, R., Netto, S.L., Da Silva, E.A.: survey on performance metrics for objectdetection algorithms. In: 2020 International Conference on Systems, Signals and Image Processing (IWSSIP). pp. 237242. IEEE (2020) 33. Meissen, F., Müller, P., Kaissis, G., et al.: Robust detection outcome: metric for pathology detection in medical images. In: Medical Imaging with Deep Learning"
        }
    ],
    "affiliations": [
        "Helmholtz AI and Helmholtz Munich, Germany",
        "Imperial College London, UK",
        "Kings College London, UK",
        "Munich Center for Machine Learning, Germany",
        "Technical University of Munich, Germany"
    ]
}