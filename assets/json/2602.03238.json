{
    "paper_title": "The Necessity of a Unified Framework for LLM-Based Agent Evaluation",
    "authors": [
        "Pengyu Zhu",
        "Li Sun",
        "Philip S. Yu",
        "Sen Su"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation."
        },
        {
            "title": "Start",
            "content": "The Necessity of Unified Framework for LLM-Based Agent Evaluation Pengyu Zhu 1 Li Sun 1 Philip S. Yu 2 Sen Su 1 3 6 2 0 2 3 ] . [ 1 8 3 2 3 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce proposal aimed at standardizing agent evaluation. 1. Introduction As large language model (LLM)based agents become increasingly prevalent (Xi et al., 2025), evaluating agentic capability has emerged as central criterion for assessing modern LLMs (OpenAI et al., 2024; Team et al., 2025; Anthropic, 2025). Unlike traditional LLMs that exhibit primarily static inputoutput behavior (Ni et al., 2025), agents operate as integrated systems with internal mechanisms for planning and memory, acting upon external environments through tool interactions(Xi et al., 2025). This system-level nature fundamentally expands the evaluation space and introduces challenges to the consistency, completeness, and 1Beijing University of Posts and Telecommunications 2University of Illinois Chicago 3Chongqing University of Posts and Telecommunications. Correspondence to: Pengyu Zhu <whfelingyu_zhupengyu@bupt.edu.cn>. Preprint. February 4, 2026. fairness of agent evaluation, as evaluation outcomes now depend on how the surrounding system components are instantiated. To address the paradigm shift, growing number of agent benchmarks have been proposed, aiming to measure autonomous decision-making (Mialon et al., 2024), tool invocation (Patil et al., 2025b), and task execution (Yao et al., 2022) in interactive environments. However, despite their rapid proliferation, existing agent benchmarks are almost exclusively developed within isolated, researcher-designed frameworks for agent evaluation, with benchmark outcomes tightly coupled to distinct environment designs, system prompts, tool abstractions, and memory mechanisms, along with other specific configurations. As result, benchmark outcomes are deeply intertwined with agent specific design choices, obscuring whether reported performance gains reflect genuine improvements in agentic capability. Without unified evaluation standard, benchmarks remain incomparable, and improvements cannot be reliably attributed to the agentic capability. Collectively, these discrepancies reveal systemic structural flaw in current evaluation practices, rather than isolated limitations of individual benchmarks. Without shared evaluation standard, it remains unclear whether the reported performance gains reflect genuine improvements in agentic capability or artifacts of framework-specific design choices. We therefore take the position that unified framework for LLM-based agent evaluation is not optional, but necessary. In this paper, we first provide background on agent evaluation to establish the context for our position. We then analyze the major components of agent evaluation frameworks, examining how variations in each component influence evaluation outcomes and contribute to inconsistency. Building on this analysis, we decompose the evaluation framework into two core elementsthe sandbox and the evaluation methodologyand articulate the properties each must satisfy to support reliable and meaningful evaluation. Finally, we present our proposed approach toward unified framework for LLM-based agents and address alternative viewpoints to demonstrate the robustness of our position. The Necessity of Unified Framework for LLM-Based Agent Evaluation ical agent integrates LLM with auxiliary modules for planning, memory, and tools, enabling it to reason, act, and maintain state across multiple interaction steps. To operationalize these components, agents must be instantiated within an execution system that coordinates model inference, action execution, and environment interaction. this instantiation is commonly realIn practice, ized using open-source agent plantforms such as LangChain (Mavroudis, 2024), LangGraph (LangChain AI, 2024), AutoGPT (Significant Gravitas, 2023), and smolagents (Roucher et al., 2025), or through benchmark-specific implementations. These plantforms define how the agents components are orchestrated at runtime, transforming abstract agent designs into runnable systems rather than isolated model calls. As result, the concrete behavior of an agent is shaped not only by the underlying model, but also by the design and configuration of its execution plantforms, which governs planning loops, memory updates, and tool invocation procedures. Figure 1. Overview of LLM-based agent evaluation, where correctness is assessed over trajectories, final answer, and environment state changes rather than single output. 2. Background on Agent Evaluation 2.1. From LLM Evaluation to Agent Evaluation Classical LLM benchmarks evaluate models as static input output systems, where performance is assessed by the correctness or quality of the response for fixed prompt (Ni et al., 2025). Such benchmarks are effective for measuring foundational capabilities, including language understanding (Hendrycks et al., 2021a), code generation (Chen et al., 2021), mathematical reasoning (Hendrycks et al., 2021b), logical problem solving (Rein et al., 2024), and sentiment analysis (Chen et al., 2024). These tasks typically admit deterministic ground-truth answers or well-defined evaluation criteria, enabling models to be assessed independently across samples. As result, traditional LLM benchmarks characterize models as static mappings. This static evaluation paradigm becomes insufficient once LLMs are deployed as agents. Unlike passive responders to prompts, agents operate through multi-step decision-making trajectories, in which actions interact with and modify external environments, generating observations that influence subsequent decisions (Xi et al., 2025). Thus, evaluation no longer concerns single output, but the outcome of closed-loop interaction between the agent and the changing environmental state. 2.2. Core Components and Instantiation of Agents LLM-based agents are not only models, but composite systems composed of multiple functional components. typ2.3. Agent Benchmarks and Evaluation Constructing benchmarks for LLM-based agents differs from assembling traditional instructionresponse datasets. Rather than specifying isolated inputoutput pairs, an agent benchmark must jointly define task objectives, the tools available for action, and an explicit environment that governs state representations and transition dynamics. Only when these components are coherently integrated does the benchmark constitute an executable evaluation setting in which agent behavior can be meaningfully assessed. Correspondingly, the criteria for evaluating agents are inherently multi-dimensional. Beyond output-level correctness and quality, agent evaluation must quantify efficiency metrics, including the conciseness of tool invocations, trajectory length, execution time, and token consumption. Furthermore, rigorous evaluation necessitates systematic analysis of failure cases to enable precise error attribution. 3. Sources of Variance in Evaluation"
        },
        {
            "title": "Frameworks",
            "content": "From the perspective of agent evaluation, the primary object of interest is the agentic capability of LLM, rather than the quality or effectiveness of any particular evaluation framework. In principle, evaluation should therefore aim to minimize extraneous sources of variation, ensuring that observed performance differences can be attributed to the LLMs agentic behavior itself. In practice, however, current agent evaluations are conducted in diverse and largely unstandardized ecosystem of evaluation frameworks. As result, evaluation outcomes are influenced not only by the LLM, but also by range of 2 The Necessity of Unified Framework for LLM-Based Agent Evaluation framework-level design choices. These include, but are not limited to, inference configuration, prompting and planning strategies, memory mechanisms, tool invocation, as well as the external environments. Each of these components can introduce systematic variation into agent behavior and evaluation outcomes, thereby confounding the measurement of agentic capability. In this section, we analyze these components in turn and identify how each serves as source of variance in evaluation frameworks. 3.1. Inference Configuration Inference configuration constitutes foundational source of variance in agent evaluation, as it governs not only decoding behavior but also the execution interface through which models are invoked. From the perspective of agent evaluation, the goal is to measure agentic performance across LLMs as consistently as possible across implementations, rather than conflating it with variability introduced by providerspecific invocation protocols, deployment-level constraints, or inference configuration choices. In practice, however, such inference configurations are rarely standardized across evaluation setups, introducing substantial and often unacknowledged variability into agent evaluation outcomes. Inference Interfaces and Protocols. primary source of variance in agent evaluation arises from heterogeneity in inference interfaces and provider-specific execution protocols. Even when invoking the same model with identical prompts, differences in protocol-level design, including safety enforcement, request handling, and interface constraints, can lead to divergent execution behavior. This variance manifests in several concrete ways. First, provider-specific content moderation and safety filtering semantics can alter or block agent inputs prior to inference. In agent tasks, identical prompts may execute normally when submitted through the OpenAI API, while being partially filtered or rejected when submitted through Googles GenAI interface, where stricter safety filters are applied (Team et al., 2024). Because these safety mechanisms often rely on configurable thresholds, the extent of filtering may vary across requests, resulting in inconsistent input representations for the agent despite identical task specifications. Second, differences in provider-level request handling can cause hard execution failures that are unrelated to agentic capability. Even when invoking the same GPT-family model with the same prompt, requests submitted via managed cloud deployments such as Azure OpenAI (Microsoft, 2026) may fail due to content management policy enforcement, 1 whereas the identical prompt executes successfully 1e.g., \"The response was filtered due to the prompt triggering through OpenAIs native API. From the standpoint of agent evaluation, such failures are indistinguishable from agent incompetence, despite being induced by protocol-level policy checks rather than deficiencies in reasoning or planning. Third, inference protocols impose interface-specific constraints on tool definitions and function schemas, which can directly affect agent execution. We will discuss the implications of such tool-level incompatibilities in Section 3.4. Collectively, these cases demonstrate that inference protocols are not neutral execution channels, but systematic sources of evaluation variance that confound the measurement of agentic capability. Inference Parameters and Stochasticity. Inference parameters constitute second and independent source of variance, which is further compounded by inherent execution stochasticity. Although it is common practice to set the temperature to zero to improve reproducibility, prior work has shown that LLM inference is not fully deterministic even under greedy decoding (Song et al., 2025). Execution-level effects such as floating-point non-associativity in parallel GPU operations, as well as variations in batching strategies and resource allocation, introduce irreducible stochasticity. This phenomenon is further amplified by the high-entropy nature of agent decision-making (Dong et al., 2025). As result, identical prompts executed under ostensibly identical settings can yield divergent token-level outputs, which may compound over long-horizon agent trajectories and ultimately affect tool invocations and task completion. Inference Engine Variability. Even in open-source settings, inference variability persists due to differences in deployment engines. Models executed under different inference engines, such as vLLM (Kwon et al., 2023), SGLang (Zheng et al., 2024), or Transformer (Wolf et al., 2020), can exhibit divergent behavior despite identical model, owing to engine-specific implementation differences. Such discrepancies have been widely reported by the community and observed across deployments. In agent settings that involve long-horizon reasoning and repeated decision-making, these seemingly minor execution-level differences can accumulate, resulting in substantial divergence in agent trajectories. 3.2. Prompting and Planning Strategies Prompting and planning strategies introduce another major source of variance in agent evaluation. In LLM-based agents, the ability to invoke tools and perform multi-step planning is not inherent to the base model alone, but is largely elicited through carefully designed system prompts. These prompts specify the rules governing agent behavior, Azure OpenAIs content management policy. Please modify your prompt and retry.\" 3 The Necessity of Unified Framework for LLM-Based Agent Evaluation including tool usage formats, action constraints, and planning procedures, and thereby dictating whether an agent can execute tasks correctly and efficiently. Prompting. In practice, system prompts vary substantially across agent frameworks and benchmarks. Open-source agent frameworks, such as smolagents and LangChain, typically employ highly detailed system prompts that encode extensive generation rules, planning logic, and tool invocation conventions to support wide range of applications. In contrast, many agent benchmarks (Yao et al., 2025; Barres et al., 2025; Li et al., 2025) adopt lightweight, benchmarkspecific system prompts tailored to their particular task and evaluation setup. While such prompts may be well-suited for given benchmark, they introduce strong, frameworkdependent inductive biases that shape agent behavior. From the perspective of agent evaluation, this variability poses fundamental challenge. Different system prompts impose different behavioral constraints and task interpretations on the same LLM, often leading to substantial performance differences. Consequently, evaluation outcomes may reflect the effectiveness of prompt engineering rather than the intrinsic agentic capability of the model. This is particularly problematic when the goal is to assess general-purpose agentic capability, rather than performance under specific, handcrafted prompting regime. Planning Strategies. Beyond prompting, planning strategies contribute to evaluation variance. Planning enables agents to decompose complex tasks into subgoals, reason over intermediate states, and revise actions based on feedback (Huang et al., 2024). Many contemporary agent systems adopt variants of the ReAct (Yao et al., 2023) paradigm, while alternative approaches such as Chain-of-Thought prompting (Wei et al., 2022) and Plan-and-Execute frameworks (Wang et al., 2023) are also employed. Importantly, even when nominally following the same ReAct paradigm, differences in implementation details, such as planning granularity, action constraints, and reflection mechanisms, can systematically alter agent trajectories and outcomes. Taken together, variability in prompting and planning strategies constitutes systematic source of evaluation variance that should not be conflated with differences in agentic capability. This observation further underscores the necessity of unified framework for agent evaluation that standardizes prompting and planning components, enabling fair and comparable assessment of LLM-based agents independent of prompt-specific design choices. 3.3. Memory Mechanisms Memory mechanisms constitute critical yet often overlooked source of variance in agent evaluation. In LLMbased agents, memory enables coherence across interactions and actions, as well as adaptation based on accumulated experience (Xi et al., 2025). As such, memory design choices introduce structural variance that can be easily conflated with the LLMs inherent agentic capability. Memory Formatting. primary and fundamental source of variance arises from how past interactions are represented and fed back to the LLM. Different agent frameworks adopt distinct memory serialization formats, determining how execution trajectories, tool calls, and outcomes are presented in the models context. Well-structured memory representationssuch as explicitly separating observations, actions, tool results, and error statescan substantially improve the models ability to understand its own trajectory. Some agent frameworks further introduce specialized memory encodings for failed tool executions or invalid actions, enabling the agent to recognize errors and adjust its strategy to recover toward correct execution path. In contrast, flat or unstructured memory representations may obscure causal relationships between actions and outcomes, making it significantly harder for the LLM to reason. Consequently, differences in memory formatting alone can lead to large performance gaps, independent of the underlying models agentic capability. Short-Term Memory Management. Beyond formatting, variance also arises from how short-term memory is managed under context window constraints. In long-horizon tasks, interaction histories frequently exceed the models context limit, forcing agent frameworks to decide what information to retain or discard. Because the strategies vary widely, ranging from naive First-In-First-Out (FIFO) truncation (Packer et al., 2024) to complex summarization (Park et al., 2023) or retrieval-augmented mechanisms (Packer et al., 2024), the actual context presented to the model differs across implementations. Consequently, benchmark results often reflect the effectiveness of the agent frameworks memory management strategy rather than the LLMs inherent agentic reasoning. Long-Term Memory Management. In tasks that explicitly require persistent knowledge across extended trajectories or episodes, long-term memory mechanisms introduce additional sources of variance. Agent frameworks differ widely in how long-term memory is stored, indexed, retrieved, and integrated back into the agents context. These differences affect retrieval accuracy, latency, and relevance, yet are rarely standardized across evaluation setups. As result, performance on long-term memory benchmarks may reflect the sophistication of the memory infrastructure rather than the LLMs capability to reason over stored knowledge. In summary, memory mechanisms are not passive buffers, 4 The Necessity of Unified Framework for LLM-Based Agent Evaluation but active components that shape agent behavior by determining what past information is accessible, how it is structured, and when it is retrieved. Without standardized memory representations and management strategies, agent evaluation outcomes become tightly coupled to agent framework-specific design choices, obscuring the true agentic capabilities of the underlying model. 3.4. Tool Invocation The definition of tools is critical in determining whether LLMs can invoke them correctly and compliantly during inference. As result, many benchmarks adopt open-source agent frameworks or custom-built tool interfaces to gain flexibility; however, this flexibility often leads to inconsistent agent execution behavior and outcomes. Tool Representation. One source of variability arises from how tools are represented and passed to LLMs. Differences in the structure of tool descriptions, argument parameters, and return values as prompts can lead to discrepancies in execution behavior. Although platforms like OpenAI impose additional fields for tool invocation, significant differences still exist across platforms. For example, both OpenAI (OpenAI, 2026) and Gemini (Google, 2026) APIs impose strict constraints on tool specifications, requiring descriptive tool names, free of spaces or special characters, and limited to maximum length (e.g., 64 characters). In contrast, when the same agent is executed using locally deployed models via inference engines (e.g., vLLM), these constraints are often absent, allowing for more flexible tool naming and schema definitions. As result, differences in tool representation and specification can lead to divergent outcomes. Parameter Type Support. Additionally, parameter types and function-calling semantics may differ across interfaces, further exacerbating variance in tool invocation behavior. For instance, the parameter type dict[str, Any] is unsupported in the OpenAI official API, resulting in an error 2, whereas both Gemini and other deployed open-source models handle it without issue. These discrepancies highlight the impact of platform-specific differences on agent evaluation, reinforcing the need for standardized interfaces to ensure fair comparisons. Therefore, widely divergent tool definitions and invocation protocols introduce systematic noise that compromises evaluation validity. By conflating intrinsic tool use reasoning with mere API compliance, these discrepancies render crossplatform comparisons unreliable, reinforcing the urgent need for standardized interfaces. 2e.g., \"Invalid schema for function xxx: any is not valid under any of the given schemas.\" 3.5. External Environments For LLM-based agents, the external environment defines the world in which tasks are executed. In real-world deployments, such environments are inherently dynamic and complex. However, the goal of agent evaluation differs from deployment: it seeks to assess agentic capability rather than account for unconstrained environmental shifts. From an evaluation perspective, the environment must function as standardized laboratory setting. To ensure internal validity, the environment must be deterministic and reproducible. Without strict control, environmental variance acts as confounding factor, making it impossible to disentangle whether failure arises from deficiencies in the agent or from external perturbations, such as information drift, beyond the agents control. representative example is the BrowseComp benchmark (Wei et al., 2025), which evaluates browsing agents through interaction with live web search APIs. While this design reflects real-world usage, the absence of fixed reference sources makes task resolution non-traceable and undermines reproducibility, as solutions depend on volatile web content. This issue was subsequently addressed by BrowseCompPlus (Chen et al., 2025), which stabilized the benchmark by snapshotting live web content into static retrieval corpus. The resulting analysis revealed that substantial portion of tasks in the original benchmark had become unsolvable due to inaccessible or outdated websites. Similar challenges persist in other web-based benchmarks, such as BFCL (Patil et al., 2025a), where URL expiration and content updates continue to affect task solvability. This finding highlights key principle: without controllable and static external environment, evaluation results become transient and difficult to interpret. To avoid conflating agentic failure with environmental instability, environments must be explicitly defined, versioned, and reproducible. 4. The Necessity and Features of Unified"
        },
        {
            "title": "Evaluation Framework",
            "content": "In the previous section, we identified key sources of variance in evaluation frameworks. To address these challenges and ensure valid, reliable, and fair agent evaluations, it is essential to establish unified evaluation framework. This framework should consist of two key parts: the Sandbox and the Evaluation Methodology. This section discusses their necessary to achieve such consistency, and discusses their roles in mitigating variability. 4.1. Sandbox The concept of sandbox, originally from computer security (Goldberg et al., 1996), refers to an isolated execution 5 The Necessity of Unified Framework for LLM-Based Agent Evaluation environment that restricts the actions of untrusted code. In the context of LLM-based agents, we define the sandbox not merely as container, but as unified evaluation substrate. It encapsulates the entire agent system and its controllable environment, acting as bridge between the model and the deterministic requirements of scientific measurement. For the agent system, as discussed in Section 3.1 to Section 3.4, inconsistencies in any component of the agent can have significant impact on results. To address these issues, unified agent instantiation architecture is required to standardize the agents components, including LLM inference, system prompts, planning, memory and tools. Since the primary goal of all agent benchmarks is to evaluate the LLMs agentic capability, standardization ensures that evaluation results reflect the LLMs agentic capability itself, rather than artifacts induced by system-level inconsistencies. For the environment, as discussed in Section 3.5, the evaluation framework must provide controllable and objective virtual world in which agents operate. The environment should be explicitly defined, fully observable, and stable across evaluation runs, such that all data dependencies encountered during agent execution can be traced, audited, and reproduced. By enforcing static and deterministic world state, the sandbox ensures that observed successes or failures can be attributed to the agents reasoning behavior rather than external volatility. This capability is essential for the evaluation framework, as it allows environmental effects to be systematically isolated rather than implicitly entangled with agentic performance. The primary function of this sandboxed architecture is to enforce hermetic determinism, isolating agent execution from external state such that observed behavior reflects the agents policy rather than incidental environmental contingencies. As result, evaluation runs conducted at different times are constrained to follow functionally equivalent execution trajectories, making outcomes comparable across runs. Such determinism is essential for rigorous agent research, enabling precise failure diagnosis and meaningful benchmarking without confounding effects from environmental drift or system-level variability. Beyond control and reproducibility, the sandbox plays critical role in AI safety and security evaluation. As agents are increasingly tested for offensive capabilities and defensive robustness (Yu et al., 2025; Liu et al., 2024), the sandbox must serve as secure \"Cyber Range\" (Ganguli et al., 2022). From an ethical standpoint, deploying agents in real-world environments for adversarial tasks is unacceptable, as it risks irreversible side effects to external systems and violates safety protocols. sandbox provides contained simulation where dangerous behaviors can be tested and studied without real-world consequences. Moreover, without such an isolated simulation, security assessments lack standardized \"threat model.\" Attacks that succeed in an unconstrained environment may fail in hardened one; thus, the sandbox ensures that security evaluations reflect genuine agentic capability rather than environmental vulnerabilities. In conclusion, the sandbox is not merely technical convenience but foundational and ethical cornerstone of unified framework, without which valid agent evaluation becomes unfeasible and unreliable. 4.2. Evaluation Methodology While the sandbox ensures controlled and reproducible execution substrate, unified framework also requires principled evaluation methodologies to interpret agent behavior within this substrate. Currently, benchmarks differ significantly in their evaluation methods, leading to substantial inconsistencies. Many benchmarks rely heavily on proprietary evaluation and calculation methods, which, while suited to their specific context, lack the generalizability needed for fair comparison. For instance, MultiAgentBench (Zhu et al., 2025) uses \"Task Completion Metrics,\" AgentBench (Liu et al., 2024) refers to it as \"score,\" and BFCL (Patil et al., 2025a) uses \"ACC,\" but at their core, they all assess whether task was completed in accordance with the ground truth or evaluation rules. Despite their conceptual similarity, the absence of explicit standardization prevents these metrics from being directly aligned or compared across benchmarks. Many agent benchmarks adopt pass@k evaluation protocols (Yao et al., 2025; Barres et al., 2025; OpenAI et al., 2024; Anthropic, 2025), where success is defined by at least one successful execution among trials. However, the choice of influences reported performance, conflating agentic capability with sensitivity to inference randomness rather than reflecting robust decision-making ability. Without standardized inference configurations or principled aggregation over stochasticity, evaluation results risk reflecting favorable sampling regimes rather than intrinsic reasoning performance. Furthermore, many evaluation methodologies fail to fully align with the nature of agent-based tasks, which require comprehensive assessment of both the agent and its interaction with the environment. Task completion should be considered alongside the agents reasoning process, environmental interactions, and the impact of its actions, as these factors collectively define the agents overall performance. For example, BFCL evaluates only the output of the LLMs tool calls, neglecting the reasoning process and the consequences of the agents actions. As result, such methodologies fail to adequately capture the agents autonomous capabilities and decision-making process, which are crucial for true evaluation of agentic performance. In terms of evaluation dimensions, current benchmarks also exhibit significant discrepancies. While task completion is an important metric, it is not sufficient to assess an agents The Necessity of Unified Framework for LLM-Based Agent Evaluation Figure 2. unified framework where the sandbox integrates standardized dataset and unified agent architecture, coupled with multidimensional evaluation. full capability. Additional metrics should include reasoning steps, resource consumption, failure analysis, and communication in multi-agent scenarios. However, many benchmarks rely on narrow set of metrics, often focusing solely on task completion, which limits the comprehensiveness of their evaluations. Even for the same metric, the evaluation content can differ significantly. For example, failure analysis in τ -bench (Yao et al., 2025) categorizes errors as \"Wrong Info,\" \"Wrong Argument,\" \"Wrong Decision,\" and \"Partially Resolved,\" while AgentBench uses categories such as \"Context Limit Exceeded,\" \"Invalid Format,\" \"Invalid Action,\" and \"Task Limit Exceeded.\" This lack of standardized failure categories limits both the comprehensiveness of the evaluations and the ability to compare across benchmarks. To address these challenges, unified evaluation methodology is indispensable to standardize protocols and metrics across benchmarks, ensuring that observed performance gains reflect genuine improvements in agentic capability rather than benchmark-specific evaluation artifacts. 5. Feasible Proposal of Unified Framework for Agent Evaluation Building on the identified need for standardization, we propose feasibility framework to unify agent evaluation. The framework comprises Sandbox that integrates standardized dataset for preparation and unified agent architecture to enforce deterministic execution, coupled with multidimensional evaluation methodology for comprehensive capability assessment. Crucially, the effectiveness of any unified framework for agent evaluation does not stem solely from its technical design, but from broad community adoption and collective adherence. Without widespread and consistent use by researchers, standardization remains nominal and fails to achieve meaningful comparability across studies. 5.1. Standardized Dataset Composition To overcome the limitations of traditional LLM benchmarks, we propose modern agent evaluation dataset composed of three key components: the Instruction Set (I), the Tool Set (T ), and the Environment Set (E). These components serve the following roles: defines the input tasks, provides the tools required for task execution, and defines the agents operational world. These sets must be tightly integrated to ensure that tasks in rely on tools in , which in turn interact with to achieve task completion. The Instruction Set (I) largely follows the structure of traditional benchmarks, but must be designed to reflect agentoriented tasks, with instructions paired with well-defined ground-truth or rule-based evaluation criteria. In contrast, the Tool Set (T ) requires standardized definitions, as tools are central to agent behavior. This requirement naturally aligns with the agent framework discussed in Section 5.2, which enables consistent tool abstraction and implementation. Concretely, we advocate Python-based standardized tool protocol that supports uniform instantiation, extensibility, and seamless integration into agent systems. The Environment Set (E), as crucial component of the sandbox, must ensure that the environment is static, versioncontrolled, and reproducible. To meet these requirements, the sandbox replaces dynamic, real-world systemssuch as live internet or production databaseswith versioncontrolled, mocked counterparts, such as snapshot-based 7 The Necessity of Unified Framework for LLM-Based Agent Evaluation web corpora and local file systems. Current practices often use tools like Docker or local files (e.g., JSONL) to construct environment. Therefore, it is essential to standardize the formats and types of these data to ensure consistency and reproducibility across evaluations. 5.2. Unified Agent System Architecture As discussed in Section 4.1, standardizing agent system instantiation is essential to control system-level variance in agent evaluation. Therefore, we recommend adopting unified open-source agent frameworks, such as smolagents, which is designed for generality and extensibility. Using shared agent frameworks provides common execution substrate, ensuring that agent instantiation follows consistent conventions during evaluation. standard agent system comprises components including the LLM, memory, planning, and tools. Agent frameworks integrate these components in standardized manner, yielding consistent prompt construction, planning control flow, tool invocation, and memory handling across evaluation runs. This consistency helps ensure that observed performance differences reflect agentic behavior rather than framework-specific implementation effects. For researchers developing custom agent frameworks, we advocate adherence to shared architectural standard to preserve cross-benchmark comparability. 5.3. Multidimensional Evaluation Methodology Agent evaluation should be inherently multidimensional, extending beyond simple output correctness to reflect the full execution of an autonomous system. At the outcome level, evaluation must assess not only whether the agent produces correct final response, but also whether its actions induce the expected changes in the environment. Beyond outcomes, process-level evaluation is equally critical. The sequence of tool invocations, their arguments, and execution order collectively define the agents reasoning trajectory and should be evaluated for correctness and consistency. Following approaches such as τ -bench (Yao et al., 2025), this can be operationalized by using simulator to execute predefined ground-truth trajectory, producing goldstandard environment snapshot. The agents execution is then evaluated by comparing its resulting environment state against this reference. For semantic evaluation of textual outputs, we advocate the use of rule-based validation where applicable, or unified Judge-LLM calibrated for high agreement with human judgments, avoiding variance introduced by heterogeneous judge models. Given the inherent stochasticity of LLM inference, we advocate for pass@k as robustness-aware evaluation protocol. Crucially, to prevent statistical artifacts from masking genuine capability gains, we propose benchmark-wide standardization of k, thereby ensuring strict comparability. 8 Efficiency constitutes critical evaluation dimension. capable agent should not merely arrive at correct solution, but do so with minimal resource expenditure. Accordingly, evaluation should quantify resource efficiency, including token consumption, execution latency, and the number of interaction steps, as well as other relevant indicators. Finally, failure analysis must be unified, comprehensive, and automated. Current benchmarks employ heterogeneous and often incompatible failure categorizations, resulting in fragmented error spaces that obscure the true sources of agent failure. We therefore propose the establishment of standardized and exhaustive failure taxonomy for agent execution, covering all major classes of reasoning, planning, tool-use, and environment-interaction errors. To ensure scalability and consistency, this taxonomy should be paired with automated classification models and workflows that systematically attribute failure causes. Such unified attribution is essential for large-scale diagnosis of agent weaknesses and for enabling meaningful, cross-benchmark comparisons. 6. Alternative View range of alternative views exist regarding the necessity and practicality of unified framework for agent evaluation. shared evaluation substrate may implicitly privilege dominant architectural paradigms, thereby disadvantaging unconventional agent designs whose capabilities do not align with prescribed interaction formats. Excessive standardization therefore risks narrowing methodological diversity and constraining architectural innovation. To mitigate this risk, unified framework should remain continuously extensible, adapting to emerging agent capabilities and structural paradigms, while confining standardization to evaluationrelevant interfaces. In this way, the primary objective of unification is not to prescribe how agents should be built, but to ensure that their evaluation remains consistent, comparable. Moreover, our emphasis on deterministic sandbox environments, while essential for reproducibility and traceability, necessarily reduces ecological validity, since real-world agents operate in dynamic environments. Sandbox evaluation should thus be viewed as controlled examination of agentic capability, complementing rather than replacing real-world deployment and practice. Finally, protocol-level efforts such as MCP (Hou et al., 2025) improve interoperability across tool interfaces but leave evaluation methodology largely unaddressed. By standardizing the syntax of tool interaction rather than the criteria by which agent behavior is assessed, protocol unification alone is insufficient to guarantee scientifically comparable benchmark results. Taken together, these considerations clarify the scope and intent of our position: unified framework is essential for the rigorous advancement of agent evaluation. The Necessity of Unified Framework for LLM-Based Agent Evaluation"
        },
        {
            "title": "References",
            "content": "Anthropic. System card: Claude opus 4 claude sonnet 4, 2025. URL https://www.anthropic.com/ claude-4-system-card. Technical Report. Barres, V., Dong, H., Ray, S., Si, X., and Narasimhan, K. τ 2-bench: Evaluating conversational agents in dualcontrol environment, 2025. URL https://arxiv. org/abs/2506.07982. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., HerbertVoss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/ 2107.03374. Chen, Y., Yan, S., Liu, S., Li, Y., and Xiao, Y. EmotionQueen: benchmark for evaluating empathy of large language models. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 2149 2176, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-acl.128. URL https://aclanthology. org/2024.findings-acl.128/. Chen, Z., Ma, X., Zhuang, S., Nie, P., Zou, K., Sharifymoghaddam, S., Liu, A., Green, J., Patel, K., Meng, R., Su, M., Li, Y., Hong, H., Shi, X., Liu, X., Thakur, N., Zhang, C., Gao, L., Chen, W., and Lin, J. Browsecompplus: more fair and transparent evaluation benchIn First Workshop on mark of deep-research agent. Multi-Turn Interactions in Large Language Models, 2025. URL https://openreview.net/forum? id=YJAA2PzfDi. Dong, G., Mao, H., Ma, K., Bao, L., Chen, Y., Wang, Z., Chen, Z., Du, J., Wang, H., Zhang, F., Zhou, G., Zhu, Y., Wen, J.-R., and Dou, Z. Agentic reinforced policy optimization, 2025. URL https://arxiv.org/abs/ 2507.19849. Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, 9 K., Jones, A., Bowman, S., Chen, A., Conerly, T., DasSarma, N., Drain, D., Elhage, N., El-Showk, S., Fort, S., Hatfield-Dodds, Z., Henighan, T., Hernandez, D., Hume, T., Jacobson, J., Johnston, S., Kravec, S., Olsson, C., Ringer, S., Tran-Johnson, E., Amodei, D., Brown, T., Joseph, N., McCandlish, S., Olah, C., Kaplan, J., and Clark, J. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned, 2022. URL https://arxiv.org/abs/2209.07858. Goldberg, I., Wagner, D., Thomas, R., and Brewer, E. A. secure environment for untrusted helper applications In Proceedings of the 6th confining the wily hacker. Conference on USENIX Security Symposium, Focusing on Applications of Cryptography - Volume 6, SSYM96, pp. 1, USA, 1996. USENIX Association. Google. Function calling gemini api google ai for developers. https://ai.google.dev/gemini-api/ docs/function-calling, 2026. Accessed: 202601-16. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask In International Conference language understanding. on Learning Representations, 2021a. URL https:// openreview.net/forum?id=d7KBjmI3GmQ. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021b. URL https://openreview.net/forum? id=7Bywt2mQsCe. Hou, X., Zhao, Y., Wang, S., and Wang, H. Model context protocol (mcp): Landscape, security threats, and future research directions, 2025. URL https://arxiv.org/ abs/2503.23278. Huang, X., Liu, W., Chen, X., Wang, X., Wang, H., Lian, D., Wang, Y., Tang, R., and Chen, E. Understanding the planning of llm agents: survey, 2024. URL https: //arxiv.org/abs/2402.02716. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. LangChain AI. Langgraph: language agents as graphs. https://github.com/ langchain-ai/langgraph, 2024. Accessed: 2026-01-15. Build resilient The Necessity of Unified Framework for LLM-Based Agent Evaluation Li, J., Zhao, W., Zhao, J., Zeng, W., Wu, H., Wang, X., Ge, R., Cao, Y., Huang, Y., Liu, W., Liu, J., Su, Z., Guo, Y., Zhou, F., Zhang, L., Michelini, J., Wang, X., Yue, X., Zhou, S., Neubig, G., and He, J. The tool decathlon: Benchmarking language agents for diverse, realistic, and long-horizon task execution, 2025. URL https:// arxiv.org/abs/2510.25726. Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Ding, H., Men, K., Yang, K., Zhang, S., Deng, X., Zeng, A., Du, Z., Zhang, C., Shen, S., Zhang, T., Su, Y., Sun, H., Huang, M., Dong, Y., and Tang, J. Agentbench: Evaluating LLMs as agents. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=zAdUB0aCTQ. Mavroudis, V. LangChain v0.3. working paper or preprint, December 2024. URL https://hal.science/ hal-04817573. Mialon, G., Fourrier, C., Wolf, T., LeCun, Y., and Scialom, T. GAIA: benchmark for general AI assistants. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=fibxvahvs3. Microsoft. Azure OpenAI service content filtering. https://learn.microsoft.com/en-us/ azure/ai-services/openai/concepts/ content-filter, 2026. Accessed: 2026-01-16. Ni, S., Chen, G., Li, S., Chen, X., Li, S., Wang, B., Wang, Q., Wang, X., Zhang, Y., Fan, L., Li, C., Xu, R., Sun, L., and Yang, M. survey on large language model benchmarks, 2025. URL https://arxiv. org/abs/2508.15361. OpenAI. Function calling - openai api documentation. https://platform.openai.com/docs/ guides/function-calling, 2026. Accessed: 2026-01-16. OpenAI, :, Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., adry, A., Baker-Whitcomb, A., Beutel, A., Borzunov, A., Carney, A., Chow, A., Kirillov, A., Nichol, A., Paino, A., Renzin, A., Passos, A. T., Kirillov, A., Christakis, A., Conneau, A., Kamali, A., Jabri, A., Moyer, A., Tam, A., Crookes, A., Tootoochian, A., Tootoonchian, A., Kumar, A., Vallone, A., Karpathy, A., Braunstein, A., Cann, A., Codispoti, A., Galu, A., Kondrich, A., Tulloch, A., Mishchenko, A., Baek, A., Jiang, A., Pelisse, A., Woodford, A., Gosalia, A., , and others. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. Packer, C., Wooders, S., Lin, K., Fang, V., Patil, S. G., Stoica, I., and Gonzalez, J. E. Memgpt: Towards llms as operating systems, 2024. URL https://arxiv. org/abs/2310.08560. Park, J. S., OBrien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST 23, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701320. doi: 10.1145/3586183.3606763. URL https://doi. org/10.1145/3586183.3606763. Patil, S. G., Mao, H., Yan, F., Ji, C. C.-J., Suresh, V., Stoica, I., and Gonzalez, J. E. The berkeley function calling leaderboard (BFCL): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning, 2025a. URL https: //openreview.net/forum?id=2GmDdhBdDk. Patil, S. G., Mao, H., Yan, F., Ji, C. C.-J., Suresh, V., Stoica, I., and Gonzalez, J. E. The berkeley function calling leaderboard (BFCL): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning, 2025b. URL https: //openreview.net/forum?id=2GmDdhBdDk. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. URL https: //openreview.net/forum?id=Ti67584b98. Roucher, A., del Moral, A. V., Wolf, T., von Werra, L., and Kaunismäki, E. smolagents: smol library to build great agentic systems. https://github.com/ huggingface/smolagents, 2025. Significant Gravitas. Autogpt. https://github.com/ Significant-Gravitas/AutoGPT, 2023. Accessed: 2026-01-21. Song, Y., Wang, G., Li, S., and Lin, B. Y. The good, the bad, and the greedy: Evaluation of LLMs should not ignore non-determinism. In Chiruzzo, L., Ritter, A., and Wang, L. (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 41954206, Albuquerque, New Mexico, April 2025. AsISBN 979sociation for Computational Linguistics. 8-89176-189-6. doi: 10.18653/v1/2025.naacl-long. 211. URL https://aclanthology.org/2025. naacl-long.211/. 10 The Necessity of Unified Framework for LLM-Based Agent Evaluation Team, G., Georgiev, P., Lei, V. I., Burnell, R., Bai, L., Gulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S., Mariooryad, S., Ding, Y., Geng, X., Alcober, F., Frostig, R., Omernick, M., Walker, L., Paduraru, C., Sorokin, C., Tacchetti, A., Gaffney, C., Daruki, S., Sercinoglu, O., Gleicher, Z., Love, J., Voigtlaender, P., Jain, R., Surita, G., Mohamed, K., Blevins, R., Ahn, J., Zhu, T., Kawintiranon, K., Firat, O., Gu, Y., Zhang, Y., Rahtz, M., Faruqui, M., Clay, N., Gilmer, J., CoReyes, J., Penchev, I., Zhu, R., Morioka, N., Hui, K., Haridasan, K., Campos, V., Mahdieh, M., Guo, M., Hassan, S., Kilgour, K., Vezer, A., Cheng, H.-T., de Liedekerke, R., Goyal, S., Barham, P., Strouse, D., and others. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. URL https://arxiv.org/abs/2403.05530. Team, G., Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., and others. Gemini: family of highly capable multimodal models, 2025. URL https://arxiv.org/abs/2312.11805. Wang, L., Xu, W., Lan, Y., Hu, Z., Lan, Y., Lee, R. K.- W., and Lim, E.-P. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 26092634, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.147. URL https: //aclanthology.org/2023.acl-long.147/. Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=_VjQlMeSB_J. Wei, J., Sun, Z., Papay, S., McKinney, S., Han, J., Fulford, I., Chung, H. W., Passos, A. T., Fedus, W., and Glaese, A. Browsecomp: simple yet challenging benchmark for browsing agents, 2025. URL https://arxiv.org/ abs/2504.12516. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. Transformers: State-of-the-art natural language processing. In Liu, Q. and Schlangen, D. (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https:// aclanthology.org/2020.emnlp-demos.6/. Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., Zheng, R., Fan, X., Wang, X., Xiong, L., Zhou, Y., Wang, W., Jiang, C., Zou, Y., Liu, X., Yin, Z., Dou, S., Weng, R., Qin, W., Zheng, Y., Qiu, X., Huang, X., Zhang, Q., and Gui, T. The rise and potential of large language model based agents: survey. Sci. China Inf. Sci., 68(2), 2025. URL https: //doi.org/10.1007/s11432-024-4222-0. Yao, S., Chen, H., Yang, J., and Narasimhan, K. R. Webshop: Towards scalable real-world web interaction with grounded language agents. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id=R9KnuFlvnU. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning In The Eleventh Inand acting in language models. ternational Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=WE_vluYUL-X. Yao, S., Shinn, N., Razavi, P., and Narasimhan, K. R. {$tau$}-bench: benchmark for underline{T}oolunderline{A}gent-underline{U}ser interaction in realworld domains. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=roNSXZpUDN. Yu, M., Meng, F., Zhou, X., Wang, S., Mao, J., Pan, L., Chen, T., Wang, K., Li, X., Zhang, Y., An, B., and Wen, Q. survey on trustworthy llm agents: In Proceedings of the Threats and countermeasures. 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2, KDD 25, pp. 62166226, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400714542. doi: 10.1145/ 3711896.3736561. URL https://doi.org/10. 1145/3711896.3736561. Zheng, L., Yin, L., Xie, Z., Sun, C., Huang, J., Yu, C. H., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., Barrett, C., and Sheng, Y. SGLang: Efficient execution of structured language model programs. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/ forum?id=VqkAKQibpq. Zhu, K., Du, H., Hong, Z., Yang, X., Guo, S., Wang, Z., Wang, Z., Qian, C., Tang, R., Ji, H., and You, J. MultiAgentBench : Evaluating the collaboration and competition of LLM agents. In Che, W., Nabende, J., 11 The Necessity of Unified Framework for LLM-Based Agent Evaluation Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 85808622, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-2510. doi: 10.18653/v1/2025.acl-long.421. URL https: //aclanthology.org/2025.acl-long.421/."
        }
    ],
    "affiliations": [
        "Beijing University of Posts and Telecommunications",
        "Chongqing University of Posts and Telecommunications",
        "University of Illinois Chicago"
    ]
}