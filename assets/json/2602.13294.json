{
    "paper_title": "VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction",
    "authors": [
        "Jiarong Liang",
        "Max Ku",
        "Ka-Hei Hui",
        "Ping Nie",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Evaluating whether Multimodal Large Language Models (MLLMs) genuinely reason about physical dynamics remains challenging. Most existing benchmarks rely on recognition-style protocols such as Visual Question Answering (VQA) and Violation of Expectation (VoE), which can often be answered without committing to an explicit, testable physical hypothesis. We propose VisPhyWorld, an execution-based framework that evaluates physical reasoning by requiring models to generate executable simulator code from visual observations. By producing runnable code, the inferred world representation is directly inspectable, editable, and falsifiable. This separates physical reasoning from rendering. Building on this framework, we introduce VisPhyBench, comprising 209 evaluation scenes derived from 108 physical templates and a systematic protocol that evaluates how well models reconstruct appearance and reproduce physically plausible motion. Our pipeline produces valid reconstructed videos in 97.7% on the benchmark. Experiments show that while state-of-the-art MLLMs achieve strong semantic scene understanding, they struggle to accurately infer physical parameters and to simulate consistent physical dynamics."
        },
        {
            "title": "Start",
            "content": "VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction Jiarong Liang * 1 Max Ku * 1 Ka-Hei Hui 2 Ping Nie 3 Wenhu Chen 1 1University of Waterloo 2Autodesk AI Lab 3Independent https://tiger-ai-lab.github.io/VisPhyWorld/ Abstract Evaluating whether Multimodal Large Language Models (MLLMs) genuinely reason about physical dynamics remains challenging. Most existing benchmarks rely on recognition-style protocols such as Visual Question Answering (VQA) and Violation of Expectation (VoE), which can often be answered without committing to an explicit, testable physical hypothesis. We propose VisPhyWorld, an execution-based framework that evaluates physical reasoning by requiring models to generate executable simulator code from visual observations. By producing runnable code, the inferred world representation is directly inspectable, editable, and falsifiable. This separates physical reasoning from rendering. Building on this framework, we introduce VisPhyBench, comprising 209 evaluation scenes derived from 108 physical templates and systematic protocol that evaluates how well models reconstruct appearance and reproduce physically plausible motion. Our pipeline produces valid reconstructed videos in 97.7% on the benchmark. Experiments show that while state-of-the-art MLLMs achieve strong semantic scene understanding, they struggle to accurately infer physical parameters and to simulate consistent physical dynamics. 6 2 0 2 9 ] . [ 1 4 9 2 3 1 . 2 0 6 2 : r 1. Introduction Recent advances in Multimodal Large Language Models (MLLM) have led to impressive performance on wide range of visual and language tasks (Fu et al., 2024). However, assessing whether such models exhibit principled physical reasoning remains challenging. Existing evalu- *Equal contribution 1University of Waterloo 2Autodesk AI Lab 3Independent. Correspondence to: Jiarong Liang <jiarongliangcs@gmail.com>, Max Ku <m3ku@uwaterloo.ca>, Wenhu Chen <wenhu.chen@uwaterloo.ca>. Preprint. February 17, 2026. 1 Figure 1. MLLMs struggle to simulate physical dynamics. Under the same inputs, code generated with rigid-body simulation backends (Three.js/P5.js) produces more physically consistent rollouts, whereas non-physics backends (SVG/Manim) often exhibit implausible motion or contact artifacts such as interpenetration. ation protocols often rely on recognition-based queries or surface-level judgments, which can obscure whether correct outputs arise from coherent physical reasoning or from learned visual correlations (Chen et al., 2023; Shen et al., 2025). Most benchmarks probe physical understanding through passive recognition tasks such as Visual Question Answering (VQA)-style or Violation of Expectation (VoE)- inspired recognition tasks (e.g. CLEVRER (Yi et al., 2020), GRASP (Jassim et al., 2024), MVPBench (Krojer et al., 2025))). These settings can reward dataset-driven guessing, encouraging memorized priors and surface-level pattern matching rather than genuine causal physical understanding (Pezeshkpour & Hruschka, 2023; Keluskar et al., 2024). This challenge is particularly acute for MLLMs, which typically output only text and therefore do not provide predictive likelihoods or measures of surprise commonly used to evaluate generative world models (Garrido et al., 2025). We therefore argue that in this context, evaluation should require reconstruction and re-simulation, forcing models to commit to an explicit physical visuals rather than merely select an answer or text reasoning. We propose VisPhyWorld, paradigm shift: using executable code as test of physical understanding, as illustrated in Figure 2. VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction Figure 2. Unlike traditional VQA paradigms, VisPhyWorld accesses physical understanding evaluation by requiring MLLMs to actively reconstruct scenes via executable code, offering superior reasoning explainability compared to traditional paradigms. VisPhyWorld probes the physical reasoning capabilities of MLLMs through visual-to-code reconstruction. Given two key frames (and optionally object detections), the model produces executable simulation code that recreates the scene and rolls it forward to synthesize future frames as shown in Figure 3. This process not only produces the video but does so in fully interpretable and editable manner. Beyond the rendered video, VisPhyWorld exposes the generated code itself as reasoning artifact, making the models physical logic directly inspectable. We also introduce VisPhyBench, standardized evaluation suite with systematic protocol that assesses how well models reconstruct appearance and reproduce physically plausible motion across complementary perspectives. Our investigation reveals critical insight: while current stateof-the-art LLMs excel at semantic recognition, they exhibit significant limitations in fine-grained physical comprehension, often failing to parameterize simple Newtonian dynamics correctly even in simple 2D setting, let alone in 3D environments. In summary, our contributions are threefold: (1) We propose VisPhyWorld, framework that uses LLMs to interpret raw video frames and generate executable simulation code for predicting future motion. To our knowledge, this is the first paradigm that evaluates physical reasoning in MLLMs through code reconstruction and re-simulation. By making object states and dynamics explicit, VisPhyWorld provides direct and interpretable view of models physical understanding. 2 (2) We introduce VisPhyBench, unified evaluation protocol comprising 209 scenes derived from 108 physical templates that assesses physical understanding through the lens of code-driven resimulation in both 2D and 3D scenes, integrating metrics from different aspects. (3) We provide an in-depth analysis of current MLLM, demonstrating that despite their linguistic prowess, they fail to grasp the fundamental dynamics of realworld motion. Our results reveal critical gap: while models can accurately describe scene contents, they struggled to reconstruct the scene in way that conformed to the laws of physics, indicating that they rely on superficial visual pattern matching rather than grounded understanding of physical causality. Table 1. VisPhyWorld uniquely turns physical reasoning into an executable hypothesis and enables multimetric, diagnostic evaluation beyond relative scoring. Benchmark Future Visual Generation Evaluates MLLM Outputs Executable Hypothesis PHYRE (Bakhtin et al., 2019) CLEVRER (Yi et al., 2020) IntPhys (Riochet et al., 2020) PhyGenBench (Meng et al., 2024) MVP (Krojer et al., 2025) PhysicsIQ (Motamed et al., 2025b) WorldModelBench (Li et al., 2025) IntPhys2 (Bordes et al., 2025) PhyWorld (Kang et al., 2025) VisPhyWorld (Ours) Evaluation Paradigm Relative (Actions) Relative (QA) Relative (VoE) Relative (QA) Relative (QA) Relative (QA) Absolute (VLM Judge) Relative (VoE) Reconstruction (Video) Reconstruction (Code) VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction Figure 3. VisPhyWorld Framework. (1) System & Data Construction: We process raw video sequences to extract key frames (Istart, Ilater) and detection contexts using multimodal agents. (2) Pipeline & Simulation Flow: An LLM-based agent performs motion analysis and generates raw executable code, which is then sanitized and rendered. (3) Evaluation Benchmark: We propose multi-metric benchmark integrating semantic and physical fidelity to compare generated videos ˆX with ground truth X. (4) Detailed Case: example illustrating how VisPhyWorld translates collision scene (red ball hits block stack) into executable simulation logic. 2. Related Work Intuitive physics. Understanding the world is commonly studied through physical reasoning tasks that probe models ability to infer object dynamics, interactions, and causal relationships from visual input (Melnik et al., 2023; Fung et al., 2025). Inspired by findings from developmental psychology showing that infants exhibit sensitivity to physical violations (Baillargeon et al., 1985), prior work on intuitive physics investigates whether models can anticipate physically plausible outcomes from visual observations. This has been studied through video prediction benchmarks that evaluate the consistency of predicted future dynamics, as well as Violation-of-Expectation (VoE) paradigms (Riochet et al., 2020; Margoni et al., 2024; Jassim et al., 2024), which assess whether physically implausible events elicit higher predictive surprise. These approaches are well suited to generative world models with explicit prediction objectives. However, they do not naturally extend to MLLMs, which primarily produce textual outputs rather than predictive distributions and therefore cannot be evaluated using likelihood-based or generative video protocols (Garrido et al., 2025). Efforts on several prominent datasets and benchmarks have been made (Rajani et al., 2020; Yi et al., 2020; Baradel et al., 2020), including Phyre (Bakhtin et al., 2019; Li et al., 2024), Physion (Bear et al., 2022; Tung et al., 2023), and IntPhys (Riochet et al., 2020; Bordes et al., 2025), have been proposed to evaluate intuitive physics using videos generated from physics engines. More recent benchmarks such as PhysicsIQ (Motamed et al., 2025b), PhyGenBench (Meng et al., 2024), and WorldModelBench (Li et al., 2025) extend this setting to generative video models, focusing on whether predicted videos exhibit physically plausible and temporally consistent dynamics. In parallel, researchers have developed MLLMbased evaluators (Motamed et al., 2025a), such as VideoPhy (Bansal et al., 2024; 2025) and VideoScore (He et al., 2024b; 2025), to assess physical understanding in multimodal models. These approaches typically formulate evaluation as recognition-based tasks like VQA. While effective for probing high-level physical knowledge, such protocols make it difficult to determine whether model performance reflects genuine physical reasoning or reliance on appearance-based heuristics and dataset-specific biases. Our framework complements previous works by requiring explicit and executable physical hypotheses evaluated through simulation. Table 1 compares our work with prior works. Executable World Representations for Visual and Motion Generation. Representing visual scenes as executable programs is foundational paradigm in computer graphics and simulation, where structured code specifies objects, motion, and physical interactions to enable interpretable and controllable world representations (Foley et al., 1996). Recent advances in multimodal large language models have 3 VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction begun to enable the generation of executable code for visual content. Early efforts primarily focus on static visualizations, such as data plots and vector graphics, translating high-level semantic intent into low-level graphical instructions (Galimzyanov et al., 2024; Yang et al., 2024; Goswami et al., 2025; Ni et al., 2025b;a; Rodriguez et al., 2024; Yang et al., 2025; Lin et al., 2025). These methods demonstrate the feasibility of using code as structured intermediate between language and visual output. Subsequent work extends code-based generation to animations and motion, enabling programmatic specification of object trajectories and temporal behaviors (Zhang et al., 2023; He et al., 2024a; Liu et al., 2024; Lv et al., 2024; Ku et al., 2025). While these approaches show that MLLM can generate executable programs that produce coherent motion, they are primarily designed for content creation or presentation, and rarely assess whether the generated programs correspond to physically consistent dynamics or reflect an underlying understanding of physical laws. In contrast to prior work that treats executable visual generation as an end goal, our work uses executable world representations as diagnostic interface for physical reasoning. Rather than evaluating visual realism or animation quality, we assess whether models can reconstruct and resimulate physically consistent dynamics from visual observations to enable direct inspection. 3. VisPhyWorld We introduce VisPhyWorld, framework that uses MLLM to interpret visual observations and reconstruct the underlying physical scene as executable code. We evaluate the rendered outputs under unified protocol using multimetric suite. 3.1. Problem Definition We focus on 2D and 3D physical scenes involving common interactions, e.g., ball collisions and box sliding. We represent each scene as sequence of image frames with three color channels as in Equation 1, where H, , and denote frame height, frame width, and number of frames, respectively. = (It)T t=1, It R3HW , (1) Input. Given scene, the MLLM backbone receives {I start, later, D}. We select two key frames from X, where start = Its and later = Itl , typically corresponding to an early frame and later frame (e.g., ts = 1, tl = 10). Optionally, we provide detection context for start listing objects with categories, bounding boxes, and coarse attributes (details in Appendix B.2). We obtain with GPT-5.2 (OpenAI, 2025c) on the first frame and parsing its output into structured object list; if unavailable, we set = . Outputs. VisPhyWorld produces four interpretable artifacts: (i) textual motion analysis text; (ii) machinereadable first-frame JSON specification encoding object 4 LPIPS Gemini CLIP-Img RAFT-EPE DINO BERTScore-F1 CLIP-Cap GPT-5 (threejs) Qwen3-VL-Plus (threejs) GPT-4.1 (threejs) SVD img2vid Gemini 3 Pro (threejs) Veo 3.1 Claude 4.5 Sonnet (threejs) Metric GPT-5 GPT-4.1 Gemini 3-Pro Claude 4.5 Qwen3 VL SVD Veo-3.1 LPIPS 0.1736 CLIP-Img 0.8930 DINO 0.8556 CLIP-Cap 0.2632 BERTScore-F1 0.8436 RAFT-EPE 33.65 Gemini 3. 0.1818 0.8933 0.8304 0.2610 0.8522 33.71 3.06 0.1399 0.8973 0.8405 0.2567 0.8460 36.20 3.80 0.1602 0.8957 0.8305 0.2588 0.8468 36.20 2.39 0.2207 0.3408 0.8717 0.6677 0.7837 0.6528 0.2650 0.2533 0.8466 35.05 2.12 N/A 45.46 1.43 0.2102 0.8564 0.8839 0.2681 N/A 32.71 2. Figure 4. Key metrics on VisPhyBench. We compare code-driven reconstruction (multiple MLLMs) against pixel-space baselines (Veo 3.1 and SVD) under the unified evaluation protocol. layout and inferred parameters; (iii) an executable program code; and (iv) rendered video ˆX = ( ˆIt) ˆT t=1 obtained by executing the executable program C. 3.2. VisPhyWorld Architecture (I start, later, D) fLLM (A, S, C) Rphys ˆX. (2) VisPhyWorld implements composite mapping as stated in Equation 2. We include as lightweight, text-only diagnostic of the models basic scene understanding: whether it can correctly describe salient motions and interactions between the key frames, separately from code generation. We treat as an explicit, falsifiable physical hypothesis: executing it with renderer Rphys under fixed configuration yields ˆX, separating hypothesis construction from execution and enabling controlled comparisons across LLM backbones. To ensure well-defined evaluation, we apply lightweight validation prior to execution and allow single automatic repair attempt upon failure; if both attempts fail, we fall back to minimal valid scene. Further implementation details, including prompt templates and renderer settings, are deferred to Appendix B, with robustness analyses in Appendix B.5. 3.3. Benchmark, Metrics, and Baselines Dataset Construction. We build on and extend the 2D data from the PhyWorld dataset (Kang et al., 2025), using the PHYRE engine (Bakhtin et al., 2019) for rendering to form VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction Table 2. Overall leaderboard on VisPhyBench. Columns are grouped into: reconstructionperceptual quality (LPIPS), visual semantic consistency (CLIP-Img, DINO), textvideo and analysis-text consistency (CLIP-Cap, BERTScore-F1), motion / physical plausibility (RAFT-EPE), and holistic overall quality (Gemini). Higher is better (), lower is better (). denotes metrics that are unavailable or not applicable for given method. Model Ours (GPT-5, threejs) Ours (GPT-5, p5js) Ours (GPT-4.1, threejs) Ours (GPT-4.1, p5js) Ours (Gemini-3-Pro, threejs) Ours (Gemini-3-Pro, p5js) Ours (Claude Sonnet 4.5, threejs) Ours (Claude Sonnet 4.5, p5js) Ours (Qwen3-VL-Plus, threejs) Ours (Qwen3-VL-Plus, p5js) SVD (img2vid) Veo-3.1 Reconst. & Perceptual Visual Semantic Consistency & Analysis-Text Motion / Physical Plausibility Holistic Quality LPIPS CLIP-Img DINO CLIP-Cap BERTScore-F1 RAFT-EPE Gemini TextVideo 0.1736 0.2926 0.1818 0.3520 0.1399 0.3302 0.1602 0.3250 0.2207 0.5478 0.3408 0.2102 0.8930 0.8134 0.8933 0.7545 0.8973 0.7460 0.8957 0.7612 0.8717 0.6446 0.6677 0.8564 0.8556 0.7580 0.8304 0.6786 0.8405 0.6721 0.8305 0.7098 0.7837 0.5478 0.6528 0. 0.2632 0.2331 0.2610 0.2192 0.2567 0.2184 0.2588 0.2177 0.2650 0.2032 0.2533 0.2681 0.8436 0.8360 0.8522 0.8253 0.8460 0.8396 0.8468 0.8224 0.8466 0.8358 33.6473 34.3433 33.7110 37.6993 36.2030 33.1013 36.1985 34.1425 35.0493 20.8187 45.4606 32. 3.50 3.52 3.06 2.15 3.80 2.35 2.39 2.56 2.12 1.46 1.43 2.62 the 2D subset of VisPhyBench. We additionally curate 3D subset rendered with Three.js and simulated using Cannon.js for rigid-body dynamics. Overall, VisPhyBench comprises 108 templates and 209 videos, each paired with first-frame JSON annotations. VisPhyBench scenes are annotated with coarse difficulty levels, as summarized in Table 3. We construct small test split by subsampling from the full dataset to enable rapid sanity checks and lightweight evaluation. Eight STEM-trained annotators rate each raw clip on 15 scale (higher indicates greater difficulty), and we use the mean rating as the final difficulty score. The mean score is then mapped to easy, medium, or hard using fixed, interpretable cutoffs aligned with the rating scale (easy = 12, medium = 3, hard = 45). The resulting distribution is naturally skewed, reflecting the relative rarity of challenging interactions in our template set. Scenes cover diverse object configurations (stacks, ramps, collisions) and motion patterns (slides, bounces, topples). For the 2D subset, the camera is fixed and orthographic; for the 3D subset, we use fixed perspective camera. In both settings, the background is set to white to focus on physical dynamics. Since templates are executable programs instantiated by sampling seeds, we summarize template composition and object statistics in Appendix B.3. Inputs (I start, later, D) follow Section 3.1. Table 3. Difficulty stratification of VisPhyBench splits Split Easy Medium Hard sub (209) test (49) 114 29 67 17 28 3 Evaluation Metrics. We report per-metric means over all (1) Reconscenes and group metrics into five families. struction and perceptual quality. We report PSNR (Huynh-Thu & Ghanbari, 2008) and SSIM (Wang et al., 2004) for frame-wise reconstruction, together with LPIPS (Zhang et al., 2018), FSIM (Zhang et al., 2011), VSI (Zhang et al., 2014), and DISTS (Ding et al., 2020) to compute on aligned (2) Visual semantic consistency. We comframe pairs. pute CLIP-based image similarity (CLIP-Img) (Radford et al., 2021) and DINO feature similarity (Caron et al., 2021), which emphasize object identity and scene layout (3) Textvideo and analysis-text conbeyond exact pixels. sistency. We compute CLIP textimage similarity (CLIP Cap) (Radford et al., 2021) between the analysis and sampled video frames, and use ROUGE-L (Lin, 2004) and BERTScore-F1 (Zhang et al., 2020) to compare the analysis with GPT-generated reference description derived from the (4) Motion and physical plausibility. ground-truth video. We use RAFT-based optical-flow diagnostics (Teed & Deng, 2020) with automatic temporal alignment, reporting RAFT end-point error (EPE) and the estimated temporal offset (and, when relevant, flow magnitude and angular statistics) to quantify motion consistency. Because flow discrepancy alone can be misleading as proxy for physical plausibility, we interpret RAFT metrics jointly with holistic perceptual/- physics judgments rather than using RAFT-EPE in isolation; (5) Subjective overall quality. as discussed in Section 4. We use Gemini-2.5-Pro videovideo judge (110) with short textual justification, and separately report pipeline success rate based on whether valid video is produced. Video Model Baselines. We include Stable Video Diffusion (SVD) img2vid (Blattmann et al., 2023), conditioned only on start, and Veo-3.1, conditioned on (I start, later, prompt) in pixel space. 3.4. Engine Evaluation and Selection We evaluate four rendering backends, i.e., Three.js (three.js contributors, 2026), P5.js (p5.js contributors, 2026), SVG (Scalable Vector Graphics), and Manim (The Manim Community Developers, 2024), to understand how the choice of visualization engine affects multimodal LLM-based reconstruction. As shown in Figure 1, consistent pattern emerges: Three.js and P5.js achieve markedly better reconstruction and motion fidelity because they support native integration with rigid-body physics solvers, allowing the 5 VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction generated programs to offload gravity, contact constraints, friction, and collision response to physically grounded engine. In contrast, SVG and Manim are primarily nonphysics-based rendering systems: they excel at deterministic drawing and scripted animation, but lack intrinsic rigid-body dynamics. In our experimental setting, SVG and Manim serve as non-interactive, script-based backends and do not expose comparable physics API or closed-loop simulation stepping; consequently, as illustrated in Figure 1, they often yield physically implausible behaviors, such as objects remaining static or interpenetrating. Importantly, this gap suggests limitation of current MLLM: without access to true physics solver, they fail to consistently infer and apply Newtonian dynamics from visual evidence, and instead revert to heuristic motion scripting. For this work, we therefore prioritize Three.js and P5.js so that our evaluation emphasizes physically grounded re-simulation rather than non-physical animation artifacts. 4. Experiments Evaluation setup. We evaluate VisPhyWorld and all baselines on VisPhyBench. For each configuration, we generate one video per scene, compute all metrics, and report per-metric means over the evaluation split; unless otherwise stated, higher is better. We consider five multimodal LLM backbones: GPT-5 (OpenAI, 2025b), GPT-4.1 (OpenAI, 2025a), Gemini-3-Pro (Google AI for Developers, 2026), Claude Sonnet 4.5 (Anthropic, 2025), and Qwen3VL-Plus (Alibaba Cloud, 2026). We evaluate two code backends, Three.js (three.js contributors, 2026) and P5.js (p5.js contributors, 2026). All LLM runs use the same prompt and two key frames; only the model and engine identifiers change. For each run, we aggregate metrics into five families: reconstruction & perceptual quality, visual semantic consistency, textvideo & analysis-text consistency, motion (automatic RAFT-based metrics (Teed & Deng, 2020)), and subjective overall quality (Gemini-2.5-Pro (Google, 2025) judge). We observe consistent trends across scenes; perscene metric distributions and significance analyses are reported in Appendix D.2, Fig. 21. Overall leaderboard. Table 2 summarizes performance across five metric families on VisPhyBench, while Table 4 reports pixel-space fidelity (PSNR, SSIM) and execution success rates. Overall, most models achieve strong reconstruction and perceptual scores and maintain reasonable visual-semantic consistency; these results support our central claim that, once the task is cast as executable hypotheses under fixed physics engine, most modern MLLM can reconstruct synthetic physical events with high fidelity, and the remaining gaps become diagnosable rather than opaque. First, our benchmark jointly evaluates visual reconstruction/semantics and language-mediated reasoning, and we observe that these two dimensions can dissociate across models. Some backends achieve the strongest perceptual Table 4. Average PSNR/SSIM and generation success rate. Model Engine PSNR SSIM Success Three.js p5.js Three.js p5.js Three.js p5.js GPT-5 GPT-5 GPT-4.1 GPT-4.1 Gemini-3-Pro Gemini-3-Pro Claude Sonnet 4.5 Three.js Claude Sonnet 4.5 p5.js Qwen3-VL-Plus Qwen3-VL-Plus Three.js p5.js SVD (img2vid) Veo-3.1 20.54 16.36 19.74 14.83 21.26 15.57 20.75 15.36 18.66 9.14 14.44 20.04 0.9370 0.7440 0.9337 0.6830 0.9445 0.6943 0.9406 0.7160 0.9306 0.4296 0.8802 0. 0.990 0.979 0.948 1.000 0.957 0.963 0.995 1.000 0.936 1.000 1.000 1.000 and semantic alignment to the reference frames, with low LPIPS and high CLIP and DINO, indicating that they are effective at extracting correct object identities and global layouts from the visual input. For example, Gemini-3-Pro (threejs) attains the lowest LPIPS together with the highest CLIP-Img, and it also yields the strongest pixel-level reconstruction in Table 4. In contrast, Veo-3.1 does not produce an executable simulator and thus lacks interpretable intermediate states for diagnosis. Others attain the best analysis-text agreement, suggesting stronger descriptive and causal narration even when perceptual scores are not the top: GPT-4.1 (threejs) achieves the highest BERTScore-F1 despite higher LPIPS than Gemini-3-Pro (threejs). This dissociation implies that the benchmark is not merely measuring overall model quality; instead, it teases apart seeing the scene from explaining it. Second, the choice of code backend affects reconstruction quality. Across LLMs, Three.js variants yield better perceptual reconstruction than their P5.js counterparts, as reflected by lower LPIPS in most pairs, despite sharing the same conditioning inputs and prompt. Concretely, for GPT-5, switching to Three.js reduces LPIPS error by nearly 40% (0.29 0.17) and boosts structural similarity (SSIM) from 0.74 to 0.94. Visually, this corresponds to better preservation of object identity, as illustrated in Figure 1. Since the physics engine is fixed, this performance gap confirms that the simulators expressivity affects the models ability to ground visual evidence. In other words, program structure and simulator interface materially affect how well model can translate visual evidence into stable physical hypothesis. Third, pixel-space baselines exhibit complementary profile: they can score competitively on some frame-feature semantics, but their failures are harder to attribute to specific physical causes, such as friction, restitution, or contact timing, since the generation process does not expose interpretable latent variables. Veo-3.1 attains reasonable semantic similarity, for example reaching DINO 0.88, yet it does not expose an explicit simulator state for diagnosis 6 VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction Table 5. Text analysis consistency on the VisPhyBench. We compare model analyses against GPT-5.1-generated reference descriptions using ROUGE-L and BERTScore. Backbone Tool ROUGE-L F1 BERTScore F1 Three.js p5.js Three.js p5.js Three.js p5.js GPT-5 GPT-5 GPT-4.1 GPT-4.1 Gemini-3-Pro Gemini-3-Pro Claude Sonnet 4.5 Three.js Claude Sonnet 4.5 p5.js Qwen3-VL-Plus Qwen3-VL-Plus Three.js p5.js 0.2186 0.2057 0.2383 0.1689 0.2141 0.1886 0.2168 0.1599 0.2022 0.1733 0.8436 0.8360 0.8522 0.8253 0.8460 0.8396 0.8468 0.8224 0.8466 0.8358 or controlled interventions and often exhibits deficiencies in physical understanding by producing trajectories with implausible motion or contact events(see Sec. 4.2). Conversely, our code-driven approach maintains competitive semantic and motion scores while exposing executable states; e.g., GPT-5 (threejs) achieves DINO 0.8556 with RAFT-EPE 33.6473. This enables controlled interventions (e.g., varying friction/mass while holding layout fixed) that can isolate whether an error originates from object discovery, state initialization, or contact modeling, aligning with our goal of turning physics understanding into testable, executable object. Finally, we report holistic quality using Gemini-2.5-Pro judge (see Appendix C.6), which aggregates multiple perceptual and physical cues into single preference signal. This holistic score aligns with strong visual alignment for some backends. For example, Gemini-3-Pro (threejs) reaches the highest Gemini score, 3.80, while penalizing visibly implausible or degraded generations. For instance, Qwen3-VL-Plus (p5js) scores 1.46 alongside poor perceptual/semantic alignment. This judge complements the automatic metrics by capturing visually salient failure modes (e.g., missing motion, implausible contacts) that may not be fully reflected in any single metric family. Together, these results indicate that VisPhyBench and our metric suite jointly provide multi-view, diagnostic measurement of LLM visual and physical competence under executable simulation. Motion and physical plausibility. Assessing physical plausibility requires balancing raw motion statistics with perceptual coherence. While RAFT-EPE measures optical flow discrepancy, relying on it in isolation can be misleading; for instance, Qwen3-VL-Plus in P5.js attains the lowest RAFT-EPE, 20.82, despite poor reconstruction fidelity in Table 4. Consequently, we adopt joint evaluation strategy: model is considered to demonstrate valid physical understanding only when it achieves favorable RAFT-EPE, which reflects motion-trajectory agreement as detailed in Appendix C, and high Gemini holistic score, which reflects 7 Figure 5. This case shows that VisPhyWorld exhibits strong physical grounding, correctly simulating the collision dynamics. More examples are in the Appendix. Table 6. Ablation on iterative self-repair (retry) on the VisPhyBench. No retry counts only samples that succeed on the first generation+render attempt; 1 retry allows one additional attempt with renderer error feedback appended to the prompt. Engine Success (no retry) Success (1 retry) Fixed by retry Three.js P5.js 0.979 0.853 0.990 0.979 0.010 0. perceptually coherent outcomes under physics-focused rubric in Appendix C.6. Furthermore, the Gemini evaluator returns textual justification that explicitly comments on physical plausibility, including collisions, contact consistency, and implausible motion, providing qualitative sanity check alongside the quantitative flow metrics. 4.1. Ablation on iterative self-repair (retry) VisPhyWorld includes an iterative self-repair mechanism: if the first generation+render attempt fails, we append concise renderer error log tail and the previous attempt to the next LLM call and retry once. Table 6 reports the success rate on the VisPhyBench with and without this retry mechanism. Overall, the retry mechanism provides substantial gains, suggesting that most failures are due to correctable surface-level issues (e.g., missing canvas hooks, minor API misuse, or initialization errors) rather than irrecoverable scene-understanding errors. VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction Figure 6. GPT-5 reconstructs object identities and collision dynamics most faithfully over time. Pixel-space baselines (Veo-3.1 and SVD/img2vid) generate trajectories with implausible motion/contact events due to the lack of an explicit physics hypothesis. Figure 7. This example highlights the dissociation between semantic alignment and correct physical dynamics: although Claude shows clear reconstruction deficits, its visual-semantic scores remain relatively high. 4.2. Case Study We present diagnostic case study, shown in Figs. 5 and 6, featuring gravity-driven multi-body interactions that require precise physical reasoning. GPT-5 in Three.js shows strong physical grounding by correctly simulating the collision dynamics, achieving Gemini 10.0 with DINO 0.926. In contrast, pixel-space baselines such as Veo-3.1 achieve high semantic similarity, reaching DINO 0.835, but fail on event logic with Gemini 2.0, indicating plausible appearance with hallucinated dynamics. The case also motivates joint evaluation: Qwen3-VL-Plus attains low RAFT-EPE, 121.30 versus 118.66 for GPT-5, by producing static or empty outputs, yet is penalized by Gemini with score of 4.0. Together, these results show that optical-flow errors alone are insufficient; credible physical understanding requires both correct motion and holistic visual coherence. Figure 7 extends our diagnostic analysis beyond 2D templates to perspective-rendered 3D scene with depthdependent contacts and occlusions. Consistent with our 2D findings, we observe the same conclusion in 3D: strong appearance matching alone does not guarantee physically faithful dynamics. Valid physical understanding is only evidenced when both motion dynamics and holistic visual coherence are simultaneously satisfied. For example, Claude4.5 and Qwen3-VL-Plus exhibit clear reconstruction deviations in this sample, yet their visual-semantic scores do not separate substantially from other models, highlighting dissociation between semantic alignment and correct physical dynamics. More broadly, the 3D setting is noticeably more challenging for current MLLMs, underscoring the necessity of incorporating 3D scenes when evaluating reconstructionbased physical reasoning. Limitations and Discussion While VisPhyWorld shows promising results on physicsaware video generation and evaluation, it has several limitations. First, our experiments are conducted on synthetic, simulator-driven scenes with controlled object layouts and camera motion, so generalization to high-resolution, in-thewild videos remains untested. Fundamentally limited by the capabilities of current MLLM and the complexity of modern engines, VisPhyWorld can reliably generate code only for relatively simple rigid-body scenes: although we experimented with large engines such as Unreal Engine and Blender, we found that, without human intervention, existing MLLMs cannot, within small fixed number of calls, autonomously produce and repair simulation code to render stable, visually plausible video in these more complex environments. Finally, we currently target relatively short clips with moderate motion complexity, and do not explicitly address long-horizon interactions, complex 3D reasoning, or stylized or heavily cluttered scenes, which we leave for future work. Future work could integrate stronger 3D perception for scene initialization and agentic workflows with domain-specific fine-tuning. Conclusions In this work, we introduced VisPhyWorld, framework that advances the evaluation of physical understanding by requiring MLLMs to reconstruct scenes as executable code, thereby decoupling visual mimicry from physically grounded reasoning. By benchmarking state-of-the-art models on our proposed VisPhyBench, we exposed consistent dichotomy: while current models excel at semantic scene parsing, they struggle with precise physical parameterization; when forced to commit to an executable hypothesis, models that rely on pixel-space generation often fail to reproduce even basic Newtonian dynamics. Our findings suggest 8 VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction that progress toward robust world modeling may benefit from moving beyond purely statistical pattern matching in pixel space toward hybrid representations that ground visual perception in verifiable, executable physical laws. We believe this direction offers path toward more transparent and verifiable evaluations of physical understanding. VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction Impact Statement This work advances the interpretability and reliability of generative AI by transforming opaque video prediction into transparent, executable code, which is essential for deploying reliable world models in safety-critical domains like robotics. By grounding generation in explicit symbolic logic, our approach offers mechanism to audit and verify physical hallucinations, potentially mitigating risks associated with black-box simulations. While improved physical reasoning capabilities could enhance synthetic media generation, the inherent falsifiability and inspectability provided by our code-driven paradigm serve as crucial safeguard against unverifiable generation. References Alibaba Cloud. Alibaba cloud model studio: Visual understanding (qwen-vl). https://www.alibabacloud. com/help/en/model-studio/vision, 2026. Accessed: 2026-01-15. Anthropic. Claude sonnet 4.5. https://www. anthropic.com/news/claude-sonnet-4-5, 2025. Accessed: 2026-01-15. Baillargeon, R., Spelke, E. S., and Wasserman, S. Object permanence in five-month-old infants. Cognition, 20(3):191208, 1985. ISSN 0010-0277. https://doi.org/10.1016/0010-0277(85)90008-3. doi: https://www.sciencedirect.com/ URL science/article/pii/0010027785900083. Bakhtin, A., van der Maaten, L., Johnson, J., Gustafson, L., and Girshick, R. Phyre: new benchmark for physical reasoning, 2019. URL https://arxiv.org/abs/ 1908.05656. Bansal, H., Lin, Z., Xie, T., Zong, Z., Yarom, M., Bitton, Y., Jiang, C., Sun, Y., Chang, K.-W., and Grover, A. Videophy: Evaluating physical commonsense for video generation, 2024. URL https://arxiv.org/abs/ 2406.03520. Bansal, H., Peng, C., Bitton, Y., Goldenberg, R., Grover, A., and Chang, K.-W. Videophy-2: challenging actioncentric physical commonsense evaluation in video generation, 2025. URL https://arxiv.org/abs/2503. 06800. Baradel, F., Neverova, N., Mille, J., Mori, G., and Wolf, C. Cophy: Counterfactual learning of physical dynamics, 2020. URL https://arxiv.org/abs/1909. 12000. Bear, D. M., Wang, E., Mrowca, D., Binder, F. J., Tung, H.-Y. F., Pramod, R. T., Holdaway, C., Tao, S., Smith, K., Sun, F.-Y., Fei-Fei, L., Kanwisher, N., Tenenbaum, J. B., Yamins, D. L. K., and Fan, J. E. Physion: Evaluating physical prediction from vision in humans and machines, 2022. URL https://arxiv.org/abs/2106.08261. Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., Jampani, V., and Rombach, R. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023. URL https://arxiv.org/abs/ 2311.15127. Bordes, F., Garrido, Q., Kao, J. T., Williams, A., Rabbat, M., and Dupoux, E. Intphys 2: Benchmarking intuitive physics understanding in complex synthetic environments, 2025. URL https://arxiv.org/abs/ 2506.09849. Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging properties in self-supervised vision transformers, 2021. URL https: //arxiv.org/abs/2104.14294. Chen, W., Yin, M., Ku, M., Lu, P., Wan, Y., Ma, X., Xu, J., Wang, X., and Xia, T. Theoremqa: theorem-driven In The 2023 Conference question answering dataset. on Empirical Methods in Natural Language Processing, 2023. Ding, K., Ma, K., Wang, S., and Simoncelli, E. P. Image quality assessment: Unifying structure and texture similarity. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 11, 2020. ISSN 1939-3539. doi: 10.1109/tpami.2020.3045810. URL http://dx. doi.org/10.1109/TPAMI.2020.3045810. Foley, J. D., van Dam, A., Feiner, S. K., and Hughes, J. F. Computer Graphics: Principles and Practice. AddisonWesley, second edition, 1996. ISBN 0201848406. Fu, C., Zhang, Y.-F., Yin, S., Li, B., Fang, X., Zhao, S., Duan, H., Sun, X., Liu, Z., Wang, L., Shan, C., and He, R. Mme-survey: comprehensive survey on evaluation of multimodal llms, 2024. URL https://arxiv.org/ abs/2411.15296. Fung, P., Bachrach, Y., Celikyilmaz, A., Chaudhuri, K., Chen, D., Chung, W., Dupoux, E., Gong, H., Jegou, H., Lazaric, A., Majumdar, A., Madotto, A., Meier, F., Metze, F., Morency, L.-P., Moutakanni, T., Pino, J., Terver, B., Tighe, J., Tomasello, P., and Malik, J. Embodied ai agents: Modeling the world, 2025. URL https:// arxiv.org/abs/2506.22355. Galimzyanov, T., Titov, S., Golubev, Y., and Bogomolov, E. Drawing pandas: benchmark for llms in generating plotting code, 2024. URL https://arxiv.org/ abs/2412.02764. 10 VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction Garrido, Q., Ballas, N., Assran, M., Bardes, A., Najman, L., Rabbat, M., Dupoux, E., and LeCun, Y. Intuitive physics understanding emerges from self-supervised pretraining on natural videos, 2025. URL https://arxiv.org/ abs/2502.11831. Google. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. URL https://arxiv. org/abs/2507.06261."
        },
        {
            "title": "Google AI",
            "content": "for Developers. Gemini 3 developer guide. https://ai.google.dev/gemini-api/ docs/gemini-3, 2026. Accessed: 2026-01-15. Goswami, K., Mathur, P., Rossi, R., and Dernoncourt, F. Plotgen: Multi-agent llm-based scientific data visualization via multimodal feedback, 2025. URL https: //arxiv.org/abs/2502.00988. He, L., Song, Y., Huang, H., Aliaga, D., and Zhou, X. Kubrick: Multimodal agent collaborations for synthetic video generation, 2024a. URL https://arxiv. org/abs/2408.10453. He, X., Jiang, D., Zhang, G., Ku, M., Soni, A., Siu, S., Chen, H., Chandra, A., Jiang, Z., Arulraj, A., Wang, K., Do, Q. D., Ni, Y., Lyu, B., Narsupalli, Y., Fan, R., Lyu, Z., Lin, Y., and Chen, W. Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation, 2024b. URL https: //arxiv.org/abs/2406.15252. He, X., Jiang, D., Nie, P., Liu, M., Jiang, Z., Su, M., Ma, W., Lin, J., Ye, C., Lu, Y., Wu, K., Schneider, B., Do, Q. D., Li, Z., Jia, Y., Zhang, Y., Cheng, G., Wang, H., Zhou, W., Lin, Q., Zhang, Y., Zhang, G., Huang, W., and Chen, W. Videoscore2: Think before you score in generative video evaluation, 2025. URL https://arxiv.org/abs/ 2509.22799. Huynh-Thu, Q. and Ghanbari, M. Scope of validity of psnr in image/video quality assessment. Electronics Letters, 44:800 801, 02 2008. doi: 10.1049/el:20080522. Jassim, S., Holubar, M., Richter, A., Wolff, C., Ohmer, X., and Bruni, E. Grasp: novel benchmark for evaluating language grounding and situated physics understanding in multimodal language models, 2024. URL https: //arxiv.org/abs/2311.09048. Kang, B., Yue, Y., Lu, R., Lin, Z., Zhao, Y., Wang, K., Huang, G., and Feng, J. How far is video generation from world model: physical law perspective, 2025. URL https://arxiv.org/abs/2411.02385. 11 Keluskar, A., Bhattacharjee, A., and Liu, H. Do llms understand ambiguity in text? case study in open-world question answering, 2024. URL https://arxiv.org/ abs/2411.12395. Krojer, B., Komeili, M., Ross, C., Garrido, Q., Sinha, K., Ballas, N., and Assran, M. shortcut-aware videoqa benchmark for physical understanding via minimal video pairs, 2025. URL https://arxiv.org/abs/ 2506.09987. Ku, M., Chong, T., Leung, J., Shah, K., Yu, A., and Chen, W. Theoremexplainagent: Towards multimodal explanations for llm theorem understanding, 2025. URL https:// arxiv.org/abs/2502.19400. Li, D., Fang, Y., Chen, Y., Yang, S., Cao, S., Wong, J., Luo, M., Wang, X., Yin, H., Gonzalez, J. E., Stoica, I., Han, S., and Lu, Y. Worldmodelbench: Judging video generation models as world models, 2025. URL https: //arxiv.org/abs/2502.20694. Li, S., Wu, K., Zhang, C., and Zhu, Y. I-phyre: Interactive physical reasoning, 2024. URL https://arxiv. org/abs/2312.03009. Lin, C.-Y. ROUGE: package for automatic evaluaIn Text Summarization Branches tion of summaries. Out, pp. 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: //aclanthology.org/W04-1013/. Lin, K. Q., Zheng, Y., Ran, H., Zhu, D., Mao, D., Li, L., Torr, P., and Wang, A. J. Vcode: multimodal coding benchmark with svg as symbolic visual representation, 2025. URL https://arxiv.org/abs/2511.02778. Liu, S., Ren, Z., Gupta, S., and Wang, S. Physgen: Rigidbody physics-grounded image-to-video generation. In European Conference on Computer Vision (ECCV), 2024. Lv, J., Huang, Y., Yan, M., Huang, J., Liu, J., Liu, Y., Wen, Y., Chen, X., and Chen, S. Gpt4motion: Scripting physical motions in text-to-video generation via blenderoriented gpt planning, 2024. URL https://arxiv. org/abs/2311.12631. Margoni, F., Surian, L., and Baillargeon, R. The violationof-expectation paradigm: conceptual overview. Psychological Review, 131(3):716748, 2024. doi: 10.1037/ rev0000450. Melnik, A., Schiewer, R., Lange, M., Muresanu, A., Saeidi, M., Garg, A., and Ritter, H. Benchmarks for physical reasoning ai, 2023. URL https://arxiv.org/abs/ 2312.10728. VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction Meng, F., Liao, J., Tan, X., Shao, W., Lu, Q., Zhang, K., Cheng, Y., Li, D., Qiao, Y., and Luo, P. Towards world simulator: Crafting physical commonsense-based benchmark for video generation, 2024. URL https: //arxiv.org/abs/2410.05363. Motamed, S., Chen, M., Gool, L. V., and Laina, I. Travl: recipe for making video-language models better judges of physics implausibility, 2025a. URL https://arxiv. org/abs/2510.07550. Riochet, R., Castro, M. Y., Bernard, M., Lerer, A., Fergus, R., Izard, V., and Dupoux, E. Intphys: framework and benchmark for visual intuitive physics reasoning, 2020. URL https://arxiv.org/abs/1803.07616. Rodriguez, J. A., Puri, A., Agarwal, S., Laradji, I. H., Rodriguez, P., Rajeswar, S., Vazquez, D., Pal, C., and Pedersoli, M. Starvector: Generating scalable vector graphics code from images and text, 2024. URL https://arxiv.org/abs/2312.11556. Motamed, S., Culp, L., Swersky, K., Jaini, P., and Geirhos, R. Do generative video models understand physical principles?, 2025b. URL https://arxiv.org/abs/ 2501.09038. Ni, Y., Cai, S., Chen, X., Liang, J., Lyu, Z., Deng, J., Zou, K., Nie, P., Yuan, F., Yue, X., and Chen, W. Viscoder2: Building multi-language visualization coding agents, 2025a. URL https://arxiv.org/abs/2510.23642. Ni, Y., Nie, P., Zou, K., Yue, X., and Chen, W. Viscoder: Fine-tuning llms for executable python visualization code generation, 2025b. URL https://arxiv. org/abs/2506.03930. Shen, H., Wu, T., Han, Q., Hsieh, Y., Wang, J., Zhang, Y., Cheng, Y., Hao, Z., Ni, Y., Wang, X., Wan, Z., Zhang, K., Xu, W., Xiong, J., Luo, P., Chen, W., Tao, C., Mao, Z., and Wong, N. Phyx: Does your model have the wits for physical reasoning?, 2025. URL https://arxiv. org/abs/2505.15929. Teed, Z. and Deng, J. Raft: Recurrent all-pairs field transforms for optical flow, 2020. URL https://arxiv. org/abs/2003.12039. The Manim Community Developers. Manim Mathematical Animation Framework, April 2024. URL https: //www.manim.community/. OpenAI. Introducing gpt-4.1 in the api. https:// openai.com/index/gpt-4-1/, 2025a. Accessed: 2026-01-15. three.js contributors. Three.js javascript 3d library. https://threejs.org/, 2026. Accessed: 202601-15. OpenAI. Introducing gpt-5. https://openai.com/ index/introducing-gpt-5/, 2025b. Accessed: 2026-01-15. OpenAI. GPT-5.2 Model (openai api documentation). https://platform.openai.com/docs/ models/gpt-5.2, 2025c. Accessed 2026-01-06. p5.js contributors. p5.js. https://p5js.org/, 2026. Accessed: 2026-01-15. Pezeshkpour, P. and Hruschka, E. Large language models sensitivity to the order of options in multiple-choice questions, 2023. URL https://arxiv.org/abs/ 2308.11483. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. Rajani, N. F., Zhang, R., Tan, Y. C., Zheng, S., Weiss, J., Vyas, A., Gupta, A., XIong, C., Socher, R., and Radev, D. Esprit: Explaining solutions to physical reasoning tasks, 2020. URL https://arxiv.org/abs/ 2005.00730. 12 Tung, H.-Y., Ding, M., Chen, Z., Bear, D., Gan, C., Tenenbaum, J. B., Yamins, D. L., Fan, J. E., and Smith, K. A. Physion++: Evaluating physical scene understanding that requires online inference of different physical properties, 2023. URL https://arxiv.org/abs/2306. 15668. Wang, Z., Bovik, A., Sheikh, H., and Simoncelli, E. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13 (4):600612, 2004. doi: 10.1109/TIP.2003.819861. Yang, Y., Cheng, W., Chen, S., Zeng, X., Zhang, J., Wang, L., Yu, G., Ma, X., and Jiang, Y.-G. Omnisvg: unified scalable vector graphics generation model. arXiv preprint arxiv:2504.06263, 2025. Yang, Z., Zhou, Z., Wang, S., Cong, X., Han, X., Yan, Y., Liu, Z., Tan, Z., Liu, P., Yu, D., Liu, Z., Shi, X., and Sun, M. Matplotagent: Method and evaluation for llm-based agentic scientific data visualization, 2024. Yi, K., Gan, C., Li, Y., Kohli, P., Wu, J., Torralba, A., and Tenenbaum, J. B. Clevrer: Collision events for video representation and reasoning, 2020. URL https:// arxiv.org/abs/1910.01442. VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction Zhang, L., Zhang, L., Mou, X., and Zhang, D. Fsim: feature similarity index for image quality assessment. IEEE Transactions on Image Processing, 20(8):2378 2386, 2011. doi: 10.1109/TIP.2011.2109730. Zhang, L., Shen, Y., and Li, H. Vsi: visual saliencyinduced index for perceptual image quality assessment. IEEE Transactions on Image Processing, 23(10):4270 4281, 2014. doi: 10.1109/TIP.2014.2346028. Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as perceptual metric, 2018. URL https://arxiv.org/ abs/1801.03924. Zhang, S., Ma, J., Wu, J., Ritchie, D., and Agrawala, M. Editing motion graphics video via motion vectorization and transformation. ACM Trans. Graph., dec 2023. doi: 10.1145/3618316. Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y. Bertscore: Evaluating text generation with bert, 2020. URL https://arxiv.org/abs/1904.09675. 13 VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction A. Case Study Figure 8. detailed case study (ID 2). 14 VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction Figure 9. detailed case study (ID 3). 15 VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction Figure 10. detailed case study (ID 4). 16 VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction Figure 11. detailed case study (ID 5). 17 VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction Figure 12. detailed case study (ID 6). 18 VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction Figure 13. detailed case study (ID 7). 19 VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction Figure 14. detailed case study (ID 8). 20 VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction B. Reproducibility Details This appendix documents the reproducibility-critical components of VisPhyWorld: (i) the prompting protocol used to elicit an executable scene hypothesis, (ii) the optional detection context format, (iii) deterministic execution constraints for rendering, and (iv) robustness protocols that ensure well-defined evaluation. B.1. Prompting Protocol for Scene Hypotheses VisPhyWorld uses single-call prompting protocol that asks the model to (1) summarize the observed motion between two keyframes and (2) propose an executable scene hypothesis that reproduces the event. To ensure comparability across models, we enforce fixed output format and small set of execution constraints (e.g., single canvas and bounded duration), which are handled by the renderer (Appendix B.4). The full prompt template is shown in Figure 15. Scene Analysis & Code Generation Prompt You are an expert in 2D physics, rigid-body simulation, and JavaScript. Given two key frames from short video and an optional list of detected objects, your goals are: (1) Scene and motion analysis (38 sentences): Describe the main objects (shapes, colors, approximate sizes) visible in the first frame. Describe how these objects move between the first and the second frame (who moves, who stays still, collisions, stacks that topple, etc.). Explain the likely physical causes of the motion (gravity, contact forces, friction, impulses). (2) Simulation code generation: Produce ONE complete HTML document that: Imports the required rendering/physics libraries (or uses provided local copies if available). Creates 2D-like scene with an orthographic camera. Adds rigid bodies (balls, boxes, planks) matching the layout of the first frame. Initializes positions and orientations so that the first rendered frame closely matches the first image. Assigns velocities or impulses consistent with the observed motion between the two images. Runs physics simulation and renders frames to single canvas element. Uses the provided recording helper to export finite-duration clip. Return your answer in the following format: (A) Analysis section: plain English paragraphs. (B) Code section: single fenced block html <!DOCTYPE html> . . . </html> Do not include any other Markdown fences or extra HTML documents. Figure 15. Full multimodal LLM prompt template used by VisPhyWorld for both motion analysis and code generation. B.2. Detection Context To reduce ambiguity in object discovery and initialization, VisPhyWorld can optionally provide structured detection context for the first keyframe start. is per-sample JSON annotation containing list of objects with coarse geometry and appearance attributes. All coordinates are in pixel space with origin at the image top-left (x increases rightward, increases downward). Schema. Each detected object provides: (i) unique identifier id; (ii) coarse category (e.g., circle, rectangle, line, shape); (iii) an RGB color triplet color rgb; (iv) tight bounding box bbox as {x min, min, VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction 3D Scene Analysis & Code Generation Prompt (dataset 3D) You are an expert in 3D rigid-body physics, Three.js, and JavaScript. Given two key frames from short video (rendered from fixed camera) and an optional list of detected objects, your goals are: (1) Scene and motion analysis (38 sentences): Describe the major objects and supports visible in the first frame (shapes, colors, approximate sizes, and relative depth if evident). Describe how these objects move between the first and the second frame, focusing on contacts, impacts, and constraint satisfaction. Explain the likely physical causes of the motion (gravity, contact forces, friction, impulses). (2) Simulation code generation: Produce ONE complete HTML document that: Uses Three.js for rendering and Cannon.js for rigid-body simulation. Creates 3D scene with fixed perspective camera (no camera motion). Initializes objects and supports so the first rendered frame closely matches the first image. Assigns velocities or impulses consistent with the observed motion between the two images. Runs the physics simulation deterministically and renders frames to single canvas element. Uses the provided recording helper to export finite-duration clip. Return your answer in the following format: (A) Analysis section: plain English paragraphs. (B) Code section: single fenced block html <!DOCTYPE html> . . . </html> Do not include any other Markdown fences or extra HTML documents. Figure 16. 3D prompt variant used for dataset 3D. It mirrors Figure 15 but switches the execution target from 2D (orthographic) to 3D (fixed perspective) while keeping the output format identical. 22 VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction max, max, width, height}; (v) centroid position.center x/center y; (vi) coarse size descriptor (e.g., radius pixels for circles, length pixels/thickness pixels for bars); and (vii) an optional orientation.angle deg for elongated primitives. Example. { \"image_size\": {\"width\": 512, \"height\": 512}, \"coordinate_system\": {\"origin\": \"top_left\", \"x_axis\": \"to_right\"...}, \"objects\": [ {\"id\":\"red_ball\",\"category\":\"circle\",\"color_rgb\":[240,78,70], \"position\":{\"center_x\":363.6,\"center_y\":155.2}, \"bbox\":{\"x_min\":348,\"y_min\":140,\"x_max\":378,\"y_max\":172,\"width\":32...}, \"size\":{\"radius_pixels\":16.5}} ] } B.3. VisPhyBench Templates and Stochasticity VisPhyBench templates are defined as executable PHYRE-style task scripts. Unlike static assets, each template is instantiated by sampling seeds (e.g., object placements and sizes), so single rendered snapshot does not capture the full diversity. We therefore summarize object composition over the full sub split using the detection context on start (see Table 7). Table 7. Object category statistics on VisPhyBench. Category Scenes (%) Scenes (count) Objects (count) circle line rectangle shape triangle composite shape 100.0 83.2 62.8 24.6 6.3 7.3 191 159 120 47 12 14 779 344 321 47 16 3D templates. In addition to PHYRE-style 2D scripts, we include set of programmatic 3D templates implemented in Three.js + Cannon.js. These 3D templates use simple rigid-body primitives (e.g., spheres, boxes, ramps, barriers) under fixed perspective camera and white background, and are designed to probe depth-aware contacts and occlusions not present in purely 2D scenes. Because is defined in 2D pixel space from first-frame detector, the category statistics above are reported for the 2D portion of the split; for the 3D subset we instead rely on the executable template specification and deterministic rendering protocol (Appendix B.4). B.4. Deterministic Execution and 2D Constraint VisPhyWorld executes each generated scene hypothesis under fixed, deterministic configuration to ensure comparability across models. Canonicalization and validation. Raw model outputs may contain extraneous text or malformed markup. Before execution, we extract the HTML payload (from fenced html block when present, otherwise the outermost <html>...</html> segment), and canonicalize it into standard executable template that injects the required libraries and trusted recording helper. We additionally validate basic requirements (e.g., existence of drawable canvas and finite numeric states). Retry and fallback behaviors are described in Appendix B.5. Execution contract. For each sample, the renderer produces fixed-length clip ˆX at the reference frame rate and duration associated with that sample. All runs use fixed physics time step and fixed camera configuration; as result, variability in ˆX is attributable to the generated hypothesis rather than nondeterministic execution. 2D constraint. Although the underlying physics engine supports full 3D dynamics, we restrict motion to 2D plane by (i) initializing all bodies with = 0 and (ii) projecting the state back to the plane at each simulation step (clamping out-of-plane position and angular components to zero). This avoids uncontrolled 3D degrees of freedom while preserving rigid-body contact dynamics. 3D execution. For our 3D subset, we disable the 2D clamping rule and execute full 3D rigid-body dynamics with the same deterministic protocol (fixed physics time step, fixed recording duration, and fixed camera parameters). To preserve comparability across models, we keep the camera static and normalize all rendered videos to match the reference FPS, duration, and resolution of the corresponding ground-truth clip. 23 VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction Deterministic Rendering Protocol (High-Level) Given model-generated scene hypothesis C, the renderer produces the output clip ˆX under fixed protocol: Parse & canonicalize: Extract the HTML payload and wrap it into standard executable template with fixed library versions and trusted recorder. Validate: Check minimal execution requirements (e.g., drawable canvas and finite numeric states). Execute deterministically: Run physics with fixed time step and fixed orthographic camera, producing frames at the samples reference FPS. Enforce 2D: Initialize with = 0 and clamp out-of-plane components each step. Export: Record fixed-duration clip and convert it to standard format for downstream evaluation. Figure 17. High-level deterministic rendering protocol used in VisPhyWorld. Low-level implementation details are included in the released codebase. B.5. Robustness: Automatic Retry and Fallback To handle syntax errors or runtime exceptions in model-generated programs, we implement lightweight robustness protocol that ensures evaluation is well-defined for all samples. Error-conditioned single-step repair. If the initial program fails to execute (e.g., syntax error, missing canvas, or runtime exception), we capture execution diagnostics (e.g., JavaScript console logs and error traces), summarize them, and provide the summary to the model for single repair attempt. Fallback and well-defined evaluation. If the repair attempt also fails, we execute minimal hand-crafted fallback template (Figure 18) that guarantees valid canvas and finite motion. This prevents missing outputs and ensures the evaluation pipeline does not crash; such samples receive correspondingly poor scores on the metrics. Success criteria. We distinguish two notions of success. Model-success counts sample as successful only if the modelgenerated hypothesis executes and produces non-empty clip without invoking the fallback. System-success additionally counts fallback clips as successful, and is used only to guarantee that the evaluation pipeline is well-defined. Unless otherwise stated, success rates reported in the main paper use Model-success. Fallback Template (Simplified Sketch) The fallback template guarantees valid canvas and finite motion when model generation fails: <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\" /> <title>VisPhyWorld Fallback Scene</title> <script src=\"three.min.js\"></script> <script src=\"cannon.min.js\"></script> <script src=\"recording.js\"></script> </head> <body style=\"margin:0;overflow:hidden;\"> <canvas id=\"visphyworld-canvas\"></canvas> <script> // Setup renderer, camera, scene, lights // Create flat ground plane and one spherical body // Run simulation loop and export finite-duration clip </script> </body> </html> Figure 18. High-level structure of the fallback template used when both model attempts fail. 24 VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction C. Evaluation Metrics: Definitions & Protocols This appendix defines the metric families used in the main paper. All metrics are computed per scene and then averaged over the evaluated split. Unless otherwise noted, frame-wise metrics are computed after temporal alignment (Appendix C.1). C.1. Default Evaluation Hyperparameters We report our default evaluation hyperparameters for reproducibility. Unless otherwise stated, we uniformly sample frames every sample every=3 frames for all frame-wise metrics. For temporal alignment, we use coarse-to-fine strategy with coarse offset search up to max offset=30 (in sampled frames), stack window window=3, and offset penalty offset penalty=0.05. The coarse search uses downsample=64, top k=5, and max samples=16. When DTW is enabled, we compute frame features using 48 48 grayscale thumbnail and step penalty of 0.005. Table 8. Default evaluation hyperparameters used throughout the paper. Setting Value sample every=3 max offset=30 (sampled frames) Frame sampling Coarse offset search Stack refinement window window=3 Offset penalty Coarse downsample Top-k candidates Max coarse samples DTW feature size DTW step penalty offset penalty=0.05 downsample=64 top k=5 max samples=16 48 48 grayscale 0.005 C.2. Reconstruction & Perceptual Quality PSNR and SSIM: Computed frame-wise between aligned reference and generated videos. SSIM is averaged across the channel and RGB channels. LPIPS, FSIM, VSI, DISTS: Deep and structural perceptual metrics computed on aligned frames. We use the piq library implementation. C.3. Visual Semantic Consistency CLIP-Img: Cosine similarity between CLIP (ViT-B/32) embeddings of reference and generated frames, measuring high-level semantic/layout consistency. DINO Similarity: Cosine similarity of DINO ViT features, which is more sensitive to object structure and less biased by text supervision than CLIP. C.4. TextVideo & Analysis Consistency CLIP-Cap: Similarity between the generated motion-analysis text and the generated video frames. Text Metrics (ROUGE, BERTScore): We compare the generated analysis against an automatically produced reference description of the original video (generated by strong LLM). This validates whether the model correctly perceives and verbalizes the events in the input video. C.5. Motion & Physical Plausibility RAFT Optical Flow: We compute End-Point Error (EPE), flow magnitude difference, and angular error between the optical flow fields of the reference and generated videos. Temporal Alignment: We use coarse-to-fine alignment strategy (Figure 19) combining offset search and Dynamic Time Warping (DTW) to handle temporal shifts before metric computation. C.6. Subjective Quality (Gemini Judge) We employ Gemini-2.5-Pro as holistic judge. The prompt (Figure 20) asks the model to compare the reference and generated videos and assign score (110) with justification. D. Detailed Experimental Results We provide the full breakdown of experimental results across all metrics and models. 25 VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction"
        },
        {
            "title": "Temporal Alignment Procedure",
            "content": "(1) Coarse Search: Downsample frames to grayscale vectors. Compute correlation for offsets 30 frames. Keep top-k candidates. (2) Stack Refinement: Build frame stacks (w = 3). Minimize cost = MSE + 0.5 MAE + 0.1 Angular. (3) DTW: Run Dynamic Time Warping on low-res features to align variable-speed sequences. Figure 19. Temporal alignment procedure used before computing frame-wise metrics. Gemini-based Physics & Video Consistency Prompt You are an expert evaluator of physical simulations and video quality. Compare the provided reference video (Ground Truth) with the generated video. Your goal is to determine if the generated video accurately reconstructs the physical event shown in the reference. Focus on the following dimensions: Physical Plausibility (Crucial): Do the objects obey rigid-body physics laws (gravity, collisions, friction) in the given setting (2D or 3D)? Are there any hallucinations such as objects passing through each other (ghosting), floating unnaturally, or failing to move when hit? Motion Consistency: Does the trajectory, speed, and timing of the movement align with the reference? Scene Semantics: Are the correct objects (color, shape, count) present in the correct layout? Visual Fidelity: Overall clarity, ignoring minor rendering style differences if the physics is correct. Return JSON object with the keys: score: integer between 1 and 10. 10: Perfect physical and visual match. 1: Physical laws are violated (e.g., phantom collision, static scene when motion is expected), even if the image looks realistic. justification: Brief explanation, specifically pointing out any physical violations if present. Figure 20. Prompt template used for Gemini-based evaluation. Note that the prompt is explicitly designed to penalize physical violations (e.g., incorrect collision logic), ensuring the score reflects physical understanding rather than just perceptual similarity. 26 VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction D.1. VisPhyBench Difficulty Stratification Table 3 in the main paper reports the difficulty distribution; here we provide additional details on the stratification and split construction. All annotators are graduate students with STEM backgrounds. D.2. Per-Scene Distributions and Significance (Sub Split) Mean scores can obscure whether improvements are driven by small subset of scenes. To address this, we report (i) perscene metric distributions via boxplots (Figure 21) and (ii) paired bootstrap confidence intervals over per-scene differences  (Table 9)  . We use paired resampling because all methods are evaluated on the same set of scenes (N = 209), and define mean improvement so that positive values indicate better performance by VisPhyWorld (GPT-5, threejs), taking metric direction into account ( / ). Figure 21. Per-scene boxplot distributions on VisPhyBench for representative metric families (higher is better unless marked ). D.3. Reconstruction & Perceptual Metrics Table 10 details the pixel-level and perceptual metrics. Gemini-3-Pro consistently achieves the best perceptual scores (LPIPS, FSIM), while Three.js backends generally outperform P5.js. D.4. Visual Semantic Consistency Table 11 compares semantic understanding. GPT-5 and Gemini-3-Pro show strong alignment with the ground truth in terms of CLIP and DINO scores. D.5. Text & Physical Consistency Table 12 and Table 13 (below) provide the remaining metrics on text analysis quality and physical motion fidelity. 27 VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction Metric Comparison Mean improvement 95% bootstrap CI VisPhyWorld (GPT-5, p5js) Veo-3.1 SVD (img2vid) VisPhyWorld (GPT-5, p5js) Veo-3.1 SVD (img2vid) VisPhyWorld (GPT-5, p5js) Veo-3.1 SVD (img2vid) VisPhyWorld (GPT-5, p5js) Veo-3.1 SVD (img2vid) LPIPS LPIPS LPIPS CLIP-Img CLIP-Img CLIP-Img DINO DINO DINO CLIP-Cap CLIP-Cap CLIP-Cap BERTScore-F1 VisPhyWorld (GPT-5, p5js) BERTScore-F1 Veo-3.1 BERTScore-F1 SVD (img2vid) RAFT-EPE RAFT-EPE RAFT-EPE Gemini Gemini Gemini VisPhyWorld (GPT-5, p5js) Veo-3.1 SVD (img2vid) VisPhyWorld (GPT-5, p5js) Veo-3.1 SVD (img2vid) 0.1143 0.0365 0.1674 0.0754 0.0365 0.2258 0.0957 -0.0276 0.2036 0.0299 -0.0050 0.0101 0.0077 N/A N/A 0.6294 -0.7078 11.9706 -0.0108 0.9153 2.0635 [0.0743, 0.1567] [0.0310, 0.0420] [0.1581, 0.1764] [0.0477, 0.1044] [0.0285, 0.0446] [0.2159, 0.2355] [0.0643, 0.1290] [-0.0340, -0.0217] [0.1917, 0.2155] [0.0243, 0.0356] [-0.0098, -0.0002] [0.0050, 0.0151] [0.0059, 0.0094] [N/A, N/A] [N/A, N/A] [-0.7743, 2.0695] [-1.8371, 0.4628] [8.8544, 15.1865] [-0.5081, 0.5027] [0.4233, 1.3968] [1.7090, 2.4444] Table 9. Paired bootstrap confidence intervals (VisPhyBench sub, = 209). Mean improvement is defined so that positive values indicate VisPhyWorld (GPT-5, threejs) performs better (for metrics we compute baselineours; for metrics oursbaseline). Table 10. Detailed breakdown of Reconstruction and Perceptual Metrics. Model PSNR SSIM LPIPS FSIM VSI DISTS VisPhyWorld (GPT-5, threejs) VisPhyWorld (GPT-5, p5js) VisPhyWorld (GPT-4.1, threejs) VisPhyWorld (GPT-4.1, p5js) VisPhyWorld (Gemini-3-Pro, threejs) VisPhyWorld (Gemini-3-Pro, p5js) VisPhyWorld (Claude Sonnet 4.5, threejs) VisPhyWorld (Claude Sonnet 4.5, p5js) VisPhyWorld (Qwen3-VL-Plus, threejs) VisPhyWorld (Qwen3-VL-Plus, p5js) SVD (img2vid) Veo-3.1 20.54 16.36 19.74 14.83 21.26 15.57 20.75 15.36 18.66 9.14 14.44 20.04 0.9370 0.7440 0.9337 0.6830 0.9445 0.6943 0.9406 0.7160 0.9306 0. 0.8802 0.9354 0.1736 0.2926 0.1818 0.3520 0.1399 0.3302 0.1602 0.3250 0.2207 0.5478 0.3408 0.2102 0.9014 0.9105 0.9064 0.8977 0.9225 0.9055 0.9118 0.9030 0.8972 0.8797 0.8239 0.8561 0.8432 0.8193 0.8309 0.8112 0.8539 0.8220 0.8374 0.8162 0.8099 0. 0.7585 0.8586 0.1883 0.2724 0.2040 0.3348 0.1859 0.3384 0.2001 0.3109 0.2373 0.4396 0.3459 0.1755 Table 11. Visual Semantic Consistency Metrics. Model CLIP-Img DINO VisPhyWorld (GPT-5, threejs) VisPhyWorld (GPT-5, p5js) VisPhyWorld (GPT-4.1, threejs) VisPhyWorld (GPT-4.1, p5js) VisPhyWorld (Gemini-3-Pro, threejs) VisPhyWorld (Gemini-3-Pro, p5js) VisPhyWorld (Claude Sonnet 4.5, threejs) VisPhyWorld (Claude Sonnet 4.5, p5js) VisPhyWorld (Qwen3-VL-Plus, threejs) VisPhyWorld (Qwen3-VL-Plus, p5js) SVD (img2vid) Veo-3.1 0.8930 0.8134 0.8933 0.7545 0.8973 0.7460 0.8957 0.7612 0.8717 0.6446 0.6677 0.8564 0.8556 0.7580 0.8304 0.6786 0.8405 0.6721 0.8305 0.7098 0.7837 0.5478 0.6528 0. 28 VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction Table 12. TextVideo and Analysis-Text Consistency Metrics. Model CLIP-Cap ROUGE-L F1 BERTScore-F1 VisPhyWorld (GPT-5, threejs) VisPhyWorld (GPT-5, p5js) VisPhyWorld (GPT-4.1, threejs) VisPhyWorld (GPT-4.1, p5js) VisPhyWorld (Gemini-3-Pro, threejs) VisPhyWorld (Gemini-3-Pro, p5js) VisPhyWorld (Claude Sonnet 4.5, threejs) VisPhyWorld (Claude Sonnet 4.5, p5js) VisPhyWorld (Qwen3-VL-Plus, threejs) VisPhyWorld (Qwen3-VL-Plus, p5js) SVD (img2vid) Veo-3.1 0.2632 0.2331 0.2610 0.2192 0.2567 0.2184 0.2588 0.2177 0.2650 0.2032 0.2533 0.2681 0.2186 0.2057 0.2383 0.1689 0.2141 0.1886 0.2168 0.1599 0.2022 0.1733 0.8436 0.8360 0.8522 0.8253 0.8460 0.8396 0.8468 0.8224 0.8466 0. Table 13. Motion and Physical Plausibility Metrics (Selected columns). Model RAFT-EPE RAFT-Angle Align-Err VisPhyWorld (GPT-5, threejs) VisPhyWorld (GPT-5, p5js) VisPhyWorld (GPT-4.1, threejs) VisPhyWorld (GPT-4.1, p5js) VisPhyWorld (Gemini-3-Pro, threejs) VisPhyWorld (Gemini-3-Pro, p5js) VisPhyWorld (Claude Sonnet 4.5, threejs) VisPhyWorld (Claude Sonnet 4.5, p5js) VisPhyWorld (Qwen3-VL-Plus, threejs) VisPhyWorld (Qwen3-VL-Plus, p5js) SVD (img2vid) Veo-3. 33.6473 34.3433 33.7110 37.6993 36.2030 33.1013 36.1985 34.1425 35.0493 20.8187 45.4606 32.7145 68.5500 75.8555 67.7974 82.9492 62.4494 81.5723 71.7979 78.2841 75.6650 80.7413 84.7314 77.0550 0.0210 0.0279 0.0249 0.0397 0.0192 0.0184 0.0210 0.0277 0.0350 0.8567 0.0746 0."
        }
    ],
    "affiliations": [
        "Autodesk AI Lab",
        "Independent",
        "University of Waterloo"
    ]
}