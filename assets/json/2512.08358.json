{
    "paper_title": "TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels",
    "authors": [
        "Jiahao Lu",
        "Weitao Xiong",
        "Jiacheng Deng",
        "Peng Li",
        "Tianyu Huang",
        "Zhiyang Dou",
        "Cheng Lin",
        "Sai-Kit Yeung",
        "Yuan Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 8 5 3 8 0 . 2 1 5 2 : r TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels Jiahao Lu1 Weitao Xiong1,5 Jiacheng Deng2 Peng Li1 Tianyu Huang3 Zhiyang Dou4 Cheng Lin6 Sai-Kit Yeung Yuan Liu1 1HKUST 2USTC 3CUHK 4HKU 5XMU 6MUST https://igl-hkust.github.io/TrackingWorld.github.io/"
        },
        {
            "title": "Abstract",
            "content": "Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, novel pipeline for dense 3D tracking of almost all pixels within world-centric 3D coordinate system. First, we introduce tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in world-centric coordinate frame."
        },
        {
            "title": "Introduction",
            "content": "Estimating long-term motion in dynamic videos remains persistent challenge in computer vision [1, 2, 3, 4]. Fine-grained motion tracking is crucial for understanding object dynamics, modeling camera motion, and facilitating the generation of temporally and geometrically consistent videos [5, 6, 7]. In recent years, dense 2D pixel tracking [8, 9, 10, 1, 11, 12, 13, 14, 15, 16] has emerged as an active research topic, with notable advancements such as CoTrackers [17, 1], which employs transformers to iteratively update 2D tracks and has driven progress in 2D motion analysis. This development also motivates many recent works for 3D tracking. Early 3D tracking works like OmniMotion [2, 18] adopt optimization-based approaches to estimate 3D motion, while subsequent feedforward methods such as SpatialTracker [3] and DELTA [4] leverage extracted features to directly estimate the 3D tracking in feedforward manner without per-sequence optimization. These 3D tracking methods demonstrate substantial potential for downstream applications, including detailed 3D motion analysis and high-fidelity novel view synthesis, highlighting the growing importance of monocular 3D tracking as critical research frontier. Upon analyzing all existing 3D tracking methods, we observe that these existing methods still suffer from two noticeable shortcomings. First, these methods [4, 3, 2] cannot distinguish the camera Corresponding authors 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Figure 1: TrackingWorld estimates world-centric dense tracking results from monocular videos. Our model can accurately estimate camera poses and achieve disentangled 3D track modeling of static and dynamic components, not just limited to one foreground dynamic object. We only visualize subset of foreground dynamic point trajectories and apply fading color to background static points. motion and the dynamic object motion. All these methods assume static camera and just model the 3D flow within the camera coordinate system. However, many downstream tasks like motion analysis or novel-view-synthesis require distinguishing camera motion from the dynamic object motion. Moreover, some recent works [19] also show that explicitly considering camera poses in motion estimation improves the 3D tracking quality. Only some very recent works [20, 21, 22] try to estimate the 3D tracks in the world-centric coordinate system and enable distinguishing camera motions from dynamic object motions. Estimating camera motion is still challenging for monocular video containing dynamic objects because only static scenes provide cues for camera pose estimation. The second shortcoming is that existing methods are mostly limited to tracking sparse pixels in the first frame of the video and cannot track all pixels in all frames (e.g., new objects emerging in the intermediate frames). Tracking all pixels brings huge computational complexity to all tracking methods. Recent works like DELTA [4] propose to upsample the sparse tracking points with neural networks to produce dense 3D tracks. However, DELTA is still limited to tracking the first frame of the video, and how to estimate the dense 3D tracks for all pixels of all frames still remains an unexplored problem. In this paper, we propose TrackingWorld, 3D tracking method that enables dense 3D tracking of almost all pixels of all frames from monocular video within world-centric coordinate system. almost all means we filter some noisy and outlier tracks to ensure robustness and accuracy. Specifically, TrackingWorld takes monocular video and the monocular estimation from foundation models as input, including sparse tracks [4, 1], depth maps [23], and coarse foreground dynamic masks [24]. Then, TrackingWorld produces high quality dense 3D tracks for almost all pixels of the monocular video and the camera poses for every frame. TrackingWorld addresses the above shortcomings with the following strategies. First, to enable the dense tracking of almost all pixels, we utilize the track upsampler of DELTA [4] and track every frame iteratively. We find that the tracking upsampler module of DELTA [4] is applicable to arbitrary 2D tracks, which are utilized by TrackingWorld to upsample the input sparse 2D tracks to dense 2D tracks. Then, we not only track the pixels of the first frame but also repeat it on all subsequent frames. To reduce computational complexity, we observe that many regions of subsequent frames have already been seen in the first or previous frames. Therefore, we delete the redundant tracks corresponding to these overlapping regions. Second, to accurately separate the camera motion from the dynamic object motion, we estimate the 3D tracks and the camera poses from the upsampled dense 2D tracks and the input estimated depth maps. key challenge lies in the inaccuracy of the estimated dynamic masks, which often fail to capture dynamic background objects. This limitation leads to suboptimal bundle adjustment interfered by dynamic background objects, ultimately compromising the accuracy of both camera pose estimation and object motion tracking. Thus, we treat all points in the initial static regions as potentially dynamic but impose an as-static-as-possible constraint for the camera pose estimation, which effectively helps us rule out the dynamic background points for an accurate camera pose estimation. Finally, we utilize the estimated camera poses along with the depth maps to convert all the 2D tracks into 3D tracks in the world coordinate. To comprehensively evaluate whether our proposed method can effectively achieve dense 3D tracking of almost all pixels across all frames within world-centric coordinate system, we conduct evaluations 2 from multiple perspectives: 1. Camera pose estimation accuracy; 2. Depth accuracy of the dense 3D tracks; 3. Sparse 3D tracking performance; 4. Accuracy of the dense 2D tracking results. Our empirical analysis demonstrates that the proposed method yields superior performance across all metrics, confirming its effectiveness in establishing accurate and consistent 3D tracks over time."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 2D Point Tracking The task of tracking arbitrary points [8, 9, 10, 1, 11, 12, 13, 14, 15, 16] across video frames is first introduced by PIPs [8], which leverages deep learning to tackle point tracking based on optical flow. Built upon RAFT [25], PIPs computes inter-frame correlation maps and uses decoder to iteratively refine tracking results. TAP-Vid [9] further improves the problem formulation, introducing three standardized benchmarks along with TAP-Net, dedicated model for point tracking. TAPIR [10] advances performance by combining matching stage with refinement stage, enhancing tracking accuracy. CoTrackers [17, 1] observe that strong correlations exist across different point trajectories, and exploit this insight by training on unrolled sequences over long videos, which significantly improve long-term tracking performance. Drawing inspiration from DETR [26], TAPTR [12] proposes an end-to-end transformer-based architecture, where each point is represented as query token in the decoder, enabling direct modeling of point dynamics. LocoTrack [11] extends traditional 2D correlation features to 4D correlation volumes and introduces lightweight correlation encoder, achieving better efficiency while preserving accuracy. 2.2 3D Point Tracking While previous works have primarily focused on 2D point tracking, recent research has increasingly focused on 3D point tracking [2, 18, 3, 4, 27, 28, 29, 30, 31, 22, 20, 21]. Early 3D tracking methods, such as OmniMotion [2], adopt optimization-based approaches to estimate 3D motion. Subsequent work like OmniTrackFast [18] aims to reduce the optimization time and enhance robustness. More recently, increasing attention has shifted toward feedforward-based methods. For example, SpatialTracker [3] represents points in (u, v, d) coordinate system, combining image-plane coordinates with depth information. It incorporates depth priors and uses triplane representation to enable effective 3D tracking. Building upon this idea, DELTA [4] also adopts the UVD coordinate system, but takes different approach by decoupling appearance and depth correlations. DELTA introduces coarse-to-fine trajectory estimation strategy, allowing for efficient dense tracking across the entire frame rather than being limited to sparse set of locations. In contrast to the aforementioned methods that focus on UVD (2.5D) representations, several concurrent works have recently explored 3D tracking in world-centric coordinate system. St4RTrack [20] adopts DUSt3R [32]-like framework to establish pairwise correspondences, but this approach may suffer from drift during long-term tracking. TAPIP3D [21] primarily focuses on sparse tracking and is inherently unable to recover camera motion. In comparison, our method introduces comprehensive pipeline for dense 3D tracking that can robustly capture newly emerging objects within world-centric coordinate system. 2.3 4D Reconstruction 4D reconstruction [33, 34, 35, 36, 19, 37, 38, 39, 40, 24] aims to recover both camera motion and object motion within scene. The problem of non-rigid structure from motion is highly ill-posed. To overcome this limitation, variety of approaches have been proposed. RobustCVD [33] refines depth estimation using 3D geometric constraints, while CasualSAM [34] finetunes depth network guided by predicted motion masks. MegaSaM [35] integrates monocular depth priors and motion probability maps into differentiable SLAM paradigm. Inspired by DUSt3R [32], several data-driven methods such as MonST3R [37], Align3R [38], and Cut3r [40] adopt 3D point cloud representations to enable full 4D reconstruction. In addition, Uni4D [24], multi-stage optimization framework, leverages multiple pretrained models to improve reconstruction in dynamic scenes. Its core contribution lies in the use of foundation models to achieve effective separation of static and dynamic elements within the scene. Our method also adopts an optimization-based framework to decouple camera motion and object motion. However, unlike prior works that primarily focus on 4D reconstruction, our approach targets higher-level taskdense tracking of every pixelwhich enables fine-grained correspondence estimation across time. By focusing on dense pixel-level tracking, our method 3 Figure 2: Overview. Given video sequence, TrackingWorld first generates dense 2D tracking results that are capable of capturing newly emerging objects in the scene. These 2D trajectories are then fed into an optimization-based framework to transform them into world-centric 3D space. Specifically, we begin by estimating the initial camera poses for each frame at the clip level. We then perform dynamic background refinement to exclude potentially dynamic regions and refine the camera poses. Based on the optimized poses, we finally reconstruct the trajectories of all dynamic regions. provides more detailed and temporally consistent understanding of dynamic scenes, making it well-suited for applications such as motion analysis, scene understanding, and video editing [5, 6, 7]."
        },
        {
            "title": "3 Method",
            "content": "3.1 Overview Given video consisting of frames {It RHW3 = 1, . . . , }, the goal of TrackingWorld is to estimate the corresponding dense 3D trajectories (3D tracks) {Tt RMt3 = 1, . . . , } of almost all pixels, where Mt denotes the number of tracked points at timestep t, along with the camera poses {πt R34t = 1, ..., }. Our proposed TrackingWorld framework, illustrated in Fig. 2, achieves this through two main components: first, generating dense 2D tracking results that are capable of following nearly every object in the scene; second, back-projecting these dense 2D tracking results into world-centric 3D space. Preprocessing with vision foundation models. For the monocular video, we first preprocess it with 2D tracking model, foreground dynamic mask estimation module, and monocular depth estimation module to get set of 2D tracks, dynamic masks, and depth maps for all frames. For the 2D tracking model, we choose the CoTrackerV3 [1] or the 2D tracking part of DELTA [4]. For the dynamic mask estimation method, we follow Uni4D [24] to apply VLM [41] and GroundingSAM [42, 43] to segment out foreground dynamic objects. Alternatively, we could also choose the SegmentAnyMotion [44] to get dynamic masks. For the depth estimation, we choose UniDepth [23]. Note that all these predictions are not required to be accurate, and we may also adopt other foundation models for this purpose. 3.2 Dense 2D Tracking for Every Pixel In this section, our target is to achieve dense 2D tracking of almost any pixels in the video. We achieve this through two modules: First, we lift the input sparse 2D tracks for frame to dense 2D tracks; Second, we repeat tracking on every frame and eliminate the overlapped redundant 2D tracks. Sparse to dense tracks. Given the sparse 2D tracks Psparse R( )T 2 for specific frame, this module aims to lift the sparse 2D tracks to dense 2D tracks Pdense R(HW )T 2. means the downsampled factor. We achieve this by utilizing the upsampler module of DELTA [4]. The upsampler module takes the sparse tracks Psparse and features defined on the sparse 2D tracks Fsparse T C, where is the feature dimension, as inputs and predicts weight matrix R( s )(HW ). Then, the upsampled dense 2D tracks are Pdense = WT Psparse, where actually only correlates dense track in Pdense with its neighboring 2D tracks in Psparse. We find that this upsampler module is not only compatible with DELTAs 2D tracks but also generalizes to arbitrary 2D tracks, so we adopt it here to upsample the arbitrary input sparse 2D tracks into dense 2D tracks for specific frame. (1) Tracking every frame. Based on the above upsampler, we further enable tracking of almost all pixels of all frames. To achieve this, we conduct 2D tracking and the sparse-to-dense upsampling on all frames in the video. However, this leads to large redundancy on the tracking points because most regions are already seen in the previous frames, while only few regions are new. To avoid wasting computation on these redundant 2D tracks in the subsequent computation, if pixel resides near the tracking trajectory of arbitrary visible previous 2D tracks, then we discard the pixel. More details can be found in A.6 of the supplementary material. 3.3 Lifting 2D Tracks to World-centric 3D Tracks In this module, we will estimate the camera poses of all frames and lift the dense 2D tracks estimated by the previous section to 3D tracks in the world-centric coordinate system. We achieve this through the following three steps: First, we utilize the input estimated coarse dynamic masks and estimate the camera poses using only the coarse static regions. However, the dynamic masks are usually not accurate enough, and some dynamic objects in the background still remain. Second, we utilize an as-static-as-possible constraint to further improve the camera pose estimation and find out the dynamic objects in the background. Finally, we transform all 2D tracks within the dynamic regions into 3D tracks in the world-centric coordinate system. In this step, we want to estimate per-frame camera poses {πt Initial camera pose estimation. SE(3)} from the 2D tracks on static regions and the estimated depth maps. We first utilize the input dynamic foreground masks to select 2D tracks Pstatic RNstaticT 2 on these static regions. Then, for each static 2D track, we unproject its location at timestep t1 into the 3D space using the monocular depth map Dstatic RNstaticT 1. The resulting 3D points are subsequently reprojected into the image plane at timestep t2 using the camera poses. Then, we define the projection loss to optimize the camera poses Lproj = Ninliers(cid:88) (cid:88) (cid:88) t1 t2 πt2π1 t1 (Pstatic(i, t1), Dstatic(i, t1)) Pstatic(i, t2)2 2, (2) where πt() means project with the camera pose on timestep t, and Pstatic(i, t) R2 means the position of the i-th static 2D track on timestep t, Dstatic(i, t) means the depth value for the i-th static point on time step t, and Ninliers denotes the number of static 2D tracks whose projection errors fall within the threshold τ . To further improve the computational efficiency for camera pose estimation, we first divide the entire video into clips and estimate camera poses within each clip in parallel. After estimating camera poses within each clip, we estimate the pose between clips to merge the camera poses together. Dynamic background refinement. The foreground dynamic object masks are usually not accurate enough, so some dynamic objects in the background still exist in the assumed static regions and prevent us from accurately estimating camera poses. Thus, we further refine the camera pose estimation by treating these static regions as dynamic and introducing an as-static-as-possible constraint. Specifically, each static 2D track corresponds to unique 3D point in the world-centric coordinate system, denoted as Tstatic RNstatic3. We initialize Tstatic by back-projecting the static 2D tracks using the depth estimated by UniDepth and the camera poses obtained from the previous stage: Initial camera pose estimation. Notably, for each 2D track RT 2, we only back-project the visible timesteps and take the average of the resulting 3D points. To better model the potentially dynamic regions that are not accurately segmented, we introduce an additional object motion term Ostatic RNstaticT 3, which captures residual object motions over time. With this term, the timedependent world-centric static tracking becomes (3) static(i, t) R3 means the 3D coordinate of the i-th static point at timestep and Ostatic(i, t) where is the corresponding 3 dimensional offset. We then jointly optimize the camera poses πt and the static 3D coordinates static(i, t) = Tstatic(i) + Ostatic(i, t), static using bundle adjustment loss: Lba = Nstatic(cid:88) (cid:88) i=1 t=1 πt(T static(i, t)) Pstatic(i, t)2 2 , (4) where Pstatic(i, t) is the observed 2D projection of the i-th track at timestep t. In addition to the bundle adjustment loss, we also compute depth consistency loss Ldc to enforce the consistency between the projected depth maps from static and the estimated monocular depth maps, as introduced in the supplementary material. To ensure that residual motion remains minimal for genuinely static regions, we regularize the offset Ostatic with an as-static-as-possible constraint Lasap = Ostatic(i, t)1, (cid:88) (5) where we minimize the L1 norms of offsets to make all points as static as possible. This Lasap enables the accurate camera estimation and also models the dynamics of background objects. i,t Dynamic object tracking. In this step, our target is to lift the 2D tracks of dynamic regions to 3D tracks. We also include the dynamic background points with Ostatic(i, )2 ε here as the dynamic 3D tracks. For these dynamic 3D tracks, we directly represent their 3D coordinates by Tdynamic RNdynamicT 3. Similar to the 3D static tracks, we initialize the dynamic 3D tracks by back-projecting them using the depths predicted by UniDepth and the camera poses refined in the second stage. Based on Tdynamic, we also compute the projection loss in Eq. 4, the depth consistency loss Ldc, as-rigid-as-possible loss Larap [45, 24], and temporal smoothness loss Lts [24]. All the details of these loss terms are included in the supplementary material. The final outputs are the dynamic 3D tracks Tdynamic, static 3D tracks Discussion. The tracking module TrackingWorld differs from previous 3D tracking methods, DELTA [4] and SpatialTracker [3] by explicitly estimating the camera poses, which enables the estimation of 3D tracks in the world-centric coordinate system. The explicit separation between camera motion and object motion also improves the quality of 3D tracking because of the better decomposition, as demonstrated by experimental results in Tab. 3. In comparison with the existing dynamic video camera pose estimation methods, like Uni4D [24], we do not just assume single dynamic foreground object but also model the background object motion in the camera pose estimation for better performance. Instead of simply discarding these dynamic background objects, we also track their 3D points in the world-centric coordinate system, enabling tracking almost all pixels. static, and the camera poses πt."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Implementation details All experiments are conducted on an RTX 4090 GPU. We use CoTrackerV3 [1] and DELTA [4] to obtain dense tracking results, and adopt UniDepth [24] as the depth prior. The entire framework takes 20 minutes to produce dense world-centric 3D tracking results for 30-frame video. All baseline methods are run on the datasets using their official implementations and default hyperparameters. More details about hyperparameters can be found in the supplementary materials. 4.2 Quantitative comparisons To demonstrate the capability of our method in dense 3D tracking within world-centric coordinate system, we evaluate the following performance: 1. Camera pose estimation accuracy; 2. Depth accuracy of dense 3D tracks; 3. Sparse 3D tracking performance; 4. Dense 2D tracking performance. 4.2.1 Camera pose estimation results Benchmarks and metrics. We evaluate camera pose estimation performance on three dynamic datasets: Sintel [46], Bonn [47], and TUM-D [48]. For all three datasets, we adopt the same settings as MonST3R [37]. Following [49, 50, 51], we report three ATE (Absolute Trajectory Error), RTE (Relative Translation Error), and RRE (Relative Rotation Error). ATE measures the deviation between estimated and ground truth trajectories after alignment. RTE and RRE evaluate the average local translation and rotation errors over consecutive pose pairs, respectively. Comparison with existing methods. Tab. 1 presents the quantitative comparison between our method and existing approaches. To recover the camera pose, we first obtain dense tracking results, followed by optimization process that refines the camera pose and world-centric dense tracking. As shown in the table, regardless of whether the dense tracking is derived from DELTA [4] or CoTrackerV3 [1], our method consistently achieves more accurate pose estimation than previous approaches across all three datasets. Category Method Sintel Bonn ATE RTE RRE ATE RTE RRE ATE RTE RRE TUM-D DROID-SLAM [52] Pose only DPVO [51] COLMAP [53] 0.175 0.084 1.912 0.115 0.072 1.975 0.559 0.325 7.302 / / / / / / / / / / / / / 0.076 0.059 7.689 / / Joint depth MonST3R [37] & pose Robust-CVD [33] DUSt3R [32] 0.360 0.154 3.443 0.153 0.026 3.528 / 0.601 0.214 11.43 0.046 0.014 1.836 0.083 0.017 3.567 0.111 0.044 0.780 0.029 0.007 0.612 0.063 0.009 1.217 Align3R [38] (Depth Pro [54]) 0.128 0.042 0.432 0.023 0.007 0.620 0.027 0.018 0.446 0.116 0.046 0.603 0.017 0.006 0.561 0.039 0.007 0.434 Uni4D* [24] 0.103 0.039 0.439 0.016 0.005 0.561 0.014 0.005 0.338 Ours (CoTrackerV3 [1]) 0.088 0.035 0.410 0.016 0.005 0.564 0.016 0.005 0.333 Ours (DELTA [4]) / / Table 1: Camera pose estimation results. We evaluate our model on three datasets: Sintel, Bonn, and TUM-D. Best results are highlighted. means using ground truth camera intrinsics as input. * means reproduced by 2D tracks from DELTA, the same as Ours(DELTA). 4.2.2 Depth accuracy of the dense 3D tracks Benchmarks and metrics. Since our method does not aim to optimize 2D tracking accuracy directly, but rather focuses on how to transform 2D tracking into dense world-centric tracking, we evaluates the accuracy of the camera-centric depth for each tracked point. Specifically, we compare the predicted depth with the ground-truth depth only for tracked points that lie within the image bounds. As multiple tracked points may track the same pixel, we retain the one with the smaller depth value for evaluation, assuming it more likely corresponds to the visible surface. Similar to the camera pose benchmark, we evaluate on the same datasets and under identical settings: Sintel, Bonn, and TUM-D. Following prior works [55, 37], we align the estimated dense tracking depth with the ground truth using single scale and shift before computing the evaluation metrics. We primarily report two metrics: Abs Rel (absolute relative error) and the percentage of inlier points with δ < 1.25 . Comparison with existing methods. Tab. 2 reports the results of dense tracking depth estimation. Thanks to our optimization-based bundle adjustment, which enforces strong 3D geometric consistency, the estimated tracking depth is significantly improved across all datasets. Method Depth Prior Sintel Bonn Abs Rel δ < 1.25 Abs Rel δ < 1.25 Abs Rel δ < 1.25 TUM-D ZoeDepth [56] DELTA [4] DELTA [4] Depth Pro [54] Unidepth [23] DELTA [4] Ours (CoTrackerV3 [1]) Unidepth [23] Ours (DELTA [4]) Unidepth [23] 0.814 0.813 0.636 0.219 0.218 46.1 50.7 63.1 73.1 73.3 0.168 0.160 0.153 0.054 0.058 88.5 90.6 90.5 97.2 97. 0.239 0.222 0.178 0.089 0.084 70.5 78.4 85.6 91.5 92.3 Table 2: Depth accuracy of the dense 3D tracks. Best results are highlighted. 7 Category Method Feed. CoTrackerV3 [1]+Uni [23] 13.6 14.3 SpatialTracker [3] 15.3 DELTA [4] ADT PStudio AJ APD3D OA AJ APD3D OA 87.7 79.5 75.7 88.5 14.1 91.5 13.8 90.1 15.1 22.8 23.7 24. 21.3 22.3 22.9 OmniTrackFast [18] Optim. Ours (CoTrackerV3 [1]) Ours (DELTA [4]) 8.6 22.5 23.4 18.2 31.5 32. 63.9 6.4 88.5 14.2 90.1 15.1 12.2 24.0 25.6 81.8 87.7 75.7 Table 3: Sparse 3D tracking results. Feed. means feedforward methods while Optim means optimization-b ased method. 4.2.3 Sparse 3D tracking results Method CVO-Clean EPE IoU EPE CVO-Final RAFT [25] CoTracker [17] SpatialTracker [3] DOT-3D [59] DELTA [4] CoTrackerV3 [1] + Up 2.48 1.51 1.84 1.33 1.14 1.24 57.6 75.5 68.5 79.0 78.9 80. 2.63 1.52 1.88 1.38 1.39 1.35 IoU 56.7 75.3 68.1 78.8 78.2 80.6 Table 4: Long-range optical flow results. Benchmarks and Metrics. To evaluate the performance of 3D sparse tracks, we conduct experiments on two datasets, ADT [57] with moving cameras, and PStudio [58] with static cameras. For each dataset, the video subsets for evaluation are selected at fixed intervals: for ADT, we sample one video every 100 videos, and for PStudio, one video every 20 videos. The sparse 3D tracking result are evaluated in camera coordinates. As for evaluation metrics, we adopt Average Jaccard (AJ), which jointly evaluates the accuracy of both spatial position and occlusion estimation, serving as comprehensive indicator of tracking quality; APD3D (< δavg) which measures the average percentage of tracked points whose errors fall within given threshold δ, reflecting geometric accuracy; Occlusion Accuracy (OA) which evaluates the precision of occlusion state prediction across frames. Comparison with existing methods. Since our method primarily focuses on dense tracking, we maintain the optimization of dense tracking results even when evaluating sparse tracking performance. To this end, we sample evaluation points from the optimized dense tracking set. As shown in Tab. 3, our method achieves higher 3D geometric consistency in tracking. For scenes with camera motion (ADT), the explicit separation between camera motion and object motion leads to significant improvements in both AJ and APD3D. In contrast, for scenes with static cameras (PStudio), the benefits from geometric optimization are relatively limited, resulting in smaller performance gains. It is worth noting that OA mainly evaluates the visibility accuracy of tracking points. Since we directly adopt the visibility maps predicted by DELTA/CoTrackerV3, the OA scores remain consistent with those of DELTA/CoTrackerV3. 4.2.4 Accuracy of dense 2D tracks Benchmarks and Metrics. We evaluate the dense 2D tracking performance on the CVO [60] test set, which consists of two subsets: CVO-Clean and CVO-Final, with the latter incorporating motion blur. Each subset contains approximately 500 videos with 7 frames. For evaluation, we adopt the following metrics: the end-point error (EPE) between the predicted and ground-truth optical flows for all points, and the intersection-over-union (IoU) between the predicted and ground-truth occluded regions in visible masks. Comparison with existing methods. To verify the accuracy of the 2D dense tracks generated by the upsampler module (Up) introduced in Sec. 3.2, we conduct additional long-range optical flow experiments, as shown in Tab. 4. The results demonstrate that the upsampler module generalizes well to other 2D trackers, such as CoTrackerV3, achieving comparable performance with DELTA. 4.3 Qualitative results Fig. 3 qualitatively visualizes the world-centric dense tracking results produced by our method on the DAVIS [61] dataset. For each video sequence, the second row displays 3D tracking results on temporally spaced keyframes, making the changes in object trajectories more perceptible while avoiding visual clutter. The third row presents continuous 3D tracks across all frames, offering comprehensive view of motion consistency and trajectory completeness. As discussed in Sec. 3.3, by separating dynamic and static elements, we can generate stable tracking results for both the static background and dynamic objects. 4.4 Ablation study Ablation study on the different components. As shown in Tab. 5, we conduct ablation studies to validate our major design choices. Specifically, the different configurations are as follows: 1) without 8 Figure 3: Qualitative results on DAVIS dataset. Our method can output both reliable camera trajectories and world centric dense tracking. The second row visualizes 3D tracking results on temporally spaced keyframes, while the third row shows complete tracks across continuous frames. Setting Sintel ATE RTE RRE Abs Rel δ < 1.25 w/o T.E.F 0.171 0.047 0.748 w/o pose-init. 0.659 0.153 1.382 w/o D.O.T 0.088 0.035 0.410 w/o Ninliers 0.089 0.035 0.414 w/o Ostatic 0.092 0.036 0.459 w/o Ldc 0.093 0.036 0.441 Full 0.088 0.035 0.410 / 0.230 0.468 0.220 0.224 0.234 0. / 72.4 73.0 72.9 72.6 71.2 73.3 Table 5: Ablation study on Sintel dataset. Figure 4: Effectiveness of Ostatic. Key regions are highlighted in red. tracking every frame (w/o T.E.F): In this setting, we only track from the first frame, which leads to the loss of many critical cues for pose estimation, thereby resulting in significant performance drop. 2) without initial camera pose estimation (w/o pose-init.): We observe that under this setting, it becomes difficult to jointly optimize both camera poses and 3D tracks effectively good initialization of the camera poses is necessary to achieve satisfactory results. 3) without dynamic object tracking (w/o D.O.T): In this setting, the depths of dynamic tracks are directly obtained from UniDepth predictions without further refinement. As shown in the table, optimizing dynamic tracks is crucial for achieving better performance. 4) without selecting the inliers whose reprojection errors are within threshold τ (w/o Ninliers): By filtering all static points and optimizing with nearly static points, we can effectively reduce the influence of outlier trajectories and obtain more accurate camera poses. 5) without the object motion term Ostatic (w/o Ostatic): We do not consider the dynamic objects in the assumed static background and directly optimize the camera poses with all static background. We show the projected background static points (in green and red dots) in Fig. 4. As we can see, the apple (in the red dots) is considered background static region but is actually dynamic. Without modeling dynamic points in the background, the points of the apple are incorrectly projected onto incorrect regions. 6) without the depth consistency loss (w/o Ldc): Ldc can enforce the consistency between the projected depths and the estimated monocular depths, which helps suppress abnormal depth estimations to some extent. Ablation on different depth estimation models. We conducted an ablation study using three commonly used monocular depth estimation models: ZoeDepth [56], Depth Pro [54], and UniDepth [23]. For all experiments, we fixed the tracking component to DELTA [4] and evaluated both the camera pose estimation accuracy and the depth accuracy of the dense 3D tracks on the Sintel dataset. As shown in Tab. 6, our method consistently improves over raw depth predictions across all depth models, especially in downstream tasks such as camera pose estimation. This demonstrates that our pipeline is robust to different depth estimation backbones. 9 ATE RTE RPE Abs Rel δ < 1.25 Method ZoeDepth Depth Pro UniDepth / / / / / / / / / Ours (ZoeDepth) Ours (Depth Pro) Ours (UniDepth) 0.093 0.101 0. 0.038 0.036 0.035 0.418 0.434 0.410 0.814 0.813 0.636 0.236 0.228 0.218 46.1 50.7 63.1 72.1 72.6 73. Table 6: Ablation study on different depth estimation models. Ablation on dynamic mask segmentators. As shown in Tab. 7, we also evaluate different sources of dynamic mask segmentations and observe comparable performance, further demonstrating the robustness of our pipeline. Method ATE RTE RPE Abs Rel δ < 1.25 Ours + VLM + GroundingSAM 0.088 0.093 Ours + Segment Any Motion 0.035 0.041 0.410 0.379 0.218 0.224 73.3 73.3 Table 7: Ablation study on different dynamic mask segmentators. Necessity of the 2D upsampler module. The 2D upsampler is crucial for achieving efficient dense tracking. Directly predicting dense 2D correspondences (e.g., using CoTrackerV3 [1]) is computationally expensive and memory-intensive, with no clear accuracy gain. To validate this, we compare CoTrackerV3 with and without our upsampler on the CVO-Clean dataset (7-frame sequences). As shown in Tab. 8, the upsampler improves both accuracy (lower EPE, higher IoU) and drastically reduces runtime (approximately 12 speed-up). This supports our design choice. Method EPE IoU Avg. Time (min) CoTrackerV3 CoTrackerV3 + Up 1.45 1. 76.8 80.9 3.00 0.25 Table 8: Ablation on the 2D upsampler module."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose TrackingWorld, novel method for dense 3D tracking of almost all pixels of all frames from monocular video within world-centric coordinate system. The key idea of TrackingWorld is to explicitly disentangle camera motion from foreground dynamic motion while densely tracking newly emerging objects. We first introduce tracking upsampler to densify sparse 2D tracks and apply it to capture newly emerging objects. Finally, we design an efficient optimizationbased framework to lift dense 2D tracks into consistent 3D world-centric trajectories. Extensive evaluations across multiple dimensions demonstrate the effectiveness of our system."
        },
        {
            "title": "References",
            "content": "[1] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudo-labelling real videos. arXiv preprint arXiv:2410.11831, 2024. [2] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski, and Noah Snavely. Tracking everything everywhere all at once. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1979519806, 2023. 10 [3] Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker: Tracking any 2d pixels in 3d space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2040620417, 2024. [4] Tuan Duc Ngo, Peiye Zhuang, Chuang Gan, Evangelos Kalogerakis, Sergey Tulyakov, Hsin-Ying Lee, and Chaoyang Wang. Delta: Dense efficient long-range 3d tracking for any video. arXiv preprint arXiv:2410.24211, 2024. [5] Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, and Ning Yu. Go-with-the-flow: Motion-controllable video diffusion models using real-time warped noise. In CVPR, 2025. [6] Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, et al. Diffusion as shader: 3d-aware video diffusion for versatile video generation control. arXiv preprint arXiv:2501.03847, 2025. [7] Haonan Qiu, Zhaoxi Chen, Zhouxia Wang, Yingqing He, Menghan Xia, and Ziwei Liu. Freetraj: Tuningfree trajectory control in video diffusion models. arXiv preprint arXiv:2406.16863, 2024. [8] Adam Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. Particle video revisited: Tracking through occlusions using point trajectories. In European Conference on Computer Vision, pages 5975. Springer, 2022. [9] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adria Recasens, Lucas Smaira, Yusuf Aytar, Joao Carreira, Andrew Zisserman, and Yi Yang. Tap-vid: benchmark for tracking any point in video. Advances in Neural Information Processing Systems, 35:1361013626, 2022. [10] Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, and Andrew Zisserman. Tapir: Tracking any point with per-frame initialization and temporal refinement. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1006110072, 2023. [11] Seokju Cho, Jiahui Huang, Jisu Nam, Honggyu An, Seungryong Kim, and Joon-Young Lee. Local all-pair correspondence for point tracking. In European Conference on Computer Vision, pages 306325. Springer, 2024. [12] Hongyang Li, Hao Zhang, Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, and Lei Zhang. Taptr: Tracking any point with transformers as detection. In European Conference on Computer Vision, pages 5775. Springer, 2024. [13] Artem Zholus, Carl Doersch, Yi Yang, Skanda Koppula, Viorica Patraucean, Xu Owen He, Ignacio Rocco, Mehdi SM Sajjadi, Sarath Chandar, and Ross Goroshin. Tapnext: Tracking any point (tap) as next token prediction. arXiv preprint arXiv:2504.05579, 2025. [14] Jinyuan Qu, Hongyang Li, Shilong Liu, Tianhe Ren, Zhaoyang Zeng, and Lei Zhang. Taptrv3: Spatial and temporal context foster robust tracking of any point in long video. arXiv preprint arXiv:2411.18671, 2024. [15] Tingyang Zhang, Chen Wang, Zhiyang Dou, Qingzhe Gao, Jiahui Lei, Baoquan Chen, and Lingjie Liu. Protracker: Probabilistic integration for robust and accurate point tracking. arXiv preprint arXiv:2501.03220, 2025. [16] Qiaole Dong and Yanwei Fu. Online dense point tracking with streaming memory. arXiv preprint arXiv:2503.06471, 2025. [17] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. In European Conference on Computer Vision, pages 1835. Springer, 2024. [18] Yunzhou Song, Jiahui Lei, Ziyun Wang, Lingjie Liu, and Kostas Daniilidis. Track everything everywhere fast and robustly. In European Conference on Computer Vision, pages 343359. Springer, 2024. [19] Ruijie Zhu, Yanzhe Liang, Hanzhi Chang, Jiacheng Deng, Jiahao Lu, Wenfei Yang, Tianzhu Zhang, and Yongdong Zhang. Motiongs: Exploring explicit motion guidance for deformable 3d gaussian splatting. arXiv preprint arXiv:2410.07707, 2024. [20] Haiwen Feng, Junyi Zhang, Qianqian Wang, Yufei Ye, Pengcheng Yu, Michael Black, Trevor Darrell, and Angjoo Kanazawa. St4rtrack: Simultaneous 4d reconstruction and tracking in the world. arXiv preprint arXiv:2504.13152, 2025. 11 [21] Bowei Zhang, Lei Ke, Adam Harley, and Katerina Fragkiadaki. Tapip3d: Tracking any point in persistent 3d geometry. arXiv preprint arXiv:2504.14717, 2025. [22] Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah Snavely, and Aleksander Holynski. Stereo4d: Learning how things move in 3d from internet stereo videos. CVPR, 2025. [23] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. Unidepth: Universal monocular metric depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1010610116, 2024. [24] David Yifan Yao, Albert Zhai, and Shenlong Wang. Uni4d: Unifying visual foundation models for 4d modeling from single video. arXiv preprint arXiv:2503.21761, 2025. [25] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. [26] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213229. Springer, 2020. [27] Bo Wang, Jian Li, Yang Yu, Li Liu, Zhenping Sun, and Dewen Hu. Scenetracker: Long-term scene flow estimation network. arXiv preprint arXiv:2403.19924, 2024. [28] Weirong Chen, Ganlin Zhang, Felix Wimbauer, Rui Wang, Nikita Araslanov, Andrea Vedaldi, and Daniel Cremers. Back on track: Bundle adjustment for dynamic scene reconstruction. arXiv preprint arXiv:2504.14516, 2025. [29] Jenny Seidenschwarz, Qunjie Zhou, Bardienus Duisterhof, Deva Ramanan, and Laura Leal-Taixé. Dynomo: Online point tracking by dynamic online monocular gaussian reconstruction. arXiv preprint arXiv:2409.02104, 2024. [30] Seokju Cho, Jiahui Huang, Seungryong Kim, and Joon-Young Lee. Seurat: From moving points to depth. arXiv preprint arXiv:2504.14687, 2025. [31] Yoni Kasten, Wuyue Lu, and Haggai Maron. Fast encoder-based 3d from casual videos via point track processing. arXiv preprint arXiv:2404.07097, 2024. [32] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2069720709, 2024. [33] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16111621, 2021. [34] Zhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Rubinstein, Noah Snavely, and William Freeman. Structure and motion from casual videos. In European Conference on Computer Vision, pages 2037. Springer, 2022. [35] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast, and robust structure and motion from casual dynamic videos. arXiv preprint arXiv:2412.04463, 2024. [36] Jiahao Lu, Jiacheng Deng, Ruijie Zhu, Yanzhe Liang, Wenfei Yang, Tianzhu Zhang, and Xu Zhou. Dn-4dgs: Denoised deformable network with temporal-spatial aggregation for dynamic scene rendering. arXiv preprint arXiv:2410.13607, 2024. [37] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. arXiv preprint arXiv:2410.03825, 2024. [38] Jiahao Lu, Tianyu Huang, Peng Li, Zhiyang Dou, Cheng Lin, Zhiming Cui, Zhen Dong, Sai-Kit Yeung, Wenping Wang, and Yuan Liu. Align3r: Aligned monocular depth estimation for dynamic videos. arXiv preprint arXiv:2412.03079, 2024. [39] Yukang Cao, Jiahao Lu, Zhisheng Huang, Zhuowen Shen, Chengfeng Zhao, Fangzhou Hong, Zhaoxi Chen, Xin Li, Wenping Wang, Yuan Liu, et al. Reconstructing 4d spatial intelligence: survey. arXiv preprint arXiv:2507.21045, 2025. 12 [40] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. arXiv preprint arXiv:2501.12387, 2025. [41] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [42] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. [43] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. [44] Nan Huang, Wenzhao Zheng, Chenfeng Xu, Kurt Keutzer, Shanghang Zhang, Angjoo Kanazawa, and Qianqian Wang. Segment any motion in videos. arXiv preprint arXiv:2503.22268, 2025. [45] Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from single video. arXiv preprint arXiv:2407.13764, 2024. [46] Daniel Butler, Jonas Wulff, Garrett Stanley, and Michael Black. naturalistic open source movie for optical flow evaluation. In Computer VisionECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part VI 12, pages 611625. Springer, 2012. [47] Emanuele Palazzolo, Jens Behley, Philipp Lottes, Philippe Giguere, and Cyrill Stachniss. Refusion: 3d reconstruction in dynamic environments for rgb-d cameras exploiting residuals. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 78557862. IEEE, 2019. [48] Jürgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. benchmark for the evaluation of rgb-d slam systems. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages 573580. IEEE, 2012. [49] Weirong Chen, Le Chen, Rui Wang, and Marc Pollefeys. Leap-vo: Long-term effective any point tracking In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern for visual odometry. Recognition, pages 1984419853, 2024. [50] Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. Particlesfm: Exploiting dense point trajectories for localizing moving cameras in the wild. In European Conference on Computer Vision, pages 523542. Springer, 2022. [51] Zachary Teed, Lahav Lipson, and Jia Deng. Deep patch visual odometry. Advances in Neural Information Processing Systems, 36, 2024. [52] Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. Advances in neural information processing systems, 34:1655816569, 2021. [53] Johannes Lutz Schönberger and Jan-Michael Frahm. Structure-from-motion revisited. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [54] Aleksei Bochkovskii, Amaël Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. arXiv preprint arXiv:2410.02073, 2024. [55] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. arXiv preprint arXiv:2409.02095, 2024. [56] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023. [57] Xiaqing Pan, Nicholas Charron, Yongqian Yang, Scott Peters, Thomas Whelan, Chen Kong, Omkar Parkhi, Richard Newcombe, and Yuheng Carl Ren. Aria digital twin: new benchmark dataset for egocentric 3d machine perception. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2013320143, 2023. [58] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser Sheikh. Panoptic studio: massively multiview system for social motion capture. In Proceedings of the IEEE international conference on computer vision, pages 33343342, 2015. 13 [59] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. Dense optical tracking: Connecting the dots. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19187 19197, 2024. [60] Guangyang Wu, Xiaohong Liu, Kunming Luo, Xi Liu, Qingqing Zheng, Shuaicheng Liu, Xinyang Jiang, Guangtao Zhai, and Wenyi Wang. Accflow: Backward accumulation for long-range optical flow. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1211912128, 2023. [61] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 724732, 2016. [62] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv preprint arXiv:2406.09414, 2024. [63] Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang, Yujun Shen, Matteo Poggi, and Yiyi Liao. Learning temporally consistent video depth from video diffusion priors. arXiv preprint arXiv:2406.01493, 2024. [64] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. arXiv preprint arXiv:2503.11651, 2025. 14 Appendix / supplemental material A.1 More implementation details For hyperparameters, the stride in Sec. 3.2 follows the same setting as in DELTA and is set to 4. The temperature τ is set to 0.1. Each video clip contains 5 frames. The perturbation parameter ε is also set to 0.1. A.2 Extended explanation of optimization losses In dynamic background refinement (Stage 2), the depth consistency loss Ldc is defined as Ldc = Nstatic(cid:88) (cid:88) i=1 t=1 d(T static(i, t), πt) Dstatic(i, t)2 2 , (6) where d() denotes the depth function, which computes the depth of the static point static(i, t) after it has been transformed into the camera coordinate system at timestep t, Dstatic(i, t) means the depth value for the i-th static point on time step t. The total loss is defined as, Lstatic = λbaLba + λdcLdc + λasapLasap, (7) where the weights are set as follows for all datasets: λba = 1, λdc = 1, and λasap = 5. In dynamic object tracking (Stage 3), the as-rigid-as-possible loss Larap [45, 24] is used to constrain geometric deformation and prevent extreme shape changes. Specifically, for each dynamic control point in Tdynamic, we apply KNN to find its nearest neighbors among other tracks and enforce that the relative distances between these neighboring points remain consistent over time. The loss is formulated as: Larap = (cid:88) (cid:88) (cid:88) t=1 jN (k) (Tdynamic(k, t) Tdynamic(j, t)) (Tdynamic(k, 1) Tdynamic(j, 1))2 2 , (8) where (k) denotes the set of nearest neighbors of control point k, and Tdynamic(k, t) is the position of point at timestep t. In addition to the geometric constraint, we also introduce temporal constrainttemporal smoothness loss Lts [24]to penalize abrupt changes across frames and ensure temporal coherence. It is defined as: Lts = (cid:88) (cid:88) t=1 Tdynamic(k, t) Tdynamic(k, 1)2 2 , (9) which enforces first-order smoothness in the trajectories of dynamic control points. The total loss is defined as: Ldyn = λbaLba + λdcLdc + λarapLarap + λtsLts, (10) where the weights are set as follows for all datasets: λba = 1, λdc = 1, λarap = 100, λts = 10. A.3 Speeding-up the optimization Directly using all tracks in camera pose optimization can be computationally prohibitive due to their large quantity. To mitigate this issue, we propose simple strategy that reduces optimization overhead while preserving the final trajectory density. Specifically, we apply downsampling to the static tracking points while keeping the dynamic points intact. The rationale is twofold: (1) Optimizing static points involves jointly estimating both camera poses and static trajectories, which is computationally more expensive. In contrast, optimizing dynamic points only requires recovering dynamic trajectories, incurring much lower cost. (2) Dynamic motion is of primary importance for understanding the scene. Downsampling dynamic points may cause the loss of fine-grained motion details, which we aim to preserve. Specifically, we first downsample the static tracking points on the image plane by factor of 1 ϖ2 . Let denote the timestep, and let its coordinate in that frame be Pstatic(i, t). We perform downsampling by 15 Figure 5: Effectiveness of the speeding-up strategy. dividing both the and components of Pstatic(i, t) by ϖ and rounding them to the nearest integers: (cid:18) Px Py (cid:19)"
        },
        {
            "title": "Pdown",
            "content": "static (i, t) = round static(i, t) ϖ , static(i, t) ϖ , (11) where round() denotes rounding to the nearest integer. We then concatenate the initial frame index with the downsampled coordinate to form spatiotemporal key (t, Pdown static (i, t)). unique operation is applied to this set of keys to eliminate duplicates and retain only unique spatiotemporal locations. As result, we obtain reduced yet representative set of tracking points Pdown static for subsequent optimization. Next, we optimize the downsampled tracking points with the procedures described in the main paper. Upon completion of this optimization, we obtain the world-centric tracking points Tdown static , and then perform an upsampling process to densify the tracking points and reconstruct the full-resolution trajectories. Specifically, for each tracking point i, we consider its position on the image plane in the initial frame, denoted as Pstatic(i, t). Using the associated depth value Dstatic(i, t) and the estimated camera intrinsics, we back-project this point into the camera-centric 3D space to obtain P3d static can be back-projected into 3D space to yield Pdown,3d static(i, t). Similarly, the downsampled point Pdown . static To facilitate the upsampling, we search for the nearest neighbors of P3d static(i, t) among the downsampled 3D points Pdown,3d . Importantly, we restrict the search to those downsampled points that are visible in the current frame. Once the nearest neighbors are identified, we perform inverse-distance weighted interpolation to reconstruct the high-resolution trajectories. Concretely, we first compute the indices and distances of the neighbors using the knn module: static idx, dists = knn(P3d static(i, t), Pdown,3d static , = + 1), (12) where = + 1 to include the query point itself. We then compute the interpolation weights as follows: wx = ˆwx = 1 distsx + ϵ wx wy , (cid:80) , (13) (14) where ϵ is small constant added for numerical stability. Finally, we recover the full-resolution world-centric trajectories by computing weighted aggregation over the downsampled points: static = F( ˆwx, Tdown where denotes the weighted aggregation function. The visualization of the proposed speeding-up strategy is shown in Fig. 5, and its quantitative effectiveness on the Sintel dataset is reported in Table 9. static ), Tfull (15) speeding-up strategy ATE RTE RRE Abs Rel δ < 1.25 (min) Sintel 0.089 0.088 0.034 0.035 0.414 0.410 0.207 0. 73.8 73.3 60 8 Table 9: Quantitative evaluation of the speeding-up strategy on the Sintel dataset. The proposed method achieves similar accuracy with reduced optimization time. 16 A.4 Consistent Video Depth Generation. Our method is capable of producing temporally consistent video depth, as it performs dense 3D tracking for nearly all pixels. This enables the generation of consistent depth sequences through straightforward interpolation-based propagation step. Assume we have obtained dense 3D tracking points across video sequence. For each frame {1, 2, . . . , }, let Mt denote the set of visible 3D tracking points and Dt their corresponding camera-centric depths. We also denote the raw monocular depth prediction for frame as Draw . To align the raw monocular depth with our optimized tracking-based depth, we compute per-point local scale ratio rather than global frame-wise scale. For each tracking point Mi Mt with image-plane projection pi t, the scale ratio is defined as: Dt(pi t) (pi Draw t) ri = . (16) For an arbitrary pixel pt in frame t, its final aligned depth ˆDt(pt) is obtained by propagating the local scale information from nearby 3D tracking points. Let Nt(pt) Mt denote its nearest neighbors in the image plane. For each neighbor Mj Nt(pt), we compute the 3D distance based on their raw-depth-lifted coordinates: Pt = Draw (pt) 1 pt, Qj = Draw (pj ) 1 pj , = Pt Qj dj where is the camera intrinsic matrix and denotes the homogeneous coordinate of pixel p. 2, We assign each neighbor an inverse-distance weight: wj = 1 dj + ϵ , wj = wj j=1 wj (cid:80)k , (17) (18) (19) where ϵ is small constant for numerical stability. The interpolated local scale ratio at pixel pt is then computed as: rpt = (cid:88) j= wj rj , and the final aligned depth is given by: ˆDt(pt) = rpt Draw (pt). (20) (21) Through this weighted propagation, accurate scale information from the dense 3D tracks is effectively diffused across the entire image, producing temporally consistent and spatially coherent video depth sequence. As shown in Tab. 10, A.5 Runtime comparison with baseline To further evaluate the efficiency and effectiveness of our optimization strategy, we compare our method with strong baseline that combines camera poses and consistent depths from Uni-4D [24] with dense camera-centric 3D tracking from DELTA. Since both methods share the same 3D tracking backbone, the comparison focuses on optimization runtime and accuracy. Uni4D estimates camera poses in streaming manner, where the pose between consecutive frames is computed step by step. This results in total runtime that scales linearly with the video length. In contrast, our approach performs pose estimation in clip-to-global parallel manner, significantly improving computational efficiency. In terms of 3D tracking accuracy, our method achieves higher precision by effectively distinguishing static and dynamic regions using dynamic masks, and by jointly optimizing both camera poses and 3D trajectories. This joint optimization leads to improved geometric consistency and reconstruction quality. 17 Category Method Sintel Bonn Abs Rel δ < 1.25 Abs Rel δ < 1.25 Abs Rel δ < 1.25 TUM Depth Anything V2 [62] Single-frame Depth Pro [54] ZoeDepth [56] depth Unidepth [23] Video depth ChronoDepth [63] DepthCrafter [55] DUSt3R [32] Joint video MonST3R [37] depth & pose Align3R (Depth Pro) [38] Ours (DELTA [4]) Ours (CoTrackerV3 [1]) 0.348 0.418 0.467 0.473 0.687 0.292 0.422 0.335 0.263 0.222 0.232 59.2 55.9 47.3 63. 48.6 69.7 54.2 58.6 64.1 72.6 71.4 0.106 0.068 0.087 0.057 0.100 0.075 0.144 0.063 0.058 0.058 0.054 92.1 97.4 94.8 97. 91.1 97.1 84.5 96.4 97.1 97.3 97.3 0.211 0.126 0.176 0.113 / / 0.239 0.301 0.111 0.086 0.090 78.0 89.3 74.5 91. / / 71.1 55.8 88.9 92.3 91.7 Table 10: Video depth estimation results. We evaluate our model on three datasets: Sintel, Bonn and TUM D. Best results are highlighted. We report detailed quantitative results on camera pose estimation (Sintel, frames 3050) and worldcoordinate 3D tracking (ADT, first 64 frames) in Tab. 11. All experiments are conducted on the same hardware configuration for fair comparison. The results demonstrate that our method achieves both higher accuracy and lower runtime compared to the Uni4D baseline. The improvement mainly stems from our clip-to-global parallel optimization strategy and the integration of dynamic mask filtering, which together enhance efficiency and reconstruction quality. Setting Sintel ADT ATE RTE RPE Avg. Time (min) APD3D Avg. Time (min) Uni4D + DELTA 0.118 0.087 Ours (DELTA) 0.048 0.036 0.610 0.406 19 68.95 75.18 28 20 Table 11: Runtime and accuracy comparison on Sintel (3050 frames) and ADT (first 64 frames). A.6 More ablation study Ablation on the filtering mechanism. In this section, we introduce the details of the filtering process. We first compute the complement set by discarding pixels that lie in any previously visible 2D track trajectory. However, this operation may introduce isolated pixels, which are typically not meaningful for downstream scene understanding. To mitigate this, we construct connected components from the newly selected 2D tracking points and apply size threshold τ to remove small components. Only connected regions with more than τ pixels are retained. This ensures that the preserved 2D tracks are geometrically meaningful and more likely to correspond to coherent object parts. We found that this filtering procedure consistently improves accuracy, as most filtered points are either outliers or redundantly close to already tracked regions. Moreover, the threshold τ is robust across different scenes: we use fixed value of τ = 50 in all experiments without additional tuning. Setting Sintel Bonn ATE RTE RPE ATE RTE RPE w/o Filtering w/ Filtering 0.105 0.088 0.038 0. 0.442 0.410 0.018 0.016 0.007 0.005 0.601 0.564 Table 12: Ablation study on the filtering mechanism on Sintel and Bonn datasets. Filtering improves both camera pose estimation and tracking stability. A.7 More visualization A.7.1 Comparison on camera pose estimation Fig. 6 illustrates qualitative comparisons of camera pose estimation on the Sintel [46], Bonn [47], and TUM-D [48] datasets. We evaluate our method against two joint depth and pose estimation baselines: DUSt3R [32] and MonST3R [37]. As shown, our method produces camera trajectories that are more stable and better aligned with the ground truth, reflecting enhanced robustness and accuracy. A.7.2 World-centric dense tracking results We present additional visualizations of the world-centric tracking results in Fig. 7. To enhance clarity, we only visualize the point clouds on temporally spaced keyframes. However, the displayed trajectories are computed by connecting 3D tracks across all frames, capturing the complete motion over time. For more vivid and dynamic results, please refer to the accompanying video in the supplementary material. A.8 Limitation and future work Our method currently relies on several auxiliary models to obtain 2D tracks, monocular depths, and dynamic masks. This dependence introduces additional computational overhead and imposes stringent quality requirements on these components. In the future, feed-forward solutions may offer more suitable and efficient direction. For instance, St4RTrack [20] adopts feed-forward design, but its pair-wise matching strategy inherently suffers from drift accumulation, which requires global optimization for correction. Inspired by VGGT [64], promising direction may involve jointly processing all frames to directly predict the state of each frame across time. This could potentially enable more consistent and globally coherent trajectory estimation. A.9 Assets availability The datasets used in this study and their respective licenses are listed below: Sintel [46]: Available at http://sintel.is.tue.mpg.de/. This dataset is intended for optical flow evaluation. Please refer to the official website for specific license information. Bonn RGB-D Dynamic Dataset [47]: Available at https://www.ipb.uni-bonn.de/ data/rgbd-dynamic-dataset/, licensed under the Creative Commons AttributionNonCommercial-ShareAlike 3.0 Unported License. TUM RGB-D Dataset [48]: Available at https://cvg.cit.tum.de/data/datasets/ rgbd-dataset, licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0). DAVIS [61]: Available at https://davischallenge.org/. According to the DAVIS 2017 Challenge, the dataset is licensed under the Creative Commons Attribution 4.0 License. Aria Digital Twin (ADT) [57]: Available at https://www.projectaria.com/ datasets/adt/. Provided by Meta Reality Labs Research; please consult the official website for license details. Panoptic Studio Dataset [58]: Available at https://www.cs.cmu.edu/hanbyulj/ panoptic-studio/. Provided by Carnegie Mellon University; please refer to the project website for license information. CVO Dataset [60]: Available at https://github.com/mulns/AccFlow/blob/main/ data/README.md, licensed under the MIT License. 19 Figure 6: Camera pose estimation comparison on the Sintel [46] Bonn [47] and TUM-D [48] datasets. Figure 7: More Qualitative results. Our method can output 3D tracks in world-centric coordinate system."
        }
    ],
    "affiliations": [
        "CUHK",
        "HKU",
        "HKUST",
        "MUST",
        "USTC",
        "XMU"
    ]
}