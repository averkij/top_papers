{
    "paper_title": "MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at Scale",
    "authors": [
        "Ran Xu",
        "Yuchen Zhuang",
        "Yishan Zhong",
        "Yue Yu",
        "Xiangru Tang",
        "Hang Wu",
        "May D. Wang",
        "Peifeng Ruan",
        "Donghan Yang",
        "Tao Wang",
        "Guanghua Xiao",
        "Carl Yang",
        "Yang Xie",
        "Wenqi Shi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce MedAgentGYM, the first publicly available training environment designed to enhance coding-based medical reasoning capabilities in large language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across 129 categories derived from authentic real-world biomedical scenarios. Tasks are encapsulated within executable coding environments, each featuring detailed task descriptions, interactive feedback mechanisms, verifiable ground-truth annotations, and scalable training trajectory generation. Extensive benchmarking of over 30 LLMs reveals a notable performance disparity between commercial API-based models and open-source counterparts. Leveraging MedAgentGYM, Med-Copilot-7B achieves substantial performance gains through supervised fine-tuning (+36.44%) and continued reinforcement learning (+42.47%), emerging as an affordable and privacy-preserving alternative competitive with gpt-4o. By offering both a comprehensive benchmark and accessible, expandable training resources within unified execution environments, MedAgentGYM delivers an integrated platform to develop LLM-based coding assistants for advanced biomedical research and practice."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 5 0 4 4 0 . 6 0 5 2 : r MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at Scale Ran Xu, Yuchen Zhuang, Yishan Zhong, Yue Yu, Xiangru Tang, Hang Wu, May D. Wang, Peifeng Ruan, Donghan Yang, Tao Wang, Guanghua Xiao, Carl Yang, Yang Xie, Wenqi Shi Emory University, Georgia Tech, Yale University, UT Southwestern Medical Center MedAgentGym: https://github.com/wshi83/MedAgentGym MedAgentGym: https://huggingface.co/MedAgentGym"
        },
        {
            "title": "Abstract",
            "content": "We introduce MedAgentGym, the first publicly available training environment designed to enhance coding-based medical reasoning capabilities in large language model (LLM) agents. MedAgentGym comprises 72, 413 task instances across 129 categories derived from 12 authentic real-world biomedical scenarios. Tasks are encapsulated within executable coding environments, each featuring detailed task descriptions, interactive feedback mechanisms, verifiable ground-truth annotations, and scalable training trajectory generation. Extensive benchmarking of over 25 LLMs reveals notable performance disparity between commercial API-based models and open-source (OSS) LLMs. Leveraging MedAgentGym, Med-Copilot-7B achieves substantial performance gains through supervised fine-tuning (+36.44%) and continued reinforcement learning (+42.47%), emerging as an affordable and privacy-preserving alternative competitive with gpt-4o. By offering both comprehensive benchmark and accessible, expandable training resources within unified execution environments, MedAgentGym delivers an integrated platform to develop LLM-based coding assistants for advanced biomedical research and practice. (a) Benchmark evaluations of SOTA and domain-specific LLMs across eight environments in MedAgentGym. (b) Overall score of MedAgentGym. Figure 1: An overview of leader-board evaluation on MedAgentGym. Substantial performance disparities between proprietary and OSS LLMs highlight the critical need for continued development of privacy-preserving, affordable LLMs to enhance complex code-based medical reasoning. Equal contribution. Correspondence to: {Yang.Xie,Wenqi.Shi}@UTSouthwestern.edu Preprint."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in large language models (LLMs) have significantly enhanced complex reasoning [49, 13], including code generation [7] and scientific discovery [69, 75, 93]. Concurrently, the exponential growth of healthcare data has intensified the need for specialized computational tools to support clinical practice, biomedical research, and healthcare administration [85, 40]. Many of these tasks, such as patient triage, risk prediction, and electronic health records (EHRs) querying, increasingly depend on computing systems to accurately process and interpret medical information [47, 33, 84]. However, the combination of deep medical expertise and proficient programming skills remains challenging for many clinicians and biomedical researchers. Intelligent systems that automate code generation for medical applications hold substantial promise for bridging this gap and accelerating healthcare AI innovations, yet existing LLMs still fall short of enabling fully automated medical workflows [82]. Designing effective medical coding agents poses unique challenges beyond general-purpose coding [4, 25]. Direct deployment of proprietary LLMs in clinical settings is often impractical due to privacy risks and substantial operational costs [43], whereas open-source (OSS) models typically lack robust clinical reasoning capabilities. Consequently, enhancing the medical coding proficiency of OSS LLMs remains critically important. Medical applications inherently require agents to manage diverse data modalities, such as unstructured clinical notes, structured EHRs, and complex biomedical sequences, while adhering to rigorous standards and regulatory compliance. However, the absence of comprehensive, code-centric medical benchmarks and specialized training environments further restricts the development of agents capable of generalizing effectively to real-world clinical scenarios. In this study, we introduce MedAgentGym, unified training environment specifically designed for optimizing coding capabilities of LLM agents in medical reasoning. MedAgentGym is grounded in diverse, real-world biomedical scenarios and offers the following key features: Comprehensive suite of code-centric medical reasoning tasks. Unlike prior benchmarks for natural language-based medical reasoning [23, 50, 77], MedAgentGym introduces rich collection of 72,413 coding instances across 129 categories grounded in real-world biomedical scenarios2. Tasks span structured medical information retrieval [33, 58, 79, 30, 52], numerical medical reasoning [28, 21], bioinformatics modeling [71, 82, 57], and machine learning (ML) [84, 80]. MedAgentGym also involves diverse data modalities, including clinical notes, laboratory reports, EHR tables, and biological sequences, which require domain-specific reasoning capabilities. Efficient and scalable training infrastructure. MedAgentGym provides an optimized, userfriendly environment to accelerate agent training. Each task is encapsulated within executable, isolated, and reproducible Docker environments with pre-install dependencies, supporting multithreading, parallel execution, and sequential sampling. MedAgentGym ensures efficient trajectory collection and facilitate large-scale automated evaluation compatible with diverse agent scaffolds. Extensive benchmarking of frontier LLMs. We systematically benchmark more than 25 APIbased proprietary and OSS LLMs across all tasks in MedAgentGym. This large-scale evaluation reveals the limitations of current LLMs in solving complex medical coding and justifies the urgent need for further research. Enabling lightweight yet capable coding agents for medical reasoning. MedAgentGym effectively improves lightweight OSS models (Figure 1), as demonstrated by Med-Copilot-7B achieving significant improvements of +36.44% by supervised fine-tuning (SFT) and +42.47% by continued reinforcement learning (RL), matching leading proprietary LLMs (e.g., gpt-4o) as an affordable and privacy-preserving alternative. In summary, MedAgentGym fills critical gap in developing medical coding agents by offering the first unified, task-rich environment tailored to the demands of healthcare applications. By combining diverse real-world biomedical tasks, an efficient and reproducible infrastructure, and large-scale benchmarking across frontier LLMs, MedAgentGym provides foundation for advancing LLM coding toward medical applications. We hope MedAgentGym will foster progress in building efficient, reliable, and clinically grounded AI agents to support real-world research and practice in healthcare. 2We emphasize that MedAgentGym mainly focuses on computational code generation for medical reasoning, rather than traditional medical coding systems [67] such as ICD-9 or ICD-10. 2 Table 1: Summary of related medical reasoning benchmarks with data resources and task execution environments. MedAgentGym is among the first publicly available training environments for improving LLM-based coding agents in biomedicine, uniquely integrating executable environments, interactive feedback, and task-isolated run-time facilities for coding-based medical reasoning. DS, Bioinfo, and ML denote data science, bioinformatics (biostatics), and machine learning, respectively. Features () Datasets () Domain Task Environment & Facility Scale (#Instances) QA Coding Database DS Bioinfo ML Execution Interaction Isolation Training # Train # Test # Traj. (cid:34) MedMCQA [50] (cid:34) MedQA [22] (cid:34) PubMedQA [23] (cid:34) BioASQ [77] MedAgentsBench [72] (cid:34) (cid:34) MIRAGE [89] (cid:34) HealthBench [1] (cid:37) EHRSQL [33] (cid:37) MedCalcBench [28] MedAgentBench [21] (cid:37) (cid:37) BioCoder [71] (cid:37) BioDSBench [82] (cid:37) EHRSHOT [84] (cid:37) MedAgentGym (Ours) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:37) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:34) (cid:34) (cid:37) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:37) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:34) (cid:37) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:37) (cid:34) (cid:37) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:37) (cid:37) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) 3K 4.18K 11.4K 1.27K 500 450 140 745 862 7.66K 5K (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) 15.5K 1.73K 10.1K 1.05K 300 1.24K 128 15 59.2K 13.2K 6.7K (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) The remainder of the paper is organized as follows: we review related work in section 2, describe our task curation and environment design in section 3, and present the experimental setup along with benchmark results for MedAgentGym in section 4. In section 5, we fine-tune Med-Copilot to showcase the effectiveness of MedAgentGym. We then conclude the paper in section 6."
        },
        {
            "title": "2 Related Works",
            "content": "Medical Agents (Coding). Recent advances have demonstrated that LLMs exhibit strong capabilities in medical reasoning and planning leveraging extensive biomedical knowledge [66, 45, 39], fueling increased interest in developing LLM-based autonomous agents tailored specifically for medical tasks [24, 10, 34, 38, 73, 29]. In particular, LLM-based agents have shown promise in specialized computational tasks, including querying EHR databases [65], performing biostatistical calculations [57], and conducting bioinformatics analyses [71, 82, 74]. As shown in Figure 2, integrating coding capabilities into LLM-based agents further enhances performance on tasks traditionally approached through natural language reasoning (e.g., MIMIC-III, eICU [33]), as well as numerical and rule-based medical reasoning (e.g., MedCalcBench [28]). However, existing coding-based medical agents rely primarily on prompt engineering without systematic improvement, limiting their robustness and scalability when addressing complex and diverse coding tasks in real-world biomedical scenarios. In contrast, MedAgentGym specifically targets reasoning-intensive coding tasks by introducing unified, scalable, and interactive training environment that systematically improves the coding-based medical reasoning capabilities of LLM agents. Figure 2: Coding empowers computational medical reasoning (w/ gpt-4-turbo). Medical Reasoning Models. Recent advancements have substantially improved medical reasoning capabilities of LLMs through RL [17, 31, 95, 20, 86, 3, 32, 78, 37]. For example, M1 [17] improves by distilling knowledge from the reasoning traces generated by DeepSeek-R1 [13]. MedS3 [20] employs Monte Carlo Tree Search (MCTS) to generate rule-verifiable reasoning trajectories and employs process-reward models to select optimal reasoning paths during inference. Similarly, HuatuoGPT-o1 [3] and ClinicalGPT-R1 [32] integrate domain-specific verifiers to guide RL finetuning processes for improved clinical reasoning. Extending beyond language modeling, MedR1 [31], Med-RLVR [95], and MedXpertQA [99] adapt RL methodologies to vision-language models, effectively addressing medical visual question answering tasks. Despite these developments, current medical reasoning models predominantly target natural language-based reasoning, with 3 limited attention given to coding-intensive scenarios common in biomedical research and clinical practice. Medical Reasoning Benchmarks. Table 1 summarizes representative medical reasoning benchmarks, many of which primarily assess LLM performance through closed-form medical QA tasks. In addition, AgentClinic [61] further evaluates diagnosis prediction within simulated clinical scenarios, while MedHELM [15] provides comprehensive evaluations across various medical NLP tasks. Despite these extensive benchmarking efforts, existing benchmarks including recent concurrent works such as MedAgentBoard [98], HealthBench [1], and MedCaseReasoning [87] typically focus on evaluation scenarios, with limited emphasis on dedicated training environments aimed at systematically improving medical reasoning capabilities [76], especially within coding-intensive and interactive medical scenarios. Medical Agent Training Environments. To advance medical agents with narrative reasoning, AgentClinic [61] and AgentHospital [35] simulate hospital workflows focused on diagnostic tasks, while MediQ [36] offers interactive simulations designed for medical information retrieval. Beyond medicine, specialized environments have emerged for systematically evaluating and improving LLM agents across diverse tasks [97, 83], such as software engineering [51, 91, 92], reasoning [68], web browsing [8], agent planning and collaboration [88, 63], data science [14, 25, 94, 96], machine learning engineering [46, 16, 2, 70], automated research [6, 27, 59, 60], and scientific discovery [75, 93]. Inspired by these interactive training frameworks, MedAgentGym uniquely targets real-world biomedical scenarios, aiming to rigorously benchmark and systematically enhance codingbased medical reasoning capabilities of LLM agents."
        },
        {
            "title": "3.1 Problem Formulation",
            "content": "We formulate coding-based medical reasoning as structured problem-solving task: given problem description , the goal is to generate code snippet that produces an output Y. Each instance (x, y) is paired with ground truth output y, and the correctness is verified using : {0, 1}, where = I(y = y). Existing medical reasoning datasets typically provide only question-answer pairs (x, y) without code solutions or only include single predefined code solution per task. To address this constraint, MedAgentGym enables scalable generation and sampling of multiple coding trajectories c(0), c(1), , c(k) with corresponding executions y(0), y(1), , y(k) through parallel execution of LLM agents. Each trajectory is either single-turn or multi-turn, depending on task complexity and user requirements. Crucially, MedAgentGym captures both positive trajectories {c(i)y(i) = y} that succeed and negative trajectories {c(i)y(i) = y} including error messages as learning signals. The details of the data and the interactive environment are in section 3.2 and section 3.3."
        },
        {
            "title": "3.2 Data Construction: From Individual Datasets to Comprehensive and Unified Benchmark",
            "content": "Task and Data Identification. MedAgentGym focuses on verifiable medical reasoning tasks that benefit from code-based solutions. Clinically, we prioritize tasks originating from real-world healthcare settings (Figure 3) and validated by multidisciplinary panel of healthcare experts. For example, MedAgentGym involves MIMIC-III and eICU in EHRSQL [33] collected from 222 hospital staff members and annotated by human programmers. Computationally, we integrate diverse biomedical coding tasks, ranging from structured medical information retrieval to open-ended biomedical research, ensuring comprehensive coverage and task diversity. Figure 3: Diversity analysis of MedAgentGym. Verifiable Instances Preparation. To standardize tasks across various sources, each instance in MedAgentGym is structured with: (1) problem description, (2) verifiable ground-truth outputs, and 4 Table 2: Dataset statistics for MedAgentGym and its lightweight subset designed for leaderboard evaluation. For open-ended tasks without explicit ground truth (e.g., ML coding), we follow standard RL settings by using the same dataset for training and evaluation. +Only the test set is utilized for external evaluation, while the corresponding training data remains accessible. Similarly, we provide subset of training data in leaderboard for efficiency and convenience, without restricting the incorporation of supplementary training resources. Dataset Type #Patients #Table #Elements Category #Train #Test #Total #Train #Test #Total Data Sources Tasks (all) Tasks (leader-board) Training and Internal Validation (In-Distribution) MIMIC-III [26, 33] eICU [52, 33] TREQS [79] MedCalcBench [28] MedAgentBench [21] BioCoder [71] EHRSHOT [84] BioDSBench [82] MedAgentGym (Internal) External Validation (Out-of-Distribution)+ Tabular Tabular Tabular Text Tabular Text Tabular Text <1K <1K 100 1K 100 63K 65K EHR-SeqSQL [58] EHRCon [30] MIMIC-Extract [80] N-PowerAI [57] MedAgentGym (External) Tabular Tabular Tabular Text <1K 46K 35K 82K Overall MedAgentGym 146K 17 10 5 31 63 17 13 4 34 80 1.4M 1.5M 2.5M 700K 1.2M 7.3M 1.4M 35K 1.4M 9 9 4 55 10 8 15 12 4 3 3 6 16 9,318 6,213 8,988 10,053 433 981 15 50 36,036 18,950 3,229 3 960 23,142 1,122 611 996 1,047 109 157 15 49 4,106 7,913 976 3 240 9,132 10,440 6,824 9,984 11,100 542 1,138 15 99 40, 26,863 4,205 3 1200 32,271 552 559 897 1,005 239 981 15 50 4,283 1,000 1,000 3 960 2,963 581 610 995 1,046 59 156 15 49 3,511 500 500 3 240 1,243 1,133 1,169 1,892 2,051 298 1,137 15 99 7, 1,500 1,500 3 1200 4,203 7.4M 129 59,175 13,238 72, 7,243 4,754 11,997 (3) optional data resources (e.g., EHRs). Additionally, standardized system and user prompts are designed to initiate the problem-solving process (see appendix F). MedAgentGym is highly flexible, easily accommodating new tasks that include clear descriptions and verifiable ground-truth outputs. For coding-centric tasks that provide only reference code implementations (e.g., BioCoder [71]), we validate task correctness based on the execution output of these reference solutions, generating definitive output signatures. This transformation is necessary because multiple valid code implementations may yield identical execution results, making the execution outcomerather than the code itselfa more reliable and consistent verification signal. For tasks involving additional data resources (e.g., EHRSQL [33]), we include metadata on data access and sources. Additional task-specific preparation details are documented in appendix B. Data Statistics. MedAgentGym is unified training environment built upon large-scale, high-quality dataset comprising approximately 72,000 task instances across 1,293 categories from 12 real-world biomedical scenarios. Notably, with MedAgentGym, we collect large-scale agent trajectories to support coding agent development (section 5). To ensure reproducible and robust evaluation, we define clear train/test splits, separate internal and external validation sets, and perform n-gram (n = 10) string match to eliminate the data contamination issue. Table 2 provides statistics for MedAgentGym. To accommodate diverse research needs, we offer two versions of MedAgentGym: (1) comprehensive, full-scale dataset for extensive exploration and detailed analysis, and (2) balanced, lightweight subset for efficient leaderboard training and evaluation."
        },
        {
            "title": "3.3 Coding Environment: From Static Benchmark to Interactive Interface",
            "content": "Isolated and Executable Environment. To ensure robust and reproducible coding-based medical reasoning, MedAgentGym provides isolated executable environments through Docker containers tailored to each task. These containers come pre-installed with all required dependencies, including specialized biomedical packages (e.g., AlignIO in BioCoder [71]), facilitating reliable task execution. To address critical data safety concerns, each Docker environment guarantees: (1) environmental integrity, where isolation prevents contamination or data corruption potentially caused by LLMgenerated code, preserving both the computational environment and the underlying data systems [91]; (2) medical data security, where secure containerization enforces compliance with medical data usage policies, safeguarding sensitive patient information. Additionally, MedAgentGym supports extensive flexibility for integrating new tasks, where users can define customized Docker environments through 5 configuration files. If certain packages are not initially available, terminal tool allows LLM agents to dynamically install the required dependencies within their isolated environments. Interactive Feedback. MedAgentGym incorporates interactive feedback mechanisms, effectively bridging LLMs with coding interpreters: (1) robust parsing: To begin, the output generated by LLMs is formatted in structured JSON, facilitating straightforward parsing and code execution. In cases of execution errors, iterative JSON regeneration is employed to maximize successful code execution rates. (2) debugging and error grounding: Compile-time and runtime error messages are systematically translated into unified natural language format, making them more accessible to LLMs and significantly improving debugging efficiency and interpretability. Efficient Trajectory Collection. Each task in MedAgentGym is packaged in reproducible Docker image with built-in support for multi-threading, parallel execution, and sequential sampling. Specifically, we integrate two widely used multi-threading backend engines, Ray3 and Joblib4, to accelerate trajectory sampling. This infrastructure ensures efficient and scalable trajectory collection, supporting both extensive experimentation and systematic evaluation across multiple scenarios. Plug-and-Play. key strength of MedAgentGym lies in its flexible and modular architecture, which readily supports the integration of new biomedical coding tasks. This inherent extensibility enables MedAgentGym to continually adapt to evolving advancements in biomedical sciences and artificial intelligence methodologies. Additionally, its trajectory sampling approach allows the straightforward transformation of traditional, non-executable medical reasoning tasks into coding-based scenarios with verifiable outputs, significantly broadening the scope and complexity of tasks that can be systematically evaluated. Moreover, users can define custom Docker environments through configuration files, and, if specific software packages are initially absent, built-in terminal tool facilitates dynamic installation within each isolated execution environment, further improving MedAgentGym in runtime adaptability and user-friendliness."
        },
        {
            "title": "4.1 Experiments Setup",
            "content": "Agent Scaffolds. Following CodeAct [81], we establish default agent scaffold for systematically evaluating coding-based medical reasoning. Interactions within MedAgentGym are modeled as Partially Observable Markov Decision Process (POMDP), focusing on sampled medical reasoning tasks P. At each timestep t, the agent observes ot and selects an action at+1 based on interaction history. We define four primary action types: (a) request_info: retrieve relevant data from sources such as EHRs; (b) terminal: manage dependencies or local files within isolated Docker environments. (c) code_execution: execute code generated by LLMs through an integrated interpreter; and (d) debugging: translate code execution errors into natural language explanations enriched with detailed error information for LLM comprehension. Tasks and Datasets. Building upon MedAgentGym, we train and evaluate Med-Copilot on 7,794 coding-based medical reasoning tasks from 113 categories across 8 datasets: (1) MIMIC-III [26] and (2) eICU [52] from EHRSQL [33], (3) TREQS [79], (4) MedCalcBench [28], (5) MedAgentBench [21], (6) BioCoder [71], (7) EHRSHOT [84], and (8) BioDSBench [82]. Moreover, we conduct experiments for out-of-distribution evaluation on 4,203 tasks from the following 4 datasets: (9) EHR-SeqSQL [58], (10) EHRCon [30], (11) MIMIC-Extract [80], and (12) N-PowerAI [57]. Note that we do not consider general medical question-answering tasks [23, 50, 22], as they are orthogonal to coding-aided reasoning. We include detailed task and dataset information in appendix B. Baselines. We extensively benchmark the following state-of-the-art LLMs on MedAgentGym. (i) API-based proprietary LLMs, including gpt-4o-mini [19], gpt-4o [19], gpt-4.1-mini [48], gpt4.1 [48], and gpt-o4-mini [49]; (ii) open-source small (OSS) models, including gemma-3 [11], Qwen3 [55], Qwen2.5 [90], Llama-3 [9], Ministral [44], and DeepSeek-R1 [13]; (iii) coding LLMs, including Qwen2.5-Coder-7B-Instruct and -14B-Instruct [18]; and (iv) medical reasoning LLMs or medical domain-specific LLMs, including medgemma-4b-it (gemma-3-4b-pt) [12], HuatuoGPTo1-7B (Qwen2.5-7B-Instruct) [3], m1-7B-23K (Qwen2.5-7B-Instruct) [17], MedReason-8B 3https://github.com/ray-project/ray 4https://joblib.readthedocs.io/en/stable/ 6 Table 3: Test set (zero-shot) results of MedAgentGym. Bold indicates the best result at each scale. : We only consider Microsoft Azure OpenAI API services due to credentialed health data use agreement. and denote coding LLMs and medical reasoning LLMs, respectively. Notations are consistent across tables. We present the best test-set performance of Med-Copilot fine-tuned on the MedAgentGym (section 5) training set for reference only. Datasets () Baselines () / Metrics () MIMIC. SR eICU TREQS MedCalc. MedAgent. BioCoder BioDS. EHRSHOT SR SR SR SR SR SR Acc Avg. Score API-based Proprietary LLMs gpt-4o-mini (2024-07-28) [19] gpt-4o (2024-08-06) [19] gpt-4.1-mini (2025-04-14) [48] gpt-4.1 (2025-04-14) [48] gpt-o4-mini (2025-04-16) [49] OSS (Base Size): < 10B parameters Qwen3-1.7B [55] Qwen3-4B [55] gemma-3-4b-it [11] medgemma-4b-it [12] Qwen3-8B [55] Qwen2.5-7B-Instruct [90] Llama-3.1-8B-Instruct [9] Ministral-8B-Instruct-2410 [44] Qwen2.5-Coder-7B-Instruct [18] HuatuoGPT-o1-7B [3] m1-7B-23K [17] MedReason-8B [86] Med-Copilot (Qwen2.5-7B-Instruct) OSS (Large Size): 10 - 30B parameters Qwen3-14B [55] Qwen2.5-14B-Instruct [90] DeepSeek-R1-Distill-Qwen-14B [13] Qwen2.5-Coder-14B-Instruct [18] Baichuan-M1-14B-Instruct [78] Med-Copilot (Qwen2.5-14B-Instruct) OSS (XL Size): > 30B parameters Qwen3-32B [55] Qwen2.5-32B-Instruct [90] QwQ-32B [56] DeepSeek-R1-Distill-Qwen-32B [13] Llama-3.3-70B-Instruct [9] DeepSeek-R1-Distill-Llama-70B [13] 35.97 43.04 62.79 69.36 76.45 20.12 27.23 27.36 15.51 29.08 13.08 16.67 16.70 9.12 4.99 6.88 9.12 64.13 31.50 17.21 35.12 41.82 4.50 64.54 52.48 54.56 62.31 62.18 39.93 64.59 16.57 43.44 63.44 64.75 70.16 10.62 30.77 29.10 13.11 34.53 15.57 25.00 14.92 10.66 7.04 9.56 9.51 66. 31.97 14.07 38.52 44.26 12.19 63.52 60.95 45.41 56.72 58.36 25.08 64.92 38.39 53.47 69.75 74.97 74.47 15.08 28.85 24.52 14.85 37.37 12.76 19.17 25.39 15.63 7.04 7.04 9.15 72.02 30.05 16.43 32.96 35.78 7.36 76.08 53.82 62.81 66.15 65.82 24.98 56. 73.11 73.97 84.36 86.23 78.45 46.24 52.80 42.49 41.89 54.59 25.91 27.53 49.81 24.62 38.05 28.24 43.31 90.06 61.38 27.40 48.09 33.75 1.82 92.45 63.82 69.96 67.69 60.14 84.99 76.96 40.38 54.23 54.23 57.63 59.32 16.95 15.25 18.64 17.62 20.34 30.36 16.95 22.03 18.75 18.64 9.32 21.46 52. 22.03 35.59 32.20 30.42 21.46 54.32 45.93 40.67 47.46 43.56 39.40 28.81 30.12 30.12 47.46 52.95 42.94 15.38 19.16 17.95 26.74 20.51 21.79 18.59 23.72 10.60 28.21 20.26 31.42 34.62 22.60 29.49 21.29 26.28 16.34 43.56 32.67 27.45 42.31 28.66 27.55 32. 57.35 58.16 63.47 67.35 73.47 6.12 20.41 8.16 17.82 24.49 10.20 9.19 12.24 17.24 19.88 14.71 17.42 69.39 26.53 16.33 24.49 22.45 17.42 92.96 28.57 22.45 14.29 26.53 24.49 42.86 7.84 33.53 48.28 87.93 50.07 1.87 23.85 4.37 1.33 25.71 5.42 2.36 7.79 10.55 5.03 0.00 3.88 29. 26.77 4.45 11.39 28.37 0.00 43.56 47.29 18.13 55.05 31.17 29.93 33.42 37.47 48.75 61.72 70.15 65.67 16.55 27.29 21.57 18.61 30.83 17.43 16.97 22.27 14.65 16.11 12.00 18.16 59.90 31.60 20.12 30.51 32.89 10.14 66.37 48.19 42.68 51.50 47.05 37.04 50. (Llama-3.1-8B-Instruct) [86], and Baichuan-M1-14B-Instruct [78]. Additional model details are available in appendix C. Evaluation Metrics. Following existing agent benchmarks [41], we adopt success rate (SR) as the primary evaluation metric. For database, data science, and bioinformatics tasks with explicit ground truths, we compare LLM-generated code execution outputs with reference solutions using exact match. For open-ended ML tasks in clinical decision support, we measure performance using accuracy (Acc) across provided test cases. Note that these code generation tasks inherently have infinite solution spaces, unlike traditional classification problems with bounded solution spaces (e.g., even random guessing can yield around 50% accuracy in binary classification). The overall score is computed by averaging performance across tasks in test sets of MedAgentGym (leaderboard), providing comprehensive evaluation of coding-based medical reasoning capabilities within MedAgentGym. Implementation Details. We limit interactions to maximum of 15 turns per session, providing agents full access to interaction histories and constraining runtime to 120 seconds per session. Input tokens are capped at 32, 768, with output limited to 8, 192 tokens per round. We use Python 3.10 as the primary language for agent-code execution due to its modular design and suitability for biomedical computations. To enable interactive feedback (section 3.3), we employ rule-based parser converting LLM outputs to JSON, facilitating seamless code execution, and utilize gpt-4.1-mini to translate execution errors into grounded explanations. We configure all baseline LLMs following established best practices for reproducibility. Specifically, instruction-following LLMs are configured with temperature of zero, while reasoning models use temperature of 0.6. For all experiments with Qwen-3 series, we switch to thinking mode for optimal performance under complex reasoning scenarios (e.g., logic, math, and coding). See appendix for additional implementation details."
        },
        {
            "title": "4.2 Results: Benchmarking LLMs and Reasoning Models with MedAgentGym",
            "content": "Table 3 benchmarks the state-of-the-art LLMs on MedAgentGym. We summarize key observations from our zero-shot leaderboard evaluation as follows: Significant Performance Gap Between Commercial API-based and OSS LLMs. Commercial API-based LLMs substantially outperform OSS models across all task categories. Specifically, o4-mini excels in structured medical information retrieval tasks, while gpt-4.1 demonstrates superior performance in complex computational tasks, such as medical calculation and bioinformatics analysis. In comparison, even strong OSS models such as Qwen3-32B lag significantly behind with more than 20% gap on performance. Interestingly, scaling OSS models from 4B to 14B yields minimal gains, with noticeable improvement only at 32B, indicating that coding-based medical reasoning capability emerges at larger scales. This evident performance gap highlights the critical need for continued development of lightweight OSS LLMs that match commercial performance while addressing real-world privacy and cost constraints. Task-Specific Performance Variations between Structured and Open-ended Medical Tasks. LLMs consistently perform better on structured tasks (e.g., database queries, medical calculations) compared to open-ended tasks requiring advanced coding and reasoning (e.g., data analysis, outcome prediction). MedCalcBench further challenges models with tasks requiring precise recall of domain knowledge, such as medical formulas and diagnostic guidelines. This highlights the diverse task composition in MedAgentGym, enabling comprehensive evaluation of coding-based medical reasoning. Enhanced Performance in Reasoning Models. OSS reasoning models, such as QwQ-32B and DeepSeek-R1 series, demonstrate superior performance in structured database querying and knowledge-intensive biomedical research tasks, indicating inherently robust reasoning capabilities essential for computational medical reasoning tasks in MedAgentGym. Suboptimal Outcomes in Dedicated Coding and Medical Domain-Specific LLMs. Both codingspecialized LLMs and domain-specific medical reasoning models deliver suboptimal performance, revealing that coding-based medical reasoning represents unique capability not adequately captured by specialization in either coding or medical reasoning. Surprisingly, medical reasoning models consistently underperform relative to their base models except for knowledge-intensive tasks (e.g., MedCalcBench, BioCoder), showing that fine-tuning in medical QA may reduce generalization and instruction-following ability. These findings highlight the need for training data integration to jointly support coding skills and medical reasoning, rather than treating them as separate objectives. 5 Med-Copilot: Training LLMs as Coding Agents for Medical Reasoning In this section, we leverage MedAgentGym to systematically enhance lightweight OSS LLMs as proficient coding agents (Med-Copilot) for medical reasoning. We first explore two-stage finetuning framework using SFT and direct preference optimization (DPO) (section 5.1), followed by detailed analysis of model scaling behaviors (section 5.2). We then introduce self-improvement strategy to further boost agent performance (section 5.3) and conduct additional analysis on model generalization, ablation, and error patterns (section 5.4)."
        },
        {
            "title": "5.1 Two-Stage Fine-tuning with Trajectory Sampling",
            "content": "Training Setup. We select Qwen-2.5-Instruct-7B and -14B [90] as our backbones. To enable effective evaluation within MedAgentGym, we utilize consistent CodeAct-style scaffold, allowing LLM agents to iteratively reason and refine biomedical code through interactive environment feedback. Detailed training setups, including hyperparameters, are provided in appendix D. Trajectory Sampling. MedAgentGym facilitates efficient parallel trajectory sampling using ray and joblib backends. Specifically, we roll out (1) 2,137 successful trajectories using gpt-4.1-mini with Table 4: Trajectory Composition (%). Actions () request info terminal code debug MIMIC-III eICU TREQS MedCalc. Structured MedAgent. BioCoder BioDS. EHRSHOT Open-ended 71.07 72.17 64.27 51.88 0 0 0 0 0 MedAgentGym 32.71 0 0 0 0 0 0.29 6.30 0.43 1.76 0.14 28.84 27.13 35.54 74.91 41. 100 96.11 87.60 59.43 85.79 57.11 0.08 0.7 0.19 25.09 6.52 0 3.60 6.90 40. 12.46 10.04 8 (a) Structured Tasks (b) Open-Ended Tasks (c) Overall Figure 4: Med-Copilot SFT performance on MedAgentGym across various backbone LLMs. Table 5: Med-Copilot model performance on MedAgentGym finetuned with sampled trajectories. Datasets () Base () / Metrics () Qwen2.5-7B-Instruct +SFT +DPO +SFT&DPO Qwen2.5-14B-Instrust +SFT +DPO +SFT&DPO MIMIC-III SR 13.08 57.83 49.59 64.13 17.21 61.45 57.49 64.54 eICU TREQS MedCalc. MedAgent. BioCoder BioDS. EHRSHOT Avg. SR 15.57 61.48 43.61 66.91 14.07 62.46 59.18 63.52 SR 12.76 72.66 46.68 72.02 16.43 76.38 70.45 76.08 SR 25.91 89.06 49.20 90.06 27.40 94.36 71.32 92.45 SR 30.36 50.85 45.25 52.54 35.59 52.54 47.46 54.32 SR 21.79 28.33 30.13 34.62 29.49 39.80 42.95 43.56 SR 10.20 55.10 69.39 69.39 16.33 89.80 91.84 92.96 Acc 5.42 15.62 26.43 29.55 4.45 34.58 41.33 43.56 Score 16.89 53.87 45.04 59.90 20.12 63.92 60.25 66.37 (+36.98) (+28.15) (+43.02) (+43.80) (+40.13) (+46.25) temperature of 0 to warm up the fine-tuning for smaller OSS models. Each successful trajectory contains 9.25 turns between the LLM and the code interpreter on average. Although MedAgentGym contains extensive training data and allows repeated sampling, the current trajectory count primarily reflects computational budget constraints. Table 4 details the proportion of action types (section 4.1) in trajectories. Structured tasks predominantly involve data retrieval (over 50%) from databases or resources, complemented by coding and debugging steps. In contrast, open-ended tasks require significant coding and debugging efforts due to diverse question types, often necessitating terminal interactions to install specialized biomedical packages. In addition to 2,137 positive trajectories for SFT, we prepare additional trajectory pairs for RL such as DPO, including (2) 1,646 offline pairs sampled from gpt-4.1-mini, and (3) 2,939 online pairs. For both types, we use the initial prompt interactions as shared context and contrast successful final codes against intermediate erroneous attempts. We release all 6K trajectories above to accelerate coding agent development. Two-Stage Fine-Tuning. We benchmark two policy improvement methods: (1) SFT directly mimics high-reward trajectories consisting exclusively of successful outcomes, whereas (2) DPO optimizes the policy by favoring selected responses over rejected sampling. We further consider two-stage fine-tuning framework, initially warming up with SFT and subsequently refining with DPO. Results: SFT and DPO. Figure 4 highlights substantial performance gains from SFT across four OSS backbone LLMs of varying sizes. Table 5 compares several post-training methods, revealing that simple SFT over successful trajectories significantly boosts performance on structured coding tasks, demonstrating its effectiveness in capturing structured coding patterns. Besides, DPO is particularly beneficial for optimizing open-ended task performance. Although DPO alone slightly underperforms compared to SFT, combining an initial SFT warm-up with subsequent DPO further improves overall results by leveraging their complementary strengths."
        },
        {
            "title": "5.2 Scaling LLM Agent Improvements with MedAgentGym",
            "content": "Verifier Training Setup. In addition to directly training coding agents, MedAgentGym facilitates the development of an outcome-supervised reward model (ORM) to evaluate generated solutions effectively. Inspired by prior work [5, 51], we formalize the verifier task as predicting the probability that given trajectory successfully solves coding task. Formally, we represent trajectory as an interleaved sequence τ = [o1, a1, o2, a2, , on, an], [0, 1], where each observation ok comprises elements such as task descriptions, code execution results, and error feedback. We finetune Qwen2.5-7B-Instruct model as verifier with binary predictions YES (ly) or NO (ln), from which we compute success probability: = exp(ly)/(exp(ly) + exp(ln)). 9 Figure 5: Scalable improvements of LLM agents in MedAgentGym. For inference-time scaling, we employ = 0 for the initial rollout and = 0.6 for the rest. For train-time scaling, we set = 0. (a) Self-Improvement (b) Effect of Debug (c) Error Types Figure 6: Additional studies. For better visualization, we include ML as part of Bio-info. category. Verifier Training Data. We construct the verifier training dataset by combining two sets of trajectories originally sampled for agent training: (1) off-policy trajectories, consisting of 2,742 samples from gpt-4.1-mini; and (2) on-policy trajectories, comprising 2,939 samples generated by the agent (Qwen2.5-7B-Instruct). Combining both onand off-policy trajectories, we ensure balanced dataset of successful and unsuccessful trajectories, filtering to fit within maximum context length of 32k tokens. Results: Inference-Time Scaling. We introduce two additional evaluation metrics: (1) Pass@K: the fraction of tasks solved by at least one trajectory from sampled attempts; and (2) Best@K: the fraction of accurately selects successful trajectories. Figure 5 (left) illustrates the performance scaling with increasing trajectory sampling. Pass@K significantly improves from 17.0% at = 1 to 45.0% at 16, while Best@K shows steady advancement from 17.0% to 41.7%. The relatively small gap between metrics indicates that our trained verifier effectively identifies successful trajectories, unleashing its potential as reward model for integration into advanced online RL frameworks such as Proximal Policy Optimization (PPO) [62] and Group Relative Policy Optimization (GRPO) [64]. Results: Training-Time Scaling. Figure 5 (right) examines agent performance as function of increased training data volumes (25%, 50%, 75%, and 100%) in SFT. We observe consistent performance improvements with greater training data availability, suggesting additional computational resources dedicated to sampling further trajectories are likely to yield continued performance gains."
        },
        {
            "title": "5.3 Model Performance Scaling with Self-improvement",
            "content": "Self-Improvement Training Setup. Beyond expert-generated trajectories, we explore selfimprovement by refining the model using its own outputs. We employ rejection sampling fine-tuning (filtered behavior cloning), using the verifier from section 5.2 to score rollouts. We collect 4,298 trajectory pairs, each comprising the highest-scored correct and lowest-scored incorrect trajectories per prompt. Starting from Qwen2.5-7B-Instruct, we perform SFT on 1,000 randomly sampled successful trajectories, followed by DPO using eight new rollouts per task and another 4,298 scored pairs. We repeat this DPO step iteratively (iDPO) for further refinement. 10 Results: Rejection Sampling (RS) and iDPO. Figure 6(a) illustrates consistent performance gains across one SFT stage and two subsequent DPO stages. However, we observe diminishing returns over successive iterations. Initially, rejection sampling SFT significantly boosts performance by effectively capturing successful coding patterns. Subsequent DPO stages show smaller incremental improvements, reflecting the models diminishing exploration space as it tackles increasingly challenging tasks, ultimately converging toward an approximate Nash equilibrium."
        },
        {
            "title": "5.4 Additional Experimental Analysis",
            "content": "External Evaluation. Table 6 summarizes external evaluation results on MedAgentGym. Med-Copilot models modestly improve performance on open-ended, reasoning-intensive tasks (e.g., MIMIC-Extract). However, improvements remain limited, with occasional declines in certain domainspecific tasks, indicating challenges in generalizing across specialized biomedical contexts. We hypothesize that incorporating advanced optimization techniques, such as PPO [62], along with carefully curated training dataset integrating both general and domain-specific data, may effectively mitigate these limitations. Table 6: External test set results of MedAgentGym. EHR-SeqSQL EHRCon MIMIC-Extract N-PowerAI SR 50.80 58.40 70.60 78.20 100.00 Datasets () Base () / Metrics () API-based Proprietary LLMs (for reference) gpt-4o-mini [19] gpt-4o [19] gpt-4.1-mini [48] gpt-4.1 [48] gpt-o4-mini [49] OSS LLMs Qwen3-1.7B [55] Qwen3-4B [55] Qwen3-8B [55] Qwen2.5-7B-Inst [90] Med-Copilot (SFT, 7B) Med-Copilot (DPO, 7B) Med-Copilot (7B) Qwen3-14B [55] Qwen2.5-14B-Inst [90] Med-Copilot (14B) R1-Dis-Qwen-14B [13] Qwen3-32B [55] 33.60 44.80 52.00 42.20 42.40 42.60 43.40 69.00 46.40 42.20 56.00 64.80 SR Acc SR 23.20 35.79 52.40 63.00 51.00 17.20 26.20 31.40 27.20 28.80 26.20 23.00 45.00 39.20 40.80 40.80 54. 2.67 9.82 5.62 10.41 16.88 1.90 4.59 6.82 1.34 1.95 4.19 2.14 9.24 4.51 2.75 2.37 12.17 16.03 20.71 25.66 33.53 36.15 14.72 19.30 20.12 11.66 10.48 12.23 14.82 23.59 21.57 25.89 17.60 31.26 Avg. Score 26.03 34.69 43.20 51.06 53. 16.86 23.72 27.59 20.60 20.91 21.31 20.84 36.71 27.92 27.91 29.19 42.16 Effect of Interactive Coding. Figure 6(b) shows that removing debugging capabilities significantly Interactive coding mechanism in MedAgentGym decreases model performance across all tasks. substantially contributes to successful coding-based medical reasoning by enabling the model to effectively interpret and rectify execution errors. Error Analysis. Figure 6(c) summarizes common error types encountered by the strongest evaluated LLM, gpt-4.1. Loop-related issues dominate, accounting for 50.39% of errors, where agents repeatedly execute the same action in the final turns, indicating difficulty in adapting or exploring alternative strategies. This highlights the need to promote effective exploration and enhance robustness in solving complex medical reasoning tasks. Additional experimental results, including cost analysis and human studies, are available in appendix E."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce MedAgentGym, publicly available training environment for enhancing coding-based medical reasoning in LLM agents. With 72,413 biomedical tasks across 129 categories, MedAgentGym provides interactive, executable coding environments for training and benchmarking. Experiments show that OSS LLMs fine-tuned using MedAgentGym through SFT and DPO significantly narrow the gap with proprietary LLMs. Building upon MedAgentGym, Med-Copilot further exemplifies how systematic training and trajectory sampling methods can improve coding proficiency for medical reasoning tasks, making MedAgentGym valuable resource for advancing biomedical coding agents. MedAgentGym thus provides foundational resource to accelerate the development of coding agents in biomedicine, facilitating the rigorous evaluation and iterative advancement of LLMs from structured clinical coding tasks to more advanced open-ended computational research questions."
        },
        {
            "title": "References",
            "content": "[1] R. K. Arora, J. Wei, R. S. Hicks, P. Bowman, J. Quinonero-Candela, F. Tsimpourlas, M. Sharman, M. Shah, A. Vallone, A. Beutel, J. Heidecke, and K. Singhal. Healthbench: Evaluating large language models towards improved human health. OpenAI Blog, 2025. 11 [2] J. S. Chan, N. Chowdhury, O. Jaffe, J. Aung, D. Sherburn, E. Mays, G. Starace, K. Liu, L. Maksin, T. Patwardhan, et al. Mle-bench: Evaluating machine learning agents on machine learning engineering. arXiv preprint arXiv:2410.07095, 2024. [3] J. Chen, Z. Cai, K. Ji, X. Wang, W. Liu, R. Wang, J. Hou, and B. Wang. Huatuogpt-o1, towards medical complex reasoning with llms. arXiv preprint arXiv:2412.18925, 2024. [4] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [5] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [6] J. Coelho, J. Ning, J. He, K. Mao, A. Paladugu, P. Setlur, J. Jin, J. Callan, J. Magalhães, B. Martins, et al. Deepresearchgym: free, transparent, and reproducible evaluation sandbox for deep research. arXiv preprint arXiv:2505.19253, 2025. [7] G. DeepMind. Alphaevolve: coding agent for scientific and algorithmic discovery. Google DeepMind Blog, 2025. [8] A. Drouin, M. Gasse, M. Caccia, I. H. Laradji, M. D. Verme, T. Marty, D. Vazquez, N. Chapados, and A. Lacoste. Workarena: How capable are web agents at solving common knowledge work tasks? In Forty-first International Conference on Machine Learning, 2024. [9] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [10] S. Gao, R. Zhu, Z. Kong, A. Noori, X. Su, C. Ginder, T. Tsiligkaridis, and M. Zitnik. Txagent: An ai agent for therapeutic reasoning across universe of tools. arXiv preprint arXiv:2503.10970, 2025. [11] Gemma. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [12] Google. Medgemma hugging face. https://huggingface.co/collections/google/ medgemma-release-680aade845f90bec6a3f60c4, 2025. Accessed: [2025-05-20]. [13] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [14] S. Guo, C. Deng, Y. Wen, H. Chen, Y. Chang, and J. Wang. DS-agent: Automated data science by empowering large language models with case-based reasoning. In Forty-first International Conference on Machine Learning, 2024. [15] HAI@Stanford. Holistic evaluation of large language models for medical applications. Blog Post, 2025. [16] Q. Huang, J. Vora, P. Liang, and J. Leskovec. Mlagentbench: Evaluating language agents on machine learning experimentation. arXiv preprint arXiv:2310.03302, 2023. [17] X. Huang, J. Wu, H. Liu, X. Tang, and Y. Zhou. m1: Unleash the potential of test-time scaling for medical reasoning with large language models. arXiv preprint arXiv:2504.00869, 2025. [18] B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [19] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [20] S. Jiang, Y. Liao, Z. Chen, Y. Zhang, Y. Wang, and Y. Wang. Meds3: Towards medical small language models with self-evolved slow thinking, 2025. [21] Y. Jiang, K. C. Black, G. Geng, D. Park, A. Y. Ng, and J. H. Chen. Medagentbench: Dataset for benchmarking llms as agents in medical applications. arXiv preprint arXiv:2501.14654, 2025. [22] D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, and P. Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021. 12 [23] Q. Jin, B. Dhingra, Z. Liu, W. Cohen, and X. Lu. PubMedQA: dataset for biomedical research question answering. In K. Inui, J. Jiang, V. Ng, and X. Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 25672577, Hong Kong, China, Nov. 2019. Association for Computational Linguistics. [24] Q. Jin, Z. Wang, Y. Yang, Q. Zhu, D. Wright, T. Huang, W. J. Wilbur, Z. He, A. Taylor, Q. Chen, et al. Agentmd: Empowering language agents for risk prediction with large-scale clinical tool learning. arXiv preprint arXiv:2402.13225, 2024. [25] L. Jing, Z. Huang, X. Wang, W. Yao, W. Yu, K. Ma, H. Zhang, X. Du, and D. Yu. DSBench: In The Thirteenth How far are data science agents from becoming data science experts? International Conference on Learning Representations, 2025. [26] A. E. Johnson, T. J. Pollard, L. Shen, L.-w. H. Lehman, M. Feng, M. Ghassemi, B. Moody, P. Szolovits, L. Anthony Celi, and R. G. Mark. Mimic-iii, freely accessible critical care database. Scientific data, 3(1):19, 2016. [27] H. Kang and C. Xiong. Researcharena: Benchmarking llms ability to collect and organize information as research agents. arXiv preprint arXiv:2406.10291, 2024. [28] N. Khandekar, Q. Jin, G. Xiong, S. Dunn, S. Applebaum, Z. Anwar, M. Sarfo-Gyamfi, C. Safranek, A. Anwar, A. Zhang, et al. Medcalc-bench: Evaluating large language models for medical calculations. Advances in Neural Information Processing Systems, 37:8473084745, 2024. [29] Y. Kim, C. Park, H. Jeong, Y. S. Chan, X. Xu, D. McDuff, H. Lee, M. Ghassemi, C. Breazeal, and H. W. Park. MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making. In Advances in Neural Information Processing Systems, 2024. [30] Y. Kwon, J. Kim, G. Lee, S. Bae, D. Kyung, W. Cha, T. Pollard, A. JOHNSON, and E. Choi. EHRCon: Dataset for checking consistency between unstructured notes and structured tables in electronic health records. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [31] Y. Lai, J. Zhong, M. Li, S. Zhao, and X. Yang. Med-r1: Reinforcement learning for generalizable medical reasoning in vision-language models. arXiv preprint arXiv:2503.13939, 2025. [32] W. Lan, W. Wang, C. Ji, G. Yang, Y. Zhang, X. Liu, S. Wu, and G. Wang. Clinicalgpt-r1: Pushing reasoning capability of generalist disease diagnosis with large language model. arXiv preprint arXiv:2504.09421, 2025. [33] G. Lee, H. Hwang, S. Bae, Y. Kwon, W. Shin, S. Yang, M. Seo, J.-Y. Kim, and E. Choi. Ehrsql: practical text-to-sql benchmark for electronic health records. Advances in Neural Information Processing Systems, 35:1558915601, 2022. [34] B. Li, T. Yan, Y. Pan, J. Luo, R. Ji, J. Ding, Z. Xu, S. Liu, H. Dong, Z. Lin, et al. Mmedagent: Learning to use medical tools with multi-modal agent. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 87458760, 2024. [35] J. Li, S. Wang, M. Zhang, W. Li, Y. Lai, X. Kang, W. Ma, and Y. Liu. Agent Hospital: Simulacrum of Hospital with Evolvable Medical Agents. arXiv preprint arXiv:2405.02957, 2024. [36] S. Li, V. Balachandran, S. Feng, J. Ilgen, E. Pierson, P. W. W. Koh, and Y. Tsvetkov. Mediq: Question-asking llms and benchmark for reliable interactive clinical reasoning. Advances in Neural Information Processing Systems, 37:2885828888, 2024. [37] S. S. Li, J. Mun, F. Brahman, J. S. Ilgen, Y. Tsvetkov, and M. Sap. Aligning llms to ask good questions case study in clinical reasoning. arXiv preprint arXiv:2502.14860, 2025. [38] Y. Liao, S. Jiang, Y. Wang, and Y. Wang. Reflectool: Towards reflection-aware tool-augmented clinical agents. arXiv preprint arXiv:2410.17657, 2024. [39] V. Liévin, C. E. Hother, A. G. Motzfeldt, and O. Winther. Can large language models reason about medical questions? Patterns, 5(3), 2024. [40] T. Liu, S. Han, X. Luo, H. Wang, P. Lu, B. Zhu, Y. Wang, K. Li, J. Chen, R. Qu, et al. Towards artificial intelligence research assistant for expert-involved learning. arXiv preprint arXiv:2505.04638, 2025. 13 [41] X. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023. [42] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [43] B. Meskó and E. J. Topol. The imperative for regulatory oversight of large language models (or generative ai) in healthcare. NPJ Digital Medicine, 6(1):120, 2023. [44] Ministral. Un ministral, des ministraux. Ministral Blog, 2025. [45] M. Moor, O. Banerjee, Z. S. H. Abad, H. M. Krumholz, J. Leskovec, E. J. Topol, and P. Rajpurkar. Foundation models for generalist medical artificial intelligence. Nature, 616(7956):259265, 2023. [46] D. Nathani, L. Madaan, N. Roberts, N. Bashlykov, A. Menon, V. Moens, A. Budhiraja, D. Magka, V. Vorotilov, G. Chaurasia, et al. Mlgym: new framework and benchmark for advancing ai research agents. arXiv preprint arXiv:2502.14499, 2025. [47] A. Nimmolrat, K. Sutham, and O. Thinnukool. Patient triage system for supporting the operation of dispatch centres and rescue teams. BMC medical informatics and decision making, 21:116, 2021. [48] OpenAI. Introducing gpt-4.1 in the api. OpenAI Blog, 2025. [49] OpenAI. Openai o3 and o4-mini system card. OpenAI Blog, 2025. [50] A. Pal, L. K. Umapathi, and M. Sankarasubbu. Medmcqa: large-scale multi-subject multichoice dataset for medical domain question answering. In Conference on health, inference, and learning, pages 248260. PMLR, 2022. [51] J. Pan, X. Wang, G. Neubig, N. Jaitly, H. Ji, A. Suhr, and Y. Zhang. Training software engineering agents and verifiers with SWE-gym. In ICLR 2025 Third Workshop on Deep Learning for Code, 2025. [52] T. J. Pollard, A. E. Johnson, J. D. Raffa, L. A. Celi, R. G. Mark, and O. Badawi. The eicu collaborative research database, freely available multi-center database for critical care research. Scientific data, 5(1):113, 2018. [53] C. Qian, E. C. Acikgoz, H. Wang, X. Chen, A. Sil, D. Hakkani-Tür, G. Tur, and H. Ji. Smart: Self-aware agent for tool overuse mitigation. arXiv preprint arXiv:2502.11435, 2025. [54] J. Qiu, X. Qi, T. Zhang, X. Juan, J. Guo, Y. Lu, Y. Wang, Z. Yao, Q. Ren, X. Jiang, X. Zhou, D. Liu, L. Yang, Y. Wu, K. Huang, S. Liu, H. Wang, and M. Wang. Alita: Generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal self-evolution, 2025. [55] Qwen. Qwen3: Think deeper, act faster. Qwen Blog, 2025. [56] Qwen. Qwq-32b: Embracing the power of reinforcement learning, March 2025. [57] P. Ruan, I. Villanueva-Miranda, J. Liu, D. M. Yang, Q. Zhou, G. Xiao, and Y. Xie. N-power ai: specialized agent framework for automated sample size and power analysis in clinical trial design. bioRxiv, pages 202502, 2025. [58] J. Ryu, S. Cho, G. Lee, and E. Choi. Ehr-seqsql: sequential text-to-sql dataset for interactively exploring electronic health records. In Findings of the Association for Computational Linguistics ACL 2024, pages 1638816407, 2024. [59] S. Schmidgall and M. Moor. Agentrxiv: Towards collaborative autonomous research. arXiv preprint arXiv:2503.18102, 2025. [60] S. Schmidgall, Y. Su, Z. Wang, X. Sun, J. Wu, X. Yu, J. Liu, Z. Liu, and E. Barsoum. Agent laboratory: Using llm agents as research assistants, 2025. [61] S. Schmidgall, R. Ziaei, C. Harris, E. Reis, J. Jopling, and M. Moor. Agentclinic: multimodal agent benchmark to evaluate ai in simulated clinical environments. arXiv preprint arXiv:2405.07960, 2024. [62] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [63] Y. Shao, V. Samuel, Y. Jiang, J. Yang, and D. Yang. Collaborative gym: framework for enabling and evaluating human-agent collaboration. arXiv preprint arXiv:2412.15701, 2024. 14 [64] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [65] W. Shi, R. Xu, Y. Zhuang, Y. Yu, J. Zhang, H. Wu, Y. Zhu, J. C. Ho, C. Yang, and M. D. Wang. EHRAgent: Code empowers large language models for few-shot complex tabular reasoning on electronic health records. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2231522339, 2024. [66] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl, et al. Large language models encode clinical knowledge. Nature, 620(7972):172180, 2023. [67] A. Soroush, B. S. Glicksberg, E. Zimlichman, Y. Barash, R. Freeman, A. W. Charney, G. N. Nadkarni, and E. Klang. Large language models are poor medical codersbenchmarking of medical code querying. NEJM AI, 1(5):AIdbp2300040, 2024. [68] Z. Stojanovski, O. Stanley, J. Sharratt, R. Jones, A. Adefioye, J. Kaddour, and A. Kopf. Reasoning gym: Reasoning environments for reinforcement learning with verifiable rewards, 2025. [69] K. Swanson, W. Wu, N. L. Bulaong, J. E. Pak, and J. Zou. The virtual lab: Ai agents design new sars-cov-2 nanobodies with experimental validation. bioRxiv, pages 202411, 2024. [70] X. Tang, Y. Liu, Z. Cai, Y. Shao, J. Lu, Y. Zhang, Z. Deng, H. Hu, K. An, R. Huang, et al. Mlbench: Evaluating large language models and agents for machine learning tasks on repositorylevel code. arXiv preprint arXiv:2311.09835, 2023. [71] X. Tang, B. Qian, R. Gao, J. Chen, X. Chen, and M. B. Gerstein. Biocoder: benchmark for bioinformatics code generation with large language models. Bioinformatics, 40, 2024. [72] X. Tang, D. Shao, J. Sohn, J. Chen, J. Zhang, J. Xiang, F. Wu, Y. Zhao, C. Wu, W. Shi, et al. Medagentsbench: Benchmarking thinking models and agent frameworks for complex medical reasoning. arXiv preprint arXiv:2503.07459, 2025. [73] X. Tang, A. Zou, Z. Zhang, Z. Li, Y. Zhao, X. Zhang, A. Cohan, and M. Gerstein. Medagents: Large language models as collaborators for zero-shot medical reasoning. In Findings of the Association for Computational Linguistics ACL 2024, pages 599621, 2024. [74] S. Tayebi Arasteh, T. Han, M. Lotfinia, C. Kuhl, J. N. Kather, D. Truhn, and S. Nebelung. Large language models streamline automated machine learning for clinical studies. Nature Communications, 15(1):1603, 2024. [75] N. Team, B. Zhang, S. Feng, X. Yan, J. Yuan, Z. Yu, X. He, S. Huang, S. Hou, Z. Nie, et al. Novelseek: When agent becomes the scientistbuilding closed-loop system from hypothesis to verification. arXiv preprint arXiv:2505.16938, 2025. [76] R. Thapa, Q. Wu, K. Wu, H. Zhang, A. Zhang, E. Wu, H. Ye, S. Bedi, N. Aresh, J. Boen, et al. Disentangling reasoning and knowledge in medical large language models. arXiv preprint arXiv:2505.11462, 2025. [77] G. Tsatsaronis, G. Balikas, P. Malakasiotis, I. Partalas, M. Zschunke, M. R. Alvers, D. Weissenborn, A. Krithara, S. Petridis, D. Polychronopoulos, et al. An overview of the bioasq large-scale biomedical semantic indexing and question answering competition. BMC bioinformatics, 16:128, 2015. [78] B. Wang, H. Zhao, H. Zhou, L. Song, M. Xu, W. Cheng, X. Zeng, Y. Zhang, Y. Huo, Z. Wang, et al. Baichuan-m1: Pushing the medical capability of large language models. arXiv preprint arXiv:2502.12671, 2025. [79] P. Wang, T. Shi, and C. K. Reddy. Text-to-sql generation for question answering on electronic medical records. In Proceedings of The Web Conference 2020, pages 350361, 2020. [80] S. Wang, M. B. A. McDermott, G. Chauhan, M. Ghassemi, M. C. Hughes, and T. Naumann. Mimic-extract: data extraction, preprocessing, and representation pipeline for mimic-iii. In Proceedings of the ACM Conference on Health, Inference, and Learning, CHIL 20, pages 222235, New York, NY, USA, 2020. Association for Computing Machinery. [81] X. Wang, Y. Chen, L. Yuan, Y. Zhang, Y. Li, H. Peng, and H. Ji. Executable code actions elicit better llm agents. In Forty-first International Conference on Machine Learning, 2024. 15 [82] Z. Wang, B. Danek, Z. Yang, Z. Chen, and J. Sun. Can large language models replace data scientists in clinical research? arXiv preprint arXiv:2410.21591, 2024. [83] Z. Wang, K. Wang, Q. Wang, P. Zhang, L. Li, Z. Yang, K. Yu, M. N. Nguyen, L. Liu, E. Gottlieb, et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025. [84] M. Wornow, R. Thapa, E. Steinberg, J. Fries, and N. Shah. Ehrshot: An ehr benchmark for few-shot evaluation of foundation models. Advances in Neural Information Processing Systems, 36:6712567137, 2023. [85] M. Wornow, Y. Xu, R. Thapa, B. Patel, E. Steinberg, S. Fleming, M. A. Pfeffer, J. Fries, and N. H. Shah. The shaky foundations of clinical foundation models: survey of large language models and foundation models for emrs. ArXiv preprint, abs/2303.12961, 2023. [86] J. Wu, W. Deng, X. Li, S. Liu, T. Mi, Y. Peng, Z. Xu, Y. Liu, H. Cho, C.-I. Choi, et al. Medreason: Eliciting factual medical reasoning steps in llms via knowledge graphs. arXiv preprint arXiv:2504.00993, 2025. [87] K. Wu, E. Wu, R. Thapa, K. Wei, A. Zhang, A. Suresh, J. J. Tao, M. W. Sun, A. Lozano, and J. Zou. Medcasereasoning: Evaluating and learning diagnostic reasoning from clinical case reports. arXiv preprint arXiv:2505.11733, 2025. [88] Z. Xi, Y. Ding, W. Chen, B. Hong, H. Guo, J. Wang, D. Yang, C. Liao, X. Guo, W. He, et al. Agentgym: Evolving large language model-based agents across diverse environments. arXiv preprint arXiv:2406.04151, 2024. [89] G. Xiong, Q. Jin, Z. Lu, and A. Zhang. Benchmarking retrieval-augmented generation for medicine. In Findings of the Association for Computational Linguistics ACL 2024, pages 62336251, 2024. [90] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [91] J. Yang, C. Jimenez, A. Wettig, K. Lieret, S. Yao, K. Narasimhan, and O. Press. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024. [92] J. Yang, K. Leret, C. E. Jimenez, A. Wettig, K. Khandpur, Y. Zhang, B. Hui, O. Press, L. Schmidt, and D. Yang. Swe-smith: Scaling data for software engineering agents, 2025. [93] J. Yuan, X. Yan, B. Shi, T. Chen, W. Ouyang, B. Zhang, L. Bai, Y. Qiao, and B. Zhou. Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback. arXiv preprint arXiv:2501.03916, 2025. [94] D. Zhang, S. Zhoubian, M. Cai, F. Li, L. Yang, W. Wang, T. Dong, Z. Hu, J. Tang, and Y. Yue. Datascibench: An llm agent benchmark for data science. arXiv preprint arXiv:2502.13897, 2025. [95] S. Zhang, Q. Liu, G. Qin, T. Naumann, and H. Poon. Med-rlvr: Emerging medical reasoning from 3b base model via reinforcement learning. arXiv preprint arXiv:2502.19655, 2025. [96] Y. Zhang, Q. Jiang, X. XingyuHan, N. Chen, Y. Yang, and K. Ren. Benchmarking data science agents. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 56775700, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. [97] W. Zhao, M. Yuksekgonul, S. Wu, and J. Zou. Sirius: Self-improving multi-agent systems via bootstrapped reasoning. arXiv preprint arXiv:2502.04780, 2025. [98] Y. Zhu, Z. He, H. Hu, X. Zheng, X. Zhang, Z. Wang, J. Gao, L. Ma, and L. Yu. Medagentboard: Benchmarking multi-agent collaboration with conventional methods for diverse medical tasks. arXiv preprint arXiv:2505.12371, 2025. [99] Y. Zuo, S. Qu, Y. Li, Z. Chen, X. Zhu, E. Hua, K. Zhang, N. Ding, and B. Zhou. Medxpertqa: Benchmarking expert-level medical reasoning and understanding. arXiv preprint arXiv:2501.18362, 2025."
        },
        {
            "title": "A Limitations and Ethical Statements",
            "content": "A.1 Limitations Our proposed MedAgentGym and associated methods require substantial computational resources for trajectory sampling, model fine-tuning, and iterative self-improvement procedures. Although we achieve significant improvements with relatively lightweight OSS LLMs, further scaling and advanced optimization methods (e.g., PPO) require increased computing infrastructure, limiting accessibility for resource-constrained research groups. Additionally, our current dataset size and trajectory collection are primarily constrained by computational budget rather than data availability, potentially limiting the full exploration of model scaling behavior. A.2 Ethical Statements Data Privacy and Licensing. We carefully curated MedAgentGym with strict adherence to ethical standards, using publicly available datasets or datasets with appropriate privacy protections and anonymizations. Table 7 lists the access requirements for the 12 datasets in MedAgentGym and the code base for data processing or task implementation. We explicitly designed isolated Docker environments to ensure data privacy and security. Nevertheless, ethical usage of our methods and models in clinical settings requires rigorous validation, transparency in limitations, and close collaboration with healthcare professionals. We encourage responsible deployment, emphasizing human oversight, continuous evaluation, and clear communication of model capabilities and uncertainties to mitigate ethical and practical risks. Table 7: Data Access and License Information of 12 datasets in MedAgentGym. Custom represents additional datasetor task-specific license and data access requirements (e.g., DUA or credentials). Dataset Data License Data Access Code License Code Access Training and Internal Validation (In-Distribution) MIMIC-III [26, 33] eICU [52, 33] TREQS [79] MedCalcBench [28] MedAgentBench [21] BioCoder [71] BioDSBench [82] EHRSHOT [84] Custom Custom Custom CC-BY-SA 4.0 MIT CC-BY-4.0 MIT Custom MIMIC-III on PhysioNet eICU on PhysioNet MIMIC-III on PhysioNet MedCalcBench MedAgentBench (FHIR Server) BioCoder on Huggingface BioDSBench EHRShot (Standford) CC-BY-4.0 CC-BY-4.0 MIT Public MIT N/A MIT Apache MIMIC-III on EHRSQL eICU on EHRSQL TREQS on GitHub MedCalcBench on GitHub MedAgentBench on GitHub BioCoder on GitHub BioDSBench on GitHub EHRSHOT on Github External Validation (Out-of-Distribution) EHR-SeqSQL [58] EHR-Con [30] MIMIC-Extract [80] N-PowerAI [57] Custom Custom Custom N/A MIMIC-III on PhysioNet MIMIC-III on PhysioNet MIMIC-III on PhysioNet N-Power AI Supp. Mat. N/A MIT MIT N/A EHR-SeqSQL on GitHub EHR-Con on GitHub MIMIC-Extract on GitHub N-Power AI on Webpage LLM Usage Statement. In compliance with the PhysioNet Credentialed Health Data Use Agreement (version 1.5.0)5, we strictly prohibit transferring confidential patient data (e.g., MIMIC-III and eICU) to third-party entities, including external online services and APIs. To responsibly utilize the Azure OpenAI Service, we adhere closely to PhysioNets guidelines on responsible GPT usage6. Specifically, we have opted out of the human review process by completing the Azure OpenAI Additional Use Case Form7, thereby ensuring no third-party entity (such as Microsoft) accesses or processes sensitive patient information. We consistently monitor our data handling practices and strictly adhere to applicable guidelines and privacy regulations, maintaining the highest ethical standards in our research and operations. 5https://physionet.org/about/licenses/physionet-credentialed-health-data-license-150/ 6https://physionet.org/news/post/gpt-responsible-use 7https://aka.ms/oai/additionalusecase"
        },
        {
            "title": "B Task and Data Details",
            "content": "B.1 Overview MedAgentGym integrates comprehensive suite of datasets that encompass both structured and openended biomedical reasoning tasks. Computational Task Category. Structured tasks primarily include database query scenarios, such as those from MIMIC-III, eICU, TREQS, EHR-SeqSQL, EHRCon, and MedCalcBench (ruleor equation-based), which require precise formulation of executable queries against structured EHR data. Open-ended tasks include biomedical data analysis and medical coding scenarios drawn from datasets such as MedAgentBench, BioCoder, BioDSBench, EHRSHOT, MIMIC-Extract, and N-PowerAI, demanding nuanced and flexible code generation for complex analysis, statistical reasoning, or clinical decision-making. Biomedical Application Category. MedAgentGym spans multiple biomedical subdomains, including Database queries (DB, including MIMIC-III, eICU, TREQS, EHR-SeqSQL, and EHRCon), Data Science (DS, including MedCalcBench and MedAgentBench), Bioinformatics (Bioinfo, including BioCoder, BioDSBench, N-PowerAI), and Machine Learning (ML, including EHRSHOT and MIMICExtract). In several empirical analysis, we include ML as part of the Bioinfo. category for better visualization. In- & Out-of-Distribution. We further categorize MedAgentGym into indistribution tasks (EHRSQL, TREQS, MedCalcBench, BioCoder, BioDSBench, MedAgentBench, EHRSHOT) and out-of-distribution tasks (EHRSeqSQL, EHRCon, MIMIC-Extract, N-PowerAI), facilitating rigorous evaluation of model generalization and adaptability. To highlight intrinsic differences between these distributions, Figure 7 shows the distribution of sampled code trajectories. The resulting visualization demonstrates significant divergence in trajectory complexity, interaction frequency, and required code refinement steps between in-distribution and out-ofdistribution tasks, underscoring the challenges posed by novel biomedical reasoning contexts. InterFigure 7: distribution similarity. B.2 Training and Internal Testing (In-Distribution) Dataset Details EHRSQL: MIMIC-III and eICU. EHRSQL [33] comprises text-to-SQL tasks that leverage electronic health records from MIMIC-III [26] and eICU [52]. They evaluate the ability of LLMs (and agents) to translate clinical questions posed by healthcare professionals into executable SQL queries. This includes handling complex queries involving temporal logic and conditional abstention. TREQS. TREQS [79] is text-to-SQL benchmark tailored specifically to clinical question answering using the MIMIC-III dataset. It emphasizes generating accurate SQL queries from template-based natural language questions against simplified schema comprising five core tables, with an emphasis on large result-set handling. MedCalcBench. MedCalcBench [28] provides structured evaluation of clinical calculation capabilities in LLMs. Each instance poses patient-specific clinical scenario requiring precise medical calculations such as clinical scores or medication dosages, accompanied by expert-curated stepwise solutions for validation. MedAgentBench. MedAgentBench [21] is simulated EHR environment designed to evaluate LLM-driven clinical workflows. It features realistic patient scenarios across ten task categories, requiring agents to perform clinical reasoning, EHR querying via FHIR interfaces, and clinical decision support. BioCoder. BioCoder [71] assesses the capability of LLMs to generate accurate bioinformatics code solutions. It comprises practical coding challenges derived from authentic bioinformatics software, requiring the generation and verification of functionally correct Python methods. BioDSBench. BioDSBench [82] evaluates LLM proficiency in biomedical data science coding tasks, involving the generation of Python or code to replicate analytical workflows derived from actual 18 biomedical research studies. Tasks span statistical analyses, data manipulations, and visualization routines. EHRSHOT. EHRSHOT [84] benchmarks LLMs on few-shot clinical prediction tasks leveraging real-world, longitudinal, deidentified EHR data. It focuses on rapid adaptation to tasks such as risk prediction and forecasting clinical outcomes given limited labeled examples. B.3 External Evaluation (Out-of-Distribution) Dataset Details EHR-SeqSQL. EHR-SeqSQL [58] extends text-to-SQL evaluation to sequential, multi-turn interactions, emulating realistic clinical dialogues. Tasks require maintaining context across multiple SQL queries, assessing LLM capability in handling compositional and contextual reasoning. EHRCon. EHRCon [30] involves assessing clinical note consistency with structured EHR records, focusing on identifying discrepancies. It serves as verification task requiring precise alignment between unstructured clinical text and corresponding database entries. MIMIC-Extract. MIMIC-Extract[80] provides structured, preprocessed time-series patient data derived from the MIMIC-III dataset, used in clinical predictive modeling such as mortality risk or intervention prediction, enabling standardized assessments of time-series reasoning capabilities. N-PowerAI. N-PowerAI [57] evaluates LLM capabilities in performing statistical sample-size and power analyses for clinical trial design. It requires multi-step statistical reasoning and the generation of precise numeric results corresponding to various clinical scenarios. B.4 Train-Test Set Split For datasets that provide predefined training, validation, and test splits, we combine the training and validation subsets into single unified training set and retain the original test subset exclusively for evaluation. In cases where datasets lack predefined splits, we randomly allocate 50% of the instances to training, assigning the remaining 50% to the test set. For tasks containing more than 1000 samples in both training and test sets, we create lighter subset through downsampling to support efficient leaderboard-based training and evaluation. Specifically, we leverage task-specific metadata to perform uniform sampling within each fine-grained category, thereby maintaining diversity, ensuring balanced representation, and preserving the original data distribution. B.5 Data Pre-processing Details B.5.1 Structured Tasks For database querying related datasets, including MIMIC-III, eICU, TREQS, and EHR-SeqSQL, each task instance is structured into JSON format comprising: (1) the contextual description and the corresponding natural-language query, (2) the ground-truth SQL query, and (3) the resulting answer from the database execution. Instances yielding null results upon SQL execution, indicating the absence of valid answer, are excluded from the dataset. For EHRCon, we organize the data into structured databases that link patient records through hospital admission IDs, complemented by separate database containing associated clinical notes. Each task is formulated as JSON object consisting of: (1) admission ID, (2) relevant medical terminology, (3) count of detected inconsistencies, and (4) binary indicator denoting the presence or absence of inconsistencies. For MedCalcBench, each instance initially consists of patient note, specific medical calculation query, ground-truth answer, and detailed step-by-step solution. To accurately evaluate the coding capabilities of LLM agents without direct guidance, we remove all intermediate calculation hints, presenting only the patient note and the calculation query for model inference. For N-PowerAI, statistical analysis tasks are augmented through attribute substitution. Specifically, each original instance is expanded 100-fold by systematically replacing an attribute with randomly chosen equivalent from predefined valid range, preserving the integrity and interpretability of the statistical context. Each augmented instance includes recalculated values for sample size (N) and statistical power, stored systematically within JSON-formatted records. 19 B.5.2 Open-ended Tasks MedAgentBench instances require LLM agents to follow natural-language instructions to perform tasks within FHIR-compliant interactive medical environment. We retain original instructions, solutions, and Medical Record Numbers (MRNs). To derive verifiable evaluation signals, we execute the provided ground-truth solutions on the server-side environment to obtain authoritative reference answers. BioCoder tasks require implementing biostatistics algorithms or addressing scientific programming challenges. Each instance comprises problem description, context-specific code, test cases, and expected outputs. While evaluation datasets already contain all necessary components, training instances initially lack context-specific code and test cases. To address this gap, we employ the o3-mini model to auto-generate relevant context code and corresponding test cases based on provided ground-truth functions. Generated functions undergo rigorous validation via code interpreter, retaining only verified, error-free instances. Additionally, we exclusively utilize the Python-based subset of BioCoder, deferring the JavaScript subset for subsequent integration. BioDSBench instances involve biomedical data analysis tasks derived from real-world datasets. Features are systematically organized into directories by task, with each tasks description and reference Python implementation captured within JSON structures. For datasets dedicated to predictive model development (e.g., EHRSHOT and MIMIC-Extract), initial features are provided in pre-processed form but necessitate additional table joining, filtering, and integration to produce final training inputs. While labels accompany these tasks, explicit reference Python implementations are not provided, as evaluation metrics directly measure the accuracy of model predictions on predefined test subsets. Distinct subsets of training, validation, and testing data and labels are explicitly maintained and separately utilized for both training and evaluation phases."
        },
        {
            "title": "C Baseline Details",
            "content": "We include additional details of the coding and medical domain-specific LLMs used in our experiments: Qwen2.5-Coder-Instruct [18] is derived from the Qwen2.5 series and further fine-tuned explicitly on large-scale coding datasets and coding-specific instruction sets. This targeted training substantially enhances their capabilities in code generation, debugging, and programmatic reasoning, outperforming general-purpose models of similar scale on coding tasks. medgemma-4b-it (gemma-3-4b-pt) [12] is medical-domain variant based on gemma architecture and fine-tuned specifically on medical QA and instruction datasets, which provide strong capabilities for medical reasoning and question answering. HuatuoGPT-o1-7B (Qwen2.5-7B-Instruct) [3], built on the Qwen2.5-7B architecture, is extensively fine-tuned in clinical reasoning datasets via PPO with verifier-based rewards to enhance complex reasoning capabilities. Specifically, it incorporates medical-specific verifier model that guides the generation of complex reasoning trajectories. HuatuoGPT-o1-7B excels in medical reasoning tasks by explicitly generating intermediate reasoning steps that facilitate iterative refinement and introspective evaluation. m1-7B-23K (Qwen2.5-7B-Instruct) [17] is fine-tuned on approximately 23,000 rigorously curated medical QA examples, significantly enhancing its domain-specific knowledge and reasoning capabilities. MedReason-8B (Llama-3.1-8B-Instruct) [86] is specifically fine-tuned for medical questionsanswering and clinical reasoning tasks. Its training emphasizes the generation of step-by-step rationales, enabling robust performance on medical reasoning and diagnostic tasks. Baichuan-M1-14B-Instruct [78] is 14B-parameter medical LLM pre-trained from scratch on approximately 20 trillion tokens of medical domain-specific content and high-quality general text. It integrates specialized modeling across over 20 medical specialties with advanced architectural modifications enhancing context understanding and long-sequence reasoning."
        },
        {
            "title": "D Additional Implementation Details",
            "content": "SFT. For SFT experiments, smaller models (up to 8B parameters) are trained using eight NVIDIA A100 GPUs, whereas the 14B-parameter model is trained on eight NVIDIA H200 GPUs. We utilize the AdamW optimizer [42] with learning rate of 1e 4. The training batch size is set to 8, and the maximum input token length per batch is configured to 40,000 tokens. DPO. DPO experiments are conducted using the same hardware configurations as SFT experiments. We employ the AdamW optimizer with reduced learning rate of 5e 6. Training utilizes batch size of 64 and KL-divergence coefficient (β) of 0.1 to regulate the divergence from the initial policy."
        },
        {
            "title": "E Additional Experimental Results",
            "content": "E.1 Effect of Pre-defined Toolset Figure 8 compares the performance of GPT-4-based agents on the MIMIC-III dataset with and without predefined toolsets integrated into our agent scaffold. This illustrates our agent scaffolds ability to flexibly accommodate external tools. Interestingly, despite providing set of predefined tools, including functions for database loading, data filtering, value retrieval, arithmetic calculations, date computations, and SQL execution (see details in [65]), we observe surprising decline in agent performance. It suggests that the LLM agent inherently generates more flexible and contextually appropriate code when unencumbered by predefined function constraints, aligning with the observations reported by [53, 54]. E.2 Human Study Figure 8: Effect of toolset. Table 8: Human Performance evaluation on structured and open-ended tasks from MedAgentGym. Dataset () Structured MIMIC-III [26, 33] eICU [52, 33] TREQS [79] EHR-SeqSQL [58] MedCalcBench [28] N-PowerAI [57] Structured Task (Total) Open-ended MedAgentBench [21] EHRCon [30] BioDSBench [82] BioCoder [71] EHRSHOT [84] MIMIC-Extract [80] Open-ended Task (Total) # Attempt # Correct SR Total Time (min) Avg Time (min) 10 8 10 10 7 7 52 6 6 3 8 5 3 31 8 5 7 8 5 6 39 6 1 0 2 80% 63% 70% 80% 71% 86% 75% 100% 17% 0% 25% 89% 94% 45% 74 63 39 67 57 96 396 89 241 195 142 185 215 1067 7.40 7.88 3.90 6.70 8.14 13.7 7.62 14.833 40.17 65.00 17.75 37.00 71.67 34. To systematically compare coding styles and performance differences between human programmers and automated agents, we conducted human evaluation involving 83 tasks randomly selected from the test subsets of the 12 datasets included in MedAgentGym. This evaluation set comprises 52 structured and 31 open-ended biomedical coding tasks. The human participants are graduate-level (Ph.D. candidates) engineers with over six years of experience in biomedical engineering, relational database querying, HTTP-based interactions, and machine learning development. Table 8 summarizes the results of human evaluation study conducted to establish reference performance benchmarks across representative structured and open-ended medical reasoning tasks from the MedAgentGym benchmark. Human experts completed selected instances from each dataset, documenting the number of attempts, correctly solved instances, overall SR, total time spent, and average time per task (in minutes). Results indicate that, on average, the human subject required approximately 4.5 times longer to solve open-ended tasks relative to structured tasks, while achieving 40% lower success rate, reflecting the increased complexity and cognitive load associated with open-ended biomedical reasoning scenarios. 21 E.3 Cost Analysis Table 9: Statistics of input and output tokens per question for API-based commercial LLMs. Datasets () MIMIC. eICU TREQS MedCalc. MedAgent. BioCoder BioDS. EHRSHOT Avg. Input gpt-4o-mini [19] gpt-4o [19] gpt-4.1-mini [48] gpt-4.1 [48] gpt-o4-mini [49] Output gpt-4o-mini [19] gpt-4o [19] gpt-4.1-mini [48] gpt-4.1 [48] gpt-o4-mini [49] 3430.83 4399.87 1869.37 3730.90 2005.11 1206.00 840.16 952.68 771.91 1586.65 1947.72 3122.02 1691.45 2979.57 1688.73 1689.71 1823.31 1430.15 1754.18 1534.84 714.72 852.41 991.78 781.86 1392.11 918.45 696.61 880.43 753.88 893. 651.92 739.48 834.73 759.64 1306.49 379.28 537.09 1000.06 787.45 2407.87 9501.86 8474.81 8087.50 7912.81 7586.32 4206.73 2821.00 2892.98 2051.20 1718.22 5166.50 5133.71 2621.79 2728.24 2193.82 4170.56 4144.91 3328.07 2846.58 3144. 5068.88 21077.12 7369.35 3035.45 50768.08 1479.87 7278.49 1308.73 1627.78 1952.88 5986.20 3235.71 4466.07 2092.14 2858.79 10484.53 9127.14 23276.67 5163.57 8083.71 4180.45 6000.75 3546.30 3124.12 8742.77 2945.02 3287.23 4328.93 1848.03 2647. Table 9 summarizes input and output token statistics for various API-based proprietary LLMs evaluated on datasets within MedAgentGym. Notably, the input and output token lengths per query vary significantly across models and tasks. Among these models, gpt-4.1-mini achieves relatively low average input and moderate output token counts, which implies more efficient token utilization during inference compared to larger variants such as gpt-4o and gpt-o4-mini. Conversely, gpt-o4-mini incurs higher average input costs. Figure 9 presents the API cost per 100 tasks. Overall, smaller GPT variants (e.g., gpt-4.1-mini and gpt-4o-mini) offer superior token-efficiency, translating into lower computational and API costs without substantial compromise in performance, demonstrating their effectiveness as cost-efficient solutions for large-scale biomedical reasoning applications. Figure 9: Cost information. E.4 Case Study To illustrate the practical utility of interactive coding mechanism, we conduct detailed case study involving typical bioinformatics coding task in Figure 10. Specifically, the task requires writing Python function (add_exchange_rxns) that modifies biochemical reaction graphs by inInitegrating exchange reactions. tially, the LLM agent-generated solution encountered an attribute error, mistakenly invoking non-existent text_type method on Graph object. Upon receiving explicit debugging feedback, the LLM agent effectively identified and corrected the mistake by utilizing the standalone text_type function rather than incorrectly calling it as method of the graph instance. This case highlights the capability of debugging in MedAgentGym environment to provide targeted, actionable debugging feedback, enabling iterative code refinement and significantly enhancing agent-generated solutions for complex biomedical programming tasks. Figure 10: Case study of gpt-4.1-mini on BioCoder."
        },
        {
            "title": "F Prompt Details",
            "content": "F.1 MIMIC-III Prompts We include prompt details for MIMIC-III tasks as follows: MIMIC-III Prompt You are biomedical expert in handling EHR data and answer questions . Your objective is to solve coding problem with given EHR data , with the goal of finally give concrete answer to the question . Assume you have knowledge of several tables : (1) Tables are linked by identifiers which usually have the suffix 'ID '. For example , SUBJECT_ID refers to unique patient , HADM_ID refers to unique admission to the hospital , and ICUSTAY_ID refers to unique admission to an intensive care unit . (2) Charted events such as notes , laboratory tests , and fluid balance are stored in series of ' events ' tables . For example the outputevents table contains all measurements related to output for given patient , while the labevents table contains laboratory test (3) Tables prefixed with 'd_ ' are dictionary tables and provide definitions for identifiers . For example , every row of chartevents is associated with single ITEMID which represents the concept measured , but it does not contain the actual name of the measurement . By joining chartevents and d_items on ITEMID , it is possible to identify the concept represented by given ITEMID . (4) For the databases , four of them are used to define and track patient stays : admissions , patients , icustays , and transfers . Another four tables are dictionaries for cross - referencing codes against their respective definitions : d_icd_diagnoses , d_icd_procedures , d_items , and d_labitems . For different tables , they contain the following information : (1) ADMISSIONS . csv : ROW_ID , SUBJECT_ID , HADM_ID , ADMITTIME , DISCHTIME , ADMISSION_TYPE , ADMISSION_LOCATION , DISCHARGE_LOCATION , INSURANCE , LANGUAGE , MARITAL_STATUS , ETHNICITY , AGE (2) CHARTEVENTS . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICUSTAY_ID , ITEMID , CHARTTIME , VALUENUM , VALUEUOM (3) COST . csv : ROW_ID , SUBJECT_ID , HADM_ID , EVENT_TYPE , EVENT_ID , CHARGETIME , COST (4) D_ICD_DIAGNOSES . csv : ROW_ID , ICD9_CODE , SHORT_TITLE , LONG_TITLE (5) D_ICD_PROCEDURES . csv : ROW_ID , ICD9_CODE , SHORT_TITLE , LONG_TITLE (6) D_ITEMS . csv : ROW_ID , ITEMID , LABEL , LINKSTO (7) D_LABITEMS . csv : ROW_ID , ITEMID , LABEL (8) DIAGNOSES_ICD . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICD9_CODE (9) ICUSTAYS . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICUSTAY_ID , FIRST_CAREUNIT , LAST_CAREUNIT , FIRST_WARDID , LAST_WARDID , INTIME (10) INPUTEVENTS_CV . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICUSTAY_ID , CHARTTIME , ITEMID , AMOUNT (11) LABEVENTS . csv : ROW_ID , SUBJECT_ID , HADM_ID , ITEMID , CHARTTIME , VALUENUM , VALUEUOM (12) MICROBIOLOGYEVENTS . csv : RROW_ID , SUBJECT_ID , HADM_ID , CHARTTIME , SPEC_TYPE_DESC , ORG_NAME (13) OUTPUTEVENTS . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICUSTAY_ID , CHARTTIME , ITEMID , VALUE (14) PATIENTS . csv : ROW_ID , SUBJECT_ID , GENDER , DOB , DOD (15) PRESCRIPTIONS . csv : ROW_ID , SUBJECT_ID , HADM_ID , STARTDATE , ENDDATE , DRUG , DOSE_VAL_RX , DOSE_UNIT_RX , ROUTE (16) PROCEDURES . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICD9_CODE , CHARTTIME (17) TRANSFERS . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICUSTAY_ID , EVENTTYPE , CAREUNIT , WARDID , INTIME , OUTTIME All the tabls are saved in the data directory {}. 23 F.2 eICU Prompts We include prompt details for eICU tasks as follows: eICU Prompt Main You are biomedical expert in handling EHR data and answer questions . Your objective is to solve coding problem with given EHR data , with the goal of finally give concrete answer to the question . Assume you have knowledge of several tables : (1) Tables are linked by identifiers whose name usually ends 'ID '. For example , PATIENTUNITSTAYID refers to unique patient , LABID refers to unique lab test , and ALLERGYID refers to unique incidence of allergy occurence . (2) Four tables are related to measurements . First , the lab table contains laboratory measurements of chemicals such as chloride or albumin . Secondly , the intake and output ( intakeoutput ) table records all fluid - related measurements such as administered normal saline ( ns ) and urination . Thirdly , the microlab table records measurements of culture of microorganisms . Fourth , the vitalperiod table describes the patients ' vitals during their stay . (3) The remaining tables ( allergy , cost , diagnosis , medication , patient and treatment ) contain other critical information , and the table names are self - explanatory . { EHR_tables } eICU Prompt Table Information For different tables , they contain the following information : (1) allergy . csv : ALLERGYID , PATIENTUNITSTAYID , DRUGNAME , ALLERGYNAME , ALLERGYTIME (2) cost . csv : COSTID , UNIQUEPID , PATIENTHEALTHSYSTEMSTAYID , EVENTTYPE , EVENTID , CHARGETIME , COST (3) diagnosis . csv : DIAGNOSISID , PATIENTUNITSTAYID , ICD9CODE , DIAGNOSISNAME , DIAGNOSISTIME (4) intakeoutput . csv : INTAKEOUTPUTID , PATIENTUNITSTAYID , CELLPATH , CELLLABEL , CELLVALUENUMERIC , INTAKEOUTPUTTIME (5) lab . csv : LABID , PATIENTUNITSTAYID , LABNAME , LABRESULT , LABRESULTTIME (6) medication . csv : MEDICATIONID , PATIENTUNITSTAYID , DRUGNAME , DOSAGE , ROUTEADMIN , DRUGSTARTTIME , DRUGSTOPTIME (7) microlab . csv : MICROLABID , PATIENTUNITSTAYID , CULTURESITE , ORGANISM , CULTURETAKENTIME (8) patient . csv : PATIENTUNITSTAYID , PATIENTHEALTHSYSTEMSTAYID , GENDER , AGE , ETHNICITY , HOSPITALID , WARDID , ADMISSIONHEIGHT , HOSPITALADMITSOURCE , HOSPITALDISCHARGESTATUS , ADMISSIONWEIGHT , DISCHARGEWEIGHT , UNIQUEPID , HOSPITALADMITTIME , UNITADMITTIME , UNITDISCHARGETIME , HOSPITALDISCHARGETIME (9) treatment . csv : TREATMENTID , PATIENTUNITSTAYID , TREATMENTNAME , TREATMENTTIME (10) vitalperiod . csv : VITALPERIODICID , PATIENTUNITSTAYID , TEMPERATURE , SAO2 , HEARTRATE , RESPIRATION , SYSTEMICSYSTOLIC , SYSTEMICDIASTOLIC , SYSTEMICMEAN , OBSERVATIONTIME All the tabls are saved in the data directory { data_directory }. F.3 TREQS Prompts We include prompt details for TREQS tasks as follows: 24 TREQS Prompt You are an biomedical expert in handling EHR data and answer questions accordingly . Your objective is to solve coding problem with given EHR data , with the goal of finally give concrete answer to the question . Assume you have knowledge of several tables : (1) Tables are linked by identifiers which usually have the suffix 'ID '. For example , SUBJECT_ID refers to unique patient . HADM_ID refers to unique admission to the hospital , and ICUSTAY_ID refers to unique admission to an intensive care unit . (2) All tables contain SUBJECT_ID ( patient identifier ) and HADM_ID ( hospital admission identifier ) . (3) The table names are self - explanatory . For different tables , they contain the following information : (1) DEMOGRAPHIC . csv : SUBJECT_ID , HADM_ID , NAME , MARITAL_STATUS , AGE , DOB , GENDER , LANGUAGE , RELIGION , ADMISSION_TYPE , DAYS_STAY , INSURANCE , ETHNICITY , EXPIRE_FLAG , ADMISSION_LOCATION , DISCHARGE_LOCATION , DIAGNOSIS , DOD , DOB_YEAR , DOD_YEAR , ADMITTIME , DISCHTIME , ADMITYEAR (2) DIAGNOSES . csv : SUBJECT_ID , HADM_ID , ICD9_CODE , SHORT_TITLE , LONG_TITLE (3) LAB . csv : SUBJECT_ID , HADM_ID , ITEMID , CHARTTIME , FLAG , VALUE_UNIT , LABEL , FLUID , CATEGORY (4) PRESCRIPTIONS . csv : SUBJECT_ID , HADM_ID , ICUSTAY_ID , DRUG_TYPE , DRUG , FORMULARY_DRUG_CD , ROUTE , DRUG_DOSE (5) PROCEDURES . csv : SUBJECT_ID , HADM_ID , ICD9_CODE , SHORT_TITLE , LONG_TITLE All the tabls are saved in the data directory { data_directory }. F.4 MedCalcBench Prompts We include prompt details for MedCalcBench tasks as follows: MedCalcBench Prompt You work in hospital , and common task in your work is to calculate some biological values of your patients . To do this , you need to identify from clinical notes what information is relevant , before using your clinical knowledge to calculate . And then write Python code to calculate the value . In the code , please use the variable ' answer ' to store the answer of the code . In the main function , please print the final answer of the code without any other text . F.5 MedAgentBench Prompts We include prompt details for MedAgentBench tasks as follows: MedAgentBench Prompt Part You are an expert in using FHIR functions to assist medical professionals . In FHIR , there are few common HTTP GET or POST requests to interact with the server . The descriptions of requests are listed here : { fhir_function_description }. MedAgentBench Prompt Part II You are given question and set of possible functions . Based on the question , you will need to write python code to achieve the purpose . 1. Write python script to invoke GET function of the FHIR server , you MUST put it in the format of nGET url ? param_name1 = param_value1 & param_name2 = param_value2 ... 2. Write python script to invoke POST function of the FHIR server , you MUST put it in the format of nPOST url n[ your payload data in JSON format ] 3. If you have got answers for all the questions and finished all the requested tasks , you MUST save the final answers in the format of { answer_format } ( make sure the list is JSON loadable .) You SHOULD NOT include any other text in the response . Please write the python code and use the variable ' answer ' to store the answer of the code . Question : { question } n. The FHIR server base URL is { fhir_api_base }. Do not directly write the GET and POST requests . MedAgentBench Prompt Answer Format answer = {\" GET \": [\"60\" ,\" S2874099 \"] , \" POST \": [\" http :// localhost :8080/ fhir / Observation \", \" payload ]} The answers to the questions are listed in \" GET \" instead of the get commands , while the post url and payload are listed in \" POST \". F.6 Biocoder Prompts We include prompt details for Biocoder tasks as follows: Biocoder Prompt You are an biomedical expert in writing bioinformatics code and answer questions accordingly . Your objective is to write python function to solve the given question . Please only write the function , do not include any other text . Please write Python function with the following signature : { signature } F.7 BioDSBench Prompts We include prompt details for BioDSBench tasks as follows: BioDSBench Prompt You are an biomedical expert in writing bioinformatics code and answer questions accordingly . Your objective is to write python code to solve the given question . Please only write the code , do not include any other text . All the required data are stored in the directory : { dataset_path } 26 F.8 EHRShot Prompts We include prompt details for EHRShot tasks as follows: BioDSBench Prompt Main You are an biomedical expert in writing machine learning code to solve EHR - relevant tasks . Your objective is to solve machine learning task based on the given data , with the goal of maximizing the performance of the model in limited steps . You must use Machine Learning / Deep Learning methods to solve the problem , the score of random guess or without any ML / DL methods will be canclled finally . You are likely to train models according to specific task requirements . You have access to GPU and several CPUs for training DL / ML models . Use CUDA and PyTorch for faster training if needed . Code requirements : - Read all data files from data_dir ={ data_dir } - Save all the predictions given by the model to file named ' predictions -{ task_name }. csv ' in the './ cache / ehrshot /{ model }/ ' directory . - Don 't add , delete , or modify any files in data_dir - Use \" print \" to output information in the feedback - No plotting or visualization is allowed - Code should be self - contained and not rely on any variables or state outside - Code must be completely runnable , otherwise it will be considered as failed - Optimize your Model / Parameters / Data Processing / Algorithm for continuous improvement - The prediction file should be csv file with the following format , where the prediction should be predicted labels instead of predicted probabilities : patient_id , prediction 115967096 , 8192 ... { feature_information } { label_information } BioDSBench Prompt Feature Information The corresponding features are stored in the following directories : { feature_directory_train }: training features for the task { feature_directory_val }: validation features for the task { feature_directory_test }: test features for the task Each of the feature files is dictionary , containing the following keys : - data_matrix : the feature vectors of the visits , where each row is embedded vector , representing single visit of patient - patient_ids : the identifiers of the patients , where each row is visit and the corresponding patient id - labeling_time : the time of the visit , where each row is visit and the corresponding time 27 BioDSBench Prompt Label Information The corresponding labels are stored in the following directories : { label_directory_train }: training labels for the task { label_directory_val }: validation labels for the task { label_directory_test }: test labels for the task Each of the label files contain the following columns : - patient_id : the identifier of the patient - value : the label value of the patient on the { task_name } task - label_type : the type of the label , which can be ' categorical '/ ' boolean ' , etc . - prediction_time : only the features before this time can be used to predict the label , used in data processing stage F.9 EHR-SeqSQL Prompts We include prompt details for EHR-SeqSQL tasks as follows: EHR-SeqSQL Prompt Part You are an biomedical expert in handling EHR data and answer questions accordingly . Your objective is to solve coding problem with given EHR data , with the goal of finally give concrete answer to the question . Assume you have knowledge of several tables : (1) Tables are linked by identifiers which usually have the suffix 'ID '. For example , SUBJECT_ID refers to unique patient , HADM_ID refers to unique admission to the hospital , and ICUSTAY_ID refers to unique admission to an intensive care unit . (2) Charted events such as notes , laboratory tests , and fluid balance are stored in series of ' events ' tables . For example the outputevents table contains all measurements related to output for given patient , while the labevents table contains laboratory test results for patient . (3) Tables prefixed with 'd_ ' are dictionary tables and provide definitions for identifiers . For example , every row of chartevents is associated with single ITEMID which represents the concept measured , but it does not contain the actual name of the measurement . By joining chartevents and d_items on ITEMID , it is possible to identify the concept represented by given ITEMID . (4) For the databases , four of them are used to define and track patient stays : admissions , patients , icustays , and transfers . Another four tables are dictionaries for cross - referencing codes against their respective definitions : d_icd_diagnoses , d_icd_procedures , d_items , and d_labitems . The remaining tables , including chartevents , cost , inputevents_cv , labevents , microbiologyevents , outputevents , prescriptions , procedures_icd , contain data associated with patient care , such as physiological measurements , caregiver observations , and billing information . For different tables , they contain the following information : (1) ADMISSIONS . csv : ROW_ID , SUBJECT_ID , HADM_ID , ADMITTIME , DISCHTIME , ADMISSION_TYPE , ADMISSION_LOCATION , DISCHARGE_LOCATION , INSURANCE , LANGUAGE , MARITAL_STATUS , ETHNICITY , AGE (2) CHARTEVENTS . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICUSTAY_ID , ITEMID , CHARTTIME , VALUENUM , VALUEUOM 28 EHR-SeqSQL Prompt Part II (3) COST . csv : ROW_ID , SUBJECT_ID , HADM_ID , EVENT_TYPE , EVENT_ID , CHARGETIME , COST (4) D_ICD_DIAGNOSES . csv : ROW_ID , ICD9_CODE , SHORT_TITLE , LONG_TITLE (5) D_ICD_PROCEDURES . csv : ROW_ID , ICD9_CODE , SHORT_TITLE , LONG_TITLE (6) D_ITEMS . csv : ROW_ID , ITEMID , LABEL , LINKSTO (7) D_LABITEMS . csv : ROW_ID , ITEMID , LABEL (8) DIAGNOSES_ICD . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICD9_CODE , CHARTTIME (9) ICUSTAYS . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICUSTAY_ID , FIRST_CAREUNIT , LAST_CAREUNIT , FIRST_WARDID , LAST_WARDID , INTIME , OUTTIME (10) INPUTEVENTS_CV . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICUSTAY_ID , CHARTTIME , ITEMID , AMOUNT (11) LABEVENTS . csv : ROW_ID , SUBJECT_ID , HADM_ID , ITEMID , CHARTTIME , VALUENUM , VALUEUOM (12) MICROBIOLOGYEVENTS . csv : RROW_ID , SUBJECT_ID , HADM_ID , CHARTTIME , SPEC_TYPE_DESC , ORG_NAME (13) OUTPUTEVENTS . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICUSTAY_ID , CHARTTIME , ITEMID , VALUE (14) PATIENTS . csv : ROW_ID , SUBJECT_ID , GENDER , DOB , DOD (15) PRESCRIPTIONS . csv : ROW_ID , SUBJECT_ID , HADM_ID , STARTDATE , ENDDATE , DRUG , DOSE_VAL_RX , DOSE_UNIT_RX , ROUTE (16) PROCEDURES . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICD9_CODE , CHARTTIME (17) TRANSFERS . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICUSTAY_ID , EVENTTYPE , CAREUNIT , WARDID , INTIME , OUTTIME All the tabls are saved in the data directory { data_directory }. F.10 EHRCon Prompts We include prompt details for EHRCon tasks as follows: EHRCon Prompt Part You are an biomedical expert in handling EHR data and answer questions accordingly . Your objective is to solve coding problem with given EHR data , with the goal of finally give concrete answer to the question . Assume you have knowledge of several tables : (1) Tables are linked by identifiers which usually have the suffix 'ID '. For example , SUBJECT_ID refers to unique patient , HADM_ID refers to unique admission to the hospital , and ICUSTAY_ID refers to unique admission to an intensive care unit . (2) Charted events such as notes , laboratory tests , and fluid balance are stored in series of ' events ' tables . For example the outputevents table contains all measurements related to output for given patient , while the labevents table contains laboratory test results for patient . (3) Tables prefixed with 'd_ ' are dictionary tables and provide definitions for identifiers . For example , every row of chartevents is associated with single ITEMID which represents the concept measured , but it does not contain the actual name of the measurement . By joining chartevents and d_items on ITEMID , it is possible to identify the concept represented by given ITEMID . 29 EHRCon Prompt Part II (4) For the databases , four of them are used to define and track patient stays : admissions , patients , icustays , and transfers . Another four tables are dictionaries for cross - referencing codes against their respective definitions : d_icd_diagnoses , d_icd_procedures , d_items , and d_labitems . The remaining tables , including chartevents , cost , inputevents_cv , labevents , microbiologyevents , outputevents , prescriptions , procedures_icd , contain data associated with patient care , such as physiological measurements , caregiver observations , and billing information . For different tables , they contain the following information : (1) ADMISSIONS . csv : ROW_ID , SUBJECT_ID , HADM_ID , ADMITTIME , DISCHTIME , ADMISSION_TYPE , ADMISSION_LOCATION , DISCHARGE_LOCATION , INSURANCE , LANGUAGE , MARITAL_STATUS , ETHNICITY , AGE (2) CHARTEVENTS . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICUSTAY_ID , ITEMID , CHARTTIME , VALUENUM , VALUEUOM (3) COST . csv : ROW_ID , SUBJECT_ID , HADM_ID , EVENT_TYPE , EVENT_ID , CHARGETIME , COST (4) D_ICD_DIAGNOSES . csv : ROW_ID , ICD9_CODE , SHORT_TITLE , LONG_TITLE (5) D_ICD_PROCEDURES . csv : ROW_ID , ICD9_CODE , SHORT_TITLE , LONG_TITLE (6) D_ITEMS . csv : ROW_ID , ITEMID , LABEL , LINKSTO (7) D_LABITEMS . csv : ROW_ID , ITEMID , LABEL (8) DIAGNOSES_ICD . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICD9_CODE , CHARTTIME (9) ICUSTAYS . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICUSTAY_ID , FIRST_CAREUNIT , LAST_CAREUNIT , FIRST_WARDID , LAST_WARDID , INTIME , OUTTIME (10) INPUTEVENTS_CV . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICUSTAY_ID , CHARTTIME , ITEMID , AMOUNT (11) LABEVENTS . csv : ROW_ID , SUBJECT_ID , HADM_ID , ITEMID , CHARTTIME , VALUENUM , VALUEUOM (12) MICROBIOLOGYEVENTS . csv : RROW_ID , SUBJECT_ID , HADM_ID , CHARTTIME , SPEC_TYPE_DESC , ORG_NAME (13) OUTPUTEVENTS . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICUSTAY_ID , CHARTTIME , ITEMID , VALUE (14) PATIENTS . csv : ROW_ID , SUBJECT_ID , GENDER , DOB , DOD (15) PRESCRIPTIONS . csv : ROW_ID , SUBJECT_ID , HADM_ID , STARTDATE , ENDDATE , DRUG , DOSE_VAL_RX , DOSE_UNIT_RX , ROUTE (16) PROCEDURES . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICD9_CODE , CHARTTIME (17) TRANSFERS . csv : ROW_ID , SUBJECT_ID , HADM_ID , ICUSTAY_ID , EVENTTYPE , CAREUNIT , WARDID , INTIME , OUTTIME All the tables are saved in the . db file at { db_location }. In addition , you have access to csv containing the clinical notes with the matching subject ids and hospital admission ids : ROW_ID , SUBJECT_ID , HADM_ID , CHARTDATE , CHARTTIME , STORETIME , CATEGORY , DESCRIPTION , CGID , ISERROR , TEXT , ADMITTIME This clinical note csv is at { note_csv }. F.11 MIMIC-Extract Prompts We include prompt details for MIMIC-EXTRACT tasks as follows: 30 MIMIC-EXTRACT Prompt PART You are an biomedical expert in writing machine learning code to solve EHR - relevant tasks . Your objective is to solve machine learning task based on the given data , with the goal of maximizing the performance of the model in limited steps . You must use Machine Learning / Deep Learning methods to solve the problem , the score of random guess or without any ML / DL methods will be canceled finally . You are likely to train models according to specific task requirements . You have access to GPU and several CPUs for training DL / ML models . Use CUDA and PyTorch for faster training if needed . Code requirements : - Read all data files from data_dir ={ data_dir } - Save all the predictions given by the model to file named ' predictions -{ task_name }. csv ' in the './ cache / ehrshot /{ model }/ ' directory . - Don 't add , delete , or modify any files in data_dir - Use \" print \" to output information in the feedback - No plotting or visualization is allowed - Code should be self - contained and not rely on any variables or state outside - Code must be completely runnable , otherwise it will be considered as failed - Optimize your Model / Parameters / Data Processing / Algorithm for continuous improvement - The prediction file should be csv file with the following format , where the prediction should be predicted labels instead of predicted probabilities : You have the data splits based on hospital admission ids . You are asked to use longitudinal EHR data within each admission instance to predict two types of tasks : (1) Classification associated with the entire duration of admission : mortality inside hospital , mortality inside ICU , length of stay beyond 3 days , length of stay beyond 7 days . All 4 are binary classification tasks using lab features only . For the first task , the output csv should have two columns : subject_id , prediction 9923 , 0 ... (2) Classification associated with hourly measurements : intervention of vasopressor in ICU , and intervention of ventilator in ICU . Use the past 6 hours of lab measurements and static demographics ( matching patient id ) to predict the 4 intervention statuses during the 4 - hour period after 6 hours . For the second task , the output csv should have three colums instead : subject_id , window_idx , prediction 140 , 4, 3 ... The corresponding features are stored in the following directories : { feature_directory_train }: training features for the task { feature_directory_val }: validation features for the task { feature_directory_test }: test features for the task 31 MIMIC-EXTRACT Prompt PART II Each of the feature files is pickled pandas dataframe : - subject_id : the unique ID of the subject - hadm_id : the unique ID of the hospital admission - icustay_id : the unique ID of the ICU session - hours_in : the number of hours since hospital admission . Counting from - The rest of the columns are organized in groups of three , where the outer level specifies the type of measurements (e.g. alanine aminotransferase and ph urine ) , and the inner level lists the count , mean and std of the measurements , respectively . The table has been imputed . { feature_information } { label_information } MIMIC-EXTRACT Prompt Lab Feature The corresponding features are stored in the following directories : { feature_directory_train }: training features for the task { feature_directory_val }: validation features for the task { feature_directory_test }: test features for the task Each of the feature files is pickled pandas dataframe : - subject_id : the unique ID of the subject - hadm_id : the unique ID of the hospital admission - icustay_id : the unique ID of the ICU session - hours_in : the number of hours since hospital admission . Counting from 0 - The rest of the columns are organized in groups of three , where the outer level specifies the type of measurements (e.g. alanine aminotransferase and ph urine ) , and the inner level lists the count , mean and std of the measurements , respectively . The table has been imputed . MIMIC-EXTRACT Prompt Static Feature The corresponding features are stored in the following directories : { feature_directory_train }: demographic training features for the task { feature_directory_val }: demographic validation features for the task { feature_directory_test }: demographic test features for the task Each of the feature files is pickled pandas dataframe : - subject_id : the unique ID of the subject - hadm_id : the unique ID of the hospital admission - icustay_id : the unique ID of the ICU session - intime : the total number of hours in the associated admission - gender_F and gender_M : one - hot boolean columns for gender - Age 1.0 , Age 2.0 , Age 3.0 , Age 4.0: one - hot boolean columns for ages groups of 10 -30 , 30 -50 , 50 -70 , and >70 , respectively - Ethnicity columns : one - hot boolean columns for ethnicity ( American Indian , Asian , Black , Hispano , Other , White ) - First care columns : one - hot boolean columns for first admitted care unit ( CCU , CSRU , MICU , SICU , TSICU ) 32 MIMIC-EXTRACT Prompt Mor Los Label The corresponding labels are stored in the following directories : { label_directory_train }: training labels for the task { label_directory_val }: validation labels for the task { label_directory_test }: test labels for the task Each of the label csv files contain the following columns : - subject_id : the unique ID of the subject - hadm_id : the unique ID of the hospital admission - mort_icu or mort_hosp or los_3 or los_7 : the boolean label for whether the patient died in the ICU , died in hospital , the length of stay exceeding 3 days , and LOS exceeding 7 days , respectively - label_type : the type of the label , which can be ' categorical '/ ' boolean ' , etc . MIMIC-EXTRACT Prompt Ventilator Vasopressor Label The corresponding labels are stored in the following directories : { label_directory_train }: training labels for the task { label_directory_val }: validation labels for the task { label_directory_test }: test labels for the task Each of the label csv files contain the following columns : - subject_id : the unique ID of the subject - 6 _hour_window_id : the 6 hour predicted window counted since the patient is admitted to hospital . - intervention_category : one of the four scenarios : Label 1 \" CONTROL \": No intervention throughout the prediction window . Label 2 \" ON INTERVENTION \": The intervention persists throughout the prediction window . Label 3 \" ONSET \": Intervention starts within the prediction window . Label 4 \" WEAN \": Intervention ends within the prediction window . - label_type : the type of the label , which can be ' categorical '/ ' boolean ' , etc . F.12 N-PowerAI Prompts We include prompt details for NPowerAI tasks as follows: NPowerAI Prompt You are scientist conducting biomedical research and constantly facing statistical problems . Sometimes , you need to find the minimum sample size to achieve specific power . In other times , you would like to know the statistical power given population size ."
        }
    ],
    "affiliations": [
        "Emory University",
        "Georgia Tech",
        "UT Southwestern Medical Center",
        "Yale University"
    ]
}