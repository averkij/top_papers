{
    "paper_title": "Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis",
    "authors": [
        "Anushka Yadav",
        "Isha Nalawade",
        "Srujana Pillarichety",
        "Yashwanth Babu",
        "Reshmi Ghosh",
        "Samyadeep Basu",
        "Wenlong Zhao",
        "Ali Nasaeh",
        "Sriram Balasubramanian",
        "Soundararajan Srinivasan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The emergence of reasoning models and their integration into practical AI chat bots has led to breakthroughs in solving advanced math, deep search, and extractive question answering problems that requires a complex and multi-step thought process. Yet, a complete understanding of why these models hallucinate more than general purpose language models is missing. In this investigative study, we systematicallyexplore reasoning failures of contemporary language models on multi-hop question answering tasks. We introduce a novel, nuanced error categorization framework that examines failures across three critical dimensions: the diversity and uniqueness of source documents involved (\"hops\"), completeness in capturing relevant information (\"coverage\"), and cognitive inefficiency (\"overthinking\"). Through rigorous hu-man annotation, supported by complementary automated metrics, our exploration uncovers intricate error patterns often hidden by accuracy-centric evaluations. This investigative approach provides deeper insights into the cognitive limitations of current models and offers actionable guidance toward enhancing reasoning fidelity, transparency, and robustness in future language modeling efforts."
        },
        {
            "title": "Start",
            "content": "Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis Anushka Yadav2*, Isha Nalawade2*, Srujana Pillarichety2*, Yashwanth Babu2*, Reshmi Ghosh1, Samyadeep Basu3, Wenlong Zhao2, Ali Nasaeh2, Sriram Balasubramaniam3, Soundararajan Srinivasan1 * equal contribution 1Microsoft, 2University of Massachusetts, Amherst, 3University of Maryland, College Park, Correspondence: reshmighosh@microsoft.com 5 2 0 2 6 ] . [ 1 9 9 6 4 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The emergence of reasoning models and their integration into practical AI chat bots has led to breakthroughs in solving advanced math, deep search, and extractive question answering problems that requires complex and multi-step thought process. Yet, complete understanding of why these models hallucinate more than general purpose language models is missing. In this investigative study, we systematically explore reasoning failures of contemporary language models on multi-hop question answering tasks. We introduce novel, nuanced error categorization framework that examines failures across three critical dimensions: the diversity and uniqueness of source documents involved (\"hops\"), completeness in capturing relevant information (\"coverage\"), and cognitive inefficiency (\"overthinking\"). Through rigorous human annotation, supported by complementary automated metrics, our exploration uncovers intricate error patterns often hidden by accuracycentric evaluations. This investigative approach provides deeper insights into the cognitive limitations of current models and offers actionable guidance toward enhancing reasoning fidelity, transparency, and robustness in future language modeling efforts."
        },
        {
            "title": "Introduction",
            "content": "Language models (LMs) have demonstrated remarkable performance on multi-hop question answering (QA) benchmarks, such as HotpotQA (Yang et al., 2018), where success requires sourcing knowledge from multiple documents. MuSiQue (Trivedi et al., 2022) extends this task by posing harder questions that reduces shortcut reasoning and provides explicit reasoning paths to better assess multi-step inference. The traditional evaluation metrics employed in these tasks, such as the final answer accuracy or the F1 score, fail to distinguish between genuine multistep inference, simple memorization (as exposed Figure 1: Illustration of Multi-hop Reasoning language model answers via three-hop reasoning chain. The back-and-forth between documents illustrates overthinking, where unnecessary steps are added beyond the ideal inference path. by counterfactual benchmarks such as CofCA; (Wu et al., 2025), and over-reliance on dataset artifacts. Moreover, emerging studies (Sakarvadia, 2024; Agarwal et al., 2024) show that errors may stem from missing knowledge recall, misinterpretation of question intent, or retrieval failures in retrievalaugmented settings. With these limitations in mind, we move beyond answer correctness and undertake an investigative exploration of reasoning failures in multi-hop QA to answer central question: How and why do reasoning models break down when stitching together information across multiple sources? To address this, we introduce diagnostic framework that decomposes reasoning behavior along three core dimensions: (1) Hops hop is discrete step or transition in the reasoning process where the model moves from one piece of information (e.g., fact, source, or knowledge base entry) to another in order to bridge connections and form complete answer. (2) Coverage evaluates whether all necessary reasoning steps are covered; and (3) Overthinking refers to whether the model meanders into unnecessary or off-track reasoning. These dimensions support both qualitative annotation and targeted quantitative evaluation of reasoning fidelity. In this paper, we introduce detailed set of reasoning error categories and apply them to manually annotate model traces from six language models across three multi-hop QA datasets. Building on these annotations, we develop an automated evaluation framework that closely mirrors human judgments. In this paper, we define seven fine-grained reasoning error categories and manually annotate up to 80 responses across three datasets and six models. To scale this analysis, we introduce an LLM-asa-Judge framework that achieves 74% hop match accuracy and 5075% label agreement with human annotations on 2Wiki, MuSiQue, and HotpotQA. Contributions: We present detailed analysis of multi-hop reasoning by curating and annotating model responses on three diverse datasets: 2WikiMultiHopQA, HotpotQA, and MuSiQue. Using structured error taxonomy, we quantify the distribution of reasoning errors across models. Our study reveals common reasoning issues, such as breaking down in the middle of reasoning, adding unnecessary steps in complex cases, and providing correct answers despite flawed reasoning, especially on questions with many entities or confusing information. Finally, we evaluate the effectiveness of an LLM-as-a-Judge framework, which shows strong agreement with human annotations on simpler datasets while highlighting key limitations on more complex ones. This supports the use of scalable, semi-automated evaluation for reasoning analysis."
        },
        {
            "title": "2 Related Works",
            "content": "Reasoning in large language models has progressed from relying on statistical correlations to adopting more structured mechanisms like Chain-ofThought (CoT) prompting (Wei et al., 2023). Despite this, several studies have shown that LLMs often generate unfaithful explanations that do not reflect the true reasoning path, with causal analyses suggesting that these traces are frequently post hoc rationalizations (Bao et al., 2024). Recent work on Large Reasoning Models finds that, while they can outperform standard LLMs on medium-complexity tasks, they exhibit surprising scaling limits and collapse in accuracy on high-complexity problems despite detailed reasoning traces (Shojaee et al., 2025). Traditional metrics like F1 and BLEU focus on answer correctness but overlook reasoning quality, and multihop QA benchmarks have revealed that models often exploit shortcuts to arrive at correct answers without faithfully connecting supporting evidence (Ishii et al., 2024). Heuristic-based evaluations can further mask such reasoning failures (Lanham et al., 2023), prompting the need for more targeted reasoning assessments. To address this, recent work has focused on identifying and analyzing intermediate reasoning errors. Studies show that correcting flawed steps can improve model robustness (Li et al., 2024), while ProcessBench highlights issues like process errors and logical inconsistency during multi-step reasoning (Zheng et al., 2024). Adding explicit premises to reasoning chains has been shown to improve error detection and clarity (Mukherjee et al., 2025). However, challenges remain: hallucinations and factual inconsistencies in long-form outputs are still hard to detect even with strong models (Kamoi et al., 2024), and repeated reasoning mistakes persist without explicit supervision (Tong et al., 2024). Beyond standard QA settings, more recent evaluations have looked at reasoning in complex domains. Work on Olympiad-style math problems finds that models often produce shallow or incomplete reasoning despite correct final answers (Mahdavi et al., 2025). Similarly, in multimodal settings, ErrorRadar benchmarks expose systematic reasoning failures in math-heavy questions, reinforcing the importance of fine-grained reasoning analysis beyond surface-level correctness (Yan et al., 2024). Our work builds on these insights by explicitly annotating multi-hop reasoning traces and categorizing failure patterns across diverse QA datasets, enabling more principled and scalable evaluation of reasoning quality."
        },
        {
            "title": "3.1 Task Formalization",
            "content": "We define multi-hop QA as the task of responding to complex questions, undertaken by reasoning models, that necessitate synthesizing information from multiple sources through chain of reasoning steps. hop, denoted by hi, refers to distinct reasoning step wherein the model extracts supporting evidence from unique document dj D. The number of hops in reasoning path corresponds to the number of unique documents accessed, regardless of how much content is extracted from each. Figure 1 illustrates this hop-based reasoning process, highlighting how models transition between documents. For question and collection of documents = {d1, d2, . . . , dm}, the task is to predict (1) an answer (a textual span within one of the documents in D), and (2) reasoning path = (h1, h2, . . . , hnmodel) representing the sequence of reasoning hops. The model hop count is defined as Nmodel = P, (1) where denotes the length of the models hop sequence. The gold hop count is defined as Stage 3: Final Schema with Meta-Evaluation Markers The final schema addressed these issues through clearer definitions. We formally defined reasoning hop, excluding repeated entity mentions within the same document to avoid inflated hop counts. We also distinguished overthinking via cross-document exploration (Nmodel > Ngold) from verbose or circular reasoning within document, captured by separate overthinking flag. These changes transformed the taxonomy into more principled and disjoint framework. Annotator agreement improved significantly as category boundaries became more interpretable and semantically grounded. Ngold = , (2) 3.3 Definitions of Reasoning Categories where is the gold-standard reasoning path required to answer the question."
        },
        {
            "title": "3.2 Refining Reasoning Categories",
            "content": "To diagnose reasoning failures in multi-hop QA, we refined our error taxonomy through three iterative stages. Each stage addressed prior shortcomings and improved inter-annotator agreement, as shown in Figure 3. Full definitions for Stage 1 and Stage 2 are in the Appendix. Stage 1: Coarse Conceptual Labels Our initial taxonomy used four loosely defined labels: Effective, Underthinking, Overthinking, and Faulty. These arose from manual trace inspection but lacked clear definitions. Annotators struggled to distinguish between concise reasoning and underthinking, or between verbose, incorrect reasoning and overthinking. The lack of formal notion of reasoning hops made error tracing difficult. Faulty served as catch-all for various errors, reducing analytical usefulness. Stage 2: Structured Hop-Based Categorization In the second stage, we introduced 10-category taxonomy based on Nmodel, Ngold, hop correctness, and answer accuracy to support structured error analysis. As manual evaluation scaled, new ambiguities emerged. Category 8 (early hallucinations) often overlapped with Category 6 (underspecified chains) and question misinterpretation. Annotators also struggled to distinguish shortcut reasoning from flawed logic. These overlaps revealed that even structurally driven categories needed stronger semantic clarity. Following iterative refinement and extensive pilot annotations, we arrived at final taxonomy that enabled high inter-annotator agreement. As shown in Figure 3, this version resolved prior ambiguities by enforcing stricter hop semantics and introducing meta-evaluation markers to capture surface-level verbosity independently from structural reasoning failure. Table 1 summarizes our final taxonomy, providing precise operational definitions for each reasoning error category used in our annotation pipeline. These categories, combined with the meta-evaluation markers of overthinking and coverage, provide comprehensive coverage of potential reasoning errors, enabling systematic and insightful error diagnosis in multi-hop QA models. Meta-Evaluation Markers To further enhance our analytical granularity, we introduced meta-evaluation markers: Overthinking: This marker captures indicators of cognitive inefficiency in the models reasoning. It is applied when: 1) the model includes nonessential information from gold documentssuch as background details, tangential facts, or calculationsthat do not aid in progressing the reasoning chain; and 2) the model demonstrates repetitive or circular behavior, such as repeatedly checking the same entity or relation more than twice. Coverage: This marker addresses the completeness of source-document utilization, specifically evaluating whether the model successfully retrieves all necessary source documents. Low coverage indicates gaps in retrieval or attention, leading to Reasoning Category Definition Nmodel = Ngold; Fully Correct Hops The model executes the exact number of required gold reasoning hops, and each hop is logically sound, complete, and correct. Nmodel = Ngold; Partially Correct Hops The model executes the correct number of reasoning steps, but one or more hops involve incorrect documents, entities, or relations. The model reasoning is partially misaligned with the gold reasoning path. Nmodel < Ngold; Fully Correct Hops The model executes fewer hops than required, yet all executed reasoning steps are correct and directly correspond to subset of the required hops. This indicates incomplete but partially correct reasoning. Nmodel < Ngold; Partially Correct Hops The model executes fewer reasoning steps than required, omitting essential hops and introducing incorrect hops within the shortened chain. The reasoning is both incomplete and partially incorrect. Nmodel > Ngold; Trailing Irrelevance The model initially executes all required reasoning steps but then continues with additional irrelevant hops. These extra steps occur after completing the required reasoning and reflect the models extraneous elaboration. Nmodel > Ngold; Early Irrelevance The model introduces irrelevant reasoning steps before or interspersed among the required hops. These interruptions disrupt logical reasoning progression, resulting in confusion, distraction or circular reasoning. The required reasoning steps may be partially addressed or incorrect."
        },
        {
            "title": "Question\nMisinterpretation",
            "content": "The model misunderstands the original question during its early reasoning steps, often focusing on incorrect entities or setting up the wrong task, leading to fundamentally flawed reasoning. Table 1: Definitions of Reasoning Categories in Multi-Hop QA. Nmodel denotes the number of reasoning hops executed by the model; Ngold is the number of required gold hops. incomplete reasoning chains or unsupported conclusions."
        },
        {
            "title": "4 Experimental Setup",
            "content": "Models We analyze six language models that span range of architectures, parameter scales, and accessibility. Our primary focus is on four open-source distilled modelsDEEPSEEK-R1DISTILL-LLAMA-8B, DEEPSEEK-R1-DISTILLLLAMA-70B, DEEPSEEK-R1-DISTILL-QWEN7B, and DEEPSEEK-R1-DISTILL-QWEN-14B. To complement these, we include two original reasoning models: CLAUDE 3.7 SONNET, proprietary reasoning model, and DEEPSEEK-R1, an openweight reasoning model. For all DeepSeek models, we set the generation temperature to 0.6, following the recommendations of Liu et al. (2024), to mitigate endless repetition or incoherent outputs. For Claude 3.7 Sonnet, we use deterministic setting with the temperature set to 0. Datasets We evaluate model reasoning across three multi-hop QA datasets of increasing difficulty: 2WikiMultiHopQA (Ho et al., 2020), which emphasizes structured multi-hop reasoning; HotpotQA (Yang et al., 2018), which includes distractors and diverse reasoning types like comparisons; and MuSiQue (Trivedi et al., 2022), highcomplexity benchmark designed to minimize shortcuts through dense context and sub-question dependencies. Dataset details are provided in Table 5 in the appendix."
        },
        {
            "title": "Question Types",
            "content": "To enable systematic reasoning analysis, we categorize multi-hop questions into five distinct types based on their logical structure: Compositional, Comparison, Intersection, Inference, and Bridge Comparison. These categories reflect the types of reasoning steps required to arrive at the correct answer."
        },
        {
            "title": "Detailed definitions and illustrative examples for",
            "content": "each type are provided in Appendix  (Table 4)  . Figure 2: Examples of Reasoning Error Categories. Representative outputs illustrating the main error categories in multi-hop reasoning for single example. The correct entities are highlighted in green, incorrect in red, and irrelevant or extraneous information in yellow."
        },
        {
            "title": "4.1 Annotation Process",
            "content": "To evaluate the reasoning quality of model outputs, we develop structured human annotation pipeline encompassing three key stages: Sampling and Generation: We uniformly sampled 240 questions across HotpotQA, 2WikiMultiHopQA, and MuSiQue. Six models answered each question using standardized prompting strategy designed to minimize instruction-induced bias. Final Answer and Meta Eval Markers: The final answers were evaluated for correctness using automated matching, with manual verification for paraphrased or non-exact responses. Simultaneously, we annotated: 1) the Nmodel, 2) the binary Coverage Marker and 3) the Overthinking Marker . Reasoning Category Assignment: Each response was categorized into one of our predefined reasoning error types. (see Section 3.3). Figures 7 and 8 show our custom annotation interface, which facilitates structured error labeling, flag toggling, and hop trace visualization. In total, we annotated 1,440 model outputs. Following the removal of examples with missing answers in the context caused by dataset artifacts, 1,080 examples were retained for analysis. Figure 3: Improvement in Inter-Annotator Agreement Across Refinement Stages. Raw agreement and Cohens kappa both increase substantially as the reasoning error taxonomy evolves from loosely defined to formally structured categories, with the highest agreement achieved after Stage 3 refinements."
        },
        {
            "title": "5.1 Reasoning Fidelity and Answer Accuracy",
            "content": "Figure 4 summarizes model behavior on fully correct hop alignment (Nmodel = Ngold) and finalanswer accuracy across datasets. We see that reasoning Fidelity holds in Simpler Tasks but collapses in Complex Chains. Across all datasets, Claude 3.7 achieves the highest accuracy. (a) 2Wiki (b) HotpotQA (c) MuSiQue Figure 4: Relationship Between Reasoning Fidelity and Answer Accuracy Across Datasets. Each subplot shows model performance on (a) 2Wiki, (b) HotpotQA, and (c) MuSiQue. Each point represents the performance of model, with the x-axis showing the fraction of fully correct reasoning traces (Nmodel = Ngold) and the y-axis showing final answer accuracy. The dotted diagonal (y = x) marks perfect alignment; points above the line indicate models that answer correctly even when reasoning is imperfect. High Reasoning Fidelity on 2Wiki: All models perform strongly on the 2Wiki dataset, with the majority of models have around 80% datapoints with Nmodel = Ngoldcorrespondence: fully correct hops, and near-perfect final answer accuracy This confirms that current LMs reliably handle simple multi-hop questions. Inefficient Reasoning in HotpotQA: Performance on HotpotQA shows the highest concentration of (Nmodel > Ngold) (Figure 9b). While the final-answer accuracy remains high, the presence of semantically dense and distractor-filled paragraphs leads models to over-explore the context, often beyond the required inference chain. This behavior highlights the limitations of current LMs in maintaining focused reasoning under noisy, multidocument settings. Intermediate Fidelity and Model-Specific Patterns emerge on the MuSiQue dataset: Larger models demonstrate intermediate reasoning fidelity (4565%) alongside relatively high answer accuracy. Smaller models exhibit poor performance on both metrics, underscoring difficulties in complex multi-hop contexts. Notably, DeepSeek-R1 shows the greatest divergence, achieving very high answer accuracy despite substantially lower reasoning fidelity."
        },
        {
            "title": "Datasets",
            "content": "Figure 9 show the distribution of reasoning error types in the MuSiQue, 2Wiki-MultiHopQA, and HotpotQA datasets. Our analysis reveals the following insights: Claude 3.7 Sonnet Sets the Bar for Stable and Precise Reasoning: Among all evaluated models, Claude 3.7 Sonnet demonstrates the most stable and controlled reasoning behavior. It consistently maintains high rates of fully correct reasoning while keeping all other error typesespecially early and trailing irrelevancesignificantly lower than both DeepSeek-R1 and the distilled model variants. Overhopping is the Most Persistent and Systemic Reasoning Failure: Across all datasets and models, overhopping (Nmodel > Ngold categories in Figure 9) is consistently higher than other errors. This often stems from contextual redundancy or ambiguity, pushing models to overexplore rather than terminate. The Qwen family of models particularly struggles with this issue: frequently displaying early and trailing irrelevance errorseven at larger scalesindicating proclivity for recall over precise reasoning. Scaling Models Improves Simple Reasoning but Leaves Complex Errors Unresolved: As shown in Figure 9, increasing model size leads to more examples with fully correct hops (the leftmost bars in each subplot), particularly on simpler tasks like 2Wiki (Figure 9a). However, for more complex datasets such as HotpotQA and MuSiQue (Figure 9b, c), the gains from scaling plateau. Even the largest models still exhibit substantial numbers of early irrelevance and trailing irrelevance errors (the right-side bars). This persistent error pattern indicates that, while scale enhances basic multihop reasoning, it does not fully resolve deeper reasoning challenges in complex or distractor-heavy settings. Deepseek-R1 Distilled Models Rival the Deepseek-R1 Counterpart in Multi-hop Tasks: On both simple and moderately complex datasets, distilled LLaMA variants show strong reasoning alignment. The LLaMA 70B variant performs almost similarly or even better than the original Deepseek-R1 model. 5.3 Relationship Between Reasoning Errors and Final Answer Correctness Figure 10 examines the relationship between reasoning trace quality and final answer correctness across datasets. Correctness Tightly Coupled with Reasoning Quality: As seen in the leftmost bars in Figure 10, across all datasets, final correct answers almost exclusively emerge from the (Nmodel = Ngold), Fully Correct Hops category. Even minor deviations in the reasoning chain, such as exceeding hops or partial hops, reduce the likelihood of correctness. Answer Correctness is Sensitive to Missing Hops: Looking at the \"Partially Correct Hops\" bars in all three panels of Figure 10, we see that incomplete reasoning rarely yields correct answer. This confirms that failure to cover all necessary facts, even in part, is definitive bottleneck in LMs reasoning chains. Smaller Models are More Fragile to Reasoning Errors: As shown in Figure 10 across all datasets, smaller models such as LLaMA-8B and Qwen-7B exhibit higher propensity for reasoning errors to cascade into incorrect final answers. In contrast, larger models like DeepSeek-R1 and Claude 3 Sonnet demonstrate greater robustness, with fewer incorrect answers arising from these types of reasoning errors. Early Irrelevance is More Detrimental than Trailing Irrelevance: In every panel of Figure 10, the \"Early Irrelevance\" category shows that \"Answer Incorrect\" bars are higher compared to that corresponding to \"Trailing Irrelevance.\" This suggests that irrelevant reasoning steps introduced early in the chain are more disruptiveto the models final answer. 5.4 Overthinking Trends and Their Impact We systematically examine the prevalence and impact of overthinking across different models and datasets, highlighting how this phenomenon influences overall model performance and error rates. Overthinking Surges in Complex Reasoning Tasks: As shown in the MuSiQue results (see Figure 9c and Table 2), overthinking rises markedly across all models, with rates ranging from 36.7% to 61.7%. Notably, DeepSeek-R1-Distill-Qwen7B reaches the highest overthinking rate of 61.7%, while even advanced models such as Claude 3 Sonnet and DeepSeek-R1 exhibit elevated rates. This trend suggests that task complexity, rather than model scale, is the primary driver of overthinking. Overthinking is Systematic Source of Incorrect Answers: significant portion of incorrect answers are accompanied by overthinking, especially in MuSiQue (see Figure 11a). Although HotpotQA and 2Wiki contain fewer errors labeled as overthinking (see Table 2), when overthinking does occur, it almost always results in incorrect answers (see shaded bars in Figure 11c and Figure 11b). This finding suggests that the negative impact of overthinking is not just limited to complex datasets but also arises from the logical incoherence it introduces, irrespective of task difficulty. Overthinking is not merely harmless elaboration, but systematic driver of reasoning collapse and failure to reach final answer."
        },
        {
            "title": "5.5 Distribution across Question types",
            "content": "Across the datasets, the multi-hop questions can be grouped into four key categories: Bridge Comparison questions, Comparison questions, Compositional questions, and Inference Questions (details of the question types and examples are in Section 4). Figure 12 shows the distribution of reasoning error types across question categories for all the models. We observe the following trends across different question types: Bridge Comparison Questions Are Consistently Solved, Especially from 2Wiki: Bridge questions (mainly from 2Wiki) yield 94100% fully correct hops across all models. Even smaller models like Qwen-7B and LLaMA-8B perform well, Model 2Wiki-MultiHopQA HotpotQA MuSiQue DeepSeek-R1-Distill-Llama-8B DeepSeek-R1-Distill-Llama-70B DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Qwen-14B DeepSeek-R1 Claude 3.7 Sonnet 41.2% 19.1% 26.5% 30.9% 27.9% 22.1% 29.3% 12.0% 41.3% 28.0% 18.7% 22.7% 48.3% 41.7% 61.7% 50.0% 53.3% 36.7% Table 2: Overthinking Rates by Model and Dataset. Percentage of answers with Overthinking for each model on the 2Wiki-MultiHopQA, HotpotQA, and MuSiQue datasets. The results highlight the substantial increase in overthinking in more complex MuSiQue dataset. Figure 5: Hop-wise Distribution of Reasoning Errors in LLaMA Family Models. The left and right panels compare error trends for LLaMA-8B and LLaMA-70B, respectively. Results highlight the decline in fully correct reasoning with greater hop count, and the increasing prevalence of overhopping errors on harder questions. while Claude 3.7 Sonnet and Qwen-14B make no errors. These questions often contain explicit reference to entities or co-occurrence patterns that mirror the pre-training distribution of the model, allowing models to resolve them through recognition of patterns at the surface level rather than deep reasoning. Symmetric Structures Trigger Redundant Reasoning and Overhopping: Found in HotpotQA and 2Wiki, comparison questions show 5068% fully correct rates, with 2545% of errors due to early or trailing irrelevance. Their symmetric phrasing encourages exploration of both options, even when one suffices. Claude occasionally bypasses intermediate hops while still producing correct answers, suggesting reliance on shortcut-style or selective reasoning paths. Compositional Reasoning Exposes Integration Failures: Compositional questions strain models ability to synthesize disjoint facts. Smaller models (Qwen-7B, LLaMA-8B, DeepSeek-R1) show many partially correct chains, even with correct hop counts. Claude and LLaMA-70B perform better, suggesting that scale and architecture improve integration. Misinterpretation errors are also higher here. Inference Questions Are the Most Error-Prone and Trigger Overthinking: Inference questions, heavily present in MuSiQue and 2Wiki, demand implicit reasoning and multi-step logic without strong lexical cues. These questions yield the broadest error types, early/trailing irrelevance, misinterpretation, and underhopping. Qwen-7B answers only 10% correctly, with 30% misinterpretation. Even DeepSeek-R1 shows 37% trailing irrelevance. Only Claude and LLaMA-70B manage modest control (5055% correct), highlighting the inherent difficulty of inference. Inference and Compositional Tasks Drive Overthinking: Overhopping is most common in inference questions, reaching 70% in Qwen-7B, 65% in LLaMA-8B, and 60% in DeepSeek-R1 and Qwen14B. Lack of clear stopping cues leads models to overgenerate. Bridge questions show minimal overhopping (<20%) due to their bounded structure. Even Large Models Struggle to Combine Retrieved Evidence: In compositional and inference settings, all models including large ones like DeepSeek-R1 and Claude 3.7 Sonnet exhibit partially correct reasoning chains, even when hop counts match the gold labels. This reveals that accurate retrieval alone is insufficient; models must also effectively link and reason over retrieved evidence. These synthesis failures suggest weaknesses in chain-of-thought alignment and reasoning coordination, particularly under pressure from semantically distant facts."
        },
        {
            "title": "5.6 Hop-wise Error Distribution",
            "content": "To better understand how reasoning evolves across multi-hop inference chains, we analyze the distribution of reasoning errors at the hop level. Figure 5 presents hop-wise error trends for the LLaMA family, while Figure 13 illustrates these trends for other models. Larger Models Are More Stable Across Hop Counts: As the number of required reasoning steps increases, most models exhibit clear drop in fully correct reasoning (Nmodel = Ngold). For example, in Figure 5 (left panel), DeepSeek-R1Distill-Llama-8B achieves 53% accuracy on 2-hop questions, but this drops to 16% for 3-hop and just 9% for 4-hop examples. In contrast, larger models such as DeepSeek-R1-Distill-Llama-70B (Figure 5, right panel) and Claude 3.7 Sonnet (Figure 13d) show much greater stability across hop lengths, maintaining relatively consistent performance even as reasoning depth increases. This suggests these models have stronger capacity to follow and complete longer reasoning chains without deviation. Overhopping Is Major Error Source in Harder Questions: For 4-hop questions, the most prominent error across several models is early irrelevance (Nmodel > Ngold). This is especially clear for DeepSeek-R1-Distill-Qwen-7B (Figure 13a), where 73% of 4-hop examples are categorized as early irrelevance. Both Claude 3.7 Sonnet and DeepSeek-R1-Distill-Qwen-14B (Figure 13b and d) show 45% early irrelevance at 4 hops. These results indicate that, in more complex tasks, models frequently continue reasoning beyond what is necessary, retrieving irrelevant or redundant information. Shallow Collapse in Qwen-7B, Depth Limitations in Claude 3.7 Sonnet: Qwen-7B (Figure 13a) shows signs of partial reasoning at 3 hops but collapses almost entirely into early irrelevance (73%) at 4 hops, abandoning intermediate reasoning strategies. This suggests that, under high reasoning load, smaller models tend to default to over-retrieval. In contrast, Claude 3.7 Sonnet (Figure 13d) maintains strong performance up to 3 hops but shows spike in early irrelevance (45%) at 4 hops. Even advanced models, therefore, encounter depth calibration issues, struggling to determine when to stop in extended reasoning chains."
        },
        {
            "title": "6 Automated Evaluation Results",
            "content": "Figure 6: Two-Step LLM-Assisted Evaluation Workflow. high-level overview of the two-step decomposition that improves annotation accuracy and consistency for complex multi-hop reasoning tasks. While extensive manual evaluations provide detailed and reliable insights into model reasoning behaviors, it is difficult to scale, particularly for complex queries from datasets like MuSiQue, where each annotation can take approximately four minutes per data point. Hence, We develop framework for automating the annotation process to significantly improve evaluation efficiency. 6.1 Evaluation Workflow We employed an LLM-as-a-Judge framework to automate the annotation task, using gpt-4.1-mini1 as our judging model. Utilizing LLM as judge for annotating reasoning failures helped us scale the process significantly and reduced evaluation time, achieving approximately 20x increase in efficiency compared to manual annotation. To ensure parity with manual annotation process and high fidelity analysis, we provided the Judge LLM with the same detailed annotation guidelines used by human annotators, but prompt-engineered the guidelines with explicit formatting instructions and clear definitions. The Judge LLM had access to question, relevant context documents, and the final response from the reasoning models. Consistent with findings for multi-step judging process in (Wang et al., 2023; Zheng et al., 2024), we adopt two-step annotation process as illustrated in Figure 6: 1. Hop Breakdown: First, the Judge LLM identifies and annotates the reasoning hops present in the models response. 2. Reasoning Classification: Next, the Judge uses these annotated hops to accurately categorize the response into one of our predefined error categories. This decomposition significantly improved annotation accuracy and consistency, aligning with findings from recent literature indicating that multi-step judging processes enhance reliability and accuracy in complex evaluation tasks (Wang et al., 2022)."
        },
        {
            "title": "Annotations",
            "content": "To further validate our LLM-as-a-Judge pipeline, we evaluate the consistency of annotations across six models and three datasets: MUSIQUE, 2WIKI, and HOTPOTQA. Based on the results presented in Table 3, our LLM-as-a-Judge framework demonstrates promising potential for automating error categorization tasks traditionally performed by human annotators. Across all models and datasets, agreement rates vary, indicating model-specific and dataset-specific challenges. For example, models like DeepSeek-R1 and LLaMA 70B exhibit notably higher agreement rates, particularly on simpler 1A state-of-the-art model that is different from the models analyzed in this work. datasets like 2WIKI, achieving above 90%. Conversely, the more challenging MUSIQUE dataset consistently shows lower agreement scores, underscoring inherent complexities and subtle reasoning errors that the Judge LLM struggles to replicate accurately. These results imply that while LLM-as-a-Judge systems are highly effective at automating error categorization for straightforward multi-hop reasoning tasks, complexities in certain datasets highlight the continuing necessity for human judgment or advanced refinement of Judge LLM instructions. The observed variability underscores that further investigation is essential to understand and mitigate factors contributing to lower Judge-model agreement rates, such as nuanced reasoning steps or subtle misinterpretations. Nonetheless, the substantial reduction in annotation time and generally high fidelity in simpler contexts strongly support the viability and efficiency of integrating LLM-as-a-Judge frameworks into broader NLP evaluation pipelines. Table 3: LLM-as-a-Judge Agreement (%) with Human Annotations Across Models and Datasets"
        },
        {
            "title": "Model",
            "content": "HotpotQA 2Wiki MuSiQue LLaMA 8B DeepSeek-R1 LLaMA 70B Qwen 7B Claude 3.7 Qwen 14B 65.3 76.0 72.0 66.6 73.3 65.3 73.5 91.1 92.6 75.0 91.1 88.2 53.3 62.6 75.0 46.6 76.6 78."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduce hop-based diagnostic framework for multi-hop QA that captures reasoning fidelity through fine-grained error categories and metamarkers for coverage and overthinking. Analysis of six LMs across three datasets reveals high fidelity in simple settings but persistent overhopping, misinterpretation, and synthesis failures in complex and distractor-rich tasks. Our two-step LLM-as-aJudge method achieves up to 92% agreement with humans on simpler datasets while cutting evaluation time by 20x, although challenges remain for nuanced reasoning. These findings call for evaluation and training strategies that bridge the gap between correct answers and reasoning that is both efficient and faithful for truly reliable multi-hop QA systems."
        },
        {
            "title": "References",
            "content": "Chirag Agarwal, Sree Harsha Tanneru, and Himabindu Lakkaraju. 2024. Faithfulness vs. plausibility: On the (un)reliability of explanations from large language models. Preprint, arXiv:2402.04614. Guangsheng Bao, Hongbo Zhang, Cunxiang Wang, Linyi Yang, and Yue Zhang. 2024. How likely do llms with cot mimic human reasoning? Preprint, arXiv:2402.16048. Mansi Sakarvadia. 2024. Towards interpreting language models: case study in multi-hop reasoning. Preprint, arXiv:2411.05037. Parshin Shojaee, Iman Mirzadeh, Keivan Alizadeh, Maxwell Horton, Samy Bengio, and Mehrdad Farajtabar. 2025. The illusion of thinking: Understanding the strengths and limitations of reasoning models via the lens of problem complexity. Preprint, arXiv:2506.06941. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. Preprint, arXiv:2011.01060. Yongqi Tong, Dawei Li, Sizhe Wang, Yujia Wang, Fei Teng, and Jingbo Shang. 2024. Can llms learn from previous mistakes? investigating llms errors to boost for reasoning. Preprint, arXiv:2403.20046. Ai Ishii, Naoya Inoue, Hisami Suzuki, and Satoshi Sekine. 2024. Analysis of LLMs spurious correct answers using evidence information of multihop QA datasets. In Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024), pages 2434, Bangkok, Thailand. Association for Computational Linguistics. Ryo Kamoi, Sarkar Snigdha Sarathi Das, Renze Lou, Jihyun Janice Ahn, Yilun Zhao, Xiaoxin Lu, Nan Zhang, Yusen Zhang, Ranran Haoran Zhang, Sujeeth Reddy Vummanthala, Salika Dave, Shaobo Qin, Arman Cohan, Wenpeng Yin, and Rui Zhang. 2024. Evaluating llms at detecting errors in llm responses. Preprint, arXiv:2404.03602. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukošiute, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, and 11 others. 2023. Measuring faithfulness in chain-of-thought reasoning. Preprint, arXiv:2307.13702. Xiaoyuan Li, Wenjie Wang, Moxin Li, Junrong Guo, Yang Zhang, and Fuli Feng. 2024. Evaluating mathematical reasoning of large language models: focus on error identification and correction. Preprint, arXiv:2406.00755. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, and 1 others. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437. Hamed Mahdavi, Alireza Hashemi, Majid Daliri, Pegah Mohammadipour, Alireza Farhadi, Samira Malek, Yekta Yazdanifard, Amir Khasahmadi, and Vasant Honavar. 2025. Brains vs. bytes: Evaluating llm proficiency in olympiad mathematics. Preprint, arXiv:2504.01995. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:533550. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. Preprint, arXiv:2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. Preprint, arXiv:2201.11903. Jian Wu, Linyi Yang, Zhen Wang, Manabu Okumura, and Yue Zhang. 2025. CofCA: STEP-WISE counterfactual multi-hop QA benchmark. In The Thirteenth International Conference on Learning Representations. Yibo Yan, Shen Wang, Jiahao Huo, Hang Li, Boyan Li, Jiamin Su, Xiong Gao, Yi-Fan Zhang, Tianlong Xu, Zhendong Chu, Aoxiao Zhong, Kun Wang, Hui Xiong, Philip S. Yu, Xuming Hu, and Qingsong Wen. 2024. Errorradar: Benchmarking complex mathematical reasoning of multimodal large language models via error detection. Preprint, arXiv:2410.04509. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. CoRR. Sagnik Mukherjee, Abhinav Chinta, Takyoung Kim, Tarun Anoop Sharma, and Dilek Hakkani-Tür. 2025. Premise-augmented reasoning chains improve error identification in math reasoning with llms. Preprint, arXiv:2502.02362. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2024. Processbench: Identifying process errors in mathematical reasoning. Preprint, arXiv:2412.06559. Question Type Bridge Entity / Reasoning Example Compositional: Requires chaining intermediate entities. Bridge: Versus Doc 1: Versus (Versace) is the diffusion line of Italian..., gift by the founder Gianni Versace. Doc 2: Gianni Versace was shot and killed outside... Question: Why did the founder of Versus die? Inference: Demands implicit reasoning via unstated bridge facts Comparison: Involves comparing attributes across entities Bridge Comparison: Combines inference followed by comparison Bridge: Grandchild Doc 1: Dambar Shah was the father of Krishna Shah... Doc 2: Krishna Shah was the father of Rudra Shah... Question: Who is the grandchild of Dambar Shah? Compare: Age of Persons Doc 1: Theodor Haecker (18791945) was a... Doc 2: Harry Vaughan Watkins (18751945) was a... Question: Who lived longer, Theodor Haecker or Harry Vaughan Watkins? Bridge: Directors Nationality Doc 1: FAQ: Frequently Asked Questions directed by Carlos Atanes... Doc 2: The Big Money directed by John Paddy Carstairs... Doc 3: Carlos Atanes is Spanish film director. Doc 4: John Paddy Carstairs was British film director. Question: Are both directors of FAQ: Frequently Asked Questions and The Big Money from the same country? Table 4: Examples of Different Question Types with Highlighted Entities."
        },
        {
            "title": "A Appendix A",
            "content": "Stage 1 Categories (Coarse Taxonomy) 1. Effective reasoning: The model performs all required reasoning steps and correctly answers the question. The explanation is concise, coherent, and logically complete. 2. Underthinking: The model provides insufficient reasoning, skipping essential steps or offering vague justifications. The response may appear shallow or overly brief, regardless of answer correctness. 3. Overthinking: The model introduces excessive or tangential reasoning, often by exploring irrelevant paths or repeating information. This may include unnecessary document traversal or redundant entity comparisons. 4. Faulty reasoning: The reasoning chain is logically flawed or factually incorrect. This may involve wrong inference, unsupported claims, or internal contradictions, even if the structure appears complete. Stage 2 Categories (Structured Taxonomy) Let Nmodel denote the number of reasoning steps predicted by the model, and Ngold denote the number of hops required according to the gold standard. 1. Category 1: Nmodel = Ngold; all hops correct; final answer correct The model follows the required inference path, makes all correct hops, and provides the correct final answer. 2. Category 2: Nmodel = Ngold; all hops correct; final answer incorrect The reasoning path is structurally correct, but the final answer is wrong due to errors in aggregation or conclusion. 3. Category 3: Nmodel = Ngold; one or more hops incorrect/hallucinated The hop count matches, but one or more steps are logically or factually incorrect. The final answer may or may not be correct. 4. Category 4: Nmodel < Ngold; all predicted hops correct; final answer incorrect The model correctly predicts subset of the required hops but misses key steps, leading to an incorrect answer. 5. Category 5: Nmodel < Ngold; all predicted hops correct; final answer correct (shortcut) The model answers correctly using valid but incomplete subset of required reasoning hops. shortcut was taken. 6. Category 6: Nmodel < Ngold; one or more hops incorrect/hallucinated The model generates fewer hops than required, with some being inaccurate or irrelevant. The chain is both incomplete and partially flawed. 7. Category 7: Nmodel > Ngold; irrelevant hops after gold path (trailing overthinking) After attempting the required reasoning, the model continues with superfluous or irrelevant steps, leading to overgeneration. 8. Category 8: Nmodel > Ngold; irrelevant or hallucinated hops before/interleaved Irrelevant hops occur early in the reasoning process or are interleaved with required steps, disrupting logical progression. 9. Category 9: Nmodel = 0 No reasoning path is shown; the model outputs an answer directly without generating any hops. 10. Category 10: Question misinterpretation The reasoning chain reflects misunderstanding of the question, regardless of hop count or structural form."
        },
        {
            "title": "Dataset Details",
            "content": "A.1 Setup 2WikiMultiHopQA (Ho et al., 2020): Includes 10 Wikipedia paragraphs per question, retrieved from structured and unstructured sources. Each instance provides gold reasoning paths and supporting facts to ensure multihop inference. HotpotQA (Yang et al., 2018): Each question is paired with two gold documents and eight distractors (10 in total). Designed to test both answer accuracy and reasoning transparency, including bridge and comparison questions with annotated supporting sentences. MuSiQue (Trivedi et al., 2022): Presents 20document contexts per question. Constructed to reduce shortcut-based reasoning by enforcing sub-question dependencies and including challenging unanswerable distractors."
        },
        {
            "title": "B Additional Figures",
            "content": "Dataset Question Types #Hops Difficulty HotpotQA 2WikiMultiHopQA MuSiQue Composition, Comparison Composition, Comparison, Inference, Bridge Comparison Composition, Inference 2 2, 4 EasyMedium MediumHard 2, 3, 4 Hard Table 5: Comparison of datasets in terms of question types, hop complexity, and difficulty level. Figure 7: Example of Input given to annotators Figure 8: Example of Output labeled by the annotators (a) 2Wiki (b) HotpotQA Figure 9: Distribution of reasoning error types across datasets. (a) 2WIKI, (b) HOTPOTQA, (c) MUSIQUE. (c) MuSiQue (a) 2Wiki (b) HotpotQA Figure 10: Answer correctness breakdown by reasoning category across datasets. (a) 2WIKI, (b) HOTPOTQA, (c) MUSIQUE. (c) MuSiQue (a) MuSiQue (b) HotpotQA Figure 11: Overthinking Trends with Answer Incorrectness across Datasets. (a) MuSiQue, (b) HotpotQA, and (c) 2Wiki. (c) 2Wiki (a) LLaMA-8B (Distill) (b) LLaMA-70B (Distill) (c) Qwen-7B (Distill) (d) Qwen-14B (Distill) (e) DeepSeek-R1 (f) Claude 3 Sonnet Figure 12: Distribution of reasoning error types across question types for six models. Each subfigure shows model-specific trends in how question type impacts reasoning errors. (a) Qwen-7B (Distill) (b) Qwen-14B (Distill) (c) DeepSeek-R (d) Claude 3 Sonnet Figure 13: Hop-wise distribution of reasoning errors on MuSiQue for four models. Subplots (a)(d) show how models vary in reasoning step correctness and overhopping behavior."
        }
    ],
    "affiliations": [
        "Microsoft",
        "University of Maryland, College Park",
        "University of Massachusetts, Amherst"
    ]
}