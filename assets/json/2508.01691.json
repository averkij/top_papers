{
    "paper_title": "Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and Regional Languages Around the Globe",
    "authors": [
        "Tiantian Feng",
        "Kevin Huang",
        "Anfeng Xu",
        "Xuan Shi",
        "Thanathai Lertpetchpun",
        "Jihwan Lee",
        "Yoonjeong Lee",
        "Dani Byrd",
        "Shrikanth Narayanan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Voxlect, a novel benchmark for modeling dialects and regional languages worldwide using speech foundation models. Specifically, we report comprehensive benchmark evaluations on dialects and regional language varieties in English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai, Spanish, French, German, Brazilian Portuguese, and Italian. Our study used over 2 million training utterances from 30 publicly available speech corpora that are provided with dialectal information. We evaluate the performance of several widely used speech foundation models in classifying speech dialects. We assess the robustness of the dialectal models under noisy conditions and present an error analysis that highlights modeling results aligned with geographic continuity. In addition to benchmarking dialect classification, we demonstrate several downstream applications enabled by Voxlect. Specifically, we show that Voxlect can be applied to augment existing speech recognition datasets with dialect information, enabling a more detailed analysis of ASR performance across dialectal variations. Voxlect is also used as a tool to evaluate the performance of speech generation systems. Voxlect is publicly available with the license of the RAIL family at: https://github.com/tiantiaf0627/voxlect."
        },
        {
            "title": "Start",
            "content": "Voxlect: Speech Foundation Model Benchmark for Modeling Dialects and Regional Languages Around the Globe Tiantian Feng1, Kevin Huang1, Anfeng Xu1, Xuan Shi1, Thanathai Lertpetchpun1, Jihwan Lee1, Yoonjeong Lee1, Dani Byrd1, Shrikanth Narayanan1 5 2 0 2 3 ] . [ 1 1 9 6 1 0 . 8 0 5 2 : r Abstract. We present Voxlect, novel benchmark for modeling dialects and regional languages worldwide using speech foundation models. Specifically, we report comprehensive benchmark evaluations on dialects and regional language varieties in English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai, Spanish, French, German, Brazilian Portuguese, and Italian. Our study used over 2 million training utterances from 30 publicly available speech corpora that are provided with dialectal information. We evaluate the performance of several widely used speech foundation models in classifying speech dialects. We assess the robustness of the dialectal models under noisy conditions and present an error analysis that highlights modeling results aligned with geographic continuity. In addition to benchmarking dialect classification, we demonstrate several downstream applications enabled by Voxlect. Specifically, we show that Voxlect can be applied to augment existing speech recognition datasets with dialect information, enabling more detailed analysis of ASR performance across dialectal variations. Voxlect is also used as tool to evaluate the performance of speech generation systems. Voxlect is publicly available with the license of the RAIL family at: https://github.com/tiantiaf0627/voxlect. Keywords: Speech Learning, Automatic Speech Recognition, Deep Learning, Dialect Classification, Speech"
        },
        {
            "title": "Foundation Model",
            "content": "1."
        },
        {
            "title": "Introduction",
            "content": "A dialect is defined as variety of language spoken by particular regional and/or social group. Specifically, dialects often differ from their standard language in terms of pronunciation, grammar, and vocabulary, whereas the more commonly used term accent typically refers to differences in pronunciation and prosody (intonation, rhythm, phrasing). In this paper, we primarily focus on dialect classification. For example, the English language has multiple dialects worldwide, including American, British, Singaporean, and Indian English, each with its distinct linguistic features. common example is the word for mechanical lift, where elevator is used in American English, while lift is preferred in British English. Similarly, Mandarin Chinese has many regional dialects across China, such as the Beijing and the Sichuan dialects. For example, while Beijing Mandarin > úù] in the retroflex place of articulation, the Sichuan dialects articulates the retroflex consonants such as zh [ > typically merge them with their alveolar counterparts, pronouncing them as [ ts]. Apart from dialectal variation within single language, some countries, such as India, have wide range of regional languages (as well as their dialectal varieties), with neighboring states speaking distinct but culturally connected languages, such as Tamil, Telugu, and Malayalam. Classifying dialects (as well as regional languages) is important for building robust speech technologies that accommodate diverse linguistic contexts. However, automatic speech recognition (ASR) systems often exhibit substantial disparities across dialectal varieties of the same language. For example, while many ASR systems perform well on speech samples of widely explored varieties, their accuracy drops significantly for under-represented dialects such as African American Vernacular English and Chicano English, reflecting biases in their training corpora [1, 2]. Similarly, KeSpeech [3] demonstrates that the state-of-the-art ASR systems show significantly lower performance on eight Mandarin dialectal varieties compared to Standard Mandarin. Such performance differences can reduce the reliability and usability of the systems and their applications, such 1 University of Southern California, Los Angeles, CA, USA (corresponding emails: tiantiaf@usc.edu. 1 Tiantian Feng et al. Voxlect: Benchmark for Modeling Dialects and Regional Languages Table 1. Comparison of Voxlect with existing literature in modeling speaker dialects or regional languages. Dataset/Study"
        },
        {
            "title": "Speaker Dialects Covered",
            "content": "GLOBE [10] ParaSpeechCaps [11] Vox-Profile [12] AIShell-3 [13] KeSpeech [3] ADI [14]"
        },
        {
            "title": "Mandarin",
            "content": "Global English varieties Country-level English varieties Global English varieties"
        },
        {
            "title": "Arabic varieties",
            "content": "CommonAccent [9] Multilingual Voxlect (Ours)"
        },
        {
            "title": "Multilingual",
            "content": "Global English varieties, Germany, Italian, Spanish Global English varieties and 10 non-English dialects (Arabic, Mandarin and Cantonese, Tibetan, Spanish, German, French, Italian Thai, Indic, Brazilian Portuguese as virtual assistants for speakers of under-resourced dialects. By explicitly modeling and recognizing varying dialects, we can not only have better understanding of the limitations of current speech technologies but also advance the development of more reliable and robust language technologies. The current literature on modeling speaker dialects has largely focused on English varieties. For example, the Edinburgh International Accents of English Corpus (EdAcc) [4] includes English speech from participants with variety of first language (L1) backgrounds, such as L1-Indian languages. Moreover, the British Isles Speaker Corpus[5] provides high-quality audio recordings of English utterances from speakers across the British Isles, such as Scotland, Wales, Northern Ireland, and Ireland. In contrast, research on modeling speaker dialects in non-English languages remains relatively limited. Much of the existing work in this area focuses instead on broader language identification (LID) tasks [6, 7]. One notable effort in this field is CommonVoice [8], large-scale multilingual speech dataset that includes self-reported speaker dialect labels. Building on this, CommonAccent [9] presents one of the few benchmarking efforts for speaker dialect classification in three nonEnglish languages (German, Spanish, & Italian). Nonetheless, its language coverage is still limited, excluding other widely spoken language families such as Chinese and Arabic. In this paper, we present Voxlect, one of the first benchmarks for classifying dialects and regional languages from multilingual speech data. Our proposed Voxlect benchmark show unique contributions compared to prior works: (1) Unlike previous studies that focus on limited set of dialects, Voxlect enables dialect and regional language classification across an extensive list of languages, including English, Mandarin, Indic Languages, Spanish, German, Italian, French, Brazilian Portuguese, Tibetan, and Arabic. (2) To address inconsistencies in dialect labeling in different datasets, Voxlect maps dialect labels into unified taxonomy for each language, enabling more consistent cross-corpus analysis. (3) We show the broad utility of Voxlect through two applications in ASR performance evaluation and speech generation system assessment, showing that dialect-level distinctions matter in real-world systems. Our extensive experiments confirm that Voxlect yields reliable estimates of speaker dialects across languages spoken worldwide, which presents unique opportunities for data mining, modeling, and knowledge discovery in dialectal speech data. 2. Related Work Table 1 summarizes key prior works on speaker dialect modeling. We categorize the existing literature based on whether it focuses on English or non-English languages."
        },
        {
            "title": "2.1 Modeling English Dialects",
            "content": "We summarize several representative works in modeling English Dialects in Table 1. In particular, CommonAccent [9] introduced dialect classification benchmark using samples from CommonVoice-en [8]. It reported strong performance in recognizing English accents, such as American, Canadian, and British English, using the wav2vec-xlsr model. GLOBE [10] is similar effort that develops classifiers using HuBERT pre-trained models on CommonVoice-en to predict similar dialect labels. In addition to directly modeling English-speaking dialects, ParaSpeechCaps [11] uses language models to process Wikipedia pages to augment the country-level dialect information associated with each celebrity speaker. Finally, our prior benchmark, Vox-Profile [12], 2 Tiantian Feng et al. Voxlect: Benchmark for Modeling Dialects and Regional Languages explores speaker dialect classification by unifying English dialect labels from more than ten datasets. This benchmark provides high-performing English-speaking dialect classification models using Whisper Families [7] and WavLM [15], enabling robust English dialect recognition across diverse speaker samples."
        },
        {
            "title": "2.2 Modeling Non-English Dialects",
            "content": "Table 1 presents related works on modeling speaker dialects in non-English languages. There has been growing interest in dialect modeling for Mandarin. For example, AIShell-3 [13] provides over 80 hours of multispeaker Mandarin speech, annotated with regional accents. More recently, KeSpeech [3] introduced large-scale dataset covering eight major Mandarin subdialects, comprising speech recordings from 27,237 speakers, with total duration of 1,542 hours. For Arabic, Sullivan et al. [14] conducted experiments for Arabic dialect classification, exploring model performance across both five major dialect groups and more fine-grained set of 17 specific dialect labels. In addition to modeling speaker dialects within single language, CommonAccent [9] reports experiments modeling speaker dialects in several languages, including German, Spanish, and Italian. Compared to these prior efforts, our proposed Voxlect benchmark supports dialect classification across more extensive list of spoken languages and dialects, creating opportunities to develop robust and reliable speech technologies that accommodate different linguistic backgrounds. 3."
        },
        {
            "title": "Voxlect Benchmark",
            "content": "In this section, we introduce the design of the Voxlect Benchmark. We begin by describing the dialect labels used for each classification. We then outline the speech foundation models used in our experiments. Overall, Figure 11 presents the overview of the Voxlect Benchmark that uses speech foundation models to classify speaker dialects in languages such as Mandarin, German, and Arabic. Figure 1. Overview of dialect classification in the Voxlect benchmark. The figure plots the geographic distribution of dialects across several language families, including major Indic languages in Southern India and dialects of Mandarin, Arabic, Thai, Spanish, Brazilian Portuguese, and German. Detailed dialect labels and corresponding datasets are provided in Table 2. (The contours shown in the drawing are only schematic, and precise boundaries of dialects are specified in Table 2.) 1Disclaimer: The map visualizations in this work are only for illustration purposes for presenting the broad geographic distribution of dialects. The highlighted parts only correspond to those dialects that may be present in experiments. We acknowledge that single highlighted area may include other dialects or languages. The maps are based on administrative boundary data from GADM-4.1 [16]. The authors make no claims regarding the accuracy, completeness, or any interpretations of the geographic data provided, and are not responsible for any implications arising from its use. 3 Tiantian Feng et al. Voxlect: Benchmark for Modeling Dialects and Regional Languages Table 2. The labels of speaker dialect in Voxlect benchmark. The green, blue, and violet colors indicate speaker dialects from North America, the British Isles, and other regions or language backgrounds, respectively. For Indic languages, the color green indicates languages of Hindi, Urdu, and English, which are spoken in many regions across India. Moreover, the green and blue indicate Spanish dialects from Europe and Latin America. Similarly, green indicates French and German dialects spoken within France and Germany, while blue represents dialects spoken outside these two countries."
        },
        {
            "title": "Datasets",
            "content": "#Train Utterances"
        },
        {
            "title": "English",
            "content": "North America, English, Welsh Scottish, Northern Irish, Germanic Irish, Germanic, Romance, Slavic, Semitic, Oceania South Africa, Southeast Asia East Asia, South Asia, Other CommonVoice-en [8]; CSLU-FAE [17]; EdAcc [4]; British Isles [5]; L2-ARCTIC [18]; TIMIT [19]; VoxPopuli [20]; ESLTTS [21]; Fair-Speech [22]; ParaSpeechCaps [11] Nigerian-English [23]; Hispanic-English [24];"
        },
        {
            "title": "Arabic",
            "content": "Egyptian, Levantine, Maghrebi, Peninsular Modern Standard Arabic (MSA) MASC [25]; SADA [26]; Dvoice [27]"
        },
        {
            "title": "Mandarin\nand Cantonese",
            "content": "Standard/Beijing/Northeastern Mandarin Ji-Lu Mandarin, Southwestern Mandarin, Jiang-Huai Mandarin, Lan-Yin Mandarin, Zhongyuan Mandarin, Jiao-Liao Mandarin, Cantonese KeSpeech [3]; CommonVoice-yue [8]; CommonVoice-hk [8]"
        },
        {
            "title": "Chinese Tibetan",
            "content": "U-Tsang, Kham, Amdo TIBMD [28] Indic Languages & Indian English Hindi, Urdu, English, Punjabi, Dogri Kashmiri, Sanskrit, Assamese, Manipuri Bengali, Odia, Maithili, Santali, Gujarati Bodo, Marathi, Nepali, Konkani, Sindhi Tamil, Telugu, Kannada, Malayalam IndicVoices [29]; CommonVoice-en [8]"
        },
        {
            "title": "Thai",
            "content": "Khummuang, Korat, Pattani, Thai-central Thai-Dialect-Corpus [30]"
        },
        {
            "title": "German",
            "content": "Penisular, Mexican, Chileno, Andino-Pacífico Central America and the Caribbean, Rioplatense CommonVoice-sp [8]; Latin American Spanish [31] France, Switzerland/Belgium/Germany Africa, Canada CommonVoice-fr [8]; African Accented French German-Non-NRW Area, German-NRW Area, Switzerland, Austria, Other"
        },
        {
            "title": "Italian",
            "content": "Central, Northern, Southern CommonVoice-de [8] CommonVoice-it [8]; ITALIC [32]"
        },
        {
            "title": "Brazilian\nPortuguese",
            "content": "Minas Gerais, Recife, São Paulo CORAA [33] 247,098 341,154 544,867 29, 247,302 302,087 123,102 56,280 71,158 14, 26,"
        },
        {
            "title": "3.1 Labeling Dialects and Regional Languages",
            "content": "We note that some dialect labels (e.g., English) have varied usage across existing datasets, highlighting the need for standardized dialect labeling scheme within each language. This standardization process allows us to combine different available datasets for training reliable dialect classification models. For most non-English cases, dialect labels are defined based on the available dialectal information in the relevant datasets. Specifically, we propose knowledge-driven dialect categorization for English, Spanish, and French, while adopting existing conventions to group available dialectal data from established datasets for the remaining dialects and regional languages. We summarize the dialect labels and associated experimental datasets in Table 2. English Existing work on modeling English dialects often comes with inconsistent labeling schemes [10, 9, 34]. To address this issue, we adopt the dialect taxonomy proposed in our previous Vox-Profile benchmark [12]. In particular, we first categorize two broad regional categories: North America and the British Isles. Within the British Isles, we further distinguish varieties as English (England), Scottish, Northern Irish, Welsh, and Irish. For regions and language backgrounds outside these two, we first define dialects from Oceania and South Africa as two representative English-speaking regions. Next, following common geographic conventions, we categorize English dialects spoken in Asia into three major regions: East Asia, South Asia, and Southeast Asia. These Asian regions contain significant population of native speakers and dialects of English, such as Indian English and Singaporean English. On top of that, we categorize English accent groups based on their first language (L1) influence, such as Germanic (e.g., German), Slavic (e.g., Russian), Romance (e.g., Spanish, French), and Semitic (e.g., Arabic, Hebrew) language backgrounds. L1 backgrounds with limited data samples in existing datasets are grouped into miscellaneous general category labeled Other. For instance, this includes dialects spoken by individuals with Uralic language backgrounds (e.g., Finnish) or from African 4 Tiantian Feng et al. Voxlect: Benchmark for Modeling Dialects and Regional Languages regions outside South Africa. Compared to Vox-Profile [12], Voxlect expands the benchmark by integrating the ParaSpeechCaps [11] dataset, adding approximately 90,627 additional speech utterances in conversational contexts to the English dialect classification tasks. Arabic Arabic dialect classification has been well-established in the literature [25, 26, 14]. However, we were unable to obtain the dataset used for the fine-grained 17-dialect classification in [14]. Therefore, we follow conventions in datasets MASC [25] and SADA [26] and categorize Arabic into five major groups: Egyptian, Levantine (e.g., Lebanon), Peninsular (e.g., Saudi Arabia), Maghrebi (e.g., Morocco), and Modern Standard Arabic (MSA). Mandarin and Cantonese The classification of Mandarin dialects has been deeply explored within Chinese linguistics. We follow the Mandarin dialect labels listed by KeSpeech [3]. Given that there are limited speech samples for Beijing Mandarin and Northeastern Mandarin in KeSpeech and that these two dialects share mutual intelligibility with Standard Mandarin, we group these three varieties into single category labeled simply Mandarin. However, we highlight that Beijing Mandarin and Northeastern Mandarin do differ from Standard Mandarin in certain lexical items and pronunciations. In addition to classifying Mandarin dialects, we add Cantonese samples from CommonVoice-yue [8] and CommonVoice-hk [8] to enrich the coverage of Chinese languages. Chinese Tibetan Tibetan is language spoken across parts of China, India, Nepal, and Bhutan. Since there are limited datasets of Tibetan dialects, we classify three major dialects of Greater Tibet in China as presented in TIBMD [28]: Ü-Tsang, Kham, and Amdo. Indic Languages and Indian English Many languages, from number of language families, are spoken in India, and each is typically considered separate language rather than dialect. In this paper, we consider 22 major Indic languages (shown in Table 2) as listed in IndicVoices [29]. Moreover, given the widespread use of English across India and its status as an official language of the country, we specifically include Indian English (labels associated with India in CommonVoice-en) as separate language category to reflect its presence in the multilingual context. Thai We use the dialect labels described in the Thai-Dialect-Corpus [30], one of the most notable efforts to document dialectal variation among Thai speakers. Here, we consider four major dialects: Thai-central, the standard Thai mainly spoken by Central Thai but also across Thailand (e.g., Bangkok); Khammuang, spoken in northern Thailand (e.g., Chiang Mai); Korat, dialect influenced by both Central Thai and Isan (Northeastern Thai); and Pattani, Malay-influenced dialect (Southern Thai). Spanish We categorize Spanish dialects into two broad groups based on geographic landscapes guided by [35]: Peninsular Spanish (spoken in Europe) and Latin American Spanish. Moreover, we break down Latin American Spanish into five major dialectal groups: Mexican, Central America and the Caribbean (e.g., Costa Rica), Chileno (e.g., Chile), Andino-Pacífico (e.g., Peru), and Rioplatense (e.g., Argentina). French French is spoken by geographically diverse population worldwide. In addition to its use in France, there is notable French-speaking population in its neighboring European countries such as parts of Switzerland, Belgium, and Germany. And apart from Europe, French is widely spoken in regions of Africa and Canada (e.g., Quebec). Therefore, we define four French dialect categorizations: France (standard French), the neighboring countries of France (French varieties spoken in Switzerland, Belgium, and Germany), Africa, and Canada. That said, we choose not to refine dialect labels for speakers from Frances neighboring countries due to the limited availability of speech samples. German Akin to our approach to categorizing French dialects, we group German dialects into varieties spoken within Germany and those in other countries with German-speaking populations, adopting the label categories from CommonVoice-de [8]. Having German-North Rhine-Westphalia (NRW) as separate group owing to its large number of utterances in the dataset, the German dialects are grouped into the following five categories: German-NRW (western Germany), German (non-NRW area), Swiss, Austrian, and the other German-speaking populations. Italian Given the limited speech datasets presented for Italian dialects and limited speech samples available for individual cities, we create common regional taxonomy and group Italian dialects into three major categories: Northern Italian (e.g., Venetian), Central Italian (e.g., Tuscan), and Southern Italian (e.g., Sicilian). 5 Tiantian Feng et al. Voxlect: Benchmark for Modeling Dialects and Regional Languages Brazilian Portuguese There are limited European Portuguese speech datasets with academically friendly licenses, and thus we are unable to include them. In contrast, we rely on CORAA [33] for studying dialects in Brazilian Portuguese. Based on this dataset, we categorize Brazilian Portuguese into three representative regional groups: São Paulo, Minas Gerais, and Recife. Figure 2. Overview of speaker dialect classification architecture in the Voxlect benchmark."
        },
        {
            "title": "3.2 Speech Foundation Models",
            "content": "In Voxlect, we evaluate several widely studied speech foundation models, including the Massively Multilingual Speech (MMS) [6], WavLM [36], and Whisper [7] family. Among these, MMS and Whisper are multilingual models trained on large-scale cross-lingual datasets, while WavLM is trained only on English. Given the generalizability of these speech foundation models, existing works [37, 12] have shown that direct fine-tuning of hidden outputs from the encoder layers can achieve strong performance in wide range of downstream speech modeling tasks. We present our modeling architecture in Figure 2. In our implementation, we first compute weighted average of the hidden states across all encoder layers, including both convolutional and transformer layers. The aggregated output is then processed through 1D-pointwise convolutional layers. Finally, we average the convolutional outputs to obtain the final embeddings, which are passed through fully connected layers for classification. To further improve the classification performance, we integrate LoRa [38] into all fine-tuning experiments as an effective approach for adapting speech foundation models. 4. Experiments"
        },
        {
            "title": "4.1 Datasets",
            "content": "We sampled 30 publicly available data sources to conduct benchmark experiments with total of over 2 million speech samples from the 11 language groups detailed above. For all datasets used in dialect classification experiments, we resample audio to 16 kHz to match the sampling rate of speech foundation models. Audio clips shorter than 3 seconds are excluded, as such short utterances are insufficient for robust dialect classification. To manage computational constraints during training, all samples are truncated to maximum duration of 15 seconds. For English dialect classification, we discard samples labeled as British, as this lacks specificity on regional varieties such as Scottish. In Spanish dialect experiments, we exclude Colombian speakers from the Latin American Spanish dataset [31], given the dialectal overlap between the Caribbean and Andino-Pacífico varieties. Due to the large size of the IndicVoice [29] dataset, we subsample maximum of 10 utterances per speaker. The detailed distribution of dialects or regional languages in each language group is in the Appendix."
        },
        {
            "title": "4.2 Experimental Details",
            "content": "All experiments were conducted using fixed random seed to ensure reproducibility. During training, we applied several data augmentations to the input waveforms: the Gaussian noise was added with probability of 1.0, using an SNR range of 330 dB; time masking was applied with probability of 1.0, using masking ratio between 10% and 15%; time stretching was used with probability of 1.0, with stretch rates ranging from 0.9 to 1.1; and polarity inversion was applied with probability of 0.5. We use learning rate of [0.0001, 0.0005] and training epoch of 15. We perform the training for 5 epochs on Thai and Arabic dialect classification 6 Tiantian Feng et al. Voxlect: Benchmark for Modeling Dialects and Regional Languages Table 3. Comparison of different speech foundation models in classifying speaker dialects. Overall, the results show that multilingual models including Whisper-Large and MMS-LID-256 achieve the overall best performance, while the speech foundation model pre-trained only with English data (WavLM+) shows relatively lower performance in dialect prediction. Bold and underlines indicate the best and the second best performance, respectively. English Arabic Acc F1 Acc F1 Mandarin and Cantonese Acc Indic Lang and Indian English Acc F1 Tibetan Thai Acc Acc F1 79.4 65.2 80.4 67.2 78.3 83.0 0.705 0.508 0.714 0.545 0.688 0. 79.2 84.9 91.8 88.1 93.0 94.2 0.681 0.784 0.860 0.813 0.892 0.923 79.5 78.5 82.9 77.4 81.7 82. 0.655 0.663 0.708 0.643 0.712 0.702 63.8 67.6 82.6 64.6 75.0 77.1 0.634 0.688 0.795 0.642 0.748 0. 80.6 79.4 86.4 79.6 78.5 82.0 Spanish French Germany Italian Acc F1 Acc F1 Acc Acc F1 0.853 0.851 0.935 0.896 0.931 0.943 0.627 0.721 0.783 0.656 0.620 0. 91.0 90.4 95.9 93.2 95.4 96.3 Brazilian Portuguese F1 Acc 66.2 58.1 77.4 62.5 64.3 77.8 0.662 0.568 0. 0.630 0.650 0.789 59.8 83.3 86.4 72.7 83.1 87.0 0.544 0.665 0.706 0.520 0.667 0.712 87.6 89.8 96. 78.9 82.5 93.6 0.769 0.737 0.906 0.691 0.753 0.875 64.0 56.3 76.9 60.2 61.9 73.9 0.671 0.604 0. 0.614 0.622 0.745 97.0 95.1 99.1 86.8 94.6 98.6 0.966 0.942 0.990 0.819 0.920 0.980 Self-Supervised WavLM+ MMS-300M MMS-LID 256 Whisper Family Whisper Tiny Whisper Small Whisper Large Self-Supervised WavLM+ MMS-300M MMS-LID 256 Whisper Family Whisper Tiny Whisper Small Whisper Large due to faster convergence rate. Our experiments indicate that models perform better with learning rate of 0.0005. The pre-trained model weights are downloaded from Huggingface. Moreover, we freeze the pre-trained weights in all experiments, while we apply LoRa with rank size of 64 to the feedforward layer as suggested by our previous works [39, 12]. Specifically, we use batch size of 16 for training the Whisper and WavLM models, and reduce the batch size to 6 for MMS-LID-256 due to its larger parameter size. All pre-trained model checkpoints are downloaded from Huggingface. For evaluation, we report utterance-level Macro-F1 and accuracy on the test set for each language group. The default test set from each data source is used as the evaluation set; otherwise, we randomly select 20% of speakers as the test set. 5. Benchmark Results"
        },
        {
            "title": "5.1 Dialect Classification Results",
            "content": "Table 3 presents comparative analysis of speech foundation models in classifying speaker dialects of 11 language groups. Overall, the models that are pre-trained with multilingual data, particularly Whisper-Large and MMS-LID 256, consistently achieve the best performance in most experiments. Specifically, WhisperLarge achieves the highest accuracy and Macro-F1 in 5 out of 11 language groups, including Arabic (MacroF1 = 0.923), Mandarin and Cantonese (Macro-F1 = 0.889), and Thai (Macro-F1 = 0.943). On the other hand, MMS-LID-256, multilingual model fine-tuned for LID, outperforms others in languages like Tibetan (Macro-F1 = 0.783), German (Macro-F1 = 0.906), and Brazilian Portuguese (Macro-F1 = 0.990). In contrast, WavLM+, which is pre-trained only on English data, performs relatively poorly compared to multilingual models, especially across typologically distant languages such as Indic (Macro-F1 = 0.634) and Arabic (MacroF1 = 0.681). These results highlight the advantages of multilingual models in classifying dialectal variations across different linguistic contexts."
        },
        {
            "title": "5.2 Geographical Proximity and Classification",
            "content": "To better understand the performance of these dialect classification models, we visualize both the confusion matrix and the most significant misclassification patterns for two language groups, Spanish and Mandarin, in Figure 3. Given that Whisper-Large yields consistently better classification performance than most other models, we create the visualization based on Whisper-Large classification. Each map highlights the regions of major dialect groups, with arrows and percentages showing the most frequent misclassifications on the evaluation set. Overall, we identify consistent pattern observed in the strong influence of geographic proximity on classification errors. Specifically, dialects spoken in neighboring regions or states are more likely to be confused by the dialect classification model. For example, in the Mandarin group, the highest confusion occurs between Zhongyuan and Ji-Lu Mandarin (21.3%), while in the Spanish group, Caribe and Central dialects 7 Tiantian Feng et al. Voxlect: Benchmark for Modeling Dialects and Regional Languages Figure 3. Confusion matrices and geographic visualizations of dialect classification errors for Spanish and Mandarin. The maps highlight the most significant misclassification patterns, with arrows indicating frequently confused dialect pairs. Figure 4. Comparison of relative differences in dialect prediction performance between Whisper-Large and MMS-LID-256 under different noise levels. Each dot represents the performance change for single dialect classifier of language, and * indicates statistically significant differences (p<0.05). are often misclassified as Andino-Pacífico (16.2%). In contrast, dialects that are more geographically distant, such as Peninsular (European) Spanish versus Latin American varieties, or Cantonese versus Mandarin, are less frequently confused. The confusion matrix for each dialect classifier is presented in the Appendix. These findings suggest that geographical proximity among speakers of varieties of language often correlates with greater linguistic similarity. While this may present challenge for discrete dialect classification, it is consistent with the linguistic study [40, 41] of the evolution languages, which recognizes that dialects emerge through contact (and isolation) patterns between peoples over time."
        },
        {
            "title": "5.3 Robustness of Dialect Classification",
            "content": "We further investigate the robustness of dialect classification under varied noise levels and utterance lengths. The goal here is to assess how robust the fine-tuned models are to real-world scenarios, where acoustic conditions may be degraded and speaking durations vary. Robustness to Noise: We introduce the Gaussian noise at three signal-to-noise ratio (SNR) levels: 25dB, 15dB, and 5dB. We compare the relative performance changes of the two best-performing fine-tuned models (Whisper-Large and MMS-LID-256) against their performance on clean speech. The comparisons in Figure 4 show that Whisper-Large and MMS-LID-256 demonstrate similar robustness and relatively smaller performance degradation at moderate noise levels (SNR = 15 or 25 dB). However, under the high noise level (SNR = 5 dB), MMS-LID-256 shows significantly larger drop in dialect prediction performance compared to Whisper-Large. 8 Tiantian Feng et al. Voxlect: Benchmark for Modeling Dialects and Regional Languages Table 4. Comparison of dialect classification (Macro-F1) between short (6 sec) and long (>6 sec) utterances. MMS-LID-256 Whisper-Large v3 Short"
        },
        {
            "title": "Arabic\nMandarin\nIndic Language\nTibetan\nThai\nSpanish\nGerman\nFrench",
            "content": "0.856 0.707 0.690 0.774 0.921 0.780 0.896 0.707 0.842 0.710 0.977 0.831 0.949 0.765 0.917 0.690 0.920 0.702 0.659 0.703 0.921 0.781 0.858 0.702 0.934 0.701 0.971 0.827 0.966 0.778 0.892 0.726 Robustness to Utterance Length: We compare differences in dialect classification between short and long utterances in Table 4. Specifically, we use threshold of 6 seconds to define short utterances. The results indicate that, in more than half the cases, the classification performance of both models improves with longer utterances. Particularly, we observe performance improvement of 0.3 F1 in Indic language classification with longer utterances. 6. Data-centric Applications of Voxlect Here, we demonstrate how Voxlect facilitates two main speech technology applications: analysis of ASR models and automated evaluation of speech generation systems."
        },
        {
            "title": "6.1 Automatic Speech Recognition",
            "content": "Here, we investigate whether we can leverage the predicted dialect labels to analyze ASR performance across different dialects. We first predict dialect labels for existing datasets and examine whether these predicted labels lead to the same insights as using the ground truth dialect labels. Particularly, we predict dialect labels for the test sets of Mandarin and German. For ease of visualization, we include German, Austrian, Swiss, and Other for German, and standard Mandarin, Ji-Lu Mandarin, Southwestern Mandarin, and Zhongyuan Mandarin for Mandarin. We use fine-tuned Whisper-Large models to predict dialects and MMS for ASR. Figure 5. ASR performance trends, grouped by ground truth and labels predicted by Voxlect. 9 Tiantian Feng et al. Voxlect: Benchmark for Modeling Dialects and Regional Languages Table 5. Comparison of human evaluation and automated evaluation by Voxlect in assessing quality of dialect characteristics in generated speech of Chinese dialects. Human(1-5) Voxlect(0-100%) Shanghai(Jiang-Huai) Sichuan(Southwestern) Tianjin(Ju-Lu) Zhengzhou(Zhongyuan) Cantonese 2.85 3.35 1.90 3.15 3.50 36.6% 40.4% 20.5% 32.3% 53.4% When computing ASR performance with generated labels, we include only test samples with predicted dialect probability above 0.7. As shown in Figure 5, ASR performance trends using predicted labels from Voxlect closely align with those based on the ground truth labels. In the Mandarin ASR evaluation, speakers of standard Mandarin consistently have lower WER than those using regional sub-dialects, regardless of whether ground truth or predicted labels are used. Among the sub-dialects, Southwestern Mandarin shows the highest WER in both labeling methods. Similarly, we observe that, in the German ASR experiment, speakers labeled as German (Non-NRW area) demonstrate lower WER compared to those labeled as Austria, Swiss, or Other. Particularly, the \"Other\" category shows the highest WER across both ground truth and predicted labels. These findings show that Voxlect provides reliable tool for identifying limitations in ASR models."
        },
        {
            "title": "6.2 Evaluation of TTS Systems",
            "content": "With the rise of media personalization, the technology of generating dialectal speech is popularized in public services and entertainment. Here, we generate speech samples in specific dialects and investigate whether human evaluations align with evaluations generated by Voxlect regarding the dialect characteristics of the generated speech. Given the availability of publicly accessible TTS models, we focus our experiments on Mandarin. We note that related evaluations for English speech generation are presented in Vox-Profile [12]. Specifically, we use 10 text prompts designed by phonetician and reference speakers drawn from the KeSpeech test set. To generate speech in five distinct Chinese dialects, we utilize the CosyVoice-2 [42]. We use the prompt 用{方 言}说这句话 (which translates to \"Use dialect to say this sentence\"), specifying each of the five dialects: Cantonese, Sichuan (Southwestern), Tianjin (Ji-Lu), Zhengzhou (Zhongyuan), and Shanghai (Jiang-Huai). These were selected to represent range of major dialectal regions in China. The details of the prompts are in the Appendix. We invite colleagues with native language background in these dialects to assess the dialect characteristics of the generated speech samples. Participants were asked to rate the quality of the dialect characteristics on 5-point scale. We compare the average predicted probability of the corresponding dialects using Voxlect with the averaged human ratings in Table 5. The results indicate that the human ratings closely match the model predictions, with the target dialect of Tianjin (Ju-Lu) receiving the lowest ratings and Cantonese receiving the highest scores in both human and machine evaluation. 7. Limitations and Responsible Use While Voxlect introduces comprehensive benchmark for dialect and regional language classifications, several limitations remain. First, the dialect labels often originate from self-reports, which may contain labeling noise. Second, the benchmark is constrained by the availability of public datasets with existing labels. Thus, refined labels in specific areas (e.g., Paris of France) are not precisely captured, and Mandarin dialects such as those used in Hainan are not studied. Moreover, many globally spoken dialects and languages, such as regional varieties of Korean, Eastern European, or African languages, are currently not included. Third, the robustness of our proposed benchmark has not yet been evaluated beyond different noise levels and utterance lengths, such as cross-domain generalization (e.g., training on read speech and evaluating on natural speech). Finally, dialect classification may potentially expose private data about speakers and raise privacy concerns. While all datasets used in this work are publicly available and Voxlect adheres to the scope of the data usage and license, we take additional measures to mitigate the risk of misuse. Specifically, we release the code and model checkpoints under the Responsible AI License (RAIL) to require responsible use of Voxlect. Users should respect the privacy and consent of the data subjects and adhere to the relevant laws and regulations in their jurisdictions when using Voxlect. 10 Tiantian Feng et al. Voxlect: Benchmark for Modeling Dialects and Regional Languages 8. Conclusion and Future Work In this work, we propose Voxlect, benchmark for predicting dialects and regional languages using speech foundation models. This benchmark includes large-scale machine learning experiments using over 2 million speech utterances from 30 public data sources. We experiment with widely adopted speech foundation models and release suite of high-performing dialects and regional language classification models. Benchmarks of this kind in multilingual contexts are rarely represented in the literature. Our dialect classification results demonstrate that geographic proximity is reflected in dialect similarity. While this creates challenges for strict classification, it could bring insight into the cultural and historical factors impacting language evolution. Moreover, Voxlect enables wide range of downstream applications, including the analysis of speech recognition performance and the evaluation of speech generation systems. Our next steps include expanding the scope of the benchmark by integrating dialects of additional languages. For example, we plan to include dialectal variations in Korean, which show rich regional diversity such as the Seoul, Gyeongsang, and Jeolla dialects. We also plan to apply our benchmark models to enrich existing speech datasets with dialect data, which supports the development of downstream applications such as speech generation. 11 Tiantian Feng et al. Voxlect: Benchmark for Modeling Dialects and Regional Languages"
        },
        {
            "title": "Appendix",
            "content": "A. Data Distribution Attached is the training distribution for different dialects. Figure 6. Training distribution for Thai dialect classification. Figure 7. Training distribution for French dialect classification. Figure 8. Training distribution for German dialect classification. Tiantian Feng et al. Voxlect: Benchmark for Modeling Dialects and Regional Languages Figure 9. Training distribution for Arabic dialect classification. Figure 10. Training distribution for Mandarin dialect and Cantonese classification. Figure 11. Training distribution for Tibetan (in China) dialect classification. Tiantian Feng et al. Voxlect: Benchmark for Modeling Dialects and Regional Languages Figure 12. Training distribution for English dialect classification. Figure 13. Training distribution for Brazilian Portuguese dialect classification. Figure 14. Training distribution for Spanish dialect classification. Tiantian Feng et al. Voxlect: Benchmark for Modeling Dialects and Regional Languages B. Confusion Matrix Confusion matrices for Thai, French, German, Italian, Arabic, and Indic languages are shown in Figure 15, 16, 17, 18, 19, and 20, respectively. Figure 15. Confusion Matrix of Thai dialect prediction. Figure 16. Confusion Matrix of French dialect prediction. 15 Tiantian Feng et al. Voxlect: Benchmark for Modeling Dialects and Regional Languages Figure 17. Confusion Matrix of German dialect prediction. Figure 18. Confusion Matrix of Italian dialect prediction. Figure 19. Confusion Matrix of Arabic dialect prediction. 16 Tiantian Feng et al. Voxlect: Benchmark for Modeling Dialects and Regional Languages Figure 20. Confusion Matrix of Indic languages prediction. 17 Tiantian Feng et al. Voxlect: Benchmark for Modeling Dialects and Regional Languages C. Prompt The text prompts for the speech generation of Mandarin speech are listed below: 1. \"早上小兔子玉玉背着胡萝卜书包去上学\" 2. 今天是个特别的日子,年度南方森林运动会 3. 班主任熊猫翁大大说玉玉你代表咱们班参加吧 4. 有一次北风和太阳正在争论谁比较有本事 5. 他们正好看到有个人走过那个人穿著一件斗篷 6. 他们就说了谁可以让那个人脱掉那件斗篷就算谁比较有本事 7. 于是北风就拼命地吹怎料他吹得越厉害那个人就越是用斗篷包紧自己最后北风没办法只 好放弃 8. 接著太阳出来晒了一下那个人就立刻把斗篷脱掉了于是北风只好认输了 9. 中国疆域广阔人口众多首都在北京官方语言是普通话使用汉字同时中华各民族儿女也使用 民族语文 10. 这里风光旖旎众多山河湖海凭借南北各种美景吸引了众多游客 18 Tiantian Feng et al. Voxlect: Benchmark for Modeling Dialects and Regional Languages"
        },
        {
            "title": "References",
            "content": "[1] Camille Harris, Chijioke Mgbahurike, Neha Kumar, and Diyi Yang, Modeling gender and dialect bias in automatic speech recognition, in Findings of the Association for Computational Linguistics: EMNLP 2024, 2024, pp. 1516615184. [2] Kalvin Chang, Yi-Hui Chou, Jiatong Shi, Hsuan-Ming Chen, Nicole Holliday, Odette Scharenborg, and David Mortensen, Self-supervised speech representations still struggle with african american vernacular english, in Proc. Interspeech 2024, 2024, pp. 46434647. [3] Zhiyuan Tang, Dong Wang, Yanguang Xu, Jianwei Sun, Xiaoning Lei, Shuaijiang Zhao, Cheng Wen, Xingjun Tan, Chuandong Xie, Shuran Zhou, et al., Kespeech: An open source speech dataset of mandarin and its eight subdialects, in Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. [4] Ramon Sanabria, Nikolay Bogoychev, Nina Markl, Andrea Carmantini, Ondrej Klejch, and Peter Bell, The edinburgh international accents of english corpus: Towards the democratization of english asr, in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 15. [5] Isin Demirsahin, Oddur Kjartansson, Alexander Gutkin, and Clara Rivera, Open-source multi-speaker corpora of the english accents in the british isles, in Proceedings of the twelfth language resources and evaluation conference, 2020, pp. 65326541. [6] Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, et al., Scaling speech technology to 1,000+ languages, Journal of Machine Learning Research, vol. 25, no. 97, pp. 152, 2024. [7] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever, Robust speech recognition via large-scale weak supervision, in International Conference on Machine Learning. PMLR, 2023, pp. 2849228518. [8] Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber, Common voice: massively-multilingual speech corpus, in Proceedings of the Twelfth Language Resources and Evaluation Conference, 2020, pp. 42184222. [9] Juan Zuluaga-Gomez, Sara Ahmed, Danielius Visockas, and Cem Subakan, Commonaccent: Exploring large acoustic pretrained models for accent classification based on common voice, in Proc. Interspeech 2023, 2023, pp. 52915295. [10] Wenbin Wang, Yang Song, and Sanjay Jha, Globe: high-quality english corpus with global accents for zero-shot speaker adaptive text-to-speech, in Proc. Interspeech 2024, 2024, pp. 13651369. [11] Anuj Diwan, Zhisheng Zheng, David Harwath, and Eunsol Choi, Scaling rich style-prompted text-to-speech datasets, arXiv preprint arXiv:2503.04713, 2025. [12] Tiantian Feng, Jihwan Lee, Anfeng Xu, Yoonjeong Lee, Thanathai Lertpetchpun, Xuan Shi, Helin Wang, Thomas Thebaud, Laureano Moro-Velazquez, Dani Byrd, et al., Vox-profile: speech foundation model benchmark for characterizing diverse speaker and speech traits, arXiv preprint arXiv:2505.14648, 2025. [13] Yao Shi, Hui Bu, Xin Xu, Shaoji Zhang, and Ming Li, Aishell-3: multi-speaker mandarin tts corpus, in Proc. Interspeech 2021, 2021, pp. 27562760. [14] Peter Sullivan, AbdelRahim Elmadany, and Muhammad Abdul-Mageed, On the robustness of arabic speech dialect identification, in Proc. Interspeech 2023, 2023, pp. 53265330. [15] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al., Wavlm: Large-scale self-supervised pre-training for full stack speech processing, IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 15051518, 2022. [16] GADM, Database of global administrative areas, version 4.1, https://gadm.org/, 2025, Accessed on 28 July 2025. [17] Lander, Cslu: 22 languages corpus (ldc2005s26), Linguistic Data Consortium, 2005. [18] Guanlong Zhao, Sinem Sonsaat, Alif Silpachai, Ivana Lucic, Evgeny Chukharev-Hudilainen, John Levis, and Ricardo GutierrezOsuna, L2-arctic: non-native english speech corpus, in Proc. Interspeech 2018, 2018, pp. 27832787. [19] John Garofolo, Lori Lamel, William Fisher, Jonathan Fiscus, and David Pallett, Darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1, NASA STI/Recon technical report n, vol. 93, pp. 27403, 1993. [20] Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux, Voxpopuli: large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation, in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021, pp. 9931003. [21] Wenbin Wang, Yang Song, and Sanjay Jha, Usat: universal speaker-adaptive text-to-speech approach, IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. [22] Irina-Elena Veliche, Zhuangqun Huang, Vineeth Ayyat Kochaniyan, Fuchun Peng, Ozlem Kalinli, and Michael Seltzer, Towards measuring fairness in speech recognition: Fair-speech dataset, in Proc. Interspeech 2024, 2024, pp. 13851389. [23] Crowdsourced high-quality nigerian english speech data set., Open Speech and Language Resources, 2019. [24] William Byrne, Eva Knodt, Jared Bernstein, and Farzhad Emami, Hispanic-english database (ldc2014s05), Linguistic Data Consortium, 2014. [25] Mohammad Al-Fetyani, Muhammad Al-Barham, Gheith Abandah, Adham Alsharkawi, and Maha Dawas, Masc: Massive arabic speech corpus, in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 10061013. [26] Sadeen Alharbi, Areeb Alowisheq, Zoltán Tüske, Kareem Darwish, Abdullah Alrajeh, Abdulmajeed Alrowithi, Aljawharah Bin Tamran, Asma Ibrahim, Raghad Aloraini, Raneem Alnajim, et al., Sada: Saudi audio dataset for arabic, in ICASSP 20242024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2024, pp. 1028610290. [27] Benelallam, AM Naira, and Allak, Dvoice: an open source dataset for automatic speech recognition on moroccan dialectal arabic (2021), . [28] Yue Zhao, Xiaona Xu, Jianjian Yue, Wei Song, Xiali Li, Licheng Wu, and Qiang Ji, An open speech resource for tibetan multi-dialect and multitask recognition, International Journal of Computational Science and Engineering, vol. 22, no. 2-3, pp. 297304, 2020. [29] Tahir Javed, Janki Nawale, Eldho George, Sakshi Joshi, Kaushal Bhogale, Deovrat Mehendale, Ishvinder Sethi, Aparna Ananthanarayanan, Hafsah Faquih, Pratiti Palit, et al., Indicvoices: Towards building an inclusive multilingual speech dataset for indian languages, in Findings of the Association for Computational Linguistics ACL 2024, 2024, pp. 1074010782. Tiantian Feng et al. Voxlect: Benchmark for Modeling Dialects and Regional Languages [30] Artit Suwanbandit, Burin Naowarat, Orathai Sangpetch, and Ekapol Chuangsuwanich, Thai dialect corpus and transfer-based curriculum learning investigation for dialect automatic speech recognition, in Proc. Interspeech, 2023, vol. 2. [31] Adriana Guevara-Rukoz, Isin Demirsahin, Fei He, Shan-Hui Cathy Chu, Supheakmungkol Sarin, Knot Pipatsrisawat, Alexander Gutkin, Alena Butryna, and Oddur Kjartansson, Crowdsourcing latin american spanish for low-resource text-to-speech, in Proceedings of the Twelfth Language Resources and Evaluation Conference, 2020, pp. 65046513. [32] Alkis Koudounas, Moreno La Quatra, Lorenzo Vaiani, Luca Colomba, Giuseppe Attanasio, Eliana Pastor, Luca Cagliero, and Elena Baralis, Italic: An italian intent classification dataset, in Proc. Interspeech 2023, 2023, pp. 21532157. [33] Arnaldo Candido Junior, Edresson Casanova, Anderson Soares, Frederico Santos de Oliveira, Lucas Oliveira, Ricardo Corso Fernandes Junior, Daniel Peixoto Pinto da Silva, Fernando Gorgulho Fayet, Bruno Baldissera Carlotto, Lucas Rafael Stefanel Gris, et al., Coraa asr: large corpus of spontaneous and prepared speech manually validated for speech recognition in brazilian portuguese, Language Resources and Evaluation, vol. 57, no. 3, pp. 11391171, 2023. [34] Jinzuomu Zhong, Korin Richmond, Zhiba Su, and Siqi Sun, Accentbox: Towards high-fidelity zero-shot accent generation, in ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2025, pp. 15. [35] Melvyn Resnick, Phonological variants and dialect identification in Latin American Spanish, vol. 201, Walter de Gruyter, 2012. [36] Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Lingwei Meng, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, et al., Wavllm: Towards robust and adaptive speech large language model, arXiv preprint arXiv:2404.00656, 2024. [37] Leonardo Pepino, Pablo Riera, and Luciana Ferrer, Emotion recognition from speech using wav2vec 2.0 embeddings, in Proc. Interspeech 2021, 2021, pp. 34003404. [38] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al., Lora: Low-rank adaptation of large language models., ICLR, vol. 1, no. 2, pp. 3, 2022. [39] Tiantian Feng and Shrikanth Narayanan, Peft-ser: On the use of parameter efficient transfer learning approaches for speech emotion recognition using pre-trained speech models, in 2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII). IEEE, 2023, pp. 18. [40] Jack Chambers and Peter Trudgill, Dialectology, Cambridge University Press, 1998. [41] John Nerbonne, Measuring the diffusion of linguistic change, Philosophical Transactions of the Royal Society B: Biological Sciences, vol. 365, no. 1559, pp. 38213828, 2010. [42] Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, et al., Cosyvoice 2: Scalable streaming speech synthesis with large language models, arXiv preprint arXiv:2412.10117, 2024."
        }
    ],
    "affiliations": [
        "University of Southern California, Los Angeles, CA, USA"
    ]
}