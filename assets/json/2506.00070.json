{
    "paper_title": "Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics",
    "authors": [
        "Dongyoung Kim",
        "Sumin Park",
        "Huiwon Jang",
        "Jinwoo Shin",
        "Jaehyung Kim",
        "Younggyo Seo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are often heuristically constructed and not explicitly optimized for improving robot control. Furthermore, SFT often leads to issues such as catastrophic forgetting and reduced generalization performance. To address these limitations, we introduce Robot-R1, a novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control. Robot-R1 learns to predict the next keypoint state required for task completion, conditioned on the current scene image and environment metadata derived from expert demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples reasoning-based responses and reinforces those that lead to more accurate predictions. Our experiments show that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Despite having only 7B parameters, Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and primitive movement reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 0 7 0 0 0 . 6 0 5 2 : r Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics Dongyoung Kim1, Sumin Park1 , Huiwon Jang1, Jinwoo Shin1,4 Jaehyung Kim2, Younggyo Seo3 1KAIST, 2Yonsei University, 3UC Berkeley, 4Real World Inc. kingdy2002@kaist.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are often heuristically constructed and not explicitly optimized for improving robot control. Furthermore, SFT often leads to issues such as catastrophic forgetting and reduced generalization performance. To address these limitations, we introduce ROBOT-R1, novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control. ROBOT-R1 learns to predict the next keypoint state required for task completion, conditioned on the current scene image and environment metadata derived from expert demonstrations. Inspired by the DeepSeek-R1 learning approach, ROBOT-R1 samples reasoning-based responses and reinforces those that lead to more accurate predictions. To rigorously evaluate ROBOT-R1, we also introduce new benchmark that demands the diverse embodied reasoning capabilities for the task. Our experiments show that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Despite having only 7B parameters, ROBOT-R1 even surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and primitive movement reasoning."
        },
        {
            "title": "Introduction",
            "content": "Recently, Large Vision-Language Models (LVLMs) have shown significant promise in robotics [1, 2]. By jointly processing visual inputs and natural language, LVLMs offer powerful interface for interpreting complex real-world scenarios and enabling high-level reasoning on robot control [36]. Specifically, they have been employed in robotics by providing high-level actions in the form of textual descriptions [7] and latent representations [8, 9] to generate low-level actions, or by directly generating low-level actions [10]. These capabilities not only improve the performance and generalization of robotic systems but also enhance the human-robot interaction interface by enabling intuitive, language-driven control. Despite these advances, LVLMs often struggle to translate their general commonsense into the nuanced embodied reasoning required for controlling robots. For example, they often fail to accurately understand the spatial relationships crucial for low-level control [11] or fail to generate high-level plans [12]. Consequently, achieving high performance in robotic tasks typically requires additional training in domain-specific data. common approach involves Supervised Fine-Tuning (SFT) using question-answering datasets that pair task instructions with descriptions of various embodied reasoning types, such as action planning and spatial reasoning [1315]. However, embodied reasoning *Equal advising. Preprint. Under review. Figure 1: Illustration of the ROBOT-R1 framework. (a) ROBOT-R1 uses robot states and image observations from expert demonstrations to create dataset. (b) These data are reformulated into three different multiple-choice question answering (MCQA) tasks: predicting next states, current states, and primitive movements. (c) During training, an LVLM solves MCQA tasks with reasoning which is then optimized using the GRPO algorithm [17] to reinforce reasoning pathways. tasks for SFT are often heuristically designed and thus not fully optimized for actual robot action prediction; for instance, the linguistic expressions in SFT datasets often fail to capture the precise quantitative details essential for low-level robot control. Moreover, models trained via SFT often struggle with input and output formats they were not trained on (i.e., distribution shifts), which leads to catastrophic forgetting that degrades previously acquired knowledge, including general conversational abilities [16]. Meanwhile, recent advances led by Deepseek-R1 [17] have demonstrated the effectiveness of reinforcement learning (RL) in eliciting and reinforcing reasoning pathways, often achieving superior performance and generalization compared to SFT methods. Inspired by this, we introduce ROBOT-R1, novel framework that employs RL to effectively train LVLMs with embodied reasoning capabilities tailored specifically for robotic control (see Figure 1). The key idea of ROBOT-R1 is to train LVLMs to predict the next keypoint state necessary for task completion through reasoning process, and to optimize this reasoning via RL to maximize prediction accuracy. However, since the keypoint lies in continuous space, the action space is vast, making exploration particularly challenging. This makes it difficult for the model to efficiently learn reasoning strategies through trial and error. To address this, Robot-R1 reformulates the next-state prediction problem as multiple-choice question-answering (QA) problem. This discrete formulation narrows the action space, making the learning process more efficient. To further enhance state understanding, we add two auxiliary QA tasks: (i) current state prediction QA, where the model predicts the robots current state from visual observations, and (2) primitive movement prediction QA, where the model predicts rule-based linguistic descriptions of state changes (e.g., move up, move down). To analyze how ROBOT-R1 improves embodied reasoning capabilities in trained models, we design novel benchmark called the ROBOT-R1 Bench (see Figure 2). Through this benchmark, we observe that models trained with ROBOT-R1 achieve over 28% improvement in embodied reasoning tailored for low-level action control. In particular, despite having only the 7B parameters, ROBOTR1 outperforms several major commercial models, including GPT-4o [18], in this domain. To evaluate whether the embodied reasoning abilities acquired through ROBOT-R1 transfer effectively to other tasks, we conduct evaluations on two external benchmarks. First, on EmbodiedBench Manipulation [19], vision-driven robot agent benchmark, ROBOT-R1 yields 31% improvement in task performance. Moreover, on SpatialRGPTbench [20], which tests 3D spatial reasoning, name achieves approximately 40% improvement in quantitative metrics and about 60% improvement in qualitative metrics. These results contrast with conventional SFT approaches, which show limited improvement in embodied reasoning and tend to exhibit performance degradation when applied 2 to out-of-distribution tasks. This suggests that ROBOT-R1 can effectively learn generalizable and diverse embodied reasoning capabilities simply by next-state prediction."
        },
        {
            "title": "2 Related Work",
            "content": "Embodied reasoning for robot control. Integrating Large Vision-Language Models (LVLMs) into robotic systems has recently emerged as promising direction [1, 2]. These models leverage their vision-language understanding capabilities to interpret task instructions and process visual observations. Based on this understanding, LVLMs have been used to generate low-level actions directly [10, 21] or to produce high-level action [37, 12, 22]. Recent efforts have focused on enhancing the embodied reasoning capabilities of LVLMs to improve performance in complex longhorizon tasks [23, 24]. For instance, their ability to reason over language and visual inputs has been exploited for action planning, high-level action prediction, and spatial reasoning [2527]. common approach involves constructing embodied reasoning question-answering (QA) datasets, which are designed to aid robot control [15, 11, 2831]. These datasets are then used to fine-tune models via supervised fine-tuning (SFT). However, notable limitation is that such embodied reasoning datasets are often heuristically constructed and may not be explicitly optimized for robot control. Reinforcement learning for encourage reasoning. In large language models (LLMs), generating intermediate reasoning steps commonly known as Chain-of-Thought (CoT) reasoning has proven to be an effective strategy for improving performance across wide range of tasks [32]. This success has motivated substantial research into enhancing reasoning abilities through various ways, such as prompting techniques or the distillation of reasoning paths from stronger models [3336]. More recently, reinforcement learning (RL) has emerged as strong alternative to SFT for training reasoning models. In particular, research such as DeepSeek-R1 [17] has introduced RL-based frameworks where the model first generates reasoning trace, then produces an answer based on this trace, and is optimized using reward signals based on answer correctness. This RL-based approach not only improves reasoning quality but also enhances sample efficiency and generalization across variety of tasks [3739]. Consequently, it has been successfully applied to domains such as mathematics [40, 41], agentic tasks [42, 43], and even vision-related problems [44, 45], yielding substantial performance gains. These advances suggest that applying RL to train embodied reasoning for robot control could effectively address key limitations of SFT-based approaches."
        },
        {
            "title": "3 Method",
            "content": "In this section, we present the components in the following order: Preliminaries: Definition of robot state, and GRPO [46] algorithm for RL training. Data Generation: How to prepare training dataset for ROBOT-R1. Multiple-choice QA Base Training: How to train LVLM via RL with the generated datasets. ROBOT-R1 Bench: Introduce new evaluation benchmark for embodied reasoning that assesses various reasoning capabilities (e.g. spatial understanding) in robotic control scenarios. 3.1 Preliminaries Robot state. In this work, we utilize Franka Panda robot whose state is represented as 7dimensional vector consisting of the end-effectors 3D cartesian position (x, y, z), orientation (roll, pitch, yaw), and binary gripper state (open/closed). For simplicity, following Liang et al. [22], we consider only the Cartesian position (x, y, z) of the end-effector as the robot state s. Problem setup. We denote the LLM policy as πθ that generates an answer πθ(q) in response to question q. reward = freward(o) is obtained by evaluating the generated answer through pre-defined reward model. This reward model is designed to assign higher values to answers that more accurately predict the next state. The LLM policy is then optimized to maximize this reward. Group relative policy optimization. We employ the Group Relative Policy Optimization (GRPO) algorithm [17] to optimize the policy. Specifically, for given query q, we generate different responses = {o1, o2, o3, , , oG} from the current policy πθold. Each response oi is evaluated using the reward model to produce corresponding reward ri, resulting in the set = {r1, r2, ...rG}. For 3 each response, we compute an advantage score Ai, using the mean and standard deviation of the rewards for that query, i.e., Ai = rimean(r) . Finally, GRPO updates the policy by maximizing this advantage while applying KL penalty, through the following objective: std(r) (cid:34) JGRPO(θ) = qP (Q),{oi}G i=1πold(Qq) 1 (cid:88) i=1 min (cid:18) πθ(oiq) πθold(oiq) Ai, clip (cid:18) πθ(oiq) πθold (oiq) (cid:19) (cid:19) , 1 ε, 1 + ε Ai βDKL(πθπref) (cid:35) , where ε and β are hyperparameters. 3.2 Data Generation For training ROBOT-R1, we use multiple-choice question answering (QA) data, which includes primary task of predicting next waypoint (i.e., keypoint), along with two auxiliary tasks: (i) estimating the current state from an image and (ii) determining the necessary movements to reach the next waypoint. These tasks are constructed using information extracted from expert demonstrations. MetaData extraction. Without any contextual information, LVLMs struggle to infer low-level states. To address this, we utilize metadata , which encodes details about the current task and the robots low-level state, and use it to generate question prompts. The metadata encompasses three essential types of information. First, it includes details about fixed reference points within the robots environment, such as the center of table in table-top manipulation scenarios. Second, it defines the 3D coordinate system, including the positive direction of each axis, which helps interpret how spatial changes are reflected in numerical state changes. Third, it incorporates the dimensions (x, y, z) of consistently present object, such as the robots end-effector, to provide reference for scale estimation. This metadata is subsequently used as conditional input during question generation. Waypoint prediction QA. The goal of this task is to predict future waypoint state. We represent demonstration as sequence of frames = {(o0, s0), (o1, s1), . . . , (oN , sN )}, where each frame consists of an observation ot and its corresponding robot state st. subset of these frames is designated as keypoints, denoted by indices = {k0, k1, . . . , kM }, which mark significant changes in the trajectory or completion of crucial subgoals [47]. The task is to predict the state sk of the next keypoint following the current time step t, where = min{kj kj > t}. Each training example is thus represented as tuple (st, ot, sk ). To make multiple-choice question, we randomly sample three distractor states sd1, sd2, sd3 from the robots valid state space. These candidates, along with the correct next waypoint sk , are shuffled to create the final question input Qwaypoint(M, st, ot, shuffle({sk , sd1, sd2, sd3})) where the correct answer is = sk . Current state prediction QA. The goal is to identify the correct current state st. The question prompt includes the metadata and the current visual observation ot. To construct multiplechoice question, we randomly sample three distractor states {s d2, d3} from the state space. The d1, question is formulated as Qstate(M, ot, shuffle({st, d3})) where the answer is = st. d1, d2, Primitive movement prediction QA. The goal of this task is to predict the primitive movement (e.g., move up, move right) that the robot should take to move from the current state toward the next waypoint. To extract labels for these movement primitives, we apply rule-based heuristics based on the change in 3D Cartesian space. Specifically, we compute the difference in position between the current and next waypoint states, sk st, and identify the primary direction of movement along the x, y, and axes. If there is non-zero change along an axis, corresponding directional command is generated (e.g., move forward for positive change in the x-axis); this label is denoted as at. If the change along an axis is less than or equal to half of the largest change across all axes, the adverb slightly is added (e.g., Slightly move backward). To construct multiple-choice question, we randomly sample three adjacent movement= {a d3}. The question is formulated as Qprimitive(M, ot, st, shuffle({at, d2, d3})) where the answer is = at. d1, d2, d1, 4 Figure 2: Illustration of the ROBOT-R1 Bench. (a) ROBOT-R1 Bench consists of human-written questions paired with corresponding ground truth (reference) answers. (b) The LVLM under evaluation takes each question along with its associated image as input and generates an answer. (c) The generated answers are scored using GPT-4o, based on predefined rubrics and ground truth answers. Table 1: Example of the ROBOT-R1 Bench question and response. Question: What is the immediate next meaningful subtask (i.e., single step within larger sequence) that should be performed given the current scene? Model Answer : <think>robot appears to be positioned above saucepan with lid. The task is to take the lid off the saucepan. The immediate next step would involve moving the gripper towards the lid to grasp it. </think> <answer>the gripper towards the lid of the saucepan.</answer> 3.3 Multiple-choice QA Base Training Following DeepSeek-R1-Zero [17], we train LVLMs to explicitly generate their reasoning process. During training, the policy is instructed to output its thought process enclosed within <think> and </think> tags, followed by the final answer enclosed within <answer> and </answer> tags. The policy is optimized using GRPO. The reward signal for RL consists of two components: format reward rf and an answer correctness reward ra, combined as = rf + ra. The format reward rf encourages adherence to the specified output structure. The answer reward ra provides rule-based positive feedback if the models answer, within <answer>...</answer>, exactly matches the correct option in the multiple-choice QA problem. 3.4 ROBOT-R1 Bench: Novel Benchmark for Evaluating Embodied Reasoning Existing visual question answering (VQA) benchmarks for embodied reasoning primarily focus on general visual understanding, without explicitly evaluating the nuanced reasoning processes behind robotic behavior [4850]. Meanwhile, robotics-focused benchmarks often rely on simulation-based evaluation [19, 51, 52], priortize task successes as the main metric, or focus on tasks that are not aligned with the table-top manipulation scenarios central to our work. While these benchmarks provide valuable insights, they do not directly assess models ability to reason about robot actions. To address this gap, we introduice ROBOT-R1 Bench, new benchmark designed to evaluate embodied reasoning through open-ended answering grounded in robot demonstrations. Design features of ROBOT-R1 Bench. In practical applications, models typically face open-ended decisions rather than selecting from pre-defined options. To better reflect this, ROBOT-R1 Bench adopts an open-ended QA format that more closely resembles practical use cases. All questions are based on images from expert demonstrations, ensuring visually grounded and realistic robotic scenarios. ROBOT-R1 Bench supports fine-grained evaluation across four key reasoning types: high-level action planning, subtask reasoning, primitive movement reasoning, and spatial reasoning. Each question is crafted to assess one of these abilities, allowing for evaluation across both high-level 5 Table 2: ROBOT-R1 Bench results. Performance on embodied reasoning tasks tailored for low-level control evaluated using the ROBOT-R1 Bench. ROBOT-R1 achieves the highest overall performance, outperforming leading commercial models across benchmark scores. Model GPT-4o-mini [18] GPT-4o [18] Claude-3-Opus [56] Claude-3.5-Haiku [57] Claude-3.5-Sonnet-v2 [58] Claude-3.7-Sonnet [59] Gemini-1.5-Flash [60] Gemini-1.5-Pro [60] Gemini-2.0-Flash [61] Qwen2.5-VL-7B-Ins [62] w/ Direct SFT w/ CoT SFT w/ ROBOT-R1 (Ours) Movement Spatial In 0.64 0. 0.40 0.68 0.96 0.76 0.60 0.76 0.52 0.64 0.12 0.84 0.96 Out 0.56 0.52 0.24 0.48 0.28 0. 0.16 0.40 0.40 0.52 0 0.56 0.56 Avg 0.60 0.72 0.32 0.58 0.62 0.62 0.38 0.58 0. 0.58 0.06 0.70 0.76 In 1.73 1.70 0.97 1.41 1.49 1.65 1.57 1.49 1.76 1.62 0.08 0.46 1. Out 1.14 1.07 0.43 0.71 0.75 1.07 0.93 1.14 1.14 1.11 0.04 0.07 1.18 Avg 1.48 1.43 0.74 1.11 1.17 1.4 1.29 1.34 1.49 1.40 0.06 0.29 1.51 decision making (action planning and subtask reasoning) and low-level control reasoning (primitive action reasoning and spatial reasoning). Dataset construction. The ROBOT-R1 Bench dataset consists of 10 tasks from RLBench [53]. For each task, we randomly sample five frames from expert demonstratrions, resulting in total of 50 images. For each image, experienced researchers manually created questions and detailed reference answers, targeting the four key reasoning abilities. The final dataset consists of 215 open-ended questions in total: 65 for spatial reasoning and 50 for each of the other three reasoning types. Evaluation. Model responses are evaluated against reference answers. For consistent and objective scoring, we adopt an LLM-as-judge approach using GPT-4o [18, 54, 55]. Based on predefined rubric tailored to each reasoning type, GPT-4o assesses the models answers for accuracy, logical coherence, and completeness, and assigns numeric score in [0, 3]. This allows for fine-grained quantitative analysis of the models embodied reasoning capabilities (See Figure 2 and Table 1)."
        },
        {
            "title": "4 Experiment",
            "content": "This section details the experimental setup for training ROBOT-R1, and the evaluation results. Our goal is to validate the effectiveness of ROBOT-R1 in learning embodied reasoning for robot manipulation tasks. We utilize the Qwen2.5-7b-VL-Ins [62] as the base model for all experiments. 4.1 Experimental Setup Training data. The training data used in our experiments are generated using the built-in data generator in RLBench [53]. We collect 50 demonstrations per task from the variation 0 settings, rendering them at 224 224 resolution using OpenGL3. Waypoints from these demonstrations are extracted using the waypoint extraction functionality provided in the ARM [47] repository1. The tasks selected from RLBench for training our model include: pick_up_cup, push_button, put_rubbish_in_bin, phone_on_base, and take_lid_off_saucepan. For generating the waypoint prediction QA training data, as described in Section 3, frame extraction interval of = 10 was used between the Table 3: ROBOT-R1 Bench results tailored for high-level control. ROBOT-R1 significantly outperforms SFT, the baseline training method. Model Planning Subtask GPT-4o [18] claude-3-7-sonnet [59] gemini-2.0-flash [61] Qwen2.5-VL-7B-Ins [62] w/ Direct SFT w/ CoT SFT w/ ROBOT-R1 (Ours) 1.96 1.72 1.84 1.66 0 0.60 1.44 2.02 1.58 1. 1.04 0.04 1.28 1.30 1https://github.com/stepjam/ARM 6 current frame (ot, st) and the selected future keypoint frame (sk ), similar to demonstration augmentation proposed in ARM. Consequently, each task contains approximately 2.5K questions, resulting in total of around 7.5K QA pairs across the three QA tasks used for training. Baseline models. The proposed ROBOT-R1 learns to predict the next waypoint state from expert demonstrations by generating an explicit reasoning process. To evaluate the importance of this learned reasoning, we establish two baseline models trained via Supervised Fine-tuning (SFT): Direct SFT: This model is fine-tuned on QA dataset to directly predict the next waypoint state (sk ) from the current observation (ot) and state (st), without any intermediate reasoning steps. This baseline helps assess the performance gain achieved by incorporating reasoning. CoT SFT: This model is fine-tuned on QA dataset where the input prompt includes manually structured Chain-of-Thought (CoT) path. This CoT path sequentially incorporates action planning, subtask reasoning, and primitive action components to guide the prediction of the next waypoint state, similar to Zawalski et al. [15]. This baseline enables comparison between the reasoning learned via ROBOT-R1 against reasoning guided by predefined, structured thought process. By comparing ROBOT-R1 against these baselines, we aim to highlight the importance of (i) incorporating reasoning at all (v.s. Direct SFT) and the adaptiveness of reasoning learned through reinforcement learning (v.s. CoT SFT). See Appendix for details. Implementation detail. ROBOT-R1 is trained using GRPO as described in Section 3. We use the hyperparameter configurations in the EasyR1 workspace.2 For the training process, we utilize batch size of 128 over single epoch. During GRPO updates, 5 samples were generated per prompt with sampling temperature of 1.0. The rollout batch size is set to 512. We use learning rate of 1.0 106 with weight decay of 1.0 102. For the SFT baselines, we use the same batch size, but learning rate is 1.0 105. We use the hyperparameters provided in the Qwen2-VL-Finetune workspace.3 4.2 Robot-R1 Bench Setup. To reduce variance in benchmark results, all models generate responses with temperature setting of 0. To ensure the robustness of our benchmark and allow for relative performance comparisons, we evaluate the performance of several widely used commercial models across different versions, including GPT [18], Claude [59, 5658], and Gemini [61, 60, 63]. Furthermore, we evaluate the performance of our own models: the base Qwen2.5-VL-7B-Instruct model [62], its variant fine-tuned via SFT, and the model trained using our proposed ROBOTR1 method (see Appendix for details). Figure 3: ROBOT-R1 Bench results. In embodied reasoning tailored for low-level control, ROBOTR1 outperforms all previously reported models. Results. Table 2 presents the results across the reasoning benchmarks focused on low-level control, and Figure 3 provides its visualization to help understanding. We find that commercial model performance improves consistently with their version updates, indicating the benchmarks sensitivity to model quality. Notably, the ROBOT-R1-trained model achieves the highest performance across both the overall benchmark and its individual components, outperforming all commercial models evaluated. Specifically, in primitive movement prediction, our method outperforms even the SFT-trained model, which is explicitly trained to predict accurate actions. Table 3 further show the performance in reasoning tasks tailored for high-level control. Here, the correlation between model version and benchmark performance is less pronounced. We hypothesize that this is due to the inherently high variability in valid high-level actions for robotic control. Nevertheless, we still observe general trend of increasing benchmark performance as model quality improves. Importantly, our ROBOT-R1 exhibits significant performance gains in subtask reasoning predicting the immediate high-level action compared to the base and the SFT-trained model. This suggests that effective high-level reasoning can emerge from training exclusively on low-level control information, even without 2https://github.com/hiyouga/EasyR1 3https://github.com/2U1/Qwen2-VL-Finetune 7 Table 4: EmbodiedBench Manipulation results. Success rate (%) of ROBOT-R1, SFT baselines, and various commercial models in evaluating the performance of EmbodiedBench Manipulation [19], which evaluates LVLM performance for vision-based robotic agents. Model Base Common Complex Spatial Visual GPT-4o-mini [18] Claude-3.5-Haiku [56] Gemini-1.5-flash [63] Qwen2.5-VL-7B-Ins [62] + Direct SFT + CoT SFT + ROBOT-R1(Ours) 4.2 12.5 14.6 6.3 0 0 12. 6.3 12.5 10.4 6.3 0 0 8.3 2.1 12.5 4.2 6.3 0 0 6.3 10.4 16.7 10.4 14.6 0 0 14. 0 13.9 8.3 11.1 0 0 16.7 Avg. 4.6 13.6 9.6 8.92 0 0 11.68 explicit high-level action supervision. Interestingly, we find that performance on action planning slightly decreases, likely because our training objective focuses primarily on next keypoint prediction rather than long-horizon planning. 4.3 EmbodiedBench Manipulation Next, to evaluate whether ROBOT-R1 not only enhances embodied reasoning capabilities but also leads to improved performance in an actual robotic agent environment, we consider the EmbodiedBench Manipulation benchmark [19], vision-driven agent assessment platform built upon the RLBench simulation environment [53]. This benchmark evaluates models ability to complete manipulation tasks by predicting low-level actions (a 7-dimension action vector) using in-context learning. reward is awarded only if task is completed successfully. Setup. We evaluate the model performance on four different tasks: Pick & Place Objects, Stack Objects, Shape Sorter Placement, and Table Wiping. To comprehensively assess instruction-following capabilities, the benchmark categorizes task instructions into five groups: (i) Base, which requires fundamental task-solving skills, (ii) Common Sense, which requires general world knowledge to resolve indirect objective references, (iii) Complex instructions, which involves longer, possibly distracting context that obscures the primary command, (iv) Spatial reasoning, where objects are referred to by their spatial relationships to other objects, and (v) Visual understanding, which requires recognizing objects based on visual attributes such as color or shape. For comparison, the experimental results of GPT-4o-mini [18] and Gemini-1.5-flash [60] are adopted directly from the values reported in their paper. Result. In Table 4, the model trained with the ROBOT-R1 framework achieves an approximate 31% performance increase over the baseline Qwen2.5-7B-Instruct model. In particular, in the Base category, which utilizes instructions similar to those used during ROBOT-R1 training, the model achieves nearly double the performance (6.3% 12.5%). This indicates that the embodied reasoning capabilities learned via ROBOT-R1 transfer effectively to downstream tasks, even in setups that differ significantly from the training distribution. In contrast, the models fine-tuned with SFT failed to complete any tasks, highlighting the limitations of SFT approaches. 4.4 Ablation Study and Analysis Spatial reasoning ability. To further evaluate whether enhanced spatial reasoning abilities of ROBOT-R1 as shown in Section 4.2 and acquired through RL for optimizing reasoning paths in action prediction generalize to broader spatial understanding scenarios, we evaluate the models performance on the SpatialRGPT benchmark [20]. This benchmark is designed to evaluate 3D spatial comprehension capabilities in LVLMs. SpatialRGPT consists of two main components: qualitative evaluation, which tests understanding of relative positional relationships between objects in an image, and quantitative evaluation, which requires accurate prediction of numerical spatial values. As shown in Table 5, the model trained with ROBOT-R1 achieves substantial performance gains, with an approximate improvement of 40% in qualitative tasks and around 60% improvement in quantitative tasks. These improvements are particularly notable when compared to the performance of models 8 Table 5: SpatialRGPT-Bench results. Accuracy (%) of ROBOT-R1, SFT baselines in evaluating the performance of VQA tasks from SpatialRGPT-Bench [20], which measures (a) how LVLMs accurately predict the positional relations between objects, and (b) how LVLMs precisely estimate the absolute object positions, where the prediction is considered correct if it lies within 25% of the ground-truth values. In contrast to SFT models that struggle to generalize to novel spatial reasoning tasks, ROBOT-R1 effectively transfers to these spatial reasoning tasks. (a) Quantitative results Model Direct Dist. Horiz. Dist. Vert. Dist. Width Height Direction Quant. Avg Qwen2.5-VL-7B-Ins [62] + Direct SFT + CoT SFT + ROBOT-R1(Ours) 1.35 4.05 1.35 6. 9.02 3.28 2.46 14.75 8.49 0.94 0 22.64 6.77 0.75 3.01 21.8 9.77 4.51 6.77 19.55 28.04 42.06 38.32 12.15 9.88 8.41 7.88 15. (b) Qualitative results Model Below/Above Left/Right Big/Small Tall/Short Wide/Thin Behind/Front Qual. Avg Qwen2.5-VL-7B-Ins [62] + Direct SFT + CoT SFT + ROBOT-R1(Ours) 35 3.33 15.83 42.5 50.48 17.14 33.33 41.9 6.6 0 45.28 31.13 17.86 9.82 39.28 37. 27.88 0 47.12 38.46 36.36 0 35.45 52.72 29.07 5.02 35.62 40.79 Table 6: Ablation study. Evaluation results on the ROBOT-R1 Bench comparing different dataset designs, varying auxiliary tasks, i.e., waypoint prediction (WP), current state prediction (SP), and primitive movement prediction (MP), and the question format. QA Type Task Type Low Level Control High Level Control Open-End MCQA WP SP MP Movement 0.68 0. - - - - - 0.40 0.54 0.76 Spatial Avg Planning Subtask Avg 1.37 1.03 1.42 1.51 1. 1.03 0.76 0.91 1.03 1.14 1.60 1.48 1.44 1.50 1.44 1.26 1.16 1.22 1.24 1. 1.43 1.17 1.33 1.37 1.37 trained solely with SFT, highlighting the effectiveness of RL-optimized reasoning in developing transferrable spatial understanding. Impact of QA design on performance. Table 6 demonstrates the effects of different QA design choices on model performance. Our results show that adding auxiliary tasks progressively improves overall performance. We also evaluate an alternative approach that uses open-ended generation instead of multiple-choice question answering (MCQA). In this setup, the model directly generats next waypoint values in an open-ended format and uses clip(1 L1, 0, 1) as the reward function, where the L1 represents the distance between the predicted and ground-truth states. While the open-ended approach initially outperforms single-task MCQA, it exhibits significant performance degradation when auxiliary tasks such as current-state prediction are introduced. Consequently, the MCQA framework demonstrates superior results. These findings highlight the critical role of question structure design in reinforcement learning for embodied reasoning (see Figure 12 for additional information about training metrics). Evolution of reasoning patterns during ROBOT-R1 training. We find that the evolution of reasoning patterns during ROBOT-R1 training diverges notably from trends reported in DeepseekR1 [17]. While reasoning models for mathematical and coding tasks tend to develop longer reasoning chains over time, ROBOT-R1 exhibits the opposite pattern of producing progressively shorter and more focused reasoning during validation accuracy increase (see Figure 4). Qualitatively, the model initially generates broad action plans or subtask reasoning in an overview format. Over time, these are gradually replaced by more concise, logically structured reasoning traces that directly link relevant embodied reasoning components (see Appendix for detailed examples). (a) Reward during ROBOT-R1 training (b) Response length during ROBOT-R1 training Figure 4: Training metrics for ROBOT-R1 (a) Shows validation accuracy change over training steps. (b) Shows mean response length over training steps."
        },
        {
            "title": "5 Conclusion",
            "content": "We have proposed ROBOT-R1, novel training framework that enhances the embodied reasoning capabilities of Large Vision-Language Models in robotic domains. Our approach trains models to predict the next state based on image observations and current states from expert demonstrations, using explicit reasoning processed optimized through reinforcement learning. Our extensive experiments demonstrate that models trained with ROBOT-R1 not only outperform Supervised Fine-Tuning (SFT) based approaches on embodied reasoning tasks but also exhibit consistent performance improvements across diverse embodied tasks while SFT methods often suffer from performance degradation."
        },
        {
            "title": "Acknowledgements",
            "content": "This paper was supported by RLWRD."
        },
        {
            "title": "References",
            "content": "[1] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. [2] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-languageaction models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. [3] Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114, 1 (2):6, 2023. [4] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Inner monologue: Embodied Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022. [5] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International conference on machine learning, pages 91189147. PMLR, 2022. [6] Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, and Yang Gao. Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning. arXiv preprint arXiv:2311.17842, 2023. 10 [7] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. [8] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. [9] Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, et al. Cogact: foundational vision-language-action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650, 2024. [10] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. [11] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. [12] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. [13] Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, et al. Hi robot: Open-ended instruction following with hierarchical vision-language-action models. arXiv preprint arXiv:2502.19417, 2025. [14] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, et al. Vision-language foundation models as effective robot imitators. arXiv preprint arXiv:2311.01378, 2023. [15] Michał Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, and Sergey Levine. Robotic control via embodied chain-of-thought reasoning. arXiv preprint arXiv:2407.08693, 2024. [16] Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, et al. Chatvla: Unified multimodal understanding and robot control with vision-language-action model. arXiv preprint arXiv:2502.14420, 2025. [17] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [18] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [19] Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, et al. Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents. arXiv preprint arXiv:2502.09560, 2025. [20] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision language models. arXiv preprint arXiv:2406.01584, 2024. [21] Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, et al. Latent action pretraining from videos. arXiv preprint arXiv:2410.11758, 2024. [22] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 94939500. IEEE, 2023. [23] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. Calvin: benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. IEEE Robotics and Automation Letters, 7(3):73277334, 2022. [24] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. Interactive language: Talking to robots in real time. IEEE Robotics and Automation Letters, 2023. [25] Jiaqi Wang, Enze Shi, Huawen Hu, Chong Ma, Yiheng Liu, Xuhui Wang, Yincheng Yao, Xuan Liu, Bao Ge, and Shu Zhang. Large language models for robotics: Opportunities, challenges, and perspectives. Journal of Automation and Intelligence, 2024. [26] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. arXiv preprint arXiv:2307.05973, 2023. [27] Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3d-vla: 3d vision-language-action generative world model. arXiv preprint arXiv:2403.09631, 2024. [28] Junjie Wen, Minjie Zhu, Yichen Zhu, Zhibin Tang, Jinming Li, Zhongyi Zhou, Chengmeng Li, Xiaoyu Liu, Yaxin Peng, Chaomin Shen, et al. Diffusion-vla: Scaling robot foundation models via unified diffusion and autoregression. arXiv preprint arXiv:2412.03293, 2024. [29] Yide Shentu, Philipp Wu, Aravind Rajeswaran, and Pieter Abbeel. From llms to actions: latent codes as bridges in hierarchical robot control. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 85398546. IEEE, 2024. [30] Junjie Wen, Yichen Zhu, Jinming Li, Zhibin Tang, Chaomin Shen, and Feifei Feng. Dexvla: Vision-language model with plug-in diffusion expert for general robot control. arXiv preprint arXiv:2502.05855, 2025. [31] Jaden Clark, Suvir Mirchandani, Dorsa Sadigh, and Suneel Belkhale. Action-free reasoning for policy generalization. arXiv preprint arXiv:2502.03729, 2025. [32] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [33] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [34] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. [35] Seungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon Seo. The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning. arXiv preprint arXiv:2305.14045, 2023. [36] Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, et al. Advancing llm reasoning generalists with preference trees. arXiv preprint arXiv:2404.02078, 2024. [37] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [38] Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025. [39] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [40] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [41] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. [42] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. [43] Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. [44] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [45] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [46] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [47] Stephen James and Andrew Davison. Q-attention: Enabling efficient learning for vision-based robotic manipulation. IEEE Robotics and Automation Letters, 7(2):16121619, 2022. [48] Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil Joshi, et al. Robovqa: Multimodal long-horizon reasoning for robotics. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 645652. IEEE, 2024. [49] xAI. Realworldqa: benchmark for real-world understanding, 2024. URL https:// huggingface.co/datasets/xai-org/RealworldQA. Released alongside Grok-1.5 Vision Preview. [50] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 110, 2018. [51] Kaizhi Zheng, Xiaotong Chen, Odest Chadwicke Jenkins, and Xin Wang. Vlmbench: compositional benchmark for vision-and-language manipulation. Advances in Neural Information Processing Systems, 35:665678, 2022. [52] Xiao Liu, Tianjie Zhang, Yu Gu, Iat Long Iong, Yifan Xu, Xixuan Song, Shudan Zhang, Hanyu Lai, Xinyi Liu, Hanlin Zhao, et al. Visualagentbench: Towards large multimodal models as visual foundation agents. arXiv preprint arXiv:2408.06327, 2024. [53] Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew Davison. Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2): 30193026, 2020. [54] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [55] Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other language models. arXiv preprint arXiv:2405.01535, 2024. [56] Anthropic. Introducing the claude 3 family: State-of-the-art models in intelligence, speed, and vision. Anthropic Blog, 2024. URL https://www.anthropic.com/news/claude-3-family. [57] Anthropic. Claude 3.5 haiku. Anthropic Blog, October 2024. URL https://www.anthropic. com/claude/haiku. [58] Anthropic. Claude 3.5 sonnet. Anthropic Blog, 2024. URL https://www.anthropic.com/ news/claude-3-5-sonnet. [59] Anthropic. Claude 3.7 sonnet. Anthropic Blog, 2024. URL https://www.anthropic.com/ news/claude-3-7-sonnet. [60] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [61] Demis Hassabis, Koray Kavukcuoglu, and Google DeepMind. Gemini 2.0: Googles new ai model for the agentic era. Google Blog, December 2024. URL https://blog.google/ technology/google-deepmind/google-gemini-ai-update-december-2024/. [62] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [63] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023."
        },
        {
            "title": "A Limitation and Social Impact",
            "content": "Limitation. In this paper, we only considered the xyz positions as state, without considering the rotation of the end-effector and the manipulation of the gripper. Therefore, ROBOT-R1 is hard to achieve reasoning about rotation or gripper movement, which are essential for complex tasks. Integrating these additional state dimensions would enhance ROBOT-R1s embodied reasoning ability for complex tasks. Social impact. ROBOT-R1 proposes method that enhances embodied reasoning for robot control using small LVLMs (under 10B parameters) with limited data. This approach is easily accessible to research labs and small companies working on robotics. We expect ROBOT-R1 to accelerate robotics research, facilitating faster application and adoption of robotic technologies in society. However, there is potential risk that robots trained by reinforcement learning execute unintended actions to achieve their goals. This potentially leads to diminished human oversight, hence the design of complementary reward to mitigate such risk should be also explored in the future."
        },
        {
            "title": "B Experiment Details",
            "content": "B.1 Computing Cost All experiments are conducted on single node consisting of four A100 80GB GPUs. Training ROBOT-R1 task approximately 12 hours for 1 epoch using the 7.5K prompts. B.2 Prompt Template for Generating Training Dataset In this section, we provide the prompt templates used to generate the training datasets. To generate the Multiple-Choice Question Answering (MCQA) dataset as denoted in Section 3.2, we utilize three different prompt templates: the waypoint prediction MCQA prompt template (Figure 5), the state prediction MCQA prompt template (Figure 6), and the primitive movement prediction MCQA prompt template (Figure 7). These templates take the current state, task instructions, and question about problem. For generating the SFT dataset, we employ prompt structure analogous to the MCQA format. Specifically, we use two different prompt templates for both Direct SFT and CoT SFT: the waypoint prediction SFT prompt template (Figure 8) and the current state prediction SFT prompt template (Figure 9). However, the SFT dataset, unlike the ROBOT-R1 dataset, includes not only prompts but also answers. The Direct SFT dataset is structured to generate immediate answers, while the CoT SFT dataset incorporates reasoning path for the waypoint prediction task. The CoT SFT dataset is designed to perform high-level action planning, subtask reasoning, and primitive movement prediction sequentially before predicting the final answer. The high-level action planning and subtask reasoning components were defined using human-created scripts. Waypoint prediction MCQA # You are Franka Robot Assistant: Task Planning and Execution System ## Task description {task_description} ## Visual Input You will receive single combined image for scene understanding: - <image>: front view of the workspace ## Coordinate System The world coordinate frame follows these conventions: - This is based on the front view. (Wrist view has the Y-axis (left and right) opposite) - X-axis: Front of table (positive) to back of table (negative) - Y-axis: Left (negative) to right (positive) - Z-axis: Down toward floor (negative) to up toward ceiling (positive) - World origin [0.25, 0, 0.752] is at the center of the table surface ## Robot Specifications - Gripper dimensions: 0.06m width (x-direction) 0.2 length (y-direction) 0.09 height (z-direction), with fingers 0.04 in length ## Current Robot State Position: [{x}, {y}, {z}] ### Choice Question Based on the provided image and current robot state, predict the next waypoint position [x, y, z] Choose the most accurate option: [[A]] {A} [[B]] {B} [[C]] {C} [[D]] {D} ## Output Format You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE enclosed within <answer> </answer> tags. Example output format: <think> [detailed reasoning process] </think> <answer> [[A]] </answer> Figure 5: Waypoint prediction MCQA prompt template 16 Current State prediction MCQA # You are Franka Robot Assistant: Task Planning and Execution System ## Robot Task description {task_description} ## Visual Input You will receive single combined image for scene understanding: - <image>: front view of the workspace ## Coordinate System The world coordinate frame follows these conventions: - This is based on the front view. (Wrist view has the Y-axis (left and right) opposite) - X-axis: Front of table (positive) to back of table (negative) - Y-axis: Left (negative) to right (positive) - Z-axis: Down toward floor (negative) to up toward ceiling (positive) - World origin [0.25, 0, 0.752] is at the center of the table surface ## Robot Specifications - Gripper dimensions: 0.06m width (x-direction) 0.2 length (y-direction) 0.09 height (z-direction), with fingers 0.04 in length ### Choice Question Lets predict the current robot state base on image [[A]] {A} [[B]] {B} [[C]] {C} [[D]] {D} ## Output Format You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE enclosed within <answer> </answer> tags. Example output format: <think> [detailed reasoning process] </think> <answer> [[A]] </answer> Figure 6: Current state prediction MCQA prompt template 17 Primitive Movement prediction MCQA # You are Franka Robot Assistant: Task Planning and Execution System ## Task description {task_description} ## Visual Input You will receive single combined image for scene understanding: - <image>: front view of the workspace ## Coordinate System The world coordinate frame follows these conventions: - This is based on the front view. - X-axis: Front of table (positive) to back of table (negative) - Y-axis: Left (negative) to right (positive) - Z-axis: Down toward floor (negative) to up toward ceiling (positive) ### Choice Question What movements are needed to get to the next keypoint to perform the task? [[A]] {A} [[B]] {B} [[C]] {C} [[D]] {D} ## Output Format You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE enclosed within <answer> </answer> tags. Example output format: <think> [detailed reasoning process] </think> <answer> [[A]] </answer> Figure 7: Primitive movement prediction MCQA prompt template Waypoint prediction QA for SFT Prompt # You are Franka Robot Assistant: Task Planning and Execution System ## Task description {task_description} ## Visual Input You will receive single combined image for scene understanding: - <image>: front view of the workspace ## Coordinate System The world coordinate frame follows these conventions: - This is based on the front view. (Wrist view has the Y-axis (left and right) opposite) - X-axis: Front of table (positive) to back of table (negative) - Y-axis: Left (negative) to right (positive) - Z-axis: Down toward floor (negative) to up toward ceiling (positive) - World origin [0.25, 0, 0.752] is at the center of the table surface ## Robot Specifications - Gripper dimensions: 0.06m width (x-direction) 0.2 length (y-direction) 0.09 height (z-direction), with fingers 0.04 in length ## Current Robot State Position: [Current State] Lets determine the next robot state to execute task Answer for CoT SFT Plan: 1. Move the gripper to align vertically and position just above the red button. 2. Move the gripper down for pressing the button. Subtask: Move the gripper to align vertically and position just above the red button. Move: slightly move front, slightly move left, and move down. Answer: [Answer] [Answer] Answer for Direct SFT Figure 8: Waypoint prediction QA template for SFT 19 Current State prediction QA for SFT Prompt # You are Franka Robot Assistant: Task Planning and Execution System ## Task description {task_description} ## Visual Input You will receive single combined image for scene understanding: - <image>: front view of the workspace ## Coordinate System The world coordinate frame follows these conventions: - This is based on the front view. (Wrist view has the Y-axis (left and right) opposite) - X-axis: Front of table (positive) to back of table (negative) - Y-axis: Left (negative) to right (positive) - Z-axis: Down toward floor (negative) to up toward ceiling (positive) - World origin [0.25, 0, 0.752] is at the center of the table surface ## Robot Specifications - Gripper dimensions: 0.06m width (x-direction) 0.2 length (y-direction) 0.09 height (z-direction), with fingers 0.04 in length Lets predict the current robot state base on image [Answer] Answer Figure 9: Current state prediction QA template for SFT B.3 Benchmark Setup To reduce variance in all benchmarks, we set the temperature of the model being evaluated to 0. Meanwhile, the ROBOT-R1 Bench uses GPT-4o [18] for LLM-as-judge, and we set the temperature of GPT-4o to 1 (default temperature) for judgment tasks. ROBOT-R1 Bench details. The evaluation dataset consists of (1) images corresponding to observations, (2) questions related to embodied reasoning, and (3) human-written ground truth answers. The evaluated models receive the images and questions as input to generate responses. To improve the accuracy of answer generation, each model receives the following system prompt for every sampling process (see Figure 10). The evaluator, GPT-4o, provides scores between 0 and 3 points using the LLM-as-judge template prompt (see Figure 11) with the question, the response of the evaluated model, the predefined ground truth answer and the evaluation rubric. To sum up, full evaluation process is conducted as shown in Table 7."
        },
        {
            "title": "C Qualitative Results",
            "content": "This section presents the differences between model responses with ROBOT-R1 and Qwen2.5-VL7b-ins (ROBOT-R1s base model). Figure 13 shows response examples of ROBOT-R1 trained with the MCQA approach. Figure 14 shows response examples of ROBOT-R1 trained with the Open-End approach from Section 4.4. Commonly, Qwen2.5-VL-7b-ins demonstrates weak connections between reasoning process components and tends to generate responses in summary format rather than 20 Table 7: Example of the working process of ROBOT-R1 Bench Question: What is the complete sequence of high-level actions needed to turn on the lamp? Model Raw Answer: <think>To determine the sequence of actions needed to turn on the lamp, we need to consider the typical steps required for such task. The lamp appears to be standalone object with no visible control panel or buttons in the image provided. Assuming the lamp can be turned on by remote control or switch, the following steps would be logical: 1. Identify the lamps location relative to the robot. 2. Determine if there is remote control or switch accessible to the robot. 3. If remote control or switch is present, use the robots arm to interact with it. 4. If the lamp is directly accessible, move the robots arm close to the lamp and interact with it. Since the image does not show remote control or switch, and the lamp itself is not directly accessible, the robot might need to perform series of actions to reach the lamp, such as moving towards it, interacting with it, and possibly using tool to activate it.</think> <answer>1. Move the robot arm towards the lamp. 2. Interact with the lamp to turn it on.</answer> Model Answer: 1. Move the robot arm towards the lamp. 2. Interact with the lamp to turn it on. Ground Truth: 1. Position the robots gripper near the lamp switch. 2. Press the lamp switch. LLM-as-Judge Result: The models answer correctly captures the intent and sequence of actions (approaching and turning on the lamp), but it uses more general terms (\"move arm towards the lamp\" and \"interact with the lamp to turn it on\") rather than explicitly stating positioning near the lamp switch and pressing it. While the core meaning is mostly preserved, the model omits the specific details of positioning near the switch and pressing it, making it slightly less precise. Score: 2 narrative format. However, after ROBOT-R1 training, the reasoning processes become coherently connected and shift toward narrative format in their responses. 21 System Prompt for Sampling in ROBOT-R1 Bench System Prompt Format You are an AI that accurately answers questions about robot actions and spatial relationships. Follow these rules strictly: 1. Answer ONLY what is asked in the question. 2. Do not include any purpose or objective of actions (remove all to... phrases). 3. Do not include any additional descriptive information. 4. Keep answers concise and focused on the core information. 5. Remove all unnecessary details about the current state or conditions. 6. Direction should be judged based on the viewpoint in the image. * up: away from the ground * down: toward the ground * forward: toward the camera (where the image was taken from) * backward: away from the camera * right: to the right side from the cameras perspective * left: to the left side from the cameras perspective TASK GUIDELINES: {task_description} Additional Style Requirements: - Use simple and clear English. - Focus on semantic correctness, not stylistic variation. - Keep sentences short and remove unnecessary details. - If multiple directions are involved, combine them clearly (e.g., Move down and slightly right). Task Description Examples Task Type: \"spatial\" - Focus only on relative positions and spatial relationships between objects. - Do not describe any action or movement. - Only describe the current spatial configuration. - Example Answer: \"The gripper is above the cup, offset to the right.\" Task Type: \"high_level\" - List major actions in chronological order (1., 2., 3., ...). - Each step should describe only the action, without mentioning the purpose. - Example Answer: \"1. Move the robot arm above the cup. 2. Lower the gripper to grasp the cup. 3. Lift the cup upward.\" Task Type: \"subtask\" - Focus on the immediate next meaningful subtask (a single self-contained action). - Describe WHAT needs to be done, not HOW to do it. - Only the immediate next step, not the final goal. - Example Answer: \"Move the gripper closer to the button.\" Task Type: \"primitive\" - Specify only mechanical movements and gripper state changes. - Use the robots perspective for directions: - Describe only the very next physical movement. - Example Answer: \"Move down and slightly right.\" Figure 10: System prompt for sampling in ROBOT-R1 Bench 22 Evulation Rubric and LLM-as-Judge prompt Rubric 0 points: Meaning completely different from ground truth - Answer has completely different meaning from ground truth - Key concepts and ideas are misinterpreted - Contains information that contradicts ground truth 1 point: Partially matches but with significant meaning differences - Some key points match but main meaning is different - Contains significant misunderstandings of core concepts - Has some correct information but overall meaning is incorrect 2 points: Mostly matches in meaning but with minor differences - Main meaning and key points are correct - Minor details or expressions are different - Overall context and intent are preserved 3 points: Meaning is equivalent to or more detailed than ground truth - Any expression that conveys the same basic meaning as ground truth - Any description that leads to the same functional outcome - Any expression that maintains the same spatial relationship between objects - Any description that achieves the same goal through equivalent means - Any expression that preserves the core meaning while using different words - Any description that maintains the same context and intent - Any expression that describes the same target location and action Additional Notes: 1. Focus on semantic equivalence rather than exact wording 2. Different expressions are acceptable if they convey the same meaning 3. Consider the overall context and intent of the answer 4. Minor differences in expression are acceptable if the core meaning is maintained 5. Paraphrasing is considered perfect match if the meaning is preserved 6. Different ways of describing the same action (e.g., press vs flip switch) should be considered equivalent 7. The specific actor (robot vs human) should not affect the score if the action described is functionally equivalent 8. Different ways of expressing the same spatial relationship (e.g., over vs toward) should be considered equivalent 9. Focus on whether the answer achieves the same functional outcome as the ground truth 10. Consider the answer as 3 if it describes the same action or state using different but equivalent words System Prompt You are an expert in evaluating the consistency between models answer and ground truth answer. Please assign score between 0-3 based on the given rubric and explain the reason in detail. You must respond in the format Score: [0-3] Reason: [explanation] in single line. You may use line breaks, but Score and Reason must be clearly separated and identifiable. User Prompt Please evaluate how well the models answer matches the ground truth answer. Evaluation Criteria: {rubric} Question: {question} Models Answer: {model_answer} Ground Truth Answer: {ground_truth} Please assign score between 0-3 and explain the reason in detail. Figure 11: Evaluation rubric and LLM-as-Judge prompt for model answer assessment. (a) Avg reward change during ROBOT-R1 training (b) Response length change during ROBOT-R1 training Figure 12: Training metrics for ROBOT-R1 across two learning approaches: Open-End QA and MCQA. Open-End QA approach directly generats next waypoint values and use L1 distance for reward which is define between the predicted and ground-truth states. We conducted 2 epoch training for Open-End QA, and 1 epoch training for MCQA (note that all models reported in the paper are based on 1 epoch training). Due to differences in dataset sizes between the two tasks, the graphs are plotted based on training steps. (a) Shows accuracy metrics for the MCQA task (left axis) and 1 - L1 distance for Open-End QA (right axis). (b) Shows mean response length over training steps for both task types. 24 Question: You are Franka Robot Assistant: Task Planning and Execution System Task description: lift the lid off the pan [...] Current Robot State: Position [0.265, -0.112, 1.154] Choice Question: Based on the provided image and current robot state, predict the next waypoint position [x, y, z]. Choose the most accurate option: [[A]] [0.072, 0.243, 0.996] [[B]] [-0.07, 0.247, 0.937] [[C]] [0.254, -0.197, 0.894] [[D]] [0.187, -0.065, 1.144] [...] GT Answer: [[C]] Qwen2.5-VL-Ins Answer: <think> To predict the next waypoint position for the robot to lift the lid off the pan, we need to consider the current position and the desired action. The robot is currently positioned above the pan, and the lid is on top of the pan. The goal is to lift the lid, which means the robot should move closer to the lid while maintaining position that allows it to grasp the lid. Lets analyze the options: Option A: [0.072, 0.243, 0.996] - positioned above the lid, reasonable for grasping action Option B: [-0.07, 0.247, 0.937] - further to the left, not efficient path Option C: [0.254, -0.197, 0.894] - further back, not aligned with lid position Option D: [0.187, -0.065, 1.144] - further up, not necessary for grasping Option has and coordinates close to the lids position, and the z-coordinate is slightly lower than the current position, which is necessary for the robot to reach the lid. Therefore, the most accurate option is A. </think> <answer> [[A]] </answer> reward: 0 ROBOT-R1 Answer: <think> To predict the next waypoint position for the robot to lift the lid off the pan, we need to consider the current position of the pan and the lid, as well as the robots current position and orientation. The lid is currently on the pan, and the robot is positioned above the pan. Given the current robot position [0.265, -0.112, 1.154], the pan is located close to the robot, and the lid is on top of the pan. We need to move the robot to position where it can grasp the lid and lift it off the pan. The most accurate position to predict for the next waypoint would be one that allows the robot to reach the lid on the pan. Looking at the coordinates of the pan and the lid, the most suitable position for the robot to reach the lid would be position above the lid, but not directly on top of it, to ensure the robot can grasp the lid without hitting the pan. The option [[C]] [0.254, -0.197, 0.894] is the closest to the lid while still being above it, making it the most accurate choice for the next waypoint. </think> <answer> [[C]] </answer> reward: 1 Figure 13: Example of model response (MCQA) 25 Question: You are Franka Robot Assistant: Task Planning and Execution System Task description: chuck way any rubbish on the table rubbish [...] Current Robot State: Position [0.275, -0.009, 1.448] [...] Lets determine the next robot state to execute task GT Answer: [0.179, -0.026, 0.846] Qwen2.5-VL-Ins Answer: <think> The task is to chuck away rubbish on the table. From the visual input, we can see that there are two red objects (possibly apples) and crumpled piece of paper on the table. The black trash bin is present, indicating where the rubbish should be placed. The robots gripper dimensions need to be considered to determine if it can pick up the paper. The gripper can easily pick up the paper, but the apples are larger and may require different strategy or second task. For this task, the focus is on the crumpled paper. The robots current position is [0.275, -0.009, 1.448]. The next step would be to move towards the crumpled paper, pick it up, and then move it to the trash bin. 1. Move the robots arm towards the crumpled paper. 2. Open the gripper to pick up the crumpled paper. 3. Move the gripper to the trash bin. 4. Close the gripper and drop the paper into the trash bin. 5. Move the robots arm back to its starting position. Lets calculate the coordinates for the paper and the trash bin: - The crumpled paper is located at approximately [0.2, -0.1, 0.75]. - The trash bin is located at approximately [0.4, -0.1, 0.75]. The robot will move from its current position to the paper and then to the trash bin. </think> <answer> [0.2, -0.1, 0.75] </answer> L1 distance between target: 0. ROBOT-R1 Answer: <think> The task is to remove any rubbish from the table. Observing the image, there are two apples and piece of paper on the table. The paper appears to be the \"rubbish\" as it is not food item. The robots gripper is positioned above the table, and the basket is nearby, suggesting it can be used to collect the rubbish. The robot needs to grab the paper and drop it into the basket. The robots current position is above the paper, and it needs to move closer to the paper to grab it. Once the paper is picked up, it should be moved to the basket and released. Steps: 1. Move the robots gripper closer to the paper. 2. Grasp the paper. 3. Move the paper towards the basket. 4. Release the paper in the basket. </think> <answer> [0.275, -0.009, 0.85] # Move closer to the paper </answer> L1 distance between target: 0.117 Figure 14: Example of model response (Open-End QA)"
        }
    ],
    "affiliations": [
        "KAIST",
        "Real World Inc.",
        "UC Berkeley",
        "Yonsei University"
    ]
}