{
    "paper_title": "GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation",
    "authors": [
        "Zhiyuan Yan",
        "Junyan Ye",
        "Weijia Li",
        "Zilong Huang",
        "Shenghai Yuan",
        "Xiangyang He",
        "Kaiqing Lin",
        "Jun He",
        "Conghui He",
        "Li Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide a complete speculation on GPT-4o's overall architecture. In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 2 8 7 2 0 . 4 0 5 2 : r GPT-ImgEval: Comprehensive Benchmark for Diagnosing GPT4o in Image Generation Zhiyuan Yan1,3*, Junyan Ye2,4*, Weijia Li2 , Zilong Huang2, Shenghai Yuan1,3, Xiangyang He6, Kaiqing Lin5, Jun He2, Conghui He4, Li Yuan1 Equal Contributors, Corresponding Authors 1Peking University, Shenzhen Graduate School, 2Sun Yat-sen University, 3Rabbitpre AI, 4Shanghai AI Laboratory, 5Shenzhen University 6The Hong Kong University of Science and Technology (Guangzhou), {zhiyuanyan@stu.,yuanli-ece@}pku.edu.cn, yejy53@mail2.sysu.edu.cn, liweij29@mail.sysu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "The recent breakthroughs in OpenAIs GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4os performance across three critical dimensions: (1) generation quality (assessed through the GenEval dataset), (2) editing proficiency (measured via the Reason-Edit dataset), and (3) world knowledge-informed semantic synthesis (evaluated using the WISE dataset). Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4os generated data, we propose classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide complete speculation on GPT-4os overall architecture. In addition, we conduct series of analyses to identify and visualize GPT-4os specific limitations and the synthetic artifacts commonly observed in its image generation. We also present comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4os outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in multimodal large language models (MLLMs) have brought remarkable progress in unified vision-language understanding and generation [48, 50, 29, 7, 28, 52, 44, 37, 58, 12, 38, 8, 9, 16]. Among these, OpenAIs newly released GPT-4o1 (where the \"o\" stands for \"omni\") has attracted widespread attention due to its surprising proficiency in image generation, editing, and 1https://openai.com/index/gpt-4o-system-card/ Figure 1: Commonly used pipelines for unified image generation and understanding, and potential decoder architectures of GPT4os image generation choice. The complete speculation architectures can be seen in the Figure 7. vision-language reasoningall within single unified architecture. With the increasingly growing use of GPT4o in real-world applications such as digital content creation, and interactive assistants, it becomes increasingly critical and necessary to systematically evaluate its image generation capabilities, weaknesses, failure cases, and other related problems in generative settings. To fill this gap, we present GPT-ImgEval, the first comprehensive benchmark designed to evaluate GPT-4os capabilities in image generation. The overall workflow of our benchmark is illustrated in Figure 2. We focus on three core image-generation tasks: text-to-image generation [23, 11, 26, 5, 33], instruction-based image editing [43, 4, 57, 1, 55, 56, 46, 19], and world knowledge-informed semantic synthesis [20, 30, 14]. For each task, we adopt either existing or purpose-built benchmarks: GenEval [17] for text-to-image generation, Reason-Edit [21] for instruction-guided editing, and WISE [31] for evaluating knowledge-grounded semantic understanding. Across all three tasks, GPT4o demonstrates strong performance, showcasing accurate compositional reasoning, fine-grained attribute control, and nuanced understanding of real-world context. Beyond benchmark evaluations, we conduct deeper analyses to uncover GPT-4os potential architectural choices. Specifically, we first explore whether GPT-4o relies on diffusion-based or autoregressive decoder head (see Figure 1). To this end, we propose model-based classification method, where standard binary classifier is trained to distinguish between images generated by the two paradigms, and then applied to GPT-4os outputs. Interestingly, the classifier consistently classifies GPT-4os images as diffusion-based, providing empirical evidence that GPT-4o may internally use diffusion head for image decoding. Then, based on empirical observations and analysis of the generated images, we further infer and examine the potential visual encoders, and subsequently propose the complete candidate architectures of GPT-4o (Figure 7). In addition, our large-scale evaluation reveals several limitations in GPT-4os generation process. These include inconsistencies in preserving original content during editing, difficulties in controlling image proportions, automatic cropping effects, high-resolution and over-refinement limitations, challenges in handling complex scenes, occasional color bias, and limitations in generating nonEnglish text. These findings provide valuable insights for future model improvements. Furthermore, we conduct comparative analysis of multi-round image generation between GPT-4o and Gemini 2.0 Flash2, strong commercial model recently released by Google. Our comparison focuses on four distinct aspects: consistency across multiple edits, instruction comprehension, support for multi-turn editing interactions, and response speed. Finally, we also briefly discuss the safety and detectability of GPT-4o-generated images. Interestingly, images generated by GPT-4o are easily detected by state-of-the-art image forensics models. This is likely due to its use of super-resolution pipeline, which amplifies upsampling interpolation artifacts and makes the images more susceptible to detection by forensic models, or because they contain noticeable watermark-like features. 2https://aistudio.google.com/prompts/new_chat 2 Figure 2: The overall workflow of our GPT-ImgEval, consisting the GPT-4o Image generation, Evaluation, and Analysis. In summary, our work makes the following key contributions: We present GPT-ImgEval, the first benchmark to quantitatively and qualitatively evaluate GPT-4os image generation capabilities across three well-established benchmarks, including text-to-image generation, editing, and comprehension-guided generation. Our comprehensive results highlight the superior image generation and comprehension capabilities of GPT4o over previous models. Based on the benchmarking results, we conduct an in-depth analysis that includes: (1) an investigation into the potential underlying architecture of GPT-4o through classifier-based image analysis, and (2) systematic empirical study of its weaknesses, including common failure modes and generation artifacts. We further provide comparative study of multi-round image editing capabilities between GPT-4o and Gemini 2.0 Flash. Additionally, we explore the AIGC safety issue by assessing the detectability of GPT-4o-generated images using existing SOTA image forensic models, revealing that such outputs remain distinguishable due to visible artifacts introduced during upsampling."
        },
        {
            "title": "2 GPT-ImgEval Evaluation Benchmark",
            "content": "2.1 GPT-4o Image Generation Setup Dataset. In this work, we evaluate GPT-4os image generation capabilities using three core datasets: GenEval [17], Reason-Edit [21], and WISE [31]. Traditional automatic evaluation metrics (such as FID or CLIPScore) primarily measure overall image quality or text-image alignment, but they are not well-suited for fine-grained or instance-level analysis. (1) GenEval adopts an object-centric framework to assess compositional image attributes, including object co-occurrence, spatial arrangement, counting, and color consistency. This makes it well-suited for evaluating GPT-4os ability to control image synthesis based on textual input. (2) Reason-Edit is dataset specifically designed for text instruction-based image editing. It covers seven distinct types of editing challenges, testing the models capability in spatial understanding, size manipulation, color changes, and commonsense reasoning. (3) WISE serves as benchmark for world knowledge-informed semantic evaluation, going beyond simple word-to-pixel mappings. It requires models to generate images grounded in real-world knowledge, including cultural context, temporal and spatial reasoning, and scientific understanding. Automation Scripts. As of April 3, 2025, GPT-4o does NOT offer an official API for imagegeneration tasks. To address this limitation, we develop custom automation scripts that interact directly with the GPT-4o web interface. These scripts simulate user input to automatically submit prompts and retrieve the generated images, enabling us to perform large-scale and repeatable evaluations of the models image generation capabilities. In order to reduce the interference of 3 Figure 3: Examples of generation results of GPT4o using GenEval [17], covering single object, two objects, counting, colors, position, and attribute binding. the same window context on the model capabilities, the image synthesis corresponding to each prompt is to reopen the window. To support the broader research and developer community, we have open-sourced our automation scripts at https://github.com/PicoTrex/GPT-ImgEval to facilitate similar workflows for others who wish to conduct automated testing or integrate GPT-4o into their own pipelines. 2.2 Text-to-Image Generation Quantitative Results. Results summarized in Table 1 evaluate text-to-image (T2I) generation on GenEval [17] across two main model categories: (1) diffusion-based approaches using frozen text encoders for direct prompt-to-image generation, and (2) methods that leverage LLMs or MLLMs to enhance the generation process. According to the table, GPT4o achieves the highest overall score of 0.84, largely outperforming both the frozen text encoder methods and the LLM/MLLMenhanced approaches. We observe that ChatGPT-4o demonstrates clear performance advantage even when compared to the state-of-the-art reasoning-based method GoT [13]. It achieves score of 0.85 on counting tasks, 0.92 on color recognition, 0.75 on spatial localization, and 0.61 on attribute binding. These results highlight the superior capabilities of GPT-4o in spatial reasoning and attribute binding, underscoring its effectiveness in text-guided image generation. Qualitative Results. Figure 3 presents qualitative examples of GPT-4os compositional text-toimage generation capabilities across six core evaluation categories in the GenEval benchmark. In the Single Object and Two Objects tasks, GPT-4o accurately generates clear, well-defined objects corresponding to the prompt (e.g., \"a photo of banana\" or \"a photo of two clocks\"). In Counting, it successfully renders the correct number of items, such as \"three sports balls\" or \"three handbags,\" demonstrating reliable numerical understanding. The Colors examples show GPT-4os ability to associate specific color attributes with the correct objects (e.g., \"a photo of blue TV\" and \"a photo of black backpack\"), while the Position examples (e.g., \"a carrot left of an orange\" and \"a cow left of stop sign\") highlight its competence in spatial reasoning and object layout. Finally, the Attribute Binding prompts challenge the model to correctly associate attributes or relationships with multiple objects. GPT-4o handles this effectively, producing well-formed scenes such as \"a photo of computer mouse and spoon\" and \"a photo of frisbee and couch\" without misplacing or merging entities. These examples collectively demonstrate GPT-4os ability to interpret complex compositional prompts and generate coherent, semantically accurate, and visually pleasing imagesindicating strong multimodal reasoning and planning capabilities. 4 Table 1: Evaluation of text-to-image generation on GenEval [17]. Obj.: Object. Attr.: Attribution. Single Obj. Two Obj. Counting Colors Position Attr. Binding Method Architecture Overall Frozen Text Encoder Mapping Methods SDv1.5 [36] SDv2.1 [36] SD-XL [34] DALLE-2 [35] SD3 (d=24) [10] Diffusion Diffusion Diffusion Diffusion Diffusion LLMs/MLLMs Enhanced Methods LlamaGen [39] Chameleon [41] LWM [27] SEED-X [15] Emu3-Gen [48] Janus [50] JanusFlow [29] GoT [13] AR AR AR AR AR AR AR (Diffusion) AR+Diffusion Head GPT-4o Unknown 0.43 0.50 0.55 0.52 0.62 0.32 0.39 0.47 0.49 0.54 0.61 0.63 0. 0.84 0.97 0.98 0.98 0.94 0.98 0.71 - 0.93 0.97 0.98 0.97 0.97 0.99 0.99 0.38 0.51 0.74 0.66 0.74 0.34 - 0.41 0.58 0.71 0.68 0.59 0. 0.92 0.35 0.44 0.39 0.49 0.63 0.21 - 0.46 0.26 0.34 0.30 0.45 0.67 0.85 0.76 0.85 0.85 0.77 0.67 0.58 - 0.79 0.80 0.81 0.84 0.83 0. 0.92 0.04 0.07 0.15 0.10 0.34 0.07 - 0.09 0.19 0.17 0.46 0.53 0.34 0.75 0.06 0.17 0.23 0.19 0.36 0.04 - 0.15 0.14 0.21 0.42 0.42 0. 0.61 2.3 Image Editing. Quantitative Results. We also evaluate GPT4o on the image editing task using the ReasonEdit benchmark [21], benchmark for qualitatively evaluating image editing performance. Following the experimental setup in [13], we employ the GPT Score to evaluate the degree of instruction adherence and the consistency of non-edited regions in image editing tasks. As shown in the bar chart (Figure 4), GPT-4o significantly outperforms all existing image editing methods on the Reason-Edit benchmark, achieving remarkable score of 0.929. This represents substantial leap of +0.357 over the best-performing method prior to 2025 (SmartEdit, 0.572), highlighting the models powerful instruction-following ability and finegrained editing control. Compared to state-ofthe-art models like GoT (0.561), CosXL-Edit (0.325), and MagicBrush (0.334), GPT-4o sets new standard in text-guided image editing. GPT4o demonstrates strong performance in both instruction adherence and image generation quality. The sharp increase in performance demonstrates the potential of integrating large multimodal language models into image editing tasks. Moreover, GPT-4os image editing process often exhibits inconsistencies in dimensions, color tone, and other global properties. However, such discrepancies are frequently obscured under the GPT-eval Score evaluation framework, which may fail to adequately capture these variations and thus introduce bias in assessing the models true performance. Figure 4: Quantitative results of model editing under the Reason-Edit benchmark [21]. We compare the performance of GPT4o with seven other SOTA image editing models. We see that GPT4o significantly outperforms other models. Qualitative Results. We present qualitative comparison of image editing in Figure 5, which illustrates the qualitative superiority of GPT-4o across range of complex image editing instructions. For tasks such as object replacement (\"replace food contains most vitamin with an orange\"), object removal, and attribute-specific substitution (\"change the middle panda to cat\"), GPT-4o consistently produces semantically accurate, visually coherent, and contextually aware results. Compared to other methods like InstructPix2Pix, MagicBrush, and SmartEdit-7B, GPT-4o shows higher spatial consistency, better localization of edits, and minimal collateral modifications. Moreover, the overall image quality produced by GPT-4o significantly surpasses that of all previous methods. Notably, in the \"cat in the mirror\" example, only GPT-4o successfully edited the reflectionretaining the 5 Figure 5: Examples of model editing results. We visualize the qualitative results of GPT4o with the other four SOTA editing generation methods. We use the Reason-Edit [21] benchmark for evaluation. real-world background while generating tiger in the mirror with matching pose. This task requires fine-grained understanding of semantics and scene structure. 2.4 World knowledge-Informed Semantic Synthesis As existing research and evaluation standards predominantly focus on image realism and shallow text-image alignment, lacking comprehensive assessment of complex semantic understanding and world knowledge integration in text-to-image generation, in addition to the two above benchmarks, we further evaluate GPT4o on recent WISE benchmark [31]. Such tasks require image generation models to possess sufficient world knowledge and reasoning capabilities prior to generation. For instance, given the prompt Octopus behavior when facing danger, the model must understand the biological response of an octopus releasing ink. Similarly, the prompt colossal sculpture in Brazil, with outstretched arms overlooking the city below requires the model to recognize and generate the iconic Brazilian landmarkChrist the Redeemer atop Corcovado Mountain. Quantitative Results. As results in Table 2 indicate, GPT-4o significantly outperforms existing specialized T2I generation methods and unified MLLM-based approaches in terms of overall WiScore. GPT-4o combines exceptional world knowledge understanding with high-fidelity image generation, demonstrating dual strength in multimodal generation tasks. This performance gap can be attributed to GPT-4os strong capacity for world knowledge retention and reasoning, which enables effective integration of knowledge during the image generation process. The results suggest that, within current unified multimodal generation frameworks, the ability to understand and reason about the world does not inherently translate into the capability to visually represent such knowledge with sufficient fidelity and accuracyyet GPT-4o manages to achieve precisely that. Qualitative Results. We conduct qualitative comparison in Figure 6, illustrating GPT-4os superior performance across several subdomains of World Knowledge-Informed Semantic Synthesis. 6 Figure 6: Visual examples of generation results on the WISE benchmark [31]. We visualize the qualitative results of GPT4o under different evaluation scenarios, following the WISE benchmark. Table 2: Normalized WiScore of different models. Due to limited resources, we only select 200 prompts from WISE [31] for GPT4o* evaluation. We will update the full results later. Model Architecture Cultural Time Space Biology Physics Chemistry Overall FLUX.1-dev [23] FLUX.1-schnell [23] PixArt-Alpha [3] playground-v2.5 [24] SD-v1-5 [36] SD-2-1 [36] SD-XL-base-0.9 [34] SD-3-medium [10] SD-3.5-medium [10] SD-3.5-large [10] Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Dedicated T2I 0.48 0.39 0.45 0.49 0.34 0.30 0.43 0.42 0.43 0.44 0.58 0.44 0.50 0.58 0.35 0.38 0.48 0.44 0.50 0.50 Unify MLLM Emu3 [48] Janus-1.3B [50] JanusFlow-1.3B [29] Janus-Pro-1B [6] Janus-Pro-7B [6] Orthus-7B-base [22] AR AR AR (Diffusion) AR AR AR+Diffusion Head Orthus-7B-instruct [22] AR+Diffusion Head show-o-demo [52] show-o-demo-512 [52] vila-u-7b-256 [51] GPT4o* AR (Diffusion) AR (Diffusion) AR Unknown 0.34 0.16 0.13 0.20 0.30 0.07 0.23 0.28 0.28 0.26 0. 0.45 0.26 0.26 0.28 0.37 0.10 0.31 0.36 0.40 0.33 0.64 0.62 0.50 0.48 0.55 0.32 0.35 0.47 0.48 0.52 0.58 0.48 0.35 0.28 0.45 0.49 0.12 0.38 0.40 0.48 0.37 0.98 0.42 0.31 0.49 0.43 0.28 0.33 0.44 0.39 0.41 0. 0.41 0.28 0.20 0.24 0.36 0.15 0.28 0.23 0.30 0.35 0.93 0.51 0.44 0.56 0.48 0.29 0.34 0.45 0.47 0.53 0.52 0.45 0.30 0.19 0.32 0.42 0.15 0.31 0.33 0.46 0.39 0.98 0.35 0.26 0.34 0.33 0.21 0.21 0.27 0.29 0.33 0. 0.27 0.14 0.11 0.16 0.26 0.10 0.20 0.22 0.30 0.23 0.95 0.50 0.40 0.47 0.49 0.32 0.32 0.43 0.42 0.45 0.46 0.39 0.23 0.18 0.26 0.35 0.10 0.27 0.30 0.35 0.31 0.89 For example, given the prompt \"A bird of prey, national symbol of the United States\", GPT-4o correctly generates bald eagle. In response to \"The particular style of helmet worn by medieval knights during tournaments,\" it produces an accurate depiction of fully enclosed medieval helmet 7 Figure 7: We present complete architectural speculation, proposing four possible candidates that differ in their choice of visual encoder while all share diffusion-based head for image decoding. with narrow eye slits. For the prompt \"The child and the leaf are standing on both sides of the teeter-totter,\" GPT-4o demonstrates an understanding of weight imbalance by generating plausibly tilted seesaw. Overall, GPT-4o effectively infers the intended semantics behind the prompts and produces high-quality, semantically aligned images."
        },
        {
            "title": "3 Potential Architectures Behind GPT4o",
            "content": "We propose three plausible architectural hypotheses that GPT4o might use for image generation (Figure 1). The three hypotheses are motivated by both the commonly used existing unified architectures [48, 50, 29, 7, 28, 52, 44, 37, 58, 12, 38, 8, 9, 16]. In the community, the main argument about this topic is the choice of generation head (decoder for image generation), i.e., the choice between architecture-(a) and architecture-(b). Below, we first introduce the two architectures in detail, and then provide our analysis and empirical evidence for the discrimination. Hypothesis-1: An AR-based Architecture with Next-Scale Prediction. The first hypothesis posits that GPT-4o employs an AR-based architecture, as shown in Figure 1(b), which performs next-scale prediction, progressive refinement strategy that starts with generating low-resolution, blurry base image and incrementally enhances it to high-resolution final output. This design is inspired by recent AR generative approaches (e.g., [42, 18]) that scale generation resolution stage by stage. This hypothesis is reflected by the animation displayed in GPT-4os image generation interface, where we visually observe the image becoming sharper and more detailed over time. as Hypothesis-2: Hybrid Architecture with Diffusion-Based Head. An alternative hypothesis is that GPT-4o follows hybrid design, combining transformer-based illustrated in Figure 1(a). AR backbone with diffusion-based generation head, In this framework, the AR model first predicts sequence of intermediate visual tokens or latent representations, which are then used as conditioning input to diffusion model responsible for decoding the final image, i.e., token [transformer] [diffusion] image pixels. This hypothesis aligns with descriptions in the OpenAI system card and is consistent with recent efforts to combine the semantic strength of AR models with the visual fidelity of diffusion models. Interestingly, we identify an \"easter-egg\" \"evidence\" provided officially by OpenAI, as shown in Figure 8, which highlights the pipeline of \"token [transformer] [diffusion] image pixels\" for image generation. Figure 8: An \"easter-egg\" example officially provided by the OpenAI, which aligns the potential architecture-(a) in Figure 1. This approach offers compelling explanation for several behaviors observed in GPT-4o. The model exhibits high image quality, texture diversity, and plausible natural sceneshallmarks of 8 Figure 9: The overall workflow of the proposed model-based discrimination method. diffusion-based image generation. At the same time, it shows strong semantic alignment with prompts, suggesting the presence of an autoregressive stage that grounds visual content in language. The hybrid structure also helps explain the \"global shift\" issues during localized editing, as diffusion models can sometimes struggle to constrain changes to small regions, especially when the conditioning is weak or too coarse. Which Architectures are behind GPT4o? To investigate the possible architectures used in GPT4o, we propose model-based methodology to make the binary discrimination, as illustrated in Figure 9. The visual decoder choice is validated based on our empirical experiments. We further infer its potential visual encoder components based on its generated images. Below is the details discussion, respectively. For the visual decoder component, we conduct classification-based analysis. We first generate 10,000 images each, using an AR-based head and diffusion-based head, with identical prompts from the GenEval benchmark. Then, binary classifier is trained to distinguish between the two types of output. When tested on images generated by GPT-4o, the classifier consistently identifies them as diffusion-based. This provides strong empirical evidence supporting the hypothesis that GPT-4o employs diffusion head, offering new insights into its image-generation mechanism. For the visual encoder component, UniTok [28] mentions that applying VQ (Vector Quantization) [45] to images can impair the models comprehension ability. Therefore, we speculate that GPT-4o likely does not utilize VQ, and instead employs continuous tokens, which is somewhat similar to the approach used in MAR [25]. We do not have access to the exact architecture of GPT-4o; however, we propose four possible architectures, which are illustrated in Figure 7."
        },
        {
            "title": "4 Weakness Analysis",
            "content": "Based on our evaluation results and qualitative inspection of GPT-4os outputs, we identified several recurring artifacts that reveal the models current limitations in image generation and editing. Here, we summarize the key artifact categories observed in GPT-4os image generation process, highlighting areas where the model struggles to meet expectations for fidelity, consistency, and control. The summary of weaknesses is not limited to our evaluations on the three datasets mentioned above. In the following, we provide detailed breakdown of each artifact category. Inconsistency in image generation. During image generation, GPT-4o often struggles to perfectly reproduce the input image when no edits are required. Even when prompts explicitly specify \"no changes,\" the model may introduce subtle modifications. This is particularly evident in image dimensions, where the output may exhibit unpredictable aspect ratio changes or automatic edge cropping and rescaling. Such behavior poses significant limitations for applications that demand precise framing or spatial alignment based on the original image dimensions. 9 Figure 10: Failure Cases and Limitations of GPT-4o. We identify several scenarios in which GPT-4o may fail, along with common artifacts present in its generated images. High-resolution & Over-refinement Limitaions. As illustrated in Figure 10(b), GPT-4o exhibits potential bias toward performing super-resolution or image enhancement operations. Even when the prompt explicitly requests blurry or low-resolution image, the model frequently generates outputs with enhanced clarity and fine detail. This behavior suggests tendency to prioritize high-frequency visual information, which may stem from internal upsampling modules or biases in the training data. Consequently, GPT-4o struggles to intentionally produce blurred, defocused, or low-detail images, thereby limiting its effectiveness in reproducing certain artistic styles or intended visual effects. Moreover, the model often enriches images with excessive detailfor instance, accurately depicting fine wrinkles on Einsteins facefurther reflecting its inherent preference for high-detail synthesis. Brush Tool Limitations. Although GPT-4o integrates brush tool intended for localized image editing, the underlying process still involves regenerating the entire image. As result, even when only small region is edited, the output may exhibit unintended changes in global properties such as texture, color, or fine details. In contrast, tools like ComfyUI support true localized inpainting, offering greater stability and control in practical editing applications. Additionally, GPT-4o-generated images frequently exhibit noticeable warm color bias. In the absence of explicit prompt constraints, the model tends to favor palette dominated by yellow, orange, and warm lighting. While such outputs may appear visually appealing in certain cases, this bias limits the stylistic diversity of the generated images. The tendency likely stems from imbalanced color distributions in the training data or learned stylistic preferences inherent to large-scale datasets. Failures in Complicated Scenes Generation. Although GPT-4o demonstrates remarkable capabilities in generating complex scenes, it still faces significant challenges in producing coherent multi-person scenarios and object-human interactions. As illustrated in Figure 10(d), the yellow boxes highlight abnormal human poses or anatomical structures, while the red boxes indicate spatially implausible object overlaps. These limitations reflect the models difficulty in spatial reasoning and maintaining image consistency under high visual complexity. Non-English Text Capability Limitation. GPT-4o exhibits superior text generation capabilities, significantly outperforming other models, particularly in rendering English fonts with clarity and consistency. However, its performance in generating Chinese text within complex scenes remains limited. As illustrated in Figure 10(e), the model frequently produces errors in Chinese signage, such as incorrect fonts or unintended use of traditional characters. This indicates that GPT-4o still faces challenges in non-English text generation. The gap may be attributed to the imbalance between English and Chinese data in training, as well as the inherently greater structural complexity and contextual dependency of Chinese characters. 10 Figure 11: Multi-round generation comparison between GPT-4o and Gemini-2.0 Flash."
        },
        {
            "title": "5 More Discussion",
            "content": "5.1 GPT-4o vs. Gemini 2.0 Flash: Comparative Analysis of Multi-round Generation. To compare GPT-4o with another powerful commercial generation model, we conduct comparative evaluation of GPT-4o and Gemini 2.0 Flash with focus on image editing consistency, instruction comprehension, multi-turn image editing capabilities, and computational efficiency. Below are our key findings: Consistency Over Edits. Both models exhibit declining consistency with an increasing number of edits. However, GPT-4o performs significantly better than Gemini in this regard. For example, when instructed to change only the color of chair, GPT-4o correctly alters just the color, while Gemini may unintentionally change the chairs shape or even its position within the image. Instruction Comprehension. Neither model achieves 100% accuracy in following instructions. In test case involving computer desk, GPT-4o was asked to modify the chair, but instead removed decorative panel on the wall. Gemini exhibited more severe failure: not only did it remove the decorative panel, but it also unintentionally erased additional objects in the scene. Multi-turn Image Editing Dialogues. GPT-4o supports multi-turn image editing dialogues, enabling continuous interaction and refinement across multiple image states. In contrast, Gemini 2.0 Flash does not appear to natively support this feature, requiring manual re-uploading of the previous image at each step. Computational Efficiency. making it more suitable for applications requiring rapid responses. In terms of speed, Gemini 2.0 Flash is considerably faster than GPT-4o, 5.2 For Safety: Is GPT4o-generated Image Detectable? GPT-4o demonstrates impressive image-generation capabilities, often producing highly photorealistic results that may appear indistinguishable to the human eye. However, our analysis shows that these images are still identifiable by current forensic detectors. As shown in Table 3, most existing AIgenerated image detectorsincluding two SOTA models, Effort [53] and FakeVLM [49]achieve over 95% accuracy in detecting GPT-4o-generated images. This highlights that, despite their realism, GPT-4o outputs remain within the scope of existing SOTA detection models. 11 Table 3: Detection results on GPT4o-generated images. The tested fake images are generated using the prompts in GenEval [17], and the real ones are from [54]. Most detection models are trained on the GenImage dataset [59]. that the training set is from [49]. Detection Model CNN-spot [47] UnivFD [32] CLIP (LoRA) DRCT [2] NPR [40] FakeVLM [49] Effort [53] Accuracy (%) 73.81 75.58 77.81 88. 78.25 99.60 94.75 One potential source of detectability lies in its internal super-resolution process. We observe that GPT-4o consistently produces sharp, high-resolution outputs, even when explicitly prompted to replicate blurry or low-resolution images. For instance, when provided with blurry input image and asked to return it unchanged, GPT-4o instead produces sharpened, high-resolution version. This suggests built-in super-resolution mechanism. Supporting this, NPR [40]a forensic model designed to detect upsampling artifactsachieves 99% detection accuracy on GPT-4o samples. These findings imply that GPT-4o-generated images may contain distinct, easy-to-recognize artifacts introduced by post-processing steps such as upscaling. In addition to its technical characteristics, GPT-4o also enforces strong safety safeguards3. The model strictly avoids generating content involving children, recognizable faces, or copyrighted materials such as logosaligning with OpenAIs robust image safety policies. These restrictions not only enhance user safety but also demonstrate responsible design practices in generative AI deployment."
        },
        {
            "title": "6 Conclusion",
            "content": "This technical report introduces GPT-ImgEval, the first comprehensive benchmark for evaluating GPT-4os image generation capabilities across three key dimensions: (1) generation quality (via GenEval), (2) instruction-based editing (via Reason-Edit), and (3) comprehension-guided generation (via WISE). Based on these evaluations, we propose model-based analysis to infer GPT-4os underlying architecture and conduct detailed studies to identify its weaknesses and common failure patterns. We further compare GPT-4o with Gemini 2.0 Flash in multi-round image editing and assess the detectability of GPT-4o-generated images. We hope our work provides meaningful insights and comprehensive benchmark to inspire future research, promote reproducibility, and drive innovation in image generation and beyond."
        },
        {
            "title": "References",
            "content": "[1] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1839218402, 2023. [2] Baoying Chen, Jishen Zeng, Jianquan Yang, and Rui Yang. Drct: Diffusion reconstruction contrastive training towards universal detection of diffusion generated images. In ICML, 2024. [3] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [4] Liang Chen, Shuai Bai, Wenhao Chai, Weichu Xie, Haozhe Zhao, Leon Vinci, Junyang Lin, and Baobao Chang. Multimodal representation alignment for image generation: Text-image interleaved control is easier than you think. arXiv preprint arXiv:2502.20172, 2025. [5] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International conference on machine learning, pp. 16911703. PMLR, 2020. [6] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [7] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 3https://openai.com/index/introducing-4o-image-generation/ 12 [8] Zisheng Chen, Chunwei Wang, Xiuwei Chen, Hang Xu, Jianhua Han, and Xiandan Liang. Semhitok: unified image tokenizer via semantic-guided hierarchical codebook for multimodal understanding and generation. arXiv preprint arXiv:2503.06764, 2025. [9] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [11] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. [12] Lijie Fan, Luming Tang, Siyang Qin, Tianhong Li, Xuan Yang, Siyuan Qiao, Andreas Steiner, Chen Sun, Yuanzhen Li, Tao Zhu, et al. Unified autoregressive visual generation and understanding with continuous tokens. arXiv preprint arXiv:2503.13436, 2025. [13] Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, et al. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025. [14] Xingyu Fu, Muyu He, Yujie Lu, William Yang Wang, and Dan Roth. Commonsense-t2i challenge: Can text-to-image generation models understand commonsense? arXiv preprint arXiv:2406.07546, 2024. [15] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. [16] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. [17] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. [18] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. arXiv preprint arXiv:2412.04431, 2024. [19] Junjie He, Yifeng Geng, and Liefeng Bo. Uniportrait: unified framework for identity-preserving single-and multi-human image personalization. arXiv preprint arXiv:2408.05939, 2024. [20] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. [21] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 83628371, 2024. [22] Siqi Kou, Jiachun Jin, Chang Liu, Ye Ma, Jian Jia, Quan Chen, Peng Jiang, and Zhijie Deng. Orthus: Autoregressive interleaved image-text generation with modality-specific heads. arXiv preprint arXiv:2412.00127, 2024. [23] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [24] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. [25] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024. [26] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. [27] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv e-prints, pp. arXiv2402, 2024. [28] Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025. [29] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv preprint arXiv:2411.07975, 2024. [30] Fanqing Meng, Wenqi Shao, Lixin Luo, Yahong Wang, Yiran Chen, Quanfeng Lu, Yue Yang, Tianshuo Yang, Kaipeng Zhang, Yu Qiao, et al. Phybench: physical commonsense benchmark for evaluating text-to-image models. arXiv preprint arXiv:2406.11802, 2024. [31] Yuwei Niu, Munan Ning, Mengren Zheng, Bin Lin, Peng Jin, Jiaqi Liao, Kunpeng Ning, Bin Zhu, and Li Yuan. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. [32] Utkarsh Ojha et al. Towards universal fake image detectors that generalize across generative models. In CVPR, pp. 2448024489, 2023. [33] Yatian Pang, Peng Jin, Shuo Yang, Bin Lin, Bin Zhu, Zhenyu Tang, Liuhan Chen, Francis EH Tay, SerNam Lim, Harry Yang, et al. Next patch prediction for autoregressive visual generation. arXiv preprint arXiv:2412.15321, 2024. [34] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [35] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. [37] Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Llamafusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024. [38] Wei Song, Yuran Wang, Zijia Song, Yadong Li, Haoze Sun, Weipeng Chen, Zenan Zhou, Jianhua Xu, Jiaqi Wang, and Kaicheng Yu. Dualtoken: Towards unifying visual understanding and generation with dual visual vocabularies. arXiv preprint arXiv:2503.14324, 2025. [39] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [40] Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, Ping Liu, and Yunchao Wei. Rethinking the up-sampling operations in cnn-based generative network for generalizable deepfake detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2813028139, 2024. [41] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [42] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. [43] Xueyun Tian, Wei Li, Bingbing Xu, Yige Yuan, Yuanzhuo Wang, and Huawei Shen. Mige: unified framework for multimodal instruction-based image generation and editing. arXiv preprint arXiv:2502.21291, 2025. [44] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. [45] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [46] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. [47] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei Efros. Cnn-generated images are surprisingly easy to spot... for now. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 86958704, 2020. [48] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [49] Siwei Wen, Junyan Ye, Peilin Feng, Hengrui Kang, Zichen Wen, Yize Chen, Jiang Wu, Wenjun Wu, Conghui He, and Weijia Li. Spot the fake: Large multimodal model-based synthetic image detection with artifact explanation. arXiv preprint arXiv:2503.14905, 2025. [50] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation, 2024a. URL https://arxiv. org/abs/2410.13848, 2024. [51] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. [52] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [53] Zhiyuan Yan, Jiangming Wang, Zhendong Wang, Peng Jin, Ke-Yue Zhang, Shen Chen, Taiping Yao, Shouhong Ding, Baoyuan Wu, and Li Yuan. Effort: Efficient orthogonal modeling for generalizable ai-generated image detection. arXiv preprint arXiv:2411.15633, 2024. [54] Meiyu Zeng, Xingming Liao, Canyu Chen, Nankai Lin, Zhuowei Wang, Chong Chen, and Aimin Yang. Chameleon: On the scene diversity and domain variety of ai-generated videos detection. arXiv preprint arXiv:2503.06624, 2025. [55] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. [56] Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient and flexible control for diffusion transformer. arXiv preprint arXiv:2503.07027, 2025. [57] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. [58] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. [59] Mingjian Zhu, Hanting Chen, Qiangyu Yan, Xudong Huang, Guanyu Lin, Wei Li, Zhijun Tu, Hailin Hu, Jie Hu, and Yunhe Wang. Genimage: million-scale benchmark for detecting ai-generated image. Advances in Neural Information Processing Systems, 36:7777177782, 2023."
        },
        {
            "title": "7 Appendix",
            "content": "Supplemental Visual Examples of Generation Results. We also include an additional visual example generated by GPT-4o under the Reason-Edit [21] benchmark (see Fig.12), along with more examples of multi-round editing (see Fig.13). Figure 12: Examples of generation results of GPT4o using Reason-Edit [21]. Details of Model-based Discriminator Between VAR and Diffusion. To investigate the potential architecture of GPT-4o, we construct training dataset and train discriminator to distinguish between diffusion-based and autoregressive-generated images. The experimental setting is detailed as follows. Flux [23] and VAR-Infinity [18] are selected to represent diffusion-based and VAR-based image generators, respectively. Using Flux-1 [dev] and Infinity-8B with default settings, we generate images with resolution of 10241024 based on prompts from GenEval [17], generating 20 images per prompt with different random seeds. pre-trained CLIP-ViT-Base-16 model is fully fine-tuned over 10 epochs for binary classification using the AdamW optimizer (learning rate: 0.00001, weight decay: 0.0004). GPT-4o-generated images are created from the same prompts from GenEval to construct the test set and mitigate prompt bias. 16 Figure 13: Multi-round generation comparison between GPT-4o and Gemini-2.0 Flash."
        }
    ],
    "affiliations": [
        "Peking University, Shenzhen Graduate School",
        "Rabbitpre AI",
        "Shanghai AI Laboratory",
        "Shenzhen University",
        "Sun Yat-sen University",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}