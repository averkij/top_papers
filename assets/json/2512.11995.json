{
    "paper_title": "V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions",
    "authors": [
        "Chenrui Fan",
        "Yijun Liang",
        "Shweta Bhardwaj",
        "Kwesi Cobbina",
        "Ming Li",
        "Tianyi Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning."
        },
        {
            "title": "Start",
            "content": "V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions Chenrui Fan*, Yijun Liang*, Shweta Bhardwaj, Kwesi Cobbina, Ming Li, Tianyi Zhou University of Maryland, College Park {cfan42,yliang17,minglii}@umd.edu Project: https://github.com/tianyi-lab/VREX 5 2 0 2 2 1 ] . [ 1 5 9 9 1 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "While many vision language models (VLMs) are developed to passively answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of active exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, Visual Reasoning with multi-step EXploration (V-REX), which is composed of benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX casts the multi-step exploratory reasoning into Chain-of-Questions (CoQ) and disentangles VLMs capability to (1) Planning: breaking down an open-ended task by dynamically selecting chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning. 1. Introduction Various practical applications of vision language models (VLMs) need to perform sophisticated multi-step visual reasoning [2, 6, 19, 20, 22, 43, 45, 48] to solve the user queries. Recent studies reveal the weakness of existing VLMs on exploratory reasoning tasks, showing that they often rely on brute-force search in the input image to allocate the potential *These authors contributed equally to this work. Figure 1. Overview of Chain-of-Questions (CoQ). The left represents the manually formulated ground truth QA chain. The middle represents the Planning task, evaluating the models capability in selecting sub-questions that are helpful to answer the original question. The right represents the Following task, evaluating the models capability in answering each sub-question. objects of interest, and rarely adjust their plans to be adaptive to collected clues [4, 18, 32]. This weakness substantially limits the application of VLMs in challenging open environments where the goals cannot be fully specified at the very beginning but need progressive planning on the fly. For example, guessing the location based on street view image [22], detecting cheating from posted images [18], or simply complicated tasks [34]. Tasks like these require multiple rounds of active exploration, sub-goal proposal, and answering the sub-questions to collect sufficient contextual clues and identify the final targets, while poor exploration may significantly undermine or distract such reasoning processes. However, recent visual reasoning and benchmarks mainly focus on math problems with visual contexts [23, 44, 46] or puzzle games with toy environments [24, 33, 37], which 1 Figure 2. An example from V-REX with corresponding planning and following tasks. In the planning task, the model is given the original question and asked to select sub-question in each step that is necessary and helpful for solving the original question. In the following task, the model is asked to answer the ground truth sub-questions step-by-step. can be addressed by single-round QA or language space reasoning. These questions are usually straightforward with highly specified instructions or targets. In addition, their input prompt and context are sufficient for the models to make concrete plan in advance and execute it without further exploration. Hence, they do not heavily rely on exploration skills, which, in contrast, are critical to open-ended tasks that need to actively collect clues given only high-level initial intents. Moreover, most of them merely evaluate the final answers and do not investigate the intermediate exploration steps, which can reflect important capabilities or fundamental flaws, such as shortcut solutions or redundancy, making the whole evaluation suite black box. Hence, it is still unclear how each step affects the subsequent reasoning and the final answer, how robust existing VLMs are to intermediatestep errors, and whether they can recover from these errors. To bridge the gap, we investigate the quality and impact of the intermediate exploration steps and develop novel evaluation suite, Visual Reasoning with multi-step EXploration (V-REX), which is composed of challenging visual reasoning tasks requiring native multi-step exploration and subgoal decomposition. Specifically, we formulate the exploratory reasoning process in novel form called Chainof-Questions (CoQ), which represents sequence of interconnected sub-questions and corresponding answers that serve as the precondition for the final answer, as shown in Figure 1 (left). In CoQ, model is prompted to ask sequence of sub-questions and answer them sequentially to collect sufficient context, thereby creating QA chain progressively on the fly to address the original question. key challenge of evaluating multi-step exploratory reasoning is the prohibitively large space for free exploration of VLMs. CoQ can reduce it to finite space with limited choices of questions and answers per step, providing tractable and controllable assessment. Specifically, CoQ enables us to disentangle the reasoning capabilities into Planning and Following when facing complex tasks that require multiple rounds of exploration with sub-goal proposals. The Planning of CoQ evaluates the models exploratory ability to dynamically break down the original open-ended task into sequence of subquestions that serve as the precondition for the final answer. For example, in Figure 2 (middle), when given the question Who is mainly responsible for the accident?, the model should raise exploratory questions about the conditions of the two vehicles, rather than questions about the number of vehicles. The Following of CoQ evaluates the models ability to answer the subquestions to collect sufficient contextual clues and identify the final targets, as shown in Figure 2 (right). An overview of CoQ is shown in Figure 1, where the two dimensions serve as the complementary parts of the exploratory reasoning process, indicating holistic and fine-grained diagnosis of the reasoning capabilities of VLMs. V-REX comprises 702 samples and 2, 504 questions spanning 4 reasoning categories and 15 scenarios as shown in Figure 3. Each sample contains 2 to 6 reasoning steps, with 3.57 steps on average. In our experiments, we extensively evaluate VLMs of different model families, ranging from open-source to proprietary models, including relatively small models and larger ones. The evaluation lead to some unrevealed observations and shed novel insights to improving the exploratory reasoning capability of VLMs. Contributions: We introduce V-REX, the first benchmark for assessing the multi-step exploratory reasoning capability of VLMs with novel form of Chain-of-Questions (CoQ). In our setting, the models planning and following abilities can be disentangled and evaluated separately, providing more fine-grained and interpretable evaluation of the reasoning capabilities of VLMs. Key Findings: 1. By following the hints of CoQ, VLMs consistently achieve better performance on final questions, demon2 Figure 3. Scenarios in V-REX. With various samples, V-REX spans 15 real-world scenarios across 4 reasoning categories (Deduction, Guessing, Navigation, & Retrieval), covering diverse settings such as diagrams, time estimation, GUI interpretation, and others. strating the importance of exploration in visual reasoning. 2. The scaling law holds for V-REX, and models of the same size show much less variance in Following performance than in Planning performance; this indicates need to improve VLMs exploratory abilities. 3. Following and Planning capabilities both contribute positively to the overall performance of the model. 4. Smaller models are better at Following than Planning while larger models have more balanced performance. 5. VLMs are better at recovering from failed Planning than from failed Following. 2. Related Work 2.1. Exploratory Visual Reasoning Inspired by recent advances on the reasoning ability of LLMs [14, 29], efforts have been made to replicate similar success on the multimodal domain. [36, 38, 49] adopt GRPO algorithm to train R1-style VLMs. [15] designs an effective training pipeline to elicit VLMs ability to explore in visual space and general long and complex chain-of-thought. Meanwhile, multiple benchmarks have been proposed to evaluate the visual reasoning ability of VLMs. VisuLogic [46] measures the ability of VLMs on genuine vision-centric reasoning tasks. ZeroBench [34] evaluates VLMs on extremely difficult tasks. CaughtCheating [18] investigates the VLMs capability on cheating detection tasks, revealing the weakness of existing models on real detective-level tasks. More benchmarks evaluate VLMs visual reasoning abilities from different aspects, including cognitive reasoning [31], visual grid reasoning [33], visual comparison reasoning [3], cultural reasoning [35], multi-image reasoning [9], and colorrelated reasoning [21]. However, although these benchmarks cover wide range of visual reasoning tasks, they mainly focus on the exploration skills of VLMs on answer space and neglect the exploration of question space, that is, the models ability to propose helpful sub-questions, set subgoals that would guide the reasoning process effectively. Our benchmark, V-REX, fills the gap by evaluating both exploration of answer space and question space in disentangled manner. 2.2. Evaluation with Intermediate Steps Most visual reasoning benchmarks [9, 31, 34, 46] evaluate the ability of VLMs in end-to-end approach. Given complex visual reasoning tasks, the models are prompted to directly produce the final answer, and performance is measured solely by answer accuracy. Chen et al. [5] introduces process reward model (PRM) to score intermediate reasoning steps. But they rely on Monte-Carlo estimation and LLM-as-aJudge as PRM for intermediate steps, which are not reliable for benchmark evaluation. LlamaV-o1 [39] adopts finegrained evaluation framework that compares each generated reasoning step to ground-truth trace, but it also relies on LLM-based judgments when assessing alignment or correctness. By breaking down the complex visual reasoning tasks into multiple sub-tasks and evaluating the intermediate steps, V-REX can more accurately assess the ability of VLMs to reason step-by-step in Planning and Following dimensions. 3. Exploratory Reasoning in Visual Space 3.1. Chain-of-Questions (CoQ) To investigate the intermediate exploratory reasoning processes of vision language models (VLMs), we conceptualize their reasoning pathways in the CoQ format as QA chain with sequence of interconnected sub-questions Qt and 3 corresponding answers AQt. The reasoning process can then be denoted as: {(Q1, AQ1), (Q2, AQ2), ..., (QT , AQT )}, where is the number of reasoning steps. This formulation helps formulate the models exploratory reasoning process as an iterative alternation between generating exploratory sub-questions and providing corresponding answers. It allows us to decouple the models reasoning capability into two fundamental dimensions: the proficiency in formulating informative intermediate questions (Planning), and the accuracy in addressing these sub-questions (Following). However, evaluating these abilities in the infinite exploratory space is not feasible. It is challenging to design reliable evaluation protocol because it involves directly rating the quality of open-ended reasoning processes. To address this, we propose to evaluate the models ability to answer series of multiple-choice questions (MCQ) with the CoQ process, reformulating the evaluation into more tractable and controllable assessment in finite space. 3.2. Planning of CoQ An important dimension of exploratory reasoning capability is Planning: the capacity to strategically chart course of exploration through the reasoning space. Effective planning entails the models ability to dynamically adapt its sequence of inquiry based on evolving intermediate evidence and to accurately discern the most informative next step at each stage. As illustrated in the middle of Figure 2, we branch over the question node at each step of the pre-defined groundtruth QA chain by adding several uninformative, distracting questions. The planning evaluation then becomes asking the model to identify the most helpful sub-question at step conditioned on the previous CoQ history: Qt p(Qorigin, Q1, Q1 , Q2, Q2 , . . . , Qt1, Qt1 ) (1) where Qorigin is the original problem, Qt is the question selected by model at step t, and is the ground-truth Qt answer for Qt. For simplicity, we omit the notation of the image in the above equation. By directly providing the corresponding ground-truth answer to the model at Qt each step, we ensure that the model does not need to solve the sub-questions itself but only needs to decide which question to ask next. This design explicitly isolates the Planning ability from Following. 3.3. Following of CoQ For the Following dimension, our objective is to evaluate the models ability to accurately answer intermediate questions based on both visual cues from different regions of the image and knowledge obtained through preceding reasoning steps. Specifically, we present the VLM with pre-constructed, sequential chain of QA steps and require the model to provide answers at each stage en route to the final conclusion. As depicted in the right part of Figure 2, we introduce distractor options at each answer node, transforming each step into multiple-choice setting. Specifically, at reasoning step Figure 4. Statistics of V-REX, including question distributions (left) and reasoning-step distributions (right). in the chain, we augment the correct answer with several incorrect alternatives. The model is tasked with selecting the correct answer to question at step, conditioned on the image and the accumulated conversation history: , ) p(Q , . . . , t1, AQ 2, AQ 1, AQ , (2) AQ 2 t1 where is the ground-truth question at each step and AQ is the corresponding model-generated response. This methodology enables principled evaluation of the models capacity to faithfully follow an intended reasoning path. 4. V-REX Benchmark V-REX consists of 702 samples and 2, 504 questions, with each sample containing 2 to 6 reasoning steps and 3.57 steps on average. The detailed distributions of questions and reasoning steps are illustrated in Figure 4. The dataset spans 4 reasoning categories and 15 application scenarios, supporting fine-grained evaluation of VLMs exploratory reasoning capabilities. 4.1. Taxonomy To thoroughly evaluate exploratory reasoning, V-REX defines 4 reasoning categories where exploration plays cen4 tral role: Deduction, Guessing, Navigation, and Retrieval. Each category reflects different way VLMs explore and reason across multi-step problems. Deduction category measures models ability to discover, infer, and apply logical or causal relationships from structured information. It includes 4 application scenarios: (i) Flowchart, (ii) Pattern, (iii) Property, and (iv) Relationship. Guessing category involves uncertainty and incomplete information, requiring models to infer hidden or missing factors through hypothesis exploration. It includes 5 scenarios: (i) Responsibility, (ii) Intention, (iii) Location, (iv) Time, and (v) Topic. Navigation category measures the models ability to explore by traversing spatial layouts or procedural paths, planning step-wise movements through the scene while maintaining global consistency. It includes 4 scenarios: (i) Map, (ii) GUI, (iii) Traffic, and (iv) Trend. Retrieval category measures the models ability to locate, gather dispersed information by thoroughly exploring the visual input. It includes 2 scenarios: (i) Counting and (ii) Word Puzzle. 4.2. Data Curation We begin by elaborating on the methodology for constructing the ground-truth QA chains for each sample, followed by an explanation of how these chains are systematically adapted to probe the distinct following and planning capabilities of VLMs within the reasoning space. 4.2.1. Ground-Truth QA Chain As discussed in Section 3, we use QA chains to model the reasoning traces of VLMs when solving multi-step reasoning problems. We denote the ground-truth QA chain as sequence {(Q 1, 2, ), ..., (Q , )}, where Q 1 2 is the final question and is the final answer, valid QA chain should have the following properties: ), (Q Helpfulness. [1, 1], > t, is helpful t2. It means that every question should t for answering be helpful for the following questions. Correctly Ordered. [2, ], < t, t does not . It means that every question should depend on not depend on its subsequent question. We rely on human experts to collect high-quality groundtruth QA chains on visual reasoning tasks across all categories. The images are sourced from websites and publicly available benchmarks, with detailed sources provided in Appendix 7. Specifically, five PhD-level annotators are initially assigned three scenarios each for ground-truth QA chain construction. We then have two rounds of cross-verification. For each round, annotators are randomly shuffled to three other unseen scenarios. They are required to ensure the validity of the reasoning chain, helpfulness of the intermediate steps, and make sure they are in the correct order. At the end 5 of each verification round, they would give feedback to the original annotator and the previous verifier. 4.2.2. Planning Task To evaluate VLMs exploratory ability in the question space, we construct samples for the Planning dimension based on ground-truth QA chains. We develop two-stage automatic generation pipeline, consisting of Distractor Construction and Question Integration, which leverages LLM for generation, filtering, and integration. Distractor Construction stage aims to create alternative questions that are contextually relevant but unhelpful for solving the final question. We design two complementary strategies to capture local and global variability in reasoning: step-level and chain-level distractor generation. In step-level generation, LLM (GPT-5) produces distractors for each reasoning step that are locally plausible but deliberately misleading. In contrast, chain-level generation considers the entire distracting QA chain holistically. These chains are self-consistent yet subtly deviate from the ground truth, allowing evaluation of whether models can distinguish globally misleading reasoning paths from the true reasoning chain. Question Integration stage aims to integrate the generated distractors with the ground-truth chain into unified dataset through automatic filtering and refinement, creating controlled benchmark for evaluating models exploration reasoning in complex question spaces. More details are provided in the Appendix 9. 4.2.3. Following Task For Following dimension, we manually create few plausible distractor choices for each answer step. These distractors are combined with the ground-truth answer to form series of multiple-choice questions. 4.3. Evaluation Metrics We use intermediate accuracy to measure both Following and Planning ability of the model. Specifically, for QA chain of steps, Planning ability is quantified as:"
        },
        {
            "title": "1\nT − 1",
            "content": "T 1 (cid:88) t=1 I[Q = Qt] (3) where is the ground-truth question at step and Qt is the models chosen question at step t. Similarly, Following ability is quantified as:"
        },
        {
            "title": "1\nT − 1",
            "content": "T 1 (cid:88) t=1 I[A = AQ ] (4) where I[] is the indicator function, AQ chosen answer for the ground-truth question and is the ground-truth answer. is the models at step t, Table 1. Performance of various VLMs (grouped by size) on V-REX. The best performance in each VLM group is highlighted in bold. Model Deduction Guessing Navigation Retrieval Average Deduction Guessing Navigation Retrieval Average Planning Following 31.5 37.1 42.4 31.9 44.4 52.7 63.1 47.9 62.5 58.8 62.5 69.6 53.0 59.1 74.7 66.8 70.1 53.5 66.2 69. 76.4 78.6 76.4 85.6 83.2 81.7 77.9 93.9 94.4 94.8 86.2 87.8 LLaVA-OV-1B [17] InternVL3-1B [50] InternVL3.5-1B [42] Qwen3-VL-2B-IT [47] Qwen3-VL-2B-Think [47] InternVL3-2B [50] InternVL3.5-2B [42] Qwen2.5-VL-3B [1] Qwen3-VL-4B-IT [47] Qwen3-VL-4B-Think [47] InternVL2.5-4B [8] InternVL3.5-4B [42] LLaVA-OV-7B [17] Qwen2.5-VL-7B [1] Qwen3-VL-8B-IT [47] Qwen3-VL-8B-Think [47] InternVL2.5-8B [8] InternVL3-8B [50] InternVL3.5-8B [42] InternVL3-9B [50] InternVL3-14B [50] InternVL3.5-14B [42] InternVL2.5-26B [8] InternVL2.5-38B [8] InternVL3-38B [50] InternVL3.5-38B [42] GPT-4o [30] GPT-5 [27] O1 [29] O3 [28] Gemini 2.0 Flash [13] Gemini 2.5 Flash [11] 5. Results 5.1. Main Results 37.1 47.3 43.5 37.5 60.6 53.8 60.8 59.3 65.4 83.4 57.9 65.4 42.7 67.6 84.7 86.7 75.1 44.5 69.6 81.6 86.4 73.0 71.1 85.0 91.3 85.3 69.6 91.8 93.2 94.4 93.4 89. VLMs: <7B 43.9 42.4 41.1 28.4 41.9 36.9 42.7 37.3 36.8 42.2 55.4 51.8 35.1 42.3 42.3 33.0 48.8 51.9 57.5 48.9 58.9 62.0 61.2 63.4 VLMs: 7B10B 35.4 36.7 46.8 48.7 50.6 40.1 47.3 57.5 48.0 57.1 71.6 68.0 66.9 51.2 64.1 73. VLMs: >10B 56.7 53.3 52.6 59.8 65.2 58.0 74.8 70.7 70.0 78.8 81.6 77.3 VLMs: Proprietary 63.7 58.1 61.3 62.1 54.6 49.6 72.7 84.4 85.3 86.1 80.0 77. 27.9 42.5 42.2 34.1 48.4 64.1 63.2 51.0 70.9 63.6 69.0 66.8 61.0 64.7 80.0 69.7 71.8 66.8 73.1 84.1 79.8 77.9 79.9 85.0 86.6 84.3 79.7 93.9 92.4 93.3 85.9 84.5 33.8 37.4 44.9 50.2 39.2 51.6 59.3 61.2 55.7 56.2 55.3 63.7 58.6 65.8 59.2 63.0 57.9 54.9 55.6 56. 68.8 64.9 64.4 71.3 68.1 65.9 73.1 76.6 73.0 80.2 74.3 69.8 49.3 50.2 45.2 61.0 51.6 60.8 51.5 57.7 63.8 57.8 60.9 58.7 62.1 64.5 64.2 60.3 59.9 58.8 61.0 66.3 65.9 62.3 61.0 69.0 65.2 63.0 78.0 86.2 80.4 83.0 67.6 74. 64.0 67.1 69.8 80.3 72.9 80.7 71.3 83.4 81.6 82.7 77.6 81.6 71.9 81.0 88.3 84.9 81.4 81.6 82.8 82.9 85.7 85.1 85.5 90.8 91.8 84.1 87.1 90.9 90.7 89.9 88.7 87.1 56.1 61.0 62.2 48.0 48.4 61.0 60.2 66.7 56.9 60.2 65.9 72.4 74.0 73.2 57.7 58.5 66.7 69.1 61.8 69. 69.1 72.4 69.9 74.8 69.1 64.2 67.5 84.6 74.0 83.7 78.9 78.0 50.8 53.9 55.5 59.9 53.0 63.5 60.5 67.2 64.5 64.2 64.9 69.1 66.6 71.1 67.3 66.7 66.5 66.1 65.3 68.6 72.4 71.2 70.2 76.5 73.5 69.3 76.4 84.6 79.5 84.2 77.4 77. across model scales. Smaller models (<7B and 7B10B) show highly variable strengths, with some excelling in Deduction or Guessing and others in Navigation or Retrieval. As the model size increases beyond 10B, the expertise distribution becomes more concentrated. On the Following task, InternVL2.5-38B achieves the best accuracy in 3 of 4 categories, and InternVL3-38B similarly dominates in 3 categories on the Planning task. Proprietary models such as GPT-5 and o3 also achieve top accuracy across multiple categories. These patterns suggest that larger models develop more unified and generalizable reasoning capabilities, rather than relying on specialized strengths in isolated categories. Table 1 summarizes VLM performance across the Following and Planning tasks, revealing consistent scaling trends and distinct strengths across model families. Here, the performance refers to Following and Planning abilities. Overall, model performance increases steadily with size. Larger models show stronger multimodal reasoning and coordination capabilities. GPT-5 and o3 achieve the highest average performance on both tasks. Notably, the performance gap between large open-sourced and proprietary models has narrowed considerably. Several large open-source VLMs match or even exceed proprietary models in some reasoning categories, particularly Deduction and Navigation. While zooming into different reasoning categories, the result reveals that VLMs exhibit distinct areas of expertise 6 distractive reasoning paths or omit essential reasoning cues in Planning task settings, ultimately disrupting their decisionmaking process. As result, instead of improving reasoning consistency, adding exploration steps may introduce additional cognitive noise that hinders accurate question or answer selection. The failure cases are shown in Appendix 13. Finding Scaling laws persist on V-REX, while performance variance among models of the same size is smaller in Following than in Planning. To investigate whether the general scaling law holds on our Following and Planning tasks, we plot the model abilities against parameter size for both tasks. As in Figure 6, colors denote the sample categories and marker shapes represent different model families. We also use dashed lines with shaded bands to aggregate the ability of models of the same scale. We observe an upward ability trend as the model size increases, suggesting that the scaling law continues to hold. However, the ability variance among model families of the same size is smaller for the Following task than for the Planning task, indicating that the Following ability is more stable and converged across model families. The relatively uniform following ability across models indicates that, given well-structured question, models of the same size can effectively leverage visual cues and contextual information to reach similar outcomes. This finding underscores the critical role of Planning as distinguishing factor in multi-step visual reasoning, particularly for problems that require more open-ended exploration strategies. Finding 3 Following and Planning abilities both contribute positively to the overall performance of the model. We examine how models Planning and Following abilities relate to its overall performance. Here, overall performance refers to model accuracy in directly answering the final question of each sample, which captures its end-to-end task competence. As depicted in Figure 7, both the Following-overall and Planning-overall correlations are markedly strong, indicating that improvements in either dimension tend to yield gains in overall performance. Quantitatively, the Pearson correlation coefficient between Following ability and overall performance is 0.948, while that between Planning ability and overall performance is 0.858. These results indicate that Following ability remains primary determinant of model end-to-end capability. However, the larger variance in Planning ability across VLMs suggests that current models exhibit more pronounced disparities in their capacity for strategic reasoning. Figure 5. The ratio of change on final accuracy brought by CoQ evaluation across all VLMs. The X-axis denotes task categories, and the Y-axis represents the performance changing ratio. 5.2. Main Findings Finding 1 By following the hints of CoQ, VLMs consistently achieve better performance on final questions, demonstrating the importance of exploration in visual reasoning. To assess the effect of the hints in CoQ in VLMs, we compare the accuracy performance on final questions with and without these hints across all models. For quantitative analysis, we compute the performance changing ratio for each VLM and reasoning category as (AccCoQ Acc)/Acc. The detailed accuracies on 4 categories for each VLM can be found in Appendix 11. Figure 5 illustrates the distribution of performance changing ratios across all models for the Following and Planning tasks. Most ratios are positive, demonstrating that the model achieves higher accuracy on final questions when intermediate steps are introduced. This indicates that the partial hints we provide to the models on both tasks (i.e., questions in Following task and answers in Planning task) help VLMs in reasoning, which justifies the validity of our manually constructed ground-truth QA chains. The Retrieval category shows relatively minor improvements in both tasks, likely because it depends more on factual recall and direct visual matching than on complex reasoning. Consequently, the CoQ process contributes less to tasks where success relies primarily on precise information retrieval rather than the hierarchical organization of reasoning processes. Another possible reason is that the human-designed reasoning paths may diverge from the models intrinsic retrieval strategy. This mismatch suggests that while hints in CoQ effectively help structures with deductive or sequential reasoning, it may not optimally support tasks dominated by information lookup or dense visual-textual association. Moreover, when comparing the Following and Planning tasks, few VLMs exhibit decreased performance when provided with CoQ hints across all categories. For some models, the introduction of intermediate steps can generate Figure 8. The ratio between Following and Planning ability at different model sizes. The abilities of Planning and Following are more balanced when the ratios are closer to 1.0. Smaller models are better at Following than Planning while larger models have more balanced ability. stantially greater proficiency in Following compared to Planning. As model size increases, however, this ratio approaches unity and stabilizes, reflecting that larger models achieve more balanced performance across both reasoning dimensions. This trend suggests that scaling model capacity enhances not only overall performance but also enables models to more evenly develop planning-oriented reasoning capabilities alongside Following skills. Finding 5 VLMs are better at recovering from failed planning steps than from failed following steps. We assess models capability to recover from failure, defined as their ability to reach the correct final answer despite making at least one incorrect intermediate step. This ability is crucial in real-world settings, where models often take suboptimal reasoning paths. Table 2 reports failure-recovery performance for models with more than 10B parameters. The results indicate that all models exhibit some ability to recover, with recovery from failed planning generally exceeding recovery from failed following. This suggests that models are relatively more robust to suboptimal plans. Distracting questions provide fewer clues but do not necessarily derail the final answer. In contrast, errors in the answer space are more likely to propagate to an incorrect final answer. Another notable observation is that recovery from the failed following process does not differ significantly between smaller open-source models and larger proprietary models. By contrast, recovery from failed planning is markedly stronger in larger proprietary models. This indicates that larger proprietary models are less susceptible to suboptimal plans and to distraction from uninformative cues along the reasoning chain. Figure 6. Following and Planning ability on models of different sizes (logarithmic scale for the x-axis). Overall, the models ability on both tasks positively correlates with model size. Notably, the variance of following ability among same-sized models is smaller than that of Planning ability. Finding 4 Smaller models are better at Following than Planning while larger models have more balanced performance. Figure 8 presents the ratio of Following to Planning ability as function of model size. The results indicate that smaller models exhibit pronounced imbalance, with subFigure 7. Correlation between Following and Overall performance (left), and between Planning and Overall performance (right). Both Following and Planning abilities are positively correlated with the overall performance of models. Table 2. VLMs ability to recover from exploration failures. The percentages of each models responses that contain at least one wrong intermediate step but still arrive at correct final answers. Model From Failed Planning From Failed Following VLMs: >10B 65.9 65.5 66.9 69.2 65.1 65. VLMs: Proprietary 78.9 84.3 79.0 80.6 69.5 72.7 55.9 52.6 62.1 63.2 52.9 45.6 56.5 53.6 59.5 57.3 59.4 51.8 InternVL3-14B InternVL3.5-14B InternVL2.5-26B InternVL2.5-38B InternVL3-38B InternVL3.5-38B GPT-4o GPT-5 o1 o3 Gemini 2.0 Flash Gemini 2.5 Flash 6. Conclusion We introduced V-REX, benchmark that evaluates multi-step exploratory visual reasoning through structured Chain-of-Questions (CoQ) formulation. By disentangling reasoning into Planning and Following, V-REX provides fine-grained insights that are hidden in end-toend evaluation. Experiments on different VLMs reveal consistent scaling trends, strong correlations between intermediate abilities and final performance, and notable imbalance in smaller models, favoring Following over Planning. Our findings highlight exploratory reasoning, especially effective planning, as key challenge for future VLMs and establish V-REX as foundation for advancing deliberate, stepwise visual reasoning."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5vl technical report, 2025. 6, 19 [2] Tianyi Bai, Zengjie Hu, Fupeng Sun, Qiu Jiantao, Yizhen Jiang, Guangxin He, Bohan Zeng, Conghui He, Binhang Yuan, and Wentao Zhang. Multi-step visual reasoning with visual tokens scaling and verification. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. 1 [3] Jie Cai, Kangning Yang, Lan Fu, Jiaming Ding, Jinlong Li, Huiming Sun, Daitao Xing, Jinglin Shen, and Zibo Meng. Comparebench: benchmark for visual comparison reasoning in vision-language models, 2025. 3 [4] Declan Iain Campbell, Sunayana Rane, Tyler Giallanza, C. Nicol`o De Sabbata, Kia Ghods, Amogh Joshi, Alexander Ku, Steven Frankland, Thomas L. Griffiths, Jonathan D. Cohen, and Taylor Whittington Webb. Understanding the limits of vision language models through the lens of the binding problem. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 1 [5] Honghao Chen, Xingzhou Lou, Xiaokun Feng, Kaiqi Huang, and Xinlong Wang. Unveiling chain of step reasoning for vision-language models with fine-grained rewards, 2025. [6] Jian Chen, Ming Li, Jihyung Kil, Chenguang Wang, Tong Yu, Ryan Rossi, Tianyi Zhou, Changyou Chen, and Ruiyi Zhang. Visr-bench: An empirical study on visual retrieval-augmented generation for multilingual long document understanding, 2025. 1 [7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. 19 [8] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 6 [9] Ziming Cheng, Binrui Xu, Lisheng Gong, Zuhe Song, Tianshuo Zhou, Shiqi Zhong, Siyu Ren, Mingxiang Chen, Xiangchao Meng, Yuxin Zhang, Yanlin Li, Lei Ren, Wei Chen, Zhiyuan Huang, Mingjie Zhan, Xiaojie Wang, and Fangxiang Feng. Evaluating mllms with multimodal multi-image reasoning benchmark, 2025. 3 [10] Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, and Yue Wang. Physbench: Benchmarking and enhancing vision-language models for physical world understanding. arXiv preprint arXiv:2501.16411, 2025. 13 [11] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 6, 19 [12] Saloni Dalal. timeguessr, 2025. 13 [13] Google DeepMind. Gemini 2.0 flash, 2025. 6, 19 [14] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, and et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. 3 [15] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language In Proceedings of the IEEE/CVF Conference on models. Computer Vision and Pattern Recognition (CVPR), pages 90629072, 2025. 3 [16] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 9 [17] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer, 2024. 6, 19 [18] Ming Li, Chenguang Wang, Yijun Liang, Xiyao Wang, Yuhang Zhou, Xiyang Wu, Yuqing Zhang, Ruiyi Zhang, and Tianyi Zhou. Caughtcheating: Is your mllm good cheating detective? exploring the boundary of visual perception and reasoning, 2025. 1, 3 [19] Ming Li, Ruiyi Zhang, Jian Chen, Chenguang Wang, Jiuxiang Gu, Yufan Zhou, Franck Dernoncourt, Wanrong Zhu, Tianyi Zhou, and Tong Sun. Towards visual text grounding of multimodal large language model, 2025. 1 [20] Yunxin Li, Zhenyu Liu, Zitao Li, Xuanyu Zhang, Zhenran Xu, Xinyu Chen, Haoyuan Shi, Shenyuan Jiang, Xintong Wang, Jifang Wang, Shouzheng Huang, Xinping Zhao, Borui Jiang, Lanqing Hong, Longyue Wang, Zhuotao Tian, Baoxing Huai, Wenhan Luo, Weihua Luo, Zheng Zhang, Baotian Hu, and Min Zhang. Perception, reason, think, and plan: survey on large multimodal reasoning models, 2025. 1 [21] Yijun Liang, Ming Li, Chenrui Fan, Ziyue Li, Dang Nguyen, Kwesi Cobbina, Shweta Bhardwaj, Jiuhai Chen, Fuxiao Liu, and Tianyi Zhou. Colorbench: Can vlms see and understand the colorful world? comprehensive benchmark for color perception, reasoning, and robustness, 2025. [22] Yi Liu, Junchen Ding, Gelei Deng, Yuekang Li, Tianwei Zhang, Weisong Sun, Yaowen Zheng, Jingquan Ge, and Yang Liu. Image-based geolocation using large vision-language models, 2024. 1 [23] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, 2024. 1 [24] Zesen Lyu, Dandan Zhang, Wei Ye, Fangdi Li, Zhihang Jiang, and Yao Yang. Jigsaw-puzzles: From seeing to understanding to reasoning in vision-language models, 2025. 1 [25] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. 13 [26] Marcelo Moreno. geoguessr, 2024. 13 [27] OpenAI. Gpt-5 system card, 2025. 6, 14, 19 [28] OpenAI. Openai o3 and o4-mini system card. Technical report, OpenAI, 2025. 6, [29] OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, and et al. Openai o1 system card, 2024. 3, 6 [30] OpenAI, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and etc. Gpt-4o system card, 2024. 6, 19 [31] Pranshu Pandya, Vatsal Gupta, Agney Talwarr, Tushar Kataria, Dan Roth, and Vivek Gupta. NTSEBENCH: Cognitive reasoning benchmark for vision language models. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 36803708, Albuquerque, New Mexico, 2025. Association for Computational Linguistics. 3 [32] Atin Pothiraj, Elias Stengel-Eskin, Jaemin Cho, and Mohit Bansal. Capture: Evaluating spatial reasoning in vision language models via occluded object counting. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 80018010, 2025. 1 [33] Yufan Ren, Konstantinos Tertikas, Shalini Maiti, Junlin Han, Tong Zhang, Sabine Susstrunk, and Filippos Kokkinos. Vgrpbench: Visual grid reasoning puzzle benchmark for large vision-language models, 2025. 1, 3 [34] Jonathan Roberts, Mohammad Reza Taesiri, Ansh Sharma, Akash Gupta, Samuel Roberts, Ioana Croitoru, Simion-Vlad Bogolin, Jialu Tang, Florian Langer, Vyas Raina, Vatsal Raina, Hanyi Xiong, Vishaal Udandarao, Jingyi Lu, Shiyang Chen, Sam Purkis, Tianshuo Yan, Wenye Lin, Gyungin Shin, Qiaochu Yang, Anh Totti Nguyen, David I. Atkinson, Aaditya Baranwal, Alexandru Coca, Mikah Dang, Sebastian Dziadzio, Jakob D. Kunz, Kaiqu Liang, Alexander Lo, Brian Pulfer, Steven Walton, Charig Yang, Kai Han, and Samuel Albanie. Zerobench: An impossible visual benchmark for contemporary large multimodal models, 2025. 1, [35] Burak Satar, Zhixin Ma, Patrick Amadeus Irawan, Wilfried Ariel Mulyawan, Jing Jiang, Ee-Peng Lim, and ChongWah Ngo. Seeing culture: benchmark for visual reasoning and grounding. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2223822254, Suzhou, China, 2025. Association for Computational Linguistics. 3 [36] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, and Tiancheng Zhao. Vlmr1: stable and generalizable r1-style large vision-language model, 2025. 3 [37] Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, and Xiang Yue. Visualpuzzles: Decoupling multimodal reasoning evaluation from domain knowledge, 2025. 1 [38] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Xiansheng Chen, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning of vision language models, 2025. 3 [39] Omkar Thawakar, Dinura Dissanayake, Ketan Pravin More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Ilmuz Zaman Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, Hisham Cholakkal, Ivan Laptev, Mubarak Shah, Fahad Shahbaz Khan, and Salman Khan. LlamaV-o1: Rethinking step-by-step visual reasoning in LLMs. In Findings of the Association for Computational Linguistics: ACL 2025, pages 2429024315, Vienna, Austria, 2025. Association for Computational Linguistics. 3 [40] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. [41] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision 10 dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. 13 [42] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. 6, 19 [43] Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao Fei. Multimodal chainof-thought reasoning: comprehensive survey, 2025. 1 [44] Zhikai Wang, Jiashuo Sun, Wenqi Zhang, Zhiqiang Hu, Xin Li, Fan Wang, and Deli Zhao. Benchmarking multimodal mathematical reasoning with explicit visual dependency, 2025. 1 [45] Guowei Xu, Peng Jin, Ziang Wu, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models In Proceedings of the IEEE/CVF Inreason step-by-step. ternational Conference on Computer Vision (ICCV), pages 20872098, 2025. [46] Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, Wenhai Wang, Jifeng Dai, and Jinguo Zhu. Visulogic: benchmark for evaluating visual reasoning in multi-modal large language models, 2025. 1, 3 [47] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 6, 19 [48] Haoxuan You, Rui Sun, Zhecan Wang, Long Chen, Gengyu Wang, Hammad Ayyubi, Kai-Wei Chang, and Shih-Fu Chang. IdealGPT: Iteratively decomposing vision and language reasoning via large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1128911303, Singapore, 2023. Association for Computational Linguistics. 1 [49] Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1-zeros aha moment in visual reasoning on 2b non-sft model, 2025. 3 [50] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 6,"
        },
        {
            "title": "Table of Contents for Appendix",
            "content": ". . . . . . . . . . . . . . . . . . . 10. Experiment Details . . 7 . Image Sources . . 8 . Detailed Taxonomy . . 9 . Data Curation for Planning . 9.1. Generation Process . . 9.2. Generation Prompt . . . . . . . . 10.1. Implementation Details . . 10.2. Evaluation Prompts . . . . . . . . . . . . . . . . . 11. Final Accuracy for VLMs on V-REX . . 12. Stepwise Recovery from Failure Analysis. . . 13. Cases Study. . . . . . . . 13.1. Success cases . . . . . 13.2. Failure cases of CoQ . 13.3. Failure cases of Planning . . 13.4. Failure cases of Following . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 . 13 . 13 . 13 . 14 . 19 . 19 . 19 . 19 . 20 . 20 . 20 . 20 . 21 . 22 12 7. Image Sources We collect images for V-REX from combination of website sources and publicly available visual reasoning benchmarks. Specifically, our dataset incorporates images from PhysBench[10], MathVision[41], CV-Bench[40], and InfographicQA[25]. In addition, we include real-world images sourced from the online platforms GeoGuessr[26] and TimeGuessr[12]. Together, these diverse sources ensure broad coverage across the 15 application scenarios represented in V-REX. 8. Detailed Taxonomy Deduction. This category addresses rule-based and logical reasoning, requiring models to explore potential causal or relational patterns to reach consistent conclusions. Exploration involves hypothesizing, verifying, and generalizing implicit rules from limited observations. Flowchart Deduction assesses whether models can reason through multi-step logical structures by tracing dependencies in flow diagrams. Pattern Deduction examines the ability to identify mathematical or visual regularities and extrapolate unseen elements. Property Deduction tests the capacity to infer physical or material properties based on observed patterns, and Relationship Deduction measures understanding of inter-object relationships and causal interactions in structured visual data. Together, these tasks capture the models exploratory capacity to discover and apply underlying rules within multi-step reasoning processes. Guessing. This category examines reasoning under uncertainty, requiring models to explore incomplete or ambiguous evidence to infer missing information. Exploration is reflected in the models ability to generate and evaluate multiple hypotheses before settling on the most plausible one. Intention Guessing measures models ability to infer human or object intentions from dynamic visual cues. Location Guessing requires identifying plausible locations based on contextual evidence, while Responsibility Guessing examines causal reasoning in scenarios such as traffic incidents or shared actions. Time Guessing tests temporal inference based on environmental cues, and Topic Guessing evaluates the ability to infer abstract or thematic concepts from multimodal contexts. These tasks together assess exploratory reasoning through uncertainty resolution and context-driven hypothesis testing. Navigation. This category highlights spatial and sequential reasoning, requiring models to explore action sequences, spatial configurations, and reasoning trajectories. Exploration is reflected in the models ability to plan, simulate, and adjust its reasoning path while preserving overall coherence. Map Navigation tests spatial reasoning and route planning, while GUI Navigation evaluates the models ability to reason through interface layouts and procedural tasks. Traffic Navigation focuses on understanding dynamic interactions under traffic scenarios and rules. Trend Navigation examines the ability to interpret sequential or time-evolving visual patterns. Together, these tasks assess exploratory reasoning as models must simulate and adjust multi-step plans in dynamic spatial or temporal settings. Retrieval. This category emphasizes global evidence integration, prompting the model to explore multiple visual or textual components. Exploration emerges through the models capacity to locate, select, and combine dispersed cues into coherent reasoning outcome. Counting Retrieval tests whether models can locate and aggregate quantitative information across visual regions, while Word Puzzle Retrieval evaluates the ability to identify, interpret, and reason over embedded textual elements. These tasks together measure exploratory reasoning through multi-step evidence gathering and integration, assessing how effectively models combine local details into coherent global understanding. Together, these four categories and fifteen sub-tasks form comprehensive taxonomy for evaluating exploratory reasoning. They capture complementary aspects of how VLMs hypothesize, search, plan, and integrate knowledge across diverse multimodal reasoning scenarios. 9. Data Curation for Planning 9.1. Generation Process To evaluate VLMs exploratory abilities within the question space, we design Planning task in which models must navigate reasoning chains while confronted with multiple plausible yet distracting questions. This setup probes whether model can select the most informative sub-questions amid competing alternatives. The dataset is constructed using two-stage pipeline composed of Distractor Construction and Question Integration. The pipeline uses powerful LLMs for generation, filtering, and refinement, resulting in question sets that are semantically rich, diverse, and challenging for VLMs. 13 Figure 9. Generation Pipeline for Planning. For each QA chain, GPT-5 generates candidate distractors using two complementary strategies: (1) step-level, which introduces diverse distracting questions at each reasoning step, and (2) chain-level, which constructs full distracting reasoning chains. For each sample, we select 1 most challenging distracting chain and 2 diverse distracting questions per step to form the final distractor set. Distractor Construction. This stage focuses on producing alternative questions that appear contextually relevant but deviate from the intended reasoning path. We design two complementary strategies to capture both local and global variability within reasoning chains. The first strategy, step-level distractor generation, focuses on each reasoning step individually. At every step, the LLM is prompted with the preceding context, including ground-truth questions and previously generated distractors, to propose new step-specific questions that remain plausible yet lead to divergent reasoning directions. This encourages local diversity and tests models ability to identify the correct continuation among multiple confounding options. The second strategy, chain-level distractor generation, operates at global level. Given an entire ground-truth QA chain, the LLM generates multiple challenger chains that are logically coherent within themselves but deviate from the original reasoning trajectory. These distractors evaluate whether model can differentiate between globally consistent yet semantically incorrect reasoning paths. To ensure higher-quality distractors, we employ GPT-5 for all question generation. Question Integration. After generating both step-level and chain-level distractors, we integrate them into unified Planning dataset through refinement and selection process. First, we use strong open-sourced LLM (Qwen3-VL-32B-Instruct) to perform inference over mixture of chain-level distractors and ground-truth questions, and automatically identify the chain that causes the highest confusion or reasoning inconsistency for each sample. This model-guided evaluation highlights the most challenging and coherent distractor chains, which are then retained for integration. Next, GPT-5 is prompted to select the most diverse and difficult step-level distractors that differ from the retained chains while preserving contextual plausibility. These selected distractors are then merged with the original ground-truth questions to form the final multi-choice Planning dataset, where each reasoning step includes both correct and misleading alternatives. Through this two-stage generation and integration pipeline, the Planning task systematically probes VLMs ability to explore and reason within complex question spaces. It reveals whether models can plan consistent reasoning trajectories when confronted with subtle yet semantically misleading alternatives. 9.2. Generation Prompt To support high-quality reasoning across both step-level and chain-level distractor generation, we design two structured system prompts that guide the closed-source GPT-5[27] model in producing challenging and misleading multi-step reasoning traces. The prompts require the model to base every generated question on explicit visual evidence and to articulate coherent progression of reasoning across steps, ensuring that each distractor remains contextually grounded while deviating from the 14 System Prompt for Step-Level Generation (Part 1) You will be given: 1. final question that must be answered based on an image. 2. (Optional) Intermediate questions and confusing questions for earlier steps. - If this is step 1, you will only receive the final question. - If this is step > 1, you will receive all intermediate and confusing questions from steps 1 to N-1 as context. Your task: For the current reasoning step, generate 5 confusing or misleading questions that: - Are visually and contextually relevant to the scene. - Sound appropriate regardless of previous intermediate choices. - Are NOT logically useful for solving the final answer. - Should confuse the models step planning. - If the final question contains condition (\"If the input is xxx\"), repeat that condition in the confusing questions for the first step. In other words: The confusing questions must appear contextually coherent but strategically irrelevant . They should make an incorrect planning choice seem superficially reasonable. intended reasoning path. To accommodate different forms of reasoning complexity, the prompts specify detailed instructions for paraphrasing gold questions, generating plausible yet strategically irrelevant alternatives, and preserving internal coherence within each distractor chain. Additionally, the prompts introduce explicit guidelines for separating task-relevant cues from unhelpful or misleading ones, preventing the model from inadvertently producing reasoning that supports the correct answer. The prompts are further enhanced with internal checklists, stylistic constraints, and strict prohibitions against helpful reasoning patterns, enabling robust and controlled distractor generation. By enforcing consistent output formats, grounding each question in the visual scene, and providing precise instructions for multi-step dependencies, these prompts facilitate the production of reliable paraphrased questions, step-level distractors, and globally coherent distracting chains. This design supports interpretable and reproducible distractor construction, ensuring that the generated alternatives effectively challenge the reasoning capabilities of modern VLMs. 15 System Prompt for Step-Level Generation (Part 2) For each unhelpful question, provide: - short factual answer. - An unhelpfulness_score (1-3). INTERNAL REASONING CHECKLIST (do not output): 1. Coherent with any prior intermediate steps. 2. Shares entities or context with the main task. 3. Does NOT provide causal, counting, spatial, or comparative reasoning. 4. Focuses on irrelevant attributes or secondary objects. 5. Appears plausible as next-step question. FORBIDDEN QUESTION TYPES: - Overlap with any intermediate questions reasoning step. - Provide causal, temporal, or numerical clues needed for the final answer. - Help the model disambiguate the correct reasoning step. Prefer \"side-path\" confusion questions: - About background objects or irrelevant actions. - About minor attributes (pose, color, expression, small details). - About goals or comparisons unrelated to the final task. OUTPUT RULES: - Keep grounded, natural, image-based. - No hallucinations. - Maintain stylistic consistency. - Return ONLY valid JSON: { \"question_N\": [ {\"paraphrased_intermediate_question\": \"string\", \"answer\": \"string\", \" unhelpfulness_score\": int}, {\"unhelpful_question\": \"string\", \"answer\": \"string\", \"unhelpfulness_score\": int}, {\"unhelpful_question\": \"string\", \"answer\": \"string\", \"unhelpfulness_score\": int}, {\"unhelpful_question\": \"string\", \"answer\": \"string\", \"unhelpfulness_score\": int}, {\"unhelpful_question\": \"string\", \"answer\": \"string\", \"unhelpfulness_score\": int}, {\"unhelpful_question\": \"string\", \"answer\": \"string\", \"unhelpfulness_score\": int} ] } System Prompt for Chain-Level Distractor Generation (Part 1) You will be given: 1. An image. 2. final question that must be answered based on the image. 3. Several intermediate questions that form the correct reasoning chain. Your task: Generate 2 confusing reasoning chains. For each gold step i, produce: - paraphrase of the gold intermediate question_i. - One misleading question (plus its answer) for Chain 1. - One misleading question (plus its answer) for Chain 2. Each confusing question must: - Be visually and contextually relevant to the image. - Sound coherent and stylistically close to the intermediate/final question. - Remain consistent across steps (a plausible but incorrect chain). - Not be logically useful for solving the final question. - Confuse the models step planning by offering tempting side-path. - Repeat any conditional clause from the final question (e.g., \"If the input is X\") in the confusing questions for the first step. INTERNAL REASONING CHECKLIST (do not output): 1. Shares entities or scene context with the gold question. 2. Maintains consistent distractor storyline across steps. 3. Avoids reasoning patterns (causal, temporal, spatial, numerical, comparative) that lead to the correct answer. 4. Focuses on irrelevant or secondary attributes, actions, or objects. FORBIDDEN HELPFUL QUESTION TYPES: - Repeating or clarifying any gold reasoning step. - Providing causal, numerical, or spatial clues related to the final answer. - Helping to disambiguate or verify the correct reasoning path. PREFERRED QUESTION STYLE: - \"Side-path\" questions about background details or secondary objects. - Attributes and interactions unrelated to the final goal. - Natural, grounded, and plausible but strategically irrelevant. 17 System Prompt for Chain-Level Distractor Generation (Part 2) You are given: 1) An image. 2) final question to be answered from the image. 3) The gold intermediate questions forming the correct chain. YOUR GOAL: Produce multiple WRONG but PLAUSIBLE reasoning chains that appear consistent but do not help solve the final question. REQUIREMENTS: A. PARAPHRASE (MASK THE GOLD): Paraphrase each gold question_i concisely. B. TWO WRONG CHAINS: For each step, generate exactly one misleading question (with its factual answer) for Chain 1 and Chain 2. C. SYNTACTIC CLOSENESS: Reuse nouns/verbs from gold question_i but alter intent. D. TEMPORAL CONFUSION: You may intentionally repeat misleading question from an earlier step to cause planning confusion. E. CHAIN CONSISTENCY: Keep each wrong chain internally coherent. F. NOT HELPFUL: Avoid reasoning that contributes to answering the final question. G. IMAGE-GROUNDED: No hallucinations. STYLE GUIDELINES: - Natural and close to the gold question tone. - Prefer next-step-looking but irrelevant side-path questions. FORBIDDEN HELPFUL CONTENT: - Clarifying or repeating any gold reasoning step. - Providing causal/temporal/spatial checks relevant to the final answer. OUTPUT FORMAT (JSON only): { \"paraphrased_gt_questions\": { \"question_1\": \"string\", \"answer_1\": \"string\", \"question_2\": \"string\", \"answer_2\": \"string\", ... }, \"consistent_confusing_questions1\": { \"question_1\": \"string\", \"answer_1\": \"string\", \"question_2\": \"string\", \"answer_2\": \"string\", ... }, \"consistent_confusing_questions2\": { \"question_1\": \"string\", \"answer_1\": \"string\", \"question_2\": \"string\", \"answer_2\": \"string\", ... } } 18 10. Experiment Details 10.1. Implementation Details To comprehensively evaluate the exploratory capabilities of VLMs, we assess total of 32 models spanning broad range of state-of-the-art families, including both proprietary and open-source systems. The evaluated models include GPT-4o[30], GPT5[27], O1[16], O3[28], Gemini-2 Flash[13], Gemini-2.5 Flash[11], LLaVA-OV[17], InternVL2.5[7], InternVL3-Instruct[50], InternVL3.5-Instruct[42], Qwen2.5-VL[1], and Qwen3-VL-Instruct (IT) / Think[47]. OpenAI and Gemini models are accessed through their official APIs, while all open-source models are locally deployed for evaluation. This model suite spans wide range of architectural designs and parameter scales, from 1B to 38B parameters, covering both multimodal proprietary systems and fully open-source implementations. Such diversity enables balanced and comprehensive assessment of exploratory reasoning behavior across different computational budgets and training paradigms. To ensure fair comparison, all open-source models are evaluated under standardized experimental environment using single NVIDIA A100 GPU (80GB), with identical decoding and temperature settings across all experiments. 10.2. Evaluation Prompts We provide 2 prompts to assess model behavior under the Following and Planning tasks. For the Planning task, the model is given the final question at the beginning of the full interaction. At each step, it receives the accumulated conversation history, the image, and set of candidate intermediate questions consisting of both gold questions and distractors. The prompt requires the model to independently select the most helpful intermediate question from these alternatives and to output its choice in precise, constrained format. For the Following task, the model is provided with the conversation history, the image, and the corresponding question and answer candidates at each step. The prompt instructs the model to identify the correct answer for the given question, ensuring strict adherence to the human-designed reasoning chain."
        },
        {
            "title": "Evaluation Prompt for Planning task",
            "content": "Choose the single most helpful intermediate question from the options below. On the FIRST line, output ONLY the option letter in parentheses (e.g., (A)). That first line must match exactly: ˆ([A-F])$ After blank line, you MAY add brief reasoning."
        },
        {
            "title": "Evaluation Prompt for Following task",
            "content": "You need to choose the most likely answer from the following options, make sure the option letter is in the parentheses like (X).: 11. Final Accuracy for VLMs on V-REX The accuracies on final questions under different settings (w/o CoQ, under the Planning task, and under the Following task) are shown in Table 3. Table 3 reports the final-question accuracies of all 32 evaluated VLMs across these three evaluation modes. Under the w/o CoQ setting (Acc), the model answers the final question directly without relying on any intermediate reasoning steps. AccP lan reflects the final-question accuracy after the model explores the question space by selecting the most informative intermediate question at each step in the presence of distractors. AccF ollow measures the final-question accuracy after the model navigates the answer space by choosing the correct answer to each intermediate question along the ground-truth reasoning chain. Together, these results provide comprehensive view of how model performance changes when guided by CoQ-based reasoning, highlighting the role of structured exploration in improving visual reasoning. 19 Table 3. Final accuracy over 4 categories and 32 VLMs with and without CoQ. Model Deduction Guessing Navigation Retrieval Overall Acc AccP lan AccF ollow Acc AccP lan AccF ollow Acc AccP lan AccF ollow Acc AccP lan AccF ollow Acc AccP lan AccF ollow LLaVA-OV-1B InternVL3-1B InternVL3.5-1B Qwen3-VL-2B-IT Qwen3-VL-2B-Think InternVL3-2B InternVL3.5-2B Qwen2.5-VL-3B Qwen3-VL-4B-IT Qwen3-VL-4B-Think InternVL2.5-4B InternVL3.5-4B LLaVA-OV-7B Qwen2.5-VL-7B Qwen3-VL-8B-IT Qwen3-VL-8B-Think InternVL2.5-8B InternVL3-8B InternVL3.5-8B InternVL3-9B InternVL3-14B InternVL3.5-14B InternVL2.5-26B InternVL2.5-38B InternVL3-38B InternVL3.5-38B GPT-4o GPT-5 O1 O3 Gemini 2.0 Flash Gemini 2.5 Flash 26.4 26.4 29.7 24.2 33.0 40.7 36.3 47.5 47.3 51.6 36.3 45.1 44.0 60.8 40.7 59.3 39.6 41.8 47.3 46.2 52.7 45.1 53.8 61.4 61.5 53.8 68.3 67.0 62.6 76.2 73.3 60.4 27.5 31.9 30.8 20.9 42.9 47.3 40.7 40.7 44.0 54.9 41.8 49. 52.7 35.2 42.9 54.9 53.8 47.3 62.6 56.0 57.1 59.3 62.6 64.8 65.9 59.3 59.3 87.9 76.9 79.1 67.0 68.1 30.8 27.5 37.4 40.7 40.7 44.0 40.7 55.4 54.9 48.4 42.9 48.4 48.4 60.8 57.1 59.3 53.8 50.5 53.8 54.9 63.7 54.9 56.0 69.2 72.5 60. 70.3 68.1 68.1 74.3 73.3 61.5 50.4 49.6 43.0 66.1 44.6 54.5 42.1 62.6 65.3 57.0 54.5 49.6 57.9 64.2 63.6 57.9 55.4 57.0 52.9 61.2 56.2 48.8 60.3 61.8 66.1 57.9 59.3 77.7 77.7 72.4 67.5 72.7 49.6 49.6 34.7 38.8 51.2 62.8 54.5 58.7 63.6 56.2 57.0 66. 62.8 67.8 62.8 48.8 67.8 67.8 62.0 71.9 77.7 65.3 72.7 74.4 77.7 75.2 83.5 84.3 89.3 86.8 78.5 76.0 VLMs: <7B 58.2 60.2 59.2 61.2 39.8 67.3 52.0 71.4 67.3 63.3 68.4 73.5 55.1 63.3 56.1 37.8 55.1 74.5 80.6 73.5 76.5 69.4 81.6 84. 47.9 50.4 48.8 63.6 51.2 61.2 53.7 60.2 68.6 61.2 62.0 59.5 VLMs: 7B10B 66.1 68.3 69.4 65.3 63.6 68.6 57.9 65.3 64.5 64.5 71.9 72.7 66.1 63.6 68.4 70.4 73.5 66.3 69.4 66.3 67.3 73.5 VLMs: >10B 80.6 69.4 76.5 82.7 78.6 81.6 74.5 72.4 83.7 74.5 82.7 81.6 87.8 82.7 88.8 85.7 84.7 87.8 85.7 88.8 VLMs: Proprietary 74.0 80.2 77.7 80.5 69.9 71.9 84.7 85.7 90.8 86.7 79.6 86. 94.9 93.9 93.9 93.9 87.8 91.8 59.2 69.4 77.6 77.6 62.2 78.6 70.4 79.6 82.7 71.4 80.6 85.7 68.4 78.6 89.8 78.6 80.6 80.6 85.7 83.7 90.8 91.8 87.8 95.9 93.9 74.5 94.9 91.8 93.9 93.9 93.9 91.8 48.8 24.4 41.5 26.8 43.9 39.0 51.2 56.1 58.5 46.3 41.5 58. 61.0 48.8 63.4 61.0 46.3 43.9 56.1 48.8 53.7 58.5 56.1 65.9 63.4 63.4 56.1 78.0 75.6 80.5 63.4 70.7 48.8 22.0 29.3 34.1 53.7 43.9 43.9 58.5 58.5 53.7 58.5 56.1 53.7 56.1 68.3 58.5 48.8 53.7 61.0 56.1 48.8 48.8 53.7 63.4 58.5 63. 70.7 78.0 68.3 78.0 70.7 75.6 48.8 36.6 41.5 34.1 41.5 48.8 31.7 53.7 48.8 63.4 43.9 61.0 70.7 65.9 53.7 65.9 51.2 53.7 48.8 51.2 63.4 58.5 68.3 65.9 58.5 63.4 63.4 85.4 82.9 80.5 73.2 78.0 45.9 40.1 43.3 44.6 40.3 50.4 45.4 59.4 59.6 54.6 50.2 56. 57.8 61.1 60.3 61.1 52.7 52.3 55.9 57.4 60.8 55.4 61.7 67.9 67.4 64.2 67.1 77.1 76.7 79.0 70.9 72.7 45.2 41.7 37.7 32.9 50.7 57.1 54.9 57.8 60.7 58.5 59.7 64.1 60.9 57.9 64.4 59.2 63.3 62.6 68.3 66.7 68.1 64.8 68.4 72.6 72.0 71. 77.1 86.0 82.1 84.5 76.0 77.9 46.7 46.0 51.3 54.0 48.9 58.1 49.1 62.2 63.7 61.1 57.3 63.6 63.4 68.4 67.5 67.3 62.3 63.4 61.5 63.8 70.6 67.4 71.0 75.9 72.8 65.5 75.6 81.4 80.7 82.3 77.6 75.8 12. Stepwise Recovery from Failure Analysis To study the models ability to recover from failure, we investigate the relationship between the number of wrong planning or following steps in CoQ versus the final accuracy of the model, averaged over all models. As shown in Figure 10 and Figure 11, they display slightly different patterns. The final accuracy decreases monotonically as the number of wrong following steps increases. This reflects that more misinformation in intermediate steps would generally lead to worse results. However, the final accuracy of failed planning is not monotonic. We hypothesize that when the number of wrong planning steps is large, the model might take an entirely different approach and might still arrive at correct conclusion. Also, the accuracy drops more sharply in Figure 11 than in Figure 10, which further verifies that models are generally more robust to wrong following steps than wrong planning steps. 13. Cases Study 13.1. Success cases We showcase representative success cases enabled by CoQ across different task settings in Figure 12 and 13. Decomposing the final question into sub-questions helps the model correct perceptual mistakes or stay aligned with the intended reasoning path, ultimately enabling it to reach the correct final answer. 13.2. Failure cases of CoQ While CoQ helps organize multi-step reasoning, its advantages are limited for Retrieval category. These problems depend largely on straightforward factual lookup or direct visual identification, rather than layered reasoning or extended inference. Since Retrieval questions typically involve minimal reasoning depth, adding CoQ chain often introduces extra steps that 20 Figure 10. Stepwise result of recovery from failed planning Figure 11. Stepwise result of recovery from failed following offer little value and may not influence the final prediction. Figure 14 illustrates failure case where the model correctly answers the question without CoQ. However, when the model is required to engage in step-wise exploration, it identifies the appropriate sub-questions yet still fails to reach the correct final answer. This indicates that the bottleneck lies not in hierarchical reasoning or exploration, but in the models fundamental perceptual ability to extract fine-grained information from visually dense image. Figure 15 further shows mismatch between the human-designed reasoning path and the models natural reasoning trajectory. Under the Planning task, the model struggles to align its question selection with the intended human chain. Even under the Following task where it is required to adhere to the predefined CoQ, the model still fails to generate the correct final answer. This suggests that when the solution does not depend on multi-step abstraction, the CoQ structure can conflict with the models native retrieval process. 13.3. Failure cases of Planning Although hierarchical question planning is intended to guide models toward more structured and interpretable reasoning, we observe that some VLMs experience performance degradation when intermediate CoQ steps are introduced. The expanded reasoning space can create distracting side-paths, causing models to focus on visually salient but semantically irrelevant sub-questions or to overlook key reasoning cues that are necessary for solving the task. In such situations, the additional steps fail to strengthen the intended chain of thought and instead introduce extra cognitive noise that disrupts both step selection and final answer prediction. As shown in Figure 16 and Figure 17, models may shift their attention toward superficial objects or answer-option patterns, diverging from the human-designed chain and ultimately producing incorrect conclusions even with correct initial steps. 21 Figure 12. Success case for Planning task. Without CoQ, the model fails to reach the correct final answer due to perceptual and counting errors. Under the Planning setting, however, once the model selects the correct intermediate question, it receives the corresponding answer, which compensates for its perceptual limitations and enables it to arrive at the correct final prediction. Figure 13. Success case for Following task. By decomposing the final question into informative sub-questions, the model correctly identifies the algorithm in the flowchart and uses this intermediate insight to arrive at the correct final answer. 13.4. Failure cases of Following Although the human-designed CoQ path is intended to guide the model to answer the final question with manually-crafted decomposition steps, we observe that sometimes the model would fail to answer the final question correctly after following the CoQ path. Figure 18 and Figure 19 show two failure cases in the Following task that represent two typical failure modes. As shown in Figure 18, the model correctly answers all the intermediate questions about the book cover, but it fails to leverage the information extracted from previous reasoning steps to answer the final question correctly. Different from the previous case, the model in Figure 19 incorrectly answers the intermediate question about the least numbers of protusions and indentations of the missing puzzle piece, leading to an incorrect final answer. 22 Figure 14. Failure case in the Retrieval category where CoQ provides limited benefit. The model can directly identify the correct word from the puzzle without CoQ. However, when required to follow or plan using the CoQ chain, the model becomes misled by intermediate steps and incorrectly answers Whistle. The example shows how structured CoQ exploration can interfere with tasks that primarily require precise visual matching rather than sequential reasoning. 23 Figure 15. Failure case in the Retrieval category where CoQ provides limited benefit. Without CoQ, the model naturally checks each car one by one and correctly identifies three fully visible cars. Under Planning, the imposed CoQ chain forces the model into reasoning path that diverges from its native strategy, leading to incorrect intermediate questions and an incorrect final count. Even under Following, the model cannot recover, showing that the human-designed chain is incompatible with the models retrieval-oriented reasoning. Figure 16. Failure case in the Planning task. Although the model correctly identifies the repeating cycle and the position of the fourteenth animal, its third step selects an irrelevant question about birds. This detour misleads the final prediction, causing the model to answer with bird rather than the correct frog. 25 Figure 17. Failure case in the Planning task. The model selects chain of CoQ steps centered on color descriptions, which are uninformative for determining the intended concept of the drawing. This distractive chain introduces additional noise and prevents the model from connecting the pig and canned meat sketches to the correct answer Spam. 26 Figure 18. Failure case in the Following task. Although the model correctly answers all the intermediate questions about the book cover, it fails to leverage the information extracted from previous reasoning steps to answer the final question correctly. Figure 19. Failure case in the Following task. The model incorrectly answers the intermediate question about the least numbers of protusions and indentations of the missing puzzle piece, leading to an incorrect final answer."
        }
    ],
    "affiliations": [
        "University of Maryland, College Park"
    ]
}