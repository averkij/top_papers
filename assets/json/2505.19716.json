{
    "paper_title": "Concise Reasoning, Big Gains: Pruning Long Reasoning Trace with Difficulty-Aware Prompting",
    "authors": [
        "Yifan Wu",
        "Jingze Shi",
        "Bingheng Wu",
        "Jiayi Zhang",
        "Xiaotian Lin",
        "Nan Tang",
        "Yuyu Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing chain-of-thought (CoT) distillation methods can effectively transfer reasoning abilities to base models but suffer from two major limitations: excessive verbosity of reasoning traces and inadequate adaptability to problem difficulty. Long reasoning traces significantly increase inference costs, and uniform-length solutions prevent base models from learning adaptive reasoning strategies. To address these issues, we propose a difficulty-aware prompting (DAP) method to dynamically shorten reasoning traces without performance loss. In our approach, a large teacher model first judges each problem's difficulty and then rewrites its reasoning traces to an appropriate shorter length, yielding concise yet complete reasoning traces. Leveraging the DAP pipeline, we curate a distilled dataset called LiteCoT consisting of 100K concise reasoning examples, with solutions averaging only 720 tokens (an order of magnitude shorter than typical CoTs). Using LiteCoT, we distilled a new family of reasoning models called Liter (1.5B, 7B, and 32B) based on the Qwen2.5 architecture. Experiments show that a student model fine-tuned on just 100K of these difficulty-pruned CoT samples outperforms a model distilled on 800K original Long CoT samples, while significantly reducing training and inference costs. Our method also generalizes well: across 11 diverse benchmarks, the shorter difficulty-aware CoTs achieve equal or better accuracy than Long chains, using far fewer tokens. For example, on the challenging AIME24 exam, our approach reaches $74.2\\%$ Pass@1 using only about 5K inference tokens, surpassing other methods that consume many more tokens. Our code and data are available at https://github.com/Evanwu1125/LiteCoT."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 6 1 7 9 1 . 5 0 5 2 : r Concise Reasoning, Big Gains: Pruning Long Reasoning Trace with Difficulty-Aware Prompting Yifan Wu1 Jingze Shi2 Bingheng Wu2 Jiayi Zhang1,3 Xiaotian Lin1 Nan Tang1 Yuyu Luo1 1HKUST (Guangzhou) 2Indepent Researcher 3DeepWisdom"
        },
        {
            "title": "Abstract",
            "content": "Existing chain-of-thought (CoT) distillation methods can effectively transfer reasoning abilities to base models but suffer from two major limitations: excessive verbosity of reasoning traces and inadequate adaptability to problem difficulty. Long reasoning traces significantly increase inference costs, and uniform-length solutions prevent base models from learning adaptive reasoning strategies. To address these issues, we propose difficulty-aware prompting (DAP) method to dynamically shorten reasoning traces without performance loss. In our approach, large teacher model first judges each problems difficulty and then rewrites its reasoning traces to an appropriate shorter length, yielding concise yet complete reasoning traces. Leveraging the DAP pipeline, we curate distilled dataset called LiteCoT consisting of 100K concise reasoning examples, with solutions averaging only 720 tokens (an order of magnitude shorter than typical CoTs). Using LiteCoT, we distilled new family of reasoning models called Liter (1.5B, 7B, and 32B) based on the Qwen2.5 architecture. Experiments show that student model fine-tuned on just 100K of these difficulty-pruned CoT samples outperforms model distilled on 800K original Long CoT samples, while significantly reducing training and inference costs. Our method also generalizes well: across 11 diverse benchmarks, the shorter difficulty-aware CoTs achieve equal or better accuracy than Long chains, using far fewer tokens. For example, on the challenging AIME24 exam, our approach reaches 74.2% Pass@1 using only about 5K inference tokens, surpassing other methods that consume many more tokens. Our code and data are available at https://github.com/Evanwu1125/LiteCoT."
        },
        {
            "title": "Introduction",
            "content": "Reasoning language models (RLMs) like DeepSeek-R1 [1] and OpenAI-o1 [2] have achieved impressive results on complex tasks (e.g., math reasoning) by generating explicit chain-of-thought (CoT) traces. Fine-tuning smaller base models on such CoT traces, i.e., form of model distillation, can effectively transfer reasoning capabilities. For example, using the inputoutput traces from powerful reasoner as supervised training data can substantially boost smaller models performance [35]. Existing CoT Distillation Strategies. Researchers have explored various strategies to improve CoT distillation data quality. For example, S1 [6] filtered the 59K initial data collected from multiple angles, such as difficulty and diversity, and finally obtained 1K high-quality reasoning traces for distillation. Light R1 [7] filtered 100M raw questionanswer pairs down to 70K high-quality reasoning traces for distillation, and applied two-stage model fine-tuning process. ReasonFlux [8] designed templates to synthesize additional long reasoning examples from existing data. LIMO [3] used classifier LLM to select only the most challenging questions for distillation data. These efforts have improved data quality in some aspects, but two key limitations remain in practice. First, even after careful filtering or synthesis, the CoT outputs from the large teacher (i.e., strong LLM) are very long Preprint. Figure 1: (a) Training tokens comparison of LiteCoT with prior distillation methods (32B model); (b) Inference efficiency and accuracy on the challenging AIME24 benchmark (32B model). Our Liter models trained with LiteCoT achieve higher accuracy (74.2% Pass@1) at substantially reduced inference token counts compared to prior methods. (e.g., the maximum can contain 32K tokens), which will incur heavy computational cost and time to train on. Second, current reasoning traces distilled methods lack adaptability in reasoning trace length. In practice, simple problems might be solvable with only few intuitive steps, whereas truly complex questions demand more elaborate multi-step reasoning. However, existing strategies do not differentiate solution lengths by problem difficulty. Large models often overthink even simple queries [9], and if all distilled solutions are uniformly verbose, the student model never learns to adjust reasoning detail to the task at hand [1012]. In short, existing CoT distillation methods tend to produce one-size-fits-all long traces, failing to adapt to actual problem complexity. Therefore, our goal is to construct new distilled reasoning dataset that addresses these challenges by being both concise and difficulty-adaptive. Shorter, more concise reasoning traces can cut down training time and token usage, while difficulty-adaptive traces mean the model sees only as much reasoning as necessary, i.e., brief for easy tasks and more thorough for harder ones. Intuitively, models trained on such tailored data can reason more efficiently by avoiding redundant computations on simple tasks and effectively allocating resources to complex ones. Simple Yet Effective CoT Distillation Method: Difficulty-Aware Prompting (DAP). To achieve this, our key idea is to use powerful teacher models ability to both control reasoning length and assess problem difficulty when properly prompted. As shown in Figure 2, we first design prompt templates for three levels of problem difficulty (simple, medium, and complex), which have different requirements on the structure and level of detail of the output reasoning path. We then take the long reasoning trace in the original distilled data as input, and require DeepSeek-R1 to rewrite the lengthy original reasoning chain into new reasoning path that is simpler and has more optimized length, combining the structured instructions in the prompt corresponding to the selected difficulty. LiteCoT Dataset. Leveraging the proposed DAP pipeline, we constructed new distilled dataset called LiteCoT, containing 100K concise, difficulty-adaptive reasoning samples. Each solution in LiteCoT averages approximately 720 tokens, representing an order-of-magnitude reduction in CoT length compared to existing distillation datasets, e.g., S1 [6] and OpenThoughts [13], which typically contain between 5K and 10K tokens per sample. As shown in Figure 1(a), LiteCoT significantly reduces training token counts while achieving the best results, clearly demonstrating the effectiveness of concise, structured reasoning inputs. Liter Models and Experiments. Using LiteCoT, we distilled new family of reasoning models named Liter (1.5B, 7B, 14B and 32B) based on Qwen2.5 architecture. Comprehensive evaluations across 11 diverse benchmarks demonstrate the superiority of models trained on our compact LiteCoT dataset. Remarkably, models distilled on the LiteCoT dataset (100K examples) consistently outperform those distilled on 800K original Long CoT examples, despite the latter having eight times more training data and substantially greater token exposure. Specifically, as shown in Figure 1(b), our Liter achieves Pass@1 accuracy of 74.2% using only around 5K inference tokens per example on the challenging AIME24 benchmark. This performance significantly surpasses previous methods, which require substantially more tokens to achieve comparable or even lower accuracy. 2 Figure 2: Overview of the DAP pipeline. First, the DeepSeek R1 model generates initial Long CoT data from collected user queries. Then, difficulty-aware prompt guides the model to rewrite concise CoT outputs, adapting reasoning length dynamically based on problem difficulty."
        },
        {
            "title": "2 Related Work",
            "content": "LLM Distillation. Knowledge distillation in large language models has evolved from traditional probability distribution matching to sequence-level imitation, where student models directly learn from teacher outputs. Models like DeepSeek-R1 [1], OpenAI-o1 [2] generate high-quality chain-ofthought (CoT) reasoning paths that, when used as training data, significantly improve smaller models reasoning abilities [13, 14]. Recent work by [3, 8, 1518] demonstrated that small collection of highquality reasoning examples can be more effective than large volumes of lower-quality data. However, the original CoT output from reasoning models often contains substantial redundant information, directly increasing training data size and computational costs for distillation [19, 20]. Reasoning Pruning via Budget Control. To mitigate this, researchers have developed various budget control techniques to control the length of CoT output. However, most existing strategies mainly focus on inference-time optimizations [6, 21], such as LightThinker [22],Sketch-of-Thought [23] and AdaR1 [24]. In contrast, training efficiency remains relatively underexplored. Existing methods for this purpose fall into two main categories: (1) Learning-based approaches [25, 26] that train auxiliary models to produce shorter reasoning traces, which can adaptively switch between Long and Short CoTs but often incur substantial additional training cost. (2) Prompt engineering-based pipelines that heuristically rewrite CoTs without training extra models. Examples include DLCoT [20], which introduces structured framework (DLCoT) to segment, simplify, and optimize Long CoT traces via prompt-based rewriting, and Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning [27], which combines Long and structure-preserved Short CoTs to train models that balance reasoning depth and efficiency. However, to the best of our knowledge, no prior work has explored using difficulty-aware prompting to dynamically adapt CoT length based on problem complexity, leaving critical gap in scalable and cost-effective CoT distillation."
        },
        {
            "title": "3 LiteCoT Dataset: Concise Reasoning with Big Gains",
            "content": "3.1 Difficulty-Aware Prompting for Curating LiteCoT We now describe our difficulty-aware prompting (DAP) pipeline, designed to construct concise and difficulty-adaptive distilled reasoning dataset. The pipeline consists of two sequential steps: (1) generating initial long reasoning solutions, and (2) refining these chains using difficulty-aware prompts to produce shorter, complexity-aligned reasoning traces. Step 1 (Long CoT Generation). In conventional distillation, teacher LLM Mteacher is used to generate reasoning chain CoTL,i for each query Qi in the training set D. Formally: CoTL,i = Mteacher(Qi) for = 1, 2, . . . , . The collected set of long chains is CL = (Qi, CoTL,i)N i=1. These Long CoTs often contain redundant content and do not adjust in length for query difficulty. Step 2 (Difficulty-aware CoT Refinement). In this step, the teacher model takes each (Qi, CoTL,i) pair as input, along with specially designed prompt PDA that instructs it to produce shorter solution appropriate for the difficulty of Qi. This prompt PDA provides different guidelines depending on whether question is easy, medium, or hard. For example, it might include: For easy questions: 3 Figure 3: Token count distribution of LiteCoT compared with previous CoT distillation datasets. provide brief solution. . . For hard questions: include all necessary detailed steps . . .. Guided by PDA, the teacher first assesses the difficulty of the problem and then refines the solution accordingly: 1. Difficulty Assessment: Guided by PDA, the model first assesses the difficulty of the input query Qi, classifying it into predefined set of difficulty levels = {keasy, kmedium, khard}. This assessment process can be represented as: di = Mteacher(Qi, CoTL,i, PDA) (1) where di is the assessed difficulty level. The prompt PDA contains explicit instructions for different assessments. 2. Difficulty-Adaptive CoT Refinement: Based on the assessed difficulty level di, the model follows the sub-prompt or structured output template Pdi PDA within PDA designed for that specific difficulty di. It then prunes, reorganizes, or guides the generation of new, difficultymatched short CoT, CoTS,di. This process can be represented as: CoTS,di = Mteacher((Qi, CoTL,i), PDA, di) Or more specifically, if Pdi is an explicit output template: CoTS,di = Mteacher(input = (Qi, CoTL,i), template = Pdi(di)) (2) (3) where Pdi(di) denotes the specific instruction part within PDA for difficulty di (e.g., For easy questions: . . ., For medium questions: . . ., For hard questions: . . .). Applying this process to all training samples yields new distilled dataset CS = (Qi, CoTS,i)i = 1N comprised of shortened, difficulty-graded reasoning traces. In addition, the refined CoTs are much shorter than the originals (for easy and medium queries, Length(CoT S, i) Length(CoTL,i)). This adaptive reduction strategy is expected to significantly improve the efficiency of model training while preserving essential reasoning for challenging tasks. Ultimately, student model trained on CS should learn to solve problems with an appropriate amount of reasoning. 3.2 LiteCoT Overview LiteCoT Overview. Using the DAP pipeline, we created distilled dataset, named LiteCoT, of 100K questionsolution pairs with difficulty-adaptive CoTs. Table 1 highlights the token savings of our approach compared to prior distillation data. Other budget-control methods still produce CoT solutions averaging several thousand tokens each (e.g., 5K10K tokens for ReasonFlux [8] and OpenThoughts [13]), often necessitating tens or hundreds of thousands of training samples. In stark contrast, LiteCoTs CoT samples average only about 720 tokens, representing an order-of-magnitude improvement in token efficiency. Table 1: Performance comparison of different budget control methods. Tokens denotes average tokens per CoT sample. Datasets #-Samples Tokens Light-R1 [7] OpenThoughts [13] OpenThoughts2 [13] S1 [6] LIMO [3] SKY-T1 [14] ReasonFlux [8] LiteCoT (ours) 75.9K 113.9K 1M 1K 817 16K 1K 100K 6382 6801 5021 5122 6984 4095 10500 720 Figure 3 clearly shows this advantage: the token-count distribution for LiteCoT is markedly shifted toward shorter reasoning chains compared to other datasets. Please refer to Appendix for details. 4 Training the Liter with LiteCoT. Using LiteCoT, we distilled new family of reasoning models, named Liter (1.5B, 7B, and 32B), based on the Qwen2.5. We followed the training strategy from Open-R1 [28] and fine-tuned the Liter models for 3 epochs on the LiteCoT dataset. Specifically, we employed learning rate of 5e-5 with cosine learning rate scheduler, including 10% warm-up phase. In addition, we increased the RoPE frequency to 100K, effectively extending the context length from 4K to 8K tokens. Please refer to Appendix for more training or evaluation details."
        },
        {
            "title": "4 Experiments",
            "content": "We conduct extensive experiments to evaluate the effectiveness of LiteCoT data. Our evaluations address three key questions: (Exp-1) Does training on short difficulty-aware CoTs, LiteCoT, actually improve model performance compared to long CoTs? (Exp-2) Can our DAP method rival or surpass models distilled with an order of magnitude more data from powerful teacher? (Exp-3) How does our DAP approach compare to other recent CoT distillation and compression techniques? 4.1 Experiment Setup Training Data. Our experimental evaluation consists of three stages. All data comes from the OpenThoughts [13] dataset and is processed using our DAP distillation process. For Exp-1: Use 25K OpenThoughts samples. Their original Long CoT form is used as the baseline training data; at the same time, these samples are processed by the DAP process to generate 25K corresponding Short CoT data for comparison training. For Exp-2: Expand the amount of OpenThoughts data to 100K. These samples are processed by the DAP process to generate 100K short CoT data(i.e., LiteCoT), which constitute the training set of this experiment. For Exp-3: Directly use the 25K short CoT data generated in Exp-1 as the training data for this stage. In Exp-1 and Exp-3, due to the small amount of training data, the models we Base Models. selected are all instruction versions in order to obtain better instruction following capabilities on small datasets. For Exp-1: Qwen2.5-0.5B-Instruct, Qwen2.5-Math-1.5B-Instruct, Qwen2.5-Math7B-Instruct, Qwen2.5-14B-Instruct of the Qwen2.5 series, and Llama3.1-8B-instruct of the Llama3.1 series are included as base models. For Exp-2: Aiming to make fair comparison with the results in the DeepSeek R1 report, we selected the same base version models used in the report: Qwen2.5math-1.5B, Qwen2.5-math-7B, Qwen2.5-14B, and Qwen2.5-32B. For Exp-3: Similar to Exp-1, the model Qwen2.5-math-7B-Instruct is selected as the baseline model. Benchmark Datasets. We evaluate model accuracy on diverse suite of 11 benchmarks, covering math word problems (MATH500 [29], GSM8K [30]), competition-level math (AIME24 [31]/AIME25 [32],OlympiadBench [33], MinervaMath [34]), general QA (GPQA) [35], and academic exams (GaoKao2023 [36], MMLU-STEM [37], etc.). Baselines. For Exp-1, we mainly use the model directly distilled from the original Long CoT as the baseline to evaluate the relative improvement of our method. For Exp-2, we include series of current mainstream reasoning models in the benchmark comparison range, including representative models such as S1 [6] and OpenThoughts [13]. For Exp-3, our benchmark comparison focuses on different Long CoT optimization or decompose methods. These methods include strategies that optimize at the prompt engineering level, such as Chain-of-Draft [38] and Budget Aware Reasoning [39]; in addition, we also include methods that require training additional models to simplify the output content, such as LLMLingua2 [40]. 4.2 Experimental Results and Analysis 4.2.1 Exp-1: LiteCoT vs. Long CoT Baselines We evaluate the fine-tuned models trained on our LiteCoT and Long CoT data, comparing their overall performance and analyzing the accuracy-speed trade-off. Overall Performance. As shown in Table 2, we comprehensively compare the average accuracy of Qwen2.5 series models and Llama3.1-8B trained with Short CoT and Long CoT on 11 different benchmarks. The average performances show that all models trained and distilled with short CoT 5 Table 2: Performance comparison of Long CoT distilled models and short CoT distilled models. Green means short CoT distilled models outperform the corresponding Long CoT distilled models. Pink means the opposite (QW and LM refer to Qwen2.5-Instruct and Llama3.1-Instruct). Benchmark MATH500 AIME24 GPQA GSM8K Minerva Math GaoKao2023 OlympiadBench College Math Sat Math MMLU STEM ACM23 Overall QW-0.5B long short QW-1.5B long short QW-3B QW-7B short long short long QW-14B long short LM-8B short long 18.6 0.0 22.7 33.6 3.7 23.4 5.9 19.6 37.5 29.7 2.5 17. 11.6 0.0 21.2 20.5 4.4 11.9 1.5 9.8 12.5 9.2 0.0 9.3 62.8 3.3 28.8 85.1 25.4 65.2 37.2 46.9 43.8 33.9 47.5 43.6 56.4 0.0 27.3 84.4 19.5 62.9 36.3 47.2 3.1 10.4 45.0 35. 47.8 3.3 23.7 77.3 14.3 43.3 15.4 35.6 68.8 58.1 30.0 38.0 46.0 0.0 22.7 61.4 11.8 39.5 10.8 31.2 34.4 21.4 17.5 27.0 66.2 23.3 29.8 95.5 33.9 66.5 42.8 48.5 96.9 64.4 62.5 57. 64.8 3.3 32.3 95.0 31.2 66.2 37.5 47.5 90.6 60.5 57.5 53.3 65.0 13.3 40.4 90.8 29.0 56.9 28.4 41.9 96.9 71.3 42.5 52.4 64.4 10.0 38.4 85.1 21.3 49.9 19.1 32.1 62.5 56.9 27.5 42. 34.2 3.3 32.3 73.7 10.3 32.2 13.0 24.7 43.8 33.9 5.0 27.9 32.2 0.0 25.3 26.8 7.7 25.2 11.6 1.61 25.0 29.0 25.0 19.0 Figure 4: Inference time vs. accuracy across eight benchmarks. Models trained with short CoT (filled symbols) demonstrate significantly faster inference times compared to Long CoT (hollow symbols) while maintaining or exceeding accuracy levels. data significantly outperform the corresponding models distilled with Long CoT data. For example, the overall average performance of Qwen2.5-1.5B-Instruct (short) reached 43.6%, while its Long CoT version was 35.7%; Qwen2.5-7B-Instruct (short) reached 57.3%, higher than the 53.3% of the Long CoT version. This trend is consistently reflected in models of different parameter sizes. The relationship between inference speed and accuracy. Figure 4 clearly shows that for all models (Llama3.1-8B and Qwen2.5 series models), the use of Short CoT (filled points) generally achieves inference time that is much lower than Long CoT (hollow points). More importantly, this significant speed improvement does not come at the expense of accuracy; on the contrary, in most benchmarks, Short CoT is not only faster, but its accuracy can also reach or even exceed the level of Long CoT. For example, on multiple benchmarks such as GSM8K and MMLU STEM, Short CoT data points all show better speed-accuracy balance. This finding spans models of different parameter sizes and variety of task types, and strongly proves that the DAP Short CoT method can significantly improve inference efficiency while maintaining or even optimizing the models inference accuracy. 4.2.2 Exp-2: Comparing our Liter Models with Mainstream Distilled LLMs We first compared our Liter models against models distilled from DeepSeek-R1 (Exp-2.1). To further evaluate the effectiveness of our Liter models, we compared our Liter 7B and 32B variants against broad range of mainstream distilled models across multiple reasoning benchmarks (Exp-2.2). 6 Table 3: Performance Comparison of Language Models on Reasoning Benchmarks. Since there are too many models to be tested, we only select some of them for retesting, and we directly copy the results of all other models from their papers or blogs. represents the copied results, represents the results retested by ourself. DS-R1-Distill represents the official Distilled Models in the DeepSeek-R1 report. OB represents the Olympiad Bench. Bold means the first performance and underline means the second performance. Model AIME24 MATH500 GPQA AMC23 OB OVERALL GPT-4o Claude3.5-Sonnet GPT-o1-preview GPT-o1-mini DeepSeek-V3 Mathstral-7B-V0.1 DS-Coder-V2-Instruct Qwen2.5-Math-72B-Instruct NuminaMath-72B-CoT LLaMA3.1-70B-Instruct LLaMA3.1-405B-Instruct DS-Distilled-LLaMA3.1-70B rStar-Math Qwen2.5-Math-1.5B Qwen2.5-Math-1.5B-instruct ReasonFlux-1.5B DS-R1-Distilled-1.5B Liter-1.5B Qwen2.5-Math-7B Qwen2.5-math-7B-instruct Openthinker-7B Openthinker2-7B Light-R1-7B OpenR1-Qwen-7B DS-R1-Distilled-7B Bespoke-Stratos-7B OlympicCoder-7B ReasonFlux-7B Liter-7B Qwen2.5-32B-instruct Openthinker-32B Openthinker2-32B DS-R1-Distilled-32B Light-R1-32B LIMO-32B S1.1-32B ReasonFlux-32B Bespoke-Stratos-32B Sky-T1-32B-preview QwQ-32B-Preview Liter-32B Frontier Models 9.3 16.0 44.6 56.7 76.6 78.1 85.5 90.0 56.1 59.4 73.3 60.0 Open-Sourced Reasoning LLMs 39.2 0.0 13.3 30.0 3.3 23.3 - 70.0 43. 0.0 10.0 20.0 28.9 36.7 10.0 13.3 31.3 50.0 53.3 48.7 55.5 20.0 20.7 36.7 53.3 13.3 68.0 63.3 72.6 70.0 40.0 43.3 73.3 63.3 43.3 76.7 76.7 59.1 - - - - 46.7 51.1 65.2 - 0.0 - - 33.8 35.4 33.3 - 42.9 49.3 46.5 21.2 49.1 37.8 25.3 - 53. 33.8 63.5 65.1 62.1 61.1 51.0 52.5 63.1 58.1 56.8 65.1 63.6 90.2 57.8 75.3 85.6 64.0 65.4 73.8 94.5 88.2 1.5B 51.2 60.0 70.4 83.9 87.2 7B 74.2 82.6 83.2 88.4 93.2 87.8 92.8 82.0 74.8 88.6 93. 32B 79.4 90.6 96.0 95.2 94.2 95.0 89.2 96.4 93.0 82.4 92.2 96.6 7 47.5 44.6 90.0 95.0 85.0 37.5 57.5 70.0 70.0 50.0 - - 80.0 22.5 60.0 72.5 55.0 60. 45.0 62.6 74.5 89.5 85.0 88.5 77.5 - 63.0 80.0 82.5 37.5 - 80.0 90.0 77.5 52.5 75.0 70.0 - - 72.5 87.5 43.3 - - 65.3 55.4 21.5 37.6 49.0 32.6 27.7 34.8 - 63.1 16.7 38.1 49.0 31.4 42.7 21.8 39.0 - - 52.3 - 49.8 - - 54.8 50. - - 56.3 53.0 - 43.9 56.6 34.7 - - 54.1 53.9 46.56 - - 73.4 65.78 - - - - 42.62 - - - 18.08 - - 46.6 52.4 36.86 - - - 66.06 - 64.94 - - - 66.62 - - 72.14 74.58 - 56.48 63.32 67.5 - - 72.12 75. Figure 5: Radar chart show the performance comparison of the Vanilla, DeepSeek-R1-Distilled, and Liter versions of the Qwen2.5 model with different parameter sizes (1.5B, 7B, 14B, 32B) on multiple math and reasoning benchmarks (Math500, AIME24, OlympiadBench, AIME25, GPQA, AMC23). Figure 6: Scatter plots comparing inference time and accuracy of different methods across multiple benchmarks. Our method (Ours) consistently achieves optimal accuracy while significantly reducing inference time compared to existing methods. Exp-2.1: Comparison with DeepSeek-R1-Distilled Models. Figure 5 shows the performance of four models of different parameter sizes (1.5B, 7B, 14B and 32B) in the Qwen2.5 series on six key benchmarks after three training methods: original weights (Vanilla), DeepSeek-R1 distilled models (DeepSeek-R1-Distilled) and our Liter. It shows that the performance of the model representing Liter (Gray blue area) is consistent and significantly better than the other two methods at all parameter scales. Specifically, taking the Qwen2.5-7B model as an example, our method achieved an accuracy of 60.6% on Math500, while DeepSeek-R1 distilled and the original model achieved 55.8% and 42.6% respectively; on the more challenging OlympiadBench, our model scored 30.2%, also ahead of DeepSeek R1 distilleds 21.1% and the original models 10.0%. This performance advantage is particularly evident in the Qwen2.5-32B model with larger number of parameters. For example, on the AIME24 benchmark, the accuracy of the DAP distilled model is as high as 76.7%, far exceeding the 48.0% of DeepSeek R1 distillation and the 13.3% of the original model. These specific numerical comparisons strongly prove that the model trained on the LiteCoT can achieve comprehensive performance superiority over other training methods on multiple core benchmarks Exp-2.2: Comparison with Mainstream RLMs of Similar Parameter Sizes. We conducted an extensive horizontal comparison involving our Liter models (1.5B, 7B, and 32B) against various publicly available mainstream reasoning models of comparable parameter scales. Evaluations were carried out on five representative benchmarks: AIME24, MATH500, GPQA, AMC23, and OlympiadBench, with results presented in Table 3. Overall, whether at the parameter scale of 1.5B, 7B, or 32B, the models trained by the DAP distillation method show comparable or better performance than the current mainstream or even top reasoning models. At the 1.5B level, Ours-1.5B achieved an accuracy of 87.2% on MATH500, significantly better than most of the same-level comparison models, and showed strong competitiveness on OlympiadBench (42.7%). Entering the 7B level, Ours-7B performs particularly well, achieving an accuracy of 93.8% on MATH500, and leading the other 7B parameter models listed in the table with 53.3% on AIME24 and GPQA respectively, while also being at the top level on OlympiadBench (50.2%). When the model scale is expanded to 32B, the performance advantage of Ours-32B is further consolidated, for example, it ranks among the top on multiple difficult benchmarks such as MATH500 (96.6%), AIME24 (76.7%), and GPQA (63.6%). Table 4: Performance comparison of different CoT decompose methods. COD represents Chain-ofDraft, Lingua-2 means LLMLingua-2. Benchmark Vanilla COD Lingua-2(0.9) Lingua-2(0.5) BudgetAware(2048) Ours AIME24 AIME25 GPQA GSM8K Minerva Math GaoKao2023en OlympiadBench College Math Sat Math MMLU STEM ACM23 13.3 10.0 27.3 94.8 33.9 64.2 34.5 45.1 90.6 68.8 60. 13.3 13.3 27.3 94.4 29.4 62.3 39.4 46.9 50.0 42.7 45.0 16.7 23.3 32.1 94.8 26.5 64.4 33.8 43.0 93.8 69.5 45.0 0.0 0.0 21.2 72.4 11.4 34.5 13.5 15.7 75.0 43.7 27.5 13.3 3.3 34.8 94.4 33.8 65.2 36.6 42.8 78.1 58.9 52.5 23.3 16.7 29.8 95.5 33.9 66.5 42.8 48.5 96.9 64.4 62.5 4.2.3 Exp-3: Comparison with other CoT Optimization methods. We continue to compare models trained with DAP distilled data against those using other CoT decomposition techniques, focusing on overall performance and the speed-accuracy trade-off to analyze their key differences and advantages. Overall Performance of Different CoT Decomposition Methods. Table 4 clearly shows the performance comparison of different CoT decomposition methods on multiple benchmarks, including the original baseline model (Vanilla), Chain-of-Draft (COD), two LLMLingua-2 with different compression rates (Lingua-2(0.9) and Lingua-2(0.5)), BudgetAware(2048), and our DAP method (Ours). From the accuracy data of each benchmark, our DAP method (Ours) shows the strongest overall performance. For example, on the challenging AIME24 benchmark, Ours achieved an accuracy of 23.3%, significantly higher than the other methods. On the important SAT Math benchmark, Ours also leads all other comparison methods with an accuracy of 96.9%, including COD (50.0%), LLMLingua-2(0.9) (93.8%), and BudgetAware(2048) (78.1%). It is worth noting that although some methods such as LLMLingua-2 may show good single-point performance on specific benchmarks (such as Lingua-2(0.9) 32.1% on GPQA), ours method can maintain or be close to the leading level on most benchmarks. The relationship between inference speed and accuracy. Figure 6 fully demonstrates the superiority of the DAP distillation method (labeled as Ours, purple star with gold rim), with additional results shown in Appendix 7. In the tests across eight different benchmarks, the Ours method has the best trade-off in almost all speed-accuracy two-dimensional planes. Specifically, compared with other mainstream CoT decomposition techniques such as Chain-of-Draft, LLMLingua-2, and BudgetAware, our method not only achieves the highest accuracy in most cases (for example, 62.5% on ACM23 and 96.9% on Sat Math), but also achieves the shortest or very competitive inference times. In comparison, other methods either compromise on accuracy (such as LLMLingua-2 (0.5) or COD with high compression ratios) or far exceed Ours in reasoning time (such as LLMLingua-2 (0.9) with low compression ratios on some tasks). This better price/performance ratio consistently achieved on multiple benchmarks strongly demonstrates the unique advantages of the DAP distillation method."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduced Difficulty-Aware Prompting, new pipeline for CoT distillation. Leveraging DAP, we created LiteCoT, difficulty-adaptive CoT dataset consisting of 100K distilled reasoning examples with an average length of only 720 tokens per sample, far shorter than existing alternatives. Utilizing LiteCoT, we distilled and trained new family of reasoning models named Liter (1.5B, 7B, and 32B), based on the Qwen2.5 architecture. Extensive experiments across diverse reasoning benchmarks clearly demonstrate the effectiveness of the LiteCoT dataset and Liter models, consistently outperforming previous approaches that rely on substantially larger datasets with uniformly verbose reasoning chains."
        },
        {
            "title": "References",
            "content": "[1] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [2] OpenAI, Introducing openai o1, 2025. [Online]. Available: https://openai.com/o1/ [3] Y. Ye, Z. Huang, Y. Xiao, E. Chern, S. Xia, and P. Liu, Limo: Less is more for reasoning, arXiv preprint arXiv:2502.03387, 2025. [4] N. Ho, L. Schmid, and S.-Y. Yun, Large language models are reasoning teachers, arXiv preprint arXiv:2212.10071, 2022. [5] B. Liu, X. Li, J. Zhang, J. Wang, T. He, S. Hong, H. Liu, S. Zhang, K. Song, K. Zhu et al., Advances and challenges in foundation agents: From brain-inspired intelligence to evolutionary, collaborative, and safe systems, arXiv preprint arXiv:2504.01990, 2025. [6] N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. Hajishirzi, L. Zettlemoyer, P. Liang, E. Candès, and T. Hashimoto, s1: Simple test-time scaling, arXiv preprint arXiv:2501.19393, 2025. [7] L. Wen, Y. Cai, F. Xiao, X. He, Q. An, Z. Duan, Y. Du, J. Liu, L. Tang, X. Lv et al., Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond, arXiv preprint arXiv:2503.10460, 2025. [8] L. Yang, Z. Yu, B. Cui, and M. Wang, Reasonflux: Hierarchical llm reasoning via scaling thought templates, arXiv preprint arXiv:2502.06772, 2025. [9] X. Chen, J. Xu, T. Liang, Z. He, J. Pang, D. Yu, L. Song, Q. Liu, M. Zhou, Z. Zhang et al., Do not think that much for 2+ 3=? on the overthinking of o1-like llms, arXiv preprint arXiv:2412.21187, 2024. [10] Y. Sui, Y.-N. Chuang, G. Wang, J. Zhang, T. Zhang, J. Yuan, H. Liu, A. Wen, S. Zhong, H. Chen et al., Stop overthinking: survey on efficient reasoning for large language models, arXiv preprint arXiv:2503.16419, 2025. [11] R. Chen, Z. Zhang, J. Hong, S. Kundu, and Z. Wang, Seal: Steerable reasoning calibration of large language models for free, arXiv preprint arXiv:2504.07986, 2025. [12] C.-H. Chiang and H.-y. Lee, Over-reasoning and redundant calculation of large language models, arXiv preprint arXiv:2401.11467, 2024. [13] O. T. Team, Open Thoughts, Jan. 2025. [14] N. Team, Sky-t1: Train your own o1 preview model within $450, https://novasky-ai.github.io/posts/sky-t1, 2025, accessed: 2025-01-09. [15] X. Guan, L. L. Zhang, Y. Liu, N. Shang, Y. Sun, Y. Zhu, F. Yang, and M. Yang, rstar-math: Small llms can master math reasoning with self-evolved deep thinking, arXiv preprint arXiv:2501.04519, 2025. [16] J. Zhang, J. Xiang, Z. Yu, F. Teng, X. Chen, J. Chen, M. Zhuge, X. Cheng, S. Hong, J. Wang et al., Aflow: Automating agentic workflow generation, arXiv preprint arXiv:2410.10762, 2024. [17] J. Xiang, J. Zhang, Z. Yu, F. Teng, J. Tu, X. Liang, S. Hong, C. Wu, and Y. Luo, Self-supervised prompt optimization, arXiv preprint arXiv:2502.06855, 2025. [18] X. Lin, Y. Qi, Y. Zhu, T. Palpanas, C. Chai, N. Tang, and Y. Luo, Lead: Iterative data selection for efficient llm instruction tuning, arXiv preprint arXiv:2505.07437, 2025. [19] X. Qu, Y. Li, Z. Su, W. Sun, J. Yan, D. Liu, G. Cui, D. Liu, S. Liang, J. He et al., survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond, arXiv preprint arXiv:2503.21614, 2025. [20] Y. Luo, Y. Song, X. Zhang, J. Liu, W. Wang, G. Chen, W. Su, and B. Zheng, Deconstructing long chain-of-thought: structured reasoning optimization framework for long cot distillation, arXiv preprint arXiv:2503.16385, 2025. [21] F. Teng, Z. Yu, Q. Shi, J. Zhang, C. Wu, and Y. Luo, Atom of thoughts for markov llm test-time scaling, arXiv preprint arXiv:2502.12018, 2025. [22] J. Zhang, Y. Zhu, M. Sun, Y. Luo, S. Qiao, L. Du, D. Zheng, H. Chen, and N. Zhang, Lightthinker: Thinking step-by-step compression, CoRR, vol. abs/2502.15589, 2025. [Online]. Available: https://doi.org/10.48550/arXiv.2502.15589 10 [23] S. A. Aytes, J. Baek, and S. J. Hwang, Sketch-of-thought: Efficient llm reasoning with adaptive cognitiveinspired sketching, arXiv preprint arXiv:2503.05179, 2025. [24] H. Luo, H. He, Y. Wang, J. Yang, R. Liu, N. Tan, X. Cao, D. Tao, and L. Shen, Adar1: From long-cot to hybrid-cot via bi-level adaptive reasoning optimization, 2025. [Online]. Available: https://arxiv.org/abs/2504.21659 [25] Q. Yu, K. Wu, Z. Chen, C. Zhang, M. Mei, L. Huang, F. Tan, Y. Du, K. Liu, and Y. Zhu, Rethinking the generation of high-quality cot data from the perspective of llm-adaptive question difficulty grading, arXiv preprint arXiv:2504.11919, 2025. [26] H. Zhao, H. Wang, Y. Peng, S. Zhao, X. Tian, S. Chen, Y. Ji, and X. Li, 1.4 million open-source distilled reasoning dataset to empower large language model training, arXiv preprint arXiv:2503.19633, 2025. [27] B. Yu, H. Yuan, Y. Wei, B. Wang, W. Qi, and K. Chen, Long-short chain-of-thought mixture supervised fine-tuning eliciting efficient reasoning in large language models, arXiv preprint arXiv:2505.03469, 2025. [28] H. Face, Open r1: fully open reproduction of deepseek-r1, January 2025. [Online]. Available: https://github.com/huggingface/open-r1 [29] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt, Measuring mathematical problem solving with the math dataset, arXiv preprint arXiv:2103.03874, 2021. [30] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano et al., Training verifiers to solve math word problems, arXiv preprint arXiv:2110.14168, 2021. [31] AI-MO, Aime 2024, 2024. [Online]. Available: https://huggingface.co/datasets/AI-MO/ aimo-validation-aime [32] , Aime 2025, 2025. [Online]. Available: https://huggingface.co/datasets/yentinglin/aime_ [33] C. A. Choquette-Choo, K. Dvijotham, K. Pillutla, A. Ganesh, T. Steinke, and A. Thakurta, Correlated noise provably beats independent noise for differentially private learning, arXiv preprint arXiv:2310.06771, 2023. [34] math ai, Minervamath, 2025. [Online]. Available: https://huggingface.co/datasets/math-ai/minervamath [35] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman, Gpqa: graduate-level google-proof q&a benchmark, in First Conference on Language Modeling. [36] X. Zhang, C. Li, Y. Zong, Z. Ying, L. He, and X. Qiu, Evaluating the performance of large language models on gaokao benchmark, arXiv preprint arXiv:2305.12474, 2023. [37] TIGER-Lab, Mmlu-stem, 2024. [Online]. Available: https://huggingface.co/datasets/TIGER-Lab/ MMLU-STEM [38] S. Xu, W. Xie, L. Zhao, and P. He, Chain of draft: Thinking faster by writing less, arXiv preprint arXiv:2502.18600, 2025. [39] T. Han, Z. Wang, C. Fang, S. Zhao, S. Ma, and Z. Chen, Token-budget-aware llm reasoning, arXiv preprint arXiv:2412.18547, 2024. [40] Z. Pan, Q. Wu, H. Jiang, M. Xia, X. Luo, J. Zhang, Q. Lin, V. Rühle, Y. Yang, C.-Y. Lin et al., Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression, arXiv preprint arXiv:2403.12968, 2024."
        },
        {
            "title": "A Difficulty Aware Prompt",
            "content": "This section provides the complete Difficulty Aware Prompt used in the study. These prompts are designed to guide the model in assessing the difficulty of problem and generating reasoning procedures of appropriate length accordingly. 11 Yaml config for running Qwen2.5-Math-7B on LiteCoT As an assistant, you need to first assess the complexity of the problem and adopt an appropriate thinking framework before providing the final solution. Structure your response into two main sections: Thought and Solution. First evaluate the complexity of the problem, then choose suitable thinking framework, and describe the thought process as detailed as possible: 1. For simple problems: **Analysis:** [Understand the core elements and goals of the problem] **Approach:** [Propose direct solution methods] **Summary:** [Concisely summarize the solution approach and key points] 2. For moderately complex problems: **Analysis:** [Understand the problem and identify key points and challenges] **Initial Approach:** [Propose preliminary solutions] **Reflection:** [Evaluate the pros and cons of the initial approach] **Improvement:** [Refine the solution based on reflection] **Summary:** [Summarize the key points of the final solution] 3. For highly complex problems: **Analysis:** [Analyze various aspects of the problem and break down its structure] **Problem Decomposition:** [Break complex problem into manageable sub-problems] **Sub-problem Processing:** (Repeat the following steps for each sub-problem) - Sub-problem 1: * Initial approach * Reflection * Improved solution - Sub-problem 2: * Initial approach * Reflection * Improved solution 12 - ...(adjust according to the actual number of sub-problems) **Integration:** [Integrate sub-problem solutions into complete solution] **Overall Reflection:** [Evaluate the integrated complete solution] **Final Optimization:** [Make final optimizations based on overall reflection] **Summary:** [Summarize key points of the final comprehensive solution] The solution section should maintain logical, accurate, and concise expression, detailing the steps needed to reach the conclusion, formatted as: **Solution:** [Provide the final solution here]"
        },
        {
            "title": "B Extra Experiment Details",
            "content": "B.1 Training Details Our training environment is equipped with 8 A800 GPUs with 80GB memory, and uses the DeepSpeed ZeRO-3 framework for distributed training. To further optimize GPU memory, we also introduced technologies such as Tensor Parallelism and liger kernel. Here, we share the YAML and DeepSpeed configuration files used for qwen2.5-Math-7B model training as an example. Yaml config for running Qwen2.5-Math-7B on LiteCoT # Logging and Output arguments log_level: info logging_strategy: steps logging_steps: 10 save_total_limit: 1 report_to: # - tensorboard - wandb save_strategy: steps save_steps: 100 output_dir: /output_path overwrite_output_dir: true # Model arguments model_name_or_path: /model_path model_revision: main trust_remote_code: True torch_dtype: bfloat16 attn_implementation: sdpa # Data training arguments dataset_name: /dataset_path dataset_format: parquet # dataset_configs: # - all 13 max_seq_length: 8192 dataset_num_proc: 8 packing: true # SFT trainer config preprocessing_num_workers: 8 seed: 233 do_train: true num_train_epochs: 3 per_device_train_batch_size: 4 do_eval: true eval_strategy: steps eval_steps: 10 per_device_eval_batch_size: 1 optim: adamw_torch learning_rate: 5.0e-5 lr_scheduler_type: cosine_with_min_lr lr_scheduler_kwargs: min_lr_rate: 0.1 warmup_ratio: 0.1 weight_decay: 0.0001 gradient_accumulation_steps: 4 gradient_checkpointing: true gradient_checkpointing_kwargs: use_reentrant: false max_grad_norm: 1.0 bf16: true use_liger: true DeepSpeed config for running Qwen2.5-Math-7B on LiteCoT compute_environment: LOCAL_MACHINE debug: false deepspeed_config: deepspeed_multinode_launcher: standard offload_optimizer_device: none offload_param_device: none zero3_init_flag: true zero3_save_16bit_model: true zero_stage: 3 tensor_parallel: tp_size: 8 offload_optimizer: device: none pin_memory: true offload_param: device: none train_batch_size: 16 train_micro_batch_size_per_gpu: 4 gradient_accumulation_steps: 4 bf16: enabled: true optimizer: type: AdamW params: lr: 5.0e-5 betas: [0.9, 0.999] 14 eps: 1e-8 weight_decay: 0.0001 bias_correction: true main_process_port: 29500 distributed_type: DEEPSPEED downcast_bf16: no machine_rank: 0 main_training_function: main mixed_precision: bf16 num_machines: 1 num_processes: 8 rdzv_backend: static same_network: true tpu_env: [] tpu_use_cluster: false tpu_use_sudo: false use_cpu: false B.2 Evaluation Details In our evaluation process, the evaluation scripts mainly come from two sources. For the four benchmarks AIME24, AIME25, GPQA, and MATH500, we use Hugging Faces LightEval library https://github.com/huggingface/lighteval/tree/main for model evaluation. For the remaining benchmarks focusing on mathematical reasoning ability, including GSM8K, Minerva Math, GaoKao2023en, OlympiadBench, College Math, Sat Math, MMLU STEM, and ACM23, we use the evaluation code provided in the Qwen2.5 code repository https://github.com/QwenLM/Qwen2. 5-Math/tree/main/evaluation/data. All evaluation experiments are performed uniformly under our specific GPU environment, NVIDIA H20 with 96GB memories. Next, we will share the shell (.sh) script files used to evaluate the AIME24, GPQA, MATH500, and AIME25 benchmarks based on Lighteval for reference. sh file used for evaluating AIME24, AIME25, GPQA and MATH500 MODEL=/model_path NUM_GPUS=n export CUDA_VISIBLE_DEVICES=\"0,...,n-1\" MODEL_ARGS=\"pretrained=$MODEL,dtype=bfloat16,tensor_parallel_size= $NUM_GPUS,max_model_length=32768,gpu_memory_utilization=0.8 ,generation_parameters={max_new_tokens:32768,temperature:0.6,top_p:0.95}\" OUTPUT_DIR=data/evals/$MODEL # AIME 2024 TASK=aime24 lighteval vllm $MODEL_ARGS \"lighteval$TASK00\" --use-chat-template --output-dir $OUTPUT_DIR # AIME 2025 TASK=aime25 lighteval vllm $MODEL_ARGS \"lighteval$TASK00\" --use-chat-template --output-dir $OUTPUT_DIR # MATH-500 TASK=math_500 lighteval vllm $MODEL_ARGS \"lighteval$TASK00\" --use-chat-template 15 --output-dir $OUTPUT_DIR # GPQA Diamond TASK=gpqa:diamond lighteval vllm $MODEL_ARGS \"lighteval$TASK00\" --use-chat-template --output-dir $OUTPUT_DIR B.3 Benchmark Details In our evaluation part, we totally evaluate our models on 11 benchmarks. They are: MATH500: Tests secondary school mathematics with open-ended problem-solving questions requiring step-by-step solutions, typically comprising 500 problems. AIME24: Assesses advanced high school mathematics (American Invitational Mathematics Examination level) through integer-answer competition-style problems, with specific recent contest (e.g., AIME 2024) having 30 questions. GPQA: Evaluates graduate-level physics quantitative reasoning with challenging, multi-step, free-response questions written by physics experts, containing 448 questions. GSM8K: Focuses on grade school mathematics (grades 3-8) with multi-step arithmetic word problems requiring basic operations, featuring approximately 1,319 test questions. Minerva Math: Covers broad range of quantitative reasoning from elementary mathematics to advanced levels using free-response math problems, often drawing from existing datasets and typically used with around 2,000 questions from its test set (MATH subset). GaoKao2023: Tests Chinese National College Entrance Examination (Gaokao) mathematics questions from 2023, featuring complex problems covering wide high-school curriculum, with the math section typically having around 20-22 questions per exam paper version. OlympiadBench: Assesses mathematical Olympiad problem-solving abilities with highly challenging, proof-oriented or complex computational questions from various international and national Olympiads, containing 185 problems. College Math: Covers US college-level mathematics topics with multiple-choice and freeresponse questions spanning calculus, algebra, and statistics, with one common dataset having 1,760 questions. Sat Math: Evaluates high school mathematics relevant to college readiness (SAT exam) through multiple-choice and grid-in questions on algebra, problem-solving, and data analysis, with the math sections of an official SAT practice test typically containing 58 questions. MMLU STEM: subset of the Massive Multitask Language Understanding (MMLU) benchmark, it tests expertise in STEM fields (Science, Technology, Engineering, and Mathematics) at high school and college levels via multiple-choice questions, with the STEM subset comprising several thousand questions across various specific STEM subjects (e.g., the \"Mathematics\" part alone has 250 dev and 1175 test questions). ACM23: Assesses competitive programming skills (similar to ACM International Collegiate Programming Contest problems) with algorithmic reasoning challenges requiring code solutions or detailed algorithmic steps, with specific contest sets varying but problem collections often containing hundreds of problems (the user might be referring to specific years problem set or curated collection). B.4 Additional Experiment Results"
        },
        {
            "title": "C Dataset Comparison",
            "content": "C.1 Cases of Samples In this section, we will show the model distillation results for the same problem in different datasets. We will show the comparison of three different difficulty levels: easy, medium, and hard. Because 16 Figure 7: Additional experiment results comparing inference time and accuracy of different methods across multiple benchmarks. Our method (Ours) consistently achieves optimal accuracy while significantly reducing inference time compared to existing methods. the original answer for hard is too long, we only show the Litecot data and OpenThoughts data for easy and medium difficulty levels here. Easy Question: Return your final response within boxed{}. Find all natural numbers that, when divided by 7, give the same quotient and remainder. solutions of easy problems in LiteCoT <begin_of_thought> **Analysis:** The problem requires finding all natural numbers ( ) such that when divided by 7, the quotient ( ) equals the remainder ( ). Using the division algorithm, ( = 7q + ), with ( 0 leq < 7 ). The condition ( = ) must hold. **Approach:** Substitute ( = ) into the division equation to get ( = 8r ). Since ( ) must be valid remainder (1 leq ( ) leq 6, as ( = 0 ) would make ( = 0 ), not natural number), compute ( ) for ( = 1, 2, ..., 6 ). **Summary:** The natural numbers are ( 8r ) for ( = 1 ) to ( 6 ), yielding ( 8, 16, 24, 32, 40, 48 ). <end_of_thought> <begin_of_solution> **Solution:** To find all natural numbers ( ) such that when divided by 7, the quotient equals the remainder: 1. Use the division formula: ( = 7q + ), where ( 0 leq < 7 ). 2. Set ( = ), leading to ( = 7r + = 8r ). 3. Since ( ) must be natural number (remainder geq 1) and ( < 7 ), valid values for ( ) are ( 1, 2, 3, 4, 5, 6 ). 4. Calculate ( ) for each ( ): - ( = 1 Rightarrow = 8 ) - ( = 2 Rightarrow = 16 ) - ( = 3 Rightarrow = 24 ) - ( = 4 Rightarrow = 32 ) - ( = 5 Rightarrow = 40 ) 17 - ( = 6 Rightarrow = 48 ) Thus, the natural numbers are: [ boxed{8, 16, 24, 32, 40, 48} ] <end_of_solution> solutions of easy problems in OpenThoughts <begin_of_thought> Okay, so need to find all natural numbers that, when divided by 7, give the same quotient and remainder. Let me start by understanding the problem properly. First, lets recall that when we divide natural number by 7, we can express it in the form of the division algorithm: = 7q + r, where is the quotient and is the remainder. The remainder must satisfy 0 len < 7 because were dividing by 7. So, the possible values for are 0, 1, 2, 3, 4, 5, 6. Now, the problem states that the quotient and the remainder are the same. That means = r. So substituting that into the equation, we get = 7q + = 8q. Therefore, must be multiple of 8. But wait, we need to ensure that the remainder is indeed equal to and that it satisfies the condition 0 len < 7. Since = q, and must be less than 7, must also be less than 7. Because if were 7 or more, then the remainder would be 7 or more, which isnt allowed in division by 7. So can be 0, 1, 2, 3, 4, 5, or 6. However, is natural number. Depending on the definition, natural numbers usually start from 1, so must be at least 1. If = 0, then = 8*0 = 0, which might not be considered natural number. Let me check if 0 is included here. If the problem defines natural numbers as starting from 1, then can only be from 1 to 6. But if 0 is included, then 0 would also be solution. But let me confirm the standard definition. In mathematics, natural numbers typically start from 1, so 0 is excluded. Therefore, must be at least 1. So can be 1, 2, 3, 4, 5, 6. Therefore, the corresponding values would be 8*1=8, 8*2=16, 8*3=24, 8*4=32, 8*5=40, 8*6=48. But wait, need to verify if each of these numbers actually satisfies the original condition. Lets take each and divide by 7 to see if the quotient and remainder are equal. Starting with n=8: 8 divided by 7 is 1 with remainder of 1. Here, quotient q=1 and remainder r=1, so they are equal. That works. Next, n=16: 16 divided by 7 is 2 with remainder of 2. So q=2 and r=2. Thats good. n=24: 24 divided by 7 is 3 with remainder of 3. Because 7*3=21, 18 24-21=3. So q=3, r=3. Correct. n=32: 32 divided by 7 is 4 with remainder 4 (7*4=28, 32-28=4). So q=4, r=4. n=40: 40 divided by 7 is 5 with remainder 5. 7*5=35, 40-35=5. So q=5, r=5. n=48: 48 divided by 7 is 6 with remainder 6. 7*6=42, 48-42=6. So q=6, r=6. Okay, all of these check out. Now, if we consider q=7, then n=8*7=56. Lets divide 56 by 7: quotient is 8, remainder is 0. Here, would have been 8 (but according to our original substitution, q=r=7). Wait, this is where the problem arises. If q=7, then r=7, but 7 is not valid remainder when dividing by 7. The remainder must be less than 7. Therefore, q=7 is invalid because it would lead to remainder of 7, which isnt allowed. Hence, the maximum can be is 6. Therefore, the natural numbers are 8, 16, 24, 32, 40, 48. Wait second, let me make sure there are no other numbers beyond these that could satisfy the condition. Suppose someone claims that there is larger number where q=r. Lets say q=7, but as saw, would have to be 7, which is not allowed. Similarly, for q=8, r=8 which is even more invalid. So the only possible values are 1 through 6, leading to n=8q where q=1 to 6. But just to be thorough, lets check if theres different approach. Suppose we set up the equation: When is divided by 7, quotient equals remainder r. So, by division algorithm: = 7q + But since = r, substitute: = 7q + = 8q Additionally, the remainder must satisfy 0 len < 7. Since = q, this translates to 0 len < 7. However, since is natural number (assuming starting at 1), must be at least 1. Therefore, {1,2,3,4,5,6}, leading to {8,16,24,32,40,48}. Therefore, these are all the natural numbers that satisfy the condition. Let me check if n=0 is allowed. If the problem includes 0 as natural number, which is sometimes the case in certain contexts, then 0 would be solution as well (0 divided by 7 is 0 with remainder 0). However, since the problem specifies \"natural numbers,\" and given that in most mathematical contexts natural numbers start from 1, we can exclude 0. Thus, the final answer should be the numbers 8, 16, 24, 32, 40, and 48. <end_of_thought> <begin_of_solution> 19 To find all natural numbers ( ) that, when divided by 7, yield the same quotient ( ) and remainder ( ), we start with the division algorithm: [ = 7q + ] where ( 0 leq < 7 ). The problem states ( = ), so substituting ( = ) gives: [ = 7q + = 8q. ] Since ( = ) must satisfy ( 0 leq < 7 ), and ( ) is natural number (starting from 1), ( ) must be in ( {1, 2, 3, 4, 5, 6} ). Substituting these values into ( = 8q ): - For ( = 1 ): ( = 8 times 1 = 8 ). - For ( = 2 ): ( = 8 times 2 = 16 ). - For ( = 3 ): ( = 8 times 3 = 24 ). - For ( = 4 ): ( = 8 times 4 = 32 ). - For ( = 5 ): ( = 8 times 5 = 40 ). - For ( = 6 ): ( = 8 times 6 = 48 ). Verification confirms each ( ) satisfies ( text{quotient} = text{remainder} ) when divided by 7. For example: - ( 8 div 7 = 1 ) with remainder ( 1 ), - ( 16 div 7 = 2 ) with remainder ( 2 ), etc. **Final Answer:** The natural numbers are (boxed{8, 16, 24, 32, 40, 48}). <end_of_solution> Medium Question: Return your final response within boxed{}. Let ( h_{1}, h_{2}, h_{3} ) be the heights of triangle, and ( ) be the radius of the inscribed circle. Prove that ( h_{1} + h_{2} + h_{3} geq 9r ). solutions of medium problems in LiteCoT <begin_of_thought> **Analysis:** The problem requires proving an inequality involving triangle heights and inradius. Key elements are the relationships between heights (h_1, h_2, h_3), inradius (r), area (S), and semiperimeter (p). The challenge lies in connecting these quantities through known formulas and applying an inequality. **Initial Approach:** Express heights in terms of area and sides (h_i = 2S/a_i). Use the identity = pr to substitute into the sum of heights. This leads to h_1 + h_2 + h_3 = 2pr(1/a + 1/b + 1/c). To establish the inequality, relate 1/a + 1/b + 1/c to semiperimeter via Cauchy-Schwarz. **Reflection:** The Cauchy-Schwarz inequality (for vectors (a, b, c) and (1/a, 1/b, 1/c)) provides (a + + c)(1/a + 1/b + 1/c) geq 9. Since + + = 2p, this bounds 1/a + 1/b + 1/c geq 9/(2p). Substituting back confirms the required inequality. **Improvement:** Verify the validity of Cauchy-Schwarz application and ensure all substitutions align with triangle properties. No logical gaps detected. **Summary:** The proof hinges on expressing heights via area, applying Cauchy-Schwarz to relate reciprocals of sides to semiperimeter, and substituting into the sum of heights to derive the inequality. <end_of_thought> <begin_of_solution> **Solution:** 1. **Express heights via area**: For triangle with sides (a, b, c) and corresponding heights (h_1, h_2, h_3), the area (S) is: [ h_1 = frac{2S}{a},quad h_2 = frac{2S}{b},quad h_3 = frac{2S}{c}. ] 2. **Sum of heights**: [ h_1 + h_2 + h_3 = 2Sleft(frac{1}{a} + frac{1}{b} + frac{1}{c}right). ] 3. **Relate (S) to inradius (r)**: Using (S = cdot r) (where (p = frac{a+b+c}{2}) is semiperimeter): [ h_1 + h_2 + h_3 = 2prleft(frac{1}{a} + frac{1}{b} + frac{1}{c}right). ] 4. **Apply Cauchy-Schwarz inequality**: [ (a + + c)left(frac{1}{a} + frac{1}{b} + frac{1}{c}right) geq (1 + 1 + 1)^2 = 9. ] Since (a + + = 2p), this simplifies to: [ frac{1}{a} + frac{1}{b} + frac{1}{c} geq frac{9}{2p}. ] 5. **Substitute into height sum**: [ h_1 + h_2 + h_3 geq 2pr cdot frac{9}{2p} = 9r. ] Thus, the inequality (h_1 + h_2 + h_3 geq 9r) is proven. [ boxed{h_{1} + h_{2} + h_{3} geq 9r} ] <end_of_solution> 21 solutions of medium problems in OpenThoughts <begin_of_thought> Alright, so need to prove that the sum of the heights of triangle is at least nine times the inradius. Let me start by recalling some formulas related to triangles, heights, and the inradius. First, the height (altitude) corresponding to side of triangle is given by the formula ( h_a = frac{2Delta}{a} ), where ( Delta ) is the area of the triangle and ( ) is the length of the side. Similarly, ( h_b = frac{2Delta}{b} ) and ( h_c = frac{2Delta}{c} ). So, the sum of the heights ( h_1 + h_2 + h_3 ) would be ( frac{2Delta}{a} + frac{2Delta}{b } + frac{2Delta}{c} ). On the other hand, the inradius ( ) is related to the area and the semiperimeter ( ) by the formula ( = frac{Delta}{s} ), where ( = frac{a + + c}{2} ). So, the inradius is the area divided by the semiperimeter. Our goal is to show that ( h_1 + h_2 + h_3 geq 9r ). Lets substitu te the expressions for the heights and the inradius in terms of the area and the sides. First, express ( h_1 + h_2 + h_3 ) as ( 2Delta left( frac{1}{a} + frac{1}{b} + frac{1}{c} right) ). The inradius ( ) is ( frac{Delta}{s} ), so ( 9r = frac{9Delta}{s} ). Therefore, the inequality we need to prove becomes: ( 2Delta left( frac{1}{a} + frac{1}{b} + frac{1}{c} right) geq frac{9Delta}{s} ) Since ( Delta ) is positive (its the area of triangle), we can divide both sides by ( Delta ) without changing the inequality direction: ( 2 left( frac{1}{a} + frac{1}{b} + frac{1}{c} right) geq frac{9}{s} ) Simplifying further, multiply both sides by ( ): ( 2s left( frac{1}{a} + frac{1}{b} + frac{1}{c} right) geq 22 ) So, the inequality reduces to proving that ( 2s left( frac{1}{a} + frac{1}{b} + frac{1}{c} right) geq 9 ), where ( = frac{a + + c}{2} ). Let me write this more explicitly. Substitute ( = frac{a + + c}{ 2} ), so: ( 2 times frac{a + + c}{2} times left( frac{1}{a} + frac{1}{b } + frac{1}{c} right) geq 9 ) The 2 and the denominator 2 cancel out, so: ( (a + + c) left( frac{1}{a} + frac{1}{b} + frac{1}{c} right) geq 9 ) Ah, now this looks familiar. This is classic inequality that might related to the AM-HM inequality or Cauchy-Schwarz. Let me recall the Cauchy-Schwarz inequality in the form: ( (x_1y_1 + x_2y_2 + x_3y_3)^2 leq (x_1^2 + x_2^2 + x_3^2)(y_1^2 + _2^2 + y_3 ^2) ) But perhaps another form is more useful here. The Cauchy-Schwarz inequality for sequences ( (a, b, c) ) and ( left( frac{1}{a}, frac{1}{b}, frac{1}{c} right) ) gives: ( left( times frac{1}{a} + times frac{1}{b} + times frac {1}{c} ri ght)^2 leq (a^2 + b^2 + c^2) left( frac{1}{a^2} + frac{1}{b^2} + frac{1}{c^ 2} right) ) But this simplifies to ( (1 + 1 + 1)^2 leq (a^2 + b^2 + c^2) left( frac{1}{a ^2} + frac{1}{b^2} + frac{1}{c^2} right) ), which is 9 leq someth ing. Not ure if thats directly helpful here. Alternatively, consider the AM-HM inequality for three positive real umbers: 23 ( frac{a + + c}{3} geq frac{3}{frac{1}{a} + frac{1}{b} + frac {1}{c}} ) Rearranging this gives: ( (a + + c)left( frac{1}{a} + frac{1}{b} + frac{1}{c} right) geq 9 ) Which is exactly the inequality we need! So, according to the AM-HM in equality, the product of the sum of the sides and the sum of their reciprocals indeed reater than or equal to 9. Therefore, the original inequality ( h_1 + h_2 + h_3 geq 9r ) holds true. But wait, let me double-check the steps to make sure didnt skip any thing cruc ial. Starting from the heights: ( h_1 = frac{2Delta}{a} ), similarly for h2, h3. Then sum is ( 2 Delta(fra c{1}{a} + frac{1}{b} + frac{1}{c}) ). Then, is ( frac{Delta}{s } ), so 9 is ( frac{9Delta}{s} ). Therefore, the inequality becomes: ( 2Delta(frac{1}{a} + frac{1}{b} + frac{1}{c}) geq frac{9Delta }{s} ) Cancel Delta: ( 2(frac{1}{a} + frac{1}{b} + frac{1}{c}) geq frac{9}{s} ) Multiply both sides by s: ( 2s(frac{1}{a} + frac{1}{b} + frac{1}{c}) geq 9 ) But since = (a + + c)/2, substituting: ( 2*(a + + c)/2 * (sum reciprocals) = (a + + c)(sum reciprocals) geq 9 ) Which is exactly the AM-HM inequality. So, yes, the steps are correct. But hold on, AM-HM gives (a + + c)/3 geq 3/(1/a + 1/b + 1/c), which 24 rearrange to (a + + c)(1/a + 1/b + 1/c) geq 9. So thats exactly the inequa lity we ha ve here. Therefore, equality holds when = = c, i.e., in an equilat eral trian gle. So, this inequality is tight for equilateral triangles. Therefore, the original inequality ( h_1 + h_2 + h_3 geq 9r ) is pr oven by ap plying the AM-HM inequality to the sides of the triangle. Alternatively, another approach might involve expressing the heights terms of the inradius and other elements, but since this path worked out, mayb thats ufficient. However, to be thorough, let me consider if there are other ways or ossible pitfalls. Wait, another thought: The formula for the inradius is = Delta/s, = Del ta/r. Then, since h1 + h2 + h3 = 2Delta(1/a + 1/b + 1/c), we can writ the ineq uality as: 2Delta(1/a + 1/b + 1/c) geq 9r = 9Delta/s Cancelling Delta, we get 2(1/a + 1/b + 1/c) geq 9/s, which is equiva lent to 2s (1/a + 1/b + 1/c) geq 9. Then substituting = (a + + c)/2, we have : 2*(a + + c)/2*(1/a + 1/b + 1/c) = (a + + c)(1/a + 1/b + 1/c) geq 9 Which again is the same inequality. So, regardless of the path, we end up with he AM-HM inequality. Alternatively, if didnt recall AM-HM, maybe using Cauchy-Schwarz: ( (sqrt(a))^2 + (sqrt(b))^2 + (sqrt(c))^2 ) * ( (1/sqrt(a))^2 + (1/sqr t(b))^2 + (1/sqrt(c))^2 ) geq (1 + 1 + 1)^2 = 25 But thats the same as (a + + c)(1/a + 1/b + 1/c) geq 9. So, Cauchy -Schwarz lso gives the same result. Therefore, the inequality holds by Cauchy-S chwarz as well. Therefore, regardless of the method used, the key inequality needed is (a + + c)(1/a + 1/b + 1/c) geq 9, which is standard result. Therefore, the original inequality is proven. Another angle: Lets consider specific cases. For an equilateral trian gle, lets check if equality holds. Suppose all sides are equal, say length a. hen, the emiperimeter = 3a/2. The inradius = Delta/s. The area Delta = (s qrt(3)/4)a ^2. So = (sqrt(3)/4 a^2)/(3a/2) ) = (sqrt(3)/4 a^2) * (2)/(3a) ) = ( sqrt(3)/6) a. The heights h1, h2, h3 are all equal to (sqrt(3)/2)a. So sum of hei ghts is 3* (sqrt(3)/2)a = (3sqrt(3)/2)a. Then 9r = 9*(sqrt(3)/6)a = (3sqrt(3)/2)a . Therefor e, sum of heights equals 9r in this case, so equality holds, which mat ches the ondition when AM-HM equality holds, i.e., when all sides are equal. For non-equilateral triangle, lets take simple case. Suppose = 3, = 4, = 5 (right-angled triangle). Then, semiperimeter = (3 + 4 + 5)/2 = 6. Area Delta = (3*4)/2 = 6. Inradius = Delta/s = 6/6 = 1. Heights: h1 = 2 Delta/a = 12/3 = 4, h2 = 12/4 = 3, h3 = 12/5 = 2.4. Sum of heights: 4 + 3 + 2.4 = 9.4. 9r = 9*1 = 9. So 9.4 geq 9, which holds. So the inequality is satisfied here. Another example: = 2, = 2, = 3. Then = (2 + 2 + 3)/2 = 3.5. Ar 26 ea using erons formula: sqrt(3.5*(3.5 - 2)*(3.5 - 2)*(3.5 - 3)) = sqrt(3.5*1.5 *1.5*0.5) = sqrt(3.5*1.5*0.75) = sqrt( (3.5*0.75)*1.5 ) = sqrt(2.625*1.5) appro sqrt(3.9 375) approx 1.984. So = 1.984 / 3.5 approx 0.567. Heights: h1 = 2 Delta/a pprox 2*1.984 /2 approx 1.984, h2 same as h1, h3 = 2*1.984 /3 approx 1.323. Su approx 1.984 + 1.984 + 1.323 approx 5.291. 9r approx 5.103. So 5. 291 geq 5 .103. Holds as well. Wait, but according to the calculation, the sum of heights is approxim ately 5.29 1, which is greater than 9r approx 5.103. So, even in this case, the inequality holds. But let me check more extreme triangle. Suppose triangle with side approach ing degeneracy. Lets take = 1, = 1, approaching 2 (but still valid tria ngle). Lets compute when approaches 2, say = 2 - epsilon, where epsilon very small. Then semiperimeter = (1 + 1 + 2 - epsilon)/2 = (4 - epsilon)/2 = 2 - epsilon/2. Area Delta using Herons formula: sqrt( s(s - a)(s - b)(s - c) ) = sqrt( (2 - epsilon/2)(2 - epsilon/2 -1)(2 - epsilon/2 -1)(2 - epsilon /2 - (2 - epsilon)) ) = sqrt( (2 - epsilon/2)(1 - epsilon/2)(1 - psilon/2)( epsilon/2) ). As epsilon approaches 0, this becomes sqrt(2*1*1*0) = 0. So Del ta approaches 0, and = Delta/s approaches 0. The heights: h1 and h2 (correspo nding to sides a=1 and b=1) will be h1 = 2Delta/a = 2Delta/1 = 2Del ta, h2 sim ilarly 2Delta, and h3 = 2Delta/c approx 2Delta/(2 - epsilon) 27 Del ap prox ta. So sum of heights approx 2Delta + 2Delta + Delta = 5Delta. 9 approx 9*(Delta/s) approx 9*(Delta/(2 - epsilon/2)) approx (9/2)Delt a. So 5De lta vs (9/2)Delta. Since 5 > 4.5, then 5Delta geq (9/2)Delta when Delta > 0 . But as Delta approaches 0, both sides approach 0, but 5Delta is st ill greate r. However, in this limit, the inequality holds because 5Delta geq ( 9/2)Delta to 5 geq 4.5, which is true. But this is bit hand-wavy because whe epsilon approaches 0, the triangle becomes degenerate. However, for valid on-degener ate triangle, epsilon must be greater than 0, so Delta is positive, and the in equality holds. Alternatively, consider very skewed triangle where one side is much longer tha the others. But perhaps in all cases, the sum of the heights is stil greater than 9r. Wait, but in the example with sides 2,2,3, the sum of heights was appr oximately 5.291 and 9r approx 5.103. The difference isnt huge, but the inequal ity holds. So even in relatively \"flat\" triangle, the inequality still holds. Therefore, it seems that through multiple examples and applying the AM -HM inequa lity, the original statement is valid. Another approach could be using trigonometric identities. Lets denote the trian gle with sides a, b, opposite to angles A, B, respectively. The he ights can be expressed as h_a = sin = sin B, similarly for others. But 28 thi might co mplicate things. Alternatively, since = (a + - c)/2 * tan(C/2) etc ., but per haps this is not useful here. Alternatively, express everything in terms of and the semiperimeter. Since = Delta/s, and Delta = r*s. Then, the heights can be written as h_a = 2Delta/a = 2r*s/a. Similarly, h_b = 2r*s/b, h_c = 2r*s/c. Therefore, sum of he ights is 2 r*s (1/a + 1/b + 1/c). We need to show that this is geq9r. Divide bot sides by (positive), so need to show 2s(1/a + 1/b + 1/c) geq9, which is the same as efore. So again, we come back to the same inequality. Therefore, regardless of the approach, the crux is proving (a + + c) (1/a + 1/b + 1/c) geq9, which is direct consequence of the AM-HM inequality. Hence, the original inequality holds. In conclusion, the key steps are: 1. Express the heights in terms of the area and sides. 2. Express the inradius in terms of the area and semiperimeter. 3. Formulate the inequality in terms of the sides and semiperimeter. 4. Apply the AM-HM inequality to establish the required result. 5. Verify with specific examples to ensure validity. Therefore, the inequality ( h_1 + h_2 + h_3 geq 9r ) is proven. <end_of_thought> <begin_of_solution> To prove that the sum of the heights of triangle ( h_1 + h_2 + h_3 ) is at east nine times the inradius ( ), we proceed as follows: 1. **Express Heights and Inradius**: - Heights: ( h_a = frac{2Delta}{a}, , h_b = frac{2Delta}{b}, , h_c = frac{2Delta}{c} ), where ( Delta ) is the area and ( a, b, ) 29 are the si des. - Sum of heights: ( h_1 + h_2 + h_3 = 2Deltaleft(frac{1}{a} + frac{1}{b} + frac{1}{c}right) ). - Inradius: ( = frac{Delta}{s} ), where ( = frac{a + + c}{2} ) the semiperimeter. 2. **Formulate the Inequality**: Substitute ( h_1 + h_2 + h_3 ) and ( 9r ) into the inequality: [ 2Deltaleft(frac{1}{a} + frac{1}{b} + frac{1}{c}right) geq rac{9Delt a}{s}. ] Cancel ( Delta ) and multiply through by ( ): [ 2sleft(frac{1}{a} + frac{1}{b} + frac{1}{c}right) geq 9. ] 3. **Simplify Using Semiperimeter**: Substitute ( = frac{a + + c}{2} ): [ (a + + c)left(frac{1}{a} + frac{1}{b} + frac{1}{c}right) ge 9. ] 4. **Apply the AM-HM Inequality**: The Arithmetic Mean-Harmonic Mean (AM-HM) inequality states: [ frac{a + + c}{3} geq frac{3}{frac{1}{a} + frac{1}{b} + frac {1}{c}}, ] which rearranges to: [ (a + + c)left(frac{1}{a} + frac{1}{b} + frac{1}{c}right) ge 9. ] Equality holds if and only if ( = = ) (equilateral triangle ). 5. **Conclusion**: The inequality ( h_1 + h_2 + h_3 geq 9r ) follows directly from the AM-HM inequality applied to the sides of the triangle. Examples and edge cas es confirm 30 the validity of the result. **Final Answer**: boxed{h_{1} + h_{2} + h_{3} geq 9r} <end_of_solution>"
        }
    ],
    "affiliations": [
        "DeepWisdom",
        "HKUST (Guangzhou)",
        "Independent Researcher"
    ]
}