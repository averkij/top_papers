{
    "paper_title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
    "authors": [
        "Chulun Zhou",
        "Chunkang Zhang",
        "Guoxin Yu",
        "Fandong Meng",
        "Jie Zhou",
        "Wai Lam",
        "Mo Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks."
        },
        {
            "title": "Start",
            "content": "IMPROVING MULTI-STEP RAG WITH HYPERGRAPHBASED MEMORY FOR LONG-CONTEXT COMPLEX RELATIONAL MODELING Chulun Zhou1, Chunkang Zhang, Guoxin Yu, Wai Lam1, Mo Yu2 The Chinese University of Hong Kong1, WeChat AI2 {clzhou,wlam}@se.cuhk.edu.hk, moyumyu@global.tencent.com zkang5051@gmail.com Fandong Meng2, Jie Zhou2, 5 2 0 D 0 3 ] . [ 1 9 5 9 3 2 . 2 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Multi-step retrieval-augmented generation (RAG) has become widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMEM, hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMEM on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks."
        },
        {
            "title": "INTRODUCTION",
            "content": "Single-step retrieval-augmented generation (RAG) often proves insufficient for resolving complex queries within long contexts (Trivedi et al., 2023; Shao et al., 2023; Cheng et al., 2025), motivating the shift toward multi-step RAG methods that iteratively interleave retrieval with reasoning. To effectively capture dependencies across steps and condense the lengthy processing history, many approaches incorporate working memory mechanisms inspired by human cognition (Lee et al., 2024; Zhong et al., 2024). However, current memory-enhanced multi-step RAG methods still face challenges in complex relational modeling, especially for resolving global sense-making tasks over long contexts. During multi-step RAG execution, straightforward implementation of working memory mechanism is to let large language model (LLM) summarize the interaction history into plaintext description of current problem-solving state. This strategy has been widely adopted since early studies (Li et al., 2023; Trivedi et al., 2023) as well as in commercial systems (Jones, 2025; Shen & Yang, 2025). Nonetheless, such unstructured memory mechanisms cannot be manipulated with sufficient *Equal contribution. 1We release our code at https://github.com/Encyclomen/HGMem : Co-corresponding authors."
        },
        {
            "title": "Under review",
            "content": "accuracy across steps and often lose the ability to back-trace references to retrieved texts. Consequently, recent research has shifted toward structured or semi-structured working memory, typically with predefined schemas such as relational tables (Lu et al., 2023), knowledge graphs (Oguz et al., 2022; Xu et al., 2025), or event-centric bullet points (Wang et al., 2025). However, existing memory mechanisms often treat memory as static storage that continually accumulates meaningful but primitive facts. This view overlooks the evolving nature of human working memory, which incrementally incorporates higher-order correlations from previously memorized content. This capacity is particularly crucial for resolving global sense-making tasks that involve complex relational modeling over long contexts. In such scenarios, the required knowledge for tackling query is often composed of complex structures that extend beyond predefined schemas, and reasoning over long lists of primitive facts is both inefficient and prone to confusion with mixed or irrelevant information. Current memory mechanisms in multi-step RAG systems lack these abilities, preventing memory from effectively guiding LLMs interaction with external data sources. These limitations highlight the need for working memory with stronger representational capacity. In this paper, we propose hypergraph-based memory mechanism (HGMEM) for multi-step RAG systems, which enables memory to evolve into more expressive structures that support complex relational modeling to enhance LLMs understanding over long contexts. Hypergraphs, as generalization of graphs, are particularly well-suited for this purpose (Feng et al., 2019). In our design, memory is structured as hypergraph composed of hyperedges, each treated as distinct memory point that represents specific perspective of the memorized information. Initially, these memory points encode low-order primitive facts. As the LLM interacts with external environments, higherorder correlations among memory points gradually emerge and are progressively integrated into the memory through update, insertion, and merging operations. At each step before response generation, the LLM examines the current memory and generates subqueries, enabling adaptive memory-based evidence retrieval for both focused local investigation and broad global exploration. This rich and structured memory facilitates broader contextual awareness and stronger reasoning in real-world applications by offering several advantages. First, it maintains an integrated body of knowledge around the focal problem by synthesizing primitive evidence and intermediate thoughts, typically going beyond predefined schemas and providing global perspective over the evidence. Second, it offers structured and accurate guidance for the LLMs sustained interactions in two ways: (1) enabling subsequent reasoning to start from representational propositions rather than from long list of disparate primitive facts; and (2) leveraging the topological structure of hypergraph to guide subquery generation and evidence retrieval in more accurate manner. We conduct extensive experiments on several challenging tasks involving global sense-making questions within long contexts. The results show that our HGMEM achieves significant improvements over competitive RAG baselines, confirming the advantages."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 WORKING MEMORY MECHANISMS FOR MULTI-STEP RAG Starting from ReAct (Yao et al., 2023), many multi-step RAG systems have incorporated reflections to integrate available information for subsequent decisions. These reflections can be regarded as simple form of memory. With the development of structured indexing for RAG, working memory also borrows this idea. Prevailing studies (Li et al., 2023; 2025a; Shen & Yang, 2025; Chhikara et al., 2025; Xu et al., 2025) save agent behavior, such as task decomposing, execution tracking, and result verification, to manage task context more effectively, representing step toward explicit working memory for complex multi-agent coordination. This idea also matured in chain-of-thought (CoT) and multi-round RAG, where working memory is represented as iteratively updated records of reasoning steps or retrieved evidence. For example, IRCOT (Trivedi et al., 2023) and ComoRAG (Wang et al., 2025) employ dynamic memory workspace to iteratively consolidate past knowledge or steps and incorporate new evidence, supporting scalable and iterative reasoning across multiple steps. Some studies take step further to adopt graph-structured working memory to enhance multi-step RAG (Liu et al., 2024; Li et al., 2025a). ERA-CoT (Liu et al., 2024) aids LLMs in understanding context through series of pre-defined reasoning substeps performing entity-relationship analysis."
        },
        {
            "title": "Under review",
            "content": "KnowTrace (Li et al., 2025a) equips LLMs with graph-based working memory to trace relevant knowledge through multi-step RAG execution. However, the working memories of these graphenhanced work do not effectively support modeling high-order correlations among multiple entities/relationships as each edge in their graphs can intrinsically describe at most binary relationships. By contrast, due to the high-order nature of hypergraph structure, our HGMEM naturally enables its working memory to evolve into more expressive forms capable of flexibly modeling high-order n-ary (n > 2) relations. This advantage helps to fully unleash the reasoning capability of LLMs for multi-step RAG, especially crucial for resolving global sense-making questions that require complex reasoning and deep understanding over long contexts."
        },
        {
            "title": "2.2 RAG WITH STRUCTURED KNOWLEDGE INDEX",
            "content": "There is long line of work that studies managing extended corpora through structured knowledge indexing to enhance RAG. Though different from our focus on working memory mechanism, these work can be viewed as building structured (and static) long-term memory before actually tackling user queries, thus are relevant. Specifically, tree-structured methods, such as RAPTOR (Sarthi et al., 2024), T-RAG (Fatehkia et al., 2024), and TreeRAG (Tao et al., 2025), organize text chunks or entity hierarchies, enabling multi-level or bidirectional retrieval to enhance context integration. Another line of research focuses on building graph-structured index to flexibly represent knowledge for enhancing RAG systems (Xu et al., 2024a; Edge et al., 2024; Guo et al., 2024; Li et al., 2025b). For example, GraphRAG (Edge et al., 2024) and LightRAG (Guo et al., 2024) build entity graphs and community-level summaries, or leverage graph-enhanced indexing for dual-level retrieval, leading to improvements in global reasoning, retrieval efficiency, and response diversity. CAM (Li et al., 2025b) proposes constructivist agentic memory that flexibly assimilates and accommodates input texts within hierarchical graph. HypergraphRAG (Luo et al., 2025) and PropRAG (Wang, 2025) adopt hypergraph to build their structured knowledge index and design retrieval/search algorithms for query resolving. In addition, there are also range of other memory mechanisms, essentially structured knowledge index, that simulate long contexts or dialog histories as long-term memory to improve RAG systems. According to the form of memory representation, they can be basically classified as contextual memory (Chen et al., 2023; Gutierrez et al., 2024; Lee et al., 2024; Li et al., 2024b; Gutierrez et al., 2025) and parametric memory (Qian et al., 2025). However, these existing studies merely leverage their structured index (or memory) as static storage, which are typically constructed during an offline indexing stage before actually responding to user queries."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "We introduce HGMEM, the hypergraph-based memory mechanism designed to facilitate better contextual awareness and reasoning in multi-step RAG settings with structured data sources, especially for long-context tasks that require complex global sense-making. 3.1 PROBLEM FORMULATION In this work, we consider the kind of tasks for LLMs to resolve query based on given document. Besides the plain texts, we assume that the document has been preprocessed into graph through an offline graph-building stage, where entities and relationships are extracted from the document passage.Formally, let us denote the document as segmented into set of small manageable text chunks {d1, d2, ..., dD}, and the derived graph as composed of nodes VG and edges EG corresponding to the extracted entities and relationships, respectively. Each node VG or edge EG is associated with the source text chunks in which its embodied entity/relationship appears, which is recorded during the offline graph construction. Meanwhile, the nodes, edges, and text chunks are embedded into high-dimensional vectors for vector-based retrieval. For resolving the query, LLMs have access to both the document and its derived graph as structured data sources."
        },
        {
            "title": "Under review",
            "content": "Figure 1: (i) The RAG system at its t-th interaction step. ①: The LLM adaptively generates set of subqueries Q(t) for either local investigation or global exploration (see Section 3.4). ②: Q(t) are used to retrieve information from and G. ③: VQ(t), E(VQ(t) ) and D(VQ(t)) are obtained through graph-based indexing and vector-based matching. ④: The LLM evolves current memory M(t) into M(t+1) using Equation 2. (ii) The structure of our proposed hypergraph-based memory that evolves through update, insertion and merging operations. 3.2 MULTI-STEP RAG SYSTEM WITH MEMORY When dealing with tasks requiring comprehensive understanding, especially over the long context, RAG systems usually resort to multi-step approaches with an underlying memory mechanism, where retrieval operations are interleaved with intermediate reasoning to support broader contextual awareness. Given target query ˆq, the LLM iteratively interacts with and while managing memory to store relevant information for ultimately resolving ˆq. During each interaction step t, the LLM judges whether the content of the current memory has been sufficient with respect to the target query. If the memory is deemed sufficient, it immediately produces response. Otherwise, it analyzes current memory and generates several subqueries Q(t) that aim at fetching more information from the external environment to enrich the memory. The prompts for generating subqueries are given in Appendix E. Let RV (Q) define the entity retrieval operation fetching the most relevant nodes to query set from candidate node set using vector-based matching: RV (Q) = (cid:91) qQ nv argmax (sim(hq, hv)), (1) where nv is the number of retrieved entities per query, hq is the vector representation of q, hv is the vector representation of v, and sim(, ) is the cosine similarity function. As illustrated in Figure 1 (i), at the t-th step, if the LLM proceeds to generate subqueries Q(t) based on current memory M(t) maintained until the previous step, it retrieves set of the most relevant entities VQ(t) = RVG (Q(t)) from VG. Then, via graph-based indexing, the relationships and text chunks associated with the entities in VQ(t) are also obtained, represented as E(VQ(t) ) and D(VQ(t) ), respectively.2 Subsequently, the LLM analyzes and consolidates this retrieved information into the 2We also use vector-based filtering to keep at most ne relationships and nd text chunks."
        },
        {
            "title": "Under review",
            "content": "memory, evolving memory into M(t+1), which can be formalized as M(t+1) LLM(M(t), VQ(t) , E(VQ(t)), D(VQ(t))). Note that, at the initial step (t = 0), we treat the target query ˆq as special subquery belonging to Q(0), i.e. Q(0)={ˆq}. Further details about the memory storage, subquery generation and the dynamics of memory evolving will be elaborated in Section 3.3, Section 3.4 and Section 3.5, respectively. (2)"
        },
        {
            "title": "3.3 HYPERGRAPH-BASED MEMORY STORAGE",
            "content": "When the LLM interacts with the document and the graph G, it continuously consolidates relevant information into the memory storage M, which is modeled as hypergraph: = (VM, EM), (3) where VM = {v1, v2, ...} is the vertex set and EM = {e1, e2, ...} is the hyperedge set. It should be noted that the vertices in VM are actually equivalent to those nodes in VG, both embodying identified entities. Particularly, VM is subset of VG. In our implementation, we ensure that each vertex vi VM must also exist in G.3 Formally, every vertex vi VM is represented as vi = (Ωent vi , Dvi), (4) where Ωent stands for the information of its embodied entity, including name and description, and vi Dvi denotes the set of text chunks associated with this vertex vi. Similarly, every hyperedge ej EM is represented as ej = (Ωrel ej , Vej ), where Ωrel ej characterizes the description of the embodied relationship and Vej is the set of involved vertices subordinate to this hyperedge ej. Particularly, the hyperedges can be treated as separate memory points, each of which corresponds to certain aspect of the entire information stored in current memory, as shown in Figure 1 (ii). Unlike those binary edges EG that connect at most two nodes in the external graph, hyperedge can connect an arbitrary number (two or more) of vertices. In this way, our hypergraph-based memory is capable of flexibly modeling high-order correlation among multiple vertices (n 2). As result, the whole memory as hypergraph can effectively support complex relational modeling, ensuring strong expressiveness to enhance LLMs reasoning. (5) 3.4 ADAPTIVE MEMORY-BASED EVIDENCE RETRIEVAL As described in Section 3.2, at each step of our RAG workflow, with respect to the target query, the LLM determines whether to immediately produce response or proceed to acquire more information from the external documents and graph G. ) is deemed insufficient, the LLM first analyzes M(t) and generates several subqueries Q(t) indicating what to further explore. Specifically, we design an adaptive memory-based evidence retrieval strategy for either local investigation or global exploration with Q(t): If current memory M(t) = (V (t) , (t) (i) Local Investigation: When the LLM plans to more deeply investigate some specific memory points, its generated subqueries are utilized to trigger local evidence retrieval over G. Concretely, suppose Q(t) especially targets at inspecting ej (t) , the nodes corresponding to the vertices Vej subordinate to ej are used as anchor nodes on G. Thereafter, using the operation defined by Equation 1, entity retrieval is conducted within the neighborhood of these anchors, which is formalized as Vq = RN (Vej )(q), (Vej ) = (cid:91) (NM(t) (v) NG(v)), (6) vVej where NM(t) (v) represents the neighboring vertices of over M(t) and NG(v) represents the neighboring nodes of over G. 3If any vertex does not exist in VG, we forcibly insert it, along with its associated relationships, into G."
        },
        {
            "title": "Under review",
            "content": "Figure 2: An illustration of memory evolving dynamics. Each point is equivalent to hyperedge in the hypergraph. M(t) evolves into M(t+1) through update, insertion and merging operations. (ii) Global Exploration: When there are unexplored aspects transcending the scope of current memory, the LLM resorts to generating subqueries for exploring broader information from the external documents and graph, not pertinent to any existing memory point. For Q(t), the process of entity retrieval can be written as Vq = RC(M(t))(q), C(M(t)) = VG VM(t) , where C(M(t)) represents the available scope comprised of all nodes except those already existing in the current memory. (7) Then, as in Section 3.2, the associated relationships E(Vq) and text chunks D(Vq) are obtained via graph-based indexing. Finally, following Equation 2, the LLM evolves its current memory M(t) into M(t+1). Under such strategy, the RAG system is able to adaptively combine both local investigation and global exploration for more flexible information retrieval during interaction with external data sources. 3.5 DYNAMIC OF MEMORY EVOLVING Once set of subqueries have been generated at the t-th step, following Equation 2, the LLM analyzes the retrieved information and consolidates useful content into the current memory M(t), resulting in the evolved memory M(t+1). As shown in Figure 1 (ii), on the basis of hypergraphbased memory storage, the dynamic of memory evolving in our proposed HGMEM involves the following three types of operations: Update. According to the retrieved information, if there are certain existing memory points whose descriptions should be modified, the update operation will revise the descriptions of corresponding hyperedges without changing their subordinate entities. Insertion. The insertion operation should be evoked when some content of the retrieved information is suitable to be inserted as additional memory points into the current memory, which creates new hyperedges in the hypergraph. Merging. After insertion and update, the LLM inspects current memory and selectively merges existing memory points that are more suitable to constitute single semantically/logically cohesive unit. With respect to the target query ˆq, suppose the memory points ei=(Ωrel , Vei) and ei , Vej ) are to be merged into high-order memory point ek=(Ωrel ej=(Ωrel , Vek ), its description ek ej and subordinate vertices are acquired as Ωrel ek , Ωrel ej , ˆq) (8) LLM(Ωrel ei Vek = Vei Vej . Then, the newly merged memory point is added into the hyperedge set EM(t) of the current memory M(t). This merging operation over the hypergraph-based memory builds higher-order correlations among multiple existing memory points, facilitating the resolution of queries that require complex relational modeling with disparate facts. In this way, besides continuously accumulating primitive facts during the LLMs interactions with external data sources, the memory also gradually evolves into more sophisticated forms, capturing higher-order correlations for complex relational modeling. Figure 2 gives concrete example illustrating the dynamics of memory evolving."
        },
        {
            "title": "3.6 MEMORY-ENHANCED RESPONSE GENERATION",
            "content": "M , (t) in current memory When the LLM exceeds its maximum interaction steps or the content M(t) = (V (t) ) has been deemed sufficient, response is immediately produced according to the information stored in current memory. Concretely, besides the descriptions of all memory points (i.e. (t) in current memory are also provided to the LLM for producing the final response. ), the text chunks associated with all the entities (t)"
        },
        {
            "title": "4.1 DATASETS",
            "content": "We choose generative sense-making question answering (QA) (Edge et al., 2024; Guo et al., 2024) and long narrative understanding (Li et al., 2024a; Xu et al., 2024b; Yu et al., 2025; Kocisky et al., 2018; Karpinska et al., 2024; Yen et al., 2025; Zhou et al., 2025) as our evaluation tasks. For generative sense-making QA, similar to the setups used in previous works (Edge et al., 2024; Guo et al., 2024), we retain portion of long documents with more than 100k tokens from Longbench V2 (Bai et al., 2025). From each retained document, we use GPT-4o to generate several global sense-making queries that satisfy the following requirements: 1) The queries should target the overall understanding of the whole provided documents, instead of only concentrating on several specific phrases or sentence pieces. 2) The queries should require high-level understandings and global reasoning over disparate evidence scattered across the whole paragraph. For long narrative understanding, we use three public benchmarks including NarrativeQA (Kocisky et al., 2018), NoCha (Karpinska et al., 2024) and Prelude (Yu et al., 2025). Both tasks require global comprehension and complex sensemaking over disparate evidence across long contexts. Details about the usage and statistics of data used in our experiments are given in Appendix A. 4.2 IMPLEMENTATION DETAILS Offline Graph Construction. For all the datasets used in our experiments, we first segment every document into text chunks of 200 tokens with 50 overlapping tokens between adjacent chunks. Then, GPT-4o is utilized to preprocess each of the chunkized documents into graph using the open-sourced tool provided by LightRAG (Guo et al., 2024). After building the graph, we adopt bge-m3 (Chen et al., 2024) as the embedding model to convert all the entities, relationships and text chunks into vector representations managed by nano vector database. System Deployment and Configuration. Our RAG system is comprised of the backbone LLM and the hypergraph-based memory. We choose GPT-4o and Qwen2.5-32B-Instruct as the representatives of advanced closed-source and open-source LLMs, respectively. During experiments, GPT-4o is accessed through the official API while Qwen2.5-32B-Instruct is locally deployed with VLLM (Kwon et al., 2023). For the configuration of LLM inference, we set the temperature to 0.8 and the maximum number of output tokens to 2,048. As for the hypergraph-based memory, we employ the hypergraph-db package to maintain and manage the hypergraph at runtime. The vector representations of the nodes, hyperedges and associated text chunks in the hypergraph are also generated by bge-m3 embedding model. 4.3 BASELINES AND EVALUATION METRICS In our experiments, we compare our proposed HGMEM to two types of baseline methods, i.e. traditional RAG and multi-step RAG, which utilize plain texts and/or graph-structured data sources. Among these methods, DeepRAG (Guan et al., 2025) and ComoRAG (Wang et al., 2025) are equipped with working memory, while the others are not. The details of these comparison methods can be found in Appendix B. To ensure fair comparison, all baselines operate on similar number of retrieved passages. In the case of single-step RAG, this means retrieving the same average number of text chunks as our HGMEM. For multi-step RAG methods, we approximate comparability by constraining them to rewrite the same maximum number of subqueries and perform the same maximum number of steps, while requiring retrieval of the same average number of chunks per step."
        },
        {
            "title": "Under review",
            "content": "Table 1: The overall experimental results on four benchmarks. The second column Working Memory distinguishes whether the corresponding method is equipped with working memory that enhances LLMs during RAG execution. The best scores in each dataset are bolded. HGMEM consistently outperforms other comparison methods across all datasets. Type GPT-4o Working Memory Method Longbench NarrativeQA NoCha Prelude Comprehensiveness Diversity Acc (%) Acc (%) Acc (%) Traditional RAG Multi-step RAG Ours Qwen2.5-32B-Instruct Traditional RAG Multi-step RAG Ours NaiveRAG GraphRAG LightRAG HippoRAG v2 DeepRAG ComoRAG HGMEM NaiveRAG GraphRAG LightRAG HippoRAG v2 DeepRAG ComoRAG HGMEM 61.62 60.39 61.55 58.92 63.62 62.18 65. 61.41 60.78 60.82 56.66 61.45 60.74 64.18 64.20 64.02 63.37 61.27 65.98 65.82 69.74 62.25 62.16 62.73 60.80 63.56 61.28 66.51 52.00 53.00 44.00 34.00 45.00 54.00 55.00 37.00 44.00 40.00 33.00 44.00 44.00 51.00 67.46 70.63 71.43 72.22 67.46 63.49 73. 64.29 62.70 59.52 68.25 66.40 57.60 70.63 60.00 59.26 61.48 54.81 56.30 54.07 62.96 52.59 50.37 60.74 51.85 51.11 50.37 62.22 For generative sense-making QA, we adopt the following two metrics to assess the qualities of model responses: 1) Comprehensiveness measures how well the model response comprehensively covers and addresses all aspects and necessary details with respect to the target query. 2) Diversity indicates how rich and diverse the response is in providing various perspectives and insights related to the query. We employ GPT-4o as the judge to evaluate the model responses according to the grading criteria that gives scores ranging from 0 to 100 based on two-step scoring scheme, as detailed in Appendix F. For long narrative understanding, including NarrativeQA, Nocha, and Prelude, we uniformly use prediction accuracy (Acc) as the reported metric. Specifically, for NarrativeQA, prior studies (Bulian et al., 2022; Wang et al., 2024; Zhou et al., 2025) have shown that conventional token-level metrics such as Exact Match and F1 score usually fail to reflect actual semantic equivalence between hypothesis and reference answer, especially for abstractive answers. Therefore, we also apply GPT-4o for judging whether the LLMs prediction fully entails the reference answer, producing binary True/False decision."
        },
        {
            "title": "5 RESULTS AND ANALYSIS",
            "content": "5.1 OVERALL RESULTS Table 1 reports the overall results across all evaluation tasks. Our HGMEM consistently outperforms both single-step and multi-step RAG baselines on every dataset. Importantly, our HGMEM with Qwen2.5-32B-Instruct matches or even outperforms baselines powered by the stronger GPT-4o, underscoring its value in resource-efficient scenarios. The baselines exhibit mixed performance patterns reflecting their respective representational strengths. For instance, HippoRAG v2 relies on knowledge triples, which provide strong fact representation but limited coverage of events and plots. As result, it performs well on NoCha but falls behind NaiveRAG on NarrativeQA. In contrast, GraphRAG and LightRAG excel at building global representations but are weaker at capturing fine-grained details, leading them to outperform other baselines on Prelude and NarrativeQA. The two multi-step RAG methods, which mainly employ working memory to iteratively generate subqueries in chaining fashion, struggle with sensemaking questions, where integrating higher-order relationships is essential. In comparison, our HGMEM provides strong compositional representations that span from facts to plots, equipping LLM reasoning with high-order correlations and integrated evidence. This enables it to meet the diverse requirements posed by the evaluation tasks."
        },
        {
            "title": "Under review",
            "content": "Figure 3: Prediction accuracies at different steps using Qwen2.5-32B-Instruct on long narrative understanding datasets. Table 2: Ablation results using Qwen2.5-32B-Instruct. w/. GE Only and w/. LI Only stand for subquery generation strategies with Global Exploration and Local Investigation, respectively. w/o. Update and w/o. Merging refer to HGMEM ablating update and merging operations during memory evolving, respectively. Ablation Type Method Retrieval Strategy Memory Evolution HGMEM w/. GE Only w/. LI Only HGMEM w/o. Update w/o. Merging Longbench NarrativeQA Nocha Prelude Comprehensiveness Diversity Acc (%) Acc (%) Acc (%) 64.18 59.25 61. 64.18 62.48 61.76 66.51 61.67 63.82 66.51 64.92 61.80 51.00 47.00 43.00 51.00 50.00 43.00 70.63 68.25 63. 70.63 68.25 61.11 62.22 59.26 60.00 62.22 60.00 57.78 5.2 PERFORMANCE AT DIFFERENT STEPS During the execution of our multi-step RAG system, the memory progressively evolves and guides the LLM to proceed with retrieval and reasoning. To inspect the effects of memory evolving over multiple interaction steps, we force the LLM to generate responses at every step for total of six turns, even if it originally decides to terminate the iteration earlier. Figure 3 presents the performances at different steps using Qwen2.5-32B-Instruct on long narrative understanding tasks. Note that t=0 represents the initial step when the target query ˆq is used for retrieval. We can observe that our HGMEM achieves its best performance at t=3, mostly outperforming NaiveRAG and LightRAG baselines across steps. More steps bring no further improvements at higher cost. 5.3 ABLATION STUDIES Evidence Retrieval Strategy. When the LLM determines to acquire more information from and G, our HGMEM adopts an adaptive memory-based evidence retrieval strategy for either focused local investigation or broad global exploration (Section 3.4). To investigate the effects of such strategy, in Table 2, we compare our strategy to the variants that involve only Local Investigation or Global Exploration, represented as w/. LI Only and w/. GE Only, respectively. The results show that both w/. LI Only and w/. GE Only significantly underperforms the adaptive strategy across all datasets, demonstrating the effectiveness and necessity of adaptively combining the two modes of evidence retrieval. Effects of Update and Merging Operations. The memory evolving in our HGMEM involves update, insertion and merging operations, where merging is especially critical to building highorder correlations from primitive facts. Because insertion is indispensable, we just carry out ablation experiments on all datasets using Qwen2.5-32B-Instruct to assess the effects of update and merging operations, as shown in Table 2. Compared to the HGMEM, removing either operation leads to performance drop, while removing merging (w/o. Merging) causes substantially larger degradation than removing update (w/o. Update). It reflects the effectiveness of both operations, especially highlighting the importance of high-order correlations built through merging operations."
        },
        {
            "title": "Under review",
            "content": "Table 3: Average number of entities per hyperedge (Avg-Nv) in final memory and prediction accuracy (Acc) for subset of 120 sampled primitive and sense-making queries. Query Type Method NarrativeQA Nocha Prelude Avg-Nv Acc (%) Avg-Nv Acc (%) Avg-Nv Acc (%) Primitive Sense-making HGMEM w/o. Merging HGMEM w/o. Merging 3.35 3.32 7.07 4.10 70.00 70.00 40.00 30.00 3.78 3. 7.97 3.80 60.00 65.00 70.00 60.00 3.85 3.73 5.25 3.74 55.00 60. 60.00 55."
        },
        {
            "title": "5.4 DISSECTING QUERY RESOLVING: PRIMITIVE VS. SENSE-MAKING",
            "content": "To better understand how our proposed HGMEM brings improvement to the evaluation tasks, we conduct targeted analysis across different query types. Specifically, we randomly sample 40 queries from each long narrative understanding dataset used in our experiments, yielding total of 120 queries. These are then manually categorized into two representative types: Primitive Query: Queries that primarily require locating directly associated chunks, which can often be resolved with local evidence and focus on straightforward factual information. Sense-making Query: Queries that require deeper comprehension by connecting and integrating multiple pieces of evidence, emphasizing the construction of higher-order relationships and interpretation beyond surface retrieval. We compare both prediction accuracy and the average number of entities per hyperedge (Avg-Nv) in memory before generating final responses. The latter serves as quantitative indicator of relationship complexity. Table 3 shows that on sense-making queries, our full HGMEM achieves higher accuracy with considerably larger Avg-Nv than HGMEM w/o. Merging, demonstrating that forming higher-order correlations enhances comprehension. In contrast, for primitive queries, HGMEM yields comparable or slightly lower accuracy relative to HGMEM w/o. Merging. This is likely because the full model still tends to associate additional pieces of relevant evidence (as indicated by the slightly higher Avg-Nv), even though the primitive evidence alone is sufficient to answer straightforward queries, resulting in redundancy. Notably, the Avg-Nv on sense-making queries consistently exceeds that on primitive queries, especially when merging is applied. Taken together, these results indicate that HGMEM improves contextual understanding by constructing high-order correlations for complex relational reasoning, rather than relying on shallow accumulation of surface facts."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we propose HGMEM, the hypergraph-based memory mechanism that aims at improving multi-step RAG by enabling the evolving of memory into more sophisticated forms for complex relational modeling. In HGMEM, the memory is structured as hypergraph composed of set of hyperedges as separate memory points. HGMEM allows the memory to progressively establish high-order correlations among previously accumulated primitive facts during the execution of multi-step RAG systems, guiding LLMs to organize and connect thoughts for focal problem. Extensive experiments and in-depth analysis validate the effectiveness of our method over strong RAG baselines on challenging datasets featuring global sense-making questions over long context."
        },
        {
            "title": "7 REPRODUCIBILITY STATEMENT",
            "content": "To ensure reproducibility, we introduce the usage and statistics of our used datasets in Section 4.1 and Appendix A. We also give the implementation details about the offline graph construction, system deployment and configuration in Section 4.2. Appendix gives the prompts for updating, inserting and merging memory points for memory evolving during multi-step RAG execution. Appendix describes the procedures for subquery generation with detailed prompts. Appendix gives the evaluation prompts for scoring model responses in the generative sense-making QA task."
        },
        {
            "title": "REFERENCES",
            "content": "Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. In Proceedings of Association for Computational Linguistics, pp. 36393664, 2025. Jannis Bulian, Christian Buck, Wojciech Gajewski, Benjamin Borschinger, and Tal Schuster. Tomayto, tomahto. beyond token-level answer equivalence for question answering evaluation. CoRR, abs/2202.07654, 2022. Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. Walking down the memory maze: Beyond context limit through interactive reading. CoRR, abs/2310.05029, 2023. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. BGE m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. CoRR, abs/2402.03216, 2024. Mingyue Cheng, Yucong Luo, Jie Ouyang, Qi Liu, Huijie Liu, Li Li, Shuo Yu, Bohou Zhang, Jiawei Cao, Jie Ma, Daoyu Wang, and Enhong Chen. survey on knowledge-oriented retrievalaugmented generation. CoRR, abs/2503.10677, 2025. Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building production-ready AI agents with scalable long-term memory. CoRR, abs/2504.19413, 2025. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. From local to global: graph RAG approach to query-focused summarization. CoRR, abs/2404.16130, 2024. Masoomali Fatehkia, Ji Kim Lucas, and Sanjay Chawla. T-RAG: lessons from the LLM trenches. CoRR, abs/2402.07483, 2024. Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 35583565, 2019. Xinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, and Jie Zhou. Deeprag: Thinking to retrieval step by step for large language models. CoRR, abs/2502.01142, 2025. Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. Lightrag: Simple and fast retrievalaugmented generation. CoRR, abs/2410.05779, 2024. Bernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neurobiologically inspired long-term memory for large language models. In Proceedings of Neural Information Processing Systems, 2024. Bernal Jimenez Gutierrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, and Yu Su. From RAG to memory: Non-parametric continual learning for large language models. CoRR, abs/2502.14802, 2025. Nicola Jones. Openais deep researchtool: is it useful for scientists? Nature, 2025. Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, and Mohit Iyyer. One thousand and one pairs: novel challenge for long-context language models. In Proceedings of EMNLP, pp. 1704817085, 2024. Tomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328, 2018. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the Symposium on Operating Systems Principles, pp. 611626, 2023."
        },
        {
            "title": "Under review",
            "content": "Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John F. Canny, and Ian Fischer. human-inspired reading agent with gist memory of very long contexts. In Proceedings of International Conference on Machine Learning, 2024. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. CAMEL: communicative agents for mind exploration of large language model society. In Proceedings of Neural Information Processing Systems, 2023. Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. Loogle: Can long-context language models understand long contexts? In Proceedings of Association for Computational Linguistics, pp. 1630416333, 2024a. Rui Li, Quanyu Dai, Zeyu Zhang, Xu Chen, Zhenhua Dong, and Ji-Rong Wen. Knowtrace: Bootstrapping iterative retrieval-augmented generation with structured knowledge tracing. CoRR, abs/2505.20245, 2025a. Rui Li, Zeyu Zhang, Xiaohe Bo, Zihang Tian, Xu Chen, Quanyu Dai, Zhenhua Dong, and Ruiming Tang. CAM: constructivist view of agentic memory for llm-based reading comprehension. CoRR, abs/2510.05520, 2025b. Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li, Wanli Ouyang, Wenbo Su, and Bo Zheng. Graphreader: Building graph-based agent to enhance long-context abilities of large language models. In Findings of Empirical Methods in Natural Language Processing, pp. 1275812786, 2024b. Yanming Liu, Xinyue Peng, Tianyu Du, Jianwei Yin, Weihao Liu, and Xuhong Zhang. Era-cot: Improving chain-of-thought through entity relationship analysis. In Proceedings of Association for Computational Linguistics, pp. 87808794, 2024. Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, and Yunsheng Wu. Memochat: Tuning llms to use memos for consistent long-range open-domain conversation. CoRR, abs/2308.08239, 2023. Haoran Luo, Haihong E, Guanting Chen, Yandan Zheng, Xiaobao Wu, Yikai Guo, Qika Lin, Yu Feng, Ze-min Kuang, Meina Song, Yifan Zhu, and Luu Anh Tuan. Hypergraphrag: Retrieval-augmented generation with hypergraph-structured knowledge representation. CoRR, abs/2503.21322, 2025. Barlas Oguz, Xilun Chen, Vladimir Karpukhin, Stan Peshterliev, Dmytro Okhonko, Michael Sejr Schlichtkrull, Sonal Gupta, Yashar Mehdad, and Scott Yih. Unik-qa: Unified representations of structured and unstructured knowledge for open-domain question answering. In Findings of the Association for Computational Linguistics: NAACL, pp. 15351546, 2022. Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Defu Lian, Zhicheng Dou, and Tiejun Huang. Memorag: Boosting long context processing with global memory-enhanced retrieval augmentation. In Proceedings of WWW 2025, pp. 23662377, 2025. Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. Manning. RAPTOR: recursive abstractive processing for tree-organized retrieval. In Proceedings of International Conference on Learning Representations, 2024. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. In Findings of EMNLP, pp. 92489274. Association for Computational Linguistics, 2023. Minjie Shen and Qikai Yang. From mind to machine: The rise of manus AI as fully autonomous digital agent. CoRR, abs/2505.02024, 2025. Wenyu Tao, Xiaofen Xing, Yirong Chen, Linyi Huang, and Xiangmin Xu. Treerag: Unleashing the power of hierarchical storage for enhanced knowledge retrieval in long documents. In Findings of the Association for Computational Linguistics, pp. 356371, 2025."
        },
        {
            "title": "Under review",
            "content": "Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. trieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. ceedings of the Association for Computational Linguistics, pp. 1001410037, 2023. Interleaving reIn ProJingjin Wang. Proprag: Guiding retrieval with beam search over proposition paths. CoRR, abs/2504.18070, 2025. Juyuan Wang, Rongchen Zhao, Wei Wei, Yufeng Wang, Mo Yu, Jie Zhou, Jin Xu, and Liyan Xu. Comorag: cognitive-inspired memory-organized RAG for stateful long narrative reasoning. CoRR, abs/2508.10419, 2025. Yang Wang, Alberto Garcia Hernandez, Roman Kyslyi, and Nicholas Kersting. Evaluating quality of answers for retrieval-augmented generation: strong LLM is all you need. CoRR, abs/2406.18064, 2024. Liyan Xu, Jiangnan Li, Mo Yu, and Jie Zhou. Fine-grained modeling of narrative context: coherence perspective via retrospective questions. In Proceedings of the Association for Computational Linguistics, pp. 58225838, 2024a. Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. A-MEM: agentic memory for LLM agents. CoRR, abs/2502.12110, 2025. Zhe Xu, Jiasheng Ye, Xiangyang Liu, Tianxiang Sun, Xiaoran Liu, Qipeng Guo, Linlin Li, Qun Liu, Xuanjing Huang, and Xipeng Qiu. Detectiveqa: Evaluating long-context reasoning on detective novels. CoRR, abs/2409.02465, 2024b. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In Proceedings of International Conference on Learning Representations, 2023. Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. HELMET: how to evaluate long-context models effectively and thoroughly. In The Thirteenth International Conference on Learning Representations, 2025. Mo Yu, Tsz Ting Chung, Chulun Zhou, Tong Li, Rui Lu, Jiangnan Li, Liyan Xu, Haoshu Lu, Ning Zhang, Jing Li, and Jie Zhou. PRELUDE: benchmark designed to require global comprehension and reasoning over long contexts. CoRR, abs/2508.09848, 2025. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 1972419731, 2024. Chulun Zhou, Qiujing Wang, Mo Yu, Xiaoqian Yue, Rui Lu, Jiangnan Li, Yifan Zhou, Shunchi Zhang, Jie Zhou, and Wai Lam. The essence of contextual understanding in theory of mind: study on question answering with story characters. In Proceedings of the Association for Computational Linguistics, pp. 2261222631, 2025."
        },
        {
            "title": "Under review",
            "content": "Table 4: Statistics of data used in our experiments. #Documents, Avg. #Tokens and #Queries represent the number of documents, average tokens per document and the total number of queries, respectively. Longbench (Financial) Longbench (Governmental) Longbench (Legal) NarrativeQA Nocha Prelude #Documents Avg. #Tokens #Queries 20 266k 100 22 256k 98 7 194k 10 218k 100 4 139k 126 5 280k"
        },
        {
            "title": "A DATASET STATISTICS",
            "content": "Generative Sense-making QA. We retain portion of long documents with more than 100k tokens from Longbench V2 (Bai et al., 2025), which was originally comprised of six major task categories designed to assess the ability of LLMs to handle long-context problems. In our experiments, we select three domains of documents from the category of single-document QA, including Financial, Governmental and Legal. Long Narrative Understanding. We use the following public benchmarks: NarrativeQA (Kocisky et al., 2018): It is one of the most widely used benchmarks for story question answering. Because of its question construction strategy over high-level book summaries, the task places greater emphasis on synthesis and inference beyond local texts. In contrast, many other existing long-context QA tasks can often be solved with only local evidence, as shown by studies in (Yu et al., 2025). For evaluation, we randomly sample 10 long books exceeding 100k tokens, together with their associated queries, from the complete benchmark. NoCha (Karpinska et al., 2024): The task involves discriminating minimally different pairs of true and false claims about English fictional books. Although the format may appear different from sense-making questions, NoCha is explicitly designed to require constructing global understanding of the book in relation to the focal statement. Since the official test set is hidden, we conduct experiments using only the publicly released subset. Prelude (Yu et al., 2025): This benchmark assesses LLMs global comprehension and deep reasoning by requiring them to determine whether characters prequel story is consistent with the original book. Most instances of this task demand integrating multiple pieces of evidence or even forming holistic impression of the characters storyline. In our experiments, we use all English books included in Prelude for evaluation. Table 4 gives the detailed statistics about the data used in our experiments, including the number of documents, average tokens per document and the total number of queries. Generative sense-making QA task involves documents from Longbench V2 benchmark in Financial, Government and Legal domains. Long narrative understanding task uses NarrativeQA, Nocha and Prelude benchmarks."
        },
        {
            "title": "B COMPARISON BASELINES",
            "content": "In our experiments, we compare our methods to traditional RAG and Multi-step RAG methods. Traditional RAG includes: NaiveRAG just uses the target query to retrieve set of text chunks from the document for dealing with queries. GraphRAG (Edge et al., 2024) constructs knowledge graph from plain-text documents and build hierarchy of communities of closely related entities before using an LLM to make responses. LightRAG (Guo et al., 2024) also builds graph structure and employs dual-level retrieval strategy from both low-level and high-level evidence discovery. HippoRAG v2 (Gutierrez et al., 2025) creates knowledge graph and adopts the Personalized PageRank algorithm with dense-sparse integration of passages into the graph search process for resolving queries."
        },
        {
            "title": "Under review",
            "content": "Figure 4: The prompt for updating and inserting memory points during memory evolving in HGMEM. Figure 5: The prompt for merging memory points during memory evolving in HGMEM."
        },
        {
            "title": "Under review",
            "content": "Table 5: Statistics of the cost of online multi-step RAG execution in our HGMEM and other baselines with working memory.Avg-Token is the average count of tokens processed by LLMs per question, while Avg-Time stands for the average inference latency per question. Method NarrativeQA Nocha Prelude Avg-Token Avg-Time Avg-Token Avg-Time Avg-Token Avg-Time HGMEM w/o. Merging DeepRAG ComoRAG 4436.43 4154.02 3904.18 5083.26 15.84 14. 13.94 18.15 5252.73 4750.32 4724.07 5503.98 18.76 16.97 16.87 19.66 5421.74 4897. 4514.66 7827.56 19.36 17.49 16.12 27.96 Multi-step RAG includes: DeepRAG (Guan et al., 2025) conducts multi-step reasoning as Markov Decision Process by iteratively decomposing queries. ComoRAG (Wang et al., 2025) undergoes multi-step interactions with external data sources with dynamic memory workspace, iteratively generateing probing queries and integrating the retrieved evidence into global memory pool."
        },
        {
            "title": "C COST COMPARISON",
            "content": "We conduct cost comparison between our HGMem and other baselines with working memory in terms of token consumption and inference latency. Note that the cost of online multi-step RAG execution is the real concern for fair comparison because the offline graph construction is just for building query-agnostic indexing structure. With this focus, we measure the average token consumption and inference latency of HGMEM, ComoRAG and DeepRAG in Table 5. From the statistics, we can observe that the cost of our HGMem is basically of the same level with those of DeepRAG and ComoRAG while consistently achieving better performance. We can also see that the merging operation, which is the core operation for forming high-order correlation in our HGMem, just introduces minor computational overhead."
        },
        {
            "title": "D PROMPTS FOR MEMORY EVOLVING",
            "content": "Section 3.5 describes the dynamics of memory evolving in HGMEM, which consists of update, insertion and merging operations. The prompts for these three types of operations are given in Figure 4 and Figure 5."
        },
        {
            "title": "E PROMPTS FOR SUBQUERY GENERATION",
            "content": "During our multi-step RAG execution, the LLM needs to generate subqueries for acquiring information from external data sources. First, it raises relevant concerns that either target at specific memory points or aim at probing useful information outside current memory. Then, the LLM generates corresponding subqueries according to the raised concerns. The prompts for raising concerns and generating subqueries are given in Figure 6 and Figure 7, respectively. EVALUATION PROMPTS FOR GENERATIVE SENSE-MAKING QA For the evaluation of generative sense-making QA, we leverage GPT-4o as an evaluator to assess the quality of model responses. Given the target query and the source paragraph from which the query originated, the GPT-4o evaluator first indicates the level of comprehensiveness/diversity and then gives final score within the value range of the corresponding level. Detailed prompts for such LLMas-a-Judge evaluation. Figure 8 and Figure 9 give the prompts for scoring the comprehensiveness and diversity, respectively."
        },
        {
            "title": "Under review",
            "content": "Figure 6: The prompt for raising concerns either targeting at specific memory points or probing useful information outside the current memory. Figure 7: The prompt for generating subqueries based on previously raised concerns."
        },
        {
            "title": "Under review",
            "content": "Figure 8: The prompt for evaluating the comprehensiveness of model response. Figure 9: The prompt for evaluating the diversity of model response."
        },
        {
            "title": "G CASE STUDY",
            "content": "As shown in Table 6, we present two representative cases highlighting HGMems distinct reasoning advantages over LightRAG from the perspective of forming high-order correlations and the strategy of adaptive memory-based evidence retrieval during memory evolving. The first case is from NarrativeQA, where the question requires inferring the underlying cause of Xodars enslavementa relation not explicitly stated in the original text. LightRAG just makes incorrect surface-level predictions based on the retrieved content. While DeepRAG stores the knowledge in the memory, it does not form high-order correlation and fails to predict correctly. In contrast,"
        },
        {
            "title": "Under review",
            "content": "HGMem progressively evolves its memory and establishes high-order correlations from primitive evidences accumulated from past interactions, uncovering that Xodars punishment originates from his defeat by Carter. The second case is from Nocha, where the query mixes factual and misleading details. The LLM raises subquery about the source of the name White Sands. Using the strategy of local investigation, it particularly conducts in-depth inspection about the related memory point (Point 1) in current memory and verifies that there is no clear evidence showing the name was given by Anne. However, LightRAG mistakenly recognizes that the name White Sands was given by Anne and DeepRAG doesnt qualify the correctness of White Sands. Together, these examples show that HGMem enables deeper and more accurate contextual understanding beyond superficial text retrieval."
        },
        {
            "title": "H A TOY EXAMPLE",
            "content": "To illustrate the core workflow of our method, we present toy example in Figure 10. Given the query Why is Xodar given to Carter as slave?, the LLM first retrieves relevant evidence, converting it into structured representation (corresponding to Point 0 in the figure). It then generates sub-queries based on current memory to retrieve missing reasoning elements. In the subsequent iteration, newly retrieved evidence is integrated into the memory storage through update, insertion and merging operations, yielding unified representation that includes high-order memory points capturing complex relationships beyond surface content in original data sources. Finally, the LLM leverages its evolved memory to produce an answer to the target query. This example illustrates how the memory evolves during the multi-step RAG execution to iteratively refine its understanding and support complex relational modeling."
        },
        {
            "title": "Under review",
            "content": "Table 6: Illustrative Cases on NarrativeQA and Nocha, where red and blue stand for the relevant answer and its corresponding source, respectively Source Question NarrativeQA Why is Xodar given to Carter as slave? Golden Answer Relevent Chunks As his punishment for being defeated by Carter previously 1. Xodar was brought from the adjoining room. Issus glared at him, baleful light in her hideous eyes. And such as you are Dator of the First Born? she squealed. For the disgrace you have brought upon the Immortal Race, you shall be degraded to rank below the lowest. No longer be you Dator, but forevermore slave of slaves.... 2. ... LightRAG Retrieval&Prediction Prediction: Xodar is given to Carter as slave due to his degradation by Issus after falling from favor. Issus degrades Xodar to rank below the lowest and commands that he serve as slave to Carter, who previously conquered Xodar in the prison on the Isle of Shador. Nocha Answer TRUE if the statement is true, otherwise answer FALSE [Statement]: Anne assigns romantic names to her surroundings, like Lake of Shining Waters and White Sands. [Question]: Based on the context provided, is the above statement TRUE or FALSE? False 1. Oh, dont like that name, either. shall call itlet me seethe Lake of Shining Waters. Yes, that is the right name for it. know because of the thrill. When hit on name that suits exactly, it gives me thrill. Do things ever give you thrill? Matthew ruminated. ... 2. said Anne dreamily. Is it as nice as it sounds? Just when you said shore road, saw it in picture in my mind, as quick as that! And White Sands is pretty name, too, but dont like it as well as Avonlea. Avonlea is lovely name. It just sounds like music. ... Prediction: [Answer]: TRUE [Explanation]: The statement is true based on the information provided in the data tables. In the Relationships table, there is record that indicates Anne of Green Gables gave the romantic name Lake of Shining Waters to Barrys Pond. Additionally, in the Sources table, Anne expresses preference for the name Avonlea over White Sands, suggesting she has fondness for romantic names. Generated Subqueries: DeepRAG Retrieval&Prediction HGMEM Retrieval&Prediction Generated Subquery: 1. What factors influenced Issuss decision to demote Xodar from Dator to slave? 2.: What specific orders or instructions did Issus give to ensure Xodar was transferred to Carter as slave? 1. Is there evidence in the provided memory or data that Anne has given romantic names to other locations besides Barrys Pond? 2. Does the extended info suggest that Anne applies her romantic naming to all her surroundings? Memory: Memory: 1. Xodar, originally member of the First Born, was degraded to slave status by Issus due to his fall from favor. This demotion signifies... 2. Issus, in display of her absolute authority, degrades Xodar to the lowest status of slave, specifically instructing him to serve Carter. This act is designed to further demean Xodar and to test Carters response. 1. Anne gives the romantic name Lake of Shining Waters to Barrys Pond, indicating her imaginative and poetic nature towards her surroundings. 2. Anne finds Lovers Lane picturesque and romantic, appreciating its natural beauty and atmosphere for thoughtful reflection. This highlights her imaginative and poetic nature towards her surroundings. Prediction: Prediction: Xodar is given to Carter as slave because Issus seeks to degrade and humiliate him. This act undermines Xodars former prestige and weakens his influence, while tying Carters treatment and success to the actions of once-respected figure, likely intended to provoke internal conflict and dissatisfaction. Generated Subquery: [Answer]: TRUE [Explanation]: The statement is true. There is record that indicates Anne of Green Gables gave the romantic name Lake of Shining Waters to Barrys Pond, indicating her imaginative and poetic nature towards her surroundings. Generated Subqueries: 1. What specific incident of Xodar was considered disgrace to the Immortal Race, leading directly to his punishment by Issus? (Global Exploration) 2.: What role does Carter have in the decision to give Xodar to him as slave? (Global Exploration) 3.... 1.: Is there any evidence in Anne of Green Gables that suggests Anne gave romantic name to place known as White Sands? (Local Investigation) 2.: Whats the actual name of White Sands in the book Anne of Green Gables? (Local Investigation) 3.... Original Memory: Original Memory: Point I: Subordinate Entities: Xodar, Issus, Cowardly injustice Description: Xodar is punished by Issus as form of cowardly injustice for the disgrace he has brought upon the Immortal Race. Point II: Subordinate Entities: Xodar, Issus, Degradation Description: Issus degrades Xodar to rank below the lowest, Xodar is no longer Dator, but forever slave of slaves, to fetch and carry for the lower orders that serve in the gardens of Issus. Memory After Evolving (Guided by Subquery 1): Point (Merged): Subordinate Entities: Xodar, Issus, Cowardly injustice, Degradation Description: Xodars degradation into slave is result of his disgraceful defeat by John Carter, which brought shame to the Immortal Race, leading Issus to punish him as form of cowardly injustice. This reflects the societal judgment and loss of honor within the Immortal Races culture. Point I: Subordinate Entities: Diana, Barrys pond, Lovers lane, Gilbert, Anne, White sands, Anne of Green Gables Description: ... Anne has habit of giving romantic names to her surroundings, such as calling Barrys Pond the Lake of Shining Waters. While she does romanticize places, there is no specific mention of her assigning romantic name to White Sands. Memory after Evolving (Guided by Subquery 1): Point (Updated): Subordinate Entities: Diana, Barrys pond, Lovers lane, Gilbert, Anne, White sands, Anne of Green Gables Description: ...Anne has habit of giving romantic names to her surroundings, such as calling Barrys Pond the Lake of Shining Waters. However, there is no specific mention of her assigning romantic name to White Sands. White Sands is instead associated with Gilbert and an educational opportunity offered to Anne by the White Sands trustees. Prediction: Prediction: Xodar is given to Carter as slave due to his disgraceful defeat by John Carter, which brought shame to the Immortal Race. Issus, the goddess, punishes Xodar for his cowardice, degrading him to slave, and he is subsequently treated as such by Carter. [Answer]: FALSE [Explanation]: According to the provided information, Anne does give romantic name to Barrys Pond, referring to it as the Lake of Shining Waters. However, there is no evidence in the memory or data tables that Anne assigns romantic name to White Sands. Instead, White Sands is associated with Gilbert and an educational opportunity for Anne, and it is mentioned as geographical location without any indication that Anne romanticizes it. Therefore, the statement is false because it incorrectly includes White Sands as one of the locations Anne romanticizes."
        },
        {
            "title": "Under review",
            "content": "Figure 10: toy example of HGMem workflow from the NarrativeQA dataset of GPT-4o"
        }
    ],
    "affiliations": [
        "The Chinese University of Hong Kong",
        "WeChat AI"
    ]
}