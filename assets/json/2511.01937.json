{
    "paper_title": "Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR",
    "authors": [
        "Abdelaziz Bounhar",
        "Hadi Abdine",
        "Evan Dufraisse",
        "Ahmad Chamma",
        "Amr Mohamed",
        "Dani Bouch",
        "Michalis Vazirgiannis",
        "Guokan Shang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) trained for step-by-step reasoning often become excessively verbose, raising inference cost. Standard Reinforcement Learning with Verifiable Rewards (RLVR) pipelines filter out ``easy'' problems for training efficiency, leaving the model to train primarily on harder problems that require longer reasoning chains. This skews the output length distribution upward, resulting in a \\textbf{model that conflates ``thinking longer'' with ``thinking better''}. In this work, we show that retaining and modestly up-weighting moderately easy problems acts as an implicit length regularizer. Exposing the model to solvable short-chain tasks constrains its output distribution and prevents runaway verbosity. The result is \\textbf{\\emph{emergent brevity for free}}: the model learns to solve harder problems without inflating the output length, \\textbf{ despite the absence of any explicit length penalization}. RLVR experiments using this approach on \\textit{Qwen3-4B-Thinking-2507} (with a 16k token limit) achieve baseline pass@1 AIME25 accuracy while generating solutions that are, on average, nearly twice as short. The code is available at \\href{https://github.com/MBZUAI-Paris/Frugal-AI}{GitHub}, with datasets and models on \\href{https://huggingface.co/collections/MBZUAI-Paris/k2-think-mini-68dcfa8b114686a4bd3dc2bc}{Hugging Face}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 7 3 9 1 0 . 1 1 5 2 : r Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR Abdelaziz Bounhar1, Hadi Abdine1, Evan Dufraisse1, Ahmad Chamma1, Amr Mohamed1, Dani Bouch1, Michalis Vazirgiannis1,2, Guokan Shang1 1MBZUAI, 2École Polytechnique {abdelaziz.bounhar,guokan.shang}@mbzuai.ac.ae"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) trained for step-by-step reasoning often become excessively verbose, raising inference cost. Standard Reinforcement Learning with Verifiable Rewards (RLVR) pipelines filter out easy problems for training efficiency, leaving the model to train primarily on harder problems that require longer reasoning chains. This skews the output length distribution upward, resulting in model that conflates thinking longer with thinking better. In this work, we show that retaining and modestly up-weighting moderately easy problems acts as an implicit length regularizer. Exposing the model to solvable short-chain tasks constrains its output distribution and prevents runaway verbosity. The result is emergent brevity for free: the model learns to solve harder problems without inflating the output length, despite the absence of any explicit length penalization. RLVR experiments using this approach on Qwen3-4B-Thinking-2507 (with 16k token limit) achieve baseline pass@1 AIME25 accuracy while generating solutions that are, on average, nearly twice as short. The code is available at GitHub, with datasets and models on Hugging Face."
        },
        {
            "title": "Introduction",
            "content": "Recently, large language models (LLMs) have begun to rapidly advance the frontier of machine intelligence through test time scaling via step-by-step reasoning. Scaling inference budget and training with RLVR have enabled models to achieve strong performance on competition-level mathematics and coding tasks by producing extended chains of thought. However, this progress often incurs at cost: reasoning models tend to be overly verbose, generating excessively long solutions that increase inference latency and memory usage. common design choice in RLVR training pipelines is to filter out easy problems to maximize training efficiency, with training typically beginning in medium-difficulty samples and gradually shifting toward harder instances [Mistral-AI et al., 2025, Ji et al., 2025b,a]. This design choice is not arbitrary; it follows from the mechanics of Group Relative Policy Optimization (GRPO) [Shao et al., 2024], wherein groups with either all-correct or all-incorrect rollouts yield zero advantage and therefore provide no learning signal. Consequently, both easy and unsolvable hard problems are typically excluded, as they are unlikely to produce meaningful policy updates. This leaves the model to learn primarily from problems that inherently require longer reasoning chains. Over time, this imbalance shifts the output length distribution upward, leading the policy to reward verbosity even when many of the generated tokens are redundant. The outcome is systematic drift towards unnecessarily long outputs, where models conflate thinking longer with thinking better. This phenomenon can also be understood from an information-theoretic perspective. Since conditioning reduces entropy, an autoregressive model can lower the expected uncertainty of the final answer simply by extending its output sequence, even if the additional tokens contribute little structure or substance. In effect, verbosity becomes statistical shortcut to entropy reduction rather than reflection of genuine reasoning. In this paper, we revisit the training-efficiency heuristic of discarding easy problems and instead argue for their value. We show that retaining, and upweighting moderately easy problems provides natural counterbalance: they act as length regularizers. By exposing the policy to tasks that admit concise solutions and training under limited context window, the model is implicitly pressured to maintain efficiency in its output distribution in order to obtain rewards on harder samples. Crucially, this yields emergent brevity for free: The model learns to solve harder problems without inflating the solution length, despite the absence of any explicit length-penalty term. Experiments on Qwen3-4BThinking-2507 with 16k-token budget show that our method preserves baseline pass@1 accuracy on AIME25, while reducing solution length by nearly (2). This demonstrates that concision and performance are not in opposition: carefully curating the training data is and has always been the key. Our contributions are two fold: Implicit length regularization: We show that emphasizing moderately easy problems naturally regularizes output length, reducing verbosity without explicit reward shaping. Empirical validation: On Qwen3-4B-Thinking-2507 with 16k-token budget, our method preserves baseline pass@1 accuracy on AIME25 while reducing average solution length by nearly (2). Together, these findings highlight that data curation, not only reward design or model size, plays critical role in shaping the efficiency of reasoning models."
        },
        {
            "title": "2 Prelude",
            "content": "We consider an autoregressive language model parameterized by θ, defining policy πθ over token sequences. For query and response = (y1, . . . , yT ), the likelihood under the policy is πθ(yx) = (cid:89) t=1 πθ(yt x, y<t). (1) Each response is evaluated by verifier r(x, y) defined over appropriate domains, which assigns scalar reward indicating correctness1. RLVR seeks to optimize πθ so as to maximize the expected verifier score ExD,yπθ [r(x, y)] where is the training dataset. Group Relative Policy Optimization (GRPO). Instead of relying on value model as in PPO [Schulman et al., 2017], GRPO [Shao et al., 2024] uses groups of responses {yi}G i=1 πθold(x) for the same query sampled from training dataset to estimate the expected reward, a.k.a. the value function. The GRPO objective function is defined as JGRPO(θ) = xD,{yi}G i=1πθold (x) 1 (cid:88) i=1 1 yi yi (cid:88) t=1 min (wi,t(θ)Ai, clip(wi,t(θ), 1 ϵ, 1 + ϵ)Ai) , where each response receives an advantage computed relative to the group: Ai = r(x, yi) mean (cid:0){r(x, yi)}G std (cid:0){r(x, yi)}G (cid:1) i=1 i=1 (cid:1) , and wi,t(θ) = πθ(yi,t x, yi,<t) πθold (yi,t x, yi,<t) (2) (3) (4) is the importance sampling weight applied at the token level. Because rollouts are generated from the behavior policy πθold, while the objective is to optimize expectations under the updated policy πθ, correction is required. Importance sampling provides this correction by reweighting each sampled 1Throughout this work, we use binary verifiable rewards r(x, y) {0, 1}, where 1 denotes correct solution and 0 failure. 2 token according to how much more or less likely it is under the new policy than under the old one. Formally, this ensures that expectations with respect to πθ can be estimated from samples drawn under πθold : Eyπθ [f (y)] = Eyπθold (cid:20) πθ(y) πθold (y) (cid:21) (y) . (5) Applied at the token level, wi,t(θ) rescales the contribution of each sampled token so that the update direction reflects the current policy. Since these ratios can vary dramatically, GRPO follows PPO in applying clipping, i.e., min(wi,t(θ)Ai, clip(wi,t(θ), 1 ϵ, 1 + ϵ)Ai) , (6) as mean of stability. Difficulty and vanishing advantage. By construction, if all rollouts are correct (r = 1 for all) or all are incorrect (r = 0 for all), then Ai = 0 for every i, see (3). Such groups do not provide gradient signal. Consequently, easy problems (solved with probability 1) and hard problems (solved with probability 0) are systematically excluded from the RLVR pipelines. Training is therefore efficient when done on samples that satisfy 0 < Pr [r(x, y) = 1] < 1. (7) Length bias from difficulty imbalance. Medium and hard problems inherently require longer reasoning chains. Filtering out easy problems therefore biases the effective training distribution toward longer outputs. Over successive updates, the policy may learn that reward is typically associated with extended completions, skewing the output length distribution upward. Empirically, this manifests as models producing unnecessarily long solutions, even when concise reasoning would suffice. Information-Theoretic view. Let be query, denote the (final) answer token (or deterministic function of the full response), and Zt (Y1, . . . , Yt) be the length-t prefix produced by the autoregressive policy. We treat (x, Y, Zt) as jointly distributed under the rollout process. By the chain rule of entropy, H(Y X, Zt) H(Y X, Zt+1) = I(Y ; Yt+1 X, Zt) 0, (8) so H(Y X, Zt+1) H(Y X, Zt) for all t. That is, conditioning on longer prefix reduce the conditional entropy of the final answer. This statement holds irrespective of how Yt+1 is generated (it may even be semantically vacuous), as it is simply property of conditional entropy and mutual information. Implication for RLVR. In the absence of any explicit penalty on output length or token semantics, and with rewards depending solely on the correctness of the final answer, policy can (weakly) lower the uncertainty of its own final prediction by emitting additional tokens i.e., by increasing before committing to (given sufficiently large context length). On medium/hard instances where reward is sparse and longer chains are more common, this creates systematic incentive for longer completions: reducing H(Y X, Zt) via extra tokens is never penalized and may correlate with higher success rates. Over training, this induces an upward drift in output length, leading models to conflate thinking longer with thinking better even when many added tokens are redundant. In effect, verbosity becomes statistical shortcut for reducing uncertainty in the final boxed answer, rather than genuine indicator of deeper reasoning."
        },
        {
            "title": "3 Methodology",
            "content": "We focus only on math data. Length regularization via moderately easy problems. Consider problem with success-rate parameter defined as = Pr [r(x, y) = 1 πθ] . (9) The easy problems correspond to 1, and the hard ones to 0. In standard RLVR pipelines, trivially easy and hard problems are often discarded for efficiency: easy problems have high success 3 (a) Token count distribution as function of empirical success rate p. (b) Distribution ρ(p) after filtering out trivial and hard cases with {0, 1}. Figure 1: Empirical success-rate analysis. rate probability, while very hard ones rarely contribute useful gradients, as their lengthy solutions either exceed the context window or fail verification, yielding zero reward. Consequently, the effective training signal is dominated by medium-to-hard instances, which inherently require longer reasoning chains. Reward shaping. We adopt binary verifiable reward based on exact string matching of the extracted final answer. The model encloses its final prediction within boxed{}, allowing deterministic parsing and verification. Let ˆa(x) denote the models predicted answer extracted from its output for given query x. reward of 1 is assigned if the normalized prediction matches the normalized ground truth y, and 0 otherwise: r(x, y) = Data Curation. (cid:40)1, if ˆa(x) = y, 0, otherwise. (10) Stage 1 Emergent brevity. Although batches are sampled uniformly at random, with mixed difficulty2, the global dataset distribution is deliberately imbalanced, containing higher proportion of moderately easy problems. Our design departs from standard RLVR pipelines, which typically discard trivially easy samples for efficiency. Because short, solvable problems provide stable positive rewards associated with structured and concise reasoning traces, they tend to dominate the effective reward signal. Very hard problems, by contrast, contribute little due to sequence truncation or verification failure when the policy cannot yet solve them. Over time, this inductive bias in the reward signal implicitly encourages shorter, more efficient reasoning traces. Formally, the expected reward over the training distribution can be expressed as E(x,y)D[r(x, y)] = (cid:90) 1 0 ρ(p) dp, (11) where ρ(p) denotes the empirical density of problems with success probability p. In our dataset, ρ(p) is intentionally skewed toward easy problems, so these samples dominate the reward signal. During RL, the gradient of JGRPO(θ) therefore receives stronger, more stable updates from solvable problems within the token limit, while hard problems often yielding long or truncated completions contribute negligible gradients. This imbalance constrains the learned output distribution: since rewards arise predominantly from shorter, solvable trajectories, verbosity ceases to be profitable strategy. The policy instead optimizes for correctness within limited context, giving rise to what we term emergent brevity for free, the model maintains accuracy on hard problems while producing substantially shorter solutions. In effect, easy samples function as implicit length regularizers, shaping the models reasoning behavior toward concise and efficient thought. empirical success-rate The target model Qwen3-4B-Thinking-2507 based on 16 rollouts per-prompt exhibited bimodal pattern, with large mass at = 0 and = 1. This pattern indicates that many problems are either trivially solved or currently unsolved by the base policy (given budget of 16k tokens), while relatively few distribution computed using our ρ(p) 2Curriculum learning is applied only in the second stage. 4 lie in the intermediate difficulty range where learning gradients are most informative. Figure 1b isolates that central region by excluding samples with {0, 1}, highlighting the subset that drives effective RLVR optimization. Additionally, Figure 1a shows that reasoning length varies systematically with difficulty, with easy problems requiring small number of tokens. Remark 1. In our experiments, we do not filter out the samples with = 0, rather only those with = 1, since such problems can become solvable if the model is given larger reasoning window or improved intermediate steps. Indeed, as shown in Appendix Figure 4, when sampling 256 of these initially unsolved problems and allowing up to 42k tokens context, significant fraction becomes solvable. Stage 2 Improvement via Curriculum RLVR. Having obtained concise and efficient policy after Stage 1, we proceed with second reinforcement phase based on Curriculum RLVR. Our goal in this stage is to enrich the models knowledge and reasoning capabilities on wider domain of mathematical problems while maintaining the same 16k-token limit. Training is conducted on filtered subset of the DeepMath-103 dataset [He et al., 2025], which contains problems grouped by difficulty. We follow the difficulty annotations provided by the authors to structure progressive curriculum: training begins with moderately solvable instances and gradually incorporates harder problems as the policys competence improves. We filtered the DeepMath-103 dataset through multi-step process. First, we removed samples already present in the Stage 1 dataset. We then retained only those with the correct format and sufficient difficulty. Following the pre-filtering procedure from this repository, we excluded Multiple-Choice, True/False, Yes/No, and formal Proof-based questions, as in [Mistral-AI et al., 2025]. We also discarded examples with inconsistent answers across the three DeepSeek-R1 generations, those asking for counterexamples or lacking single correct answer, and those that were ill-posed or underspecified. We adopted the annotations from the same repository, produced using gpt-5-mini-2025-08-07 with verbosity=\"medium\" and reasoning_effort=\"minimal\". After this stage, the dataset was reduced from 103k to 57k samples. Next, we filtered by difficulty. For each of the nine second-level math domains in the DeepMath dataset, we sampled around 30 examples for difficulty levels 5 to 9 and evaluated the pass@1 performance across these levels. Model accuracy varied across domains: since training datasets typically overrepresent precalculus, calculus, and algebra problems, performance was higher in those areas. For each domain, we retained all difficulty levels starting from the one with less than 75% success, yielding final set of 14.5k samples (see Table 3 for cut details). Empirically, we find that performing curriculum RLVR on this subset gives us the best performances compared to random shuffling."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup We fine-tune Qwen3-4B-Thinking-2507 using GRPO with verifiable binary reward function as defined in (10). The training data consist of verifiable mathematical problems curated and filtered as described in Section 3. Our RLVR implementation is based on the verl framework [Sheng et al., 2025]. Training is performed in two stages: Stage 1 (emergent brevity) runs for 1,050 optimization steps, corresponding to single epoch over the curated dataset. Stage 2 (curriculum RLVR) continues training for 255 additional steps, corresponding to two curriculum epochs over our filtered subset of DeepMath-103. All stages use fixed 16k-token generation limit and share identical optimization hyperparameters3. summary of the main hyperparameters is provided in Appendix Table 4. 3Stage 2 also includes short warm-up phase before resuming full-rate training. 5 (a) Average response length. (b) Minimum response length. (c) Response length clip ratio. (d) Policy entropy. (e) Accuracy on AIME25. Figure 2: Training dynamics during Stage 1 (emergent brevity). Early training is dominated by overly long, truncated generations with high entropy and low accuracy. As learning progresses, average response length and clip ratio decrease sharply, entropy stabilizes, and validation accuracy on AIME25 improves steadilyshowing that conciseness and correctness co-emerge. 4.2 Training Dynamics Figure 2 summarizes the evolution of key metrics during training Stage 1. At the beginning of optimization, the model displays pronounced verbosity, reflected by response-length clipping ratio exceeding 15%, indicating that many generations are prematurely truncated at the 16k-token limit. As training progresses, the average response length steadily decreases while the minimum length increases, suggesting that the model learns to produce more compact yet complete reasoning traces. This reduction in verbosity coincides with sharp decline in the response-clipping ratio, confirming that the policy increasingly completes its reasoning within the available context budget. Entropy dynamics provide additional insight into this transition. Entropy decreases sharply in the early phase as the policy shifts from exploration to exploitation, stabilizing around consistent reasoning patterns that yield reliable verifier rewards. Around mid-training (steps 400600), entropy rises slightly again, indicating renewed exploration which can indicate that the model begins tackling more diverse or harder samples, yet without reverting to the excessively long outputs observed initially. This interplay between entropy and response length supports the interpretation of emergent brevity as stable equilibrium: the policy reduces uncertainty through more efficient reasoning rather than through longer sequences. Validation accuracy on AIME25 (bottom-right panel) increases steadily from roughly 33%4 to about 70% throughout Stage 1, showing that conciseness and reasoning competence improve in tandem rather than in opposition. By the end of Stage 1, the policy achieves strong accuracy while maintaining concise, self-terminating outputs, consistent with the intended effect of implicit length regularization. During Stage 2 (Curriculum RLVR), the overall behavior remains qualitatively similar: the model continues to generate short, efficient reasoning traces, although the minimum response length increases to 1,200 tokens due to the increased difficulty. 4Evaluation conducted under 16k-token generation budget."
        },
        {
            "title": "5 Evaluation",
            "content": "We evaluate our method on verifiable mathematical reasoning tasks, focusing on the efficiencyaccuracy trade-off induced by easy-sample regularization and curriculum RLVR. Experiments are conducted on Qwen3-4B-Thinking-2507, chosen for its open availability and strong baseline reasoning performance. All experiments use verifiable binary rewards and follow the GRPO training procedure described in Section 3. We report both standard accuracy and our proposed EfficiencyAdjusted Accuracy (EAA; Definition 1) to jointly assess performance and output conciseness. 5.1 Reasoning Benchmarks We evaluate models on diverse reasoning benchmarks spanning mathematics, STEM, and instruction following. Mathematics AIME25 [Ye et al., 2025]: The 2025 American Invitational Mathematics Examination, containing 30 integer-answer problems. Omni-MATH-Hard [Gao et al., 2024]: The hardest subset of Omni-MATH, retaining only Olympiad-level problems rated 910 in difficulty (100 problems total). MATH-5005: held-out set of 500 problems from the original MATH benchmark introduced in Lets Verify Step by Step [Lightman et al., 2023]. GSM-Plus [Li et al., 2024]: robustness extension of GSM8K with controlled perturbations (e.g., rewording, distractors, numerical changes) to assess consistency under input variations. STEM GPQA-Diamond [Rein et al., 2023]: 198 expert-written, Google-proof multiplechoice questions across biology, physics, and chemistry. Instruction Following IFEval [Zhou et al., 2023]: 500 prompts designed to test precise adherence to explicit textual instructions with verifiable outcomes. 5.2 Metrics Definition 1 (Efficiency Adjusted Accuracy (EAA)). To jointly evaluate reasoning accuracy and conciseness, we define the Efficiency Adjusted Accuracy (EAA) metric. Let [0, 1] denote the pass@k (or accuracy) and [Lmin, Lmax] the mean output length in tokens of model on given benchmark. For tunable penalty exponent γ > 0, we define EAAγ(a, L) = exp γ (cid:20) (cid:18) Lmin (cid:19)(cid:21) Lmax Lmin . (12) This formulation linearly rescales output length to the unit interval, so that shorter completions (L Lmin) preserve the full score, whereas longer ones (L Lmax) are increasingly penalized depending on γ. Interpretation. EAAγ remains bounded in [0, 1] and decreases monotonically with output length. The exponent γ controls the strength of the penalty: γ = 1 yields linear trade-off between accuracy and brevity, γ > 1 enforces sharper preference for concise completions, and 0 < γ < 1 applies milder adjustment. 5.3 Results We refer to our Stage 1 and Stage 2 models as Frugal-Math-4B-Stage1, and Frugal-Math-4B-Stage2 respectively. Table 1 summarizes the reasoning performance of models ranging from 3B to 30B parameters with similar scale active number of parameters under 42k-token decoding limit, while Table 2 reports 5https://huggingface.co/datasets/HuggingFaceH4/MATH-500 7 their corresponding average output lengths. Each cell in Table 1 contains two metrics: the left value is standard Pass@1 (accuracy using LLM-as-a-Judge for Omni-Hard and average scores for IFEval), while the right value is EAA (accuracy normalized by output length) using γ=3.0. Table 1: Reasoning benchmark performance (similar active number of parameters models, max output 42 tokens). Model Qwen3-30B-A3B-Thinking-2507 SmolLM3-3B Phi-4-mini-reasoning Qwen3-4B-Thinking-2507 Frugal-Math-4B-Stage1 (ours) Frugal-Math-4B-Stage2 (ours) Size 30B 3B 4B 4B 4B 4B GPQA Diamond 70.7125.26 27.7801.38 30.3003.05 67.1703.68 AIME25 86.6709.79 30.0011.44 40.0012.83 73.3303.65 Omni-Hard 08.0900.63 35.2614.20 32.3718.39 04.6200.23 GSM_PLUS 90.2990.29 83.4829.39 87.1061.12 89.0516.71 IFEVAL 41.3541.35 71.2103.55 51.5822.05 38.5720.79 MATH_500 97.8008.15 90.8045.35 90.8044.21 97.6004.86 Average 65.8229.25 56.4217.55 55.3626.94 61.7208.32 63.6431.22 (-03.53)(+27.54) 70.2070.20 (+03.03)(+66.52) 60.0043.73 (-13.33)(+40.08) 70.0070.00 (-03.33)(+66.35) 35.8431.54 (+31.22)(+31.31) 47.4047.40 (+42.78)(+47.17) 89.2404.44 (+00.19)(-12.27) 89.0011.15 (-00.05)(-05.56) 39.9122.43 (+01.34)(+01.64) 39.4923.20 (+00.92)(+02.41) 95.0055.51 (-02.60)(+50.65) 95.2095.20 (-02.40)(+90.34) 63.9431.48 (+02.22)(+23.16) 68.5552.86 (+06.83)(+44.56) At this maximum context length, Frugal-Math-4B-Stage2 achieves an average accuracy of 68.55% and an EAA of 52.86, outperforming its base model (61.72 / 8.32) by +6.83 and +44.54, respectively. The Stage 1 variant also improves to 63.94 / 31.48, showing that our Stage 1 fine-tuning yields substantially better token efficiency. Compared to larger or similar-sized baselines, the 30B Qwen3-A3B model achieves 65.82 / 29.25, while Phi-4-mini-reasoning and SmolLM3-3B trail behind at 55.36 / 26.94 and 56.42 / 17.55, confirming that the Frugal models preserve or slightly improve overall accuracy while delivering far better efficiency. The length analysis in Table 2 reinforces this efficiency narrative. While Qwen3-4B-Thinking-2507 generates on average 11491 tokens per sample, our Frugal-Math-4B variants drastically reduce this to 6270 for Stage 1 and 5712 for Stage 2. The efficiency gains are most pronounced on harder mathematical reasoning tasks, notably AIME25 and Omni-Hard, where solution chains are typically long. On these benchmarks, Stage 2 achieves comparable or higher accuracy using 5561% fewer tokens, indicating that it learns to reason more efficiently, while still reaching correct final answers. In contrast, for easier arithmetic problems such as GSM_PLUS, where all models already reach high accuracy with short outputs, the advantage is less pronounced; Stage 2s generations are slightly longer (+5.6%) and EAA shows small regression. This pattern suggests that the Frugal-Math-4B models allocate reasoning effort adaptivelycompressing complex reasoning when needed but not over-optimizing brevity on tasks that are inherently simple. Table 2: Average output length (tokens) per benchmark under the 42 k-token decoding budget. Model Size GPQA Diamond AIME25 Omni-Hard GSM_PLUS IFEVAL MATH_500 Average Length Qwen3-30B-A3B-Thinking-2507 SmolLM3-3B Phi-4-mini-reasoning Qwen3-4B-Thinking-2507 Frugal-Math-4B-Stage1 (ours) Frugal-Math-4B-Stage2 (ours) 30B 3B 4B 4B 4B 4B 7208.61 8966.65 8338.75 8882.41 6925.65 6290.44 17887.8 13136.2 13811.7 21090.1 10604.1 9367.67 26960.1 17076.9 15009.4 29642. 12380.3 11611.9 1373.03 1634.25 1461.65 1791.69 2123.68 1892.89 1179.44 5521.41 2409.01 2073.93 2013.00 1949.61 5069.94 3695.5 3714.96 5465. 3574.92 3162.40 9946.49 8338.48 7457.58 11491.04 6270.28 5712.49 The scaling curves in Figure 3 illustrate how different models behave under increasing generation budgets (8k 16k 32k 42k). For the hard reasoning tasks, AIME25 and Omni-Hard, our Frugal-Math-4B models, particularly Stage 2, demonstrate superior efficiency at lower budgets. At 8k and 16k tokens, they already achieve accuracy levels close to or exceeding larger models, while maintaining higher EAA across all budgets. This indicates that the Frugal models can solve complex, multi-step mathematical problems correctly with much shorter reasoning chains. In contrast, Qwen3-4B-Thinking-2507 and Qwen3-30B-A3B-Thinking-2507 continue to improve with larger decoding budgets (32k and 42k) achieving better accuracies, but their EAA remains consistently lower, suggesting that their accuracy gains rely on significantly longer outputs. For simpler arithmetic tasks like GSM_PLUS, where all models rapidly converge to high accuracy even under small budgets, our method provides limited benefits. The Frugal-Math models produce slightly longer outputs and show nearly equivalent performance, reflecting that these tasks already require minimal reasoning depth and offer little room for further compression. Overall, these scaling results highlight the strength of our approach on reasoning-intensive tasks, where Frugal-Math-4B models maintain more favorable accuracy-per-token ratio and deliver strong performance even under tight output constraints. 8 Figure 3: Scaling behavior under varying generation budgets (8 16 32 42 k). The top panels show Pass@1 accuracy and the bottom panels show Efficiency-Adjusted Accuracy for the three benchmarks; AIME25, GSM Plus, and Omni-Hard."
        },
        {
            "title": "6 Outlooks",
            "content": "Our findings show that retaining moderately easy problems naturally regularizes reasoning length without explicit penalties. Future work may extend this idea to other domains such as coding or logical reasoning, explore adaptive curricula balancing easyhard samples, and combine implicit and explicit regularization for finer control of brevity."
        },
        {
            "title": "7 Limitations",
            "content": "Our study focuses on math reasoning with verifiable rewards and single 4B model. Generalization to open-ended or larger-scale settings remains unexplored and is part of ongoing research. Moreover, brevity is observed empirically rather than theoretically explained, and its understanding warrants further study."
        },
        {
            "title": "References",
            "content": "Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omnimath: universal olympiad level mathematic benchmark for large language models, 2024. URL https://arxiv.org/abs/2410.07985. Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, et al. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. arXiv preprint arXiv:2504.11456, 2025. Yunjie Ji, Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Han Zhao, and Xiangang Li. Am-thinking-v1: Advancing the frontier of reasoning at 32b scale, 2025a. URL https://arxiv.org/abs/2505.08311. Yunjie Ji, Sitong Zhao, Xiaoyu Tian, Haotian Wang, Shuaiting Chen, Yiping Peng, Han Zhao, and Xiangang Li. How difficulty-aware staged reinforcement learning enhances llms reasoning 9 capabilities: preliminary experimental study, 2025b. URL https://arxiv.org/abs/2504. 00829. Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, and Wei Bi. Gsm-plus: comprehensive benchmark for evaluating the robustness of llms as mathematical problem solvers, 2024. URL https://arxiv.org/abs/2402.19255. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/2305.20050. Mistral-AI, :, Abhinav Rastogi, Albert Q. Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, Léonard Blier, Lucile Saulnier, Matthieu Dinot, Maxime Darrin, Neha Gupta, Roman Soletskyi, Sagar Vaze, Teven Le Scao, Yihan Wang, Adam Yang, Alexander H. Liu, Alexandre Sablayrolles, Amélie Héliou, Amélie Martin, Andy Ehrenberg, Anmol Agarwal, Antoine Roux, Arthur Darcet, Arthur Mensch, Baptiste Bout, Baptiste Rozière, Baudouin De Monicault, Chris Bamford, Christian Wallenwein, Christophe Renaudin, Clémence Lanfranchi, Darius Dabert, Devon Mizelle, Diego de las Casas, Elliot Chane-Sane, Emilien Fugier, Emma Bou Hanna, Gauthier Delerce, Gauthier Guinet, Georgii Novikov, Guillaume Martin, Himanshu Jaju, Jan Ludziejewski, Jean-Hadrien Chabran, Jean-Malo Delignon, Joachim Studnia, Jonas Amar, Josselin Somerville Roberts, Julien Denize, Karan Saxena, Kush Jain, Lingxiao Zhao, Louis Martin, Luyu Gao, Lélio Renard Lavaud, Marie Pellat, Mathilde Guillaumin, Mathis Felardos, Maximilian Augustin, Mickaël Seznec, Nikhil Raghuraman, Olivier Duchenne, Patricia Wang, Patrick von Platen, Patryk Saffer, Paul Jacob, Paul Wambergue, Paula Kurylowicz, Pavankumar Reddy Muddireddy, Philomène Chagniot, Pierre Stock, Pravesh Agrawal, Romain Sauvestre, Rémi Delacourt, Sanchit Gandhi, Sandeep Subramanian, Shashwat Dalal, Siddharth Gandhi, Soham Ghosh, Srijan Mishra, Sumukh Aithal, Szymon Antoniak, Thibault Schueller, Thibaut Lavril, Thomas Robert, Thomas Wang, Timothée Lacroix, Valeriia Nemychnikova, Victor Paltz, Virgile Richard, Wen-Ding Li, William Marshall, Xuanyu Zhang, and Yunhao Tang. Magistral, 2025. URL https://arxiv.org/abs/2506. 10910. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pages 12791297, 2025. Yixin Ye, Yang Xiao, Tiantian Mi, and Pengfei Liu. Aime-preview: rigorous and immediate evaluation framework for advanced mathematical reasoning, 2025. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models, 2023. URL https://arxiv.org/abs/2311.07911. Context-Length Effects on Data Difficulty Filtering To ensure meaningful analysis of model competence, we filter the dataset based on the empirical success rate = Pr[r(x, y) = 1 πθ], computed over multiple stochastic generations per prompt. We re-evaluate 256 of the initially unsolved problems at 16k tokens budget by allowing up to 42k tokens context. As shown in 10 Figure 4: Distribution ρ(p) after scaling maximum response length to 42k tokens. Figure 4, the resulting distribution ρ(p) remains dominated by low success-rate probability cases, but small yet non-trivial mass appears at higher p-values. This confirms that subset of previously unsolved tasks can be recovered through longer reasoning chains rather than fundamental limitations in model ability. Table 3: Stage 2 Filtering - Difficulty level (start included) for each second-level math domain. Math Domain Starting Difficulty Level Algebra Calculus Precalculus Discrete Mathematics Number Theory Geometry Other Applied Mathematics 7 7 7 7 6 7"
        },
        {
            "title": "B Training Hyperparameters",
            "content": "Table 4: Hyperparameters and system configuration for RL fine-tuning."
        },
        {
            "title": "Value",
            "content": "Base model RL algorithm Reward type Rollout group size (G) Clipping thresholds (1 ϵ, 1 + ϵ) Maximum completion length Batch size (per step) Learning rate Warmup schedule Optimizer Hardware Qwen3-4B-Thinking-2507 GRPO Verifiable binary reward (exact match) 16 (0.8, 1.28) 16,384 tokens 128 1 106 Linear, first 5% of steps AdamW 250 NVIDIA H200 GPU days"
        }
    ],
    "affiliations": [
        "MBZUAI",
        "École Polytechnique"
    ]
}