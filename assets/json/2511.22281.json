{
    "paper_title": "The Collapse of Patches",
    "authors": [
        "Wei Guo",
        "Shunqi Mao",
        "Zhuonan Liang",
        "Heng Wang",
        "Weidong Cai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Observing certain patches in an image reduces the uncertainty of others. Their realization lowers the distribution entropy of each remaining patch feature, analogous to collapsing a particle's wave function in quantum mechanics. This phenomenon can intuitively be called patch collapse. To identify which patches are most relied on during a target region's collapse, we learn an autoencoder that softly selects a subset of patches to reconstruct each target patch. Graphing these learned dependencies for each patch's PageRank score reveals the optimal patch order to realize an image. We show that respecting this order benefits various masked image modeling methods. First, autoregressive image generation can be boosted by retraining the state-of-the-art model MAR. Next, we introduce a new setup for image classification by exposing Vision Transformers only to high-rank patches in the collapse order. Seeing 22\\% of such patches is sufficient to achieve high accuracy. With these experiments, we propose patch collapse as a novel image modeling perspective that promotes vision efficiency. Our project is available at https://github.com/wguo-ai/CoP ."
        },
        {
            "title": "Start",
            "content": "Wei Guo, Shunqi Mao, Zhuonan Liang, Heng Wang, Weidong Cai School of Computer Science, The University of Sydney {wei.guo, shunqi.mao, zhuonan.liang, heng.wang, tom.cai}@sydney.edu.au 5 2 0 2 7 2 ] . [ 1 1 8 2 2 2 . 1 1 5 2 : r Figure 1. Patch synthesis in random and collapse orders. We autoregressively generate rooster image patches following random order (above) and collapse order (below). The latter synthesizes prominent rooster features and reduces image uncertainty more effectively."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Observing certain patches in an image reduces the uncertainty of others. Their realization lowers the distribution entropy of each remaining patch feature, analogous to collapsing particles wave function in quantum mechanics. This phenomenon can intuitively be called patch collapse. To identify which patches are most relied on during target regions collapse, we learn an autoencoder that softly selects subset of patches to reconstruct each target patch. Graphing these learned dependencies for each patchs PageRank score reveals the optimal patch order to realize an image. We show that respecting this order benefits various masked image modeling methods. First, autoregressive image generation can be boosted by retraining the state-of-the-art model MAR. Next, we introduce new setup for image classification by exposing Vision Transformers only to high-rank patches in the collapse order. Seeing 22% of such patches is sufficient to achieve high accuracy. With these experiments, we propose patch collapse as novel image modeling perspective that promotes vision efficiency. Our project is available at https://github.com/wguo-ai/CoP. Images are more than collections of independent pixels or patches: they are structures of mutual dependence [42]. Observing parts of an image often reveals information about others. This observation (realization) process of an image, when modeled along patch sequence, is then analogous to the collapse of wave function in quantum mechanics [14]: once particular patch is measured, the remaining unrealized patches uncertainty reduces around the observed evidence. Intuitively, we refer to this image uncertainty reduction process as the collapse of patches. Different patches collapse the uncertainty of the residual image with different effectiveness, which is nontrivial phenomenon. Seeing the beak of rooster first, for instance, constrains the leftover image information more than seeing background field, as shown by the comparison of uncertainty reduction effects from an autoregressive (AR) image generator [27] following different patch synthesis orders in Fig. 1. In reality, human painter also starts with sketch of their subjects important parts in order to neatly constrain the underlying visual uncertainty [12]. Aside from image synthesis, the human ability to correlate partial visual contents differently in scene is also essential for completing vision tasks with high efficiency [32]. Most modern vision models, however, image patches as uniformly correlated samples in masked image modeling (MIM), e.g., for stochastic AR generation [5, 11, 24, 27, 35, 45, 47, 48] or masked classification [6, 16, 19, 28, 5153]. This assumption ignores the contributive differences among patches during collapse. treat In this work, we formalize the problem of patch collapse respecting its patch-wise priorities. We assume that when certain patches of an image are observed, the feature distributions of the unobserved patches shift from broad, high-entropy shapes to concentrated, low-entropy states. The further assumption is that an images patches can be ranked based on their elicited shift strengths during this process. To study how this collapse unfolds, we train Collapse Masked Autoencoder (CoMAE) whose encoder selectively masks image patches with noise injection, conditioning the decoder in reconstructing target patch. This encoder-predicted mask is continuous vector weighting each patchs collapse contribution between 0 and 1. Our experiments validate the assumptions above: only subset of patches are most responsible for each given patchs collapse, as polarized selection weights emerge to optimize reconstructions. Furthermore, we observe that this selection set varies across patches, suggesting that each patch has distinct collapse dependency and contributes to the global image certainty with different effectiveness. To analyze these patch dependencies at the image level, we map out directed acyclic graph of patches with the CoMAE selection masks as edge weights. Applying PageRank [3] to this graph yields an ordering of patches by their collapse independence, defining an optimal uncertainty reduction sequence in which an image realizes itself. We term these sequences collapse orders. Our visualizations show that high-rank patches in the collapse order outlines important shapes in an image. Additionally, we observe that the inter-class collapse orders across ImageNet [7] samples exhibit moderate similarity, which suggests their depiction of consistent underlying structure behind different images. We demonstrate that collapse order offers beneficial supervision to MIM methods. When integrated into an AR image generator MAR [27], respecting the collapse order improves sample quality both quantitatively and qualitatively over the original model. Alongside generation, respecting the collapse order also leads to efficient image classification: by training on patches with high collapse priorities, Vision Transformer (ViT) [10] can maintain high accuracy while processing just 22% of the image content. In contrast, conventional full-image classifiers sacrifice computation to redundant patches that contribute little discriminative value. In summary, our work has the following contributions: 1. We introduce and formalize the problem of patch collapse, which offers novel perspective to describe image structures that are fundamental to vision modeling. 2. We present CoMAE, an effective method to pinpoint the image-level order of patch collapse. 3. By supervising with the collapse order, we improve MIM methods performance in AR image generation and masked image classification. These experiments show the generalizable applicability of patch collapse modeling to different vision tasks. 2. Related Works Stochastic Masked Image Modeling. Inspired by masked language modeling [8], masked image modeling (MIM) predicts corrupted local units from an image to learn generalizable visual features. Stochastic MIM (SMIM) methods reconstruct randomly masked image portions to obtain self-supervised visual features, assuming uniform patch correlations. The pioneering denoising autoencoder [49] explores robust feature learning through partial latent pattern corruption. Later, Context Encoder [33] employs convolutional neural networks to inpaint stochastic image regions representation learning. for Masked Autoencoders (MAE) [16] apply high random masking ratios (e.g., 75%) in an asymmetric transformer for scalable learning. SimMIM [51] simplifies this process with larger patches and RGB regression. Painter [50] expands MIM to various image-to-image mapping tasks. MixMAE [28] incorporates image mixing alongside MIM for data augmentation. VideoMAE [46] extends MIM to consider spatio-temporal masking for videos, while OmniMAE [15] explores masked modeling with multimodal data. Recently, CAPI [6] enhances SMIM features by an unmasked teacher. predicting missing patches w.r.t. MIMIR [52] improves SMIMs adversarial robustness via mutual information-based reconstruction. These SMIM methods learn generalizable visual features but ignore the variance of inter-patch dependencies, leading to inefficient representations. In contrast, our CoMAE adaptively masks trivial patches to model patch dependencies accurately, deriving stronger guidance for MIM methods. Adaptive Masked Image Modeling. Adaptive MIM (AMIM) methods adjust image-wise masking during training to effectively target informative regions. CMAE [19] unifies contrastive learning with MIM for stronger representation disambiguation. SiamMAE [53] leverages asymmetric masking for video correspondences. AttMask [21] generates attention-based masks from teacher model for AMIM. AdaMAE [2] masks visible video tokens adaptively with an additional sampling network. SemMAE [25] uses semantic masks from ViT to mask an images dominant shapes. CL-MAE [29] employs curriculum learning to adapt MIM to harder masks that hinder reconstruction. The recent RAM++ [56] uses adaptive masks for blind image restoration, while Self-Guided MAE [41] introduces selfguided informed masking based on early-stage patch clustering in MIM. Although AMIM methods efficiently adapt to data salience, they do not explicitly model image uncertainty reduction across patches. Our CoMAE formalizes patch collapse, identifying the global patch orders for image realization. CoMAE provides unique perspective of image structures absent in prior methods, implemented with AMIM but not limited to its coverage in vision modeling. Autoregressive Image Generation. Autoregressive (AR) models generate images sequentially with localized representations, often employing masking for unified learning. Classic AR methods, i.e., PixelRNN [48] and PixelCNN [47], follow fixed raster orders during generation. VQ-GAN [11] generates quantized image tokens in the same order. MaskGIT [5] predicts quantized tokens with scheduled stochastic order. MAGE [26] learns discrete image tokens and their generation in unified manner. MAR [27] follows the same order but replace MaskGITs discrete tokens with continuous latents. VAR [45] employs next-scale prediction for AR. MAGVIT [55] extends AR generation to videos. Recently, HMAR [24] improves AR efficiency with hierarchical multi-step prediction. xAR [35] generalizes next-token generation to flexible units such as cell or scales (next-x predictio). While different random or fixed orders are explored by these methods, our collapse order offers data-salient strategy in modeling the generation priorities of AR units, yielding significant gains without drastic remodeling and retraining. Our strategy can also be integrated with various next-x methods straightforwardly. Efficient Vision Transformers via Token Pruning. Token pruning methods enhance the computational efficiency of Vision Transformers (ViTs) by selectively removing redundant tokens. DynamicViT [34] estimates token importance scores to hierarchically prune trivial tokens. Similarly, ATS [13] and A-ViT [54] prune tokens with attention scores. AdaViT [30] learns decision policy model to drop various inference units. EViT [1] fuse attention-pruned tokens into single representation to retain more information. SPViT [23] employs an attention-based selector for this fusion. For semantic segmentation, DToP [44] dynamically prunes easy tokens via early exiting based on confidence thresholds. While these methods investigates feature pruning across the model architecture, our collapse order directly operates on explicit image patches. We show that vision efficiency can be significantly boosted by pruning on the model-agnostic image space alone. This decoupling of data and model also suggests that our method can readily combine with token pruning techniques. 3. Method For modeling efficiency, we first pass an image through variational autoencoder (VAE) [22, 36, 38] to represent patches with set of embeddings as {en}N . Each patchs feature distribution is conditioned on other patches, which can be expressed by probability distribution . Our problem can then be formulated as finding ranking function (en) such that: en {ei}N i=n (cid:17) (cid:16) Hc = (cid:88) i=1 (cid:104) H (cid:16) en {ei; (ei) > (en)}N i=n (cid:17)(cid:105) , (1) where measures the distribution entropy, is minimized. To model R, we first learn each patchs dependencies on other patches with an autoencoders encoded masking weights in Sec. 3.1 through reconstruction. directed acyclic graph of patches can then be drawn in Sec. 3.2, where the edge weights are patch dependencies. Computing the PageRank [3] scores of this graph yields the output of R, which constitutes our collapse order as the optimal patch realization sequence to reduce image uncertainty. With the collapse order identified, we propose to improve the existing stochastic AR image generator MAR [27] by respecting this image structure in Sec. 3.3. To accomplish this, we train an MAR model with collapse order guidance. Finally, we investigate our collapse modelings effectiveness on image classification by training Vision Transformer (ViT) [10] with collapse-order masks in Sec. 3.4. 3.1. Collapse Masked Autoencoder Following the motivation that some patches are more responsible for specific patchs collapse than others, we assume masking patches from {ei}N i=n leaves shape approximately unchanged. To find these influential patches, we train Collapse Masked Autoencoder (CoMAE) model as shown in Fig. 2 (a). During reconstruction, the encoder follows ViT [10] to pool visual information from {ei}N i=n with self-attention blocks. It predicts soft weight vector [0, 1]n for patch selection as: wn = (cid:16) {ei}N i=n ; qn (cid:17) , (2) where qn is learned positional embedding to inform the encoder of the under-reconstruction patch location. We then mask each patch embedding by noise injection w.r.t. wn as: em = αiei + (1 αi) (0, I) , = n, where α is an exponential decay interpolant with hyperparameter σ for controlled steepness: (3) (cid:32) αi = exp (cid:0)1 wi 2σ2 (cid:1)2 (cid:33) . (4) Figure 2. Pipeline overview. Given an image, the CoMAE encoder selects the most influential patches needed to reconstruct each patch, while trivial patches are masked with heavier noise injection. These selection weights form patch dependency graph on which we compute the PageRank scores to determine the collapse order of patches, where higher-rank patches are less dependent on the rest of the image. Finally, we use this ranking to supervise image generation and classification tasks to follow the correct patch processing order. The decoder is another stack of ViT-like self-attention blocks that pools information from the selected patches to reconstruct the target patch n: (cid:16) = {em }N i=n ; qn (cid:17) . (5) The patch reconstruction loss is simply Lr = en n1. We alternatively optimize and with Lr in batch-wise manner during training. Please refer to Sec. 8 for additional architecture details of CoMAE. Polarization. To test our assumptions in Sec. 1 that there exists priority ranking of patches in patch collapse, we observe the training metrics of CoMAE in Sec. 4.1. It can be seen that polarizes to 0 and 1 as the reconstruction loss Lr converges to minima instead of staying uniform. This effect confirms that different patches contribute to an images uncertainty reduction with different effectiveness, since only subset of patches significantly contribute to each target patchs feature collapse. Contrastive Regularization. Do different patches rely on different subset selections for their feature collapse? To answer this further question, we inquisitively add contrastive objective to encourage the diversity of as: Lc = 1 (cid:88) i=1 log exp (sim (wi, wi) /τ ) j=1 exp (sim (wi, wj) /τ ) (cid:80)N , (6) where τ is learnable temperature and sim (, ) measures the cosine similarity between two vectors. We then define the total loss to be = Lr + 0.01Lc and retrain CoMAE. This ablation, detailed in Sec. 4.1, shows that CoMAE plateaus at significantly higher Lr without Lc, assigning similar to all patches. With Lc, Lr is significantly reduced by diverse masks across patches and escapes local minima. Thus, its reasonable to assume that the one-tomany dependency of each patch during collapse is diverse. 3.2. Patch Ranking instances of can be encoded for all patches in an image. Together they form patch dependency graph with adjacency matrix where Aij = wij. To rank the independency of patches, we compute their PageRank scores from A. patch with high PageRank score has more influence on other patches. Please see Sec. 6 for formal proof linking this ranking mechanism to the objective optimal dependency ranking R. 3.3. Collapsing Autoregressive Image Generation As shown in Fig. 3 (a), autoregressive (AR) image generators often follow random orders to generate patches sequentially. This stochastic arrangement assumes that patch dependencies follow uniform distribution in an image, inducing inefficiency in image uncertainty reduction. We apply our patch sequencing learned from CoMAE as extra supervision to retrain stochastic AR model, MAR [27], which we call Collapsed Mask Autoregressive Model Implementation of CoMAE. Both the encoder and decoder of CoMAE have 12 attention blocks following ViT [10], with embedding dimensions 64 and 256 respectively. four-layer residual MLP is appended to the decoder to output the final 16-dim target patch token. The encoder and decoder are alternatively optimized in batch-wise manner. We train CoMAE for 512 epochs with cosine annealing schedule decaying learning rate from 1e-4 to 0. Implementation of CMAR. Training MAR from scratch is computationally daunting for our resources. Instead, we treat the pretrained MAR as an order-agnostic prior and fine-tune it for 24,000 steps (batch size 32) on the same ImageNet data with our collapse order. Due to restricted computational resources, we only experiment on the MARB variant. We linearly warmp up the learning rate to 1e-7 during the first 10% steps and then decay it to 0 with cosine annealing. The model weights are updated with per-step estimated mean average (EMA) of rate 0.99999. During inference, we set CMARs classifier-free guidance (CFG) [18] scale to 3.0 from our ablations in Tab. 3. We keep the original MAR at CFG 2.9 for its optimal performance. Implementation of CViT. We fine-tune the ImageNet21k pretrained ViT-Base [10] model on ImageNet-1k for classification of 1000 image classes. The training expands 3 epochs with cosine annealing schedule decaying learning rate from 1e-4 to 0. The model weights are updated by perstep EMA with rate 0.9999. We apply random horizontal flip of training images for data augmentation. 4.1. Properties of CoMAE We provide experiments to corroborate our statements of CoMAEs behaviors. First, we show that CoMAE emerges polarized selections instead of uniformly distributing the weights in during reconstruction optimization. To quantify polarization, we define Mask Entropy metric as: Hmask = 1 (cid:88) (cid:88) i=1 j=1 wi log wi j, (7) where is the number of samples and is the number of patches. lower Hmask reflects more polarized mask distribution in w. As shown in Fig. 6, Hmask grows smaller as reconstruction loss optimizes, indicating that only subset of patches is responsible for target patchs collapse. Next, we observe that different target patches depend on different patch subsets for their collapse. Our contrastive ablations in Tab. 1 reveal that this diverse selection further polarizes masks and significantly optimizes reconstruction. Additionally, we provide visualization of the collapse orders illustrated in Fig. 4. The patches outlining major objects have higher ranks in each image, which aligns intuFigure 3. Comparison of generators and classifiers. Our generator (CMAR) and classifier (CViT) respect the collapse order."
        },
        {
            "title": "Contrastive Mask Entropy Reconstruction Loss",
            "content": "4.816 4.267 8.392 1.567 Table 1. Ablation of CoMAEs contrastive regularization. Contrastive guidance aids reconstruction and mask polarization. (CMAR) in Fig. 3 (b). CMAR learns to generate patches following the collapse order instead of stochastic orders. For each training sample, we first pass it through the CoMAE encoder and obtain its patch ranks. We then mask random amount of low-rank patches and learn to generate them from high-rank ones. Since this sampling order of patches is deterministic, CMAR is more likely to overfit than the original model. To compensate for this effect, we replace 10% sampling sequences with random ranks as form of regularization. We also keep the random image flip data augmentation from MAR. 3.4. Collapsing Image Classification Conventionally, an image classifier has access to the entire input image as in Fig. 3 (c). Since we show earlier that the realization of an image follows collapse order, its intuitive to ask if such classifiers can maintain accurate when only high-rank (i.e. highly independent) patches in the collapse order are present. Correspondingly, we train ViT classifier on ImageNet [7] under two settings: full-image and masked. We mask 0 99% low-rank patches with cosine annealing schedule favoring lower mask rates, which results in Collapsed Vision Transformer (CViT) depicted in Fig. 3 (d). Instead of replacing the masked patches with mask tokens, CViT directly drops them from the input sequence for efficiency. 4. Experiments and Results We conduct all our experiments on center-cropped 256 256 images from ImageNet-1k [7]. These images are first processed by KL-16 VAE [37] used by MAR [27] into 256 16-dim tokens. We use RTX5090 GPU for all trainings. Figure 4. Visualization of collapse order. The left figure shows image patches with different collapse ranks indicated by circle sizes. The right figure connects the top-ranked 64 patches by collapse order. One can observe that top patches outline important shapes in each image."
        },
        {
            "title": "Method",
            "content": "MAR w/o CFG FID tFID IS Pre. Rec. FID tFID w/ CFG IS Pre. Rec. 7.114 3.498 194.50 0.784 0.571 5. 2.330 281.48 0.822 0.571 MAR+C (Ours) CMAR (Ours) 7.173 7. 3.563 3.600 193.80 190.92 0.788 0.781 0.568 0.572 5.956 5.928 2.321 2. 284.78 280.55 0.826 0.818 0.566 0.576 Table 2. Generation performance of different AR methods. The first place is bolded. MAR+C denotes the unfine-tuned MAR results following our collapse order. CMAR tests MAR fine-tuned with collapse order. itively with humans visual scanning order [20]. This emergent similarity between image synthesis and recognition is particularly intriguing, as it might suggest convergence of optimal scanning order between these opposite tasks. Finally, we visualize class-wise collapse order patterns in Fig. 5. One can see that similar collapse orders emerge among instances from the same class, since there are multiple closely aligned patch indices indicated by the vertical heat lines. The inter-class collapse order patterns are also similar judged by the locations of their heat lines. Therefore, its reasonable to assume that the identified collapse order exhibit moderate consistency over classes and image instances, which suggests the existence of common structure behind different images realization. 4.2. Benchmarking CMAR We compare our collapse orders effect by quantitative metrics in Tab. 2. MAR denotes the original model with random-order generation, and MAR+C is the same model inferred with collapse order. We measure generation performance by Fr`echet Inception Distance (tFID) [17] and Inception Score (IS) [39] following the original work [27] on 50,000 augmented training samples. Additionally, we measure the original FID, precision, and recall against the 10,000 samples in standard reference batch [9]. Our CMAR achieves significant 4% gain in tFID despite of very minor degradation in IS (0.3%). The finetuning-less MAR+C achieves the highest IS score and has tFID behind CMAR. Our two methods slightly degrades Figure 5. Class-wise collapse order patterns. These heatmaps show sample patch indices sorted in collapse order for each class. Figure 6. Training of CoMAE (last 140 epochs). Mask entropy drops together with reconstruction loss. Figure 7. Qualitative comparison of ARs. MAR is framed in purple. Our MAR+C and CMAR results are in blue and green respectively. CFG FID tFID IS Pre. Rec. Order FID tFID IS Pre. Rec. 2.9 3.0 3.1 5.913 5.928 5. 2.219 2.238 2.240 276.375 280.55 284.50 0.816 0.818 0.819 0.582 0.576 0.572 Ascend Descend 6.005 5. 2.267 2.238 269.27 280.55 0.782 0.818 0.556 0.576 Table 3. Ablation of CMARs CFG. First place is bolded. Table 4. Ablation of CMARs synthesis order. CMAR trained with descending collapse ranks has the best performance. in metrics during generation without CFG, possibly due to their higher reliance on the conditioning from class-specific labels and orders which requires stronger conditioning guidance provided by higher CFG scales. The qualitative samples of MAR+C and CMAR are shown in Fig. 7. It can be observed that CMAR synthesizes slightly more realistic images than MAR+C, while there are significant artifacts in the baseline MAR such as object compositional defects or content confusion. 4.3. Properties of CMAR We conduct an ablation of CFG scales for CMAR in Tab. 3. lower CFG achieves lower FIDs at the expense of IS, while higher CFG degrades in FIDs with an increase of IS. Therefore, we choose 3.0 as our optimal CFG scale to balance performance, different from the original setting at 2.9. Intuitively, this shift in CFG scale can be explained by CMARs stronger reliance on conditioning labels in order to address diverse collapse orders in different classes. Should CMAR follow ascending or descending patch Method T1@0% T5@0% T1@30% T1@30% T1@50% T5@50% T1@78% T5@78% AuC ViT DynamicViT [34] ViT+C (Ours) RViT CViT(Ours) 82.91 81.74 82.84 83.10 83.11 96.28 95.64 96. 96.46 96.50 80.03 81.44 78.09 81.33 81.37 94.80 95.46 93.82 95.60 95.69 74.38 77.54 71. 78.94 79.39 91.48 93.30 89.58 94.50 94.63 22.38 20.66 31.04 67.27 70.57 36.76 37.09 49. 87.23 88.94 57.16 56.32 57.27 70.86 72.19 Table 5. Classification accuracy of ViT under different settings. First place is bolded. CViT achieves superior classification performance throughout mask rates. ViT+C degrades slightly without masking but outperforms ViT at 78% masking. than ViT and DynamicViT when extremely sparse 78% patches are masked. Our AuC superiority under both scenarios suggests that the overall classification performance benefits from patch collapse modeling. 4.5. Properties of CViT Our CViT maintains high classification accuracy with significant portion of low-collapse-rank patches masked, as visualized in Fig. 8. To analyze the accuracy decay, we find the knee of this concave accuracy curve with the Kneedle algorithm [40]. CViTs performance maintains until 78% patches are omitted, at which point its accuracy drops to 70.6%. As we apply masking by dropping out patches from the input sequence instead of replacing them with mask tokens, the computational cost is reduced by 95.16% from the (cid:0)n2(cid:1) complexity of attention process. Furthermore, CViT also achieves the highest accuracy without masking. This result suggests that our patch collapse modeling can also benefit full-image classification. Its also worth noticing that RViT, although following random order during training, also experiences performance gain at lower mask ratios. However, its accuracy is lower than CViT at each level, suggesting our collapse orders superiority over stochastic modeling. 5. Conclusion In this work, we introduce novel modeling of the local feature uncertainty reduction process in images as patch collapse. By training Collapse Masked Autoencoder to reconstruct target patch relying on other tiles, and analyzing its resultant patch dependency graph with PageRank, we are able to identify an optimal ordering of patches during image realization that maximally reduces uncertainty, i.e., the collapse order of patches. Experiments show that respecting this order benefits masked image modeling methods in: (1) autoregressive image generation, where the state-of-the-art model MAR is boosted in FID and IS, and (2) image classification, which leads to ViT that can maintain high accuracy despite seeing only 22% image patches. We hope our patch collapse modeling will encourage new perspective on salient visual structures, benefiting future exploration on efficient and scalable vision methods. Figure 8. ViT accuracy curves. Our CViT outperforms baselines consistently along different mask ratios. ranks for generation? We train separate model in each direction in Tab. 4 for ablation. The ascending model performs significantly worse than the descending one. This observation is consistent with the intuition that patches most relied upon by others should be generated first. 4.4. Benchmarking CViT We compare CViT with four baselines under different patch mask ratios in Tab. 5. The ViT model sees full images during training and is randomly masked during inference. ViT+C is ViT variant that infers from collapse masks instead of random masking without modifying ViTs training. RViT is trained in the same way as CViT but with random masks. Although DynamicViT [34] is token-pruning method that operates on model instead of data space, we include it to compare the effects of these separate pruning perspectives. Specifically, we employ the DeiT-B variant of DynamicViT which has similar parameter sizes as our ViT. We test these models performance with top-k classification accuracy and an Area under Curve (AuC) metric that integrates over top-1 accuracies along the 0 99% mask rates. CViT leads in almost all metrics, demonstrating our collapse orders efficiency in capturing salient visual information from sparse high-rank patches. This effect is also observed in the training-less ViT+C, whose accuracy is higher"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Proof of Optimal Collapse Ranking We give formal proof below to show that computing PageRank scores of the adjacency matrix in Section 3.2 from CoMAE gives the optimal ranking in Eq. (1) that minimizes image uncertainty (cumulative patch entropy). Definition 1 (Cumulative conditional entropy). Define the cumulative conditional entropy of an ordered prefix by: Under Assumption of Submodularity, greedy maximization of the marginal gain (j S) gives (1 1/e)- approximation to the minimization of Hc [31]. Since the linear model assumes (j S) rj, selecting nodes in descending order of rj yields an optimal prefix sequence for the cumulative entropy minimization objective. Finally, note that the fixed-point equation above matches the PageRank formulation: Hc(S) := (cid:88) n=1 H(cid:0)en {ej : S}(cid:1), (8) where measures the distribution entropy. Assumption 1 (Linear influence model). There exists β RN 0 such that the expected marginal reduction in Hc by observing patch is approximately given by: (j S) (cid:0)(I cP )1β(cid:1) j. (9) = (1 c)β + cP r, (15) after normalizing β so that it sums to one. Hence, is precisely the PageRank (or personalized PageRank) vector for , completing the proof. 7. Additional Qualitative Results We provide more qualitative samples generated by our CMAR in Fig. 9. It can be observed that our method synthesizes high-fidelity images across broad range of classes. Assumption 2 (Submodularity). The entropy Hc is monotone and approximately submodular in S. 8. Model Architectures Theorem 1 (Optimal collapse ranking). Let [0, 1]N be the learned dependency matrix with Aij indicating the influence of patch on patch i. Let be the corresponding column-stochastic matrix, and let (0, 1). Ordering patches in descending order of: := (I cP )1β, (10) minimizes the linearized proxy of Hc at each prefix. If β is constant or interpreted as personalized teleport vector, this ordering corresponds to PageRank on . Proof. By the Neumann series expansion of Eq. (10): = (I cP )1β = (cid:88) t=0 ctP tβ, (11) we derive the following recurrence for r: cP = cP (cid:88) t=0 ctP tβ = (cid:88) t=0 ct+1P t+1β = (cid:88) s=1 csP sβ, (12) where = + 1. Therefore, β + cP = β + (cid:88) s=1 csP sβ = (cid:88) s=0 csP sβ = r. (13) Thus, satisfies the fixed-point equation: = β + cP r. (14) The architectures of MAR [27] and Vision Transformer [10] are well-documented in their original papers. We elaborate on our novel CoMAE architecture in Fig. 10 below. Given the patch token sequence, the CoMAE encoder concatenates learned [cls] token in front of it. The target patch is dropped to avoid information leakage during reconstruction. This sequence is then transformed by Rotary Position Embedding (RoPE) [43] to inform the encoder of each patchs relative image location. Next, we pass the sequence through 12 self-attention blocks with an embedding dimension of 256, retaining the processed [cls] token as output. Finally, selection weight vector is obtained by passing this [cls] token through linear projection, followed by tanh normalization to keep each entry between [0, 1]. We take the original input patch token sequence (without encoder processing) and inject them with Gaussian noise following Eq. (3) in Section 3.1. Again, the target patch is dropped for reconstruction. To inform the decoder which patch is currently under reconstruction, we append [tgt cls] token in front of the entire sequence. The decoder then processes this sequence with 12 self-attention blocks. The embedding dimension is set as 64. Finally, we retrieve the first [tgt] token and pass it through feedforward head to obtain the reconstructed target patch. 9. Limitations and Future Improvements As our collapse order describes local image units as patches, its subject to the constraints of this representation: (1) each region has the same fixed size and (2) the shape of each Figure 9. Additional qualitative samples of CMAR. Our method generates high-fidelity images across classes. 10. Ethical Statement Our method studies the optimal scanning order of patches in image synthesis and classification. Since the identified high-rank patches in our collapse sequence have shown greater influence on these downstream tasks, visual patch attacks [4] could leverage them for more concentrated prompt engineering. However, the same information can also be adopted by visual models to enhance their robustness on high-rank patches to guard against such attacks. We will release our implementation code and model weights to aid the design of these defenses."
        },
        {
            "title": "References",
            "content": "[1] Not all patches are what you need: Expediting vision transformers via token reorganizations. Not all patches are what you need: Expediting vision transformers via token reorganizations. In ICLR, 2021. 3 [2] Wele Gedara Chaminda Bandara, Naman Patel, Ali Gholami, Mehdi Nikkhah, Motilal Agrawal, and Vishal Patel. AdaMAE: Adaptive masking for efficient spatiotemporal learning with masked autoencoders. In CVPR, pages 14507 14517, 2023. 2 [3] Sergey Brin and Lawrence Page. The anatomy of largescale hypertextual web search engine. Computer networks and ISDN systems, 30(1-7):107117, 1998. 2, 3 [4] Tom Brown, Dandelion Mane, Aurko Roy, Martın Abadi, arXiv preprint and Justin Gilmer. Adversarial patch. arXiv:1712.09665, 2017. 3 [5] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. MaskGIT: Masked generative image transformer. In CVPR, pages 1131511325, 2022. 2, [6] Timothee Darcet, Federico Baldassarre, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Cluster and predict latent patches for improved masked image modeling. arXiv preprint arXiv:2502.08769, 2025. 2 [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, pages 248255, 2009. 2, 5 [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, pages 4171 4186, 2019. 2 [9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 6 [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 2, 3, 5, 1 [11] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, pages 1287312883, 2021. 2, Figure 10. Architecture of CoMAE. Both the encoder and decoder follow ViT to pool information into [cls] token. patch doesnt reflect object saliency. In more generalized setup, the image units can be expressed with salient local features such as segmentation maps or class activation maps. This extra layer of saliency fits more closely with our assumption of image locality during the collapse process. For now, we keep the modeling simple to study the preliminary feasibility of formalizing image collapse and leave these explorations to future works. Additionally, we were unable to train more variants for CoMAE, CMAR, and CViT in this work due to limited computation. While we show that the current collapse order can already boost MIM methods in image generation and classification, scaling up training should achieve even higher performance gains. For instance, CoMAE can identify collapse orders more accurately with deeper encoder, and larger MAR models can be converted into CMARs with longer training. More vision tasks can also be tested with collapse order for optimization, such as segmentation and detection. We will conduct these experiments if more computational resources become available in the future. Finally, an alternative CoMAE design remains unexplored. Instead of training decoder from scratch, we could directly employ MAR or similar autoregressive image generators for decoding. These pretrained decoders decouple encoder learning from the reconstruction objective, making it more efficient. Furthermore, CoMAE can also be deployed on the representation space of Representation Autoencoders (RAEs) [57] to identify the process of representation collapse instead of patch collapse. We deem these directions as having high potentials in improving CoMAE and will study them soon. [12] Judith Fan, Wilma Bainbridge, Rebecca Chamberlain, and Jeffrey Wammes. Drawing as versatile cognitive tool. Nature Reviews Psychology, 2(9):556568, 2023. 1 [13] Mohsen Soroush Abbasi Koohpayegani, Fayyaz, Farnoush Rezaei Jafari, Sunando Sengupta, Hamid Reza Vaezi Joze, Eric Sommerlade, Hamed Pirsiavash, and Jurgen Gall. Adaptive token sampling for efficient vision transformers. In ECCV, pages 396414. Springer, 2022. 3 [14] Richard Phillips Feynman. Space-time approach to nonrelativistic quantum mechanics. Reviews of Modern Physics, 20(2):367, 1948. 1 [15] Rohit Girdhar, Alaaeldin El-Nouby, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Omnimae: Single model masked pretraining on images and videos. In CVPR, pages 1040610417, 2023. 2 [16] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, pages 1600016009, 2022. 2 [17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 30, 2017. 6 [18] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshop on Deep Generative Models and Downstream Applications, 2021. 5 [19] Zhicheng Huang, Xiaojie Jin, Chengze Lu, Qibin Hou, Ming-Ming Cheng, Dongmei Fu, Xiaohui Shen, and Jiashi Feng. Contrastive masked autoencoders are stronger vision learners. IEEE TPAMI, 46(4):25062517, 2023. [20] Laurent Itti, Christof Koch, and Ernst Niebur. model of saliency-based visual attention for rapid scene analysis. IEEE TPAMI, 20(11):12541259, 2002. 6 [21] Ioannis Kakogeorgiou, Spyros Gidaris, Bill Psomas, Yannis Avrithis, Andrei Bursuc, Konstantinos Karantzalos, and Nikos Komodakis. What to hide from your students: Attention-guided masked image modeling. In ECCV, pages 300318, 2022. 2 [22] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 3 [23] Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei Niu, Mengshu Sun, Xuan Shen, Geng Yuan, Bin Ren, Hao Tang, et al. Spvit: Enabling faster vision transformers via latency-aware soft token pruning. In ECCV, pages 620640. Springer, 2022. 3 [24] Hermann Kumbong, Xian Liu, Tsung-Yi Lin, Ming-Yu Liu, Xihui Liu, Ziwei Liu, Daniel Fu, Christopher Re, and David Romero. Hmar: Efficient hierarchical masked autoIn CVPR, pages 25352544, regressive image generation. 2025. 2, 3 [25] Gang Li, Heliang Zheng, Daqing Liu, Chaoyue Wang, Bing Su, and Changwen Zheng. Semmae: Semantic-guided masking for learning masked autoencoders. NeurIPS, 35:14290 14302, 2022. 2 [26] Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked generative encoder to unify representation learning and image synthesis. In CVPR, pages 21422152, 2023. [27] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In NeurIPS, pages 5642456445, 2024. 1, 2, 3, 4, 5, 6 [28] Jihao Liu, Xin Huang, Jinliang Zheng, Yu Liu, and Hongsheng Li. Mixmae: Mixed and masked autoencoder for efficient pretraining of hierarchical vision transformers. In CVPR, pages 62526261, 2023. 2 [29] Neelu Madan, Nicolae-Catalin Ristea, Kamal Nasrollahi, Thomas Moeslund, and Radu Tudor Ionescu. Cl-mae: Curriculum-learned masked autoencoders. In WACV, pages 24922502, 2024. 2 [30] Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan, Zuxuan Wu, Yu-Gang Jiang, and Ser-Nam Lim. Adavit: Adaptive vision transformers for efficient image recognition. In CVPR, pages 1230912318, 2022. 3 [31] George Nemhauser, Laurence Wolsey, and Marshall Fisher. An analysis of approximations for maximizing submodular set functionsi. Mathematical programming, 14 (1):265294, 1978. 1 [32] Aude Oliva and Antonio Torralba. The role of context in object recognition. Trends in cognitive sciences, 11(12):520 527, 2007. [33] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei Efros. Context encoders: Feature learning by inpainting. In CVPR, pages 25362544, 2016. 2 [34] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic token sparsification. NeurIPS, 34:1393713949, 2021. 3, 8 [35] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Beyond next-token: Next-x predicIn ICCV, pages tion for autoregressive visual generation. 1578115791, 2025. 2, 3 [36] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference In ICML, pages 12781286. in deep generative models. PMLR, 2014. 3 [37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. 5 [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. [39] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. NeurIPS, 29, 2016. 6 [40] Ville Satopaa, Jeannie Albrecht, David Irwin, and Barath Finding kneedle in haystack: DetectRaghavan. In 2011 31st internaing knee points in system behavior. tional conference on distributed computing systems workshops, pages 166171. IEEE, 2011. 8 [41] Jeongwoo Shin, Inseo Lee, Junho Lee, and Joonseok Lee. NeurIPS, 37:58929 Self-guided masked autoencoder. 58954, 2024. 3 [57] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025. 3 [42] Eero Simoncelli and Bruno Olshausen. Natural image statistics and neural representation. Annual review of neuroscience, 24(1):11931216, 2001. [43] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 1 [44] Quan Tang, Bowen Zhang, Jiajun Liu, Fagui Liu, and Yifan Liu. Dynamic token pruning in plain vision transformers for semantic segmentation. In ICCV, pages 777786, 2023. 3 [45] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. NeurIPS, 37:8483984865, 2024. 2, 3 [46] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. VideoMAE: Masked autoencoders are data-efficient learners for self-supervised video pre-training. In NeurIPS, pages 1007810093, 2022. 2 [47] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. NeurIPS, 29, 2016. 2, 3 [48] Aaron Van Den Oord, Nal Kalchbrenner, and Koray In ICML, Kavukcuoglu. Pixel recurrent neural networks. pages 17471756. PMLR, 2016. 2, [49] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, pages 1096 1103, 2008. 2 [50] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Images speak in images: generalist Tiejun Huang. In Proceedings of painter for in-context visual learning. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68306839, 2023. 2 [51] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: simple In CVPR, pages framework for masked image modeling. 96539663, 2022. 2 [52] Xiaoyun Xu, Shujian Yu, Zhuoran Liu, and Stjepan Picek. MIMIR: Masked image modeling for mutual arXiv preprint information-based adversarial robustness. arXiv:2312.04960, 2023. 2 [53] Hongwei Xue, Peng Gao, Hongyang Li, Yu Qiao, Hao Sun, Houqiang Li, and Jiebo Luo. Stare at what you see: Masked In CVPR, pages image modeling without reconstruction. 2273222741, 2023. 2 [54] Hongxu Yin, Arash Vahdat, Jose Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov. A-vit: Adaptive tokens for efficient vision transformer. In CVPR, pages 1080910818, 2022. [55] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, MingHsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In CVPR, pages 1045910469, 2023. 3 [56] Zilong Zhang, Chujie Qin, Chunle Guo, Yong Zhang, Chao Xue, Ming-Ming Cheng, and Chongyi Li. Ram++: Robust representation learning via adaptive mask for all-in-one image restoration. arXiv preprint arXiv:2509.12039, 2025."
        }
    ],
    "affiliations": [
        "School of Computer Science, The University of Sydney"
    ]
}