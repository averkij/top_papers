{
    "paper_title": "Phantom: Subject-consistent video generation via cross-modal alignment",
    "authors": [
        "Lijie Liu",
        "Tianxiang Ma",
        "Bingchuan Li",
        "Zhuowei Chen",
        "Jiawei Liu",
        "Qian He",
        "Xinglong Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent video through textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, a unified video generation framework for both single and multi-subject references. Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn cross-modal alignment via text-image-video triplet data. In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages. The project homepage is here https://phantom-video.github.io/Phantom/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 9 7 0 1 1 . 2 0 5 2 : r PHANTOM: SUBJECT-CONSISTENT VIDEO GENERATION VIA CROSS-MODAL ALIGNMENT Lijie Liu, Tianxiang Ma, Bingchuan Li, Zhuowei Chen, Jiawei Liu, Qian He, Xinglong Wu Intelligent Creation Team, ByteDance"
        },
        {
            "title": "ABSTRACT",
            "content": "The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent video through textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, unified video generation framework for both single and multi-subject references. Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn crossmodal alignment via text-image-video triplet data. In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages. The project homepage is here https://phantom-video.github.io/Phantom/."
        },
        {
            "title": "INTRODUCTION",
            "content": "The rise of diffusion models (Peebles & Xie, 2023; Ho et al., 2020) is rapidly reshaping the field of generative modeling at an astonishing pace. Among them, the advancements in video generation brought by diffusion models are particularly remarkable. In the visual domain, video generation needs to pay more attention to the continuity and consistency of multiple frames compared to image generation, which poses additional challenges. Inspired by the scaling laws of large language models (OpenAI, 2024; Touvron et al., 2023; Yang et al., 2024a), the focus of video generation has shifted towards investigating foundational large models, such as those similar to Sora (OpenAI, 2023; Polyak et al., 2024; Yang et al., 2024b; Kong et al., 2024; Zheng et al., 2024), which have demonstrated exciting visual effects and are paving the way for new paradigm in Artificial Intelligence Generated Content (AIGC). Nevertheless, up to now, various vertical applications of video generation tasks still lag behind image generation tasks. Currently, foundational video generation models primarily focus on two major tasks: text-tovideo(OpenAI, 2023) and image-to-video (Blattmann et al., 2023). Text-to-video (T2V) leverages language models to understand input text instructions and generate visual content describing the expected characters, movements, and backgrounds. While it allows for creative and imaginative content combinations, it often struggles with generating consistently predictable results due to inherent randomness. On the other hand, image-to-video (I2V) typically provides the first frame of an image along with optional text descriptions to transform static image into dynamic video. While more controllable, the content richness is often limited by the strict copy-paste (Polyak et al., 2024; Chen et al., 2025) nature of the first frame. Capturing the subject from the image and flexibly generating video based on text prompts, combining the diversity and controllability of image and text joint generation, we term this subject-consistent video generation as subject-to-video (S2V) (Huang et al., 2025; Chen et al., 2025; MiniMax, 2024). Its essence lies in balancing the dual-modal prompts of text and image, requiring the model to simultaneously align text instructions with the reference image content. Equal Contributions Project Lead Figure 1: Subject-consistent generation examples using our method, with reference images and corresponding video frames (text prompts omitted). The last three rows show multiple reference subjects. 2 In the subject-to-video task setting, existing works primarily focus on identity preservation for individuals. For example, ID-Animator (He et al., 2024), derived from the Adapter (Ye et al., 2023) concept in image tasks, injects facial information into the model to generate ID-preserving videos. ConsisID (Yuan et al., 2024) further distinguishes between high and low-frequency ID information for model injection. However, these works have been validated on small datasets (around 10k), which limits their ability to fully align facial information with text descriptions. MovieGen (Polyak et al., 2024) scales up to larger dataset (around 10M) and trains on larger model (30b parameters), demonstrating realistic and appealing results. The most aligned approaches for this task are demonstrated by the commercial tools Vidu (Vidu, 2024), Pika (Pika, 2024) and Keling (keling, 2024), which support generating subject-consistent videos using multiple reference images and text descriptions. The reference subject elements are not limited to characters, ID, or clothing but also include buildings, landscapes, and other components. Figure 2: Relationship in cross-modal video generation tasks. In summary, the subject-to-video task aims to deeply and simultaneously align the conditions of both text and image modalities. To achieve this, we first constructed data structure consisting of text-image-video triplets. Unlike T2V, we have re-annotated the video captions (Team et al., 2023) to focus on describing the appearance and action of the subjects in the video. Additionally, the reference images for subjects are not naively taken from single video frame but are sampled across multiple videos, ensuring that the generated video does not simply copy-paste the images like I2V. Furthermore, we redesigned the joint image-text injection model based on existing foundational video models to ensure effective learning of cross-modal data representations. Overall, we developed an algorithmic framework for subject-to-video, which is competitive with existing solutions on the market (Vidu, 2024; Pika, 2024; keling, 2024). Notably, it demonstrates superior performance in maintaining subject consistency compared to current ID-preserving expert models (Yuan et al., 2024; MiniMax, 2024)."
        },
        {
            "title": "2 PHANTOM",
            "content": "2.1 DATA PIPELINE Figure 3: Data processing pipeline for cross-modal video generation To achieve subject-to-video (S2V) alignment, we construct image, text, and video triplet data structures for cross-modal learning (Figure 3), requiring videos to be simultaneously paired with both images and text. Since our S2V capability is fine-tuned from text-to-video (T2V) base model, we aim to reuse the T2V captioning scheme, leveraging vision-language models to directly comprehend videos. Inspired by MoiveGen (Polyak et al., 2024), we categorize image prompts into two types: in-paired and cross-paired. In-paired data involves selecting keyframes from videos as reference images. While this ensures consistency between the subjects in the images and videos, the high visual similarity may lead the model to overlook the text prompts, resulting in generated videos that simply copy-paste the input images. To mitigate this issue, we construct cross-paired data by matching 3 elements across different videos and filtering out clips with high visual similarity. Cross-paired data can come from different segments of the same long video or by retrieving reference subjects from database. After constructing the cross-paired data pipeline, further segmentation is required based on application scenarios. We define subject-to-video (S2V) as extracting primary elements from an image and generating video controlled by text. These primary elements include people, animals, objects, backgrounds, and more. Additionally, interactions between multiple elements can further categorize scenarios, such as multi-person interactions, human-pet interactions, and human-object interactions. By segmenting the data sources according to these application scenarios, we can quantitatively supplement missing data types. For example, virtual try-on applications require specific collections of model images and garment layouts. Furthermore, since our S2V capabilities are fine-tuned from the text-to-video (T2V) base model, bypassing the base model pre-training stage, we need to filter for higher data quality rather than merely increasing data quantity. 2.2 FRAMEWORK Figure 4: Overview of the Phantom architecture The Phantom architecture is illustrated in Figure 4, where the model is divided into an untrained input head and trainable DiT (Peebles & Xie, 2023) module. For the DiT part, we refer to the MMDiT (Esser et al., 2024) structure, which is one of the mainstream choices for both image and video foundation models. The MMDiT module undergoes pre-training similarly to conventional T2V and first-last-frame I2V processes (Zeng et al., 2024), thus acquiring the necessary capabilities. In the input head, the video encoder (Yang et al., 2024b) and text encoder (Yang et al., 2024a) inherit weights from the base model, encoding input video and text prompts into corresponding latent features. Crucially, the reference image is encoded by specific vision encoder and then concatenated with video features and text features separately. These concatenated features are input to the vision branch and text branch of DiT for computation. This approach modifies only the models input without affecting the DiT structure itself. Specifically, the vision encoder is composed of Variational AutoEncoder (VAE) (Esser et al., 2021) and CLIP (Zhai et al., 2023; Radford et al., 2021). Image features concatenated with video latents reuse the 3D VAE (Yang et al., 2024b) to maintain consistency in the visual branch input. Meanwhile, image features concatenated in the text branch use CLIP to provide high-level semantic information, thus compensating for the limitations of the low-level VAE features."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "3.1 EVALUATION MATERIALS Phantom is fine-tuned from video generation foundation model based on the DiT architecture. The T2V and I2V pre-training stages are excluded from this evaluation. We focus on assessing the subject consistency generation capability, with additional independent evaluations for face ID-based 4 Figure 5: Video quality evaluation (left) and user study results for multi-subject consistency (right). video generation. Due to the lack of an established benchmark for subject-to-video, we constructed specific test set and defined evaluation metrics accordingly. We collected 50 reference images from different scenarios, covering humans, animals, products, environments, and clothing. Each reference image is paired with 3 different text prompts. To ensure confidence in each case, each text-image pair is generated with three random seeds, resulting in total of 450 videos. For scenarios with multiple reference images, we mixed the aforementioned reference images and rewrote the text prompts to obtain test set of 50 groups. Additionally, considering the unique value of portrait scenarios, we collected an additional 50 portrait reference images, including both celebrities and ordinary individuals, for independent evaluation of ID consistency. For the S2V task, the currently available state-of-the-art (SOTA) methods are closed-source commercial tools. Therefore, we evaluated and compared the latest capabilities of Vidu (Vidu, 2024), Pika (Pika, 2024), and Keling (keling, 2024). For the ID-preserving video generation task, the commercial tool Hailuo (MiniMax, 2024) demonstrated impressive results. We also evaluated the open-source algorithm ConsisID (Yuan et al., 2024). 3.2 QUANTITATIVE RESULTS We classify the S2V evaluation metrics into three categories: video quality, text-video consistency, and subject-video consistency. First, the visualization of video quality is shown in the radar chart on the left side of Figure 5. We selected six metrics provided by VBench (Huang et al., 2024) for testing and supplemented them with four inner model scores such as structure breakdown score. For textvideo consistency, we used ViCLIP (Wang et al., 2022) to directly calculate the cosine similarity score between the text and the video. For single subject consistency, we uniformly sampled 10 frames from each video and calculated the CLIP (Cherti et al., 2023) and DINO (Oquab et al., 2023) feature Direction Scores with the reference image. Additionally, we used grounded-sam to segment the subject part of the video and calculate the CLIP and DINO scores (excluding scene graphs). For ID consistency, we used three facial recognition models to measure similarity (Deng et al., 2019; Huang et al., 2020). The video quality evaluation results, shown on the left side of Figure 5, indicate that Phantom performs poorly on the Dynamic Degree metric (Huang et al., 2024) , while excelling in other metrics. As shown in Tables 1 and 2, Phantom leads in overall metrics for subject consistency (Identity Consistency) and prompt following. For multi-subject video generation, due to high error rates in automated subject detection and matching, we conducted user study. We surveyed 20 users, who rated the methods on scale of 1 to 3 (1: unusable, 2: usable, 3: satisfactory). The evaluation results, displayed in the bar chart on the right side of Figure 5, show that Phantoms multi-subject performance is comparable to commercial solutions, with some advantages in subject consistency. 5 Methods ConsisID Hailuo-ID Phantom-ID Identity Consistency Prompt Following FaceSim-Arc FaceSim-Cur FaceSim-glink ViCLIP-T 0.538 0.542 0.581 0.417 0.504 0.529 0.470 0.557 0.590 21.76 23.31 24.12 Table 1: Comparison of different methods based on identity consistency and prompt following Methods Subject Consistency Prompt Following CLIP-I DINO-I CLIP-I-Seg DINO-I-Seg ViCLIP-T Vidu2.0 Pika2.1 Keling1.6 Phantom-IP 0.706 0.697 0.732 0.714 0.511 0.498 0.554 0.523 0.724 0.712 0.715 0.731 0.544 0.534 0.569 0.538 22.78 23.05 21.62 23.41 Table 2: Comparison of different methods based on single subject consistency and prompt following. Boldface indicates the highest scores in each column, and underline indicates the second-highest scores. 3.3 QUALITATIVE RESULTS Here, we present the comparison results of several typical cases in Figures 6, 7, 8. Each generated video is displayed with four evenly sampled frames, including the first and last frames. Figures 6 and 7 respectively show the results of generating single and multiple subject consistency. It can be seen that Vidu (Vidu, 2024) and Phantom exhibit balanced performance in subject consistency, visual effect, and text response. Pika (Pika, 2024) performs poorly in subject consistency. Keling (keling, 2024) has notable issue: some cases are very similar to I2V. For instance, the first frame of character videos almost matches the input reference image, leading to low success rates in virtual try-on scenarios. Additionally, the laptop case shows that the evaluated methods tend to cause deformations in rigid body movements. Figure 8 shows the results of video generation for facial ID preservation. The open-source method ConsisID (Yuan et al., 2024) tends to exhibit motion blur, and has weak text response. Hailuo (MiniMax, 2024) excels in visual aesthetics, but there is some loss in facial similarity. Our results are balanced across all dimensions, with particular advantage in ID consistency."
        },
        {
            "title": "4 CONCLUSION",
            "content": "We propose Phantom, method for subject-consistent video generation using text-image-video triplet structure for cross-modal alignment. We redesigned the joint text-image model injection method based on existing video foundation models to effectively learn cross-modal data forms. This algorithm framework is highly competitive, particularly in unifying facial ID preservation tasks. Experimental results show that Phantom maintains an advantage over some commercial solutions."
        },
        {
            "title": "REFERENCES",
            "content": "Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Kwot Sin Lee, Ivan Skorokhodov, Kfir Aberman, Jun-Yan Zhu, Ming-Hsuan Yang, and Sergey Tulyakov. Multi-subject open-set personalization in video generation. arXiv preprint arXiv:2501.06187, 2025. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for 6 Figure 6: Comparative results of single reference subject-to-video generation Figure 7: Comparative results of multi-reference subject-to-video generation 8 Figure 8: Comparative results of video generation for face ID preservation 9 contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 28182829, 2023. Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 46904699, 2019. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, and Jie Id-animator: Zero-shot identity-preserving human video generation. arXiv preprint Zhang. arXiv:2404.15275, 2024. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu, Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue Huang. Curricularface: adaptive curriculum learning loss for deep face recognition. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5901 5910, 2020. Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, and Kun Gai. Conceptmaster: Multi-concept video customization on diffusion transformer models without test-time tuning. arXiv preprint arXiv:2501.04698, 2025. Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, et al. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503, 2024. keling. Image to video elements feature. https://klingai.com/image-to-video/ multi-id/new/, 2024. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. MiniMax. Hailuo s2v-01. https://www.minimaxi.com/en/news/s2v-01-release/, 2024. OpenAI. Sora. https://openai.com/, 2023. Accessed: February 10, 20245. OpenAI. Chatgpt (gpt-4 version). https://chat.openai.com/, 2024. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Pika. Pikascenes. https://pika.art/ingredients/, 2024. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 10 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Vidu. Reference to video. https://www.vidu.com/, 2024. Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, arXiv preprint Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv:2412.15115, 2024a. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024b. Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identity-preserving text-to-video generation by frequency decomposition. arXiv preprint arXiv:2411.17440, 2024. Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: High-dynamic video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 88508860, 2024. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language In Proceedings of the IEEE/CVF International Conference on Computer image pre-training. Vision, pp. 1197511986, 2023. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024."
        }
    ],
    "affiliations": [
        "Intelligent Creation Team, ByteDance"
    ]
}