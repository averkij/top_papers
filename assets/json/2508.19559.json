{
    "paper_title": "Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference",
    "authors": [
        "Rongzhi Li",
        "Ruogu Du",
        "Zefang Chu",
        "Sida Zhao",
        "Chunlei Han",
        "Zuocheng Shi",
        "Yiwen Shao",
        "Huanle Han",
        "Long Huang",
        "Zherui Liu",
        "Shufan Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Serving Large Language Models (LLMs) is a GPU-intensive task where traditional autoscalers fall short, particularly for modern Prefill-Decode (P/D) disaggregated architectures. This architectural shift, while powerful, introduces significant operational challenges, including inefficient use of heterogeneous hardware, network bottlenecks, and critical imbalances between prefill and decode stages. We introduce HeteroScale, a coordinated autoscaling framework that addresses the core challenges of P/D disaggregated serving. HeteroScale combines a topology-aware scheduler that adapts to heterogeneous hardware and network constraints with a novel metric-driven policy derived from the first large-scale empirical study of autoscaling signals in production. By leveraging a single, robust metric to jointly scale prefill and decode pools, HeteroScale maintains architectural balance while ensuring efficient, adaptive resource management. Deployed in a massive production environment on tens of thousands of GPUs, HeteroScale has proven its effectiveness, increasing average GPU utilization by a significant 26.6 percentage points and saving hundreds of thousands of GPU-hours daily, all while upholding stringent service level objectives."
        },
        {
            "title": "Start",
            "content": "Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference Rongzhi Li1,2,, Ruogu Du1,, Zefang Chu1,, Sida Zhao1, Chunlei Han1, Zuocheng Shi1, Yiwen Shao1, Huanle Han1, Long Huang1, Zherui Liu1, Shufan Liu1 1ByteDance Seed, 2National University of Singapore, Equal contribution"
        },
        {
            "title": "Abstract",
            "content": "Serving Large Language Models (LLMs) is GPU-intensive task where traditional autoscalers fall short, particularly for modern Prefill-Decode (P/D) disaggregated architectures. This architectural shift, while powerful, introduces significant operational challenges, including inefficient use of heterogeneous hardware, network bottlenecks, and critical imbalances between prefill and decode stages. We introduce HeteroScale, coordinated autoscaling framework that addresses the core challenges of P/D disaggregated serving. HeteroScale combines topology-aware scheduler that adapts to heterogeneous hardware and network constraints with novel metric-driven policy derived from the first large-scale empirical study of autoscaling signals in production. By leveraging single, robust metric to jointly scale prefill and decode pools, HeteroScale maintains architectural balance while ensuring efficient, adaptive resource management. Deployed in massive production environment on tens of thousands of GPUs, HeteroScale has proven its effectiveness, increasing average GPU utilization by significant 26.6 percentage points and saving hundreds of thousands of GPU-hours daily, all while upholding stringent service level objectives. Date: August 28, 2025 Correspondence: Shufan Liu at liushufan.amos@bytedance.com 5 2 0 2 7 2 ] . [ 1 9 5 5 9 1 . 8 0 5 2 : r"
        },
        {
            "title": "1 Introduction",
            "content": "Large Language Models (LLMs) are revolutionizing applications from chatbots [11, 38] to search engines [52], but serving them is extremely GPUintensive [36]. State-of-the-art models have limited throughput, making resource efficiency critical [13, 18, 53, 55]. LLM traffics strong diurnal pattern [22, 43, 49] necessitates effective autoscaling to prevent massive resource waste. However, traditional autoscalers like Kubernetess Horizontal Pod Autoscaler (HPA) [29, 31] struggle to meet the demands of modern LLM serving architectures, particularly in PrefillDecode (P/D) disaggregated settings [40, 42, 44, 57, 58]. P/D disaggregation separates the compute-intensive prefill phase from the memory-bound decode phase, allowing for independent optimization. While this improves efficiency, it introduces trio of interconnected scaling and scheduling challenges at large scale that naive solutions cannot address: 1. Heterogeneous Hardware Inefficiency: Prefill and decode phases have different computational demands (compute-intensive vs. memory-bandwidth-bound, respectively) [27]. naive remedy is to run on homogeneous pool of general-purpose GPUs, but this over-provisions compute for decode or HBM bandwidth for prefill. Recent measurements show that such one-size-fits-all provisioning inflates cost per generated token by 41% compared with phase-aware heterogeneous deployment [58]. Homogeneous pools therefore increase cost and aggravate fragmentation in large heterogeneous clusters. 2. Network Bottlenecks: Transferring the large KeyValue (KV) cache [45] is bandwidth-intensive operation requiring high-throughput interconnects [41, 57]. baseline, topology-agnostic scheduler that treats all GPUs as flat resource pool places instances wherever capacity is available. Our empirical observations show that such placements across different network switches can reduce the available bandwidth for KV cache transfer by approximately 20%. This bandwidth reduction directly leads to higher transfer latency, creating severe performance bottleneck. 3. Architectural Imbalance: Maintaining an optimal ratio of prefill-to-decode (P/D) instances is crucial to prevent one phase from bottlenecking the other, which cripples overall throughput [41, 46]. naive approach would be to apply standard HPA to each pool independently based on GPU utilization. This fails because decode-phase GPU utilization is misleading metric, often staying high due to KV cache Independent memory pressure even at low loads. scaling thus leads to architectural imbalance, starving one phase while the other sits idle and crippling overall throughput. To address these challenges, we present HeteroScale, an autoscaling system for P/D disaggregated LLM services. It delivers coordinated, topology-aware, and resource-efficient scaling strategy through three key innovations: Framework for Heterogeneous Resource Management. Our system elevates the P/D ratio and hardware requirements to first-class scheduling constraints. The scheduler intelligently places different service roles on the most suitable hardware types, honoring network affinity and P/D balance simultaneously. This maximizes performance and eliminates the waste inherent in homogeneous allocation strategies. Novel Abstractions for Network-Aware Scheduling. We introduce the Deployment Group to enforce network affinity constraints and the RDMA Subgroup to manage resource priority based on network topology. These abstractions ensure that prefill and decode instances are co-located for low-latency KV cache transfer while optimizing the use of scarce, high-performance hardware. Comprehensive Analysis of Scaling Policies with Production Data. We present the first large-scale empirical analysis of autoscaling metrics for P/D disaggregated serving, using massive production datasets. Our study establishes decode Tokens-PerSecond (TPS) as the most robust signal, unlike conventional hardware metrics which we found to be misleading. This core data-driven insight enables our coordinated scaling policy, which uses this single signal to scale both prefill and decode pool together, thus maintaining architectural balance. HeteroScales capabilities are proven in one of the worlds largest production environments at ByteDance. Managing tens of thousands of GPUs, it consistently delivers substantial performance benefits, saving hundreds of thousands of GPU-hours daily while boosting average GPU utilization by 26.6 percentage points and SM activity by 9.2 percentage points. Crucially, these efficiency improvements are achieved while upholding all Service Level Objectives (SLOs), establishing its architecture as new benchmark for robust, large-scale LLM serving."
        },
        {
            "title": "2 Background and Motivation",
            "content": ""
        },
        {
            "title": "2.1 P/D Disaggregated Serving",
            "content": "LLM inference consists of two distinct phasesprefill and decodecharacterized by its own computational demands and operational behaviors. In the prefill phase, the model processes the entire input prompt in parallel, building KV cache for all input tokens in one go. In contrast, the decode phase operates autoregressively, generating one token at time while relying on previously created KV cache. These phases exhibit fundamentally different computational characteristics. From computation perspective, prefill is highly compute-intensive and benefits from substantial parallelism, whereas decode is inherently sequential and less demanding in raw computation power. Their memory usage also diverges: prefills memory footprint grows with the length of the input prompt, while decodes memory demand increases with the size of the accumulated KV cache. In terms of batching efficiency, prefill can fully exploit large, homogeneous batches to maximize throughput, but decode, restricted to producing one token per step, gains far less from batching and quickly becomes memory-bound. Moreover, the two phases affect latency differently: prefill determines the initial response latency (Time-To-First-Token, TTFT), while decode dictates the token generation speed (TimeBetween-Tokens, TBT) [3, 4]. Traditional LLM serving executes both the prefill and decode phases on the same instance. In contrast, P/D disaggregated serving [25, 40, 41, 44, 57] separates these phases onto distinct sets of instances, enabling each to be optimized independently. Such separation improves resource utilization by allowing prefill and decode nodes to be provisioned individually, even on heterogeneous hardware, so that their differing compute and memory demands can be better matched. It also enhances batching efficiency, since the prefill and decode nodes have different constraints on the batch size. Together, these benefits lead to lower latency, higher throughput, and more cost-effective LLM services."
        },
        {
            "title": "2.2 Motivation and Challenges",
            "content": "HeteroScale is deployed on the Seed Serving Platform, ByteDances production LLM serving environment, large-scale Kubernetes-based system managing thousands of GPUs across multiple data centers. In this environment, conventional autoscalers like the Kubernetes HPA fall short for the unique demands of LLM services. Their reliance on narrow hardware metrics cannot capture the dynamic resource demands in P/D disaggregated settings , and they lack the coordination mechanisms to manage interdependent components like prefill and decode pools, which must be scaled in balanced manner. This coordination gap creates substantial operational overhead, challenge that becomes even more severe for disaggregated Mixture-of-Experts (MoE) serving [58]. HeteroScales design is tailored to the specific challenges of this environment, which is characterized by three key features: Hardware Heterogeneity: The platform manages deep hierarchy of heterogeneous hardware. Resources are organized into logical clusters within actual Kubernetes clusters (also known as physical clusters), which contain service deployments running on diverse machine configurations, including various GPU types (e.g. NVIDIA H20 and L20) with high-speed RDMA interconnects. Hierarchical Network Topology: The network architecture is explicitly hierarchical, comprising racks of machines under local switches (S0), which are aggregated into minipods (S1) and bigpods (S2). This topology is critical for P/D disaggregated serving, where the transfer of large KV caches necessitates network-aware placement to minimize latency. Co-location and Grouping Abstractions: Services are managed through logical abstractions, most notably the Deployment Group. This composite object bundles the different roles of service (e.g., prefill and decode) that must share common scheduling domain, such as being co-located under the same network switch. While roles within Deployment Group can be scaled independently, this abstraction creates complex scheduling interdependencies. These characteristics, such as hardware heterogeneity, strict network topology, and the need for coordinated scaling of disaggregated components, created practical challenges in metric selection and resource management that motivated the design of HeteroScale. When deploying and operating P/D-disaggregated LLM serving at scale on the Seed Serving Platform, we encountered several significant operational challenges. Challenge 1: Scheduling with constraints. Prefill and decode workers may operate on heterogeneous hardware and carry different SLO priorities, since they predominantly influence distinct performance metricsTTFT for prefill and TBT for decode. Shifts in workload characteristics, including model choice, prompt length, and response length, can alter the demand distribution between the two phases. Such imbalances risk leaving resources underutilized in one 3 pool while creating bottlenecks in the other, making coordinated scaling between them non-trivial task. Additionally, P/D disaggregation introduces strong network affinity constraints: prefill and decode nodes must exchange large KV cache data efficiently, which directly contributes to latency. Scheduling must account for multiple constraints, maintaining balanced capacity between pools while reducing the overhead of inter-node communication. Challenge 2: Heterogeneous resource management. Our clusters are composed of highly heterogeneous resources, including GPUs of different generations, memory capacities, compute capabilities, and interconnect bandwidths. The services running on these clusters are equally diversesome rely on homogeneous hardware pools, while others require heterogeneous configurations. Coordinating allocation in this mixed environment is challenging. First, scaling conflicts can arise, such as resource contention between heterogeneous and homogeneous services when they share the same hardware pool. Second, mismatched placementswhere services are not aligned with their optimal hardwarecan lead to performance degradation, wasted capacity, and increased fragmentation, ultimately reducing overall efficiency. Challenge 3: Metric selection. In traditional autoscaling systems such as Kubernetess Horizontal Pod Autoscaler (HPA) [29], decisions are often driven by coarse-grained metrics like GPU utilization. However, in LLM serving with P/D disaggregated, these metrics can be misleading. Decode nodes, for instance, may show high GPU utilization despite operating at low workload due to KV cache memory pressure. This calls for deeper investigation into metrics that better capture the true workload and performance characteristics of each pool. Challenge 4: Large-scale production environment. Given the nature of our production environment, spanning tens of thousands of GPUs and processing trillions of tokens daily, practical solution should be highly scalable and capable of handling diverse operational demands. It must be compatible with various types of services and support different deployment modesincluding common deployments, P/D disaggregation, and disaggregated MoE servingwhile accommodating heterogeneous resources. Additionally, the design should keep maintenance costs low, ensure high reliability under dynamic workloads. These challenges motivat the development of HeteroScale, our comprehensive autoscaling system designed specifically for P/D disaggregated LLM serFigure 1 HeteroScale System Architecture vices."
        },
        {
            "title": "3 System Design",
            "content": "HeteroScale is designed to address the unique challenges of autoscaling P/D disaggregated LLM services. The system consists of three main layers: autoscaling layer with policy engine, federated pre-scheduling layer and sub-cluster scheduling layer. This section details the design of each layer and the key mechanisms that enable efficient autoscaling."
        },
        {
            "title": "3.1 System Architecture",
            "content": "Figure 1 illustrates the overall architecture of HeteroScale. The system is designed as layered architecture with clear separation of concerns: Autoscaling Layer with Policy Engine: Provides configuration validation, storage, APIs, metrics collection, and various policies for making scale-in and scale-out decisions. Federated Pre-Scheduling Layer: Executes scaling decisions, which is in charge of managing resource allocation, assembling topological resource view, and scheduling Deployment Groups. This layer interfaces with the underlying infrastructure to implement scaling actions, including scaling in/out existing resources or creating batches of Geployment Groups with specified P/D ratios. Sub-cluster Scheduling Layer: All scaling, creation, 4 and deletion of Deployment Groups originating in the pre-scheduling layer are delegated through this layer, which propagates the calls down to the underlying Kubernetes API server, where the corresponding CRDs are created or updated. At the same time, this layer exposes the node API and supplies it upward for topology assembly. The layers communicate through well-defined interfaces, allowing for modular development and testing. The monitoring component provides feedback to the policy engine, creating closed-loop control system that continuously adapts to changing workload conditions."
        },
        {
            "title": "3.2 Autoscaling Layer with Policy Engine",
            "content": "The auto-scaling framework integrates configuration management and policy engine capabilities to deliver holistic solution for automated scaling decisions. The configuration management module empowers backend engineers and operators to define production service parameters aligned with desired SLOs and performance targets. Concurrently, the policy engine periodically evaluates these configurations across services, leveraging real-time metric observations to execute scaling actions."
        },
        {
            "title": "3.3 Scaling Policies",
            "content": "One of the main challenges in autoscaling P/D disaggregated services lies in maintaining architectural balance under dynamic loads. Scaling one component in isolation risks bottlenecks that undermine disaggregation. Therefore, HeteroScales policy engine is designed not merely to adjust capacity, but to act as coordinated orchestrator for the interdependent prefill and decode stages. We implement two policy paradigms that provide complementary control strategies: periodic policy for predictable, coarse-grained adjustments, and metrics-driven policy for fine-grained, real-time optimization. These policies translate high-level service objectives into concrete scaling actions that preserve the crucial P/D ratio across heterogeneous hardware. In addition, we develop workload-aware framework for policy curation. 3.3.1 Periodic Scaling Policy The periodic scaling policy adjusts resources based on time-of-day patterns, enabling proactive scaling based on expected workload patterns. This policy is beneficial for services with predictable traffic patterns, such as day-night cycles or weekly patterns. By defining scaling schedules according to periods with static target instance and P/D ratios, expected resources could be automatically allocated to target services. In production environments, periodic scaling is employed for services that operate under specific constraints or involve experimental configurations, which are not amenable to metrics-driven scaling policies."
        },
        {
            "title": "3.3.2 Metrics-driven Scaling Policy",
            "content": "While traditional autoscalers often default to hardware utilization, our investigation revealed that such metrics could be misleading in P/D disaggregated environment. This necessitates principled empirical search for metric that could serve as robust proxy for the overall health and load of the system. viable metric must not only track load pressure with high signal-to-noise ratio but also be universally applicable across diverse models and hardware. Failing to identify the correct signal would lead to instability and resource waste. To characterize P/D-disaggregated serving stack, we group candidate metrics into three classes: Throughput: Prefill and decode TPS; Hardware: Prefill/decode GPU SM activity and GPU utilization [1, 2, 6]; Latency: TTFT and TBT [3, 4]. We used metric traces from an open-domain dialogue service (Figure 2) with hundreds of GPUs for preliminary verification. To demonstrate that these insights are modality-agnostic, we repeated the study on visionlanguage search service; the results appear in the appendix (Figure 8). The same qualitative patterns hold for other workloads such as web search, long-form content understanding, real-time audio conversation, real-time video processing, and code generation. Throughput Metrics Analysis. We found that throughput metrics show significant differences between peak and off-peak periods, with high signal-to-noise ratios, accurately reflecting service load conditions (as shown in Figure 2a). These metrics respond quickly to traffic changes, enabling timely scaling operations. Due to the interference of the KV cache hit rate, prefill TPS measurements under caching are unreliable for autoscaling, thus only \"KV cache missed prefill TPS\" and decode TPS are considered. Hardware-level Metrics Analysis. Hardware-level metrics show clear polarization: the hardware metrics 5 (a) Normalized TPS (b) Normalized latencies (c) GPU utilization (d) SM activity Figure 2 Performance metrics of an open-domain dialogue service with Doubao-Seed-1.6-thinking without autoscaling. of prefill stage instances (such as prefill GPU utilization and SM activity) respond sensitively to load changes with high signal-to-noise ratios, while the decode stage hardware metrics (such as decode GPU utilization and SM activity) maintain high values even under slight pressure, showing low sensitivity to load changes (Figure 2c, 2d). This difference stems from the distinct characteristics of the two stages. The prefill stage is compute-intensive, so its hardware utilization is linearly correlated to load. In contrast, the decode stage is memory-bound, with large portion of its utilization coming from KV cache storage and data transfer operations. These memory operations keep hardware-level metrics at consistently high levels regardless of modest drops in workload, making them far less sensitive to real load changes. Latency Metrics Analysis. Latency metricsTTFT and TBTcapture workload pressure but react in distinctly non-linear fashion (Figure 2b). When the workload is on the lower end, both curves remain nearly flat; once the load moves to the higher end, they shoot upward abruptly. Such cliff-like transition makes it impossible to scale resources proportionally from latency alone and instead demands negative-feedback controller. Among the two, TBT provides clearer signal, whereas TTFT suffers from lower signal-to-noise ratio, limiting its effectiveness as primary autoscaling indicator. From the above analysis, clear pattern emerges: throughput and hardware-level metrics vary proportionally and predictably with changes in resource allocation, whereas latency metrics exhibit nonlinear, threshold-driven behavior. This dichotomy demands hybrid policy approach rather than onesize-fits-all solution. Instead, hybrid approach is requiredone that leverages proportional control for linear metrics and negative-feedback mechanisms for non-linear ones. Guided by this insight, we design two complementary scaling algorithms that together form the basis of HeteroScales stable and efficient autoscaling framework. The full algorithms are detailed in Appendix B. Proportional Control for Linear Metrics The key innovation here is its application in coordinated fashion: the scaling signal from one component (e.g., decode TPS) is used to calculate the required capacity for both the prefill and decode pools, strictly enforcing the target P/D ratio. This transforms simple algorithm into powerful mechanism for maintaining architectural integrity. The system calculates the desired total capacity based on the target-per-instance metric. Then it applies the P/D ratio to determine the final instance counts for each role, preventing the instabilities that would arise from scaling them independently. Negative Feedback for Non-Linear Metrics For highly non-linear, cliff-like metrics such as latency (TTFT and TBT), proportional response would be dangerously unstable, leading to severe over-provisioning and oscillation. Here, we apply more conservative negative feedback strategy. This approach functions as safety mechanism rather than primary scaling driver. It uses multi-tier system of thresholds to trigger fixed, incremental adjustments only when SLOs are at risk of being breached. For example, moderate latency increase triggers small, fixedpercentage scaling event (e.g., 10%), while severe breach triggers larger, more urgent response (e.g., 20%). This cautious, step-based approach prevents the system from overreacting to latencys volatile behavior, providing crucial stability layer that complements the primary proportional strategy. 3.3.3 Workload-centric Policy Curation In HeteroScale, autoscaling follows workload-centric pipeline (Algorithm 1). It begins with pressure test that, given service and its workload profile, identifies the optimal P/D ratio and estimates the expected performance metric under load. Next, each candidate scaling policy is simulated under these baseline conditions to assess its effectiveness. Finally, 6 the system selects the policy that maximizes the chosen objective (e.g., throughput while maintaining SLO compliance). On our production platform, decode TPS serves as the primary scaling metric on our production platform. comprehensive evaluation presented in Section 4 highlights the effectiveness and limitations of the aforementioned real-time metrics for scaling P/D-disaggregated services. Algorithm 1: Workload-centric Policy Curation Input: Service S; workload W; candidate policies Output: Optimal policy popt; optimal P/D ratio ropt; expected metric ˆm (ropt, ˆm) PressureTest(S, W); for each do score[p] Simulate(S, W, p, ropt, ˆm) end popt arg max pP return (popt, ropt, ˆm); score[p];"
        },
        {
            "title": "3.4 Federated Pre-Scheduling Layer",
            "content": "The federated pre-scheduling layer provides higherlevel view of GPU resources and makes scheduling decisions. It is responsible for translating the \"what to scale\" decisions from the policy engine into \"where to place\" decisions on the physical infrastructure. This is achieved through heterogeneous resource management framework and network affinity-aware scheduling algorithm that considers both service requirements and global cluster efficiency. The detailed logic for this scheduling process is available in Appendix B. Heterogeneous Resource Management Framework Managing heterogeneous GPU resources efficiently is essential to maximize resource utilization and service performance. HeteroScale implements heterogeneous resource management framework that efficiently allocates and manages diverse GPU resources. Algorithm 4 in appendix outlines the heterogeneous resource allocation process. The algorithm takes into account resource requests, available resources by type, service priorities, and resource constraints. It sorts requests by priority and attempts to allocate preferred resources for each request. If preferred resources are not available, it tries alternative compatible resources. 7 Deployment Group Abstraction for Network Affinity In P/D disaggregated services, performance is critically dependent on the latency and bandwidth of the network link connecting prefill and decode instances, as this link carries the KV cache [7]. To minimize this communication overhead, instances must often be co-located within network domain that supports high-speed interconnects like RDMA, such as under common aggregation switch (e.g., S2). To manage this co-location requirement while allowing for flexible scaling, HeteroScale introduces the Deployment Group abstraction. Deployment Group is logical container for the prefill and decode roles of single service. Key characteristics include: Shared Scheduling Domain: All instances within group are bound by common network affinity constraint. For high-performance services, this may mandate placement under the same S2 switch. For services with less stringent needs, this constraint can be relaxed to the physical cluster level. Independent Scaling Roles: Within group, the prefill and decode roles function as independent deployment units that can be scaled out or in separately, subject to the systems P/D ratio maintenance logic. This abstraction enables more sophisticated, priorityaware scaling strategy. When service requests scale-out, the scheduler evaluates whether to expand an existing Deployment Group or provision new one in different network domain. This decision is not merely based on available capacity but is guided by the priority of the underlying hardware resources. For instance, the system may choose to create new Deployment Group in lower-priority resource pool to conserve high-priority resources within an existing groups domain for more critical workloads. This intelligent placement prevents service from being bottlenecked by local resource exhaustion while also enabling global, priority-based optimization of valuable, network-proximate hardware. Prioritizing Resources with RDMA Subgroups The strategic placement of Deployment Groups requires method for valuing different resource pools. In large-scale cluster, not all hardware is equal. rack containing multiple GPU types under single S1 switch is scarce, high-value asset, as it allows prefill and decode roles to be placed on different, specialized hardware while maintaining minimal network latency. To prevent services with loose affinity requirements from consuming these premium resources, HeteroScale introduces priority system, which are logical collections of S1 or S2 switches as illustrated in Figure 3. This system is implemented through RDMA Subgroups. An RDMA Subgroup is logical collection of one or more S1/S2 switches, classified into distinct priority tier based on the hardware they contain. By assigning priorities to these hardware pools, the scheduler can make more intelligent resource allocation decisions. We define three primary priority tiers, ranked from lowest to highest: Low Priority: S2 Homogeneous GPU Subgroups. These contain S2 switches where all underlying GPUs are of single type. They are the most common resource configuration and are suitable for the widest range of services. Medium Priority: S2 Heterogeneous GPU Subgroups. In these subgroups, an S2 switch manages mix of GPU types, but each of its underlying S1 switches remains homogeneous. This allows for hardware specialization within the broader S2 domain. High Priority: S1 Heterogeneous GPU Subgroups. These are the most valuable pools, containing S1 switches that directly connect machines with different GPU types. They enable the most demanding heterogeneous P/D configurations with the tightest possible network affinity. When making placement decision, the scheduler uses this hierarchy to guide its choice. For service with low-affinity requirements (e.g., only needing cluster-level co-location), the scheduler will strongly prefer to create or expand Deployment Groups within low-priority subgroups. Conversely, when service explicitly requires high-affinity, heterogeneous setup (e.g., different GPUs under one S1 switch), the scheduler filters for compatible high-priority subgroups. This ensures that scarce, high-performance resource pools are reserved for the workloads that need them most, optimizing global cluster efficiency. P/D Ratio Maintenance One of the key challenges in P/D disaggregated serving is maintaining an optimal ratio between prefill and decode instances. This ratio depends on various factors, including model architecture, prompt length distribution, and generation length distribution. HeteroScale implements P/D ratio maintenance mechanism that maintains the preset ratio during scaling operations. In the online environment, we use fixed P/D ratio for scaling. This P/D ratio is derived from service pressure tests and historical empirical data. The P/D ratio calculation and maintenance process takes into account the current number of prefill and decode instances, the target P/D ratio, scaling threshold, and historical workload data. It first calculates the current P/D ratio and checks if an adIf an justment is needed based on the threshold. adjustment is required, it calculates the optimal instance counts based on workload data and applies smooth transition to avoid abrupt changes. The prefill and decode instances are always scaled in or out in simultaneously. This approach is intended to prevent scenario where, after either prefill or decode instances are successfully scaled out individually, the other one fails to scale due to insufficient resources, thereby avoiding the issue of an imbalanced P/D ratio. However, under certain circumstances, scaling prefill and decode instances simutaneously may also lead to P/D ratio imbalance. For instance, after we create new Deployment Group with fixed P/D ratio, prefill and decode instances may start out of order due to differences in configurations and startup strategies. This can result in temporary imbalance of the P/D ratio, which affects SLOs such as TTFT. To address this this issue, we have added corresponding support at service framework level: if the ratio of prefill to decode instances in ready state deviates significantly from the configured ratio, service discovery for the role with larger quantity will be suspended, and registration will proceed only after the other instances from the other role complete their service discovery registration and instances in ready state recover to tolerable P/D ratio. The Affinity-Aware Scheduling Algorithm The core of HeteroScales scheduling algorithm lies in its network affinity-aware scheduling loop, which translates scaling policies into concrete pod placements. This process, outlined in Algorithm 4, ensures that every placement decision honors both the services specific network requirements and the global resource priorities of the cluster. The scheduling cycle proceeds as follows: 1. Topology Discovery: At the start of each cycle, the controller builds fresh topological resource tree (Figure 3). This provides live, hierarchical view of all available GPUs and their precise locations within the network fabric (node, S1/S2 switch). 8 vices Deployment Groups to scale in, typically targeting those occupying high-priority resource pools to free them up. To maintain consistent state, the released resources are not immediately added back to the available pool; rather, the entire resource view is rebuilt from the underlying cluster state at the beginning of the next scheduling cycle."
        },
        {
            "title": "Extending to Disaggregated MoE",
            "content": "HeteroScale extends to disaggregated MoE by adapting its Deployment Group abstraction for services with distinct prefill and decode components. The prefill stage, itself comprising attention (attn) and feedforward (ffn) instances, is co-located by the scheduler within high-affinity S1 switch, while the entire prefill-decode pair is placed under common S2 switch. This hierarchical scheduling enables dualratio control: maintaining strict attn-to-ffn ratio within prefill replicas and proportional balance between prefill and decode components, which ensures architectural stability and performance."
        },
        {
            "title": "3.5 Sub-cluster Scheduling Layer",
            "content": "All operations on Deployment Groups initiated in the pre-scheduling layer are routed through this component, which delegates the requests to the Kubernetes API server for the creation or update of the corresponding CRDs. At the same time, this layer exposes the node API upward, enabling topology construction. Detailed discussion of this layer is outside the scope of this paper."
        },
        {
            "title": "3.6 System Stability Mechanisms",
            "content": "Ensuring system stability is critical for autoscaling systems, particularly in production environments. HeteroScale implements several mechanisms to maintain system stability: Anti-flapping Mechanisms. Rapid oscillations between scaling in and scaling out, commonly referred to as flapping, can lead to resource waste and system instability. HeteroScale mitigates this problem through combination of complementary mechanisms. First, it enforces cooling periods, ensuring minimum interval between scaling actions to prevent rapid oscillations. This is reinforced by hysteresis thresholds, which use different trigger points for scaling out and scaling in, creating buffer zone that promotes stability. In addition, dampening factors are applied to moderate the scale of adjustments, further smoothing the systems response to changing conditions. Figure 3 Topological resource tree illustrating the hierarchical organization of resources from data centers (VDCs) down to individual nodes. This view, which includes network domains like S1/S2 RDMA Subgroups, is used by the federated pre-scheduler to make network affinity-aware placement decisions. 2. Request Sorting: All pending scaling requests generated by the autoscaling policy engine are sorted, primarily by service priority, to ensure that critical workloads are allocated resources first. 3. Candidate Evaluation (for Scale-Out): For each highpriority scale-out request, the scheduler identifies all valid placement options. This involves evaluating both expanding the services existing Deployment Groups and creating new ones in different, compatible network domains. 4. Priority-Based Selection: Each candidate placement is scored based on the priority of its associated RDMA Subgroup. The scheduler then selects the optimal placement that satisfies the services affinity constraints while minimizing the consumption of highpriority resources. For instance, it will prefer to place low-affinity service in low-priority subgroup, even if this requires creating new Deployment Group, thereby preserving high-value hardware for workloads that require it. 5. Virtual Allocation: Once placement decision is made, the chosen resources are virtually deducted from the topological tree for the remainder of the cycle. This atomic reservation prevents them from being considered for lower-priority requests within the same loop. Handling scale-in requests is more straightforward. The scheduler simply selects one or more of the ser9 Disaster Recovery Measures System failures or unexpected events can interrupt autoscaling operations and threaten service stability. the Seed Serving Platformemploys set of disaster recovery measures. One key mechanism is soft scaling in, in which instances identified for removal are withdrawn from service discovery but kept running. During this observation period, the system monitors SLOs such as latency in real time. If performance remains within targets, the instances are then terminated; however, if degradation is detected, they are reinstated immediately, avoiding the startup delay associated with provisioning new instances. The platform also preserves critical state information to enable fast resumption of normal operations after failure. Finally, when resources become constrained, it applies graceful degradation strategies, maintaining essential functionality while temporarily reducing non-critical services."
        },
        {
            "title": "4 Evaluation",
            "content": "To validate the effectiveness of HeteroScales design, we conducted series of targeted experiments in both controlled environments and production settings with P/D disaggregated LLM services powered by DoubaoSeed-1.6-thinking at ByteDance. HeteroScale has been successfully deployed across multiple services on tens of thousands of GPUs, demonstrating significant and consistent improvements in resource utilization in large-scale, real-world environment."
        },
        {
            "title": "4.1 P/D Ratio",
            "content": "Given the nature of heterogeneous hardware and the distinct bottlenecks in the prefill (P) and decode (D) stages, achieving maximum performance often requires an asymmetric allocation of prefill and decode instances [41, 46]. We conducted two P/D ratio investigation experiments on different services, each with unique input-output length distributions and SLO constraints, running on 16 nodes, each equipped with eight H20 GPUs. Service had an average input length of roughly 3k tokens and an output length of about 350 tokens, giving an input-output (I/O) ratio of 8.5. Its SLOs were set to TTFT 1 and TBT 40 ms. Service had longer input length of around 7.8k tokens and an output length of about 700 tokens, corresponding to an I/O ratio of 11, with SLOs of TTFT 1 and TBT 20 ms. As shown in Figure 4, both services exhibit clear midrange peak in throughput as the P/D ratio varies, with underperformance on either side due to SLO violations. At lower ratios, scarce prefill instances cause TTFT to exceed preset threshold, capping throughput despite idle decode capacity. In contrast, higher ratios lead to excess prefill instances that overwhelm decode resources, pushing TBT beyond its limit, and dampening maximum throughput. This optimal P/D ratio is not fixed; in our production experience, it spans considerable rangefrom 1P/5D to 9P/1Ddepending on the input-output length distribution, hardware configurations, and SLO priorities. This variability underscores the need for flexible scheduling algorithm capable of adapting to diverse P/D ratios."
        },
        {
            "title": "4.2 Metrics-based Autoscaling Policy",
            "content": "As discussed in Section 3.3.2, we selected eight candidate metrics for evaluation: TPS, SM activity, and GPU utilizationeach measured independently for prefill and decode instances (specifically, prefill SM activity, decode SM activity, prefill GPU utilization, decode GPU utilization, prefill TPS, and decode TPS)in addition to two latency-based metrics (TTFT and TBT). To systematically evaluate these metrics, we conducted replay of trace collected from production workloads, enabling comparison across key performance dimensions. 4.2.1 Workload To ground the experiments in realistic conditions, we sampled workload traces from an open-domain dialogue service. As one of the most prevalent applications of LLMs, this type of workloads serve as representative benchmark for evaluating serving performance. The daily workload reveals distinct diurnal patterns (see Figure 5). User activity remains low during latenight and early-morning hours, followed by sharp increase in the morning. After midday dip, activity rises again toward secondary peak in the afternoon, then gradually declines and stabilizes. These fluctuations present significant challenges for autoscaling, particularly in detecting and reacting to rapid shifts in demand. To validate the selected metrics, we extracted an eight-hour workload segment spanning from morning to mid-afternoon. This segment includes two prominent peaks and valleys, offering balanced and challenging testbed for assessing each metrics responsiveness to real-world workload variability. 10 (a) Service (b) Service Figure 5 Daily Workload (Norm.) Figure 4 Maximum TPS with different P/D ratios for two services with distinct inputoutput length distributions and SLO constraints (a) Prefill TPS (b) Decode TPS (c) Prefill GPU Util (d) Decode GPU Util (e) Prefill SM Activity (f) Decode SM Activity (g) Time To First Token (TTFT) (h) Time Between Tokens (TBT) Figure 6 Testing environment: Autoscaling results with different metrics (a) TPS/GPU (Norm.) (b) GPU Utilization (c) SM Activity (d) Normalized Latencies Figure 7 Production environment: Performance metrics of an open-domain dialogue service with TPS-based autoscaling 4.2.2 Experiment Results The evaluation was performed under standardized conditions: all experiments began with identical numbers of prefill and decode instances, shared the same resource quotas, and applied uniform scaling thresholds calibrated to induce scaling behavior under equivalent load conditions. Scaling eventsincluding the timing and quantity of instance additions or removalswere recorded and visualized in Figure 6 to assess each metrics responsiveness and suitability. More service metrics could be found in Appendix C. TPS-based Autoscaling. Experimental results (Figures 6a and 6b) show that TPS-based autoscaling responds effectively to workload dynamics: during peak periods, the system promptly scales out resource instances to match the surge, ensuring that request demands are met without performance degradation; conversely, during valley periods, it efficiently scales in to avoid resource waste, maintaining costeffectiveness. Typically, within given service, the input-output length distribution remains relatively stable, leading to consistent behavior in both decode TPS and prefill TPS. This correlation allows the two metrics to be used interchangeably in principle. As result, both forms of TPS metrics demonstrate sufficient reliability and responsiveness to be considered viable options for autoscaling. Utilization-based Autoscaling. The effectiveness of this strategy differs significantly between the prefill and decode phases. The GPU utilization of prefill 11 instances exhibits reasonable validity as scaling signal (see Figure 6c), though it is less sensitive to workload changes compared with TPS-based metrics. In contrast, GPU utilization for decode instances is ineffective for guiding scaling decisions. Figure 6d shows that decode GPU utilization remains at high level even when the workload decreases, which aligns with our observation in Figure 2c. Similar behaviors are observed with SM activity metrics. Prefill SM activity correlates well with workload fluctuations and may be strong candidate for further investigation. Decode SM activity, however, exhibits the same limitation as decode GPU utilizationpersistently high values even during low request volumesmaking it unsuitable as reliable autoscaling indicator. Latency-based Autoscaling. We evaluated two latencyrelated metricsTTFT and TBTas potential autoscaling signals. As discussed in Section 3.3.2, these metrics rely on negative feedback strategy, which inherently leads to delayed reactions, overshooting, and frequent oscillations in resource allocation. This pattern is especially evident in our experiments with TTFT (see Figure 6g), where scale-out actions tend to overshoot the required capacity, followed by frequent corrective adjustments. TBT demonstrates relatively smoother responses and fewer fluctuations compared to TTFT, reacting reasonably to workload variations (see Figure 6h). However, tuning the associated hyperparameters poses significant challenge. The nonlinear relationship between latency and resource allocation necessitates multi-tier feedback-driven scaling mechanism. This design inevitably introduces numerous hyperparameters. For example, one must decide how wide each adjustment interval should be and how much additional capacity to provision at each step. achieving balance between meeting SLOs and maximizing throughput demands that the system operates within narrow and highly sensitive configuration range, which further complicates the parameter space. In practice, tuning so many interdependent parameters is prohibitively difficult in large-scale production environments, making this approach challenging to deploy reliably. Based on the comparative analysis, TPS-based and prefill SM activity-based autoscaling strategies exhibit the most reliable responsiveness to workload variations. To simplify large-scale deployment in our production environment, we select the TPS-based metric due to its strong alignment with business objectives and straightforward configurability. Among TPS options, decode TPS is preferred, as it can be uniformly distributed across instances. In contrast, prefill instances often differ in hardware configurations, introducing additional complexity in metric normalization. To ensure consistent scaling behavior and reduce operational overhead, decode TPS is adopted as the primary autoscaling signal in our production system."
        },
        {
            "title": "4.3 Production Deployment Analysis",
            "content": "To validate its effectiveness in real-world setting, HeteroScale has been deployed in production at ByteDance, where it now manages tens of thousands of GPUs across numerous services. On daily basis, these services collectively process trillions of prefill tokens and generate hundreds of billions of decode tokens. comparative analysis on representative day revealed substantial utilization gains. Services with HeteroScale enabled showed 26.6 percentage points increase in GPU utilization and 9.2 percentage point increase in SM activity compared to services without autoscaling. Furthermore, an analysis comparing performance on recent date with date prior to the scaled deployment showed that overall GPU utilization increased by 8.6 percentage points and SM activity rose by 6.5 percentage points. Hundreds of thousands of GPU-hours are saved each day. These all underscores the systems broad impact on cluster efficiency. The TPS-based policy is the most widely adopted, managing 64% of the total GPU fleet under HeteroScales control. detailed comparison of the two main policies showed that the dynamic, metricsdriven approach yields higher efficiency. The TPSbased policy delivered 10.0 percentage points higher GPU utilization and an 11.1 percentage points higher SM activity compared to the periodic policy. This suggests that its ability to react to real-time workload fluctuations allows it to maintain higher resource pressure more consistently than the static, time-based schedules of the periodic policy. For more granular view, we collected and analyzed performance data from the same open-domain dialogue service shown in Figure 2. To demonstrate that our approach is effective across different modalities, we also gathered data for vision-language search service, with the results detailed in Appendix A. As illustrated in Figure 7a, the number of both prefill and decode instances closely tracks the TPS variations, confirming that the essential P/D ratio is wellmaintained throughout the workload fluctuations. With HeteroScale, the overall GPU usage is reduced by 41.3%. This efficiency gain is reflected in the average prefill GPU utilization, which increased from 12 46.8% to 76.2%, and prefill SM activity, which rose from 36.6% to 62.5%. Concurrently, decode GPU utilization remained high (86.0% 82.2%) and decode SM activity increased from 53.0% to 61.6%a characteristic of the memory-bound decode stageensuring high performance while the system dynamically adjusts the number of active instances to match the load. The latency metrics, TTFT and TBT, both vary within smaller, more stable range compared to the non-autoscaled service. The apparent large fluctuations in Figure 7d are an artifact of normalization. The occasional spikes observed in TTFT are caused by temporary P/D ratio imbalances that can occur during scaling operations. We have addressed this by implementing soft P/D ratio maintenance mechanism at the service discovery level, which controls the registration of new instances to prevent such imbalances. The data from the vision-language search service, presented in Figure 9 in Appendix A, shows similar pattern of improved performance and stability, underscoring the general applicability across modalities of HeteroScale. The metrics for the autoscaled service may appear similar to the non-autoscaled service at times because the policys minimum instance count is set to relatively high threshold. This configuration is deliberate choice to maintain higher number of instances during off-peak hours, prioritizing service stability and readiness for sudden traffic surges."
        },
        {
            "title": "5.1 LLM Serving Systems",
            "content": "Many systems optimize LLM serving. Traditional systems like vLLM [32], TensorRT-LLM [37], SGLang [56] and DeepSpeed-Inference [5] focus on singleor multi-GPU inference via continuous batching, kernel fusion, and memory optimizations. More recent systems address prefill/decode disaggregation, including DistServe [57], P/D-Serve [25], SplitWise [39], TetriInfer [20], MemServe [19], and Mooncake [41]. Earlier systems such as Orca [54], FastServe [48], and AlpaServe [33] optimize batching, scheduling, or multiplexing without separating prefill and decode. Orca proposes iteration-level scheduling and selective batching; FastServe uses pre-emptive scheduling to cut latency; and AlpaServe targets statistical multiplexing to handle bursty workloads."
        },
        {
            "title": "5.2 Autoscaling Technologies",
            "content": "Cloud autoscaling includes rule-based approaches like Kubernetes HPA [29], VPA [30], and KEDA [28], and learning-based ones like AutoScale [14], DeepScaling [47], and Resource Central [10]. However, these systems are not designed for P/D disaggregated LLM services."
        },
        {
            "title": "5.3 Heterogeneous Resource Management",
            "content": "Systems like Heterogeneity-aware Scheduler [35], Tiresias [17], and Gandiva [50] optimize allocation for mixed workloads. Preemption mechanisms such as Morpheus [26], Themis [34], and AntMan [51] improve multi-tenant sharing. GPU-aware frameworks, including HexGen [23] and Mélange [16], leverage hardware heterogeneity by partitioning pipelines and solving cost-aware scheduling across GPU pools."
        },
        {
            "title": "5.4 Network-aware Scheduling",
            "content": "Network-aware scheduling has been explored through systems like Sinbad [8], Varys [9], and NetCache [24], which optimize data placement to reduce network congestion. Topology-aware systems like Firmament [15], Paragon [12], and Quincy [21] consider network topology in placement decisions, informing our approach."
        },
        {
            "title": "6 Future Work",
            "content": "While HeteroScale provides comprehensive solution for autoscaling P/D disaggregated LLM services, the field is rapidly evolving. We plan to extend this work in several key directions: Exploration of Advanced and Agnostic Metrics: To refine scaling decisions, we will investigate more granular metrics, including internal statistics from inference engines like vLLM, TensorRT-LLM, or SGLang, which can offer deeper insights into system bottlenecks. key goal is to identify and validate metrics that are model-agnostic, hardwareagnostic, and workload-agnostic. Such universal metrics would significantly simplify the configuration and deployment of autoscaling across diverse and growing landscape of services. Dynamic P/D Ratio Adaptation: Building on our current fixed-ratio mechanism, we will explore methods for making minor, dynamic adjustments to the P/D ratio in real-time. This would allow the system to automatically compensate for workload driftsubtle changes in user behavior, prompt complexity, or generation lengththereby continu13 ously optimizing the balance between prefill and decode resources to maximize efficiency. KV Cache-Aware Autoscaling: We plan to develop KV cache-aware autoscaling paradigm. By directly incorporating metrics such as cache hit rates, eviction statistics, and memory pressure into the policy engine, HeteroScale could make more intelligent scaling decisions."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper introduced HeteroScale, system that successfully addresses the primary challenges of scaling Prefill-Decode (P/D) disaggregated LLM services. By tackling hardware inefficiency, network latency, and architectural imbalance head-on, our work represents significant step forward for large-scale AI infrastructure. Key innovations, including network-aware scheduling abstractions and coordinated, metricsdriven scaling policy, have proven highly effective. The systems value is not just theoretical; its successful deployment in demanding, large-scale production environment has yielded substantial improvements in resource efficiency and major operational savings, without compromising performance. The design and proven results of HeteroScale establish new and effective benchmark for building the robust, efficient, and scalable LLM serving platforms of the future."
        },
        {
            "title": "References",
            "content": "[1] Struct GPUInfodefinition of gpu utilization. https: //docs.nvidia.com/holoscan/sdk-user-guide/ api/cpp/structholoscan_1_1GPUInfo.html, Accessed 2025-7-24. . [2] Sm activity metric in nsight compute. https:// developer.nvidia.com/docs, . See Nsight discussion of sm__cycles_active. [3] Megha et al. Agarwal. Llm inference per2023. https://www.databricks.com/blog/ formance URL llm-inference-performance-engineering-best-practices. Best practices, engineering: based ai chatbots. arXiv preprint arXiv:2406.16937, 2024. [12] Christina Delimitrou and Christos Kozyrakis. Paragon: Qos-aware scheduling for heterogeneous datacenters. Acm SIGPLAN Notices, 48(4):7788, 2013. and 8 Deploy DeepSeek-V3/R1 671b [13] dzhsurf. benchthroughput on marks. https://github.com/dzhsurf/ deepseek-v3-r1-deploy-and-benchmarks, 2025. Reports 33 t/s single-user and 620 output t/s ( 3,000 total t/s) at 100-way concurrency on one 8 H100 node. [4] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S. Gulavani, Alexey Tumanov, and Ramachandran Ramjee. Taming throughput-latency tradeoff in llm inference with sarathi-serve, 2024. URL https: //arxiv.org/abs/2403.02310. [5] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale. In SC22: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 115. IEEE, 2022. [6] Roanak Baviskar. Gpu utilization is misleading metric, 2025. URL https://www.trainy.ai/blog/ gpu-utilization-misleading. [7] Shiyang Chen, Rain Jiang, Dezhi Yu, Jinlai Xu, Mengyuan Chao, Fanlong Meng, Chenyu Jiang, Wei Xu, and Hang Liu. Kvdirect: Distributed disaggregated llm inference. arXiv preprint arXiv:2501.14743, 2024. [8] Mosharaf Chowdhury, Srikanth Kandula, and Ion Stoica. Leveraging endpoint flexibility in dataintensive clusters. ACM SIGCOMM Computer Communication Review, 43(4):231242, 2013. [9] Mosharaf Chowdhury, Yuan Zhong, and Ion Stoica. Efficient coflow scheduling with varys. In the 2014 ACM conference on Proceedings of SIGCOMM, pages 443454, 2014. [10] Eli Cortez, Anand Bonde, Alexandre Muzio, Mark Russinovich, Marcus Fontoura, and Ricardo Bianchini. Resource central: Understanding and predicting workloads for improved resource management in large cloud platforms. In Proceedings of the 26th Symposium on Operating Systems Principles, pages 153167, 2017. [11] Sumit Kumar Dam, Choong Seon Hong, Yu Qiao, and Chaoning Zhang. complete survey on llm- [14] Anshul Gandhi, Mor Harchol-Balter, Ram Raghunathan, and Michael Kozuch. Autoscale: Dynamic, robust capacity management for multi-tier data centers. ACM Transactions on Computer Systems (TOCS), 30(4):126, 2012. [15] Ionel Gog, Malte Schwarzkopf, Adam Gleave, Robert NM Watson, and Steven Hand. Firmament: Fast, centralized cluster scheduling at scale. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), pages 99115, 2016. [16] Tyler Griggs, Xiaoxuan Liu, Jiaxiang Yu, Doyoung Kim, Wei-Lin Chiang, Alvin Cheung, and Ion Stoica. Melange: Cost efficient large language model serving by exploiting gpu heterogeneity. arXiv preprint arXiv:2404.14527, 2024. [17] Juncheng Gu, Mosharaf Chowdhury, Kang Shin, Yibo Zhu, Myeongjae Jeon, Junjie Qian, Hongqiang Liu, and Chuanxiong Guo. Tiresias: {GPU} cluster manager for distributed deep learning. In 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19), pages 485 500, 2019. [18] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [19] Cunchen Hu, Heyang Huang, Junhao Hu, Jiang Xu, Xusheng Chen, Tao Xie, Chenxi Wang, Sa Wang, Yungang Bao, Ninghui Sun, and Yizhou Shan. Memserve: Flexible mem pool for building disaggregated llm serving with caching, 2024. URL https: //arxiv.org/abs/2406.17565. [20] Cunchen Hu, Heyang Huang, Liangliang Xu, Xusheng Chen, Jiang Xu, Shuang Chen, Hao Feng, Chenxi Wang, Sa Wang, Yungang Bao, et al. Inference without interference: Disaggregate llm inference for mixed downstream workloads. arXiv preprint arXiv:2401.11181, 2024. [21] Michael Isard, Vijayan Prabhakaran, Jon Currey, Udi Wieder, Kunal Talwar, and Andrew Goldberg. Quincy: fair scheduling for distributed computing clusters. In Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles, pages 261276, 2009. [22] Shashwat Jaiswal, Kunal Jain, Yogesh Simmhan, Anjaly Parayil, Ankur Mallick, Rujia Wang, Renee St. Amant, Chetan Bansal, Victor Rühle, Anoop Kulkarni, Steve Kofsky, and Saravan Rajmohan. Serving models, fast and slow:optimizing heterogeneous llm inferencing workloads at scale, 2025. URL https://arxiv.org/abs/2502.14617. [23] Youhe Jiang, Ran Yan, Xiaozhe Yao, Yang Zhou, Beidi Chen, and Binhang Yuan. Hexgen: Generative inference of large language model over heterogeneous environment. arXiv preprint arXiv:2311.11514, 2023. [24] Xin Jin, Xiaozhou Li, Haoyu Zhang, Robert Soulé, Jeongkeun Lee, Nate Foster, Changhoon Kim, and Ion Stoica. Netcache: Balancing key-value stores with fast in-network caching. In Proceedings of the 26th symposium on operating systems principles, pages 121136, 2017. [25] Yibo Jin, Tao Wang, Huimin Lin, Mingyang Song, Peiyang Li, Yipeng Ma, Yicheng Shan, Zhengfan Yuan, Cailong Li, Yajing Sun, Tiandeng Wu, Xing Chu, Ruizhi Huan, Li Ma, Xiao You, Wenting Zhou, Yunpeng Ye, Wen Liu, Xiangkun Xu, Yongsheng Zhang, Tiantian Dong, Jiawei Zhu, Zhe Wang, Xijian Ju, Jianxun Song, Haoliang Cheng, Xiaojing Li, Jiandong Ding, Hefei Guo, and Zhengyong Zhang. P/dserve: Serving disaggregated large language model at scale, 2024. URL https://arxiv.org/abs/2408. 08147. [26] Sangeetha Abdu Jyothi, Carlo Curino, Ishai Menache, Shravan Matthur Narayanamurthy, Alexey Tumanov, Jonathan Yaniv, Ruslan Mavlyutov, Inigo Goiri, Subru Krishnan, Janardhan Kulkarni, et al. Morpheus: Towards automated {SLOs} for enterprise clusters. In 12th USENIX symposium on operating systems design and implementation (OSDI 16), pages 117134, 2016. [27] Aditya Kamath, Ramya Prabhu, Jayashree Mohan, Simon Peter, Ramachandran Ramjee, and Ashish Panwar. Pod-attention: Unlocking full prefill-decode overlap for faster llm inference. In Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, pages 897912, 2025. [28] KEDA. Keda: Kubernetes event-driven autoscaling. https://keda.sh/, 2023. Accessed: 2025-07-24. [29] Kubernetes. Kubernetes horizontal pod auhttps://kubernetes.io/docs/tasks/ toscaler. run-application/horizontal-pod-autoscale/, 2023. Accessed: 2025-07-24. [30] Kubernetes. Kubernetes vertical pod autoscaler. https://github.com/kubernetes/autoscaler/ tree/master/vertical-pod-autoscaler, Accessed: 2025-07-24. 2023. [31] Kubernetes Authors. Kubernetes: Production-grade container orchestration. https://kubernetes.io/, 2023. Accessed: 2025-07-24. [32] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention, 2023. URL https://arxiv. org/abs/2309.06180. [33] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin, Yanping Huang, Zhifeng Chen, Hao Zhang, Joseph Gonzalez, et al. Alpaserve: Statistical multiplexing with In model parallelism for deep learning serving. 17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23), pages 663 679, 2023. [34] Kshiteej Mahajan, Arjun Balasubramanian, Arjun Singhvi, Shivaram Venkataraman, Aditya Akella, Amar Phanishayee, and Shuchi Chawla. Themis: Fair and efficient {GPU} cluster scheduling. In 17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20), pages 289 304, 2020. [35] Deepak Narayanan, Keshav Santhanam, Fiodar Kazhamiaka, Amar Phanishayee, and Matei Za- {Heterogeneity-Aware} cluster scheduling haria. policies for deep learning workloads. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), pages 481498, 2020. Mastering [36] NVIDIA. Inference optimization, https://developer.nvidia.com/blog/ mastering-llm-techniques-inference-optimization. llm techniques: URL 2023. [37] NVIDIA. Tensorrt-llm: deep learning compiler for large language models. https://github.com/ NVIDIA/TensorRT-LLM, 2023. Accessed: 2025-07-24. [38] OpenAI. Introducing chatgpt. https://openai. com/blog/chatgpt, 2022. [39] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed Maleki, and Ricardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting. In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA), pages 118132. IEEE, 2024. 16 [40] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed Maleki, and Ricardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting, 2024. URL https://arxiv.org/abs/2311.18677. [41] Ruoyu Qin, Zheming Li, Weiran He, Mingxing Zhang, Yongwei Wu, Weimin Zheng, and Xinran Xu. Mooncake: kvcache-centric disaggregated architecture for llm serving. arXiv preprint arXiv:2407.00079, 2024. [42] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. Deepspeedmoe: Advancing mixture-of-experts inference and training to power next-generation ai scale, 2022. URL https://arxiv.org/abs/2201.05596. [43] Jovan Stojkovic, Chaojie Zhang, Íñigo Goiri, Josep Torrellas, and Esha Choukse. Dynamollm: Designing llm inference clusters for performance and energy efficiency, 2024. URL https://arxiv.org/abs/2408. 00741. [44] Foteini Strati, Sara Mcallister, Amar Phanishayee, Jakub Tarnawski, and Ana Klimovic. Déjàvu: Kvcache streaming for fast, fault-tolerant generative llm serving, 2024. URL https://arxiv.org/abs/ 2403.01876. [45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [46] Yuxin Wang, Yuhan Chen, Zeyu Li, Xueze Kang, Yuchu Fang, Yeju Zhou, Yang Zheng, Zhenheng Tang, Xin He, Rui Guo, Xin Wang, Qiang Wang, Amelie Chi Zhou, and Xiaowen Chu. Burstgpt: real-world workload dataset to optimize llm serving systems, 2025. URL https://arxiv.org/abs/2401. 17644. [47] Ziliang Wang, Shiyi Zhu, Jianguo Li, Wei Jiang, KK Ramakrishnan, Yangfei Zheng, Meng Yan, Xiaohong Zhang, and Alex Liu. Deepscaling: microservices autoscaling for stable cpu utilization in large scale cloud systems. In Proceedings of the 13th symposium on cloud computing, pages 1630, 2022. [48] Bingyang Wu, Yinmin Zhong, Zili Zhang, Shengyu Liu, Fangyue Liu, Yuanhang Sun, Gang Huang, Xuanzhe Liu, and Xin Jin. Fast distributed inference serving for large language models. arXiv preprint arXiv:2305.05920, 2023. [49] Tian Xia, Ziming Mao, Jamison Kerney, Ethan Jackson, Zhifei Li, Jiarong Xing, Scott Shenker, and Ion Stoica. Skylb: locality-aware cross-region load balancer for llm inference. arXiv:2505.24095, 2025. arXiv preprint [50] Wencong Xiao, Romil Bhardwaj, Ramachandran Ramjee, Muthian Sivathanu, Nipun Kwatra, Zhenhua Han, Pratyush Patel, Xuan Peng, Hanyu Zhao, Quanlu Zhang, et al. Gandiva: Introspective cluster scheduling for deep learning. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), pages 595610, 2018. [51] Wencong Xiao, Shiru Ren, Yong Li, Yang Zhang, Pengyang Hou, Zhi Li, Yihui Feng, Wei Lin, and Yangqing Jia. {AntMan}: Dynamic scaling on {GPU} clusters for deep learning. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), pages 533548, 2020. [52] Haoyi Xiong, Jiang Bian, Yuchen Li, Xuhong Li, Mengnan Du, Shuaiqiang Wang, Dawei Yin, and Sumi Helal. When search engine services meet large language models: Visions and challenges, 2024. URL https://arxiv.org/abs/2407.00128. [53] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [54] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: distributed serving system for transformer-based In 16th USENIX Symposium generative models. on Operating Systems Design and Implementation (OSDI 22), pages 521538, 2022. [55] Yineng Zhang, Michael Feil, and Philip Kiely. Day zero benchmarks for qwen 3 with sglang on baseten. https://www.baseten.co/blog/ day-zero-benchmarks-for-qwen-3-with-sglang-on-baseten, 2025. Benchmarks show 45 t/s per-user and 1,400 total t/s on 4 H100 GPUs; extrapolates to low-thousands t/s on 8 H100 GPUs. [56] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. Sglang: Efficient execution of structured language model programs. Advances in neural information processing systems, 37:62557 62583, 2024. [57] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. {DistServe}: Disaggregating prefill and decoding for goodput-optimized large language model serving. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pages 193 210, 2024. 17 [58] Ruidong Zhu, Ziheng Jiang, Chao Jin, Peng Wu, Cesar Stuardo, Dongyang Wang, Xinlei Zhang, Huaping Zhou, Haoran Wei, Yang Cheng, et al. Megascaleinfer: Serving mixture-of-experts at scale with disaggregated expert parallelism. arXiv preprint arXiv:2504.02263, 2025."
        },
        {
            "title": "Appendix",
            "content": ""
        },
        {
            "title": "A Extra Analysis of Service Metrics",
            "content": "(a) Normalized TPS (a) TPS and GPU Count (Normalized) (b) Normalized latencies (b) Normalized latencies (c) GPU utilization (c) GPU utilization (d) SM activity (d) SM activity Figure 8 Performance metrics of vision-language search service with Doubao-Seed-1.6-thinking without autoscaling. Figure 9 Performance metrics of vision-language search service with Doubao-Seed-1.6-thinking with TPSbased autoscaling."
        },
        {
            "title": "B Scaling Algorithms",
            "content": "This section provides the detailed pseudo-code for the scaling and scheduling algorithms implemented in HeteroScale. Algorithm 2: Proportional Control Scaling Input: Current Instances Icurr, Observed Metric Mcurr, Target Metric per Instance Mtarget, Scaling thresholds θout, θin, Cooling periods Cout, Cin, Last scaling timestamp Tlast Output: Scaling decision (ScaleOut, ScaleIn, NoChange), final instance count If inal Iexpected Icurr Mcurr Mtarget Iexpected ; cooling CurrentT ime() Tlast; if > 1 + θout and cooling Cout then Icurr ; return (ScaleOut, Iexpected); end else if < 1 θin and cooling Cin then return (ScaleIn, Iexpected); end else return (NoChange, Icurr); end Algorithm 3: Negative Feedback Control Scaling Input: Current Instances Icurr, Current Latency Lcurr, Target Latency Ltarget, Latency thresholds αout, βout, γin, Cooling periods Cout, Cin, Last scaling timestamp Tlast Output: Scaling decision (ScaleOut, ScaleIn, NoChange), final instance count If inal if Lcurr Ltarget αout then Iexpected Icurr 1.2; end else if Lcurr Ltarget βout then Iexpected Icurr 1.1; end else if Lcurr Ltarget γin then Iexpected Icurr 0.95; end else return (NoChange, Icurr); end cooling CurrentT ime() Tlast; if cooling Cup then return (ScaleOut, Iexpected); end else if cooling Cin then return (ScaleIn, Iexpected); end else return (NoChange, Icurr); end 21 Algorithm 4: Network Affinity-aware Scheduling and Allocation Input: Requests R, Resources A, Priorities Output: Allocation list Allocations Allocations [ ]; reeV iew ConstructTreeView(A); SortedReqs SortByPriority(R, ); for each SortedReqs do GetAffinityConstraints(r); odsDelta CalcPodsDelta(r); if ype(r) = ScaleOut then SGs FilterRDMASubGroups(C); SGs SortByGroupPriority(SG); for sg SGs do odsAlloc 0; while CanAssignOnePod(sg) and odsDelta > 0 do AssignOnePod(sg); UpdateTreeView(T reeV iew); odsDelta odsDelta 1; odsAlloc odsAlloc + 1; end append(Allocations, (r, g, odsAlloc)); if odDelta = 0 then break; end end end else if ype(r) = ScaleIn then DGs GetDeploymentGroups(r); DGs SortByP riority(DGs); for each dg DGs do odsDeduct 0; while CanDeductOnePod(dg) and odDelta > 0 do DeductOnePod(dg); odsDelta odsDelta 1; odsDeduct odsDeduct + 1; end append(Allocations, (r, dg, odsDeduct)); if odDelta = 0 then break; end end end end return Allocations;"
        },
        {
            "title": "C Experiment Results of Different Autoscaling Policies",
            "content": "Figure 10 Autoscaled by Prefill TPS Figure 11 Autoscaled by Decode TPS Figure 12 Autoscaled by Prefill GPU Utilization Figure 13 Autoscaled by Decode GPU Utilization Figure 14 Autoscaled by Prefill SM Activity Figure 15 Autoscaled by Decode SM Activity Figure 16 Autoscaled by TTFT Figure 17 Autoscaled by TBT"
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "National University of Singapore"
    ]
}