{
    "paper_title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
    "authors": [
        "Jung Yi",
        "Wooseok Jang",
        "Paul Hyunbin Cho",
        "Jisu Nam",
        "Heeji Yoon",
        "Seungryong Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 1 8 0 5 0 . 2 1 5 2 : r Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression"
        },
        {
            "title": "Jisu Nam Heeji Yoon Seungryong Kim",
            "content": "KAIST AI https://cvlab-kaist.github.io/DeepForcing Figure 1. Our training-free approach, Deep Forcing, achieves comparable visual quality to training-based baselines, such as Rolling Forcing [17] and LongLive [31]. Notably, Deep Forcing enables minute-long video generation while maintaining visual quality and dynamics without requiring any additional training."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naıvely applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12 extrapolation (e.g. 5s-trained 60s+ generation) with bet1 ter imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation. 1. Introduction Recent advances in video diffusion models [10, 25, 33] have demonstrated remarkable capabilities in synthesizing short video clips (e.g, 5081 frames) with both high visual fidelity and coherent motion dynamics. Building upon this progress, emerging interactive systems, such as world models [1, 18, 34], now require autoregressive video generation (e.g., 1-2 minutes), where frames are streamed sequentially in real time for immediate downstream applications [18, 21, 34]. Unlike conventional offline video generation, which synthesizes entire clips at once, autoregressive video generation operates in an online manner, with each frame generated, emitted, and consumed instantaneously. Self Forcing [12] and its variants [8, 17, 31] have become standard in this field, leveraging causal attention mask and the Key-Value (KV) cache from previous frames. However, this existing autoregressive formulation is inherently susceptible to error accumulation over long horizons, as each predicted frame depends on previously generated and potentially imperfect frames [12, 27, 35]. Such accumulation error leads to fidelity degradation, in which visual quality deteriorates as colors drift toward oversaturation, textures blur, and fine details disappear. To mitigate this, several works [4, 22] introduce history corruption by adding noise to previous frames during training. While this improves robustness to noisy generated histories at inference, discrepancy remains between generated noise and artificially injected noise, leaving models vulnerable to long-horizon drift. Self Forcing [12] aims to reduce this gap by training on self-generated histories, but their heavy reliance on past frames KV caches still leads to accumulated errors. On the other hand, recent LLM studies [9, 16, 20], inspired by StreamingLLM [29], introduce an attention sink, in which newly generated tokens attend strongly to small set of initial global tokens, helping stabilize the attention distribution and improve overall performance. Motivated by these insights, we propose Deep Forcing, novel tuning-free method that addresses error accumulation in long-horizon video generation. Our approach is able to generate minute-long video that maintain both visual fidelity and motion stability, while requiring no fine-tuning, outperforming even training-based apFigure 2. Comparison of KV Cache Management. (a) Self Forcing [12] adopts FIFO policy that discards the earliest tokens regardless of their importance, often losing critical context and degrading generation quality. In contrast, our (b) Deep Forcing performs selective eviction by preserving Deep Sink tokens and applying KV-cache compression, effectively mitigating visual degradation during long-horizon generation. proaches [17, 31]  (Fig. 1)  . Specifically, we observe that the pre-trained Self Forcing [12] inherently exhibits strong attention sink behavior, not only attending to the first few tokens but also strongly attending to intermediate tokens. Building on this finding, we first introduce Deep Sink, which (i) maintains large deepsink ratio (typically 4060%) and (ii) dynamically adjusts the Relative Positional Embedding (RoPE) for long video generation. Second, we further present Participative Compression that retains only the most informative tokens in the KV cache by selecting them based on their importance to queries from recent frames. This removes redundancy and significantly reduces fidelity degradation caused by noise accumulation from outdated tokens. We implement our method on top of the pre-trained Self Forcing. Comprehensive evaluations using VBench [13], user studies, and VLM assessments demonstrate that our training-free approach significantly enhances the baseline without any fine-tuning. We also achieve state-of-the-art performance on several metrics, even surpassing trainingbased methods [17, 31]. Our ablation studies further validate the effectiveness of each design choice. Our contribution is summarized as follows: We propose tuning-free autoregressive video generation framework, dubbed Deep Forcing, that significantly mitigates error accumulation in long-horizon generation. We introduce Deep Sink, which stabilizes long-horizon generation by leveraging the inherent attention sink behavior of Self Forcing [12] while adjusting the relative positional gap. We present Participative Compression, lightweight KV selection mechanism that removes redundant tokens. Our training-free method achieves state-of-the-art performance on VBench and user studies, surpassing existing training-based approaches. 2 Figure 3. Overview of Deep Forcing. (a) Deep Forcing maintains substantially enlarged attention sink (Deep Sink) covering approximately half the context window, combined with Participative Compression for the remaining rolling portion. Temporal RoPE adjustment aligns the sink tokens temporal indices with current frames to maintain temporal coherence. (b) Participative Compression computes query-averaged attention scores between recent tokens and candidate tokens, selecting the top-C most important tokens to retain in the compressed cache while evicting redundant tokens. 2. Related Work Autoregressive Video Diffusion. growing line of work [4, 12, 14, 24, 35] combines diffusion modeling with autoregressive (AR) prediction to support long-horizon or streaming video generation. MAGI-1 [24] generates videos chunk-by-chunk autoregressively with progressive denoising, enabling streaming generation. CausVid [35] converts pre-trained bidirectional diffusion transformer into causal AR generator with KV caching. Building on these ideas, Self Forcing [12] addresses the traininference mismatch by conditioning the model on its own generated frames. Rolling Forcing [17] proposes expanding the diffusion window, and LongLive [31] incorporates KV recaching to maintain visual continuity while ensuring prompt adherence across scene transitions. In contrast, our method is fully tuning-free. We show that the pre-trained Self Forcing already has attention-sink behavior and demonstrate how to leverage it effectively to surpass existing training-based methods. Attention Sink. Recent work has revealed that attention in autoregressive models concentrates disproportionately on initial tokens, termed attention sinks [29]. StreamingLLM [29] showed that retaining these sink tokens within sliding window enables robust generation beyond the training context length. Building on this insight, recent autoregressive video models [17, 31] maintain the first three frames as attention sinks via model distillation or fine-tuning. We demonstrate that the pre-trained autoregressive video diffusion model [12] exhibits inherent attention sink behavior that can be effectively leveraged without training, requiring deeper context preservation. KV Cache Compression. The linearly growing KV cache in autoregressive generation motivates compression strategies that reduce memory footprint while preserving generation quality. As the cache grows, attention becomes distributed across increasingly many tokens, diluting focus on critical context and degrading output quality. To address this, recent works employ attention-based token selection for long-context LLM generation. H2O [37] and SnapKV [16] preserve important tokens based on cumulative attention scores and observation windows, respectively. D2O [26] dynamically allocates budgets across layers, while MorphKV [9] maintains constant-size caches through correlation-aware ranking. While these methods target language models, similar memory constraints arise in autoregressive video diffusion, where temporal context must be efficiently maintained across frames. We extend these principles through Participative Compression. 3. Preliminaries Autoregressive Video Diffusion. Autoregressive video diffusion models [5, 12, 24] produce each frame or chunk conditioned on previously generated frames within denoising diffusion process. Given video sequence of frames x1:N = the autoregressive model applies the (x1, x2, . . . , xN ), chain rule to factorize the joint distribution as p(x1:N ) = (cid:89) i=1 p(xi x<i). (1) diffusion model parameterizes each conditional p(xi x<i), generating the i-th frame by conditioning on previously generated frames x<i = (x1, x2, . . . , xi1). Self Forcing. Self Forcing [12] generates videos in an autoregressive manner using rolling KV cache mechanism, producing frames or frame chunks sequentially, enabling efficient long video generation. Each frame is encoded into latent tokens through the VAE encoder. The method maintains fixed-size cache of length that stores key-value pairs corresponding to the most recent frames. When the cache reaches capacity, the oldest entry is evicted to accommodate new frames, thereby maintaining sliding context window over the most recent frames. During generation, self-attention is computed between queries from the frame(s) currently being generated and the keys and values of cached context frames. Specifically, Self Forcing employs 4-step denoising process with noise schedule {t0 = 0, t1 = 250, t2 = 500, t3 = 750, t4 = 1,000} (T = 4), totaling 5 noise levels. Each frame is denoised iteratively across these timesteps. At denoising step j, the model processes noisy intermediate frame xi tj , conditioned on the KV cache of previously generated clean frames. The predicted clean frame is then perturbed with Gaussian noise at lower noise level tj1 via the forward diffusion process Ψ, producing xi tj1 for the next denoising iteration. Formally, this process is expressed as: (cid:1), = Ψ(cid:0)Gθ(xi tj , tj, KV ), tj1 xi tj1 (2) where xi t4 previously generated frames. (0, I) and KV denotes the KV cache from 4. Method 4.1. Overview We propose novel training-free method to mitigate error accumulation in long-horizon video generation. Drawing inspiration from the attention sink mechanism in large language models (LLMs) [29], our work begins by thoroughly investigating the attention mechanism within the pre-trained Self Forcing [12]. Based on this investigation, we introduce two core components: Deep Sink, which maintains approximately half of the sliding window as attention sinks, and Participative Compression, which selectively retains important tokens in the KV cache, while evicting redundant ones. Our method is illustrated in Fig. 3 4.2. Deep Sink Motivation. Self Forcing [12] employs sliding window to autoregressively extrapolate video frames. However, because the model is distilled from short video clips (e.g., 5-second segments), its frame fidelity deteriorates significantly when generating sequences that extend far beyond its training domain. This degradation is known challenge in autoregressive systems. In the LLM domain, the attention sink mechanism [29] was introduced as simple yet effective tech4 Figure 4. Attention weight distribution across earlier frames. Query-averaged attention showing how the last chunk (frames 1921) attends to earlier KV cache entries (frames 0-18). We visualize two representative attention heads from different layersL1H1 (layer 1, head 1) and L5H10 (layer 5, head 10)demonstrating that substantial attention is maintained across the entire context window, not just initial frames. See Appendix for additional heads analysis. nique to mitigate performance drift during sliding-window inference. While several works [17, 31] have investigated adapting the attention sink mechanism to redistribute attention probabilities and stabilize LLM performance, no prior work has explored how to achieve similar stabilizing effect in autoregressive video diffusion models in training-free manner. To address this gap, we first analyze the attention behavior of the pre-trained Self Forcing. As illustrated in Fig. 4, we specifically examine how newly generated latent frames attend to earlier frames in the KV cache. Contrary to the conventional understanding [29] that only small set of initial KV tokens (latent frames) needs to be retained, our analysis reveals: most attention heads allocate substantial weight to not only the earliest tokens, but also assign comparable attention to the middle of the sequence. Deepening Sink Size. Based on this observation, we hypothesize that more tokens up to the middle of the initial sequence are essential for high-quality video generation. To evaluate this hypothesis, we measured the influence of different attention sink sizes on the generation quality of long videos. To rigorously assess long-horizon generation quality, we first define our key metrics from VBench [13]: Overall Consistency and Aesthetic Quality, which use Virent sliding window into two parts: the sink (Ksink, Vsink) for deep sink tokens and the tail (Ktail, Vtail) for the rest. = (cid:2) Ksink Ktail (cid:3), = (cid:2) Vsink Vtail where (cid:2) (cid:3) denotes concatenation. (cid:3), (3) (4)"
        },
        {
            "title": "Let stail denote the first frame index of the tail and let",
            "content": "ssink be the last frame index of the deep sink. We then define sink, which is the temporal gap between stail and ssink, as follows: sink = stail ssink. (5) We apply sink to (time) sink , which is temporal component of Ksink, using the RoPE temporal frequency vector ωt: (time) sink (time) sink exp(cid:0)i ωt sink (cid:1), (6) where is the imaginary unit, denotes element-wise multiplication. This further rotates Ksink to align the relative temporal positions of sink and tail tokens. 4.3. Participative Compression Motivation. While Deep Sink effectively mitigates fidelity degradation compared to the baseline Self Forcing [12], it alone cannot fully alleviate quality degradation in minute-long video generation. When extrapolating from 5-second training clips to sequences more than 12 longer, critical issue emerges: degeneration, where visual fidelity and overall quality progressively deteriorate. This phenomenon is well-documented in autoregressive longcontext generation [9, 11]: when generating beyond training length, indiscriminate token retention causes attention to dilute across both relevant and irrelevant context, introducing compounding noise. Beyond its training distribution, the growing KV cache retains increasingly irrelevant tokens, further diluting attention. Recent analysis of video diffusion models reveals that attention concentrates on small subset of semantically critical tokens, with the majority contributing minimally to generation [32]suggesting that pruning low-attention tokens can substantially reduce computation with limited impact on quality. Building on this insight and importance-aware compression [9, 16, 37], we propose Participative Compression, which dynamically identifies and retains only contextually relevant tokens while pruning those that could contribute to attention dilution and error accumulation. Overview. Self Forcing [12] implements the rolling KV cache by evicting the earliest frame when the cache is filled. In comparison, Participative Compression (PC) operates at the token level, selectively removing redundant tokens by Figure 5. Ablation study on Deep Sink depth. We evaluate the effect of sink depth on video quality using Aesthetic Drift () and Overall Consistency () metrics on 50-second videos from the first 21 prompts in MovieGen [19]. Drift CLIP [28] and LAION aesthetic predictor [15], respectively. Following standard practice in long video generation [17, 35, 36], we compute Quality as the absolute difference in aesthetic quality between the first and the last five seconds of each 50-second generated video. Our results demonstrate clear trend  (Fig. 5)  : as the sink frame size increases, Overall Consistency improves and Aesthetic Quality Drift (Quality ) decreases. This finding suggests that intermediate frames function as crucial anchors, effectively maintaining both temporal coherence and visual fidelity throughout the long generation process. Consequently, we find that effective attention sinking in Self Forcing [12] emerges from deep, extended temporal anchoring, mechanism that differs from the shallow, initial-frame fixation used in StreamingLLM [29]. Drift Temporal RoPE Adjustment. RoPE [23] is widely used as the positional embedding in video diffusion models, and recent architectures [6, 24, 25, 33] commonly adopt 3D RoPE, which encodes temporal, height, and width dimensions separately. However, attention sinks in video require the model to attend to past frames, and directly applying 3D RoPE under this setting leads to large temporal discrepancies, where tokens at vastly different timestamps (e.g., = 1 vs. = 200) are forced to attend to each other. This breaks the continuity of video and results in (1) flickering, (2) fidelity degradation, and (3) roll-back, where previously sinked frames are regenerated (a detailed analysis is provided in the Appendix A). To address this, we propose selectively adjusting only the temporal dimensions while preserving the original spatial encoding. Specifically, we selectively modify the temporal RoPE index by applying temporal offset to the attention sinks temporal index. This reduces the temporal gap between the attention sink and the remaining tokens, while preserving the spatial indices unchanged. We divide the key and value caches and in the cur5 ranking them according to their aggregated attention scores from recent frames, rather than using simple FIFO (FirstIn, First-Out) policy as illustrated in Fig 2. PC introduces two key hyperparameters: (1) Budget (N ), the target number of tokens to retain after compression, and (2) Recent (R), the number of tokens from the most recent frames that are excluded from compression, in addition to the sink frame tokens that are always preserved. PC is applied when the sliding window reaches maximum length tokens, compressing the cache to size . The compression operates on Kcand, Vcand, which contain all tokens except those from the first and the most recent R. Recent: Krct, Vrct containing tokens from the most recent frames, excluded from compression to preserve local coherence. Candidate: Kcand, Vcand containing all intermediate tokens between the Sink and Recent, subject to compression. For each token in Kcand, Vcand, PC computes an importance score by summing its attention weights from all recent framestokens frequently attended to are deemed critical for maintaining temporal coherence. PC then selects the top = tokens with the highest importance scores to form Ktop, Vtop. The final KV cache contains tokens: sink tokens, compressed tokens, and recent tokens. Top-C Selection. PC selectively retains the most important candidate tokens based on their relevance to current generation, evicting those not selected by the Top-C operator. To determine which tokens to retain, PC computes attention scores between the recent queries (Qrct) and candidate keys (Kcand). We aggregate these scores across all recent queries by summing along the query dimension, producing unified importance score ϕj for each candidate key: ϕj = (cid:80)R r=1 r kj, (7) where indexes the candidate keys, qr denotes the r-th query in Qrct, and kj denotes the j-th key in Kcand. Higher ϕj indicates higher importance for current generation. We then form the importance vector ϕ = [ϕ1, ϕ2, . . . , ϕKcand] and select the Top-C tokens with the highest scores: Ktop = Top-C(ϕ). (8) Finally, the compressed cache is formed by concatenating the preserved components in temporal order: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: Algorithm 1 Participative Compression with Deep Sink Input: KV cache [K, ] of size ; Sink size S; Recent R; TopC capacity C; Timestep t; first time step ; 1: if MAX WINDOW LENGTH and = then // Partition cache into three regions Isink [0, S) Irct [M R, ) Icand [S, R) if Icand > 0 and > 0 then First frames Last frames Candidate tokens/frames // Compute importance scores (Eq. 7) Qrct Q[Irct] Kcand K[Icand] for = 1 to Icand do kj ϕj (cid:80)R r=1 Recent queries Candidate keys Aggregate attention // Select top-C tokens (Eq. 8) ϕ [ϕ1, ϕ2, . . . , ϕIcand] Itop TOPC(ϕ) // Temporal RoPE Unification (Section 4.3) top stop stop base top (time) (time) top exp(iωttop) Select highest else Itop // Assemble compressed cache (Eq. 9) Kcompressed [Ksink Ktop Krct] Vcompressed [Vsink Vtop Vrct] return Kcompressed, Vcompressed 23: 24: else 25: return K, No compression yields compact cache structure combining long-term initial context (Sink), selectively important intermediate tokens (Top-C), and fresh recent context (Recent), all within fixed budget of . Temporal RoPE Unification. After selecting the Top-C tokens, we apply RoPE adjustment to maintain temporal dimension consistency, following the same approach as Deep Sink (Section 4.2). We adjust only the temporal dimension of the Top-C keys RoPE while preserving their spatial information intact. Let stop denote the desired absolute temporal position where the Top-C block should be aligned, and let stop base represent the current temporal position of each cached Top-C key. We compute the temporal adjustment: Kcompressed = (cid:2) Ksink Ktop Krct (cid:3), (9) top = stop stop base. (10) where Krct contain keys from the first and most recent R, respectively. Values (Vtop) are processed identically. This This temporal shift top is then applied to (time) , which is temporal component of Ktop, re-aligning each Top-C key top Table 1. Quantitative comparison on long video generation. We evaluate Deep Forcing against open-source autoregressive video diffusion generation baselines on 30-second and 60-second videos across multiple quality metrics on VBench-Long [13]. Model Throughput (FPS) Dynamic Degree Motion Smoothness Overall Consistency Imaging Quality Aesthetic Quality Subject Consistency Background Consistency Trained with Attention Sink Rolling Forcing [17] LongLive [31] Trained without Attention Sink CausVid [35] Self Forcing [12] Deep Forcing (Ours) Trained with Attention Sink Rolling Forcing [17] LongLive [31] Trained without Attention Sink CausVid [35] Self Forcing [12] Deep Forcing (Ours) 15.79 18. 15.78 15.78 15.75 15.79 18.16 15.78 15.78 15.75 30.71 45.55 47.21 36.62 57.56 31.35 43. 46.44 31.98 57.19 30 seconds 20.99 20.16 19.15 20.50 20.54 60 seconds 20.64 20. 18.78 18.63 20.38 98.75 98.76 98.08 98.63 98.27 98.69 98.75 98.09 98.21 98.23 70.58 69. 66.36 68.58 69.31 70.25 69.11 65.84 66.33 69.27 60.24 61.51 59.77 59.44 60.68 59.75 61. 59.42 56.45 59.86 98.12 97.97 97.92 97.34 97.34 97.97 97.85 97.81 96.82 96.96 96.91 96. 96.77 96.47 96.48 96.76 96.74 96.75 96.31 96.32 using the complex phase rotation defined by the RoPE temporal frequencies ωt: (time) top (time) top exp(cid:0)i ωt top (cid:1). (11) where is the imaginary unit, and denotes element-wise multiplication. This rotation adjusts the temporal positioning of Ktop to create continuous temporal sequence across all three cache components (Sink, Top-C, Recent), preventing temporal discontinuities that would otherwise cause fidelity degradation, flickering, and roll-back artifacts as demonstrated in Appendix A. Efficiency. The complexity of Participative Compression (PC) might initially suggest significant computational overhead. However, its computational burden is minimized due to its sparse activation criteria. PC is only engaged under two specific conditions: When the sliding context window is completely filled, and during the first diffusion time step (t = ). Even though the Top-C selection mechanism involves gathering and sorting tokens, our efficiency analysis justifies this operation in the Appendix E. 5. Experiments 5.1. Experimental settings Implementation details. We implement chunk-wise Self Forcing [12] as our base model. We evaluate both quantitatively and qualitatively using Deep Sink (DS) combined with Participative Compression (PC). We set the hyperparameters for Deep Sink and Participative Compression as follows: sink size = 10, budget = 16, and recent = 4. We compare against baseline autoregressive video diffusion models including CausVid [35], Self Forcing [12], Rolling Forcing [17], and LongLive [31]. 7 Evaluation. We conduct evaluations under two settings. First, we evaluate long video generation on VBenchLong [13] using 128 prompts from MovieGen [19], following the same prompt selection protocol as Self Forcing++ [8]. Each prompt is refined using Qwen/Qwen2.57B-Instruct [30] following Self Forcing [12]. Second, we conduct user preference study to evaluate color consistency, dynamic motion, subject consistency, and overall quality. Additional implementation details are provided in the Appendix G. Third, we evaluate visual stability using the state-of-the-art vision-language model (VLM) Gemini 2.5-Pro [7], following the protocol of Self Forcing++ [8]. 5.2. Results in Long Video Generation Quantitative results. Our quantitative results are presented in Table 1. Despite being training-free method built upon Self Forcing, which was not trained for long video generation, our approach achieves performance comparable to methods explicitly distilled or trained for long videos [17, 31]. As shown in Table 1, we achieve superior performance in overall consistency and imaging quality compared to LongLive [31], and better aesthetic quality than Rolling Forcing [17]. Notably, our method also excels in dynamic degree, producing more dynamic motions than trained methods [17, 31], despite not being explicitly optimized for this aspect. We attribute this to our training-free approach, which avoids the potential motion constraints introduced when models are explicitly trained to anchor with attention sinks. Qualitative results. The qualitative results in Figure 8 demonstrate strong visual quality, with our training-free method producing high-fidelity frames comparable to or better than training-based baselines. The results visually confirm our quantitative performance, where we achieve Figure 6. Qualitative ablation results over 30-second generation: Comparison of Self Forcing (SF) [12], SF with Deep Sink (SF+DS), and SF with both Deep Sink and Participative Compression (Deep Forcing). Baseline SF exhibits severe color drift. SF+DS improves stability but shows residual artifacts. Deep Forcing maintains consistent visual quality. competitive scores without any fine-tuning. Notably, our videos exhibit more dynamic motion in both camera and subject movement, yielding more visually expressive results compared to existing approaches. Although subject consistency is lower in VBench-Long metrics, the bottom example in Figure 8 demonstrates that our training-free approach maintains better overall quality with limited degradation compared to training-based methods. Additional qualitative results are provided in Appendix F. User study. To further validate these observations, we conducted user study with 24 participants evaluating multiple aspects of the generated videos. The user study was conducted following the Two-Alternative Forced Choice (2AFC) protocol, where users are instructed to choose which is better between two videos (from Deep Forcing and baseline), in regard to color consistency, dynamic motion, subject consistency, and overall quality. As shown in Table 2, participants demonstrated clear preference for Deep Forcing over the baselines across all evaluated aspects. This includes high preference in terms of subject consistency, highlighting Deep Forcings ability to retain the subject with minimal identity drift throughout the video. These corroborate our qualitative assessment that perceptual quality remains high despite lower VBench-Long [13] subject consistency scores. VLM evaluation. For further comparison with the baselines, we evaluate visual stability using the state-of-the-art visionlanguage model (VLM) Gemini 2.5-Pro [7]. Following the protocol of Self Forcing++ [8], we use the same Table 2. User study results. Values represent percentage of votes favoring our Deep Forcing over the baselines. Method Color Cons. Dyn. Motion Subject Cons. Overall Quality CausVid Self Forcing LongLive Rolling Forcing 98.9% 85.9% 71.2% 76.7% 95.8% 86.9% 83.5% 76.7% 96.8% 84.8% 72.2% 80.0% 100% 87.9% 72.2% 78.9% Table 3. Visual stability compared with the baselines. Methods are additionally categorized by whether they are trained with an attention sink. Method Attention Sink Training Visual Stability CausVid [35] Self Forcing [12] Rolling Forcing [17] LongLive [31] Deep Forcing (Ours) No No Yes Yes No 42.84 43.94 72.6 78.58 75.44 prompt to ask the VLM to score each 30-second video in terms of exposure stability and degradation. Then we normalize the resulting scores 100. As shown in Tab. 3, our training-free method achieves visual stability comparable to that of recent methods [17, 31]. 5.3. Ablation studies We conducted ablation studies to evaluate the contributions of each method. We measure relevant VBench-Long metrics on 30 second videos. Effect of Deep Sink & Participative Compression. We evaluate three variants: naive Self-Forcing [12], Self8 Table 4. Ablation on our components: Deep Sink (DS) and Participative Compression (PC). Method Dynamic Degree Overall Consistency Image Quality SF [12] (Baseline) SF [12] + DS SF [12] + DS + PC (Ours) 36.62 48.58 57.56 20.50 20.54 20.54 68.58 68.54 69.31 Forcing with only Deep Sink with sink length = 10 frames, and Self-Forcing with both Deep Sink (S = 10 frames) and Participative Compression (N = 16, = 4). As shown in Table 6, Deep Forcing demonstrates progressive improvements in dynamic degree, overall consistency, and image quality as components are added. Notably, dynamic degree improves substantially through the Deep Forcing framework, enabling the generation of significantly more dynamic scenes compared to baseline methods. While image quality shows slight decrease at 30 seconds, we have already demonstrated Deep Sinks positive impact on 50-second videos in Section 4.2. Ablation Visualization. Figure 6 visualizes our ablation study results. When generating long videos with Self Forcing (SF) alone (top row), error accumulation leads to severe fidelity degradation and visual quality deteriorates as colors drift toward over-saturation. Adding Deep Sink (SF + DS) substantially reduces fidelity degradation and maintains more consistent colors. However, subtle artifacts persist at frame 460, including slight color shift in the coffee and texture blur in the ship details. When both Deep Sink and Participative Compression are applied (Deep Forcing), noticeable degradation is effectively eliminated. This validates that our complete framework successfully mitigates long-horizon error accumulation while preserving both overall visual quality and fine-grained details. Top-C Visualization. Figure 7 visualizes subset of TopC tokens selected during the first rolling step, spatially aligned to their positions in Frame 37 when generating Frame 82. The yellow highlighted regions indicate the spatial positions of tokens selected as Top-C within the frame. These highlighted regions reveal semantic alignment with contextually important content: the robots body and background architecture, the octopus tentacles and crab, and the circular coffee cup structure. This demonstrates that our method identifies and retains semantically salient regions critical for maintaining contextual coherence in subsequent generation. 6. Conclusion We introduced Deep Forcing, training-free approach for autoregressive long video generation that effectively mit9 Figure 7. Visualization of Top-C token selection. For each example, Frame 37 (middle) shows the Top-C tokens selected for generating Frame 82 (right). Yellow highlights indicate the spatial locations of tokens chosen as Top-C. Our method effectively identifies and preserves regions that are critical for maintaining contextual coherence during subsequent generation. igates error accumulation through two key components: Deep Sink and Participative Compression. Our method achieves state-of-the-art performance on VBench-Long, user studies, and VLM evaluation without any fine-tuning, even surpassing training-based methods. By exploiting the inherent deep attention sink behavior in pre-trained Self Forcing, we enable minute-long video generation while preserving both visual fidelity and motion dynamics. This training-free paradigm offers practical and efficient solution for long video generation with autoregressive video diffusion. Limitations and Future Works. While our training-free approach substantially improves long-horizon stability, several limitations remain. Operating at inference time on frozen backbone, our method is constrained by the pretrained models capacity and biases. Additionally, our approach lacks explicit long-term memory, potentially causing gradual drift in extremely long sequences with repeated occlusions. Future work could integrate hierarchical memory modules and extend to broader video generation settings. Figure 8. Qualitative results on 30-second videos. Frame-by-frame comparison across different methods for two representative prompts. Deep Forcing (training-free) achieves temporal consistency and visual quality comparable to training-based baselines (CausVid [35], Self Forcing [12], LongLive [31], Rolling Forcing [17]) while generating more dynamic content with greater subject consistency."
        },
        {
            "title": "References",
            "content": "[1] Rtfm: real-time frame model https://www.worldlabs.ai/blog/rtfm, 2025. [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 5 [3] Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and Shelly Sheynin. Videojam: Joint appearance-motion representations for enhanced motion generation in video models. arXiv preprint arXiv:2502.02492, 2025. 5 [4] Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2024. 2, 3 [5] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, Weiming Xiong, Wei Wang, Nuo Pang, Kang Kang, Zhiheng Xu, Yuzhe Jin, Yupeng Liang, Yubing Song, Peng Zhao, Boyuan Xu, Di Qiu, Debang Li, Zhengcong Fei, Yang Li, and Yahui Zhou. Skyreelsv2: Infinite-length film generative model, 2025. 3 [6] Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, et al. Sana-video: Efficient video generation with block linear diffusion transformer. arXiv preprint arXiv:2509.24695, 2025. 5 [7] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 7, [8] Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, and Cho-Jui Hsieh. Selfforcing++: Towards minute-scale high-quality video generation. arXiv preprint arXiv:2510.02283, 2025. 2, 7, 8 [9] Ravi Ghadia, Avinash Kumar, Gaurav Jain, Prashant J. Nair, and Poulami Das. Dialogue without limits: Constantsized kv caches for extended responses in llms. ArXiv, abs/2503.00979, 2025. 2, 3, 5 [10] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. 2 [11] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. ArXiv, abs/1904.09751, 2019. 5 [12] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, Self forcing: Bridging the trainand Eli Shechtman. test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. 2, 3, 4, 5, 7, 8, 9, 10, 6 [13] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 2, 4, 7, [14] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video arXiv preprint arXiv:2410.05954, generative modeling. 2024. 3 [15] LAION-AI. aesthetic-predictor. https://github. [Accom / LAION - AI / aesthetic - predictor,. cessed 14-11-2025]. 5 [16] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Snapkv: Llm knows what Lewis, and Deming Chen. arXiv preprint you are looking for before generation. arXiv:2404.14469, 2024. 2, 3, 5 [17] Kunhao Liu, Wenbo Hu, Jiale Xu, Ying Shan, and Shijian Lu. Rolling forcing: Autoregressive long video diffusion in real time. arXiv preprint arXiv:2509.25161, 2025. 1, 2, 3, 4, 5, 7, 8, 10, 6 [18] Jack Parker-Holder and Shlomi Fruchter. Genie 3: new frontier for world models, 2025. 2 [19] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 5, 7 [20] Dachuan Shi, Yonggan Fu, Xiangchi Yuan, Zhongzhi Yu, Haoran You, Sixu Li, Xin Dong, Jan Kautz, Pavlo Molchanov, and Yingyan Celine Lin. Lacache: Laddershaped kv caching for efficient long-context modeling of large language models. In Forty-second International Conference on Machine Learning, 2025. 2 [21] Joonghyuk Shin, Zhengqi Li, Richard Zhang, Jun-Yan Zhu, Jaesik Park, Eli Schechtman, and Xun Huang. Motionstream: Real-time video generation with interactive motion controls. arXiv preprint arXiv:2511.01266, 2025. 2 [22] Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, and Vincent Sitzmann. History-guided video diffusion. arXiv preprint arXiv:2502.06764, 2025. 2 [23] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 5 [24] Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. 3, [25] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei 11 [36] Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video generation. arXiv preprint arXiv:2504.12626, 2025. 5 [37] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36: 3466134710, 2023. 3, 5 Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 5 [26] Zhongwei Wan, Xinjian Wu, Yu Zhang, Yi Xin, Chaofan Tao, Zhihong Zhu, Xin Wang, Siqi Luo, Jing Xiong, Longyue Wang, et al. D2o: Dynamic discriminative operations for efficient long-context inference of large language models. arXiv preprint arXiv:2406.13035, 2024. [27] Jing Wang, Fengzhuo Zhang, Xiaoli Li, Vincent YF Tan, Tianyu Pang, Chao Du, Aixin Sun, and Zhuoran Yang. Error analyses of auto-regressive video diffusion models: unified framework. arXiv preprint arXiv:2503.10704, 2025. 2 [28] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. 5 [29] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. 2, 3, 4, 5 [30] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 7 [31] Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, et al. Longlive: Real-time interactive long video generation. arXiv preprint arXiv:2509.22622, 2025. 1, 2, 3, 4, 7, 8, 10, 5, 6 [32] Shuo Yang, Haocheng Xi, Yilong Zhao, Muyang Li, Jintao Zhang, Han Cai, Yujun Lin, Xiuyu Li, Chenfeng Xu, Kelly Peng, et al. Sparse videogen2: Accelerate video generation with sparse attention via semantic-aware permutation. arXiv preprint arXiv:2505.18875, 2025. [33] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 5 [34] Deheng Ye, Fangyun Zhou, Jiacheng Lv, Jianqi Ma, Jun Zhang, Junyan Lv, Junyou Li, Minwen Deng, Mingyu Yang, Qiang Fu, et al. Yan: Foundational interactive video generation. arXiv preprint arXiv:2508.08601, 2025. 2 [35] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion modIn Proceedings of the Computer Vision and Pattern els. Recognition Conference, pages 2296322974, 2025. 2, 3, 5, 7, 8, 10,"
        },
        {
            "title": "Appendix",
            "content": "In this appendix, we provide comprehensive details, including: Comparison with other sink mechanisms [17, 31] (Appendix A) Qualitative results on different sink sizes (Appendix B) Participative Compression details (Appendix C) Denoising query is not just noise (Appendix D) FPS measurements (Appendix E) More qualitative results (Appendix F) User study protocol (Appendix G) Additional attention visualization (Appendix H) A. The Tale of Three Sinks Concurrent works [17, 31] propose different attention sink mechanisms for Self Forcing-like architectures, typically with model training or distillation. In this section, we compare these approaches in training-free setting to evaluate their effectiveness when applied directly to pretrained Self Forcing model. Specifically, we compare three attention sink strategies: LongLive [31], Rolling Forcing [17], and Ours. LongLive applies attention sinks to the first 3 frames without RoPE adjustment. Rolling Forcing also uses 3 frame sinks but incorporates (1) storing raw keys and (2) dynamically reapplying RoPE when rolling occurs. Qualitative comparisons are presented in Figure 9. Figure 9 shows that our method achieves substantially better results. The LongLive attention sink, which does not adjust RoPE, exhibits progressive failure modes: fidelity degradation appears at frame 800, followed by flickering artifacts at frame 801, and culminating in roll-back behavior at frame 802 where the generation reverts to early sinked frames. These issues also occur in LongLive [31], which was explicitly trained on long videos using this attention sink mechanism  (Fig. 1)  . Although Rolling Forcing attention sink [17] employs Dynamic RoPE, which reapplies positional encodings to the entire cached key set, it still exhibits severe fidelity degradation at frames 800801."
        },
        {
            "title": "This comparison demonstrates that both deep sink and",
            "content": "RoPE adjustment are essential for long video generation. B. Qualitative Results on Different Sink Size size increases to 4 and 9 frames, degradation is progressively reduced but remains visible in fine details. Once the sink size exceeds 10 frames (Sink 14), fidelity degradation is substantially reduced. However, excessively large sinks (Sink 18) exhibit repetitive generation where early frames are over-preserved. These results validate our optimal sink range of 1015 frames (4060% of the sliding window). While Deep Sink substantially mitigates degradation, it alone proves insufficient to maintain visual fidelity throughout minute-long generation across diverse scenes, as demonstrated in our extended evaluations (Section 5.3). C. Participative Compression Details In this section, we provide additional details and analysis of Participative Compression (PC) beyond what was presented in Section 4.3. Each layer maintains its own KV cache, which undergoes compression as follows: (cid:2) Ksink Kcand Krct (cid:2) Vsink Vcand Vrct (cid:3) (cid:2) Ksink Ktop Krct (cid:3). (cid:3) (cid:2) Vsink Vtop Vrct (cid:3), (12) PC compresses only the intermediate tokens between the sink and recent. This design preserves both the initial context (Ksink, Vsink) and recent context (Krct, Vrct) without modification, while compressing only the candidate tokens (Kcand, Vcand) to retain the most important visual and contextual information (Ktop, Vtop). This compression occurs independently in each layer. Importantly, PC is applied only at the first diffusion timestep (t = 1000) when the cache reaches over its maximum window length. The tokens selected at this initial timestep remain fixed throughout subsequent denoising steps. Figure 11 validates this design choice by demonstrating that attention patterns remain consistent across timesteps. The tokens deemed important when generating frames 19 21 exhibit similar importance scores across different diffusion timesteps (t = 1000, 750, 500, 250), confirming that the Top-C selection at = 1000 captures tokens that remain contextually relevant throughout the entire denoising process. Figure 10 presents qualitative comparisons across sink sizes ranging from 0 to 18 frames, as analyzed in Section 4.2. We evaluate two diverse prompts on 60-second generation. With no attention sink (Sink 0), severe fidelity degradation emerges rapidly, where the monsters texture deteriorates and colors shift noticeably by frame 230, with complete quality collapse by frame 690. Similarly, the SUV scene exhibits significant fidelity degradation. As the sink Participative Compression Ablation. As shown in Figure 3 in the main paper, Participative Compression (PC) can leverage both current denoising query tokens and clean past query tokens from previously generated frames to select the Top-C candidates. We evaluate the effect of using each type independently versus combining them together. Table 5 compares these three strategies. When Top-C is selected using only clean past tokens (Only Past), the 1 Figure 9. Qualitative results on different Attention Sink. The result shows that Deep Sink substantially outperforms both LongLivestyle [31] and Rolling Forcing-style [17] attention sinks. Table 5. Ablation on Participative Compression. Method Motion Smoothness Overall Consistency Image Quality Only Denoising Only Past Both 97.86 97.91 98.27 20.44 20.47 20.54 68.24 68.54 69. method achieves an image quality of 68.54 and overall consistency of 20.47. When selection relies solely on currently denoising tokens (Only Denoising), the noisy nature of these queries at the initial timestep (t = 1000) leads to slightly lower image quality (68.24) and motion smoothness (97.86), likely due to unstable token selection at the initial denoising step. Combining both query types (Both) achieves the highest scores across all metrics, including motion smoothness, overall consistency, and image quality. The clean past queries appear to provide relatively stable importance estimates, while the current denoising queries help ensure the selected tokens remain relevant to the immediate generation context, suggesting complementary benefits from their combination. D. Denoising Query Is Not Just Random Noise While the denoising queries at the initial timestep (t = 1000) are inherently noisy, they may still carry meaningful signal for identifying important tokens. Figure 11 suggests this by showing consistent attention patterns across timesteps, but to more conclusively demonstrate the effectiveness of noisy queries, we directly compare Top-C selection based on denoising queries versus Gaussian random selection. For denoising query-based selection, we compute QK using only the currently denoising query tokens, then select the Top-C candidates. For Gaussian random selection, we assign each candidate token score sampled from (0, 1) and select the Top-C based on these random scores. Figure 12 illustrates the stark difference. Random selection exhibits severe scene repetition and context loss, as randomly chosen anchors fail to preserve coherent contextual information. In contrast, denoising query-based selection generates context-aware videos with notably better subject consistency and context. Figure 13 further validates this through token selection frequency heatmaps over 1-minute generation, where color intensity (white to dark purple) indicates selection frequency. The x-axis spans tokens 032,760: tokens 0 15,600 are Deep Sink, 15,60028,080 are compression candidates, and 28,080+ are recent. Gaussian random selection (top) distributes uniformly across candidates, while denoising query-based selection (bottom) concentrates heavily on specific positions, particularly immediately after the sink boundary (15,600). Notably, these high-frequency positions do not correspond to fixed frames, as tokens are evicted during compression, subsequent tokens shift into these slots. The concentration at positions near 15,600 indicates that these positional slots are consistently selected regardless of frame identity, as they bridge established context (sink) and current generation (recent), serving as semantically important anchors. This positional selectivity demonstrates meaningful contextual relationships rather than arbitrary noise. We hypothesize this effectiveness stems from: (1) Self Forcings 4-step distilled diffusion enabling rapid convergence to meaningful attention patterns despite noisy queries at = 1000, and (2) per-layer KV caching allowing independent selection of semantically important tokens based 2 Figure 10. Qualitative comparison of different sink sizes on 60-second videos. As the sink size decreases, degradation becomes more severe. Once the sink size exceeds 10 frames, degradation is substantially reduced. 3 Figure 11. Attention weight consistency across diffusion timesteps. Query-averaged attention weights showing how each key frame is attended when generating the last chunk (frames 1921) at different denoising timesteps. The consistent attention patterns across timesteps (1000, 750, 500, 250, and the final clean KV cache) demonstrate that Top-C tokens selected at the initial timestep (t = 1000) remain valid and contextually relevant throughout the entire denoising process. Figure 12. Qualitative comparison: Random Top-C vs. Denoising Query Top-C. Gaussian random selection causes severe artifacts during compression - faces abruptly rotate, heads appear floating in mid-air, and random context drift occurs, resulting in incoherent scene transitions. In contrast, denoising query-based selection maintains subject consistency with natural emergent camera movements and preserves contextual coherence throughout the generation. on layer-specific contextual relevance. E. FPS measurements We evaluate inference throughput on single NVIDIA H100 GPU when generating 60-second videos. Table 6 demonstrates that Deep Forcing maintains throughput comparable to baseline Self Forcing, achieving 15.75 FPS versus 15.78 FPS. Despite the computational overhead of compression, our method balances two competing factors: (1) compressing from 21 to 16 frames requires additional computation, but (2) generating subsequent frames with only 16 cached frames incurs lower attention costs compared to full 4 Figure 13. Token-wise Top-C selection frequency heatmap during 1-minute generation. Color intensity ranges from white (rarely selected) to dark purple (frequently selected as Top-C), indicating how often each token is reused throughout the generation. The x-axis spans tokens 032,760, where 015,600 are Deep Sink tokens, 15,60028,080 are candidates for compression, and 28,080+ are recent tokens. Gaussian random selection (top) distributes selections uniformly across candidate tokens, whereas denoising query-based selection (bottom) concentrates heavily on specific semantically important tokensparticularly those immediately after the sink boundarythat effectively bridge established and newly formed context. Table 6. Throughput Comparison on single H100 GPU. Latency is measured after first rolling. Method FPS Latency(Min/Max) Self Forcing [12] Deep Forcing (Ours) 15.78 15. 0.770 / 0.776s 0.747 / 0.797s attention over 21 frames. The latency range after first rolling in Table 6 reflects this trade-off. While Deep Forcing exhibits wider latency range (0.747s to 0.797s) compared to the baseline (0.770s to 0.776s), the average latencies are nearly identical, demonstrating that compression overhead is effectively balanced by reduced attention costs. In practice, throughput oscillates between compression phases (slightly slower) and generation phases (slightly faster) as the cache alternates between 21 and 16 frames. These fluctuations average to nearly identical performance as the baseline, demonstrating that our compression mechanism effectively amortizes its overhead, enabling long-horizon generation with minimal performance penalty. F. More Qualitative Results Additional qualitative results of our method are presented in Figure 14, and Figure 15. These examples clearly show that our training-free Deep Sink and Participative Compression framework produces results on par with trainingbased methods. G. User Study Protocol To perform human evaluation, we conducted user study based on the Two-Alternative Forced Choice (2AFC) protocol [2, 3]. For each question, participants were presented with two videos generated from the same prompt and instructed to choose which video they preferred according to four evaluation criteria: (1) Color Consistency - Which video maintains more consistent color and exposure throughout, without sudden shifts in brightness, saturation, or color tone? (2) Dynamic Motion - Which video exhibits more dynamic and varied motion, including both subject movement and camera movement? (3) Subject Consistency - Which video maintains better visual consistency of the main subject throughout its duration? (Consider comparing the beginning and end of each video.) (4) Overall Quality - Overall, which video appears more realistic, natural, and of higher quality? Each participant evaluated 16 video pairs comparing Deep Forcing against each of the four baselines (CausVid [35], Self Forcing [12], LongLive [31], and Rolling Forcing [17]). For each baseline, participants were shown 4 pairwise comparisons using different prompts, with all 16 prompts being non-overlapping within each participants session. These prompts were randomly sampled from pool of 20 total prompts. With 24 total participants, this yielded 384 total video comparisons (24 participants 16 pairs), the results of which are shown in Table 2. The presentation order of videos was randomized, and participants were not informed which model generated each video. This design ensured balanced evaluation across all baseline models while avoiding prompt repetition within individual sessions. The user interface is shown in Figure 17. H. Additional Attention Visualization We provide additional attention head visualizations  (Fig.16)  beyond those shown in Fig.4 from Section 4.2. This deep attention pattern with substantial weight on both initial and intermediate tokens, emerges consistently and pervasively across layers and heads, rather than being only one or two specific heads, supporting the hypothesis that deep sinks are fundamental to Self Forcing [12]. Figure 14. Qualitative results on 30-second videos. Frame-by-frame comparison across different methods for two representative prompts. Deep Forcing (training-free) achieves temporal consistency and visual quality comparable to training-based baselines (CausVid [35], Self Forcing [12], LongLive [31], Rolling Forcing [17]) while generating more dynamic content with greater subject consistency. 6 Figure 15. Qualitative results on 60-second videos. Frame-by-frame comparison across different methods for two representative prompts. Deep Forcing (training-free) achieves temporal consistency and visual quality comparable to training-based baselines (CausVid [35], Self Forcing [12], LongLive [31], Rolling Forcing [17]) while generating more dynamic content with greater subject consistency. 7 Figure 16. Attention weight distribution across earlier frames. Query-averaged attention showing how the last chunk (frames 19-21) attends to earlier KV cache entries (frames 0-18). Each frame consists of 1,560 tokens (spatially arranged latent patches). We visualize multiple attention heads from different layers, demonstrating that substantial attention to intermediate tokens is consistent across layers and heads. Figure 17. Example of the user interface for the user study."
        }
    ],
    "affiliations": [
        "KAIST AI"
    ]
}