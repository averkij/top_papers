{
    "paper_title": "Visual Planning: Let's Think Only with Images",
    "authors": [
        "Yi Xu",
        "Chengzu Li",
        "Han Zhou",
        "Xingchen Wan",
        "Caiqi Zhang",
        "Anna Korhonen",
        "Ivan Vulić"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 9 0 4 1 1 . 5 0 5 2 : r Visual Planning: Lets Think Only with Images Yi Xu2 Chengzu Li1 Han Zhou1 Xingchen Wan3 Caiqi Zhang1 Anna Korhonen1 Ivan Vulic1 1Language Technology Lab, University of Cambridge 2University College London 3Google y.xu.23@ucl.ac.uk, xingchenw@google.com {cl917, hz416, cz391, alk23, iv250}@cam.ac.uk"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-bystep inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in selection of representative visual navigation tasks, FROZENLAKE, MAZE, and MINIBEHAVIOR. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference. Code is available at: https://github.com/yix8/VisualPlanning."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) [6, 38, 2] have demonstrated strong capabilities in language understanding and generation, as well as growing competence in complex reasoning, enabled by their chain-of-thought reasoning abilities [50]. Building on these advances, recent work extends LLMs to support multiple modalities, yielding so-called Multimodal Large Language Models (MLLMs) [42, 22]: they incorporate visual embedded information at the input to tackle broader spectrum of tasks, such as visual spatial reasoning [33, 30] and navigation [15, 29]. However, despite their multimodal inputs, these methods perform reasoning purely in the text format during inference, from captioning visual content [19] to generating verbal rationales [59]. Building on this observation, we argue that performing multimodal reasoning only in the text pathway may not always offer the most intuitive or effective strategy, particularly for tasks that depend heavily on visual information and/or are vision-first by design. Indeed, recent results from multimodal benchmarks [43, 30, 7, 8] offer growing evidence that purely language-based reasoning falls short in certain domains, particularly those involving spatial, geometric, or physical dynamics [56]. Such reliance on grounding visual information into text before reasoning introduces modality gap that Equal contribution. Preprint. Figure 1: Comparison of reasoning paradigms. The traditional approaches (top and middle rows) generate verbose and inaccurate textual plan, while the Visual Planning paradigm (bottom row) predicts the next visual state directly, forming pure image trajectory without language mediation. hinders the models ability to capture visual features and state transitions. This highlights potential shortcoming of current MLLMs: while they process image inputs, they do not naturally think in images. For instance, tasks such as planning route through maze, designing the layout of room, or predicting the next state of mechanical system are often better served by visual representations, as verbal descriptions may struggle to accurately capture complex spatial reasoning relationships. These examples suggest broader question, which we aim to tackle in this work: can models plan in non-verbal modalities, such as images, without being mediated by text? Cognitive science also offers compelling motivation for this question [36]. Dual Coding Theory [39] proposes that human cognition operates through both verbal and nonverbal channels, each capable of independent representational and inferential processes. Recent work on MLLMs incorporates interleaved text and images as reasoning steps [21, 31]. However, they still remain fundamentally text-driven and rely on tool-based visualizations as auxiliary information for reasoning traces, with reasoning still mainly embedded in verbal traces. For instance, Visual Sketchpad [21] employs external tools to generate sketches as visual aids, and MVoT [31] generates per-step visualizations from language-based actions but still reasons in text for decision-making. As such, truly visual-only reasoning paradigm that avoids any text-based reasoning proxies remains underexplored. In this work, we propose new paradigm, Visual Planning, where reasoning is structured as sequence of images, but without the mediation of language. To the best of our knowledge, this is the first attempt to investigate whether models can achieve planning purely through visual representations. Rather than generating textual rationales and answers, our approach produces step-by-step visualizations that encode planning or inference steps directly in images. As pioneering exploration, it circumvents the modality mismatch that occurs when visual problems must be forced into explanations in verbal form, reinforces state transitions, and provides new trackable interface for tasks like navigation [30], and visual problem-solving [19]. Specifically, we explore this paradigm using the Large Vision Model (LVM) [4] trained exclusively on images and video frames with no textual data. This design choice removes potential confounders introduced by language-based supervision and enables clean investigation of whether models can reason purely within the visual modality. Motivated by the success of reinforcement learning in acquiring reasoning capabilities within the language modality [16] and its strong generalization performance [11], we propose Visual Planning via Reinforcement Learning (VPRL), novel twostage reinforcement learning framework empowered by GRPO [44] for visual planning. It involves distinct initializing stage for encouraging the exploration of the policy model in the given environment, which is then followed by reinforcement learning with progress reward function. We validate the feasibility of our paradigms on grid-based navigation as representative of spatial planning tasks, including MAZE [23], FROZENLAKE [53], and MINIBEHAVIOR [25], where one agent is requested to navigate to target location successfully without violating environment constraints. 2 Our experiments reveal that the visual planning paradigm substantially surpasses the traditional textual reasoning method by supervised fine-tuning (SFT), achieving more than 40% higher average exact-match rate. In addition to better performance, our novel method VPRL exhibits stronger generalization to out-of-distribution scenarios than the SFT method in the visual planning paradigm (VPFT). To the best of our knowledge, we are the first to apply RL to image generation in the context of planning; the main contributions comprise the following: We propose new reasoning paradigm, Visual Planning, and validate the feasibility of visual reasoning without any use of text and language for reasoning. We introduce VPRL, novel two-stage training framework that applies RL to achieve visual planning via sequential image generation. We demonstrate empirically that VPRL significantly outperforms the traditional textual reasoning paradigm and supervised baselines in visual spatial planning settings, achieving substantial gains in task performance and exhibiting improved generalization."
        },
        {
            "title": "2 Visual Planning via Reinforcement Learning",
            "content": "2.1 The Visual Planning Paradigm The majority of prior visual reasoning benchmarks [14, 1, 55] can be and is typically tackled by grounding the visual information in the textual domain [18, 40, 57], followed by few steps of textual reasoning. However, once the visual content is mapped to text (e.g., object names, attributes, or relations), the problem gets reduced to language reasoning task, where the reasoning is carried out by the language model, even without reflecting any information from the visual modality during the reasoning process. Our visual planning paradigm is fundamentally different. It performs planning purely within the visual modality. We formally define visual planning as process of generating sequence of intermediate images = (ˆv1, . . . , ˆvn), where each ˆvi represents visual state that together constitute visual planing trajectory, given the input image v0. Specifically, let πθ denote generative vision model parameterized by θ. The visual planning trajectory is generated autoregressively, where each intermediate visual state ˆvi is sampled conditioned on the initial state and previously generated states: ˆvi πθ(viv0, ˆv1, ..., ˆvi1) (1) 2.2 Reinforcement Learning for Large Vision Models Reinforcement learning (RL) has shown notable advantages in improving the generalization of autoregressive models by optimizing with sequence-level rewards beyond token-level supervision signals [11]. In autoregressive image generation, an image is represented as sequence of visual tokens. Inspired by the success of RL in language reasoning [16], we introduce an RL-based training framework for visual planning empowered by Group Relative Policy Optimization (GRPO) [44]. It leverages the transitions between visual states to compute the reward signals while verifying the constraints from the environments. To enforce the policy model that generates valid actions with diverse exploration during the RL process, we then propose novel two-stage reinforcement learning framework for visual planning. In Stage 1, we first apply supervised learning to initialize the policy model with random walks. Models visual planning is then optimized by the RL training in Stage 2. Stage 1: Policy Initialization. In this stage, we initialize the model πθ by training it on random trajectories obtained by random walks in the environment. The goal here is to generate valid sequences of visual states and retain exploration capability in simulated environment. For training, each trajectory consists of sequence of visual states (v0, . . . , vn). From each trajectory, we extract 1 image pairs of the form (vi, vi+1), where vi represents the prefix sequence (v0, . . . , vi). Subsequently, given an input prefix, the model is exposed to set of plausible next states {v(j) j=1, collected from valid trajectories that share the same prefix. To prevent overfitting to the specific transition and encourage stochasticity, we randomly sample one candidate v(ℓ) i+1 from this set at each training step as the supervision target by minimizing the following loss function of visual planning i+1}K 3 Figure 2: An overview of the proposed VPRL framework, illustrated with autoregressive large vision models for image generation in the context of visual navigation task. We train the visual policy model with GRPO, using the progress reward that encourages progressing actions and penalizes invalid actions, yielding goal-aligned visual planning. via fine-tuning (VPFT): LVPFT(θ) = (vi, v(ℓ) i+1) (cid:104) log πθ (cid:0)v(ℓ) i+ (cid:12) (cid:12) vi (cid:1)(cid:105) . (2) Overall, the first stage serves as warm-up for subsequent optimization, focusing on producing visually coherent outputs and enhancing the generation quality. Stage 2: Reinforcement Learning for Visual Planning. Building on Stage 1, where the model is initialized with random trajectories, it acquires the effective exploration capability. This property is essential for RL, as it ensures coverage over all possible transitions and prevents collapse to suboptimal behaviors. Stage 2 then leverages this ability to simulate the outcomes of potential actions by generating the next visual state and guiding the model to effectively do the planning. During this stage, the RL algorithm provides feedback and rewards based on the correctness of the simulated actions, gradually enabling the model to learn effective visual planning. Specifically, given an input i+1, . . . , ˆv(G) prefix vi, the behavior model πθold samples group of candidate responses {ˆv(1) i+1}. Each response represents hypothetical visual state corresponding to planned action a(k) at time step i. To interpret these transitions, we employ rule-based parsing function that maps pairs of visual states (vi, ˆv(k) i+1) to discrete actions. The candidate response is then scored using composite reward function r(vi, ˆv(k) i+1), which quantifies whether the generated visual state represents meaningful progress toward the goal state. The reward design is described in detail in the next paragraph. Instead of relying on learned critic to estimate value functions which may introduce additional sources of uncertainty and complexity, GRPO provides more computationally efficient and interpretable training signals by computing relative advantages through comparisons within the group. In this case, the relative advantage of each candidate is A(k) = . r(k)mean{r(1),r(2),...,r(G)} std{r(1),r(2),...,r(G)} To guide the model toward producing responses with higher advantages, we update the policy πθ by maximizing the following objective: JVPRL(θ) = viD, {ˆv(k) i+1}G k=1πθold (vi) (cid:34) 1 (cid:88) i=1 (cid:16) min ρ(k)A(k), clip (cid:16) ρ(k), 1 ϵ, 1 + ϵ (cid:17) A(k)(cid:17) (cid:35) β DKL (πθ πref) , (3) where is the prefix distribution and ρ(k) = is the importance sampling ratio. πθ(ˆv(k) πθold (ˆv(k) i+1vi) i+1vi) 4 Reward Design. Unlike discrete actions or text tokens, visual outputs are sparse, high-dimensional, and not easily decomposable into interpretable units. In our visual planning framework, the challenge is even more specific: whether the generated visual state can correctly reflect the intended planning action. Therefore, the reward design focus on the progress toward the goal while validating the actions with constraints. To interpret the intended action that connects the current state vi to generated candidate state ˆv(k) i+t, we define state-action parsing function : E, where denotes the set of valid actions, and is the set of invalid transitions, such as violation of physical constraints of the environment. Formally, P(vi, ˆv(k) i+1) = (cid:40) a(k) , e(k) , if a(k) if e(k) A, E. (4) It helps to interpret model behaviors from pixel data to intended action through either standalone segmentation components [41] or rule-based scripts. Once having the intended actions, to systematically evaluate action effectiveness, we introduce the progress map D(v) that estimates the remaining steps or effort required to reach the goal from each visual state. By comparing the agents current and resulting state against the progress map, we partition into three disjoint subsets: Aopt = (cid:8)a : D(ˆv(k) i+1) < D(vi)(cid:9), Anopt = (cid:8)a : D(ˆv(k) i+1) D(vi)(cid:9), Einv = E. We then propose the progress reward function r(vi, ˆv(k) (cid:3) + αnopt I(cid:2)P(vi, ˆv(k) i+1) Aopt i+1) Anopt i+1) as: αopt I(cid:2)P(vi, ˆv(k) (cid:124) (cid:123)(cid:122) optimal (cid:125) (cid:124) (cid:123)(cid:122) non-optimal + αinv I(cid:2)P(vi, ˆv(k) i+1) Einv (cid:3) (cid:125) (cid:124) (cid:123)(cid:122) invalid (5) (cid:3) , (cid:125) where αopt, αnopt, αinv are reward coefficients. In our experiments, we set αopt = 1, αnopt = 0, and αinv = 5, thereby rewarding progressing actions, assigning zero to non-progressing actions, and heavily penalizing invalid transitions. 2.3 System Variants In addition to VPRL, we include several training system variants as baselines that differ in supervision modalities (language vs. image) and optimization methods (SFT vs. RL), allowing us to compare language-based and vision-based planning while assessing the role of reinforcement learning. Visual Planning via Fine-Tuning (VPFT). We propose Visual Planning via Fine-Tuning (VPFT) as simplified variant of our framework, which shares the same training architecture as Stage 1 in Section 2.2, but replaces random trajectories with optimal planning trajectories. For each environment, we sample distinct trajectory (vopt ) representing the minimal-step path from the initial state vopt i+1 given the prefix vopt 0 = v0 to the goal. At each step, the model is trained to predict the next state vopt i. The objective is identical to Equation 2, with supervision from the optimal trajectory. 1 , . . . , vopt 0 , vopt Supervised Fine-Tuning (SFT) in Text. In this baseline, planning is formulated in the language modality. Instead of generating an intermediate visual consequence of an action, the model produces textual description of the intended action sequence. Formally, given an visual input state and textual prompt p, which represents the task description, the model is trained to generate verbalized action sequence = (t1, . . . , tL), where each token ti Vtext represents an action. The input to the model is the concatenation of the prompt tokens and the visual tokens, and the target is the corresponding action sequence. Following prior work on supervised fine-tuning (SFT) [49] in autoregressive models, we minimize the cross-entropy loss for action prediction: LSFT(θ) = E(v,t) (cid:34) (cid:88) i=1 log πθ(ti t<i, v, p) . (6) (cid:35)"
        },
        {
            "title": "3 Experiments and Results",
            "content": "Tasks To evaluate our proposed visual planning paradigm, we select representative tasks where planning can be expressed and executed entirely in the visual modality. We focus on tasks where 5 Table 1: Performance of the closedand open-source models on FROZENLAKE, MAZE, and MINIBEHAVIOR. VPRL performs consistently the best (bold) across all tasks. denotes the posttrained model. represents texts and (cid:213) represents images. The last column AVG. reports the average performance across three tasks. Model Input Output FROZENLAKE MAZE MINIBEHAVIOR AVG. EM (%) PR (%) EM (%) PR (%) EM (%) PR (%) EM (%) PR (%) Gemini 2.0 Flash - Direct - CoT Gemini 2.5 Pro (think) + (cid:213) + (cid:213) + (cid:213) Qwen 2.5-VL-Instruct-3B - Direct - CoT - SFT LVM-3B - VPFT (ours) - VPRL (ours) + (cid:213) + (cid:213) + (cid:213) (cid:213) (cid:213) (cid:213) (cid:213) Closed-Source Model 21.2 27.6 72.0 0.9 1.3 59.0 75.4 91.6 47.6 52.5 85.0 8.3 6.9 21. Open-Source Model 14.4 13.4 76.3 79.5 93.2 0.5 0.8 33.3 59.0 74.5 31.4 29.8 35. 13.6 8.2 52.7 64.0 77.6 0.7 4.0 37.6 0.0 1.2 10.6 33.8 75.8 29.8 31.2 59. 10.0 12.5 31.0 52.2 83.8 10.1 12.8 43.7 0.5 1.1 34.3 56.1 80.6 36.3 37.8 60. 12.7 11.4 53.3 65.2 84.9 state transitions are visually observable, distinguishing them from language-centric tasks like code generation [27] or traditional visual question answering. This design allows us to analyze planning behavior without relying on textual rationales or symbolic outputs. To compare visual planning with language-based reasoning, we experiment with 3 visual navigation environments: FROZENLAKE [53], MAZE [23], and MINIBEHAVIOR [25]. All of them can be solved in both modalities, which enables direct comparison between visual planning and language reasoning strategies. FROZENLAKE: It is initially proposed by Wu et al. [53] and implemented with Gym [5]. It simulates grid-based frozen lake, where the agent is supposed to start from the designated position and find its way to the destination safely without falling into the holes. MAZE: Given an initial image describing the maze layout, the model is supposed to go through the maze from the starting point (green point) to the destination (red flag). MINIBEHAVIOR: The agent is first required to reach the printer from the starting point and pick it up. After that, the agent should go to the table and drop the printer. This task consists of 2 additional actions, including pick and drop. We construct synthetic datasets for the tasks with varying levels of complexity in patterns and environments. Details on data collection and implementation are provided in Appendix B.1. Models To explore visual planning without any language influence as confounders and enables clean investigation, we select models trained exclusively on visual data without any exposure to textual data during pretraining. For our methods (VPFT and VPRL), we use the Large Vision Model (LVM-3B) [4] as the backbone, which is only trained on image sequences and videos. For RL training, we design and provide the detailed implementation of rule-based state-action parsing function and progress map D(v) in Appendix B.3. We also include textual planning baselines for parallel comparison, where planning is formulated through language, typically as textual sequence of actions. Specifically, we evaluate Qwen 2.5VL-Instruct [3], matched in size to LVM-3B, on both inference-only (Direct2 and CoT) and posttraining settings (SFT) as baselines. We further evaluate closed-source models, including Gemini 2.0 Flash [26] (gemini-2.0-flash-002) and advanced thinking model Gemini 2.5 Pro [13] (gemini-2.5-pro-preview-03-25) as reference from state-of-the-art multimodal reasoning. Full training details and hyperparameters for all models are provided in Appendix B.4. Evaluation Metrics We adopt two complementary evaluation metrics for the selected tasks: 2Direct denotes answer prediction without being instructed to conduct intermediate reasoning. 6 Figure 3: Illustration of each task with generated visual planning traces from LVM, covering different types of actions (optimal, non-optimal and invalid). More cases can be found in Appendix C.5. Exact Match (EM) is defined as EMi = (cid:81)n I(ˆvj = vj). This metric measures whether the j=1 model successfully generates the complete and correct planning trajectory that aligns with the shortest optimal valid path. One step of deviation from the optimal solution is considered incorrect. Progress Rate (PR) is defined as PRi = 1 . PR measures the ratio of j=1 the number of consecutively correct steps (valid forward moves) from the start to the number of steps in the optimal path. This provides softer signal than Exact Match, capturing the models ability to make meaningful progress towards full solution. k=1 (cid:80)n (cid:104)(cid:81)j (cid:105) I(ˆvk = vk) Visual Planning Surpasses Textual Planning. Table 1 shows that visual planners (VPFT and VPRL) achieve the highest scores on all tasks, outperforming all language-reasoning baselines. With identical supervised training method via fine-tuning, VPFT exceeds language-based SFT by an average of over 22% in Exact Match (EM), with VPRL further widening the gap. similar trend is observed in Progress Rate (PR) as well. This highlights the advantages of the visual planning paradigm in visual-centric tasks, where language-driven approaches may be less aligned with task structure. Inference-only models, whether large closed-source systems or smaller open-source MLLMs, struggle with these planning tasks without task-specific tuning. Even the advanced thinking model Gemini 2.5 Pro achieves EM and PR almost below 50% on the more complex MAZE and MINIBEHAVIOR tasks, underscoring the challenges these tasks pose for current models despite being intuitive for humans. Gains from Reinforcement Learning. The two-stage reinforcement learning approach (VPRL) yields the highest overall performance, surpassing all system variants. After Stage 2, the model achieves near-perfect planning on the simpler FROZENLAKE task (91.6% EM, 93.2% PR) and maintains strong performance on MAZE and MINIBEHAVIOR tasks. This marks substantial improvement of more than 20% across all tasks over the supervised baseline VPFT. As expected, Stage 1 of our RL training, which enforces output format without teaching planning behavior, yields near-random performance (e.g., 11% EM on FROZENLAKE, see Table 8 in Appendix C.5). After the full Stage 2 optimization with our reward scheme, the planner achieve its best performance. This gain highlights key advantage of RL over SFT. VPRL allows the model to freely explore diverse actions and learn from their outcome, while VPFT relies on imitation and tends to fit the training distribution. By encouraging exploitation through reward-driven updates, VPRL learns to capture underlying rules and patterns, leading to stronger planning performance. Robustness with Scaling Complexity. The advantage of RL also holds when we study the performance of different methods with respect to task difficulties, where larger grid usually relates to higher difficulties. In Figure 5, as the grid size increases from 3 3 to 6 6 in the FROZENLAKE environment, Gemini 2.5 Pros EM score drops sharply from 98.0% to 38.8%. In comparison, our visual planners not only maintain higher accuracy at all grid sizes but also exhibit much flatter per7 Figure 4: Visualization of test example from FROZENLAKE comparing visual planning variants (VPFT and VPRL) with language-based reasoning variants. formance curve. Similarly, VPRL demonstrates even greater stability than VPFT, with EM remaining at 97.6% on 3 3 grids and still achieving 82.4% on 6 6, indicating strong robustness. We observe similar trends in other tasks; see Appendix C.2 for other tasks."
        },
        {
            "title": "4 Discussions and Analysis",
            "content": "Error Analysis and Case Study. Figure 3 presents visual planning traces generated by LVM across different tasks. As defined in Section 2.2, the model occasionally takes non-optimal actions that deviate from the shortest path, as seen in the FROZENLAKE example. Invalid actions include violations of physical constraints (e.g., walking through walls in MAZE or entering the table in MINIBEHAVIOR), or executing multiple actions in single step (see Appendix C.5 for examples). Figure 4 compares visual planning with language-based reasoning systems. In FROZENLAKE, Gemini 2.5 Pro misinterprets the environment size at the first step, causing cascading errors that lead to concluding an incorrect final answer. Similarly, the language-based SFT baseline makes an invalid action at the third step, reflecting difficulty in tracking states during reasoning. In contrast, visual planning avoids such failures by reasoning directly in the visual modality while reflecting the visual states per action. VPRL demonstrates the ability to take detours to bypass obstacles while still progressing toward the goal, whereas VPFT, lacking this flexibility, gets stuck and fails to reach the destination. More examples are provided in Appendix C.5. Random Policy Initialization Enables Exploration. natural follow-up question arises: could we directly use VPFT as the policy model for GRPO training rather than intentionally initialize model with random trajectories? We hypothesize that VPFT, trained via teacher-forcing, inherently limits exploration by repeatedly generating similar actions, resulting in identical rewards. In this case, it yields zero advantage, preventing policy updates and hindering effective learning. We empirically validate this hypothesis by comparing the exploration capabilities of VPFT with VPRL Stage 1 (Figure 6). We observe that VPFTs entropy rapidly declines throughout training, eventually approaching zero, indicating severe exploration limitations. Although earlier VPFT checkpoints exhibit higher entropy, they produce significantly more invalid actions. In contrast, VPRL Stage 1 demonstrates significantly higher entropy, closely approaching the entropy of the uniform random planner, while maintaining much lower invalid action ratio. These results justify the necessity of random initialization in our reinforcement learning framework to ensure robust exploration. VPRL Reduces Invalid Action Failure. Another important benefit of VPRL lies in its effectiveness in reducing invalid actions. To quantify this, we analyze all failed trajectories and compute the proportion that contains at least one invalid action, as opposed to failures caused by non-optimal but valid plans. We refer to this as the invalid-failure ratio. As shown in Table 5, VPFT exhibits high ratio ranging from 61% to 78% over three tasks, while VPRL reduces this ratio by at least 24% in all 8 Figure 5: Evaluation of model performance on FROZENLAKE under varying levels of difficulty. As the environment complexity increases with larger grid sizes, language-based reasoning methods experience sharp decline in performance, whereas visual planning methods exhibit more gradual drop, demonstrating greater robustness. Figure 6: Comparison of exploration capabilities between VPFT and VPRL Stage 1 on FROZENLAKE. VPRL Stage 1 achieves significantly better exploration efficiency, balancing high entropy with low invalid action ratio, whereas VPFT struggles with diminishing entropy and increased invalid actions over training. cases, demonstrating that VPRL not only improves success rates, but also encourage the model to stay within valid action spaces during planning."
        },
        {
            "title": "5 Related Work",
            "content": "MLLM Reasoning Recent work has extended CoT prompting [51] to MLLMs through approaches such as grounding visual inputs into symbolic representations, such as graphs or bounding boxes [59, 28]. Other approaches integrate tools to generate visualizations during reasoning [21, 61]. For example, o3 model [37] incorporates visual rationales using tools such as cropping and zooming. MVoT [31] is also essentially form of tool use: instead of relying on external modules, it invokes itself to generate visualizations of textual reasoning. These methods primarily conduct reasoning in language, with visual components merely illustrating the textual rationale rather than serving as the medium of reasoning. In this work, we take step further to explore whether multi-step planning can emerge purely within visual representations, enabling reasoning without relying on language at all. Reinforcement Learning for Visual Reasoning Reinforcement learning has been applied across wide range of vision-related tasks, especially given the rise of GRPO as in DeepSeek-R1 [16]. Concurrently, in object detection, visual perception [54] is optimized though rewarding high Intersectionover-Union (IoU) scores between predicted and ground-truth bounding boxes [45]. For visual reasoning tasks such as Visual Question Answering (VQA), GRPO has been utilized to optimize the models for longer, more coherent, and logically grounded reasoning traces in textual responses [34, 60, 58, 46]. More recently, similar methods have also been applied to image generation tasks, where the model is guided to reflect on the generated images and refine them recursively based on the alignment with given textual instructions [17, 48, 24]. These approaches focus on pixel-level fidelity and semantic alignment with text, whereas our work leverages RL for goal-oriented visual planning, optimizing multi-step decision-making through visual state transitions without any reliance on language. While prior RL-based approaches ground reasoning traces in textual outputs despite multimodal inputs, the modality mismatch limits the effectiveness of RL in bridging perception and action. For the tasks that are vision-first by design, our visual planning paradigm and two-stage training framework VPRL enable more natural and flexible policy exploration by operating entirely in the visual domain, outperforming all language-based training variants."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we present Visual Planning as new paradigm for reasoning in visually oriented tasks, challenging the prevailing reliance on language as the primary medium for structured inference. 9 By enabling models to operate entirely through visual state transitions without textual mediation, we demonstrate that purely visual representations can lead to more effective and intuitive planning, particularly in spatially grounded and dynamic tasks. Our proposed two-stage reinforcement learning framework, VPRL, empowered by GRPO, further enhances the planning capabilities of large vision models. It obtains significant gains across three visual navigation tasks, achieving over 40% improvements in task performance than language-based planning and showing stronger generalization on out-of-distribution scenarios. These findings underscore the promise of visual planning as powerful alternative to text-based approaches. We believe our work opens up rich new direction for multimodal research, offering foundation for building more intuitive, flexible, and powerful reasoning systems across wide range of domains."
        },
        {
            "title": "References",
            "content": "[1] Akula, A., Changpinyo, S., Gong, B., Sharma, P., Zhu, S.-C., and Soricut, R. CrossVQA: Scalably generating benchmarks for systematically testing VQA generalization. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 21482166, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. emnlp-main.164. URL https://aclanthology.org/2021.emnlp-main.164/. [2] Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. URL https://doi.org/10.48550/arXiv.2305.10403. [3] Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Bai, Y., Geng, X., Mangalam, K., Bar, A., Yuille, A. L., Darrell, T., Malik, J., and Efros, A. A. Sequential modeling enables scalable learning for large vision models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 2286122872, 2024. doi: 10.1109/CVPR52733.2024.02157. URL https://doi.org/10.1109/CVPR52733.2024.02157. [5] Brockman, G. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [6] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips. cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. [7] Chen, Q., Qin, L., Zhang, J., Chen, Z., Xu, X., and Che, W. M3CoT: novel benchmark for multi-domain multi-step multi-modal chain-of-thought. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 81998221, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.446. URL https://aclanthology.org/2024.acl-long.446/. [8] Cheng, Z., Chen, Q., Zhang, J., Fei, H., Feng, X., Che, W., Li, M., and Qin, L. Comt: novel benchmark for chain of multi-modal thought on large vision-language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2367823686, 2025. [9] Chern, E., Su, J., Ma, Y., and Liu, P. Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. arXiv preprint arXiv:2407.06135, 2024. [10] Choudhury, R., Zhu, G., Liu, S., Niinuma, K., Kitani, K., and Jeni, L. Dont look twice: Faster video transformers with run-length tokenization. Advances in Neural Information Processing Systems, 37:2812728149, 2024. 10 [11] Chu, T., Zhai, Y., Yang, J., Tong, S., Xie, S., Levine, S., and Ma, Y. SFT memorizes, RL generalizes: comparative study of foundation model post-training. In The Second Conference on Parsimony and Learning (Recent Spotlight Track), 2025. URL https://openreview. net/forum?id=d3E3LWmTar. [12] Esser, P., Rombach, R., and Ommer, B. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. [13] Gemini. Gemini 2.5: 2025. gemini-model-thinking-updates-march-2025/. Accessed: 2025-05-09. URL Our most March https://blog.google/technology/google-deepmind/ intelligent AI model. [14] Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the in VQA matter: Elevating the role of image understanding in Visual Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. [15] Gu, J., Stefani, E., Wu, Q., Thomason, J., and Wang, X. Vision-and-language navigation: survey of tasks, methods, and future directions. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 76067623, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.524. URL https://aclanthology.org/2022.acl-long.524/. [16] Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [17] Guo, Z., Zhang, R., Tong, C., Zhao, Z., Gao, P., Li, H., and Heng, P.-A. Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv preprint arXiv:2501.13926, 2025. [18] Gurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman, K., Luo, J., and Bigham, J. P. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 36083617, 2018. [19] Hao, Y., Gu, J., Wang, H. W., Li, L., Yang, Z., Wang, L., and Cheng, Y. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025. [20] Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. [21] Hu, Y., Shi, W., Fu, X., Roth, D., Ostendorf, M., Zettlemoyer, L., Smith, N. A., and Krishna, R. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024. [22] Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [23] Ivanitskiy, M. I., Shah, R., Spies, A. F., Räuker, T., Valentine, D., Rager, C., Quirke, L., Mathwin, C., Corlouer, G., Behn, C. D., et al. configurable library for generating and manipulating maze datasets. arXiv preprint arXiv:2309.10498, 2023. [24] Jiang, D., Guo, Z., Zhang, R., Zong, Z., Li, H., Zhuo, L., Yan, S., Heng, P.-A., and Li, H. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot. arXiv preprint arXiv:2505.00703, 2025. [25] Jin, E., Hu, J., Huang, Z., Zhang, R., Wu, J., Fei-Fei, L., and Martín-Martín, R. MiniBEHAVIOR: procedurally generated benchmark for long-horizon decision-making in In NeurIPS 2023 Workshop on Generalization in Planning, 2023. URL embodied AI. https://openreview.net/forum?id=Ghl9pYaVh5. [26] Kampf, K. and Brichtova, N. Experiment with gemini 2.0 flash native image generation, March 2025. URL https://developers.googleblog.com/en/ experiment-with-gemini-20-flash-native-image-generation/. Accessed: 202504-27. [27] Lai, Y., Li, C., Wang, Y., Zhang, T., Zhong, R., Zettlemoyer, L., Yih, W.-t., Fried, D., Wang, S., and Yu, T. Ds-1000: natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pp. 1831918345. PMLR, 2023. [28] Lei, X., Yang, Z., Chen, X., Li, P., and Liu, Y. Scaffolding coordinates to promote visionlanguage coordination in large multi-modal models. arXiv preprint arXiv:2402.12058, 2024. [29] Li, C., Zhang, C., Teufel, S., Doddipatla, R. S., and Stoyanchev, S. Semantic map-based generation of navigation instructions. In Calzolari, N., Kan, M.-Y., Hoste, V., Lenci, A., Sakti, S., and Xue, N. (eds.), Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pp. 1462814640, Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology. org/2024.lrec-main.1274/. [30] Li, C., Zhang, C., Zhou, H., Collier, N., Korhonen, A., and Vulic, I. TopViewRS: Vision-language models as top-view spatial reasoners. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 17861807, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.106. URL https://aclanthology.org/2024.emnlp-main.106/. [31] Li, C., Wu, W., Zhang, H., Xia, Y., Mao, S., Dong, L., Vulic, I., and Wei, F. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv preprint arXiv:2501.07542, 2025. [32] Linsley*, D., Kim*, J., Ashok, A., and Serre, T. Recurrent neural circuits for contour detection. In International Conference on Learning Representations, 2020. URL https://openreview. net/forum?id=H1gB4RVKvB. [33] Liu, F., Emerson, G., and Collier, N. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635651, 2023. doi: 10.1162/tacl_a_00566. URL https: //aclanthology.org/2023.tacl-1.37/. [34] Liu, Z., Sun, Z., Zang, Y., Dong, X., Cao, Y., Duan, H., Lin, D., and Wang, J. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [35] Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7. [36] Moulton, S. T. and Kosslyn, S. M. Imagining predictions: mental imagery as mental emulation. Philosophical Transactions of the Royal Society B: Biological Sciences, 364:1273 1280, 2009. [37] OpenAI. Introducing OpenAI o3 and o4-mini: Our smartest and most capable models to date. April 2025. URL https://openai.com/index/introducing-o3-and-o4-mini/. Accessed: 2025-05-16. [38] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [39] Paivio, A. Dual coding theory: Retrospect and current status. Canadian Journal of Psychology/Revue canadienne de psychologie, 45(3):255, 1991. [40] Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Ye, Q., and Wei, F. Grounding multimodal large language models to the world. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=lLmqxkfSIw. 12 [41] Ravi, N., Gabeur, V., Hu, Y.-T., Hu, R., Ryali, C., Ma, T., Khedr, H., Rädle, R., Rolland, C., Gustafson, L., et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [42] Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lillicrap, T. P., Alayrac, J., Soricut, R., Lazaridou, A., Firat, O., Schrittwieser, J., Antonoglou, I., Anil, R., Borgeaud, S., Dai, A. M., Millican, K., Dyer, E., Glaese, M., Sottiaux, T., Lee, B., Viola, F., Reynolds, M., Xu, Y., Molloy, J., Chen, J., Isard, M., Barham, P., Hennigan, T., McIlroy, R., Johnson, M., Schalkwyk, J., Collins, E., Rutherford, E., Moreira, E., Ayoub, K., Goel, M., Meyer, C., Thornton, G., Yang, Z., Michalewski, H., Abbas, Z., Schucher, N., Anand, A., Ives, R., Keeling, J., Lenc, K., Haykal, S., Shakeri, S., Shyam, P., Chowdhery, A., Ring, R., Spencer, S., Sezener, E., and et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. CoRR, abs/2403.05530, 2024. doi: 10.48550/ARXIV.2403.05530. URL https://doi.org/10.48550/arXiv.2403.05530. [43] Roberts, J., Taesiri, M. R., Sharma, A., Gupta, A., Roberts, S., Croitoru, I., Bogolin, S.-V., Tang, J., Langer, F., Raina, V., et al. Zerobench: An impossible visual benchmark for contemporary large multimodal models. arXiv preprint arXiv:2502.09696, 2025. [44] Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv preprint, abs/2402.03300, 2024. URL https://arxiv.org/abs/2402.03300. [45] Shen, H., Liu, P., Li, J., Fang, C., Ma, Y., Liao, J., Shen, Q., Zhang, Z., Zhao, K., Zhang, Q., et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [46] Team, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [47] von Werra, L., Belkada, Y., Tunstall, L., Beeching, E., Thrush, T., Lambert, N., Huang, S., Rasul, K., and Gallouédec, Q. Trl: Transformer reinforcement learning. https://github. com/huggingface/trl, 2020. [48] Wang, J., Tian, Z., Wang, X., Zhang, X., Huang, W., Wu, Z., and Jiang, Y.-G. Simplear: Pushing the frontier of autoregressive visual generation through pretraining, sft, and rl. arXiv preprint arXiv:2504.11455, 2025. [49] Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=gEZrGCozdqR. [50] Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=_VjQlMeSB_J. [51] Wei, J., Wang, X., Schuurmans, D., Bosma, M., ichter, b., Xia, F., Chi, E., Le, Q. V., and Zhou, D. Chain-of-thought prompting elicits reasoning in large language models. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2482424837. Curran Associates, Inc., 2022. [52] Wu, J., Jiang, Y., Ma, C., Liu, Y., Zhao, H., Yuan, Z., Bai, S., and Bai, X. Liquid: Language models are scalable multi-modal generators. arXiv preprint arXiv:2412.04332, 2024. [53] Wu, Q., Zhao, H., Saxon, M., Bui, T., Wang, W. Y., Zhang, Y., and Chang, S. Vsp: Assessing the dual challenges of perception and reasoning in spatial planning tasks for vlms, 2024. [54] Yu, E., Lin, K., Zhao, L., Yin, J., Wei, Y., Peng, Y., Wei, H., Sun, J., Han, C., Ge, Z., et al. Perception-r1: Pioneering perception policy with reinforcement learning. arXiv preprint arXiv:2504.07954, 2025. 13 [55] Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. [56] Zhang, H., Li, C., Wu, W., Mao, S., Vulic, I., Zhang, Z., Wang, L., Tan, T., Wei, F., et al. call for new recipes to enhance spatial reasoning in mllms. arXiv preprint arXiv:2504.15037, 2025. [57] Zhang, J., Huang, J., Jin, S., and Lu, S. Vision-language models for vision tasks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [58] Zhang, J., Huang, J., Yao, H., Liu, S., Zhang, X., Lu, S., and Tao, D. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. [59] Zhang, Z., Zhang, A., Li, M., hai zhao, Karypis, G., and Smola, A. Multimodal chain-ofthought reasoning in language models. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. [60] Zhou, H., Li, X., Wang, R., Cheng, M., Zhou, T., and Hsieh, C.-J. R1-zeros\" aha moment\" in visual reasoning on 2b non-sft model. arXiv preprint arXiv:2503.05132, 2025. [61] Zhou, Q., Zhou, R., Hu, Z., Lu, P., Gao, S., and Zhang, Y. Image-of-thought prompting for visual reasoning refinement in multimodal large language models, 2024."
        },
        {
            "title": "A Limitations and future work",
            "content": "In this work, we focus exclusively on Large Vision Model (LVM) to investigate visual planning capabilities by eliminating language as confounding factor for research purposes. As such, this choice constraints the model size to 3B as the only available size of LVM, and excludes recently released native multimodal models capable of generating multimodal outputs [9, 52]. However, we argue that the visual planning paradigm can be extended to broader multimodal generation models for use in more diverse tasks, combined with more modalities as long as they support image generation. Additionally, explicitly generating images introduces computational overhead during inference compared to textual response. However, we argue that language-based reasoning, especially for thinking models [13], can be equally or more time-consuming. In our demonstration, Gemini generated over 7,000 thinking tokens yet failed to provide the correct answer in the end. The computation overhead introduced by image generation can be alleviated through more compact image representations using fewer tokens [10], which we advocate for future research. Another limitation in this work lies in the implementation of state-action parsing function. For simplicity, we adopt the rule-based approach that compares pixel-wise features between the current state and the previous state (details in Appendix B.3). While effective in our controlled setup, this method limits generalizability to broader task settings. Nevertheless, we argue that the core idea is extensible and could be supported by well-established computer vision techniques such as segmentation [41], contour detection [32] and etc. We encourage future research to explore more robust and scalable designs for visual state-action parsing to advance visual planning systems. Broader Impact This work introduces novel paradigm of visual planning, where agents reason and act entirely within the visual modality without reliance on textual intermediaries. By demonstrating that models can plan through sequences of images, this research opens new possibilities for the way human and AI system interacts, particularly in domains like robotics, navigation, and assistive technologies, where perception and decision-making are tightly coupled. As the first step toward planning grounded purely in visual representations, our work lays the foundation for AI systems that integrate both verbal and non-verbal reasoning. We advocate for future research into more holistic multimodal thinking systems where interleaved text and image traces enable richer, more human-like reasoning, and emphasize the importance of strengthening the visual component in such traces for improved planning and cognition."
        },
        {
            "title": "B Implementation details",
            "content": "B.1 Dataset Task Action Space. FROZENLAKE and MAZE both involve four primitive navigation actions: up, down, left, and right. MINIBEHAVIOR includes more complex action space with two additional operations: pick, drop. Dataset preparation. For both FROZENLAKE and MAZE, we construct environments of grid sizes ranging from 3 3 to 6 6. For each size, we sample 1250 environments, with 1000 used for training and 250 held out for testing  (Table 2)  . Each environment here is guaranteed to have unique layout, and the agent is randomly initialized at grid from which the goal is reachable, forming the initial state v0. Due to the relatively limited diversity of environments layout in MINIBEHAVIOR, where the complexity arises primarily from the action space, sampling unique environments in small grid size becomes challenging. Therefore we focus only on grid sizes 7 7 and 8 8, allowing duplicates in layout but varying agent spawn positions to ensure sufficient data volume. To prevent data leakage, we split the dataset based on layout identity, ensuring no layout overlap between the training and test sets. We next describe the dataset construction procedures corresponding to the training setups outlined in Section 3, with the number of samples per task summarized in Table 3. SFT in Text (Baseline): For each environment, we sample an optimal trajectory consisting sequence of visual states (v0, . . . , vn) as the ground truth. Each transition between states is determined by an action, enabling us to derive corresponding verbalied action sequence Table 2: Distribution of training dataset by grid sizes for each task. Value indicates the number of environments. Grid Size Train Test Grid Size Train Test Grid Size Train Test FROZENLAKE 3 1000 250 5 1000 3 1000 250 MINIBEHAVIOR 5 1000 250 4 1000 250 MAZE 4 1000 250 6 1000 250 6 1000 250 7 796 8 801 199 (a0, . . . , an1). The input to the model is formulated by concatenating textual prompt with an image representation of the initial state v0, while the target output is the verbalized action sequence representing the optimal trajectory. The detailed prompt is provided in Appendix D. VPFT: We utilize the same set of optimal trajectories as the language-based reasoning baseline described above. In the visual scenario, each trajectory generates multiple input-target pairs by pairing the state at timestep as the input with the subsequent state at timestep + 1 as the target. VPRL: Stage 1: This dataset serves solely for format control training of the visual backbone. For each environment, we enumerate all possible trajectories from the initial state as v0 and generate corresponding input-target pairs. Duplicate pairs are filtered to maintain balanced distribution. Stage 2: To ensure fairness and comparability, this dataset uses the same input states as VPFT. VPFT*: We conduct an ablation study (indicated with *) where VPFT is also trained in two stages, mirroring the structure of VPRL. Stage 1 follows the same procedure as VPRL Stage 1, focusing on format supervision using enumerated visual inputs. Stage 2 reuses the original VPFT training pipeline, learning from optimal trajectories. Experimental results and analysis see Appendix C.4. Note: For both textual and visual planning setups, evaluation is performed using only the initial state v0 of each test environment as input. Dataset Statistics. We evaluate the performance of different system variants with in-distribution and out-of-distribution (OOD) settings. Table 2 show the training data distribution over different grid sizes across three tasks. The numbers of training and testing samples for different system variants are shown in Table 3. For OOD evaluation, the enlarged grid sizes are shown in Table 7. OOD evaluation data includes 250 samples for each task. B.2 Models Large Vision Model (LVM) [4] is an autoregressive models for image generation, which is only pretrained with image sequences with no exposure to language data. The model uses tokenizer based on the VQGAN architecture [12], which extracts visual information from raw images and encodes it into 256 tokens from fixed codebook. The image is generated in an auto-regressive manner with discrete tokens, which are then fed into the image detokenizer. Although LVM supports multiple model sizes, only the 3B-parameter version is publicly available; thus, we use this variant in our experiments. For fair comparison, we use Qwen 2.5-VL-Instruct [3] with matching parameter size as our language-based baseline. B.3 Reward Implementation We adopt rule-based state-action parsing function and progress map D(v) in VPRL. For progress map, we apply the Breadth First Search (BFS) to search for the optimal trajectories and calculates the 16 Table 3: Number of training and test samples for each task and method. For visual planning, the numbers here are represented in image pairs, which correspond to the same number of trajectories for SFT in Text. Task FROZENLAKE MAZE MINIBEHAVIOR Split Train Test Train Test Train Test SFT in Text VPFT VPRL VPFT* Stage 1 Stage 2 Stage 1 SFT 12806 14459 1000 9174 403 170621 N/A 156682 N/A 90808 N/A 12806 14459 1000 9174 403 170621 N/A 156682 N/A 90808 N/A 12806 14459 1000 9174 403 4000 1000 4000 1000 1597 403 progress at each position in the grid for each task. The progress map are then used as reward signal to guide VPRL training. Specifically, for state-action parsing function, we parse the state and identify the difference between current state and previous state through pixel-wise feature extractor. We first convert both input and predicted states into coordinate-based representation by dividing the image into grid based on its size. Each region corresponds to discrete coordinate in the environment. To reduce sensitivity to color and focus on structural differences, we convert all images to grayscale. We subsequently compute the Intersection-over-Union (IoU) between each coordinate in the predicted state and the coordinate in the input state that contains the player (input coordinate). The coordinate in the predicted state with the highest IoU is selected as the predicted agent position. The action is then inferred by comparing the start and predicted positions according to task-specific movement rules. For example, in the MAZE environment, movement across walls is not allowed and would be considered invalid. Notably, to detect the invalid transitions, such as the disappearance of agents, we also calculate the pixel-wise mean squared error (MSE) between corresponding coordinates to measure local visual differences. If two coordinates exhibit significant MSE differences exceeding predefined threshold, we treat them as the potential source and destination of movement (agent disappears from one and appears in another). If only one such coordinate is found, we treat it as disappearance event, indicating an invalid transition. In MINIBEHAVIOR, we extend this logic to identify pick and drop actions. pick is detected when the IoU between the printers location in the input and predicted states falls below threshold, indicating that the printer has been removed. drop is inferred when coordinate corresponding to the table region shows large MSE increase, suggesting the printer has been placed there. Additional edge cases in theses tasks are omitted for brevity. For reward compuation, if the predicted action is valid, we compare the progress values from the progress map D(v) between the input and predicted states. reward of 1 is given if the predicted state shows greater progress toward the goal than the input state; otherwise, the reward is 0. Invalid actions are penalized with reward of -5. Our method and reward modeling approach are readily generalizable to other visual tasks. With reference to computer vision techniques such as segmentation [41] and contour detection [32], the pixel-level analysis used in our framework can be easily extended to wide range of structured visual environments. Furthermore, our reward design is broadly applicable to planning tasks in general. Since actions in most planning settings can naturally be categorized into one of three types (valid and helpful, valid but non-progressing, or invalid), our simple reward structure remains intuitive and effective across tasks. B.4 Training details For all post-training experiments, we apply Low-Rank Adaptation (LoRA) [20] on both attention layers and feed-forward layers. The detailed hyper-parameters are shown in Table 4. Only the loss 17 Table 4: Hyper-parameters of training both textual and visual planners. Hyper-Parameters SFT in Text VPFT VPRL VPFT* Epochs Learning Rate Train Batch Size Group Size Grad Accumulation GPUs 30 1e-5 16 N/A 2 8 Stage 1 10 1.5e-4 8 N/A 1 8 Stage 2 10 5e-5 1 10 1 8 Stage 1 10 1.5e-4 8 N/A 1 SFT 30 1.5e-4 8 N/A 1 8 30 1.5e-4 8 N/A 1 8 Table 5: We compute the percentage of failed trajectories that are caused by at least one invalid action, rather than suboptimal but valid action. Lower values indicate better action validity control. Task Invalid-Failure Ratio (%) VPRL VPFT FROZENLAKE MAZE MINIBEHAVIOR 36.9 25.1 29.6 60.6 73.7 78.3 of the labels is calculated in an instruction-tuning manner [49] for SFT. The image tokenizer and detokenizer are frozen during training. We use the AdamW optimizer [35] for all training procedures. When SFT for textual planning and visual planning, we train the model for maximum of 30 epochs. For VPRL, we first do stage 1 on random trajectories for 10 epochs for the purpose of exploration. We then use GRPO to optimize the model for planning for another 10 epochs for stage 2. We sample group of 10 candidate responses per prompt to compute the advantages accordingly. To encourage balance between exploration and exploitation, we apply KL divergence penalty with coefficient β = 0.001. We use TRL library for training [47]. Weve conducted our experiments on the machine with 8A100 GPUs. B.5 Licenses Model-wise, Large Vision Model and Qwen 2.5 VL are under Apache-2.0 license. TRL is under Apache-2.0 license. We collect the MAZE dataset with our own Python scripts. FROZENLAKE is collected from OpenAI Gym under the MIT License."
        },
        {
            "title": "C Results",
            "content": "C.1 Training The reward curves with standard deviation for all tasks are shown in Figure 7. The shaded regions indicate the standard deviation across groups. For better visualization, we apply Gaussian smoothing to both the reward values and their corresponding standard deviations. C.2 Performance with Scaling Difficulties We evaluate the performance of different methods with respect to task difficulty in MINIBEHAVIOR and MAZE, as shown in Figure 8. Our visual planners consistently achieve higher accuracy across all grid sizes and exhibit notably flatter performance curves, indicating greater robustness to increasing environment complexity. Interestingly, in MINIBEHAVIOR, we observe that the accuracy of visual planners increases with grid size, which is in contrast to the trend exhibited by textual planners. We hypothesize that this is due to the fixed layout components in this task, specifically, the presence of only table and printer. This maintains consistent layout complexity across different grid sizes and allows knowledge acquired Figure 7: Reward curves with standard deviation for VPRL on FROZENLAKE, MAZE and MINIBEHAVIOR. Figure 8: Performance across different grid sizes, reflecting task difficulty. Left: MAZE. Right: MINIBEHAVIOR. Visual planners consistently maintain higher accuracy and exhibit flatter performance curves, indicating robustness to increasing complexity. in smaller grids to generalize effectively to larger grids. This suggests that visual planning better captures and transfers structural patterns in the environment. C.3 Out-of-Distribution Performance Figure 9 illustrates generated images from VPFT and VPRL on OOD scenarios across MAZE, FROZENLAKE, and MINIBEHAVIOR tasks. Notably, both models exhibit certain level of visual generalization to unseen configurations, such as larger grids with finer step granularity, despite not encountering them during training. We subsequently quantitatively test generalization by evaluating the model on OOD environments with larger grid sizes. We find that SFT models performs poorly, while VPRL still demonstrates certain level of visual planning capability as shown in Table 7. VPRL consistently outperforms VPFT in both Exact Match and Progress Rate, suggesting that it, to some degree, captures underlying planning strategies rather than merely memorizing training patterns. C.4 Ablation: The Role of Stage 1 To better understand the role of Stage 1 in our two-stage framework, we conduct an ablation study isolating its impact. The primary purpose of Stage 1 is not to improve planning performance directly, but rather to initialize policy with strong exploration capacity and valid output formats. To verify 19 Figure 9: Qualitative comparison of visual planning outputs from VPFT (top) and VPRL (bottom) on out-of-distribution (OOD) scenarios with unseen larger grid size across MAZE, FROZENLAKE, and MINIBEHAVIOR. Each example shows failure case from VPFT contrasted with successful trajectory generated by VPRL under the same environment configuration. Table 6: Exact Match performance of VPFT and VPFT* across different grid sizes in FROZENLAKE. Exact Match (%) Model 33 44 55 66 VPFT* VPFT 86.4 92.0 73.6 82.8 50.0 68.8 33.2 58.0 this, we reuse the original VPFT training pipeline, i.e., learning from optimal trajectories, but start from the Stage 1 checkpoint as VPFT*. Surprisingly, this variant yields lower final performance on FROZENLAKE compared to standard VPFT. This result supports our hypothesis that Stage 1 does not contribute to planning ability itself, but instead provides an exploration-friendly initialization that facilitates effective reinforcement learning in Stage 2. C.5 Visual Planning Results VPRL Stage 1 and Stage 2 Table 8 presents results for each stage of VPRL. After Stage 1, the model learns to generate plausible images but lacks goal-directed behavior, resulting in near-random performance across tasks. In Stage 2, reinforcement learning instills purposeful planning, enabling the model to align generations with the goal and outperform VPFT across all benchmarks. Generated Visual Planning Traces for Illustration Figure 10 shows the generated visual planning traces for FROZENLAKE, with Figure 11 for MAZE and Figure 12 for MINIBEHAVIOR. Each visual trajectory begins with the initial state as the input (the first frame), followed by sequence of intermediate states generated by VPRL that form the predicted visual plan. We include examples from three categories: (1) Optimal cases, where the model successfully generates the shortest valid path to the goal; (2) Non-optimal cases, where the agent fails to reach the goal within the optimal number of steps due to intermediate non-optimal actions; and (3) Invalid 20 Table 7: Out-of-distribution (OOD) performance on enlarged grids. Models are trained on smaller grids and evaluated on the sizes indicated in parentheses. Model VPFT VPRL FROZENLAKE (77) MAZE (77) MINIBEHAVIOR (99) EM (%) PR (%) EM (%) PR (%) EM (%) PR (%) 9.6 20.4 15.3 31.2 9.2 10. 17.8 21.6 0.0 0.4 5.8 14.7 Table 8: Performance comparison of VPRL Stage 1 and Stage 2 across all three tasks. Model FROZENLAKE MAZE MINIBEHAVIOR EM (%) PR (%) EM (%) PR (%) EM (%) PR (%) VPRL Stage 1 VPRL Stage 2 11.1 91.6 27.2 93.2 9.6 74.5 22.7 77.6 0.5 75. 14.2 83.8 cases, in which the generated trajectory contains invalid actions that violate environment constraints, preventing task completion. Notably, as illustrated in Figure 3, we still observe occasional planning errors. While reinforcement learning significantly improves generalization compared to supervised fine-tuning, it does not fully eliminate such failure cases. 21 Figure 10: Generated visual planning trajectories from VPRL on the FROZENLAKE test set. We illustrate three representative categories: optimal, non-optimal, and invalid cases. In non-optimal examples, the model occasionally enters local loops but still has the chance to make progress toward the goal, see the first and third trajectories. In invalid cases, despite significant reduction in failure rate, VPRL still exhibits errors such as disappearing agents, contradictory actions (e.g., simultaneous left and right), or unrealistic teleportation. 22 Figure 11: Generated visual planning trajectories from VPRL on the MAZE test set. We illustrate three representative categories: optimal, non-optimal, and invalid cases. In non-optimal examples, similar to FROZENLAKE, the model occasionally enters redundant loops but still progresses toward the goal. Invalid cases include maze-specific errors, such as the agent erroneously traversing through walls, violating the structural constraints of the environment. Notably, we observe that in the last invalid case, the agent is able to plan an optimal trajectory in subsequent steps. 23 Figure 12: Generated visual planning trajectories from VPRL on the MINIBEHAVIOR test set."
        },
        {
            "title": "D Prompting Templates",
            "content": "FROZENLAKE Task : Frozen Lake Shortest Path Planning You are given an image of grid - based environment . In this environment : - An elf marks the starting position . - gift represents the goal . - Some cells contain ice holes that are impassable for the elf . - The elf can move in one of four directions only : \" up \" , \" down \" , \" left \" , or \" right \". Each move transitions the elf by one cell in the corresponding absolute direction . Diagonal movement is not permitted . Your task is to analyze the image and generate the shortest valid sequence of actions that moves the elf from the starting position to the goal without stepping into any ice holes . Provide your final answer enclosed between < ANSWER > and </ ANSWER > , for example : < ANSWER > right up up </ ANSWER >. < image > MAZE Task : Maze Shortest Path Planning You are given an image of maze environment . In this environment : - green circle marks the starting position of the agent . - red flag marks the goal . - The agent can move in one of four cardinal directions only : \" up \" , \" down \" , \" left \" , or \" right \". Each move shifts the agent by exactly one cell in that direction . Diagonal movement is not permitted . - The black maze walls are impassable . The agent cannot pass through any wall segment . Your task is to analyse the image and produce the shortest valid sequence of actions that moves the agent from its starting position to the goal without crossing any wall . Provide your final answer enclosed between < ANSWER > and </ ANSWER > , for example : < ANSWER > right up < image > MINIBEHAVIOR Task : Mini - Behavior Installing the Printer You are given an image of grid - based environment . In this environment : - The red triangle represents the agent . - The white icon represents the printer , which must be picked up by the agent . - The brown tiles represent the table , where the printer must be placed . The agent can take the following actions : - \" up \" , \" down \" , \" left \" , \" right \": each action shifts the agent by exactly one cell in that direction . Diagonal movement is not permitted . - \" pick \": pick up the printer if it is in one of the four adjacent cells surrounding the agent . This action is invalid if there is no adjacent printer . - \" drop \": drop the printer onto the table if the agent is adjacent to table cell . This action is invalid if there is no adjacent table . Constraints : - The agent cannot move through the table tiles . - The agent cannot move through the printer until it has been picked up . After picking it up , the agent may move through the cell that previously contained the printer . Your task is to analyse the image and produce the shortest valid sequence of actions that allows the agent to pick up the printer and then place it on the table . Provide your final answer enclosed between < ANSWER > and </ ANSWER > , for example : < ANSWER > right down right pick left drop </ ANSWER >. < image >"
        }
    ],
    "affiliations": [
        "Google",
        "Language Technology Lab, University of Cambridge",
        "University College London"
    ]
}