{
    "paper_title": "SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers",
    "authors": [
        "Chaitanya Manem",
        "Pratik Prabhanjan Brahma",
        "Prakamya Mishra",
        "Zicheng Liu",
        "Emad Barsoum"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The demand for Large Language Models (LLMs) capable of sophisticated mathematical reasoning is growing across industries. However, the development of performant mathematical LLMs is critically bottlenecked by the scarcity of difficult, novel training data. We introduce \\textbf{SAND-Math} (Synthetic Augmented Novel and Difficult Mathematics problems and solutions), a pipeline that addresses this by first generating high-quality problems from scratch and then systematically elevating their complexity via a new \\textbf{Difficulty Hiking} step. We demonstrate the effectiveness of our approach through two key findings. First, augmenting a strong baseline with SAND-Math data significantly boosts performance, outperforming the next-best synthetic dataset by \\textbf{$\\uparrow$ 17.85 absolute points} on the AIME25 benchmark. Second, in a dedicated ablation study, we show our Difficulty Hiking process is highly effective: by increasing average problem difficulty from 5.02 to 5.98, this step lifts AIME25 performance from 46.38\\% to 49.23\\%. The full generation pipeline, final dataset, and a fine-tuned model form a practical and scalable toolkit for building more capable and efficient mathematical reasoning LLMs. SAND-Math dataset is released here: \\href{https://huggingface.co/datasets/amd/SAND-MATH}{https://huggingface.co/datasets/amd/SAND-MATH}"
        },
        {
            "title": "Start",
            "content": "2025-7-29 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers Chaitanya Manem1, Pratik Prabhanjan Brahma1, Prakamya Mishra1, Zicheng Liu1 and Emad Barsoum1 1Advanced Micro Devices, Inc. (AMD) The demand for Large Language Models (LLMs) capable of sophisticated mathematical reasoning is growing across industries. However, the development of performant mathematical LLMs is critically bottlenecked by the scarcity of difficult, novel training data. We introduce SAND-Math (Synthetic Augmented Novel and Difficult Mathematics problems and solutions), pipeline that addresses this by first generating high-quality problems from scratch and then systematically elevating their complexity via new Difficulty Hiking step. We demonstrate the effectiveness of our approach through two key findings. First, augmenting strong baseline with SAND-Math data significantly boosts performance, outperforming the next-best synthetic dataset by 17.85 absolute points on the AIME25 benchmark. Second, in dedicated ablation study, we show our Difficulty Hiking process is highly effective: by increasing average problem difficulty from 5.02 to 5.98, this step lifts AIME25 performance from 46.38% to 49.23%. The full generation pipeline, final dataset, and fine-tuned model form practical and scalable toolkit for building more capable and efficient mathematical reasoning LLMs. SAND-Math dataset is released here: https://huggingface.co/datasets/amd/SAND-MATH 5 2 0 2 J 8 2 ] . [ 1 7 2 5 0 2 . 7 0 5 2 : r Figure 1 Data Generation and Filtering pipeline for SAND-Math. Different steps in the pipeline filters the initial pool of questions to meet the Novelty, Correctness and Difficulty requirements. SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers 1. Introduction Mathematical problem-solving is emerging as pivotal area of interest as the frontier of Large Language Model (LLM) capabilities rapidly advances into domains that demand sophisticated reasoning. This critical capability is essential for driving innovation and efficiency across numerous industries. In fields ranging from the algorithmic precision of quantitative finance to the complex simulations of scientific research and engineering, the ability of AI to perform high-level mathematical reasoning promises to unlock new paradigm of automated logic, analysis, and discovery. State-of-the-art (SOTA) models such as DeepSeekR1 [6], Exaone [19], OpenAI o3 [16], and Gemini 2.5 Pro [5] have demonstrated remarkable performance on challenging mathematical benchmarks. However, these proprietary models typically do not disclose their training data or generation methods, creating major barrier for broader adoption. This raises key question on how to systematically build high-performance LLMs for mathematical reasoning without access to such \"secret\" datasets. Recent work, such as LIMO [25] and S1 [15], highlights that eliciting advanced reasoning depends more on quality as comparaed to quantity, particularly the inclusion of sufficiently challenging questions. This shifts the focus from randomly scaling data to strategically curating high-difficulty training data. Yet, finding such data at scale remains major challenge. Public datasets rich in difficult problems (for example, NuminaMath [12], and OpenR1 [8]) are valuable but face critical scalability issues. They rely heavily on manual curation from Olympiad forums and past competitions, costly and labor-intensive process and is constrained by the finite supply of existing human-authored problems. This leaves synthetic data generation as the most viable path forward. However, review of current synthetic methods, including KPDDS [11], MetaMathQA [26], WizardMath [14], and OpenMathInstruct2 [22], reveals fundamental limitation. While promising, these approaches typically transform existing datasets like GSM8K [4] and MATH [10] training splits, thus inheriting their characteristics and difficulty levels. As result, they struggle to produce problems that go beyond the complexity of their seeds, leaving gap in the scalable generation of truly novel and challenging mathematical content. In this work, we propose novel approach to bridge the gap in generating truly challenging math problems. We hypothesize that, despite lacking access to their training data, SOTA models possess metacognitive abilities[7] an implicit grasp of the structure and difficulty of mathematical problems. Thus, even minimally constrained prompts (e.g., generate an Olympiad-level number theory problem) can activate this knowledge, enabling autonomous generation of complex and diverse questions without detailed guidance. Based on this hypothesis, we introduce an automated pipeline that leverages LLMs to generate and refine high-difficulty math data. The process, illustrated in Figure 1, begins by generating problems from scratch using simple prompts. It then applies series of rigorous filters to ensure solution correctness, novelty, difficulty and decontamination, and incorporates unique difficulty hiking module to systematically further elevate problem complexity. As shown in Figure 2, the dataset generated using the proposed pipeline exceeds the difficulty of prior synthetic datasets and rivals other popular real-world curated data. Through our pipeline, the resulting dataset, and fine-tuned model trained on the dataset, we intend to accelerate the development of powerful and efficient mathematical LLMs. Our primary contributions are as follows: novel methodology for generating challenging synthetic math data by leveraging the hypothesized metacognitive abilities of SOTA LLMs, independent of seed datasets. complete pipeline incorporating quality controls for correctness and novelty, and unique, 2 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers systematic \"difficulty hiking\" process to further increase problem complexity. new, high-difficulty synthetic dataset and smaller model fine-tuned on it, demonstrating that our approach can significantly enhance the mathematical reasoning of more resource-efficient models. 2. Multi-stage Pipeline for SAND-Math We present novel, comprehensive end-to-end synthetic data generation and filtering pipeline which generates sufficiently challenging mathematics problems and solutions that elicit the reasoning capabilities of the LLMs. In the subsequent sections, we discuss each step in our pipeline, Specially, in the section 2.7 we discuss an additional novel methodology of hiking the difficulty of the existing questions systematically. 2.1. Question Generation Our pipeline begins by generating an initial pool of questions using the teacher LLM, Mteacher. To leverage the models implicit knowledge, we use minimally constrained, empirically optimized prompt (see Appendix F) across predefined mathematics branches. To keep the LLM grounded to generate solvable questions, Mteacher is prompted to co-generate corresponding solutions, though these are not the final solutions that are used for subsequent training. This first step yields an initial dataset, D0, containing 23,437 question-solution pairs: D0 = {(𝑞𝑖, 𝑠𝑖)}23, 𝑖=1 (1) where 𝑞𝑖 and 𝑠𝑖 are the 𝑖-th question and its corresponding solution. 2.2. Solution Generation For each question 𝑞𝑖 in the initial pool D0, we use teacher model, Mteacher, to generate 𝑘 distinct solutions. Guided by dedicated prompt (see Appendix G), this process yields set of detailed reasoning traces, from which we extract the final answers. The resulting dataset is defined as: D1 = {(𝑞𝑖, 𝑠𝑖, T𝑖)}23, 𝑖=1 (2) where T𝑖 = {(𝑟𝑖 𝑗, 𝑎 𝑖 𝑗)}𝑘 𝑗=1 denotes the set of 𝑘 reasoning traceanswer pairs associated with question 𝑞𝑖. 2.3. Solution Correctness Filtering To filter for correctness, we apply self-consistency [24] to the dataset D1. question 𝑞𝑖 is retained only if all 𝑘 answers in its generated solution set T𝑖 are identical: 𝑎 𝑖1 = 𝑎 𝑖2 = = 𝑎 𝑖𝑘. This convergence across diverse reasoning paths serves as strong proxy for well-posed question and verifiable answer. For each consistent question, we randomly select one reasoning trace-answer pair (𝑟𝑖, 𝑎𝑖) to form the new dataset D𝑐𝑜𝑛𝑠. This filtering stage yielded 17,578 high-quality examples, achieving 74% consistency rate. 3 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers 2.4. De-duplication and Decontamination To ensure the novelty and integrity of our dataset, we apply two final filtering stages to D𝑐𝑜𝑛𝑠. First, we de-duplicate the dataset internally to promote diversity using the semhash framework [23] with the minisilab/potion-base-8M[20] model. At similarity threshold of 0.99, this removed 1,293 near-duplicates (7.3%). Next, to prevent test set leakage, we decontaminate against our evaluation benchmarks. We retrieve the top-5 nearest neighbors for each question using paraphrase-multilingual-MiniLM-L12v2 [18] embeddings. Llama-3.3-70B-Instruct [9] judge, guided by specific prompt (see Appendix I), then identifies semantic duplicates. Remarkably, this process removed only 4 questions, underscoring the novelty of the questions generated through our method. After these two filters, the final dataset contains 16,281 internally diverse and benchmark decontaminated examples. 2.5. Difficulty Filtering and Difficulty Rating At this stage, we both filter the dataset for empirical difficulty and assign fine-grained difficulty score to each question. First, to construct our final training set, we apply performance-based filter. We use the target model as strong solver, Qwen2.5-32B-Instruct [21], to attempt each question in Dcons. Only questions where the model fails to produce the correct answer are retained. This process yielded dataset, Ddiff, containing 9,211 empirically verified difficult problems, retaining 56.6% of the data from the previous step. Concurrently, we assign difficulty rating to each question. Llama-3.3-70B-Instruct [9] judge assigns score on scale of 110, guided by rubric and reference examples from AoPS1 (see Appendix H). These ratings provide insight into our datasets distribution and serve as key input for our Difficulty Hiking methodology (Section 2.7). 2.6. Novelty Filtering To ensure our synthetic data can be safely mixed with public corpora without introducing redundancy, we apply final novelty filter to Ddiff. This step removes questions that are semantically similar to content on the public web including open-source datasets, thereby promoting genuine reasoning over memorization. Following Algorithm 1, we use each question as web search query and compute its maximum semantic similarity to the top-10 search results using Alibaba-NLP/gte-Qwen2-7B-instruct [13] embeddings. Questions exceeding similarity threshold of 𝜏 = 0.85 are discarded. This process removed 4% of the questions, resulting in our final dataset of 8,842 novel problems, which we name SAND-Math. 2.7. Difficulty Hiking: Systematically Increasing Problem Complexity To furhter difficulty of our initially generated questions, we introduce Difficulty Hiking: methodology to systematically increase problem complexity. The process involves re-prompting the teacher model, Mteacher, to rewrite question by synthesizing new constraints and concepts. The prompt (see Appendix D) provides Mteacher with four key inputs: (1) the original question and 1https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions 4 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers Figure 2 Difficulty distribution of SAND-Math (500) dataset compared with other math datasets solution, (2) its current difficulty rating, (3) relevant advanced theorem, and (4) cross-domain mathematical concept. The model is explicitly instructed to integrate these elements to achieve higher target difficulty (e.g., 8.0/10.0). To force creative synthesis, the theorem is sampled from the same mathematical branch as the question, while the concept is sampled from any branch at random (see taxonomy in Appendix J). concrete example of this transformation is provided in Appendix E. While this process is iterative, we only show results with single hiking step in this work. This methodology proved highly effective. In one iteration on sample of data, the mean difficulty score increased from 5.02 to 5.98. Figure 3 visually confirms this shift, with the proportion of questions rated 6.0+ increasing from 47.2% to 76.8%. Crucially, this enhanced data improves downstream performance: as shown in Table 2, finetuning on difficulty-hiked data boosted the average evaluation score from 72.94 to 74.39. 3. Experiments and Results 3.1. Implementation Details Our pipeline assigned distinct roles to specific models to ensure reproducible workflow. The core generative tasks were handled by DeepSeek-R1 [6], which performed initial question generation (temp=0.8), subsequent solution generation (k=2, temp=0.6), and question modification during the Difficulty Hiking stage. All judging and rating tasks were assigned to Llama-3.3-70B-Instruct [9]. This included verifying solution consistency, acting as the judge in our decontamination process, and assigning the fine-grained difficulty scores, which were averaged over 3 runs for stability. For specialized filtering and setup tasks, we utilized several tools. Internal de-duplication was performed with semhash [23] at 0.99 similarity threshold. For novelty checks, we queried self-hosted SearXNG2 instance via LangChain. The entire pipeline was executed on single node equipped with 8 AMD InstinctTM MI300X GPUs. To enhance throughput, the data generation stages of the pipeline utilized the steps mentioned in the Supercharge DeepSeek-R1 ROCmTM blog post [1]. 2https://github.com/searxng/searxng 5 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers Training Data Configuration Data Sample Size AIME25 AIME24 AMC MATH-500 Average Qwen 2.5 32B LIMO (SFT baseline) Section 1: Baselines - 817 13.33 44.50 16.67 56.30 64.38 91. 83.40 93.80 Section 2: Standalone Finetuning (vs. Real-World Problems Datasets) SAND-Math (ours) openr1_math llama_nemotron 500 500 500 41.88 43.96 17.50 52.08 53.12 20. 89.84 90.62 65.62 92.60 93.40 84.80 LIMO + SAND-Math (ours) LIMO + openr1_math LIMO + llama_nemotron Section 3: Augmentation Performance (vs. Real-World Problems Datasets) 94.00 93.80 86.20 817 + 500 817 + 500 817 + 500 92.50 92.50 71. 48.89 47.71 26.04 57.92 56.04 36.46 Section 4: Augmentation Performance (vs. Synthetic Datasets) LIMO + SAND-Math (ours) LIMO + MetamathQA LIMO + OpenmathInstruct 817 + 500 817 + 500 817 + 500 48.89 31.04 18. 57.92 46.25 38.96 92.50 47.24 64.53 94.00 56.40 72.40 44.44 71.50 69.10 70.27 47.13 73.32 72.51 55. 73.32 45.23 48.50 Table 1 Finetuning Performance Comparison of SAND-Math against SOTA Datasets on Qwen 2.5 32B. All finetuning experiments are run for 10 epochs. We report pass@1 accuracy (%). SAND-Math demonstrates competitive standalone performance and provides the strongest performance boost when used to augment the LIMO baseline. 3.2. Experimental Setup Finetuning Setup. All models were finetuned on the Qwen2.5-32B-Instruct [21] model using the LLaMA-Factory [27] framework. Our training environment was built on top of the rocm/pytorch-training Docker container [2], and all experiments were conducted on single node equipped with 8 AMD InstinctTM MI300X GPUs. We utilized DeepSpeed [17] with the ZeRO-3 stage for efficient memory management. While our overall training configuration is adopted from the LIMO setup [25], we used learning rate of 5e-6 and trained for 10 epochs with full-parameter supervised finetuning. Complete details for reproducing our training setup are provided in Appendix A. Training Datasets. To contextualize the performance of SAND-Math, we conducted finetuning experiments against curated set of publicly available datasets. Our comparison includes leading opensource synthetic datasets that also generates both question-solution pairs, such as OpenMathInstructv2[22], MetamathQA[26]. To establish more competitive baseline, we also compare against high-difficulty, real-world dataset, specifically OpenR1-Math-220k[8], Llama-Nemotron-PostTraining-Dataset [3]. To ensure fair comparison under our available compute budget, we created training splits of equal size by sampling from each of these external datasets as well as from our own SAND-Math variants. Evaluation Benchmarks and Metrics. We evaluate our models on several challenging benchmarks: AIME (2024 and 2025), AMC, and MATH500. Our evaluation, built on the LIMO codebase [25]. For AIME and AMC, we report pass@1 with 16 samples per problem (𝑛 = 16, temp=0.7), while for MATH500, we use greedy decoding (accuracy, temp = 0). 3.3. Main Results Our experiments validate SAND-Math as powerful resource for enhancing LLM reasoning. We demonstrate its effectiveness both as standalone dataset and as data supplement, attributing its success to superior difficulty distribution. 6 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers Data Size Dataset 817 LIMO only (Baseline) 817 + 1500 LIMO + SAND-Math (Base) LIMO + SAND-Math (DH) 817 + 1500 LIMO + SAND-Math (DH_w_LF) 817 + 1500 AIME25 AIME24 AMC24 MATH500 Average 71.50 91.41 72.94 92.71 74.39 93.17 93.28 74. 56.3 59.09 60.55 60.83 44.5 46.38 49.23 49.23 93.8 93.6 94.6 93 Table 2 Impact of Difficulty Hiking on Finetuning Performance. Ablation study showing the pass@1 accuracy (%) of the Qwen2.5-32B model. The model is finetuned on the LIMO dataset augmented with variants of our SAND-Math data: One(DH) version trained with \"difficulty hiked\" questions of SAND-Math sample from baseline, and another(DH_w_LF) that is additionally applies lenght filter to include only examples with sequence length under 32,768 tokens. Introducing difficulty-hiked questions yields the best average performance. Figure 3 Impact of Difficulty Hiking on Data Distribution. Comparison of question difficulty ratings for sample of SAND-Math data: before hiking, after hiking, and after_w_lf (with length filter of 32k). The process successfully converts questions from the mid-difficulty range (2.0-5.0) into the more challenging 7.0-8.0 range. Standalone Performance vs. Real-World Problems Data. As standalone dataset, SAND-Math proves highly effective. As shown in Table 1, finetuning on 500-sample subset of SAND-Math achieves an average score of 69.10, nearly matching the 70.27 score from similarly 500-sized subset of openr1_math. This demonstrates that our automated pipeline produces data of quality competitive with human curated problems, offering far more scalable alternative. Superior Performance in Data Augmentation. The primary value of SAND-Math is as an augmentation resource. Augmenting the LIMO baseline with SAND-Math yields top score of 73.32. As detailed in Table 1, this surpasses augmentations with the high-quality openr1_math (72.51) and reveals vast performance gap over other synthetic datasets like MetamathQA (45.23) and OpenmathInstruct (48.50). Difficulty Analysis. These performance disparities are explained by the training datas difficulty distribution. Figure 2 clearly shows SAND-Math is centered at much higher difficulty rating (mean 6) and covers wider complexity range than other synthetic datasets. This strong correlation between training data difficulty and downstream performance validates our central hypothesis. 7 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers Figure 4 Performance trend when augmenting the LIMO training data with SAND-Math (SM) samples. The DH (Difficulty Hiked) condition shows greater improvement with 1500 additional samples. 3.4. Impact of Difficulty Hiking To isolate the impact of our novel difficulty hiking methodology, we conducted an ablation study. The results in Table 2 show that while augmenting the LIMO baseline (71.50) with our base SAND-Math data provides +1.44 point boost, applying the difficulty hiking process yields the peak performance of 74.39 (+2.89 over baseline). This demonstrates that targeted increase in problem complexity is the primary driver of improvement. An additional length filter slightly reduces performance, suggesting valuable, complex reasoning exists at various token lengths. The performance gain is explained by Figure 3. The plot clearly shows that our hiking process transforms the datas difficulty composition, effectively reducing the number of lower and midcomplexity questions while substantially increasing the count of challenging problems in the 6.0-8.0 range. This shift creates more rigorous training curriculum that directly translates to better model performance. Superior Scaling through Difficulty Hiking. We also analyzed data efficiency, critical factor for practical applications. Figure 4 illustrates the performance trend on the AIME24 benchmark as we incrementally add samples. While both base (Normal) and difficulty-hiked (DH) versions of SAND-Math improve performance, the DH data exhibits much steeper improvement trajectory. Augmenting with just 1500 DH samples significantly outperforms the base data augmentation. This demonstrates that for given data budget, difficulty-hiked data provides superior return on investment, enabling greater performance gains with fewer examples. 4. Conclusion This work presents practical solution to key industry challenge: the scarcity of high-quality data required to build powerful mathematical LLMs. Our main contribution, the SAND-Math pipeline, effectively synthesizes novel and complex math problems by leveraging the latent abilities of SOTA models. The integrated Difficulty Hiking technique further elevates problem complexity. The primary result is clear: training on SAND-Math data significantly boosts the performance of relatively smaller models, making advanced mathematical AI more accessible. SAND-Math offers new paradigm for SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers synthetic data generation, moving beyond simple seed based augmentation to intelligent, targeted questions and solutions synthesis, and we believe this can empower practitioners to curate their own datasets. Future work will focus on scaling our experiments to larger data sizes, and applying our hiking methodology to enhance existing public datasets. While we have applied this methodology for mathematics domain only, this has the potential to offer path towards creating advanced high quality datasets on large scale to enhance the reasoning capabilities of LLMs across various dimensions."
        },
        {
            "title": "Limitations",
            "content": "Our study, while demonstrating the effectiveness of the SAND-Math pipeline, we acknowledge the following considerations regarding its current implementation and scope. First, the quality and correctness of our generated data is intrinsically linked to the capabilities of the large \"teacher\" model used. This dependency introduces computational cost and presents trade-off between generation quality and accessibility, as adopting our pipeline may be challenging for teams with more limited computational environments. Second, by design, our multi-stage pipeline prioritizes data quality and novelty over raw generation volume. The rigorous filtering for correctness, decontamination, and novelty is deliberate choice to ensure high data integrity. This naturally results in lower final yield from the initial pool of generated candidates. While our Difficulty Hiking method effectively improves some questions to augment this yield, the process reflects trade-off between the final datasets quality and the efficiency of its generation. Finally, to validate our methodology within defined scope, our finetuning experiments were conducted on representative sample of the full SAND-Math dataset. This focused study was designed to serve as clear proof-of-concept, and the strong positive results confirm the efficacy of our approach. large-scale study on the complete dataset would be valuable next step to quantify the upper bounds of performance improvement, but is beyond the scope of the current work."
        },
        {
            "title": "References",
            "content": "[1] AMD. Supercharge deepseek-r1 inference on amd instinct mi300x. https://rocm.blogs. amd.com/artificial-intelligence/DeepSeekR1-Part2/README.html, 2025. Accessed: 2025-07-25. [2] AMD. Training model with pytorch for rocm. https://rocm.docs.amd. com/en/latest/how-to/rocm-for-ai/training/benchmark-docker/ pytorch-training.html, 2025. Accessed: 2024-07-25. [3] A. Bercovich, I. Levy, I. Golan, M. Dabbah, R. El-Yaniv, O. Puny, I. Galil, Z. Moshe, T. Ronen, N. Nabwani, I. Shahaf, O. Tropp, E. Karpas, R. Zilberstein, J. Zeng, S. Singhal, A. Bukharin, Y. Zhang, T. Konuk, G. Shen, A. S. Mahabaleshwarkar, B. Kartal, Y. Suhara, O. Delalleau, Z. Chen, Z. Wang, D. Mosallanezhad, A. Renduchintala, H. Qian, D. Rekesh, F. Jia, S. Majumdar, V. Noroozi, W. U. Ahmad, S. Narenthiran, A. Ficek, M. Samadi, J. Huang, S. Jain, I. Gitman, I. Moshkov, W. Du, S. Toshniwal, G. Armstrong, B. Kisacanin, M. Novikov, D. Gitman, E. Bakhturina, J. P. Scowcroft, J. Kamalu, D. Su, K. Kong, M. Kliegl, R. Karimi, Y. Lin, S. Satheesh, J. Parmar, P. Gundecha, B. Norick, J. Jennings, S. Prabhumoye, S. N. Akter, M. Patwary, A. Khattar, D. Narayanan, R. Waleffe, J. Zhang, B.-Y. Su, G. Huang, T. Kong, P. Chadha, S. Jain, C. Harvey, E. Segal, J. Huang, S. Kashirsky, R. McQueen, I. Putterman, G. Lam, A. Venkatesan, S. Wu, V. Nguyen, M. Kilaru, A. Wang, A. Warno, A. Somasamudramath, S. Bhaskar, M. Dong, N. Assaf, S. Mor, 9 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers O. U. Argov, S. Junkin, O. Romanenko, P. Larroy, M. Katariya, M. Rovinelli, V. Balas, N. Edelman, A. Bhiwandiwalla, M. Subramaniam, S. Ithape, K. Ramamoorthy, Y. Wu, S. V. Velury, O. Almog, J. Daw, D. Fridman, E. Galinkin, M. Evans, K. Luna, L. Derczynski, N. Pope, E. Long, S. Schneider, G. Siman, T. Grzegorzek, P. Ribalta, M. Katariya, J. Conway, T. Saar, A. Guan, K. Pawelec, S. Prayaga, O. Kuchaiev, B. Ginsburg, O. Olabiyi, K. Briski, J. Cohen, B. Catanzaro, J. Alben, Y. Geifman, E. Chung, and C. Alexiuk. Llama-nemotron: Efficient reasoning models, 2025. URL https://arxiv.org/abs/2505.00949. [4] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [5] G. Comanici, E. Bieber, M. Schaekermann, I. Pasupat, N. Sachdeva, I. Dhillon, M. Blistein, O. Ram, D. Zhang, E. Rosen, L. Marris, S. Petulla, C. Gaffney, A. Aharoni, N. Lintz, T. C. Pais, H. Jacobsson, I. Szpektor, N.-J. Jiang, K. Haridasan, A. Omran, N. Saunshi, D. Bahri, G. Mishra, E. Chu, T. Boyd, B. Hekman, A. Parisi, C. Zhang, K. Kawintiranon, T. Bedrax-Weiss, O. Wang, Y. Xu, O. Purkiss, U. Mendlovic, I. Deutel, N. Nguyen, A. Langley, F. Korn, L. Rossazza, A. Ramé, S. Waghmare, H. Miller, N. Byrd, A. Sheshan, R. H. S. Bhardwaj, P. Janus, T. Rissa, D. Horgan, S. Silver, A. Wahid, S. Brin, Y. Raimond, K. Kloboves, C. Wang, N. B. Gundavarapu, I. Shumailov, B. Wang, M. Pajarskas, J. Heyward, M. Nikoltchev, M. Kula, H. Zhou, Z. Garrett, S. Kafle, S. Arik, A. Goel, M. Yang, J. Park, K. Kojima, P. Mahmoudieh, K. Kavukcuoglu, G. Chen, D. Fritz, A. Bulyenov, S. Roy, D. Paparas, H. Shemtov, B.-J. Chen, R. Strudel, D. Reitter, A. Roy, A. Vlasov, C. Ryu, C. Leichner, H. Yang, Z. Mariet, D. Vnukov, T. Sohn, A. Stuart, W. Liang, M. Chen, P. Rawlani, C. Koh, J. Co-Reyes, G. Lai, P. Banzal, D. Vytiniotis, J. Mei, M. Cai, M. Badawi, C. Fry, A. Hartman, D. Zheng, E. Jia, J. Keeling, A. Louis, Y. Chen, E. Robles, W.-C. Hung, H. Zhou, N. Saxena, S. Goenka, O. Ma, Z. Fisher, M. H. Taege, E. Graves, D. Steiner, Y. Li, S. Nguyen, R. Sukthankar, J. Stanton, A. Eslami, G. Shen, B. Akin, A. Guseynov, Y. Zhou, J.-B. Alayrac, A. Joulin, E. Farkash, A. Thapliyal, S. Roller, N. Shazeer, T. Davchev, T. Koo, H. Forbes-Pollard, K. Audhkhasi, G. Farquhar, A. M. Gilady, M. Song, J. Aslanides, P. Mendolicchio, A. Parrish, J. Blitzer, P. Gupta, X. Ju, X. Yang, P. Datta, A. Tacchetti, S. V. Mehta, G. Dibb, S. Gupta, F. Piccinini, R. Hadsell, S. Rajayogam, J. Jiang, P. Griffin, P. Sundberg, J. Hayes, A. Frolov, T. Xie, A. Zhang, K. Dasgupta, U. Kalra, L. Shani, K. Macherey, T.-K. Huang, L. MacDermed, K. Duddu, P. Zacchello, Z. Yang, J. Lo, K. Hui, M. Kastelic, D. Gasaway, Q. Tan, S. Yue, P. Barrio, J. Wieting, W. Yang, A. Nystrom, S. Demmessie, A. Levskaya, F. Viola, C. Tekur, G. Billock, G. Necula, M. Joshi, R. Schaeffer, S. Lokhande, C. Sorokin, P. Shenoy, M. Chen, M. Collier, H. Li, T. Bos, N. Wichers, S. J. Lee, A. Pouget, S. Thangaraj, K. Axiotis, P. Crone, R. Sterneck, N. Chinaev, V. Krakovna, O. Ferludin, I. Gemp, S. Winkler, D. Goldberg, I. Korotkov, K. Xiao, M. Mehrotra, S. Mariserla, V. Piratla, T. Thurk, K. Pham, H. Ma, A. Senges, R. Kumar, C. Meyer, E. Talius, N. W. Pierse, B. Sandhu, H. Toma, K. Lin, S. Nath, T. Stone, D. Sadigh, N. Gupta, A. Guez, A. Singh, M. Thomas, T. Duerig, Y. Gong, R. Tanburn, L. L. Zhang, P. Dao, M. Hammad, S. Xie, S. Rijhwani, B. Murdoch, D. Kim, W. Thompson, H.-T. Cheng, D. Sohn, P. Sprechmann, Q. Xu, S. Tadepalli, P. Young, Y. Zhang, H. Srinivasan, M. Aperghis, A. Ayyar, H. Fitoussi, R. Burnell, D. Madras, M. Dusenberry, X. Xiong, T. Oguntebi, B. Albrecht, J. Bornschein, J. Mitrović, M. Dimarco, B. K. Shamanna, P. Shah, E. Sezener, S. Upadhyay, D. Lacey, C. Schiff, S. Baur, S. Ganapathy, E. Schnider, M. Wirth, C. Schenck, A. Simanovsky, Y.-X. Tan, P. Fränken, D. Duan, B. Mankalale, N. Dhawan, K. Sequeira, Z. Wei, S. Goel, C. Unlu, Y. Zhu, H. Sun, A. Balashankar, K. Shuster, M. Umekar, M. Alnahlawi, A. van den Oord, K. Chen, Y. Zhai, Z. Dai, K.-H. Lee, E. Doi, L. Zilka, R. Vallu, D. Shrivastava, J. Lee, H. Husain, H. Zhuang, V. Cohen-Addad, J. Barber, J. Atwood, A. Sadovsky, Q. Wellens, S. Hand, A. Rajendran, A. Turker, C. Carey, Y. Xu, H. Soltau, Z. Li, X. Song, C. Li, I. Kemaev, S. Brown, A. Burns, V. Patraucean, P. Stanczyk, R. Aravamudhan, 10 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers M. Blondel, H. Noga, L. Blanco, W. Song, M. Isard, M. Sharma, R. Hayes, D. E. Badawy, A. Lamp, I. Laish, O. Kozlova, K. Chan, S. Singla, S. Sunkara, M. Upadhyay, C. Liu, A. Bai, J. Wilkiewicz, M. Zlocha, J. Liu, Z. Li, H. Li, O. Barak, G. Raboshchuk, J. Choi, F. Liu, E. Jue, M. Sharma, A. Marzoca, R. Busa-Fekete, A. Korsun, A. Elisseeff, Z. Shen, S. M. Carthy, K. Lamerigts, A. Hosseini, H. Lin, C. Chen, F. Yang, K. Chauhan, M. Omernick, D. Jia, K. Zainullina, D. Hassabis, D. Vainstein, E. Amid, X. Zhou, R. Votel, E. Vértes, X. Li, Z. Zhou, A. Lazaridou, B. McMahan, A. Narayanan, H. Soyer, S. Basu, K. Lee, B. Perozzi, Q. Cao, L. Berrada, R. Arya, K. Chen, Katrina, Xu, M. Lochbrunner, A. Hofer, S. Sharifzadeh, R. Wu, S. Goldman, P. Awasthi, X. Wang, Y. Wu, C. Sha, B. Zhang, M. Mikuła, F. Graziano, S. Mcloughlin, I. Giannoumis, Y. Namiki, C. Malik, C. Radebaugh, J. Hall, R. Leal-Cavazos, J. Chen, V. Sindhwani, D. Kao, D. Greene, J. Griffith, C. Welty, C. Montgomery, T. Yoshino, L. Yuan, N. Goodman, A. H. Michaely, K. Lee, K. Sawhney, W. Chen, Z. Zheng, M. Shum, N. Savinov, E. Pot, A. Pak, M. Zadimoghaddam, S. Bhatnagar, Y. Lewenberg, B. Kutzman, J. Liu, L. Katzen, J. Selier, J. Djolonga, D. Lepikhin, K. Xu, J. Liang, J. Tan, B. Schillings, M. Ersoy, P. Blois, B. Bandemer, A. Singh, S. Lebedev, P. Joshi, A. R. Brown, E. Palmer, S. Pathak, K. Jalan, F. Zubach, S. Lall, R. Parker, A. Gunjan, S. Rogulenko, S. Sanghai, Z. Leng, Z. Egyed, S. Li, M. Ivanova, K. Andriopoulos, J. Xie, E. Rosenfeld, A. Wright, A. Sharma, X. Geng, Y. Wang, S. Kwei, R. Pan, Y. Zhang, G. Wang, X. Liu, C. Yeung, E. Cole, A. Rosenberg, Z. Yang, P. Chen, G. Polovets, P. Nair, R. Saxena, J. Smith, S. yiin Chang, A. Mahendru, S. Grant, A. Iyer, I. Cai, J. McGiffin, J. Shen, A. Walton, A. Girgis, O. Woodman, R. Ke, M. Kwong, L. Rouillard, J. Rao, Z. Li, Y. Xu, F. Prost, C. Zou, Z. Ji, A. Magni, T. Liechty, D. A. Calian, D. Ramachandran, I. Krivokon, H. Huang, T. Chen, A. Hauth, A. Ilić, W. Xi, H. Lim, V.-D. Ion, P. Moradi, M. Toksoz-Exley, K. Bullard, M. Allamanis, X. Yang, S. Wang, Z. Hong, A. Gergely, C. Li, B. Mittal, V. Kovalev, V. Ungureanu, J. Labanowski, J. Wassenberg, N. Lacasse, G. Cideron, P. Dević, A. Marsden, L. Nguyen, M. Fink, Y. Zhong, T. Kiyono, D. Ivanov, S. Ma, M. Bain, K. Yalasangi, J. She, A. Petrushkina, M. Lunayach, C. Bromberg, S. Hodkinson, V. Meshram, D. Vlasic, A. Kyker, S. Xu, J. Stanway, Z. Yang, K. Zhao, M. Tung, S. Odoom, Y. Fujii, J. Gilmer, E. Kim, F. Halim, Q. Le, B. Bohnet, S. El-Sayed, B. Neyshabur, M. Reynolds, D. Reich, Y. Xu, E. Moreira, A. Sharma, Z. Liu, M. J. Hosseini, N. Raisinghani, Y. Su, N. Lao, D. Formoso, M. Gelmi, A. Gueta, T. Dey, E. Gribovskaya, D. Ćevid, S. Mudgal, G. Bingham, J. Wang, A. Kumar, A. Cullum, F. Han, K. Bousmalis, D. Cedillo, G. Chu, V. Magay, P. Michel, E. Hlavnova, D. Calandriello, S. Ariafar, K. Yao, V. Sehwag, A. Vezer, A. D. Lago, Z. Zhu, P. K. Rubenstein, A. Porter, A. Baddepudi, O. Riva, M. D. Istin, C.-K. Yeh, Z. Li, A. Howard, N. Jha, J. Chen, R. de Liedekerke, Z. Ahmed, M. Rodriguez, T. Bhatia, B. Wang, A. Elqursh, D. Klinghoffer, P. Chen, P. Kohli, T. I, W. Zhang, Z. Nado, J. Chen, M. Chen, G. Zhang, A. Singh, A. Hillier, F. Lebron, Y. Tao, T. Liu, G. Dulac-Arnold, J. Zhang, S. Narayan, B. Liu, O. Firat, A. Bhowmick, B. Liu, H. Zhang, Z. Zhang, G. Rotival, N. Howard, A. Sinha, A. Grushetsky, B. Beyret, K. Gopalakrishnan, J. Zhao, K. He, S. Payrits, Z. Nabulsi, Z. Zhang, W. Chen, E. Lee, N. Fallen, S. Gollapudi, A. Zhou, F. Pavetić, T. Köppe, S. Huang, R. Pasumarthi, N. Fernando, F. Fischer, D. Ćurko, Y. Gao, J. Svensson, A. Stone, H. Qureshi, A. Sinha, A. Kulshreshtha, M. Matysiak, J. Mao, C. Saroufim, A. Faust, Q. Duan, G. Fidel, K. Katircioglu, R. L. Kaufman, D. Shah, W. Kong, A. Bapna, G. Weisz, E. Dunleavy, P. Dutta, T. Liu, R. Chaabouni, C. Parada, M. Wu, A. Belias, A. Bissacco, S. Fort, L. Xiao, F. Huot, C. Knutsen, Y. Blau, G. Li, J. Prendki, J. Love, Y. Chow, P. Charoenpanit, H. Shimokawa, V. Coriou, K. Gregor, T. Izo, A. Akula, M. Pinto, C. Hahn, D. Paulus, J. Guo, N. Sharma, C.-J. Hsieh, A. Chukwuka, K. Hashimoto, N. Rauschmayr, L. Wu, C. Angermueller, Y. Wang, S. Gerlach, M. Pliskin, D. Mirylenka, M. Ma, L. Baugher, B. Gale, S. Bijwadia, N. Rakićević, D. Wood, J. Park, C.-C. Chang, B. Seal, C. Tar, K. Krasowiak, Y. Song, G. Stephanov, G. Wang, M. Maggioni, S. X. Lin, F. Wu, S. Paul, Z. Jiang, S. Agrawal, B. Piot, A. Feng, C. Kim, T. Doshi, J. Lai, Chuqiao, Xu, S. Vikram, C. Chelba, S. Krause, V. Zhuang, J. Rae, T. Denk, A. Collister, L. Weerts, X. Luo, Y. Lu, H. Garnes, N. Gupta, T. Spitz, A. Hassidim, L. Liang, 11 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers I. Shafran, P. Humphreys, K. Vassigh, P. Wallis, V. Shejwalkar, N. Perez-Nieves, R. Hornung, M. Tan, B. Westberg, A. Ly, R. Zhang, B. Farris, J. Park, A. Kosik, Z. Cankara, A. Maksai, Y. Xu, A. Cassirer, S. Caelles, A. Abdolmaleki, M. Chiang, A. Fabrikant, S. Shetty, L. He, M. Giménez, H. Hashemi, S. Panthaplackel, Y. Kulizhskaya, S. Deshmukh, D. Pighin, R. Alazard, D. Jindal, S. Noury, P. K. S, S. Qin, X. Dotiwalla, S. Spencer, M. Babaeizadeh, B. J. Chen, V. Mehta, J. Lees, A. Leach, P. Koanantakool, I. Akolzin, R. Comanescu, J. Ahn, A. Svyatkovskiy, B. Mustafa, D. DAmbrosio, S. M. R. Garlapati, P. Lamblin, A. Agarwal, S. Song, P. G. Sessa, P. Coquinot, J. Maggs, H. Masoom, D. Pitta, Y. Wang, P. Morris-Suzuki, B. Porter, J. Jia, J. Dudek, R. R, C. Paduraru, A. Ansell, T. Bolukbasi, T. Lu, R. Ganeshan, Z. Wang, H. Griffiths, R. Benenson, Y. He, J. Swirhun, G. Papamakarios, A. Chawla, K. Sengupta, Y. Wang, V. Milutinovic, I. Mordatch, Z. Jia, J. Smith, W. Ng, S. Nigam, M. Young, E. Vušak, B. Hechtman, S. Goenka, A. Zipori, K. Ayoub, A. Popat, T. Acharya, L. Yu, D. Bloxwich, H. Song, P. Roit, H. Li, A. Boag, N. Nayakanti, B. Chandra, T. Ding, A. Mehta, C. Hope, J. Zhang, I. H. Shtacher, K. Badola, R. Nakashima, A. Sozanschi, I. Comşa, A. Žužul, E. Caveness, J. Odell, M. Watson, D. de Cesare, P. Lippe, D. Lockhart, S. Verma, H. Chen, S. Sun, L. Zhuo, A. Shah, P. Gupta, A. Muzio, N. Niu, A. Zait, A. Singh, M. Gaba, F. Ye, P. Ramachandran, M. Saleh, R. A. Popa, A. Dubey, F. Liu, S. Javanmardi, M. Epstein, R. Hemsley, R. Green, N. Ranka, E. Cohen, C. K. Fu, S. Ghemawat, J. Borovik, J. Martens, A. Chen, P. Shyam, A. S. Pinto, M.-H. Yang, A. Ţifrea, D. Du, B. Gong, A. Agarwal, S. Kim, C. Frank, S. Shah, X. Song, Z. Deng, A. Mikhalap, K. Chatziprimou, T. Chung, T. Creswell, S. Zhang, Y. Jun, C. Lebsack, W. Truong, S. Andačić, I. Yona, M. Fornoni, R. Rong, S. Toropov, A. S. Soudagar, A. Audibert, S. Zaiem, Z. Abbas, A. Rusu, S. Potluri, S. Weng, A. Kementsietsidis, A. Tsitsulin, D. Peng, N. Ha, S. Jain, T. Latkar, S. Ivanov, C. McLean, A. GP, R. Venkataraman, C. Liu, D. Krishnan, J. Dsa, R. Yogev, P. Collins, B. Lee, L. Ho, C. Doersch, G. Yona, S. Gao, F. T. Ferreira, A. Ozturel, H. Muckenhirn, C. Zheng, G. Balasubramaniam, M. Bansal, G. van den Driessche, S. Eiger, S. Haykal, V. Misra, A. Goyal, D. Martins, G. Leung, J. Valfridsson, F. Flynn, W. Bishop, C. Pang, Y. Halpern, H. Yu, L. Moore, Yuvein, Zhu, S. Thiagarajan, Y. Drori, Z. Xiao, L. Dery, R. Jagerman, J. Lu, E. Ge, V. Aggarwal, A. Khare, V. Tran, O. Elyada, F. Alet, J. Rubin, I. Chou, D. Tian, L. Bai, L. Chan, L. Lew, K. Misiunas, T. Bilal, A. Ray, S. Raghuram, A. Castro-Ros, V. Carpenter, C. Zheng, M. Kilgore, J. Broder, E. Xue, P. Kallakuri, D. Dua, N. Yuen, S. Chien, J. Schultz, S. Agrawal, R. Tsarfaty, J. Hu, A. Kannan, D. Marcus, N. Kothari, B. Sun, B. Horn, M. Bošnjak, F. Naeem, D. Hirsch, L. Chiang, B. Fang, J. Han, Q. Wang, B. Hora, A. He, M. Lučić, B. Changpinyo, A. Tripathi, J. Youssef, C. Kwak, P. Schlattner, C. Graves, R. Leblond, W. Zeng, A. Andreassen, G. Rasskin, Y. Song, E. Cao, J. Oh, M. Hoffman, W. Skut, Y. Zhang, J. Stritar, X. Cai, S. Khanna, K. Wang, S. Sharma, C. Reisswig, Y. Jun, A. Prasad, T. Sholokhova, P. Singh, A. G. Rosenthal, A. Ruoss, F. Beaufays, S. Kirmani, D. Chen, J. Schalkwyk, J. Herzig, B. Kim, J. Jacob, D. Vincent, A. N. Reyes, I. Balazevic, L. Hussenot, J. Schneider, P. Barnes, L. Castro, S. R. Babbula, S. Green, S. Cabi, N. Duduta, D. Driess, R. Galt, N. Velan, J. Wang, H. Jiao, M. Mauger, D. Phan, M. Patel, V. Galić, J. Chang, E. Marcus, M. Harvey, J. Salazar, E. Dabir, S. S. Sheth, A. Mandhane, H. Sedghi, J. Willcock, A. Zandieh, S. Prabhakara, A. Amini, A. Miech, V. Stone, M. Nicosia, P. Niemczyk, Y. Xiao, L. Kim, S. Kwasiborski, V. Verma, A. M. Oflazer, C. Hirnschall, P. Sung, L. Liu, R. Everett, M. Bakker, Ágoston Weisz, Y. Wang, V. Sampathkumar, U. Shaham, B. Xu, Y. Altun, M. Wang, T. Saeki, G. Chen, E. Taropa, S. Vasanth, S. Austin, L. Huang, G. Petrovic, Q. Dou, D. Golovin, G. Rozhdestvenskiy, A. Culp, W. Wu, M. Sano, D. Jain, J. Proskurnia, S. Cevey, A. C. Ruiz, P. Patil, M. Mirzazadeh, E. Ni, J. Snaider, L. Fan, A. Fréchette, A. Pierigiovanni, S. Iqbal, K. Lee, C. Fantacci, J. Xing, L. Wang, A. Irpan, D. Raposo, Y. Luan, Z. Chen, H. Ganapathy, K. Hui, J. Nie, I. Guyon, H. Ge, R. Vij, H. Zheng, D. Lee, A. Castaño, K. Baatarsukh, G. Ibagon, A. Chronopoulou, N. FitzGerald, S. Viswanadha, S. Huda, R. Moroshko, G. Stoyanov, P. Kolhar, A. Vaucher, I. Watts, A. Kuncoro, H. Michalewski, S. Kambala, B.-O. Batsaikhan, A. Andreev, I. Jurenka, M. Le, Q. Chen, W. A. Jishi, S. Chakera, Z. Chen, A. Kini, 12 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers V. Yadav, A. Siddhant, I. Labzovsky, B. Lakshminarayanan, C. G. Bostock, P. Botadra, A. Anand, C. Bishop, S. Conway-Rahman, M. Agarwal, Y. Donchev, A. Singhal, F. de Chaumont Quitry, N. Ponomareva, N. Agrawal, B. Ni, K. Krishna, M. Samsikova, J. Karro, Y. Du, T. von Glehn, C. Lu, C. A. Choquette-Choo, Z. Qin, T. Zhang, S. Li, D. Tyam, S. Mishra, W. Lowe, C. Ji, W. Wang, M. Faruqui, A. Slone, V. Dalibard, A. Narayanaswamy, J. Lambert, P.-A. Manzagol, D. Karliner, A. Bolt, I. Lobov, A. Kusupati, C. Ye, X. Yang, H. Zen, N. George, M. Bhutani, O. Lacombe, R. Riachi, G. Bansal, R. Soh, Y. Gao, Y. Yu, A. Yu, E. Nottage, T. Rojas-Esponda, J. Noraky, M. Gupta, R. Kotikalapudi, J. Chang, S. Deur, D. Graur, A. Mossin, E. Farnese, R. Figueira, A. Moufarek, A. Huang, P. Zochbauer, B. Ingram, T. Chen, Z. Wu, A. Puigdomènech, L. Rechis, D. Yu, S. G. S. Padmanabhan, R. Zhu, C. ling Ko, A. Banino, S. Daruki, A. Selvan, D. Bhaswar, D. H. Diaz, C. Su, S. Scellato, J. Brennan, W. Han, G. Chung, P. Agrawal, U. Khandelwal, K. C. Sim, M. Lustman, S. Ritter, K. Guu, J. Xia, P. Jain, E. Wang, T. Hill, M. Rossini, M. Kostelac, T. Misiunas, A. Sabne, K. Kim, A. Iscen, C. Wang, J. Leal, A. Sreevatsa, U. Evci, M. Warmuth, S. Joshi, D. Suo, J. Lottes, G. Honke, B. Jou, S. Karp, J. Hu, H. Sahni, A. A. Taïga, W. Kong, S. Ghosh, R. Wang, J. Pavagadhi, N. Axelsson, N. Grigorev, P. Siegler, R. Lin, G. Wang, E. Parisotto, S. Maddineni, K. Subudhi, E. Ben-David, E. Pochernina, O. Keller, T. Avrahami, Z. Yuan, P. Mehta, J. Liu, S. Yang, W. Kan, K. Lee, T. Funkhouser, D. Cheng, H. Shi, A. Sharma, J. Kelley, M. Eyal, Y. Malkov, C. Tallec, Y. Bahat, S. Yan, Xintian, Wu, D. Lindner, C. Wu, A. Caciularu, X. Luo, R. Jenatton, T. Zaman, Y. Bi, I. Kornakov, G. Mallya, D. Ikeda, I. Karo, A. Singh, C. Evans, P. Netrapalli, V. Nallatamby, I. Tian, Y. Assael, V. Raunak, V. Carbune, I. Bica, L. Madmoni, D. Cattle, S. Grover, K. Somandepalli, S. Lall, A. Vázquez-Reina, R. Patana, J. Mu, P. Talluri, M. Tran, R. Aggarwal, R. Skerry-Ryan, J. Xu, M. Burrows, X. Pan, E. Yvinec, D. Lu, Z. Zhang, D. D. Nguyen, H. Mu, G. Barcik, H. Ran, L. Beltrone, K. Choromanski, D. Kharrat, S. Albanie, S. Purser-haskell, D. Bieber, C. Zhang, J. Wang, T. Hudson, Z. Zhang, H. Fu, J. Mauerer, M. H. Bateni, A. Maschinot, B. Wang, M. Zhu, A. Pillai, T. Weyand, S. Liu, O. Akerlund, F. Bertsch, V. Premachandran, A. Jin, V. Roulet, P. de Boursac, S. Mittal, N. Ndebele, G. Karadzhov, S. Ghalebikesabi, R. Liang, A. Wu, Y. Cong, N. Ghelani, S. Singh, B. Fatemi, Warren, Chen, C. Kwong, A. Kolganov, S. Li, R. Song, C. Kuang, S. Miryoosefi, D. Webster, J. Wendt, A. Socala, G. Su, A. Mendonça, A. Gupta, X. Li, T. Tsai, Qiong, Hu, K. Kang, A. Chen, S. Girgin, Y. Xian, A. Lee, N. Ramsden, L. Baker, M. C. Elish, V. Krayvanova, R. Joshi, J. Simsa, Y.-Y. Yang, P. Ambroszczyk, D. Ghosh, A. Kar, Y. Shangguan, Y. Yamamori, Y. Akulov, A. Brock, H. Tang, S. Vashishtha, R. Munoz, A. Steiner, K. Andra, D. Eppens, Q. Feng, H. Kobayashi, S. Goldshtein, M. E. Mahdy, X. Wang, Jilei, Wang, R. Killam, T. Kwiatkowski, K. Kopparapu, S. Zhan, C. Jia, A. Bendebury, S. Luo, A. Recasens, T. Knight, J. Chen, M. Patel, Y. Li, B. Withbroe, D. Weesner, K. Bhatia, J. Ren, D. Eisenbud, E. Songhori, Y. Sun, T. Choma, T. Kementsietsidis, L. Manning, B. Roark, W. Farhan, J. Feng, S. Tatineni, J. Cobon-Kerr, Y. Li, L. A. Hendricks, I. Noble, C. Breaux, N. Kushman, L. Peng, F. Xue, T. Tobin, J. Rogers, J. Lipschultz, C. Alberti, A. Vlaskin, M. Dehghani, R. Sharma, T. Warkentin, C.-Y. Lee, B. Uria, D.-C. Juan, A. Chandorkar, H. Sheftel, R. Liu, E. Davoodi, B. D. B. Pigem, K. Dhamdhere, D. Ross, J. Hoech, M. Mahdieh, L. Liu, Q. Li, L. McCafferty, C. Liu, M. Mircea, Y. Song, O. Savant, A. Saade, C. Cherry, V. Hellendoorn, S. Goyal, P. Pucciarelli, D. V. Torres, Z. Yahav, H. Lee, L. L. Sjoesund, C. Kirov, B. Chang, D. Ghoshal, L. Li, G. Baechler, S. Pereira, T. Sainath, A. Boral, D. Grewe, A. Halumi, N. M. Phu, T. Shen, M. T. Ribeiro, D. Varma, A. Kaskasoli, V. Feinberg, N. Potti, J. Kahn, M. Wisniewski, S. Mohamed, A. M. Hrafnkelsson, B. Shahriari, J.-B. Lespiau, L. Patel, L. Yeung, T. Paine, L. Mei, A. Ramirez, R. Shivanna, L. Zhong, J. Woodward, G. Tubone, S. Khan, H. Chen, E. Nielsen, C. Ionescu, U. Prabhu, M. Gao, Q. Wang, S. Augenstein, N. Subramaniam, J. Chang, F. Iliopoulos, J. Luo, M. Khan, W. Kuo, D. Teplyashin, F. Perot, L. Kilpatrick, A. Globerson, H. Yu, A. Siddiqui, N. Sukhanov, A. Kandoor, U. Gupta, M. Andreetto, M. Ambar, D. Kim, P. Wesołowski, S. Perrin, B. Limonchik, W. Fan, J. Stephan, I. Stewart-Binks, R. Kappedal, T. He, S. Cogan, R. Datta, T. Zhou, J. Ye, L. Kieliger, A. Ramalho, K. Kastner, F. Mentzer, W.-J. Ko, A. Suggala, 13 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers T. Zhou, S. Butt, H. Strejček, L. Belenki, S. Venugopalan, M. Ling, E. Eltyshev, Y. Deng, G. Kovacs, M. Raghavachari, H. Dai, T. Schuster, S. Schwarcz, R. Nguyen, A. Nguyen, G. Buttimore, S. B. Mallick, S. Gandhe, S. Benjamin, M. Jastrzebski, L. Yan, S. Basu, C. Apps, I. Edkins, J. Allingham, I. Odisho, T. Kocisky, J. Zhao, L. Xue, A. Reddy, C. Anastasiou, A. Atias, S. Redmond, K. Milan, N. Heess, H. Schmit, A. Dafoe, D. Andor, T. Gangwani, A. Dragan, S. Zhang, A. Kachra, G. Wu, S. Xue, K. Aydin, S. Liu, Y. Zhou, M. Malihi, A. Wu, S. Gopal, C. Schumann, P. Stys, A. Wang, M. Olšák, D. Liu, C. Schallhart, Y. Mao, D. Brady, H. Xu, T. Mery, C. Sitawarin, S. Velusamy, T. Cobley, A. Zhai, C. Walder, N. Katz, G. Jawahar, C. Kulkarni, A. Yang, A. Paszke, Y. Wang, B. Damoc, Z. Borsos, R. Smith, J. Li, M. Gupta, A. Kapishnikov, S. Prakash, F. Luisier, R. Agarwal, W. Grathwohl, K. Chen, K. Han, N. Mehta, A. Over, S. Azizi, L. Meng, N. D. Santo, K. Zheng, J. Shapiro, I. Petrovski, J. Hui, A. Ghafouri, J. Snoek, J. Qin, M. Jordan, C. Sikora, J. Malmaud, Y. Kuang, A. Świetlik, R. Sang, C. Shi, L. Li, A. Rosenberg, S. Zhao, A. Crawford, J.-T. Peter, Y. Lei, X. Garcia, L. Le, T. Wang, J. Amelot, D. Orr, P. Kacham, D. Alon, G. Tyen, A. Arora, J. Lyon, A. Kurakin, M. Ly, T. Guidroz, Z. Yan, R. Panigrahy, P. Xu, T. Kagohara, Y. Cheng, E. Noland, J. Lee, J. Lee, C. Yip, M. Wang, E. Nehoran, A. Bykovsky, Z. Shan, A. Bhagatwala, C. Yan, J. Tan, G. Garrido, D. Ethier, N. Hurley, G. Vesom, X. Chen, S. Qiao, A. Nayyar, J. Walker, P. Sandhu, M. Rosca, D. Swisher, M. Dektiarev, J. Dillon, G.-C. Muraru, M. Tragut, A. Myaskovsky, D. Reid, M. Velic, O. Xiao, J. George, M. Brand, J. Li, W. Yu, S. Gu, X. Deng, F.-X. Aubet, S. H. Yeganeh, F. Alcober, C. Smith, T. Cohn, K. McKinney, M. Tschannen, R. Sampath, G. Cheon, L. Luo, L. Liu, J. Orbay, H. Peng, G. Botea, X. Zhang, C. Yoon, C. Magalhaes, P. Stradomski, I. Mackinnon, S. Hemingray, K. Venkatesan, R. May, J. Kim, A. Druinsky, J. Ye, Z. Xu, T. Huang, J. A. Abdallah, A. Dostmohamed, R. Fellinger, T. Munkhdalai, A. Maurya, P. Garst, Y. Zhang, M. Krikun, S. Bucher, A. S. Veerubhotla, Y. Liu, S. Li, N. Gupta, J. Adamek, H. Chen, B. Orlando, A. Zaks, J. van Amersfoort, J. Camp, H. Wan, H. Choe, Z. Wu, K. Olszewska, W. Yu, A. Vadali, M. Scholz, D. D. Freitas, J. Lin, A. Hua, X. Liu, F. Ding, Y. Zhou, B. Severson, K. Tsihlas, S. Yang, T. Spalink, V. Yerram, H. Pankov, R. Blevins, B. Vargas, S. Jauhari, M. Miecnikowski, M. Zhang, S. Kumar, C. Farabet, C. L. Lan, S. Flennerhag, Y. Bitton, A. Ma, A. Bražinskas, E. Collins, N. Ahuja, S. Kudugunta, A. Bortsova, M. Giang, W. Zhu, E. Chi, S. Lundberg, A. Stern, S. Puttagunta, J. Xiong, X. Wu, Y. Pande, A. Jhindal, D. Murphy, J. Clark, M. Brockschmidt, M. Deines, K. R. McKee, D. Bahir, J. Shen, M. Truong, D. McDuff, A. Gesmundo, E. Rosseel, B. Liang, K. Caluwaerts, J. Hamrick, J. Kready, M. Cassin, R. Ingale, L. Lao, S. Pollom, Y. Ding, W. He, L. Bellot, J. Iljazi, R. S. Boppana, S. Han, T. Thompson, A. Khalifa, A. Bulanova, B. Mitrevski, B. Pang, E. Cooney, T. Shi, R. Coaguila, T. Yakar, M. Ranzato, N. Momchev, C. Rawles, Z. Charles, Y. Maeng, Y. Zhang, R. Bansal, X. Zhao, B. Albert, Y. Yuan, S. Vijayanarasimhan, R. Hirsch, V. Ramasesh, K. Vodrahalli, X. Wang, A. Gupta, D. Strouse, J. Ni, R. Patel, G. Taubman, Z. Huo, D. Gharibian, M. Monteiro, H. Lam, S. Vasudevan, A. Chaudhary, I. Albuquerque, K. Gupta, S. Riedel, C. Hegde, A. Ruderman, A. György, M. Wainwright, A. Chaugule, B. K. Ayan, T. Levinboim, S. Shleifer, Y. Kalley, V. Mirrokni, A. Rao, P. Radhakrishnan, J. Hartford, J. Wu, Z. Zhu, F. Bertolini, H. Xiong, N. Serrano, H. Tomlinson, M. Ott, Y. Chang, M. Graham, J. Li, M. Liang, X. Long, S. Borgeaud, Y. Ahmad, A. Grills, D. Mincu, M. Izzard, Y. Liu, J. Xie, L. OBryan, S. Ponda, S. Tong, M. Liu, D. Malkin, K. Salama, Y. Chen, R. Anil, A. Rao, R. Swavely, M. Bilenko, N. Anderson, T. Tan, J. Xie, X. Wu, L. Yu, O. Vinyals, A. Ryabtsev, R. Dangovski, K. Baumli, D. Keysers, C. Wright, Z. Ashwood, B. Chan, A. Shtefan, Y. Guo, A. Bapna, R. Soricut, S. Pecht, S. Ramos, R. Wang, J. Cai, T. Trinh, P. Barham, L. Friso, E. Stickgold, X. Ding, S. Shakeri, D. Ardila, E. Briakou, P. Culliton, A. Raveret, J. Cui, D. Saxton, S. Roy, J. Azizi, P. Yin, L. Loher, A. Bunner, M. Choi, F. Ahmed, E. Li, Y. Li, S. Dai, M. Elabd, S. Ganapathy, S. Agrawal, Y. Hua, P. Kunkle, S. Rajayogam, A. Ahuja, A. Conmy, A. Vasiloff, P. Beak, C. Yew, J. Mudigonda, B. Wydrowski, J. Blanton, Z. Wang, Y. Dauphin, Z. Xu, M. Polacek, X. Chen, H. Hu, P. Sho, M. Kunesch, M. H. Manshadi, E. Rutherford, B. Li, S. Hsiao, I. Barr, A. Tudor, M. Kecman, 14 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers A. Nagrani, V. Pchelin, M. Sundermeyer, A. P. S, A. Karmarkar, Y. Gao, G. Chole, O. Bachem, I. Gao, A. BC, M. Dibb, M. Verzetti, F. Hernandez-Campos, Y. Lunts, M. Johnson, J. D. Trapani, R. Koster, I. Brusilovsky, B. Xiong, M. Mohabey, H. Ke, J. Zou, T. Sabolić, V. Campos, J. Palowitch, A. Morris, L. Qiu, P. Ponnuramu, F. Li, V. Sharma, K. Sodhia, K. Tekelioglu, A. Chuklin, M. Yenugula, E. Gemzer, T. Strinopoulos, S. El-Husseini, H. Wang, Y. Zhong, E. Leurent, P. Natsev, W. Wang, D. Mahaarachchi, T. Zhu, S. Peng, S. Alabed, C.-C. Lee, A. Brohan, A. Szlam, G. Oh, A. Kovsharov, J. Lee, R. Wong, M. Barnes, G. Thornton, F. Gimeno, O. Levy, M. Sevenich, M. Johnson, J. Mallinson, R. Dadashi, Z. Wang, Q. Ren, P. Lahoti, A. Dhar, J. Feldman, D. Zheng, T. Ulrich, L. Panait, M. Blokzijl, C. Baetu, J. Matak, J. Harlalka, M. Shah, T. Marian, D. von Dincklage, C. Du, R. Ley-Wild, B. Brownfield, M. Schumacher, Y. Stuken, S. Noghabi, S. Gupta, X. Ren, E. Malmi, F. Weissenberger, B. Huergo, M. Bauza, T. Lampe, A. Douillard, M. Seyedhosseini, R. Frostig, Z. Ghahramani, K. Nguyen, K. Krishnakumar, C. Ye, R. Gupta, A. Nazari, R. Geirhos, P. Shaw, A. Eleryan, D. Damen, J. Palomaki, T. Xiao, Q. Wu, Q. Yuan, P. Meadowlark, M. Bilotti, R. Lin, M. Sridhar, Y. Schroecker, D.-W. Chung, J. Luo, T. Strohman, T. Liu, A. Zheng, J. Emond, W. Wang, A. Lampinen, T. Fukuzawa, F. Campbell-Ajala, M. Roy, J. Lee-Thorp, L. Wang, I. Naim, Tony, N. ên, G. Bensky, A. Gupta, D. Rogozińska, J. Fu, T. S. Pillai, P. Veličković, S. Drath, P. Neubeck, V. Tulsyan, A. Klimovskiy, D. Metzler, S. Stevens, A. Yeh, J. Yuan, T. Yu, K. Zhang, A. Go, V. Tsang, Y. Xu, A. Wan, I. Galatzer-Levy, S. Sobell, A. Toki, E. Salesky, W. Zhou, D. Antognini, S. Douglas, S. Wu, A. Lelkes, F. Kim, P. Cavallaro, A. Salazar, Y. Liu, J. Besley, T. Refice, Y. Jia, Z. Li, M. Sokolik, A. Kannan, J. Simon, J. Chick, A. Aharon, M. Gandhi, M. Daswani, K. Amiri, V. Birodkar, A. Ittycheriah, P. Grabowski, O. Chang, C. Sutton, Zhixin, Lai, U. Telang, S. Sargsyan, T. Jiang, R. Hoffmann, N. Brichtova, M. Hessel, J. Halcrow, S. Jerome, G. Brown, A. Tomala, E. Buchatskaya, D. Yu, S. Menon, P. Moreno, Y. Liao, V. Zayats, L. Tang, S. Mah, A. Shenoy, A. Siegman, M. Hadian, O. Kwon, T. Tu, N. Khajehnouri, R. Foley, P. Haghani, Z. Wu, V. Keshava, K. Gupta, T. Bruguier, R. Yao, D. Karmon, L. Zintgraf, Z. Wang, E. Piqueras, J. Jung, J. Brennan, D. Machado, M. Giustina, M. Tessler, K. Lee, Q. Zhang, J. Moore, K. Daugaard, A. Frömmgen, J. Beattie, F. Zhang, D. Kasenberg, T. Geri, D. Qin, G. S. Tomar, T. Ouyang, T. Yu, L. Zhou, R. Mathews, A. Davis, Y. Li, J. Gupta, D. Yates, L. Deng, E. Kemp, G.-Y. Joung, S. Vassilvitskii, M. Guo, P. LV, D. Dopson, S. Lachgar, L. McConnaughey, H. Choudhury, D. Dena, A. Cohen, J. Ainslie, S. Levi, P. Gopavarapu, P. Zablotskaia, H. Vallet, S. Bahargam, X. Tang, N. Tomasev, E. Dyer, D. Balle, H. Lee, W. Bono, J. G. Mendez, V. Zubov, S. Yang, I. Rendulic, Y. Zheng, A. Hogue, G. Pundak, R. Leith, A. Bhoopchand, M. Han, M. Žanić, T. Schaul, M. Delakis, T. Iyer, G. Wang, H. Singh, A. Abdelhamed, T. Thomas, S. Brahma, H. Dib, N. Kumar, W. Zhou, L. Bai, P. Mishra, J. Sun, V. Anklin, R. Sukkerd, L. Agubuzu, A. Briukhov, A. Gulati, M. Sieb, F. Pardo, S. Nasso, J. Chen, K. Zhu, T. Sosea, A. Goldin, K. Rush, S. A. Hombaiah, A. Noever, A. Zhou, S. Haves, M. Phuong, J. Ades, Y. ting Chen, L. Yang, J. Pagadora, S. Bileschi, V. Cotruta, R. Saputro, A. Pramanik, S. Ammirati, D. Garrette, K. Villela, T. Blyth, C. Akbulut, N. Jha, A. Rrustemi, A. Wongpanich, C. Nagpal, Y. Wu, M. Rivière, S. Kishchenko, P. Srinivasan, A. Chen, A. Sinha, T. Pham, B. Jia, T. Hennigan, A. Bakalov, N. Attaluri, D. Garmon, D. Rodriguez, D. Wegner, W. Jia, E. Senter, N. Fiedel, D. Petek, Y. Liu, C. Hardin, H. T. Lehri, J. Carreira, S. Smoot, M. Prasetya, N. Akazawa, A. Stefanoiu, C.-H. Ho, A. Angelova, K. Lin, M. Kim, C. Chen, M. Sieniek, A. Li, T. Guo, S. Baltateanu, P. Tafti, M. Wunder, N. Olmert, D. Shukla, J. Shen, N. Kovelamudi, B. Venkatraman, S. Neel, R. Thoppilan, J. Connor, F. Benzing, A. Stjerngren, G. Ghiasi, A. Polozov, J. Howland, T. Weber, J. Chiu, G. P. Girirajan, A. Terzis, P. Wang, F. Li, Y. B. Shalom, D. Tewari, M. Denton, R. Aharoni, N. Kalb, H. Zhao, J. Zhang, A. Filos, M. Rahtz, L. Jain, C. Fan, V. Rodrigues, R. Wang, R. Shin, J. Austin, R. Ring, M. SanchezVargas, M. Hassen, I. Kessler, U. Alon, G. Zhang, W. Chen, Y. Ma, X. Si, L. Hou, A. Mirhoseini, M. Wilson, G. Bacon, B. Roelofs, L. Shu, G. Vasudevan, J. Adler, A. Dwornik, T. Terzi, M. Lawlor, H. Askham, M. Bernico, X. Dong, C. Hidey, K. Kilgour, G. Liu, S. Bhupatiraju, L. Leonhard, 15 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers S. Zuo, P. Talukdar, Q. Wei, A. Severyn, V. Listík, J. Lee, A. Tripathi, S. Park, Y. Matias, H. Liu, A. Ruiz, R. Jayaram, J. Tolins, P. Marcenac, Y. Wang, B. Seybold, H. Prior, D. Sharma, J. Weber, M. Sirotenko, Y. Sung, D. Du, E. Pavlick, S. Zinke, M. Freitag, M. Dylla, M. G. Arenas, N. Potikha, O. Goldman, C. Tao, R. Chhaparia, M. Voitovich, P. Dogra, A. Ražnatović, Z. Tsai, C. You, O. Johnson, G. Tucker, C. Gu, J. Yoo, M. Majzoubi, V. Gabeur, B. Raad, R. Rhodes, K. Kolipaka, H. Howard, G. Sampemane, B. Li, C. Asawaroengchai, D. Nguyen, C. Zhang, T. Cour, X. Yu, Z. Fu, J. Jiang, P.-S. Huang, G. Surita, I. Iturrate, Y. Karov, M. Collins, M. Baeuml, F. Fuchs, S. Shetty, S. Ramaswamy, S. Ebrahimi, Q. Guo, J. Shar, G. Barth-Maron, S. Addepalli, B. Richter, C.-Y. Cheng, E. Rives, F. Zheng, J. Griesser, N. Dikkala, Y. Zeldes, I. Safarli, D. Das, H. Srivastava, S. M. Khan, X. Li, A. Pandey, L. Markeeva, D. Belov, Q. Yan, M. Rybiński, T. Chen, M. Nawhal, M. Quinn, V. Govindaraj, S. York, R. Roberts, R. Garg, N. Godbole, J. Abernethy, A. Das, L. N. Thiet, J. Tompson, J. Nham, N. Vats, B. Caine, W. Helmholz, F. Pongetti, Y. Ko, J. An, C. H. Hu, Y.-C. Ling, J. Pawar, R. Leland, K. Kinoshita, W. Khawaja, M. Selvi, E. Ie, D. Sinopalnikov, L. Proleev, N. Tripuraneni, M. Bevilacqua, S. Lee, C. Sanford, D. Suh, D. Tran, J. Dean, S. Baumgartner, J. Heitkaemper, S. Gubbi, K. Toutanova, Y. Xu, C. Thekkath, K. Rong, P. Jain, A. Xie, Y. Virin, Y. Li, L. Litchev, R. Powell, T. Bharti, A. Kraft, N. Hua, M. Ikonomidis, A. Hitron, S. Kumar, L. Matthey, S. Bridgers, L. Lax, I. Malhi, O. Skopek, A. Gupta, J. Cao, M. Rasquinha, S. Põder, W. Stokowiec, N. Roth, G. Li, M. Sander, J. Kessinger, V. Jain, E. Loper, W. Park, M. Yarom, L. Cheng, G. Guruganesh, K. Rao, Y. Li, C. Barros, M. Sushkov, C.-S. Ferng, R. Shah, O. Aharoni, R. Kumar, T. McConnell, P. Li, C. Wang, F. Pereira, C. Swanson, F. Jamil, Y. Xiong, A. Vijayakumar, P. Shroff, K. Soparkar, J. Gu, L. B. Soares, E. Wang, K. Majmundar, A. Wei, K. Bailey, N. Kassner, C. Kawamoto, G. Žužić, V. Gomes, A. Gupta, M. Guzman, I. Dasgupta, X. Bai, Z. Pan, F. Piccinno, H. N. Vogel, O. Ponce, A. Hutter, P. Chang, P.-P. Jiang, I. Gog, V. Ionescu, J. Manyika, F. Pedregosa, H. Ragan, Z. Behrman, R. Mullins, C. Devin, A. Pyne, S. Gawde, M. Chadwick, Y. Gu, S. Tavakkol, A. Twigg, N. Goyal, N. Elue, A. Goldie, S. Venkatachary, H. Fei, Z. Feng, M. Ritter, I. Leal, S. Dasari, P. Sun, A. R. Rochman, B. ODonoghue, Y. Liu, J. Sproch, K. Chen, N. Clay, S. Petrov, S. Sidhwani, I. Mihailescu, A. Panagopoulos, A. Piergiovanni, Y. Bai, G. Powell, D. Karkhanis, T. Yacovone, P. Mitrichev, J. Kovac, D. Uthus, A. Yazdanbakhsh, D. Amos, S. Zheng, B. Zhang, J. Miao, B. Ramabhadran, S. Radpour, S. Thakoor, J. Newlan, O. Lang, O. Jankowski, S. Bharadwaj, J.-M. Sarr, S. Ashraf, S. Mondal, J. Yan, A. S. Rawat, S. Velury, G. Kochanski, T. Eccles, F. Och, A. Sharma, E. Mahintorabi, A. Gurney, C. Muir, V. Cohen, S. Thakur, A. Bloniarz, A. Mujika, A. Pritzel, P. Caron, A. Rahman, F. Lang, Y. Onoe, P. Sirkovic, J. Hoover, Y. Jian, P. Duque, A. Narayanan, D. Soergel, A. Haig, L. Maggiore, S. Buch, J. Dean, I. Figotin, I. Karpov, S. Gupta, D. Zhou, M. Huang, A. Vaswani, C. Semturs, K. Shivakumar, Y. Watanabe, V. K. Rajendran, E. Lu, Y. Hou, W. Ye, S. Vashishth, N. Nti, V. Sakenas, D. Ni, D. DeCarlo, M. Bendersky, S. Bagri, N. Cano, E. Peake, S. Tokumine, V. Godbole, C. Guía, T. Lando, V. Selo, S. Ellis, D. Tarlow, D. Gillick, A. Epasto, S. R. Jonnalagadda, M. Wei, M. Xie, A. Taly, M. Paganini, M. Sundararajan, D. Toyama, T. Yu, D. Petrova, A. Pappu, R. Agrawal, S. Buthpitiya, J. Frye, T. Buschmann, R. Crocker, M. Tagliasacchi, M. Wang, D. Huang, S. Perel, B. Wieder, H. Kazawa, W. Wang, J. Cole, H. Gupta, B. Golan, S. Bang, N. Kulkarni, K. Franko, C. Liu, D. Reid, S. Dalmia, J. Whang, K. Cen, P. Sundaram, J. Ferret, B. Isik, L. Ionita, G. Sun, A. Shekhawat, M. Mohammad, P. Pham, R. Huang, K. Raman, X. Zhou, R. Mcilroy, A. Myers, S. Peng, J. Scott, P. Covington, S. Erell, P. Joshi, J. G. Oliveira, N. Noy, T. Nasir, J. Walker, V. Axelrod, T. Dozat, P. Han, C.-T. Chu, E. Weinstein, A. Shukla, S. Chandrakaladharan, P. Poklukar, B. Li, Y. Jin, P. Eruvbetine, S. Hansen, A. Dabush, A. Jacovi, S. Phatale, C. Zhu, S. Baker, M. Shomrat, Y. Xiao, J. PougetAbadie, M. Zhang, F. Wei, Y. Song, H. King, Y. Huang, Y. Zhu, R. Sun, J. V. Franco, C.-C. Lin, S. Arora, Hui, Li, V. Xia, L. Vilnis, M. Schain, K. Alarakyia, L. Prince, A. Phillips, C. Habtegebriel, L. Xu, H. Gui, S. Ontanon, L. Aroyo, K. Gill, P. Lu, Y. Katariya, D. Madeka, S. Krishnan, S. S. Raghvendra, J. Freedman, Y. Tay, G. Menghani, P. Choy, N. Shetty, D. Abolafia, D. Kukliansky, 16 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers E. Chou, J. Lichtarge, K. Burke, B. Coleman, D. Guo, L. Jin, I. Bhattacharya, V. Langston, Y. Li, S. Kotecha, A. Yakubovich, X. Chen, P. Petrov, T. Powell, Y. He, C. Quick, K. Garg, D. Hwang, Y. Lu, S. Bhojanapalli, K. Kjems, R. Mehran, A. Archer, H. van Hasselt, A. Balakrishna, J. Kearns, M. Guo, J. Riesa, M. Sazanovich, X. Gao, C. Sauer, C. Yang, X. Sheng, T. Jimma, W. V. Gansbeke, V. Nikolaev, W. Wei, K. Millican, R. Zhao, J. Snyder, L. Bolelli, M. OBrien, S. Xu, F. Xia, W. Yuan, A. Neelakantan, D. Barker, S. Yadav, H. Kirkwood, F. Ahmad, J. Wee, J. Grimstad, B. Wang, M. Wiethoff, S. Settle, M. Wang, C. Blundell, J. Chen, C. Duvarney, G. Hu, O. Ronneberger, A. Lee, Y. Li, A. Chakladar, A. Butryna, G. Evangelopoulos, G. Desjardins, J. Kanerva, H. Wang, A. Nowak, N. Li, A. Loo, A. Khurshudov, L. E. Shafey, N. Baddi, K. Lenc, Y. Razeghi, T. Lieber, A. Sinha, X. Ma, Y. Su, J. Huang, A. Ushio, H. Klimczak-Plucińska, K. Mohamed, J. Chen, S. Osindero, S. Ginzburg, L. Lamprou, V. Bashlovkina, D.-H. Tran, A. Khodaei, A. Anand, Y. Di, R. Eskander, M. R. Vuyyuru, J. Liu, A. Kamath, R. Goldenberg, M. Bellaiche, J. Pluto, B. Rosgen, H. Mansoor, W. Wong, S. Ganesh, E. Bailey, S. Baird, D. Deutsch, J. Baek, X. Jia, C. Lee, A. Friesen, N. Braun, K. Lee, A. Panda, S. M. Hernandez, D. Williams, J. Liu, E. Liang, A. Autef, E. Pitler, D. Jain, P. Kirk, O. Bunyan, J. S. Elias, T. Yin, M. Reid, A. Pope, N. Putikhin, B. Samanta, S. Guadarrama, D. Kim, S. Rowe, M. Valentine, G. Yan, A. Salcianu, D. Silver, G. Song, R. Singh, S. Ye, H. DeBalsi, M. A. Merey, E. Ofek, A. Webson, S. Mourad, A. Kakarla, S. Lattanzi, N. Roy, E. Sluzhaev, C. Butterfield, A. Tonioni, N. Waters, S. Kopalle, J. Chase, J. Cohan, G. R. Rao, R. Berry, M. Voznesensky, S. Hu, K. Chiafullo, S. Chikkerur, G. Scrivener, I. Zheng, J. Wiesner, W. Macherey, T. Lillicrap, F. Liu, B. Walker, D. Welling, E. Davies, Y. Huang, L. Ren, N. Shabat, A. Agostini, M. Iinuma, D. Zelle, R. Sathyanarayana, A. Dolimpio, M. Redshaw, M. Ginsberg, A. Murthy, M. Geller, T. Matejovicova, A. Chakrabarti, R. Julian, C. Chan, Q. Hu, D. Jarrett, M. Agarwal, J. Challagundla, T. Li, S. Tata, W. Ding, M. Meng, Z. Dai, G. Vezzani, S. Garg, J. Bulian, M. Jasarevic, H. Cai, H. Rajamani, A. Santoro, F. Hartmann, C. Liang, B. Perz, A. Jindal, F. Bu, S. Seo, R. Poplin, A. Goedeckemeyer, B. Ghazi, N. Khadke, L. Liu, K. Mather, M. Zhang, A. Shah, A. Chen, J. Wei, K. Shivam, Y. Cao, D. Cho, A. S. Scarpati, M. Moffitt, C. Barbu, I. Jurin, M.-W. Chang, H. Liu, H. Zheng, S. Dave, C. Kaeser-Chen, X. Yu, A. Abdagic, L. Gonzalez, Y. Huang, P. Zhong, C. Schmid, B. Petrini, A. Wertheim, J. Zhu, H. Nguyen, K. Ji, Y. Zhou, T. Zhou, F. Feng, R. Cohen, D. Rim, S. M. Phal, P. Georgiev, A. Brand, Y. Ma, W. Li, S. Gupta, C. Wang, P. Dubov, J. Tarbouriech, K. Majumder, H. Li, N. Rink, A. Suman, Y. Guo, Y. Sun, A. Nair, X. Xu, M. Elhawaty, R. Cabrera, G. Han, J. Eisenschlos, J. Bai, Y. Li, Y. Bansal, T. Sellam, M. Khan, H. Nguyen, J. Mao-Jones, N. Parotsidis, J. Marcus, C. Fan, R. Zimmermann, Y. Kochinski, L. Graesser, F. Behbahani, A. Caceres, M. Riley, P. Kane, S. Lefdal, R. Willoughby, P. Vicol, L. Wang, S. Zhang, A. Gill, Y. Liang, G. Prasad, S. Mariooryad, M. Kazemi, Z. Wang, K. Muralidharan, P. Voigtlaender, J. Zhao, H. Zhou, N. DSouza, A. Mavalankar, S. Arnold, N. Young, O. Sarvana, C. Lee, M. Nasr, T. Zou, S. Kim, L. Haas, K. Patel, N. Bulut, D. Parkinson, C. Biles, D. Kalashnikov, C. M. To, A. Kumar, J. Austin, A. Greve, L. Zhang, M. Goel, Y. Li, S. Yaroshenko, M. Chang, A. Jindal, G. Clark, H. Taitelbaum, D. Johnson, O. Roval, J. Ko, A. Mohananey, C. Schuler, S. Dodhia, R. Li, K. Osawa, C. Cui, P. Xu, R. Shah, T. Huang, E. Gruzewska, N. Clement, M. Verma, O. Sercinoglu, H. Qian, V. Shah, M. Yamaguchi, A. Modi, T. Kosakai, T. Strohmann, J. Zeng, B. Gunel, J. Qian, A. Tarango, K. Jastrzębski, R. David, J. Shan, P. Schuh, K. Lad, W. Gierke, M. Madhavan, X. Chen, M. Kurzeja, R. Santamaria-Fernandez, D. Chen, A. Cordell, Y. Chervonyi, F. Garcia, N. Kannen, V. Perot, N. Ding, S. Cohen-Ganor, V. Lavrenko, J. Wu, G. Evans, C. N. dos Santos, M. Sewak, A. Brown, A. Hard, J. Puigcerver, Z. Zheng, Y. Liang, E. Gladchenko, R. Ingle, U. First, P. Sermanet, C. Magister, M. Velimirović, S. Reddi, S. Ricco, E. Agustsson, H. Adam, N. Levine, D. Gaddy, D. Holtmann-Rice, X. Wang, A. Sathe, A. G. Roy, B. Bratanič, A. Carin, H. Mehta, S. Bonacina, N. D. Cao, M. Finkelstein, V. Rieser, X. Wu, F. Altché, D. Scandinaro, L. Li, N. Vieillard, N. Sethi, G. Tanzer, Z. Xing, S. Wang, P. Bhatia, G. Citovsky, T. Anthony, S. Lin, T. Shi, S. Jakobovits, G. Gibson, R. Apte, L. Lee, M. Chen, A. Byravan, P. Maniatis, K. Webster, A. Dai, P.-C. Chen, J. Pan, A. Fadeeva, 17 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers Z. Gleicher, T. Luong, and N. K. Bhumihar. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. URL https://arxiv.org/abs/2507.06261. [6] DeepSeek-AI, D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, X. Zhang, X. Yu, Y. Wu, Z. F. Wu, Z. Gou, Z. Shao, Z. Li, Z. Gao, A. Liu, B. Xue, B. Wang, B. Wu, B. Feng, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Chen, D. Ji, E. Li, F. Lin, F. Dai, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Ding, H. Xin, H. Gao, H. Qu, H. Li, J. Guo, J. Li, J. Wang, J. Chen, J. Yuan, J. Qiu, J. Li, J. L. Cai, J. Ni, J. Liang, J. Chen, K. Dong, K. Hu, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Zhao, L. Wang, L. Zhang, L. Xu, L. Xia, M. Zhang, M. Zhang, M. Tang, M. Li, M. Wang, M. Li, N. Tian, P. Huang, P. Zhang, Q. Wang, Q. Chen, Q. Du, R. Ge, R. Zhang, R. Pan, R. Wang, R. J. Chen, R. L. Jin, R. Chen, S. Lu, S. Zhou, S. Chen, S. Ye, S. Wang, S. Yu, S. Zhou, S. Pan, S. S. Li, S. Zhou, S. Wu, S. Ye, T. Yun, T. Pei, T. Sun, T. Wang, W. Zeng, W. Zhao, W. Liu, W. Liang, W. Gao, W. Yu, W. Zhang, W. L. Xiao, W. An, X. Liu, X. Wang, X. Chen, X. Nie, X. Cheng, X. Liu, X. Xie, X. Liu, X. Yang, X. Li, X. Su, X. Lin, X. Q. Li, X. Jin, X. Shen, X. Chen, X. Sun, X. Wang, X. Song, X. Zhou, X. Wang, X. Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. Zhang, Y. Xu, Y. Li, Y. Zhao, Y. Sun, Y. Wang, Y. Yu, Y. Zhang, Y. Shi, Y. Xiong, Y. He, Y. Piao, Y. Wang, Y. Tan, Y. Ma, Y. Liu, Y. Guo, Y. Ou, Y. Wang, Y. Gong, Y. Zou, Y. He, Y. Xiong, Y. Luo, Y. You, Y. Liu, Y. Zhou, Y. X. Zhu, Y. Xu, Y. Huang, Y. Li, Y. Zheng, Y. Zhu, Y. Ma, Y. Tang, Y. Zha, Y. Yan, Z. Z. Ren, Z. Ren, Z. Sha, Z. Fu, Z. Xu, Z. Xie, Z. Zhang, Z. Hao, Z. Ma, Z. Yan, Z. Wu, Z. Gu, Z. Zhu, Z. Liu, Z. Li, Z. Xie, Z. Song, Z. Pan, Z. Huang, Z. Xu, Z. Zhang, and Z. Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [7] A. Didolkar, A. Goyal, N. R. Ke, S. Guo, M. Valko, T. Lillicrap, D. Rezende, Y. Bengio, M. Mozer, and S. Arora. Metacognitive capabilities of llms: An exploration in mathematical problem solving, 2024. URL https://arxiv.org/abs/2405.12205. [8] H. Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https: //github.com/huggingface/open-r1. [9] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan, A. Yang, A. Fan, A. Goyal, A. Hartshorn, A. Yang, A. Mitra, A. Sravankumar, A. Korenev, A. Hinsvark, A. Rao, A. Zhang, A. Rodriguez, A. Gregerson, A. Spataru, B. Roziere, B. Biron, B. Tang, B. Chern, C. Caucheteux, C. Nayak, C. Bi, C. Marra, C. McConnell, C. Keller, C. Touret, C. Wu, C. Wong, C. C. Ferrer, C. Nikolaidis, D. Allonsius, D. Song, D. Pintz, D. Livshits, D. Wyatt, D. Esiobu, D. Choudhary, D. Mahajan, D. Garcia-Olano, D. Perino, D. Hupkes, E. Lakomkin, E. AlBadawy, E. Lobanova, E. Dinan, E. M. Smith, F. Radenovic, F. Guzmán, F. Zhang, G. Synnaeve, G. Lee, G. L. Anderson, G. Thattai, G. Nail, G. Mialon, G. Pang, G. Cucurell, H. Nguyen, H. Korevaar, H. Xu, H. Touvron, I. Zarov, I. A. Ibarra, I. Kloumann, I. Misra, I. Evtimov, J. Zhang, J. Copet, J. Lee, J. Geffert, J. Vranes, J. Park, J. Mahadeokar, J. Shah, J. van der Linde, J. Billock, J. Hong, J. Lee, J. Fu, J. Chi, J. Huang, J. Liu, J. Wang, J. Yu, J. Bitton, J. Spisak, J. Park, J. Rocca, J. Johnstun, J. Saxe, J. Jia, K. V. Alwala, K. Prasad, K. Upasani, K. Plawiak, K. Li, K. Heafield, K. Stone, K. El-Arini, K. Iyer, K. Malik, K. Chiu, K. Bhalla, K. Lakhotia, L. Rantala-Yeary, L. van der Maaten, L. Chen, L. Tan, L. Jenkins, L. Martin, L. Madaan, L. Malo, L. Blecher, L. Landzaat, L. de Oliveira, M. Muzzi, M. Pasupuleti, M. Singh, M. Paluri, M. Kardas, M. Tsimpoukelli, M. Oldham, M. Rita, M. Pavlova, M. Kambadur, M. Lewis, M. Si, M. K. Singh, M. Hassan, N. Goyal, N. Torabi, N. Bashlykov, N. Bogoychev, N. Chatterji, N. Zhang, O. Duchenne, O. Çelebi, P. Alrassy, P. Zhang, P. Li, P. Vasic, P. Weng, P. Bhargava, P. Dubal, P. Krishnan, P. S. Koura, P. Xu, Q. He, Q. Dong, R. Srinivasan, R. Ganapathy, R. Calderer, R. S. Cabral, R. Stojnic, R. Raileanu, R. Maheswari, R. Girdhar, R. Patel, R. Sauvestre, R. Polidoro, R. Sumbaly, R. Taylor, SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers R. Silva, R. Hou, R. Wang, S. Hosseini, S. Chennabasappa, S. Singh, S. Bell, S. S. Kim, S. Edunov, S. Nie, S. Narang, S. Raparthy, S. Shen, S. Wan, S. Bhosale, S. Zhang, S. Vandenhende, S. Batra, S. Whitman, S. Sootla, S. Collot, S. Gururangan, S. Borodinsky, T. Herman, T. Fowler, T. Sheasha, T. Georgiou, T. Scialom, T. Speckbacher, T. Mihaylov, T. Xiao, U. Karn, V. Goswami, V. Gupta, V. Ramanathan, V. Kerkez, V. Gonguet, V. Do, V. Vogeti, V. Albiero, V. Petrovic, W. Chu, W. Xiong, W. Fu, W. Meers, X. Martinet, X. Wang, X. Wang, X. E. Tan, X. Xia, X. Xie, X. Jia, X. Wang, Y. Goldschlag, Y. Gaur, Y. Babaei, Y. Wen, Y. Song, Y. Zhang, Y. Li, Y. Mao, Z. D. Coudert, Z. Yan, Z. Chen, Z. Papakipos, A. Singh, A. Srivastava, A. Jain, A. Kelsey, A. Shajnfeld, A. Gangidi, A. Victoria, A. Goldstand, A. Menon, A. Sharma, A. Boesenberg, A. Baevski, A. Feinstein, A. Kallet, A. Sangani, A. Teo, A. Yunus, A. Lupu, A. Alvarado, A. Caples, A. Gu, A. Ho, A. Poulton, A. Ryan, A. Ramchandani, A. Dong, A. Franco, A. Goyal, A. Saraf, A. Chowdhury, A. Gabriel, A. Bharambe, A. Eisenman, A. Yazdan, B. James, B. Maurer, B. Leonhardi, B. Huang, B. Loyd, B. D. Paola, B. Paranjape, B. Liu, B. Wu, B. Ni, B. Hancock, B. Wasti, B. Spence, B. Stojkovic, B. Gamido, B. Montalvo, C. Parker, C. Burton, C. Mejia, C. Liu, C. Wang, C. Kim, C. Zhou, C. Hu, C.-H. Chu, C. Cai, C. Tindal, C. Feichtenhofer, C. Gao, D. Civin, D. Beaty, D. Kreymer, D. Li, D. Adkins, D. Xu, D. Testuggine, D. David, D. Parikh, D. Liskovich, D. Foss, D. Wang, D. Le, D. Holland, E. Dowling, E. Jamil, E. Montgomery, E. Presani, E. Hahn, E. Wood, E.-T. Le, E. Brinkman, E. Arcaute, E. Dunbar, E. Smothers, F. Sun, F. Kreuk, F. Tian, F. Kokkinos, F. Ozgenel, F. Caggioni, F. Kanayet, F. Seide, G. M. Florez, G. Schwarz, G. Badeer, G. Swee, G. Halpern, G. Herman, G. Sizov, Guangyi, Zhang, G. Lakshminarayanan, H. Inan, H. Shojanazeri, H. Zou, H. Wang, H. Zha, H. Habeeb, H. Rudolph, H. Suk, H. Aspegren, H. Goldman, H. Zhan, I. Damlaj, I. Molybog, I. Tufanov, I. Leontiadis, I.-E. Veliche, I. Gat, J. Weissman, J. Geboski, J. Kohli, J. Lam, J. Asher, J.-B. Gaya, J. Marcus, J. Tang, J. Chan, J. Zhen, J. Reizenstein, J. Teboul, J. Zhong, J. Jin, J. Yang, J. Cummings, J. Carvill, J. Shepard, J. McPhie, J. Torres, J. Ginsburg, J. Wang, K. Wu, K. H. U, K. Saxena, K. Khandelwal, K. Zand, K. Matosich, K. Veeraraghavan, K. Michelena, K. Li, K. Jagadeesh, K. Huang, K. Chawla, K. Huang, L. Chen, L. Garg, L. A, L. Silva, L. Bell, L. Zhang, L. Guo, L. Yu, L. Moshkovich, L. Wehrstedt, M. Khabsa, M. Avalani, M. Bhatt, M. Mankus, M. Hasson, M. Lennie, M. Reso, M. Groshev, M. Naumov, M. Lathi, M. Keneally, M. Liu, M. L. Seltzer, M. Valko, M. Restrepo, M. Patel, M. Vyatskov, M. Samvelyan, M. Clark, M. Macey, M. Wang, M. J. Hermoso, M. Metanat, M. Rastegari, M. Bansal, N. Santhanam, N. Parks, N. White, N. Bawa, N. Singhal, N. Egebo, N. Usunier, N. Mehta, N. P. Laptev, N. Dong, N. Cheng, O. Chernoguz, O. Hart, O. Salpekar, O. Kalinli, P. Kent, P. Parekh, P. Saab, P. Balaji, P. Rittner, P. Bontrager, P. Roux, P. Dollar, P. Zvyagina, P. Ratanchandani, P. Yuvraj, Q. Liang, R. Alao, R. Rodriguez, R. Ayub, R. Murthy, R. Nayani, R. Mitra, R. Parthasarathy, R. Li, R. Hogan, R. Battey, R. Wang, R. Howes, R. Rinott, S. Mehta, S. Siby, S. J. Bondu, S. Datta, S. Chugh, S. Hunt, S. Dhillon, S. Sidorov, S. Pan, S. Mahajan, S. Verma, S. Yamamoto, S. Ramaswamy, S. Lindsay, S. Lindsay, S. Feng, S. Lin, S. C. Zha, S. Patil, S. Shankar, S. Zhang, S. Zhang, S. Wang, S. Agarwal, S. Sajuyigbe, S. Chintala, S. Max, S. Chen, S. Kehoe, S. Satterfield, S. Govindaprasad, S. Gupta, S. Deng, S. Cho, S. Virk, S. Subramanian, S. Choudhury, S. Goldman, T. Remez, T. Glaser, T. Best, T. Koehler, T. Robinson, T. Li, T. Zhang, T. Matthews, T. Chou, T. Shaked, V. Vontimitta, V. Ajayi, V. Montanez, V. Mohan, V. S. Kumar, V. Mangla, V. Ionescu, V. Poenaru, V. T. Mihailescu, V. Ivanov, W. Li, W. Wang, W. Jiang, W. Bouaziz, W. Constable, X. Tang, X. Wu, X. Wang, X. Wu, X. Gao, Y. Kleinman, Y. Chen, Y. Hu, Y. Jia, Y. Qi, Y. Li, Y. Zhang, Y. Zhang, Y. Adi, Y. Nam, Yu, Wang, Y. Zhao, Y. Hao, Y. Qian, Y. Li, Y. He, Z. Rait, Z. DeVito, Z. Rosnbrick, Z. Wen, Z. Yang, Z. Zhao, and Z. Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. [10] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 19 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers [11] Y. Huang, X. Liu, Y. Gong, Z. Gou, Y. Shen, N. Duan, and W. Chen. Key-point-driven data synthesis with its enhancement on mathematical reasoning, 2024. URL https://arxiv. org/abs/2403.02333. [12] J. LI, E. Beeching, L. Tunstall, B. Lipkin, R. Soletskyi, S. C. Huang, K. Rasul, L. Yu, A. Jiang, Z. Shen, Z. Qin, B. Dong, L. Zhou, Y. Fleureau, G. Lample, and S. Polu. [https://huggingface.co/AI-MO/NuminaMath-CoT](https: Numinamath. //github.com/project-numina/aimo-progress-prize/blob/main/report/ numina_dataset.pdf), 2024. [13] Z. Li, X. Zhang, Y. Zhang, D. Long, P. Xie, and M. Zhang. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281, 2023. [14] H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, Y. Tang, and D. Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct, 2025. URL https://arxiv.org/abs/2308.09583. [15] N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. Hajishirzi, L. Zettlemoyer, P. Liang, E. Candès, and T. Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/ abs/2501.19393. [16] OpenAI. Introducing GPT-o3. https://openai.com/index/ introducing-o3-and-o4-mini/, 2024. Accessed: 2024-05-17. [17] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion parameter models, 2020. URL https://arxiv.org/abs/1910.02054. [18] N. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL http://arxiv.org/abs/1908. 10084. [19] L. A. Research, S. An, K. Bae, E. Choi, K. Choi, S. J. Choi, S. Hong, J. Hwang, H. Jeon, G. J. Jo, H. Jo, J. Jung, Y. Jung, H. Kim, J. Kim, S. Kim, S. Kim, S. Kim, Y. Kim, Y. Kim, Y. Kim, E. H. Lee, H. Lee, H. Lee, J. Lee, K. Lee, W. Lim, S. Park, S. Park, Y. Park, S. Yang, H. Yeen, and H. Yun. Exaone 3.5: Series of large language models for real-world use cases, 2024. URL https://arxiv.org/abs/2412.04862. [20] T. v. D. Stephan Tulkens. Model2vec: Turn any sentence transformer into small fast model, 2024. URL https://github.com/MinishLab/model2vec. [21] Q. Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. [22] S. Toshniwal, W. Du, I. Moshkov, B. Kisacanin, A. Ayrapetyan, and I. Gitman. OpenmathinstructarXiv preprint 2: Accelerating ai for math with massive open-source instruction data. arXiv:2410.01560, 2024. [23] T. van Dongen and S. Tulkens. Semhash: Fast semantic text deduplication & filtering, 2025. URL https://github.com/MinishLab/semhash. [24] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Selfconsistency improves chain of thought reasoning in language models, 2023. URL https: //arxiv.org/abs/2203.11171. SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers [25] Y. Ye, Z. Huang, Y. Xiao, E. Chern, S. Xia, and P. Liu. Limo: Less is more for reasoning, 2025. URL https://arxiv.org/abs/2502.03387. [26] L. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. [27] Y. Zheng, R. Zhang, J. Zhang, Y. Ye, Z. Luo, Z. Feng, and Y. Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372. SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers"
        },
        {
            "title": "Appendices",
            "content": "A. Detailed Training Steps All models trained in this work are full-parameter finetuned using the LLaMA-Factory [27] framework. Training was conducted on single node equipped with 8 AMD InstinctTM MI300X GPUs. This section provides the key hyperparameters and brief guide to the training setup on AMD GPUs. A.1. Hyperparameters The core hyperparameters used for our full-parameter finetuning experiments are detailed in Table 3. Hyperparameter Value Learning Rate LR Scheduler Type Warmup Ratio Number of Training Epochs Gradient Accumulation Steps Cutoff Length Flash Attention Implementation fa2 DeepSpeed Strategy 5.0𝑒-6 cosine 0.0 10 1 32,768 ZeRO-3 Table 3 Key training hyperparameters used in our experiments. A.2. Training Setup The following steps outline the process for replicating our training environment. 1. Launch the Docker Container: The experiments were run inside the ROCmTM PyTorch training container [2]. 1 docker run -it --ipc=host --cap-add=SYS_PTRACE --network=host 2 3 --device=/dev/kfd --device=/dev/dri --security-opt seccomp=unconfined --group-add video --privileged -w /workspace rocm/pytorch-training:v25. 2. Install LLaMA-Factory: Clone the repository and install the required dependencies. specific version of DeepSpeed was used for compatibility. 1 git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git 2 cd LLaMA-Factory 3 pip install -e \".[torch,metrics]\" --no-build-isolation 4 pip install deepspeed==0.16.9 3. Prepare Training Files: Download the required SAND-Math training splits from HuggingFace and format them into the Alpaca-style JSON format, as described in the documentation3. Add corresponding entry for the new dataset file in data/dataset_info.json. 3https://github.com/hiyouga/LLaMA-Factory/tree/main/data 22 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers Create training configuration YAML file. Our configuration is adapted from the LIMO example4. For experiments that augment the LIMO dataset, the dataset field in the configuration should be set to limo,sand_math assuming the LIMO data5 is also available. 4. Launch Training: Execute the training run using the prepared configuration file. 1 llamafactory-cli train examples/train_full/train_sand_math.yaml B. Dataset Characteristics: Consistency, Novelty, and Difficulty Our multi-stage filtering pipeline produced high-quality dataset characterized by four key attributes. First, the novelty of the questions was ensured by maintaining an exceptionally low contamination rate of just 0.2%, measured as the proportion of questions with similarity score exceeding 𝜏 = 0.85 when compared to web search results and existing test sets. Second, self-consistency filtering stage was applied, requiring that two independently sampled solutions yield the same final answer; this criterion was satisfied by 85% of the generated question-solution pairs. Third, difficulty filtering stepdependent on performance by the target model (Qwen2.5-32B-Instruct)was used to retain only 41% of the consistent questions that were deemed sufficiently challenging. Finally, the overall retention rate across all stages of filtering resulted in final dataset, Dfinal, comprising approximately 35% of the initially generated questions. The difficulty distribution of our dataset SAND-Math was assessed using model-agnostic method (described in Section 2.5). C. Algorithm for Novelty Filtering Algorithm 1 Novelty Filtering Algorithm Require: Input dataset D𝑑𝑖 𝑓 𝑓 = {𝑞1, 𝑞2, ..., 𝑞𝑁 }; Similarity threshold 𝜏 = 0.85; Number of search results 𝐾 = 20; Sentence Transformer model 𝑀𝑆𝑇 ; Meta Search Engine 𝑆𝑒𝑎𝑟𝑐ℎ(); Final dataset 𝑓 𝑖𝑛𝑎𝑙 1: 𝑓 𝑖𝑛𝑎𝑙 2: for each question 𝑞𝑖 D𝑑𝑖 𝑓 𝑓 do 𝑅𝑒𝑠𝑢𝑙𝑡𝑠 𝑆𝑒𝑎𝑟𝑐ℎ(𝑞𝑖, 𝐾) 3: 𝑚𝑎𝑥_𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡 𝑦 0 𝑞𝑒𝑚𝑏 𝑀𝑆𝑇 .𝑒𝑛𝑐𝑜𝑑𝑒(𝑞𝑖) for each snippet 𝑠 𝑗 in 𝑅𝑒𝑠𝑢𝑙𝑡𝑠 do Get top search results (URL, snippet) Get embedding for the question Initialize the final dataset 5: 6: 4: 𝑠𝑒𝑚𝑏 𝑀𝑆𝑇 .𝑒𝑛𝑐𝑜𝑑𝑒(𝑠 𝑗) 𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡 𝑦 𝑐𝑜𝑠𝑖𝑛𝑒_𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡 𝑦(𝑞𝑒𝑚𝑏, 𝑠𝑒𝑚𝑏) if 𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡 𝑦 > 𝑚𝑎𝑥_𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡 𝑦 then 𝑚𝑎𝑥_𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡 𝑦 𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡 𝑦 Get embedding for the snippet 7: 8: 9: 10: 11: Check if highest similarity is below threshold Retain the question if 𝑚𝑎𝑥_𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡 𝑦 𝜏 then 𝑓 𝑖𝑛𝑎𝑙 𝑓 𝑖𝑛𝑎𝑙 {𝑞𝑖} 12: 13: return 𝑓 𝑖𝑛𝑎𝑙 4https://github.com/GAIR-NLP/LIMO/blob/main/train/examples/train_limo.yaml 5https://github.com/GAIR-NLP/LIMO/blob/main/train/data/limo.json 23 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers D. Difficulty Hiking Prompt Question difficulty hiking prompt. You are an expert math problem crafter specializing in very hard Olympiad level questions. problem provided below into new problem with target difficulty of **challenging Olympiad problem** (IMO Shortlist level)."
        },
        {
            "title": "Your task is to transform the original",
            "content": "Central Theorem: Supporting Concept/Tool: {} {} **Transformation Instructions for the New Problem:** **Deep Synthesis of Concepts:** The solution to the new problem 1. must *critically depend* on the interplay between the original problems core theme and the newly introduced **Central Theorem**, **Supporting Concept/Tool**. This synthesis should feel natural and integral to the problem. 2. **Reliance on Olympiad-Level Theorem:** The application of **Central Theorem** must be non-trivial, essential for reaching the solution, and demonstrate deep understanding of the theorem. superficial application or alternative simpler methods should not suffice. 3. **Central Theorem must be disguised:** Central Theorem must be cleaverly disguised. the problem. Do not use the **Central Theorem** name in **Multiple Non-Trivial Intermediate Steps/Lemmas:** Design the 4. problem so its solution requires at least 2-3 distinct, non-obvious intermediate steps. initial problem setup, any necessary lemmas, the application of **Central Theorem**, **Supporting Concept/Tool**, and the derivation of the final answer. These steps should logically connect the **High Degree of Abstraction or Generalization (If 5. Appropriate):** If appropriate, replace concrete numbers from the original with parameters, or frame the question more generally to enhance the conceptual challenge. **Clarity and Soundness:** The new problem statement must be 6. clear, unambiguous, and mathematically sound. **Answer Format:** The problem must be constructed such that it 7. has **single final integer answer.** 24 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers **Output Format:** Return the text of the new difficult problem enclosed within <Q> and </Q> tags and new solution enclosed within <S> and </S>. **Original Problem:** ({original_problem}) E. Difficulty Hiking Examples Challenging problems are key component for building high-quality SFT datasets that enhance the mathematical reasoning capabilities of LLMs. As detailed in Section 2.7, our approach systematically increases problem complexity by transforming an existing problem based on several key transformations such as incorporating new advanced theorem and supporting concept etc. Although the LLM is prompted to incorporate all elements, it often applies one of several transformation strategies in isolation. These strategies include: Incorporating the central theorem. Incorporating the supporting concept. Increasing the level of abstraction or generalization. Adding new constraints. complete list of the transformation instructions provided to the LLM can be found in the prompt in Appendix D. This section provides example scenarios for each of the strategies listed above, demonstrating how the LLM applied that transformation to make problem more difficult. Example: Difficulty Hiking using Central Theorem"
        },
        {
            "title": "Original Question",
            "content": "Question: Determine the number of distinct real solutions 𝜃 [0, 2𝜋) satisfying the equations: tan(2𝜃 + sin 𝜃) = tan(𝜃 sin 𝜃) sin2 𝜃 + sin 𝜃 = 0 Branch: Algebra Initial Difficulty: 2.0 / 10."
        },
        {
            "title": "Transformation Inputs",
            "content": "Theorem (from same branch): Cauchys Functional Equation ( 𝑓 (𝑥 + 𝑦) = 𝑓 (𝑥) + 𝑓 ( 𝑦)) (and its solutions over ℚ, ℝ with conditions) Concept: Sprague-Grundy Theorem (Nim-values, for impartial games conceptual awareness) Target Difficulty: 8.0 Hiked Question (Output) Question: Consider function 𝑓 : ℝ ℝ that satisfies the following conditions: 1. 2. 𝑓 (𝑥 + 𝑦) = 𝑓 (𝑥) + 𝑓 ( 𝑦) for all real numbers 𝑥 and 𝑦. 𝑓 (𝜋) = 0. SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers 3. For all 𝜃 [0, 2𝜋), the equation 𝑓 (2𝜃) = 2 𝑓 (𝜃) + 𝑓 (sin2 𝜃) holds. Additionally, consider an impartial combinatorial game played on the set of non-negative integers. The rules are: From position 𝑛 0, player can move to any integer 𝑚 such that 0 𝑚 < 𝑛 and 𝑓 (𝑚) = 𝑓 (𝑛) 1. The position 0 is terminal (no moves). Let 𝜔(𝑘) denote the Grundy number (nimber) of position 𝑘 in this game. It is known that 𝜔 is eventually periodic. Determine the number of integers 𝑘 in the set {0, 1, 2, . . . , 2024} for which 𝜔(𝑘) = 0. New Difficulty: 7.0 / 10.0 Answer Analysis The problem can be solved by first determining the specific function 𝑓 that satisfies the given conditions, and then analyzing the combinatorial game based on this function. The specified theorem, Cauchys Functional Equation, is the cornerstone for finding the function 𝑓 . . . . . . . . . . . . . Since this is true for all 𝑘, we just need to count the number of elements in the set. The set contains 2024 0 + 1 = 2025 integers. Example: Difficulty Hiking using Supporting Concept"
        },
        {
            "title": "Original Question",
            "content": "Question: Let 𝑝 be an odd prime. Find the number of subsets 𝑆 {1, 2, . . . , 𝑝 1} such that both the sum and the product of the elements of 𝑆 are congruent to 0 modulo 𝑝. Branch: Number Theory Initial Difficulty: 1.0 / 10."
        },
        {
            "title": "Transformation Inputs",
            "content": "Theorem (from same branch): Eulers Totient Theorem Concept: Cyclic Groups (Conceptual understanding, e.g., (ℤ/𝑝ℤ) is cyclic) Target Difficulty: 8.0 Hiked Question (Output) Question: Let 𝑛 = 9. Consider the set 𝐺 = {𝑘 1 𝑘 8, gcd(𝑘, 9) = 1}. Find the number of subsets 𝑆 𝐺 such that the sum of the elements in 𝑆 is divisible by 9, and the product of the elements in 𝑆 is congruent to 1 modulo 9. New Difficulty: 6.0 / 10."
        },
        {
            "title": "Answer Analysis",
            "content": "The problem asks for the number of subsets 𝑆 of the set 𝐺 = {𝑘 1 𝑘 8, gcd(𝑘, 9) = 1} that satisfy two conditions: the sum of elements is divisible by 9, and the product of elements is congruent to 1 modulo 9. 26 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers Step 1: Identify using the concept of Cyclic Groups First, we identify the elements of the set 𝐺 and analyze its structure as cyclic group under multiplication modulo 9. . . . . . . . . . . . . . . The total number of such subsets is 2. The use of the cyclic group concept was essential to transform the difficult product condition (cid:206)𝑠𝑆 𝑠 1 (mod 9) into much simpler linear congruence on the exponents, (cid:205) 𝑖 0 (mod 6), making the problem systematic and solvable. Example: Difficulty Hiking by increasing Abstraction Original Question Question: bag contains red and blue balls. When two balls are drawn with replacement, the probability that both are red is 1 4 . Additionally, the probability of drawing at least one red ball in two draws is 3 4 . Determine the total number of balls in the bag. Branch: Probability Initial Difficulty: 2.0 / 10."
        },
        {
            "title": "Transformation Inputs",
            "content": "Theorem (from same branch): Lovasz Local Lemma (Tool for showing existence when events are mostlyïndependent - very advanced). Concept: Infinite Descent Method (Fermats Method). Target Difficulty: 8.0 Hiked Question (Output) Question: Determine the smallest positive integer 𝑛 such that for any assignment of the color red or blue to each integer in the set {1, 2, . . . , 𝑛}, there is an arithmetic progression of length 4 that is monochromatic (all red or all blue) New Difficulty: 7.0 / 10."
        },
        {
            "title": "Higher Abstraction Explanation",
            "content": "The original problem is concrete probability calculation involving specific numerical probabilities (1/4 and 3/4) in single scenario (one bag of balls). The new problem generalizes this by asking for universal property that must hold for *any* assignment of colors (a class of functions) on set of integers 1, 2, ..., n. It replaces concrete calculations with search for threshold parameter where specific combinatorial structure (a monochromatic arithmetic progression) is guaranteed to exist, which represents significant increase in abstraction and conceptual challenge. Example: Difficulty Hiking using Central Theorem"
        },
        {
            "title": "Original Question",
            "content": "Question: Let 𝑛 be positive integer. Determine the number of positive integers 𝑛 for which sec2(𝜋𝑛) tan2(𝜋𝑛) + sin(𝜋𝑛) 2 is non-negative integer. SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers Branch: Number Theory Initial Difficulty: 1.0 / 10.0 Transformation Inputs Theorem (from same branch): Möbius Inversion Formula Concept: Primitive Roots (Existence criteria, properties) Target Difficulty: 8.0 Hiked Question (Output) Question: Let 𝑛 be square-free positive integer for which there exists primitive root modulo 𝑛. Determine the number of such 𝑛 for which the sum 𝑛 𝑘=1 gcd(𝑘,𝑛)=1 sin2 (cid:19) (cid:18) 𝜋𝑘 𝑛 is an integer. New Difficulty: 7.0 / 10.0 F. Question Generation Prompt In this section, we present the prompt used for question generation, which is shown below. {primary _math_branch} and {secondary_math_branch} are randomly selected from the list of mathematics branches. Question Generation Prompt Generate one novel math problem and solution with difficulty level at National or International Olympiads. It must have single non-negative integer as the answer. The problem should primarily focus on ({primary_math_branch}) and incorporate clever mix of elements from ({secondary_math_branch}). Your response should be formatted as follows: <Q> Problem Statement </Q> <S> Step-by-step solution, concluding with the final answer enclosed in boxed{}. </S> 28 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers G. Solution Generation Prompt"
        },
        {
            "title": "Question Generation Prompt",
            "content": "Answer the mathematics question. final answer with in boxed{} question: question"
        },
        {
            "title": "Think step by step and put your",
            "content": "H. Question Difficulty Rating Prompt Question difficulty rating prompt. # ROLE: Expert Math Problem Difficulty Assessor # TASK: Analyze the provided math problem and solution to assign difficulty score based on the provided reference materials. # REFERENCE MATERIALS: <difficulty_reference> ## Difficulty Level Descriptions (1.0 - 10.0 Scale) ({Long difficulty descriptions for each difficulty level such as 1.0, 1.2 ... }) ## Example Problems by Difficulty Level ({Example question and answer for each difficulty level 1.0, 1.2 ... }) **Analyze:** Carefully read the provided Math Problem # INSTRUCTIONS: 1. and its Solution. required techniques, and the complexity of the argument. particularly clever steps, non-obvious insights, or reliance on advanced theorems. Identify the core mathematical concepts, Note any 2. **Compare:** Compare the analyzed problem to the Example Problems by Difficulty Level and the Difficulty Level Descriptions provided in the reference materials. it fits in terms of typical competition level (AMC 8/10/12, AIME, USA(J)MO, IMO) and the type of thinking required."
        },
        {
            "title": "Consider where",
            "content": "3. **Score:** Assign difficulty score between **1.0 and 10.0**, using increments of **0.5** (e.g., 3.0, 3.5, 4.0). The score must be consistent with the provided reference scale. 29 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers **Summarize:** Write brief paragraph summarizing the 4. problems core topic and mathematical area(s). summary within <S> and </S> tags."
        },
        {
            "title": "Enclose this",
            "content": "5. **Assign Score:** Place the difficulty score assigned in step 3 within <D> and </D> tags. 6. **Justify:** Write paragraph explaining the reasoning behind the assigned difficulty score, explicitly referencing the comparison made in step 2 (e.g., \"This problem involves techniques similar to example 3.5...\" or \"The required insight aligns with the description for level 6.0...\"). problem or solution (like multi-step reasoning, specific theorems, type of creativity needed) that justify the score. Enclose this justification within <R> and </R> tags."
        },
        {
            "title": "Mention aspects of the",
            "content": "# OUTPUT FORMAT: <S>[Your brief paragraph summarizing the problem.]</S> <D>[The assigned score, e.g., 4.5]</D> <R>[Your paragraph justifying the score based on analysis and comparison to references.]</R> # INPUT PROBLEM & SOLUTION: ## Math Problem: <question> ## Solution: <solution> I. Question Decontamination Prompt Question de-duplication prompt. Determine whether the provided new question is identical to or If paraphrased version of any of the existing questions listed. it is identical or paraphrased, respond with **yes** otherwise, respond with **no**. Please ensure your response is only yes or no, with no additional commentary. New Question: ({synthetic_question}) Existing Questions: ({list_of_similiar_questions}) 30 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers J. Math Theorems and Concepts Taxonomy"
        },
        {
            "title": "Taxonomy",
            "content": "Number Theory: # topic Divisibility and Prime Factorization: tools_concepts: - Division Algorithm (𝑎 = 𝑏𝑞 + 𝑟) - Greatest Common Divisor (GCD) & Least Common Multiple (LCM) (Properties, gcd*lcm = ab) - Euclidean Algorithm (For GCD and linear Diophantine solutions) - Bézouts Identity (𝑎𝑥 + 𝑏𝑦 = gcd(𝑎, 𝑏)) - Prime Numbers & Composite Numbers (Definitions, Sieve of Eratosthenes) - Fundamental Theorem of Arithmetic (Unique Prime Factorization) - p-adic Valuation (𝑣𝑝(𝑛)) - Legendres Formula (for 𝑣𝑝(𝑛!)) theorems: - Euclids Theorem on Infinitude of Primes - Dirichlets Theorem on Arithmetic Progressions (Existence Statement) 𝑏 (mod 𝑚)) (Solvability, number of Modular Arithmetic: tools_concepts: - Congruence Relation () (Properties) - Complete Residue System (CRS)& Reduced Residue System (RRS) - Linear Congruences (𝑎𝑥 solutions) - Modular Inverse (Existence and calculation) - Order of an Element modulo (ord𝑛(𝑎)) (Properties) - Primitive Roots (Existence criteria, properties) - Quadratic Residues& Non-Residues (Definition) - Legendre Symbol (( 𝑎 Theorems: - Fermats Little Theorem (FLT) - Eulers Totient Theorem - Wilsons Theorem - Chinese Remainder Theorem (CRT) (Solvability and construction) - Lagranges Theorem (for polynomial roots modulo p) - Lifting The Exponent Lemma (LTE) - Eulers Criterion - Law of Quadratic Reciprocity (and properties for ( 1 𝑝 )) 𝑝 )) (Definition and properties) 𝑝 ), ( Diophantine Equations: tools_concepts: - Linear Diophantine Equations (𝑎𝑥 + 𝑏𝑦 = 𝑐) (Structure of solutions) - Pythagorean Triples (𝑥2 + 𝑦2 = 𝑧2) (Parametrization) 31 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers - Pells Equation (𝑥2 𝐷𝑦2 = 1) (Structure of solutions, fundamental solution) - Factoring Techniques for Diophantine Equations (Difference of squares, sum/difference of cubes, etc.) - Modular Arithmetic Constraints for Diophantine Equations (Proving no solutions) - Infinite Descent Method (Fermats Method) - Vieta Jumping Technique - Bounding/Ordering Variables in Diophantine Equations theorems: - Thues Theorem (Finiteness of solutions - conceptual awareness) - Catalans Conjecture (Mihailescus Theorem) (Specific unique solution 32 23 = 1) Number Theoretic Functions: tools_concepts: - 𝜙(𝑛) (Eulers Totient Function) (Formula, multiplicativity) - 𝑑(𝑛) or 𝜏(𝑛) (Number of Divisors Function) (Formula, multiplicativity) - 𝜎(𝑛) (Sum of Divisors Function) (Formula, multiplicativity, 𝜎𝑘 (𝑛)) - 𝜇(𝑛) (Möbius Function) (Definition, multiplicativity) - Floor Function (𝑥)& Fractional Part ({𝑥}) (Properties) - Definitions of Perfect Numbers, Amicable Numbers theorems: - Möbius Inversion Formula Polynomials in Number Theory: tools_concepts: - Integer-valued polynomials (Properties) - Cyclotomic Polynomials (Φ𝑛(𝑥)) (Definition, properties, values) theorems: - Rational Root Theorem - Gausss Lemma (on polynomial content and irreducibility) - Eisensteins Criterion (for irreducibility over ℚ) Algebra: Polynomials: tools_concepts: - Polynomial Long Division and Remainder Theorem - Factor Theorem and Root Theorem - Vietas Formulas (Relating roots and coefficients) - Symmetric Sums of Roots (Expressing symmetric polynomials in terms of elementary symmetric polynomials) - Properties of Polynomial Roots (Real, complex, conjugate pairs) - Divisibility of Polynomials - Polynomial Interpolation (e.g., Lagrange Interpolation concept) - Integer Roots and Rational Root Theorem - Content of Polynomial - Cyclotomic Polynomials (Algebraic properties, connection to roots 32 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers of unity) theorems: - Fundamental Theorem of Algebra (Existence of roots in ℂ) - Newtons Sums (Relating power sums of roots and elementary symmetric polynomials) - Gausss Lemma (on polynomial content and irreducibility over ℚ) - Eisensteins Criterion (for irreducibility over ℚ) - Lucass Theorem (for binomial coefficients modulo - often used with polynomials over finite fields) Inequalities: tools_concepts: - Basic Inequality Properties (Transitivity, addition/multiplication by constants) - Completing the Square - Trivial Inequality (𝑥2 0) - Rearrangement of Terms / Substitution Techniques - Homogenization and Normalization - Convexity/Concavity of Functions (Conceptual basis for Jensens) - Smoothing Principle (Reducing variables or making terms closer) theorems: - AM-GM Inequality (Arithmetic Mean - Geometric Mean) - GM-HM Inequality (Geometric Mean - Harmonic Mean) - Weighted AM-GM Inequality - Cauchy-Schwarz Inequality (Engel form, Titus Lemma) - Rearrangement Inequality - Jensens Inequality (for convex/concave functions) - Muirheads Inequality (for comparing symmetric sums) - Schurs Inequality - Holders Inequality - Minkowskis Inequality - Nesbitts Inequality (Specific common Olympiad inequality) Functional Equations: tools_concepts: - Substitution of Specific Values (e.g., x=0, y=1, y=x, y=-x) - Checking for Injectivity, Surjectivity, Bijectivity - Finding Fixed Points ( 𝑓 (𝑥) = 𝑥) - Exploiting Symmetry - Iteration of the function ( 𝑓 ( 𝑓 (𝑥)), etc.) - Reduction to Known Equations (e.g., Cauchy forms) - Assuming properties (continuity, differentiability) to find candidate solutions (then verifying for all reals if needed) - Domain and Range Analysis theorems: standard forms - Cauchys Functional Equation ( 𝑓 (𝑥 + 𝑦) = 𝑓 (𝑥) + 𝑓 ( 𝑦)) (and its solutions over ℚ, ℝ with conditions) # Often, the \"theorems\" are the well-known solutions to 33 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers - Jensens Functional Equation ( 𝑓 ( 𝑥+𝑦 - DAlemberts Functional Equation (Cosine form: 2 𝑓 (𝑥) 𝑓 ( 𝑦)) - DAlemberts Functional Equation (Sine form: 𝑓 (𝑥)2 𝑓 ( 𝑦)2) 2 ) = 𝑓 (𝑥 )+ 𝑓 ( 𝑦) ) 𝑓 (𝑥 + 𝑦) + 𝑓 (𝑥 𝑦) = 𝑓 (𝑥 + 𝑦) 𝑓 (𝑥 𝑦) = Sequences and Series: tools_concepts: - Arithmetic Progressions (AP) (Definition, sum formula) - Geometric Progressions (GP) (Definition, sum formula, infinite GP sum) - Recurrence Relations (Definition, finding terms) - Linear Homogeneous Recurrence Relations with Constant Coefficients (Method of characteristic equation) - Linear Non-Homogeneous Recurrence Relations - Telescoping Sums and Products - Bounding Sequences (Monotonicity, boundedness) - Summation Techniques ((cid:205) 𝑘, (cid:205) 𝑘2, (cid:205) 𝑘3) - Difference Operator / Finite Calculus (less common, but tool) theorems: - Binets Formula (for Fibonacci numbers - example of solving recurrence) - Master Theorem for divide-and-conquer recurrences (more CS, but spirit can appear) # Specific convergence/divergence tests are usually beyond Olympiad scope unless very elementary # Focus on concrete applications rather than deep Abstract Algebra (Elements relevant to Olympiads): tools_concepts: theory - Basic Group Properties (Closure, associativity, identity, inverse - often in modular arithmetic or transformations) - Cyclic Groups (Conceptual understanding, e.g., (ℤ/𝑝ℤ) is cyclic) - Lagranges Theorem (Order of subgroup divides order of group - often used in Number Theory contexts) - Group Actions on Sets (Conceptual basis for Burnsides Lemma/Polya Enumeration) - Basic Ring and Field Properties (e.g., ℤ𝑝 is field, properties of polynomial rings - conceptual) theorems: - Cayleys Theorem (Every group is isomorphic to group of permutations - conceptual) # Most deep theorems from abstract algebra are beyond typical Olympiad scope, but their spirit or basic consequences can be used. Combinatorics: Basic Counting Principles: tools_concepts: 34 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers (cid:1)) (Properties of binomial coefficients, Pascals - Addition Principle (Rule of Sum) - Multiplication Principle (Rule of Product) - Permutations (𝑃(𝑛, 𝑘)) - Combinations ((cid:0)𝑛 𝑘 Identity) - Stars and Bars (Combinations with repetition, solutions to 𝑥1 + ... + 𝑥𝑘 = 𝑛) - Casework and Complementary Counting (Strategy) - Bijection Principle (Counting one set by mapping to another) - Double Counting (Establishing an identity by counting quantity in two ways) theorems: - Binomial Theorem ((𝑥 + 𝑦)𝑛 = (cid:205) (cid:0)𝑛 𝑘 - Multinomial Theorem - Vandermondes Identity (for binomial coefficients) - Lucass Theorem (for (cid:0)𝑛 𝑘 (cid:1) (mod 𝑝)) (cid:1) 𝑥 𝑘 𝑦𝑛𝑘) Advanced Counting Techniques: tools_concepts: - Principle of Inclusion-Exclusion (PIE) (General formula and applications) - Derangements (!n or 𝐷𝑛) - Recursion and Recurrence Relations (Setting up combinatorial recurrences) - Generating Functions (Ordinary - OGF) (Representing sequences, solving recurrences, coefficient extraction) - Exponential Generating Functions (EGF) (For labeled objects, permutations) - Rook Polynomials (For counting placements on board with restrictions - less common) theorems: # Many \"theorems\" here are specific results derived using these techniques or identities related to generating functions. Graph Theory: tools_concepts: - Basic Definitions (Vertices, edges, degree, directed/undirected graphs, weighted graphs) - Paths, Cycles, Connectedness, Components - Trees (Properties: connected) - Bipartite Graphs (Characterization: - Adjacency Matrix and Incidence Matrix (Representations) - Graph Isomorphism - Eulerian Paths and Circuits (Conditions based on vertex degrees) - Hamiltonian Paths and Cycles (No simple general condition, often no odd cycles) 1 edges, unique paths, acyclic and 𝑛 35 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers NP-complete - problems focus on specific cases or properties) - Planar Graphs (Definition, Kuratowskis Theorem - conceptual awareness) - Graph Coloring (Vertex coloring, chromatic number, edge coloring) - Matchings in Graphs (e.g., Halls Marriage Theorem condition) - Network Flows (Max-flow min-cut theorem - conceptual) theorems: - Handshaking Lemma ((cid:205) deg(𝑣) = 2𝐸) - Eulers Formula for Planar Graphs (𝑉 𝐸 + 𝐹 = 2) - Mantels Theorem (Max edges in triangle-free graph) - Turans Theorem (Generalization of Mantels Theorem - extremal graph theory) - Halls Marriage Theorem (Condition for perfect matching in bipartite graphs) - Cayleys Formula (Number of spanning trees in 𝐾𝑛 is 𝑛𝑛2) - Ramseys Theorem (Existence of monochromatic cliques in edge-colored complete graphs, e.g., 𝑅(3, 3) = 6) Combinatorial Designs& Extremal Combinatorics: tools_concepts: - Set Systems (Families of subsets) - Intersecting Families, Antichains - Pigeonhole Principle (PHP) (Simple and generalized forms, advanced applications) - Extremal Principle (Considering objects with maximal/minimal properties) - Design Theory Basics (e.g., Balanced Incomplete Block Designs - BIBD - conceptual) - Latin Squares, Orthogonal Latin Squares theorems: - Sperners Theorem (Max size of an antichain) - Dilworths Theorem (Relating antichains and chain decompositions in posets) - Erdos-Ko-Rado Theorem (Max size of an intersecting family of k-subsets) # Many results in this area are specific extremal bounds rather than general named theorems taught widely. Probabilistic Method& Combinatorial Probability: tools_concepts: - Basic Probability (Sample space, events, independence, conditional probability) - Expected Value (Linearity of Expectation) - Indicator Variables (Random variables, powerful tool with linearity of expectation) - Basic Probabilistic Method (Showing existence by proving probability > 0) 36 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers - Alteration Method (Modifying random structure to get desired properties) - Lovasz Local Lemma (Advanced tool for rare events - conceptual awareness) theorems: # Results are often existence proofs rather than named theorems, e.g., \"There exists graph with property X...\" # Markovs Inequality, Chebyshevs Inequality (Basic probabilistic bounds) Combinatorial Games& Processes: tools_concepts: - Invariants (Quantities that remain unchanged during process) - Monovariants (Quantities that strictly increase or decrease) - Winning and Losing Positions (P-positions, N-positions) - Symmetry Arguments in Games - Strategy Stealing Arguments - Sprague-Grundy Theorem (Nim-values, for impartial games - conceptual awareness) - Coloring Arguments (e.g., tiling problems) theorems: # Often specific to the game or process being analyzed. # Zermelos Theorem (Existence of winning strategy in finite, perfect information, two-player games with no draws) Geometry: Triangle Geometry (Advanced): tools_concepts: - Special Points (Centroid, Incenter, Circumcenter, Orthocenter, Excenters, Nagel Point, Gergonne Point, Isodynamic Points, Isogonal Conjugates, Symmedian Point/Lemoine Point) and their properties/collinearities/concurrencies. - Euler Line and Nine-Point Circle (Properties and relations) - Simson Line (and its generalizations) - Pedal Triangles - Cevas Theorem (Trigonometric, standard, and converse forms) - Menelaus Theorem (Standard and converse forms) - Angle Bisector Theorem (Internal and External) - Stewarts Theorem - Rouths Theorem (for ratios in cevians) - Properties of Medians, Altitudes, Angle Bisectors (lengths, intersections) - Isogonal Conjugacy (Definition and key properties) - Isotomic Conjugacy (Definition and key properties) - Brocard Points and Brocard Angle theorems: - Morleys Trisector Theorem - Napoleons Theorem - Van Aubels Theorem - Feuerbachs Theorem (Nine-point circle tangency to SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers incircle/excircles) - Lesters Theorem (Circumcenter, nine-point center, and Brocard points are concyclic) Circles and Cyclic Quadrilaterals: tools_concepts: - Power of Point Theorem (for intersecting secants, tangents, chords) - Radical Axis (of two circles, properties, construction) - Radical Center (of three circles) - Coaxal Circles (Pencils of circles) - Properties of Cyclic Quadrilaterals (Opposite angles sum to 180, external angle property) - Properties of Tangential Quadrilaterals (Opposite sides sum equally) - Ptolemys Theorem and Ptolemys Inequality (for cyclic and general quadrilaterals) - Caseys Theorem (Generalized Ptolemys Theorem for tangent circles) - Directed Angles (modulo 𝜋 or 2𝜋, for handling configurations carefully) theorems: - Brahmaguptas Formula (Area of cyclic quadrilateral) - Japanese Theorem for Cyclic Quadrilaterals - Miquels Theorem and Miquel Point (for complete quadrilaterals or triangles with points on sides) Geometric Transformations: tools_concepts: - Homothety (Dilation) (Properties, center of homothety, composition) - Rotation (Properties, center of rotation, composition) - Reflection (Properties, composition creating rotations/translations) - Translation - Spiral Similarity (Composition of homothety and rotation) - Inversion (Properties: preserves angles (conformal), changes distances in specific way, center of inversion) - Glide Reflection theorems: # Many \"theorems\" are properties preserved or created by these transformations. # e.g., \"Homothety maps line to parallel line.\" # e.g., \"Inversion preserves cross-ratios of four concyclic points if center of inversion is also on the circle.\" (Special case) maps circles/lines to circles/lines, Analytic and Vectorial Geometry (as Tools): tools_concepts: 38 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers - Cartesian Coordinates (Distance, slope, midpoint, equation of line/circle) - Vector Addition, Subtraction, Scalar Multiplication - Dot Product (for angles, perpendicularity, lengths) - Cross Product (for area, normal vectors, collinearity in 3D, orientation) - Barycentric Coordinates (Representing points in triangle, proving concurrency/collinearity, area ratios) - Complex Numbers in Geometry (Representing points, vectors, rotations, similarities, conditions for collinearity/concyclicity, roots of unity for regular polygons) - Distance Formulas (Point-line, point-plane) theorems: # Results are often derived using these tools rather than being standalone theorems from this \"topic\" for Olympiads. # e.g., Shoelace Formula (for area of polygon using coordinates). # e.g., Conditions for collinearity/concurrency using determinants or vector dependencies. Projective Geometry (Elements for Olympiads): tools_concepts: - Points at Infinity, Line at Infinity - Duality Principle - Cross-Ratio (of four collinear points or four concurrent lines, invariance under projection) - Harmonic Bundles and Harmonic Conjugates (Cross-ratio = -1, geometric constructions) - Perspective Triangles - Complete Quadrilaterals and Complete Quadrangles (Harmonic properties) - Poles and Polars (with respect to conic, especially circle) theorems: - Desargues Theorem (Perspective triangles from point and line) - Pappuss Hexagon Theorem - Pascals Theorem (for hexagons inscribed in conic) - Brianchons Theorem (for hexagons circumscribed about conic) # La Hires Theorem Solid Geometry (Euclidean 3D): tools_concepts: - Lines and Planes in 3D (Intersection, parallelism, perpendicularity, skew lines) - Dihedral Angles, Solid Angles - Properties of Basic Solids (Prisms, pyramids, cylinders, cones, spheres) - Eulers Formula for Polyhedra (𝑉 𝐸 + 𝐹 = 2) - Regular Polyhedra (Platonic solids, properties) SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers - Coordinate Geometry in 3D (Distance, equations of planes/lines, vector operations) - Cavalieris Principle (for volumes) theorems: # Fewer named \"theorems\" at Olympiad level compared to 2D; more about spatial reasoning and application of 2D principles in planes. # e.g., Relationship between slant height, height, and base radius of cone. # Desargues Theorem can be proven using 3D perspective. Combinatorial Geometry: tools_concepts: - Convex Hull (Properties, algorithms like Graham scan - conceptual) - Geometric Inequalities (e.g., Triangle inequality, Ptolemys inequality, Isoperimetric inequality - basic form) - Covering, Packing, and Tiling Problems (Basic examples) - Incidence Problems (e.g., Sylvester-Gallai problem spirit) - Coloring Geometric Configurations - Hellys Theorem (Intersection of convex sets - conceptual) theorems: - Sylvester-Gallai Theorem (Given points, not all collinear, there is line containing exactly two of them) - Picks Theorem (Area of simple polygon whose vertices are integer lattice points) # Erdos-Anning Theorem # De Bruijn-Erdos theorem (projective plane version) Probability: Foundations and Basic Computations: tools_concepts: - Sample Space, Event Space, Probability Measure (Axioms of Probability) - Equally Likely Outcomes (Classical Definition: Total) - Complementary Events (𝑃( 𝐴) = 1 𝑃( 𝐴)) - Union and Intersection of Events (𝑃( 𝐴 𝐵), 𝑃( 𝐴 𝐵)) - Conditional Probability (𝑃( 𝐴𝐵) = 𝑃( 𝐴 𝐵)/𝑃(𝐵)) - Multiplication Rule for Conditional Probability (𝑃( 𝐴 𝐵) = 𝑃( 𝐴𝐵)𝑃(𝐵)) - Independent Events (𝑃( 𝐴 𝐵) = 𝑃( 𝐴)𝑃(𝐵)) - Law of Total Probability (Partitioning sample space, 𝑃( 𝐴) (cid:205) 𝑃( 𝐴𝐵𝑖)𝑃(𝐵𝑖)) - Bayes Theorem - Tree Diagrams (For visualizing multi-stage experiments) theorems: - Booles Inequality / Union Bound (𝑃(𝐴𝑖) (cid:205) 𝑃( 𝐴𝑖)) - Bonferroni Inequalities (Generalizations of PIE for probability) Favorable / = Discrete Random Variables and Expectation: tools_concepts: 40 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers - Random Variable (Definition, discrete case) - Probability Mass Function (PMF) - Expected Value (Expectation) of Discrete Random Variable (𝐸[𝑋] = (cid:205) 𝑥 𝑃(𝑋 = 𝑥)) - Linearity of Expectation (𝐸[𝑋 + 𝑌 ] = 𝐸[𝑋] + 𝐸[𝑌 ], 𝐸[𝑐𝑋] = 𝑐𝐸[𝑋] - powerful even if X, not independent) - Indicator Variables (Bernoulli random variables, 𝐼 𝐴 occurs, 0 otherwise; 𝐸[𝐼 𝐴] = 𝑃( 𝐴)) - Variance and Standard Deviation (Basic definitions, properties like 𝑉 𝑎𝑟(𝑋) = 𝐸[𝑋 2] (𝐸[𝑋])2) - Covariance (For 𝑉 𝑎𝑟(𝑋 + 𝑌 ), conceptual) - Common Discrete Distributions (Bernoulli, Binomial, Geometric, Hypergeometric - recognizing their structure in problems) theorems: # Linearity of Expectation itself is theorem-level concept in its application. # Formulas for E[X] and Var(X) for common distributions (e.g., E[Bin(n,p)]=np) 1 if = tools_concepts: Combinatorial Probability (Heavy Overlap with Combinatorics Branch): - Counting Techniques (Permutations, combinations, stars and bars, PIE) applied to find favorable/total outcomes. - Problems involving Selections, Arrangements, Distributions with probabilistic questions. - Derangements (Probability that no item is in its original position). - Matching Problems (e.g., Hat-check problem). - Probabilistic arguments for existence (If 𝑃(Property holds) then an object with that property exists). theorems: # Many \"theorems\" are combinatorial identities used in probabilistic context. # e.g., using Vandermondes Identity to sum probabilities in hypergeometric settings. > 0, Geometric Probability: tools_concepts: - Calculating probabilities as ratios of lengths, areas, or volumes. - Problems involving random points in geometric figures (intervals, squares, circles, cubes). - Buffons Needle Problem (Classic example - conceptual understanding). - Coordinate Geometry methods to define regions and calculate areas/volumes. - Symmetry arguments to simplify calculations. theorems: # Results are usually derived from geometric formulas rather than 41 SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers specific \"probability theorems\" for geometry. # Bertrands Paradox (Illustrates importance of defining \"random\" precisely - conceptual) Stochastic Processes& Recurrence Relations (Introductory/Discrete): tools_concepts: - Random Walks (1D, 2D - simple cases, probability of reaching state, expected number of steps). - Markov Chains (Finite state space, transition probabilities, transition matrix - basic concepts). - First Step Analysis (Setting up recurrence relations for probabilities or expected values in processes). - Gamblers Ruin Problem (Classic example). - Waiting Time Problems (e.g., expected number of trials until first success - Geometric distribution). - Absorption Probabilities and Expected Time to Absorption (for simple Markov chains). theorems: # Solutions to standard recurrence relations for probabilities/expectations. # Existence of steady-state distributions for certain types of Markov chains (conceptual). Advanced Probabilistic Techniques (Rare, but can inspire hard problems): tools_concepts: - Probabilistic Method (Using probability to prove deterministic combinatorial results - beyond basic existence). - Alteration Method (Refining random construction). - Second Moment Method (Using variance to show concentration around the mean - very advanced for Olympiads). - Martingales (Sequence of random variables where conditional expectation of next given current is current - highly advanced, but simplest forms/ideas might inspire). theorems: - Markovs Inequality (𝑃(𝑋 𝑎) 𝐸[𝑋]/𝑎) - Chebyshevs Inequality (𝑃(𝑋 𝜇 𝑘𝜎) 1/𝑘2) - Chernoff Bounds / Hoeffdings Inequality (Exponential tail bounds - conceptual awareness, very rare). - Lovasz Local Lemma (Tool for showing existence when events are \"mostly\" independent - very advanced)."
        }
    ],
    "affiliations": [
        "Advanced Micro Devices, Inc. (AMD)"
    ]
}