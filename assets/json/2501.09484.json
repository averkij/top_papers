{
    "paper_title": "Exploring the Inquiry-Diagnosis Relationship with Advanced Patient Simulators",
    "authors": [
        "Zhaocheng Liu",
        "Quan Tu",
        "Wen Ye",
        "Yu Xiao",
        "Zhishou Zhang",
        "Hengfu Cui",
        "Yalun Zhu",
        "Qiang Ju",
        "Shizheng Li",
        "Jian Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Online medical consultation (OMC) restricts doctors to gathering patient information solely through inquiries, making the already complex sequential decision-making process of diagnosis even more challenging. Recently, the rapid advancement of large language models has demonstrated a significant potential to transform OMC. However, most studies have primarily focused on improving diagnostic accuracy under conditions of relatively sufficient information, while paying limited attention to the \"inquiry\" phase of the consultation process. This lack of focus has left the relationship between \"inquiry\" and \"diagnosis\" insufficiently explored. In this paper, we first extract real patient interaction strategies from authentic doctor-patient conversations and use these strategies to guide the training of a patient simulator that closely mirrors real-world behavior. By inputting medical records into our patient simulator to simulate patient responses, we conduct extensive experiments to explore the relationship between \"inquiry\" and \"diagnosis\" in the consultation process. Experimental results demonstrate that inquiry and diagnosis adhere to the Liebig's law: poor inquiry quality limits the effectiveness of diagnosis, regardless of diagnostic capability, and vice versa. Furthermore, the experiments reveal significant differences in the inquiry performance of various models. To investigate this phenomenon, we categorize the inquiry process into four types: (1) chief complaint inquiry; (2) specification of known symptoms; (3) inquiry about accompanying symptoms; and (4) gathering family or medical history. We analyze the distribution of inquiries across the four types for different models to explore the reasons behind their significant performance differences. We plan to open-source the weights and related code of our patient simulator at https://github.com/LIO-H-ZEN/PatientSimulator."
        },
        {
            "title": "Start",
            "content": "Exploring the Inquiry-Diagnosis Relationship with Advanced Patient Simulators Quan Tu Gaoling School of Artificial Intelligence, Renmin University of China Beijing, China quantu@ruc.edu.cn Wen Ye Baichuan Inc. Beijing, China yewen@baichuan-inc.com Zhaocheng Liu Baichuan Inc. Beijing, China lio.h.zen@gmail.com 5 2 0 2 6 ] . [ 1 4 8 4 9 0 . 1 0 5 2 : r Yu Xiao Baichuan Inc. Beijing, China xiaoyu@baichuan-inc.com Yalun Zhu Baichuan Inc. Beijing, China zhuyalun@baichuan-inc.com Hengfu Cui Baichuan Inc. Beijing, China cuihengfu@baichuan-inc.com Shizheng Li Baichuan Inc. Beijing, China lishizheng@baichuan-inc.com Zhishou Zhang Baichuan Inc. Beijing, China zhangzhishou@baichuan-inc.com Qiang Ju Baichuan Inc. Beijing, China liulifeng@baichuan-inc.com Jian Xie Baichuan Inc. Beijing, China richard@baichuan-inc.com Abstract Online medical consultation (OMC) restricts doctors to gathering patient information solely through inquiries, making the already complex sequential decision-making process of diagnosis even more challenging. Recently, the rapid advancement of large language models has demonstrated significant potential to transform OMC. However, most studies have primarily focused on improving diagnostic accuracy under conditions of relatively sufficient information, while paying limited attention to the \"inquiry\" phase of the consultation process. This lack of focus has left the relationship between \"inquiry\" and \"diagnosis\" insufficiently explored. In this paper, we first extract real patient interaction strategies from authentic doctorpatient conversations and use these strategies to guide the training of patient simulator that closely mirrors real-world behavior. By inputting medical records into our patient simulator to simulate patient responses, we conduct extensive experiments to explore the relationship between \"inquiry\" and \"diagnosis\" in the consultation process. Experimental results demonstrate that inquiry and Medical expert consultants. Corresponding authors. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference17, July 2017, Washington, DC, USA 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM https://doi.org/10.1145/nnnnnnn.nnnnnnn diagnosis adhere to the Liebigs law: poor inquiry quality limits the effectiveness of diagnosis, regardless of diagnostic capability, and vice versa. Furthermore, the experiments reveal significant differences in the inquiry performance of various models. To investigate this phenomenon, we categorize the inquiry process into four types: (1) chief complaint inquiry; (2) specification of known symptoms; (3) inquiry about accompanying symptoms; and (4) gathering family or medical history. We analyze the distribution of inquiries across the four types for different models to explore the reasons behind their significant performance differences. We plan to opensource the weights and related code of our patient simulator at https://github.com/LIO-H-ZEN/PatientSimulator. ACM Reference Format: Zhaocheng Liu, Quan Tu, Wen Ye, Yu Xiao, Zhishou Zhang, Hengfu Cui, Yalun Zhu, Qiang Ju, Shizheng Li, and Jian Xie. 2025. Exploring the InquiryDiagnosis Relationship with Advanced Patient Simulators. In . ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn"
        },
        {
            "title": "1 Introduction\nOnline medical consultation (OMC) [3, 15], as an emerging form\nof medical service, has greatly enhanced the convenience of seek-\ning medical care, especially in regions with insufficient medical\nresources. However, compared to traditional face-to-face consulta-\ntions, online consultations present notable limitations. Due to the\nabsence of direct physical examinations and auxiliary diagnostic\ntools, physicians must rely solely on patients‚Äô descriptions and in-\nquiries to gather relevant information. This approach restricts a\ncomprehensive assessment of the patient‚Äôs health condition and\nsignificantly increases the complexity of diagnosis.",
            "content": "Conference17, July 2017, Washington, DC, USA Zhaocheng Liu, Quan Tu, Wen Ye, Yu Xiao, Zhishou Zhang, Hengfu Cui, Yalun Zhu, Qiang Ju, Shizheng Li, and Jian Xie Figure 1: Using the same patient records and doctor model, our patient simulator (shown on the right in the figure) is compared to the baseline patient simulator (prompt engineering on GPT-4o, shown on the left in the figure). Online consultation dialogues are divided into inquiry and diagnosis stages, with representing the doctor and representing the patient in the figure. Based on the predefined set of dialogue strategies outlined in this paper, the dialogue strategies output by our model are highlighted in purple. The output from our patient simulator may contain emotions or proactive questions, marked in green. In contrast, the baseline tends to provide more comprehensive symptoms in the first round, with additional symptoms and resulting significant differences highlighted in red. These dimensions illustrate that our model better approximates real patient. In recent years, large language models (LLMs) have demonstrated remarkable capabilities across various domains and tasks. Notably, models such as OpenAIs o1 [22] have introduced groundbreaking reasoning abilities by employing techniques akin to an internalized chain-of-thought [35] process. Building on the core strengths of general-purpose LLMs, domain-specific models [6, 25, 28, 32, 39] tailored for healthcare have also emerged. In the field of clinical medicine, numerous studies [12, 13, 19, 26, 34, 36] have validated the performance of these models, suggesting their potential for transformative applications in medical practice. For instance, on the MedQA (USMLE) benchmark [12], models like GPT-4 [1] with MedPrompt [20], Med-Gemini-L 1.0 [25], and o1-preview [36] have achieved performance levels surpassing those of human experts. However, most doctor models focus on improving diagnostic accuracy under relatively sufficient information conditions, which clearly diverges from the primary challenges faced in online consultations. OMC can be divided into two key stages: \"inquiry\" and \"diagnosis\". Existing research has paid relatively little attention to the \"inquiry\" stage, and this oversight have hindered deeper understanding of the relationship between \"inquiry\" and \"diagnosis.\" Although some studies [16, 17, 24, 26, 34] have attempted to evaluate or improve doctor models by simulating clinical environments. These studies use prompt engineering to construct patient agents, but the simulated results show significant discrepancies compared to the behavior of real patients. For example, real-life patients may exhibit concern and anxiety about their condition when responding. In their initial description of symptoms, they tend to urgently convey the symptoms they are most concerned about, rather than providing comprehensive list of all symptoms. Additionally, real patients might actively ask questions to alleviate their emotions. Moreover, real-life patients cannot always patiently answer questions. If doctor (especially doctor agent) keeps asking questions repeatedly, real patients may decide to exit the conversation or Exploring the Inquiry-Diagnosis Relationship with Advanced Patient Simulators Conference17, July 2017, Washington, DC, USA refuse to respond. Unfortunately, these issues are difficult to address effectively through prompt engineering alone, necessitating the exploration of novel paradigm for simulating patients. In addition, to the best of our knowledge, existing studies that provide dynamic simulation environments have yet to thoroughly investigate the relationship between \"inquiry\" and \"diagnosis\" and its impact on overall outcomes. In this paper, we extract patient dialogue strategies from real doctor-patient conversations to guide the development of patient simulator that closely resembles actual interactions. Initially, we annotate and standardize open-source real doctor-patient conversations using LLMs, and then summarize set of patient dialogue strategies. From this set, we manually select strategies that meet specific criteria, such as ensuring dialogue rounds are complete and excluding follow-up visits in favor of initial consultations. Given that the amount of usable training data is limited after selection and lacks corresponding medical records, we synthesize doctorpatient dialogue data using in-context learning. Specifically, our synthesis uses two types of input: 1. Various formats of disease medical records (similar to context in MedQA); 2. randomly selected dialogue strategy from the curated set. Ultimately, we train our model entirely on this synthesized doctor-patient dialogue data along with the relevant medical records. After evaluation, our patient simulator demonstrates lower hallucination rate in terms of dialogue and medical record consistency, although the rate of unrelated responses is slightly higher. In addition, there is significant improvement in anthropomorphism, including emotions and dialogue strategies. It is important to note that the slightly higher rate of unrelated responses does not necessarily indicate worse model performance because real patients also exhibit some degree of refusal to answer. Our unrelated responses mainly occur towards the end of dialogues, especially when the doctor model poses numerous questions, prompting the patient simulator to ask questions actively instead of responding directly. Based on our patient simulator, we conduct extensive experiments to explore the relationship between \"inquiry\" and \"diagnosis\" and their impact on the accuracy of final diagnoses. Specifically, we utilize our patient simulator to fix patient simulations, while interacting with this simulator through different doctor models for fixed number of rounds to generate inquiry records. Subsequently, each inquiry record is diagnosed using various doctor models. Upon analyzing the diagnostic accuracy of the inquiries produced by different doctor models, we find that some models consistently yield inquiries with significantly high or low accuracy, regardless of which doctor model performed the diagnosis. This indicates that there are significant differences in the inquiries generated by different doctor models. Furthermore, when comparing inquiries of relatively high and low quality, and observing the accuracy differences after being diagnosed by doctor models with distinctly different diagnostic capabilities, we argue that the relationship between \"inquiry\" and \"diagnosis\" adheres to the Liebigs law. In other words, if the quality of the inquiry is lacking, strong diagnostic capabilities alone are insufficient to achieve good outcomes, and vice versa. To further analyze the differences in inquiry processes among different doctor models, we categorize the inquiries into four types: (1) chief complaint inquiry; (2) specification of known symptoms; Figure 2: Prompts for synthesizing patient simulator training dialogues. (3) inquiry about accompanying symptoms; (4) gathering family or medical history. We calculate the distribution of inquiry records across these four types for different inquiry models. By comparing the distribution differences and diagnostic accuracy among these models, we uncover certain correlation. For instance, when model asks more questions to specify known symptoms, resulting in relatively fewer inquiries of other types, the final diagnostic accuracy tends to be lower. Therefore, our findings indicate that effectively allocating inquiries within the limited opportunities (generally 3 to 5 rounds that patients can comfortably accept) is problem worth exploring in depth."
        },
        {
            "title": "2.1 Methods\nSome studies [16, 17, 24, 26, 34] have attempted to assess or en-\nhance doctor models by creating simulated clinical environments.\nIn these studies, prompt engineering is often used to construct\npatient agents. However, the interactions produced by this method\nsignificantly differ from those with real patients. Specifically, real-\nlife patients may exhibit concerns and anxieties about their medical\nconditions when they communicate. They tend to urgently express\nthe primary symptom that worries them the most during the initial\ndescription, rather than providing an exhaustive list of symptoms.\nAdditionally, real patients may actively seek information to allay\ntheir emotional distress. Moreover, they are unlikely to remain\npatient indefinitely when responding to questions. If physicians\n(especially doctor agents) persistently ask questions, real patients\nmight choose to terminate the conversation or refuse to answer.",
            "content": "Conference17, July 2017, Washington, DC, USA Zhaocheng Liu, Quan Tu, Wen Ye, Yu Xiao, Zhishou Zhang, Hengfu Cui, Yalun Zhu, Qiang Ju, Shizheng Li, and Jian Xie {ùëë1, ùëù1, ùëë2, ùëù2, . . . , ùëëùëõ, ùëùùëõ }, where ùëëùëñ represents the i-th round of doctor dialogue and ùëùùëñ represents the i-th round of patient dialogue. We divide it into ùëõ SFT data instances, that is, {ùëë1, ùëù1}, {ùëë1, ùëù1, ùëë2, ùëù2}, ..., {ùëë1, ùëù1, . . . , ùëëùëõ, ùëùùëõ }. It is important to note that for each SFT data instance, we only retain the dialogue strategy tags for the label (the last turn of the patient dialogue). The strategy tags in the preceding dialogues are removed. This is to align with the estimated scenarios of the patient simulator, as the doctor model is not expected to output our dialogue strategy tags. The model needs to learn to predict the appropriate dialogue strategy and the content to be conveyed in the absence of dialogue strategy tags in the context. We train the LoRA [10] weights of the patient simulator on the Qwen2.5-72B-Instruct [37] model. Finally, we plan to open-source the weights and related code at https://github.com/LIO-H-ZEN/PatientSimulator."
        },
        {
            "title": "2.2 Evaluation Results",
            "content": "Table 1: Evaluation results of different patient simulators based on our defined Hallucination Rate (HR), Irrelevant Response Rate (IRR) and Anthropomorphism Score (AS). The final row presents the consistency results, derived from sample checks, between the performance of GPT-4o and human evaluations across these three indicators. Model Qwen2.5-72B-Instruct AgentClinic ours HR IRR AS 4.97% 3.71% 0.31% 7.48% 0.93% 4.79% 0.28 0.31 0.87 Alignment with Human Evaluation 99% 100% 90.6% To evaluate the training status of our patient simulator and compare the effects with the baseline [26], we conduct extensive experiments. We begin by considering real-life scenarios and design set of concise and practical patient simulator metrics, which primarily include the following three indicators: Hallucination Rate (HR): The proportion of dialogue turns where the patient produces responses contradicting the medical record. By inputting the medical record and each round of dialogue content, GPT-4o assigns score (0 or 1), and the calculated proportion is evidently better when it is lower. Irrelevant Response Rate (IRR): The proportion of dialogue turns where the patient does not address the questions posed by the doctor model. It involves inputting the doctors inquiries and the patients responses, with GPT-4o assigning score of 0 or 1. Since certain level of irrelevant answers is also present in real patients, this metric does not necessarily need to be as low as possible and serves as reference value during application. Anthropomorphism Score (AS): Analyzing the anthropomorphic behaviors exhibited by the patient agent throughout the dialogue, such as expressions of emotion, proactive questioning, and the degree of colloquialism in responses. It is scored by GPT-4o on scale from 0 to 1, with values closer to 1 indicating higher level of anthropomorphism. Figure 3: The system prompt of our patient simulator. Unfortunately, it is evident that such real patient behaviors are difficult to replicate through prompt engineering. In order to simulate real patients as accurately as possible, it is necessary to rely on authentic doctor-patient dialogue datasets. In this paper, we utilize the MedDialog [38] dataset. Initially, we conduct essential data screening to remove non-consultative records (e.g., patient scheduling and registration) and to select complete initial consultation dialogues. Then, we manually provide seed set of commonly used dialogue strategy tags found in doctor-patient interactions. GPT-4o [21] is employed to expand this seed set, resulting in candidate set of dialogue strategy tags (see Appendix A). Based on the candidate set of dialogue strategy tags, GPT-4o is further used to annotate the selected complete initial consultation dialogues. Each dialogues tags are concatenated in sequence to form dialogue strategy flow. Finally, high-quality dialogue strategies are manually selected from the deduplicated set. For example, the following is selected dialogue strategy flow: [Doctor: Greeting],[Patient: Greeting],[Doctor: Chief Complaint Inquiry],[Patient: Provide Information],[Patient: Express Concerns],[Doctor: Gathering Family or Medical History], [Patient: Provide Information], [Doctor: Evaluation], [Doctor: Explanation], [Patient: Explanation Request], [Doctor: Answering], [Patient: Seek Advice], [Doctor: Medical Advice], [Patient: Discuss Treatment Options], [Doctor: Arrangement], [Patient: Seek Help], [Doctor: Medical Advice], [Patient: Thanks], [Doctor: Goodbye], [Patient: Stop]. Due to the limited availability of usable patient-doctor dialogue data for training after selection, and the absence of corresponding medical records, we synthesize patient-doctor dialogue data to facilitate the training process. We utilize the Chinese medical record dataset released by CCKS 2019 [8] as candidate set of medical records. In each data synthesis iteration, medical record is randomly selected, and dialogue strategy flow is randomly chosen from the curated set of dialogue strategy flows. Through in-context learning, we synthesize patient-doctor dialogues that align with the selected dialogue strategy flow. For detailed prompts, please refer to Figure 2. The format of this synthetic doctor-patient dialogue is shown on the right side of Figure 1. Each round of conversation between the doctor and the patient is preceded by several dialogue strategy tags. We construct supervised fine-tuning (SFT) dataset entirely based on this synthetic doctor-patient dialogue dataset. Specifically, in the training and prediction phases, our patient simulator requires only the input of patient medical records into simple system prompt (see Figure 3). Given doctor-patient dialogue Exploring the Inquiry-Diagnosis Relationship with Advanced Patient Simulators Conference17, July 2017, Washington, DC, USA"
        },
        {
            "title": "Impact on Diagnostic Accuracy",
            "content": "We aim to explore the relationship between \"inquiry\" and \"diagnosis,\" as well as its impact on the accuracy of the final diagnosis, by leveraging our patient simulator. This section provides detailed description of the experimental setup and its results."
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "Table 2: The distribution of the models used for inquiry and diagnosis. Model GPT-4o GPT-4o-mini claude-3-5-sonnet o1-mini o1-preview Inquiry Diagnosis Firstly, we describe the scenario of OMC. As depicted in Figure 1, there are only two roles: the doctor and the patient. The process begins with the doctor collecting patient information through inquiries before providing diagnosis and medical advice. In practice, these inquiries often involve multiple rounds, which we denote as ùëõ rounds. However, the number of rounds should not be excessive; generally, inquiries of up to 5 rounds are acceptable to patients. The diagnosis typically occurs in the (ùëõ + 1)ùë°‚Ñé round. With further discussions between the doctor and patient, the diagnosis may be updated in later rounds. Secondly, for simplicity, we set the inquiries to ùëõ rounds (1 ùëõ 5) and the diagnosis to be given in the (n+1)th round (i.e., the diagnosis accuracy is calculated based on the content of the (n+1)th round). In our experiments, we conduct tests for ùëõ values of 1, 2, 3, 4, and 5. The patient side consistently uses our patient simulator and medical records are provided by AgentClinics MedQA-Extend. On the doctors side, different inquiry models interact with the patient simulator for fixed ùëõ rounds to generate inquiry records. Subsequently, different doctor models were used to perform diagnosis on these inquiry records. The specific distribution of the models used for inquiry and diagnosis is shown in Table 2. The o1-mini and o1-preview participated only in diagnosis, as these models have stronger reasoning capabilities and are more suitable for diagnostic purposes. Thirdly, to address the variations in the output formats of different diagnostic models, thereby facilitating the accurate computation of diagnostic accuracy using LLMs, we design workflow (as illustrated in Figure 4). This process begins with inputting the complete dialogue content, followed by extracting the diagnostic results. These results are then subjected to necessary modifications before being compared with the ground truth (GT). The primary purpose of these modifications is to avoid false negatives that may arise from discrepancies such as aliases or differences in the granularity of the disease name. Tasks like result extraction, modification, and comparison are among the most common for LLMs and can yield preliminary results even without complex prompts. In practice, the Figure 4: Workflow for assessing diagnostic accuracy in conversations using LLMs. Our patient simulator is compared with Qwen-72B-Instruct and AgentClinic [26], the latter of which implements patient agents through prompt engineering on GPT-4. The former serves to evaluate our training process, while the latter is used to benchmark the simulation effectiveness of our patient simulator. AgentClinic utilizes multiple biased prompts that could potentially interfere with HR and IRR outcomes; therefore, these biased prompts are excluded from our experiments, and only the fundamental system prompt for the patient agent from AgentClinic is retained. Additionally, as Qwen2.5-72B-Instruct does not incorporate any system prompts, it is challenging to simulate patient scenarios. To ensure consistency, Qwen2.5-72B-Instruct also adopts the same system prompt used in AgentClinic. The experimental results, as shown in Table 1, indicate that our patient simulator significantly outperforms all baselines in terms of Hallucination Rate. This is likely due to our inclusion of patient medical records in the system prompt during training, whereas the baseline approaches rely on prompt engineering. From the perspective of IRR, our method achieves significantly lower value compared to our training starting point (Qwen2.5-72B-Instruct). However, the IRR of our method is notably higher than that of the GPT-4-based AgentClinic. This discrepancy may arise from differences in the underlying foundation models, as well as the selected dialogue strategy flow, where patients choose to ask questions proactively rather than responding to the doctors inquiries. It is important to note that lower IRR is not necessarily better and should only be considered as reference metric. Lastly, with respect to AS, our model outperforms all baselines by significant margin, confirming that our training paradigm is capable of successfully guiding the model to emulate realistic dialogue strategy flow, resembling real patients. To verify the reliability of the metrics implemented in the prompt engineering of GPT-4o, we conduct manual random sampling inspection and calculate their consistency with human evaluations. As indicated in the last row of Table 1, our implementation of the three metrics demonstrates sufficient reliability. Conference17, July 2017, Washington, DC, USA Zhaocheng Liu, Quan Tu, Wen Ye, Yu Xiao, Zhishou Zhang, Hengfu Cui, Yalun Zhu, Qiang Ju, Shizheng Li, and Jian Xie Figure 5: Patients consistently use our patient simulator, and doctors initially employ different models to interact with the simulator for fixed rounds (x-axis, values are 1, 2, 3, 4, 5) to generate inquiry records. These records are then diagnosed using different doctor models, and the diagnostic accuracy (y-axis) is calculated. key to achieving satisfactory outcomes lies in constructing robust test set and conducting multiple iterations (e.g., iterations of examples and instructions). Through sampling inspections, the inconsistency between our workflow and human evaluations remains below 1%."
        },
        {
            "title": "3.2 Experimental Results\nOur experimental results are presented in Figure 5. Patients con-\nsistently utilize our patient simulator, while doctors interact with\nthe simulator using various models for a fixed number of rounds\n(x-axis, where n values are 1, 2, 3, 4, 5) to generate inquiry records.\nSubsequently, these records are diagnosed by five different doctor\nmodels, as shown in Table 2, and the diagnostic accuracy (y-axis)\nis computed.",
            "content": "Firstly, we analyze Subfigures 2 to 6, excluding the first Subfigure in the upper left corner of Figure 5. These five Subfigures present the accuracy rates of the same three sets of inquiries processed through five different diagnostic models. By examining each Subfigure individually, it becomes apparent that under the same inquiry rounds and diagnostic models, there are significant differences in the accuracy rates of inquiries generated by different models. For example, in Subfigure 6, after 5 inquiry rounds and under the o1-preview diagnostic model, the accuracies for Claude, GPT-4o and GPT-4o-mini [21] are 0.439, 0.481, and 0.5, respectively. Furthermore, across all five Subfigures, the inquiries produced by the model claude-3-5-sonnet consistently exhibit relatively lower accuracy levels, regardless of the diagnostic model used. These indicate that there are significant differences in inquiry capabilities among the different models. Secondly, by comparing the accuracy rates of the same inquiry rounds and models in Subfigures 2 to 6, we observe that different models exhibit varying diagnostic capabilities. Among them, o1-preview demonstrates the strongest diagnostic ability, while GPT-4o-mini shows the weakest. This result correlates with the inherent reasoning capabilities of the models, aligning with intuitive expectations. By further integrating the performance of diagnostic and inquiry abilities, it is observed that there is no significant correlation between the two. For instance, while GPT-4o-mini exhibits weaker diagnostic capabilities, it performs relatively well in inquiry tasks, whereas GPT-4o demonstrates strong performance in both Exploring the Inquiry-Diagnosis Relationship with Advanced Patient Simulators Conference17, July 2017, Washington, DC, USA Chief Complaint Inquiry: This refers to asking patients about their most significant discomfort, the most prominent symptoms, or signs they experience, which often represent the primary reason for the visit. precise chief complaint provides an initial indication of the severity and urgency of the condition and offers diagnostic clues for identifying potential systemic diseases. Specification of Known Symptoms Onset and duration of illness: Each disease has unique characteristics regarding its onset and progression; thus, detailed inquiry into the onset of illness is essential for differential diagnosis. Some diseases have an acute onset, such as cerebral embolism, while others progress more slowly, like pulmonary tuberculosis. The duration of illness refers to the time from disease onset to the point of clinical consultation or hospitalization. If multiple symptoms appear, it is essential to trace back to the time of the initial symptom and document the entire medical history in chronological order. For instance, the patient may experience palpitations for 3 months and recurrent nocturnal dyspnea for 2 weeks. The characteristics of the main symptoms: The location, nature, duration, and intensity of symptoms, along with factors that alleviate or worsen them, are essential for diagnosing the affected system or organ and determining the pathological changes site, extent, and nature. For instance, upper abdominal pain often points to issues with the stomach, duodenum, or pancreas, while acute pain in the right lower abdomen typically suggests appendicitis. The type of painwhether burning, colicky, distention, or dulland whether symptoms are continuous or intermittent, as well as their onset and relief patterns, are diagnostically significant. Inquiry about Accompanying Symptoms: On the basis of the primary symptoms, series of accompanying symptoms often emerge. These accompanying symptoms are crucial for differential diagnosis or indicating possible complications. For instance, diarrhea may be common symptom of various underlying causes, making it difficult to diagnose specific disease based solely on this symptom. However, by inquiring about the accompanying symptoms, the diagnostic direction becomes clearer. For example, diarrhea accompanied by vomiting may suggest acute gastroenteritis caused by consumption of contaminated food or toxic substances, whereas diarrhea with sensation of incomplete evacuation, when considered along with seasonality and dietary habits, is more likely associated with dysentery. Gathering Family or Medical History: Family history: It is important to inquire about the health and disease conditions of the patients parents, siblings, and children. Particular attention should be paid to whether there are diseases similar to that of the patient, or hereditary diseases such as hemophilia, albinism, familial hypothyroidism, diabetes, and mental illnesses. Diagnosis and treatment history: If the patient has already received medical treatment at other healthcare facilities prior to this visit, it is essential to inquire about the previous diagnoses, treatments, and their outcomes. If treatment has been administered, thorough understanding of the medications used, including their names, dosages, durations, and effects, is necessary to inform the current diagnosis and treatment plan. Figure 6: Examples of four types of inquiry with representing the doctor and representing the patient in the figure. areas. This observation suggests that when developing medical AI models, if single model struggles to excel in both inquiry and diagnostic abilities, dividing the tasks into two specialized models could serve as viable solution. Thirdly, comparing Subfigure 2 with Subfigures 3 to 6 for the same inquiry rounds and models reveals that the accuracy rates in Subfigure 2 are significantly lower than those in Subfigures 3 to 6. This is due to the weaker diagnostic capability of GPT-4omini, leading to lower ceiling for the final accuracy. Conversely, comparing Subfigure 6 with the others for the same rounds and inquiry models shows that Subfigure 6 surpasses the others in accuracy rates. This is attributed to the superior diagnostic ability of o1-preview, resulting in higher ceiling. Observations from Subfigures 1 and 3 to 6 indicate that diagnostic accuracy increases significantly with more inquiry rounds. Furthermore, regardless of the diagnostic model used, records based on Claude inquiries consistently perform poorly. Hence, we conclude that \"inquiry\" and \"diagnosis\" adhere to the Liebigs law: if the quality of the inquiry is insufficient, achieving good results is challenging even with strong diagnostic capabilities, and vice versa."
        },
        {
            "title": "4.1 Four Types of Inquiry\nExamples of the four types are provided in Figure 6. Based on the\nexamples in our inquiry records and the systematic descriptions\nin relevant medical materials [2, 5, 29, 33], a detailed discussion of\nthese types is presented as follows:",
            "content": "Conference17, July 2017, Washington, DC, USA Zhaocheng Liu, Quan Tu, Wen Ye, Yu Xiao, Zhishou Zhang, Hengfu Cui, Yalun Zhu, Qiang Ju, Shizheng Li, and Jian Xie Figure 7: The comparison focuses on the distribution of four inquiry types across GPT-4o, GPT-4o-mini, and Claude-3-5-sonnet as inquiry models, segmented by inquiry rounds. The x-axis represents the inquiry models, while the y-axis indicates the proportion of the four inquiry types. Past medical history (PMH): PMH encompasses the patients prior health status and previously diagnosed conditions, including infectious diseases, injuries, surgical procedures, immunization records, and allergy history, with particular emphasis on factors closely related to the current illness."
        },
        {
            "title": "4.2 Experimental Results\nWe employ GPT-4o to annotate the inquiry records into above four\ntypes, with the prompt used detailed in the Appendix B. Our ex-\nperimental results are shown in Figure 7. Segmented by inquiry\nrounds, the comparison focuses on the distribution of four inquiry\ntypes across GPT-4o, GPT-4o-mini, and Claude-3-5-sonnet as in-\nquiry models. The x-axis represents the inquiry models, while the\ny-axis indicates the proportion of the four inquiry types.",
            "content": "Firstly, as shown in Subfigure 1 of Figure 7, in the vast majority of cases, all inquiry models choose to ask about the chief complaint during the first round. This aligns with expectations, as the doctor models do not possess any information about the patient during the initial round and thus typically begin with question such as \"What symptoms have you been experiencing that brought you here today?\" However, there is small subset involving inquiries about accompanying symptoms, particularly with the use of GPT4o-mini and Claude. Such initial questions often include: \"Hello, could you tell me if youve had any discomfort in recent days, like fever, cough, or any other uneasy feelings?\" or \"Good morning, you seem bit pale; have you been experiencing any symptoms like dizziness, fatigue, or loss of appetite?\" Although whether these instances should be tagged as inquiries about accompanying symptoms remains debatable, the comparison shows that these inquiries indeed interfere with the collection of the patients chief complaint. This might be the main reason why GPT-4o consistently performs the best in the first round in subfigures 2 to 6 of Figure 5. Secondly, as shown in Subfigures 25 of Figure 7, Claude demonstrates significantly higher proportion of specification of known symptoms during multi-turn inquiry compared to other models. This leads to noticeable reduction in the proportions of other inquiry types. Considering that each type of inquiry is crucial for the diagnostic process, we hypothesize that this might indicate relative weakness in Claudes overall inquiry capability compared to other models. Correspondingly, in Subfigures 26 of Figure 5, Exploring the Inquiry-Diagnosis Relationship with Advanced Patient Simulators Conference17, July 2017, Washington, DC, USA the inquiry records generated by Claude are generally associated with the lowest final diagnostic accuracy. Furthermore, when comparing GPT-4o and GPT-4o-mini, the latter consistently exhibits higher proportion of gathering family or medical history across turns (except for the fourth turn). Based on Subfigure 6 of Figure 5 (where o1-preview is used as the diagnostic model), the contribution of family history to diagnostic accuracy becomes evident starting from the third turn. The focus on subfigure 6 is specifically motivated by the fact that o1-preview demonstrates the strongest diagnostic capability among all models, allowing us to minimize the confounding effects of different levels of diagnostic performance."
        },
        {
            "title": "Medicine",
            "content": "The benchmarks to evaluate LLMs in medicine can be categorized into static and dynamic types based on whether they provide simulated environment. Static benchmarks primarily assess medical knowledge and typically use multiple-choice format. The MedQA [12] dataset contains medical question-answer pairs derived from the US, Mainland China, and Taiwan Medical Licensing Exams. It features 4-5 multiple-choice questions with correct answers, supported by explanations or references. These questions, ranging from diagnosis to treatment selection, are often challenging even for medical students. The LLMs receive comprehensive context, including patient history, demographics, and symptoms, to generate responses. And similar multiple-choice formats are employed by PubMedQA [13], MedMCQA [23], MMLU clinical topics [9], and MultiMedQA [27]. Dynamic benchmarks assess the performance of doctor models through role-playing scenarios involving doctors and patients, utilizing LLMs. AMIE [34] diagnoses simulated patients through history-taking. AgentClinic [26] is an open-source multimodal benchmark designed to assess the capability of LLMs to function as agents in simulated clinical settings. Additionally, many other studies [14, 16, 17, 24, 30] provide simulated clinical environments to evaluate or enhance physician models. However, in these studies, patient simulations predominantly rely on prompt engineering, which does not accurately replicate real patient behavior. Unlike passive simulated responses, real patients often express anxiety about their conditions, ask questions to alleviate concerns, and may not always be cooperative. They might terminate the conversation or refuse to answer if doctors repeatedly question them. Furthermore, to the best of our knowledge, it is noteworthy that within existing studies providing dynamic simulation environments, in-depth exploration into the relationship between \"inquiry\" and \"diagnosis\" and its impact on the overall consultation outcome remains scarce."
        },
        {
            "title": "6 Conclusion\nIn this paper, we extract real dialogue strategy flows from authen-\ntic doctor-patient conversations, and after manual selection, these\nflows are combined with patient records to generate synthetic data.\nThese dialogue strategy flows, along with patient records, guide\nthe training of a patient simulator, resulting in a patient simulation\nwith significantly fewer hallucinations, and more closely resem-\nbling a real patient. Based on this simulator, we conduct extensive\nexperiments to explore the relationship between inquiries and di-\nagnoses in medical consultations, as well as their impact on the\nfinal diagnostic accuracy. Our experimental results demonstrate: 1)\nThere are significant differences in inquiry strategies among differ-\nent models; 2) Inquiries and diagnoses adhere to the Liebig‚Äôs law.\nFurthermore, we classify inquiries into four types based on data\ncases and relevant definitions in diagnostics: (1) chief complaint\ninquiry; (2) specification of known symptoms; (3) inquiry about\naccompanying symptoms; and (4) gathering family or medical his-\ntory. We label and analyze the distribution of inquiries generated by\ndifferent models according to these four types. By comparing distri-\nbution differences and diagnostic accuracy variations, we uncover\nthe specific differences in inquiry strategies among the models.\nOur results indicate that how to allocate limited inquiry oppor-\ntunities (commonly 3 to 5 rounds are acceptable to patients) is a\nresearch-worthy issue, which we intend to explore further in future\nwork.",
            "content": "References [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Herhert Adler. 1997. The history of the present illness as treatment: whos listening, and why does it matter? The Journal of the American Board of Family Practice 10, 1 (1997), 2835. [3] Ibrahim Al-Mahdi, Kathleen Gray, and Reeva Lederman. 2015. Online Medical Consultation: review of literature and practice. In Proceedings of the 8th Australasian workshop on health informatics and knowledge management, Vol. 164. Australian Computer Society Sydney, 97100. [4] Anthropic. 2024. Claude 3.5 Sonnet. https://www.anthropic.com/news/claude-35-sonnet. [5] Bickley. 2012. Bates Guide to Physical Examination and History Taking. Lippincott Williams & Wilkins. [6] Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, and Benyou Wang. 2024. HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs. arXiv preprint arXiv:2412.18925 (2024). [7] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). [8] Xianpei Han, Zhichun Wang, Jiangtao Zhang, Qinghua Wen, Wenqi Li, Buzhou Tang, Qi Wang, Zhifan Feng, Yang Zhang, Yajuan Lu, et al. 2020. Overview of the CCKS 2019 knowledge graph evaluation track: entity, relation, event and QA. arXiv preprint arXiv:2003.03875 (2020). Conference17, July 2017, Washington, DC, USA Zhaocheng Liu, Quan Tu, Wen Ye, Yu Xiao, Zhishou Zhang, Hengfu Cui, Yalun Zhu, Qiang Ju, Shizheng Li, and Jian Xie and better alignment to human preferences. arXiv preprint arXiv:2311.06025 (2023). [33] Armand Trousseau. 1873. Lectures on clinical medicine. Vol. 2. Lindsay & Blakiston. [34] Tao Tu, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Nenad Tomasev, et al. 2024. Towards conversational diagnostic ai. arXiv preprint arXiv:2401.05654 (2024). [35] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems 35 (2022), 2482424837. [36] Yunfei Xie, Juncheng Wu, Haoqin Tu, Siwei Yang, Bingchen Zhao, Yongshuo Zong, Qiao Jin, Cihang Xie, and Yuyin Zhou. 2024. Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor? arXiv preprint arXiv:2409.15277 (2024). [37] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2.5 Technical Report. arXiv preprint arXiv:2412.15115 (2024). [38] Guangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang, Sicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi Zeng, Xiangyu Dong, Ruoyu Zhang, et al. 2020. MedDialog: Large-scale medical dialogue datasets. In Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP). 92419250. [39] Kaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li, Ganqu Cui, Biqing Qi, Xuekai Zhu, et al. 2024. Ultramedical: Building specialized generalists in biomedicine. arXiv preprint arXiv:2406.03949 (2024). [9] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 (2020). [10] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [11] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088 (2024). [12] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences 11, 14 (2021), 6421. [13] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. Pubmedqa: dataset for biomedical research question answering. arXiv preprint arXiv:1909.06146 (2019). [14] Shreya Johri, Jaehwan Jeong, Benjamin Tran, Daniel Schlessinger, Shannon Wongvibulsin, Zhuo Ran Cai, Roxana Daneshjou, and Pranav Rajpurkar. 2023. Guidelines For Rigorous Evaluation of Clinical LLMs For Conversational Reasoning. medRxiv (2023), 202309. [15] Sabrina Kessler. 2023. Online Medical Consultation Services. The International Encyclopedia of Health Communication (2023), 14. [16] Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang, Weizhi Ma, and Yang Liu. 2024. Agent hospital: simulacrum of hospital with evolvable medical agents. arXiv preprint arXiv:2405.02957 (2024). [17] Shuyue Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan Ilgen, Emma Pierson, Pang Wei Koh, and Yulia Tsvetkov. 2024. MediQ: Question-Asking LLMs and Benchmark for Reliable Interactive Clinical Reasoning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. [18] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. DeepSeek-V3 Technical Report. arXiv preprint arXiv:2412.19437 (2024). [19] Jie Liu, Wenxuan Wang, Zizhan Ma, Guolin Huang, Yihang SU, Kao-Jung Chang, Wenting Chen, Haoliang Li, Linlin Shen, and Michael Lyu. 2024. Medchain: Bridging the Gap Between LLM Agents and Clinical Practice through Interactive Sequential Benchmarking. arXiv preprint arXiv:2412.01605 (2024). [20] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, et al. 2023. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. arXiv preprint arXiv:2311.16452 (2023). [21] OpenAI. 2024. GPT-4o system card. https://openai.com/index/gpt-4o-systemcard/. [22] OpenAI. 2024. Openai o1 system card. https://openai.com/index/openai-o1system-card/. [23] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022. Medmcqa: large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on health, inference, and learning. PMLR, 248 260. [24] Huachuan Qiu and Zhenzhong Lan. 2024. Interactive agents: Simulating counselor-client psychological counseling via role-playing llm-to-llm interactions. arXiv preprint arXiv:2408.15787 (2024). [25] Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, et al. 2024. Capabilities of gemini models in medicine. arXiv preprint arXiv:2404.18416 (2024). [26] Samuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, and Michael Moor. 2024. AgentClinic: multimodal agent benchmark to evaluate AI in simulated clinical environments. arXiv preprint arXiv:2405.07960 (2024). [27] Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2023. Large language models encode clinical knowledge. Nature 620, 7972 (2023), 172180. [28] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Mohamed Amin, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, et al. 2025. Toward expert-level medical question answering with large language models. Nature Medicine (2025), 18. [29] Mark Swartz. 2014. Textbook of physical diagnosis E-book: history and examination. Elsevier Health Sciences. [30] Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark Gerstein. 2023. Medagents: Large language models as collaborators for zero-shot medical reasoning. arXiv preprint arXiv:2311.10537 (2023). [31] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023). [32] Yuanhe Tian, Ruyi Gan, Yan Song, Jiaxing Zhang, and Yongdong Zhang. 2023. Chimed-gpt: chinese medical large language model with full training regime Exploring the Inquiry-Diagnosis Relationship with Advanced Patient Simulators Conference17, July 2017, Washington, DC, USA Patient Dialogue Strategy Labels Description [Greeting] [Describe Condition] [Detail Symptoms] [Ask Questions] [Confirm] [Express Concerns] [Seek Help] [Provide Information] [Discuss Treatment Options] [Disagree] [Explanation Request] [Seek Advice] [Complaint or Feedback] [Request Prescription] [Inquire about Treatment Options] [Share Feelings] [Request Recommendation] [Thanks] [Disagree] [Emotional Expression] [Express Concerns] [Ask about Side Effects] [Seek Understanding] The patient initiates the conversation by greeting the doctor. The patient describes their symptoms or medical history. The patient elaborates on specific physical discomforts or symptoms. The patient asks further questions regarding the doctors advice or the condition. The patient confirms or shows understanding of what the doctor has said. The patient expresses worries about the condition or the treatment outcome. The patient requests support or assistance from the doctor. The patient proactively offers relevant health information or past medical history. The patient discusses possible treatment options with the doctor. The patient expresses different opinion on the doctors advice or diagnosis. The patient asks the doctor to further explain the test results or treatment plan. The patient requests professional advice or suggestions from the doctor. The patient offers opinions or suggestions regarding the medical service or treatment process. The patient asks the doctor to prescribe medication. The patient asks about feasible treatment options and expected outcomes. The patient shares their feelings about the condition or treatment, such as pain, anxiety, etc. The patient asks the doctor to recommend other specialists or tests. The patient expresses gratitude for the doctors help or advice. The patient expresses different opinion on the doctors advice or treatment plan. The patient expresses emotional reactions to the condition, such as depression, anger, gratitude, etc. The patient expresses anxiety or concerns about their health condition or treatment plan. The patient inquires about possible side effects of the medication or treatment. The patient hopes the doctor will provide more explanation and understanding of their condition. [Ask about Follow-up Arrangements] The patient inquires about subsequent tests, follow-up visits, or treatment plans. [Stop] The patient ends the conversation. Inquiry Type Annotation Prompt The detailed prompt of inquiry type annotation is as shown in Figure 8. Figure 8: Inquiry Type Annotation Prompt"
        },
        {
            "title": "A Candidate Set of Dialogue Strategy Tags",
            "content": "Doctor Dialogue Strategy Labels [Greeting] [Explanation] [Answering] [Clarification] [Medical Advice] [Confirmation] [Concern] [Comfort] [Diagnosis] [Education] [Chief Complaint Inquiry] [Recommendation] [Inquiring about Symptoms] Description The doctor initiates the conversation by greeting the patient. The doctor explains the patients condition, treatment plan, or medication use. The doctor responds to the patients questions or concerns. The doctor or patient clarifies certain issues. The doctor offers health advice or lifestyle guidance. The doctor or patient confirms certain information or understanding. The doctor expresses concern and attention for the patient. The doctor shows care and comfort to the patient. The doctor identifies the patients condition based on symptoms and examination. The doctor identifies the illness or other problem of the patient. The doctor asks the patient to describe their primary health concern. The doctor gives health advice or suggests lifestyle changes. The doctor asks about the patients symptoms, medical history, and related information. [Inquiry about Accompanying Symptoms] The doctor asks about other symptoms alongside the main issue. [Gathering Family or Medical History] [Evaluation] [Arrangement] [Prescription] [Farewell] The doctor asks about the patients past medical history or family medical history. The doctor assesses the patients symptoms. The doctor arranges for follow-up tests or appointments. The doctor prescribes medication or treatment plans for the patient. The doctor concludes the conversation."
        }
    ],
    "affiliations": [
        "Baichuan Inc.",
        "Gaoling School of Artificial Intelligence, Renmin University of China"
    ]
}