{
    "paper_title": "Where do Large Vision-Language Models Look at when Answering Questions?",
    "authors": [
        "Xiaoying Xing",
        "Chia-Wen Kuo",
        "Li Fuxin",
        "Yulei Niu",
        "Fan Chen",
        "Ming Li",
        "Ying Wu",
        "Longyin Wen",
        "Sijie Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Vision-Language Models (LVLMs) have shown promising performance in vision-language understanding and reasoning tasks. However, their visual understanding behaviors remain underexplored. A fundamental question arises: to what extent do LVLMs rely on visual input, and which image regions contribute to their responses? It is non-trivial to interpret the free-form generation of LVLMs due to their complicated visual architecture (e.g., multiple encoders and multi-resolution) and variable-length outputs. In this paper, we extend existing heatmap visualization methods (e.g., iGOS++) to support LVLMs for open-ended visual question answering. We propose a method to select visually relevant tokens that reflect the relevance between generated answers and input image. Furthermore, we conduct a comprehensive analysis of state-of-the-art LVLMs on benchmarks designed to require visual information to answer. Our findings offer several insights into LVLM behavior, including the relationship between focus region and answer correctness, differences in visual attention across architectures, and the impact of LLM scale on visual understanding. The code and data are available at https://github.com/bytedance/LVLM_Interpretation."
        },
        {
            "title": "Start",
            "content": "Where do Large Vision-Language Models Look at when Answering Questions? Xiaoying Xing1,2,, Chia-Wen Kuo1, Li Fuxin3, Yulei Niu1, Fan Chen1, Ming Li1, Ying Wu2, Longyin Wen1, Sijie Zhu1,* 1Bytedance Intelligent Creation, 2Northwestern University 3Oregon State University 5 2 0 2 8 1 ] . [ 1 1 9 8 3 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Vision-Language Models (LVLMs) have shown promising performance in vision-language understanding and reasoning tasks. However, their visual understanding behaviors remain underexplored. fundamental question arises: to what extent do LVLMs rely on visual input, and which image regions contribute to their responses? It is non-trivial to interpret the free-form generation of LVLMs due to their complicated visual architecture (e.g. multiple encoders and multi-resolution) and variable-length outputs. In this paper, we extend existing heatmap visualization methods (e.g. iGOS++ [27]) to support LVLMs for open-ended visual question answering. We propose method to select visually relevant tokens that reflect the relevance between generated answers and input image. Furthermore, we conduct comprehensive analysis of stateof-the-art LVLMs on benchmarks designed to require visual information to answer. Our findings offer several insights into LVLM behavior, including the relationship between focus region and answer correctness, differences in visual attention across architectures, and the impact of LLM scale on visual understanding. The code and data will be released at https://github.com/bytedance/ LVLM_Interpretation. 1. Introduction The emerging Large Vision and Language Models (LVLM) [3, 15, 28, 42] have exhibited strong visual instruction following abilities and achieved remarkable performance on multimodal tasks, such as Visual Question Answering (VQA) [2]. Despite different design choices and implementation details, most LVLMs follow the representative visual instruction tuning [34] paradigm to align the visual features from pre-trained vison encoders [35, 38] to pre-trained LLM [45, 53]. This enables LVLMs to incorporate visual understanding while retaining the rich knowledge and reasoning abilities of LLMs. Corresponding author, sijiezhu@bytedance.com This work was done during the first authors internship at ByteDance Figure 1. Focus regions of LLaVA-1.5 when answering counting questions. The model may correctly focus on the relevant region and produce the correct answer (top left), or it may fail despite attending to the right region due to misinterpretation (top right). In some cases, incorrect focus leads to wrong answers (bottom right), while occasionally, the model answers correctly despite attending to irrelevant regions (bottom left), highlighting challenges in visual grounding and generalization. However, the underlying mechanisms behind the visual understanding capabilities of LVLMs remain unclear. Beyond evaluating model performance on various benchmarks, it is crucial to interpret where the LVLM focuses when generating responses, as it can provide insights into why an answer is correct or incorrect and facilitate targeted improvements for multimodal tasks. For instance, as shown in Figure 1, an LVLM may attend to the correct region but still misinterpret the content (e.g., the top-right example), fail to locate the relevant region entirely (e.g., bottom-right), or even produce correct answers based on irrelevant regions (e.g., bottom-left), which can lead to poor generalization. Before the era of LVLMs, popular way to interpret visual models is to derive saliency heatmap of the input image, representing the relevance of the image regions to the output [4, 10, 39]. Despite the rapid growth of LVLM research, little effort has been made to interpret LVLMs. Existing works [6, 19] often explain single-label outputs or 1 individual tokens within sentence. However, LVLMs generate open-ended responses consisting of multiple tokens with variable lengths, requiring holistic interpretation of the entire output rather than isolated components. Interpreting open-ended responses of LVLMs has several challenges, (1) Vision-Language Interaction: LVLMs involve intricate interactions between vision and language modalities, and often exhibit strong bias towards the language priors. Hence it is hard to determine the contribution of each modality to the response. (2) Autoregressive Generation: Unlike classification models, LVLMs autoregressively generate freeform text, making it difficult to interpret the model behavior considering the entire output. (3) Model Architecture Complexity: Current LVLMs often use multi-resolution or multi-encoder architectures, making it unreliable to align the features across layers to the corresponding spatial regions in the image. To address these challenges, we propose method that enables model interpretation for LVLMs and open-ended responses. It reveals the significant image regions that lead to the generated response, providing insights into the reasoning process. To the best of our knowledge, this is the first heatmap visualization method that applies to any LVLM structure and generate global interpretation for openended responses. Using this method, we conduct in-depth studies of state-of-the-art LVLMs on benchmarks designed to evaluate the visual understanding, and obtain several insights related to the model behaviors. (1) Effect of LLM Scale: Simply increasing the LLM size does not significantly alter the models visual attention behavior. (2) Impact of Vision Architecture: Different vision architectures could lead to distinct attention patterns. Multi-resolution models tend to focus on finer details, while multi-encoder models often attend to broader regions. (3) Performance vs. Visual Understanding: Performance and visual understanding behavior are not always aligned; current LVLMs may produce correct answers while attending to irrelevant regions, suggesting that accuracy alone is insufficient to comprehensively reflect the models capabilities. The main contributions of this work are as follows: Proposed to extract the visually relevant tokens from the open-ended responses, which are representative of the vision-related parts of the variable-length responses. Extended existing visual interpretation methods to interpret LVLMs for free-form text output with several technical improvements. Conducted in-depth studies of LVLM visual behavior, analyzing how different models attend to image regions when answering visual-related questions. 2. Related Work Visual Instruction Tuning. Benefiting from the advancement of LLM, current LVLMs often seek to equip LLMs with visual understanding capabilities, which is achieved by visual instruction tuning [34]. They utilize modality connector to align the visual embeddings into prompts that the language models can comprehend [29, 30]. Despite the great progress, existing LVLMs still suffer from hallucination [25, 32, 51, 54] due to insufficient visual understanding capabilities. Improving the visual capabilities of LVLMs is crucial to their further improvement and application, and several benchmarks [11, 43, 44] have been proposed to highlight the growing attention to this issue. Visual Attention Visualization. We mainly focus on popular family of interpretation methods, which aims to highlight the image regions most relevant to the models output. Existing heatmap visualization methods can be mainly categorized to gradient-based approaches [39, 40, 48, 50] and perturbation-based approaches [14, 18, 27, 37]. Gradient-based methods often backpropagate the output of the model and generate the saliency heatmap using the gradients or some variants [4, 9]. Perturbation-based methods derive the influence of input regions by introducing perturbation and analyzing their impact on model output. They often apply mask to the input image and formulate an optimization problem to identify the most influential regions. Another line of work aligns the model attention with human attention [13, 16, 41] for better grounding. However, deep networks often make decisions with mechanisms different from humans, hence human attention is not very relevant for analyzing models. Different from them, we analyze the intrinsic behavior of LVLMs. Traditional model interpretation methods are still limited to interpreting single output, and can not directly apply to free-form generative models. Besides, current LVLMs often involve complex multimodal structures with multi-resolution and multiple vision encoders, which makes the gradient-based and transformer interpretation methods [10, 21] hard to apply. LVLM Interpretation. Recently the interpretation of LVLMs has drawn increasing interest, as it provides valuable guidance for model development. pipeline [6] has been proposed to visualize the attention of LLaVA [34]. Besides, some works visualize the model attention across different layers and either propose to improve the efficiency of LVLMs by pruning the redundant image tokens [12, 52], or alleviate the hallucination problem through modifying the decoding process [23]. However, these methods mainly interpret the LVLMs from internal attention. The relevance of different regions on the input image to the output remains under-explored. recent work [19] combines an openworld localization model with the LVLM to generate object localization for the output tokens using the vision embedding, while still limited to object-centric interpretation. 3. Method In this section, we first formulate the task and introduce some background knowledge about optimizationbased heatmap visualization. Next, we propose visually relevant token selection strategy and generalize the visualization method to open-ended responses of LVLMs with several improvements. 3.1. Preliminaries Task Formulation. Given an input image and question about the image, the LVLM generates an open-ended answer in natural language. The answers are generated In autoregresautoregressively and may vary in length. sive text generation, words are tokenized and sequentially predicted conditioned on previous tokens. Suppose the answer consists of tokens, represented as sequence = [a1, a2 al]. At each step t, the model samples the next token at according to: at (ata1, a2 at1; I, Q) (1) To investigate where the model focuses while generating the answer, we aim to obtain heatmap that highlights the importance of each image pixel to the model output. In the next section we introduce the optimization-based heatmap visualization method iGOS++ [27]. Optimization-based Heatmap Visualization derive the heatmap by solving an optimization problem. Suppose the model predicts score for the output given I, Q, the optimization has two main objectives: deletion and insertion. Deletion progressively removes pixels from in the order of their heatmap values, aiming to minimize the models prediction score . Insertion starts with baseline image without visual information (e.g. fully blurred image) and gradually restores pixels according to their heatmap values, optimizing the heatmap to maximize . Hence the resulting heatmap highlights the most influential regions that contribute to the models final prediction. The insertion and deletion operation can be denoted as: Φ(I, I, ) = + (1 ) (2) where denotes the Hadamard product. Defining deletion and insertion masks Mx and My for each objective, the final heatmap is obtained as their combination: = Mx My. The whole objective function is as follows: min =(Mx,My) (Φ(I, I, Mx)) (Φ(I, I, 1 My)) +f (Φ(I, I, )) (Φ(I, I, 1 )) + g(M ) (3) where g(M ) = λ11 1 + λ2BT (M ) g(M ) is regularization term consisting of an L1 norm to promote sparsity and Bilateral Total Variation (BTV) norm [27] to enforce smoothness. The objective minimizes the deletion scores when applying Mx and , and maximizes the insertion scores with My and . However, existing heatmap visualization methods cannot directly apply to the open-ended responses of LVLMs, Figure 2. Top figure: answers generated by LLaVA-1.5 given the original image and fully blurred baseline image. Bottom figure: conditional probability of the original answer given input image and baseline image. Most tokens in the response are not very dependent on the visual information. since these models do not inherently produce prediction score. Furthermore, we find that this optimization process can be simplified by jointly optimizing single heatmap for both deletion and insertion objectives, and the stability of the non-convex optimization can also be improved. We will discuss how to generalize this approach to open-ended responses in Section 3.2 and our improvements to the optimization method in Section 3.3. 3.2. Visually Relevant Token Selection To extend heatmap visualization methods to free-form text outputs, representative prediction score is required for optimization. simple way to obtain single score is by averaging all token probabilities. However, autoregressive text generation produces responses of variable length, where token-image correlations may vary significantly. Specifically, token probabilities are strongly influenced by position within the sentence and the word. At the sentence level, certain words can be inferred based on syntax, making them less informative for assessing visual relevance. Within word, subword token probabilities also vary significantly. As illustrated in Figure 2, the conditional probabilities of certain tokens remain largely unchanged when the input is blurred, particularly for those that can be inferred from sentence structure and context (e.g., punctuation, painting, was, by) and the subsequent tokens within word (e.g. ardo, da, V, inci). In contrast, the first token in the sentence and some visually relevant tokens often exhibit notable probability drop, as the first token in the response often decide the subsequent sentence structure, and the visually relevant tokens highly depend on the specific visual input. To this end, we propose to extract the most visually relevant tokens and derive the prediction score from them to achieve representative interpretation of the whole output. We formulate the relevance of the output tokens to the input image as the log-likelihood ratio [5, 46] between the prediction with and without visual information. Due to the autoregressive nature of LVLMs, the probability of generating answer can be decomposed as the joint probability of its tokens. For simplicity, we omit I, and denote the conditional probability of the next token as (ata1 at1). The probability of the whole sentence is then: (aI, Q) = (a1)P (a2a1) (ala1 al1) (4) To measure the influence of visual information, we introduce baseline image that does not provide any visual information to answer the question. The probability of generating the original answer given is denoted as: (a I, Q) = (a1) (ala1 al1), where (ata1 at1) represents (ata1 at1; I, Q). This term can be efficiently computed in single forward pass by concatenating and as the textual prompt to the model. The log-likelihood ratio (LLR) quantifies the difference in answer confidence with and without visual information: LLR = log (aI, Q) log (a I, Q) (cid:89) (cid:89) (ata1 at1) log = log (ata1 at1) (5) (cid:88) = log (ata1 at1) log (ata1 at1) (cid:88) It can be observed that LLR of the whole output is the sum of LLRt of each token at. Therefore, to identify tokens most influenced by visual information, we apply threshold to filter those with the highest log-likelihood ratio. The set of crucial tokens is selected as: = {ak LLRk > α; = 1} (6) Finally, we define the prediction score as the cumulative log-likelihood of the crucial tokens. It ensures that only visually relevant parts of the response contribute to the interpretation, filtering out the influence of linguistic structures and leading to more faithful interpretation of the models reliance on visual information. (cid:88) = log (aka1 . . . ak1) (7) akK 3.3. Adaptation to LVLMs Non-convex Optimization. In practice, the optimization process is non-convex and difficult to converge. Besides, the reasoning process of LVLMs is complicated, with responses often related to multiple regions in the image, leading to scattered attention maps that are hard to optimize. To address this, we simplify the objective function by directly optimizing single mask for both the deletion and insertion objectives. To further improve stability, we incorporate 4 graduated non-convexity [22] (GNC) to reduce the risk of getting into local optima and the oscillations during optimization. Instead of directly solving highly non-convex problem, GNC begins with convex approximation to provide more stable starting point, and gradually introduces non-convexity into the optimization process. Specifically, we add an exponentially decayed L2 norm to the objective function, yielding the final formulation: (Φ(I, I, )) (Φ(I, I, 1 )) + g(M ) min g(M ) = λ11 1 + λ2eγt1 2 + λ3BT (M ) (8) and we set λ1 = 1, λ2 = 0.1, λ3 = 10 as the default value. Multi-encoder and Multi-resolution. Current LVLMs often leverage multiple vision encoders [43] or multiresolution [28, 31] input images to enhance visual understanding, which introduces challenges when applying the mask to the input image. For the multi-encoder architectures, the input image is processed by multiple vision encoders before integrating their features. To accommodate this, we apply single unified mask to the input image before passing it through all encoders, ensuring consistency across different feature extractors. For the multi-resolution methods, they often crop the input image into variable-sized patches according to the original resolution, where the operation is nondifferentiable and obstructs the optimization. To overcome this, we implement an equivalent differentiable cropping operation (replace pillow and numpy operations with tensor operations), ensuring that the mask undergoes the same transformation as the image patches. This allows us to apply the multi-resolution mask to the corresponding image patches while maintaining differentiability. 4. Experiment We leverage our improved heatmap visualization method to interpret the open-ended output of LVLMs. To gain insights into the visual behavior of state-of-the-art opensource LVLMs, we conduct quantitative and qualitative experiments, focusing on the following key research questions. Q1: Do LVLMs rely on the input image when answering visual questions? Q2: Where do different LVLMs attend when generating variable-length responses? Q3: What is the relationship between answer correctness and focus region? Q4: How do the vision encoder and LLM components influence visual behavior? 4.1. Models and Datasets We evaluate LVLMs that employ representative strategies to improve visual instruction following capabilities: LLaVA1.5 [33] leverages fully connected cross-modal adapter and incorporates academic-related data [20] to enhance visual instruction tuning. LLaVA-OneVision [28] employs multi-resolution input images, hence captures finer image"
        },
        {
            "title": "MMStar",
            "content": "CV-Bench"
        },
        {
            "title": "LVLM",
            "content": "LLaVA-1.5-7b LLaVA-OV-7b Cambrian-8b Del Method Grad-CAM 0.679 0.778 T-MM 0.401 IIA 0.366 Ours Grad-CAM 0.341 0.528 T-MM IIA 0.576 0.305 Ours Grad-CAM 0.416 0.375 Ours Ins Del 0.689 0.441 0.869 0.418 0.333 0.805 0.292 0.811 0.406 0.501 0.575 0.589 0.621 0.551 0.317 0.778 0.391 0.513 0.334 0.657 Ins Del 0.651 0.372 0.869 0.283 0.457 0.870 0.402 0.953 0.443 0.550 0.594 0.620 0.660 0.578 0.295 0.870 0.471 0.656 0.340 0. LLaVA-Bench Ins 0.379 0.378 0.824 0.864 0.549 0.655 0.612 0.803 0.582 0.786 Ins Del 0.685 0.587 0.808 0.461 0.371 0.884 0.358 0.965 0.334 0.562 0.589 0.630 0.541 0.553 0.301 0.924 0.452 0.599 0.372 0.849 Table 1. Quantitative comparison of interpretation methods in terms of Deletion score (lower is better), Insertion score (higher is better) on the filtered dataset using different LVLMs. The bold numbers denote the best results for each model and dataset. CV-Bench MMStar 7.5 / 10.4 LLaVA-1.5 8.9 / 18.5 LLaVA-OV 13.6 / 20.2 4.7 / 5.7 4.2 / 5.0 1.1 / 2.3 Cambrian MMVP 25.3 / 16.3 16.3 / 15.0 3.3 / 7.3 Table 2. Percentage (%) of samples that the answer probability decreases by less than 30% / 30%-70% without visual information. details. Cambrian [43] proposes Spatial Vision Aggregator to integrate visual features from multiple encoders. Given these architectural differences, we analyze how they affect the visual behaviors. We select recent datasets that target at evaluating the visual instruction following capabilities of LVLMs: MMStar [11] contains 1,500 human-reviewed vision-dependent questions that most LVLMs fail to answer correctly without visual input. CV-Bench [43] constructs visioncentric benchmark with 2,638 manually verified imagerelated questions. MMVP [44] selects subset of 300 questions with the images that CLIP [38] fails to distinguish. 4.2. Statistical Analysis To investigate Q1 (whether LVLMs rely on visual input), we conduct statistical analysis comparing the models responses with and without visual information. Using the visual relevance metric in Eq. 5, we compute the probability of an answer given the original image versus fully blurred image. Table 2 reports the percentage of samples where the answer probability decreases by less than 30% or between 30%-70% without visual information. The results indicate that most responses are affected by the image to varying degrees. Notably, LLaVA-1.5 exhibits lower reliance on visual input on MMStar and MMVP; with 25.3% MMVP samples showing probability drop of less than 30% when the image is blurred. The responses of Cambrian are more influenced by the visual contents; with answer probabilities decreasing by more than 70% for 90% of samples across datasets. Most compared models have lower density of small probability drops on MMStar, suggesting they rely more heavily on image when answering MMStar questions. 4.3. Comparison of Visualization Methods Our proposed token selection method can be applied to various interpretation methods and extend them to open-ended responses of LVLMs. However, some previous interpretation techniques are not well-suited for LVLMs. To demonstrate the advantage of our proposed method over other heatmap visualization methods, we compare the gradientbased method Grad-CAM [39], transformers interpretation methods T-MM [9] and IIA [4], and our improved optimization-based method based on IGOS++ [27]. Evaluation Metric. We follow the commonly used deletion and insertion [36] scores to assess the heatmaps. Deletion removes pixels from the original image in descending order of their heatmap values, and calculates the output scores given the intermediate image to derive deletion curve. The deletion score is the area under the curve (AUC). Similarly, the insertion score measures how quickly the output score increases when adding pixels to baseline image. Lower deletion score and higher insertion score indicate heatmap that better reflects areas the model attends to. For fair comparison across models with varying prediction score distributions, we normalize the scores according to those of the original image and baseline image, following [26]. Data Selection. For subsequent studies, we filter out the samples where models answers are largely independent of the image, as it is infeasible to study where the model attends if it does not need the image. We keep the samples where all models have clear probability differences with and without visual information, remaining 35% samples from MMVP, 47% of MMStar samples, and 34% of CVBench. The original datasets are multiple-choice questions, so we remove the choices and instructions to relax them into open-ended questions. Additionally, we evaluate on LLaVA-Bench [34] designed for open-ended VQA. Experiment Results. With the selected data, we generate heatmaps using different interpretation methods and compare their deletion and insertion scores in Table 1. As Question: What are the words in the image? LLaVA-1.5: The words in the image are Happy Easter. Input image Grad-CAM T-MM Question: Are the ears of the dog erect or drooping? LLaVA-OV: The ears of the dog are erect. IIA Ours Input image Grad-CAM T-MM IIA Ours Figure 3. Qualitative comparison of different explanation methods. The tokens in red denote the selected crucial tokens. Our method consistently generates meaningful heatmaps. Cambrian uses multiple vision encoders including nontransformer-based models, T-MM and IIA are not applicable. Our method consistently outperforms all baselines, with the lowest deletion scores and highest insertion scores across datasets and models. We further compare qualitative heatmap visualizations in Figure 3, showing that our method generates more meaningful heatmaps and adapts better to diverse model structures. These results ensure the reliability of our method for the subsequent analysis. 4.4. Ablation Study We conduct ablation studies on the key components of our method including the visual relevant token selection, baseline image choice and single-mask optimization. The results are presented in Table 3, where Proposed refers to our default configuration with the proposed token selection strategy, using blurred image as the baseline image, and optimizing single mask with GNC. Additional parameter studies are provided in Appendix 6. Baseline Image. The baseline image should contain minimal visual information [18], while maintaining distribution consistent with natural images to avoid introducing adversarial artifacts. We compare different choices of the baseline image in Table 3, including blurred image, all-zero input, and random noise. Results indicate that the blurred baseline image achieves the best balance, minimally disturbing the input image distribution. Visually Relevant Tokens. An important contribution of our method is handling long-sentence responses by identifying crucial tokens based on the log-likelihood ratio. To demonstrate its effectiveness, we compare the token selection strategy with (1) Computing the joint probability of the whole sentence. (2) Detecting keywords using off-the-shelf tagging method [7, 8]. As shown in Table 3, our proposed method achieves overall best results of deletion and insertion scores. In comparison, computing the joint probability"
        },
        {
            "title": "MMStar",
            "content": "CV-Bench Del 0.366 Ins Del 0.811 0.292 Ins Del 0.402 0.953 Comparison of output token selection strategy 0.409 0.402 0.403 0.367 0.286 0. 0.784 0.809 0.906 0.931 Ins 0.965 0.967 0."
        },
        {
            "title": "Proposed",
            "content": "Joint prob. Keywords"
        },
        {
            "title": "Blank\nNoise",
            "content": "0.407 0.381 0.644 0.696 0.317 0.295 0.805 0.838 0.403 0.392 0.837 0. Comparison with separate masks for insertion/deletion."
        },
        {
            "title": "Separate",
            "content": "0.373 0.819 0.279 0.965 0.383 0. Table 3. Ablation study of visually relevant token selection, the baseline image and simplifying separate masks to single mask. The bold numbers denote the best and second-best results among the compared variants. may introduce irrelevant tokens that do not reflect visual information. Moreover, the keywords detection method operates at the word level instead of the token level, which does not align well with the tokenized output of LLM. Single Mask Optimization. Next we evaluate the efficacy of replacing separate masks for deletion and insertion with single mask for optimization. As shown in Table 3, using separate masks provides slightly better scores. However, it significantly increases computation time, with an average runtime per sample of 19.1s, compared to only 7.4s for the single-mask approach. The visualized results show that optimizing single mask can also yield reasonable results. Therefore, our simplified optimization achieves better balance between performance and efficiency. 4.5. Focus region analysis Heatmap Visualization. We present qualitative results in Figure 4 to address Q2 about the focus region of different LVLMs when generating outputs. We highlight the visually relevant tokens in red. The results lead to several observations: (1) Cambrian reveals more compositional [26] image understanding, which means it tends to jointly consider the entire image and involve more comprehensive information in its responses (e.g. in example (a) Cambrian provides more information about the chickens color). In contrast, LLaVA-OV shows more disjunctive behaviors and focuses on specific regions, performing better on questions requiring detailed understanding (e.g. in example (c), only LLaVA-OV correctly determines whether the individuals are facing the front or back. This aligns well with their respective architectures: Cambrian aggregates multiple vision encoders to extract broader visual information, while LLaVA-OV adopts multi-resolution strategy to extract detailed features. (2) The models responses may involve contents not asked for in the questions but are related to some image regions. For instance, in Figure 4(b) LLaVA-OV describes the golden dome as it attends to that region, while Cambrian mentions the landmarks name, likely due to its (a) Question: What color is the chickens body? (b) Question: Can you see people in this image? Input image LLaVA-1.5 The chickens body is black and white. LLaVA-OV black. Input image Cambrian The chickens body is primarily black, with some iridescent blue and green feathers. (c) Question: Do the individuals in the picture face the front or the back? LLaVA-1.5 Yes, there is person visible in the image, standing on the steps in front of the building. LLaVA-OV Yes, there is person in the image. They appear to be walking up the steps towards the building with the golden dome. Cambrian Yes, there is person visible in the image, walking down the steps of the Al Aqsa Mosque. (d) Question: What color are the flowers in the background? Input image LLaVA-1.5 The individuals in the picture face the front, as they are walking together on the beach. LLaVA-OV The individuals in the picture are facing away from the camera, looking towards the horizon. Cambrian The individuals in the picture are facing the front, moving towards the horizon. Input image LLaVA-1.5 The flowers in the background are purple. LLaVA-OV yellow. Cambrian The flowers in the background are purple. Figure 4. Comparison of the generated response and focus region of different LVLMs. Tokens in red are the selected visual relevant tokens. Figure 5. Answer correctness and focus region plausibility across four quadrants. Each color stands for different question category, including spatial, attribute, counting, global context, and reasoning questions. global scene attention. (3) When LVLMs give incorrect answers, their focus regions often reveal the underlying cause. In example (d), LLaVA-OV locates the pistil instead of the petal hence answers yellow for the color of the flower. More visualization results are included in Appendix 9. Answer vs. Focus Region. To address Q3, we conduct subjective analysis on the relationship between answer correctness and focus region plausibility (i.e., whether the focus region aligns with human intuition). We randomly select 100 samples for and categorize them into spatial, attribute, counting, global context, and reasoning questions. The results are summarized in Figure 5, where we classify model behaviors into four quadrants and calculate the percentage of samples in each quadrant. The key observations include: (1) All models tend to provide correct answers with plausible focus regions when answering global context questions. Conversely, models often fail to answer reasoning and spatial questions even when attending to the right regions. (2) LLaVA-1.5 has lower chance to locate the most relevant regions. LLaVA-OV has better focus plausibility on most question types, though it does not necessarily lead to better accuracy. Cambrian performs well on counting questions, yet often attends to regions that do not align with human intuition. These findings suggest that focus region plausibility does not always correlate with answer correctness. In Appendix 7, we provide additional experiments to compare the model focus and human attention. Influence of Vision Architecture and LLM. Since the LVLMs consist of vision encoders and LLM, we investigate the impact of them on the focus region, respectively. To study the influence of LLM scale, we compare LLaVAOV 0.5b, 7b, 72b and Cambrian 3b, 8b, 13b. For models with the same LLM but different vision architectures, we further include Mini-Gemini [31] as they provide high-"
        },
        {
            "title": "LLM",
            "content": "Vision Encoder Del Ins LLaVA-OV-0.5b [28] LLaVA-OV-7b LLaVA-OV-72b Cambrian-3b [43] Cambrian-8b Cambrian-13b qwen2-0.5b [47] qwen2-7b qwen2-72b Phi-3-3.8B [1] Comparison of models with different LLMs SigLIP [49] SigLIP SigLIP Multi-encoder LLaMA3-8B [17] Multi-encoder Multi-encoder Vicuna1.5-13B 0.313 0.305 0.304 0.424 0.375 0."
        },
        {
            "title": "Comparison of models with different vision architectures",
            "content": "LLaVA-1.5-7b Mini-Gemini-7b [31] Mini-Gemini-7b-HD Cambrian-13b Mini-Gemini-13b Mini-Gemini-13b-HD Vicuna1.5-7B [53] Vicuna1.5-7B Vicuna1.5-7B Vicuna1.5-13B Vicuna1.5-13B Vicuna1.5-13B CLIP [38] CLIP-L ConvNext-L Multi-encoder CLIP-L ConvNext-L 0.366 0.474 0.478 0.415 0.473 0.471 0.792 0.778 0.754 0.664 0.657 0.692 0.811 0.669 0.661 0.692 0.674 0. Table 4. Comparison of models with different LLM scales and vision architectures. For Cambrian models, they use combination of 4 vision encoders: CLIP ViT-L [38], SigLIP ViT [49], OpenCLIP ConvNeXt-XXL [24] and DINOv2 ViT-L [35]. For Mini-Gemini models, HD denotes high resolution with an additional vision encoder of ConvNext-L. Question: Is the duck floating? Input image LLaVA-OV-0.5b No, the duck is not floating; it appears to be standing on rock. LLaVA-OV-7b no LLaVA-OV-72b No, the duck is not floating; it is standing on rock. Cambrian-3b No, the duck is standing on rock. Cambrian-8b No Question: Is the elderly person in the picture standing or sitting? Cambrian-13b No, the duck is not floating. It is standing on rock, which is evident from the view of its legs and feet. Input image LLaVA-1.5-7b The elderly person in the picture is sitting in wheelchar. Mini-Gemini-7b The elderly person in the picture is sitting in walker. Mini-Gemini-7b-HD The elderly person in the picture is sitting in wheelchair. Cambrian-13b The elderly person in the picture is sitting in wheelchair. Mini-Gemini-13b The elderly person in the picture is sitting in walker. Mini-Gemini-13b-HD The elderly person in the picture is sitting in wheelchair. Figure 6. Comparison of models with different LLM scales and vision architectures. The first row compares models with varying LLM sizes while keeping the vision architecture fixed, showing that increasing the LLM scale has minimal impact on visual behavior. The second row compares models with the same LLM but different vision encoders, indicating vision architectures may affect the focus region. resolution models (denoted as HD) with an additional vision encoder. The quantitative results are shown in Table 4 and we show qualitative results in Figure 6. It can be observed that merely increasing the LLM scale does not essentially change the focus region, despite the differences in response phrasing. In contrast, given the same LLM, varying the vision architecture more significantly affects the focus regions. To verify these observations, we conduct statistical significance test. It shows significant impact of vision architecture (p=0.0008) but no statistical significance for LLM scale (p=0.121), which aligns with our observations. We show more visualization results in Appendix 8. 5. Conclusion In this paper we propose method to generalize existing visual interpretation methods to support the autoregressive, open-ended responses of LVLMs. We introduce visually relevant token selection strategy that detects the crucial tokens in variable-length outputs and associates them with specific image regions. With the interpretation method, 8 we conduct comprehensive analysis of state-of-the-art open-source LVLMs with diverse model structures on visual instruction following benchmarks that require visual results provide several information. The experiment insights into model behaviors, including the relevance relationship between of the responses to visual answer correctness and focus region, influence of vision architectures and LLM scales. Despite some limitations discussed in Appendix 10, these findings emphasize the need for evaluation beyond standard accuracy metrics, offering insights into potential improvements of LVLMs. input,"
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 8 [2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425 2433, 2015. 1 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1 [4] Oren Barkan, Yuval Asher, Amit Eshel, Noam Koenigstein, et al. Visual explanations via iterated integrated attributions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20732084, 2023. 1, 2, 5 [5] Lionel Barnett and Terry Bossomaier. Transfer entropy as log-likelihood ratio. Physical review letters, 109(13): 138105, 2012. 4 [6] Gabriela Ben Melech Stan, Estelle Aflalo, Raanan Yehezkel Rohekar, Anahita Bhiwandiwalla, Shao-Yen Tseng, Matthew Lyle Olson, Yaniv Gurwicz, Chenfei Wu, Nan Duan, and Vasudev Lal. Lvlm-intrepret: an interpretability In Proceedings of tool for large vision-language models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81828187, 2024. 1, 2 [7] Ricardo Campos, Vıtor Mangaravite, Arian Pasquali, Alıpio Mario Jorge, Celia Nunes, and Adam Jatowt. text feature based automatic keyword extraction method for single documents. In European conference on information retrieval, pages 684691. Springer, 2018. 6 [8] Ricardo Campos, Vıtor Mangaravite, Arian Pasquali, Alıpio Jorge, Celia Nunes, and Adam Jatowt. Yake! keyword extraction from single documents using multiple local features. Information Sciences, 509:257289, 2020. [9] Hila Chefer, Shir Gur, and Lior Wolf. Generic attentionmodel explainability for interpreting bi-modal and encoderdecoder transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 397406, 2021. 2, 5 [10] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 782791, 2021. 1, 2 [11] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. 2, 5 [12] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, pages 1935. Springer, 2025. 2 [13] Xianyu Chen, Ming Jiang, and Qi Zhao. Predicting huIn Proceedman scanpaths in visual question answering. ings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2 [14] Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. Advances in neural information processing systems, 30, 2017. [15] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Instructblip: Towards generalFung, and Steven Hoi. purpose vision-language models with instruction tuning, 2023. 1 [16] Abhishek Das, Harsh Agrawal, C. Lawrence Zitnick, Devi Parikh, and Dhruv Batra. Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions? In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016. 2, 1 [17] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 8 [18] Ruth Fong and Andrea Vedaldi. Interpretable explanaIn Protions of black boxes by meaningful perturbation. ceedings of the IEEE international conference on computer vision, pages 34293437, 2017. 2, 6 [19] Loris Giulivi and Giacomo Boracchi. Explaining multimodal large language models by analyzing their vision perception. arXiv preprint arXiv:2405.14612, 2024. 1, 2 [20] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answerIn Proceedings of the IEEE conference on computer ing. vision and pattern recognition, pages 69046913, 2017. [21] Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1296312971, 2021. 2 [22] Elad Hazan, Kfir Yehuda Levy, and Shai Shalev-Shwartz. On graduated optimization for stochastic non-convex probIn International conference on machine learning, lems. pages 18331841. PMLR, 2016. 4 [23] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and 9 Nenghai Yu. Opera: Alleviating hallucination in multimodal large language models via over-trust penalty and In Proceedings of the IEEE/CVF retrospection-allocation. Conference on Computer Vision and Pattern Recognition, pages 1341813427, 2024. 2 [24] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, 2021. If you use this software, please cite it as below. [25] Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang, Fei Huang, and Shikun Zhang. Hallucination augmented contrastive learning for multimodal large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2703627046, 2024. 2 [26] Mingqi Jiang, Saeed Khorram, and Li Fuxin. Comparing the decision-making mechanisms by transformers and cnns via explanation methods. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95469555, 2024. 5, 6 [27] Saeed Khorram, Tyler Lawson, and Li Fuxin. igos++ integrated gradient optimized saliency by bilateral perturbations. In Proceedings of the Conference on Health, Inference, and Learning, pages 174182, 2021. 1, 2, 3, 5 [28] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 4, 8 [29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. [30] Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Hanwang Zhang, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, Siliang Tang, and Yueting Zhuang. Empowering vision-language models to follow interleaved vision-language instructions. arXiv preprint arXiv:2308.04152, 2023. 2 [31] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. 4, 7, 8 [32] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large In The multi-modal models via robust instruction tuning. Twelfth International Conference on Learning Representations, 2023. 2 [33] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 4 [34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 1, 2, 5 [35] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. 1, 8 [36] Petsiuk. Rise: Randomized input sampling for explanation of black-box models. arXiv preprint arXiv:1806.07421, 2018. [37] Zhongang Qi, Saeed Khorram, and Fuxin Li. Visualizing In deep networks by optimizing with integrated gradients. CVPR workshops, pages 14, 2019. 2 [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1, 5, 8 [39] Ramprasaath Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618626, 2017. 1, 2, 5 [40] Karen Simonyan. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013. 2 [41] Ekta Sood, Fabian Kogel, Philipp Muller, Dominike Thomas, Mihai Bˆace, and Andreas Bulling. Multimodal integration of human-like attention in visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26482658, 2023. 2 [42] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [43] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 2, 4, 5, 8 [44] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. 2, 5 [45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1 [46] Barnet Woolf. The log likelihood ratio test (the g-test). Annals of human genetics, 21(4):397409, 1957. 4 10 [47] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [48] MD Zeiler. Visualizing and understanding convolutional netIn European conference on computer vision/arXiv, works. 2014. 2 [49] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. 8 [50] Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. Top-down neural attention by excitation backprop. International Journal of Computer Vision, 126(10):10841102, 2018. 2 [51] Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah Smith. How language model hallucinations can snowball. arXiv preprint arXiv:2305.13534, 2023. 2 [52] Xiaofeng Zhang, Chen Shen, Xiaosong Yuan, Shaotian Yan, Liang Xie, Wenxiao Wang, Chaochen Gu, Hao Tang, and Jieping Ye. From redundancy to relevance: Enhancing explainability in multimodal large language models. arXiv preprint arXiv:2406.06579, 2024. 2 [53] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. 1, [54] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. arXiv preprint arXiv:2310.00754, 2023. 2 11 Where do Large Vision-Language Models Look at when Answering Questions?"
        },
        {
            "title": "Supplementary Material",
            "content": "For complementary we conduct additional ablation studies on the parameter settings. We also compare the visualized model focus region with human attention. Besides, more qualitative results of the responses and the corresponding focus regions are presented to compare the visual behaviors of different model structures. Finally, we include short discussion about the limitations of this work. 6. Additional Ablation Studies For our proposed heatmap visualization method, there is an important technical improvement to achieve better optimization, which introduces graduated non-convexity by adding an exponentially decayed L2 norm. In this section, we conduct parameter studies to evaluate the impact of the weight of the L2 norm. We use LLaVA-v1.5-7b for the following experiments. We first vary the scale of the L2 norm λ2 among [0.0, 0.1, 1.0, 10.0] and compare the deletion and insertion scores on different datasets, as shown in Table 5. It demonstrates that introducing proper scale of L2 norm can benefit the optimization and obtain better results in terms of the deletion and insertion. However, if the scale of the regularization is too large, it may disturb the objective function and degrade the performance. We select λ2 = 0.1 and α = 0.2 for the exponential decay rate."
        },
        {
            "title": "MMStar",
            "content": "Del 0.381 0.366 0.514 0.513 Ins 0.794 0.811 0.705 0.735 Del 0.295 0.292 0.420 0.412 Ins 0.953 0.953 0.846 0.858 CV-Bench Ins 0.965 0.965 0.885 0.894 Del 0.398 0.402 0.524 0. λ2 0.0 0.1 1.0 10.0 Table 5. Parameter study of the scale of the L2 norm in graduated non-convexity. The bolded numbers denote the best results among the compared parameter settings. In the main experiments we select λ2 = 0.1. 7. Comparison with Human Attention In this paper we mainly investigate the intrinsic behavior of the LVLMs in terms of the focus region when generating the open-ended responses. The model attention is an objective fact which does not need to be aligned with human attention/behavior. However, we conduct experiments to compare the model behavior with human attention to gain deeper insights. We evaluate the LVLMs on VQAHAT dataset [16], which consists of human visual attention maps over the images in the VQA dataset [2]. In Table 6 IOU Rank correlation LLaVA-1.5 0.010 LLaVA-OV 0.012 -0.2010.003 -0.1950. Table 6. Comparison of model focus region with human attention on VQA dataset, evaluated by the IOU and rank correlation. we show the soft IOU and rank correlation between the focus region of LLaVA-1.5/LLaVA-OV and human attention labels on VQA-HAT dataset. IOU measures the overlap between the model attention maps and human attention maps, defined as the ratio of their intersection to their union and the range is between 0 and 1. Rank correlation is statistical measure that quantifies the similarity between the rankings of two variables, commonly used to assess monotonic relationships. The results indicate that the focus regions of LVLMs can significantly differ from human attention, with small IOU and negative rank correlation values. 8. Influence of the Vision Architecture and"
        },
        {
            "title": "LLM scale",
            "content": "In the main experiments, we studied the influence of the vision architecture and LLM scale on the visual behavior (i.e., focus region when generating the responses). Here we present more qualitative results to compare the LVLMs with different vision architectures and LLM scales, respectively. In Figure 7, we compare the responses and corresponding focus region of LLaVA-OV 0.5b, 7b, 72b. In Figure 8, we compare the responses and corresponding focus region of Cambrian 3b, 8b, 13b. In Figure 9, we compare LLaVA-1.57b, Mini-Gemini-7b, and Mini-Gemini-7b-HD, as well as Cambrian-13b with Mini-Gemini-13b, and Mini-Gemini13b-HD. Within each group of models, they use the same LLM (i.e., Vicuna1.5-7b and Vicuna1.5-13b, respectively). It can be observed that the responses of the models with different LLM scales may have different expressions, but the corresponding focus regions often have similar structures. It indicates that the focus region is not significantly affected by the scale of the LLM. Instead, the LLM scale primarily influences the linguistic expression of the responses. In contrast, with the same LLM, vision architecture may have more significant influence on the focus region related to the model outputs. 9. Additional Qualitative Results In Figure 10, 11 and 12 we show more qualitative results, comparing the responses and focus regions of LLaVA-1.5, 1 LLaVA-OV and Cambrian. We mainly categorize the questions into spatial, attribute, counting, global and reasoning questions according to their knowledge types. In most cases, the responses of the models well align with the focus regions, especially for some specially contents involved in the responses. For example, in the first example of Figure 11, the responses of LLaVA-OV and Cambrian mention additional detail of the accordion, which is also highlighted in their heatmaps. Regarding counting questions, for all compared models it is difficult to precisely locate the target objects, hence may lead to wrong answers, as shown in the third row of Figure 10. For spatial and reasoning questions, we observe that in some cases even though the models focus on the most relevant regions, they still may fail to give the correct answer, since these questions may have higher requirements of spatial reasoning and scientific knowledge. 10. Limitations In this paper we propose visually relevant token selection method and extend existing interpretation methods to support open-ended responses of LVLMs with several technical improvements. It illustrates the model generation by deriving heatmap of the focus region on the image, and can be applied to various model structures with multi-encoder and multi-resolution. In this section, we discuss about the potential limitations of our proposed method. Since the objective function aims to minimize the deletion score (i.e., replacing pixels in the original image with those in the baseline image) and maximize the insertion score (i.e., replacing pixels in the baseline image with those in the original image), the optimization may not be effective when the output scores given the original image and baseline image do not have significant difference. This is also natural that we can not get the focus region if the responses of the model are not highly related to the input image. In such cases, other interpretation methods can be used as complementary. 2 Question: From which angle is this image taken? Question: What does the center button say? Input image LLaVA-OV-0.5b The image is taken from side angle, showing the snakes profile. LLaVA-OV-7b The image is taken from side angle, slightly above the snake. The snakes body is visible in profile, and its head is turned towards the camera, giving clear view of its eye and the pattern on its skin. LLaVA-OV-72b The image is taken from side angle, slightly above the level of the snakes body. This perspective allows for clear view of the snakes profile and the texture of its scales. Input image LLaVA-OV-0.5b The center button says OK SELECT. LLaVA-OV-7b The center button on the remote control in the image is labeled OK SELECT. LLaVA-OV-72b The center button on the remote control is labeled OK and also has the word SELECT beneath it, indicating that it is used to confirm selections or navigate through menus on device such as television or cable box. Question: Is the statement correct: There is no letter on this image. Question: Are there any clouds? Input image LLaVA-OV-0.5b No, the letter is present on this image. LLaVA-OV-72b No, the statement is incorrect. The letter is visible on the keyboard in the image. LLaVA-OV-7b The statement is incorrect. There is letter on the keyboard in the image, specifically in the second row from the top, towards the left side of the image. Input image LLaVA-OV-0.5b Yes, there are some scattered clouds in the sky. LLaVA-OV-7b yes LLaVA-OV-72b Yes, there are few scattered clouds in the sky. Figure 7. Comparison of the responses and focus region of LLaVA-OV with different LLM scales. The tokens in red denote the selected crucial tokens. The responses of the models with different scales often have different expressions, but the corresponding focus regions are often similar. Question: Is the sharks belly visible in this image? Question: Can you see letter in the image? Input image Cambrian-3b No, the sharks belly is not visible in this image. The angle and the position of the shark, combined with the lighting and the depth of the water, obscure the view of its underside. Cambrian-8b Yes, the sharks belly is visible in this image. It appears to be lighter shade of blue compared to the rest of its body, which is typical for many shark species as their underside is often less dark than their dorsal side. Cambrian-13b Yes, the sharks belly is visible in this image. It appears to be lighter shade of blue compared to the rest of its body, which is typical for many shark species as it helps them blend in with the oceans surface when viewed from below. Input image Cambrian-3b Yes, the letter is clearly visible on the keyboard, located in the second row from the top and the third column from the left. Cambrian-8b Yes, the letter is clearly visible in the image, located on the keyboard. Cambrian-13b Yes, the letter is clearly visible on the keyboard. It is on the top row, second from the left. The key is white with black lettering, which is standard color scheme for keys on many keyboards. The key is in focus and there are no obstructions. Question: How many spots are on the animal in the image? Question: How many cup(s) of drink are there in the image? Input image Cambrian-3b There are three spots on the animal in the image. Cambrian-8b There are three spots on the animal in the image. Cambrian-13b There are three spots on the animal in the image. Input image Cambrian-3b There are two cup(s) of drink in the image. Cambrian-8b 2 Cambrian-13b 2 Figure 8. Comparison of the responses and focus region of Cambrian with different LLM scales. The tokens in red denote the selected crucial tokens. Cambrian often tend to attend to the whole image for more comprehensive visual understanding. 4 Question: How many spots are on the animal in the image? Input image LLaVA-1.5-7b The animal in the image has many spots on its wings. Mini-Gemini-7b The image shows butterfly with many spots on its wings. Cambrian-13b There are three spots on the animal in the image. Mini-Gemini-13b The animal in the image, which is butterfly, has three spots on its wings. Mini-Gemini-13b-HD The animal in the image, butterfly, has two spots on its wings. Mini-Gemini-7b-HD The image shows butterfly with many spots on its wings. However, cannot provide an exact number of spots without more specific information about the butterfly species. Question: Where is the yellow animals head lying in this image? Input image LLaVA-1.5-7b The yellow animals head is lying on the floor in this image. Mini-Gemini-7b The yellow animals head is lying on the floor in this image. Mini-Gemini-7b-HD The yellow animals head is lying on the floor in the image. Cambrian-13b The yellow animals head is lying on the floor, resting comfortably on the patterned rug. Mini-Gemini-13b The yellow animals head is lying on the floor, next to the black dog. Mini-Gemini-13b-HD The yellow animals head is lying on the floor, next to black dog. Figure 9. Comparison of the focus region of LVLMs with different vision architectures and LLM scales. The tokens in red denote the selected crucial tokens. HD denotes the high-resolution vision encoder. 5 Question: Is the flowers center protruding or indented? Question: Is the person weighing themselves barefoot? Input image LLaVA-1.5 The flowers center is protruding, as it has yellow center and white petal. LLaVA-OV The flowers center is protruding, as indicated by the text The flowers center is protruding. Cambrian The flowers center is not protruding but rather appears to be indented. This is indicated by the way the petals are arranged around the center, with the edges of the petals slightly overlapping the center, suggesting that the center is recessed into the petals. Input image LLaVA-1.5 Yes, the person is weighing themselves barefoot on the scale. LLaVA-OV no. Cambrian Based on the image, the person is not weighing themselves barefoot. They are wearing white sandals with straps that are fastened around the ankles. Question: Is there pomegranate in the picture? Question: Is the sky in the background dark blue or light blue? Input image LLaVA-1.5 Yes, there is pomegranate in the picture, along with other fruits and vegetables. LLaVA-OV Yes, there is pomegranate in the picture. Its located on the right side of the image, near the top. Cambrian Yes, there is pomegranate in the picture. It is located on the right side of the image, next to the glass of orange juice. The pomegranate is cut open, revealing its juicy red seeds, which are common ingredient in fruit juices and smoothies. Input image LLaVA-1.5 The sky in the background is light blue. LLaVA-OV The sky in the background is light blue color. Cambrian The sky in the background is light blue, providing serene backdrop to the mountainous landscape. Question: How many balls are on the table? Question: How many computer monitors are in this picture?"
        },
        {
            "title": "Input image",
            "content": "LLaVA-1.5 There are three balls on the table. LLaVA-OV There are four balls on the table."
        },
        {
            "title": "Input image",
            "content": "LLaVA-1.5 There are three computer monitors in this picture. LLaVA-OV 3 Cambrian There are six balls on the table, each with distinct color and number, arranged in triangle formation at the start of game of snooker. Cambrian There are three computer monitors visible in the picture. The first is smaller monitor to the left, the second is larger central monitor, and the third is laptop screen on the right. Figure 10. Qualitative results of the responses of different models and the corresponding focus regions on the image, including attribute (the first and second row) and counting questions (the third row). Question: Who is wearing black pants in the image? Question: Which country is highlighted? Input image LLaVA-1.5 man is wearing black pants in the image. LLaVA-OV The man playing the accordion Cambrian The individual wearing black pants in the image is the person playing the accordion. Input image LLaVA-1.5 The country highlighted in the image is Australia. LLaVA-OV The highlighted country is Solomon Islands. Cambrian The country highlighted in the image is Nauru. It is small island nation located in the South Pacific Ocean, known for its unique political structure and its role in international diplomacy. Question: What is at the top of the food web? Question: Which label marks the egg phase? Input image LLaVA-OV Organism LLaVA-1.5 At the top of the food web, there is large bird, which is likely heron or similar species. Cambrian In the depicted food web, the top predator is the bird, which is labeled as Organism D. This bird is shown as the final consumer in this particular ecosystem, preying on Organism C, the fish. Input image LLaVA-1.5 The label that marks the egg phase is labeled C. LLaVA-OV The egg phase is marked by label B. Cambrian In the diagram, the label corresponds to the egg phase of the grasshoppers life cycle. This is the stage where the grasshopper is in its embryonic form, protected within the egg, before it hatches into nymph. Question: What is the position of the woman in relation to the man in the image? Question: Which object is closer to the bowl (highlighted by red box), the microwave (highlighted by blue box) or the bin (highlighted by green box)?"
        },
        {
            "title": "Input image",
            "content": "LLaVA-1.5 The woman is standing next to the man in the image. LLaVA-OV The woman is standing to the left of the man in the image. Cambrian The woman is standing next to the man in the image. Input image LLaVA-1.5 In the image, the bin (highlighted by green box) is closer to the bowl (highlighted by red box) than the microwave (highlighted by blue box). The bin is positioned next to the bowl, while the microwave is located further away in the kitchen. LLaVA-OV The bin (highlighted by green box) is closer to the bowl (highlighted by red box). Cambrian The microwave, highlighted by the blue box, is closer to the bowl than the bin. The bowl is positioned on the countertop, and the microwave is directly adjacent to it, while the bin is further away, placed on the floor. Figure 11. Qualitative results of the responses of different models and the corresponding focus regions on the image, including global (the first row), reasoning (the second row) and spatial (the third row) questions. 7 Question: Does the person in the picture have something in their hand? Question: What are all the letter in the image? Input image LLaVA-1.5 Yes, the person in the picture is holding water bottle in their hand. LLaVA-OV Yes, the person in the picture is holding water bottle. Cambrian Yes, the person in the picture is holding blue water bottle in their right hand. Question: Is there any accessory on the wrists? Input image LLaVA-1.5 The letters in the image are Sugar. LLaVA-OV The letters in the image are UGA. Cambrian The image contains the letters SUGAR spelled in stylized manner using what appears to be powdered sugar. Question: How many trucks are in the image? Input image LLaVA-1.5 Yes, there is watch on the wrists of the person using the computer. LLaVA-OV Yes, there is black wristband on the left wrist. Cambrian Yes, the person is wearing black wristwatch on their left wrist. Input image LLaVA-1.5 There are two trucks in the image. LLaVA-OV One Cambrian There is one truck in the image. Figure 12. Qualitative results of the responses of different models and the corresponding focus regions on the image."
        }
    ],
    "affiliations": [
        "Bytedance Intelligent Creation",
        "Northwestern University",
        "Oregon State University"
    ]
}