{
    "paper_title": "The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in Text-to-Image Models",
    "authors": [
        "Alfio Ferrara",
        "Sergio Picascia",
        "Elisabetta Rocchetti"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-image diffusion models have demonstrated remarkable capabilities in generating artistic content by learning from billions of images, including popular artworks. However, the fundamental question of how these models internally represent concepts, such as content and style in paintings, remains unexplored. Traditional computer vision assumes content and style are orthogonal, but diffusion models receive no explicit guidance about this distinction during training. In this work, we investigate how transformer-based text-to-image diffusion models encode content and style concepts when generating artworks. We leverage cross-attention heatmaps to attribute pixels in generated images to specific prompt tokens, enabling us to isolate image regions influenced by content-describing versus style-describing tokens. Our findings reveal that diffusion models demonstrate varying degrees of content-style separation depending on the specific artistic prompt and style requested. In many cases, content tokens primarily influence object-related regions while style tokens affect background and texture areas, suggesting an emergent understanding of the content-style distinction. These insights contribute to our understanding of how large-scale generative models internally represent complex artistic concepts without explicit supervision. We share the code and dataset, together with an exploratory tool for visualizing attention maps at https://github.com/umilISLab/artistic-prompt-interpretation."
        },
        {
            "title": "Start",
            "content": "2025 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING, AUG. 31 SEP. 3, 2025, ISTANBUL, TURKEY THE COW OF REMBRANDT ANALYZING ARTISTIC PROMPT INTERPRETATION IN TEXT-TO-IMAGE MODELS Alfio Ferrara, Sergio Picascia, Elisabetta Rocchetti Department of Computer Science, Università degli Studi di Milano, Via Celoria, 18, 20133 Milan, Italy 5 2 0 2 J 1 3 ] . [ 1 3 1 3 3 2 . 7 0 5 2 : r ABSTRACT Text-to-image diffusion models have demonstrated remarkable capabilities in generating artistic content by learning from billions of images, including popular artworks. However, the fundamental question of how these models internally represent concepts, such as content and style in paintings, remains unexplored. Traditional computer vision assumes content and style are orthogonal, but diffusion models receive no explicit guidance about this distinction during training. In this work, we investigate how transformer-based text-to-image diffusion models encode content and style concepts when generating artworks. We leverage cross-attention heatmaps to attribute pixels in generated images to specific prompt tokens, enabling us to isolate image regions influenced by content-describing versus style-describing tokens. Our findings reveal that diffusion models demonstrate varying degrees of content-style separation depending on the specific artistic prompt and style requested. In many cases, content tokens primarily influence object-related regions while style tokens affect background and texture areas, suggesting an emergent understanding of the content-style distinction. These insights contribute to our understanding of how large-scale generative models internally represent complex artistic concepts without explicit supervision. We share the code and dataset, together with an exploratory tool for visualizing attention maps at https://github.com/umilISLab/ artistic-prompt-interpretation. Index Terms text-to-image generation, diffusion models, cross-attention analysis, content-style disentanglement 1. INTRODUCTION Nowadays, we employ models that take textual prompt as input and generate an image as output, an image which, in most cases, closely reflects the description provided in the prompt. These text-to-image (txt2img) models are trained on billions of images sourced from the internet, including artworks from open-access, labeled online repositories such as WikiArt [1]. Through this training, the models implicitly acquire knowledge of art, artists, and artistic movements [2]. This learned knowledge is often leveraged when the model is 979-8-3503-2411-2/25/$31.00 2025 IEEE Fig. 1: (Top left) image generated by txt2img model using the prompt painting of giraffe in the Analytical Cubism style. with the corresponding heatmaps for (top center) content and (top right) style components. (Bottom) image and heatmaps from cow with Rembrandt style. instructed, for instance, to generate painting of giraffe in the Analytical Cubism style. This leads to intriguing questions: How does the txt2img model interpret stylistic instructions? And how does it represent the interplay between the content to depict and the style to apply? Historically, the field of Computer Vision (CV) has treated the notions of content and style as orthogonal, assuming that content is independent of style, and vice versa [3]. However, such distinction is not explicitly provided to diffusion models during training. Moreover, disentangling content from style goes beyond simply associating content with what is depicted and style with how it is depicted. This motivates deeper investigation into how these models internally conceptualize and separate these elements. In this work, we focus on analyzing the behavior of transformer-based txt2img diffusion models when generating artworks, with particular emphasis on how they encode the concepts of style and content. To achieve this, we leverage the models cross-attention heatmaps, which allow us to attribute each pixel in the generated image to specific tokens in the input prompt. This allows to isolate image regions primarily influenced by tokens describing content, and those If these two regions influenced by tokens describing style. overlap significantly, it may suggest that the model interprets content and style as similar or entangled; if they are distinct, it may indicate that the model conceptually separates the two. Figure 1 shows the cross-attention heatmaps corresponding to the words giraffe and Analytical Cubism, which define the content and style in the prompt painting of giraffe in the Analytical Cubism style. In this example, the two heatmaps highlight largely complementary regions: the content component giraffe activates areas corresponding to the animal itself, while the style component Analytical Cubism influences the rest of the image. This suggests separation of influence between content and style components during image generation. However, this distinction is not evident in the bottom example of Figure 1, which reveals an interesting behavior: the model appears to apply Rembrandts style by dressing the cow, since part of the activated region associated with the style component includes the cows clothing. The remainder of this paper is organized as follows: Section 2 reviews foundational concepts and related work in computer vision and contentstyle disentanglement. Section 3 outlines our methodology for investigating the models understanding of content and style. Section 4 describes our experimental setup and configurations. Section 5 presents our results and offers interpretations of how the model distinguishes (or conflates) content and style. Finally, Section 6 concludes the paper and discusses potential directions for future research. 2. RELATED WORK In this section, we begin by introducing key concepts related to txt2img models and their underlying mechanisms. We then review relevant literature on the interpretability of these models. Finally, we introduce the task of contentstyle disentanglement and discuss prior work that addresses this challenge. txt2img generation and diffusion models. Our work centers on the study of txt2img diffusion models. These models learn to predict the amount of noise that has been progressively added to an image, transforming it into Gaussian noise during training. At inference time, the model reverses this process to generate an image from noise only [4]. In txt2img generation, diffusion models are conditioned on textual prompts using text encoders. standard architecture for such models typically includes language encoder (e.g., CLIP [5]) for generating word embeddings, variational autoencoder (VAE) [6] to translate between latent representations and images, and time-conditional U-Net [7] for denoising latent vectors [8]. Interpretability of txt2img models. Given the increasing use and impact of these models, there is growing need for interpretability tools tailored to txt2img diffusion architectures. Among the various approaches offered by the field of Explainable AI, our focus lies on cross-attentionbased methods. These models rely on cross-attention mechanisms to connect the textual information embedded in input prompts with the visual content generated during denoising. This mechanism enables the generation of heatmaps that attribute regions of the image to specific words in the prompt. Several recent works leverage this property to explain diffusion-based models by producing cross-attention attribution maps [9, 10, 11]. These maps can be used to investigate how textual prompts shape visual outputs, analyze syntactic structures through headdependent heatmap interactions, and explore semantic phenomena such as feature entanglement. Additionally, attention-based explanations have proven valuable for downstream tasks such as object segmentation, as they visually delineate what the model perceives as coherent objects [9, 12, 13, 14]. Beyond segmentation, attention maps also facilitate the discovery of biases and inconsistencies in model behavior [11]. Content-style disentanglement. Distinguishing between style and content in art remains longstanding challenge in the field of Computer Vision (CV) [3]. While contentthe subject or objects depicted in an artworkis typically straightforward to identify, style is far more elusive. There is no universally accepted definition of style, and its componentssuch as color usage, brushstrokes, composition, and perspective [15, 16]often overlap with content, the nomaking disentanglement difficult [3]. Moreover, tion of style can vary depending on the specific CV task. Previous work has tackled this challenge by attempting to disentangle the internal representations of models, aiming to separate the latent space into distinct content and style components [3, 17, 18, 19]. While these approaches primarily focus on generating images that match target style, our goal is different. In this work, we aim to investigate how txt2img diffusion models perceive the artistic concepts of content and style by analyzing attention heatmaps. This approach allows us to probe the internal mechanisms of these models, offering insights into how they distinguishor conflatestyle and content. 3. METHODOLOGY The objective of this study is to quantify how transformerbased txt2img diffusion models distinguish between content and style concepts when generating paintings. Specifically, we analyze the spatial distribution of cross-attention values assigned to image pixels from content and style tokens in the conditioning prompt. We systematically construct prompts with varying content and style components, then analyze the spatial distribution of cross-attention values between these tokens and image pixels during generation. To measure content-style separation, we threshold attention maps into binary masks and compute their Intersection over Union (IoU), comparing this overlap against baseline token overlap to determine whether the model spatially distinguishes content from style or treats these concepts as entangled. Due to the need for internal representations, our work only applies to open-source txt2img models. Prompt Templates. When generating images with txt2img models, prompts typically contain various modifiers that influence the output [20]. To maintain experimental clarity and focus on our research question, we specifically isolated content and style components as variables in our prompt templates, deliberately excluding additional This approach modifiers that might confound analysis. aligns with established methodologies in the prompt engineering literature [21], which inspired us in the selection of four distinct prompt templates, to ensure robust results regardless of the specific phrasing: (i) painting of <CONTENT> in the <STYLE> style; (ii) <STYLE> painting of <CONTENT>; (iii) <CONTENT> in the <STYLE> style; (iv) <CONTENT> with <STYLE> style. Taking these four templates, <CONTENT> should be replaced with any subject to be depicted in the generated image (e.g., physical object, person, landscape, etc.), and <STYLE> should be replaced with the name of an artist or artistic movement that defines the desired stylistic approach (e.g., Picasso, Art Nouveau, etc.). Heatmap Extraction. To quantify the influence of specific prompt tokens on generated image regions, we employ Diffusion Attentive Attribution Maps (DAAM) following the methodology established by Tang et al. [9]. This technique enables us to extract token-specific attribution maps that visualize the spatial relationship between textual concepts and visual elements. Formally, we analyze the denoising network of the diffusion model, typically implemented as U-Net architecture comprising sequential downsampling and upsampling convolutional blocks. These blocks compute an internal representation hi,t at every time step t. The internal representations are then conditioned on the word embeddings of the prompt using multi-headed cross-attention layers. (i) [x, y, l, k] is the cross-attention array, normalized to [0, 1], connecting the k-th word to the intermediate coordinates (x, y) for the i-th downsampling block and l-th head. To generate unified attribution map, we aggregate crossattention arrays across spatial, temporal, and architectural dimensions. First, we normalize all intermediate attention arrays through bicubic interpolation to match the original image dimensions (w, h). We then compute the token-specific attribution map DR for the k-th token as: DR [x, y] = ( (i) tj ,k,l[x, y]) (cid:88) tj ,i,l (1) The resulting DR is normalized heatmap with values in [0, 1], where higher intensities indicate stronger attribution between the k-th token and the corresponding image pixel at coordinates (x, y). IoU Evaluation. To quantitatively analyze the spatial relationships between content and style representations, we transform the continuous attribution heatmaps into binary segmentation masks by applying threshold parameter τ to each normalized attribution map DR : DIτ [x, y] = I(DR [x, y] τ ) (2) where I() denotes the indicator function, yielding binary mask where pixels with attention values exceeding τ are set to 1, and all others to 0. For each generated image, we compute the Intersection over Union (IoU) between the content token mask DIτ and the style token mask DIτ : DIτ DIτ DIτ DIτ IoUCS = (3) To establish whether content and style tokens exhibit distinctive spatial attention patterns or tend to focus on overlapping image regions, we compare IoUCS against baseline metric, mIoUB. The baseline defines the mean IoU computed across all possible token pairs in the prompt, with the constraint that each pair includes exactly one token from the set {C, S} (either content or style) and one token from the remaining tokens in the prompt. We use this comprehensive baseline since we cannot exclude priori lexical categories, e.g. stopwords, because they can still carry valuable information [22]; therefore, including all tokens provides an unbiased measure of the general attention landscape. This baseline serves to characterize the general overlap in attention across the prompt: high mIoUB suggests that tokens generally attend to similar image regions, whereas low value indicates more spatially differentiated attention patterns. Given the variability in mIoUB across different prompts, we introduce metric to quantify the deviation of IoUCS from the baseline. This difference metric = mIoUB IoUCS serves as our primary indicator: positive value suggests that content and style tokens attend to distinct spatial regions (lower mutual overlap compared to other pairs), while negative indicates that content and style tokens exhibit higher spatial correlation than expected, suggesting conceptual entanglement in the internal representations of the model. We employ paired t-test to determine statistical significance in the difference between IoUCS and mIoUB. 4. EXPERIMENTAL SETUP In this section, we describe how we construct the prompts to generate images, the chosen txt2img model and the choices of threshold τ for binary heatmap construction. Prompt construction. To populate the templates described in Section 3 with diverse and representative content elements, we utilize the 80 object class labels from the MS COCO dataset [23]. For style components, we incorporate comprehensive collection of 50 style descriptors from the WikiArt dataset [1], comprising 23 individual artists and 27 artistic movements. The combinatorial expansion of these templates, content classes, and style descriptors yields substantial experimental corpus of 16,000 unique prompts, which are subsequently processed by txt2img model. Image Generation. We generate images using the Stable Diffusion XL1 [24] (SDXL) text2img model, with 30 inference steps. SDXL is one of the most established open-source txt2img models available on the HuggingFace platform. Threshold τ . We explored two strategies for selecting the most prominent regions in the heatmaps. The first strategy employs fixed absolute threshold τ , applied directly to the continuous values of DR . We evaluated range of thresholds, specifically τ {0.1, 0.2, . . . , 0.9}. This approach aligns more closely with the method used in [9], where fixed threshold of τ = 0.4 was adopted for downstream analysis. The second strategy adopts relative threshold, where τ is set to the p-th percentile value of the DR distribution, with {0.1, 0.2, . . . , 0.9}. This percentile-based method adapts to the characteristics of the heatmaps generated by specific txt2img model, and guarantees minimum percentage support of 1 for the sets used in the IoU computation. 5. RESULTS In this section, we present both quantitative and qualitative results illustrating how SDXL behaves when selecting between content and style tokens as sources of information during the image generation process. Quantitative evaluation. Figure 2 illustrates how IoUCS, mIoUB, and their difference vary across different threshold (τ ) configurations. Notably, IoUCS remains consistently and significantly lower than mIoUB, particularly for fixed thresholds and percentile-based thresholds between 0.2 and 0.6. This suggests that, on average, the heatmap regions corresponding to content and style tokens do not substantially overlap. This observation is further supported by statistical analysis: we perform paired t-tests and measure the standardized distance between their means in terms of standard deviations. For all threshold configurations, the null hypothesis of equal means can be rejected with p-values below 0.001, while, on average, IoUCS lies 0.64 standard deviations away from mIoUB. From now on, we comment results obtained setting fixed τ = 0.4. Our analysis reveals that the distribution of remains stable across different prompt templates but exhibits substantial variation based on the specific content and style tokens employed. Table 1 presents the highest and lowest average values observed for both content and style components. We recall that high values indicate scenarios where content and 1https://huggingface.co/stabilityai/ stable-diffusion-xl-base-1.0 Fig. 2: Content-Style IoU score (IoUCS, solid line) is compared against the Baseline IoU score (mIoUB, dashed line) and their difference (, dotted line) across varying threshold values (τ ). Markers on the IoU curves are sized proportionally to the mean support of the corresponding sets used in the computation. In the left plot, thresholds are fixed; in the right plot, thresholds are derived from percentile values based on the DR distribution. style components direct attention to spatially disjoint regions within the generated image. Examination of content components with elevated values reveals predominance of animal-related terms, whereas person exhibits the lowest value. Within style components representing artists, we find the only occurrence of negative mean : the Dutch painter Rembrandt. Furthermore, our analysis demonstrates that figurative art movements correlate with higher values, while abstract art movements correspond to lower values, suggesting systematic relationship between artistic representation style and attention distribution patterns. Content Style (Artist) Style (Movement) giraffe banana sheep bear zebra ... cell phone tie hot dog bed person Sargent 0.43 (0.08) Durer 0.39 (0.09) 0.37 (0.12) Dali 0.37 (0.11) Konchalovsky 0.35 (0.12) Monet ... 0.10 (0.17) Kustodiev 0.06 (0.17) Pissarro 0.04 (0.08) van Gogh 0.04 (0.15) Kirchner 0.03 (0.09) Rembrandt 0.39 (0.08) 0.38 (0.09) 0.35 (0.09) 0.35 (0.08) 0.34 (0.09) New Realism Rococo 0.35 (0.10) 0.33 (0.10) 0.29 (0.09) Minimalism 0.29 (0.11) 0.27 (0.11) Ukiyo Art Nouveau ... Impressionism 0.21 (0.17) 0.13 (0.18) Cubism 0.11 (0.18) Abstract Expr. Expressionism 0.09 (0.16) 0.07 (0.14) 0.15 (0.11) 0.14 (0.15) 0.12 (0.17) 0.09 (0.13) -0.07 (0.17) Baroque Table 1: Highest and lowest average values for type of components. Standard deviation reported in parenthesis. Qualitative evaluation. Figure 3 illustrates representative examples of generated images with their corresponding content and style attention heatmaps. On the left, Figure 3a shows examples of clear spatial separation between content and style components, representing the predominant behavior observed throughout our study. Conversely, at the center, Figure 3b exhibits cases with low scores, indicating substantial overlap between content and style attention regions. Notably, in these overlapping cases, the primary subject matter appears integrated with the background rather than dis- (a) (b) (c) Fig. 3: Examples of generated images with clearly distinct content and style components (a), with overlapping content and style components (b), with edge cases (c). tinctly separated as in the previous example. Figure 3c, on the right, reveals intriguing edge cases in model behavior. In the upper example, the carrot generated with Raphael Kirchners style is accompanied by female figure, consistent with Kirchners predominant subject matter of women in his paintings. The lower example presents rare instance of positive values observed in painting generated with Rembrandt style. This behavior is observed when the Rembrandt token is used in combination with the content person, contrasting with the typically low values observed for this component in all the other cases. This anomaly likely stems from Rembrandts extensive self-portraiture, causing the Rembrandt style token to attend strongly to facial features and human forms. 6. CONCLUSIONS This work investigates how transformer-based txt2img diffusion models convey the concepts of content and style when generating paintings. We analyse IoU scores computed on DAAM corresponding to the content and style components in the input prompt. Results indicate that, on average, content and style components tend to influence complementary regions of the generated images. The degree of separation is non-trivial and varies primarily depending on the specific content and style terms used. We identify and analyze edge casesinstances where values are particularly high or lowand provide illustrative examples. Furthermore, we highlight cases suggesting that the models training data can significantly affect this behavior: subjects that appear frequently in stylistic contexts may be internalized by the model as stylistic elements themselves. Future work will extend this analysis by exploring alternative methodological choices, such as variations in crossattention extraction, heatmap resolution, and overlap metrics, and by evaluating carefully selected content and style components that more closely reflect realistic artistic scenarios. We plan to expand our investigation to other txt2img models and are pursuing collaborations with domain experts for human evaluation and deeper analysis of results. 7. REFERENCES [1] Babak Saleh and Ahmed M. Elgammal, Large-scale classification of fine-art paintings: Learning the right metric on the right feature, CoRR, vol. abs/1505.00855, 2015. [2] Antonio Somaini, Algorithmic images: Artificial intelligence and visual culture, Grey Room, , no. 93, pp. 74115, 10 2023. [3] Yankun Wu, Yuta Nakashima, and Noa Garcia, Not only generative art: Stable diffusion for content-style disentanglement in art analysis, in Proceedings of the 2023 ACM International Conference on Multimedia Retrieval, ICMR 2023, Thessaloniki, Greece, June 12-15, 2023. 2023, pp. 199208, ACM. [4] Jonathan Ho, Ajay Jain, and Pieter Abbeel, Denoising diffusion probabilistic models, in Advances in Neural Information Processing Systems. 2020, vol. 33, pp. 68406851, Curran Associates, Inc. [5] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever, Learning transferable visual models from natural language supervision, in Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event. 2021, vol. 139 of Proceedings of Machine Learning Research, pp. 87488763, PMLR. [6] Diederik P. Kingma and Max Welling, Auto-encoding in 2nd International Conference variational bayes, on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. [7] Olaf Ronneberger, Philipp Fischer, and Thomas Brox, U-net: Convolutional networks for biomedical image segmentation, in Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015 - 18th International Conference Munich, Germany, October 5 - 9, 2015, Proceedings, Part III. 2015, vol. 9351 of Lecture Notes in Computer Science, pp. 234241, Springer. [8] Robin Rombach, Andreas Blattmann, Dominik Lorenz, High-resolution Patrick Esser, and Björn Ommer, image synthesis with latent diffusion models, in IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022. 2022, pp. 1067410685, IEEE. [9] Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin, and Ferhan Ture, What the DAAM: Interpreting Stable Diffusion Using Cross Attention, in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023, pp. 56445659, Association for Computational Linguistics. [10] Alec Helbling, Tuna Han Salih Meral, Benjamin Hoover, Pinar Yanardag, and Duen Horng Chau, Conceptattention: Diffusion transformers learn highly interpretable features, CoRR, vol. abs/2502.04320, 2025. [11] Gabriela Ben Melech Stan, Estelle Aflalo, Raanan Yehezkel Rohekar, Anahita Bhiwandiwalla, Shao-Yen Tseng, Matthew Lyle Olson, Yaniv Gurwicz, Chenfei Wu, Nan Duan, and Vasudev Lal, Lvlm-intrepret: An interpretability tool large vision-language models, in The 3rd Explainable AI for Computer Vision (XAI4CV) Workshop, XAI4CV 2024, Workshop at CVPR 2024, Seattle, WA, USA, June 18, 2024, Proceedings, 2024, pp. 81828187. for [12] Junjiao Tian, Lavisha Aggarwal, Andrea Colaco, Zsolt Kira, and Mar González-Franco, Diffuse, attend, and segment: Unsupervised zero-shot segmentation using stable diffusion, in IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024. 2024, pp. 35543563, IEEE. [13] Jinglong Wang, Xiawei Li, Jing Zhang, Qingyuan Xu, Qin Zhou, Qian Yu, Lu Sheng, and Dong Xu, Diffusion model is secretly training-free open vocabulary semantic segmenter, IEEE Trans. Image Process., vol. 34, pp. 18951907, 2025. [14] Chaofan Ma, Yuhuan Yang, Chen Ju, Fei Zhang, Jinxiang Liu, Yu Wang, Ya Zhang, and Yanfeng Wang, Diffusionseg: Adapting diffusion towards unsupervised object discovery, CoRR, vol. abs/2303.09813, 2023. [15] Daniel J. Graham, James M. Hughes, Helmut Leder, and Daniel N. Rockmore, Statistics, vision, and the analysis of artistic style, WIREs Computational Statistics, vol. 4, no. 2, pp. 115123, 2012. [16] Gowthami Somepalli, Anubhav Gupta, Kamal Gupta, Shramay Palta, Micah Goldblum, Jonas Geiping, AbMeasuring hinav Shrivastava, and Tom Goldstein, CoRR, vol. style similarity in diffusion models, abs/2404.01292, 2024. [17] Dmytro Kotovenko, Artsiom Sanakoyeu, Sabine Lang, and Bjorn Ommer, Content and style disentanglement for artistic style transfer, in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 44224431. [18] Hadi Kazemi, Seyed Mehdi Iranmanesh, and Nasser Nasrabadi, Style and content disentanglement in generative adversarial networks, in 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2019, pp. 848856. [19] Aviv Gabbay and Yedid Hoshen, Improving stylecontent disentanglement in image-to-image translation, CoRR, vol. abs/2007.04964, 2020. [20] Jonas Oppenlaender, taxonomy of prompt modifiers for text-to-image generation, Behav. Inf. Technol., vol. 43, no. 15, pp. 37633776, 2024. [21] Vivian Liu and Lydia Chilton, Design Guidelines for Prompt Engineering Text-to-Image Generative Models, in Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. 2022, CHI 22, pp. 123, Association for Computing Machinery. [22] Yu Bai, Heyan Huang, Cesare Spinoso-Di Piano, MarcAntoine Rondeau, Sanxing Chen, Yang Gao, and Jackie Chi Kit Cheung, Identifying and analyzing performance-critical tokens in large language models, 2025. [23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick, Microsoft COCO: Common Objects in Context, in Computer Vision ECCV 2014. 2014, pp. 740755, Springer International Publishing. [24] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach, Sdxl: Improving latent diffusion models for high-resolution image synthesis, in The Twelfth International Conference on Learning Representations, 2024."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Università degli Studi di Milano, Via Celoria, 18, 20133 Milan, Italy"
    ]
}