{
    "paper_title": "VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting Generation with Flexible Pose and Multi-View Joint Modeling",
    "authors": [
        "Hyojun Go",
        "Byeongjun Park",
        "Hyelin Nam",
        "Byung-Hoon Kim",
        "Hyungjin Chung",
        "Changick Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose VideoRFSplat, a direct text-to-3D model leveraging a video generation model to generate realistic 3D Gaussian Splatting (3DGS) for unbounded real-world scenes. To generate diverse camera poses and unbounded spatial extent of real-world scenes, while ensuring generalization to arbitrary text prompts, previous methods fine-tune 2D generative models to jointly model camera poses and multi-view images. However, these methods suffer from instability when extending 2D generative models to joint modeling due to the modality gap, which necessitates additional models to stabilize training and inference. In this work, we propose an architecture and a sampling strategy to jointly model multi-view images and camera poses when fine-tuning a video generation model. Our core idea is a dual-stream architecture that attaches a dedicated pose generation model alongside a pre-trained video generation model via communication blocks, generating multi-view images and camera poses through separate streams. This design reduces interference between the pose and image modalities. Additionally, we propose an asynchronous sampling strategy that denoises camera poses faster than multi-view images, allowing rapidly denoised poses to condition multi-view generation, reducing mutual ambiguity and enhancing cross-modal consistency. Trained on multiple large-scale real-world datasets (RealEstate10K, MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms existing text-to-3D direct generation methods that heavily depend on post-hoc refinement via score distillation sampling, achieving superior results without such refinement."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 5 5 8 5 1 . 3 0 5 2 : r VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting Generation with Flexible Pose and Multi-View Joint Modeling Hyojun Go1* Byeongjun Park1,2* Hyungjin Chung1 2 KAIST 1 EverEx Hyelin Nam1 Byung-Hoon Kim1, Changick Kim2 3 Yonsei University https://gohyojun15.github.io/VideoRFSplat/ Figure 1. Generated 3D Gaussian Splattings and rendered views from diverse texts by VideoRFSplat. VideoRFSplat directly generates realistic 3D scenes from text without SDS [37, 60] refinement, outperforming prior methods [21, 37] that rely on SDS refinements."
        },
        {
            "title": "Abstract",
            "content": "We propose VideoRFSplat, direct text-to-3D model leveraging video generation model to generate realistic 3D Gaussian Splatting (3DGS) for unbounded real-world scenes. To generate diverse camera poses and unbounded spatial extent of real-world scenes, while ensuring generalization to arbitrary text prompts, previous methods fine-tune 2D generative models to jointly model camera poses and multi-view images. However, these methods suffer from instability when extending 2D generative models to joint modeling due to the modality gap, which necessitates additional models to stabilize training and inference. In this work, we propose an architecture and sampling strategy to jointly model multi-view *Equal contribution, Corresponding author images and camera poses when fine-tuning video generation model. Our core idea is dual-stream architecture that attaches dedicated pose generation model alongside pretrained video generation model via communication blocks, generating multi-view images and camera poses through separate streams. This design reduces interference between the pose and image modalities. Additionally, we propose an asynchronous sampling strategy that denoises camera poses faster than multi-view images, allowing rapidly denoised poses to condition multi-view generation, reducing mutual ambiguity and enhancing cross-modal consistency. Trained on multiple large-scale real-world datasets (RealEstate10K, MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms existing text-to-3D direct generation methods that heavily depend on post-hoc refinement via score distillation sampling, achieving superior results without such refinement. 1 1. Introduction Generating realistic 3D scenes from text has garnered increasing attention in AR, gaming, and robotics. Early works [41, 92] primarily relied on Neural Radiance Fields (NeRF) [50, 54, 60] to model 3D scenes, but their computationally intensive volumetric rendering poses limitations for real-time rendering. Recently, 3D Gaussian Splatting (3DGS) [31] has emerged as promising alternative, enabling real-time rendering while preserving highfidelity details. Consequently, recent 3D generation approaches [12, 38, 42, 70, 81, 85, 94] have increasingly adopted 3DGS for its efficiency and high-quality results. To generate 3DGS, several approaches [12, 38, 42, 48, 71, 75, 85] use Score Distillation Sampling (SDS) [60] with 2D diffusion models [63], but this requires time-expensive per-scene optimization. Another line of work adopts pipelinebased methods, where generated images are reconstructed into 3DGS using separate reconstruction models [70, 81, 94] or multi-stage frameworks [15, 65, 86, 87, 98]. While pipeline-based methods can produce high-quality results, their reliance on sequential processing increases the risk of failure and adds unnecessary complexity [40]. To overcome these limitations, recent works have explored direct 3DGS generation [21, 23, 37, 40, 82, 91], which enables end-to-end inference, eliminating per-scene optimization and complex multi-stage pipelines. key approach in this direction is fine-tuning 2D generative models [33, 34, 63, 72], which possess strong text-to-2D generation capabilities, to extend their generalizability to 3DGS from arbitrary texts. Despite the growing number of works targeting 3D generation, the majority focuses on object-level synthesis, leaving real-world scene generation comparatively underexplored. Object-level generation typically operates in controlled, predefined settings as seen in Objaverse datasets [16, 17], where objects are captured with fixed or simple camera trajectories. In contrast, real-world scenes present additional challenges due to their complex and diverse scene-specific camera motions, as well as their unbounded nature with diverse backgrounds and large-scale spatial variations [21, 37]. These pose fundamental challenges to developing generative models for direct 3DGS generation, introducing difficulties distinct from object-level generation. To address these challenges, previous works [21, 37] have adopted generative models that learn the joint distribution of multi-view images and camera trajectories. By incorporating the camera pose into the generative formulation, the model (1) implicitly infers the scenes pose from the text prompt, allowing it to handle diverse and complex camera poses without requiring user inputs. (2) The model generates consistent multi-view images aligned with these inferred scene-specific viewpoints, effectively capturing unbounded environments with diverse backgrounds and spatial variations. However, prior works [21, 36, 37] have suffered from instability in extending 2D generative models to joint modeling due to the modality gap, hindering high-quality generation and alignment between generated images and camera poses. To address this, several stabilization techniques leveraging external models have been proposed. For instance, NVComposer [36] leverages Dust3R [77] distillation to improve consistency, while SplatFlow [21] and director3D [37] rely on other 2D models and refinements during sampling. While these help reduce instability, dependency on external models hinders seamless integration into single model. In this paper, to eliminate external dependency, we present VideoRFSplat, direct 3DGS generation model that introduces an architecture and sampling strategy for jointly generating camera poses and multi-view images when leveraging video generation model. Our core idea is dual-stream architecture, side-attaching dedicated rectified flow-based pose generation model alongside pre-trained video generation model, jointly trained to generate multi-view image latents and camera poses simultaneously. This side-attached pose diffusion model runs parallel to the video models forward stream, interacting at specific layers while maintaining separate forward paths. This design minimizes interference between the two modalities, allowing each to specialize independently and ensuring consistency between poses and multiview images. Similar to previous approaches [21, 37, 82], Gaussian splat decoder decodes 3DGS from the generated poses and image latents in feed-forward fashion. Then, we decouple the timesteps of the pose and multiview generation models, allowing each to operate at different noise levels independently. Unlike standard approaches that synchronize timesteps and noise levels across modalities, our design permits flexible asynchronous sampling. This approach is motivated by our observation that synchronized denoising of multi-view images and camera poses, particularly at early timesteps, leads to mutual ambiguity, increasing uncertainty and causing unstable generation. To mitigate this issue, we design the pose modalityfound to be more robust to faster denoisingto undergo more rapid denoising process than the images. By doing so, the clearer pose information effectively reduces the ambiguity in the pose modality, stabilizing the sampling. Furthermore, we propose an asynchronous adaptation of Classifier-Free Guidance (CFG) that enables the clearer pose to better guide multi-view image generation. Moreover, the proposed asynchronous sampling strategy with decoupled timesteps naturally extends to the camera conditional generation task. We train VideoRFSplat on RealEstate10K [99], MVImgNet [90], DL3DV-10K [43], and ACID [45] datasets. Notably, VideoRFSplat achieves superior performance without relying on SDS++ refinement [37], surpassing existing text-to-3D direct generation methods that depend on SDS++ [21, 37], demonstrating effectiveness of our approach and eliminating dependencies on external models. 2 2. Related Works 2.1. Diffusion Models and Rectified Flow Diffusion models [28, 67, 68] have recently achieved stateof-the-art performance in high-fidelity generation across diverse modalities, including images [18, 20, 35, 57, 58], videos [5, 29, 66], and text [56]. In particular, large-scale text-to-video diffusion models [4, 30, 83] have gained attention, trained on extensive text-video datasets to support arbitrary text inputs. These models inherently possess the capability to generate multiple coherent frames, making them attractive for adaptation to 3D tasks such as image-to-3D generation [39, 40, 46, 89]. Rectified Flow (RF) models [1, 44, 47] provide an alternative to conventional diffusion methods by defining linear transition from data to noise. Motivated by the success of Stable Diffusion 3 [19] in scaling RF for text-toimage generation, recent generative models for images and videos [33, 34, 72] have increasingly adopted RF. Following this, we leverage Mochi [72], high-performing RF-based video generative model, as the backbone for our method. 2.2. 3D Generative Models Scene Representation. Early 3D generation methods synthesize explicit representations such as point clouds [53, 55, 73], shapes [97], and voxel grids [62, 64]. However, these approaches typically struggle to achieve photorealistic outcomes. Neural Radiance Fields (NeRF) [50, 54] significantly advanced photorealism in 3D generation [9, 60], but their computationally intensive volumetric rendering limits real-time applications. Recently, 3D Gaussian Splatting (3DGS) [31] has emerged as promising alternative, enabling real-time rendering and high visual fidelity. Building on these advances, our method utilizes 3DGS to achieve rendering efficiency and visual quality. Text-to-3DGS Generation. Several methods [12, 71] leverage Score Distillation Sampling (SDS) [60] for 3DGS generation, requiring expensive per-scene optimization. Alternatively, pipeline-based methods first generate images and then reconstruct them into 3DGS representations using separate 3DGS reconstruction models [70, 81, 94]. While these methods accelerate generation compared to SDS-based methods, their sequential processes introduce complexity and susceptibility to failure [40]. To further simplify and improve stability, recent works have shifted towards direct 3DGS generation models, enabling amortized, end-to-end inference [21, 23, 37, 40, 82, 91]. However, the majority of these direct generation methods [23, 40, 91] focus on objectlevel synthesis, typically utilizing datasets with consistent, canonical viewpoints such as Objaverse datasets [16, 17]. In contrast, real-world scene generation presents greater challenges due to variations in scale, complexity, and scenespecific camera trajectories. Early methods for real-world scene generation [15, 65, 86, 87, 98] employ progressive depth-based warping and diffusion-based inpainting, suffering from distortions due to per-view inpainting. Another approach conditions models on predefined camera poses [82] or both images and poses [39], but relies on manual or heuristic camera trajectories, limiting flexibility for diverse scene-specific motions. Alternatively, joint modeling methods have recently been proposed to simultaneously generate multi-view images and corresponding camera trajectories directly from text prompts [21, 37]. For instance, Director3D [37] first generates camera trajectories and then synthesizes images, while SplatFlow [21] generates both concurrently within unified model. However, to achieve high-quality generation, these methods rely on SDS refinement [37] or incorporate additional models during sampling, hindering seamless integration into single model. In this work, we propose sampling and architectural approach that enables superior 3DGS generation without such refinements. 3. Preliminary: Rectified Flow Rectified Flows (RFs) [1, 44, 47] define generative model that starts from simple noise distribution q0 (often (0, I)) and evolves samples via an ODE: dXt = vt(Xt) dt, X0 q0, [0, 1], (1) where vt(Xt) is modeled by neural network vt(Xt) = uθ(Xt, 1 t). To train uθ, one pairs samples from the data distribution p0 and noise distribution q0 (collectively denoted p1) and considers linear path Yt = tY1 +(1t)Y0. This induces the ODE dYt = ut(Yt Y1) dt with ut(Yt Y1) = Y1 Y0, and the marginal vector field is (cid:90) ut(Yt) = ut(Yt Y1) pt(Yt Y1) pt(Yt) p1(Y1) dY1. (2) (cid:2)ut(Yt) Minimizing the flow-matching loss LFM = Et,Yt uθ(Yt, t)2(cid:3) is typically replaced by the more efficient conditional flow-matching objective: LCFM = Et,Y1p1 (cid:2)ut(Yt Y1) uθ(Yt, t)2(cid:3), (3) whose gradients coincide with those of LFM [44]. After training, samples are generated by solving the ODE dXt = uθ(Xt, 1 t) dt from = 0 to = 1. 4. VideoRFSplat In this section, we present VideoRFSplat, framework for generating 3DGS from text prompts for real-world scenes. As illustrated in Fig. 2, our model consists of two main components: 1) dual-stream pose-video joint model, where pose generation module is side-attached to pretrained video generation model, jointly trained to generate multi-view image latents and camera poses. 2) Gaussian Splat decoder, which reconstructs 3DGS from the generated image latents and camera poses in feed-forward manner. In Section 4.1, 3 Figure 2. VideoRFSplat Overview. (a) VideoRFSplat consists of dual-stream pose-video model and Gaussian Splat decoder. To minimize pose-image interference, the pose model is side-attached to the pre-trained video model, interacting through communication blocks. With separate timesteps for pose and video models, this enables asynchronous sampling, reducing ambiguity and improving sampling stability. (b) Communication block, where cross-attention facilitates bidirectional information exchange between the pose and image modalities. we first present the architecture and sampling process of the dual-stream pose-video joint model. Then, Section 4.2 details the Gaussian Splat decoder. 4.1. Dual-Stream Pose-Video Joint Model Architecture. We start by considering set of K-view i=1 (Ii RHW 3) and their camimages = {Ii}K era projection matrices = {Pi}K i=1(Pi = Ki[RiTi]), where Ki, Ri, and Ti denote intrinsic, rotation, translation parameters. Then, we define the latents of the images as = E(I0, ..., IK). The goal of the dual-stream pose-video joint model is to generate and simultaneously. Building on RayDiffusion [93], which demonstrated the effectiveness of modeling camera poses as rays, and on recent jointgeneration methods [21, 36], we adopt the Plucker ray representation [59, 93]. Each ray ri is parameterized as di, mi, where di = wi and mi = (R Ti) di. Here, wi denotes the corresponding 2D pixel coordinates. For brevity, we denote the full set of rays as = {ri}K K1 i=1. To extend pre-trained 2D diffusion models for joint generation of camera poses and multi-view images, previous works [21, 36, 82] typically concatenate image latents with camera ray embeddings by aligning their spatial resolutions. However, we observe that this approach degrades text generalization and overall generation quality (see Table 4 and Fig. 7). We attribute this degradation to interference from sharing the same forward path and parameters for image and camera pose. Specifically, since pre-trained video models are trained on natural video distributions, incorporating Plucker ray embeddings within this shared structure introduces conflicts due to the significant distribution gap, further degrading text generalization and generation quality. As result, this architectural limitation prevents fully leveraging 4 the capabilities of 2D models, leading SplatFlow [21] to rely on SDS for improved quality. To reduce interference, we propose dual-stream architecture with dedicated submodules for pose and image generation, communicating via cross-attention at intermediate layers (see Fig. 2 (a)). The pose generation model adopts transformer-based architecture [72, 74], explicitly conditioned on textual prompts and pose-specific timestep to generate camera rays [93], forming structurally selfcontained module for camera pose generation. Communication between the two modules occurs at specific intermediate layers through cross-attention mechanisms (see Fig. 2 (b)). Specifically, let hR and hI be the intermediate outputs of the pose and video models, respectively. We concatenate the pose timestep embedding and hR, as well as the video timestep embedding and hI, then update them bidirectionally via cross-attention: hR CrossAttention(hI, hR) and hI CrossAttention(hR, hI). This exchange enables controlled interaction between the two models while preserving their specialized forward paths and reducing interference between pose and multi-view modalities. Further details on the architecture are provided in Appendix A. Asynchronous Sampling. Following Eq. 2, joint modeling of multi-view image latents and their corresponding camera rays involves estimating the conditional expectation of the marginal vector field at timestep t, represented as E[uI are conditional vector fields for multi-view images and camera rays conditioned on text prompts. Here, It and Rt represent the corresponding intermediate noisy data at timestep t. Previous joint modeling approaches [21, 36] adopt synchronous timestep strategy, assuming same timestep for both modalities. However, during joint sampling, images and camera poses are inferred It, Rt], where uI and uR , uR fixed intrinsic dimensionality compared to images, which is also overparameterized as ray representations. Thus, asynchronous sampling effectively mitigates mutual ambiguity in joint generation while avoiding these issues. Additionally, the faster-denoised pose can more effectively guide multi-view image generation by adapting CFG [27] within our asynchronous framework. Specifically, from the viewpoint of each modality, when the opposite modalitys timestep is close to 1, its state approximates Gaussian noise distribution, thus behaving as an unconditional modality [2, 3]. For instance, when tR 1, we have E[uI It] [2, 3]. Leveraging this property, we can adapt CFG with strength to incorporate an unconditional term for the camera rays as follows: It, R1] E[uI (1+s)uθ(ItI , RtR , c, tI, tR)suθ(ItI , R1, c, tI, 1). (5) Camera Conditioned Generation. Conversely, when the opposite modalitys timestep approaches 0, the generation becomes conditioned on the opposite modality. For instance, when tR 0, the conditional vector field for images becomes E[uI It, R0], allowing generation of cameraconditioned images. Although our main task is text-to-3DGS generation, we will demonstrate that our model naturally supports camera-conditioned multi-view image generation. 4.2. Gaussian Splat Decoder With the dual-stream pose-video joint model presented in the previous section, the remaining task is to decode the generated latents into 3DGS representation. To accomplish this, we employ Gaussian Splat decoder, 3D CNN-based architecture with attention layers, which takes the generated and as input, and outputs pixel-aligned 3D Gaussian Splatting representation {µj, αj, Σj, cj}HW [6, 11]. Similar to feed-forward 3DGS methods [6, 7, 11, 14, 69], our Gaussian Splat decoder learns to decode 3DGS in data-driven way, enabling fast reconstruction. j=1 We train the Gaussian Splat decoder by rendering the decoded 3DGS to novel target views and minimizing combination of L1 and LPIPS [95] losses against ground-truth images, weighted as Ltarget = LL1 + 0.1 LLPIPS. Furthermore, to support camera-conditioned generation, we incorporate reconstruction objective for the source views using the same weighted combination of L1 and LPIPS losses Lsource = LL1 +0.1 LLPIPS. Further details on Gaussian Splat Decoder are provided in Appendix A. 5. Experimental Results Here, we demonstrate the effectiveness of VideoRFSplat for text-to-3DGS generation. Our primary result is that VideoRFSplat, without SDS optimization, outperforms previous direct text-to-3DGS methods that employ SDS optimization. 5 Figure 3. Failure analysis of synchronized sampling and the effectiveness of asynchronous sampling. (Left) Early in sampling (t > 0.85), synchronous sampling induces excessive oscillations in camera poses, causing divergence and misalignment with images. Then, misaligned poses lead to inconsistent multi-view generation, particularly in the background. (Right) Asynchronous sampling stabilizes joint generation, leading to coherent multi-view generation. together, creating mutual dependency. In early sampling, both modalities are noisy and incomplete, each relying on ambiguous signals from the other. This mutual ambiguity amplifies uncertainty, potentially causing divergence and unstable generation outcomes, as illustrated in Fig. 3. , uR tR (cid:2)[uI tI To address this, we propose an asynchronous timestep strategy, decoupling the timesteps of pose and multi-view generation modules and enabling one modality to denoise faster, thereby reducing ambiguity during sampling. To enable this, we use the following loss: Lours := EtI ,tR ] uθ(ItI , RtR, c, tI, tR)2(cid:3) , (4) where θ denotes parameters, is the conditioning text prompt, and we omit the expectation with respect to and for brevity. This loss enables vector field prediction even with different timesteps for pose and image modalities. Since our architecture conditions each modalitys module on its own timestep embedding, implementing asynchronous timesteps simply involves providing separate timestep inputs to each module as tR and tI as in Fig. 2 (a). During sampling, we denoise the pose modality faster than images, as it is robust to fast denoising. In implementation, both modalities start from Gaussian noise, with the poses sampling timestep adjusted as tR = max(tI δ, 0) as shown in Fig. 4. However, this design introduces two potential risks: (1) excessively rapid denoising of the pose modality at early stages may introduce instability, and (2) since the pose modality reaches timestep 0 faster, it must complete generation in fewer sampling steps. Despite these concerns, we find that the pose modality remains robust across wide range of δ. We hypothesize that this robustness stems from its Figure 4. Asynchrnous schedule (δ = 0.2). traditional wooden gate with red lanterns. large stone fountain surrounded by lush greenery and clear blue sky. pair of cowboy boots in barn. Light blue computer mouse with green light . Original w/ SDS++ Original w/ SDS++ Text Director3D [37] SplatFlow [21] Original Ours Figure 5. Qualitative comparison of text-to-3DGS generation on DL3DV [43] and MVImgNet [90] validation sets as well as T3Bench [24]. Rendered scenes: First two rows from DL3DV, the third row from T3Bench, and the last row from MVImgNet. Despite not using SDS++ [37], VideoRFSplat generates detailed, visually consistent scenes, producing appropriate scene-specific camera poses. 5.1. Experimental Setups Due to space constraints, we present brief overview of the experimental setup. Comprehensive details of all experimental setups are available in Appendix B. Datasets and Training. We utilize four real-world datasets for training: RealEstate10K [99], MVImgNet [90], DL3DV10K [43], and ACID [45]. Since these datasets lack paired textual descriptions, we generate textual annotation using InternVL-2.5-26B [13], producing multiple captions per sequence. We will release these annotations to support future research. For the video generation backbone, we use Mochi [72]. We resize all frames to 320512 during training, setting = 8, as in SplatFlow [21] and Director3D [37]. Baselines. To our knowledge, SplatFlow [21] and Director3D [37] are current main competitors among available direct text-to-3DGS generation methods. As both methods use SDS++ [37] as refinement step, we compare two variants for each method: with and without SDS++. Additionally, for evaluations on T3Bench [24], we include previously reported baseline results [21, 37]. 6 Method BRISQUE NIQE CLIPScore Quality Metrics CLIPScore DreamFusion [60] Magic3D [41] LatentNeRF [49] SJC [76] Fantasia3D [10] ProlificDreamer [78] Director3D [37] Director3D (w/ SDS++) [37] SplatFlow [21] SplatFlow (w/ SDS++) [21] VideoRFSplat 90.2 92.8 88.6 82.0 69.6 61.5 37.1 32.3 35.6 32.4 32.2 10.48 11.20 9.19 10.15 7.65 7.07 6.41 4.35 5.88 4.24 4. - - - - - - 32.0 32.9 28.9 33.2 33.6 Table 1. Quantitative results on T3Bench [24]. VideoRFSplat outperforms all baselines without SDS++ refinement. Method MVImgNet [90] DL3DV [43] FID-10K CLIPScore FID-2.4K CLIPScore Director3D [37] Director3D (w/ SDS++) [37] SplatFlow [21] SplatFlow (w/ SDS++) [21] 39.55 41.80 34.85 35.46 30.48 31.00 31.43 32.30 88.44 95.88 79.91 85. 30.04 31.68 30.06 31.90 30.33 VideoRFSplat 32.5 Table 2. Quantitative results on MVImgNet [90] and DL3DV [43] validation sets. VideoRFSplat achieves the higher performance across all metrics without SDS++ refinement. 73.69 33. Evaluation Protocol. Following previous works [21, 37], we evaluate our model on the MVImgNet and DL3DV validation datasets, as well as the T3Bench benchmark [24]. MVImgNet and DL3DV serve as in-domain tests for our method, Director3D, and SplatFlow, while T3Bench tests out-of-domain prompts. As in [21], we use FID [26] to assess image quality and CLIP score for text-image alignment on the MVImgNet and DL3DV validation sets. For T3Bench evaluations, we adopt established benchmark metrics, including CLIP score [25], NIQE [52], and BRISQUE [51]. 5.2. Main Results: Text-to-3DGS Generation Quantitative results. Tables 1 and 2 present quantitative comparisons of VideoRFSplat against baselines on T3Bench, MVImgNet, and DL3DV. VideoRFSplat consistently outperforms baselines across all metrics without relying on SDS++ refinement. On T3Bench, VideoRFSplat achieves the lowest BRISQUE (32.2) and NIQE (4.20), indicating improved image quality. On MVImgNet, VideoRFSplat achieves significantly lower FID (30.33) and higher CLIP score (33.0) than baselines. Similarly, on DL3DV, it achieves the lowest FID (73.69) and highest CLIP score (32.5). We attribute these strong results, achieved without SDS-based refinement, to three key factors: (1) leveraging powerful pre-trained video generation model, (2) adopting more effective architectural design, and (3) incorporating an improved sampling strategy. We analyze these factors in the following sections. Qualitative results. Consistent with our quantitative findings, Fig. 5 shows that VideoRFSplat qualitatively outperforms SplatFlow and Director3D without SDS++ refinement. Method BRISQUE NIQE Multi-view Rendered Vanilla sampling Vanilla sampling (w/o Eq. 4) δ = 0.1, w/o modified CFG δ = 0.2, w/o modified CFG δ = 0.3, w/o modified CFG δ = 0.4, w/o modified CFG δ = 0.5, w/o modified CFG δ = 0.1, Asynchronous Sampling δ = 0.2, Asynchronous Sampling δ = 0.3, Asynchronous Sampling δ = 0.4, Asynchronous Sampling δ = 0.5, Asynchronous Sampling Vanilla sampling - 200 steps Async sampling - 200 steps (δ = 0.2) 34.9 35.3 34.0 34.1 33.9 34.6 35.2 32.8 32.2 33.0 33.8 34. 33.8 31.8 5.32 5.64 4.43 4.39 4.36 4.67 4.9 4.30 4.20 4.27 4.52 4.76 4.56 4.19 33.3 33. 33.8 34.0 34.0 34.2 34.3 33.8 34.0 33.9 33.8 33.8 33.7 34.2 32.8 32.8 33.2 33.2 33.3 33.3 33.1 33.4 33.6 33.5 33.4 33. 33.1 33.7 Table 3. Ablation study on asynchronous sampling. We also report CLIP scores on multi-view images to assess text alignment of not lifted images to 3DGS. . i . s . i . s A black and white photograph framed in dark mahogany wooden rocking horse in childs playroom Figure 6. Effectiveness of asynchronous sampling. Asynchronous sampling enhances camera poses for better key object framing, while vanilla sampling mispredicts poses, misframing key objects. Specifically, Director3D often produces blurry outputs, lacking sharpness and detail for realism. Similarly, SplatFlow often struggles to render precise textures and maintain consistent fidelity. In contrast, VideoRFSplat consistently produces sharper, more detailed, and realistic images without SDSbased refinement, at higher resolutions. These results highlight VideoRFSplats ability to generate high-quality 3DGS well-aligned with text prompts, underscoring its effectiveness over SDS-dependent methods. Additional qualitative results are available in Appendix C. 5.3. Analysis: Asynchronous Sampling Here, we delve into the effectiveness and characteristics of our asynchronous sampling. In addition to metrics for T3Bench, we compute CLIP scores on generated multi-view images to assess text alignment for unlifted images to 3DGS. Effectiveness of asynchronous sampling. Table 3 shows that asynchronous sampling achieves better quality metrics and CLIP scores than vanilla sampling, with peak improve7 ments at δ = 0.2. Moreover, asynchronous sampling remains robustly effective even with 200 sampling steps. As shown in Fig.6, asynchronous sampling not only mitigates the divergence issues in Fig.3 but also yields better camera trajectories that correctly frame key objects, whereas vanilla sampling mispredicts camera trajectories, resulting in misframed key objects. We hypothesize that uncertainty in early sampling leads to unstable pose-image interactions, destabilizing camera pose generation and ultimately degrading multi-view image quality. In contrast, asynchronous sampling mitigates this instability, resulting in more stable and improved generation. Analysis on δ From Table 3, we identify three key insights: (1) First, asynchronous sampling consistently performs well across all δ values tested, regardless of whether the modified CFG is applied, showing its robustness. Even at δ = 0.1, the lowest value tested, it outperforms vanilla sampling and maintains consistent gains up to δ = 0.5. (2) Second, without the modified CFG, multi-view CLIP scores continue to increase as δ increases, indicating that faster pose denoising reduces ambiguity and strengthens prompt alignment in multi-view images. However, beyond δ = 0.3, the rendered CLIP score and quality metrics decline, showing that excessively fast pose denoising destabilizes camera poses and degrades 3DGS quality. (3) Lastly, applying modified CFG generally improves quality metrics and rendered CLIP scores compared to not using it (except at δ = 0.4). This suggests that modified CFG better aligns with rapidly denoised camera poses, improving rendered image quality. Notably, unlike the case without modified CFG, increasing δ does not monotonically improve multi-view CLIP scores; instead, scores peak at δ = 0.2 before declining. Nevertheless, rendered CLIP scores remain higher than those without modified CFG. This suggests that while the modified CFG strengthens pose-image alignment and improves 3DGS quality, excessive guidance from unstable pose at high δ may degrade prompt alignment. Effect of training with Eq. 4 Additionally, we evaluate performance when the training scheme does not employ timestep division as outlined in Eq. 4. Our proposed training scheme, which divides timesteps, demonstrates slightly better results. This suggests that our approach of dividing timesteps during training is not detrimental and achieves comparable or marginally improved performance. 5.4. Analysis: Architectural Approach Method NIQE BRISQUE CLIPScore To validate the effectiveness of our architectural design, we compare it against the design scheme of SplatFlow [21], which uses channel concatenation for joint pose-image generation. We trained both Table 4. Architecture comparison SplatFlow [21] Ours 42.09 39.11 29.8 32. 7.83 6.19 pink bicycle leaning against fence striped beach umbrella standing tall on sandy beach Figure 7. Architecture Comparison. For each example, Left: channel concat architecture (SplatFlow). Right: our architecture. Method FID-8K TransErr RotErr CLIPScore MotionCtrl [79] CameraCtrl [22] VideoRFSplat 69.04 62.97 43.07 0.109 0.088 0. 0.5648 0.5176 0.4223 29.8 30.1 31.1 Table 5. Results on camera conditioned generation. VideoRFSplat can perform camera conditioned generation. models under identical conditions for 60K iterations with Mochi [72] and then compared their multi-view results. Table 4 shows that our architecture consistently outperforms channel concatenation of SplatFlow across all metrics. Our model achieves better image quality, with lower NIQE (6.19 vs. 7.83) and BRISQUE (39.11 vs. 42.09) scores, and significantly higher CLIPScore (32.5 vs. 29.8), indicating better text alignment. Figure 7 qualitatively illustrates these improvements, preserving more details of images than SplatFlow. This highlights that parameter-sharing like channel concatenation can degrade joint pose-image generation, underscoring the benefits of our dual-stream approach. 5.5. Results: Camera Conditioned Generation With the camera timestep set to 0, VideoRFSplat generates images following given camera trajectories. We evaluate this both quantitatively and qualitatively. For evaluation, we use 1000 sequences from RealEstate10K [99] with extracted camera trajectories and captions to generate images. We assess pose alignment via normalized translation and rotation errors from ParticleSFM [96]-estimated poses against ground truth. We compare VideoRFSplat against MotionCtrl [79] and CameraCtrl [22], reporting quantitative results in Table 5. VideoRFSplat outperforms other methods in FID-8K (43.07), translation error (0.063), rotation error (0.4223), and CLIPScore (31.1). These results confirm that VideoRFSplat generates images following camera trajectories. Qualitative results are available in the Appendix C. 6. Conclusion In this paper, we introduced VideoRFSplat, direct text-to3DGS model for real-world scenes. Built upon video generative model, our approach utilizes dual-stream architecture to minimize the interference between pose and image modalities. Moreover, we propose asynchronous sampling, which accelerates pose denoising while enhancing CFG, leading to more stable joint sampling and improved generation quality. Through extensive quantitative and qualitative evaluations, VideoRFSplat achieves state-of-the-art performance in direct text-to-3DGS generation, surpassing existing baselines even without SDS refinements. We believe that adapting 2D generative models for joint multi-view and pose generation is crucial for real-world scene generation, and VideoRFSplat offers valuable insights into both sampling strategies and architectural design to advance this direction."
        },
        {
            "title": "References",
            "content": "[1] Michael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. 3 [2] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analyticdpm: an analytic estimate of the optimal reverse variarXiv preprint ance in diffusion probabilistic models. arXiv:2201.06503, 2022. 5 [3] Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu. One transformer fits all distributions in multi-modal diffusion at scale. In International Conference on Machine Learning, pages 16921717. PMLR, 2023. 5, 13 [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 [5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2256322575, 2023. 3 [6] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1945719467, 2024. [7] Anpei Chen, Haofei Xu, Stefano Esposito, Siyu Tang, and Andreas Geiger. Lara: Efficient large-baseline radiance fields. In European Conference on Computer Vision, pages 338355. Springer, 2024. 5 [8] Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:24081 24125, 2025. 13 [9] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen Tu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf: unified approach to 3d generation and reconstruction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 24162425, 2023. 3 [10] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highIn Proceedings of the quality text-to-3d content creation. IEEE/CVF international conference on computer vision, pages 2224622256, 2023. 7 [11] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In European Conference on Computer Vision, pages 370386. Springer, 2024. [12] Zilong Chen, Feng Wang, Yikai Wang, and Huaping Liu. Text-to-3d using gaussian splatting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2140121412, 2024. 2, 3 [13] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 6, 14 [14] Zequn Chen, Jiezhi Yang, and Heng Yang. Pref3r: Posefree feed-forward 3d gaussian splatting from variable-length image sequence. arXiv preprint arXiv:2411.16877, 2024. 5 [15] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free genarXiv preprint eration of 3d gaussian splatting scenes. arXiv:2311.13384, 2023. 2, 3 [16] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36:3579935813, 2023. 2, 3 [17] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1314213153, 2023. 2, 3 [18] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [19] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 3 [20] Hyojun Go, Yunsung Lee, Seunghyun Lee, Shinhyeok Oh, Hyeongdon Moon, and Seungtaek Choi. Addressing negative transfer in diffusion models. Advances in Neural Information Processing Systems, 36:2719927222, 2023. 3 [21] Hyojun Go, Byeongjun Park, Jiho Jang, Jin-Young Kim, Soonwoo Kwon, and Changick Kim. Splatflow: Multi-view rectified flow model for 3d gaussian splatting synthesis. arXiv preprint arXiv:2411.16443, 2024. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15 [22] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 8 [23] Xianglong He, Junyi Chen, Sida Peng, Di Huang, Yangguang Li, Xiaoshui Huang, Chun Yuan, Wanli Ouyang, and Tong He. Gvgen: Text-to-3d generation with volumetric representation. In European Conference on Computer Vision, pages 463479. Springer, 2024. 2, 3 9 [24] Yuze He, Yushi Bai, Matthieu Lin, Wang Zhao, Yubin Hu, Jenny Sheng, Ran Yi, Juanzi Li, and Yong-Jin Liu. T3 bench: Benchmarking current progress in text-to-3d generation. arXiv preprint arXiv:2310.02977, 2023. 6, [25] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 7 [26] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 7 [27] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5 [28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [29] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 3 [30] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 3 [31] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 3 [32] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [33] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, 3 [34] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2, 3 [35] Yunsung Lee, JinYoung Kim, Hyojun Go, Myeongho Jeong, Shinhyeok Oh, and Seungtaek Choi. Multi-architecture multiexpert diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1342713436, 2024. 3 [36] Lingen Li, Zhaoyang Zhang, Yaowei Li, Jiale Xu, Xiaoyu Li, Wenbo Hu, Weihao Cheng, Jinwei Gu, Tianfan Xue, and Ying Shan. Nvcomposer: Boosting generative novel view synthesis with multiple sparse and unposed images. arXiv preprint arXiv:2412.03517, 2024. 2, 4 [37] Xinyang Li, Zhangyu Lai, Linning Xu, Yansong Qu, Liujuan Cao, Shengchuan Zhang, Bo Dai, and Rongrong Ji. Director3d: Real-world camera trajectory and 3d scene generation from text. Advances in Neural Information Processing Systems, 37:7512575151, 2025. 1, 2, 3, 6, 7, [38] Zongrui Li, Minghui Hu, Qian Zheng, and Xudong Jiang. Connecting consistency distillation to score distillation for text-to-3d generation. In European Conference on Computer Vision, pages 274291. Springer, 2024. 2 [39] Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos Plataniotis, Sergey Tulyakov, and Jian Ren. Wonderland: Navigating 3d scenes from single image. arXiv preprint arXiv:2412.12091, 2024. 3 [40] Chenguo Lin, Panwang Pan, Bangbang Yang, Zeming Li, and Yadong Mu. Diffsplat: Repurposing image diffusion models for scalable gaussian splat generation. arXiv preprint arXiv:2501.16764, 2025. 2, 3 [41] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, MingYu Liu, and Tsung-Yi Lin. Magic3d: High-resolution textIn Proceedings of the IEEE/CVF to-3d content creation. conference on computer vision and pattern recognition, pages 300309, 2023. 2, 7 [42] Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, and Karsten Kreis. Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 85768588, 2024. 2 [43] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learningbased 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22160 22169, 2024. 2, 6, 7, [44] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [45] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite nature: Perpetual view generation of natural scenes from single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1445814467, 2021. 2, 6, 13 [46] Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, and Yueqi Duan. Reconx: Reconstruct any scene from sparse views with video diffusion model. arXiv preprint arXiv:2408.16767, 2024. 3 [47] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3 [48] Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, Xihui Liu, and Ziwei Liu. Humangaussian: Text-driven 3d human generation with gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66466657, 2024. 2 [49] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1266312673, 2023. 7 [50] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2, 3 [51] Anish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik. No-reference image quality assessment in the spatial domain. IEEE Transactions on image processing, 21(12): 46954708, 2012. 7 [52] Anish Mittal, Rajiv Soundararajan, and Alan Bovik. Making completely blind image quality analyzer. IEEE Signal processing letters, 20(3):209212, 2012. 7 [53] Shentong Mo, Enze Xie, Ruihang Chu, Lanqing Hong, Matthias Niessner, and Zhenguo Li. Dit-3d: Exploring plain diffusion transformers for 3d shape generation. Advances in neural information processing systems, 36:6796067971, 2023. 3 [54] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4):115, 2022. 2, 3 [55] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. 3 [56] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. [57] Byeongjun Park, Hyojun Go, Jin-Young Kim, Sangmin Woo, Seokil Ham, and Changick Kim. Switch diffusion transformer: Synergizing denoising tasks with sparse mixture-of-experts. In European Conference on Computer Vision, pages 461477. Springer, 2024. 3 [58] Byeongjun Park, Sangmin Woo, Hyojun Go, Jin-Young Kim, and Changick Kim. Denoising task routing for diffusion models. In The Twelfth International Conference on Learning Representations, 2024. 3 [59] Julius Plucker. Analytisch-geometrische Entwicklungen. GD Baedeker, 1828. 4 [60] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In The Eleventh International Conference on Learning Representations, 2023. 1, 2, 3, 7 [61] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [62] Xuanchi Ren, Jiahui Huang, Xiaohui Zeng, Ken Museth, Sanja Fidler, and Francis Williams. Xcube: Large-scale 3d generative modeling using sparse voxel hierarchies. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 42094219, 2024. 3 [63] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 2 [64] Aditya Sanghi, Rao Fu, Vivian Liu, Karl DD Willis, Hooman Shayani, Amir Khasahmadi, Srinath Sridhar, and Daniel Ritchie. Clip-sculptor: Zero-shot generation of high-fidelity and diverse shapes from natural language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1833918348, 2023. 3 [65] Jaidev Shriram, Alex Trevithick, Lingjie Liu, and Ravi Ramamoorthi. Realmdreamer: Text-driven 3d scene generation with inpainting and depth diffusion. arXiv preprint arXiv:2404.07199, 2024. 2, 3 [66] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. [67] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. pmlr, 2015. 3 [68] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 3 [69] Stanislaw Szymanowicz, Eldar Insafutdinov, Chuanxia Zheng, Dylan Campbell, Joao Henriques, Christian Rupprecht, and Andrea Vedaldi. Flash3d: Feed-forward generalisable 3d scene reconstruction from single image. arXiv preprint arXiv:2406.04343, 2024. 5 [70] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, pages 118. Springer, 2024. 2, 3, 13 [71] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. In The Twelfth International Conference on Learning Representations, 2024. 2, 3 [72] Genmo Team. Mochi 1. https://github.com/ genmoai/models, 2024. 2, 3, 4, 6, 8, [73] Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, Karsten Kreis, et al. Lion: Latent point diffusion models for 3d shape generation. Advances in Neural Information Processing Systems, 35:1002110039, 2022. 3 [74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 4 [75] Alexander Vilesov, Pradyumna Chari, and Achuta Kadambi. Cg3d: Compositional generation for text-to-3d via gaussian splatting. arXiv preprint arXiv:2311.17907, 2023. 2 [76] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1261912629, 2023. 7 [77] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20697 20709, 2024. 2 [78] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and [90] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, et al. Mvimgnet: largescale dataset of multi-view images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 91509161, 2023. 2, 6, 7, 13 [91] Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, and Baining Guo. Gaussiancube: structured and explicit radiance reprearXiv preprint sentation for 3d generative modeling. arXiv:2403.19655, 2024. 2, 3 [92] Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing Liao. Text2nerf: Text-driven 3d scene generation with neural radiance fields. IEEE Transactions on Visualization and Computer Graphics, 30(12):77497762, 2024. 2 [93] Jason Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, and Shubham Tulsiani. Cameras as rays: Pose estimation via ray diffusion. arXiv preprint arXiv:2402.14817, 2024. 4, 13 [94] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. In European Conference on Computer Vision, pages 119. Springer, 2024. 2, 3 [95] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [96] Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. Particlesfm: Exploiting dense point trajectories In European for localizing moving cameras in the wild. Conference on Computer Vision, pages 523542. Springer, 2022. 8 [97] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation and completion through point-voxel diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 58265835, 2021. 3 [98] Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, and Achuta Kadambi. Dreamscene360: Unconstrained text-to-3d scene generation with panoramic gaussian splatting. In European Conference on Computer Vision, pages 324342. Springer, 2024. 2, 3 [99] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Stereo magnification: Learning and Noah Snavely. view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018. 2, 6, 8, 13 diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36:84068441, 2023. 7 [79] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [80] Sangmin Woo, Byeongjun Park, Hyojun Go, Jin-Young Kim, and Changick Kim. Harmonyview: Harmonizing consistency and diversity in one-image-to-3d. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1057410584, 2024. 13 [81] Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, and Gordon Wetzstein. Grm: Large gaussian reconstruction model for efficient 3d reconstruction and generation. In European Conference on Computer Vision, pages 120. Springer, 2024. 2, 3 [82] Yuanbo Yang, Jiahao Shao, Xinyang Li, Yujun Shen, Andreas Geiger, and Yiyi Liao. Prometheus: 3d-aware latent diffusion models for feed-forward text-to-3d scene generation. arXiv preprint arXiv:2412.21117, 2024. 2, 3, 4 [83] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 3 [84] Vickie Ye, Ruilong Li, Justin Kerr, Matias Turkulainen, Brent Yi, Zhuoyang Pan, Otto Seiskari, Jianbo Ye, Jeffrey Hu, Matthew Tancik, and Angjoo Kanazawa. gsplat: An open-source library for Gaussian splatting. arXiv preprint arXiv:2409.06765, 2024. 13 [85] Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 67966807, 2024. [86] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William Freeman, and Jiajun Wu. Wonderworld: Interactive 3d arXiv preprint scene generation from single image. arXiv:2406.09394, 2024. 2, 3 [87] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, et al. Wonderjourney: In Proceedings of Going from anywhere to everywhere. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66586667, 2024. 2, 3 [88] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 13 [89] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 3 12 A. Details of VideoRFSplat Here, we present additional details on VideoRFSplat, expanding on the discussion in Section 4 to address space constraints. We first delve into the architecture and camera optimization of the dual-stream pose-video joint model in Section A.1, emphasizing key modifications from the Mochi video generation model. Following that, Section A.2 details the architecture of the Gaussian Splat decoder. Lastly, in Section A.3, we outline the previously omitted aspects of CFG for camera-conditioned generation. A.1. Dual-Stream Pose-Video Joint Model Architecture. We adopt Mochi [72] as the backbone for our video generation model without any architectural modifications. Mochi utilizes an Asymmetric Diffusion Transformer architecture with full 3D attention, enabling highfidelity video synthesis. The model consists of approximately 10 billion parameters, and it employs the T5-XXL model [61] as its text encoder to extract text-conditional embeddings. For further architectural details, we refer the reader to [72]. For the pose generation module, we employ the same Asymmetric Diffusion Transformer architecture as Mochi but with more efficient configuration. Specifically, we set the hidden size to 256, the patch size to 2, the number of attention heads to 4, and the input size of the ray embedding to 10 16. While the video generation model consists of 48 transformer blocks, the pose model is designed with 16 blocks for computational efficiency. Both models share the same text encoder. The pose model is trained from scratch. The communication block is placed at every three blocks in the video model and at every single block in the pose model, allowing periodic information exchange between the two streams. Communication is omitted in the final layer to maintain independent final representations. To prevent significant changes in the initial output of the video model, weights and biases of the linear layer in the communication block are initialized to zero. Ray to camera parameters. To recover camera parameters from generated rays, we follow slightly optimized version of the RayDiffusion approach [93], which is utilized in SplatFlow [21]. In summary, the camera center is estimated by minimizing the mismatch between rays. Next, the projection matrix is computed using least-squares approach and decomposed into the intrinsic matrix and rotation. Finally, optimization with the Adam optimizer [32] is applied to refine the intrinsic and rotation matrices, enforcing shared intrinsic parameters across all views. Then, we slightly refine camera poses to be consistent with source views. A.2. Details of Gaussian Splat Decoder The Gaussian Splat Decoder follows 3D-CNN architecture, adopted from the decoder of Mochi [72]. To enhance global context modeling, we add two attention layers to each residual block in the lowest layer and do not use causal 3D convolutions [88]. Additionally, we incorporate Plucker ray embeddings as inputs to the decoder. Specifically, the embeddings follow the format of LGM [70], forming 9channel representation. These embeddings are transformed to match the resolution of the intermediate representations in each decoder block and are injected via additional 3D convolution layers. The Gaussian Splat Decoder outputs depth, opacity, RGB, rotation, and scale, forming an 11-channel output. To accommodate this, we introduce an additional 1 1 convolution layer in the output layer. For 3DGS rendering, we utilize Gsplat [84] library, which offers the efficient implementation. A.3. Details of Camera Conditioned Generation Classifier-Free Guidance details. To implement cameraconditioned generation, we follow the formulation presented in [3]. Since our generation task involves multi-view generation conditioned on both text prompt and camera trajectory, we decompose these two conditions within the Classifier-Free Guidance (CFG) framework like HarmonyView [80]: (cid:104) (1 + sc)uθ(ItI , RtR , c, tI, tR) scuθ(ItI , RtR , cnull, tI, tR) + (1 + sR)uθ(ItI , R0.05, c, tI, 0.05) sRuθ(ItI , R1, c, tI, 1) (cid:105) /2, (6) where sR and sc represent the guidance strengths for the camera pose and text conditions, respectively, and cnull denotes the null text condition used in CFG. Also, following DiffusionForcing [8], we slightly noise the camera pose condition to the better conditional mechanism as tR = 0.05. B. Further Details of Experimental Setups Training dataset. We use multiple datasets for training, each containing multi-view images with camera annotations. The MVImgNet [90] dataset originally consisted of 219,188 scenes with camera parameters. After filtering out erroneous scenes, we retained approximately 200K scenes. For validation, 1.25K scenes were allocated for each specific task. The DL3DV [43] dataset, which initially contained 10K scenes, was similarly processed, with 300 sequences designated for the validation set. Additionally, we incorporate the RealEstate10K [99] and ACID [45] datasets in our training pipeline. However, during downloading and preprocessing, approximately 20% of the total data was lost due to filtering and quality control steps. These datasets collectively provide diverse set of multi-view scenarios: MVImgNet consists of object-centric video, capturing various objects within controlled environments. 13 DL3DV primarily contains outdoor scenes, featuring complex natural landscapes and diverse conditions. ACID focuses on aerial scenes, providing wide range of viewpoints from aerial. RealEstate10K comprises indoor scenes, primarily focused around residential rooms and houses. To generate text captions, we extract multiple captions per sequence for the DL3DV, RealEstate10K, and ACID datasets, enabling fine-grained descriptions for each sequence. Each sequence is divided into groups of 32 images, and captions are generated using the InternVL2.5-26B model [13]. In contrast, for the MVImgNet dataset, we generate single caption per scene to provide concise summary of the overall content. Since MVImgNet is an object-centric dataset, the main object remains consistent across all images within sequence, making single caption sufficient for describing the entire scene. To ensure that the captions accurately capture the main object while remaining concise, we randomly select one of the following three prompts: Briefly describe the main object in the image, including its color and key features, in single concise sentence. Describe the main object and its surroundings in 15 words or fewer, using keywords or short phrase. Summarize the main objects color, texture, and shape in no more than 15 words, using concise phrase. Training Details. We train the joint pose-video model with batch size of 16 for 120K iterations. We use an initial learning rate of 5 105 with cosine decay schedule and warm-up period of 1000 steps. Training is implemented using Fully Sharded Data Parallel (FSDP) for memory efficiency, and we optimize the model using the Adam optimizer. For timestep sampling, we uniformly sample both pose and multi-view timesteps. To further improve training efficiency, text embeddings are precomputed and stored prior to training. For training the Gaussian Splat decoder, we use batch size of 8 for 400K iterations with learning rate of 5 105. During the first 300K iterations, we render 13 target views for training, and in the final 100K iterations, we increase the number of target views to 19. Inference and Evaluation. For inference, we follow the default timestep schedule used in Mochi. Unless otherwise specified, we use 64 sampling steps. Additionally, as shown in the ablation study, we set δ = 0.2 as the default. Metrics computation and evaluation splits are configured following the setup of SplatFlow [21]. Additionally, during sampling, one possible approach for modified Classifier-Free Guidance (CFG) is to continuously sample random poses from (0, I). However, we found that pre-sampling the poses and keeping them fixed during generation leads to improved stability. 14 rainbow over waterfall pair of lovebirds in golden cage Figure 8. Generated multi-view images from image-first asynchronous sampling. Accelerating the denoising of multi-view images, instead of the pose modality, leads to severe degeneration. C. Additional Results C.1. Image-First Asynchronous Sampling We primarily accelerate the denoising of the pose modality to reduce mutual ambiguity during the sampling process. However, the effectiveness of asynchronous sampling for faster multi-view images remains an open question. To explore this, we conduct an experiment where images denoising is performed at an accelerated rate with δ = 0.2. We illustrate the generated multi-view images from such image-first asynchronous sampling in Fig. 8. As shown in the figure, accelerating the denoising of multi-view images, rather than the pose modality, results in significant failure in generation. This observation suggests that, unlike the pose modality, the image modality is not as robust to accelerated denoising, leading to severe artifacts. C.2. Additional Qualitative Comparison We present additional qualitative comparison results of textto-3DGS in Fig. 9. Consistent with Fig. 5, our VideoRFSplat can generate more realistic and detailed scenes than baselines without relying on SDS refinements. C.3. Additional Qualitative Results We illustrate additional qualitative results of VideoRFSplat in Fig. 10, 11, and 12. As shown in the results, our VideoRFSplat can generate high quality 3DGS aligned with text prompts. C.4. Results on Camera Conditioned Generation Due to the limited space, we hereby supplement qualitative results on camera conditioned generation. Figure 13 shows that VideoRFSplat accurately generates images following camera trajectories while aligned with text prompts. hot air balloon in clear sky. pair of neon running shoes waiting by the treadmill. vintage pocket watch with golden chain. wall with two movie posters and lighted sign that reads 4. .. classical architectural structure with two prominent female statues on either side.. Original w/ SDS++ Original w/ SDS++ Text Director3D [37] SplatFlow [21] Original Ours Figure 9. Additional qualitative comparison with Director3D [37] and SplatFlow [21]. Our VideoRFSplat generates more realistic scenes compared to baselines without relying on SDS++ [37]. 15 Figure 10. Additional qualitative results.We present eight rendered scenes along with their corresponding camera poses from text prompts, with image border colors indicating the respective cameras. 16 Figure 11. Additional qualitative results.We present eight rendered scenes along with their corresponding camera poses from text prompts, with image border colors indicating the respective cameras. 17 Figure 12. Additional qualitative results.We present eight rendered scenes along with their corresponding camera poses from text prompts, with image border colors indicating the respective cameras. Figure 13. Qualitative results on camera-conditioned generation. We present generated multi-view images from given text and camera trajectory conditions. VideoRFSplat can perform camera-conditioned generation."
        }
    ],
    "affiliations": [
        "EverEx",
        "KAIST",
        "Yonsei University"
    ]
}