{
    "paper_title": "VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks",
    "authors": [
        "Yu Feng",
        "Nathaniel Weir",
        "Kaj Bostrom",
        "Sam Bayless",
        "Darion Cassel",
        "Sapana Chaudhary",
        "Benjamin Kiesl-Reiter",
        "Huzefa Rangwala"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but they cannot reliably verify their own logic. Even when they reach correct answers, the underlying reasoning may be flawed, undermining trust in high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a neuro-symbolic method that extracts and verifies formal logical arguments from CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order logic and identifies premises that ground the argument in source context, commonsense knowledge, or prior reasoning steps. The symbolic representation enables automated solvers to verify logical validity while the NL premises allow humans and systems to identify ungrounded or fallacious reasoning steps. Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT effectively identifies flawed reasoning, and serves as a strong predictor of final answer correctness. We also leverage VeriCoT's verification signal for (1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct preference optimization (DPO) using verification-based pairwise rewards, further improving reasoning validity and accuracy."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 6 4 0 . 1 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "VERICOT: NEURO-SYMBOLIC CHAIN-OF-THOUGHT VALIDATION VIA LOGICAL CONSISTENCY CHECKS , Nathaniel Weir, Kaj Bostrom, Sam Bayless, Yu Feng Darion Cassel, Sapana Chaudhary, Benjamin Kiesl-Reiter, Huzefa Rangwala University of Pennsylvania Amazon Web Services"
        },
        {
            "title": "ABSTRACT",
            "content": "LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but they cannot reliably verify their own logic. Even when they reach correct answers, the underlying reasoning may be flawed, undermining trust in high-stakes scenarios. To mitigate this issue, we introduce VERICOT, neuro-symbolic method that extracts and verifies formal logical arguments from CoT reasoning. VERICOT formalizes each CoT reasoning step into first-order logic and identifies premises that ground the argument in source context, commonsense knowledge, or prior reasoning steps. The symbolic representation enables automated solvers to verify logical validity while the NL premises allow humans and systems to identify ungrounded or fallacious reasoning steps. Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VERICOT effectively identifies flawed reasoning, and serves as strong predictor of final answer correctness. We also leverage VERICOTs verification signal for (1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on VERICOT-distilled datasets and (3) preference fine-tuning (PFT) with direct preference optimization (DPO) using verification-based pairwise rewards, further improving reasoning validity and accuracy."
        },
        {
            "title": "INTRODUCTION",
            "content": "Chain-of-Thought (CoT) (Wei et al., 2022) in natural language (NL) has emerged as powerful technique for improving large language model (LLM) performance on tasks that require reasoning. LLMs like DeepSeek-R1 (DeepSeek AI, 2025) and OpenAIs o1 (OpenAI, 2024b) demonstrate strong reasoning capabilities using CoT. However, these models still frequently make logical errors in their reasoning chains, even when the final answer is correct (OpenAI, 2024a). For example, an LLM might generate the question answering (QA) CoT depicted in Figure 1, which leads to correct answer, but an intermediate step might be false (e.g., Charlie is at most 15 instead of at most 18). When LLMs generate logically invalid CoT steps, it hinders their trust and usefulness, regardless of whether the final answers are correct. This issue is particularly critical in high-stakes domains such as biomedical or legal reasoning, where users consider the accuracy of the reasoning path as important as that of the final answer. This behavior can be attributed in part to an inherent limitation of LMs: they predict text without an explicit mechanism for verifying the logical validity of the resulting semantics (Bender & Koller, 2020; Ji et al., 2023) Prior work has attempted to mitigate this through self-refinement of the models output based on diverse feedback sources, such as dynamic retrieval of facts from external knowledge bases (Peng et al., 2023), separate critic model (Paul et al., 2024), dynamic use of program execution (Chen et al., 2024; Olausson et al., 2024), or symbolic checking (Olausson et al., 2023; Quan et al., 2024a; Lalwani et al., 2025; Ye et al., 2023; Ling et al., 2023; Pan et al., 2023). However, these approaches do not ensure the logical validity of the entirety of an LLMs output. The recently introduced Explanation-Refiner (Quan et al., 2024b) makes progress towards this goal by performing iterative autoformalization and refinement of NL explanations elicited from an LLM, guided by theorem prover, for NLI task. However, there is yet to be verification method that simultaneously (1) applies over LLM Chain-of-Thought steps, (2) formalizes the context grounding of every reasoning step, and (3) improves LLM reasoner capabilities with respect to logical validity in domains other than code/math. To address these limitations, we introduce VERICOT: Symbolic Verification for Work done during internship at AWS"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: VERICOT verification of Chain-of-Thought for the SARA dataset (Holzenberger et al., 2020). Even if the final answer is correct, CoT that contains an invalid step hurts user trust and raises questions of LLM faithfulness. As shown in 2.1, VERICOT autoformalizes each step of the CoT into symbolic logic, producing formula that ensures each one follows logically from distilled list of NL premises, each of which it annotates with their source type (e.g. Commonsense or Context) If the CoT cannot be represented this way, it is unverifiable. Chain-of-Thought, neuro-symbolic algorithm that grounds and validates the logical consistency of Chain-of-Thought reasoning, identifying logical errors and making implicit premises explicit. VERICOT maintains growing set of first-order-logic (FOL) premises inferred from NL context. It autoformalizes each CoT step into first-order-logic formula, and uses constraint solver to check whether it is logically entailed by the premises and previously formalized steps. If the step isnt entailed, VERICOT either autoformalizes sufficient set of supporting premises from available context (question text, documents, or common sense) or identifies reason why the step cannot be validated (e.g., because premise could not be inferred from context, or because the step contradicts the inferred premises). Through this process, VERICOT provides multi-faceted feedback. It identifies whether CoT can be represented in formal logic, how the CoTs steps are logically supported, and what underlying NL premises need to be accepted in order to accept the CoTs reasoning. To our knowledge, this is the first neuro-symbolic validator of CoT traces in non-math/code domains. Our evaluation demonstrates that VERICOT can detect ungrounded or incorrect reasoning, and VERICOT-validation is strong predictor of final answer correctness, with validated CoT attaining higher precision than task-level accuracy (3.3). Building on this verification capability, we leverage VERICOT to actively enhance an LLMs reasoning (3.4). First, we use VERICOT for inference-time self-reflection, where its validity-oriented feedback prompts the model to self-correct, yielding an average 46% relative improvement in CoT verification pass rate and consistent 41% relative gains in producing accurate and verifiable task outcomes across multiple datasets. Second, beyond inferencetime correction, we build upon recent work using formal reasoning for RL verification (Leang et al., 2025) and use VERICOT to create high-fidelity dataset of verified CoTs for Supervised Fine-tuning (SFT) and as source of pairwise reward signals for preference fine-tuning with direct preference optimization (DPO). These fine-tuning strategies improve the models ability to generate logically consistent CoTs by 18% (relative) while matching or exceeding base task accuracy in less formal domains like biomedical and legal reasoning, since VERICOT can supply stronger supervision signals especially when task labels are unavailable."
        },
        {
            "title": "2 NEURO-SYMBOLIC COT VERIFICATION ALGORITHM",
            "content": "Our goal is to validate the correctness of Chain-of-Thought reasoning by ensuring that individual reasoning steps can be translated into consistent first-order-logic formulas. We assume we are given natural language context (which includes question, an optional conversation history, and source document with information relevant to the question) and CoT consisting of NL steps C1, . . . , Cn. As described in Alg. 1, VERICOT autoformalizes each step Ci into first-order logic formula Fi, then uses constraint solver to determine whether Fi can be logically derived from our current"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1: VERICOT Overview 1. Initialize: Set F0 = , P0 = , errors = . 2. For each CoT step Ci (i = 1, . . . , n): (a) Autoformalization (2.2): Formalize step Ci into logic formula Fi . If Ci cannot be adequately expressed in our supported logic fragment or syntax errors persisted after multiple attempts: errors.add(i, untranslatable). Continue on to next CoT step Ci+1. (b) Consistency check: If the formalized Fi contradicts our established knowledge (Fi1 = Fi): errors.add(i, contradiction). Continue on to next CoT step Ci+1. (c) Entailment check: If Fi is logically implied by our established knowledge (Fi1 = Fi): set Fi = Fi1 {Fi} and Pi = Pi1 and continue on to next step Ci+1. Otherwise, try to generate supporting premises: (d) Premise generation (2.3): Generate premise Pi to support Fi.1 Check if Pi is consistent with established statements (Fi1 = Pi) Optional: Use LLM-as-judge evaluation (2.4) to check that Pi is attributable to source context. If Pi still fails to helps entail Fi (Fi1 {Pi} Fi): errors.add(i, ungrounded). set Pi = Pi1 {Pi} and Fi = Fi1 {Pi} {Fi} and continue on to next step Ci+1. 3. Return: Sets Pn, Fn, errors. knowledge (Fi1) or requires additional premises. We distinguish the following logical relationships between new statement and our established knowledge: Fi is entailed by Fi1, meaning it necessarily follows from what we already know (Fi1 = Fi), Fi is contradicted by Fi1, meaning it is inconsistent with what we know (Fi1 = Fi), Fi is consistent with, but not entailed by, what we already know (Fi1 Fi, Fi1 Fi). VERICOT identifies CoT as valid if it can infer consistent set of logical premises from the NL context that are sufficient to entail every step of the CoT. If valid, VERICOT returns: 1. self-consistent set of FOL premises P, where each premise represents the formalization of an NL source (either from the provided context, or from common sense). 2. For each NL step Ci, FOL formula Fi (the formalization of Ci) such that Fi. If invalid, VERICOT identifies error reasons for all steps that were not entailed. These reasons are used as feedback signal for inference-time self-reflection or fine-tuning data distillation (3.4): 1. Ungrounded: VERICOT could not identify sufficient, non-contradictory set of strengthening premises from context that would entail Fi (e.g., Fi1 {Pi} Fi). 2. Contradiction: Existing statements contradict Fi (e.g., Fi1 Fi). 3. Untranslatable: Ci could not be translated into the FOL subset supported by VERICOT, or syntax errors persisted after multiple attempts. Logic formulas are encoded in SMT-LIB (Barrett et al., 2016), which supports fragment of first-order logic with theories including linear arithmetic, uninterpreted functions, and quantifiers. We use the SMT solver Z3 (de Moura & Bjørner, 2008) to perform the logical consistency and entailment checks described above. To build intuition, we first walk through simplified example that demonstrates our approach on the CoT depicted in Figure 1."
        },
        {
            "title": "2.1 EXAMPLE OF COT VERIFICATION",
            "content": "Step 1: Infer premises from the users question. For the first step, Charlie was born in 2005 and lives with his parent Bob, VERICOT autoformalizes it as follows (see 2.2): F1 := birthYear (charlie) = 2005 LivesWith(charlie, bob) Parent(bob, charlie) 1Pi can be conjunction of multiple statements that are then evaluated individually by LLM-as-judge (2.4)."
        },
        {
            "title": "Preprint",
            "content": "At this initial point, the set F0 of previously established statements is empty, so F0 F1. Since the current steps statement cannot be derived from F0, we check if supporting premises can be inferred from the provided context. Using an LLM, VERICOT is able to derive supporting premise P1 from the question (in this case, P1 happens to be identical to F1). We add P1 to our premise set P1, confirm (via constraint solver) that P1 F0 F1, and finally, add both (identical) statements to F1. Step 2: Infer premises from common sense. For the second step, Charlie is at most 18 years old in 2023, VERICOT autoformalizes it as: F2 := age(charlie, 2023) 18 This statement cannot be derived from F1, which contains only information about Charlies birth year and living situation, but no direct facts about his age. To bridge this gap, our premise generation approach (see 2.3) identifies commonsense assertion relating age to birth year: Someones age is at most the current year minus their birth year, autoformalized as: P2 := x, y. age(x, y) birthYear (x) We can derive age(charlie, 2023) 18 from the combination of F1 and this new premise, so we obtain P2 by adding P2 to P1, and we set F2 = F1 {P2, F2}. Note that we reused the existing constant charlie and the predicate birthYear from our established vocabulary, introducing only the new predicate age as needed. Also note that if the step had made an incorrect claim like Charlie is at most 15, our verification would report the step as inconsistent because P2 contradicts it. Step 3: Infer premises from source document. For the third step, child who is 18 or younger and lives with parent qualifies, we formalize it as: F3 := x. (age(x, 2023) 18 y. LivesWith(x, y) Parent(y, x)) Qualifies(x) This universal statement cannot be derived from F2. However, the source document contains stronger statement, which VERICOT autoformalizes as new premise, child qualifies for benefits if they are under 21 and live with their parent, we formalize it as: P3 := x. (age(x, 2023) < 21 y. LivesWith(x, y) Parent(y, x)) Qualifies(x) With P3 strengthened by adding P3 to P2, we can derive P3 F2 = F2, and so we update F3 = F2 {P3, F3} accordingly. Note that if this rule were not present in the source document or context, our verification would report the CoT step as ungrounded. Step 4: Conclusion. The last step, Therefore, Charlie qualifies for benefits in 2023, formalizes as: F4 := Qualifies(charlie) This conclusion can now be derived from F3 without requiring additional premises: we know Charlie is at most 18 (hence under 21), lives with his parent Bob, and the qualification rule applies to all children meeting these criteria. Had the step wrongly said Charlie does not qualify for benefits in 2023, we would have identified it as inconsistent. Below we describe each modular component of VERICOT in more detail."
        },
        {
            "title": "2.2 AUTOFORMALIZATION",
            "content": "Our autoformalization approach works in two stages, both using LLMs to translate NL into SMT-LIB. When attempting to translate CoT step Ci into its logical representation Fi, the first stage uses an LLM prompt that includes the previously produced logical vocabulary as context. The LLM is prompted to generate structured, intermediate representation that combines SMT-LIB with metadata about which text in Ci corresponds to which parts of the resulting Fi, using only the variables and types already present in the vocabulary. The second stage extends the vocabulary if there are segments of the input text that the LLM deems relevant to the CoTs logical argument but cannot express with the existing vocabulary. In this stage, we use another LLM prompt to generate new SMT-LIB declarations (e.g., declare-fun, declare-sort). The new declarations are added to the vocabulary, and the first stage is attempted again. In our implementation, we allow this to repeat up to three times before giving up (in which case the current step is marked as untranslatable, or the current premise is discarded)."
        },
        {
            "title": "Preprint",
            "content": "In the following example, when VERICOT initially attempts to translate C2 (Charlie is at most 18 years old in 2023) based on the available vocabulary, it first reports that the vocabulary is insufficient, then extends it accordingly to translate C2 into F2: Context: Charlie was born in 2005 and lives with Bob. Document: child qualifies for benefits if they are under 21 and live with their parent. Question: Does Charlie qualify for benefits in 2023? Initial (Failing) LLM Translation Output F2: ; current year 2023 -- UNTRANSLATABLE (assert false) ; charlie age <18 -- UNTRANSLATABLE (assert false) CoT Steps: C1: Charlie was born in 2005 and lives with parent Bob C2: Charlie is at most 18 years old in 2023 Previously Produced Vocabulary of F1: ; represents person (declare-sort Person) ; specific person Charlie (declare-const charlie Person) ; specific person Bob (declare-const bob Person) ; birth year of person (declare-fun birth_year (Person) Int) ; whether person lives with another person (declare-fun lives_with (Person Person) Bool) New Declarations Added to Vocabulary: ; current year for calculation (declare-const current_year Int) ; age of person in given year (declare-fun age_in_year (Person Int) Int) Successful LLM Translation Output F2: ; current year 2023 (= current_year 2023) ; charlie age <18 (<= (age_in_year charlie current_year) 18) Successful autoformalization allows us to represent the step as F2, but not necessarily ensure that it is entailed by the growing logical formula, which is the focus of 2.3."
        },
        {
            "title": "2.3 PREMISE GENERATION",
            "content": "Chain-of-Thought steps arent always directly implied by existing statements. Instead, they might rely on information from the context, like problem details (Charlie is Bobs son) or support documents (a child qualifies for benefits if they are under 21), or from common sense (a father is parent). When CoT steps formula Fi is neither entailed nor contradicted by existing statements, VERICOT prompts an LLM to identify supporting premises from the context or commonsense. We attempt to build premise formula Pi that is sufficient to entail Fi as follows: We first generate multiple noncontradictory candidate NL premises and translate each of them into logic formula using the autoformalization process from 2.2. We perform round of premise regeneration if the natural language meaning of any new declarations from this step is not captured in the generated premises. For each candidate premise p, we then check whether it is consistent with the established statements (i.e., whether Fi1 is satisfiable) and keep only those that are. Finally, we conjoin all remaining candidate premises to form the final premise formula Pi."
        },
        {
            "title": "2.4 LLM-AS-JUDGE PREMISE EVALUATION",
            "content": "VERICOT ensures that the CoT is FOL-representable and presents list of inferred premises as one necessary basis for accepting the CoTs logic. However, it has not verified that whether the premises themselves might be accepted. It may have enumerated the premise the sky is purple as necessary for accepting the CoT, but this is generally not an acceptable statement on its own. To provide increased assurance that the inferred premises are reliable, VERICOT uses an LLM-as-Judge approach (LLMaj) to identify spurious premises after the premise generation process is complete. While the LLM generating premises in Alg. 1 can produce confabulations or omit relevant details, using LLM-as-Judge reduces the likelihood that these errors go undetected. We evaluate all premises using the LLM-as-Judge approach. For premises inferred from source text, we provide judge LLMs with both the source document and the NL version of the premise, prompting them to evaluate whether the premise is attributable to the source text. For premises inferred from common sense, we prompt judge LLMs to evaluate whether the premise is acceptable given the provided context and targeted reasoning step (omitting the attribution requirement)."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "Evaluation Models. We use Claude-3.5-Sonnet-V2 (Anthropic, 2024) through API calls as the executor of the proposed VERICOT. We fine-tune Qwen2.5-7b-Instruct (Qwen Team, 2024)."
        },
        {
            "title": "ProofWriter",
            "content": "ER DSB VERICOT-NoPrem VERICOT 14.8 10.0 3.3 45.2 83.3 96.1 100 94."
        },
        {
            "title": "BioASQ",
            "content": "ER DSB VERICOT-NoPrem VERICOT 1.5 5.9 2.9 25.3 80.0 72.2 57.1 84.3 Legalbench-SARA ER DSB VERICOT-NoPrem VERICOT 6.8 4.8 0.6 15. 92.0 94.1 50.0 87.0 12.3 9.5 3.3 42.5 1.2 4.2 1.6 21.3 6.3 4.5 0.3 13.2 75.8 74.8 75.8 75.8 81.4 75.7 81.4 81. 80.0 77.7 80.0 80.0 Table 1: Verification results across benchmarks. For each dataset, we report verification pass rate (%), verifier precision (%), verified correct answer rate (VCAR, %), and task accuracy (%, i.e., correct answer rate). VERICOT consistently achieves the highest values across the main outcome metrics (pass rate and VCAR)."
        },
        {
            "title": "Score",
            "content": "Legalbench-SARA"
        },
        {
            "title": "Grounded contextual premise\nAcceptable commonsense premise\nNecessary commonsense premise",
            "content": "93.5 83.9 77.0 87.3 93.0 81.1 96.4 90.5 77.4 Table 2: Extended analysis of LLMas-Judge premise evaluation breakdown. The generated premises of all types are highly acceptable under LLMaj."
        },
        {
            "title": "3.1 DATASETS",
            "content": "We consider three datasets. ProofWriter (Tafjord et al., 2021) contains small rulebases of facts and rules. Each rulebase has set of questions which can either be proven true or false using proofs of various depths. LegalBench (Guha et al., 2023) is benchmark designed to evaluate the legal reasoning capabilities of LLMs crowdsourced from legal professional; we use the SARA (Holzenberger et al., 2020) subset from LegalBench, which evaluates statutory reasoning in tax law. BioASQ (Nentidis et al., 2023; 2024) is series of annual open challenges focused on advancing biomedical semantic indexing and QA, specifically we use task b, which is biomedical QA with PubMed abstracts as context. Appendix Table 5 shows data statistics."
        },
        {
            "title": "3.2 BASELINES",
            "content": "We compare VERICOT against three baselines using Claude-3.5-Sonnet-V2. Explanation-Refiner (ER) (Quan et al., 2024b): an iterative auto-formalization and refinement approach guided by theorem prover, originally proposed for NLI tasks. In our adaptation, we treat the CoT steps as the explanation, the question as the premise, and the model-generated answer as the hypothesis. Direct SMT Baseline (DSB): direct formalization approach that decomposes reasoning steps with designated types and generates SMT-LIB expressions simultaneously via few-shot prompting. For reasoning steps based on document, background, or commonsense premises, only consistency check is applied; for inference steps, entailment check is performed. VERICOT without Explicit Premise Generation (VERICOT-NoPrem): variant of our VERICOT pipeline with generating intermediate premises turned off. Verification is performed using the same check protocol as in the Direct SMT Baseline."
        },
        {
            "title": "3.3 VERICOT AS AN EFFECTIVE VERIFICATION ALGORITHM",
            "content": "We first evaluate VERICOT as verification framework for CoTs, demonstrating that it achieves high verification performance by capturing larger proportion of verifiable reasoning with strong precision and introduces an innovative mechanism for analyzing the premise grounding of reasoning steps. Direct evaluation of VERICOT. We evaluate verification methods using four complementary metrics: verification pass rate, verifier precision, verified correct answer rate (VCAR), and overall task accuracy. Pass rate measures how often CoTs are deemed verifiable. Precision captures the proportion of correct answers among verified CoTs, directly reflecting the reliability of VERICOT decisions. VCAR combines these two by quantifying the overall fraction of CoTs that are both verified and correct. Finally, task accuracy corresponds to the final task-level correctness rate. As"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Proportional distribution of outcome scenarios before and after self-reflection (3.4) under VERICOT. Categories include successful verification (Valid) and failure cases: Contradiction, Ungrounded, Untranslatable as described in 2. Self-reflection significantly reduce errors. shown in Table 1, VERICOT achieves the highest pass rates across all benchmarks, leading to the strongest verified correct answer rate. Importantly, its precision is consistently high, exceeding the final task accuracy, demonstrating that VERICOT-validated CoTs provide more reliable signal of correctness than the raw CoTs. These results highlight VERICOT as an effective and accurate verification algorithm that achieves broader coverage with better reliability. Detailed analysis of instances that fail the VERICOT verification. We first conduct quantitative analysis by presenting the proportional distribution of each possible outcome scenario under VERICOT before and after self-reflection(introduced below in 3.4). Ungrounded error is the most prominent, highlighting that CoT often overgenerates assumptions, producing reasoning steps that appear plausible but unsupported. Self-reflection significantly increases the proportion of valid outcomes while reducing ungrounded and contradiction errors. The rate of translation-related errors remains largely unchanged. Additionally, we provide three illustrative examples in A.2, each corresponding to one type of failure case. For each example, we present the updated CoT along with its successful verification after self-reflection, as detailed in 3.4. Quantitative analysis of NL premise quality. We present the results of VERICOT premise evaluation as described in 2.4. For commonsense premises, we additionally evaluate whether the premise is necessary to support the corresponding reasoning step. The results in Table 2 confirm the high quality of premises identified by VERICOT. key strength of VERICOT is reliably locating reasoning steps in context or commonsense, making implicit premises explicit. For users to evaluate grounding, the challenge shifts from locating supporting premises to evaluating the quality of the premises that VERICOT has already identified as shown in Appendix Tables 3, 4 and 5."
        },
        {
            "title": "3.4 APPLICATIONS OF VERICOT’S VERIFICATION SIGNALS",
            "content": "The verification signals produced by VERICOT provide structured feedback that improves CoT reasoning along three dimensions: user transparency, inference-time self-reflection, and fine-tuning. First, by auto-formalizing CoTs and making premises explicit, VERICOT increases transparency, enabling users to directly and more effectively inspect the reasoning chain. Second, VERICOT supports self-reflection at inference time by exposing granular errors in faulty CoTs, thereby providing actionable signals that help correct CoT. Third, the same structured signals can be incorporated into supervised fine-tuning and direct preference optimization (DPO) (Rafailov et al., 2024), guiding models toward producing more verifiable CoTs and ultimately achieving higher task accuracy. Inference-time Self-reflection. We first evaluate whether VERICOT feedback can be used to guide models in producing better CoTs at inference time. Our process is as follows: if CoT fails to pass the verification check, we prompt the model to self-correct its reasoning. We adopt two variants(Base, LLMaJ): Base uses only verifier signals without LLMaJ premise evaluation, while VERICOT-w LLMaJ leverages the full set of signals for self-reflection. Given all relevant information for every step, including the original reasoning steps, any added premises, the corresponding formalizations, errors, check results (with execution results and the models parameter assignment), and optional LLMaJ premise evaluation, the model is prompted to revise its CoT reasoning during inference. This generates an updated CoT, which is then re-evaluated. Table 3 reports verification pass rates, verifier precision, verified correct answer rates (VCAR), and final task accuracy after self-reflection, together with the corresponding improvements relative to"
        },
        {
            "title": "Method",
            "content": "Pass Rate PR Precision Precision VCAR VCAR Task Accuracy ER DSB VERICOT-NoPrem VERICOT-Base VERICOT-w LLMaj ER DSB VERICOT-NoPrem VERICOT-Base VERICOT-w LLMaj ER DSB VERICOT-NoPrem VERICOT-Base VERICOT-w LLMaj 21.5 12.3 30.7 60.1 60.6 3.8 8.5 29.7 33.4 36. 19.6 9.0 16.1 25.2 23."
        },
        {
            "title": "ProofWriter",
            "content": "80.2 90.6 88.8 90.1 90."
        },
        {
            "title": "BioASQ",
            "content": "84.6 76.9 81.6 86.3 83.8 -3.1 -5.5 -11.2 -4.0 -3.7 +4.6 +4.7 +24.5 +2.0 -0.5 Legalbench-SARA 91.6 93.8 84.2 85.5 85.5 -0.4 -0.3 +34.2 -1.5 -1. +6.7 +2.3 +27.4 +14.9 +15.4 +2.3 +2.6 +26.8 +8.1 +11.6 +12.8 +5.2 +15.5 +10.0 +7.8 17.2 11.1 27.3 54.1 54.8 3.2 6.5 24.2 28.8 30.9 18.0 8.4 13.6 21.6 19. +4.9 +1.6 +24.0 +11.6 +12.3 +2.0 +2.3 +22.6 +7.5 +9.6 +11.7 +3.9 +13.3 +8.4 +6.5 75.8 73.2 72.4 74.1 73.7 81.4 78.8 79.8 82.0 80.5 80.0 80.2 80.3 81.4 81. Table 3: Verification results after self-reflection. In addition to the metrics in Table 1, we report absolute changes compared to pre-reflection (). VERICOT-Base uses only verifier signals without LLMaj evaluation, while VERICOT-w LLMaj leverages the full set of signals for self-reflection. VERICOT(Base,w LLMaj) achieves the strongest improvements (VCAR and accuracy) across all benchmarks while maintaining precision at similar or higher level compared to baselines."
        },
        {
            "title": "Precision VCAR Task Accuracy",
            "content": "Qwen2.5-7B-Instruct (direct) + SFT Random Distilled CoTs + SFT Verified CoTs + SFT Verified CoTs + DPO"
        },
        {
            "title": "BioASQ",
            "content": "22.8 22.9 22.9 26."
        },
        {
            "title": "ProofWriter",
            "content": "Qwen2.5-7B-Instruct (direct) + SFT Random Distilled CoTs + SFT Verified CoTs + SFT Verified CoTs + DPO 21.8 24.6 23.2 27.8 83.1 85.7 85.5 83.5 76.7 71.4 81.5 82.8 18.9 19.6 19.6 22.3 16.7 17.6 18.9 23. 77.4 78.5 79.7 79.4 47.5 47.3 51.1 51.8 Table 4: SFT and DPO results on BioASQ and ProofWriter. We report pass rate (%), verifier precision (%), verified correct answer rate (VCAR, %), and task accuracy (%). Strategies include the base model (direct), fine-tuning with random distilled CoTs, verified CoTs, and DPO with verified CoTs. pre-reflection values (PR, Precision, and VCAR). Across all benchmarks, VERICOT with selfreflection yields consistent gains in both coverage ( average +12.3%(absolute)/ +46.4% (relative)) and verified correct answer rate ( average +9.5%(absolute)/ +41.1% (relative)), indicating that refinement encourages the generation of reasoning that is more frequently verifiable and leads to correct answers. Notably, VERICOT achieves the highest absolute pass rates and verified correct answer rate, while maintaining similar level of precision and slightly better final task accuracy. These results highlight the effectiveness of VERICOTs verification signals in guiding self-reflection. Note that LLMaj with additional LLMaj evluation is only slightly better than Base. This is expected, as verifier results provide much more informative error signals. Supervised Fine-tuning. To harness the benefits of verification for model improvement, we build high-fidelity datasets consisting of verified, logically consistent CoT examples (pass both verifier checks and LLMaj evaluation) and apply them for supervised fine-tuning (SFT). We start from the Qwen2.5-7B-Instruct model and distill supervision signals from Claude-3.5-Sonnet-v2. Table 4 benchmarks different distillation strategies to evaluate the role of verification in data curation. Specifically, we compare (i) the baseline performance of Qwen2.5-7B-Instruct, (ii) SFT with randomly"
        },
        {
            "title": "Preprint",
            "content": "sampled distilled CoTs, (iii) SFT with distilled CoTs that pass VERICOT verification. As shown in Table 4, requiring CoTs to pass verification (ii vs. iii) yields an average 3% improvement in final accuracy. This is because the verifiers higher precision ensures that verified CoTs contain greater proportion of correct answers than randomly selected ones. This highlights VERICOTs signal as particularly valuable, as it can distill better data when gold answers are unavailable. Preference Fine-tuning. The verification signals from each reasoning step can further be utilized as pairwise reward signals during preference fine-tuning (PFT) with DPO to enable more reliable CoT generation. We randomly resample CoTs for examples that initially pass VERICOT verification, keep those that fail in the new attempt, and then construct chosen/rejected CoT pairs, where the chosen/rejected one passes/fails verification. When applying DPO on top of SFT , we observed two key improvements across all datasets as shown in Table 4: the verification pass rate increased by 4.3% (an 18.4% relative improvement), while the verified correct answer rate improved by 3.4% (a 17.7% relative gain). This demonstrates that preference fine-tuning on data generated by VERICOT is especially helpful for generating verified CoTs because the pairwise reward signal provides stronger supervision signal to distinguish correct reasoning traces from incorrect ones."
        },
        {
            "title": "4 RELATED WORK",
            "content": "Solver-based Verification of Natural Language Reasoning. The most relevant work to ours is Quan et al. (2024b), where neuro-symbolic pipeline is introduced to formalize natural-language explanations (for NLI) into logical forms using LLMs, and then performs theorem-proverbased verification and refinement of those explanations. While Quan et al. (2024b) focuses on verifying and refining explanations post hoc for NLI tasks, our framework not only translates and verifies each step of CoT into first-order logic (SMT-LIB) but also grounds the reasoning chains in context and is tailored for multi-step reasoning beyond NLI explanations. Additionally, work like Ling et al. (2023) and Vacareanu et al. (2024) uses LLMs to assess the logical consistency of each reasoning step, whereas our method adds formal solver checks, context grounding, and correction, yielding more objective verification than LLM-only pipelines. Solver-empowered Logical Reasoning in LLMs. Recent works have explored solver-assisted QA with LLMs (Pan et al., 2023; Poesia et al., 2024; Ye et al., 2023). Specifically, Pan et al. (2023) translates NL into logic and uses solvers with iterative repair; Ye et al. (2023) prompt LLMs to produce declarative constraints for SAT solving; Poesia et al. (2024) constrain LLM outputs via symbolic state tracking. Xu et al. (2024) propose symbolic CoT that incrementally generates formal reasoning steps to encourage faithfulness to input context and logical constraints without solver. We differ by combining context-grounded SMT-LIB translation with step-wise verification and using solver feedback for on-the-fly reasoning correction. The RL aspect of our paper is most related to Leang et al. (2025), who demonstrate the effectiveness of Theorem Prover-as-a-Judge feedback for learning to solve math problems. The main difference is that their approach logically grounds all statements to provided, symbolic context (Leans mathlib), limiting it to math domains, whereas our approach logically grounds statements to logical premises inferred from NL context, allowing VERICOT to be applied to arbitrary NL domains. Structured Explanation Generation for LLM Reasoning Our work relates to growing literature on surfacing the underlying premises and stepwise reasoning for task-specific conclusions. VERICOT surfaces NL premises that, when formalized, provide directed, tree-like logical basis for symbolic conclusion; this is similar to the fully NL entailment tree structure (Dalvi et al., 2021; Bostrom et al., 2021) which uses NL textual entailment as the logical glue between each layer of inferences. Versions of entailment tree-generating algorithms such as Entailer (Tafjord et al., 2022), NELLIE (Weir et al., 2024), and ADGV (Sprague et al., 2022) map from task answers to entailment trees rooted in underlying conjunctive premises. Maieutic Prompting (Jung et al., 2022) produces related structures rooted in underlying arguments for/against believing recursive inferences."
        },
        {
            "title": "5 LIMITATIONS",
            "content": "As both autoformalization and premise inference rely on LLMs, it is possible that either step can be incorrect (either by mis-translating the CoT into an unrepresentative formula, or by introducing premise that is not well-founded). formalization may be incorrect simply because the LLM used made mistake during translation, or because the text to be translated fundamentally cannot be represented in the subset of SMT-LIB that we support. As result, while VERICOT can prove that"
        },
        {
            "title": "Preprint",
            "content": "the formalization of the Chain-of-Thought necessarily follows from the inferred premises, it cannot guarantee that the NL CoT or the premises are correct."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we present neuro-symbolic framework that validates Chain-of-Thought reasoning by autoformalizing each intermediate step into first-order logic and grounding it in formal premises inferred from NL context or commonsense knowledge. Across the ProofWriter, LegalBench, and BioASQ datasets, VERICOT reliably detects ungrounded or incorrect reasoning in CoT traces, which we show is strong predictor of final answer correctness. We further show that these verification signals can guide models to self-correct during inference and through supervised fine-tuning and preference fine-tuning for better CoT reasoning."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Introducing claude 3.5 sonnet. Anthropic press release, Claude.ai, June 20 2024. Accessed 2025-08-09. Clark Barrett, Pascal Fontaine, and Cesare Tinelli. The Satisfiability Modulo Theories Library (SMT-LIB). www.SMT-LIB.org, 2016. Emily M. Bender and Alexander Koller. Climbing towards NLU: On meaning, form, and understanding in the age of data. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 51855198, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. acl-main.463. URL https://aclanthology.org/2020.acl-main.463/. Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, and Greg Durrett. Flexible generation of natural language deductions. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wentau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 62666278, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.506. URL https: //aclanthology.org/2021.emnlp-main.506/. Xinyun Chen, Maxwell Lin, Nathanael Scharli, and Denny Zhou. Teaching large language models to self-debug. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=KuPixIqPiq. Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter Clark. Explaining answers with entailment trees. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 73587370, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.585. URL https://aclanthology.org/2021. emnlp-main.585/. Leonardo de Moura and Nikolaj Bjørner. Z3: An Efficient SMT Solver. In TACAS, pp. 337340, 2008. DeepSeek AI. Deepseek-r1. https://en.wikipedia.org/wiki/DeepSeek, 2025. Accessed: 2025-08-09. Neel Guha, Julian Nyarko, Daniel E. Ho, Christopher Re, Adam Chilton, Aditya Narayana, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore, Diego Zambrano, Dmitry Talisman, Enam Hoque, Faiz Surani, Frank Fagan, Galit Sarfaty, Gregory M. Dickinson, Haggai Porat, Jason Hegland, Jessica Wu, Joe Nudell, Joel Niklaus, John Nay, Jonathan H. Choi, Kevin Tobia, Margaret Hagan, Megan Ma, Michael Livermore, Nikon Rasumov-Rahe, Nils Holzenberger, Noam Kolt, Peter Henderson, Sean Rehaag, Sharad Goel, Shang Gao, Spencer Williams, Sunny Gandhi, Tom Zur, Varun Iyer, and Zehua Li. Legalbench: collaboratively In Thirty-seventh built benchmark for measuring legal reasoning in large language models. Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=WqSPQFxFRC."
        },
        {
            "title": "Preprint",
            "content": "Nils Holzenberger, Andrew Blair-Stanek, and Benjamin Van Durme. dataset for statutory reasoning in tax law entailment and question answering, 2020. URL https://arxiv.org/abs/2005. 05257. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Comput. Surv., 55(12), March 2023. ISSN 0360-0300. doi: 10.1145/3571730. URL https://doi.org/10.1145/3571730. Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. Maieutic prompting: Logically consistent reasoning with recursive explanations. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 12661279, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. emnlp-main.82. URL https://aclanthology.org/2022.emnlp-main.82/. Abhinav Lalwani, Tasha Kim, Lovish Chopra, Christopher Hahn, Zhijing Jin, and Mrinmaya Sachan. Autoformalizing natural language to first-order logic: case study in logical fallacy detection, 2025. URL https://arxiv.org/abs/2405.02318. Joshua Ong Jun Leang, Giwon Hong, Wenda Li, and Shay Cohen. Theorem prover as judge for synthetic data generation. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2994129977, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025. acl-long.1448. URL https://aclanthology.org/2025.acl-long.1448/. Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su. Deductive verification of chain-of-thought reasoning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= I5rsM4CY2z. Anastasios Nentidis, Georgios Katsimpras, Anastasia Krithara, Salvador Lima Lopez, Eulalia FarreMaduell, Luis Gasco, Martin Krallinger, and Georgios Paliouras. Overview of BioASQ 2023: The Eleventh BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering, pp. 227250. Springer Nature Switzerland, 2023. ISBN 9783031424489. doi: 10.1007/ 978-3-031-42448-9 19. URL http://dx.doi.org/10.1007/978-3-031-42448-9_ 19. Anastasios Nentidis, Georgios Katsimpras, Anastasia Krithara, Salvador Lima-Lopez, Eul`alia FarreMaduell, Martin Krallinger, Natalia Loukachevitch, Vera Davydova, Elena Tutubalina, and Georgios Paliouras. Overview of bioasq 2024: The twelfth bioasq challenge on large-scale biomedical semantic indexing and question answering. In Lorraine Goeuriot, Philippe Mulhem, Georges Quenot, Didier Schwab, Giorgio Maria Di Nunzio, Laure Soulier, Petra Galuˇsˇcakova, Alba Garcıa Seco de Herrera, Guglielmo Faggioli, and Nicola Ferro (eds.), Experimental IR Meets Multilinguality, Multimodality, and Interaction, Cham, 2024. Springer Nature Switzerland, Springer Nature Switzerland. ISBN 978-3-031-71908-0. doi: https://doi.org/10.1007/978-3-031-71908-0 1. URL https://link.springer.com/chapter/10.1007/978-3-031-71908-0_1. Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang, Armando Solar-Lezama, Joshua Tenenbaum, and Roger Levy. LINC: neurosymbolic approach for logical reasoning by combining In Houda Bouamor, Juan Pino, and Kalika language models with first-order logic provers. Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 51535176, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.313. URL https://aclanthology.org/2023. emnlp-main.313/. Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama. Is self-repair silver bullet for code generation? In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024, 2024. URL https: //openreview.net/forum?id=y0GJXRungR."
        },
        {
            "title": "Preprint",
            "content": "OpenAI. Limitations of openai-o1-system-card/, 2024a. correct answers; Accessed: 2025-08-09. reasoning models. https://openai.com/index/ Describes common reasoning errors despite OpenAI."
        },
        {
            "title": "Openai",
            "content": "o1 system card. https://openai.com/index/ openai-o1-system-card/, 2024b. Accessed: 2025-08-09. Liangming Pan, Alon Albalak, Xinyi Wang, and William Wang. Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 38063824, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.248. URL https://aclanthology.org/2023. findings-emnlp.248/. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. REFINER: Reasoning feedback on intermediate representations. In Yvette Graham and Matthew Purver (eds.), Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 11001126, St. Julians, Malta, March 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.eacl-long. 67. URL https://aclanthology.org/2024.eacl-long.67/. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. Check your facts and try again: Improving large language models with external knowledge and automated feedback, 2023. URL https: //arxiv.org/abs/2302.12813. Gabriel Poesia, Kanishk Gandhi, Eric Zelikman, and Noah Goodman. Certified deductive reasoning with language models. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=yXnwrs2Tl6. Xin Quan, Marco Valentino, Louise Dennis, and Andre Freitas. Enhancing ethical explanations of large language models through iterative symbolic refinement. In Yvette Graham and Matthew Purver (eds.), Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 122, St. Julians, Malta, March 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.eacl-long.1. URL https://aclanthology.org/2024.eacl-long.1/. Xin Quan, Marco Valentino, Louise A. Dennis, and Andre Freitas. Verification and refinement of natural language explanations through LLM-symbolic theorem proving. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 29332958, Miami, Florida, USA, November 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.172. URL https://aclanthology.org/2024.emnlp-main.172/. Qwen Team. Qwen2.5: party of foundation models. Blog post; recommended citation for Qwen-2.5, September 2024. Includes Qwen2.5-7B and instruction-tuned variants. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. URL https://arxiv.org/abs/2305.18290. Zayne Sprague, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. Natural language deduction In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), with incomplete information. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 82308258, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.564. URL https://aclanthology.org/ 2022.emnlp-main.564/. Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. ProofWriter: Generating implications, proofs, and abductive statements over natural language. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 36213634, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/"
        },
        {
            "title": "Preprint",
            "content": "v1/2021.findings-acl.317. URL https://aclanthology.org/2021.findings-acl. 317/. Oyvind Tafjord, Bhavana Dalvi Mishra, and Peter Clark. Entailer: Answering questions with faithful and truthful chains of reasoning. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 20782093, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.134. URL https://aclanthology.org/ 2022.emnlp-main.134/. Robert Vacareanu, Anurag Pratik, Evangelia Spiliopoulou, Zheng Qi, Giovanni Paolini, Neha Anna John, Jie Ma, Yassine Benajiba, and Miguel Ballesteros. General purpose verification for chain of thought prompting, 2024. URL https://arxiv.org/abs/2405.00204. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum? id=_VjQlMeSB_J. Nathaniel Weir, Peter Clark, and Benjamin Van Durme. Nellie: neuro-symbolic inference engine for grounded, compositional, and explainable reasoning. 2024. Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, and Wynne Hsu. Faithful logical reasoning via symbolic chain-of-thought. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1332613365, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.720. URL https://aclanthology.org/2024.acl-long.720/. Xi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. SatLM: Satisfiability-aided language models using declarative prompting. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=TqW5PL1Poi."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 DATA STATISTICS"
        },
        {
            "title": "ProofWriter",
            "content": ""
        },
        {
            "title": "BioASQ",
            "content": "5,049 LegalBench (SARA) - 340 367 and labels sampled only (no CWA-Depth-5 from the OWAsubsets. entailment/contradicanswers). Unknown https://huggingface.co/ Randomly Depth-5 Contains tion Source: datasets/renma/ProofWriter. Restricted to Task 12b: Test Results of Phase B. 272 instances from SARA Entailment and 95 instances from SARA Numeric. Table 5: Dataset statistics and descriptions. A.2 EXAMPLES OF FAILURE CASES UNDER VERICOT We present three illustrative cases of different types of errors, each of which, after self-reflection, passes verification. Specifically, these include (1) an ungrounded example from ProofWriter as shown in Appendix Table 3 (refer to full details in A.2.1): the model initially is over-claiming. During self-reflection, however, it is able to identify the overclaimed portion, rephrase it appropriately, and thus generate verified CoT. Question context Question: Based on the above information, is the following statement true, false, or unknown? The mouse visits the rabbit Context (partially omitted): ... The mouse eats the tiger. The mouse is green. The rabbit does not chase the tiger. The rabbit eats the dog. ... If something visits the mouse then the mouse is big. If something eats the tiger then the tiger visits the mouse. ... If something is big then it visits the rabbit. Step-wise CoT: 1. The mouse has three direct properties: the mouse eats the tiger, the mouse is green, and there is rule stating that if something visits the mouse then the mouse is big 2. Since the mouse eats the tiger and no other eating relationships exist except this explicitly stated, and there is rule stating that if something eats the tiger then the tiger visits the mouse, we can conclude that the tiger visits the mouse 3. Since the tiger visits the mouse, and there is rule stating that if something visits the mouse then the mouse is big, we can conclude that the mouse is big 4. Since the mouse is big, and there is rule stating that if something is big then it visits the rabbit, we can conclude that the mouse visits the rabbit 5. The statement The mouse visits the rabbit is true based on the logical chain of the mouse being big and the rule about big things visiting the rabbit Output premise list Grounded in Context: 1. The mouse is green 2. The mouse eats the tiger 3. mouse is big 4. tiger visits the mouse 5. rabbit If something visits the mouse then the If something eats the tiger then the If something is big then it visits the Commonsense: 1. The mouse and tiger are different animals 2. The rabbit and tiger are different animals 3. The rabbit and mouse are different animals Output Logic (Omit less important ones) Verification result % [...] (assert (animal_eats mouse tiger)) (assert (forall ((a Animal)) (=> (animal_visits mouse) (animal_is_big mouse)))) (assert (forall ((a Animal) (b Animal)) (= (animal_eats b) (and (= mouse) (= tiger))))) (assert (=> (and (animal_eats mouse tiger) (forall ((a Animal)) (=> (animal_eats tiger) (animal_visits tiger mouse)))) (animal_visits tiger mouse))) Fail due to ungroundedness. In step 2, no other eating relationships exist except this explicitly stated with (assert (forall ((a Animal) (b Animal) ) (= (animal_eats b) (and (= mouse) (= tiger))))) can not be grounded. Figure 3: Highlights of an ungrounded example from Proofwriter before self-reflection. (2) contradiction example from BioASQ as shown in Appendix Table 4 (refer to full details in A.2.2): the reasoning contains subtle logical flaw that is difficult to detect. Our VeriCoT framework successfully identifies this issue and prompts the model to explore an alternative reasoning path during self-reflection, ultimately resulting in verified CoT. (3) an untranslatable example from LegalBench-SARA as shown in Appendix Table 5 (refer to full details in A.2.3): when the translation error is presented to the model for self-reflection, the model recognizes that the erroneous reasoning step is unnecessary for the final solution. By removing this step, the model produces verified CoT."
        },
        {
            "title": "Preprint",
            "content": "Question context Question: Can Connexin hemi channels be used for drug delivery? Context omitted Step-wise CoT: 1. Connexin hemichannels are channels in cell membranes that can allow molecules to pass through and are part of gap junction channels. 2. Connexin hemichannels allow for the non-selective passage of molecules up to 1,000 Da. 3. The gap junction channel can transport macro polymer therapeutic molecules, which is evidence for drug delivery potential. 4. Therefore, Connexin hemi channels are also permeable to therapeutic macromolecules, makes them highly attractive targets for delivering drugs. Output premise list Grounded in Context: 1. Connexin hemichannels are channels in membranes that pass molecules. 2. Connexin hemichannels are part of gap junction. 3. Connexin hemichannels allow for the non-selective passage of molecules up to 1,000 Da. 4. The ability to deliver large polymeric therapeutic macromolecules via the gap junction route provides evidence for drug delivery potential. Commonsense: 1. Any macro polymeric therapeutic molecule has molecular weight > 1, 000 Da. Output Logic (Omit less important ones) Verification result Failed due to contradiction. the axioms show In step 4, macro-polymer molecules are > 1000 Da, but connexin hemichannels permit molecules 1000 Da. Thus, the same molecule would need to be both 1000 and > 1000, which is impossible. only % [...] (assert (forall ((m Molecule) (c Channel)) (=> (and (IsConnexinHemichannel c) (CanEnterViaChannel c)) (<= (MolecularWeight m) 1000)))) (assert (forall ((m Molecule)) (=> (IsMacroPolymerTherapeutic m) (> (MolecularWeight m) 1000)))) (assert (forall ((c Channel) (m Molecule)) (=> (and (IsGapJunctionChannel c) (IsMacroPolymerTherapeutic m)) (and (CanEnterViaChannel c) (=> (CanEnterViaChannel c) (HasDrugDeliveryPotential c))) ))) (assert (exists ((c Channel) (m Molecule)) (and (IsConnexinHemichannel c) (IsMacroPolymerTherapeutic m) (CanEnterViaChannel c)))) Figure 4: Highlights of contradiction example from BioASQ before self-reflection. Question context Question: Determine whether the following statements are entailed under the statute. Reply with either: Entailment, Contradiction. Context: Alice has paid $3200 to Bob for agricultural labor done from Feb 1st, 2017 to Sep 2nd, 2017. Alice is an American employer. Statement: Section 3306(c)(1) applies to Alice employing Bob for the year 2017. CoT: 1. Section 3306(c)(1) of the statute addresses agricultural labor 2. The work performed by Bob for Alice is specifically agricultural labor 3. The basic scope of Section 3306(c)(1) covers all agricultural labor 4. While Alice is an American employer and there might be jurisdictional implications, the statute fragment does not provide information about geographical limitations 5. Bobs employment falls under the basic scope of Section 3306(c)(1) Output premise list Grounded in Context: 1. Section 3306(c)(1) applies to agricultural labor 2. Alice employed Bob to perform agrito cultural September 2nd, 2017, as evidenced by Alices payment of $3200 for this work from February 1st labor Output Logic (Omit less important ones) Verification result Fail due to untranslatability. In step 4, the phrase there might be jurisdictional implication cannot be rendered under SMI-LIB rules because it expresses possibility rather than definitive statement. % [...] (assert (and (employs alice bob) (performs_agricultural_labor bob) (paid alice bob agricultural_labor 3200 20170201 20170902))) (assert (forall ((p Person)) (=> (performs_agricultural_labor p) (covered_by_section_3306c1 p))) ) (assert )) (covered_by_section_3306c1 bob Figure 5: Highlights of an untranslatable example from LegalBench-SARA before self-reflection."
        },
        {
            "title": "Preprint",
            "content": "A.2.1 UNGROUNDED EXAMPLE FROM PROOFWRITER"
        },
        {
            "title": "Ungrounded example from ProofWriter",
            "content": "Reference information: The dog does not eat the rabbit. The mouse eats the tiger. The mouse is green. The rabbit does not chase the tiger. The rabbit eats the dog. The rabbit is young. The tiger is green. If something visits the mouse then the mouse is big. If something eats the tiger then the tiger visits the mouse. If the dog is young and the dog does not visit the mouse then the mouse does not visit the rabbit. If something is green and it chases the rabbit then the rabbit does not eat the mouse. If something is green then it visits the dog. If something visits the rabbit and the rabbit is young then it is round. If something is round and it visits the dog then it chases the tiger. If something is big then it visits the rabbit. Question: Based on the above information, is the following statement true, false, or unknown? The mouse visits the rabbit. Answer: True Original CoT: Let me solve this step by step. 1. First, lets identify what we know about the mouse: * The mouse eats the tiger * The mouse is green * If something visits the mouse then the mouse is big 2. Since the mouse eats the tiger and and no other eating relationships exist except this explicitly stated, according to the rule: * If something eats the tiger then the tiger visits the mouse * Therefore, the tiger visits the mouse 3. Since the tiger visits the mouse, according to the rule: * If something visits the mouse then the mouse is big * Therefore, the mouse is big 4. Now, since the mouse is big, according to the rule: * If something is big then it visits the rabbit * Therefore, the mouse visits the rabbit 5. We can directly conclude that the mouse visits the rabbit based on these logical steps. Therefore, the statement The mouse visits the rabbit is TRUE. VERICOT Output before self-reflection:"
        },
        {
            "title": "Decomposed CoT Steps",
            "content": "1. The mouse has three direct properties: the mouse eats the tiger, the mouse is green, and there is rule stating that if something visits the mouse then the mouse is big 2. Since the mouse eats the tiger and no other eating relationships exist except this explicitly stated, and there is rule stating that if something eats the tiger then the tiger visits the mouse, we can conclude that the tiger visits the mouse 3. Since the tiger visits the mouse, and there is rule stating that if something visits the mouse then the mouse is big, we can conclude that the mouse is big 4. Since the mouse is big, and there is rule stating that if something is big then it visits the rabbit, we can conclude that the mouse visits the rabbit 5. The statement The mouse visits the rabbit is true based on the logical chain of the mouse being big and the rule about big things visiting the rabbit Step 1 Step Context: The mouse has three direct properties: the mouse eats the tiger, the mouse is green, and there is rule stating that if something visits the mouse then the mouse is big."
        },
        {
            "title": "Preprint",
            "content": "; ======================= ; Schema ; ======================= ; represents an animal entity (declare-sort Animal) ; constant representing the mouse (declare-const mouse Animal) ; constant representing the tiger (declare-const tiger Animal) ; true if one animal eats another (declare-fun animal_eats (Animal Animal) Bool) ; true if an animal is green (declare-fun animal_is_green (Animal) Bool) ; true if an animal is big (declare-fun animal_is_big (Animal) Bool) ; true if one animal visits another (declare-fun animal_visits (Animal Animal) Bool) ; ======================= ; Added premises ; ======================= ; The mouse is green (document) (assert (animal_is_green mouse)) ; The mouse and tiger are different animals (commonsense) (assert (not (= mouse tiger))) ; The mouse eats the tiger (document) (assert (animal_eats mouse tiger)) ; If something visits the mouse then the mouse is big (doccument) (assert (forall ((a Animal)) (=> (animal_visits mouse) (animal_is_big mouse)))) ; ======================= ; Step logic ; ======================= ; mouse eats tiger ; mouse is green ; if animal visits mouse then mouse is big (assert (animal_eats mouse tiger)) (assert (animal_is_green mouse)) (assert (forall ((a Animal)) (=> (animal_visits mouse) (animal_is_big mouse)))) Evaluation: Pass Step 2 Step Context: Since the mouse eats the tiger and no other eating relationships exist except this explicitly stated, and also there is rule stating that if something eats the tiger then the tiger visits the mouse, we can conclude that the tiger visits the mouse."
        },
        {
            "title": "Preprint",
            "content": "; ======================= ; Schema ; ======================= ; (none new) ; ======================= ; Added premises ; ======================= ; If something eats the tiger then the tiger visits the mouse (document) (assert (forall ((a Animal)) (=> (animal_eats tiger) (animal_visits tiger mouse)))) ; ======================= ; Step logic ; ======================= ; The mouse eats the tiger and no other eating relationships exist except those explicitly stated ; if if mouse eats tiger and if eats tiger then (assert (animal_eats mouse tiger)) (assert (forall ((a Animal) (b Animal)) (= (animal_eats b) (and (= mouse) (= tiger))))) (assert (=> (and (animal_eats mouse tiger) tiger visits mouse (forall ((a Animal)) (=> (animal_eats tiger) (animal_visits tiger mouse)))) (animal_visits tiger mouse))) Evaluation: Fail due to Ungrounded. Reason: no other eating relationships exist except this explicitly stated cannot be grounded. Step 3 Step Context: Since the tiger visits the mouse, and there is rule stating that if something visits the mouse then the mouse is big, we can conclude that the mouse is big. ; ======================= ; Schema ; ======================= ; (none new) ; ======================= ; Added premises ; ======================= ; (none new) ; ======================= ; Step logic ; ======================= ; if tiger visits mouse and if any animal visits mouse then mouse is big (assert (=> (and (animal_visits tiger mouse) (forall ((a Animal)) (=> (animal_visits mouse) (animal_is_big mouse)))) (animal_is_big mouse))) Evaluation: Pass Step 4 Step Context: Since the mouse is big, and there is rule stating that if something is big then it visits the rabbit, we can conclude that the mouse visits the rabbit."
        },
        {
            "title": "Preprint",
            "content": "; ======================= ; Schema ; ======================= ; constant representing the rabbit (declare-const rabbit Animal) ; ======================= ; Added premises ; ======================= ; If something is big then it visits the rabbit (document) (assert (forall ((a Animal)) (=> (animal_is_big a) (animal_visits rabbit)))) ; ======================= ; Step logic ; ======================= ; if mouse big and if big then visits rabbit then mouse visits rabbit (assert (=> (and (animal_is_big mouse) (forall ((a Animal)) (=> (animal_is_big a) (animal_visits rabbit)))) (animal_visits mouse rabbit))) Evaluation: Pass Step Step Context: The statement The mouse visits the rabbit is true based on the logical chain of the mouse being big and the rule about big things visiting the rabbit. ; ======================= ; Schema ; ======================= ; (none new) ; ======================= ; Added premises ; ======================= ; (none new) ; ======================= ; Step logic ; ======================= ; if mouse is big and big animals visit rabbit then mouse visits rabbit (assert (=> (and (animal_is_big mouse) (forall ((a Animal)) (=> (animal_is_big a) (animal_visits rabbit)))) (animal_visits mouse rabbit))) Evaluation: Pass VERICOT Output after self-reflection: Decomposed CoT Steps 1: From the reference information, we know three direct facts about the mouse: the mouse eats the tiger, the mouse is green, and there is rule stating that if something visits the mouse then the mouse is big. 2: According to the reference information rule If something eats the tiger then the tiger visits the mouse, and since we know the mouse eats the tiger, we can conclude that the tiger visits the mouse. 3: Since we have established that the tiger visits the mouse, and we know from the reference information that If something visits the mouse then the mouse is big, we can conclude that the mouse is big. 4: The reference information states If something is big then it visits the rabbit. Since we have established that the mouse is big, applying this rule means that the mouse visits the rabbit."
        },
        {
            "title": "Preprint",
            "content": "5: Therefore, based on the logical chain starting from the mouse eating the tiger, leading to the tiger visiting the mouse, making the mouse big, and ending with the rule about big things visiting the rabbit, we can conclude that the statement The mouse visits the rabbit is true. Step 1 Step Context: From the reference information, we know three direct facts about the mouse: (1) The mouse eats the tiger, (2) The mouse is green, and (3) There is rule stating that if something visits the mouse then the mouse is big. ; ======================= ; Schema ; ======================= ; represents an animal entity (declare-sort Animal) ; constant representing the mouse (declare-const mouse Animal) ; constant representing the tiger (declare-const tiger Animal) ; true if one animal eats another (declare-fun animal_eats (Animal Animal) Bool) ; true if an animal is green (declare-fun animal_is_green (Animal) Bool) ; true if an animal is big (declare-fun animal_is_big (Animal) Bool) ; true if one animal visits another (declare-fun animal_visits (Animal Animal) Bool) ; ======================= ; Added premises ; ======================= ; The mouse is green (document) (assert (animal_is_green mouse)) ; The mouse and tiger are different animals (commonsense) (assert (not (= mouse tiger))) ; The mouse eats the tiger (document) ; mouse eats tiger (assert (animal_eats mouse tiger)) ; If something visits the mouse then the mouse is big (document) ; if visits mouse then mouse is big (assert (forall ((a Animal)) (=> (animal_visits mouse) (animal_is_big mouse)))) ; ======================= ; Step logic ; ======================= ; mouse eats tiger ; mouse is green ; if animal visits mouse then mouse is big (assert (animal_eats mouse tiger)) (assert (animal_is_green mouse)) (assert (forall ((a Animal)) (=> (animal_visits mouse) (animal_is_big mouse)))) Evaluation: Pass"
        },
        {
            "title": "Preprint",
            "content": "Step 2 Step Context: According to the reference information rule If something eats the tiger then the tiger visits the mouse, and since we know the mouse eats the tiger, we can conclude that the tiger visits the mouse. ; ======================= ; Schema ; ======================= ; (none new) ; ======================= ; Added premises ; ======================= ; If something eats the tiger then the tiger visits the mouse (document) ; if eats tiger then tiger visits mouse (assert (forall ((a Animal)) (=> (animal_eats tiger) (animal_visits tiger mouse)))) ; ======================= ; Step logic ; ======================= ; if eats tiger then tiger visits mouse ; if mouse eats tiger then tiger visits mouse (assert (forall ((a Animal)) (=> (animal_eats tiger) (animal_visits tiger mouse)))) (assert (=> (animal_eats mouse tiger) (animal_visits tiger mouse))) Evaluation: Pass Step 3 Step Context: Since we have established that the tiger visits the mouse, and we know from the reference information that If something visits the mouse then the mouse is big, we can conclude that the mouse is big. ; ======================= ; Schema ; ======================= ; (none new) ; ======================= ; Added premises ; ======================= ; (none new) ; ======================= ; Step logic ; ======================= ; if tiger visits mouse then mouse is big (assert (=> (animal_visits tiger mouse) (animal_is_big mouse))) Evaluation: Pass"
        },
        {
            "title": "Preprint",
            "content": "Step 4 Step Context: The reference information states If something is big then it visits the rabbit. Since we have established that the mouse is big, applying this rule means that the mouse visits the rabbit. ; ======================= ; Schema ; ======================= ; constant representing the rabbit (declare-const rabbit Animal) ; ======================= ; Added premises ; ======================= ;If something is big then it visits the rabbit (document) ; if is big then visits rabbit (assert (forall ((a Animal)) (=> (animal_is_big a) (animal_visits rabbit)))) ; ======================= ; Step logic ; ======================= ; if is big then visits rabbit ; if mouse is big then mouse visits rabbit (assert (forall ((a Animal)) (=> (animal_is_big a) (animal_visits rabbit)))) (assert (=> (animal_is_big mouse) (animal_visits mouse rabbit))) Evaluation: Pass Step 5 Step Context: Therefore, based on the logical chain starting from the mouse eating the tiger, leading to the tiger visiting the mouse, making the mouse big, and ending with the rule about big things visiting the rabbit, we can conclude that the statement The mouse visits the rabbit is true."
        },
        {
            "title": "Preprint",
            "content": "; ======================= ; Schema ; ======================= ; (none new) ; ======================= ; Added premises ; ======================= ; (none new) ; ======================= ; Step logic ; ======================= ; mouse eats tiger => tiger visits mouse ; tiger visits mouse => mouse is big ; mouse is big => mouse visits rabbit ; mouse visits rabbit (assert (=> (animal_eats mouse tiger) (animal_visits tiger mouse))) (assert (=> (animal_visits tiger mouse) (animal_is_big mouse))) (assert (=> (animal_is_big mouse) (animal_visits mouse rabbit))) (assert (animal_visits mouse rabbit)) Evaluation: Pass A.2.2 CONTRADICTION EXAMPLE FROM BIOASQ Contradiction example from BioASQ Question: Can Connexin hemi channels be used for drug delivery? Gold Answer: Yes Context: The permeability of Cx43 channels to small molecules and macromolecules makes them highly attractive targets for delivering drugs directly into the cytoplasm. Cancer cells overexpressing Cx43 may be more permeable and sensitive to chemotherapeutics. In this context, certain channels lead to transitory plasma membrane permeability changes, such as pannexin, connexin hemichannels that are channels in membranes that pass molecules, TRPV1-4 and P27, which allow for the non-selective passage of molecules up to 1,000 Da. Gap junction channels, composed of connexin proteins, provide mechanism for direct transfer of small molecules across membranes, and recent evidence suggests that the transfer of larger, polymer-like molecules such as microRNAs may be possible. Collectively, these results reveal that polymeric macromolecules can be delivered to cells via gap junctions, suggesting that the gap junction route can be used for the delivery of macro polymeric therapeutic molecules, which provides evidence for drug delivery potential. Original CoT: Let me break this down step by step. 1. First, lets understand what connexin hemichannels are: * They are channels in cell membranes that can allow molecules to pass through. * They are part of gap junction. 2. Key characteristic of connexin hemichannels: * They allow non-selective passage of molecules up to 1,000 Da. 3. Evidence for drug delivery potential: * Gap junction route can transport various types of macro polymeric therapeutic molecules. Conclusion: Yes, connexin hemichannels can be used for drug delivery. The evidence strongly supports this, showing they can transport various types of therapeutic molecules, making them valuable targets for drug delivery applications. Decomposed CoT Steps 1. Connexin hemichannels are channels in cell membranes that can allow molecules to pass through and are part of gap junction channels. 2. Connexin hemichannels allow for the non-selective passage of molecules up to 1,000 Da."
        },
        {
            "title": "Preprint",
            "content": "3. The gap junction route can transport macro polymer therapeutic molecules, which is evidence for drug delivery potential. 4. Therefore, Connexin hemi channels are permeable to therapeutic macromolecules, makes them highly attractive targets for delivering drugs. Step 1 Step Context: Connexin hemichannels are channels in cell membranes that can allow molecules to pass through and are part of gap junction channels. ; ======================= ; Schema ; ======================= ; represents molecule (declare-sort Molecule) ; represents channel (declare-sort Channel) ; whether channel is connexin hemichannel (declare-fun IsConnexinHemichannel (Channel) Bool) ; whether channel is gap junction channel (declare-fun IsGapJunctionChannel (Channel) Bool) ; whether channel is located in cell membrane (declare-fun IsInCellMembrane (Channel) Bool) ; whether an entity is considered channel (declare-fun IsChannel (Channel) Bool) ; whether molecule can enter through channel (declare-fun CanEnterViaChannel (Molecule Channel) Bool) ; whether one channel is part of another channel (declare-fun PartOf (Channel Channel) Bool) ; ======================= ; Added premises ; ======================= ; Connexin hemichannels are channels in membranes that pass molecules (document) (assert (forall ((c Channel)) (=> (IsConnexinHemichannel c) (and (IsChannel c) (IsInCellMembrane c) (exists ((m Molecule)) (CanEnterViaChannel c))))))) ; Connexin hemichannels are part of gap junction channels (document) (assert (forall ((h Channel)) (=> (IsConnexinHemichannel h) (exists ((g Channel)) (and (IsGapJunctionChannel g) (PartOf g)))))) ; ======================= ; Step logic ; ======================= ; Connexin hemichannels are channels in membranes that pass molecules ; Connexin hemichannels are part of gap junction channels (assert (forall ((c Channel)) (=> (IsConnexinHemichannel c) (and (IsChannel c) (IsInCellMembrane c) (exists ((m Molecule)) (CanEnterViaChannel c))))))) (assert (forall ((h Channel)) (=> (IsConnexinHemichannel h) (exists ((g Channel)) (and (IsGapJunctionChannel g) (PartOf g)))))) Evaluation: Pass Step 2 Step Context: Connexin hemichannels allow for the non-selective passage of molecules up to 1,000 Da."
        },
        {
            "title": "Preprint",
            "content": "; ======================= ; Schema ; ======================= ; returns the molecular weight of molecule (declare-fun MolecularWeight (Molecule) Real) ; ======================= ; Added premises ; ======================= ; Connexin hemichannels allow for the non-selective passage of molecules up to 1,000 Da (document) (assert (forall ((m Molecule) (c Channel)) (=> (and (IsConnexinHemichannel c) (CanEnterViaChannel c)) (<= (MolecularWeight m) 1000)))) ; ======================= ; Step logic ; ======================= ; If molecule can enter via connexin channel, its weight must be <= 1000 (assert (forall ((m Molecule) (c Channel)) (=> (and (IsConnexinHemichannel c) (CanEnterViaChannel c)) (<= (MolecularWeight m) 1000)))) Evaluation: Pass Step 3 Step Context: Gap junction route can transport macro polymer therapeutic molecules, which is evidence for drug delivery potential."
        },
        {
            "title": "Preprint",
            "content": "; ======================= ; Schema ; ======================= ; whether molecule is macro polymer therapeutic molecule (declare-fun IsMacroPolymerTherapeuticmolecule (Molecule) Bool) ; whether channel has potential for drug delivery (declare-fun HasDrugDeliveryPotential (Channel) Bool) ; ======================= ; Added premises ; ======================= ; The ability to deliver macro polymeric therapeutic molecules via the gap junction route, ; which provides evidence for drug delivery potential (document) (assert (forall ((c Channel) (m Molecule)) (=> (and (IsGapJunctionChannel c) (IsMacroPolymerTherapeuticmolecule m)) (and (CanEnterViaChannel c) (=> (CanEnterViaChannel c) (HasDrugDeliveryPotential c)))))) ; Any marco polymeric therapeutic molecule has molecular weight > 1,000 Da ( commonsense) (assert (forall ((m Molecule)) (=> (IsMacroPolymerTherapeuticmolecule m) (> (MolecularWeight m) 1000)))) ; ======================= ; Step logic ; ======================= ; If gap junction channel transports large polymeric therapeutic macromolecules, ; then it has drug-delivery potential (assert (forall ((c Channel) (m Molecule)) (=> (and (IsGapJunctionChannel c) (IsMacroPolymerTherapeuticmolecule m)) (and (CanEnterViaChannel c) (=> (CanEnterViaChannel c) (HasDrugDeliveryPotential c)))))) Evaluation: Pass Step 4 Step Context: Therefore, Connexin hemichannels are permeable to therapeutic macromolecules, making them valuable targets for delivering drugs."
        },
        {
            "title": "Preprint",
            "content": "; ======================= ; Schema ; ======================= ; (none new) ; ======================= ; Added premises ; ======================= ; (none new) ; ======================= ; Step logic ; ======================= ; Connexin hemichannels are permeable to therapeutic macromolecules (assert (exists ((c Channel) (m Molecule)) (and (IsConnexinHemichannel c) (IsMacroPolymerTherapeuticmolecule m) (CanEnterViaChannel c)))) ; Connexin hemichannels are permeable to therapeutic macromolecules, ; -> they are highly attractive drug-delivery targets (assert (forall ((c Channel) (m Molecule)) (=> (and (IsConnexinHemichannel c) (IsMacroPolymerTherapeuticmolecule m) (CanEnterViaChannel c)) (HasDrugDeliveryPotential c)))) Evaluation: Fail due to contradiction. Reason: The axioms show macro-polymer molecules are > 1000 Da, but connexin hemichannels only permit molecules 1000 Da. Thus, the same molecule would need to be both 1000 and > 1000, which is impossible. Decomposed CoT Steps 1. Connexin hemichannels are channels in cell membranes that can allow molecules to pass through and are part of gap junction channels. 2. Connexin hemichannels allow for the non-selective passage of molecules up to 1,000 Da. 3. Cx43 channels are permeable to both small molecules and macromolecules, makes them highly attractive targets for delivering drugs. 4. Since Cx43 is connexin hemi channel and supports drug delivery, we infer that connexin hemi channels can be used for drug delivery. Step 1 Step Context: connexin hemi channel are channels in cell membranes that can allow molecules to pass through and are part of gap junction channels."
        },
        {
            "title": "Preprint",
            "content": "; ======================= ; Schema ; ======================= ; represents molecule (declare-sort Molecule) ; represents channel (declare-sort Channel) ; whether channel is connexin hemichannel (declare-fun IsConnexinHemichannel (Channel) Bool) ; whether channel is gap junction channel (declare-fun IsGapJunctionChannel (Channel) Bool) ; whether channel is located in cell membrane (declare-fun IsInCellMembrane (Channel) Bool) ; whether an entity is considered channel (declare-fun IsChannel (Channel) Bool) ; whether molecule can enter through channel (declare-fun CanEnterViaChannel (Molecule Channel) Bool) ; whether one channel is part of another channel (declare-fun PartOf (Channel Channel) Bool) ; ======================= ; Added premises ; ======================= ; connexin hemmichannels are channels in membranes that pass molecules( documents) (assert (forall ((c Channel)) (=> (IsConnexinHemichannel c) (and (IsChannel c) (IsInCellMembrane c) (exists ((m Molecule)) (CanEnterViaChannel c)))))) ; Connexin hemichannels are part of gap junction channels ( documents) (assert (forall ((h Channel)) (=> (IsConnexinHemichannel h) (exists ((g Channel)) (and (IsGapJunctionChannel g) (PartOf g)))))) ; ======================= ; Step logic ; ======================= ; Connexin hemichannels are channels in membranes that pass molecules (assert (forall ((c Channel)) (=> (IsConnexinHemichannel c) (and (IsChannel c) (IsInCellMembrane c) (exists ((m Molecule)) (CanEnterViaChannel c)))))) ; Connexin hemichannels are part of gap junction channels (assert (forall ((h Channel)) (=> (IsConnexinHemichannel h) (exists ((g Channel)) (and (IsGapJunctionChannel g) (PartOf g)))))) Evaluation: pass Step 2 Step Context: Connexin hemi channels allow for the non-selective passage of molecules up to 1,000 Da."
        },
        {
            "title": "Preprint",
            "content": "; ======================= ; Schema ; ======================= ; returns the molecular weight of molecule (declare-fun MolecularWeight (Molecule) Real) ; ======================= ; Added premises ; ======================= ; Connexin hemmichannels allow for the non-selective passage of molecules up to 1,000 Da. (assert (forall ((m Molecule) (c Channel)) (=> (and (IsConnexinHemichannel c) (CanEnterViaChannel c)) (<= (MolecularWeight m) 1000)))) ; ======================= ; Step logic ; ======================= ; If drug can enter via connexin channel, its weight must be <= 1000 (assert (forall ((m Molecule) (c Channel)) (=> (and (IsConnexinHemichannel c) (CanEnterViaChannel c)) (<= (MolecularWeight m) 1000)))) Evaluation: Pass Step Step Context: Cx43 channels are permeable to both small molecules and macromolecules, makes them highly attractive targets for delivering drugs."
        },
        {
            "title": "Preprint",
            "content": "; ======================= ; Schema ; ======================= ; whether channel is specifically connexin 43 (Cx43) (declare-fun IsCx43 (Channel) Bool) ; whether molecule is small molecule (declare-fun IsSmallMolecule (Molecule) Bool) ; whether molecule is macromolecule (declare-fun IsMacromolecule (Molecule) Bool) ; whether channel is permeable to given molecule (declare-fun PermeableTo (Channel Molecule) Bool) ; whether channel can deliver drugs (channel-level, independent of molecule) (declare-fun CanDeliverDrug (Channel) Bool) ; ======================= ; Added premises ; ======================= ; The permeability of Cx43 channels to small molecules and macromolecules, makes them highly attractive targets for delivering drugs directly into the cytoplasm. (document) (assert (forall ((c Channel) (m Molecule)) (=> (and (IsCx43 c) (or (IsSmallMolecule m) (IsMacromolecule m))) (and (CanEnterViaChannel c) (=> (CanEnterViaChannel c) (CanDeliverDrug c)))))) ; Any large macromolecule has molecular weight > 1,000 Da. (commonsense) (assert (forall ((m Molecule)) (=> (IsMacromolecule m) (> (MolecularWeight m) 1000)))) ; Any small macromolecule has molecular weight <= 1,000 Da. (commonsense) (assert (forall ((m Molecule)) (=> (IsSmallMolecule m) (<= (MolecularWeight m) 1000)))) ; ======================= ; Step logic ; ======================= ; Combined: if is Cx43 and is small or macromolecule, then ; (1) permeates via c; and (2) given that permeability, can deliver drugs. (assert (forall ((c Channel) (m Molecule)) (=> (and (IsCx43 c) (or (IsSmallMolecule m) (IsMacromolecule m))) (and (CanEnterViaChannel c) (=> (CanEnterViaChannel c) (CanDeliverDrug c)))))) Evaluation: Pass Step 4 Step Context: Since Cx43 is connexin hemi channel and supports drug delivery, we infer that connexin hemi channels can be used for drug delivery."
        },
        {
            "title": "Preprint",
            "content": "; ======================= ; Schema ; ======================= ; (none new) ; ======================= ; Added premises ; ======================= ; Cx43 channels are type of connexin channel (Commonsense) ; Cx43 is type of connexin hemichannel (assert (forall ((c Channel)) (=> (IsCx43 c) (IsConnexinHemichannel c)))) ; ======================= ; Step logic ; ======================= ; Cx43 channels are type of connexin channel ; Cx43 is type of connexin hemichannel (assert (forall ((c Channel)) (=> (IsCx43 c) (IsConnexinHemichannel c)))) ; If specific Cx43 can deliver drugs, infer that connexin hemichannels can. ; Every drug that Cx43 channel can deliver, can also be delivered by some Connexin hemichannel. (assert (exists ((c Channel)) (and (IsCx43 c) (CanDeliverDrug c)))) (assert (forall ((c Channel)) (=> (and (IsCx43 c) (CanDeliverDrug c)) (exists ((h Channel)) (and (IsConnexinHemichannel h) (CanDeliverDrug h)))))) Evaluation: Pass A.2.3 UNTRANSLATABLE EXAMPLE FROM SARA Untranslatable example from SARA Reference information: Section 3306(c)(1) applies to agricultural labor. Question: Determine whether the following statements are entailed under the statute. Description: Alice has paid $3200 to Bob for agricultural labor done from Feb 1st, 2017 to Sep 2nd, 2017. Alice is an American employer. Statement: Section 3306(c)(1) applies to Alice employing Bob for the year 2017. Reply with either: Entailment, Contradiction. Answer: Entailment Original CoT: Let me solve this step by step. First, lets identify the key elements in the statute: * The statute in question is Section 3306(c)(1). * Its primary subject is agricultural labor. Lets analyze the given description of the situation: * Employee: Bob * Employer: Alice (an American employer) * Type of work: The work is explicitly identified as agricultural labor. Lets compare the situation to the statutes basic scope: * The statute, Section 3306(c)(1), specifically and directly addresses agricultural labor. * The work performed by Bob is agricultural labor. * Therefore, the situation falls squarely within the fundamental subject matter of this statutory section. Important consideration: * The fact that the work was performed for an American employer raises potential jurisdictional or geographical questions. * However, the provided statute fragment does not contain any information about geographical limitations."
        },
        {
            "title": "Preprint",
            "content": "Based on the basic provision: * Since the work is agricultural labor, and Section 3306(c)(1) explicitly covers agricultural labor, the situation described falls under the basic scope of this section. VERICOT Output before self-reflection: Decomposed CoT Steps 1. Section 3306(c)(1) of the statute addresses agricultural labor 2. The work performed by Bob for Alice is specifically agricultural labor 3. The basic scope of Section 3306(c)(1) covers all agricultural labor 4. While Alice is an American employer and there might be jurisdictional implications, the statute fragment does not provide information about geographical limitations 5. Bobs employment falls under the basic scope of Section 3306(c)(1) Step 1 Step Context: Section 3306(c)(1) of the statute addresses agricultural labor ; ======================= ; Schema: ; ======================= ; represents section of statute (declare-sort StatuteSection) ; specific section 3306(c)(1) (declare-const section_3306c1 StatuteSection) ; whether section deals with agricultural labor (declare-fun deals_with_agricultural_labor (StatuteSection) Bool) ; ======================= ; Add premises: ; ======================= ; Section 3306(c)(1) applies to agricultural labor (document) (assert (deals_with_agricultural_labor section_3306c1)) ; ======================= ; Step Logic: ; ======================= ; sec 3306c1 abt agri labor (assert (deals_with_agricultural_labor section_3306c1)) Evaluation: Pass Step 2 Step Context: The work performed by Bob for Alice is specifically agricultural labor"
        },
        {
            "title": "Preprint",
            "content": "; ======================= ; Add schema: ; ======================= ; represents person (declare-sort Person) ; represents Alice (declare-const alice Person) ; represents Bob (declare-const bob Person) ; whether person performs agricultural labor (declare-fun performs_agricultural_labor (Person) Bool) ; whether person employs another person (declare-fun employs (Person Person) Bool) ; ======================= ; Add premises: ; ======================= ; Alice employed Bob to perform agricultural labor from February 1st to September 2nd, 2017, as evidenced by Alices payment of $3200 for this work (document) ; Dates encoded as YYYYMMDD ; paid employer worker laborType amount startDate endDate (declare-fun paid (Person Person LaborType Int Int Int) Bool) (assert (and (employs alice bob) (performs_agricultural_labor bob) (paid alice bob agricultural_labor 3200 20170201 20170902))) ; ======================= ; Step logic: ; ======================= ; Bob performs agri labor for Alice (assert (and (performs_agricultural_labor bob) (employs alice bob))) Evaluation: Pass Step 3 Step Context: The basic scope of Section 3306(c)(1) covers all agricultural labor"
        },
        {
            "title": "Preprint",
            "content": "; ======================= ; Add schema: ; ======================= ; predicate: whether persons work is covered by 3306(c)(1) (declare-fun covered_by_section_3306c1 (Person) Bool) ; ======================= ; Add premises: ; ======================= ; person performs labor, therefore anyone perform agricultural labor is covered by Section 3306(c)(1) (commonsense) (assert (forall ((p Person)) (=> (performs_agricultural_labor p) (covered_by_section_3306c1 p)))) ; ======================= ; Step logic: ; ======================= ; sec 3306c1 covers agri labor ; if person does agri labor then covered by sec 3306c1 (assert (deals_with_agricultural_labor section_3306c1)) (assert (forall ((p Person)) (=> (performs_agricultural_labor p) (covered_by_section_3306c1 p)))) Evaluation: Pass Step 4 Step Context: While Alice is an American employer and there might be jurisdictional implications, the statute fragment does not provide information about geographical limitations ; ======================= ; Schema: ; (none new) ; ======================= ; ======================= ; Add premises: ; (none new) ; ======================= ; ======================= ; Step logic: ; (none new) ; ======================= Evaluation: Fail due to Untranslatable Reason: The phrase there might be jurisdictional implication cannot be rendered under SMI-LIB rules because it expresses possibility rather than definitive statement. Step 5 Step Context: Bobs employment falls under the basic scope of Section 3306(c)(1)"
        },
        {
            "title": "Preprint",
            "content": "; ======================= ; Schema: ; (none new) ; ======================= ; ======================= ; Add premises: ; (none new) ; ======================= ; ======================= ; Step logic: ; ======================= (assert (covered_by_section_3306c1 bob)) Evaluation: Pass VERICOT Output after self-reflection: Decomposed CoT steps 1. Section 3306(c)(1) explicitly applied to agricultural labor as shown by the statute text agricultural labor. 2. Bob performed agricultural labor. 3. Since Bobs work qualifies as agricultural labor, and Section 3306(c)(1) covers agricultural labor, Bobs employment falls under the basic scope of Section 3306(c)(1). Step 1 Step Context: Section 3306(c)(1) explicitly addresses agricultural labor, which means that any person who performs agricultural labor is covered by Section 3306(c)(1)."
        },
        {
            "title": "Preprint",
            "content": "; ======================= ; Schema ; ======================= ; represents statute section (declare-sort StatuteSection) ; represents type of labor (declare-sort LaborType) ; represents person (declare-sort Person) ; specific statute section 3306(c)(1) (declare-const section_3306c1 StatuteSection) ; specific labor type: agricultural labor (declare-const labor_agricultural LaborType) ; whether section addresses labor type (declare-fun section_addresses_labor (StatuteSection LaborType) Bool) ; whether person performed labor type (declare-fun performed_labor (Person LaborType) Bool) ; whether persons employment is covered by statute section (declare-fun employment_covered (Person StatuteSection) Bool) ; ======================= ; Added premises ; ======================= ; Section 3306(c)(1) explicitly applies to agricultural labor as shown by the statute text agricultural labor (document) (assert (section_addresses_labor section_3306c1 labor_agricultural)) ; person performs labor, therefore anyone performing agricultural labor is covered by Section 3306(c)(1) (commonsense) (assert (forall ((p Person)) (=> (performed_labor labor_agricultural) (employment_covered section_3306c1)))) ; ======================= ; Step logic ; ======================= ; Section 3306(c)(1) explicitly addresses agricultural labor ; -> anyone who performed agricultural labor is covered (assert (section_addresses_labor section_3306c1 labor_agricultural)) (assert (forall ((p Person)) (=> (performed_labor labor_agricultural) (employment_covered section_3306c1)))) Evaluation: Pass Step 2 Step Context: Bob performed agricultural labor."
        },
        {
            "title": "Preprint",
            "content": "; ======================= ; Schema ; ======================= ; Bob performed agricultural labor. (declare-const Bob Person) ; ======================= ; Added premises ; ======================= ; Alice has paid $3200 to Bob for agricultural labor done from Feb 1st, 2017 to Sep 2nd, 2017. (document) ; Dates encoded as YYYYMMDD ; paid employer worker laborType amount startDate endDate (declare-fun paid (Person Person LaborType Int Int Int) Bool) (declare-const Alice Person) (assert (performed_labor Bob labor_agricultural)) (assert (paid Alice Bob labor_agricultural 3200 20170201 20170902)) ; ======================= ; Step logic ; ======================= (assert (performed_labor Bob labor_agricultural)) Evaluation: Pass Step 3 Step Context: Since Bobs work qualifies as agricultural labor, and Section 3306(c)(1) covers agricultural labor, Bobs employment falls under the basic scope of Section 3306(c)(1). ; ======================= ; Schema ; (none new) ; ======================= ; ======================= ; Added premises ; (none new) ; ======================= ; ======================= ; Step logic ; ======================= ; derive Bobs coverage (assert (=> (and (performed_labor Bob labor_agricultural) (forall ((p Person)) (=> (performed_labor labor_agricultural) (employment_covered section_3306c1)))) (employment_covered Bob section_3306c1))) Evaluation: Pass"
        }
    ],
    "affiliations": [
        "Amazon Web Services",
        "University of Pennsylvania"
    ]
}