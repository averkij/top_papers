{
    "paper_title": "SeqTex: Generate Mesh Textures in Video Sequence",
    "authors": [
        "Ze Yuan",
        "Xin Yu",
        "Yangtian Sun",
        "Yuan-Chen Guo",
        "Yan-Pei Cao",
        "Ding Liang",
        "Xiaojuan Qi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training native 3D texture generative models remains a fundamental yet challenging problem, largely due to the limited availability of large-scale, high-quality 3D texture datasets. This scarcity hinders generalization to real-world scenarios. To address this, most existing methods finetune foundation image generative models to exploit their learned visual priors. However, these approaches typically generate only multi-view images and rely on post-processing to produce UV texture maps -- an essential representation in modern graphics pipelines. Such two-stage pipelines often suffer from error accumulation and spatial inconsistencies across the 3D surface. In this paper, we introduce SeqTex, a novel end-to-end framework that leverages the visual knowledge encoded in pretrained video foundation models to directly generate complete UV texture maps. Unlike previous methods that model the distribution of UV textures in isolation, SeqTex reformulates the task as a sequence generation problem, enabling the model to learn the joint distribution of multi-view renderings and UV textures. This design effectively transfers the consistent image-space priors from video foundation models into the UV domain. To further enhance performance, we propose several architectural innovations: a decoupled multi-view and UV branch design, geometry-informed attention to guide cross-domain feature alignment, and adaptive token resolution to preserve fine texture details while maintaining computational efficiency. Together, these components allow SeqTex to fully utilize pretrained video priors and synthesize high-fidelity UV texture maps without the need for post-processing. Extensive experiments show that SeqTex achieves state-of-the-art performance on both image-conditioned and text-conditioned 3D texture generation tasks, with superior 3D consistency, texture-geometry alignment, and real-world generalization."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 5 8 2 4 0 . 7 0 5 2 : r SEQTEX: GENERATE MESH TEXTURES IN VIDEO SEQUENCE Ze Yuan,1 Xin Yu,1 Yangtian Sun,1 Yuan-Chen Guo,2 Yan-Pei Cao,2 Ding Liang,2 Xiaojuan Qi1 1HKU, 2VAST Equal contribution, Project lead, Corresponding author Figure 1: SeqTex is capable of generating high-quality and diverse textures for meshes, conditioned on either an image or text input. Furthermore, the generated textures maintain strong correspondence with the meshs geometry and ensure high consistency of detailed textures across various viewing angles."
        },
        {
            "title": "ABSTRACT",
            "content": "Training native 3D texture generative models remains fundamental yet challenging problem, largely due to the limited availability of large-scale, high-quality 3D texture datasets. This scarcity hinders generalization to real-world scenarios. To address this, most existing methods finetune foundation image generative models to exploit their learned visual priors. However, these approaches typically generate only multi-view images and rely on post-processing to produce UV texture mapsan essential representation in modern graphics pipelines. Such two-stage pipelines often suffer from error accumulation and spatial inconsistencies across the 3D surface. In this paper, we introduce SeqTex, novel end-to-end framework that leverages the visual knowledge encoded in pretrained video foundation models to directly generate complete UV texture maps. Unlike SeqTex: Generating Mesh Textures in Video Sequence previous methods that model the distribution of UV textures in isolation, SeqTex reformulates the task as sequence generation problem, enabling the model to learn the joint distribution of multi-view renderings and UV textures. This design effectively transfers the consistent image-space priors from video foundation models into the UV domain. To further enhance performance, we propose several architectural innovations: decoupled multi-view and UV branch design, geometryinformed attention to guide cross-domain feature alignment, and adaptive token resolution to preserve fine texture details while maintaining computational efficiency. Together, these components allow SeqTex to fully utilize pretrained video priors and synthesize high-fidelity UV texture maps without the need for post-processing. Extensive experiments show that SeqTex achieves state-of-the-art performance on both image-conditioned and text-conditioned 3D texture generation tasks, with superior 3D consistency, texture-geometry alignment, and real-world generalization. Our project page is https://yuanze1024.github.io/SeqTex/. Keywords Video Diffusion Models, Diffusion Techniques, Texture Generation"
        },
        {
            "title": "Introduction",
            "content": "Manually texturing 3D models is highly labor-intensive process, often requiring skilled artists to dedicate hours to crafting single asset. This challenge is particularly acute in game and film production, where thousands of high-quality textured models are essential for constructing complex scenes and conveying compelling narratives. Despite rapid progress in generative modeling-driven by unprecedented advances in data availability and computational resources-which has transformed fields such as natural language processing [1], image and video synthesis [2, 3, 4, 5, 6], and 3D shape generation [7, 8, 9], progress in 3D texture generation has lagged significantly. key bottleneck is the scarcity of large-scale, high-quality 3D datasets with consistent UV parameterizations and diverse, realistic textures. Early methods [10, 11, 12, 13] relied on curated datasets like ShapeNet [14], which suffer from limited scale and narrow category diversity, thereby restricting their generalizability. More recent efforts, such as TEXGen [15], have introduced hybrid neural architectures to better model the UV space and leverage larger training sets. However, data limitations continue to hinder their generalization to diverse real-world scenarios. Given the remarkable ability of image and video generative models to synthesize rich and diverse textures, natural direction is to adapt these models for 3D texture generation. However, their learned priors operate in the projected 2D image space, making direct application to UV texture mapping challenging. The UV space represents an unwrapped surface manifold where pixel adjacency is governed by UV seams rather than 3D spatial continuity, introducing discontinuities. Consequently, most existing approaches generate multi-view images [16, 17, 18], followed by iterative back-projection and blending [16, 17, 18], or additional UV inpainting [17, 18], to produce complete UV texture maps. These multi-stage pipelines are not end-to-end and are prone to error accumulation and spatial inconsistencies on the textured surface. In this paper, we present SeqTex, novel end-to-end framework that harnesses the rich visual priors of pre-trained video foundation models to directly generate complete UV texture maps. Instead of predicting the UV map in isolation, we reformulate the task as sequence generation problem: the model outputs series of images comprising multi-view renderings of the object, with the UV texture map generated as the final frame. This design enables joint optimization of multi-view synthesis and UV texture generation within single-stage pipeline. SeqTex offers three key advantages: (1) By aligning the task with the temporal structure inherent in video foundation models, it effectively transfers learned visual knowledge from video data to the texture domain; (2) By incorporating multi-view context, the model learns to integrate and align information across different viewpoints, resulting in more coherent and realistic UV textures; (3) The unified architecture allows for training with additional high-quality multi-view-only datasets, enhancing the models generalization and robustness. SeqTex, built upon pre-trained video diffusion transformer, introduces three key innovations to enable high-quality 3D texture synthesis: (1) Decoupled Multi-View (MV) and UV Texture Learning: To bridge the domain gap between spatially coherent multi-view (MV) frames and the discontinuous layout of UV maps, we design separate processing branches. The MV branch efficiently adapts video priors using LoRA, while the UV branch is fully fine-tuned to generate high-fidelity texture maps. (2) Geometry-Informed Attention: We leverage the geometric consistency of attributes such as 3D coordinates and surface normals across the MV and UV domains. By embedding this geometric information into the attention mechanism, we guide UV tokens to attend to MV tokens corresponding to spatially proximate 3D locations and orientations, enabling effective information transfer from the MV to the UV domain. (3) Adaptive Token Resolution: To capture fine-grained texture details in UV maps without incurring excessive computational overhead, we introduce an adaptive resolution strategy: UV tokens are processed at higher spatial resolution than MV tokens. This design preserves texture details while maintaining computational efficiency. SeqTex: Generating Mesh Textures in Video Sequence As illustrated in Fig. 1, our approach effectively transfers world knowledge from pre-trained video foundation models to the UV texture space, enabling the generation of diverse nd realistic 3D scenes with strong generalization capabilities. Our key contributions are summarized as follows: We propose novel end-to-end framework that adapts pre-trained video generative models to synthesize complete 3D texture maps. To the best of our knowledge, this is the first method of its kind. We introduce decoupled architecture with dedicated branches for multi-view rendering and UV texture generation, incorporate geometry-informed attention to enhance cross-view and cross-domain information transfer, and employ adaptive resolution to efficiently capture fine-grained texture details. Our method achieves state-of-the-art performance on both image-conditioned and text-conditioned 3D texture generation tasks, consistently surpassing prior methods in 3D consistency, geometric alignment, and visual fidelity, while maintaining competitive inference speed."
        },
        {
            "title": "2 Related Work",
            "content": "Texture Generation Methods. Recent years have seen rapid advances in generative modeling for 3D texture synthesis, yet key limitations persist in scalability, detail fidelity, and real-world generalization. predominant paradigm involves leveraging pre-trained 2D diffusion or flow-matching models at test time [19, 20, 21, 22, 23, 24, 25, 26, 27, 28], which optimize textures via image-space supervision or view-by-view inpainting. While effective in certain cases, these methods often suffer from long optimization times, lack holistic 3D awareness, and are prone to inter-view inconsistencies due to missing global context. Another line of research focuses on training feed-forward generative models directly on 3D data. Early approaches [10, 11, 12, 13] were restricted to category-specific datasets [14], limiting their generalization to diverse, real-world assets. More recent efforts [15] have attempted to address this by using larger training sets and hybrid network architectures for end-to-end texture synthesis. Nevertheless, data scarcity remains fundamental barrier; available 3D texture data is orders of magnitude smaller than the datasets used to train leading image or video models, which restricts robust generalization to open-world scenarios. Video Generative Models. Video generation models [6, 4, 5, 29] have recently achieved remarkable progress, enabling the synthesis of high-quality, unprecedentedly realistic videos. Trained on large-scale video datasets, these models acquire rich world knowledge, which has been successfully adapted for downstream tasks such as controllable video generation [30, 31, 4, 32], video and image editing [33, 34, 35], multi-view generation [36, 37], and geometry prediction [38, 39]. These applications benefit from operating in the projected image space, where learned priors can be directly applied. However, for 3D texture generation, the target representation lies in the UV space, which is fundamentally different from natural images, making it challenging to leverage this world knowledge. In this work, we propose novel method that adapts pre-trained video generative models to synthesize complete 3D texture maps in an end-to-end manner. To the best of our knowledge, this is the first work to utilize video models for direct UV texture generation."
        },
        {
            "title": "3 SeqTex",
            "content": "3.1 Overview Given an untextured 3D mesh, we aim to design single-stage, feed-forward network SeqTex (see Figure 2) that synthesizes high-fidelity UV texture maps conditioned on reference image and, optionally, textual prompt, all in single end-to-end pass. Our key insight is to harness the generative power of pre-trained video diffusion model and adapt it for UV texture synthesis. To achieve this, we decompose the task into two subproblems multi-view image synthesis and UV texture mapping and recast their joint prediction as sequence generation problem, thereby enabling the direct application of video diffusion models. This formulation offers two major advantages. First, by predicting multi-view images alongside the UV texture map, SeqTex capitalizes on the strong frame synthesis capabilities of video models and uses these synthesized views to guide coherent UV texture generation. This design also enables the incorporation of large, multi-view-only datasets during training, further enhancing generalization. Second, treating the entire process as sequential prediction task permits unified, end-to-end optimization, mitigating the error accumulation common in multi-stage pipelines. The remainder of this section is organized as follows. Section 3.2 explains how we repurpose pre-trained video diffusion model for our task. Section 3.3 introduces our core architectural innovations for handling the hybrid multi-view and UV sequence representation. Finally, Section 3.4 details our multi-task training strategy. 3 SeqTex: Generating Mesh Textures in Video Sequence Figure 2: Overview of our approach and core insights. (a) Joint UV and multi-view synthesis with video priors: Given an untextured 3D mesh and conditioning inputs (image or text), SeqTex generates complete UV texture map. Unlike prior works that predict only single texture map, our approach jointly synthesizes multi-view images and the UV texture, thereby leveraging the rich generative priors of pre-trained video models. (b) SeqTex pipeline: Geometric image and text conditions are tokenized and injected alongside noised multi-view and UV tokens. flow matching modelinitialized from pre-trained video foundation model and then fine-tuned for our taskdenoises these tokens to yield coherent multi-view renderings and high-fidelity UV map. (c) Multi-View (MV) branch (LoRA fine-tuned): MV tokens are refined under geometric and textual guidance. Geometry-informed attention is applied exclusively among MV tokens to enforce view consistency and produce set of aligned multi-view outputs. (d) UV branch (fully fine-tuned): Guided by geometric cues, UV tokens (queries) attend to both MV and UV tokens (keys and values) via Geo Attention. This enables the network to integrate multi-view information into coherent UV space for accurate and seamless texture synthesis. 3.2 Video Diffusion Model for UV Texture Synthesis To integrate multi-view image synthesis and UV texture mapping within unified framework, we adapt pre-trained transformer-based video diffusion model [4]. We treat the sequence of multi-view and UV frames as video, encoding each frame into the latent space using the VAE [40] associated with the video model. Unless otherwise stated, all subsequent operations are performed in this latent domain. At each diffusion timestep t, we construct noised token sequence: 3, (1) 1:4 are the noised multi-view image latents and is the noised UV texture latent. Following rectified flow 1:4, 1] and where matching [41], these latents are obtained by linearly interpolating between the clean latents S1 = [I 1 Gaussian noise samples 0 (0, I): St = [I 4, t], 2, 1, = tI 1 + (1 t)X (i {1, 2, 3, 4}), = tU 1 + (1 t)X 0 . (2) To provide 3D structural guidance and establish correspondence between frames, we condition the model on geometric cues (cgeo) derived from the input mesh, represented in both multi-view and UV domains. Unlike ISA4D [42], which employs normalized device coordinates (NDC) as an intermediary, our cgeo comprises global positions (i.e., global coordinates) and global normals. These global attributes provide consistent geometric information across both the multi-view and UV domains, enabling finer-grained guidance. Among these two conditions, as discussed in [18], global coordinates serve as coarse-grained indicators, conveying the overall outline of the target object, while global normals provide detailed cues to guide the generation of fine-grained results. To align with the latent tokens, these geometric features are also encoded into the latent space. Additionally, we incorporate optional textual guidance (ctext). 4 SeqTex: Generating Mesh Textures in Video Sequence For training, we employ rectified flow objective at each sampled timestep t: = EX 0,S1,cgeo,ctext,t (cid:2)u(St, cgeo, ctext, t; θ) t2(cid:3) , where the velocity target represents the direction from the noised latent to the clean latent. Its formulation is: 1 , . . . , 1 4 , 1 0 = [I 1 1 0 4 ]. (3) (4) At inference, the model iteratively denoises pure noise input to simultaneously generate coherent multi-view images and high-fidelity UV texture map in the latent space. These latent outputs are then projected back to the pixel space using the VAE decoder. 3.3 SeqTex Architecture Built upon pre-trained video diffusion transformer, we introduce the MUV Block, which is modified transformer block tailored for UV texture synthesis. It preserves the strong multi-view generation priors of the foundation model. Decoupled MV and UV Processing Branches. To synthesize high-quality multi-view (MV) images and precisely aligned UV texture map in single pass, it is crucial to bridge the domain gap between the MV space and the UV space. MV images preserve the spatial continuity of the 3D projection, whereas the UV space unwraps the surface, potentially scattering adjacent 3D points across the map. Sharing parameters, especially in normalization layers, can lead to feature interference and suboptimal learning. We therefore split the architecture into two dedicated streams: an MV branch and UV branch, which interact via the Geo Attention through cgeo within each MUV block. In the MV branch (Fig. 2(c)), we fine-tune only lightweight Low-Rank Adaptation (LoRA) modules while keeping the base video diffusion transformer frozen. This strategy preserves the powerful priors of the pre-trained model for multi-view synthesis and dramatically reduces computational overhead. The MV branch ingests only multi-view tokens, forming specialized network for generating high-quality, view-consistent images. To inject 3D geometric guidance for the MV branch, we condition on clean position and normal maps cgeo via Geo-attention: Geo-Attention(Q, K, V, cgeo) = Softmax( QK dk )V, = RoPE(Q) + cgeo, = RoPE(K) + cgeo, where Q, K, and are linear-projected query, key and value tokens within the attention module: = Linearq(tokenq), = Lineark(tokene), = Linearv(tokene). Note that both tokenq and tokene represent the MV tokens in this branch. (5) (6) (7) In the UV branch (Fig. 2(d)), we fully fine-tune the backbone rather than using LoRA, since UV texture synthesis differs substantially from the original image/video-generation task that the pre-trained model was optimized for. This full fine-tuning allows the network to adapt its weights to handle the unique challenges of generating UV textures. We also integrate Geo-Attention into the UV branch, mirroring the architecture used in the MV branch. The critical distinction lies in how we build the token streams: tokenq contains only the UV tokens, while tokene temporally concatenates both MV and UV tokens. By injecting geometric guidanceposition and normal mapsinto queries and keys, Geo-Attention steers UV tokens to focus on relevant regions in both multi-view and UV tokens. This mechanism allows UV tokens to selectively attend to geometrically aligned MV tokens, effectively narrowing the domain gap between image and UV spaces. As result, the model can transfer the strong spatial priors learned in the MV branch into accurate, coherent UV textures, ensuring that the generated UV map aligns precisely with the synthesized multi-views. Adaptive Token Resolution. To further reduce computation without sacrificing visual fidelity, we adopt an adaptive token resolution strategy. UV textures, where fine-grained detail is paramount, are processed at higher spatial resolution (HU WU ), while multi-view images are generated at lower resolution (HM WM ). By downsampling MV tokens, we substantially reduce computational cost, while the elevated resolution of the UV branch ensures that critical texture details are preserved. We also adapt our positional encodings for this mixed-resolution setup. In the temporal dimension, multi-view tokens are assigned positions 1-4, and the UV token receives position 5. For the spatial dimensions, RoPE frequencies retain the native scaling strategy of the pre-trained video model, which is interpolation scaling for CogVideoX [29] and extrapolation for Wan2.1 [4], to maximally preserve its learned priors. Further implementation details are provided in the supplementary material. 5 SeqTex: Generating Mesh Textures in Video Sequence 3.4 Multi-task Learning and Material Representation Multi-task Learning. Our framework supports training multiple tasks within unified sequence representation. We define two primary tasks: image-to-texture (img2tex) and geometry-to-multi-view (geo2mv). We adopt flexible conditioning strategy [43, 44] that assigns distinct noise levels to different frames based on the task. Frames are categorized as: Denoising Frames (DF): Frames the model must denoise, with noise level sampled from predefined schedule. Conditioning Frames (CF): Frames provided as known information, assigned minimal noise level. Nonsense Frames (NF): Frames irrelevant to the current task, assigned the maximum noise level (effectively pure noise). This scheme ensures unified operation for all tasks. The specific frame assignments for each task are detailed in Table 1. The geo2mv task enables the seamless integration of additional multi-view-only datasets, as the UV frame is treated as nonsense frame, thereby mitigating the issue of 3D data scarcity. Table 1: Frame type assignments and noise scheduling for different tasks. Task DF (Denoising) CF (Condition) NF (Nonsense) img2tex MV images (excl. Ii), UV map (U ) MV image Ii geo2mv All MV images (I1:4) None None UV map (U ) Material Representation. In recent texture generation research, two primary choices exist for material representation during training and generation: training data with lighting (baked-in illumination) and data without lighting (typically generating albedo, and potentially additional maps like roughness and metallicity). Given our dual training tasks img2tex and geo2mv we adopt distinct material representations for each task. For img2tex, we utilize lighting-free maps, specifically albedo maps. We posit that albedo data facilitates consistency across different viewpoints and potentially across different domains. Conversely, for geo2mv, we employ illuminated images as training data. This decision is driven by two key considerations: Compatibility with video foundation models: Natural video data inherently contains lighting. Using illuminated images helps mitigate model degradation by aligning with the data distribution these foundation models are trained on. Broader Dataset Compatibility: Illuminated images enable the use of wider range of multi-view datasets, including real-world scanned model datasets, and ensure compatibility with potential future datasets comprising pure video sequences."
        },
        {
            "title": "4 Experiments",
            "content": "We use the dataset from TEXGen [15] for training, which contains 120,400 textured meshes: 120,000 for the training set and 400 for the evaluation set. Unlike TEXGen, we generate diverse caption styles using Claude [45] based on multi-view renderings including detailed descriptions, concise summaries, and tags to comprehensively capture the various aspects of each model. Given the complexity and time-intensive nature of processing textured 3D data (e.g., UV parameterization and texture baking), we further curate multi-view (MV) dataset from multiple sources, including Objaverse-XL [46], DTC [47], RenderPeople [48], Vroid [49], and THuman [50], to support training via the geo2mv task. We apply filters to ensure high aesthetic quality in both geometry and texture, yielding approximately 80,000 3D models. For these models, we render only their geometry and RGB values in the pixel domain, without UV re-parameterization or texture baking. Notably, while the 3D texture dataset contains only albedo maps, we incorporate comprehensive material information (such as metallic and roughness) using Physically Based Rendering (PBR). This ensures that the images generated by the MV branch better align with the prior distribution of the video model. To balance performance and computational efficiency, we set the resolution to 512 512 pixels for MV and 1024 1024 pixels for UV. Further training details are provided in the supplementary material. 6 SeqTex: Generating Mesh Textures in Video Sequence 4.1 Image-Conditioned Texture Generation Following [15], our primary evaluations and comparisons focus on image-conditioned generation, where both text prompt and pose-aligned view image are provided as inputs. We compare our method against existing texture generation approaches, including TEXGen [15], TEXTure [25], Text2Tex [26], and Paint3D [17]. Figure 3: Qualitative comparison of texture generation methods. To ensure the generated textures align with the ground-truth, we adapt TEXTure, Text2Tex, and Paint3D to accept an additional image condition, following the methodology of TEXGen [15]. Qualitative comparisons are presented in Fig. 3. As an end-to-end texture generation method operating directly in the UV domain, our approach produces textures that align better with the input conditions and exhibit higher fidelity than those from TEXTure and Text2Tex. Our method typically generates textures with more intricate patterns compared to the oversmoothed results from Paint3D. Relative to TEXGen, our technique demonstrates better understanding of semantic information across different object parts and maintains improved consistency across views. For quantitative evaluation, we assess synthesis quality on validation set of 400 object instances. Following [15], we compute two established perceptual metricsFréchet Inception Distance (FID) and Kernel Inception Distance (KID) using ground-truth renderings as the reference. As shown in Table 2, our method achieves statistically significant improvements in synthesis quality over existing approaches, while exhibiting computational efficiency comparable to TEXGen under identical hardware configurations. 4.2 Text-Conditioned Texture Generation To integrate text-conditioned inputs into our image-based pipeline, we first generate image conditions from user-provided text prompts using generators like SDXL [51] or FLUX [52]. These generators, combined with ControlNet [53], process geometric cues (i.e., depth and normal maps from specific viewpoint) to produce high-quality, detailed single-view images. These synthesized images then serve as input conditions for our texture generation pipeline, which remains SeqTex: Generating Mesh Textures in Video Sequence Table 2: Quantitative results for image-conditioned generation. FID and KID (104) are evaluated between multi-view renderings and ground-truth images. Our method achieves state-of-the-art texture quality with competitive inference speed."
        },
        {
            "title": "Methods",
            "content": "FID() KID() Time() TEXTure [25] Text2Tex [26] Paint3D [17] TEXGen [15] Ours 48.31 49.85 43.55 34.53 30.27 48.00 47.38 25.73 11.94 1.21 80s 344s 95s 10s 12s unchanged. Visualizations in Fig. 4 demonstrate our methods exceptional generalization and condition-adherence capabilities. For the quantitative evaluation of text-conditioned generation, standard metrics like FID and KID are unsuitable, as they measure correspondence to ground-truth renderings. This is problematic when high-quality generated texture may legitimately differ from the ground truth. Therefore, we adopt the following evaluation protocol: We conduct user study, following TEXGen [15], to compare our results against other methods and collect user preference scores. Since no established metrics exist for this generative task, we employ Claude 3.5 Sonnet [45], Multimodal Large Language Model (MLLM), to provide objective scores based on texture renderings. Implementation details are in the supplementary material. The quantitative results are presented in Table 3. Table 3: Quantitative comparison of text-conditioned 3D texture generation. Our method achieves the highest user preference rate and MLLM score, outperforming previous approaches by substantial margin. Methods TEXTure Text2Tex Paint3D TEXGen Ours Preference (%) () MLLM Score () 2.15 63. 1.43 64.48 7.88 63.5 31.50 70.42 57.04 74.84 4.3 Ablation Studies In this section, we conduct extensive ablation studies to validate the key design choices in our framework. Effectiveness of Video Priors To demonstrate the importance of video priors from the pre-trained video foundation model, we train comparative model with an identical architecture but from randomly initialized weights. As shown in Table 4 and Fig. 5, the randomly initialized model fails to generate high-fidelity texture maps or sufficiently detailed MV images. We also observe that fine-tuning from pre-trained weights leads to faster convergence and superior optimization, whereas the randomly initialized model tends to converge to suboptimal local minima. Effectiveness of Joint MV-UV Modeling The unified modeling of MV and UV frames allows the UV frame to leverage video priors from the foundation model, resulting in high-fidelity texture generation. To validate this design, we train an alternative model that generates only UV texture maps using the UV branch, initialized from the same pre-trained weights. As evidenced by Fig. 5, the absence of MV guidance causes significant degradation in performance, particularly in capturing details and adhering to the prompt. Table 4: Quantitative ablation results for different experimental settings. To reduce the cost of full training, we use lower resolution setting (384 384 for MV and 768 768 for UV) for rapid experimentation. W/ Video Prior W/ UV Sep. Albedo 3D Data (cid:33) (cid:37) (cid:33) (cid:37) (cid:33) (cid:33) (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) (cid:33) 8 FID() KID() 44.62 35.62 41. 33.12 31.82 7.59 32.59 6.69 SeqTex: Generating Mesh Textures in Video Sequence Figure 4: Visualization of text-conditioned texture generation on the real-scan DTC dataset. Given an untextured mesh, text prompt, and corresponding image condition, our method creates textures that align well with the geometry. The 3D priors from the video foundation model ensure view consistency. SeqTex: Generating Mesh Textures in Video Sequence Figure 5: Visualization of ablation study results. Removing the video prior, joint MV-UV prediction, or the decoupled MV-UV branch leads to degraded texture fidelity, instruction-following ability, and loss of semantic or geometric details. Effectiveness of Decoupled MV-UV Branch Decoupling the MV and UV branches prevents interference between them, as they operate in fundamentally distinct representation domains. To assess this design choice, we trained model without decoupled branches, where MV and UV tokens are concatenated along the temporal dimension and processed through unified branch. The performance differences, quantified in Table 4, reveal potential interference between the MV and UV branches. Furthermore, Fig. 5 clearly shows that without decoupling, the model tends to lose details. Choice of Material Representation We ablate the choice of material representation for the img2tex task, with quantitative results summarized in Table 4. Training img2tex with illuminated (PBR) texture maps proves to be suboptimal. During inference, this configuration generates artifacts characterized by unnaturally dark regions. As anticipated, this occurs because illuminated (baked) textures inherently contain black regions in unlit areas (see Fig. 6). Consequently, the model learns to replicate these shadow artifacts, producing erroneous dark patches in novel contexts where such lighting conditions do not exist. Figure 6: Comparison of training with albedo vs. PBR texture maps. PBR texture maps contain baked-in shadows in occluded areas. model trained on such data learns to simulate these shadows, leading to incorrect artifacts during inference. Figure 7: Qualitative comparison of three training strategies: 3D-only, Split 3D, and Hybrid. The 3Donly strategy struggles with view consistency. Both Split 3D and Hybrid strategies improve multi-view coherence. Hybrid training, which incorporates additional PBR multiview data, further enhances the models understanding of lighting and shadow effects. Effectiveness of Multi-task Training & Dataset Composition We first clarify two related but distinct concepts: the geo2mv task and the externally curated MV dataset. Integrating our additional MV dataset via the geo2mv task enhances texture generation generalization. However, the geo2mv task itself does not strictly require this external dataset, as the multi-view renderings from our core 120k Objaverse models can also support this training. To systematically evaluate the efficacy of each component, we conduct controlled ablations under the following conditions: 3D-only: The model is fine-tuned exclusively on 3D data for the img2tex task. 10 SeqTex: Generating Mesh Textures in Video Sequence Split 3D: The 3D data is partitioned in 3:2 ratio (approximating the 120k:80k distribution of the main training) for concurrent img2tex and geo2mv training. Hybrid: The model is trained on the full 3D dataset and the external MV dataset for their respective tasks. All configurations started from base model pre-trained for 10k iterations on the img2tex task to ensure basic convergence while conserving computational resources. Figure 7 demonstrates two key findings: 1) Without the emphasis provided by the MV task (3D-only vs. Split 3D), the model struggles to maintain multi-view consistency. 2) Incorporating additional PBR MV data (Hybrid vs. Split 3D) improves the models understanding of lighting and shadow effects, which are absent in the albedo-only data."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we present SeqTex, native texture model that unlocks pretrained video foundation models for end-to-end UV texture map generation. SeqTex departs from traditional pipelines by reframing 3D texture synthesis as joint sequence modeling task over multi-view images and UV texture maps. This approach enables the direct transfer of rich, consistent image-space priors into the UV domain, addressing long-standing challenges of data scarcity and UV spatial discontinuity. Our architecture features decoupled MV-UV branch design for specialized representation learning, geometry-informed attention to ensure precise alignment between the image and UV domains, and adaptive token resolution to efficiently capture fine-grained texture details while maintaining computational efficiency. These innovations allow SeqTex to effectively utilize pretrained video priors without compromising model capacity or training stability. Extensive experiments demonstrate that SeqTex achieves state-of-the-art results on both imageand text-conditioned 3D texture generation tasks. The generated textures exhibit superior 3D consistency, accurate texture-geometry alignment, and high visual fidelity, while generalizing effectively to real-world scenarios. SeqTex establishes strong baseline for integrating vision foundation models into practical 3D content creation pipelines and opens new avenues for scalable and robust texture synthesis."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. [3] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [4] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [5] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets. CoRR, abs/2311.15127, 2023. [7] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. [8] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214, 2023. 11 SeqTex: Generating Mesh Textures in Video Sequence [9] Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, and Yan-Pei Cao. Triposr: Fast 3d object reconstruction from single image. arXiv preprint arXiv:2403.02151, 2024. [10] Yawar Siddiqui, Justus Thies, Fangchang Ma, Qi Shan, Matthias Nießner, and Angela Dai. Texturify: Generating textures on 3d shape surfaces. arXiv preprint arXiv:2204.02411, 2022. [11] Alexey Bokhovkin, Shubham Tulsiani, and Angela Dai. Mesh2tex: Generating mesh textures from image queries. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 89188928, 2023. [12] Xin Yu, Peng Dai, Wenbo Li, Lan Ma, Zhengzhe Liu, and Xiaojuan Qi. Texture generation on 3d meshes with point-uv diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 42064216, October 2023. [13] An-Chieh Cheng, Xueting Li, Sifei Liu, and Xiaolong Wang. TUVF: learning generalizable texture UV radiance fields. CoRR, abs/2305.03040, 2023. [14] Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. [15] Xin Yu, Ze Yuan, Yuan-Chen Guo, Ying-Tian Liu, Jianhui Liu, Yangguang Li, Yan-Pei Cao, Ding Liang, and Xiaojuan Qi. Texgen: generative diffusion model for mesh textures. ACM Transactions on Graphics (TOG), 43(6):114, 2024. [16] Zehuan Huang, Yuan-Chen Guo, Haoran Wang, Ran Yi, Lizhuang Ma, Yan-Pei Cao, and Lu Sheng. Mv-adapter: Multi-view consistent image generation made easy. arXiv preprint arXiv:2412.03632, 2024. [17] Xianfang Zeng, Xin Chen, Zhongqi Qi, Wen Liu, Zibo Zhao, Zhibin Wang, Bin Fu, Yong Liu, and Gang Yu. Paint3d: Paint anything 3d with lighting-less texture diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 42524262, 2024. [18] Raphael Bensadoun, Yanir Kleiman, Idan Azuri, Omri Harosh, Andrea Vedaldi, Natalia Neverova, and Oran Gafni. Meta 3d texturegen: Fast and consistent texture generation for 3d objects. arXiv preprint arXiv:2407.02430, 2024. [19] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [20] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. In CVPR, pages 1266312673. IEEE, 2023. [21] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. In ICCV, pages 2218922199. IEEE, 2023. [22] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 300309, 2023. [23] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv preprint arXiv:2305.16213, 2023. [24] Yu-Ying Yeh, Jia-Bin Huang, Changil Kim, Lei Xiao, Thu Nguyen-Phuoc, Numair Khan, Cheng Zhang, Manmohan Chandraker, Carl Marshall, Zhao Dong, et al. Texturedreamer: Image-guided texture synthesis through geometry-aware diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 43044314, 2024. [25] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3d shapes. In ACM SIGGRAPH 2023 conference proceedings, pages 111, 2023. [26] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Nießner. Text2tex: Text-driven texture synthesis via diffusion models. In ICCV, pages 1851218522. IEEE, 2023. [27] Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp, and Kangxue Yin. Texfusion: Synthesizing 3d textures with text-guided image diffusion models. In ICCV, pages 41464158. IEEE, 2023. [28] Xin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Song-Hai Zhang, and Xiaojuan Qi. Text-to-3d with classifier score distillation. arXiv preprint arXiv:2310.19415, 2023. [29] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 12 SeqTex: Generating Mesh Textures in Video Sequence [30] Weifeng Chen, Yatai Ji, Jie Wu, Hefeng Wu, Pan Xie, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-avideo: Controllable text-to-video generation with diffusion models. arXiv e-prints, pages arXiv2305, 2023. [31] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems, 36:75947611, 2023. [32] Mark YU, Wenbo Hu, Jinbo Xing, and Ying Shan. Trajectorycrafter: Redirecting camera trajectory for monocular videos via diffusion models. arXiv preprint arXiv:2503.05638, 2025. [33] Xin Yu, Tianyu Wang, Soo Ye Kim, Paul Guerrero, Xi Chen, Qing Liu, Zhe Lin, and Xiaojuan Qi. Objectmover: Generative object movement with video prior. arXiv preprint arXiv:2503.08037, 2025. [34] Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. Unireal: Universal image generation and editing via learning real-world dynamics. arXiv preprint arXiv:2412.07774, 2024. [35] Shaoteng Liu, Tianyu Wang, Jui-Hsien Wang, Qing Liu, Zhifei Zhang, Joon-Young Lee, Yijun Li, Bei Yu, Zhe Lin, Soo Ye Kim, et al. Generative video propagation. arXiv preprint arXiv:2412.19761, 2024. [36] Qi Zuo, Xiaodong Gu, Lingteng Qiu, Yuan Dong, Weihao Yuan, Rui Peng, Siyu Zhu, Liefeng Bo, Zilong Dong, Qixing Huang, et al. Videomv: Consistent multi-view generation based on large video generative model. 2024. [37] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitrii Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. SV3D: Novel multi-view synthesis and 3D generation from single image using latent video diffusion. In European Conference on Computer Vision (ECCV), 2024. [38] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. arXiv preprint arXiv:2409.02095, 2024. [39] Qihang Zhang, Shuangfei Zhai, Miguel Angel Bautista, Kevin Miao, Alexander Toshev, Joshua Susskind, and Jiatao Gu. World-consistent video diffusion with explicit 3d modeling. arXiv preprint arXiv:2412.01821, 2024. [40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [41] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [42] Ruizhi Shao, Yinghao Xu, Yujun Shen, Ceyuan Yang, Yang Zheng, Changan Chen, Yebin Liu, and Gordon Wetzstein. Interspatial attention for efficient 4d human video generation. arXiv preprint arXiv:2505.15800, 2025. [43] Zeqi Xiao, Yushi Lan, Yifan Zhou, Wenqi Ouyang, Shuai Yang, Yanhong Zeng, and Xingang Pan. Worldmem: Long-term consistent world simulation with memory. arXiv preprint arXiv:2504.12369, 2025. [44] Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2024. [45] Anthropic. Claude. https://claude.ai/, 2024. [46] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36:3579935813, 2023. [47] Zhao Dong, Ka Chen, Zhaoyang Lv, Hong-Xing Yu, Yunzhi Zhang, Cheng Zhang, Yufeng Zhu, Stephen Tian, Zhengqin Li, Geordie Moffatt, et al. Digital twin catalog: large-scale photorealistic 3d object digital twin dataset. arXiv preprint arXiv:2504.08541, 2025. [48] Renderpeople. https://renderpeople.com/3d-people, 2022. [49] Shuhong Chen, Kevin Zhang, Yichun Shi, Heng Wang, Yiheng Zhu, Guoxian Song, Sizhe An, Janus Kristjansson, Xiao Yang, and Matthias Zwicker. Panic-3d: Stylized single-view 3d reconstruction from portraits of anime characters. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [50] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qionghai Dai, and Yebin Liu. Function4d: Real-time human volumetric capture from very sparse consumer rgbd sensors. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR2021), June 2021. 13 SeqTex: Generating Mesh Textures in Video Sequence [51] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [52] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [53] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [54] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. arXiv preprint arXiv:2312.02696, 2023. [55] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: unified predictor-corrector framework for fast sampling of diffusion models. NeurIPS, 2023. 14 SeqTex: Generating Mesh Textures in Video Sequence"
        },
        {
            "title": "6 Supplementary Material",
            "content": "6.1 Additional Visualizations Additional visualization results are presented in Fig. 8. 6.2 Implementation Details During training, we do not employ logit-normal time sampling [41]; instead, we implement loss reweighting technique that assigns higher weights to intermediate noise timesteps and lower weights (potentially zero) to the initial and final timesteps. We use the AdamW optimizer with learning rate of 1 104, betas of 0.9 and 0.999, and weight decay of 0.01. The learning rate schedule includes 200-step warmup phase followed by cosine annealing. Our training setup consists of four nodes, each equipped with eight A800 GPUs. We use per-GPU batch size of 1 and gradient accumulation over 4 steps, resulting in an effective global batch size of 128. We note that classifier-free guidance (CFG) is ineffective for the img2tex task, which we attribute to the stronger conditioning provided by the image input compared to the text prompt. Exponential Moving Average (EMA) is applied following the methodology in EDM2 [54], with standard deviation of 0.05. Training is performed using bfloat16 (bf16) precision on the 1.3 billion parameter version of the Wan2.1 model. To balance performance and computational cost, we set the resolution to 512 512 for multi-view (MV) and 1024 1024 for UV. Based on insights from the Wan paper, we estimate the flow shift to be 5.0. For inference, we use the UniPC scheduler [55] with 30 steps to accelerate the process. Following [43], during both training and inference, kmax is set to 1000 to mask UV information for the geo2mv task, while kmin is set to 15 (out of 1000) to provide the image condition. The main experiment is conducted in two stages. The first stage focuses on UV generation, training the model solely on the img2tex task. The second stage introduces the geo2mv task and incorporates the new MV dataset. 6.3 Evaluation of Text-Conditioned Texture Generation As mentioned in the main paper, we use an MLLM score as metric to evaluate the performance of different methods. Specifically, we use Claude 3.5 Sonnet, with the prompt shown in Fig. 9. 6.4 Modified 3D RoPE To support different resolutions for the MV and UV inputs, as mentioned in the main text, we modify the standard 3D RoPE. The pseudo-code is provided in Alg. 1. In summary, we maintain the respective spatial resolutions of the inputs but retrieve the RoPE features for the UV map immediately following the time indices of the MV features. 6.5 Limitations Although we adopt VAE to encode geometry information for alignment with the RGB latent space, this compression inevitably introduces distortion. We find that the reconstructed geometry maps appear visually identical to the originals; however, quantitative analysis reveals non-negligible differences, which may impair the controllability of the geo attention mechanism, finally causing blurred details. These discrepancies highlight limitation in our current approach and indirectly justify the necessity of high-resolution texture synthesis for preserving fine-grained geometric features. We hypothesize that the root cause lies in the domain gap between RGB images and geometry maps in the VAEs training data. Due to time constraints, we leave more principled solution to this issue as future work. 15 SeqTex: Generating Mesh Textures in Video Sequence Figure 8: Top: An indoor scene where all objects are textured using SeqTex. Bottom: Close-up renderings of selected objects from the scene above. 16 SeqTex: Generating Mesh Textures in Video Sequence Figure 9: The prompt used to evaluate text-conditioned texture generation performance. Algorithm 1: 3D Rotary Position Embedding (RoPE) Generation Input: input_freqs, mv_shape, uv_shape, embed_dim Output: 3D RoPE embeddings (mv_frames, mv_height, mv_width) mv_shape; (uv_frames, uv_height, uv_width) uv_shape; d_part embed_dim/6; (freq_t, freq_h, freq_w) split(input_freqs, [d_part, d_part, d_part]); // Generate Multi-View RoPE components mv_ft reshape(freq_t[: mv_frames], (mv_frames, 1, 1, 1)); mv_ft expand(mv_ft, (mv_frames, mv_height, mv_width, 1)); mv_fh reshape(freq_h[: mv_height], (1, mv_height, 1, 1)); mv_fh expand(mv_fh, (mv_frames, mv_height, mv_width, 1)); mv_fw reshape(freq_w[: mv_width], (1, 1, mv_width, 1)); mv_fw expand(mv_fw, (mv_frames, mv_height, mv_width, 1)); // Generate UV RoPE components uv_ft reshape(freq_t[mv_frames : mv_frames + uv_frames], (uv_frames, 1, 1, 1)); uv_ft expand(uv_ft, (uv_frames, uv_height, uv_width, 1)); uv_fh reshape(freq_h[: uv_height], (1, uv_height, 1, 1)); uv_fh expand(uv_fh, (uv_frames, uv_height, uv_width, 1)); uv_fw reshape(freq_w[: uv_width], (1, 1, uv_width, 1)); uv_fw expand(uv_fw, (uv_frames, uv_height, uv_width, 1)); // Combine and reshape rope_mv concat([mv_ft, mv_fh, mv_fw], dim = 1); rope_mv reshape(rope_mv, (mv_frames mv_height mv_width, 1)); rope_uv concat([uv_ft, uv_fh, uv_fw], dim = 1); rope_uv reshape(rope_uv, (uv_frames uv_height uv_width, 1)); return concat([rope_mv, rope_uv], dim = 0);"
        }
    ],
    "affiliations": [
        "HKU",
        "VAST"
    ]
}