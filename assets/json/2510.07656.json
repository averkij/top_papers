{
    "paper_title": "MONKEY: Masking ON KEY-Value Activation Adapter for Personalization",
    "authors": [
        "James Baker"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Personalizing diffusion models allows users to generate new images that incorporate a given subject, allowing more control than a text prompt. These models often suffer somewhat when they end up just recreating the subject image, and ignoring the text prompt. We observe that one popular method for personalization, the IP-Adapter automatically generates masks that we definitively segment the subject from the background during inference. We propose to use this automatically generated mask on a second pass to mask the image tokens, thus restricting them to the subject, not the background, allowing the text prompt to attend to the rest of the image. For text prompts describing locations and places, this produces images that accurately depict the subject while definitively matching the prompt. We compare our method to a few other test time personalization methods, and find our method displays high prompt and source image alignment."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 6 5 6 7 0 . 0 1 5 2 : r MONKEY: Masking ON KEY-Value Activation Adapter for Personalization James Baker University of Maryland, Baltimore County jbaker15@umbc.edu"
        },
        {
            "title": "Abstract",
            "content": "Personalizing diffusion models allows users to generate new images that incorporate given subject, allowing more control than text prompt. These models often suffer somewhat when they end up just recreating the subject image, and ignoring the text prompt. We observe that one popular method for personalization, the IP-Adapter automatically generates masks that we definitively segment the subject from the background during inference. We propose to use this automatically generated mask on second pass to mask the image tokens, thus restricting them to the subject, not the background, allowing the text prompt to attend to the rest of the image. For text prompts describing locations and places, this produces images that accurately depict the subject while definitively matching the prompt. We compare our method to few other test time personalization methods, and find our method displays high prompt and source image alignment."
        },
        {
            "title": "1 Introduction",
            "content": "Diffusion models have become dominant paradigm for image generation (Ho et al., 2020; Rombach et al., 2022), competitive with autoregressive models (Parmar et al., 2018; Xiong et al., 2025), and largely superseding GANs (Goodfellow et al., 2014). However, despite their expressive power, end users often desire more control over who or what appears in the generated image. Personalization methods address this need by allowing model to incorporate specific person or object into new generations while retaining the flexibility of text-based conditioning and adhering to the text prompt. Existing personalization methods fall into the categories of test-time fine tuning, such as Dreambooth (Ruiz et al., 2023) and Textual Inversion (Gal et al., 2022), or adapter-based methods like IP-Adapter (Ye et al., 2023) and InstantID (Wang et al., 2024). Both categories often struggle to balance fidelity to the subject with faithfulness to the text prompt. In particular, when strong visual features of the subject dominate, the model tends to reproduce the original subject image rather than composing it naturally into new scenes or contexts. This results in limited generalization and weaker prompt alignment. We observe that one popular personalization methodIP-Adapterimplicitly performs segmentation. Specifically, the adapter produces attention maps in the intermediate UNet transformer blocks, which attend to semantic content features, that effectively separate the subject from the background. This implies that UNets combined with pretrained adapter already contain sufficient information to localize the subject without explicit supervision. Given this, we propose obtaining the implicit subject mask from IP-Adapter, which we apply in second pass to mask the image tokens so that they only attned to the subject region. This ensures that the text prompt can guide generation in the background, allowing higher text alignment while still preserving the subject features. Our contributions are: We identify how different IP Ada[ter tokens at different layers correspond to the subject and background of generated images We use this for two-stage inference process, where we use the first stage of inference to generate the mask, and then regenerate the image using the mask on the relevant image tokens to better align thr background with the text prompt. 1 Our method, which we call MONKEY Adapter, requires training no new weights or additional modules, and either outperforms other adapter-bvased personalization methods or exists on the pareto frontier of text and subject alignment."
        },
        {
            "title": "2.1 Personalization",
            "content": "Text-to-image diffusion models Croitoru et al. (2023); Chen et al. (2025) have been incredibly impressive in their ability to generate diverse, realistic images. While generally prompted on just text, diffusion model can also be prompted on particular instance of category. For example, user may want to generate pictures of specific cat or person, instead of generic cat or person. This is the task of Personalization. user should be able to supply few, or even single image of concept (such as face, object or person) and be able to reliably generate new images of that concept that match input prompts. We follow the dichotomy of (Zhang et al., 2024) of dividing personalization methods into test-time fine tuning or pretrained adaptation. The former group requires the diffusion model to be retrained on each new concept. Some examples are Dreambooth (Ruiz et al., 2023), Textual Inversion (Gal et al., 2022) and Imagic (Kawar et al., 2023). Pretrained adaptation does not require any new training, and concept images can be supplied to the diffusion model and personalized images immediately created at inference. Some examples include ID-Aligner (Chen et al., 2024), Face2Diffusion (Shiohara & Yamasaki, 2024), and most relevant to this work, IP-Adapter Ye et al. (2023). IP-Adapter embeds the concept image using CLIP (Radford et al., 2021) and learns an embedding for each layer of the UNet. The layer-wise embedding attends to the output of each layer using cross-attention. 2.2 Extracting Features from UNets Unets (Ronneberger et al., 2015) are common backbone model for diffusion (Ho et al., 2020), where they are trained to predict the noise from corrupted image. At inference time, they gradually reduce pure noise to an actual image. Like many large models, deeper layers of UNets correspond to higer-level semantic features (Haas et al., 2024; Xu et al., 2024; Frenkel et al., 2024; Schaerf et al., 2025). This has been used for personalization, such as learning new token for each layer for each new concept (Voynov et al., 2023; Agarwal et al., 2023). Different timesteps of the inference process are also more influential than others in determining the content features of the final images (Yu et al., 2023; Zhang et al., 2023b; Agarwal et al., 2023)."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Attention Maps The original IP-Adapter Ye et al. (2023) checkpoint, for each layer of the diffusion model, produces 4 tokens that are concatenated to the text tokens during the cross attention stage. During cross attention, the intermediate features ϕℓ are passed to each layer, and multiplied by the query matrix to get and then the text and image token embeddings are multiplied by the key and value matrices to get and . The final output of the attention layer is sof tmax( QKT ) component represents the degree of correspondence between the semantic embeddings of each token and each spatial location in the image. Given that deeper levels of UNets correspond to semantic features, we decided to plot the activations of the text and ip tokens of an intermediate layer of each part of the generated images. ) . The sof tmax( QKT We use the https://huggingface.co/SimianLuo/LCM_Dreamshaper_v7 checkpoint from huggingface, which is latent consistency (Luo et al., 2023) distillation of https://huggingface.co/Lykon/ dreamshaper-7. For the IP-Adapter, we used the standard https://huggingface.co/h94/IP-Adapter checkpoint. We used the second transformer in the first up-block layer named up_blocks.1.attentions. 1.transformer_blocks.0.attn2. For some visualization of the attention maps of other layers, refer to A. 2 Figure 1: Token Attention Maps 3 As can be seen in figures 1, we can observe that different keys attend to different regions of the image; ip1 attends to the actual subject itself, meanwhile ip2 and ip3 attend to the background."
        },
        {
            "title": "3.2 Inference",
            "content": "Diffusion models themselves are deterministic. The noise they denoise to generate realistic images are randomly drawn from gaussian normal distribution, but the same initial noise will produce the same output. Given that we can extract mask that captures the subject, second pass with the same initial random noise with, where we mask the ip tokens will recreate the foreground components of the original image that feature the subject, but the background will only be attended to by the text tokens. For the first image, we used 4 inference steps and averaged the masks from the second and third steps to generate the mask. Then for the final image, we used 8 inference steps and applied the mask to the third to sixth steps. We call our approach the MONKEY Adapter, standing for Masking ON KEY-Value Activation Adapter."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Models We compared our method to few other test-time personalization methods. Given that our method did not require training any new models or weights, we chose works that also did not train any new weights, those being FreeGraftor (Yao et al., 2025), RectifID (Sun et al., 2024), MASA (Cao et al., 2023) and TF-I2I (Hsiao et al., 2025), in addition to the baseline ip-adapter checkpoint with the ip token scale set to 0.5 and 1.0. 4.2 Data We used two image datasets: the Dreambooth dataset (Ruiz et al., 2023), consisting of objects and animals, and our own curated dataset of illustrations of characters from the card game Magic the Gathering, which featured mostly humanoids with various fantastical features and diverse appearances. For the text prompts, we used set of prompts relating to background and location such as on top of green grass with sunflowers around it and on cobblestone street, generated by asking ChatGPT (OpenAI, 2023). complete list can be found in A.1. 4.3 Results We used DINOv2 Identity (Oquab et al., 2024), CLIP Image and CLIP Text embedding similarity. The first two measure similarity between the source and generated images, and the third measures similarity between the text prompt and generated image. Scores for each dataset are shown in 2 and 1. We provide visual comparisons using some handpicked results in Model TF-I2I RectifID MASA FreeGraftor IP Base Monkey Magic Dreambooth CLIP-T DINO CLIP-I CLIP-T DINO CLIP-I 0.723 0.543 0.775 0.679 0.806 0.743 0.271 0.278 0.184 0.258 0.233 0. 0.091 0.126 0.299 0.29 0.565 0.493 0.247 0.273 0.186 0.253 0.282 0.318 0.484 0.468 0.614 0.535 0.824 0.763 0.413 0.14 0.511 0.471 0.621 0.529 Table 1: ScoresBest results are bolded, second best are italicized Our method exists on the pareto frontier of other methods. For the Dreambooth dataset, it offers the highest text alignment and the third-best or second-best image alignment, using the CLIP-I and DINO metrics, respectively. For the magic dataset, it offered the second best text and image and text alignment. 4 (a) Dreambooth dataset (b) Magic dataset Figure 2: CLIP Image vs CLIP Text similarities for each method"
        },
        {
            "title": "5 Conclusion",
            "content": "We presented simple inference-time masking strategy that improves the balance between subject image alignment and text prompt alignment in adapter-based personalization of diffusion models. By leveraging the implicit subject masks already present in IP-Adapter activations, our method enhances compositional control without training any new weights or retraining pre-existing weights. Future work can extend this approach toward multi-subject personalization and combining MONKEY Adapter with complementary personalization techniques such as ControlNet (Zhang et al., 2023a)."
        },
        {
            "title": "References",
            "content": "Aishwarya Agarwal, Srikrishna Karanam, Tripti Shukla, and Balaji Vasan Srinivasan. An image is worth multiple words: Multi-attribute inversion for constrained text-to-image synthesis, 2023. URL https: //arxiv.org/abs/2311.11919. Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuningfree mutual self-attention control for consistent image synthesis and editing, 2023. URL https://arxiv. org/abs/2304.08465. Hang Chen, Qian Xiang, Jiaxin Hu, Meilin Ye, Chao Yu, Hao Cheng, and Lei Zhang. Comprehensive exploration of diffusion models in image generation: survey. Artificial Intelligence Review, 58(4):99, 2025. Weifeng Chen, Jiacheng Zhang, Jie Wu, Hefeng Wu, Xuefeng Xiao, and Liang Lin. Id-aligner: Enhancing identity-preserving text-to-image generation with reward feedback learning, 2024. URL https://arxiv. org/abs/2404.15449. Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(9):1085010869, September 2023. ISSN 1939-3539. doi: 10.1109/tpami.2023.3261988. URL http://dx.doi.org/10.1109/TPAMI. 2023.3261988. Yarden Frenkel, Yael Vinker, Ariel Shamir, and Daniel Cohen-Or. Implicit style-content separation using b-lora, 2024. URL https://arxiv.org/abs/2403.14572. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-to-image generation using textual inversion, 2022. URL https://arxiv.org/abs/2208.01618. 5 Table 2: Image Comparisons. Prompts used (from left to right): on top of green grass with sunflowers around it, on cobblestone street, with wheat field in the background, on the beach, on top of pink fabric, in the jungle, and on top of purple rug in the forest. The top image is the source image, followed by the images generated by (from second to top to bottom), TF-I2I, RectifID, MASA, FreeGraftor, Baseline IP-Adapter, and MONKEY (ours) Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014. URL https://arxiv.org/abs/ 1406.2661. René Haas, Inbar Huberman-Spiegelglas, Rotem Mulayoff, Stella Graßhof, Sami S. Brandt, and Tomer Michaeli. Discovering interpretable directions in the semantic latent space of diffusion models, 2024. URL https://arxiv.org/abs/2303.11073. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. URL https: //arxiv.org/abs/2006.11239. Teng-Fang Hsiao, Bo-Kai Ruan, Yi-Lun Wu, Tzu-Ling Lin, and Hong-Han Shuai. Tf-ti2i: Training-free textand-image-to-image generation via multi-modal implicit-context learning in text-to-image models, 2025. URL https://arxiv.org/abs/2503.15283. 6 Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models, 2023. URL https://arxiv.org/abs/ 2210.09276. Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference, 2023. URL https://arxiv.org/abs/2310.04378. OpenAI. Gpt-4 technical report. https://cdn.openai.com/papers/gpt-4.pdf, 2023. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2024. URL https://arxiv.org/abs/2304.07193. Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer, 2018. URL https://arxiv.org/abs/1802.05751. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2022. URL https://arxiv.org/abs/2112.10752. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation, 2015. URL https://arxiv.org/abs/1505.04597. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation, 2023. URL https: //arxiv.org/abs/2208.12242. Ludovica Schaerf, Andrea Alfarano, Fabrizio Silvestri, and Leonardo Impett. Training-free style and content transfer by leveraging u-net skip connections in stable diffusion, 2025. URL https://arxiv.org/abs/ 2501.14524. Kaede Shiohara and Toshihiko Yamasaki. Face2diffusion for fast and editable face personalization, 2024. URL https://arxiv.org/abs/2403.05094. Zhicheng Sun, Zhenhao Yang, Yang Jin, Haozhe Chi, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Yang Song, Kun Gai, and Yadong Mu. Rectifid: Personalizing rectified flow with anchored classifier guidance, 2024. URL https://arxiv.org/abs/2405.14677. Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. P+: Extended textual conditioning in text-to-image generation, 2023. URL https://arxiv.org/abs/2303.09522. Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds, 2024. URL https://arxiv.org/abs/2401.07519. Jing Xiong, Gongye Liu, Lun Huang, Chengyue Wu, Taiqiang Wu, Yao Mu, Yuan Yao, Hui Shen, Zhongwei Wan, Jinfa Huang, Chaofan Tao, Shen Yan, Huaxiu Yao, Lingpeng Kong, Hongxia Yang, Mi Zhang, Guillermo Sapiro, Jiebo Luo, Ping Luo, and Ngai Wong. Autoregressive models in vision: survey, 2025. URL https://arxiv.org/abs/2411.05902. Youcan Xu, Zhen Wang, Jun Xiao, Wei Liu, and Long Chen. Freetuner: Any subject in any style with training-free diffusion, 2024. URL https://arxiv.org/abs/2405.14201. 7 Zebin Yao, Lei Ren, Huixing Jiang, Chen Wei, Xiaojie Wang, Ruifan Li, and Fangxiang Feng. Freegraftor: Training-free cross-image feature grafting for subject-driven text-to-image generation, 2025. URL https: //arxiv.org/abs/2504.15958. Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models, 2023. URL https://arxiv.org/abs/2308.06721. Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and Jian Zhang. Freedom: Training-free energyguided conditional diffusion model, 2023. URL https://arxiv.org/abs/2303.09833. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023a. URL https://arxiv.org/abs/2302.05543. Xulu Zhang, Xiaoyong Wei, Wentao Hu, Jinlin Wu, Jiaxin Wu, Wengyu Zhang, Zhaoxiang Zhang, Zhen Lei, and Qing Li. survey on personalized content synthesis with diffusion models. arXiv preprint arXiv:2405.05538, 2024. Yuxin Zhang, Weiming Dong, Fan Tang, Nisha Huang, Haibin Huang, Chongyang Ma, Tong-Yee Lee, Oliver Deussen, and Changsheng Xu. Prospect: Prompt spectrum for attribute-aware personalization of diffusion models, 2023b. URL https://arxiv.org/abs/2305.16225."
        },
        {
            "title": "A Appendix",
            "content": "You may include other additional sections here. A.1 Text Prompts complete list of all text prompts used: in the jungle in the snow on the beach on cobblestone street on top of pink fabric on top of wooden floor with city in the background with mountain in the background with blue house in the background on top of purple rug in forest with wheat field in the background with tree and autumn leaves in the background with the Eiffel Tower in the background floating on top of water floating in an ocean of milk on top of green grass with sunflowers around it 8 on top of mirror on top of the sidewalk in crowded street on top of dirt road on top of white rug A.2 Additional IP Attention Maps 9 Figure 3: first transformer in the first up-block layer 10 Figure 4: second transformer in the first up-block layer Figure 5: first transformer in the second up-block layer 12 Figure 6: first transformer in the mid-block layer"
        }
    ],
    "affiliations": [
        "University of Maryland, Baltimore County"
    ]
}