{
    "paper_title": "R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO",
    "authors": [
        "Huanjin Yao",
        "Qixiang Yin",
        "Jingyi Zhang",
        "Min Yang",
        "Yibo Wang",
        "Wenhao Wu",
        "Fei Su",
        "Li Shen",
        "Minghui Qiu",
        "Dacheng Tao",
        "Jiaxing Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this work, we aim to incentivize the reasoning ability of Multimodal Large Language Models (MLLMs) via reinforcement learning (RL) and develop an effective approach that mitigates the sparse reward and advantage vanishing issues during RL. To this end, we propose Share-GRPO, a novel RL approach that tackle these issues by exploring and sharing diverse reasoning trajectories over expanded question space. Specifically, Share-GRPO first expands the question space for a given question via data transformation techniques, and then encourages MLLM to effectively explore diverse reasoning trajectories over the expanded question space and shares the discovered reasoning trajectories across the expanded questions during RL. In addition, Share-GRPO also shares reward information during advantage computation, which estimates solution advantages hierarchically across and within question variants, allowing more accurate estimation of relative advantages and improving the stability of policy training. Extensive evaluations over six widely-used reasoning benchmarks showcase the superior performance of our method. Code will be available at https://github.com/HJYao00/R1-ShareVL."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 3 7 6 6 1 . 5 0 5 2 : r R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO Huanjin Yao2,3*, Qixiang Yin4*, Jingyi Zhang1, Min Yang2, Yibo Wang3, Wenhao Wu5 Fei Su4, Li Shen1, Minghui Qiu2, Dacheng Tao1, Jiaxing Huang1(cid:12) 1Nanyang Technological University 2ByteDance 4Beijing University of Posts and Telecommunications Equal Contribution (cid:12) Corresponding Author 3Tsinghua University 5The University of Sydney"
        },
        {
            "title": "Abstract",
            "content": "In this work, we aim to incentivize the reasoning ability of Multimodal Large Language Models (MLLMs) via reinforcement learning (RL) and develop an effective approach that mitigates the sparse reward and advantage vanishing issues during RL. To this end, we propose Share-GRPO, novel RL approach that tackle these issues by exploring and sharing diverse reasoning trajectories over expanded question space. Specifically, Share-GRPO first expands the question space for given question via data transformation techniques, and then encourages MLLM to effectively explore diverse reasoning trajectories over the expanded question space and shares the discovered reasoning trajectories across the expanded questions during RL. In addition, Share-GRPO also shares reward information during advantage computation, which estimates solution advantages hierarchically across and within question variants, allowing more accurate estimation of relative advantages and improving the stability of policy training. Extensive evaluations over six widely-used reasoning benchmarks showcase the superior performance of our method. Code will be available at https://github.com/HJYao00/R1-ShareVL."
        },
        {
            "title": "Introduction",
            "content": "The recent success of Reinforcement Learning (RL) in Large Language Models (LLMs), such as Kimi-K1.5 [1] and DeepSeek-R1 [2], shows its promise in incentivizing models long-chain reasoning capability, enabling LLMs to tackle complex tasks such as mathematical and scientific reasoning. The core design of these advances (e.g., GRPO [3] in Deepseek-R1) lies in online reinforcement learning without the need of reward models, which encourages an LLM to generate group of reasoning paths and iteratively refine its reasoning process with group relative advantage estimation mechanism based on rule-based reward functions. Typically, simple reward strategy is adopted: reasoning paths leading to correct answers receive higher rewards, while those leading to incorrect answers receive lower ones, where the model is optimized via the group relative advantages estimated from the rewards. Inspired by these advancements, we aim to develop simple and effective reinforcement learning method for Multimodal LLMs (MLLMs) to incentivize their long-chain reasoning ability. simple way is to directly apply these LLM online reinforcement learning methods like GRPO on MLLMs. However, we empirically observe that directly applying GRPO on MLLMs suffers from sparse reward and advantage vanishing issues, leading to degraded performance in enhancing MLLMs reasoning capability [4, 5, 6]: (1) Sparse reward: Most current MLLMs, especially smaller ones, exhibit very limited long-chain reasoning capability. As result, only few generated reasoning paths receive positive rewards, Preprint. Figure 1: (a) Share-GRPO expands the question space via semantically consistent transformations, and then explores diverse reasoning trajectories from different question variants and shares the discovered trajectories among them. (b) Share-GRPO provides denser rewards and higher valid advantage ratios compared to GRPO, demonstrating its effectiveness in mitigating sparse reward and advantage vanishing issues. (c) Share-GRPO outperforms the baseline and other SOTA RL-based reasoning MLLMs on both mathematical and general reasoning benchmarks. especially on challenging questions and particularly during the early stage of training. This leads to sparse rewarding, inefficient exploration and instable training in GRPO-like methods. (2) Advantage vanishing: GRPO-like methods compute relative advantages by comparing the rewards of group of responses sampled from given question, leading to advantage vanishing when receiving homogeneous responses. Specifically, along reinforcement learning process, the model tends to gradually predict similar and all correct responses for well-learned questions, and similar and all incorrect responses for poor-learned questions. In this way, the relative advantages tend to approach zero when the group of responses become more homogeneous, and collapse to zero when all responses receive identical rewards (e.g., all correct or all incorrect), resulting ineffective reinforcement learning. Motivated by these observations, we propose Share-GRPO, novel approach that introduces the concept of information sharing into MLLM reinforcement learning to mitigate sparse reward and advantage vanishing issues. The core idea of Share-GRPO lies in exploring and sharing diverse reasoning trajectories over expanded question space as shown in Fig. 1 (a). Specifically, ShareGRPO first expands the question space for given question via data transformation techniques, and then encourages MLLM to effectively explore diverse reasoning trajectories over the expanded question space and shares the discovered reasoning trajectories across the expanded questions during reinforcement learning. In this way, each expanded question variant can both contribute and benefit from the reasoning trajectories generated by others in the expand question space, allowing the model to jointly explore and learn from shared solution space across expanded questions. In addition, Share-GRPO also shares reward information during advantage computation, which estimates solution advantages hierarchically across and within question variant, allowing more accurate estimation of relative advantages and improving the stability of policy training. Specifically, we estimate advantages at two levels: local level, which consists of responses generated from each individual question variant, and global level, which aggregates responses across all variants of the same seed question. This hierarchical advantage estimation enables more robust and fine-grained relative advantage computation, where the local level captures intra-variant structure and variance while the global level exploits cross-variant diversity and complementarity and stabilizes reward signals. In this way, Share-GRPO effectively mitigates the sparse reward and advantage vanishing issues: (1) Share-GRPO expands the question space and enables more diverse solution space for each given question, which effectively increases the likelihood of generating successful reasoning response and thus mitigates the sparse rewarding issue as illustrated in the left curve of Fig. 1 (b). (2) Share-GRPO allows the model to explore diverse reasoning trajectories from the expanded question space and 2 shares the discovered reasoning trajectories, ultimately mitigating the advantage vanishing issue effectively as illustrated in the right curve in Fig. 1 (b). (3) Share-GRPO estimates solution advantages hierarchically across and within question variant, which enables more accurate estimation of relative advantages and stable reinforcement learning process. In summary, the main contributions of this work are summarized as follows: First, we introduce the concept of information sharing into MLLM reinforcement learning, and propose Share-GRPO which explores and shares diverse reasoning trajectories over expanded question space, effectively mitigating the sparse reward and advantage vanishing issues. To the best of our knowledge, this is the first work that explores information sharing for MLLM reasoning reinforcement learning. Second, we design hierarchical advantage estimation method by sharing reward information, which estimates solution advantages hierarchically across and within question variant, allowing accurate and robust advantage estimation. Third, extensive experiments on 6 MLLM reasoning benchmarks demonstrate the superiority of our proposed methods as illustrated in Fig. 1 (c)."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Multimodal Large Language Model Multimodal Large Language Models (MLLMs) [7, 8, 9, 10, 11, 12, 13, 14, 15, 16] demonstrate outstanding performance in semantic understanding of cross-domain visual content and multimodal reasoning. Early research on MLLMs primarily focused on text-image alignment and the integration of multiple modalities [17, 18, 19, 20, 21]. Subsequently, models like GPT-4V [22] achieved breakthroughs in cross-modal understanding through multimodal instruction fine-tuning, enabling them to support simple tasks such as image captioning, visual question answering and OCR. More complex tasks, such as mathematical reasoning, document understanding, etc., require MLLMs to be able to perform complex logical deductions. For MLLM reasoning, models such as MultimodalCoT [23] and LLaVA-CoT [24] employ chain-of-thought (CoT) reasoning, breaking down the multimodal reasoning process into step-by-step inference steps while leveraging multimodal data to improve the models reasoning capabilities. Additionally, Mulberry [25] proposes CoMCTS to generate effective reasoning paths through multi-model collaboration. Different from these studies, this work focuses on reinforcement learning to improve MLLM reasoning capability. 2.2 Reinforcement Learning for Multimodal Large Language Model Reasoning Reinforcement learning has become an essential technology for enhancing the capabilities of MLLMs. Early research primarily focused on Reinforcement Learning from Human Feedback(RLHF) [26, 27, 28, 29], which aligns the outputs of multimodal models with human preferences by incorporating human feedback signals. Recently, DeepSeek-R1 [2] utilizes simple rule-based reward function to provide effective and reliable reward signals during the RL process. This indicates that the Group Relative Policy Optimization (GRPO) with result-level rewards effectively enhances the reasoning ability of LLMs. In the multimodal domain, researchers have begun exploring the use of RL to enhance the visual reasoning capabilities of MLLMs. Recent works, such as Vision-R1 [30] and MM-Eureka [5] have open-sourced large-scale SFT cold start data and RL data. R1-V [31], ReasonRFT [32], R1-VL [4] and other methods [33, 34, 35, 36, 37] have designed various rule-based reward functions to enhance the reasoning abilities of MLLMs, such as geometric understanding and spatial perception. Unlike these methods, our ShareGRPO explores information sharing for MLLM reasoning reinforcement learning to mitigate sparse reward and advantage vanishing issues. 2.3 Information Sharing in Deep Learning Information sharing is key strategy in deep learning, enabling more effective learning through the exchange of signals across modalities, tasks, or hierarchical model components. In multimodal learning, models such as ViLBERT [38] and LXMERT [39] employ cross-modal attention to achieve fine-grained information fusion between vision and language streams. In contrastive learning (e.g., SimCLR [40], MoCo [41]), shared representations across augmented views enhance feature robustness. This concept extends to reinforcement learning, especially in multi-task and multi-agent settings, where information sharing improves sample efficiency and mitigates sparse rewards. Methods like Distral [42] and PopArt [43] promote shared policy structures, while agents 3 in multi-agent RL benefit from shared value functions or communication protocols [44, 45]. [46] further demonstrate that shared representations enhance generalization in multi-task RL. Unlike prior work, we introduce information sharing into MLLM reasoning reinforcement learning to mitigate sparse rewards and advantage vanishing for more effective reasoning learning."
        },
        {
            "title": "3 Method",
            "content": "This section first provides the preliminary of Group Relative Policy Optimization (GRPO), and then presents the proposed Share-GRPO that introduces the concept of information sharing into MLLM reinforcement learning. Further details are elaborated in the subsequent subsections. 3.1 Preliminary Group Relative Policy Optimization (GRPO). GRPO [3] is variant of Proximal Policy Optimization (PPO) [47], designed to enhance the performance of LLMs on complex reasoning tasks, such as mathematical and scientific reasoning. Starting with pretrained MLLM to be optimized, GRPO first uses it to initialize policy model πθ and reference model πold. For given image-text pair (I, ), the reference policy model πθold generates set of responses {o1, o2, ..., oG}. group-based reward function then computes the corresponding rewards {R1, R2, ..., RG}, which are subsequently used to estimate the advantage ˆAi for each response relative to the group: ˆAi = Ri mean (cid:0){Ri}G i=1 (cid:1) std (cid:0){Ri}G i= (cid:1) . (1) Similar to PPO, GRPO employs clipped objective with KL penalty term: JGRPO(θ) = E(I,T )pD,oπθold (I,T ) (cid:34) 1 (cid:16) πθ(oi I, ) πθold (oi I, ) πθ(oi I, ) πθold(oi I, ) , 1 ϵ, 1 + ϵ ˆAi, clip (cid:88) min (cid:32) i= (cid:17) ˆAi βDKL (πθπref ) (cid:33)(cid:35) . (2) Sparse Reward and Advantage Vanishing Issues. Despite the effectiveness of GRPO, it generally faces two challenges when applied to MLLMs: the sparse reward issue and the advantage vanishing issue. Sparse rewarding arises due to the limited reasoning ability of current MLLMs, where only few reasoning paths receive positive rewards, leading to inefficient exploration and instable training. To alleviate this, prior work such as R1-VL [4] introduces step-wise reward signals to provide dense rewards throughout the reasoning process. Advantage vanishing occurs when MLLMs generate homogeneous responses for the same question and receive identical rewards, causing the relative advantages to collapse to zero and resulting in ineffective reinforcement learning. To tackle this issue, VL-Rethinker [6] and Skywork R1 [48] select the samples with large magnitudes of advantages and reuse them in RL process, while MM-Eureka [5] employs an online filtering strategy to remove the samples with zero advantage. Different from the prior works, our Share-GRPO effectively addresses both of these two challenges by exploring and sharing diverse reasoning trajectories over expanded question space, therefore encouraging reward diversity and stable policy optimization. 3.2 Share-GRPO We propose Share-GRPO, novel online MLLM reinforcement learning framework that mitigates the sparse reward and advantage vanishing issues via exploring and sharing diverse reasoning trajectories over expanded question space. Specifically, for given question, Share-GRPO first applies semantically consistent transformation to generate set of varied but semantically equivalent questions, thereby expanding the question space. It then encourages the MLLM to explore diverse reasoning paths over the expanded question space and facilitates the sharing of discovered reasoning trajectories and their rewards across the expanded questions during the reinforcement learning process, as illustrated in Fig. 2. 4 Figure 2: Overview of the proposed Share-GRPO. For given question, Share-GRPO first applies semantically consistent transformation to generate set of varied but semantically equivalent questions, thereby expanding the question space. It then encourages the MLLM to explore diverse reasoning paths over the expanded question space and facilitates the sharing of discovered reasoning trajectories and their rewards across the expanded questions during the reinforcement learning process. 3.2.1 Reasoning Space Expansion Question Space Expansion. To expand the question space for given question, we introduce Semantically Consistent Transformation (SCT) which generates group of question variant = {Q1, Q2, ..., Qm} for each given question Qori = {Tori, Iori}. Specifically, we propose two types of transformation techniques, i.e., offline textual SCT and online multimodal SCT, for more diverse, comprehensive and flexible question space expansion. (1) Offline Textual Semantically Consistent Transformation. Prior to online reinforce learning, we first employ offline textual SCT ϕ() to rewrite the textual prompt Tori for each give question. Specifically, we prompt GPT-4o to generate semantically consistent variants, resulting in an expanded question set. The textual prompts of the generated variants differ from that of the original question Tori in syntactic structure and lexical expressions, while preserving the original intent and the corresponding correct answer: Qoffline = {ϕ(Tori), Iori}. (3) (2) Online Multimodal Semantically Consistent Transformation. During online reinforcement learning, we introduce multimodal SCT strategy to further expand the question space on the fly. Given an image Iori in the input question, we apply visual transformations ψ() to alter its visual content. Specifically, we carefully select transformations (e.g., rotation, noise injection) that preserve critical visual cues necessary for reasoning, and avoid transformations (e.g., cropping, color distortion) that may disrupt key information. Each image undergoes one randomly selected transformation with probability p. In addition, to mitigate the potential semantic inconsistencies between the visual and textual inputs after visual changes, we perform manual textual transformation τ that appends transformationspecific prompt to the corresponding textual prompt, providing contextual guidance aligned with the visual modification: Qonline = {τ (ϕ(Tori)), ψ(Iori)}. (4) Solution Space Expansion. With the expanded question space = {Q1, Q2, ..., Qm}, Share-GRPO enables to explore diverse reasoning trajectories in an enlarged solution space for each given question. Specifically, for each question Qi Q, the policy model πθ generates candidate reasoning responses, resulting in an expanded response set: = {{oQ1 , ...oQm }, ..., {oQm 1 , ..., oQ1 }}. 3.2.2 Shared Advantage Estimation With the expanded reasoning space, Share-GRPO shares reward information during advantage computation, which estimates reasoning trajectory advantages hierarchically across and within question variant. 5 Following GRPO [3], we adopt rule-based reward functions to compute the reward for each generated reasoning trajectory, i.e., = {{rQ1 }}. Specifically, we adopt an outcome-level accuracy reward, which assigns higher rewards to reasoning paths that lead to correct answers and lower rewards to those leading to incorrect ones. In addition, we employ format reward that encourages the reasoning trajectory to follow detailed step-by-step process before providing the final answer. }, ..., {rQm 1 , ..., rQ , ...rQm 1 With the computed rewards R, we propose hierarchical advantage estimation approach that computes advantage at two levels: global level, which aggregates responses across all variants of the same original question; and local level, which considers responses generated from each individual question variant. (1) Global-level Advantage Estimation. We first estimate the advantage from global perspective, where the relative advantage is computed using the rewards obtained from all question variants = {Q1, Q2, ..., Qm}: ˆAglobal i,j,k ="
        },
        {
            "title": "RQj",
            "content": "i mean (cid:16) {{rQ1 1 , ..., rQ1 }, ..., {rQm (cid:16) std {{rQ1 1 , ..., rQ1 }, ..., {rQm , ...rQm }} (cid:17) , ...rQm }} (cid:17) . (5) (2) Local-level Advantage Estimation. We also estimate the advantage at local level, where the relative advantage is computed within the responses generated from each individual question variant Qj Q. Specifically, for each question variant Qj, the local advantage is estimated as follows: ˆAlocal i,j,k = RQj mean {rQj std (cid:16) (cid:16) {rQj 1 , ...rQj } (cid:17) 1 , ...rQj } (cid:17) . (6) With the global-level advantage and local-level advantage estimated via Eqs. 5 and 6, we can obtain the final advantage as follow: ˆAhier i,j,k = (cid:40) ˆAglobal i,j,k + ˆAlocal i,j,k, ˆAglobal i,j,k , = k, = k, (7) where the local advantage ˆAlocal i,j,k is only computed when the responses are generated from the same question variant, i.e., when = k. By incorporating hierarchical advantage estimation, Share-GRPO achieves more accurate relative advantage computation, leading to more stable and effective policy training. 3.2.3 Shared Policy Optimization With the expanded reasoning space and the shared advantage estimation, Share-GRPO enables to explore and share diverse reasoning trajectories and allows more accurate advantage estimation for each given question. Then, we optimize policy model πθ by sharing diverse reasoning trajectories = {{oQ1 }} across question variants = {Q1, Q2, ..., Qm}: }, ..., {oQm 1 , ..., oQ1 , ...oQm 1 L(θ) = E(Q)pD,oπθold (Q) (cid:34) (cid:88) (cid:88) (cid:32) min 1 1 m2 i=1 j=1 k=1 πθ(oQj πθold (oQj Qk) Qk) ˆAhier i,j,k, clip (cid:16) πθ(oQj πθold (oQj Qk) Qk) , 1 ϵ, 1 + ϵ (cid:17) ˆAhier i,j,k (cid:33)(cid:35) . (8)"
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we first provide implementation details in Sec. 4.1, and then present main results in Sec. 4.2 that demonstrate the effectiveness of Share-GRPO. In Sec. 4.3, we conduct comprehensive ablation studies to examine the impact of each design in Share-GRPO. Sec. 4.4 provides more discussion and analysis of Share-GRPO. More details are elaborated in the subsequent subsections. 6 Table 1: Main Results. To examine the effectiveness of Share-GRPO, we compare our R1-ShareVL which is trained by Share-GRPO without cold-start supervised fine-tuning against SOTAs across multiple reasoning tasks, including both domain-specific and general-purpose tasks. denotes evaluation on official weights using VLMEvalKit [51]. Model MathVista MMStar MMMU MathVerse MathVision AI2D Avg. GPT-4o[52] Claude3.7-Sonnet[53] Kimi1.5[1] LLaVA-Reasoner-8B [54] LLaVA-CoT-11B[24] Mulberry-7B[25] Qwen2.5-VL-7B [55] (Base Model) X-REASONER-7B [56] R1-Onevision-7B[33] Vision-R1-7B[30] OpenVLThinker-7B[35] MM-Eureka-7B[5] ThinkLite-7B [57] R1-ShareVL-7B 63.8 66.8 70.1 50.6 54.8 63.1 68.2 69.0 64.1 73.5 70.2 73.0 74. 75.4 65.1 54.0 57.6 61.3 63.9 64.3 63.2 65.1 63.7 67.0 70.7 71.8 68. 40.0 55.0 58.6 56.4 54.2 51.9 55.3 53.1 58.1 Qwen2.5-VL-32B [55] (Base Model) MM-Eureka-32B[5] R1-ShareVL-32B Scaling to Larger Models 74.7 74.8 77.6 69.5 67.3 70.2 70.0 64.6 70. 50.8 52.0 49.2 47.1 52.4 47.9 50.3 52.2 52.8 49.9 56.5 59. 30.4 41.3 31.0 25.1 29.6 29.9 29.4 29.6 26.9 29.9 29.5 38.4 34.4 40. 84.9 78.5 78.7 60.9 83.9 58.1 84.2 59.7 82.7 57.6 84.1 59.1 59.3 83.0 84.5 61.2 84.6 64.5 85.4 63.8 86.2 67. 4.1 Implementation Details In this work, we adopt Qwen2.5-VL-7B and Qwen2.5-VL-32B [49] as our base models. For training data, we randomly sample 52K multimodal data from MM-Eureka [5]. Model optimization is carried out using EasyR1 [50] codebase, with training conducted on 8 NVIDIA H100 GPUs for the 7B model and 32 H100 GPUs for the 32B model. For the rollout parameter, we use question variant of 2, sample number of 6 per question, and probability of 0.3. For RLrelated hyperparameters, we use global batch size of 128, rollout batch size of 512, rollout temperature of 0.7, and learning rate of 1e-6. 4.2 Main Results To comprehensively examine the effectiveness of our proposed Share-GRPO, we conduct experiments on models of different sizes (i.e., 7B and 32B). Notably, unlike prior studies [4, 33, 30], we do not involve an additional cold-start stage with supervised fine-tuning. As shown in Table 1, we provide an extensive comparison against state-of-the-art models across 6 widely used and challenging benchmarks, covering diverse range of reasoning tasks from specialized domains to general-purpose reasoning. detailed description of the benchmarks can be found in the appendix. Comparison with baselines. We first compare our R1-ShareVL 7B and R1-ShareVL 32B trained by Share-GRPO with the corresponding base models, i.e., Qwen2.5-VL-7B and Qwen2.5-VL-32B. As presented in Table 1, Share-GRPO effectively improves the long-chain reasoning capabilities of MLLMs by large margins. For example, on the challenging mathematical benchmarks like MathVista and MathVerse, R1-ShareVL-7B achieves improvements of +7.2% and +3.6%, respectively. It is worth noting that, based on previous studies, RL can enhance MLLMs long-chain reasoning ability on mathematical tasks, but it often comes at the cost of degraded performance on multidiscipline and general benchmarks. For instance, ThinkLite-7B drops -0.2% and -5.5% on MMStar and MMMU, respectively. In contrast, our R1-ShareVL-7B model achieves +3.1% improvement on MMStar and comparable accuracy on MMMU, demonstrating Share-GRPOs generalization capability in enhancing reasoning across diverse tasks. When scaling our method to larger models (i.e., Qwen2.5-VL-32B) with stronger foundational capabilities, our method remains robust and consistently improves performance. In particular, R1-ShareVL-32B achieves +9.1% improvement over the baseline model on MathVerse, along with an average performance gain of +2.7%. 7 Comparison with MLLMs trained via RL. We then compare R1-ShareVL with other state-of-theart MLLMs trained by reinforcement learning approaches. Our R1-ShareVL-7B using the same base model and fewer training data outperforms MM-Eureka-7B with an average performance gain of +2.1%, especially notable improvement of +1.4% on MathVista. Notably, beyond its capability in long-chain mathematical reasoning, R1-ShareVL also exhibits stronger reasoning generalization to multi-discipline and general reasoning tasks. Specifically, compared to ThinkLite-7B which also excels in mathematical reasoning, R1-ShareVL achieves better performance on the multi-discipline benchmark MMMU and the general benchmark MMStar, outperforming it by +5.0% and +3.3%, respectively. Besides, similar conclusion can be observed on larger models: our R1-ShareVL 32B further improves overall performance compared with MM-Eureka-32B by +3.4%, demonstrating the effectiveness and generalization of Share-GRPO. 4.3 Ablation Study Table 2: Ablation study of Share-GRPO. Local Method MathVista Shared Policy Shared Advantage Offline Online Global Qwen2.5-VL-7B (Baseline) Qwen2.5-VL-7B + GRPO Ablation Study of Share-GRPO. As shown in Table 2, we conduct ablation studies to examine the individual contribution of each design in ShareGRPO, including shared policy optimization (i.e., offline and online semantically consistent transformation) and shared advantage estimation (i.e., global and local advantage estimation). Compared to the GRPO baseline, incorporating the information sharing among only offline question variants with global shared advantage estimation yields performance boost of +1.1%. Further including the information sharing among online multimodal semantically consistent transformations results in exploring and sharing more diverse reasoning paths and +0.9% performance improvement. Finally, enabling both global and local advantage estimation achieves the best result of 75.4% on MathVista, highlighting the effectiveness of hierarchical advantage computation. These results demonstrate that both policy sharing and advantage sharing contribute significantly to the final performance of Share-GRPO. Share-GRPO (Ours) 73.9 74.8 75.4 68.2 72.8 (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) 4.4 Discussion Complementarity Between Share-GRPO and Dynamic Sampling. We compare Share-GRPO with dynamic sampling [58] and further discuss their complementarity, as shown in Table 3. Dynamic sampling performs multiple rollouts to generate candidate responses, from which subset with valid rewards is selected to update the policy. Distinct from it, Share-GRPO enhances the diversity of reasoning responses and shares them during policy optimization, yielding richer reward signals. On average, GRPO with dynamic sampling achieves performance of 65.1%, while Share-GRPO yields higher score of 66.8%, demonstrating the potential of leveraging diverse reasoning paths to enrich reward feedback and enhance policy learning. Moreover, combining Share-GRPO with dynamic sampling achieves higher accuracy of 67.2% on average, highlighting the great complementarity of Share-GRPO. Table 3: The complementarity between Share-GRPO and Dynamic Sampling. Method MathVista MMStar MMMU Avg. GRPO + Dynamic Sampling Share-GRPO Share-GRPO + Dynamic Sampling 73.6 75.4 75.9 65.7 67.0 66.7 56.1 58.1 59.0 65.1 66.8 67.2 Hyperparameter studies of Share GRPO. We study the impact of the number of question variant in Semantically Consistent Transformation (SCT) in Table 4, with the number of responses per question fixed at 6. When = 2, Share-GRPO achieves an accuracy of 75.4% on MathVista, significantly outperforming the baseline GRPO. When is increased to 4, the performance of the model trained with Share-GRPO further improves, demonstrating the benefit of scaling up the number of question variants. The Impact of the Number of Sampling N. In Table 5, we compare the performance of GRPO with different sampling numbers against Share-GRPO. We observe that with = 6, the model trained 8 Figure 3: Qualitative Results of reasoning capability of R1-ShareVL on mathematical problems. Table 4: Hyperparameter study of ShareGRPO. We study the impact of the number of question variants in Share-GRPO. Table 5: The study on the Number of Sampling N. We study the impact of the number of sampling in Share-GRPO. Method Question MathVista Method Sampling MathVista ShareGRPO ShareGRPO ShareGRPO 2 3 4 75.4 75.6 75. GRPO GRPO GRPO 6 12 24 ShareGRPO ShareGRPO (3+3) (6+6) 72.3 72.8 73.0 74.7 75. using GRPO achieves score of 72.3% on MathVista. As the number of sampling increases, the performance improves to 72.8 at = 12. However, further increasing the sampling number to 24 yields only marginal gains of 0.2%, while introducing additional computational overhead. Therefore, increasing the number of sampling reaches performance ceiling, making it an ineffective way to further improve reasoning reinforcement learning. Instead of simply increasing , Share-GRPO enhances the diversity of reasoning paths and leverages the concept of information sharing to amplify reward signals and enhance training stability. By sharing responses and incorporating hierarchical advantage estimation, our R1-ShareVL 7B achieves score of 75.4% with only 6 generated responses per question, surpassing the performance of GRPO even with 24 sampled responses. 4.5 Qualitative Results Fig. 3 illustrates that Share-GRPO effectively enhances the models reasoning ability on complex mathematical problems. In this example, the model accurately interprets the question and arrives at the correct answer, showing strong performance in symbolic reasoning and function analysis. This highlights the capability of Share-GRPO to guide the model toward precise and coherent solutions in mathematically demanding tasks."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose Share-GRPO, novel reinforcement learning framework for MLLMs, which introduces the concept of information sharing to effectively mitigate the challenges of sparse rewards and advantage vanishing. Share-GRPO expands the question space by generating semantically consistent variants, and encourages MLLMs to explore and share responses across more diverse solution space. Furthermore, Share-GRPO estimates advantages hierarchically within and across question variants at both global and local levels to effectively guide optimization. We conduct extensive experiments, ablation studies and discussion, which demonstrate the superiority of our proposed methods on various reasoning benchmarks."
        },
        {
            "title": "References",
            "content": "[1] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [2] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [3] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [4] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. [5] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [6] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. [7] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [8] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [9] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [10] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. [11] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. [12] Dongchen Lu, Yuyao Sun, Zilu Zhang, Leping Huang, Jianliang Zeng, Mao Shu, and Huo Cao. Internvlx: Advancing and accelerating internvl series with efficient visual token compression. arXiv preprint arXiv:2503.21307, 2025. [13] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [14] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [15] Huanjin Yao, Wenhao Wu, Taojiannan Yang, YuXin Song, Mengxi Zhang, Haocheng Feng, Yifan Sun, Zhiheng Li, Wanli Ouyang, and Jingdong Wang. Dense connector for mllms. Advances in Neural Information Processing Systems, 37:3310833140, 2024. [16] Ziheng Wu, Zhenghao Chen, Ruipu Luo, Can Zhang, Yuan Gao, Zhentao He, Xian Wang, Haoran Lin, and Minghui Qiu. Valley2: Exploring multimodal models with scalable vision-language design. arXiv preprint arXiv:2501.05901, 2025. [17] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [18] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [19] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 10 [20] Xiang Lan, Feng Wu, Kai He, Qinghao Zhao, Shenda Hong, and Mengling Feng. Gem: Empowering mllm for grounded ecg understanding with time series and images. arXiv preprint arXiv:2503.06073, 2025. [21] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llavanext: Improved reasoning, ocr, and world knowledge, 2024. [22] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9(1):1, 2023. [23] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. [24] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step, 2024. URL https://arxiv. org/abs/2411.10440. [25] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. [26] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint arXiv:2501.12570, 2025. [27] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Ren Lu, Thomas Mesnard, Johan Ferret, Colton Bishop, Ethan Hall, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. 2023. [28] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1380713816, 2024. [29] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, et al. Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220, 2024. [30] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [31] Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/R1-V, 2025. Accessed: 2025-02-02. [32] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning. arXiv preprint arXiv:2503.20752, 2025. [33] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [34] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. arXiv preprint arXiv:2504.11468, 2025. [35] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement. arXiv preprint arXiv:2503.17352, 2025. [36] Yi Peng, Xiaokun Wang, Yichen Wei, Jiangbo Pei, Weijie Qiu, Ai Jian, Yunzhuo Hao, Jiachun Pan, Tianyidan Xie, Li Ge, et al. Skywork r1v: pioneering multimodal reasoning with chain-of-thought. arXiv preprint arXiv:2504.05599, 2025. [37] Yufei Zhan, Yousong Zhu, Shurong Zheng, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. Vision-r1: Evolving human-free alignment in large vision-language models via vision-guided reinforcement learning. arXiv preprint arXiv:2503.18013, 2025. [38] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32, 2019. [39] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490, 2019. 11 [40] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PmLR, 2020. [41] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97299738, 2020. [42] Yee Teh, Victor Bapst, Wojciech Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. Advances in neural information processing systems, 30, 2017. [43] Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado Van Hasselt. Multi-task deep reinforcement learning with popart. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 37963803, 2019. [44] Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement learning. Advances in neural information processing systems, 29, 2016. [45] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in neural information processing systems, 30, 2017. [46] Carlo DEramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters. Sharing knowledge in multi-task deep reinforcement learning. arXiv preprint arXiv:2401.09561, 2024. [47] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [48] Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei, Jianhao Zhang, Yunzhuo Hao, Xuchen Song, et al. Skywork r1v2: Multimodal hybrid reinforcement learning for reasoning. arXiv preprint arXiv:2504.16656, 2025. [49] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [50] Zheng Yaowei, Lu Junting, Wang Shenzhi, Feng Zhangchi, Kuang Dongdong, and Xiong Yuwen. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/hiyouga/EasyR1, 2025. [51] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1119811201, 2024. [52] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [53] Anthropic. Claude 3.5 sonnet, 2024. [54] Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, and Yiming Yang. Improve vision language model chain-of-thought reasoning. arXiv preprint arXiv:2410.16198, 2024. [55] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [56] Qianchu Liu, Sheng Zhang, Guanghui Qin, Timothy Ossowski, Yu Gu, Ying Jin, Sid Kiblawi, Sam Preston, Mu Wei, Paul Vozila, et al. X-reasoner: Towards generalizable reasoning across modalities and domains. arXiv preprint arXiv:2505.03981, 2025. [57] Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement. arXiv preprint arXiv:2504.07934, 2025. [58] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [59] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [60] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. 12 [61] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023. [62] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [63] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. [64] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer, 2016."
        },
        {
            "title": "A Benchmarks",
            "content": "We evaluate our models on the following benchmarks. MathVista [59] is used to evaluate the mathematical problem-solving ability of MLLMs, containing 6141 questions covering areas such as arithmetic, geometry, algebra, and statistics. MMStar [60] is an innovative multimodal assessment benchmark that includes 1500 carefully selected visual key samples, addressing issues of visual redundancy and data leakage in existing assessments. MMMU [61] is large-scale interdisciplinary multimodal understanding and reasoning benchmark that collects 11.5K multimodal questions from university exams, quizzes, and textbooks. MathVerse [62] includes 2612 multimodal mathematics problems and has manually annotated 15672 test samples, comprising 3 main types of questions and 12 subcategories, such as plane geometry, solid geometry, and functions. MathVision [63] is collection of 3,040 high-quality mathematics problems, all accompanied by visual contexts, sourced from real mathematics competitions. AI2D [64] is dataset that contains over 5000 scientific charts, which can be used for tasks such as image classification and visual question answering."
        }
    ],
    "affiliations": [
        "Beijing University of Posts and Telecommunications",
        "ByteDance",
        "Nanyang Technological University",
        "The University of Sydney",
        "Tsinghua University"
    ]
}