{
    "paper_title": "Addition is All You Need for Energy-efficient Language Models",
    "authors": [
        "Hongyin Luo",
        "Wei Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large neural networks spend most computation on floating point tensor multiplications. In this work, we find that a floating point multiplier can be approximated by one integer adder with high precision. We propose the linear-complexity multiplication L-Mul algorithm that approximates floating point number multiplication with integer addition operations. The new algorithm costs significantly less computation resource than 8-bit floating point multiplication but achieves higher precision. Compared to 8-bit floating point multiplications, the proposed method achieves higher precision but consumes significantly less bit-level computation. Since multiplying floating point numbers requires substantially higher energy compared to integer addition operations, applying the L-Mul operation in tensor processing hardware can potentially reduce 95% energy cost by element-wise floating point tensor multiplications and 80% energy cost of dot products. We calculated the theoretical error expectation of L-Mul, and evaluated the algorithm on a wide range of textual, visual, and symbolic tasks, including natural language understanding, structural reasoning, mathematics, and commonsense question answering. Our numerical analysis experiments agree with the theoretical error estimation, which indicates that L-Mul with 4-bit mantissa achieves comparable precision as float8_e4m3 multiplications, and L-Mul with 3-bit mantissa outperforms float8_e5m2. Evaluation results on popular benchmarks show that directly applying L-Mul to the attention mechanism is almost lossless. We further show that replacing all floating point multiplications with 3-bit mantissa L-Mul in a transformer model achieves equivalent precision as using float8_e4m3 as accumulation precision in both fine-tuning and inference."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 ] . [ 2 7 0 9 0 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "ADDITION IS ALL YOU NEED FOR ENERGY-EFFICIENT LANGUAGE MODELS Hongyin Luo & Wei Sun BitEnergy AI, Inc. Cambridge, MA 02142, USA {hongyin,wei}@bitenergy.ai"
        },
        {
            "title": "ABSTRACT",
            "content": "Large neural networks spend most computation on floating point tensor multiplications. In this work, we find that floating point multiplier can be approximated by one integer adder with high precision. We propose the linear-complexity multiplication (L-Mul) algorithm that approximates floating point number multiplication with integer addition operations. The new algorithm costs significantly less computation resource than 8-bit floating point multiplication but achieves higher precision. Compared to 8-bit floating point multiplications, the proposed method achieves higher precision but consumes significantly less bit-level computation. Since multiplying floating point numbers requires substantially higher energy compared to integer addition operations, applying the L-Mul operation in tensor processing hardware can potentially reduce 95% energy cost by elementwise floating point tensor multiplications and 80% energy cost of dot products. We calculated the theoretical error expectation of L-Mul, and evaluated the algorithm on wide range of textual, visual, and symbolic tasks, including natural language understanding, structural reasoning, mathematics, and commonsense question answering. Our numerical analysis experiments agree with the theoretical error estimation, which indicates that L-Mul with 4-bit mantissa achieves comparable precision as float8 e4m3 multiplications, and L-Mul with 3-bit mantissa outperforms float8 e5m2. Evaluation results on popular benchmarks show that directly applying L-Mul to the attention mechanism is almost lossless. We further show that replacing all floating point multiplications with 3-bit mantissa L-Mul in transformer model achieves equivalent precision as using float8 e4m3 as accumulation precision in both fine-tuning and inference."
        },
        {
            "title": "INTRODUCTION",
            "content": "Modern artificial intelligence (AI) systems are significant energy consumers. Because of the large scale computation needed for neural network inference, AI applications based on such models are consuming considerable amount of electricity resource. Reportly, the average electricity consumption of ChatGPT service in early 2023 was 564 MWh per day, equivalent to the total daily electricity usage of 18,000 families in the United States1. It is estimated that Googles AI service could consume as much electricity as Ireland (29.3 TWh per year) in the worst-case scenario (de Vries, 2023). Reducing the amount of computation needed by neural networks is the key to reduce both energy consumption and inference speed for large-scale AI models. Neural networks, especially large language models (LLMs) (Radford et al., 2019; Brown, 2020; Achiam et al., 2023; Touvron et al., 2023; Team et al., 2023), contain large number of floating point parameters involved in elementwise and matrix multiplication computations. In transformer (Vaswani, 2017) based LLMs, the attention mechanism is major bottleneck that limits the computation efficiency. Given input context of tokens, the complexity of standard attention mechanism computation is O(N 2), involving multiplying high dimensional tensors. Besides attention, there are also large amount of element-wise multiplication and linear transformation computations. In this work, we propose the 1https://www.eia.gov/tools/faqs/faq.php?id="
        },
        {
            "title": "Preprint",
            "content": "linear-complexity multiplication (L-Mul) algorithm, which approximates floating point multiplication with integer addition operations. The algorithm can be integrated into existing models at various levels, such as replacing the multiplication in the attention mechanism or substituting all matrix and element-wise multiplications. The proposed L-Mul method will lead to significantly reduced energy consumption for both model training and inference. In modern computing hardware, multiplications between floating point numbers consumes significantly higher energy than addition operations (Horowitz, 2014). Specifically, multiplying two 32-bit floating point numbers (fp32) costs four times energy as adding two fp32 numbers, and 37 times higher cost than adding two 32-bit integers (int32). The rough energy costs for various operations are shown in Table 1. In PyTorch (Paszke et al., 2019), the default precision for accumulating tensor multiplication results is set to fp32. While I/O and control operations are not considered, approximating fp32 multiplications with int32 additions consumes only 1/37 2.7% of the energy. When the accumulation precision is reduced to fp16, integer addition consumes approximately 4.7% of the energy required for floating-point multiplication."
        },
        {
            "title": "Floating Point",
            "content": "8-bit 32-bit 16-bit 32-bit Addition Multiplication 0.03 pJ 0.2 pJ 0.1 pJ 3.1 pJ 0.4 pJ 1.1 pJ 0.9 pJ 3.7 pJ Table 1: Energy cost of various arithmetic operations cited from Horowitz (2014). We evaluate the numerical precision of L-Mul algorithm on transformer-based language models with wide range of language and vision tasks. Experiments with full-precision model weights show that replacing standard multiplication operations with L-Mul in the attention mechanism is almost lossless for transformer-based LLMs. On natural language reasoning tasks, the average performance loss of L-Mul-based attention is 0.07% across commonsense, structured reasoning, language understanding. On vision tasks, L-Mul-based attention gained 0.12% accuracy improvement on visual question answering, object hallucination, and free-form visual instruction tasks. The experiment results are obtained by directly adapting pretrained LLMs with the standard attention implementation to the new L-Mul-based attention mechanism without any additional training. The error estimation and ablation study show that under the training-free setting, L-Mul with 4bit mantissa can achieve comparable precision as multiplying float8 e4m3 numbers, and LMul with 3-bit mantissa outperforms float8 e5m2 multiplication. We also show that fine-tuning can fix the performance gap between L-Mul and the standard multiplication. Fine-tuning model where all multiplication operations in attention mechanisms, linear transformations, and elementwise products are replaced by 3-bit-mantissa L-Mul results in comparable performance to finetuning standard model with an accumulation precision of float8 e4m3. In the expansive landscape of AI efficiency research, our approach centers on enhancing the efficiency of tensor arithmetic algorithmsa direction that is orthogonal yet complementary to prevailing efforts in I/O and control optimization (Jouppi et al., 2017; Choquette et al., 2021; Abts et al., 2022)2. We believe that truly energyand compute-efficient AI computation will emerge from holistic integration of optimizations across I/O, control, and arithmetic operations."
        },
        {
            "title": "2 METHOD",
            "content": "2.1 BACKGROUND: FLOATING-POINT NUMBERS AND TENSORS Most machine learning models, including neural networks, use floating point (FP) tensors to represent their inputs, outputs, and trainable parameters. Typical choices are 32-bit and 16-bit FP tensors (fp32 and fp16) defined by the IEEE 754 standard shown in Figure 1. 2Due to the absence of native implementation, GPUs cannot fully exploit the efficiency of the L-Mul algorithm. We recommend training and hosting L-Mul-based models on devices integrated with specialized architectural designs. Patent pending."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: 16-bit, 8-bit floating point numbers defined in IEEE 754 and on various hardware for tensor computations, and the 16-bit integer. MSB stands for most significant bit and LSB stands for least significant bit. Multiplication operations are generally more complicated than additions, and FP operation are more costly than integers (Horowitz, 2014). Table 1 shows that multiplying two fp32 numbers consumes 37 times higher energy than adding two 32-bit integers. While the complexity of integer addition is O(n) where is the number of bits used for representing the number, FP multiplication requires O(e) exponent addition, O(m2) mantissa multiplication, and rounding. Here and stand for the number of bits used for exponent and mantissa parts of the FP numbers. Modern LLM training and inference involves large number of FP calculations in tensor computation. Consider calculating the element-size and dot products of two 2-D tensors: Y1 = X, Y2 = ; A, R(N,k) Calculating Y1 involves 2 FP multiplications (Mul). If and are both fp32 tensors, consumes 37 times higher energy than adding two int32 matrices of the save size. Similarly, Calculating Y2 involves (m k) FP Mul and the same number of FP additions (Add). When and are fp32 tensors, each Mul-Add operation for two numbers consumes 0.9 + 3.7 = 4.6 (pJ) energy. If we replace the fp32 Mul with int32 Add, the energy cost becomes 0.1 + 0.9 = 1.0 (pJ), only 21.7% of the original cost. Similarly, if the inference is conducted in fp16, replacing fp16 Mul with int16 Add result in 1 (0.05 + 0.4)/(1.1 + 0.4) = 70% energy saving. 2.2 LINEAR-COMPLEXITY MULTIPLICATION (L-MUL) We propose L-Mul, FP multiplication algorithm with O(n) complexity, where is the bit size of its FP operands. Consider two FP numbers x, y, whose exponents and fractions are xe, ye and xm, ym respectively, the vanilla FP Mul result is ul(x, y) = (1 + xm) 2xe (1 + ym) 2ye = (1 + xm + ym + xm ym) 2xe+ye plus an xor operation () to decide the sign of the result. Assume xm and ym are mantissas of bits. The O(m2) mantissa multiplication operation is the complexity bottleneck of this calculation. We remove this operation and introduce new multiplication algorithm that processes mantissas with computational complexity of O(m): L-Mul(x, y) = (1 + xm + ym + 2l(m)) 2xe+ye, l(m) = if 3, if = 4, 3 if > 4. 4 (1) The offset exponent l(m) is defined according the observation shown in Figure 3. In the following sections, we show that (1) the L-Mul operation can be implemented by integer Adders, and (2) the algorithm is more accurate and efficient than fp8 multiplications. The implementation of the algorithm is shown in Figure 2, where we also added the Inline PTX Assembly code we used to simulate the process on Nvidia GPUs. While Equation (1) contains 4 addition operations, the bit format design of FP numbers helps us implement the L-Mul algorithm with one adder. Since the FP format handles 1 + xm implicitly, we do not have to compute the value of (1 + . . . ). The integer addition operation also automatically send the mantissa carry to"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Coparing the process of regular floating-point multiplication and linear-complexity multiplication (L-Mul) between two fp32 numbers. In the inline PTX Assembly code, $1 and $2 are fp32 registers storing inputs while $0 is the fp32 register for output. s1, s2, r0, r1, r2 are unsigned int32 registers storing intermediate results. Note that the assembly program is only for numerical simulation on Nvidia GPUs. The optimal implementation is at the hardware level. the exponent. If the mantissa sum is greater than 2, carry is automatically added to the exponent. This is different from the rounding process in traditional FP multiplier, where the fraction is manually rounded to 1.x and the carry is added to the exponent as an independent step. As result, the L-Mul algorithm is more efficient than traditional FP multiplication by skipping both mantissa multiplication and rounding operations. The construction of L-Mul results can be expressed using the following equation, where all bit-level calculations are performed as operations between unsigned integers. L-mul(x, y)[0] = x[0] y[0] L-mul(x, y)[1:] = x[1:] + y[1:] offset (2) We further implement the attention mechanism with L-Mul. In transformer models, the attention mechanism has high computation cost because of its O(C2) complexity to process the input context C. We found that L-Mul can replace the complicated tensor multiplications with minimal performance loss needing no additional training. In this work we implement more efficient attention mechanism as follows, = Wk, = Wq, = WV = sof tmax (cid:20) L-matmul(Q, ) (cid:21) , = L-matmul(A, H) (3) where L-matmul(Q, ) stands for matrix multiplication operation where all regular FP multiplications are implemented in L-Mul. By doing this, all FP multiplications are replaced with integer additions, which consumes significantly lower computation resource. 2.3 PRECISION AND COST ANALYSIS In this section, we show that L-Mul is more precise than fp8 e4m3 multiplications but uses less computation resource than fp e5m2. To be concise, we do not consider the rounding to nearest even mode in both error analysis and complexity estimation for both Mul and L-Mul. 2.3.1 PRECISION ESTIMATION The goal of the precision analysis is to find the precision of the L-Mul algorithm is equivalent to rounding the fraction of FP number to how many bits, e.g., fp8 with 2or 3-bit mantissas (e5m2 or e4m3). Consider positive FP numbers = (1 + xm) 2xe and = (1 + ym) 2ye, they can be written in the following format if we explicitly highlight the bits to be kept after rounding: = (1 + xk + xr) 2xe, = (1 + xk) 2xe"
        },
        {
            "title": "Preprint",
            "content": "y = (1 + yk + yr) 2ye , = (1 + yk) 2ye where xk, yk are the first bits of xm, ym, and xr, yr are the value of remaining bits that will be ignored after the k-bit rounding. x, are the rounded value of x, by keeping the first bits of the mantissa. Consider and has m-bit mantissa in their full precision. For example, Float16 numbers have 10-bit mantissa and BFloat16 contain 7 bits. The error of ul(x, y) = and its expectation are mul = ul(x, y) ul(x, y) = (xkyr + ykxr + xr + yr + xryr) 2xe+ye ek E[ek mul] = f1(m, k) E[2xe+ye ] Comparing with k-bit mantissa FP multiplication, the error of k-bit mantissa L-Mul is lmul = ek ek lmul] = E[ek E[ek mul + (xkyk 2l(k)) 2xe+ye mul] + E[xk yk 2l(k)] E[2xe+ye ] (4) (5) With the equations above, we can compute the expectation of the precision gap between k-bit L-Mul and FP multiplication: E[ek lmul] E[ek mul] = f2(k) E[2ex+ey ], E[ek lmul] = [f1(m, k) + f2(k)] E[2ex+ey ] When xm, ym are evenly distributed, we can calculate the following expectations, E[xk] = 1 2 (1 2k), E[xr] = 1 2 (2k 2m) By estimating f1(m, k) and f2(k) and further inferring E[ek mul], we find that L-Mul is more accurate than fp8 e5m2 with evenly distributed operands. However, the weight distribution is often biased in pretrained LLMs. Based on the combined weight distribution of five popular LLMs, we find that L-Mul can achieve higher precision beyond fp8 e4m3 with 5-bit mantissa operands in practice. We support both claims with estimated errors detailed in Appendix A. lmul] and E[ek 2.3.2 GATE COMPLEXITY ESTIMATION In this section, we make rough estimation for the amount of gate-level computations needed by L-Mul and fp8 multiplications. Multiplying two fpn eimj number require the following computation: sign prediction, exponent addition with offset, + 1-bit mantissa multiplication, and exponent rounding. The mantissa multiplication includes (j + 1)2 AND operations, 3 half adders and 2j 2 full adders. The exponent rounding needs half adders. In regular circuit design, full adder involves 2 AND, 2 XOR, and 1 OR. Each XOR has 4 NAND gates. As result, full adder consumes 11 gate-level computation, while half adder (no incoming carry) consumes 5 gate-level computations (1 AND and 1 XOR). In conclusion, the total amount of gate-level computation needed by fp8 Mul can be estimated as fp16 584, fp8-e4m3 325, fp8-e5m2 296 (6) L-Mul consumes 1 XOR for sign prediction, 1 half adder, and 2 full adders. The total gate count needed by 16-bit and 8-bit L-Mul can be estimated as follows, L-mul eimj = L-mul 1 + + fp16 256, L-mul int(i+j) + + fp8 157 int8 (7) L-Mul with fp8 e4m3 and fp8 e5m2 operands have similar complexity since exponent offsets are typically implemented by 8-bit unsigned integer adders. As estimated, fp16 L-Mul requires less gates than fp8 multiplications, and fp8 L-Mul is significantly more efficient. To summarize the error and complexity analysis, L-Mul is both more efficient and more accurate than fp8 multiplication."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "To prove the theoretical precision estimation and find out how L-Mul-based LLMs perform on real tasks, we conducted experiments on various benchmarks with different transformerbased large language models. We evaluated Llama-3.1-8b-Instruct (Dubey et al., 2024), mistral-7b-v0.3-Instruct (Jiang et al., 2023), Gemma2-2b-It (Team et al., 2024), and Llava-v1.5-7b (Liu et al., 2024) models, and found that the proposed method can replace different modules in transformer layers under fine-tuning or training-free settings. In this section, we first introduce the benchmarks and tasks used for evaluation, then compare the numerical error of the L-Mul algorithm against models with fp8 parameters. We also report the benchmarking results of LLMs under different precision settings."
        },
        {
            "title": "3.1 TASKS",
            "content": "Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2020) contains 57 multi-choice natural language understanding tasks covering various high-school and college subjects. With 5 few-shot examples, the LLMs for evaluation are required to find the most appropriate answer option to each question. The benchmark focuses on evaluating the language understanding and knowledge abilities related to given subjects. BigBench-Hard (BBH) (Srivastava et al., 2023) contains set of complex symbolic tasks to evaluate the structural and logic reasoning abilities of language models. In this work, we select subset of 17 multi-choice tasks to evaluate Llama and Mistral LLMs. We evaluate language models under the few-shot prompting setting for all BBH tasks. Common Sense. We put together set of 5 question answering tasks to evaluate the commonsense knowledge reasoning ability of LLMs. The set of task includes ARC-Challenge (Clark et al., 2018), CSQA (Saha et al., 2018), OBQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), and SIQA (Sap et al., 2019), covering different aspects of factual and social knowledge. Visual Question Answering. We select set of multi-choice question answering tasks based on images for evaluating both vision and language understanding abilities of visual language models. The tasks include VQAv2 (Goyal et al., 2017), VizWiz (Gurari et al., 2018), and TextVQA (Singh et al., 2019), containing both unanswerable and answerable questions with different types of answers. Visual Instruction following. We test the instruction following ability of Llava-1.5-7b model with the Llava-bench task (Liu et al., 2024) by generating free-form responses given images and corresponding instructions. Following the official evaluation guide, we evaluate the instruction following quality with GPT4o and compare the relative performance. Object Hallucination. We explore if conducting inference with lower precision infects the truthfulness of the Llava model using the POPE benchmark (Li et al., 2023), which prompt visual language models with sequence of yes/no questions about positive and negative objects. GSM8k (Cobbe et al., 2021) consists of 8.5k human-crafted grade school math problems, with test split of 1,000 problems designed to evaluate the arithmetic capabilities of language models. We conduct experiments on GSM8k in two different settings. In the training-free setting, we assess LLMs with few-shot, chain-of-thought prompting (Wei et al., 2022). Additionally, we fine-tune the Gemma2-2b-It model on the training split and evaluate its performance in zero-shot setting. 3.2 PRECISION ANALYSIS Selection of l(k). We first visualize the mean square errors obtained by different l(k) selections with different models on the GSM8k dataset in Figure 3. In the plot, we highlight the l(k) configurations that leads to lower average error than float8 e4m3 multiplications in model inference in red, and the k, l(k) combinations leading to an error between e4m3 and e5m2 are underlined. In both models, L-Mul with 3-bit mantissas is more accurate than fp8 e5m2 and L-Mul with 4-bit mantissas achieves comparable or lower error than fp8 e4m3. In section 2.3.1, we argued that the error expectation of L-Mul can be lower Mantissa size. than multiplying fp8 e4m3 multiplication while using less computation resource than multiplying fp8 e5m2 numbers. We hereby confirm the correctness of our theoretical precision estimates for"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Mean square errors obtained by different l(k) selections on Llama and Gemma models. The combinations achieving higher precision than fp8 e4m3 are highlighted in read, and those outperforming fp8 e5m2 are underlined. When = 4 and l(k) = 3, the average error of the llama model is slighly lower but very close to fp8 e4m3. Figure 4: Comparing the error levels of linear-complexity multiplication (L-Mul) against the number of mantissa bits comparing with 8-bit FP multiplication operation in different formats. the L-Mul algorithm with experimental analysis. The average errors of Llama and Gemma models are illustrated in Figure 4. The experiments demonstrated that across various sizes of LLMs, the L-Mul algorithm using 6-bit mantissa FP operands approximates the lowest average error, significantly outperforming both fp8 formats. Additionally, the 3and 4-bit mantissa L-Mul achieved accuracy on par with or exceeding that of fp8 e5m2 and fp8 e4m3 multiplication operations, respectively."
        },
        {
            "title": "Preprint",
            "content": "In the IEEE 754 format (with 1-bit sign and 5-bit exponent), using 6-bit mantissa is equivalent to rounding fp16 numbers down to fp12. By applying the complexity estimation method outlined in Equation (7), we can compute the gate count for 12-bit L-Mul operations as follows: L-mul 12 201 < p8 300 (8) The experimental results further confirm that L-Mul is more efficient and accurate than fp8 multiplication. Although we estimated gate counts as an indicator of computational complexity, the actual difference in energy cost is greater than the complexity gap suggests. Based on the energy consumption reported in Horowitz (2014), an fp8 multiplication consumes approximately 0.25 pJ to 0.4 pJ, while 16-bit L-Mul uses around 0.06 pJ of energy."
        },
        {
            "title": "3.3 BENCHMARKING",
            "content": "In this section, we demonstrate that L-Mul can replace tensor multiplications in the attention mechanism without any loss of performance, whereas using fp8 multiplications for the same purpose degrades inference accuracy. This indicates that we can achieve the same model inference performance while reducing the energy cost of attention computations by 80%. Additionally, we present the full-model fine-tuning performance when all tensor multiplication operations are replaced with L-Mul on the GSM8k benchmark. Textual tasks. Table 2 presents the evaluation results of the Llama and Mistral models on various natural language benchmarks, including MMLU, BBH, ARC-C, CSQA, PIQA, OBQA, and SIQA. In these experiments, the matrix multiplications in the attention layers, both before and after the softmax operation, were replaced with 8-bit tensor computations in different formats or L-Matmul following the implementation we discussed in Equation (3). Precision BBH MMLU ARC-R CSQA OBQA PIQA SIQA Avg. BFloat16 55.85 Float8 e4m3 55.16 Float8 e5m2 53.20 L-Mul 55.87 BFloat16 70.79 Float8 e4m3 69.91 Float8 e5m2 62.94 L-Mul 70. Mistral-7b-Instruction-v0.3 62.20 62.18 61.75 62.19 68.86 68.16 66.61 68.54 75.94 75.39 74.91 76.11 71.42 71.25 71.25 71.09 Llama-3.1-8B-Instruct 82.51 81.66 80.12 82.17 74.53 74.28 73.30 74.28 76.20 76.00 74.40 76.60 84.20 82.20 79.40 84.20 80.74 80.47 79.76 80.52 84.00 83.51 81.07 83. 44.83 44.63 44.52 45.34 45.96 45.34 45.39 46.06 69.83 69.55 68.97 69.93 74.24 73.40 71.86 74.00 Table 2: Comparing the attention mechanism implemented with 16and 8-bit tensor multiplication operations and L-Mul approximations. Note that the L-Mul computations cost significantly less resource than fp8 tensors. The results indicate that L-Mul not only requires significantly fewer computational resources but also delivers higher precision than float8-e4m3 tensors in 12 out of 14 experiments using Mistral and Llama models. This leads to minimal performance gap when compared to bf16 inference. On average, across the two models, the performance difference between bf16 and L-Mul is just 0.07%. These findings suggest that matrix multiplication operations in the attention mechanism can be seamlessly replaced with the L-Mul algorithm without any loss of accuracy or the need for additional training. GSM8k. We evaluated the performance of three language modelsMistral-7b-Instruct-v0.3, Llama3.1-7b-Instruct, and Gemma2-2b-Iton the GSM8k dataset using few-shot prompting and L-Mul-based attention. The models were tested under different numerical precision formats: bf16, fp8 e4m3, fp8 e5m2, and the L-Mul method. The results are summarized in Table 3. Notably, the L-Mul-based attention mechanism slightly improved the average performance compared to the bf16 baseline. Mistral-7b-Instruct-v0.3 and Gemma2-2b-It both exhibited improved"
        },
        {
            "title": "Preprint",
            "content": "Model Bfloat16 Float8 e4m3 Float8 e5m2 L-Mul Mistral-7b-Instruct-v0.3 Llama3.1-7b-Instruct Gemma2-2b-It Average 52.54 76.12 45.87 58.17 52.39 75.44 45.94 57.92 50.19 71.80 44.43 55. 52.92 75.63 47.01 58.52 Table 3: GSM8k accuracy with Mistral, Llama, and Gemma models with few-shot prompting and attention mechanism implemented in different precision levels. accuracies with L-Mul, achieving 52.92% and 47.01% respectively. Llama3.1-7b-Instructs accuracy with L-Mul was slightly lower than its bf16 performance but still higher than with fp8 e4m3 and fp8 e5m2. On contrary, rounding the tensors in the attention computation to fp8 e5m2 leads to significant performance drop although its more complicated than L-Mul. Vision-language tasks. The performance of the Llava-v1.5-7b model on VQA, object hallucination, and instruction following tasks are shown in Table 4. Similar to the experiments on language tasks, the attention computation is conducted with different precision/methods while other linear transformation layers are unchanged. Except for TextVQA where the accuracy gap is 0.5%, the performance of L-Mul and BFloat16 attentions are comparable. The VQA tasks are evaluated with the official evaluation scripts and the Llava-Bench results are generated by GPT-4o. Task Split BFloat16 L-Mul Task Split rand. 86.20 86.57 POPE adv. pop. all comp. Llava-Bench detail. conv. 83.17 83.19 85.13 85. 84.83 85.03 66.80 64.90 57.60 58.70 41.40 43.30 VQAv2 yes/no num. other all yes/no num. VizWiz unans. BFloat16 L-Mul 91.88 91.78 59.04 58.93 70.56 70.73 78.03 78.06 77.19 78. 45.24 50.48 71.75 73.78 all 57.50 57.50 other 38.19 38. TextVQA all 57.90 57.41 all 49.31 50.16 Evaluating the performance of different Table 4: attention implementation on the Llava-v1.5-7b model. VQAv2, VizWiz, and TextVQA are visual question answering tasks, POPE evaluates object hallucination, and Llava-Bench assesses the instruction following ability scored by GPT-4o. L-Mul with fewer bits. In this section, we explore how L-Mul-based attention precision influences the overall model performance using the MMLU benchmark with Mistral and Llama models. We implement the attention mechanism with L-Mul and only keep the first bits of the operand tensors. The results of L-Mul attention with different precision are listed in Table 5. As expected, using LMul with 4-bit mantissa achieves performance comparable to or slightly better than that of bf16 and fp8 e4m3. However, performance drops proportionally to the estimated error depicted in Figure 4. When = 3, both models significantly outperform their fp8 e5m2 counterparts, with the Llama models performance remaining close to that of fp8 e4m3. When = 2, the Llama models performance is comparable to that of fp8 e5m2 rounding. This suggests that with the Llama model, we can perform L-Mul directly on fp8 models without compromising performance. Model e4m3 e5m2 = 4 = = 2 8bit Acc. e4m3 e5m2 L-Mul Mitral Llama 62.18 68. 61.75 66.61 62.16 68.43 62.06 68.12 61.08 66.67 Table 5: The performance of Mistral models with attention mechanism implemented with k-bit tensor L-Mul. GSM8k 36.09 7.96 37.91 Table 6: Zero-shot fine-tuned Gemma22b models with 8-bit accumulation precision. L-Mul uses fp8 e4m3 inputs."
        },
        {
            "title": "Preprint",
            "content": "Full-model fine-tuning. To further explore the impact of the L-Mul algorithm, we go beyond implementing attention layers with L-Mul by replacing all multiplication operationsincluding matrix multiplications in linear transformations, element-wise multiplications, and those within attention layerswith fp8 e4m3 L-Mul for the Gemma2-2b-It model. We then fine-tune the updated model on the training set of the GSM8k corpus and evaluate both the fine-tuned fp8 and L-Mul models under zero-shot setting on the GSM8k test set. Note that the L-Mul operations in this experiment takes operands with 3-bit mantissas (k = 3) and the accumulation precision is fp8 e4m3 to explore an extremely efficient setting. The experimental results demonstrate that fine-tuned fp8 e4m3 L-Mul model achieves performance comparable to standard fine-tuned fp8 e4m3 model under fp8 accumulation precision. This suggests that L-Mul can enhance training efficiency without compromising the fine-tuned models performance. Moreover, it reveals the potential of training L-Mul native LLMs for accurate and energy-efficient model hosting."
        },
        {
            "title": "4 RELATED WORK",
            "content": "Reducing the computation needed by neural networks while maintain the performance is an important problem which entailed multiple research directions. Typical methods include neural network pruning, quantization, and improved tensor I/O implementations. Pruning. Neural network pruning focuses on improving the inference efficiency by reducing the number of connections among layers (Han et al., 2015a;b; Wang et al., 2020). Neural network pruning methods usually involves training. After important weights are identified, the neural networks are re-trained to further update the selected weights for specific tasks. Different from model pruning, the method we proposed is designed for general tasks, requiring no task-specific re-training. Optimizing tensor I/O. On regular GPUs, moving tensors between GPU SRAM and highbandwidth memory (HBM) is the main bottleneck of time and energy consumption. Reducing the I/O operations in transformer models and making the best use of the HBM can significantly improve the efficiency of AI training and inference (Dao et al., 2022; Dao; Kwon et al., 2023). Our method, which focuses on optimizing arithmetic operations, is orthogonal to this direction. Rounding and quantization. Standard neural network weights are stored as 32-bit or 16-bit FP tensors. However, the full-sized weights takes considerable amount of GPU memory. To improve the storage efficiency, both weights storage and computation can be conducted in lower precision, for example, using 16-bit, 8-bit, or 4-bit FP and Int (fp16, bf16 (Kalamkar et al., 2019), fp8-e4m3, fp8-e5m2 (Micikevicius et al., 2023), int8 (Dettmers et al., 2022), fp4, and int4 (Dettmers et al., 2024)) tensors to represent model weights. Inference with lower-bit parameters usually hurts the computation accuracy and impacts the performance of pretrained models, and Integer-based quantization methods spend significant time to handle outlier weights. comparing to the quantization methods, our method requires less computation but achieves higher accuracy."
        },
        {
            "title": "5 FUTURE WORK",
            "content": "To unlock the full potential of our proposed method, we will implement the L-Mul and L-Matmul kernel algorithms on hardware level and develop programming APIs for high-level model design. Furthermore, we will train textual, symbolic, and multi-modal generative AI models optimized for deployment on L-Mul native hardware. This will deliver high-speed and energy-efficient AI hosting solutions, reducing the energy cost for data centers, robotics, and wide spectrum of edgecomputing devices."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we introduced L-Mul, an efficient algorithm that approximates floating-point multiplication using integer addition. We first demonstrated that the algorithm exhibits linear complexity relative to the bit size of its floating-point operands. We then showed that the expected accuracy of L-Mul surpasses that of fp8 multiplications while requiring significantly less computational power."
        },
        {
            "title": "Preprint",
            "content": "To assess the practical impact of L-Mul, we evaluated it on natural language, vision, and mathematics benchmarks using popular language models. Our experiments indicate that L-Mul outperforms 8-bit transformers with lower computational consumption and achieves lossless performance when applied to computation-intensive attention layers without additional training. Based on this evidence, we argue that tensor multiplications in language models can be effectively implemented using L-Mul to preserve performance while enabling energy-efficient model deployment."
        },
        {
            "title": "REFERENCES",
            "content": "Dennis Abts, Garrin Kimmell, Andrew Ling, John Kim, Matt Boyd, Andrew Bitar, Sahil Parmar, Ibrahim Ahmed, Roberto DiCecco, David Han, et al. software-defined tensor streaming mulIn Proceedings of the 49th Annual International tiprocessor for large-scale machine learning. Symposium on Computer Architecture, pp. 567580, 2022. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Jack Choquette, Wishwesh Gandhi, Olivier Giroux, Nick Stam, and Ronny Krashinsky. Nvidia a100 tensor core gpu: Performance and innovation. IEEE Micro, 41(2):2935, 2021. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. Alex de Vries. The growing energy footprint of artificial intelligence. Joule, 7(10):21912194, 2023. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35: 3031830332, 2022. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 69046913, 2017. Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 36083617, 2018."
        },
        {
            "title": "Preprint",
            "content": "Song Han, Huizi Mao, and William Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a. Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015b. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2020. Mark Horowitz. 1.1 computings energy problem (and what we can do about it). In 2014 IEEE international solid-state circuits conference digest of technical papers (ISSCC), pp. 1014. IEEE, 2014. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Norman Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of tensor processing unit. In Proceedings of the 44th annual international symposium on computer architecture, pp. 112, 2017. Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611626, 2023. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 292305, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/ v1/2023.emnlp-main.20. URL https://aclanthology.org/2023.emnlp-main.20. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024. Paulius Micikevicius, Stuart Oberman, Pradeep Dubey, Marius Cornea, Andres Rodriguez, Ian Bratt, Richard Grisenthwaite, Norm Jouppi, Chiachen Chou, Amber Huffman, et al. Ocp 8-bit floating point specification (ofp8). Open Compute Project, 2023. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23812391, 2018. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, highperformance deep learning library. Advances in neural information processing systems, 32, 2019. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Amrita Saha, Vardaan Pahuja, Mitesh Khapra, Karthik Sankaranarayanan, and Sarath Chandar. Complex sequential question answering: Towards learning to converse over linked question answer pairs with knowledge graph. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018."
        },
        {
            "title": "Preprint",
            "content": "Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social iqa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 44634473, 2019. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, In Proceedings of the IEEE/CVF and Marcus Rohrbach. Towards vqa models that can read. conference on computer vision and pattern recognition, pp. 83178326, 2019. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 61516162, 2020. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022."
        },
        {
            "title": "A ERROR ESTIMATION",
            "content": "We calculate the error expectations with different (n, k) combinations as follows in Table 7. The values are calculated with the actual parameters of Mistral, Llama, and Gemma models. For even distribution, we use the expectations introduced in Section 2.3.1. For real distribution, we estimate the average value of possible operands using the parameters of five popular pretrained LLMs. values Even Distribution Real Distribution abs[f1(n = 7, k)] abs[f1(n = 7, k) + f2(k)] abs[f1(n = 7, k)] abs[f1(n = 7, k) + f2(k)] 0.68 0.68 0.61 0.16 2 0.35 0.43 0.33 0.18 0.17 0.30 0.16 0.18 4 5 6 0.081 0. 0.077 0.12 0.035 0.20 0.033 0.15 0.012 0.19 0.011 0.14 Table 7: Average error expectation with five different language models on floating point multiplication and L-Mul with different rounding representations when the full precision is BFloat16. stands for the bit number of the operand mantissa. We find that when the operands are distributed evenlly, L-Mul is more accurate than float8 e5m2 multiplications. However with real models, L-Mul can achieve higher precision than float8 e4m3 calculations."
        }
    ],
    "affiliations": [
        "BitEnergy AI, Inc. Cambridge, MA 02142, USA"
    ]
}