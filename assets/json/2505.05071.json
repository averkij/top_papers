{
    "paper_title": "FG-CLIP: Fine-Grained Visual and Textual Alignment",
    "authors": [
        "Chunyu Xie",
        "Bin Wang",
        "Fanjing Kong",
        "Jincheng Li",
        "Dawei Liang",
        "Gengshen Zhang",
        "Dawei Leng",
        "Yuhui Yin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks such as image-text retrieval and zero-shot classification but struggles with fine-grained understanding due to its focus on coarse-grained short captions. To address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances fine-grained understanding through three key innovations. First, we leverage large multimodal models to generate 1.6 billion long caption-image pairs for capturing global-level semantic details. Second, a high-quality dataset is constructed with 12 million images and 40 million region-specific bounding boxes aligned with detailed captions to ensure precise, context-rich representations. Third, 10 million hard fine-grained negative samples are incorporated to improve the model's ability to distinguish subtle semantic differences. Corresponding training methods are meticulously designed for these data. Extensive experiments demonstrate that FG-CLIP outperforms the original CLIP and other state-of-the-art methods across various downstream tasks, including fine-grained understanding, open-vocabulary object detection, image-text retrieval, and general multimodal benchmarks. These results highlight FG-CLIP's effectiveness in capturing fine-grained image details and improving overall model performance. The related data, code, and models are available at https://github.com/360CVGroup/FG-CLIP."
        },
        {
            "title": "Start",
            "content": "FG-CLIP: Fine-Grained Visual and Textual Alignment Chunyu Xie 1 * Bin Wang 1 * Fanjing Kong 1 Jincheng Li 1 Dawei Liang 1 Gengshen Zhang 1 Dawei Leng 1 Yuhui Yin 1 5 2 0 2 8 ] . [ 1 1 7 0 5 0 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks such as image-text retrieval and zero-shot classification but struggles with fine-grained understanding due to its focus on coarse-grained short captions. To address this, we propose Fine-Grained CLIP (FGCLIP), which enhances fine-grained understanding through three key innovations. First, we leverage large multimodal models to generate 1.6 billion long caption-image pairs for capturing globallevel semantic details. Second, high-quality dataset is constructed with 12 million images and 40 million region-specific bounding boxes aligned with detailed captions to ensure precise, context-rich representations. Third, 10 million hard fine-grained negative samples are incorporated to improve the models ability to distinguish subtle semantic differences. Corresponding training methods are meticulously designed for these data. Extensive experiments demonstrate that FGCLIP outperforms the original CLIP and other state-of-the-art methods across various downstream tasks, including fine-grained understanding, open-vocabulary object detection, image-text retrieval, and general multimodal benchmarks. These results highlight FG-CLIPs effectiveness in capturing fine-grained image details and improving overall model performance. The related data, code, and models are available at https://github.com/360CVGroup/FG-CLIP. 1. Introduction The integration of vision and language (Alayrac et al., 2022; Ramesh et al., 2022; Lin et al., 2023; Gabeff et al., 2024) has been long-standing goal in artificial intelligence, aiming *Equal contribution 1360 AI Research. Correspondence to: Dawei Leng <lengdawei@360.cn>. Proceedings of the 41 st International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). 1 to develop models that can understand and reason about the world in visually and linguistically rich manner. Recent advances in multimodal pre-training, such as CLIP (Radford et al., 2021), have made significant strides in this direction by learning joint representations of images and text through contrastive learning. These models have achieved stateof-the-art performance in variety of downstream tasks, including image-text retrieval (Pan et al., 2023; Sun et al., 2024; Zhang et al., 2024), image captioning (Mokady et al., 2021; Li et al., 2024; Yao et al., 2024), and visual question answering (Li et al., 2023a; Parelli et al., 2023; Team et al., 2024; Wang et al., 2025). However, despite their impressive capabilities, these models often struggle with fine-grained details, particularly in recognizing object attributes and their relationships. Recent works (Liu et al., 2023a; Wu et al., 2024b; Zhang et al., 2024; Zheng et al., 2024; Jing et al., 2024) point out two primary reasons for the limitations in CLIPs finegrained learning capability. First, the original CLIP models text encoder supports only up to 77 tokens, restricting its capacity to process detailed descriptions and hindering its ability to capture nuanced textual information. Second, CLIP aligns entire images with corresponding text descriptions, making it challenging to extract valuable region-specific representations from visual features. Consequently, the model struggles to achieve fine-grained alignment between image regions and their corresponding textual attributes, limiting its effectiveness in complex recognition scenarios. To address these issues, researchers have proposed extending the positional encoding to support longer token sequences (Wu et al., 2024b; Zhang et al., 2024; Zheng et al., 2024) and integrating object detection datasets into CLIP training (Zhong et al., 2022; Jing et al., 2024). By aligning bounding boxes with category labels, these methods aim to enhance regional feature extraction. Although these approaches have shown some improvements, they still fall short in fine-grained visual recognition and open-vocabulary object detection. Existing methods (Jing et al., 2024; Zhang et al., 2024) typically introduce relatively few long captions, usually on million scale, which is inadequate for effective learning of fine-grained details. Additionally, aligning image regions with category labels limits semantic diversity, restricting the models generalization to open-world scenarFG-CLIP: Fine-Grained Visual and Textual Alignment ios. Furthermore, the lack of hard fine-grained negative samples limits the models ability to distinguish between objects of the same category but with different attributes. In this work, we introduce Fine-Grained CLIP (FG-CLIP), novel approach designed to enhance CLIPs fine-grained understanding capabilities through three key innovations. First, we significantly enhance global-level semantic alignment by generating long captions using state-of-the-art large multimodal models (LMMs) (Hong et al., 2024). This process introduces 1.6 billion long caption-image pairs, providing an unprecedented scale of data that allows FG-CLIP to capture nuanced details at the global-level semantic layer, thereby enhancing its ability to perceive complex and detailed information. Second, to improve fine-grained alignment between images and text, we develop high-quality visual grounding dataset. This dataset includes detailed descriptions for 40 million bounding boxes across 12 million images, ensuring that each region is precisely annotated with context-rich captions. By creating such an extensive and richly annotated dataset, we enable the model to learn precise and contextually rich representations, significantly enhancing its performance on tasks that require fine-grained understanding. Third, to further enhance model robustness and discrimination abilities, we introduce large-scale corpus of 10 million hard fine-grained negative samples. By incorporating these challenging negative samples into the training process, FGCLIP learns to distinguish subtle differences in semantically similar but distinct pairs, thereby significantly improving its performance across various downstream tasks. Compared to previous methods, FG-CLIP demonstrates significant improvements across wide range of benchmark tasks. Our comprehensive enhancements enable the model to achieve superior performance in capturing nuanced visual details, as evidenced by our state-of-the-art results on tasks such as fine-grained understanding, bounding box classification, long caption image-text retrieval, and open-vocabulary object detection. Moreover, when utilized as the backbone for LMMs (Liu et al., 2023b), FG-CLIP also demonstrates performance improvements in tasks involving attribute analysis (Hudson & Manning, 2019), object localization (Kazemzadeh et al., 2014), and reducing output hallucination (Li et al., 2023c). We provide visualization results in Appendix to demonstrate the improvement in fine-grained understanding. These results highlight FG-CLIPs effectiveness in capturing fine-grained image details and improving overall model performance. To facilitate future research and application, we make the related models, datasets, and code publicly available at https://github.com/360CVGroup/FGCLIP. 2. Related Work 2.1. Contrastive Language-Image Pre-training Contrastive learning has emerged as powerful paradigm in multimodal pre-training, significantly advancing the field of image-text alignment. Models like CLIP have revolutionized this area by leveraging large-scale image-text pairs to learn rich representations without explicit supervision. CLIP achieves this through dual-encoder architecture that maps images and their corresponding text descriptions into shared embedding space, where semantically similar pairs are pulled closer together while dissimilar pairs are pushed apart. This approach not only simplifies data labeling but also enables zero-shot transfer to downstream tasks, demonstrating impressive performance on various benchmarks such as image classification (Deng et al., 2009; Recht et al., 2019) and image-text retrieval (Young et al., 2014; Lin et al., 2014; Urbanek et al., 2024; Chen et al., 2024a). 2.2. Fine-Grained Understanding Despite its success, CLIP faces limitations in handling finegrained visual details. Its text encoder is constrained to 77 tokens, limiting its capacity to process detailed and complex descriptions. Additionally, CLIP aligns entire images with corresponding text, making it challenging to extract valuable region-specific representations. To address these limitations, models like LongCLIP (Zhang et al., 2024) extend the maximum token length of the text encoder, enabling it to handle longer and more detailed textual information. GLIP (Li et al., 2022) and RegionCLIP (Zhong et al., 2022) introduce grounding data, enhancing the models ability to align specific regions within images with corresponding text, thereby improving performance on downstream detection tasks (Xie et al., 2018; Gupta et al., 2019; Zhou et al., 2022b; Minderer et al., 2024). However, even with these improvements, existing models still struggle to fully capture and align fine-grained features across diverse datasets. 2.3. Image-Text Datasets Image-text datasets (Gu et al., 2022; Xie et al., 2023; Fu et al., 2024) play pivotal role in the performance of multimodal models. While existing datasets such as LAION (Schuhmann et al., 2021; 2022), COCO (Lin et al., 2014), Flickr30K (Young et al., 2014), and Conceptual Captions (Sharma et al., 2018; Changpinyo et al., 2021) offer valuable resources, they often emphasize general scene descriptions, neglecting fine-grained details critical for advanced applications. Researchers have adopted several strategies to mitigate these limitations. One approach involves leveraging advanced large multimodal models (Laurençon et al., 2024; Wang et al., 2024; Wu et al., 2024c; Chen et al., 2024b; Team et al., 2024) to refine and enrich FG-CLIP: Fine-Grained Visual and Textual Alignment Figure 1. Overview of the FG-CLIP. CLSimg denotes the image class features output by the Vision Transformer (ViT), while CLStext represents the class features summarized by the text model for multiple inputs, including long captions, short captions, region captions, and positive&negative descriptions of specific regions within images. FG-CLIPs training proceeds in two stages: the first stage leverages global-level caption-image pairs to achieve initial fine-grained alignment, while the second stage supplements these with additional region-level captions, including detailed region captions and positive/negative region descriptions to further refine the alignment. text descriptions through recaptioning. For instance, LongCLIP (Zhang et al., 2024) utilizes 1 million long captionimage pairs from ShareGPT4V (Chen et al., 2024a), and FineCLIP (Jing et al., 2024) constructs dataset of 2.5 million long caption-image pairs. Although these efforts enhance data richness, they remain limited in scale compared to the vast amount of data in the image-text field. Another strategy is to implement pseudo-labeling pipelines using pre-trained object detection models (Li et al., 2023b; Ma et al., 2024; Hou et al., 2024) to automatically generate finegrained pseudo-labels for region boxes, similar to the GRIT dataset utilized in Kosmos-2 (Peng et al., 2024). These methods help improve region-specific alignment but may introduce noise due to automated labeling. Another significant challenge is the scarcity of hard finegrained negative samples. Existing datasets predominantly consist of positive examples that are relatively easy to distinguish, limiting the models ability to learn subtle variations. The absence of hard negative samples impedes true finegrained understanding, as models struggle to discern small but meaningful differences in visual and textual features. Addressing this gap is essential for developing models capable of reliably performing fine-grained recognition and alignment tasks, thereby enabling them to handle the nuanced distinctions necessary for advanced applications. 3. Approach 3.1. Fine-Grained CLIP Figure 1 provides an overview of Fine-Grained CLIP (FGCLIP). Our proposed FG-CLIP extends the traditional dualencoder architecture of CLIP to better capture fine-grained details in images and text. We leverage two-stage training paradigm to achieve this enhancement. In the first stage, FGCLIP focuses on aligning global representations of images and text using only global contrastive learning. The second stage builds on this foundation by introducing regional contrastive learning and hard fine-grained negative samples learning, leveraging region-text data to further refine the models understanding of fine-grained details. Global Contrastive Learning. Global contrastive learning aims to enhance the models fine-grained understanding by introducing method of augmenting long caption alignment utilizing Large Multimodal Models (LMMs). This approach generates additional long captions that provide richer context and finer-grained descriptions. The inclusion of long captions enables the model to perceive and align with global-level semantic details, thereby enhancing fine-grained understanding and context awareness. In addition, we retain the alignment of short caption-image pairs. The long captions complement these short captions, ensur3 FG-CLIP: Fine-Grained Visual and Textual Alignment ing that the model learns from both detailed, nuanced long captions for complex semantic information and concise, direct short captions for basic concepts. This dual approach improves the models overall performance in capturing broader spectrum of visual information. In our framework, both short and long captions are aligned with images by utilizing the [CLS] token features extracted from the text encoder for the captions and the [CLS] token features from the image encoder for the images. To accommodate longer and more detailed captions while preserving the alignment of short captions, position embeddings of FG-CLIPs text encoder are extended. Specifically, for sequences shorter than or equal to 20 tokens, we use the original position embedding directly. For longer sequences, we apply linear interpolation with factor of 4 for positions beyond 20, extending the maximum length from 77 to 248 tokens. This modification ensures that the model can effectively handle longer, more descriptive text while maintaining computational efficiency. During each training step, the model employs both short caption and long caption for every image to ensure comprehensive and fine-grained understanding. Given an imagetext pair, the outputs of both encoders are embeddings Rd for images and Rd for text, where is the dimensionality of the embedding space. We compute the similarity between each pair using the cosine similarity metric: s(v, t) = tT vt . (1) The objective function for global contrastive learning is based on the InfoNCE loss (He et al., 2020), which maximizes the similarity between matching pairs while minimizing the similarity between mismatched pairs. Specifically, the loss for batch of image-text pairs is given by: Lglobal = 1 2N (cid:88) i=1 (log (cid:80)N exp(s(vi, ti)/τ ) j=1 exp(s(vi, tj)/τ ) exp(s(ti, vi)/τ ) j=1 exp(s(ti, vj)/τ ) ), (cid:80)N + log are then processed by applying average pooling over the tokens within each detected region, resulting in set of region embeddings {rk}K k=1, where denotes the total number of valid bounding boxes across all images within batch. This approach differs from global contrastive learning, which typically relies on the [CLS] token for deriving image-level features. For text, we segment the full-image caption into phrases or sentences that correspond to individual bounding boxes, obtaining text embeddings lk. The regional contrastive loss is defined as: Lregional = 1 2K (cid:88) i=1 (log (cid:80)K exp(s(ri, li)/τ ) j=1 exp(s(ri, lj)/τ ) exp(s(li, ri)/τ ) j=1 exp(s(li, rj)/τ ) ). (cid:80)K (3) + log This encourages the model to learn fine-grained alignments between specific regions and textual descriptions. Hard Fine-Grained Negative Samples Learning. To address the scarcity of challenging fine-grained negative samples, we introduce hard negative mining strategy. We define hard negative samples as those that are semantically close but not identical to the positive sample. These hard negatives are constructed by rewriting the descriptions of bounding boxes, modifying certain attributes to create subtle differences. The specific process of obtaining hard finegrained negative samples can be found in Section 3.2. To incorporate hard negative samples into the learning process, we extend the loss function to include term for hard negatives. For each region-text pair, we compute the similarity between the regional feature and both the positive description and the corresponding negative sample descriptions. The hard negative loss Lhard is defined as: Lhard ="
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) i=1 log exp(s(ri, li,1)/τ ) j=1 exp(s(ri, li,j)/τ ) (cid:80)M , (4) (2) where denotes the total number of captions for each region, with = 1 corresponding to the positive sample, and > 1 corresponding to the negative samples. where τ is learnable temperature parameter. This global contrastive learning significantly improving its detail perception capabilities in both granular and holistic contexts. Regional Contrastive Learning. Regional contrastive learning focuses on aligning specific regions within images with corresponding text segments. To achieve this, we employ RoIAlign (He et al., 2017) to extract regionspecific features from the image. These extracted features In the second stage, we integrate all three components: Global Contrastive Learning, Regional Contrastive Learning, and Hard Fine-Grained Negative Samples Learning, to ensure comprehensive and nuanced alignment tasks. The learning objective in the second stage combines these elements: = Lglobal + α Lregional + β Lhard. (5) 4 FG-CLIP: Fine-Grained Visual and Textual Alignment Here, the hyperparameters α and β are set to 0.1 and 0.5, respectively, to balance the regional contrastive loss and the hard negative loss, ensuring that each loss operates on similar scales. This integrated approach ensures that FG-CLIP not only captures global-level semantic details but also distinguishes subtle differences in semantically similar pairs, enhancing its overall performance across various downstream tasks. 3.2. Curated Dataset In this section, we describe the meticulous process of curating datasets for our FG-CLIP model, emphasizing both scale and quality to address the limitations of existing models in fine-grained understanding. Enhancing LAION-2B Data with Detailed Recaptioning. In the first stage of training, we utilize an enhanced version of the LAION-2B dataset (Schuhmann et al., 2022), where images are recaptioned with detailed descriptions generated by large multimodal models, i.e., CogVLM2-19B (Hong et al., 2024). This approach generates more detailed and contextually rich captions, crucial for capturing subtle differences in visual content. The original LAION-2B dataset often suffers from overly generic or imprecise captions, leading to suboptimal performance in fine-grained tasks. For instance, an image of bird might be described as \"a bird\", without specifying the species or environment. Such generic captions limit the models ability to recognize fine details. By leveraging advanced large multimodal models, we generate detailed descriptions that not only identify objects but also provide rich contextual information about their attributes, actions, and relationships within the scene. For instance, rather than generic description like \"a bird\", our refined captions read \"a red-winged blackbird perched on tree branch in park.\" Utilizing cluster of 160910B NPUs, the data processing is completed in 30 days. An ablation study detailed in Section 4.5 evaluates the impact of using these high-quality, detailed captions. The results demonstrate significant improvements in model performance across various tasks, underscoring the critical role of large-scale, high-quality text annotations in enhancing both model accuracy and context understanding. Creating High-Quality Visual Grounding Data. For the second stage of training, we develop high-quality visual grounding dataset, featuring precise region-specific captions and challenging negative samples. We curate the overall dataset based on GRIT (Peng et al., 2024) images. The process begins with generating detailed image captions using CogVLM2-19B (Hong et al., 2024), ensuring comprehensive and nuanced descriptions that capture the full context of each image. Following (Peng et al., 2024), we then use SpaCy (Honnibal et al., 2020) to parse the captions and extract the referring expressions. Subsequently, the images and referring expressions are fed into the pretrained object detection model, i.e., Yolo-World (Cheng et al., 2024) to obtain the associated bounding boxes. Non-maximum suppression is applied to eliminate overlapping bounding boxes, retaining only those with predicted confidence scores higher than 0.4. This process results in 12 million images and 40 million bounding boxes with fine-grained region captions. We provide examples of the images and their corresponding captions in Appendix A. Next, to create challenging fine-grained negative samples, we modify attributes of bounding box descriptions while keeping the object names unchanged. For this task, we employ an open-source large language model, Llama-3.170B (Dubey et al., 2024), to generate 10 negative samples for each positive sample. To ensure clarity, we remove special symbols such as semicolons, commas, and line breaks from the generated descriptions. quality check of 3,000 negative samples reveals that 98.9% are qualified, with only 1.1% considered noisea level within the expected tolerance for unsupervised methods. This process generates subtle variations that better reflect real-world scenarios where objects may appear similar but differ in specific details. We illustrate examples of the fine-grained negative samples in Appendix B. The resulting dataset includes 12 million images with finegrained captions, 40 million bounding boxes with detailed region descriptions, and 10 million hard negative samples. The data pipeline utilizes cluster of 160910B NPUs and takes 7 days to complete. This comprehensive dataset enhances the models ability to capture fine-grained details and provides robust foundation for training FG-CLIP to distinguish subtle differences in visual and textual features. 4. Experiments 4.1. Implementation Details In the first stage, we employ 160ASCEND 910B NPUs and train on dataset of 1.6 billion images, each paired with short and long texts. The model is initialized with weights from the original CLIP (Radford et al., 2021). For both ViT-B and ViT-L (Dosovitskiy, 2021) configurations, the batch size per NPU is set to 384. The learnable temperature parameter τ is initialized to 0.07. We utilize the AdamW optimizer with learning rate of 1e-4, weight decay of 0.05, β1 of 0.9, β2 of 0.98, and warmup steps for the first 200 iterations. The entire training process employs DeepSpeeds Zero-2 optimization technique and Bfloat16 precision to accelerate training, and the model is trained for one epoch. In the second stage, we employ 8NVIDIA H800 GPUs and train on dataset of 12 million images. Apart from long 5 FG-CLIP: Fine-Grained Visual and Textual Alignment Table 1. Results on FG-OVD benchmark. Accuracy is reported. Table 3. Performance on open-vocabulary object detection task. Method Backbone Fine-Grained Understanding hard medium easy trivial Method Backbone AP novel OV-COCO AP base 50 AP all 50 CLIP EVA-CLIP Long-CLIP FineCLIP FG-CLIP CLIP EVA-CLIP Long-CLIP FineCLIP FG-CLIP ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-L/14 ViT-L/14 ViT-L/14 ViT-L/14 ViT-L/ 12.0 14.0 9.2 26.8 46.1 15.4 18.3 9.6 22.8 48.4 23.1 30.1 18.4 49.8 66.6 25.3 38.4 19.7 46.0 69.5 22.2 29.4 16.2 50.4 68.7 25.7 35.2 16.0 46.0 71. 58.5 58.3 51.8 71.9 83.4 38.8 62.7 39.8 73.6 89.7 Table 2. Bounding box classification results. Method Backbone BBox Classification COCO LVIS Open Images CLIP EVA-CLIP RegionCLIP CLIPSelf Long-CLIP FineCLIP FG-CLIP CLIP EVA-CLIP Long-CLIP FineCLIP FG-CLIP ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-L/14 ViT-L/14 ViT-L/14 ViT-L/14 ViT-L/14 44.2 30.6 40.0 43.7 36.7 48.4 52.3 33.8 32.1 35.6 54.5 63. 20.9 14.4 22.2 7.8 18.2 23.3 28.6 9.3 18.3 10.4 22.5 38.3 15.3 8.8 19.1 11.4 14.9 18.1 20.6 8.3 9.3 8.9 19.1 23.8 and short captions, this dataset includes high-quality visual grounding annotations and hard fine-grained negative samples. The model is initialized with weights obtained from the first stage. The batch size per GPU is set to 512. We employ the AdamW optimizer with learning rate of 1e-6, weight decay of 0.001, β1 of 0.9, β2 of 0.98, and warmup steps for the first 50 iterations. Training acceleration techniques include DeepSpeeds Zero-2 optimization, CUDAs TF32 technology, and Bfloat16 precision, and the model is trained for one epoch. 4.2. Comparisons on Fine-grained Region-level Task In this section, the primary methods included for comparison are CLIP (Radford et al., 2021), EVA-CLIP (Sun et al., 2023), Long-CLIP (Zhang et al., 2024), and FineCLIP (Jing et al., 2024). Additional methods involved in openvocabulary detection include OV-RCNN (Zareian et al., 2021), RegionCLIP (Zhong et al., 2022), Detic (Zhou et al., 2022b), VLDet (Lin et al., 2022), RO-ViT (Kim et al., 2023b), CFM-ViT (Kim et al., 2023a), F-ViT(Wu et al., 2024a), and CLIPSelf (Wu et al., 2024a). OV-RCNN RegionCLIP Detic VLDet RO-ViT RO-ViT CFM-ViT F-ViT F-ViT+CLIPSelf F-ViT+FineCLIP F-ViT+FG-CLIP F-ViT F-ViT+CLIPSelf F-ViT+FineCLIP F-ViT+FG-CLIP RN50 RN50 RN50 RN50 ViT-B/16 ViT-L/16 ViT-L/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/ ViT-L/14 ViT-L/14 ViT-L/14 ViT-L/14 17.5 26.8 27.8 32.0 30.2 33.0 34.1 17.5 33.6 29.8 35.1 24.7 38.4 40.0 41.2 41.0 54.8 51.1 50.6 - - - 41.0 54.2 45.9 51. 53.6 60.6 57.2 58.0 34.9 47.5 45.0 45.8 41.5 47.7 46.0 34.9 48.8 41.7 47.4 46.0 54.8 52.7 53.6 Fine-Grained Understanding. Based on the fine-grained benchmark FG-OVD constructed by (Bianchi et al., 2024), we evaluate open-source image-text alignment models. Unlike previous benchmarks such as MSCOCO (Lin et al., 2014) and Flickr (Young et al., 2014), which rely on global information for matching, this benchmark focuses on identifying specific local regions within images. Each region has one corresponding positive description and ten negative descriptions, with the negative samples derived from the positive text. This benchmark primarily comprises four subsets of varying difficulty levels: hard, medium, easy, and trivial. The increasing difficulty across these subsets is reflected in the degree of distinction between the texts to be matched. In the hard, medium, and easy subsets, one, two, and three attribute words are replaced, respectively. In the trivial subset, the texts are entirely unrelated. For the source collection of specific attribute words, please refer to (Bianchi et al., 2024). During testing, following FineCLIP, we first extract dense features from the model by removing the last self-attention layer as suggested by (Zhou et al., 2022a). Subsequently, we combine the bounding box information provided by the benchmark with ROIAlign to obtain representative features. These features are used to calculate similarity scores with both positive and negative sample descriptions. Top-1 accuracy is adopted as the evaluation metric. As shown in Table 1, FG-CLIP achieves significant improvements over existing models, particularly on the challenging hard and medium subsets, thanks to its hard fine-grained negative samples learning strategy. Examples of different models performance can be found in Appendix D.1. Bounding Box Classification. To assess the models local information recognition capabilities, we conduct zero-shot testing on COCO-val2017 (Lin et al., 2014), LVIS (Gupta 6 FG-CLIP: Fine-Grained Visual and Textual Alignment Table 4. Comparisons on image-level tasks, including long/short caption image-text retrieval, and zero-shot image classification. Method Backbone ShareGPT4V T2I I2T CLIP EVA-CLIP Long-CLIP FineCLIP FG-CLIP CLIP EVA-CLIP Long-CLIP FineCLIP FG-CLIP ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-L/14 ViT-L/14 ViT-L/14 ViT-L/14 ViT-L/ 78.2 90.5 94.7 70.6 96.7 86.5 91.5 95.8 73.4 97.4 79.6 85.5 93.4 73.3 94.9 83.6 89.4 95.6 82.7 96.8 DCI I2T 45.5 41.9 51.7 35.5 61.8 37.2 47.2 44.2 40.1 66.7 T2I 43.0 41.2 57.3 34.4 60.6 36.4 47.8 52.5 46.2 66.1 MSCOCO T2I I2T Flickr30k T2I I2T ImageNet-1K ImageNet-v2 Top-1 Top-1 51.8 58.7 57.6 54.5 64.1 58.0 64.2 62.8 - 68. 32.7 41.6 40.4 40.2 45.4 37.1 47.9 46.3 - 50.9 82.2 85.7 85.9 82.5 90.7 87.4 89.2 90.0 - 93.7 62.1 71.2 70.7 67.9 76.4 67.3 77.9 76.2 - 81. 68.4 74.7 66.8 55.7 69.0 76.6 80.4 73.5 60.8 76.1 61.9 67.0 61.2 48.8 61.8 70.9 73.8 67.9 53.4 69.0 et al., 2019), and Open Images (Kuznetsova et al., 2020), following the protocol of (Jing et al., 2024). This evaluation focuses on how well the model can classify objects within bounding boxes using only textual descriptions. Similar to the fine-grained understanding, we integrate known bounding box information from the benchmark with ROIAlign to obtain localized dense representations. Using all categories as textual inputs, we perform matching and recognition for each bounding box, evaluating Top-1 accuracy. As shown in Table 2, FG-CLIP achieves leading performance in bounding box classification with the help of the regional contrastive learning strategy. Notably, LongCLIP (Zhang et al., 2024), fine-tuned from CLIP using long texts, shows significant decline in performance, indicating that long texts affect regional information granularity. Furthermore, FineCLIP uses region alignment data and incorporates real-time self-distillation scheme, leading to meaningful improvements. While FineCLIP makes significant progress, FG-CLIP excels it by integrating regional and global information. This approach enhances FG-CLIPs ability to accurately recognize and classify regions within images, highlighting the effectiveness of FG-CLIPs training strategy. Open-Vocabulary Object Detection. To further evaluate the fine-grained localization capability of our method, we employ FG-CLIP as the backbone for downstream openvocabulary detection tasks. Following prior work (Wu et al., 2024a), we employ two-stage detection architecture, FVIT, with frozen visual encoder. The comparative results are summarized in Table 3. Consistent with previous studies, we report the box AP at IoU 0.5 for base, novel, and all , AP base categories (AP novel 50 ) on OV-COCO. 50 50 Notably, AP novel is the primary focus of interest, as it measures the models ability to recognize novel objects. Our findings indicate that FG-CLIP achieves leading performance in open-vocabulary detection tasks, highlighting its effectiveness in recognizing and localizing novel objects. , and AP all 50 Table 5. Comparisons on General Multimodal Benchmarks. Method LLaVA-v1.5+CLIP LLaVA-v1.5+FG-CLIP GQA POPE 61.9 +1.0 62.9 85.9 +0.9 86.8 RefCOCO val testA testB 76.2 +5.2 81.4 83.4 +3.1 86.5 67.9 +7.0 74.9 4.3. Comparisons on Image-level Task Long/short Caption Image-Text Retrieval. To evaluate retrieval performance comprehensively, we conduct experiments on both long caption and short caption image-text retrieval tasks. For long-text retrieval, we follow the protocol of Long-CLIP and use the 1K subset of ShareGPT4V (Chen et al., 2024a) provided by it as the testset. Additionally, we incorporate more challenging long caption image-text pair dataset from DCI (Urbanek et al., 2024), consisting of 7,805 pairs, into the evaluation. For short-text retrieval, we employ the classic MSCOCO 5K (Lin et al., 2014) and Flickr 1K (Young et al., 2014) evaluation sets, which are widely used benchmarks for assessing image-text alignment models. As shown in Table 4, FG-CLIP achieves significant performance improvements in both long/short caption image-text retrieval tasks. The models ability to handle diverse caption lengths highlights its versatility and robustness in multimodal alignment. Zero-shot Image Classification. We evaluate the zeroshot classification performance of our model on ImageNet1K (Deng et al., 2009) and ImageNet-v2 (Recht et al., 2019). As illustrated in Table 4, despite being marginally behind EVA-CLIP, which is trained on larger dataset, FGCLIP demonstrates stable classification performance with enhanced regional and textual understanding capabilities compared to the original baseline, CLIP. Additionally, when compared to Long-CLIP and FineCLIP, both of which aim to enhance fine-grained recognition capabilities, our model exhibits notable advantage in classification accuracy. 7 FG-CLIP: Fine-Grained Visual and Textual Alignment Table 6. Ablation study results for FG-CLIP. This table compares the performance of different configurations of our FG-CLIP model across multiple evaluation metrics, including long caption image-text retrieval (DCI), short caption image-text retrieval (MSCOCO), bounding box classification (COCO-val2017), and fine-grained understanding (FG-OVD). The results highlight the incremental improvements achieved by incorporating global contrastive learning (Lglobal), regional contrastive learning (Lregional), and hard fine-grained negative samples learning (Lhard). Method CLIP FG-CLIP Stage1 +Stage2 (Lglobal) +Stage2 (Lglobal,Lregional) +Stage2 (Lglobal,Lregional,Lhard) Long Retrieval I2T T2I Short Retrieval I2T T2I BBox Classification TopTop-5 Fine-Grained Understanding hard medium easy 45.5 58.3 62.7 62.4 61.8 43. 57.5 61.2 61.1 60.6 51.8 64.6 64.4 64.7 64.1 32.7 44.9 46.4 45.7 45.4 44. 47.2 46.8 53.7 52.3 72.3 74.2 73.6 81.2 79.7 12.0 21.8 25.4 24.5 46.1 23. 41.6 46.8 47.1 66.6 22.2 36.2 42.9 49.5 68.7 4.4. Comparisons on General Multimodal Benchmarks We compare FG-CLIP as visual feature extractor for multimodal large language models with our baseline, CLIP. Specifically, we conduct experiments using LLaVA-v1.5-7B (Liu et al., 2023b), which itself is trained using CLIP. To ensure fair comparison, all parameter configurations are kept consistent with those in the original LLaVA, and the model is trained using the data provided by LLaVA. Our evaluation focuses on benchmark sets related to attribute analysis, object localization, and output hallucination, which are GQA (Hudson & Manning, 2019), RefCOCO (Kazemzadeh et al., 2014), and POPE (Li et al., 2023c), respectively. As shown in Table 5, FG-CLIP achieves certain improvements on GQA, which involves attribute-based question answering, and on POPE, which evaluates output hallucination. Additionally, it demonstrates significant gains on RefCOCO, benchmark set that involves both attribute analysis and object localization. These results indicate the effectiveness of FG-CLIPs training strategy and the data construction, which are specifically designed to enhance fine-grained recognition and regional alignment. We provide more results in Section D.3. 4.5. Ablation Study To systematically evaluate the contributions of different components in our FG-CLIP model, we conduct an ablation study with results summarized in Table 6. Global Contrastive Learning and Detailed Recaptioning Data. We start by comparing the original CLIP model with FG-CLIP Stage 1 and Stage 2 incorporating global contrastive learning Lglobal. The results demonstrate that generating detailed captions significantly enhances performance across various tasks. Specifically, FG-CLIP Stage 1 outperforms CLIP in all metrics, highlighting the importance of fine-grained training data. Further improvements are observed when adding Lglobal in Stage 2, particularly in long caption image-text retrieval (DCI (Urbanek et al., 2024)) and fine-grained understanding (FG-OVD (Bianchi et al., 2024)). This underscores the effectiveness of detailed caption data combined with global contrastive learning in improving model performance. Regional Contrastive Learning. We introduce regional contrastive learning Lregional to evaluate its impact on capturing detailed image regions. Compared to configurations using only Lglobal, adding Lregional leads to substantial improvements in bounding box classification accuracy from 46.8% to 53.7%, and FG-OVD easy dataset accuracy from 42.9% to 49.5%. These gains highlight the effectiveness of Lregional in refining the models ability to understand finegrained details within specific image regions. Moreover, this component maintains strong performance in both retrieval and classification tasks, demonstrating its versatility. Hard Fine-Grained Negative Samples Learning. We incorporate hard fine-grained negative samples learning Lhard to distinguish subtle differences in semantically similar but distinct region-text pairs. By comparing configurations with and without Lhard, we observe significant improvements in FG-OVD performance. Accuracy on the hard dataset increases from 24.5% to 46.1%, while on the medium dataset it rises from 47.1% to 66.6% and on the easy dataset it jumps from 49.5% to 68.7%. These results underscore the importance of Lhard in distinguishing subtle semantic differences. Hard fine-grained negative samples learning effectively addresses challenge cases, thereby enhancing the models stability and discriminative power. 5. Conclusion In this work, we introduce Fine-Grained CLIP (FG-CLIP), novel approach that significantly advances fine-grained understanding. By integrating advanced alignment techniques with large-scale, high-quality datasets and hard negative samples, FG-CLIP captures global-level and region-level semantic details and distinguishes subtle differences more effectively. Extensive experiments across diverse downstream tasks validate the models superior performance. The release 8 FG-CLIP: Fine-Grained Visual and Textual Alignment of our data, code, and models aims to foster further research and innovation in the field. Looking ahead, exploring the integration of more sophisticated multimodal models and expanding dataset diversity will be crucial for pushing the boundaries of fine-grained understanding."
        },
        {
            "title": "Impact Statement",
            "content": "This paper aims to advance the field of Machine Learning, which has broad implications for society. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: visual language model for few-shot learning. In NeurIPS, volume 35, pp. 2371623736, 2022. Bianchi, L., Carrara, F., Messina, N., Gennaro, C., and Falchi, F. The devil is in the fine-grained details: Evaluating openvocabulary object detectors for fine-grained understanding. In CVPR, pp. 2252022529, 2024. Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, pp. 35583568, 2021. Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., and Lin, D. Sharegpt4v: Improving large multi-modal models with better captions. In ECCV, pp. 370387, 2024a. Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, pp. 2418524198, 2024b. Cheng, T., Song, L., Ge, Y., Liu, W., Wang, X., and Shan, Y. Yoloworld: Real-time open-vocabulary object detection. In CVPR, pp. 1690116911, 2024. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: large-scale hierarchical image database. In CVPR, pp. 248255, 2009. Dosovitskiy, A. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Fu, L., Datta, G., Huang, H., Panitch, W. C.-H., Drake, J., Ortiz, J., Mukadam, M., Lambeta, M., Calandra, R., and Goldberg, K. touch, vision, and language dataset for multimodal alignment. In ICML, pp. 1408014101, 2024. Gu, J., Meng, X., Lu, G., Hou, L., Minzhe, N., Liang, X., Yao, L., Huang, R., Zhang, W., Jiang, X., et al. Wukong: 100 million large-scale chinese cross-modal pre-training benchmark. In NeurIPS, volume 35, pp. 2641826431, 2022. Gupta, A., Dollar, P., and Girshick, R. Lvis: dataset for large vocabulary instance segmentation. In CVPR, pp. 53565364, 2019. He, K., Gkioxari, G., Dollár, P., and Girshick, R. Mask r-cnn. In ICCV, pp. 29612969, 2017. He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. In CVPR, pp. 97299738, 2020. Hong, W., Wang, W., Ding, M., Yu, W., Lv, Q., Wang, Y., Cheng, Y., Huang, S., Ji, J., Xue, Z., et al. Cogvlm2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500, 2024. Honnibal, M., Montani, I., Van Landeghem, S., Boyd, A., et al. spacy: Industrial-strength natural language processing in python. 2020. Hou, X., Liu, M., Zhang, S., Wei, P., and Chen, B. Salience detr: Enhancing detection transformer with hierarchical salience filtering refinement. In CVPR, pp. 1757417583, 2024. Hudson, D. A. and Manning, C. D. Gqa: new dataset for realworld visual reasoning and compositional question answering. In CVPR, pp. 67006709, 2019. Jing, D., He, X., Luo, Y., Fei, N., Yang, G., Wei, W., Zhao, H., and Lu, Z. Fineclip: Self-distilled region-based clip for better fine-grained understanding. In NeurIPS, 2024. Kazemzadeh, S., Ordonez, V., Matten, M., and Berg, T. Referitgame: Referring to objects in photographs of natural scenes. In EMNLP, pp. 787798, 2014. Kim, D., Angelova, A., and Kuo, W. Contrastive feature masking open-vocabulary vision transformer. In ICCV, pp. 1560215612, 2023a. Kim, D., Angelova, A., and Kuo, W. Region-aware pretraining for open-vocabulary object detection with vision transformers. In CVPR, pp. 1114411154, 2023b. Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., PontTuset, J., Kamali, S., Popov, S., Malloci, M., Kolesnikov, A., et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 128(7):19561981, 2020. Laurençon, H., Marafioti, A., Sanh, V., and Tronchon, L. Building and better understanding vision-language models: insights and future directions. arXiv preprint arXiv:2408.12637, 2024. Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, pp. 1973019742, 2023a. Gabeff, V., Rußwurm, M., Tuia, D., and Mathis, A. Wildclip: Scene and animal attribute retrieval from camera trap data with domain-adapted vision-language models. IJCV, pp. 117, 2024. Li, J., Xie, C., Wu, X., Wang, B., and Leng, D. What makes good open-vocabulary detector: disassembling perspective. arXiv preprint arXiv:2309.00227, 2023b. FG-CLIP: Fine-Grained Visual and Textual Alignment Li, L. H., Zhang, P., Zhang, H., Yang, J., Li, C., Zhong, Y., Wang, L., Yuan, L., Zhang, L., Hwang, J.-N., et al. Grounded languageimage pre-training. In CVPR, pp. 1096510975, 2022. Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do imagenet classifiers generalize to imagenet? In ICML, pp. 53895400, 2019. Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W. X., and Wen, J.- R. Evaluating object hallucination in large vision-language models. In EMNLP, 2023c. URL https://openreview. net/forum?id=xozJw0kZXF. Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and Komatsuzaki, A. Laion-400m: Open dataset of clip-filtered 400 million imagetext pairs. arXiv preprint arXiv:2111.02114, 2021. Li, Z., Yang, B., Liu, Q., Ma, Z., Zhang, S., Yang, J., Sun, Y., Liu, Y., and Bai, X. Monkey: Image resolution and text label are important things for large multi-modal models. In CVPR, 2024. Lin, C., Sun, P., Jiang, Y., Luo, P., Qu, L., Haffari, G., Yuan, Z., and Cai, J. Learning object-language alignments for openvocabulary object detection. arXiv preprint arXiv:2211.14843, 2022. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In ECCV, pp. 740755, 2014. Lin, W., Zhao, Z., Zhang, X., Wu, C., Zhang, Y., Wang, Y., and Xie, W. Pmc-clip: Contrastive language-image pre-training using biomedical documents. In MICCAI, pp. 525536, 2023. Liu, C., Zhang, Y., Wang, H., Chen, W., Wang, F., Huang, Y., Shen, Y.-D., and Wang, L. Efficient token-guided image-text retrieval with consistent multimodal contrastive training. IEEE Transactions on Image Processing, 32:36223633, 2023a. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. In NeurIPS, volume 36, pp. 3489234916, 2023b. Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, pp. 216233, 2024. Ma, C., Jiang, Y., Wu, J., Yuan, Z., and Qi, X. Groma: Localized visual tokenization for grounding multimodal large language models. In ECCV, pp. 417435, 2024. Minderer, M., Gritsenko, A., and Houlsby, N. Scaling openvocabulary object detection. In NeurIPS, volume 36, 2024. Mokady, R., Hertz, A., and Bermano, A. H. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734, 2021. Pan, J., Ma, Q., and Bai, C. prior instruction representation framework for remote sensing image-text retrieval. In ACM MM, pp. 611620, 2023. Parelli, M., Delitzas, A., Hars, N., Vlassis, G., Anagnostidis, S., Bachmann, G., and Hofmann, T. Clip-guided vision-language pre-training for question answering in 3d scenes. In CVPR, pp. 56075612, 2023. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Ye, Q., and Wei, F. Kosmos-2: Grounding multimodal large language models to the world. In ICLR, 2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In ICML, pp. 87488763, 2021. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS, volume 35, pp. 2527825294, 2022. Sharma, P., Ding, N., Goodman, S., and Soricut, R. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, pp. 25562565, 2018. Sun, Q., Fang, Y., Wu, L., Wang, X., and Cao, Y. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. Sun, Z., Fang, Y., Wu, T., Zhang, P., Zang, Y., Kong, S., Xiong, Y., Lin, D., and Wang, J. Alpha-clip: clip model focusing on wherever you want. In CVPR, pp. 1301913029, 2024. Team, G., Georgiev, P., Lei, V. I., Burnell, R., Bai, L., Gulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S., et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Urbanek, J., Bordes, F., Astolfi, P., Williamson, M., Sharma, V., and Romero-Soriano, A. picture is worth more than 77 text tokens: Evaluating clip-style models on dense captions. In CVPR, pp. 2670026709, 2024. Wang, B., Xie, C., Leng, D., and Yin, Y. Iaa: Inner-adaptor architecture empowers frozen large language model with multimodal capabilities. In AAAI, volume 39, pp. 2103521043, 2025. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al. Qwen2-vl: Enhancing visionlanguage models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. Wu, S., Zhang, W., Xu, L., Jin, S., Li, X., Liu, W., and Loy, C. C. CLIPSelf: Vision transformer distills itself for openvocabulary dense prediction. In ICLR, 2024a. URL https: //openreview.net/forum?id=DjzvJCRsVf. Wu, W., Zheng, K., Ma, S., Lu, F., Guo, Y., Zhang, Y., Chen, W., Guo, Q., Shen, Y., and Zha, Z.-J. Lotlip: Improving languageimage pre-training for long text understanding. arXiv preprint arXiv:2410.05249, 2024b. Wu, Z., Chen, X., Pan, Z., Liu, X., Liu, W., Dai, D., Gao, H., Ma, Y., Wu, C., Wang, B., et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024c. Xie, C., Li, C., Zhang, B., Han, J., Zhen, X., and Chen, J. Memory attention networks for skeleton-based action recognition. In IJCAI, pp. 16391645, 2018. Xie, C., Cai, H., Li, J., Kong, F., Wu, X., Song, J., Morimitsu, H., Yao, L., Wang, D., Zhang, X., et al. Ccmb: large-scale chinese cross-modal benchmark. In ACM MM, pp. 42194227, 2023. 10 FG-CLIP: Fine-Grained Visual and Textual Alignment Yao, Y., Yu, T., Zhang, A., Wang, C., Cui, J., Zhu, H., Cai, T., Li, H., Zhao, W., He, Z., et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Young, P., Lai, A., Hodosh, M., and Hockenmaier, J. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:6778, 2014. Zareian, A., Rosa, K. D., Hu, D. H., and Chang, S.-F. Openvocabulary object detection using captions. In CVPR, pp. 14393 14402, 2021. Zhang, B., Zhang, P., Dong, X., Zang, Y., and Wang, J. Longclip: Unlocking the long-text capability of clip. In ECCV, pp. 310325, 2024. Zheng, K., Zhang, Y., Wu, W., Lu, F., Ma, S., Jin, X., Chen, W., and Shen, Y. Dreamlip: Language-image pre-training with long captions. In ECCV, pp. 7390, 2024. Zhong, Y., Yang, J., Zhang, P., Li, C., Codella, N., Li, L. H., Zhou, L., Dai, X., Yuan, L., Li, Y., et al. Regionclip: Region-based language-image pretraining. In CVPR, pp. 1679316803, 2022. Zhou, C., Loy, C. C., and Dai, B. Extract free dense labels from clip. In ECCV, pp. 696712, 2022a. Zhou, X., Girdhar, R., Joulin, A., Krähenbühl, P., and Misra, I. Detecting twenty-thousand classes using image-level supervision. In ECCV, pp. 350368, 2022b. 11 A. Examples of Curated Visual Grounding Data FG-CLIP: Fine-Grained Visual and Textual Alignment Figure 2 shows visual grounding examples utilized in our experiments. Each example comprises an image, accompanied by its corresponding long and short captions, as well as multiple region-specific annotations, each with detailed description. Figure 2. Examples of curated visual grounding data. 12 B. Positive and Negative Descriptions Related to Image Regions FG-CLIP: Fine-Grained Visual and Textual Alignment To generate hard fine-grained negative samples, we modify the attributes of bounding box descriptions while keeping object names unchanged. Figure 3 illustrates examples of positive and corresponding negative descriptions for image regions. Figure 3. Examples of positive and negative descriptions related to image regions. 13 C. Visualization Comparison FG-CLIP: Fine-Grained Visual and Textual Alignment As illustrated in Figure 4, we present comparison of similarity matrix visualizations for different methods using challenging sample images. We utilize the dense image feature extraction strategy introduced by (Zhou et al., 2022a). In the figure, warmer colors (e.g., yellow) denote higher similarity, whereas cooler colors (e.g., blue) indicate lower relevance. Our goal is for the model to precisely comprehend and interpret the fine-grained details within the images. In the first scenario, the image contains three dogs of different colors, and we compute the similarity matrix using only the phrase \"Black dog\" with each image token. It can be observed that CLIP and EVA-CLIP fail to accurately identify the target dog, FineCLIP captures some relevant tokens but not all, whereas FG-CLIP identifies larger number of relevant tokens, demonstrating superior performance. In the second scenario, the image contains multiple black entities, and we compute the similarity matrix using only the phrase \"Black nose\", which occupies very small area within the image. CLIP fails to identify the target, while EVA-CLIP and FineCLIP locate the target but also respond to many other black regions. In contrast, FG-CLIP accurately identifies the target, showcasing its precision in fine-grained localization. In the third scenario, the image features gemstones of three different colors, and we compute the similarity matrix using only the phrase \"Red gemstone\". Both CLIP and EVA-CLIP fail to locate the target at the bottom and exhibit high responses to gemstones of other colors. FineCLIP shows slightly lower localization accuracy compared to FG-CLIP, which precisely distinguishes between the colors of different gemstones and achieves more accurate localization. Black dog CLIP EVA-CLIP FineCLIP FG-CLIP Black nose CLIP EVA-CLIP FineCLIP FG-CLIP Red gemstone CLIP EVA-CLIP FineCLIP FG-CLIP Figure 4. Feature visual comparisons of different methods. Additionally, we utilize FG-CLIP to conduct correlation analysis between different input texts and the same image. The results in Figure 5 indicate that FG-CLIP provides precise positional understanding of different targets within the image. This demonstrates the models stable visual localization capabilities and its fine-grained understanding of image content. To evaluate the impact of hard fine-grained negative samples learning, we further provide the qualitative results in Figure 6. After performing hard negative sampling, our FG-CLIP can capture the regions more accurately. For example, the highlighted region of \"Man in red clothes\" with hard negative loss in 1st row shows significantly better than that without hard negative loss. 14 FG-CLIP: Fine-Grained Visual and Textual Alignment Figure 5. Feature visual comparisons of different input texts. Figure 6. Performance of hard fine-grained negative samples learning. 15 D. Further Experiments FG-CLIP: Fine-Grained Visual and Textual Alignment Table 7. Comparisons of different methods on fine-grained benchmark. Image with region Positive and Negative Region Descriptions CLIP EVA-CLIP FineCLIP FG-CLIP Origin: table made of dark brown wood. 1: table made of pink wood. 2: table made of dark brown paper. 3: table made of light grey wood. 4: table made of dark brown wool. 5: table made of yellow wood. 6: table made of dark brown velvet. 7: table made of dark brown text. 8: table made of grey wood. 9: table made of dark brown plastic. 10: table made of green wood. Origin: brown leather handbag. 1: brown metal handbag. 2: brown text handbag. 3: brown wool handbag. 4: orange leather handbag. 5: brown paper handbag. 6: light orange leather handbag. 7: brown glass handbag. 8: dark red leather handbag. 9: dark yellow leather handbag. 10: purple leather handbag. Origin: brown dog with black nose. 1: brown dog with light red nose. 2: brown dog with dark yellow nose. 3: light blue dog with black nose. 4: brown dog with yellow nose. 5: red dog with black nose. 6: brown dog with light orange nose. 7: brown dog with light yellow nose. 8: brown dog with light blue nose. 9: dark green dog with black nose. 10: brown dog with dark purple nose. Origin: light blue plastic trash can. 1: light blue stone trash can. 2: dark purple plastic trash can. 3: light blue wool trash can. 4: dark green plastic trash can. 5: dark orange plastic trash can. 6: black plastic trash can. 7: purple plastic trash can. 8: light blue crochet trash can. 9: light blue glass trash can. 10: light orange plastic trash can. Origin: red plastic bucket. 1: green plastic bucket. 2: red metal bucket. 3: red crochet bucket. 4: red ceramic bucket. 5: red fabric bucket. 6: red stone bucket. 7: red rattan bucket. 8: red wool bucket. 9: yellow plastic bucket. 10: light green plastic bucket. 0.73 0.0 0.48 0.27 0.27 0.38 1.0 0.94 0.53 0.45 0.47 0.62 0.45 0.53 0.0 0.32 0.41 0.08 1.0 0.43 0.31 0.21 0.75 0.37 0.10 0.40 0.38 1.0 0.16 0.0 0.26 0.88 0. 0.89 0.90 0.60 0.68 0.92 0.58 0.66 0.68 0.0 1.0 0.68 0.97 0.94 0.83 0.0 0.58 0.55 1.0 0.47 0.39 0.77 0.70 0.79 0.59 0.34 0.0 0.11 0.44 0.63 0.72 0.47 1.0 0.16 0.79 0.88 1.0 0.0 0.74 0.87 0.72 0.59 0.59 0.82 0.91 0.0 0.45 0.39 0.39 0.40 1.0 0.67 0.58 0.44 0.32 0.12 0.95 1.0 0.70 0.57 0.74 0.35 0.80 0.90 0.0 0.72 0. 0.53 0.77 0.29 0.10 0.06 0.05 0.10 0.51 0.0 1.0 0.87 0.62 0.58 0.48 0.39 0.03 0.38 0.0 0.38 1.0 0.86 0.64 0.78 0.56 0.90 0.63 0.34 1.0 0.08 0.75 0.15 0.11 0.0 0.76 0.64 0.93 0.0 1.0 0.59 0.71 0.83 0.48 0.41 0.48 0.52 1.0 0.03 0.37 0.99 0.0 0.65 0.36 0.10 0.52 0.02 0.70 0.46 0.54 0.10 0.15 0.0 0.84 0.58 0.14 1.0 0. 1.0 0.0 0.48 0.55 0.21 0.14 0.01 0.55 0.61 0.26 0.46 1.0 0.37 0.50 0.13 0.53 0.60 0.56 0.0 0.93 0.73 0.27 1.0 0.80 0.90 0.0 0.86 0.32 0.85 0.83 0.52 0.13 0.73 1.0 0.77 0.64 0.29 0.65 0.87 0.77 0.75 0.0 0.64 0.88 1.0 0.68 0.61 0.39 0.58 0.43 0.48 0.0 0.31 0.67 0.28 D.1. Comparison of Different Methods on Fine-Grained Benchmark As shown in Table 7, we select several samples from the test set of FG-OVD (Bianchi et al., 2024) and visualize the comparison results of different methods. We employ the testing strategy detailed in Section 4.2 and match the text with localized dense feature. The similarity scores computed between regions and texts are normalized, where the sentence with the lowest similarity is assigned score of 0.0, and the sentence with the highest similarity is assigned score of 1.0. 16 FG-CLIP: Fine-Grained Visual and Textual Alignment FG-CLIP demonstrates strong capability in identifying these extremely difficult samples, whereas other methods struggle to achieve comparable performance. D.2. Performance Comparison on Identical Datasets Table 8. Comparisons of different methods on the same dataset. Method FineCLIP FineCLIP FG-CLIP (Ours) Data Source FineCLIP (CC2.5M) FG-CLIP (12M) FG-CLIP (12M) COCO-Box-Top-1 COCO-Retrieval-I2T COCO-Retrieval-T2I 50.7 53.5 56.1 54.4 59.6 65.9 40.2 46.2 47. To evaluate the effectiveness of our proposed FG-CLIP method, we conduct experiments on the same dataset to ensure fair comparison. Specifically, we compare FineCLIP and FG-CLIP using the 12M dataset due to time constraints, instead of the larger 1.6B+12M setup. From Table 8, the substantial improvements (Row 1 -> Row 2 and Row 2 -> Row 3) highlight that both our proposed dataset and model architecture are significant for FG-CLIP. D.3. Performance on General Multimodal Benchmarks Table 9. Comparisons on General Multimodal Benchmarks. Method GQA POPE RefCOCO val testA testB MMBench-EN MMBench-CN dev dev test test LLaVA-v1.5+CLIP LLaVA-v1.5+FG-CLIP 61.9 +1.0 62.9 85.9 +0.9 86.8 76.2 83.4 +5.2 +3.1 86.5 81.4 67.9 65.1 +7.0 +1.5 66.6 74. 66.5 +0.2 66.7 58.2 +0.6 58.8 58.4 +0.9 59.3 In addition to GQA, POPE, and RefCOCO, we conduct experiments on other general multimodal benchmarks (Liu et al., 2024). The experimental results in Table 9 show that LLaVA with FG-CLIP achieves better performance."
        }
    ],
    "affiliations": [
        "360 AI Research"
    ]
}