{
    "paper_title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks",
    "authors": [
        "Fali Wang",
        "Hui Liu",
        "Zhenwei Dai",
        "Jingying Zeng",
        "Zhiwei Zhang",
        "Zongyu Wu",
        "Chen Luo",
        "Zhen Li",
        "Xianfeng Tang",
        "Qi He",
        "Suhang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Test-time scaling (TTS) enhances the performance of large language models (LLMs) by allocating additional compute resources during inference. However, existing research primarily investigates TTS in single-stage tasks; while many real-world problems are multi-stage complex tasks, composed of a sequence of heterogeneous subtasks with each subtask requires LLM of specific capability. Therefore, we study a novel problem: the test-time compute-optimal scaling in multi-stage complex tasks, aiming to select suitable models and allocate budgets per subtask to maximize overall performance. TTS in multi-stage tasks introduces two fundamental challenges: (i) The combinatorial search space of model and budget allocations, combined with the high cost of inference, makes brute-force search impractical. (ii) The optimal model and budget allocations across subtasks are interdependent, increasing the complexity of the compute-optimal search. To address this gap, we conduct extensive pilot experiments on four tasks across six datasets, deriving three empirical insights characterizing the behavior of LLMs in multi-stage complex tasks. Informed by these insights, we propose AgentTTS, an LLM-agent-based framework that autonomously searches for compute-optimal allocations through iterative feedback-driven interactions with the execution environment. Experimental results demonstrate that AgentTTS significantly outperforms traditional and other LLM-based baselines in search efficiency, and shows improved robustness to varying training set sizes and enhanced interpretability."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 0 9 8 0 0 . 8 0 5 2 : r AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks Fali Wang1, Hui Liu2, Zhenwei Dai2, Jingying Zeng2, Zhiwei Zhang1, Zongyu Wu1, Chen Luo2, Zhen Li2, Xianfeng Tang2, Qi He2, Suhang Wang1, 1The Pennsylvania State University, University Park, PA, USA 2Amazon, Palo Alto, CA, USA {fqw5095,zbz5349,zzw5373,szw494}@psu.edu {liunhu,zejingyi,zwdai,cheluo,xianft}@amazon.com"
        },
        {
            "title": "Abstract",
            "content": "Test-time scaling (TTS) enhances the performance of large language models (LLMs) by allocating additional compute resources during inference. However, existing research primarily investigates TTS in single-stage tasks; while many real-world problems are multi-stage complex tasks, composed of sequence of heterogeneous subtasks with each subtask requires LLM of specific capability. Therefore, we study novel problem: the test-time compute-optimal scaling in multi-stage complex tasks, aiming to select suitable models and allocate budgets per subtask to maximize overall performance. TTS in multi-stage tasks introduces two fundamental challenges: (i) The combinatorial search space of model and budget allocations, combined with the high cost of inference, makes brute-force search impractical. (ii) The optimal model and budget allocations across subtasks are interdependent, increasing the complexity of the compute-optimal search. To address this gap, we conduct extensive pilot experiments on four tasks across six datasets, deriving three empirical insights characterizing the behavior of LLMs in multi-stage complex tasks. Informed by these insights, we propose AgentTTS, an LLM-agent-based framework that autonomously searches for compute-optimal allocations through iterative feedback-driven interactions with the execution environment. Experimental results demonstrate that AgentTTS significantly outperforms traditional and other LLM-based baselines in search efficiency, and shows improved robustness to varying training set sizes and enhanced interpretability."
        },
        {
            "title": "Introduction",
            "content": "Test-time scaling (TTS), which refers to allocating additional computational resources during inference, has shown promising results in improving the performance of large language models (LLMs) [2, 42, 52, 32, 19, 51, 26]. For example, Brown et al. [2] scaled inference compute via repeated sampling of candidate solutions, using verifier to select the best prediction, enabling weak small model to outperform single-sample state-of-the-art models. Despite its effectiveness, existing methods are primarily designed for single-stage tasks, where the model performs single function, such as mathematical problem solving [42, 2, 52, 32, 19, 26, 14] or code generation [2, 33]. However, many real-world applications involve multi-stage complex tasks, for which compute allocation remains underexplored. These tasks involve sequential execution of heterogeneous subtasks to accomplish complex objectives. Representative examples include retrieval-then-generation QA systems [16], waterfall-style software development (comprising requirement analysis, system design, coding, and testing) [28], and multi-agent task automation (such as task decomposition, tool selection, Work done during an internship at Amazon. Preprint. Under review. and parameter prediction) [31]. Each subtask within multi-stage workflow often requires model with specific capabilities [13]. For example, in retrieval-then-generation QA task, retrieval benefits from large models with strong long-context understanding, while generation can achieve competitive performance using smaller models with repeated sampling (see Fig. 1(a)(b)). Therefore, multi-stage tasks demand models with diverse types and levels of capabilities, challenging existing TTS methods. To bridge this gap, we formulate and study novel problem: test-time compute-optimal scaling in multi-stage complex tasks: for complex task composed of sequence of subtasks and each subtask has set of candidate models to choose from, given total compute budget, the objective is to select the appropriate model and allocate the compute budget for each subtask to maximize overall task performance. This problem presents two major challenges. (i) The search space is large due to the combinatorial choices of models and budget allocations across subtasks. For instance, in software development with three subtasks and two model options each (3B and 70B), the number of configurations can reach up to 106 (see Appendix A.1 for calculation example), with each configuration requiring time-consuming inference (often several hours), rendering brute-force search impractical. (ii) Subtasks are not independent. The compute allocation for one subtask affects the performance and budget requirements of others. As shown in Fig. 1(b-d), high-quality retrieval can significantly reduce the compute required by the downstream generation subtask to achieve peak performance. In contrast, poor retrieval quality necessitates increased computation for generation, either through additional sampling or the use of larger models, to compensate for degraded input quality. These challenges highlight the need for an efficient search strategy that can handle the large and interdependent search space. To address these challenges, we first conduct preliminary experiments to characterize the behavior of LLMs under test-time scaling in multi-stage tasks. From experiments, we derive three generalizable insights across four task types: (1) Different subtasks exhibit distinct preferences between large and small models; (2) Increasing test-time compute initially improves performance, but beyond certain point, additional compute yields diminishing or no gains; (3) The compute allocated to earlier subtasks influences the scaling dynamics and compute needs of downstream subtasks. Guided by these insights, we propose AgentTTS, novel LLM-agent-based framework designed to efficiently search for compute-optimal budget allocations in multi-stage tasks. Recent works [20, 53, 46, 25, 23] have shown that LLM-based agents are effective in planning and searching for hyperparameter optimization. Given the capability of LLMs in understanding and following instructions, AgentTTS integrates our observed test-time scaling insights into the LLM-agent search process. AgentTTS consists of three key components: the Agent, Archive, and Environment. The Agent begins by generating an initial set of trials based on Insight 1, which guides model preferences for each subtask. These trials are executed by the Environment, which evaluates them on the actual task platform and returns performance feedback. The Archive stores history of generated trials, guidelines, and feedback. In subsequent stages, the Agent produces new trials and guidelines informed by Insights 2 and 3. This iterative process continues until predefined stopping criterion is met. By leveraging LLM-based search, AgentTTS offers two advantages: (i) interpretability, through explicit guideline generation that explains decision rationales; and (ii) robustness, in navigating non-smooth search spaces commonly found in test-time scaling. Our main contributions are: (i) We study novel problem of test-time compute-optimal scaling for multi-stage complex tasks; (ii) We identify three key insights that uncover fundamental patterns of test-time scaling in multi-stage tasks and motivate the design of AgentTTS, an efficient LLM-agentbased framework for searching compute-optimal configurations in this problem setting. and (iii) Comprehensive evaluations on six datasets show that AgentTTS achieves strong search efficiency, transparent interpretability in generating new trials, and robustness to non-smooth search landscapes."
        },
        {
            "title": "2 Related Work",
            "content": "Test-time Scaling and Compute-optimal Strategy. Test-time scaling (TTS) enhances LLM performance by allocating additional compute during inference [2, 42]. Existing methods fall into two categories: sequential scaling[24, 6, 32, 26, 45], which iteratively refines outputs but depends on good initial responses, and parallel scaling[2, 52, 33, 34, 9, 32, 19, 42], which generates multiple outputs and aggregates them using reward-based selection (e.g., repeated sampling [2, 52], Bestof-N [34], or tree search [42]). Recent work reduces reliance on reward models by using LLMs as fusers [13, 17, 29, 3]. Parallel scaling is preferred for complex tasks due to better scalability and 2 broader solution coverage [32]. Hence, we adopt repeated sampling with fusion. Research on testtime compute-optimal scaling shows that small models with optimal strategies can outperform larger ones [2, 42, 19, 51, 32, 38]. Approaches include difficulty-aware model selection [32], reward-guided voting [42], and budget-aware prompting [51]. However, they focus on single-stage tasks, while we extend this to multi-stage tasks, where budgets must be adaptively allocated across interdependent subtasks to maximize overall performance. more detailed discussion is given in Appendix A.11.1. LLMs for Hyperparameter Optimization. LLMs have become powerful tools for hyperparameter optimization (HPO), surpassing traditional AutoML techniques such as Bayesian Optimization (BO) [15, 30] by leveraging contextual reasoning and prior knowledge [8]. LLM-based HPO research generally follows two directions: (1) reducing the search space and (2) directly generating hyperparameter configurations. For the former, LLMs have been used to prune large search spaces in NAS and HPO [49, 25, 23, 21], as seen in GPT-NAS [49], AutoM3L [23], and Llambo [21]. For the latter, recent works [4, 59, 55, 1, 12, 53, 18, 54] treat LLMs as autonomous optimizers. Systems like AutoMMLab [46], GENIUS [59], MLCopilot [53], and AgentHPO [20] refine trials via feedback and experience. We extend this line of work to compute-optimal test-time scaling in multi-stage tasks. more detailed introduction of related work is given in Appendix A.11.2."
        },
        {
            "title": "3 Preliminary Knowledge and Problem Definition",
            "content": "Problem Definition. We define multi-stage complex task = [T1, T2, . . . , Tn] as comprising simpler subtasks. Each subtask Ti has set of candidate models Mi Mi, where each Mi is tailored for subtask Ti. Given fixed total computational budget for the entire complex task , each subtask must be assigned portion Bi such that (cid:80)n i=1 Bi = B. For subtask Ti, there exists trade-off between using larger model with fewer inference samples and smaller model with more samples, constrained by the assigned budget Bi. Our research problem is defined as Definition 1 (Test-time compute-optimal budget allocation in multi-stage tasks). Given fixed total computational budget for multi-stage complex task , how can we optimally allocate the computational budget among subtasks (cid:55) {B1, B2, . . . , Bn}, select appropriate models Mi, and effectively distribute the allocated resources to maximize overall performance? Test-time Scaling Mode: Repeated Sampling with Fusion. We adopt the Repeated Sampling with Fusion strategy for test-time scaling, as it does not rely on additional reward models or verifiers compared to Best-of-N [34, 9] or tree search algorithms [42, 50], and it offers greater scalability compared to sequential scaling [52, 2]. Given problem p, language model with parameters θ, test-time scaling is performed by increasing the number of repeated samples k. To stimulate diverse generations, we set the temperature hyperparameter to 0.9 throughout. fusion prompt is then used to aggregate the multiple candidate solutions generated through repeated sampling: = ffuse(S, ), = {si 1 k}, (1) where denotes the set of sampled responses, each si is independently drawn from the model conditioned on the input prompt and model parameters θ. The fusion function ffuse integrates the sampled responses using fusion prompt, as described in Appendix A.9. Notably, the same LLM is used for both generating solutions and performing fusion. si (s p, θ) Insights of Test-time Scaling on Multi-stage Complex Tasks Allocating computational budgets for TTS in multi-stage complex tasks has significant challenges: (i) the search space expands exponentially as the number of subtasks increases, making exhaustive search impractical; (ii) subtasks typically require models tailored to their specific characteristics, rendering uniform model assignment inefficient; (iii) the scaling strategies employed in earlier subtasks directly impact subsequent stages. To address the challenges, we first conduct pilot experiments to understand fundamental patterns in LLM behavior under TTS in complex tasks, which pave the way to design compute-optimal scaling strategy specifically tailored for multi-stage scenarios. 4.1 Experimental Setting In this subsection, we first briefly introduce the datasets, models, and metrics used across the four multi-stage complex tasks. Then, we describe unified budget conversion approach across models 3 and tasks, followed by an example using the inference FLOPs as the primary compute cost metric. Detailed descriptions of the datasets, models, and metrics are provided in Appendix A.4. Tasks, Datasets, and Models We conduct preliminary experiments on four multi-stage tasks: (i) Retrieval-based Question Answering using 2WikiMultiHopQA [11] and HotpotQA [47] datasets, (ii) Knowledge Graph Question Answering using CWQ [35] and WebQSP [48], (iii) Task Automation, using TaskBench [31], and (iv) Automated Software Development, using ChatDev [28]. Subtasks differ in prompt and generation lengths; retrieval uses longer prompts, while QA needs longer generations. We consider models ranging from 3B to 72B. Details of task specifications, datasets, models, and evaluation metrics are provided in Appendix A.4 and A.5. Unified Budget Conversion Across Models and Tasks Different models and tasks demand varying levels of computational resources. Larger models require more inference FLOPs, and complex tasks typically involve longer input or output tokens, leading to increased compute costs. These disparities complicate fair comparisons of computational overhead across models and tasks. To address this, we propose budget normalization framework that equates the cost of fewer inference samples from larger models with that of more samples from smaller models, while explicitly accounting for task-specific compute variations. Formally, let the total computational cost of sampling times from model on task be denoted as fcost(M, S, ), and the corresponding normalized compute budget as = fbudget(M, S, ). The cost metric may reflect user-defined preferences, such as inference FLOPs, wall-clock time, or monetary cost. To establish common unit of budget, we define it as the cost of single inference pass by the smallest model (e.g., LLaMA 3B) on the lowest computationally consuming task Tlowest: fbudget(Msmallest, 1, Tlowest) = 1. (2) Given model Mℓ, sample count Sℓ, and task Tℓ, we compute the equivalent budget by equating its total cost to that of the smallest model executing Ssmallest samples on the lowest-consuming task: = fbudget(Mℓ, Sℓ, Tℓ) = Ssmallest such that fcost(Mℓ, Sℓ, Tℓ) = fcost(Msmallest, Ssmallest, Tlowest) (3) This formulation expresses the inference budget of any model-task pair as the number of equivalent samples that the smallest model would generate on the least computationally intensive task. Compute Cost Metric and Budget Definition. Different stakeholders prioritize different cost metrics. Consumers are typically concerned with monetary expenses, such as the price per million input and output tokens. LLM researchers focus on computational cost, such as inference FLOPs, due to constraints of limited GPU memory. Discussion of using API price as cost metric is in Appendix A.3. In this work, we adopt inference FLOPs as the primary cost metric throughout. The corresponding budget conversion is stated in the theory below. The proof is in Appendix A.2. Theorem 1 (Normalized Budget Function). Given configuration (Mℓ, Sℓ, Tℓ) where Mℓ is the model size, Sℓ the number of samples, and Tℓ = (Np,ℓ, Nd,ℓ) the task specification where Np,ℓ and Nd,ℓ is the average prompt and generation lengths, the equivalent normalized budget under the base configuration (Msmallest = 3B, Np,lowest = 128, Nd,lowest = 64) is: = fbudget(Mℓ, Sℓ, Tℓ) = 2αβ2Sℓ β1 + 2(αβ2 1) (4) where α = Mℓ Msmallest , β1 = Np,ℓ Nd,ℓ , β2 = Np,ℓ Np,lowest . Under this conversion, the budget intuitively represents: how many passes using 3B-parameter model on the base configuration Tlowest would incur the same cost as given sampling Sℓ times with model Mℓ on task Tℓ? The base configuration corresponds to the minimal compute setting among our subtasks (see Table 3 in the Appendix) and aligns with standard defaults used in short-form QA tasks, such as QA in WebQSP. 4.2 Preliminary Experimental Results and Insights We use the FLOPs-based budget conversion in Eq. 4 to analyze performance variance with increasing sample count and compute budget, and to guide budget allocation in our method in the next section. Fig. 1 shows how subtask performance in retrieval-based question answering varies with increasing test-time sampling and computational budget (FLOPs). In Fig. 1 (a), performance on the retrieval subtask exhibits strong positive correlation with model size, while smaller models provide only 4 Figure 1: Performance variance on 2WikiMultiHopQA with increasing sampling and inference FLOPs. Top: performance by sample count; bottom: performance by log-scaled inference FLOPs. (a) Retrieval accuracy measured by Retrieval F1 (Ret_F1). (b-d) QA performance under varying retrieval quality levels measured by Gen_EM, the exact match between the generated answer and ground truth marginal improvements even with increased budget. This suggests that retrieval benefits from larger models, where fewer high-capacity samples yield better results. In contrast, Fig. 1 (b) shows that for the question answering subtask, under limited budget (e.g., < 1014 FLOPs), LLaMA-3 3B and LLaMA-3 8B outperform LLaMA-3 70B, indicating preference for smaller models. Similar trends are observed in Fig. 10 (a)(b) and across other benchmarks (see Fig. 11-14 in Appendix A.6). These discrepancies arise from subtask-specific demands: retrieval emphasizes long-context understanding, favoring large models; while question answering primarily involves extracting information from retrieved content, where smaller models excel and can better exploit test-time scaling via repeated sampling. Thus, different subtasks exhibit distinct preferences between language models and sampling frequency, depending on the specific capabilities required by each subtask (Insight 1). Fig. 1 (b-d) reveal non-monotonic performance trend across models in the question answering subtask, which is sensitive to test-time compute. As the number of sampling repetitions increases, performance initially improves but often fluctuates or declines after reaching an optimal budget. For example, LLaMA-3 3B reaches peak performance at 10 samples (FLOPs budget 2 1013) in Fig. 9 (b), indicating that additional compute does not always yield better results in this subtask. Similar patterns are observed in Fig. 10 (b-d) and other benchmarks (see Appendix A.6). This is because, as the number of candidates grows, fusion becomes more complex and may become performance bottleneck. Smaller models, with limited capacity, struggle more under high sampling and tend to degrade, whereas larger models are more capable of handling extensive fusion and show greater tolerance. In conclusion, subtask-level test-time scaling typically exhibits an optimal budget, beyond which more budget often leads to limited or even negative returns (Insight 2). Fig. 1 (b-d) show how the performance of the question answering subtask varies under different levels of retrieval quality, with F1 scores approximately 0.80, 0.60, and 0.35, respectively. When high-quality retrieval is provided by larger model (Fig. 1 (b)), we observe: (1) the optimal test-time budget for question answering is reached earlier. For instance, LLaMA-3 8B peaks at 10 samples (budget around 2 1013 FLOPs); and (2) smaller models (3B and 8B) may outperform the larger model (70B) under the same compute budget. In contrast, when retrieval quality is low (Fig. 1 (c)(d)), the peak performance is delayed. For example, with Ret_F1 = 0.6, LLaMA-3 8B peaks at 20 samples (budget about 3 1013 FLOPs), while with Ret_F1 = 0.35, even 90 samples (budget near 1014 FLOPs) still do not reach peak performance. Moreover, under low retrieval quality, smaller models are less likely to outperform larger ones. For instance, in Fig. 1 (d), the peak performance of the 3B and 8B models does not surpass even the lowest performance of the 70B model. These findings indicate that the scaling behavior of subtask is affected by the performance of its preceding subtasks. Poor retrieval increases downstream task difficulty, requiring the model to compensate for missing information, where larger models are more beneficial. Similar trends are observed in Fig. 10 (b-d) and other benchmarks in Appendix A.6. We conclude that budget allocation for preceding subtask impacts the model selection and optimal budget of subsequent subtasks (Insight 3). 5 Algorithm 1 Compute-Optimal Test-time Budget Allocation Require: Agent A, Model set M, Subtasks , Environment E, Total budget B. 1: Initialize experiment archive: . 2: Initialize candidate trials: A.initialize(M, ) 3: Obtain feedback: E.execute(C) 4: while stopping criterion is not met do 5: 6: 7: 8: 9: end while 10: return Best-performing trial from L. Generate exploration guidelines: A.generate_guidelines(C, S) Update experiment log: {(C, S, G)} Generate new candidate trials: A.generate(G, M, , B) Obtain feedback: E.execute(C) (Insight 1, Eq. 5) (Insights 2, 3) (Eq. 5)"
        },
        {
            "title": "5 AgentTTS: Agent for Test-time Scaling Budget Allocation",
            "content": "The insights in Sec. 4 provide guidance on how to efficiently search for effective budget and model allocations in multi-stage tasks. As LLMs have shown strong capabilities in planning and reasoning, we propose AgentTTS (Agent for Test-Time compute-optimal Scaling), which integrates these insights into an LLM-based agent to autonomously navigate the compute allocation space and capture subtask-specific model preferences and optimal budget, and inter-subtask dependencies. An illustration of AgentTTS is in Fig. 2. Next, we elaborate on the design of AgentTTS. the Agent, Archive, Overview of AgentTTS. As shown in Fig. 2 (b-d), the framework consists of three core components: and Environment. the Agent generInitially, ates batch of candidate trials (each looks like (M1, B1, M2, B2, . . .), guided by Insight 1, which reflects preliminary model preferences across subtasks. These trials are stored in the Archive and forwarded to the Environment for evaluation. The resulting performance feedback is returned to the Agent and used to construct initial guidelines that suggest whether subsequent trials should prioritize smaller or larger models. Both feedback and guidelines are retained in the Archive for future reference. In subsequent iterations, the Agent generates new trials based on the existing guidelines and adds new guidelines by Insight 2 and Insight 3. The Archive continuously stores the evolving guidelines along with corresponding trials and performance feedback, while the Environment evaluates each batch of trials in practical execution environment and returns feedback. This loop repeats until predefined stopping criterion is met. The full procedure is outlined in Algo. 1, and the class diagram is shown in Fig. 18 (Appendix). Figure 2: Overview of LLM Agent for Test-time Scaling Budget Allocation. Agent component The Agent, implemented using an LLM, is responsible for generating test-time budget allocation candidate trials and guidelines, as shown in Fig. 2 (b). Although LLMs perform well in hyperparameter optimization for machine learning models, they lack knowledge of test-time scaling, relatively new technique. To address this, we incorporate Insight 1 into the initial search stage and Insights 2 and 3 into all stages. Insight 1 reveals that different subtasks prefer models with specific capabilities. Therefore, matching suitable model early steers the search toward effective configurations, reducing wasted effort on inferior options. To estimate each subtasks model preference, we perform the following during the initialization stage. Let Bmax = fbudget(Mi,max, 1, Ti) be the budget required for single sample from the largest available model Mi,max for subtask Ti and Bmin = fbudget(Mi,min, 1, Ti) the budget from the smallest model. For each subtask Ti, we compare all candidate models that fit within Bmax , while fixing all other subtasks to use their respective largest available model with one-pass inference. This ensures fair comparison across model candidates for the target subtask Ti. Under the condition that Bmax , the largest model for subtask occupies too much budget to allow > (cid:80) j=i Bmin feasible allocations for the remaining subtasks. In this case, we progressively downsize the model until the largest available model Mi,max is found. This operation is repeated for each subtask. Upon receiving initial feedback from the Environment module, the Agent summarizes the initial guidelines (Algo. Line 5) using the prompt provided in Appendix A.9, based on performance comparisons. If the large model significantly outperforms the small model, the large one is preferred. Otherwise, the small model is preferred due to its greater flexibility in subsequent exploration. The resulting guideline specifies which model should be prioritized in the later stages of the search. The instruction to identify the preferred model is used only in the initial search stage. The overall procedure is implemented in Line 2 of Alg. 1 and formalized in Eq. 5 below = (cid:40) {(..., Mi, Bi, ...) 1 N, Mi Mi, Bi = Bmax {c A.generate(G, M, , B)} , } , initial stage subsequent stages (5) where each trial is represented as = (M1, B1, . . . , Mn, Bn), Mi Mi is the selected model, and Bi is its assigned budget for subtask i. denotes the total compute budget. Mi,max is the largest available model for subtask i, and the configuration (..., Mi, Bi, ...) indicates that other subtasks are fixed to their respective available largest models with one-pass inference. Insight 2 shows that increasing test-time compute initially improves performance, but beyond an optimal point, it may cause oscillation or degradation within subtask. Each model thus has task-specific optimal budget: too little limits performance, while too much wastes compute and harms other subtasks. Identifying this optimal budget per subtask is key to maximizing overall performance efficiently. To support this, we put Insight 2 in the guideline generation prompt (Appendix A.9) and ask LLM to follow Insight 2 to: identify the search direction for finding the optimal number of samples for each subtask. This ensures that the agent focuses the next round of search within the right scope, enabling faster convergence to optimal allocation. Insight 3 indicates that budget allocation for preceding subtask affects model choice and sampling needs in downstream subtasks. Under limited budgets, optimal configurations cannot be assigned to all subtasks. Allocating more resources to one may shift the optimal setup of others, making previous configurations suboptimal. This interdependence increases search complexity, as each subtask must be re-evaluated in light of others changes. To address this, we embed an instruction into the prompt (Appendix A.9) that leverages the LLMs planning capabilities to explore allocation trade-offs across subtasks. This enables the Agent to generate search guidelines that adaptively identify critical subtasks and configurations. The instructions derived from three insights are applied concurrently throughout the search process. Based on the updated guidelines, the Agent then generates the next round of candidate trials (Algo. Line 7) for evaluation. Environment & Archive Components The Environment module executes and evaluates trials in the actual runtime environment. Upon receiving trials from the Archive, it converts them into executable scripts and submits them to the task platform for execution on small training set. After completion, performance feedback is returned to the Agent, as shown in Fig. 2(c) and Lines 3 and 8 of Algo. 1. The Archive component stores generated guidelines and candidate trials in corresponding bases, as shown in Fig. 2(d) and Lines 1 and 6 of Algo. 1. It records the search process throughout iterations and outputs the best-performing trials upon termination."
        },
        {
            "title": "6 Experiments",
            "content": "This section presents comprehensive evaluation of AgentTTS for test-time compute-optimal budget allocation in multi-stage complex tasks. We also conduct ablation studies on integrated key insights, analyze the interpretability of budget allocations, evaluate robustness to different training sizes, and compare search effectiveness under various budget conversion methods. Experimental Setup. We conduct experiments across six datasets covering four distinct task categories, as detailed in Appendix A.4. Baselines include two groups of hyperparameter optimization: traditional machine learning methods and recent LLM-based methods. The traditional baselines include Bayesian optimization (BO) [15, 30] and random search. For LLM-based approaches, we consider AgentHPO [20], MLCopilot [53], and LLM_ZS. Given that these LLM-based methods were initially proposed for hyperparameter tuning, we adapt them to the test-time budget allocation. Both 7 Figure 3: Performance trajectories across various search methods over 50 trials. X-axis: trial count; Y-axis: best performance up to each trial. The best score is obtained from the optimal trial in prior grid search and serves as the benchmark for all methods. More results are in Appendix A.7. Table 1: Comparison of search time (in hours) and test-set performance across datasets. indicates unavailable results or failure to find the optimal trial. CWQ WebQSP Hotpot 2Wiki Method Random BO LLM_ZS MLCopilot AgentHPO Ours Time 12.5 12.5 8.3 2.5 EM Time 0.66 0.60 0.70 0.70 0.70 0. 46.3 36.3 17.5 EM Time 0.71 0.71 0.71 0.72 0.74 0.74 55.3 48.4 37.4 11.1 EM Time 0.76 0.76 0.76 0.78 0.78 0.78 37.7 20.8 48.1 7.8 EM Time 0.86 0.85 0.89 0.88 0.89 0. 64.3 Taskbench p-F1 0.40 0.52 0.49 0.53 0.49 0.53 ChatDev Time Cons. 0.74 0.75 0.74 0.75 0.74 0.75 14.3 AgentHPO and MLCopilot employ two-phase pipeline: (1) feedback-driven guideline generation and (2) guideline-informed candidate trial generation. They differ in the initialization of guidelines: AgentHPO generates initial guidelines autonomously via the LLM, while MLCopilot initializes guidelines based on performance trends observed in similar tasks. In contrast, LLM_ZS directly prompts the LLM to generate candidate trials without relying on guidelines or external references. Further details of each baseline are in Appendix A.10. All methods execute 50 iterations of search on training set comprising 50 samples and are subsequently evaluated on test set containing 500 samples. We use GPT-o3-mini [27] as the LLM search agent. The default budget is set as the sum of the minimum budget required for one-pass inference on each subtask using the largest model. Main Results and Analysis. Fig. 3 shows the search trajectories of our method and the baselines for test-time compute-optimal budget allocation on the test set. The search time and test-set performance of the best trials are reported in Tab. 1. We observe: (i) Our method outperforms baselines in both search efficiency and test-set performance in Fig. 3, 15, and Tab. 1. First, ours achieves top results on most tasks and requires fewer trials, even when baselines eventually reach optimality. Second, our method reduces search time, confirming higher computational efficiency. These gains come from leveraging empirical insights to guide the search toward promising trials. Despite similar performance on the training set, our method often generalizes better, as shown in Tab. 1. For example, on 2WikiMultiHopQA, it exceeds the next best methods by 2% on the test set. This suggests that Insight 2 helps identify minimal budgets and avoid redundant sampling, improving generalization. (ii) Compared to LLM-agent-based AgentHPO and MLCopilot, our method converges faster, showing integrated insights improve search efficiency on compute-optimal allocation. (iii) Traditional methods such as BO often get stuck in local optima due to the non-smooth landscape, while random search is noise-tolerant but inefficient. In contrast, LLM-based methods use prior hyperparameter tuning knowledge to bypass suboptimal regions, achieving better search performance. Ablation Studies. To evaluate the contribution of each insight in AgentTTS, we perform ablation studies by comparing it to variants with individual insights removed. AgentTTS-w/o-Insight1 replaces Insight 1 with random initialization. AgentTTS-w/o-Insight2/3 removes prompt components addressing diminishing returns beyond optimal budget (Insight 2) and inter-subtask dependencies (Insight 3). As shown in Fig. 4(d), we observe: (1) Removing Insight 1 prevents reaching optimal configurations, highlighting the role of initial model choice in guiding subsequent exploration. (2) Without Insight 2, search efficiency drops, delaying optimal trials to 29 steps, showing the need to identify per-subtask optimal budget. (3) Excluding Insight 3 delays the optimal trial to step 38, confirming the importance of leveraging LLM planning to handle subtask dependencies. Robustness to Varying Training Sizes. small training set might lead to unstable performance, creating non-smooth search landscape that hinders optimization. To assess the robustness of search methods under this condition, we vary the training set size (50, 75, 100 samples) and evaluate their 8 Figure 4: (a-c) Search trajectories under varying training sizes on 2WikiMultiHopQA. (d) Ablation study on 2WikiMultiHopQA with total budget of 900. effectiveness in finding optimal configurations. As shown in Fig. 4(a-c), the search efficiency of LLMbased baselines and Bayesian optimization declines with smaller training sets, whereas AgentTTS maintains strong efficiency. This indicates that our empirical insights integrated in AgentTTS help the LLM agent effectively navigate non-smooth search spaces through valuable prior guidance. Interpretability of Budget Allocation To illustrate the interpretability benefit from integrating prior insights into the LLM agent for test-time compute allocation, we present three case studies on 2WikiMultiHopQA in Fig. 5, showing Insight 1 enables subtask-specific model use, favoring large models for retrieval and small ones for QA, to guide efficient trials. Figure 5: Interpreting AgentTTS Through Integrated Empirical Insights. 2Wiki means 2WikiMultiHopQA. complete case is in Appendix A.8. Performance under varying budget settings To assess the search efficiency under different budgets, we evaluate AgentTTS under two budget settings on 2WikiMultiHopQA: 500 (low budget: only one subtask can reach its optimum) and 2000 (high, full optima reachable but with larger search spaces). As shown in Fig. 6, we can conclude: (1) Our method finds optimal configurations under low budgets, confirming the benefit of integrated insights on trade-off subtask interdependence; (2) Across low to high budget settings, our method consistently outperforms baselines in search efficiency, indicating that the insight-informed LLM agent remains robust to the expanded search space."
        },
        {
            "title": "7 Conclusion",
            "content": "We study novel problem of test-time computeoptimal scaling in multi-stage complex tasks, where the search space grows exponentially with the number of model choices and subtasks, and budget allocation across subtasks is interdependent. Empirical analysis across four task types and six datasets yields three key insights: (1) subtasks have distinct preferences for small or large models; (2) test-time scaling saturates, with diminishing or negative returns beyond an optimal budget; and (3) early subtask budgets influence downstream scaling behavior. Informed by these insights, we propose AgentTTS, an LLM-agent-based framework that iteratively searches for compute-optimal configurations through interaction with actual task platforms. Experiments show that AgentTTS outperforms both traditional and LLM-based baselines in search efficiency, final test performance, and robustness to non-smooth landscapes. Figure 6: Comparative search results under low, medium, and high compute budget settings."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 9 [2] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. [3] Juntai Cao, Xiang Zhang, Raymond Li, Chuyuan Li, Shafiq Joty, and Giuseppe Carenini. Multi2: Multi-agent test-time scalable framework for multi-document processing. arXiv preprint arXiv:2502.20592, 2025. [4] Angelica Chen, David Dohan, and David So. Evoprompting: Language models for code-level neural architecture search. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=ifbF4WdT8f. [5] Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: Process supervision without process. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=VaXnxQ3UKo. [6] Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Nan Duan, and Weizhu Chen. CRITIC: Large language models can self-correct with tool-interactive critiquing. In The Twelfth International Conference on Learning Representations, 2024. URL https:// openreview.net/forum?id=Sx038qxjek. [7] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [8] Yang Gu, Hengyu You, Jian Cao, Muran Yu, Haoran Fan, and Shiyou Qian. Large language models for constructing and optimizing machine learning workflows: survey. arXiv preprint arXiv:2411.10478, 2024. [9] Lin Gui, Cristina Garbacea, and Victor Veitch. BoNBon alignment for large language models and the sweetness of best-of-n sampling. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=haSKMlrbX5. [10] Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. Reasoning with language model is planning with world model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 81548173, 2023. [11] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pages 66096625, 2020. [12] Noah Hollmann, Samuel Müller, and Frank Hutter. Large language models for automated data science: Introducing caafe for context-aware automated feature engineering. Advances in Neural Information Processing Systems, 36:4475344775, 2023. [13] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1416514178, 2023. [14] Can Jin, Hongwu Peng, Qixin Zhang, Yujin Tang, Dimitris Metaxas, and Tong Che. Two heads are better than one: Test-time scaling of multi-agent collaborative reasoning. arXiv preprint arXiv:2504.09772, 2025. [15] Donald Jones, Matthias Schonlau, and William Welch. Efficient global optimization of expensive black-box functions. Journal of Global optimization, 13:455492, 1998. [16] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. 10 [17] Zichong Li, Xinyu Feng, Yuheng Cai, Zixuan Zhang, Tianyi Liu, Chen Liang, Weizhu Chen, Haoyu Wang, and Tuo Zhao. Llms can generate better answer by aggregating their own responses. arXiv preprint arXiv:2503.04104, 2025. [18] Minhua Lin, Hui Liu, Xianfeng Tang, Jingying Zeng, Zhenwei Dai, Chen Luo, Zheng Li, Xiang Zhang, Qi He, and Suhang Wang. How far are llms from real search? comprehensive study on efficiency, completeness, and inherent capabilities. arXiv preprint arXiv:2502.18387, 2025. [19] Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. arXiv preprint arXiv:2502.06703, 2025. [20] Siyi Liu, Chen Gao, and Yong Li. Large language model agent for hyper-parameter optimization. arXiv preprint arXiv:2402.01881, 2024. [21] Tennison Liu, Nicolás Astorga, Nabeel Seedat, and Mihaela van der Schaar. Large language models to enhance bayesian optimization. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=OOxotBmGol. [22] Xiyu Liu, Zhengxiao Liu, Naibin Gu, Zheng Lin, Wanli Ma, Ji Xiang, and Weiping Wang. Relation also knows: Rethinking the recall and editing of factual associations in auto-regressive transformer language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2465924667, 2025. [23] Daqin Luo, Chengjian Feng, Yuxuan Nong, and Yiqing Shen. Autom3l: An automated multimodal machine learning framework with large language models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 85868594, 2024. [24] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. [25] Clint Morris, Michael Jurado, and Jason Zutty. Llm guided evolution-the automation of models advancing models. In Proceedings of the Genetic and Evolutionary Computation Conference, pages 377384, 2024. [26] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [27] OpenAI. Gpt-o3-mini. https://platform.openai.com/docs/models, 2025. [Online; accessed January 31, 2025]. [28] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, et al. Chatdev: Communicative agents for software development. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1517415186, 2024. [29] Jon Saad-Falcon, Adrian Gamarra Lafuente, Shlok Natarajan, Nahum Maru, Hristo Todorov, Etash Guha, Kelly Buchanan, Mayee Chen, Neel Guha, Christopher Ré, et al. Archon: An architecture search framework for inference-time techniques. arXiv preprint arXiv:2409.15254, 2024. [30] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan Adams, and Nando De Freitas. Taking the human out of the loop: review of bayesian optimization. Proceedings of the IEEE, 104 (1):148175, 2015. [31] Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, and Yueting Zhuang. Taskbench: Benchmarking large language models for task automation. Advances in Neural Information Processing Systems, 37:45404574, 2024. 11 [32] Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM testtime compute optimally can be more effective than scaling parameters for reasoning. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=4FWAwZtd2n. [33] Benedikt Stroebl, Sayash Kapoor, and Arvind Narayanan. Inference scaling laws: The limits of llm resampling with imperfect verifiers. arXiv preprint arXiv:2411.17501, 2024. [34] Hanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette. Fast best-of-n decoding via speculative rejection. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=348hfcprUs. [35] Alon Talmor and Jonathan Berant. The web as knowledge-base for answering complex In Proceedings of the 2018 Conference of the North American Chapter of the questions. Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 641651, 2018. [36] Ziyu Wan, Xidong Feng, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training. In Forty-first International Conference on Machine Learning, 2024. URL https: //openreview.net/forum?id=C4OpREezgj. [37] Fali Wang, Runxue Bao, Suhang Wang, Wenchao Yu, Yanchi Liu, Wei Cheng, and Haifeng Chen. Infuserki: Enhancing large language models with knowledge graphs via infuser-guided knowledge integration. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 36753688, 2024. [38] Fali Wang, Zhiwei Zhang, Xianren Zhang, Zongyu Wu, Tzuhao Mo, Qiuhao Lu, Wanjing Wang, Rui Li, Junjie Xu, Xianfeng Tang, et al. comprehensive survey of small language models in the era of large language models: Techniques, enhancements, applications, collaboration with llms, and trustworthiness. arXiv preprint arXiv:2411.03350, 2024. [39] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1PL1NIMMrw. [40] Zhepeng Wang, Runxue Bao, Yawen Wu, Jackson Taylor, Cao Xiao, Feng Zheng, Weiwen Jiang, Shangqian Gao, and Yanfu Zhang. Unlocking memorization in large language models with dynamic soft prompting. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 97829796, 2024. [41] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [42] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for llm problem-solving. In The Thirteenth International Conference on Learning Representations, 2025. [43] Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 36:4161841650, 2023. [44] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [45] Chenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Zheng Lin, Li Cao, and Weiping Wang. Dynamic early exit in reasoning models. arXiv preprint arXiv:2504.15895, 2025. 12 [46] Zekang Yang, Wang Zeng, Sheng Jin, Chen Qian, Ping Luo, and Wentao Liu. Autommlab: Automatically generating deployable models from language instructions for computer vision tasks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2205622064, 2025. [47] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, 2018. [48] Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. Semantic parsing via staged query graph generation: Question answering with knowledge base. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 13211331, 2015. [49] Caiyang Yu, Xianggen Liu, Yifan Wang, Yun Liu, Wentao Feng, Xiong Deng, Chenwei Tang, and Jiancheng Lv. Gpt-nas: Evolutionary neural architecture search with the generative pretrained model. arXiv preprint arXiv:2305.05351, 2023. [50] Fei Yu, Anningzhe Gao, and Benyou Wang. Ovm, outcome-supervised value models for planning in mathematical reasoning. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 858875, 2024. [51] Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, and Michael Bendersky. Inference scaling for long-context retrieval augmented generation. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=FSjIrOm1vz. [52] Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Yunhua Zhou, and Xipeng Qiu. Revisiting the test-time scaling of o1-like models: Do they truly possess test-time scaling capabilities? arXiv preprint arXiv:2502.12215, 2025. [53] Lei Zhang, Yuge Zhang, Kan Ren, Dongsheng Li, and Yuqing Yang. Mlcopilot: Unleashing the power of large language models in solving machine learning tasks. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 29312959, 2024. [54] Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, and Qingyun Wu. Offline training of language model agents with functions as learnable weights. In Forty-first International Conference on Machine Learning, 2024. [55] Shujian Zhang, Chengyue Gong, Lemeng Wu, Xingchao Liu, and Mingyuan Zhou. Automl-gpt: Automatic machine learning with gpt. arXiv preprint arXiv:2305.02499, 2023. [56] Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, and Chuang Gan. Planning with large language models for code generation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=Lr8cOOtYbfL. [57] Tong Zhang, Chunyu Lei, Zongyan Zhang, Xian-Bing Meng, and CL Philip Chen. As-nas: Adaptive scalable neural architecture search with reinforced evolutionary algorithm for deep learning. IEEE Transactions on Evolutionary Computation, 25(5):830841, 2021. [58] Zhiwei Zhang, Fali Wang, Xiaomin Li, Zongyu Wu, Xianfeng Tang, Hui Liu, Qi He, Wenpeng Yin, and Suhang Wang. Catastrophic failure of LLM unlearning via quantization. In The Thirteenth International Conference on Learning Representations, 2025. URL https:// openreview.net/forum?id=lHSeDYamnz. [59] Mingkai Zheng, Xiu Su, Shan You, Fei Wang, Chen Qian, Chang Xu, and Samuel Albanie. Can gpt-4 perform neural architecture search? arXiv preprint arXiv:2304.10970, 2023."
        },
        {
            "title": "A Technical Appendices and Supplementary Material",
            "content": "A.1 An Example of Search-space Size We take automated software development as representative example, which consists of three subtasks: coding, static testing, and dynamic testing. The average prompt and generation token lengths for each subtask are summarized in Table 3. All subtasks share the same model space = {LLaMA-3 3B, LLaMA-3 70B}. Given total compute budget of = (cid:80) 1768, which corresponds to the cost of using the largest model (LLaMA-3 70B) for all subtasks, we provide the corresponding calculation code below. The output shows the count is about 1.8 106. Bmax def compute_budget (S , M_large , Np1 , Nd1 , M_small =3 e9 , Np2 =128 , Nd2 =64) : Compute the normalized compute budget required to generate samples from model M_large on task with prompt length Np1 and generation length Nd1 . The budget is normalized with respect to the unit cost of generating 1 sample using the smallest model ( M_small ) on the lowest computationally consuming task ( prompt_length = Np2 , decoding_length = Nd2 ) . alpha = M_large / M_small beta1 = Np1 / Nd1 beta2 = Np1 / Np2 beta3 = Np2 / Nd2 # Compute normalized budget cost return beta3 * (( alpha * beta2 / beta1 ) * ( beta1 + ) - 1) # Prompt - to - generation length ratio for the target task # Prompt length scaling factor relative to the reference task # Prompt - to - generation length ratio for the reference task # Model size ratio def d _ p _ e _ n ( max_budget , model_size , prompt_len , decode_len ) : Iteratively increases ( sample count ) until budget is exceeded for given model and task configuration . for in range (1 , 10000) : if compute_budget (S , model_size , prompt_len , decode_len ) > max_budget : return - 1 return 9999 # Compute the total budget required for one sample from the largest model across tasks . = compute_budget (1 , 70 e9 , 1024 , 1024) + compute_budget (1 , 70 e9 , 1024 , 512) + compute_budget (1 , 70 e9 , 1024 , 256) # Two model sizes : small (3 ) and large (70 ) model_space = [3 e9 , 70 e9 ] # Compute the maximum feasible sample count per task using the smallest model max_s1 = d _ p _ e _ n (b , 3 e9 , 1024 , 1024) max_s2 = d _ p _ e _ n (b , 3 e9 , 1024 , 512) max_s3 = d _ p _ e _ n (b , 3 e9 , 1024 , 256) Np1_list = [1024 , 1024 , 1024] Nd1_list = [1024 , 512 , 256] # Prompt lengths for each subtask # Generation lengths for each subtask # Exhaustive search over valid configurations under the total budget constraint al d_ on ig _ co nt = 0 for s1 in range ( max_s1 + 1) : for model1 in model_space : b1 = compute_budget ( s1 , model1 , Np1_list [0] , Nd1_list [0]) if b1 > : break for s2 in range ( max_s2 + 1) : for model2 in model_space : b2 = compute_budget ( s2 , model2 , Np1_list [1] , Nd1_list [1]) if b1 + b2 > : break for s3 in range ( max_s3 + 1) : for model3 in model_space : b3 = compute_budget ( s3 , model3 , Np1_list [2] , Nd1_list [2]) if b1 + b2 + b3 > : break al d_ on ig _ co nt += 1 print ( \" Total valid configurations : \" , va id _ co fi _c un ) # The output should be : Total valid configurations : 1854841 Listing 1: Enumerating Valid (M1, S1, M2, S2, M3, S3) Triplets under Budget Constraint A.2 Inference FLOPs-Based Budget Conversion We adopt floating-point operations (FLOPs) as the primary computation cost metric for measuring inference cost. In repeated sampling scenarios, Transformer models can exploit batching to share 14 the cost of prompt encoding across multiple decoding passes. This allows for efficient reuse of the prompt representation while performing multiple, independent decoding operations. Then, we formalize the computation of FLOPs and describe how to convert the number of decoding samples between large and small model under an equivalent FLOPs budget. We use unified notation throughout in Tab. 2. Table 2: Notation used in FLOPs-based cost formulation. Symbol Description Np Nd Number of model parameters Number of Transformer layers Hidden dimension size Number of prompt tokens Number of generated (decoded) tokens per sample Number of decoding samples The calculation of budget conversion using inference FLOPs in Eq. 4 is as follows: Proof. Per-token FLOPs. For decoder-only Transformer generating tokens autoregressively, the per-token FLOPs primarily stem from two components: (i) Parameter matrix multiplications. Each non-embedding weight is used exactly once per token in matrix multiplication followed by addition, resulting in approximately 2M FLOPs per token, where is the total number of non-embedding parameters. (ii) Attention operations. At decoding step + 1, attention requires computing query-key dot products and applying the attention weights to value vectors. Both operations involve multiply-adds, contributing approximately 4LDt FLOPs per token, where is the number of layers, is the hidden size, and is the number of preceding tokens. Thus, the approximate FLOPs per token is: FLOPstoken(M, L, D, t) 2M + 4LDt. Per-phase FLOPs. The total inference cost for generation can be decomposed into two phases: (1) Prompt Encoding: FLOPsprompt(M, L, D, Np) = Np (cid:88) t=1 FLOPstoken(M, L, D, t) = 2M Np + 2LDNp(Np + 1) (2) Decoding: FLOPsdecode(M, L, D, Np, Nd) = Nd(cid:88) t=1 FLOPstoken(M, L, D, Np + t) = 2M Nd + 2LDNd(2Np + Nd + 1) (6) (7) (3) Total FLOPs: To generate decoding samples from prompt of length Np, the total FLOPs is: FLOPstotal(S, M, L, D, Np, Nd) = FLOPsprompt(M, L, D, Np) + FLOPsdecode(M, L, D, Np, Nd) (8) We denote this as fcost = FLOPstotal(S, M, L, D, Np, Nd). Budget Conversion Across Models and Tasks. Consider converting FLOPs usage between two tasks using different models: large model (ℓ) for Task 1 and smaller model (s) for Task 2. Let Mℓ, Lℓ, Dℓ and Ms, Ls, Ds be their respective parameters, and Np,1, Nd,1, and Np,2, Nd,2 the average prompt and decoding lengths for the two tasks. Suppose the large model executes Sℓ decoding samples. The equivalent number of samples Ss that the small model can generate under the same FLOPs budget is: Ss = FLOPstotal,ℓ(Sℓ, Mℓ, Lℓ, Dℓ, Np,1, Nd,1) FLOPsprompt,s(Ms, Ls, Ds, Np,2) FLOPsdecode,s(Ms, Ls, Ds, Np,2, Nd,2) (9) 15 To simplify, we ignore attention-related terms (typically less than 1% of total FLOPs) and define: β1 = Np,1 Nd,1 , β3 = Np,2 Nd, , β2 = Np,1 Np,2 , α = Mℓ Ms . Substituting these yields: (cid:18) αβ2 β1 (β1 + Sℓ) 1 (cid:19) Ss = β (10) Normalized Budget Function. We define the unit budget = 1 as the FLOPs cost of generating single sample using the smallest model (Ms = 3B) on the lowest consuming task specification, characterized by Np,lowest = 128 and Nd,lowest = 64. This setting represents the minimal computational cost among all available subtask-model combinations. Substituting into Eq. 10 yields: Ss = 2αβ2Sℓ β1 + 2(αβ2 1) (11) Thus, for configuration defined by model size Mℓ, sample count Sℓ, and task parameters Tℓ = (Np,ℓ, Nd,ℓ), the normalized budget function is: fbudget(Mℓ, Sℓ, Tℓ) = Ss = 2αβ2Sℓ β1 + 2(αβ2 1) (12) We provide the average lengths of prompt and decoding in different subtasks in Tab. 3. We provide the look-up tables when generating = 1, 5, 10, 45, 90 in Tab. 48. Table 3: Average prompt and generation token lengths (Np, Nd) for each subtask. Subtask 2WikiMultiHopQA-Retrieval 2WikiMultiHopQA-QA HotpotQA-Retrieval HotpotQA-QA CWQ-Retrieval CWQ-QA WebQSP-Retrieval WebQSP-QA Taskbench-Decomposition Taskbench-Tool Selection Taskbench-Parameter Prediction ChatDev-Code ChatDev-Static Test ChatDev-Dynamic Test Np 2048 256 2048 256 1024 256 1024 128 1024 1024 1024 1024 1024 1024 Nd 128 64 128 64 64 64 64 64 64 256 2048 1024 512 256 Table 4: Normalized compute budget when generating = 1 sample. Model Qwen2.5-72B Qwen2.5-32B Qwen2.5-7B LLaMA-3.1-70B LLaMA-3.1-8B LLaMA-3.2-3B Gemma-2-27B Gemma-2-9B Gemma-2-2B Phi-3-medium Phi-3-small Phi-3-mini 2Wiki-R 814 361 77 791 89 32 304 100 21 157 77 41 2Wiki-QA 118 51 10 115 11 3 43 13 1 21 10 4 Hot-R 814 361 77 791 89 32 304 100 21 157 77 41 Hot-QA 118 51 10 115 11 3 43 13 1 21 10 4 CWQ-R 406 179 38 395 43 15 151 49 9 77 38 CWQ-QA WQSP-R WQSP-QA 406 179 38 395 43 15 151 49 9 77 38 20 118 51 10 115 11 3 43 13 1 21 10 4 70 30 5 68 6 2 26 8 1 13 5 3 Decomp 406 179 38 395 43 15 151 49 9 77 38 20 ToolSel 478 211 45 465 51 18 178 58 11 91 45 23 ParamPred 1150 510 110 1118 126 46 430 142 30 222 110 Code 766 339 73 745 83 30 286 94 19 147 73 39 Table 5: Normalized compute budget when generating = 5 samples. Model Qwen2.5-72B Qwen2.5-32B Qwen2.5-7B LLaMA-3.1-70B LLaMA-3.1-8B LLaMA-3.2-3B Gemma-2-27B Gemma-2-9B Gemma-2-2B Phi-3-medium Phi-3-small Phi-3-mini 2Wiki-R 1006 446 96 978 110 40 376 124 26 194 96 51 2Wiki-QA 214 94 19 208 22 7 79 25 4 40 19 9 Hot-R 1006 446 96 978 110 40 376 124 26 194 96 Hot-QA 214 94 19 208 22 7 79 25 4 40 19 9 CWQ-R 502 222 47 488 54 19 187 61 12 96 47 25 CWQ-QA WQSP-R WQSP-QA 502 222 47 488 54 19 187 61 12 96 47 25 214 94 19 208 22 7 79 25 4 40 19 9 166 73 14 161 17 6 61 20 3 31 14 8 Decomp 502 222 47 488 54 19 187 61 12 96 47 ToolSel 862 382 82 838 94 34 322 106 22 166 82 44 ParamPred 4222 1875 409 4105 467 174 1582 526 115 819 409 221 Code 2302 1022 222 2238 254 94 862 286 62 446 222 120 Static 574 254 54 558 62 22 214 70 14 110 54 28 Static 1342 595 129 1305 147 54 502 166 35 259 129 69 Dynamic 478 211 45 465 51 18 178 58 11 91 45 Dynamic 862 382 82 838 94 34 322 106 22 166 82 44 16 Table 6: Normalized compute budget for each subtask when generating = 10 samples. Model Qwen2.5-72B Qwen2.5-32B Qwen2.5-7B LLaMA-3.1-70B LLaMA-3.1-8B LLaMA-3.2-3B Gemma-2-27B Gemma-2-9B Gemma-2-2B Phi-3-medium Phi-3-small Phi-3-mini 2Wiki-R 1246 553 119 1211 137 50 466 154 33 241 119 64 2Wiki-QA 334 147 31 325 35 12 124 40 7 63 31 Hot-R 1246 553 119 1211 137 50 466 154 33 241 119 64 Hot-QA 334 147 31 325 35 12 124 40 7 63 31 16 CWQ-R 622 275 59 605 67 24 232 76 15 119 59 31 CWQ-QA WQSP-R WQSP-QA 622 275 59 605 67 24 232 76 15 119 59 31 214 94 19 201 22 8 76 25 5 39 19 10 334 147 31 325 35 12 124 40 7 63 31 Decomp 622 275 59 605 67 24 232 76 15 119 59 31 ToolSel 1342 595 129 1305 147 54 502 166 35 259 129 69 ParamPred 8062 3582 782 7838 894 334 3022 1006 222 1566 782 424 Code 4222 1875 409 4105 467 174 1582 526 115 819 409 221 Static 2302 1022 222 2238 254 94 862 286 62 446 222 120 Dynamic 1342 595 129 1305 147 54 502 166 35 259 129 Table 7: Normalized compute budget for each subtask when generating = 45 samples. Model Qwen2.5-72B Qwen2.5-32B Qwen2.5-7B LLaMA-3.1-70B LLaMA-3.1-8B LLaMA-3.2-3B Gemma-2-27B Gemma-2-9B Gemma-2-2B Phi-3-medium Phi-3-small Phi-3-mini 2Wiki-R 2926 1299 283 2845 323 120 1096 364 79 567 283 153 2Wiki-QA 1174 521 112 1141 129 47 439 145 31 227 112 60 Hot-R 2926 1299 283 2845 323 120 1096 364 79 567 283 153 Hot-QA 1174 521 112 1141 129 47 439 145 31 227 112 CWQ-R 1462 649 140 1421 161 59 547 181 39 283 140 75 CWQ-QA WQSP-R WQSP-QA 1462 649 140 1421 161 59 547 181 39 283 140 75 1174 521 112 1141 129 47 439 145 31 227 112 60 470 209 45 457 52 19 176 59 13 91 45 25 Decomp 1462 649 140 1421 161 59 547 181 39 283 140 75 ToolSel 4702 2089 455 4571 521 194 1762 586 129 913 455 ParamPred 34942 15529 3395 33971 3881 1454 13102 4366 969 6793 3395 1842 Code 17662 7849 1715 17171 1961 734 6622 2206 489 3433 1715 930 Static 9022 4009 875 8771 1001 374 3382 1126 249 1753 875 474 Dynamic 4702 2089 455 4571 521 194 1762 586 129 913 455 246 Table 8: Normalized compute budget for each subtask when generating = 90 samples. Model Qwen2.5-72B Qwen2.5-32B Qwen2.5-7B LLaMA-3.1-70B LLaMA-3.1-8B LLaMA-3.2-3B Gemma-2-27B Gemma-2-9B Gemma-2-2B Phi-3-medium Phi-3-small Phi-3-mini 2Wiki-R 5086 2259 493 4945 563 210 1906 634 139 987 493 267 2Wiki-QA 2254 1001 217 2191 249 92 844 280 61 437 217 117 Hot-R 5086 2259 493 4945 563 210 1906 634 139 987 493 267 Hot-QA 2254 1001 217 2191 249 92 844 280 61 437 217 117 CWQ-R 2542 1129 245 2471 281 104 952 316 69 493 245 132 CWQ-QA WQSP-R WQSP-QA 2542 1129 245 2471 281 104 952 316 69 493 245 2254 1001 217 2191 249 92 844 280 61 437 217 117 902 400 87 877 100 37 338 112 25 175 87 47 Decomp 2542 1129 245 2471 281 104 952 316 69 493 245 132 ToolSel 9022 4009 875 8771 1001 374 3382 1126 249 1753 875 474 ParamPred 69502 30889 6755 67571 7721 2894 26062 8686 1929 13513 6755 3666 Code 34942 15529 3395 33971 3881 1454 13102 4366 969 6793 3395 Static 17662 7849 1715 17171 1961 734 6622 2206 489 3433 1715 930 Dynamic 9022 4009 875 8771 1001 374 3382 1126 249 1753 875 474 A.3 Dollar Costs of Repeated Sampling Table 9 is the API cost information for each model from Together AI. We do not convert it in the same manner as above, as the dollar serves as natural unit of price. Model Name Qwen2.5 72B Qwen2.5 32B Qwen2.5 7B LLaMA-3.1-70B LLaMA-3.1 8B LLaMA-3.2 3B Parameters 72B 32B 7B 70B 8B 3B Inference Cost (per 1M tokens) $1.20 $0.80 $0.30 $0.88 $0.18 $0.06 Table 9: Inference costs per 1M tokens for selected models from Together AI. API price as the cost metric Beyond inferencecompute FLOPs, end-users are often more concerned with the monetary cost. Therefore, we introduce an alternative budget metric: API price. Using this metric, we redraw the test-time scaling curves for the question answering subtask of 2WikiMultiHopQA, as shown in Fig. 7 (left). The results indicate that smaller models remain preferable under constrained budgets. Furthermore, as illustrated in Fig. 7 (right), the proposed insights continue to improve the search efficiency and performance of AgentTTS, demonstrating strong generalization across different cost metrics. 17 Test-time scaling and Figure 7: AgentTTS search trajectories under price budget. Left: scaling curves; Right: corresponding search trajectories. A.4 Benchmarks, Datasets, Models, Metrics, and other Experimental Setup We introduce the benchmarks, datasets, models, evaluation metrics, and other experimental setups. Retrieval-based Question Answering uses two datasets: 2WikiMultiHopQA [11] and HotpotQA [47], each providing 100 candidate text chunks per query. The task involves two subtasks: (1) retrieving relevant documents using an LLM retriever, and (2) answering the question using another LLM with the retrieved documents as context. For both datasets, the average prompt and generation lengths are 2048 and 128 for retrieval tasks, and 256 and 64 for the QA subtasks, respectively. We employ Qwen 2.5 (7B, 32B, 72B) [44] for retrieval and LLaMA-3 (3B, 8B, 72B) [7] for answering. These model families are selected due to their strong subtask performance and wide range of model sizes, which helps mitigate the influence of pre-training differences. This setup is used consistently across the other three tasks. The evaluation metrics are retrieval F1 (Ret-F1) and exact match (EM), corresponding to the two subtasks, respectively. For the Ret-F1 metric, given the ground-truth retrieval documents, precision is computed as the fraction of correctly retrieved documents among all predicted documents, while recall is the fraction of ground-truth documents that are successfully retrieved. The F1 score is then calculated as the harmonic mean of precision and recall. For the EM metric, an answer is considered correct only if it exactly matches the ground truth. The EM score is computed as the fraction of such exactly matched answers among all predictions. Knowledge Graph Question Answering This task uses two KGQA datasets: ComplexWebQuestions [35] and WebQSP [48]. The objective is to retrieve relevant knowledge triplets from 100 candidates in the knowledge graph and answer the question based on the retrieved context. The task includes two subtasks: knowledge retrieval and question answering. For both datasets, the average prompt and generation lengths are 2048 and 64 for retrieval tasks, and 256 and 64 for the QA subtasks, respectively. We use Qwen 2.5 (7B, 72B) [44] for retrieval and LLaMA-3 (8B, 70B) [7] for answering. The evaluation metrics are retrieval F1 (Ret-F1) and exact match (EM), corresponding to the two subtasks, respectively. The computation of Ret-F1 and EM is the same as above. Task Automation This task involves decomposing complex user instructions into subtasks and invoking external tools to execute them. We use the TaskBench benchmark [31] to evaluate LLM performance across three stages: task decomposition, tool selection, and parameter prediction. The average prompt and generation lengths are 2048 and 64 for task decomposition, 1024 and 256 for tool selection, and 1024 and 2048 for parameter prediction. LLaMA-3 (8B, 70B) [7] is used for all subtasks. To evaluate LLMs ability to understand and decompose complex tasks, we assess the quality of task decomposition using three complementary ROUGE metrics, including ROUGE-L. To evaluate tool selection, we introduce Node F1 (n-F1), which measures the correctness of predicted tools by comparing them against reference set, reflecting the models ability to identify appropriate tools for each subtask. To assess the accuracy of tool configuration, we use Parameter Name & Value F1 (p-F1), which evaluates both the identification of parameter names and the correctness of their assigned values, capturing the models ability to provide contextually appropriate configurations. Automated Software Development We leverage ChatDev [28], chat-driven framework based on the waterfall model, encompassing documentation, design, coding, static testing, and dynamic testing phases. Our focus is on three core subtasks: coding, static testing, and dynamic testing. The average prompt and generation lengths are 1024 and 1024 for coding, 1024 and 512 for static testing, and 1024 and 256 for dynamic testing. We use LLaMA-3 (3B, 70B) [7] for all subtasks. Consistency measures how well the generated software code aligns with the original requirement description. It is quantified as the cosine similarity between the semantic embeddings of the textual requirements and the generated code. higher score indicates greater degree of consistency with the specified requirements. Other Experimental Setup To encourage diverse generations and activate LLMs coverage during repeated sampling, we set the temperature to 0.9, while keeping all other decoding hyperparameters at their default values. Each trial is conducted using an NVIDIA H100 80GB HBM3 GPU to ensure consistent runtime performance across all datasets. For each dataset, we randomly sample 50 instances to form the training set used for search and 500 instances as the test set for final evaluation. 18 A.5 Model Selection for Subtask-Specific Analysis For each subtask, we select models from the same model family and assume that model size reflects model capacity. By comparing the performance of small and large models within the same family under test-time scaling, we can figure out whether smaller models can outperform larger ones under equivalent compute budgets. This comparison focuses solely on the relationship between test-time compute and model size. Otherwise, comparisons across different model families could introduce confounding effects due to variations in pretraining, such as differences in pre-training data or objectives. Therefore, we restrict each subtask to models within single family. During evaluation, all other non-target subtasks are fixed to use LLaMA-3 70B with one-pass inference to isolate the effect of model choice in the target subtask. For each subtask, we select the model family that achieves the best performance. Fig. 8a-8i present subtask-level performance across model sizes and families. We exclude ChatDev from this analysis, as it does not provide metrics for intermediate steps. Based on the results, we select the Qwen family for retrieval-related subtasks and the LLaMA-3 family for all others. (a) 2Wiki-Ret (b) 2Wiki-QA (c) Hotpot-Ret (d) Hotpot-QA (e) CWQ-Ret (f) CWQ-QA (g) TaskBench-Decomp (h) TaskBench-ToolSel (i) TaskBench-ParamPred Figure 8: Performance of various LLMs across retrieval, question answering, and task execution subtasks on three types of tasks. A.6 More Preliminary Experimental Results Fig. 11, 12, 13, and 14 illustrate how subtask performance evolves in KGQA, TaskBench, and ChatDev as the test-time sampling, FLOPs, and budget increase. 19 Figure 9: Performance variance on 2WikiMultiHopQA with increasing sampling, inference FLOPs, and budget. Top: performance by sample count; middle: performance by log-scaled inference FLOPs; bottom: performance by normalized budget. (a) Retrieval accuracy measured by Retrieval F1 (Ret_F1). (b-d) QA performance under varying retrieval quality levels measured by Gen_EM, the exact match between the generated answer and the ground truth. Figure 10: Performance variance on HotpotQA with increasing sampling, inference FLOPs, and budget. Top: performance by sample count; middle: performance by log-scaled inference FLOPs; bottom: performance by normalized budget. (a) Retrieval subtask performance; (b-d) Question answering performance under different retrieval quality levels. 20 Figure 11: Performance variance on CWQ with increasing sampling, inference FLOPs, and budget. Top: performance by sample count; middle: performance by log-scaled inference FLOPs; bottom: performance by normalized budget. (a) Retrieval performance; (b-d) Question answering performance under different retrieval quality levels. Figure 12: Performance variance on WebQSP with increasing sampling, inference FLOPs, and budget. Top: performance by sample count; middle: performance by log-scaled inference FLOPs; bottom: performance by normalized budget. (a) Retrieval performance; (b-d) Question answering performance under different retrieval quality levels. 21 Figure 13: Performance variance on Taskbench-DailyAPIUsage with increasing sampling, inference FLOPs, and budget. Top: performance by sample count; middle: performance by log-scaled inference FLOPs; bottom: performance by normalized budget. (a) Task decomposition performance; (b) Tool selection performance; (c-d) Parameter prediction under different tool-selection quality levels. Figure 14: Performance variance on ChatDev with increasing sampling and inference FLOPs. Top: performance by sample count; middle: performance by log-scaled inference FLOPs; bottom: performance by normalized budget. (a-c) The consistency score during coding, static test, and dynamic test, respectively. 22 A.7 More Main Experimental Results Fig. 15 is the performance trajectories on WebQSP, Taskbench, and ChatDev datasets. Figure 15: Performance trajectories across various search methods over 50 trials. X-axis: trial count; Y-axis: best performance up to each trial. The best score is obtained from the optimal trial in prior grid search and serves as the benchmark for all methods. A.8 Detailed Cases for Interpretability To illustrate the interpretability benefit from integrating prior insights into the LLM agent for test-time compute allocation, we present three case studies on 2WikiMultiHopQA in Fig. 16. The first row shows Insight 1 enables subtask-specific model use, favoring large models for retrieval and small ones for QA, to guide efficient trials. The second row shows Insight 2 pinpoints the optimal sampling range (550), narrowing the search space. The third row shows Insight 3 promotes balanced allocation, favoring one-sample high-capacity retrieval and lower-cost QA configuration, enabling effective trade-offs for better overall performance. Figure 16: Detailed Cases of Interpreting AgentTTS Through Integrated Empirical Insights. We present complete case study comparing AgentTTS and AgentHPO in Fig. 17. The budget of 850 poses challenge in balancing optimal usage across two subtasks, as the optimal retrieval configuration (Qwen2.5-72B, 1 sample) consumes 814 units, while the optimal QA setting (LLaMA3-3B, 50 samples) requires 52. We observe: (1) AgentTTS identifies optimal configurations using embedded insights, whereas AgentHPO does not; (2) Insights 1, 2, and 3 guide different stages of the search: Insight 1 selects models (Qwen2.5-72B for retrieval, LLaMA-3-3B for QA), Insight 2 focuses sampling (reaching the optimal trial: Qwen2.5-72B, 1 / LLaMA-3-3B, 15), and Insight 3 balances budget use (yielding another optimal trial: Qwen2.5-32B, 5 / LLaMA-3-3B, 20). 23 (a) AgentTTS on 2Wiki (budget = 850). (b) AgentHPO on 2Wiki (budget = 850). Figure 17: Comparison of complete trial generation and decision guidelines using AgentTTS and AgentHPO on the 2Wiki dataset under compute budget of 850. 24 A.9 Prompt Design in AgentTTS for Search Guidelines and Trial Generation We design prompts below to guide the LLM agent in generating both guidelines and candidate trials. Each prompt is constructed to provide the agent with basic task information and requirements, enabling effective reasoning and planning. For guideline generation, we distinguish between the initial and non-initial phases. The initial guideline prompt, informed by Insight 1, helps identify whether each subtask favors smaller or larger models, thereby guiding early-stage search. The non-initial guideline prompt, informed by Insights 2 and 3, reflects general test-time scaling patterns and assists in balancing budget allocations across subtasks. For trial generation, the LLM agent proposes new configurations based on the existing guidelines."
        },
        {
            "title": "Fusion Prompt",
            "content": "You have been provided with set of responses from various open-source models to the latest user query, which is query. Your task is to synthesize these responses into single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability. Once again, the query is: query Responses from models: LLM Prompt for Initial Guidelines [System Prompt] You are an expert in parameter optimization. [User Prompt] am tuning test-time parameters for complex task that can be broken down into multiple simple subtasks. The parameters include the model size and the number of samples used for each subtask. Generally, increasing the number of samples improves performance. The goal is to find the best parameter settings that maximize performance within fixed budget. will provide the task name, description, budget, and initial search history. Please help me summarize the guidelines from the search history in the following aspect: 1. The search history includes initial trials that use different model sizes under the same unit budget. In these trials, smaller models are allowed more samples, while larger models have fewer samples due to their higher cost. Please help me identify which model size we should explore further in each subtask. If the large model performs significantly better than the small model, we will choose the large one. Otherwise, if its performance is similar or only slightly better, we will prefer the smaller model. 2. As the number of samples increases, performance tends to improve and then level off. Please identify the search direction for finding the optimal number of samples for each subtask, especially the point after which more samples no longer improve results. 3. The total compute budget is shared across all subtasks and they are interdependent. If we could not assign the optimal model and sample count to every subtask, consider the following strategy. Leveraging your planning capability, prioritize each subtask, try different combinations of budget allocations for the other tasks, and balance the allocations across subtasks. Inputs: Task name: {task_name} Task description: {task_desc} Subtasks, models, {subtask_specification}, prompt/generation lengths: {model_space} Total budget: {budget} Evaluation metrics: {metrics} Main metric: {main_metric} Search history: {history} Please generate your response in the most concise wording. Response: 25 LLM Prompt for Non-Initial Guidelines [System Prompt] You are an expert in parameter optimization. [User Prompt] am tuning test-time parameters for complex task that can be broken down into multiple simple subtasks. The parameters include the model size and the number of samples used for each subtask. Generally, increasing the number of samples improves performance. The goal is to find the best parameter settings that maximize performance within fixed budget. will provide the task name, description, budget, and both old and recent search history. Please help me summarize guidelines from the recent search history in the following three aspects: 1. As the number of samples increases, performance tends to improve and then level off. Please identify the search direction for finding the optimal number of samples for each subtask, especially the point after which more samples no longer improve results. 2. The total compute budget is shared across all subtasks, and they are interdependent. If we could not assign the optimal model and sample count to every subtask, consider the following strategy. Leveraging your planning capability, prioritize each subtask, try different combinations of budget allocations for the other tasks, and balance the allocations across subtasks. 3. If you observe that the performance variance across the search history is small, please suggest that future searches focus more on exploration strategies such as crossover, mutation, and random search, rather than strictly following the existing search patterns. Inputs: Task name: {task_name} Task description: {task_desc} Subtasks, models, prompt/generation lengths: {subtask_specification}, {model_space} Total budget: {budget} Evaluation metrics: {metrics} Main metric: {main_metric} Search history: {history} Please generate your response in the most concise wording. Response: LLM Prompt for Trial Generation [System Prompt] You are an expert in parameter optimization. [User Prompt] am tuning test-time parameters for complex task decomposable into multiple subtasks. Each subtask requires selecting model and the number of samples. Increasing the number of samples generally improves performance. The objective is to identify configurations that maximize overall performance under fixed compute budget. will provide the task name, task description, list of subtasks, and the model choices available for each, the total budget, available samples, the budget calculation formula, evaluation metrics (including the main metric to optimize), search history (previous configurations tried), and insights from previous experiments. The budget for each subtask is computed with the following Python function: def compute_budget ( num_samples , model_size , prompt_length , generation_length , M_small =3 e9 , Np2 =128 , Nd2 =64) : alpha = model_size / M_small beta1 = prompt_length / ene rat ion _le ng th beta2 = prompt_length / Np2 beta3 = Np2 / Nd2 budget = beta3 * (( alpha * beta2 / beta1 ) * ( beta1 + num_samples ) - 1) return budget The total budget is the sum across all subtasks. Using the information provided, generate {batch_size} candidate configurations that follow the insights and stay within the budget. Return exactly {batch_size} new configurations in strict JSON format, following the schema: { \"subtask_1\": { \"model\": \"model_name\", \"samples\": int }, ... } Inputs: Task name: {task_name} Task description: {task_desc} Subtasks, models, prompt/generation lengths: {subtask_specification}, {model_space} Total budget: {budget} Evaluation metrics: {metrics} Main metric: {main_metric} Search history: {history} Guidelines: {experience} Response: Return only {batch_size} candidates in strict JSON format. 26 Figure 18: Class Diagram of the AgentTTS Framework. A.10 Baselines We compare two baseline categories: LLM-based approaches (AgentHPO [20], MLCopilot [53], and LLM_ZS) and traditional methods (Random Search and Bayesian Optimization [30]). AgentHPO [20] targets Hyperparameter Optimization (HPO), addressing limitations of traditional AutoML methods such as high trial cost, complex setup, and limited interpretability by leveraging LLM-powered autonomous agents. It comprises two modules: the Creator, which converts natural language inputs (e.g., dataset, model, and goals) into initial hyperparameters, and the Executor, which trains models, logs outcomes, and returns feedback. The Creator iteratively refines HPs using this feedback, forming closed-loop optimization. To adapt AgentHPO for our test-time scaling budget allocation setting, we provide AgentHPO with structured inputs similar to our method, including task descriptions, model choices, and total budget. However, AgentHPO assumes LLMs possess sufficient ML knowledge for HPO tasks, an assumption that fails in multi-stage test-time scaling, where domain-specific knowledge is lacking Wang et al. [37]. Our method addresses this gap by injecting targeted insights about test-time scaling into the agents reasoning process, improving both search efficiency and decision interpretability. MLCopilot [53] focuses on generating machine learning solutions, addressing limitations in traditional AutoML methods, such as Bayesian optimization [30], which often lack interpretability and struggle to incorporate human prior knowledge, e.g., regarding model architectures. Inspired by human design patterns, where individuals understand task by recalling relevant experiences or knowledge from prior work or textbooks, MLCopilot emulates this process by leveraging LLMs to suggest solutions for new ML tasks based on historical cases. To adapt MLCopilot to our problem setting, we provide the search history of similar task to guide the LLM in generating both prior experience and candidate trials iteratively for the target multi-stage task. However, while MLCopilot can extract surface-level patterns from related tasks, it lacks the ability to derive tailored insights necessary for efficient budget allocation in test-time scaling. This limits its effectiveness in the testtime scaling compute-optimal budget allocation problem, where fine-grained domain understanding is essential for optimal resource allocation. LLM_ZS LLM_ZS directly prompts the LLM to generate zero-shot candidate trials across multiple iterations, using only the task description, model choices, and total budget as input. Unlike methods that leverage prior knowledge or historical search experience, LLM_ZS relies solely on the LLMs pretrained knowledge to guide trial generation. Bayesian Optimization (BO) [30] is model-based method for optimizing black-box functions and is widely used in hyperparameter tuning. For test-time scaling in multi-stage tasks, BO models the performance of candidate allocation as black-box objective and employs surrogate model (e.g., Gaussian Process) to guide the search via acquisition functions (e.g., Expected Improvement). 27 It balances exploration and exploitation to navigate the combinatorial space of model-budget configurations. However, its limited interpretability hinders its applicability. Moreover, in multi-stage tasks with complex subtask interactions and non-smooth performance landscapes, BO may converge to local optima and require large number of evaluations. Random Search serves as simple yet widely used baseline for hyperparameter tuning. In the context of test-time scaling budget allocation for multi-stage settings, it randomly samples modelbudget configurations without relying on performance history or prior knowledge. Despite its simplicity, Random Search is often robust to non-smooth landscapes, which allows it to occasionally find globally optimal trials. However, its inherent randomness and lack of guidance result in significantly slower convergence compared to more informed or tailored strategies. A.11 Detailed Related Work A.11.1 Test-time Scaling and Compute-optimal Strategy Inspired by the human reasoning process that tends to engage in deeper, more deliberate thinking, several studies have proposed allocating additional compute during inference to enhance task performance [41, 39]. Further, other works [2, 42] have observed that increasing inference-time compute exhibits similar pattern akin to training scaling laws: additional inference compute consistently leads to improved task performance. This phenomenon is commonly referred to as test-time scaling (TTS). Test-time scaling technologies can be mainly classified into two categories: sequential scaling and parallel scaling. In sequential scaling, it enhances test-time computation by generating progressively longer solutions along the sequence dimension. The most prevalent method is self-revision, where first generate an initial response and then iteratively evaluate and refine it based on self-assessment [24, 6, 32]. Because sequential scaling relies on the models ability to generate reasonably accurate initial responses, it is generally more effective for simpler tasks [32]. On the other hand, parallel scaling generates multiple independent solutions in parallel and aggregates them into final answer. Common solution-level aggregation methods are Best-of-N [34, 9], which sample complete solutions and then select the best one according to verifier. While Tree-Search algorithms, viewed as parallel scaling at the token or step level, are against process-based reward model to search top-K intermediate steps and further explore, including Beam Search [50, 43] and Monte Carlo Tree Search (MCTS) [42, 32, 10, 36, 5, 56]. However, they typically rely on explicit guidance signals for candidate selection. An alternative line of work directly employs an LLM as the fuser to aggregate candidate solutions, offering greater generalization and flexibility [13, 17, 29]. Zeng et al. [52] argues that parallel scaling provides better coverage and scalability than sequential scaling, as extended sequential reasoning chains may corrupt intermediate correct outputs. Similarly, Snell et al. [32] demonstrates that parallel scaling is more suitable for complex tasks, as it requires only the ability to generate correct final answers. Therefore, we adopt parallel test-time scaling strategyspecifically, repeated sampling with fusion, to address complex multi-stage reasoning tasks, leveraging the strengths of small models in diverse sampling and robust aggregation. The configuration of test-time scaling strategies plays critical role in performance improvement, giving rise to the broader problem of optimally allocating compute and budget, commonly referred to as the test-time compute-optimal scaling strategy. Recent studies [2, 42, 19, 51, 32, 38] have shown that, in certain settings, smaller models can outperform large language models under the same compute budget. These works explore how to choose between training scaling and test-time scaling, as well as how to select among different test-time scaling techniques to optimize performance. For instance, Snell et al. [32] demonstrate that task difficulty strongly influences the optimal scaling strategy: for moderately difficult tasks, parallel search with small models is preferred, while for simpler tasks, sequential scaling with large models is more effective. Based on this observation, they propose difficulty predictor to guide scaling decisions dynamically. Liu et al. [19] further extend this line of work by showing that the choice of optimal TTS strategies is highly sensitive to the design of reward functions. Yue et al. [51] propose using linear model to fit key factors influencing strategy selection within retrieval-augmented generation (RAG) systems. Wu et al. [42] introduce Reward Balanced Search (REBASE), novel tree search algorithm that combines weighted voting to achieve Pareto-optimal trade-off between accuracy and inference cost. However, these approaches primarily focus on single-stage task scenarios and do not systematically address the challenge of budget allocation across subtasks in multi-stage complex tasks. In this work, we extend the test-time compute-optimal scaling framework to multi-stage complex tasks. 28 A.11.2 LLM for Hyperparameter Optimization LLMs have emerged as promising tools for enhancing the efficiency and effectiveness of hyperparameter optimization (HPO) by leveraging contextual understanding and prior knowledge [8, 40, 58, 22, 37], compared to traditional Automated Machine Learning (AutoML) approaches such as Bayesian Optimization (BO) [30]. Existing LLM-based HPO studies can be mainly categorized into two directions: (1) using LLMs to reduce the search space for traditional methods, and (2) enabling LLMs to autonomously search optimal hyperparameters. On one hand, LLMs can significantly reduce the vast search spaces in Neural Architecture Search (NAS) and HPO [49, 25, 23, 20]. For example, GPT-NAS [49] combines GPT models with evolutionary algorithms [57] to rapidly prune low-quality architectures, enhancing search efficiency. Morris et al. [25] introduce \"Evolution of Thought,\" allowing LLMs to refine architectures and hyperparameters based on feedback iteratively. AutoM3L [23] integrates external tools such as Ray.Tune for hyperparameter tuning, with LLMs recommending optimized search spaces. Llambo [20] formulates BO problems in natural language, enabling LLMs to propose high-potential candidates and iteratively optimize based on historical observations. On the other hand, several works [4, 18, 59, 55, 1, 12, 53, 54] empower LLMs to search hyperparameter configurations autonomously. AutoMMLab [46] and GENIUS [59] treat LLMs as black-box optimizers, iteratively refining configurations based on evaluation feedback. CAAFE [12] focuses on tabular data, where LLMs generate new semantically meaningful features based on dataset descriptions. Moreover, LLM-based agent frameworks have emerged, leveraging feedback from machine learning platforms or historical experiments. For instance, AutoML-GPT [55] automates the pipeline from preprocessing to model selection and hyperparameter tuning, adjusting strategies based on experimental logs. MLCopilot [53] predicts hyperparameters for unseen tasks by canonicalizing prior task experiences and interactively refining solutions through LLM reasoning. AgentHPO [20] autonomously processes task descriptions, conducts experiments, and iteratively improves hyperparameter search based on accumulated trials. Building upon these advancements, we extend the LLM-agent framework to the domain of test-time scaling budget allocation in multi-stage complex tasks, aiming to search for compute-optimal scaling configurations. A.12 Limitations Our problem focuses on multi-stage tasks with static stages, where the compute budget is allocated across fixed set of subtasks. However, in real-world applications, some tasks exhibit dynamic multi-stage behavior, where the actual runtime subtasks may vary depending on input conditions or user interaction. For example, in voice-based personal assistants on mobile devices, the system may dynamically decide whether to perform document retrieval, clarification, or follow-up question generation based on the users query and context. In such scenarios, it is difficult to predefine all possible subtasks and their corresponding budget allocations. Consequently, applying our LLMagent-based search framework to these dynamic multi-stage tasks presents considerable challenge. A.13 Broader Impact This work proposes AgentTTS, an LLM-agent-based framework for compute-optimal test-time scaling in multi-stage tasks. The methodology offers significant positive societal impacts. First, by improving inference efficiency and reducing unnecessary computation, it contributes to more sustainable and cost-effective deployment of large language models, especially in resource-constrained environments. Second, our framework promotes better alignment between model selection and task complexity, enabling more accessible AI solutions across diverse application domains such as education, healthcare, and low-resource software development. Nonetheless, our approach raises potential concerns. AgentTTS relies on repeated sampling from foundation LLMs, which can amplify not only their strengths but also their limitations. For instance, hallucinations in large language models may be intensified through test-time scaling integration and efficiently propagated, increasing the risk of misinformation. Moreover, LLMs are susceptible to adversarial attacks such as jailbreaks, backdoor injections, and membership inference attacks, thereby heightening the security risks associated with AgentTTS."
        }
    ],
    "affiliations": [
        "Amazon, Palo Alto, CA, USA",
        "The Pennsylvania State University, University Park, PA, USA"
    ]
}