{
    "paper_title": "Marigold-DC: Zero-Shot Monocular Depth Completion with Guided Diffusion",
    "authors": [
        "Massimiliano Viola",
        "Kevin Qu",
        "Nando Metzger",
        "Bingxin Ke",
        "Alexander Becker",
        "Konrad Schindler",
        "Anton Obukhov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Depth completion upgrades sparse depth measurements into dense depth maps guided by a conventional image. Existing methods for this highly ill-posed task operate in tightly constrained settings and tend to struggle when applied to images outside the training domain or when the available depth measurements are sparse, irregularly distributed, or of varying density. Inspired by recent advances in monocular depth estimation, we reframe depth completion as an image-conditional depth map generation guided by sparse measurements. Our method, Marigold-DC, builds on a pretrained latent diffusion model for monocular depth estimation and injects the depth observations as test-time guidance via an optimization scheme that runs in tandem with the iterative inference of denoising diffusion. The method exhibits excellent zero-shot generalization across a diverse range of environments and handles even extremely sparse guidance effectively. Our results suggest that contemporary monocular depth priors greatly robustify depth completion: it may be better to view the task as recovering dense depth from (dense) image pixels, guided by sparse depth; rather than as inpainting (sparse) depth, guided by an image. Project website: https://MarigoldDepthCompletion.github.io/"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 1 ] . [ 1 9 8 3 3 1 . 2 1 4 2 : r Marigold-DC: Zero-Shot Monocular Depth Completion with Guided Diffusion"
        },
        {
            "title": "Massimiliano Viola Kevin Qu Nando Metzger Bingxin Ke Alexander Becker\nKonrad Schindler Anton Obukhov",
            "content": "ETH Zurich Figure 1. Marigold-DC is zero-shot, generative method for depth completion. It leverages pretrained, diffusion-based monocular depth estimator as scene understanding prior and integrates sparse depth guidance into the denoising process. No fine-tuning is required, as the method utilizes test-time optimization to update the latent representation. Compared to existing methods, Marigold-DC recovers plausible depth maps even from very sparse depth observations and excels at zero-shot generalization across broad range of scenes."
        },
        {
            "title": "Abstract",
            "content": "Depth completion upgrades sparse depth measurements into dense depth maps guided by conventional image. Existing methods for this highly ill-posed task operate in tightly constrained settings and tend to struggle when applied to images outside the training domain or when the available depth measurements are sparse, irregularly distributed, or of varying density. Inspired by recent advances in monocular depth estimation, we reframe depth completion as an imageconditional depth map generation guided by sparse measurements. Our method, Marigold-DC, builds on pretrained latent diffusion model for monocular depth estimation and injects the depth observations as test-time guidance via an optimization scheme that runs in tandem with the iterative inference of denoising diffusion. The method exhibits excellent zero-shot generalization across diverse range of environments and handles even extremely sparse guidance effectively. Our results suggest that contemporary monocular depth priors greatly robustify depth completion: it may be better to view the task as recovering dense depth from (dense) image pixels, guided by sparse depth; rather than as inpainting (sparse) depth, guided by an image. Project website: https://MarigoldDepthCompletion.github.io/ 1. Introduction Depth completion aims to convert sparse depth measurements into dense depth map, using an image typically It is standard RGB or grayscale as guidance  (Fig. 1)  . 1 useful across range of computer vision applications that combine conventional cameras with sparser range sensors, e.g., robotics, autonomous driving, and 3D city modeling. Traditional approaches predominantly rely on convolutional neural networks (CNNs) [6, 7, 43, 49] or transformer models [53, 79, 83] and achieve satisfactory results within their particular problem setting. However, they do not generalize well and fail, often catastrophically, when transferred to new domains  (Fig. 1)  . This is all the more concerning because depth sensors are barely standardized and each system comes with its own sampling pattern, data gaps, and noise characteristics. In contrast, depth estimation only from single view, without depth guidance, has progressed to point where it generalizes remarkably well and can handle broad range of images in the wild [3, 52, 76, 77], which begs the question: why does depth completion not generalize at least as well? The answer, we claim, lies in the much more powerful visual prior. Modern monodepth estimators build on top of foundation models like DINOv2 [48] or Stable Diffusion [55] and inherit their rich knowledge about the structure of the visual world. We therefore bridge the gap between depth completion and monocular depth and adapt Marigold [31], latent diffusion model (LDM) for monodepth estimation, to the depth completion task. Marigold casts depth estimation from single image as generating depth map, conditioned on the input image. Marigold-DC uses this capability as basis and adds sparse depth observations as further guidance signal. Importantly, our proposed scheme is based on test-time optimization and does not alter the Marigold model; thus avoiding the risk of degrading the prior, as well as the effort of collecting or generating suitable training data. Our proposed guidance scheme exploits the iterative nature of Denoising Diffusion Probabilistic Models (DDPMs) [27, 67]. This class of models has revolutionized image generation. They are capable of synthesizing photorealistic images from pure noise, and by appropriate training, the generative process can be conditioned on various inputs, including text [55, 60] but also images and depth maps [80]. What is more, also fully trained DDPMs can, at test time, be guided towards specific outputs by injecting suitable signals into the inference process [1, 14], which opens up the possibility to repurpose them for certain applications without having to retrain them. In this paper, we leverage such test-time guidance for depth completion, by guiding the inference loop of pretrained monodepth estimator with additional depth input. Our contributions are: We rethink depth completion from the perspective of monocular depth prediction, such that it can benefit from the comprehensive visual knowledge baked into state-ofthe-art monodepth estimators, and the associated ability to generalize. We introduce computational scheme that seamlessly integrates sparse depth cues into the diffusion process of pretrained LDM, and thus achieves depth completion without any architectural modifications or retraining of that base model. We design strategy to anchor affine-invariant predictions in latent space on sparse depth cues in metric space. By dynamically adjusting the corresponding scale and shift parameters during inference, our method effectively aligns the model predictions with the available depth measurements. In experiments on several datasets, we demonstrate that Marigold-DC sets new state of the art for the depth completion task, especially in the desirable but challenging zero-shot setting. With our take on depth completion as constrained form of monocular depth estimation, we hope to close the widening performance gap between those two closely related computer vision problems and inspire further research towards depth completion methods that generalize to unseen images, environments, and sensor setups. 2. Related Work 2.1. Depth Completion Early depth completion methods rely solely on sparse depth inputs and employ classical interpolation techniques or sparsity-invariant convolutional neural networks (CNNs) [8, 34, 70]. These approaches often produce blurred predictions lacking fine structural details, especially around object boundaries. To address this, it has become common to incorporate an RGB image as guidance, enabling sharper transitions and improved extrapolation in regions without depth information. Other works [28, 43, 44, 68] leverage both sparse depth and RGB inputs using encoder-decoder multi-modal fusion networks with ResNet [25] backbone. Advancements include multi-scale prediction objectives [29, 35], intermediate representations such as surface normals [50, 74, 82] or coarse depth estimates [39], and graph-based approaches for modeling neighborhood relations [73, 84]. Post-processing refinement methods, mostly following the spatial propagation network (SPN) mechanism [40], have been proposed to enhance output quality. Notable examples include CSPN [6] and its successor CSPN++ [7], which introduce convolutions with fixed and adaptive kernels, respectively, improving efficiency. Further improvements involve non-local neighborhoods in NLSPN [49], adaptive affinity matrices in DySPN [37], and varying kernel scopes in LRRU [71]. This paradigm has been extended to three-stage method in BP-Net [69]. While most architectures process features in 2D, some methods [5, 64, 75, 79] leverage 3D geometry information, though this requires knowledge of camera intrinsics, limiting generalization. The vision transformer (ViT) [15], widely successful in computer vision, has also been explored for depth completion in works like GuideFormer [53], PointDC [79], and CompletionFormer [83]. To handle sparse and irregular patterns, SpAgNet [11] proposes sparsity-agnostic framework and obtains reasonable predictions even with <10 guidance points. For robust generalization, VPP4DC [2] revisits depth completion from fictitious stereo-matching perspective, and OGNI-DC [86] iteratively optimizes depth gradient field. Generative approaches have also been explored: DepthFM [23] employs flow matching [38, 41] conditioned on the RGB image and the sparse depth, densified with distance functions, to implement refinement. Such densification before or during processing is common strategy, as also seen in [22], which encodes blend of sparse guidance and intermediate predictions back into the latent space. The drawback of intermediate densification is that it introduces high-frequency variations around sparse guidance points, resulting in corruption of the latent codes. This requires spatial smoothing to manage artifacts, adding further design choices and complexity to the guidance framework. In contrast, our guidance approach integrates observations via test-time optimization, penalizes decoded predictions in the pixel space, and can accommodate varying sparsity levels, cf . Fig. 1. 2.2. Diffusion Models Denoising Diffusion Probabilistic Models (DDPMs) [27] generate high-quality samples by reversing Gaussian noise diffusion process. Their enhanced sample quality [14] and computational efficiency [46] have been well documented. Denoising Diffusion Implicit Models (DDIMs) [65] offer speedups with non-Markovian inference. Conditional diffusion models allow controlled generation by incorporating inputs like text [60], images [59], and semantic maps [80]. Stable Diffusion (SD) [55] exemplifies text-based image generation, using an LDM trained on the large-scale LAION-5B dataset [63]. By performing denoising in compressed latent space via U-Net [56], it reduces complexity, making the model more scalable and easier to fine-tune. separately trained variational autoencoder (VAE) maps images to and from latent space, thus embedding extensive image knowledge into the model weights, which has been utilized for various downstream tasks. 2.3. Diffusion Guidance To allow fine-grained control over the output, guidancebased diffusion [13] incorporates external supervision alongside the original conditioning, using guidance function that measures whether certain criteria are met. In guided image generation, classifier guidance [14] enables class-conditional outputs from pretrained, unconditional diffusion model, via gradients from classifier trained on ImageNet [58] images at different noise scales. Similarly, gradients from CLIP model [51] trained on noisy images can guide generation toward user-defined text caption [47]. An alternative, classifier-free guidance [26, 47], achieves similar control without training separate classifier, by parameterizing both conditional and unconditional diffusion models within the same network. The approach is further extended to handle general nonlinear inverse problems [9], using gradients calculated on the expected denoised images. Guidance is commonly framed from score-based perspective on denoising diffusion [66, 67], where an unconditional model approximates the time-dependent score function of the log data distribution. Finally, variety of universal constraints, such as segmentation masks, image style, and object location, have been applied with SD under single framework [1], fully exploiting the flexibility and control of diffusion-based image generation. 2.4. Diffusion for Monocular Depth Estimation Monocular depth estimation predicts per-pixel depth from single RGB image, challenging and ill-posed problem due to the absence of definitive depth information. Deep learning approaches leverage features learned from large datasets to tackle this. More recently, several methods have employed DDPMs for generative depth estimation. DDP [30] conditions diffusion on image features for dense visual prediction. DiffusionDepth [16] performs latent space diffusion conditioned on features from Swin Transformer [42]. DepthGen [62] and its successor DDVM [61] extend multitask diffusion models for depth estimation, addressing noisy ground truth and emphasizing pretraining on synthetic and real data for improved quality. VPD [85] utilizes pretrained SD with text input as its image feature extractor for various visual perception tasks, highlighting the semantic knowledge embedded in these models. Marigold [31] repurposes Stable Diffusion to denoise depth maps conditioned on an input image, achieving impressive zero-shot monocular depth estimation with fine details across diverse datasets. Trained purely on synthetic data and relying on the foundational knowledge of Stable Diffusion, this affine-invariant model outputs depth predictions in fixed [0, 1] range. Motivated by our view that depth completion is essentially monocular depth estimation anchored at sparse points, we develop plug-and-play optimization framework around Marigold by lifting its output to metric space, enabling effective depth completion without fine-tuning or architectural changes. 3. Method 3.1. Guided Diffusion Formulation We formulate depth completion as guided monocular depth estimation task and use Marigold, the pretrained, affine3 invariant, diffusion-based model, as our prior. At inference time, we dynamically refine its predictions by incorporating penalizing loss at sparse point measurements in metric space. Marigold generates linearly normalized depth map ˆd RW within the range [0, 1] by sampling from the conditional distribution D(d x), where RW H3 is an RGB image, and is pixel-wise corresponding depth map. Let RW denote sparse metric depth map in the same image space, where only limited subset of pixels contains valid depth values. Let further ˆa and ˆb represent the scale and shift coefficients for linear prediction scaling, maintained as additional parameterized variables throughout the process and initialized as in Sec. 3.2. The proposed modified inference pipeline for depth completion is presented in Fig. 2. We start by encoding the input image into latent space, using the SD encoder to obtain the latent code z(x) := E(x). We also sample random noise tensor z(d) (0, I) as the initial depth latent, which we also treat as an optimizable parameter. We employ the DDIM scheduler [65] for accelerated inference with = 50 steps, adopting the fix for trailing timesteps [19]. Then, at every denoising iteration t, we feed the image latent concatenated with the depth latent z(d) into the U-Net to obtain noise estimate ˆϵt := ϵθ(z(d) , z(x), t). Instead of immediately completing the current iteration and continuing with timestep 1, we preview the final, denoised depth latent z(d) 0t by computing posterior mean estimate using Tweedies formula [17]: z(d) z(d) 0t = 1 αt ˆϵt αt , (1) where αt is defined by the noise schedule. After obtaining z(d) 0t , we decode it through the SD decoder to produce predicted clean depth sample ˆdt in pixel space. This affine-invariant depth is then scaled and shifted using our parameterized coefficients to render it in metric units as := ˆdt ˆa + ˆb. We then compute the loss function ˆd(m) between the sparse depth and the metric estimate ˆdm at the pixels where is valid, resizing the prediction as needed if processing at an intermediate resolution (which is possible in Marigold). For L, we use an equally weighted sum of mean absolute error and mean squared error. Intuitively, this combination penalizes large geometric errors while also encouraging fine-scale adjustments. Given the loss, we compute gradients for both scale and shift coefficients, as well as the latent depth variable z(d) , by backpropagating through the decoder D, the Eq. (1), and the U-Net prediction. We found that scaling the gradient w.r.t. the depth latent proves effective in practice, such that its L2 norm is proportional to that of the predicted score gradient [78]. We update ˆa, ˆb, and z(d) based on their respective gradients using the Adam [32] optimizer, with learning rate of 0.005 for the Figure 2. Overview of Marigold-DC inference scheme for depth completion. Our method extends the existing Marigold architecture (above the dashed line) by incorporating task-specific guidance mechanisms (below the line). Starting from the current depth latent variable z(d) , our method calculates preview of the final denoised depth map via the Tweedie formula. This preview is then decoded and scaled using the learnable scale parameter ˆa and shift parameter ˆb. We backpropagate the loss (red arrows) between the preview and the sparse depth and adjust the latent simultaneously with the scale and shift. Finally, we execute scheduler step to proceed to the next denoising iteration. affine parameters and 0.05 for the depth latent. The former two always remain positive thanks to the parametrization described in Sec. 3.2. After this, we perform scheduler step with the previously computed noise estimate ˆϵ and proceed to the next denoising iteration. Once the process is completed and the final denoising iteration has been reached, we decode the optimized affine-invariant depth map, apply the scale and shift coefficients, and return depth prediction in metric units. The optimization loop is repeated at each timestep for total of iterations, similarly to [9, 18, 21, 36], but with additional variables included in the loop. This proposed update encourages affine-invariant predictions to align with both the image conditioning and sparse depth measurements, as standard depth completion architecture would, but with the added benefit of comprehensive representation of the visual world inherited from Stable Diffusion. This knowledge prevents overfitting to noisy depth measurements because the image prior serves as strong regularizer. Our end-to-end optimization approach is variant of guidance methods that use Bayes rule to adjust the score function with conditional term [1, 14, 78]. This often involves the 4 diffusion posterior sampling (DPS) [9] approximation of the likelihood, calculated on the expected clean samples. Our choice of simpler yet effective variant is motivated by three factors: (i) For latent inverse problems, straightforward extension of DPS lacks the theoretical guarantees of its pixelspace counterpart [57] due to extra nonlinearity introduced by and the absence of one-to-one mapping from latent to pixel space [13]; (ii) We introduce additional variables that require optimization, as the mapping from the base model output to sparse depth is linear but unknown, unlike traditional inverse approaches, which keep the mapping function between prediction and the condition fixed. (iii) Modifying the score gradient during our experiments led to less stable optimization and reduced overall performance. This way, our streamlined guidance approach can handle varying sparsity levels in the depth guidance. 3.2. Scale and Shift Parameterization Proper parameterization of scale and shift is crucial to achieve high-quality results. Notably, expressing metric depth through an affine-invariant prediction does not yield unique decomposition, as multiple affine transformations can represent the same depth structure. However, since Marigold outputs are in the unit interval, given affine parameters ˆa and ˆb, only metric values within the range [ˆb, ˆa + ˆb] can be predicted. If this interval is not expressive enough, the loss will push points in the affine-invariant space toward its boundaries, leading to irrecoverable saturation. We observe that least squares fit to the condition often produces range notably smaller than the full set of available depth values, mainly because the initial geometry is not fully accurate, especially at the far plane. The opposite is also true: overshooting the range forces optimization towards prediction with distribution of values concentrated in sub-range, even though Marigold has been trained to utilize the entire range of the decoder. To enable meaningful, non-saturating updates to all optimized components, we parameterize the scale and shift as follows (min-max initialization): ˆa = α2 (cmax cmin) ˆb = β2 cmin (2) where cmax and cmin are the maximum and minimum depth available as sparse conditioning, and α and β, initialized to one, are the parameters that receive the actual gradient updates. We ablate other initialization methods in Sec. 4.4. 3.3. Ensembling Procedure Even with anchoring at guidance points, inherent variability in the final depth maps persists with generative method, depending on the initial noise similar to the unguided setting. This variability is particularly pronounced in challenging areas, such as reflective surfaces, edges, and distant planes, where depth points are often missing due to sensor limitations or range constraints. We leverage the variability via simple ensemble method in metric space: after linearly scaling the predictions from multiple individual inferences, we compute the pixel-wise median to produce the final result. This gives us more robust estimates and, as an added benefit, generates an uncertainty map based on the median absolute deviation (MAD) between predictions. We use 10 separate predictions for evaluation, as suggested for Marigold. 4. Experiments 4.1. Evaluation Datasets We evaluate Marigold-DC in zero-shot setting on six realworld datasets unseen by the base model [31], which was trained exclusively on synthetic data from Hypersim [54] and Virtual KITTI [4]. The evaluation datasets span indoor and outdoor scenes and cover various image resolutions, sparse depth densities, acquisition devices, and noise levels. NYU-Depth V2 [45] consists of indoor scenes captured with an RGB-D Kinect sensor. We use the original test split of 654 samples. Images are downsampled to 320 240 and then center-cropped to 304 228, following established practice [6, 43, 49]. The sparse depth input is generated by sampling 500 random points from the ground truth depth map. ScanNet [12] contains room scans collected with commodity RGB-D sensor. Following the filtering in [64], we select 745 samples from the official 100 scenes for testing. Images are resized to 640 480 to align with the depth resolution, and 500 random points are sampled as sparse guidance. The VOID [72] dataset includes synchronized RGB and depth streams of indoor and outdoor scenes at resolution of 640 480, acquired via active stereo. We utilize all 800 frames from the eight designated test sequences, which provide sparse depth maps with density of up to 1500 points. iBims-1 [33] is high-quality indoor RGB-D dataset captured with laser scanner. It is characterized by low noise level, sharp depth transitions, precise details, and an extended depth range of up to 50 meters. We employ all 100 available images at 640 480 resolution and sample 1000 random depth points from the intersection of valid pixel masks (invalid, transparent, missing) as per the official evaluation protocol. The KITTI DC [70] dataset comprises driving scenes with paired RGB images and sparse LiDAR depth measurements captured at resolution of 1216 352. The semi-dense ground truth is obtained by temporally accumulating multiple consecutive LiDAR frames with error filtering [20, 70]. We use the original validation split of 1000 samples and remove outliers [10] from the guidance points based on distance from the minimum depth within local 7 7 patch, as in [2]. DDAD [24] is an autonomous driving dataset featuring 360 multi-camera setup, capturing longrange LiDAR depth up to 250 meters. The official validation set includes 3950 samples for each camera at 1936 1216 resolution. Following [2, 86], we use only the front-facing view and sample approximately 20% of the available depth measurements as sparse input, applying the same filtering as done for KITTI DC raw LiDAR [10]. 4.2. Evaluation Protocol As mentioned, Marigold-DC is zero-shot approach that requires no task-specific training, unlike our baselines, which rely on depth-completion checkpoints trained specifically for indoor (NYU-Depth V2) or outdoor (KITTI DC) environments. Our proposition for fair evaluation is to transfer the indoor checkpoints to the indoor benchmarks (ScanNet, iBims-1, and VOID) and the outdoor checkpoints to the outdoor benchmark (DDAD). For evaluation on NYU-Depth V2 and KITTI DC, we avoid expensive retraining of baselines on other datasets and instead reuse the available zero-shot results from the VPP4DC [2] paper, reporting the best of all examined training configurations and leaving the rest blank. We run inference on each dataset at its original resolution, except for NYU and DDAD, where we resize the images to 768-pixel longer side while preserving the aspect ratio. This is done for our method and Marigold due to the image sizes being too small and too large, respectively. In these cases, we resize the output to the original resolution for guidance, enabling processing at the finest level. Following recent work [2, 64, 86], we report Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) as performance metrics. 4.3. Comparison with Other Methods We compare Marigold-DC to five baselines that have achieved strong results in standard evaluation settings on NYU-Depth V2 and KITTI DC, with some also claiming good generalization. NLSPN [49], CompletionFormer [83], and BP-Net [69] are variants of the multi-modal fusion with depth refinement strategy. VPP4DC [2] reformulates the problem as fictitious stereo matching, and OGNI-DC [86] optimizes depth gradient field. To show the effectiveness of our guidance framework, we also compare to vanilla Marigold [31] using only the RGB input, followed by leastsquares estimate for scale and shift based on the sparse depth. This method is referred to as Marigold + LS below. As shown in Tab. 1, Marigold-DC outperforms the baselines in most cases and secures the highest overall ranking by significant margin. Notably, this is achieved despite relying on frozen base model trained on synthetic datasets for different task, offering true plug-and-play solution largely independent of guidance patterns. We argue that the crucial factor is to exploit the rich monocular depth estimation prior provided by the base model, while simultaneously supplying it with sparse evidence. This aligns with our hypothesis that depth completion benefits more from strong visual prior than from task-specific training. In Fig. 3, we present qualitative comparison of samples Figure 3. Qualitative comparison of all benchmarked methods on samples from four datasets. Non-generative methods struggle with dataset-specific biases, such as input resolution lock or variations in guidance sparsity. Marigold-DC demonstrates high-quality metric depth densification with strong generalization capabilities. The sampling patterns and noise characteristics vary across datasets. Black regions indicate missing depth measurements. Arrows suggest key areas of interest. from several evaluation datasets. In these examples, although the depths predicted with Marigold + LS are rich in detail, they frequently exhibit layout distortions  (Fig. 4)  and struggle to position the far plane accurately. In such cases, merely achieving correct depth ordering is insufficient, and the overall scene layout becomes critical. 6 Table 1. Quantitative comparison of Marigold-DC with state-of-the-art depth completion methods on several zero-shot benchmarks. All metrics are presented in absolute terms; bold numbers are best, underscored second best. In most cases, our method outperforms other approaches in both indoor and outdoor scenes despite not having seen real depth sample nor being trained for the depth completion task. Method ScanNet MAE RMSE IBims-1 MAE RMSE VOID 1500 MAE RMSE NYU-Depth V2 MAE RMSE KITTI DC MAE RMSE DDAD MAE RMSE NLSPN [49] (ECCV 20) CompletionFormer [83] (CVPR 23) VPP4DC [2] (3DV 24) BP-Net [69] (CVPR 24) OGNI-DC [86] (ECCV 24) Marigold + LS [31] (CVPR 24) Ours (w/o ensemble) Ours (w/ ensemble) 0.036 0.120 0.023 0.122 0.029 0.083 0.020 0.017 0.127 0.232 0.076 0.212 0.094 0. 0.063 0.057 0.049 0.058 0.062 0.078 0.059 0.154 0.062 0.045 0.191 0.206 0.228 0.289 0.186 0.286 0.205 0.166 0.427 0.642 0.166 0.270 0.175 0. 0.157 0.152 1.353 1.684 0.606 0.742 0.593 0.628 0.557 0.551 0.440 0.186 0.077 - - 0.190 0.057 0.048 0.716 0.374 0.247 - - 0. 0.142 0.124 1.335 0.952 0.413 - - 1.709 0.558 0.434 2.076 1.935 1.609 - - 3.305 1.676 1.465 9.231 2.498 9.471 2.518 1.344 6.781 8.344 2.270 1.867 6.876 8.217 14. 2.985 2.364 7.905 6.449 Metrics highlighted in gray are sourced from VPP4DC [2], reported with the best training setup. For the others, we re-evaluated all baselines in zero-shot settings, except for BP-Net and OGNI-DC on NYU and KITTI DC datasets, where we do not show results, as these models would require expensive retraining. used to update the depth latent and affine parameters. For the study, we vary the value of λbase on log-scale from 0.005 to 0.5, setting the learning rate for the depth latent to this value and the learning rate for the scale and the shift to λbase/10. The results are shown in Fig. 5: we observe the optimum around λbase = 0.05, with performance degrading in both directions. Lower values result in weak guidance, whereas higher values lead to instability in the optimization process. Number of denoising steps. We vary the number of denoising steps of the DDIM scheduler [65] and show results in Fig. 5. Unsurprisingly, increasing the number of denoising steps improves the results, though the relative gain diminishes between 25 and 50 steps, with performance saturating beyond that. Inference with less than 25 steps comes at the cost of reduced accuracy and geometric distortions. We anticipate greater speed improvements by working with the base model trained on more complex, deeper scenes with enhanced far-plane supervision. This is because the discrepancy between the initial prediction and the true linear scaling heavily influences convergence speed. Test-time ensembling. We assess the effectiveness of the proposed test-time ensembling scheme by comparing different ensemble sizes. As shown in Tab. 1 and Fig. 5, single prediction already yields competitive, often state-of-the-art results. Consistent with standard Marigold, we observe performance boost through ensembling, with errors generally decreasing as ensemble size increases, albeit with linear increase in runtime. The relative improvement diminishes for ensemble sizes > 10. This is hyper-parameter that can be easily adjusted to balance between runtime and performance. Scale and shift initialization. Proper initialization of the scale and shift parameters is crucial for converging to solution that aligns well with the sparse depth conditioning, especially when significant adjustments to the initial prediction are necessary. We compare four different initialization Figure 4. Comparison between vanilla Marigold and guided predictions on iBims-1 [33] samples. With guidance from sparse points, the scene geometry is correctly adjusted (1st row), and the depth of challenging protruding text can be recovered (2nd row). Performance degradation occurs rapidly in methods that rely on spatial propagation, leading to visible artifacts in the predictions. We hypothesize that this is primarily due to their inability to handle inputs that are sparser than those encountered during training. Our depth completion approach produces realistic results in regions lacking depth measurements and preserves fine details where other methods resort to coarse interpolation. This makes our method particularly effective in sparse settings with only few hundred or even few dozen points. 4.4. Ablation Studies We analyze the impact of some key design choices on overall performance. Aspects not targeted in each respective experiment are fixed to our reference settings: learning rate of 0.05 for the depth latent and 0.005 for affine parameters and min-max initialization for scale and shift. We re-use some of the default settings of Marigold [31], namely 50 DDIM denoising steps and an ensemble size of 10. All studies are conducted on randomly selected subset of 100 samples from the training split of NYU-Depth V2. Learning rates. Since our method involves test-time optimization, we evaluate the impact of different learning rates 7 Figure 5. Ablation of learning rate, number of inference steps, and ensemble size. Left: Empirically, we identify an optimal learning rate that balances the trade-off between guidance strength and optimization stability. Middle: Performance consistently improves with more denoising iterations, showing saturation beyond 50 steps. Right: monotonic improvement is seen with increasing ensemble size, diminishing after 10 predictions per sample."
        },
        {
            "title": "Initialization",
            "content": "Least Squares Oracle Extended Min-Max Min-Max NYU-Depth V2 MAE RMSE 0.065 0.058 0.057 0.055 0.165 0.153 0.148 0.147 Table 2. Ablation of scale and shift initialization. Min-max methods perform similarly to the oracle initialization, whereas least squares performs significantly worse. methods, each derived from the affine-invariant depth map estimate obtained after the first denoising iteration: (1) least-squares fit to align with the sparse depth; (2) minmax scaling policy to match the minimum and maximum values of the sparse input; (3) extended min-max scaling, adjusting the range by 5% beyond the far plane and 5% closer to the near plane, assuming the sparse points provide only lower bound on scene depth; (4) an oracle using the union of sparse conditioning and ground truth, which is not available in practice but provides baseline of near-perfect initialization. As shown in Tab. 2, (extended) min-max and the oracle initialization achieve comparable performance. Leastsquares performs noticeably worse. The min-max method might face issues when the point distribution is not fully representative of the actual range (e.g. the degenerate case of guidance only in narrow range). However, we are unaware of any depth completion method that does not suffer from this issue. The most robust solution within our framework would be an improved model-based initialization or an adaptive switch between min-max and least squares whenever the point distribution is considered unrepresentative. Guidance method. We compare our fully end-to-end optimization framework to an alternative guidance approach that 8 NYU-Depth V2 Score Function Direct Optim. Direct Optim. MAE RMSE Scale & Shift Depth Latent 0.058 0.055 0.150 0. Table 3. Ablation of guidance method. Direct optimization of the depth latent proves more effective than modifying the score function. Affine parameters are optimized separately in both cases. adjusts the score function with conditional term [1, 14, 78] and optimizes the scale and shift separately. Both approaches are evaluated at optimal settings, with results reported in Tab. 3. Our optimization-based method outperforms the mixed variant, demonstrating higher performance and greater stability. 5. Conclusion We have introduced Marigold-DC, depth completion method that effectively combines the generalization and robustness of monocular depth estimators with the explicit anchoring needed to solve depth completion tasks. By leveraging pretrained, affine-invariant diffusion-based model and dynamically incorporating sparse depth measurements during inference, Marigold-DC merges rich monocular depth priors with reliable sensor data. Without task-specific training, the method achieves state-of-the-art performance across six zero-shot benchmarks spanning both indoor and outdoor environments. This work aims to inspire further research on methods prioritizing generalization and robustness, extend depth completion beyond specific training domains, and make it more applicable in real-world settings. Our diffusionbased backbone introduces computational overhead due to the iterative nature of denoising diffusion models and its ensembling process. Future research directions could focus on reducing inference time, for example, by factoring the guidance as network input, cf . [81]."
        },
        {
            "title": "References",
            "content": "[1] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom GoldIn Prostein. Universal guidance for diffusion models. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 843852, 2023. 2, 3, 4, 8 [2] Luca Bartolomei, Matteo Poggi, Andrea Conti, Fabio Tosi, and Stefano Mattoccia. Revisiting depth completion from stereo matching perspective for cross-domain generalization. In 2024 International Conference on 3D Vision (3DV), pages 13601370. IEEE, 2024. 3, 5, 6, 7 [3] Aleksei Bochkovskii, Amael Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. arXiv preprint arXiv:2410.02073, 2024. 2 [4] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual KITTI 2. CoRR, abs/2001.10773, 2020. 5 [5] Yun Chen, Bin Yang, Ming Liang, and Raquel Urtasun. Learning joint 2d-3d representations for depth completion. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1002210031, Los Alamitos, CA, USA, 2019. IEEE Computer Society. [6] Xinjing Cheng, Peng Wang, and Ruigang Yang. Depth estimation via affinity learned with convolutional spatial propagation network. In Proceedings of the European Conference on Computer Vision (ECCV), pages 103119, 2018. 2, 5 [7] Xinjing Cheng, Peng Wang, Chenye Guan, and Ruigang Yang. CSPN++: Learning context and resource aware convolutional spatial propagation networks for depth completion. In AAAI Conference on Artificial Intelligence, 2019. 2 [8] Nathaniel Chodosh, Chaoyang Wang, and Simon Lucey. Deep convolutional compressed sensing for lidar depth completion. In Computer VisionACCV 2018: 14th Asian Conference on Computer Vision (ACCV), Perth, Australia, December 26, 2018, Revised Selected Papers, Part 14, pages 499513. Springer, 2019. 2 [9] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posteIn The rior sampling for general noisy inverse problems. Eleventh International Conference on Learning Representations (ICLR), 2023. 3, 4, 5 [10] Andrea Conti, Matteo Poggi, Filippo Aleotti, and Stefano Mattoccia. Unsupervised confidence for lidar depth maps and applications. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2022. IROS. 5, 6 [11] Andrea Conti, Matteo Poggi, and Stefano Mattoccia. SparIn Proceedings of the sity agnostic depth completion. IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 58715880, 2023. 3 [12] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richlyannotated 3d reconstructions of indoor scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017. 5 and Mauricio Delbracio. survey on diffusion models for inverse problems. arXiv preprint arXiv:2410.00083, 2024. 3, [14] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. arXiv preprint arXiv:2105.05233, 2021. 2, 3, 4, 8 [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. 3 [16] Yiqun Duan, Xianda Guo, and Zheng Zhu. DiffusionDepth: Diffusion denoising approach for monocular depth estimation. arXiv preprint arXiv:2303.05021, 2023. 3 [17] Bradley Efron. Tweedies formula and selection bias. Journal of the American Statistical Association, 106(496):16021614, 2011. 4 [18] Hugging Face. The hugging face diffusion models course. https://huggingface.co/learn, 2022. Online, accessed on 2024-09-01. 4 [19] Gonzalo Martin Garcia, Karim Abou Zeid, Christian Schmidt, Daan de Geus, Alexander Hermans, and Bastian Leibe. Finetuning image-conditional diffusion models is easier than you think. arXiv preprint arXiv:2409.11355, 2024. [20] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012. 5 [21] Asya Grechka, Guillaume Couairon, and Matthieu Cord. Gradpaint: Gradient-guided inpainting with diffusion models. arXiv preprint arXiv:2309.09614, 2023. 4 [22] Jakub Gregorek and Lazaros Nalpantidis. SteeredMarigold: Steering diffusion towards depth completion of largely incomplete depth maps. arXiv preprint arXiv:2409.10202, 2024. 3 [23] Ming Gui, Johannes S. Fischer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, and Bjorn Ommer. Depthfm: Fast monocular depth estimation with flow matching. arXiv preprint arXiv:2403.13788, 2024. 3 [24] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos, and Adrien Gaidon. 3d packing for self-supervised monocular depth estimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 5 [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [26] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. 3 [27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint arxiv:2006.11239, 2020. 2, 3 [13] Giannis Daras, Hyungjin Chung, Chieh-Hsin Lai, Yuki Mitsufuji, Jong Chul Ye, Peyman Milanfar, Alexandros G. Dimakis, [28] Saif Imran, Yunfei Long, Xiaoming Liu, and Daniel Morris. Depth coefficients for depth completion. In IEEE Computer 9 Vision and Pattern Recognition (CVPR), Long Beach, CA, 2019. [29] Saif Imran, Xiaoming Liu, and Daniel Morris. Depth completion with twin-surface extrapolation at occlusion boundaries. In In Proceeding of IEEE Computer Vision and Pattern Recognition (CVPR), Nashville, TN, 2021. 2 [30] Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo. Ddp: Diffusion model for dense visual prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2174121752, 2023. 3 [31] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth In Proceedings of the IEEE/CVF Conference estimation. on Computer Vision and Pattern Recognition (CVPR), pages 94929502, 2024. 2, 3, 5, 6, 7 [32] Diederik Kingma and Jimmy Ba. Adam: method for In International Conference on stochastic optimization. Learning Representations (ICLR), San Diega, CA, USA, 2015. 4 [33] Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, and Marco Korner. Evaluation of cnn-based single-image depth estimation methods. In Proceedings of the European Conference on Computer Vision Workshops (ECCV-WS), pages 331348. Springer International Publishing, 2019. 5, 7 [34] Jason Ku, Ali Harakeh, and Steven Waslander. In defense of classical image processing: Fast depth completion on the cpu. In 2018 15th Conference on Computer and Robot Vision (CRV), pages 1622. IEEE, 2018. [35] Ang Li, Zejian Yuan, Yonggen Ling, Wanchao Chi, Chong Zhang, et al. multi-scale guided cascade hourglass network for depth completion. In The IEEE Winter Conference on Applications of Computer Vision (WACV), pages 3240, 2020. 2 [36] Haotian Lin, Yixiao Wang, Mingxiao Huo, Chensheng Peng, Zhiyuan Liu, and Masayoshi Tomizuka. Joint pedestrian trajectory prediction through posterior sampling. arXiv preprint arXiv:2404.00237, 2024. 4 [37] Yuankai Lin, Hua Yang, Tao Cheng, Wending Zhou, and Zhouping Yin. DySPN: Learning dynamic affinity for imageguided depth completion. IEEE Transactions on Circuits and Systems for Video Technology, pages 11, 2023. 2 [38] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations (ICLR), 2023. 3 [39] Lina Liu, Xibin Song, Xiaoyang Lyu, Junwei Diao, Mengmeng Wang, Yong Liu, and Liangjun Zhang. Fcfr-net: Feature fusion based coarse-to-fine residual learning for depth completion. In AAAI Conference on Artificial Intelligence, 2020. 2 [40] Sifei Liu, Shalini De Mello, Jinwei Gu, Guangyu Zhong, Ming-Hsuan Yang, and Jan Kautz. Learning affinity via spatial propagation networks. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2017. [41] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3 [42] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 3 [43] Fangchang Ma and Sertac Karaman. Sparse-to-dense: Depth prediction from sparse depth samples and single image. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 47964803, 2018. 2, 5 [44] Fangchang Ma, Guilherme Venturelli Cavalheiro, and Sertac Karaman. Self-supervised sparse-to-dense: Self-supervised depth completion from lidar and monocular camera. 2019 International Conference on Robotics and Automation (ICRA), pages 32883295, 2018. 2 [45] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012. 5 [46] Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models, 2021. 3 [47] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 3 [48] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 2 [49] Jinsun Park, Kyungdon Joo, Zhe Hu, Chi-Kuei Liu, and In So Kweon. Non-local spatial propagation network for depth completion. In Proc. of European Conference on Computer Vision (ECCV), 2020. 2, 5, 6, 7 [50] Jiaxiong Qiu, Zhaopeng Cui, Yinda Zhang, Xingdi Zhang, Shuaicheng Liu, Bing Zeng, and Marc Pollefeys. Deeplidar: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 3 [52] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE TPAMI, 2020. 2 [53] Kyeongha Rho, Jinsung Ha, and Youngjung Kim. Guideformer: Transformers for image guided depth completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 62506259, 2022. 2, 3 [54] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and 10 Joshua M. Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In International Conference on Computer Vision (ICCV) 2021, 2021. [55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 2, 3 [56] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention MICCAI 2015, pages 234241, Cham, 2015. Springer International Publishing. 3 [57] Litu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alex Dimakis, and Sanjay Shakkottai. Solving linear inverse problems provably via posterior sampling with latent diffusion models. In Advances in Neural Information Processing Systems (NeurIPS), 2023. 5 [58] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115 (3):211252, 2015. 3 [59] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH, pages 110, 2022. 3 [60] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems (NeurIPS), 35:3647936494, 2022. 2, 3 [61] Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek Kar, Mohammad Norouzi, Deqing Sun, and David J. Fleet. The surprising effectiveness of diffusion models for optical flow and monocular depth estimation. arXiv preprint arXiv:2306.01923, 2023. [62] Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, and David Fleet. Monocular depth estimation using diffusion models. arXiv preprint arXiv:2302.14816, 2023. 3 [63] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems (NeurIPS), 35:2527825294, 2022. 3 [64] Yunxiao Shi, Manish Kumar Singh, Hong Cai, and Fatih Porikli. Decotr: Enhancing depth completion with 2d and 3d attentions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1073610746, 2024. 2, 5, 6 [65] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv:2010.02502, 2020. 3, 4, 7 [66] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc., 2019. [67] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations (ICLR), 2021. 2, 3 [68] Jie Tang, Fei-Peng Tian, Wei Feng, Jian Li, and Ping Tan. Learning guided convolutional network for depth completion. IEEE Transactions on Image Processing, 30:11161129, 2020. 2 [69] Jie Tang, Fei-Peng Tian, Boshi An, Jian Li, and Ping Tan. Bilateral propagation network for depth completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 97639772, 2024. 2, 6, 7 [70] Jonas Uhrig, Nick Schneider, Lukas Schneider, Uwe Franke, Thomas Brox, and Andreas Geiger. Sparsity invariant cnns. In International Conference on 3D Vision (3DV), 2017. 2, 5 [71] Yufei Wang, Bo Li, Ge Zhang, Qi Liu, Gao Tao, and Yuchao Dai. Lrru: Long-short range recurrent updating networks for depth completion. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2023. 2 [72] Alex Wong, Xiaohan Fei, Stephanie Tsuei, and Stefano Soatto. Unsupervised depth completion from visual inertial odometry. IEEE Robotics and Automation Letters, 5(2):18991906, 2020. 5 [73] Xin Xiong, Haipeng Xiong, Ke Xian, Chen Zhao, ZHIGUO CAO, and Xin Li. Sparse-to-dense depth completion revisited: Sampling strategy and graph construction. In European Conference on Computer Vision (ECCV), 2020. [74] Yan Xu, Xinge Zhu, Jianping Shi, Guofeng Zhang, Hujun Bao, and Hongsheng Li. Depth completion from sparse lidar data with depth-normal constraints. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 28112820, 2019. 2 [75] Zhiqiang Yan, Yuankai Lin, Kun Wang, Yupeng Zheng, Yufei Wang, Zhenyu Zhang, Jun Li, and Jian Yang. Tri-perspective view decomposition for geometry-aware depth completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 48744884, 2024. 2 [76] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1037110381, 2024. 2 [77] Wei Yin, Xinlong Wang, Chunhua Shen, Yifan Liu, Zhi Tian, Songcen Xu, Changming Sun, and Dou Renyin. Diversedepth: Affine-invariant depth prediction using diverse data. arXiv preprint arXiv:2002.00569, 2020. 2 [78] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William Interactive 3d arXiv preprint Freeman, and Jiajun Wu. Wonderworld: scene generation from single image. arXiv:2406.09394, 2024. 4, 11 [79] Z. Yu, Z. Sheng, Z. Zhou, L. Luo, S. Cao, H. Gu, H. Zhang, and H. Shen. Aggregating feature point cloud for depth completion. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 86988709, Los Alamitos, CA, USA, 2023. IEEE Computer Society. 2, 3 [80] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, pages 38363847, 2023. 2, 3 [81] Xiang Zhang, Bingxin Ke, Hayko Riemenschneider, Nando Metzger, Anton Obukhov, Markus Gross, Konrad Schindler, and Christopher Schroers. Betterdepth: Plug-and-play diffusion refiner for zero-shot monocular depth estimation. NeurIPS, 2024. 8 [82] Yinda Zhang and Thomas A. Funkhouser. Deep depth completion of single rgb-d image. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 175185, 2018. 2 [83] Youmin Zhang, Xianda Guo, Matteo Poggi, Zheng Zhu, Guan Huang, and Stefano Mattoccia. Completionformer: Depth completion with convolutions and vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1852718536, 2023. 2, 3, 6, [84] Shanshan Zhao, Mingming Gong, Huan Fu, and Dacheng Tao. Adaptive context-aware multi-modal network for depth completion. IEEE Transactions on Image Processing, 2021. 2 [85] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing text-to-image diffusion models for visual perception. arXiv:2303.02153, 2023. 3 [86] Yiming Zuo and Jia Deng. Ogni-dc: Robust depth completion with optimization-guided neural iterations. arXiv preprint arXiv:2406.11711, 2024. 3, 5, 6,"
        }
    ],
    "affiliations": [
        "ETH Zurich"
    ]
}