{
    "paper_title": "EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence",
    "authors": [
        "Chaoyin She",
        "Ruifang Lu",
        "Lida Chen",
        "Wei Wang",
        "Qinghua Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Ultrasound imaging has become the preferred imaging modality for early cancer screening due to its advantages of non-ionizing radiation, low cost, and real-time imaging capabilities. However, conventional ultrasound diagnosis heavily relies on physician expertise, presenting challenges of high subjectivity and low diagnostic efficiency. Vision-language models (VLMs) offer promising solutions for this issue, but existing general-purpose models demonstrate limited knowledge in ultrasound medical tasks, with poor generalization in multi-organ lesion recognition and low efficiency across multi-task diagnostics. To address these limitations, we propose EchoVLM, a vision-language model specifically designed for ultrasound medical imaging. The model employs a Mixture of Experts (MoE) architecture trained on data spanning seven anatomical regions. This design enables the model to perform multiple tasks, including ultrasound report generation, diagnosis and visual question-answering (VQA). The experimental results demonstrated that EchoVLM achieved significant improvements of 10.15 and 4.77 points in BLEU-1 scores and ROUGE-1 scores respectively compared to Qwen2-VL on the ultrasound report generation task. These findings suggest that EchoVLM has substantial potential to enhance diagnostic accuracy in ultrasound imaging, thereby providing a viable technical solution for future clinical applications. Source code and model weights are available at https://github.com/Asunatan/EchoVLM."
        },
        {
            "title": "Start",
            "content": "EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence Chaoyin She1*, Ruifang Lu2*, Lida Chen2, Wei Wang2, Qinghua Huang1 1Northwestern Polytechnical University 2The First Affiliated Hospital of Sun Yat-Sen University 5 2 0 2 8 1 ] . [ 1 7 7 9 4 1 . 9 0 5 2 : r Abstract Ultrasound imaging has become the preferred imaging modality for early cancer screening due to its advantages of non-ionizing radiation, low cost, and real-time imaging capabilities. However, conventional ultrasound diagnosis heavily relies on physician expertise, presenting challenges of high subjectivity and low diagnostic efficiency. Vision-language models (VLMs) offer promising solutions for this issue, but existing general-purpose models demonstrate limited knowledge in ultrasound medical tasks, with poor generalization in multi-organ lesion recognition and low efficiency across multi-task diagnostics. To address these limitations, we propose EchoVLM, vision-language model specifically designed for ultrasound medical imaging. The model employs Mixture of Experts (MoE) architecture trained on data spanning seven anatomical regions. This design enables the model to perform multiple tasks, including ultrasound report generation, diagnosis and visual question-answering (VQA). The experimental results demonstrated that EchoVLM achieved significant improvements of 10.15 and 4.77 points in BLEU1 scores and ROUGE-1 scores respectively compared to Qwen2-VL on the ultrasound report generation task. These findings suggest that EchoVLM has substantial potential to enhance diagnostic accuracy in ultrasound imaging, thereby providing viable technical solution for future clinical applications. Source code and model weights are available at https://github.com/Asunatan/EchoVLM. Introduction Ultrasound imaging is now key clinical diagnostic tool, thanks to its advantages such as no ionizing radiation, costeffectiveness, and real-time visualization. Due to these features, it is indispensable for early cancer detection, prenatal care, and dynamic assessment of organs like the thyroid, breast, liver, kidneys, cardiovascular and heart systems. However, traditional ultrasound diagnosis is subject to the professional expertise of radiologists. This reliance can lead to differing interpretations among observers and delays in diagnosis and treatment efficiency due to manual interpretation. Visual language models (VLMs) have achieved re- *These authors contributed equally. Corresponding author Copyright 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: Generic vs. ultrasound-specialized LLaVAOneVision performance comparison. markable advancements in multimodal perception and contextual understanding, significantly impacting medical image analysis across diverse modalities. However, applying these models to ultrasound diagnosis presents domainspecific challenges. For instance, the scarcity of ultrasoundspecific expertise and the necessity to account for organspecific anatomical variations pose significant hurdles. As demonstrated in Figure 1, our preliminary experiments with LLaVA-OneVision (Li et al. 2024) highlight the imperative need for specialized VLMs tailored to ultrasonography. The performance disparities revealed in this comparison underscore the limitations of generic models in capturing the nuanced characteristics inherent to ultrasound imaging. Such deficiencies hinder the seamless integration of multimodal learning frameworks into clinical ultrasound workflows, where precision and anatomical specificity are paramount. To address this critical gap, we introduce EchoVLM , the first ten-billion-parameter universal ultrasound-specialized VLM. Our framework is built upon three core innovations: First , we have curated the largest multi-organ ultrasound dataset to date, comprising 208,941 clinical cases and 1.47 million key imaging sections from 15 hospitals, spanning seven major organ systems: thyroid, breast, liver, kidney, gynecology, vessel, and heart. This comprehensive dataset ensures broad anatomical coverage for robust model training. Second, inspired by Self-Instruct (Wang et al. 2023), we propose few-shot prompting mechanism with expert validation , enabling the construction of multi-task instructiontuning data generation pipeline . This approach systematically synthesizes diverse diagnostic scenarios while maintaining clinical accuracy through expert oversight. Third, we integrate Dual-path MoE module with dynamic routing mechanism , enhancing the models adaptability to the heterogeneous and complex nature of ultrasound imaging tasks. This architecture allows task-specific expert subnetworks to specialize in distinct diagnostic domains, improving both efficiency and precision. In summary, our contributions are as follows: We pioneering propose the first universal ultrasoundspecialized VLM, EchoVLM, which is specifically designed to address the challenges of ultrasound diagnosis. We have compiled large-scale dataset from 15 hospitals, containing 208,941 cases and 1.47 million key imaging sections across seven major organ systems. Using this dataset, we have developed multi-task instructiontuning data generation pipeline for ultrasound. We incorporate Dual-path MoE module into the EchoVLM model. The MoE module employs dynamic routing mechanism, which significantly improves the models adaptability to the diverse and complex nature of ultrasound tasks. Related Work The advent of large language models (LLMs) has catalyzed paradigm shifts in artificial intelligence research, particularly in natural language processing domains encompassing linguistic comprehension, generative capabilities, and textcentric task execution. Nevertheless, conventional unimodal LLM architectures exhibited intrinsic limitations in visual perception modalities, thereby constraining their applicability to real-world scenarios requiring cross-modal integration. To overcome this, researchers have proposed multilevel fusion mechanisms, including abstract layers (Radford et al. 2021; Vasu et al. 2024; Zhang et al. 2024a), semantic embedding layers (Liu et al. 2023; Lin et al. 2024a,b), and cross-attention layers (Li et al. 2023b; Alayrac et al. 2022), to achieve collaborative integration of visual and textual features. In this context, the BLIP series pioneered the use of cross-attention mechanisms for dynamic interaction between visual features and textual elements, while LLaVA introduced visual instruction tuninga paradigm leveraging visual data to refine models perceptual capabilities through fine-tuning. Subsequent developments further enhanced performance by expanding datasets (Liu et al. 2024a; Zhang et al. 2024c), improving image resolution (Liu et al. 2024b; Wang et al. 2024; Hong et al. 2024) , incorporating multiimage and video understanding frameworks (Kar et al. 2024; Fan et al. 2024), optimizing projection layers (Li et al. 2026; Chen et al. 2025; Tong et al. 2024), and employing multiple visual encoders. Recent advancements emphasize finegrained understanding, integrating object localization (e.g., bounding boxes) and segmentation masks to improve spatial and semantic precision (Guo et al. 2024; Zhang et al. 2024b). Notably, recent studies have begun exploring VLM applications in medical domains, with LLaVA-Med (Li et al. 2023a) emerging as representative framework. However, these systems demonstrate significant limitations in specialized clinical contexts, particularly exhibiting deficiencies in ultrasound-specific knowledge acquisition that hinder structured report generation. Furthermore, such approaches lack sufficient context length to effectively address multiimage scenarios prevalent in ultrasound imaging. To overcome these domain-specific challenges, this study proposes the development of dedicated ultrasound-centric VLM architecture specifically optimized for diagnostic imaging workflows and medical terminology standardization in sonographic analysis."
        },
        {
            "title": "Method",
            "content": "Data Collection and Instruction-Tuning Data Generation Pipeline To develop VLM tailored to the ultrasound imaging domain, we compiled comprehensive dataset sourced from 15 hospitals. This dataset encompasses the seven major anatomical areas commonly assessed via ultrasound: the liver, kidney, thyroid, vascular system, gynecology, heart, and breast. To guarantee data quality, strict data filtration protocol was implemented: (1) Image Filtering. When exporting images from hospital databases, only single-region images were selected to prevent multi-region confusion. Additionally, we manually removed images that lacked corresponding reports to ensure alignment between imaging data and clinical documentation. (2) Text Filtering. Manually screened and eliminated reports unrelated to the specific region and those lacking corresponding images. This bidirectional filtering yielded 208,941 cases containing 1.47 million key-frame ultrasound images. Building upon this foundation, we established structured pipeline for instruction-tuning data generation using few-shot prompting strategy (see Figure 3). Medical experts were invited to develop 21 exemplar templates spanning diverse anatomical pathologies, with each template integrating three components designed to simulate realworld clinical processes: (1) Ultrasound descriptions: Detailed ultrasound findings documenting lesion location, size, morphology, echogenicity patterns, and other relevant features derived from image analysis. (2) Ultrasound diagnosis: Clinical summaries synthesizing key observational findings into standardized diagnostic statements. (3) QuestionFigure 2: This multicenter ultrasound dataset encompasses seven anatomical regions with heterogeneous clinical cases, including thyroid, heart, vessel, liver, breast, and kidney ultrasound images. answer pairs: Multidimensional question-answer sets including diagnostic interpretation, risk stratification, patient counseling, surveillance strategies, and treatment planning considerations. Subsequently, templates were systematically classified into open-ended and closed-ended categories, with tailored prompting strategies designed for each type. For open-ended templates, models were instructed to generate both questions and answers by referencing the provided examples, while for closed-ended templates, models were tasked with generating diverse questions, with answers derived from real ultrasound reports and diagnoses to ensure clinical accuracy. To remove redundancy, the generated data were processed through ROUGE-L and Simhash-based deduplication pipelines. Furthermore, the open-ended subset underwent dual validation for accuracy: automated evaluation using LLM and manual review of randomly sampled data by medical experts to ensure the quality of the generated content. This pipeline yielded final dataset of 1.8 million instruction-tuning data pairs. Further details are provided in the Supplementary Materials."
        },
        {
            "title": "Architecture of EchoVLM",
            "content": "We propose EchoVLM, vision-language model specifically engineered for ultrasound clinical analysis through targeted domain specialization of the Qwen2-VL foundation model. Central to this architecture are two pre-trained modal components: the Qwen-2 language model and modified CLIP-based visual encoder, which are synergistically combined through bridging mechanism. This integration is achieved via computation-efficient Multilayer Perceptron (MLP) projection layer specifically designed to enable precise cross-modal alignment between the textual and visual representations. Rather than naıvely fine-tuning these components on ultrasound corpora, we introduce Dualpath MoE mechanism that injects domain knowledge while shielding pre-existing representations from destructive updates. For the visual encoder, any RGB ultrasound frame RHW 3 are encoded by modified CLIP encoder to token representations = generate discrete visual [v1, v2, . . . , vm] RM C. Here, = HW/142 denotes the number of visual tokens derived from the original spatial resolution . These visual tokens are subsequently transformed into the dimensional space of the Qwen-2 via MLP projector (), yielding dimensionally-mapped visual embeddings RM D. Simultaneously, textual inputs undergo parallel processing: the input prompts are first tokenized and then embedded through the word-embedding layer w(), generating the textual token sequence = [t1, t2, . . . , tN ] RN D, where represents the number of text tokens determined by the input prompt length. The Figure 3: Overview of the proposed instruction-tuning data generation pipeline for ultrasound domain knowledge. The framework systematically produces and verifies ultrasound-oriented questionanswer pairs by categorizing prompt templates into open-ended and closed-ended classes, ensuring both diversity and clinical fidelity. Usage Stage Stage II Test images reports 1.47M 208,491 1.47M 208,491 27, 3000 instruction samples - 1.8M - Table 1: Data usage per stage: Stage uses 1.47M images and 208,491 reports; Stage II adds 1.8M instruction samples; The held-out test set contains 27 577 images and 3 000 reports. concatenated representation X0 = [V ; ] R(M +N )D is subsequently fed into the LLM which comprises multiple Transformer blocks. Each layer contains multi-head selfattention (MSA), layer-norm (LN), residual pathways, and Dual-path MoE block. Its processing flow can be expressed as: = MSA(LN(Xi1)) + Xi1, = 1, . . . , (1) Xi = Dual-path MoE(LN(X i)) + i, = 1, . . . , (2) Dual-path MoE Structurally, the Dual-path MoE layer comprises two complementary sets of experts. First, static expert is instantiated by copying the original Qwen2 Feed-Forward Network (FFN) and immediately frozen; its parameters therefore act as resilient anchor that conserves generic semantic capacity. Second, battery of active experts is appended and trained. Within this group we further distinguish (1) shared expert (S) that processes every token, thereby sustaining Anatomical Metric Breast Gynecology Heart Kidney Liver Vessel Thyroid Average BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore LLaVA-OneVision (7B) 8.36 8.18 9.22 9.46 15.69 6.87 7.72 7.88 9.49 12.33 10.55 10.77 8.99 8.66 21.54 5.83 5.44 7.40 8.29 13.93 7.70 6.94 9.01 9.93 14.27 3.20 6.21 4.36 6.16 8.34 7.10 6.72 8.38 8.98 14.52 7.09 7.43 7.89 8.71 14. Qwen2-VL (7B) 8.92 12.61 9.71 8.93 19.22 6.62 9.78 8.11 8.51 13.57 6.77 9.47 7.12 5.99 19.24 5.70 7.28 7.48 7.36 14.61 6.43 8.85 8.37 8.28 14.37 3.39 6.85 4.52 4.91 7.76 7.13 11.11 8.79 8.60 18.82 6.42 9.42 7.73 7.51 15.37 LLaVA-OneVisionUltrasound (7B) 44.61 58.82 47.22 46.27 70.86 40.90 53.78 43.51 42.67 65.96 71.47 69.97 73.15 69.03 89.39 45.69 58.23 49.17 48.20 70.23 48.68 59.21 54.36 50.95 72.23 43.50 61.22 54.49 52.43 68.99 33.44 42.89 33.89 31.32 61.15 46.90 57.73 50.83 48.70 71.26 Qwen2-VLUltrasound (7B) 41.74 60.04 49.94 43.52 72.24 42.67 55.88 44.22 44.32 65.88 68.12 71.81 73.04 63.48 87.85 37.16 47.13 38.02 36.33 62.67 40.48 51.93 46.78 41.54 65.98 37.62 60.63 55.38 46.50 70.01 38.25 51.03 41.00 37.63 66.59 43.72 56.92 49.77 44.76 70.17 Table 2: Report Generation Comparison. EchoVLM(11B) 50.76 64.38 55.21 50.14 73.00 52.52 59.19 51.64 52.76 67.65 76.48 78.18 80.64 70.73 89.80 59.23 65.78 57.14 59.78 75.89 58.01 67.06 62.40 58.44 75.68 29.54 38.15 32.29 30.38 48.07 50.55 59.10 51.16 49.87 69.54 53.87 61.69 55.78 53.16 71. universal ultrasound representation, and (2) cohort of routing experts (E) that are sparsely activated via top-2 gating function conditioned on token-level features. = αF (X) + (1 α) (cid:104) λS(X) + (cid:80)k i=1 gi(X)Ei(TopK(X)) (cid:105) (3) gi(X) = efi(X) j=1 efj (X) (cid:80)k (4) Figure 4: Architectural overview of EchoVLM. Anatomical Metric Breast Gynecology Heart Kidney Liver Vessel Thyroid Average BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore LLaVA-OneVision (7B) 7.73 15.97 10.38 17.99 29.00 6.01 15.42 9.35 20.33 25.99 4.38 14.45 5.86 13.06 29.97 6.29 16.81 8.66 19.12 29.14 7.84 20.12 10.44 22.58 32.70 3.45 11.04 5.25 11.62 24.09 5.86 15.42 8.85 17.23 28.71 5.94 15.60 8.40 17.42 28.51 Qwen2-VL (7B) 10.40 19.93 13.50 21.16 31.41 9.98 21.59 14.48 27.51 29.77 5.93 17.25 7.64 15.89 31.26 8.23 19.54 10.57 20.95 29.88 10.28 23.97 12.69 24.15 32.93 5.07 13.45 7.30 13.14 26.58 9.53 21.28 13.69 24.58 32.25 8.49 19.57 11.41 21.05 30. LLaVA-OneVisionUltrasound (7B) 64.25 74.69 66.47 72.38 75.78 30.16 45.34 40.80 33.50 50.18 56.44 70.94 63.38 60.25 71.59 70.81 79.33 70.77 72.90 83.47 65.51 73.10 68.33 68.97 77.00 50.17 60.20 58.35 55.62 67.45 49.64 61.05 55.44 58.10 66.02 55.28 66.38 60.51 60.25 70.21 Qwen2-VLUltrasound (7B) 66.22 77.17 70.63 71.42 78.37 40.46 53.13 48.44 47.32 57.81 62.19 77.15 69.77 65.14 77.63 67.37 76.87 67.23 69.71 81.46 63.23 73.63 66.49 68.75 76.14 59.56 70.69 69.54 64.98 76.82 51.86 65.01 59.56 61.06 69.69 58.70 70.52 64.52 64.05 73.99 EchoVLM Anatomical Metric 71.36 80.77 76.04 74.99 81.68 48.15 62.82 58.25 55.95 66.58 69.62 81.33 74.35 72.63 79.94 77.56 83.42 74.86 80.90 87.03 74.23 79.87 75.06 78.49 82.85 36.27 47.63 44.08 40.74 53.86 62.49 71.74 67.46 70.03 76.11 62.81 72.51 67.16 67.68 75. Breast Gynecology Heart Kidney Liver Vessel Thyroid Average BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore LLaVA-OneVision (7B) 14.28 31.43 15.52 25.73 45.74 15.05 31.56 16.42 26.61 45.79 13.98 29.15 15.34 24.83 43.63 14.93 32.01 16.46 26.64 46.80 15.09 32.33 16.23 26.69 46.93 14.34 32.29 15.73 26.87 47.68 12.77 27.65 14.10 22.92 28.71 14.35 30.92 15.69 25.76 43.61 Qwen2-VL (7B) 19.77 34.15 20.03 23.65 46.09 20.77 34.40 21.36 23.36 46.93 18.05 31.91 20.12 21.16 44.06 21.42 35.37 22.13 24.26 47.76 22.19 35.00 21.57 23.72 47.70 21.48 35.50 21.38 25.71 48.20 19.08 31.68 19.84 21.79 32.25 20.39 34.00 20.92 13.38 44.71 LLaVA-OneVisionUltrasound (7B) 31.55 37.44 27.62 26.66 49.97 29.44 38.22 28.61 26.36 50.02 29.68 34.51 26.06 26.11 46.59 28.30 38.00 28.25 25.52 49.97 33.43 38.48 27.89 28.58 51.03 31.08 36.38 27.70 26.68 49.46 30.49 36.66 28.15 26.46 66.02 30.57 37.10 27.75 26.62 51. Qwen2-VLUltrasound (7B) 27.67 37.02 27.63 23.26 48.42 26.36 38.51 29.12 23.98 49.63 21.67 34.30 25.38 20.95 45.59 26.45 38.19 28.47 24.14 49.79 29.13 38.45 28.14 24.28 49.92 24.57 36.56 27.19 22.41 48.62 25.96 37.26 28.54 23.14 69.69 25.97 37.18 27.78 23.17 51.67 EchoVLM 29.12 38.07 28.23 24.41 49.31 22.10 39.17 29.59 22.81 50.66 22.02 35.17 26.05 21.86 46.56 28.28 38.76 29.13 25.34 50.23 33.97 41.16 30.32 28.10 52.51 25.88 37.69 27.75 24.12 49.50 24.29 36.95 28.20 22.65 49.09 26.52 38.14 28.47 24.18 49.69 Table 3: Comparison results on Ultrasound diagnosis. Table 4: Comparison results on VQA. In these equations, gi quantifies the contribution of expert Ei, fi denotes the routing logits, and α is learnable scalar that modulates the equilibrium between generic knowledge and ultrasound-specific information."
        },
        {
            "title": "Training Strategy",
            "content": "Stage I: Parameter-isolated pre-training. All original parameters of the Qwen2-VL are completely frozen, and only the newly-introduced MoE blocks are activated. This isolation strategy prevents catastrophic forgetting of generic multimodal knowledge, steering the expert layers to exclusively acquire domain-specific ultrasound representations. Stage II: Cooperative instruction fine-tuning. We introduce collaborative optimization mechanism by applying low-rank adaptation (LoRA:W = W0 + W, where = AB with Rdr, Rdr) (Hu et al. 2021) for lightweight parameter tuning of the base model, while maintaining full parameter updates for the MoE components. Notably, for the architecture comprising frozen FFN layers and activated MoE modules, we develop dynamic modulation parameters λ [0, 1] to balance the contributions between general world knowledge and ultrasound domain knowledge. Training Objectives. We optimize the model by minimizing compound loss that balances the auto-regressive loss with an expert-loadbalancing penalty. The vision encoder processes input images to generate visual tokens . For textual inputs, raw text undergoes tokenization and is subsequently mapped to dense embeddings using an embedding layer, yielding textual tokens . Their concatenation forms the model input = [V, ] of length = + n. During training, the model autoregressively generates response of length L; the autoregressive loss is thus defined as: Lar = log pθ(yi X, y<i) (5) i=1 where θ is trainable parameter. To ensure balanced utilization of the experts, we introduce an auxiliary loadbalancing loss. Let ge(t) [0, 1] denote the gating weight for expert and token t, where (cid:80)E e=1 ge(t) = 1. For minibatch of tokens B, we compute the dispatch ratio Fe, representing the fraction of tokens routed to expert : Fe = 1 (cid:88) tB [g(t) e] (6) where g(t) denotes that token is routed to expert e, I[] is the indicator function that equals 1 if token is routed to expert e, and 0 otherwise. The average gating probability is defined as: (cid:88) Ge = ge(t). 1 (cid:88) tB The auxiliary balancing loss is then given by: Lbal = (cid:88) e=1 FeGe. (7) (8) The overall training objective combines both components: Ltotal = Lar + γLbal (9) where γ is hyperparameter controlling the strength of the load-balancing penalty."
        },
        {
            "title": "Experiments",
            "content": "Implementation Details As presented in Table 1, the two-stage training framework utilizes distinct datasets tailored to each phase. In Stage I, we initialize the newly introduced MoE modules using dataset comprising 208,941 processed clinical reports and 1.47 million ultrasound key frames. This phase focuses exclusively on enabling the MoE components to capture domain-specific visual and textual patterns, without altering the pre-trained parameters of the base model. Subsequently, Stage II implements Cooperative Instruction Fine-tuning , during which the model is trained on 1.8 million instruction-following samples. This stage aims to enhance the models ability to understand and respond to diverse user instructions while integrating both the base model and the MoE modules in collaborative manner. The training procedure follows the protocol established by LLaVAMed, and further implementation details, including optimization settings and hyperparameter configurations, are provided in the Supplementary Material. To ensure reproducible empirical results, we evaluate the trained model on the held-out test set, which comprises 27 577 ultrasound images and 3 000 corresponding reports. During inference, all responses are generated with greedy decoding; no sampling stochasticity is introduced, guaranteeing deterministic outputs across runs. Results Report Generation. Table 2 shows EchoVLM consistently outperforms general (LLaVA-OneVision, Qwen2-VL) and ultrasound-specialized models (LLaVA-OneVisionUltrasound, Qwen2-VL-Ultrasound) across six anatomical domains (breast, gynecology, heart, kidney, liver, thyroid), achieving state-of-the-art BERTScores (e.g., heart: 89.80; kidney: 75.89). However, the model exhibits suboptimal results in vascular analysis. We hypothesize that this stems from the long-tailed distribution of the overall dataset, with vascular cases forming the smallest proportion. This scarcity likely led domain experts to prioritize dominant anatomical patterns from majority classes (e.g., breast/liver) during annotation, inadvertently marginalizing subtle vascular features. Collectively, these results demonstrate EchoVLMs robustness for prevalent structures while underscoring that vascular performance could benefit from data rebalancing strategies. Ultrasound Diagnosis. Ultrasound diagnosis represents concise yet highly specialized synthesis of the comprehensive sonographic examination findings, requiring precise integration of anatomical observations and pathological interpretations into clinical reports. To validate EchoVLMs capability in automating this critical task, we systematically evaluate its performance against established baselines across seven anatomical domains. The results in Table 3 demonstrate that EchoVLM outperforms baseline models in most anatomical categories, achieving state-of-the-art results on kidney (BLEU-1: 77.56, BERTScore: 87.03), liver (ROUGE-1: 79.87, BERTScore: 82.85), which reflects its strong semantic alignment with clinical terminology and superior diagnostic reasoning capacity. Vision Question Answering. Ultrasound VQA typically demands models with hierarchical comprehension capabilities to satisfy heterogeneous clinical requirements. Such Task Metric Report Generation Ultrasound Diagnosis VQA BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore w/o Share Expert 49.29 58.42 52.33 49.54 70.11 59.33 67.46 61.67 64.92 71.38 31.15 35.92 27.25 26.41 48.77 Share Expert 53.87 (+4.58) 61.69 (+3.27) 55.78 (+3.45) 53.16 (+3.62) 71.38 (+1.27) 62.81 (+3.48) 72.51 (+5.05) 67.16 (+5.49) 67.67 (+2.75) 75.43 (+4.05) 26.52 (-4.63) 38.14 (+2.22) 28.47 (+1.22) 24.18 (-2.23) 49.69 (+0.92) Table 5: Ablation study of the shared expert. Task(4 Experts) Report Generation Ultrasound Diagnosis VQA Metric BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore Top1 49.93 58.80 52.97 49.87 70.54 58.71 67.75 62.16 64.73 72.20 31.11 35.79 27.04 26.73 48.75 Top2 53.87 (+3.94) 61.69 (+2.89) 55.78 (+2.81) 53.16 (+3.29) 71.38 (+0.84) 62.81 (+4.1) 72.51 (+4.76) 67.16 (+5.00) 67.67 (+2.94) 75.43 (+3.23) 26.52 (-4.59) 38.14 (+2.35) 28.47 (+1.43) 24.18 (-2.55) 49.69 (+0.94) Table 6: Ablation study of Top-K routing strategies. multi-level proficiency is imperative for decision support for both radiologists and referring physicians. Hence, comparative experiments were conducted and reported in Table 4. Collectively, EchoVLM exhibits balanced performance, surpassing the baselines in 16 of 35 metric-anatomy combinations and ranking second in an additional 10. Notably, these results demonstrate the effectiveness of leveraging MoE (Mixture of Experts) architecture in achieving effective knowledge transfer to domain-specific ultrasound tasks. However, improving answer accuracy remains pivotal avenue for future research. Ablation Study Impact of the share expert. The ablation study in Table 5 demonstrates that integrating shared experts into the MoE framework improves performance for medical report generation and ultrasound diagnosis, with BLEU-1 gains of +4.58 and +3.48, ROUGE-L increases of +3.45 and +5.49, and BERTScore improvements of +1.27 and +4.05, respectively. These results indicate that shared experts enhance cross-task knowledge transfer and multimodal coherence in tasks reTask Report Generation Ultrasound Diagnosis VQA Metric BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore BLEU-1 ROUGE-1 ROUGE-L METEOR BERTScore 0 Expert 2 Experts 4 Experts 50.33 58.75 52.74 50.33 70.51 59.66 68.02 62.45 65.59 72.16 31.19 35.93 27.21 26.60 48.89 53.87 61.69 55.78 53.16 71.38 62.81 72.51 67.16 67.67 75.43 26.52 38.14 28.47 24.18 49.69 43.72 56.92 49.77 44.76 70.17 58.70 70.52 64.52 64.05 73.99 25.97 37.18 27.78 23.17 48.72 Table 7: Ablation study of expert numbers quiring complex semantic synthesis. Impact of the Top-K Routing. To investigate the impact of expert activation mechanisms within the Mixture of Experts (MoE) architecture, we implement two widely adopted routing strategies: Top-1 and Top-2 activation. As shown in Table 6, ablation studies demonstrate that Top-2 routing generally achieves superior performance in complex multimodal tasks compared to Top-1 routing, although the magnitude of improvement varies across task characteristics. For report generation tasks, Top-2 routing enhances all evaluation metrics (e.g., +3.94 absolute improvement in BLEU1 for report generation), while ultrasound diagnostic tasks exhibit significant gains in ROUGE-L (+5.00 absolute improvement). These results indicate that activating two specialized experts facilitates more effective knowledge integration and contextual understanding. Impact of the Number of Experts. The number of experts represents critical hyperparameter in EchoVLM, where prior studies have demonstrated that scaling this parameter yields performance benefits. However, its applicability within the ultrasound domain remains underexplored, necessitating systematic investigation through ablation studies. To address this gap, we conducted an ablation experiment expanding the expert count from 0 to 4, specifically to determine whether the capacity gains observed in general vision-language models could be effectively transferred to ultrasound-specific tasks characterized by distinct modality interactions and diagnostic complexity. Our findings demonstrate statistically significant improvements in high-complexity tasks, including report generation (+10.15 BLEU-1) and ultrasound diagnosis (+4.11 BLEU-1), indicating that increased model capacity enhances fine-grained cross-modal alignment and representation learning. These results suggest that the intricate spatial-temporal patterns in ultrasound imaging and the nuanced semantic dependencies in medical reporting benefit substantially from specialized expert subnetworks. Conclusion In this study, we introduced EchoVLM, the first ten-billionparameter universal ultrasound-specialized VLM. Our work establishes three pivotal contributions: (1) the curation of the largest multi-organ ultrasound dataset comprising 208,941 clinical cases and 1.47 million images spanning seven anatomical regions; (2) the development of novel fewshot prompting mechanism with expert validation that constructs robust multi-task instruction-tuning pipeline for ultrasound diagnostics; and (3) the integration of Dualpath MoE architecture with dynamic routing that significantly enhances model adaptability to ultrasounds heterogeneous imaging characteristics. Experimental validation demonstrated EchoVLMs superior performance over Qwen2-VL and LLaVA-OneVision across multiple anatomical domains. Despite these advances, limitations persist in dataset long-tail distribution and complex visual question answering requiring multi-step reasoning. Future work will address data rebalancing, expanded anatomical coverage, longitudinal patient integration, and refined routing to advance precise, efficient AI-assisted ultrasound diagnostics. References Alayrac, J.-B.; Donahue, J.; Luc, P.; Miech, A.; Barr, I.; Hasson, Y.; Lenc, K.; Mensch, A.; Millicah, K.; Reynolds, M.; Ring, R.; Rutherford, E.; Cabi, S.; Han, T.; Gong, Z.; Samangooei, S.; Monteiro, M.; Menick, J.; Borgeaud, S.; Brock, A.; Nematzadeh, A.; Sharifzadeh, S.; Binkowski, M.; Barreira, R.; Vinyals, O.; Zisserman, A.; and Simonyan, K. 2022. Flamingo: visual language model for few-shot In Proceedings of the 36th International Conlearning. ference on Neural Information Processing Systems, NIPS 22. Red Hook, NY, USA: Curran Associates Inc. ISBN 9781713871088. Bai, J.; Bai, S.; Yang, S.; Wang, S.; Tan, S.; Wang, P.; Lin, J.; Zhou, C.; and Zhou, J. 2023. Qwen-VL: Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv:2308.12966. Chen, H.; Lin, J.; Chen, X.; Fan, Y.; Jin, X.; Su, H.; Dong, J.; Fu, J.; and Shen, X. 2025. Rethinking Visual Layer Selection in Multimodal LLMs. arXiv:2504.21447. Fan, X.; Ji, T.; Jiang, C.; Li, S.; Jin, S.; Song, S.; Wang, J.; Hong, B.; Chen, L.; Zheng, G.; Zhang, M.; Huang, C.; Zheng, R.; Xi, Z.; Zhou, Y.; Dou, S.; Ye, J.; Yan, H.; Gui, T.; Zhang, Q.; Qiu, X.; Huang, X.; Wu, Z.; and Jiang, Y.-G. 2024. MouSi: Poly-Visual-Expert Vision-Language Models. arXiv:2401.17221. Guo, Q.; Mello, S. D.; Yin, H.; Byeon, W.; Cheung, K. C.; Yu, Y.; Luo, P.; and Liu, S. 2024. RegionGPT: Towards Region Understanding Vision Language Model. arXiv:2403.02330. Hong, W.; Wang, W.; Ding, M.; Yu, W.; Lv, Q.; Wang, Y.; Cheng, Y.; Huang, S.; Ji, J.; Xue, Z.; Zhao, L.; Yang, Z.; Gu, X.; Zhang, X.; Feng, G.; Yin, D.; Wang, Z.; Qi, J.; Song, X.; Zhang, P.; Liu, D.; Xu, B.; Li, J.; Dong, Y.; and Tang, J. 2024. CogVLM2: Visual Language Models for Image and Video Understanding. arXiv:2408.16500. Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; and Chen, W. 2021. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685. Kar, O. F.; Tonioni, A.; Poklukar, P.; Kulshrestha, A.; Zamir, A.; and Tombari, F. 2024. BRAVE: Broadening the visual encoding of vision-language models. arXiv:2404.07204. Li, B.; Zhang, Y.; Guo, D.; Zhang, R.; Li, F.; Zhang, H.; Zhang, K.; Zhang, P.; Li, Y.; Liu, Z.; and Li, C. 2024. LLaVA-OneVision: Easy Visual Task Transfer. arXiv:2408.03326. Li, C.; Wong, C.; Zhang, S.; Usuyama, N.; Liu, H.; Yang, J.; Naumann, T.; Poon, H.; and Gao, J. 2023a. LLaVAMed: Training Large Language-and-Vision Assistant for Biomedicine in One Day. arXiv:2306.00890. Li, J.; Li, D.; Savarese, S.; and Hoi, S. 2023b. BLIP-2: bootstrapping language-image pre-training with frozen imIn Proceedings age encoders and large language models. of the 40th International Conference on Machine Learning, ICML23. JMLR.org. Li, X.; Zheng, Y.; Chen, H.; Chen, X.; Liang, Y.; Lai, C.; Li, B.; and Xue, X. 2026. Instruction-guided fusion of multilayer visual features in Large Vision-Language Models. Pattern Recognition, 170: 111932. Lin, B.; Ye, Y.; Zhu, B.; Cui, J.; Ning, M.; Jin, P.; and Yuan, L. 2024a. Video-LLaVA: Learning United Visual Representation by Alignment Before Projection. In Al-Onaizan, Y.; Bansal, M.; and Chen, Y.-N., eds., Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 59715984. Miami, Florida, USA: Association for Computational Linguistics. Lin, J.; Yin, H.; Ping, W.; Molchanov, P.; Shoeybi, M.; and Han, S. 2024b. VILA: On Pre-training for Visual Language Models . In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2667926689. Los Alamitos, CA, USA: IEEE Computer Society. Liu, F.; Lin, K.; Li, L.; Wang, J.; Yacoob, Y.; and Wang, L. 2024a. Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning. In The Twelfth International Conference on Learning Representations. Liu, H.; Li, C.; Li, Y.; and Lee, Y. J. 2024b. Improved BaseIn 2024 IEEE/CVF lines with Visual Instruction Tuning . Conference on Computer Vision and Pattern Recognition (CVPR), 2628626296. Los Alamitos, CA, USA: IEEE Computer Society. Liu, H.; Li, C.; Wu, Q.; and Lee, Y. J. 2023. Visual Instruction Tuning. In Oh, A.; Naumann, T.; Globerson, A.; Saenko, K.; Hardt, M.; and Levine, S., eds., Advances in Neural Information Processing Systems, volume 36, 34892 34916. Curran Associates, Inc. Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; Krueger, G.; and Sutskever, I. 2021. Learning Transferable Visual Models From Natural Language Supervision. arXiv:2103.00020. Tong, S.; Liu, Z.; Zhai, Y.; Ma, Y.; LeCun, Y.; and Xie, S. 2024. Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs . In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 9568 9578. Los Alamitos, CA, USA: IEEE Computer Society. Vasu, P. K. A.; Pouransari, H.; Faghri, F.; Vemulapalli, R.; and Tuzel, O. 2024. MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training . In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 1596315974. Los Alamitos, CA, USA: IEEE Computer Society. Wang, W.; Lv, Q.; Yu, W.; Hong, W.; Qi, J.; Wang, Y.; Ji, J.; Yang, Z.; Zhao, L.; Song, X.; Xu, J.; Chen, K.; Xu, B.; Li, J.; Dong, Y.; Ding, M.; and Tang, J. 2024. CogVLM: Visual Expert for Pretrained Language Models. In Globerson, A.; Mackey, L.; Belgrave, D.; Fan, A.; Paquet, U.; Tomczak, J.; and Zhang, C., eds., Advances in Neural Information Processing Systems, volume 37, 121475121499. Curran Associates, Inc. Wang, Y.; Kordi, Y.; Mishra, S.; Liu, A.; Smith, N. A.; Khashabi, D.; and Hajishirzi, H. 2023. Self-Instruct: Aligning Language Models with Self-Generated Instructions. arXiv:2212.10560. Zhang, B.; Zhang, P.; Dong, X.; Zang, Y.; and Wang, J. 2024a. Long-CLIP: Unlocking the Long-Text Capability of CLIP. arXiv:2403.15378. Zhang, T.; Li, X.; Fei, H.; Yuan, H.; Wu, S.; Ji, S.; Loy, C. C.; and YAN, S. 2024b. OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Zhang, Y.; Zhang, R.; Gu, J.; Zhou, Y.; Lipka, N.; Yang, D.; and Sun, T. 2024c. LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding. arXiv:2306.17107."
        },
        {
            "title": "Appendix",
            "content": "Model Architecture and Training Configuration As presented in Table 8, we detail the model architecture and our proposed two-stage training framework, which integrates Dual-path MoE for incremental training to enhance domain-specific adaptation. Specifically, the routing experts within this MoE are configured with dimensionality of 1408 (Bai et al. 2023), adhering to the Qwen model specifications, while employing Top-2 selection strategy among four experts to balance specialization and efficiency; concurrently, the shared expert dimension is established at four times that of the routing experts to ensure sufficient capacity for acquiring comprehensive general ultrasound knowledge. Additionally, the PatchMerger module facilitates the fusion and compression of visual representations by consolidating adjacent 22 tokens into single token, thereby optimizing visual feature processing. During Stage I, the base model parameters are maintained in frozen state to preserve pretrained representations, with optimization strictly confined to the newly integrated Dual-path MoE components for acquiring domain-specific visual-textual patterns. In the subsequent Stage II, Cooperative Instruction Fine-tuning is implemented via Low-Rank Adaptation (LoRA), selectively upConfig Image encoder Vision-to-text projection LLM PatchMerger rate Shared expert number Shared expert dimension Routing expert number Routing expert dimension Top-k Feature select layer Image resolution LoRA rank LoRA alpha LoRA dropout Deepspeed Epoch Optimizer Weight decay Learning rate Learning rate schdule Warmup ratio Max length Batch size per GPU GPU Gradient checkpointing Precision Auxiliary loss weight γ Training parameters Total parameters Stage Stage II CLIP-ViT-L/14 MLP Qwen2-7B 4 1 5632 4 1408 2 -1 392392 - - - 8 16 0.05 Zero2 1 AdamW 0.0 1e-3 2e-5 Cosine 0.03 32768 1 H100-80G, 2A100-80G True Bf16 0.001 3.39B 3.4B 11B Table 8: Model Architecture and Training Configuration. dating the visual encoder, vision-to-text projection layer, and large language model attention layers, while concurrently enabling full-parameter updates for the active experts within the Dual-path MoE to maintain their domain-specialized adaptation capabilities. Data Generation Pipeline Implementation. The data generation pipeline integrates rigorous deduplication protocols to ensure content diversity, implementing ROUGE-L and Simhash-based deduplication during instance generation: any newly generated instance is discarded if its ROUGE-L similarity exceeds 0.7 against any prior instance or if its Simhash similarityquantified by Hamming distance threshold of 3indicates substantial overlap. For the open-ended subset, subsequent to AI prescreening, 5% of batches (with 10 samples per batch) are randomly selected for clinical validation conducted by domain experts. If errors or inappropriate samples are identified in selected batch, the generation strategy or model associated with the batchs source data is subjected to retrospective analysis and adjustment (e.g., model replacement). This targeted revision mechanism facilitates the resolution of potential issues in the generation process, thereby sustaining enhancements to the reliability of the overall dataset. some limitations exist, including false negatives in nodule identification, suggesting that there is still room for improvement. Routing Distributions In this section, we conduct comprehensive analysis of the routing distribution within the MoE framework to rigorously evaluate its efficacy and load-balancing performance. As illustrated in Figure 5, the aggregate routing frequencies across all experts exhibit remarkably balanced pattern, demonstrating that no single expert is disproportionately overor under-utilized. Delving deeper into modalityspecific routing behavior, we observe that the overwhelming majority of dispatched tokens correspond to image data, whereas textual tokens constitute markedly smaller fraction. This pronounced imbalance originates from an inherent bias in our dataset composition: although we possess 208,941 radiological reports, these are accompanied by 1.47 million key-frame ultrasound images. Consequently, each individual report is associated with multiple images, thereby skewing the token distribution toward visual content and explaining the dominant image-centric routing observed across experts. Visualization To elucidate the internal decision-making process of VLMs, we employed the Grad-CAM heatmap visualization technique proposed by Zhang et al. (?), where warmer regions (depicted in red) in Figure 6 indicate pixels exhibiting the strongest gradient flow towards the predicted token logit, signifying that the model primarily attends to these semantically significant regions during textual output generation, whereas cooler regions (in blue) receive negligible weight allocation, implying minimal contribution to the prediction; the visualized attention patterns confirm that the VLMs language reasoning is grounded in semantically relevant visual features such as anatomical landmarks, quantitative measurements, and hemodynamic bar chartshighlighted within red bounding boxesrather than spurious correlations, yet the model also exhibits attention to non-informative regions including image edges and black areashighlighted within blue bounding boxeswhich lack semantic utility, thereby highlighting persistent challenge in VLMs: the coexistence of semantic consistency (focusing on task-critical visual elements) and spurious associations (distracting attention to noise or irrelevant artifacts); future work should enhance model robustness and interpretability through strategies such as attention regularization, constrained attention routing, and explicit mitigation of spurious associations. Case Study In Figures 7-13, we present an analysis of report generation cases covering different anatomical regions. Due to space limitations, this section randomly selects examples for discussion. This random selection method ensures an objective assessment of the models capabilities while maintaining scientific rigor. The results show that our model exhibits significant advantages over both general-purpose and specialized ultrasound models, effectively capturing clinically meaningful information from images to support accurate reasoning and report generation. Despite these advantages, (a) Breast (b) Gynaecology (c) Heart (d) Kidney (e) Liver (f) Thyroid (g) Vessel (h) Overall Figure 5: Expert routing distribution analysis for different medical imaging tasks: (a) Breast, (b) Gynaecology, (c) Heart, (d) Kidney, (e) Liver, (f) Thyroid, (g) Vessel, and (h) Overall Figure 6: Visualization results. Red boxes highlight meaningful regions, while blue boxes indicate irrelevant or meaningless areas. Figure 7: Gynecological Case Study. Figure 8: Kidney Case Study. Figure 9: Liver Case Study. Figure 10: Heart Case Study. Figure 11: Vascular Case Study. Figure 12: Breast Case Study. Figure 13: Thyroid Case Study."
        }
    ],
    "affiliations": [
        "Northwestern Polytechnical University",
        "The First Affiliated Hospital of Sun Yat-Sen University"
    ]
}