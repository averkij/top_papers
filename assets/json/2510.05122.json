{
    "paper_title": "CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation",
    "authors": [
        "Jie Zhu",
        "Yuanchen Zhou",
        "Shuo Jiang",
        "Junhui Li",
        "Lifan Guo",
        "Feng Chen",
        "Chi Zhang",
        "Fang Kong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Emotional Support Conversation (ESC) plays a vital role in alleviating psychological stress and providing emotional value through dialogue. While recent studies have largely focused on data augmentation and synthetic corpus construction, they often overlook the deeper cognitive reasoning processes that underpin effective emotional support. To address this gap, we propose \\textbf{CARE}, a novel framework that strengthens reasoning in ESC without relying on large-scale synthetic data. CARE leverages the original ESC training set to guide models in generating logically coherent and supportive responses, thereby explicitly enhancing cognitive reasoning. Building on this foundation, we further employ reinforcement learning to refine and reinforce the reasoning process. Experimental results demonstrate that CARE significantly improves both the logical soundness and supportive quality of responses, advancing the development of empathetic, cognitively robust, and human-like emotional support systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 2 2 1 5 0 . 0 1 5 2 : r DianJin-CARE CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation Jie Zhu1,2, Yuanchen Zhou2, Shuo Jiang2, Junhui Li1, Lifan Guo2, Feng Chen2, Chi Zhang2, Fang Kong1 1School of Computer Science and Technology, Soochow University 2Qwen DianJin Team, Alibaba Cloud Computing https://huggingface.co/DianJin https://modelscope.cn/organization/tongyi dianjin https://github.com/aliyun/qwen-dianjin https://tongyi.aliyun.com/dianjin"
        },
        {
            "title": "Abstract",
            "content": "Emotional Support Conversation (ESC) plays vital role in alleviating psychological stress and providing emotional value through dialogue. While recent studies have largely focused on data augmentation and synthetic corpus construction, they often overlook the deeper cognitive reasoning processes that underpin effective emotional support. To address this gap, we propose CARE, novel framework that strengthens reasoning in ESC without relying on large-scale synthetic data. CARE leverages the original ESC training set to guide models in generating logically coherent and supportive responses, thereby explicitly enhancing cognitive reasoning. Building on this foundation, we further employ reinforcement learning to refine and reinforce the reasoning process. Experimental results demonstrate that CARE significantly improves both the logical soundness and supportive quality of responses, advancing the development of empathetic, cognitively robust, and human-like emotional support systems."
        },
        {
            "title": "Introduction",
            "content": "Emotional Support Conversation (ESC) is dialogue generation task in which model acts as the supporter to help help-seeker alleviate emotional distress. Effective ESC requires understanding, empathy, and the ability to provide appropriate guidance or comfort. Early ESC datasets such as ESConv Liu et al. (2021) are crowdsourced through extensive worker training and quality control to ensure high-quality conversations. Existing ESC models typically enhance performance through structured improvements. Some approaches inject commonsense knowledge to better understand the help-seekers context (e.g., MISC Tu et al. (2022), C3KG Li et al. (2022), GLHG Peng et al. (2022)), while others employ cognitive reasoning to gradually infer the help-seekers emotional or mental state (e.g., DialogueCoT Chae et al. (2023), CueCoT Wang et al. (2023)). Additionally, persona-based methods have been proposed to improve response relevance and consistency (e.g., PAL Cheng et al. (2023)). Although these methods have achieved progress, they remain limited by the datasets themselves, which typically provide only surface-level information without capturing deeper cognitive reasoning. Recent attempts have tried to overcome dataset limitations using large language models (LLMs) for dialogue augmentation, such as AugESC Zheng et al. (2023a) and ExTES Zheng Corresponding Author. Preprint Under Review 1 DianJin-CARE Figure 1: Overview of CARE framework. et al. (2023b). However, these synthetic expansions often rely on simple scenarios and template-based dialogues, limiting their ability to simulate complex social interactions. SocialSim Chen et al. (2025) further explores simulation of ESC by enriching persona information on the help-seekers side and incorporating cognitive reasoning on the supporters side, aiming to construct more comprehensive datasets that better capture social interactions. In contrast, our approach, CARE (Cognitive-reasoning Augmented Reinforcement for ESC), explicitly leverages the original ESC training set to guide multi-step cognitive reasoning and further refines this reasoning process with reinforcement learning (RL). By modeling the supporters cognitive process while maintaining logical consistency and emotional support, CARE generates responses that are both more empathetic and human-like. Our contributions can be summarized as follows: We propose CARE, framework that enhances cognitive reasoning in ESC, improving logical consistency and supportive quality. CARE demonstrates that effective reasoning can be achieved without relying on largescale synthetic corpora. Extensive experiments show that CARE outperforms strong baselines in both automatic and human evaluations, advancing the development of empathetic and cognitively robust ESC systems."
        },
        {
            "title": "2 CARE Framework",
            "content": "Unlike prior works that rely on large-scale synthetic expansion of ESC data, CARE directly builds upon the original ESConv training set. Instead of creating new dialogues, we enrich existing conversations with structured cognitive reasoning chains, thereby constructing CARE Reasoning Data. These reasoning chains guide the model to better interpret the help-seekers psychological state and generate logically consistent supportive responses. To further improve reasoning robustness, CARE incorporates RL with multi-dimensional rewards to strengthen both the reasoning process and the final response quality, as shown in Figure 1. 2 DianJin-CARE 2.1 Task Formulation We define Emotional Support Conversation (ESC) as conditional text generation task. At each dialogue turn t, the model receives the seekers utterance ut and the dialogue history Ht = {u1, r1, . . . , ut1, rt1, ut}, where ui denotes the i-th seeker utterance and ri denotes the i-th supporter response. The goal of the supporter model is to generate response rt that not only addresses the seekers needs but also provides empathetic and supportive value. In CARE, each generated response is augmented with an explicit cognitive reasoning chain. Formally, the model outputs yt = <think> Ct </think> <answer> Rt </answer>, where Ct is the reasoning chain consisting of four structured nodes and Rt is the final supporter response from the reasoning process. Ct = (cctx , ccog , cemo , cplan ), 2.2 Cognitive Reasoning Guidance Our design of cognitive reasoning is inspired by psychological theories that emphasize understanding the interplay between cognition, emotion, and behavior in providing effective support Wu et al. (2024); Beck (2020). We also draw upon the paradigm of chain-of-thought prompting Wei et al. (2022), which structures reasoning into sequential steps to facilitate more human-like decision-making. In CARE, we define four types of reasoning nodes that capture different aspects of the help-seekers psychological experience: Context Node: Captures the external situation and emotional cues expressed by the seeker, such as feeling overwhelmed by deadlines or conflicts in personal relationships. This aligns with appraisal theories of emotion, where context triggers affective responses. Cognition Node: Represents the seekers internal interpretations or beliefs about the situation, e.g., am not competent enough or People will judge me negatively. This reflects core ideas from cognitive-behavioral theory regarding maladaptive thought patterns. Emotion Node: Models the emotional consequences of those cognitions, such as anxiety, frustration, or sadness. This step grounds the reasoning in affective states, which are central to tailoring supportive responses. Support Plan Node: Determines the most suitable supportive intention and strategy, such as providing reassurance, offering perspective, or suggesting coping mechanisms. This corresponds to the process of social support provision, where helpers translate understanding into concrete assistance. By traversing these nodes in sequence, the supporter model forms structured reasoning chain that explains not only what the seeker is experiencing, but also why they feel that way and how best to respond. This ensures that generated responses are both logically grounded and psychologically informed. 2.3 Construction of CARE Reasoning Data To operationalize cognitive reasoning, we employ large model distillation. Specifically, we use the DeepSeek-R1 model to generate reasoning chains for existing ESC dialogues through carefully designed prompts. Each dialogue turn is annotated with reasoning chain that progresses through the four defined nodes, culminating in Support Plan node. To ensure logical consistency and emotional appropriateness, we discard any reasoning chain whose Support Plan node does not align with the gold strategy. This process yields high-quality reasoning-augmented dataset without the need for large-scale synthetic data expansion. 3 DianJin-CARE 2.4 Reinforcement Learning for Cognitive Reasoning To enhance reasoning quality and response consistency, we optimize CARE with RL. The model receives scalar reward r(yt) based on multiple evaluation criteria: Format Reward rfmt(yt): Ensures that the output follows the structured format with reasoning and response tags. Formally, rfmt(yt) = (cid:26) if (yt) = true 1, 0, otherwise where (yt) returns true if it matches the format of <think> . . . </think> <answer> . . . </answer>. Cognitive Coherence Reward rcog(Ct): Evaluates whether the reasoning chain contains } and whether they are in the correct order. Formally, all four nodes {cctx , ccog , cplan , cemo (cid:26) rcog(Ct) = 1 if Ct includes all four valid nodes in order 0 otherwise Support Strategy Reward rstr(cplan against the gold strategy t annotated in the dataset. Formally, ): Compares the models selected support plan node rstr(cplan ) = (cid:26) if cplan = 1, 0, otherwise The final reward integrates these signals hierarchically: (cid:26) r(yt) = 1, 0, otherwise if rfmt(yt) = 1, rcog(Ct) = 1, rstr(cplan ) = 1 This formulation ensures that only reasoning chains with correct structure, valid cognitive flow, and accurate supportive strategy receive positive reinforcement, thereby guiding the model toward producing both interpretable and effective emotional support."
        },
        {
            "title": "3 Experimentation",
            "content": "3.1 Experimental Setups Dataset. We evaluate our approach using the ESConv dataset Liu et al. (2021), high-quality benchmark of interactions between help-seekers and supporters. The training and test sets contain 910 and 195 conversations, respectively. From the training set, we extract 12,759 instances, of which 8,186 successfully generate reasoning chains with the four defined nodes and are therefore used as SFT instances. In contrast, the remaining 4,573 instances are considered hard cases and are used as RL instances. Model Training. We adopt LLaMA-3.1-8B-Instruct as the backbone and train it in two stages: supervised fine-tuning (SFT) with LoRA and RL using GRPO Shao et al. (2024). During SFT, the learning rate is set to 5 105 and training runs for 5 epochs. In the RL stage, GRPO is applied with rollouts of 6 steps, and reward normalization to stabilize training. The policy network is optimized using AdamW Loshchilov & Hutter (2019) with learning rate of 1 106. All experiments are conducted on single node equipped with 8 NVIDIA A100 GPUs. Baselines. To provide fair comparisons, we consider two additional representative datasets alongside ESConv. AugESC Zheng et al. (2023a) expands the ESC training data through LLM-based augmentation, while ExTES Zheng et al. (2023b) synthesizes extended examples to increase coverage. Both datasets are substantially larger than ESConv. For ESConv, AugESC, and ExTES, we train baseline model on each dataset individually, whereas CARE is trained solely on the ESConv data with reasoning augmentation. 4 DianJin-CARE Models ExTES AugESC ESConv CARE (SFT) CARE (SFT-RL) B-1 14.61 12.92 13.55 14.93 15.01 B-2 5.30 2.67 5.34 5.90 6. R-L METEOR BERTScore D-1 D-2 ACC Stra. 15.11 13.50 15.41 16.72 16.79 14.64 12.74 13.25 14.52 14.56 15.35 10.93 14.68 16.01 16. 3.29 1.48 4.20 4.83 4.73 20.23 7.57 24.63 28.13 27.80 - - 26.36 28.64 30.29 Table 1: Experimental results on ESC test dataset. The best results are bolded and the second-best results are underlined. Evaluation Metric. We evaluate all models using the standard ESConv test set. Evaluation is carried out using broad set of automatic metrics. BLEU-1/2 Papineni et al. (2002), ROUGE-L Lin (2004), and METEOR Banerjee & Lavie (2005) measure n-gram overlap and semantic adequacy, while BERTScore Zhang et al. (2020) captures embedding-based semantic similarity. To evaluate response diversity, we report Distinct-1 and Distinct-2 Li et al. (2016). Finally, Acc-Strategy measures the accuracy of predicted support strategies by calculating the proportion that matches the gold strategies, providing direct indicator of cognitive correctness. 3.2 Main Results Table 1 reports the experimental results on the ESConv test set. Overall, both variants of CARE outperform all baseline models across most evaluation metrics, demonstrating the effectiveness of incorporating reasoning augmentation into emotional support dialogue generation. Specifically, CARE (SFT-RL) achieves the best performance in BLEU-2, ROUGE-L, METEOR, BERTScore, and strategy accuracy, indicating that RL further enhances content relevance and strategy correctness. CARE (SFT) also surpasses the baselines in BLEU-1 and diversity scores (D-1/D-2), highlighting the benefits of reasoning even without RL. Among the baselines, ExTES shows relatively strong results on BLEU-1 and METEOR due to its larger coverage brought by synthetic data, while ESConv maintains high strategy accuracy because it is directly trained on the benchmark dataset. In contrast, AugESC lags behind in most metrics, suggesting that LLM-based data augmentation may introduce noise or reduce alignment with real emotional support scenarios. It is worth noting that strategy accuracy is not reported for ExTES and AugESC, since the former contains synthetic dialogues without explicit strategy annotations, while the latter introduces additional strategies beyond the ESConv schema, making evaluation on the standard test set infeasible. 3.3 Ablation on Cognitive Reasoning Nodes We ablate the four nodes in our cognitive reasoning chainContext, Cognition, Emotion, and Support Planby removing one node at time while keeping prompts, decoding, and data fixed. Results in Table 2 shows that the full model performs best overall, and removing all nodes substantially degrades quality (e.g., ACC Stra. drops from 30.29 to 26.32; BERTScore from 16.75 to 15.13), indicating that explicit reasoning is foundational. Support Plan is most critical for actionable guidance and diversity: dropping it yields the largest declines in D-1 (4.73 3.92) and D-2 (27.80 22.58), with ACC Stra. reduced to 29.60 and BERTScore to 15.89. The small gains in ROUGE-L (16.79 17.24) and METEOR (14.56 14.96) likely reflect more templated phrasing rather than better reasoning. Context grounding matters for relevance and coherence: without context, BERTScore falls to 16.02, diversity decreases (D-1 4.47, D-2 27.07), and ACC Stra. drops to 30.05. Cognition and Emotion provide complementary benefits for understanding and empathetic wording: if modest, declines in removing either causes consistent, BLEU/BERTScore and ACC Stra. (e.g., Emotion ablation BERTScore 16.06, ACC Stra. 5 DianJin-CARE Context Cognition Emotion Support Plan B-1 15.01 14.83 14.75 14.63 14.74 14.07 B6.03 5.98 5.83 5.83 5.91 5.45 R-L METEOR BERTScore D-1 D-2 ACC Stra. 16.79 16.78 16.78 16.73 17.24 15.04 14.56 14.25 14.23 14.16 14.96 13. 16.75 16.02 16.18 16.06 15.89 15.13 4.73 4.47 4.71 4.64 3.92 4.52 27.80 27.07 28.43 27.75 22.58 25.82 30.29 30.05 30.09 30.12 29.60 26.32 Table 2: Ablation results on different cognitive reasoning nodes. Figure 2: Human evaluation results comparing CARE with three baselines, showing CAREs win, tie, and loss percentages. 30.12). The slight rise of D-2 when removing Cognition (28.43) reflects superficial variety but not better strategies. 3.4 Human Evaluation We conduct human evaluation on 100 randomly sampled test cases with three trained annotators with PhD-level expertise in psychology. Under identical dialogue contexts, annotators compare responses from CARE and baseline, marking which is better or noting tie if they are equally good. As shown in Figure 2, CARE demonstrates clear advantages, outperforming the baselines with winning rates of 84.33% against ESConv, 91.33% against AUGESC, and 68.42% against ExTES, while exhibiting only small proportion of losses and very few ties. These results indicate that CARE consistently generates higher-quality emotional support responses. To assess the reliability of the human evaluation, we measure inter-annotator agreement using Fleiss Kappa Fleiss (1971), which is appropriate for evaluating consistency among more than two annotators. The overall agreement score among the three expert annotators is 0.6789, indicating substantial level of agreement and confirming that the evaluation results are robust and reliable. 3.5 Case Study Figure 3 presents typical ESC scenario in which the seeker has lost job, suspects that others are talking about them, relies on friends for rent, and concludes feel Im no longer needed, like an outcast. This scenario reflects prominent cognitive distortion (mind-reading/overgeneralization) coupled with low self-worth. We compare responses from baselines with our CARE model: Reference offers reassurance and redirects attention to job searching, but prematurely downplays the concern (try not to worry), providing little validation. ESConv shifts focus to secondary detail (at least you have friends), which minimizes the seekers core feeling and risks invalidation. DianJin-CARE Figure 3: Case study: CARE vs. baselines in job-loss scenario. AugESC produces generic platitudes and well-wishes, failing to engage with the distorted belief. ExTES correctly validates the emotion (feeling like an outcast is tough) yet stops at empathy without cognitive path forward. CARE explicitly challenges the distorted inference (I do not think that is true), reframes the belief with future-oriented hope, and anchors the dialogue on recovery. This behavior reflects our cognitive reasoning chain: grounding in Context, identifying the Cognition error, acknowledging Emotion, and projecting Support Plan. Overall, the case demonstrates how reinforced cognitive reasoning goes beyond surface empathy by detecting and disputing maladaptive thoughts while maintaining validation, yielding responses that are both compassionate and oriented toward positive change."
        },
        {
            "title": "4 Conclusion",
            "content": "This paper introduces CARE, cognitive-reasoning augmented framework for ESC. Unlike prior approaches that mainly rely on large-scale synthetic data, CARE explicitly strengthens reasoning by leveraging the original ESC dataset and guiding models to generate logically coherent and supportive responses. Experimental results show that CARE significantly improves both the soundness of reasoning and the quality of emotional support, while reinforcement learning further refines the reasoning process. These findings highlight the important role of cognitive reasoning in building empathetic, reliable, and human-like conversational agents. 7 DianJin-CARE"
        },
        {
            "title": "References",
            "content": "Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pp. 6572, 2005. Judith S. Beck. Cognitive Behavior Therapy: Basics and Beyond. Guilford Press, 3rd edition, 2020. ISBN 9781462544196. Hyungjoo Chae, Yongho Song, Kai Tzu iunn Ong, Taeyoon Kwon, Minjin Kim, Youngjae Yu, Dongha Lee, Dongyeop Kang, and Jinyoung Yeo. Dialogue chain-of-thought distillation for commonsense-aware conversational agents. In Proceedings of EMNLP, pp. 56065632, 2023. Zhuang Chen, Yaru Cao, Guanqun Bi, Jincenzi Wu, Jinfeng Zhou, Xiyao Xiao, Si Chen, Hongning Wang, and Minlie Huang. Socialsim: Towards socialized simulation of emotional support conversation. In Proceedings of AAAI, pp. 12741282, 2025. Jiale Cheng, Sahand Sabour, Hao Sun, Zhuang Chen, and Minlie Huang. Pal: Personaaugmented emotional support conversation generation. In Proceeings of ACL, pp. 535554, 2023. Joseph Fleiss. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5):378, 1971. Dawei Li, Yanran Li, Jiayi Zhang, Ke Li, Chen Wei, Jianwei Cui, and Bin Wang. C3KG: Chinese commonsense conversation knowledge graph. In Findings of ACL, pp. 13691383, 2022. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. diversity-promoting objective function for neural conversation models. In Proceedings of NAACL, pp. 110119, 2016. Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 7481, 2004. Siyang Liu, Chujie Zheng, Orianna Demasi, Sahand Sabour, Yu Li, Zhou Yu, Yong Jiang, and Minlie Huang. Towards emotional support dialog systems. In Proceedings of ACL, pp. 34693483, 2021. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of ICLR, 2019. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of ACL, pp. 311318, 2002. Wei Peng, Yue Hu, Luxi Xing, Yuqiang Xie, Yajing Sun, and Yunpeng Li. Control globally, understand locally: global-to-local hierarchical graph network for emotional support conversation. In Proceedings of IJCAI, pp. 43244330, 2022. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Quan Tu, Yanran Li, Jianwei Cui, Bin Wang, Ji-Rong Wen, and Rui Yan. Misc: mixed strategy-aware model integrating comet for emotional support conversation. In Proceedings of ACL, pp. 308319, 2022. Hongru Wang, Rui Wang, Fei Mi, Yang Deng, Zezhong Wang, Bin Liang, Ruifeng Xu, and Kam-Fai Wong. Cue-cot: Chain-of-thought prompting for responding to in-depth dialogue questions with llms. In Findings of ACL, pp. 1204712064, 2023. 8 DianJin-CARE Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Jincenzi Wu, Zhuang Chen, Jiawen Deng, Sahand Sabour, Helen Meng, and Minlie Huang. COKE: cognitive knowledge graph for machine theory of mind. In Proceedings of ACL, pp. 1598416007, 2024. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In Proceedings of ICLR, 2020. Chujie Zheng, Sahand Sabour, Jiaxin Wen, Zheng Zhang, and Minlie Huang. Augesc: Dialogue augmentation with large language models for emotional support conversation. In Findings of ACL, pp. 15521568, 2023a. Zhonghua Zheng, Lizi Liao, Yang Deng, and Liqiang Nie. Building emotional support chatbots in the era of llms. arXiv preprint arXiv:2308.11584, 2023b."
        }
    ],
    "affiliations": [
        "Qwen DianJin Team, Alibaba Cloud Computing",
        "School of Computer Science and Technology, Soochow University"
    ]
}