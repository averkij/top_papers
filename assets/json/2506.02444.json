{
    "paper_title": "SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios",
    "authors": [
        "Lingwei Dang",
        "Ruizhi Shao",
        "Hongwen Zhang",
        "Wei Min",
        "Yebin Liu",
        "Qingyao Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Hand-Object Interaction (HOI) generation has significant application potential. However, current 3D HOI motion generation approaches heavily rely on predefined 3D object models and lab-captured motion data, limiting generalization capabilities. Meanwhile, HOI video generation methods prioritize pixel-level visual fidelity, often sacrificing physical plausibility. Recognizing that visual appearance and motion patterns share fundamental physical laws in the real world, we propose a novel framework that combines visual priors and dynamic constraints within a synchronized diffusion process to generate the HOI video and motion simultaneously. To integrate the heterogeneous semantics, appearance, and motion features, our method implements tri-modal adaptive modulation for feature aligning, coupled with 3D full-attention for modeling inter- and intra-modal dependencies. Furthermore, we introduce a vision-aware 3D interaction diffusion model that generates explicit 3D interaction sequences directly from the synchronized diffusion outputs, then feeds them back to establish a closed-loop feedback cycle. This architecture eliminates dependencies on predefined object models or explicit pose guidance while significantly enhancing video-motion consistency. Experimental results demonstrate our method's superiority over state-of-the-art approaches in generating high-fidelity, dynamically plausible HOI sequences, with notable generalization capabilities in unseen real-world scenarios. Project page at https://github.com/Droliven/SViMo_project."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 3 4 4 4 2 0 . 6 0 5 2 : r SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios Lingwei Dang1, Ruizhi Shao2, Hongwen Zhang 3, Wei MIN 4, Yebin Liu2, Qingyao Wu1 1School of Software Engineering, South China University of Technology 2Department of Automation, Tsinghua University 3School of Artificial Intelligence, Beijing Normal University 4Shadow AI"
        },
        {
            "title": "Abstract",
            "content": "Hand-Object Interaction (HOI) generation has significant application potential. However, current 3D HOI motion generation approaches heavily rely on predefined 3D object models and lab-captured motion data, limiting generalization capabilities. Meanwhile, HOI video generation methods prioritize pixel-level visual fidelity, often sacrificing physical plausibility. Recognizing that visual appearance and motion patterns share fundamental physical laws in the real world, we propose novel framework that combines visual priors and dynamic constraints within synchronized diffusion process to generate the HOI video and motion simultaneously. To integrate the heterogeneous semantics, appearance, and motion features, our method implements tri-modal adaptive modulation for feature aligning, coupled with 3D full-attention for modeling interand intra-modal dependencies. Furthermore, we introduce vision-aware 3D interaction diffusion model that generates explicit 3D interaction sequences directly from the synchronized diffusion outputs, then feeds them back to establish closed-loop feedback cycle. This architecture eliminates dependencies on predefined object models or explicit pose guidance while significantly enhancing video-motion consistency. Experimental results demonstrate our methods superiority over state-of-the-art approaches in generating high-fidelity, dynamically plausible HOI sequences, with notable generalization capabilities in unseen real-world scenarios. Project page at https://droliven.github.io/SViMo_project/."
        },
        {
            "title": "Introduction",
            "content": "Human-object or hand-object interaction (HOI) generation serves critical applications across gaming, animation, digital human creation, and robotic action retargeting [13, 33, 49, 54]. Some studies [7, 11, 68] construct high-precision 3D interaction datasets through laboratory-based multi-view camera arrays and motion capture systems, then train diffusion-based motion generators. These objectcentric approaches typically predict parametric human/hand motions and the corresponding object pose sequences given object meshes and initial configurations. However, existing datasets [35, 67, 34] collected in laboratory environments lack diversity in object types and interaction patterns, constraining model generalization and resulting in ambiguous object boundaries, implausible or inconsistent actions (Fig. 1, left). Moreover, the reliance on precise 3D object models fundamentally limits their zero-shot generation capabilities. Equal contributions. Email: levondang@163.com, jia1saurus@gmail.com. Corresponding Authors. Email: liuyebin@mail.tsinghua.edu.cn, qyw@scut.edu.cn. Preprint. Under review. Figure 1: Different HOI generation methods. Approaches like MDM [53] rely on limited mocap data without visual guidance, resulting in blurred boundaries and compromised plausibility and consistency. Methods like Animate Anyone [18] leverage large-scale visual priors but exhibit distortions and inconsistencies because of inadequate physical awareness. Our method marries visual priors with 3D motion constraints and eliminates dependency on pre-defined object models or pose guidance. Recent advances in large video foundation models based on Diffusion Transformers (DiT) [40] (e.g., Sora [5], CogVideo [17, 65], HunyuanVideo [22]), have shown impressive capabilities in modeling physical dynamics through large-scale video training. These models can generate interaction videos with high visual fidelity end-to-end from text or reference images. However, their pixel-level generation approaches often struggle to produce accurate and coherent hand-object interactions due to limited explicit modeling of motion dynamics and physical constraints. To address this, some methods extend image-based diffusion models (e.g., SVD [3]) by adding pose-guided pipelines [62, 18, 71, 61]. These approaches combine pose conditions and appearance features to improve human-object interaction generation. While effective, they require pose sequences or externally estimated motion trajectories as inputs, preventing full end-to-end text/image-conditioned generation. Additionally, their single-frame generation leads to poor temporal coherence, causing flickering and identity inconsistencies (Fig. 1, middle). These challenges reveal longstanding methodological contradiction: motion generation systems excel at physical constraint modeling but suffer from limited data scales. In contrast, video generation models leverage massive visual priors but lack motion plausibility. We argue that this division stems from neglecting the co-evolution mechanism between visual appearance and motion patterns: they not only share the same foundation of physical dynamics but also could leverage similar diffusion processes. Based on this insight, we propose SViMo, Synchronized Video-Motion diffusion framework that enables synchronous HOI video generation and motion synthesis within unified architecture (Fig. 1, right). The core innovation lies in extending pretrained image-tovideo foundation model into the multimodal joint generation framework through the scalable DiT architecture. To better integrate the heterogeneous features of text semantics, visual appearance, and motion dynamics, we introduce the triple modality adaptive modulation to align feature scales and employ 3D full-attention mechanism to learn their synergistic and complementary dependencies. Additionally, it is difficult for video foundation models to learn explicit 3D interaction motions directly. To bridge the representation gap and reduce optimization complexity, we project 3D motions onto 2D image planes, constructing rendered motion videos as SViMos motion representation. To further enhance the video-motion consistency, we design Vision-aware 3D Interaction Diffusion model (VID). This model generates explicit 3D hand poses and object point clouds using denoised latent codes from the synchronized diffusion, which are then reinjected into the SViMo as interaction guidance and gradient constraints. Unlike methods requiring pre-specified action series, our approach integrates video synthesis and 3D interaction generation within an end-to-end denoising pipeline. This creates closed-loop feedback mechanism where motion guidances refine video generation while video latents update motion results, enabling synergistic co-evolution of both modalities. In summary, our contributions are threefold: 2 novel synchronized diffusion model for joint HOI video and motion denoising, effectively integrating large-scale visual priors with motion dynamic constraints. vision-aware 3D interaction diffusion that generates explicit 3D interaction sequences, forming closed-loop optimization pipeline and enhancing video-motion consistency. Our method generates HOI video and motion synchronously without requiring pre-defined poses or object models. Experiment results demonstrate superior visual quality, motion plausibility, and generalization capability to unseen real-world data."
        },
        {
            "title": "2 Related Work",
            "content": "3D interaction synthesis relies on high-precision motion capture datasets, some of which focus on human action conditioned on static objects [15, 69], while others simultaneously capture interactions of both human and dynamic objects [8, 35, 63, 67, 34, 50, 12]. Building upon these datasets, existing 3D interaction generation methods employ diffusion models to either introduce intermediate contact maps or milestones for modeling kinematic features [7, 11, 25, 42, 41, 68, 58, 24, 23, 26, 32, 27, 33] or leverage physical simulations to ensure physical dynamics plausibility [60, 56, 4, 59, 37]. However, due to the limited availability of 3D interaction data, the generalization capability of these approaches remains constrained. In contrast, our method leverages large-scale visual priors, operates conveniently without requiring 3D object models, and demonstrates promising generalization potential. Interaction Video Generation. The success of image generation models [16, 4648] has inspired video generation approaches. Some works [3, 62, 39, 18, 71, 61, 57] extend 2D image denoising U-Nets into video models by inserting temporal attention layers, and enhance controllability through pose guider and reference net. Other native large video models [5, 17, 65, 22, 21] directly generate videos based on 3D DiT [40] networks. While the results of these methods are visually realistic, they lack awareness of 3D physical dynamics. Contrastively, our method enhances the dynamic plausibility of generated videos by introducing dynamic information through synergistic motion denoising and 3D interaction diffusion model. Multimodal Generative Models. Driven by advancements in vision-language models [9, 30, 55, 2, 64], researchers have explored versatile generative models that align with the multimodal essence of the physical world. Some works [20, 66] cascade single-modality generators into asynchronous pipelines, yet suffer from complex workflows and noise accumulation. Others develop end-to-end multimodal joint generation frameworks that rely on massive aligned multimodal data [1, 28, 14, 72], indirect bridging mechanisms [51], or intricate hierarchical attention strategy [31] to ensure crossmodal synchronization. Differently, our method extends native large video model into synchronized video-motion generation systems, aligning heterogeneous modality features through multimodal adaptive modulation and closed-loop feedback strategies."
        },
        {
            "title": "3 Methodology",
            "content": "We define the HOI video and motion generation task as follows: Given reference image frame RHW 3 and textual prompt , generate the future video RN HW 3 and 3D motion i=1 with time steps. Here hi RJ3 and oi RK3 are the hand sequence = {(hi, oi)}N joints trajectories and object point clouds, each has and nodes respectively. The following sections detail each component of our approach. See the Appendix for training/inference pseudo-code and additional details. 3.1 Preliminary: Basic Large Video Generation Model The core of large video generation models is diffusion and its reverse denoising process [16]. The diffusion process adds Gaussian noise to video latent codes step-by-step using q(ztzt1) = (zt; αtzt1, βtI), where βt [0, 1] increases monotonically with time step t, and αt = 1 βt. Repeated application of this formula yields the marginal distribution: (cid:89) q(ztz0) = (zt; αtz0, (1 αt)I), αt = (1 βt). (1) The above distribution converges to an isotropic Normal distribution when grows large. 3 Figure 2: Our method comprises: (1) synchronous diffusion model that jointly generates HOI videos and motions (Sec. 3.3). (2) vision-aware interaction diffusion model that generates 3D hand pose trajectories and point clouds from the formers outputs (Sec. 3.4), then feeds them back into the synchronized denoising process to establish closed-loop optimization (Sec. 3.5). Correspondingly, the reverse denoising generator with trainable weights θ predicts the clean latent code ˆz0,θ, and is optimized by: Lx0-prediction = Ez,c,t,ϵN (0,I) (cid:2)z0 Gθ(zt, c, t)2 (cid:3) . (2) During the inference phase, the trained model is utilized to iteratively denoise from pure Gaussian noise step by step, following the procedure: ˆz0,θ = Gθ(zt, c, t), zt1 (cid:18) αt(1 αt1) 1 αt zt + αt1(1 αt) 1 αt ˆz0,θ, 1 αt1 1 αt (cid:19) (1 αt)I . (3) Finally, we obtain the clean denoised latent code ˆz 0, which is then decoded to the raw video space. 3.2 Framework Overview Based on the insight that visual appearance and motion dynamics share inherent physical laws of the real world, we propose an end-to-end framework that unifies HOI video and 3D motion generation by integrating visual priors with dynamic constraints. Our framework consists of two key components. The first is SViMo, which focuses on video generation (Fig. 2, top; Sec. 3.3). It extends pre-trained image-to-video foundation model into joint video-motion generation architecture. During joint denoising, SViMo dynamically aligns visual appearance with high-level motion signals, improving both visual quality and motion plausibility. Notably, the motion representations in SViMo refer not to explicit 3D hand trajectories or object point cloud sequences. Because there is significant gap between 3D motion data and 2D video representations, directly modeling explicit 3D motions would disrupt the pre-trained foundation models visual priors and degrade performance. Instead, we project explicit 3D interactions onto the 2D image plane to create rendered motion videos as intermediate representations for SViMo. The second component is VID, which generates explicit 3D motions (Fig. 2, bottom; Sec. 3.4). It maps the output of SViMo, both the generated video and the 2D rendered motion video, into target 3D interactions, i.e., hand-object trajectories and object point clouds. Additionally, the collaboration between SViMo and VID forms closed-loop feedback pipeline, ensuring consistency between generated videos and 3D motions (Sec. 3.5). 4 3.3 Synchronized Video-Motion Diffusion The SViMo learns to predict the video and motion, given the time step, text prompt, and reference imt ), (P , I), t(cid:1). The entire video-motion generation process comprises age: (ˆzV feature embedding, multimodal feature modulation and fusion, and joint denoising. 0 ) = Gθ 0 , ˆzM , zM (cid:0)(zV rh rh rn +1) R( 1 Feature Embedding. Time step is sampled from uniform distribution and then added sinusoidal position encoding, followed by simple two-layer linear mapping to obtain the time embedding et Rdtime. For the text prompt, we use frozen pre-trained language model, Google T5 [44], to extract text embedding etext RLdp , and then calculate the semantics features text with linear projection, where is the max length of textual tokens. Thirdly, for the original reference image I, to ensure certain level of robustness, we first add random noise yielding noised. Then, we compute the compressed latent code zI through 3D video VAE, zI = E(I noised) rw dVAE. Additionally, for the target video , we also map it to the latent space with the same VAE. Then the noised latent rw dVAE are obtained based on the forward diffusion process in Eq. 1. code zV To better align it with the image condition signal during the video denoising process, we repeat the image feature zI along the temporal axis, and concatenate them along the channel-axis to get video feature embedding eV R( 1 zI . Then the visual feature 2rw ]d. The is patchified through 2D stride convolution, esulting in R[( 1 2rh rendered motion video is encoded through the same VAE and diffused to obtain zM . In contrast, to get motion embedding eM , the diffused latent code here are concatenated with interaction guidance zM 0 provided by VID (Sec. 3.4), rather than being combined with zI . After that, the motion feature is obtained in the same way as that of . Multimodal Feature Modulation and Fusion. In our SViMo framework, the DiT token sequence comprises three distinct modalities: text tokens text, video tokens and motion tokens , which differ significantly in both feature spaces and numerical scales. To bridge these disparities while preserving modality-specific characteristics, we adopt triple modality adaptive modulation method that learns modulation parameters from the timestep signal et. These parameters determine the scaling, shifting, and gating operations of each modalitys features separately. Additionally, 3D full-attention mechanism is employed to capture intraand inter-modal relationships. Take the processing of text features as an example, the DiT Block proceeds as follows: rw (2dVAE) = zV rn +1) rn +1) rh B() = {αi i=1 = MLP(et), text}2 text, γi text, βi text = LN(f text) (1 + α1 text = text + γ1 text = LN(f text = text + γ2 text text text) (1 + α2 text text text) + β1 text, (cid:0)3DFA (cid:0)(f text) + β2 text, (cid:0)FFD (cid:0)(f text, )(cid:1)(cid:1) , , )(cid:1)(cid:1) , text, , same for and , (4) where 3DFA and FFD are unified multi-head 3D full-attention and feedforward layers, and denote token concatenation and segmentation along the sequence dimension, respectively. Video-Motion Joint Denoising. The network consists of stacked DiT blocks. The video and motion features output by the final DiT block are then go through an MLP to reconstruct the VAE latent codes, yielding ˆzV 0 for motion. Finally, the SViMo is optimized according to Eq. 2: 0 for video and ˆzM LSViMo = E(zV ,zM ),(P ,I),t,ϵ (cid:2)zV 0 Gθ(zV , (P , I), t)2 2 + zM 0 Gθ(zM , (P , I), t)2 2 (cid:3) . (5) 3.4 Vision-aware 3D Interaction Diffusion The VID Mϕ generates the explicit 3D hand pose trajectories ˆh0 and object point cloud sequences ˆo0 given latent codes of videos zV at any time. The framework operates as follows: First, dual-stream 3D convolutional module encodes multi-scale spatiotemporal features from both video and motion codes. Then they are fused and subsequently injected into the 3D interaction denoising modules through cross-attention mechanisms to synthesize 3D HOI trajectory sequences. Following the x0-prediction formula in Eq. 2, the model is optimized and motions (rendered motion video) zM 5 through the following loss function: (ˆh0,ϕ, ˆo0,ϕ) = Mϕ LVID = (cid:2)(ht, ot), (zV , zM (cid:104) ), t(cid:3) , (h,o),(ˆzV θ ,ˆzM θ ),t,ϵ (cid:105) MSE(h0, ˆh0,ϕ) + Dchamfer(o0, ˆo0,ϕ) (6) . 3.5 Close-loop Feedback and Training Objectives Close-loop Feedback. To enhance the mutual promotion and co-evolution between SViMo and VID, and improve the consistency between video and 3D motions, we design closed-loop feedback mechanism. This mechanism includes two pathways: interaction guidance and gradient constraint. Specifically, for the straightforward interaction guidance strategy (Eq. 7, row 1st), we first generate the 3D ), t(cid:1), interaction from the video and motion inputs of SViMo: (h0, o0) = Mno-grad then projected it onto the 2D image plane to obtain rendered motion video , which are subsequently embedded into the VAE latent space yielding zM 0 . Finally, it is concatenated with the noised motion latent code zM mentioned in Sec. 3.3, forming an additional interaction guidance for the SViMo. On the other hand, the input of VID could comes from the output of the SViMo. Therefore, the gradient of VID will be backpropagated into the SViMo during the training process, forming gradient constraint path and thereby promoting its optimization (Eq. 7, row 2nd). (cid:0)(ht, ot), (zV , zM (zV , zM ) VIDno-grad (h0, o0) Proj. VAE zM 0 Inter. Guid. to SViMo (zV , zM zM 0 ), (zV , zM zM 0 ) SViMo Gradient (ˆzV 0 , ˆzM 0 ) VID Gradient (ˆh0, ˆo0) Loss Gradient LVID. (7) Training Objectives. The training process of our method involves two phases: initially warming up the VID based on Eq. 6, followed by closed-loop training where the SViMo and the VID are jointly optimized according to Eq. 6 and Eq. 5: = ω1LSViMo + ω2LVID."
        },
        {
            "title": "4 Experiments",
            "content": "We conducted extensive experiments to validate the effectiveness of our proposed method. More information, such as additional results and limitation discussions, is provided in the Appendix. 4.1 Experimental Setup TACO Datasets [34] is large-scale bimanual hand-object interaction dataset capturing diverse tool-use behaviors via multi-view video recordings and high-fidelity 3D motion annotations. Each task is defined as triplet <tool category, action type, target object category>, describing toolmediated interactions with objects. The dataset includes 2.5k interaction sequences, covering 20 object categories, 196 3D models, 14 participants, and 15 daily interaction types. It provides allocentric (40963000) and egocentric (19201080) video streams, totaling 5.2M frames at 30 Hz. For experiments, the dataset is split into training and test sets at 1:9 ratio. To reduce computational load, we crop hand-object interaction regions, adjust their aspect ratio to 3:2, and downsample to 49 frames at 8 FPS. This results in videos with spatiotemporal resolution 41662449, aligning with CogVideoXs [65] default settings while lowering spatial resolution. Evaluation Metric. For video evaluation, we use VBench [19] to assess two key dimensions: Content Quality (including Subject Consistency and Background Consistency) and Dynamic Quality (Temporal Smoothness and Dynamic Degree). To address the partiality of individual metrics, we multiply them to derive Overall score for holistic evaluation. For 3D interaction evaluation, we separately assessed hand poses and object point cloud sequences. For the former, we calculated MPJPE (Mean Per Joint Position Error) and Motion Smoothness metrics. For object evaluation, we measured the Chamfer Distance between generated and ground-truth point clouds. Additionally, we compute comprehensive FID score via pretrained interaction autoencoder. 4.2 Implementation Details Network architecture. Our proposed model generates hand-object interaction videos with resolution [H, W, ] = [416, 624, 49] and 3D motion sequences containing = 42 hand keypoints and 6 Table 1: Comparison of video generation results. The best and second-best results are highlighted with bold and underline formatting. Method Type Training Hunyuan-13B [22] Wan-14B [21] 3D w/o Animate Anyone [18] Easy Animate [57] 2.5D CogVideoX-5B [65] Ours 3D 3D V&M w/ Content Quality Dynamic Quality Subj. Bkg. TSmoo. Dyn. Overall 0.9632 0.9576 0.9206 0.9243 0.9404 0.9500 0.9627 0.9620 0.9302 0.9358 0.9477 0. 0.9889 0.9829 0.9671 0.9657 0.9858 0.9898 0.4900 0.8476 0.9867 0.9933 0.9933 0. 0.4493 0.7675 0.8172 0.8297 0.8727 0.8785 Figure 3: Visualization of videos. Red boxes highlight artifacts such as deformation, hallucinations, distortion, and implausible motions. Figure 4: Quantitative comparison of motion generation results. Our method achieves smoother motions, sharper object point clouds, and higher instruction compliance. = 298 object nodes. During training, timesteps are uniformly sampled from [0, 1000], and the embedding dimension is dtime = 512, while only 50 steps are sampled during inference for acceleration. Text prompts (max length = 226) are embedded into features of dimension dp = 4096. The video VAE uses spatial-temporal compression ratios [rw, rh, rn] = [8, 8, 4], producing latent codes of size [52 78 13] with dVAE = 16 channels. After patchification via two-step stride convolution, video and motion tokens are compressed to 13182 tokens each. This results in total multimodal token sequence length of 226 + 13182 2 = 26590 (text + video + motion). The DiT backbone comprises 42 DiT Blocks, each with 48 attention heads, totaling 6.02B parameters. Training Details. All models are trained on 4 NVIDIA A800-80G GPUs. With memory optimization techniques including DeepSpeed ZeRO-3 [45], gradient checkpointing, and BF16 mixed-precision trick, we achieve per-GPU batch size of 4. We first warm up the VID for 5k steps, then conduct joint training with the SViMo. To enhance computational efficiency, we initially train at reduced resolution [H , ] = [240, 368] for 30k steps before fine-tuning at full resolution for 5k steps. The weights of SViMo and VID terms in the training objectives are ω1 = 1 and ω2 = 0.05. 7 (a) (b) Figure 5: User studies for generated videos (a) and motions (b). We received 1,066 and 410 valid responses respectively, and our method significantly outperforms other baselines. 4.3 Comparison with Previous Approaches Baselines. For video generation performance, we compare with video models that following the image-animation paradigm (2.5D Video Models), including Animate Anyone [18] and Easy Animate [57], as well as native 3D large video models, including Hunyuan-13B [22], Wan-14B [21], and CogVideoX-5B [65]. Particularly, due to the high training costs of the first two 3D models, we directly utilized them for zero-shot inference. For 3D motion generation quality, we compare with the classic MDM [53] and its latest improved version EMDM [70]. To ensure fair comparison, we modified their code to align the experimental setup with ours. Quantitative and Qualitative Evaluation. As shown in Table 1, our method achieves the highest overall score in video generation. Notably, individual metrics often conflict: high scores in one aspect may compromise others. For instance, Hunyuan-13B [22] attains top subject/background consistency and second-highest temporal smoothness, but its near-static outputs (dynamic degree: 0.49) yield the lowest overall score. Wan-14B [21] also exhibits the same phenomenon, and the lack of TACO dataset fine-tuning results in poor instruction adherence and hallucinations (Fig. 3, Row 1). The 2.5D image animation methods [18, 57] achieve high dynamic degree yet exhibit inadequate content consistency and temporal smoothness, visually manifesting as distortions and temporal flickering (Fig. 3, Row 2). CogVideoX-5B [65] achieves the second-highest overall score, but the generated videos still exhibit inconsistencies, as shown on the left of the last row of Fig. 3. In contrast, our method benefits from the synchronized modeling of visual and dynamic, resulting in better comprehensive performance. Table 2: Quantitative comparison against motion generation methods. The best results are in bold. For motion generation, our method achieves superior performance across all metrics, as shown in Tab. 2. Qualitative results in Fig. 4 reveal that MDM [53] and EMDM [70] produce motions with poor instruction compliance and frame consistency. This stems from two limitations: 1) They compress both reference images and text prompts into 512 dimensions through the CLIP encoder, then simply concatenate them as denoising conditions, which dilutes the instruction signal. 2) Their motion models lack vision awareness, causing large discrepancies between generated point clouds and references. Contrastly, our approach not only preserves input condition effectiveness through triple-modality adaptive modulation mechanism, but also enhances object point cloud consistency with low-level visual priors. MDM [53] EMDM [70] Ours MPJPE MSmoo. Cham. 0.7915 0.7788 0.1577 0.3382 0.3255 0.1087 0.0365 0.0306 0.0255 0.4056 0.3681 0.1050 Method Object FID Hand HOI User Study. To validate our methods effectiveness, we conducted user studies for video and motion generation  (Fig. 5)  . For video generation, 26 image-prompt pairs were used to generate videos with six models each, yielding 1,066 valid responses from 41 participants. Our method achieved 78.42% preference rate, significantly outperforming all baselines. In motion generation, 10 image-prompt pairs produced 410 valid responses, with our results surpassing the baseline in 97.56% of cases. These results demonstrate the clear advantages of our video-motion synchronous diffusion model. 4.4 Zero-shot Generalization on Real-world Data We design manipulation tasks using common household objects such as rollers, spatulas, spoons, and bowls, and collect image-prompt pairs. These data are then input into our synchronized diffusion 8 Figure 6: Zero-shot inference on real-world data. We collect data with everyday objects and generate high-fidelity, plausible HOI videos and 3D motions, demonstrating the generalizability of our method. Table 3: Ablation studies on the synchronized diffusion and the vision-aware 3D interaction diffusion. Varients Content Dynamic Subj. Bkg. TSmoo. Dyn. Overall Hand Object HOI MPJPE MSmoo. Cham. FID SViMo w/ VID (Ours) SViMo w/ Inter. Guid. SViMo w/ Grad. Cons. SViMo w/o VID VModel w/ Pred. Mot. 0.9534 0.9522 0.9499 0.9543 0.9356 0.9546 0.9546 0.9525 0.9545 0.9392 0.9883 0.9877 0.9881 0.9883 0.9858 0.9784 0.9768 0.9757 0.9686 0. 0.8800 0.8770 0.8723 0.8719 0.8381 0.0121 0.0157 0.0141 0.0195 0.0202 0.0053 0.0060 0.0058 0.0070 0.0074 0.0019 0.0022 0.0021 0.0037 0.0040 0.0100 0.0100 0.0124 0.0546 0.0575 model to generate HOI videos and 3D interactions, as shown in Fig. 6. This demonstrates that our method can be easily generalized to real-world data. 4.5 Ablation Study Effectiveness of Symbiotic Diffusion. We argue that integrating visual priors and physical dynamics into synchronized diffusion process is essential for HOI video and motion generation. To validate our synchronized diffusion mechanism: (1) We first remove VID to avoid confounding factors (SViMo w/o VID). (2) Then we decompose it into two independent components: motion generation model with only motion loss and video generation model conditioned on groundtruth motion (VModel w/ GT Mot. Guid.). After training these models independently, we use the predicted motions from the former as conditions for the latter during inference (VModel w/ Pred. Mot.). The last two rows of Tab. 3 show that modeling video and motion independently not only leads to 3.88% decrease in video overall score (0.8719 vs. 0.8381), but also results in 5.31% degradation in motion FID (0.0546 vs. 0.0575). This highlights the importance of our synchronous diffusion model in enabling feature-level synergy between video and motion. This highlights the advantage of integrating visual priors and motion dynamics for our method. Impact of Vision-aware 3D Interaction Diffusion. The vision-aware 3D interaction diffusion model forms closed-loop feedback and co-evolution mechanism with the synchronized video-motion diffusion, by injecting interaction guidance and gradient constraints into the latter. To validate its effectiveness, we conduct four variants: removing VID entirely (SViMo w/o VID), applying only gradient constraints (SViMo w/ Grad. Cons.), providing only interaction guidance (SViMo w/ Inter. Guid.), and preserving the full VID (SViMo w/ VID). Evaluation results in Tab. 3 and training loss curves in Fig. 7 show that direct interaction guidance slightly outperforms gradient constraints, while the complete VID achieves the best performance. Figure 7: Video loss curves of different variants during the training process."
        },
        {
            "title": "5 Conclusion",
            "content": "Recognizing the synergistic co-evolution between the modeling of the visual appearance and that of the motion dynamics, we propose synchronized diffusion model that simultaneously integrates hand-object interaction video generation and motion synthesis within unified diffusion process. This approach produces both visually faithful and dynamically plausible results. Moreover, we introduce vision-aware 3D interaction diffusion model that provides interaction guidance and gradient constraints to the synchronized denoising process of the above model, forming closed-loop optimization pipeline that enhances video-motion consistency. Notably, our method requires no predefined conditions and demonstrates zero-shot generalization capabilities for real-world applications. This synchronized diffusion paradigm offers promising pathway for fusing and aligning multimodal representations as well as building world models capable of understanding complex concepts. And we believe it has potential applicability across multiple domains."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This work was supported by National Natural Science Foundation of China (NSFC) No.62125107 and No.62272172."
        },
        {
            "title": "References",
            "content": "[1] Tenglong Ao. Body of her: preliminary study on end-to-end humanoid agent. arXiv preprint arXiv:2408.02879, 2024. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [4] Jona Braun, Sammy Christen, Muhammed Kocabas, Emre Aksan, and Otmar Hilliges. Physically plausible full-body hand-object interaction synthesis. In 2024 International Conference on 3D Vision (3DV), pages 464473. IEEE, 2024. [5] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. 2024. URL https://openai. com/research/video-generation-models-as-world-simulators, 3:1, 2024. [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [7] Junuk Cha, Jihyeon Kim, Jae Shin Yoon, and Seungryul Baek. Text2hoi: Text-guided 3d motion generation for hand-object interaction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15771585, 2024. [8] Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov, Ankur Handa, Jonathan Tremblay, Yashraj Narang, Karl Van Wyk, Umar Iqbal, Stan Birchfield, et al. Dexycb: benchmark for capturing hand grasping of objects. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 90449053, 2021. [9] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. [10] Lingwei Dang, Yongwei Nie, Chengjiang Long, Qing Zhang, and Guiqing Li. Diverse human motion In Proceedings of the 30th ACM prediction via gumbel-softmax sampling from an auxiliary space. international conference on multimedia, pages 51625171, 2022. [11] Christian Diller and Angela Dai. Cg-hoi: Contact-guided 3d human-object interaction generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19888 19901, 2024. 10 [12] Zicong Fan, Omid Taheri, Dimitrios Tzionas, Muhammed Kocabas, Manuel Kaufmann, Michael Black, and Otmar Hilliges. Arctic: dataset for dexterous bimanual hand-object manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1294312954, 2023. [13] Jiawei Gao, Ziqin Wang, Zeqi Xiao, Jingbo Wang, Tai Wang, Jinkun Cao, Xiaolin Hu, Si Liu, Jifeng Dai, and Jiangmiao Pang. Coohoi: Learning cooperative human-object interaction with manipulated object dynamics. Advances in Neural Information Processing Systems, 37:7974179763, 2024. [14] Yanjiang Guo, Yucheng Hu, Jianke Zhang, Yen-Jen Wang, Xiaoyu Chen, Chaochao Lu, and Jianyu Chen. Prediction with action: Visual policy learning via joint denoising process. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [15] Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito, Jimei Yang, Yi Zhou, and Michael Black. Stochastic scene-aware motion prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1137411384, 2021. [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [17] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In The Eleventh International Conference on Learning Representations, 2023. [18] Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. [19] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [20] Yujin Jeong, Wonjeong Ryoo, Seunghyun Lee, Dabin Seo, Wonmin Byeon, Sangpil Kim, and Jinkyu Kim. The power of sound (tpos): Audio reactive video generation with stable diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 78227832, 2023. [21] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. [22] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [23] Nilesh Kulkarni, Davis Rempe, Kyle Genova, Abhijit Kundu, Justin Johnson, David Fouhey, and Leonidas Guibas. Nifty: Neural object interaction fields for guided human motion synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 947957, 2024. [24] Jihyun Lee, Shunsuke Saito, Giljoo Nam, Minhyuk Sung, and Tae-Kyun Kim. Interhandgen: Two-hand interaction generation via cascaded reverse diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 527537, 2024. [25] Jiaman Li, Alexander Clegg, Roozbeh Mottaghi, Jiajun Wu, Xavier Puig, and Karen Liu. Controllable human-object interaction synthesis. In European Conference on Computer Vision, pages 5472. Springer, 2024. [26] Jiaman Li, Jiajun Wu, and Karen Liu. Object motion guided human motion synthesis. ACM Transactions on Graphics (TOG), 42(6):111, 2023. [27] Quanzhou Li, Jingbo Wang, Chen Change Loy, and Bo Dai. Task-oriented human-object interactions generation with implicit neural representations. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 30353044, 2024. [28] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, et al. Vision-language foundation models as effective robot imitators. In The Twelfth International Conference on Learning Representations, 2024. [29] Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, Chun-Le Guo, and Ming-Ming Cheng. Amt: All-pairs multi-field transforms for efficient frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 98019810, 2023. 11 [30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [31] Kai Liu, Wei Li, Lai Chen, Shengqiong Wu, Yanhao Zheng, Jiayi Ji, Fan Zhou, Rongxin Jiang, Jiebo Luo, Hao Fei, et al. Javisdit: Joint audio-video diffusion transformer with hierarchical spatio-temporal prior synchronization. arXiv preprint arXiv:2503.23377, 2025. [32] Siqi Liu, Yong-Lu Li, Zhou Fang, Xinpeng Liu, Yang You, and Cewu Lu. Primitive-based 3d human-object interaction modelling and programming. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 37113719, 2024. [33] Xueyi Liu and Li Yi. Geneoh diffusion: Towards generalizable hand-object interaction denoising via denoising diffusion. In The Twelfth International Conference on Learning Representations, 2024. [34] Yun Liu, Haolin Yang, Xu Si, Ling Liu, Zipeng Li, Yuxiang Zhang, Yebin Liu, and Li Yi. Taco: Benchmarking generalizable bimanual tool-action-object understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2174021751, 2024. [35] Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, and Li Yi. Hoi4d: 4d egocentric dataset for category-level human-object interaction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21013 21022, 2022. [36] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [37] Zhengyi Luo, Jinkun Cao, Sammy Christen, Alexander Winkler, Kris Kitani, and Weipeng Xu. Omnigrasp: Grasping diverse objects with simulated humanoids. Advances in Neural Information Processing Systems, 37:21612184, 2024. [38] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [39] Youxin Pang, Ruizhi Shao, Jiajun Zhang, Hanzhang Tu, Yun Liu, Boyao Zhou, Hongwen Zhang, and Yebin Liu. Manivideo: Generating hand-object manipulation video with dexterous and generalizable grasping. arXiv preprint arXiv:2412.16212, 2024. [40] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [41] Xiaogang Peng, Yiming Xie, Zizhao Wu, Varun Jampani, Deqing Sun, and Huaizu Jiang. Hoi-diff: Textdriven synthesis of 3d human-object interactions using diffusion models. arXiv preprint arXiv:2312.06553, 2023. [42] Huaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, and Hujun Bao. Hierarchical generation of humanobject interactions with diffusion probabilistic models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1506115073, 2023. [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [44] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [45] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. [46] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. [47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 12 [48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [49] Himanshu Gaurav Singh, Antonio Loquercio, Carmelo Sferrazza, Jane Wu, Haozhi Qi, Pieter Abbeel, and Jitendra Malik. Hand-object interaction pretraining from videos. arXiv preprint arXiv:2409.08273, 2024. [50] Omid Taheri, Nima Ghorbani, Michael Black, and Dimitrios Tzionas. Grab: dataset of whole-body human grasping of objects. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part IV 16, pages 581600. Springer, 2020. [51] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via composable diffusion. Advances in Neural Information Processing Systems, 36:1608316099, 2023. [52] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. [53] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In The Eleventh International Conference on Learning Representations, 2023. [54] Rong Wang, Wei Mao, and Hongdong Li. Deepsimho: Stable pose estimation for hand-object interaction via physics simulation. Advances in Neural Information Processing Systems, 36:7968579697, 2023. [55] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, et al. Cogvlm: Visual expert for pretrained language models. Advances in Neural Information Processing Systems, 37:121475121499, 2024. [56] Yinhuai Wang, Jing Lin, Ailing Zeng, Zhengyi Luo, Jian Zhang, and Lei Zhang. Physhoi: Physics-based imitation of dynamic human-object interaction. arXiv preprint arXiv:2312.04393, 2023. [57] Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, and Jun Huang. Easyanimate: high-performance long video generation method based on transformer architecture. arXiv preprint arXiv:2405.18991, 2024. [58] Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, and Liang-Yan Gui. Interdiff: Generating 3d human-object interactions with physics-informed diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1492814940, 2023. [59] Sirui Xu, Hung Yu Ling, Yu-Xiong Wang, and Liang-Yan Gui. Intermimic: Towards universal whole-body control for physics-based human-object interactions. 2025. [60] Sirui Xu, Yu-Xiong Wang, Liangyan Gui, et al. Interdreamer: Zero-shot text to 3d dynamic human-object interaction. Advances in Neural Information Processing Systems, 37:5285852890, 2024. [61] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14811490, 2024. [62] Ziyi Xu, Ziyao Huang, Juan Cao, Yong Zhang, Xiaodong Cun, Qing Shuai, Yuchen Wang, Linchao Bao, Jintao Li, and Fan Tang. Anchorcrafter: Animate cyberanchors saling your products via human-object interacting video generation. arXiv preprint arXiv:2411.17383, 2024. [63] Lixin Yang, Kailin Li, Xinyu Zhan, Fei Wu, Anran Xu, Liu Liu, and Cewu Lu. Oakink: largescale knowledge repository for understanding hand-object interaction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2095320962, 2022. [64] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9(1):1, 2023. [65] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations, 2025. [66] Guy Yariv, Itai Gat, Sagie Benaim, Lior Wolf, Idan Schwartz, and Yossi Adi. Diverse and aligned audio-tovideo generation via text-to-video model adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 66396647, 2024. 13 [67] Xinyu Zhan, Lixin Yang, Yifei Zhao, Kangrui Mao, Hanlin Xu, Zenan Lin, Kailin Li, and Cewu Lu. Oakink2: dataset of bimanual hands-object manipulation in complex task completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 445456, 2024. [68] Jiajun Zhang, Yuxiang Zhang, Liang An, Mengcheng Li, Hongwen Zhang, Zonghai Hu, and Yebin Liu. Manidext: Hand-object manipulation synthesis via continuous correspondence embeddings and residual-guided diffusion. arXiv preprint arXiv:2409.09300, 2024. [69] Xiaohan Zhang, Bharat Lal Bhatnagar, Sebastian Starke, Vladimir Guzov, and Gerard Pons-Moll. Couch: Towards controllable human-chair interactions. In European Conference on Computer Vision, pages 518535. Springer, 2022. [70] Wenyang Zhou, Zhiyang Dou, Zeyu Cao, Zhouyingcheng Liao, Jingbo Wang, Wenjia Wang, Yuan Liu, Taku Komura, Wenping Wang, and Lingjie Liu. Emdm: Efficient motion diffusion model for fast and high-quality motion generation. In European Conference on Computer Vision, pages 1838. Springer, 2024. [71] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In European Conference on Computer Vision, pages 145162. Springer, 2024. [72] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pages 21652183. PMLR, 2023. SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios (Appendix) In this supplementary material, we provide additional details on our methodology and experiments, and discuss the limitations, thereby enabling more comprehensive and in-depth understanding of our proposed Synchronized diffusion for video and motion generation (SViMo). Below is the outline of all contents."
        },
        {
            "title": "Contents",
            "content": "A Pseudo-code for the Training and Inference Phases More Implementation Details B.1 VBench Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Training Loss for 3D Motions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Pretrained 3D Interaction Reconstruction Model . . . . . . . . . . . . . . . . . . . Additional Experimential Results C.1 More Ablation Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Qualitative Results of Our SViMo . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Comparisons of Our SViMo and Other Methods . . . . . . . . . . . . . . . . . . . C.4 Failure Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Limitations and Future Work Broader Impacts 16 16 17 17 17 18 18 18 20 21 Pseudo-code for the Training and Inference Phases In the main text, we introduced the Synchronized Video-Motion Diffusion Gθ (SViMo, Sec. 3.3), which focuses on video generation. We also presented the Vision-aware 3D Interaction Diffusion Mϕ (VID, Sec. 3.4), designed for motion generation. Additionally, we described closed-loop feedback mechanism between these two components (Sec. 3.5). Next, we will provide more detailed description of the training (Alg. 1) and inference process (Alg. 2) using pseudocode. Algorithm 1 Joint training process of SViMo and VID. Input: Reference image I, text prompt , target video , target 3D motion (h, o), frozen video 0 = E(M ), zI = E(I) VAE encoder E, SViMo network Gθ, VID network Mϕ. Output: Optimized parameters of SViMo (θ) and VID (ϕ). 1: = Proj(h, o) 0 = E(V ), zM 2: zV 3: while not converged do 4: 5: 6: , zM (cid:0)(ht, ot), (zV 0 = E( ) U{1, , } Calculate diffused data zV (h0, o0) = Mno-grad = Proj(h0, o0), zM (ˆzV 0,θ) = Gθ(zV zI , zM (cid:16) (ˆh0,ϕ, ˆo0,ϕ) = Mϕ = ω1LSViMo + ω2LVID update parameters θ and ϕ by gradient descent zM 0,θ, ˆzM 0 , , t) (cid:17) 0,θ), , ht, ot , zM (ht, ot), (ˆzV 0,θ, ˆzM ), t(cid:1) 7: 8: 9: 10: 11: 12: end while 13: return θ = θ, ϕ = ϕ rendered motion video projection calculate latent codes sample time step following Eq. 1 following Eq. 6 direct interaction guidance following Eq. 7 inverse denoising indirect gradient constraint (Eq. 6,7) calculate loss following Eq. 5,6 15 Algorithm 2 Generation process of videos and motions. Input: Reference image I, text prompt , parameters of the noise scheduler {αt, βt}T t=1, frozen video VAE encoder and corresponding decoder D, trained SViMo network Gθ and VID network Mϕ. Output: Generated HOI video and 3D motion (h0, o0). 2: zV zI = E(I) (0, I), zM for = T, , 1 do (0, I), (hT , oT ) (0, I) calculate latent codes initialization 4: 6: (cid:0)(ht, ot), (zV 0 = E( ) (h0, o0) = Mno-grad = Proj(h0, o0), zM (ˆzV 0 ) = Gθ(zV (cid:16) (ˆh0, ˆo0) = Mϕ (cid:16) 0 , ˆzM zI , zM (ht, ot), (ˆzV , zM ), t(cid:1) zM 0 , , t) (cid:17) 0 , ˆzM 0 ), αt1(1αt) 1 αt ˆzV 0 (cid:17) following Eq. 6 direct interaction guidance following Eq. 7 inverse denoising indirect gradient constraint (Eq. 6,7) , same as µM and µh,o t1 (cid:0)µM , σ2I(cid:1), (ht1, ot1) (cid:0)µh,o, σ2I(cid:1) constant value decode into raw video 8: zV + αt(1 αt1) 1 αt (1 αt) µV = σ2 = 1 αt1 1 αt t1 (cid:0)µV , σ2I(cid:1), zM zV end for 12: = D(zV 10: 0 ) return , (h0, o0)"
        },
        {
            "title": "B More Implementation Details",
            "content": "B.1 VBench Metrics We use some of the evaluation metrics from VBench [19] for quantitative analysis of the generated videos. The details of its implementation could not be fully presented in Sec. 4.1 of the main text due to page constraints, so we provide supplement here. Subject Consistency score is used to evaluate whether subject (e.g., person or an object) remains consistent throughout video. Specifically, we first extract image features from each frame of the video using DINO [6]. Then we calculate the cosine similarity between features of consecutive frames and between each frame and the first frame to characterize subject consistency. The calculation formula is as follows: SSubj. = 1 1 (cid:88) t=2 1 2 (d1 dt + dt1 dt) , (8) where dt is the DINO feature of the tth video frame. Background Consistency. High-quality videos should not only ensure the subjects appearance remains consistent throughout the video, but also should not overlook the consistency of the background, such as scenes, rooms, and tabletops. VBench [19] computes the background consistency score by utilizing the cosine similarity of CLIP [43] image features, with the calculation process being similar to Eq. 8: SBkg. = 1 1 (cid:88) t=2 1 2 (c1 ct + ct1 ct) , (9) where ct is the CLIP feature of the tth video frame. Temporal Smoothness score is based on prior knowledge that video motion should be smooth, i.e., linear or quadratic, over very short time intervals (a few consecutive video frames). To quantify the smoothness of generated video = {v1, v2, , v2T }, VBench [19] first removes odd-numbered frames (v1, v3, , v2T 1). Then it uses video frame interpolation model [29] to generate the removed odd-numbered frame sequence (ˆv1, ˆv3, , ˆv2T 1). Next, it calculates the mean absolute error between the reconstructed frames and the original removed frames to measure whether the generated video satisfies the prior knowledge embedded in the video frame interpolation model. 16 Finally, this value is normalized to the range [0, 1]: STSmoo. ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 255 MAE(v2t1, ˆv2t1) 255 . (10) Dynamic Degree. The three metrics mentioned above, i.e., Subject Consistency, Background Consistency, and Temporal Smoothness, can individually quantify the performance of generated videos in certain specific aspects. However, completely static video may achieve very high scores in them, which is not the desired outcome. To evaluate the dynamic degree of generated videos, VBench [19] employs RAFT [52] to calculate optical flow strengths between adjacent frames. The average of the top 5% highest optical flow intensities is taken as the videos dynamic score. Once this score exceeds predefined threshold τop, the video is considered dynamic, otherwise, it is deemed static. The overall dynamic score is calculated as follows: SDyn. ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) (cid:16) n="
        },
        {
            "title": "Avg",
            "content": "(cid:16) Top5% (cid:17) t=2:T (RAFTvt1, vt) (cid:17) , > τop (11) where is the number of generated videos. B.2 Training Loss for 3D Motions We generate 3D motions using vision-aware 3D interaction diffusion model. During training, we apply separate supervision for hand and object motions. The hand loss is computed as: , + 0.2 + 0.05 Lhand = (cid:13) 2 (cid:13) (cid:13) 2 (cid:13) (cid:13)h ˆh (cid:13) (cid:13) (cid:13) 2 (cid:13)h ˆh (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) 2 (cid:13)h ˆh (cid:13) (cid:13) (cid:13) 2 where the three terms represent the hand pose and its first-order and second-order differences between frames. For the 3D object point cloud sequence R[B,N,K,3], we first split the object into tool otool R[B,N,K/2,3] and target otarget R[B,N,K/2,3], then calculate losses separately for these two components and average them to obtain the final object loss. Firstly, we directly compute the Chamfer distance between each frames generated point cloud and the ground truth to ensure consistency in shape contours. Furthermore, to enforce smoothness of the inter-frame motion, we first reshape the data into [B N, K/2, 3], then calculate pairwise Chamfer distances between consecutive frames for both ground truth and generated motion, which captures the movement dynamic in the point cloud sequence. Finally, we compute the mean absolute error (MAE) between predicted values and ground truth motion dynamics: (12) L1 = Avg {Dchamf (o, ˆo)} , L2 = Avg (cid:40) 1 1 (cid:88) n=2 Lobj = L1 + 0.1 L2, where consists of otool and otarget. MAE (Dchamf(on, on1), Dchamf(ˆon, ˆon1)) , (13) (cid:41) B.3 Pretrained 3D Interaction Reconstruction Model To assess the difference between the distribution of the generated 3D interactions and that of the ground truth motions, we trained 3D action reconstruction model on the TACO [34] dataset using the variational autoencoder architecture from Diverse Sampling [10]. During evaluation, we utilized features from the residual graph convolutional layers of this model to compute the FID score."
        },
        {
            "title": "C Additional Experimential Results",
            "content": "C.1 More Ablation Studies Our model is built upon the CogVideoX-5B [65] model and extended to video-motion joint generation model. Additional parameters include the input and output projection layers, and triple 17 modality modulation modules in all 42 DiT Blocks, resulting in total of 6.02 billion parameters. To reduce training computational consumption, we tested two model variants. First, we added motion modality modulation modules only to even-numbered DiT Blocks while retaining only the original text and video modulation modules in odd-numbered DiT Blocks, i.e., Only Even Layers. This design slightly reduced the total parameter count. Second, we applied LoRA Training, which uses low-rank decomposition of the original model parameters to significantly decrease the number of trainable parameters. Experimental results in Tab. 4 show that both variants produce lower-quality videos and motion compared to our default settings. This indicates that effective alignment and fusion of video and motion modality features are essential for generating high-quality outputs. Table 4: Ablation studies on the synchronized diffusion and the vision-aware 3D interaction diffusion. Varients Content Dynamic Subj. Bkg. TSmoo. Dyn. Overall Hand Object HOI MPJPE MSmoo. Cham. FID Ours Only Even Layers LoRA Training 0.9534 0.9535 0.9458 0.9546 0.9552 0. 0.9883 0.9884 0.9892 0.9784 0.9757 0.9688 0.8800 0.8783 0.8631 0.0121 0.0297 0.0671 0.0053 0.0103 0.0211 0.0019 0.0127 0. 0.0100 0.1368 0.4671 C.2 Qualitative Results of Our SViMo We present two additional video and motion generation results as shown in Figure 8. These results demonstrate videos and motions that are both reasonable and consistent. Considering that image frames cannot effectively showcase video effects, we provide videos in the supplementary materials for more vivid demonstration. Figure 8: Visualization of the generated videos and motions. We provide vivid demonstrations in the video of the supplementary materials. C.3 Comparisons of Our SViMo and Other Methods We present additional video generation results from three more cases, as shown in Figures 9, 10, and 11. Other baseline methods generated results exhibit certain artifacts, such as flickering, distortion, and implausible movements, which are highlighted in red. Our method demonstrates superior performance compared to theirs. C.4 Failure Cases While our approach can generate high-realistic videos and plausible motions in most cases, it occasionally exhibits certain artifacts such as penetration, low dynamics, and object inconsistency, as shown in Figure 12. However, the overall performance is still better than the baseline method. Addressing these issues, we need to explore stronger foundational models on one hand and leverage larger-scale video-motion datasets for training on the other hand. 18 Figure 9: Comparison of video generation results. The artifacts in the videos generated by the baseline methods are highlighted in red. Refer to the video in the supplementary material for vivid demonstrations. Figure 10: Comparison of video generation results. The artifacts in the videos generated by the baseline methods are highlighted in red. Refer to the video in the supplementary material for vivid demonstrations. Figure 11: Comparison of video generation results. The artifacts in the videos generated by the baseline methods are highlighted in red. Refer to the video in the supplementary material for vivid demonstrations. Figure 12: Visualization of failure cases. Red boxes highlight artifacts."
        },
        {
            "title": "D Limitations and Future Work",
            "content": "In fine-grained hand-object interaction scenarios, simultaneously generating visually high-fidelity videos and physically plausible motions remains challenging task. While our synchronized diffusion model has made significant progress in addressing this challenge, three key limitations still exist. First, our method relies on foundation model trained on large-scale video data, which is then fine-tuned on smaller dataset of video-3D motion pairs. Although the latter has relatively smaller 20 data scale, it remains essential for the expansion. Second, while our approach can generate diverse interaction motions, the produced 3D object point clouds are currently restricted to rigid, simple objects and struggle with structurally complex geometries. Third, the capabilities of the pre-trained foundation model directly impact both training efficiency and final performance. For instance, using lightweight LoRA (low-rank adaptation) strategies with the CogVideoX [65] foundation model results in suboptimal outcomes, even full-parameter finetuned model has potential blurring artifacts when sampling at reduced resolution. To address these challenges, future work should focus on three directions. First, replacing the nondifferentiable 3D trajectory representation with differentiable neural representations (e.g., NeRF-style formulations [38]) could enable video-only supervision without requiring explicit 3D annotations. This would transform our vision-aware 3D interaction diffusion model (VID) into large reconstruction model, potentially resolving the second limitation. Second, continuous following of advanced open-source foundation models is necessary, as their evolving capabilities directly affect training stability and output quality. Third, integrating visual reinforcement learning strategies [36] could further enhance generation fidelity. These improvements would collectively advance the field toward more robust and scalable hand-object interaction synthesis."
        },
        {
            "title": "E Broader Impacts",
            "content": "Our proposed synchronized video-motion diffusion model generates hand-object interaction videos alongside corresponding motion sequences, achieving both visual realism and physically plausible dynamics. This overcomes the limitations of prior approaches that prioritized only visual quality or physical accuracy in isolation. The model demonstrates generalization capabilities to real-world scenarios, making significant progress in this research domain. Moreover, the joint diffusion paradigm offers valuable insights for cross-modal information integration in multimodal generative models. Practical applications span multiple fields, including game development, digital human animation, and embodied robotics, with potential societal benefits across these domains. While acknowledging the positive impacts of this technology, we also recognize its dual-use nature. The system could be misused to generate synthetic media that violates privacy or perpetuates biases. To address these concerns, we commit to ethical development practices and transparent implementation protocols. Our research emphasizes proactive risk mitigation to ensure responsible innovation that maximizes social benefits while minimizing potential harms."
        }
    ],
    "affiliations": [
        "Department of Automation, Tsinghua University",
        "School of Artificial Intelligence, Beijing Normal University",
        "School of Software Engineering, South China University of Technology",
        "Shadow AI"
    ]
}