{
    "paper_title": "The Surprising Agreement Between Convex Optimization Theory and Learning-Rate Scheduling for Large Model Training",
    "authors": [
        "Fabian Schaipp",
        "Alexander Hägele",
        "Adrien Taylor",
        "Umut Simsekli",
        "Francis Bach"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We show that learning-rate schedules for large model training behave surprisingly similar to a performance bound from non-smooth convex optimization theory. We provide a bound for the constant schedule with linear cooldown; in particular, the practical benefit of cooldown is reflected in the bound due to the absence of logarithmic terms. Further, we show that this surprisingly close match between optimization theory and practice can be exploited for learning-rate tuning: we achieve noticeable improvements for training 124M and 210M Llama-type models by (i) extending the schedule for continued training with optimal learning-rate, and (ii) transferring the optimal learning-rate across schedules."
        },
        {
            "title": "Start",
            "content": "The Surprising Agreement Between Convex Optimization Theory and Learning-Rate Scheduling for Large Model Training Fabian Schaipp Alexander Hagele Adrien Taylor Umut Simsekli Francis Bach 5 2 0 2 1 3 ] . [ 1 5 6 9 8 1 . 1 0 5 2 : r Abstract We show that learning-rate schedules for large model training behave surprisingly similar to performance bound from non-smooth convex optimization theory. We provide bound for the constant schedule with linear cooldown; in particular, the practical benefit of cooldown is reflected in the bound due to the absence of logarithmic terms. Further, we show that this surprisingly close match between optimization theory and practice can be exploited for learning-rate tuning: we achieve noticeable improvements for training 124M and 210M Llama-type models by (i) extending the schedule for continued training with optimal learning-rate, and (ii) transferring the optimal learning-rate across schedules."
        },
        {
            "title": "Introduction",
            "content": "Large-scale machine learning requires fine-tuned training recipe. In particular, the choice of appropriate learning-rate schedules is crucial step for classical optimization methods. This usually decomposes into the choice of schedule, determining the shape of learning rates over time, and the tuning of multiplicative base learning-rate, determining the magnitude of the step sizes. Over the years, the cosine schedule (Loshchilov & Hutter, 2017) has emerged among the most commonly used schedules in large (language) model training (Brown et al., 2020; Touvron et al., 2023). The standard practice is to set the frequency of the cosine to half of the total number of training steps (Hoffmann et al., 2022); as consequence, the entire schedule depends on the length of training, which makes it unsuitable for continued training. Recently, it has been shown that the performance of cosine can be matched by an arguably much simpler schedule, that combines constant part with cooldown period in the end (Hu et al., 2024; Hagele et al., 2024). This alternative schedule is established Corresponding email: fabian.schaipp@inria.fr Inria, Departement dInformatique de lEcole Normale Superieure, PSL Research University, Paris, France EPFL, Lausanne, Switzerland (a) Real Loss Curves (b) Theoretical Bound Figure 1: Strikingly similar: Validation loss for 210M Llama model trained with AdamW (left) and the theoretical suboptimality bound (6) from convex optimization (right). Both plots show wsd and cosine schedule with different training lengths , and with base learning-rate of cosine being twice as large as for wsd. under the name wsd (warmup-stable-decay). One distinguishing feature of wsd is the drastic decrease of the loss shortly after initiating the cooldown. However, the recent advancements in learning-rate scheduling have emerged almost exclusively from empirical rather than from theoretical considerations (Loshchilov & Hutter, 2017; Goyal et al., 2017; Hoffmann et al., 2022; Hagele et al., 2024). We do not yet have fundamental understanding that could explain the features of the above-mentioned schedules and why they perform better or worse at given task, restraining the tuning procedure to trial-and-error approach. Summary and contributions. In this paper, we show that several empirical findings on scheduling can be reproduced with suboptimality bound for SGD on convex problems that was introduced by Defazio et al. (2023). Among others, this theory allows to reproduce (i) the matching performance of cosine and wsd; (ii) the necessity and optimal length of the cooldown period in the end of training (see Section 4). In second step, we take the reverse direction and show how the theoretical bound can be exploited in practice (see Section 5): for continued training of 124M and 210M Llamastyle model, using the theoretically optimal schedule notably improves performance compared to continuing with the same learning rate; moreover, it allows to transfer the optimal learning-rate across schedules (Figs. 10 and 12). This leads us to the perhaps surprising conclusion that the empirical behavior of learning-rate schedules in (non-convex) deep learning can be described precisely with theoretical bound from non-smooth stochastic convex optimization. particular focus of this paper is put on the wsd schedule: we derive convergence result for this schedule (without warmup) in the non-smooth stochastic convex setting, see Section 3.2. Most importantly, the cooldown period of wsd leads to vanishing log-terms in the bound, which provides an explanation of the benefit of cooldown observed in practice. 2 Second, we show that the sudden drop of the loss during cooldown can be observed in upper and lower bounds of the suboptimality, as well as for non-smooth convex toy problem. Code for all experiments is available at https://github.com/fabian-sp/ lr-scheduling. Setup. We consider the training problem (x), (x) := Es[f (x, s)]. (1) min Rd Rd are the learnable parameters of machine learning model, and In the above, is loss function. The expectation is taken over the distribution of random variable , s) has (typically the training set). We assume that ( that maps to the space or set suitable subdifferential for every (for example, see Rockafellar (1970); Clarke (1983); Bolte & Pauwels (2021)). We denote elements of the subdifferential as (x, s). We study the iterates of SGD with learning-rate schedule, given by γηtgt, xt+1 = xt gt Here, γ > 0 is called the base learning-rate and ηt > 0 is called the schedule. While it might seem redundant to separate γ from (ηt), this reflects the standard practice in deep learning libraries such as Pytorch. Most importantly, for different schedules (constant, cosine, wsd,. . . ), the optimal value of γ is in general different. (xt, st), (2) N. We remark that the most commonly used optimizer for training in practice is Adam(W) (Kingma & Ba, 2015; Loshchilov & Hutter, 2019), and all empirical results we present or refer to in this paper are obtained with Adam(W). However, the theoretical results apply to SGD; we address this limitation in detail in Section 6. Cosine and wsd schedule. We now formally introduce the two running examples cosine and wsd. Without warmup, the wsd schedule is constant up to iteration T0 , then decays linearly to zero. Formally, we have ηt = 1 1 (cid:40) T0 + T0 1 T0 < T0, + 1. (3) 2(1 + cos( The cosine schedule is given by ηt = 1 + 1. Note that for both schedules we have ηT +1 = 0 (we choose + 1 in order to ensure that ηt > 0 for ). We remark that it is also common to decay the cosine to factor of 0.1 of the peak learning-rate instead of 0. 1 π)) for 1 Notation and naming convention. We will use wsd in the paper as it is the most established abbreviation in the literature; however, similar to Zhai et al. (2022); Hagele et al. (2024), we will refer to the phase where the schedule decays to zero as cooldown instead of decay, in order to avoid confusion with other terminology (e.g., weight decay). , Unless explicitly stated otherwise, denote the standard Euclidean norm and its scalar product. and 1In case the reader is uncomfortable with the notion of subdifferentials, the entire article can be read with gt being the gradient (xt, st) instead."
        },
        {
            "title": "2 Related Work",
            "content": "Learning-rate schedules. The cosine schedule (Loshchilov & Hutter, 2017) can be considered the de-facto default in large-scale deep learning. Convergence results for SGD with cosine schedule have been shown by Li et al. (2021). Recently, the wsd schedule (short for warmup-stable-decay, also called trapezoidal schedule) has been proposed as an alternative (Zhai et al., 2022; Hu et al., 2024; Hagele et al., 2024). Hagele et al. (2024) show that wsd matches the performance of cosine on LLM pretraining, while largely reducing the compute needed for scaling-law experiments, as the constant part of the schedule can be reused. Last-iterate convergence. We will see that it is crucial to use bound for the lastiterate in order to closely match empirical loss curves. This is in contrast to many standard convergence results that prove an upper bound on the quantity mint=1,...,T E[f (xt) (x)]. Due to convexity and Jensens inequality, the same bound usually holds for E[f (xT ) (x)], where xT is some (weighted) average over x1, . . . , xT } . Last-iterate results, that is, bounds on E[f (xT ) (x)], are less standard: convergence of SGD has been proven for constant step sizes (Zhang, 2004), and for decreasing step sizes in bounded domains (Shamir & Zhang, 2013). Other results are restricted to specific choice of schedule (Jain et al., 2021; Zamani & Glineur, 2023). The backbone of this article will be result from Defazio et al. (2023), which proves last-iterate bound for general schedules; compared to previous work (Orabona, 2020) it has the advantage that the bound remains meaningful if the last step size ηT is very small. { Understanding cooldown. For the wsd schedule, one can consistently observe sudden drop in train/validation loss after the start of the cooldown phase (Hagele et al., 2024). Hu et al. (2024) find that the curvature of the loss increases during cooldown; Hagele et al. (2024) expand this and conclude that the cooldown phase is smooth transition to basin in the loss landscape. More recently, Wen et al. (2024) hypothesize that the sudden drop is caused by river-valley loss landscape, that arises from heterogeneity in the stochasticity of different tokens. In this work, we will offer an additional (and potentially much simpler) model: the drop of the loss can be observed in upper and lower bounds of the suboptimality, based on first-order convex optimization theory. In particular, this phenomenon happens for toy instance of ℓ -norm regression."
        },
        {
            "title": "3 Convergence Results",
            "content": "Let us assume convexity of the objective and recall the definition of the iterates. is convex, that is, (A1) Assume that for each , s) : Rd the function ( (y, s) (x, s) g, , for all (x, s) and x, Rd. (A2) Let γ > 0 and ηt > 0. For N, consider the iterates xt+1 = xt γηtgt, gt (xt, st). 4 (4) (5) Rd be an arbitrary point of interest, for example the (local) minimum of that Let is closest to x1. We do not make any other assumption on for now. Theorem 1 (cf. Thm. 10 from Defazio et al. (2023)). Let (xt) be the iterates given by . (A2), with ηt > 0 for = 1, . . . , and γ > 0. Let x Under (A1), for any Rd and define := it holds x1 E[f (xT ) (x)] 2γ 1 t=1 ηt D2 + γ2 η2 2 gt + γ 2 1 (cid:80) (cid:88)k=1 (cid:2) ηk t=k+1 ηt t=1 (cid:88) 1 t=k ηt (cid:16) (cid:3) 2 gt η2 (cid:88)t=k (6) . (cid:17) (cid:80) (cid:80) The above result is essentially the same as (Defazio et al., 2023, Thm. 10); the only difference is that we explicitly separate γ and (ηt) which will be convenient subsequently. We refer to Appendix for technical remarks and proof. Our next goal is to compute the base learning-rate γ, given schedule ηt, that minimizes the bound in (6). We can do so by assuming bound on the expected gradient norms: (A3) Assume that there exists (Gt)t > 0 such that gt 2 G2 for all . Remark 1. In general, the choice of γ will affect the iterates (xt) and therefore the gradient norm bounds (Gt). Thus, the following Corollary can be applied only if we apply the same bound Gt independent of γ. This is the case for the standard assumption of , s) being Lipschitz with constant > 0; in that case, choose Gt = for all ( N. To minimize with respect to γ, it is convenient to define the quantities T1(η1:T , D, ) := T2(η1:T , G1:T , ) := 1 t=1 ηt 1 (cid:80) t=1 ηt 2 2 D2, t G2 η2 + t=1 (cid:88) (cid:0) (cid:1) where we denote η1:T := (η1, . . . , ηT ) and G1:T := (G1, . . . , GT ). (cid:80) 1 2 1 (cid:88)k= ηk t=k+1 ηt (cid:16) 1 t=k ηt G2 η2 , (cid:88)t=k (cid:17) (cid:80) (cid:80) Corollary 2. In the setting of Theorem 1, under (A3), for any it holds E[f (xT ) (x)] T1(η1:T , D, ) γ + γ T2(η1:T , G1:T , ). For given (Gt), and , minimizing the right-hand side of (8) with respect to γ > 0 gives the solution γ = 2(η1:T ,G1:T ,T ) . Plugging γ back into (8), we have E[f (xT ) 1(η1:T ,D,T ) f (x)] 2 (cid:112) (cid:113) T2(η1:T , G1:T , ). T1(η1:T , D, ) 5 (7) (8) Figure 2: Learning-rate schedule (left) and theoretical bound (right) for cosine and wsd, and various , with base learning-rate γ. Next, we plug in the cosine and wsd schedule into the bound from Theorem 1. Applying2 Corollary 2 with t, we get Ωt such that E[f (xt) (x)] Ωt for all = 1, . . . , T, where we define Ωt := T1(η1:t, D, t) γ + γ T2(η1:t, G1:t, t), with T1, T2 from (7). (9)"
        },
        {
            "title": "3.1 Comparison of cosine and wsd",
            "content": "N, we define both schedules (ηt)1 For training horizon +1 such that they reach ηT +1 = 0. For formal definition of wsd and cosine see (3) and thereafter. For each different training horizon , and for both schedulers, we pick the optimal base learningrate γ given by Corollary 2 and plot the bound Ωt in Fig. 2. We plot sweep of γ in Fig. 3a (with Gt = = 1 for all N). Perhaps surprisingly, the shape of the theoretical bound Ωt (for the convex case) matches closely the empirical loss curves of (the non-convex problem of) language model pretraining in Hagele et al. (2024); see Fig. 1 for side-by-side comparison. This is especially visible in the sudden drop of the loss for the wsd schedule during cooldown. However, using the last-iterate result is crucial for this: we demonstrate this with an ablation study that uses standard bound on the minimum suboptimality instead; there, the theoretical bound does not resemble empirical loss curves (cf. Appendix A). Takeaway: The last-iterate bound in Corollary 2 matches the shape of the loss curves in Hagele et al. (2024). In particular, the sudden drop for wsd during cooldown can be observed. 2For Corollary 2 we require ηt > 0 for = 1, . . . , , which is why we construct the schedule such that ηT +1 = 0 instead of ηT = 0. 6 (a) Learning-rate sweep (b) Optimal base learning-rate vs. Figure 3: Optimal base learning-rate decays with inverse square-root of training horizon (right). For cosine, it is roughly twice as large as for wsd (as 0.92/0.47 2). The dashed curve in the right-hand side plot is obtained with least-squares fit. Takeaway: The optimal base learning-rate from Corollary 2 scales 1/T with the training horizon, and is roughly twice as large for cosine as for wsd (cf. Fig. 3b). This matches empirical observations (cf. Fig. 4 in Shen et al. (2024) and Fig. 3 in Hagele et al. (2024))."
        },
        {
            "title": "3.2 Bound for wsd Schedule",
            "content": "We now derive the bound in Corollary 2 for (ηt) being the wsd schedule. To the best of our knowledge, this schedule has not been analyzed theoretically before. For this section, N. useful notation will be the harmonic number assume that Gt = > 0 for all N, and H0 := 0. We recall that Ht behaves like ln(t) 1 for Ht, defined as Ht := in the limit. As baseline, we first compute the bound for the constant schedule. k=1 (cid:80) Constant schedule. D2 2T , written as T2(η1:T , G1:T , ) = G2 If ηt = 1 for all 2 [1 + HT N, it is easy to compute T1(η1:T , D, ) = 1]. Therefore, the bound from Corollary 2 can be E[f (xT ) (x)] 2 T1(η1:T , D, ) T2(η1:T , G1:T , ) = (cid:112) DG (cid:112) 1 + HT 1. The wsd schedule. We will now compute suboptimality bound for the wsd schedule (without warmup). After stating the theorem, we show that if higher-order terms are ignored, the improvement of wsd over constant schedule is essentially due to the absence of the logarithmic term HT 1. Theorem 3. Let 1 T0 T0. Further, assume that (A3) holds with Gt = for some > 0 for all T0 < . Assume that ηt = 1 for < T0 and ηt = T0 +1 for N. 7 Then, for γ = γ from Corollary 2, we have E[f (xT ) (x)] DG 4 + T0 (cid:114) Λ1 + Λ2 (cid:2) Λ3 + Λ4 . (cid:3) where Λ1 := 2 Λ3 := (T 3(T 3 + +2T0 3(T +T0) , 1) T0)(T0 T0+2)(T +T0) , Λ4 := HT T0+1, 2 T0)2 + HT T01 T0+1 . 1 (T Λ2 := HT +T0 Looking at the bound in Theorem 3 for large , we have HT o(T T0 + 1) and thus Λ4 = o(1) with length is proportional to , that is, T0 = βT for β Λ3 3(1+β) . Ignoring Λ4, we get β)βT 2 β)(1+β)T 2 = β + 3(1 (1 T0 1 = 1)) = . Now, we assume that the cooldown (0, 1). Again, for large we obtain T0 (ln(T E[f (xT ) (x)] DG 4 1 + β (cid:114) (cid:2) 2 3 + 1+2β 3(1+β) β 3(1+β) + Λ3 . Using Lemma 7, we can estimate H(1+β)T β)T ). This yields Λ3 = H(1+β)T 2 β)T +1 In total, the term in the square-root does not contain logarithmic terms in . This is the main difference to the constant schedule, where the term in the square-root is 1 + ln(T ). See Fig. 20 for visualization. We defer additional remarks and 1 + HT the proof of Theorem 3 to Appendix F.1. 1 + ln((1 + β)T ) and H(1 β ). 1 + ln( 1+β ln((1 1 2 H(1 1 (cid:3) β)T +1 Takeaway: The wsd schedule improves over the constant schedule by logarithmic term. This improvement in the bound happens during the cooldown period (cf. Figs. 2 and 20)."
        },
        {
            "title": "4 Theoretical Simulations",
            "content": "In the following, we simulate the bound from Theorem 1 in order to analyze its dependence on the cooldown length for wsd, and on the gradient norm magnitude. Additional experiments (e.g., on the cosine cycle length, and comparison of classical schedules) and supplementary information are deferred to Appendix B. Setup. Unless explicitly mentioned, we set Gt = 1 for all theoretical simulation. We do not use warmup for neither schedule. and = 1 for the entire"
        },
        {
            "title": "4.1 Cooldown Length",
            "content": "Previously, we have set T0 = 0.8 fraction, defined as to linear-decay schedule similar to Defazio et al. (2023, Corollary 2)). In Figs. 4 and 5, we vary the cooldown . Specifically, we vary from T0 = to T0 = 1 (constant schedule for wsd. T0 8 (a) Learning-rate sweep (b) Final bound vs. cooldown fraction Figure 4: (Left) Optimal base learning-rate increases with cooldown fraction. (Right) For fixed γ, the optimal cooldown fraction can be smaller than 1. The analogous curves for real experiments with similar parabola shapes are in Fig. 21. Figure 5: Schedule (left) and theoretical convergence (right) for varying cooldown fraction. With optimal base learning-rate γ, starting the cooldown at T0 = 1 is optimal. Fig. 21 shows the analogous plot for real experiments with the same behavior. Takeaway: The theoretical simulation suggests that if the base learning-rate γ is fully tuned, then the optimal cooldown fraction is 1 (linear decay). For fixed γ, the optimal cooldown fraction can be smaller than one. The first observation is in line with empirical observations from Defazio et al. (2023) that compares many different schedules across several machine learning tasks, and find that the linear-decay schedule performs best on average. Further, it is known that the lineardecay schedule matches the exact lower-bound convergence bound for the (stochastic) convex, Lipschitz case (Defazio et al., 2023; Zamani & Glineur, 2023); see Appendix F.1 for detailed comments. The second observation matches the finding of Hagele et al. (2024): in Appendix B.6, Fig. 21 we show the analogous figure on real training data (also see Fig. 5 in (Hagele et al., 2024)). We obtain the same parabola shape for small base 9 learning-rates. However, for large enough base learning-rate, the parabola turns into monotonically decreasing curve."
        },
        {
            "title": "4.2 Gradient Norm",
            "content": "We now analyze how the bound of the expected gradient norms G1:T influences the shape of Ωt. In this section only, we assume that Gt = tα, α 0, { 0.5, . 1 } We sweep the base learning-rate γ by computing the minimal ΩT from (9) for the above choice of G1:T = (G1, . . . , GT ). We set = 400, and the cooldown fraction to 0.2 for wsd. Fig. 6 shows that the sudden drop in loss for wsd is only visible if Gt does not go to zero as . Figure 6: Assumed gradient shape (left) and theoretical converegnce (right). Only with α = 0 (constant Gt), the sudden drop for wsd is clearly visible. Takeaway: The sudden drop during cooldown is most pronounced if the gradient norms do not go to zero. Interestingly, if the gradient norms go to zero, the wsd schedule also obtains significantly better bound than cosine. So far we have observed that non-vanishing gradient norms lead to the characteristic drop in the upper bound Ωt. Next, we show that the same phenomenon can be observed for (i) suboptimality lower bound and (ii) for the actual loss of the iterates of SGD on simple non-smooth convex problem."
        },
        {
            "title": "4.3 Lower Bounds and Convexity\nIn all previous sections we analyzed an upper bound Ωt of E[f (xt)\nf (x⋆)]. How tight\nis this upper bound? To answer this, we compute lower bounds of E[f (xt)\nf (x⋆)] using\nthe PEP framework: for a given function class and algorithm, a worst-case example can\nbe constructed by solving a semidefinite program (Drori & Teboulle, 2014; Taylor et al.,\n2017a,b; Goujaud et al., 2024). Additional details are provided in Appendix B.4.",
            "content": "10 (a) wsd with cooldown fraction 0.2 (b) cosine Figure 7: PEP lower bound matches the upper bound Ωt closely in shape. Takeaway: The sudden drop during cooldown appears as well for the PEP lower bound  (Fig. 7)  . The worst-case suboptimality value at = 60 is very similar for cosine and wsd. In Section 4.2, we have shown that non-vanishing gradient norm are characteristic for the sudden drop (of the upper bound Ωt) during cooldown of wsd. We observe the same behavior for the actual loss when running gradient descent with the wsd schedule for the 2-dimensional, convex, non-smooth problem minx . The experimental details and plots are deferred to Appendix B.1. Ax b Takeaway: The sudden drop of the loss for wsd is not specific to model architecture or even to non-convexity. It can be observed when minimizing simple non-smooth, convex objective function (cf. Fig. 17)."
        },
        {
            "title": "5 Applications",
            "content": "So far, we have shown that the bound from Theorem 1 matches very closely empirical loss curves. However, the bound in Theorem 1 contains quantities that are unknown in practice, such as the gradient norm bounds Gt and D. Thus, the question arises how to convert the theoretical result into practical applications. The following two scenarios demonstrate that using the optimal schedule and base learning-rate predicted from theory improves pretraining of 124/210M Llama-style transformer (Vaswani et al., 2017; Touvron et al., 2023)."
        },
        {
            "title": "5.1 Schedule Construction for Continued Training",
            "content": "The first application is to construct learning-rate schedules for longer horizons: for example, assume we have trained model for T1 steps, but later want to continue training up to T2 > T1 steps. The main benefit of the wsd schedule is that the training steps up to the cooldown phase can be reused, thus reducing the amount of additional compute required 11 for continual training (Hagele et al., 2024). This is not true neither for the linear-decay nor for the cosine schedule, as the value of the schedule in each step depends on the total number of steps. Assume we have tuned the base learning-rate γ of wsd for the short run T1. We have seen in Fig. 3b that γ decreases with ; thus, continuing training at γ until T2 would use suboptimal learning rate. We present two options to resolve this: (B1) We have seen that γ increases with the cooldown fraction (Section 4.1). We can increase the cooldown fraction c1 for the long training run to T2, to compensate for the decrease in γ due to T1 T2. (B2) Alternatively, we can keep the same cooldown fraction, but decrease the learningrate in the steps from T1 to T2: assume piecewise constant schedule with ηt = 1 for up to the start of cooldown of the short run, and ηt = ρ for up to the start of cooldown of the long run. How do we need to set ρ, such that γ remains the optimal base-learning rate for this schedule? Simulation. We simulate both options in Figs. 8 and 9. Here, we set T1 = 4000 and T2 ranging from 1.5T1 to 4T1. We construct the extended schedule by sweeping c1 for (B1) and ρ for (B2), and picking the value where the optimal base learning-rate according to Corollary 2 is closest to γ. The cooldown phase of the short run is set to 20%. Specifically, our analysis suggests to decrease the schedule by ρ = 0.525 for T2 = 2T1 and by ρ = 0.375 for T2 = 4T1 (see Fig. 22, left). We verified that changing the values of G, D, or T1 do not affect the result (plots not shown); however the values might be different for other cooldown fractions than 20%. For (B2)  (Fig. 8)  , we conclude that by decreasing the schedule by the correct factor ρ, we can reuse the entire constant schedule from the short run, while obtaining bound Ωt close to the (theoretically worst-case optimal) bound for tuned linear-decay schedule. Importantly, keeping the same base learning-rate for the entire long run would result in significantly worse bound Ωt. For (B1)  (Fig. 9)  , the required increase in cooldown fraction is large, and hence for long extensions, only small parts of training can be reused. When doubling the training length (T2 = 2T1), the adapted cooldown fraction is roughly c1 = 0.6. As an alternative, one could use the 1/sqrt schedule, defined by ηt := 1/t, combined with cooldown (Zhai et al., 2022). Fig. 23 shows that for 1/sqrt the cooldown fraction can roughly stay the same, which however comes at the cost of larger gap to linear-decay. From theoretical and practical perspective, we conclude that the approach (B2) is preferable, as it allows to reuse the entire short run with no drawbacks in terms of the bound, and in similar fashion as before allows iteratively continuing from the newly obtained checkpoints of constant learning-rate phase. Experiments. Based on the above, we extend the training of 124M and 210M Llamastyle transformer (Touvron et al., 2023) on the SlimPajama dataset (Soboleva et al., 2023). For details on model, dataset and training procedure see Appendix B.5. We set T1 = 50k 0.001. As baseline, we use and T2 { ; sweep over 50k steps gives γ 100k, 200k } 50k 12 Figure 8: Transfering the learning-rate schedule from horizon T1 = 4000 to T2 [1.5T1, 4T1] (see also Fig. 22, left). Decreasing the learning rate (green) after the short run (at iteration 3200) leads to significant better bound Ωt as keeping it constant (grey). Dashed horizontal lines (blue) mark bounds for linear-decay schedule with tuned γ. wsd schedule that continues with the same γ over the extended training length. For the adapted schedule from (B2), we decrease the learning-rate after 40k steps linearly over 1000 steps (e.g., from 1e-3 to 5.25e-4) as precautionary measure; however, we did not observe that decrease in-one-go results in significantly different performance. We use cooldown of 20% for all runs. Considering the results in Fig. 10, we conclude that the schedule adaptation suggested by theory leads to slight but noticeable improvement in validation loss for both extended horizons. The improvement is more pronounced for the larger 210M model. Moreover, we observe sudden drop in loss after decreasing the schedule at 40k steps, analogous to what the theoretical bound predicts, albeit the loss decrease thereafter is slower than expected (cf. Fig. 8). We also test adapting the cooldown length as described in (B1): for total length of 100k steps, if cooldown is initialized immediately after 40k steps (cooldown fraction of 60%), we observe even larger improvements as in the previous case (see Fig. 24). From Figs. 10 and 24, we see that the improvement in loss of the adapted wsd schedule over naive continuation is in the range of 0.01. This raises the natural question of the relevance of such an improvement. To answer this, we estimate the slope of our loss curves3: we find that for T2 = 100k, decrease of 0.01 takes roughly 6k steps in the constant learning-rate phase; for T2 = 200k, it takes roughly 14.5k steps. This translates to 0.6B and 1.5B tokens, respectively. Notably, to match the adapted wsd schedule, this would require substantial amount of 6% and 7.25% longer training. Another way to reason about the significance of the loss improvement is through the use of scaling laws, which leads to very similar estimates (see Appendix B.5). 3We do linear regression on the loss values of the baseline run between [64 000, 84 000] for the 100k run, and between [144 000, 164 000] for the 200k run. This accounts proportionally for the cooldown. 13 (Left) Transferring the wsd schedule from horizon T1 = 4000 to T2 Figure 9: (Right) Not adapting the cooldown length leads to significant subopti- [1.5T1, 4T1]. mality. Dashed horizontal lines mark bound for the linear-decay schedule with tuned γ."
        },
        {
            "title": "5.2 Learning-Rate Transfer Across Schedules",
            "content": "One insight from Corollary 2 is that if Gt = G, then the dependence of γ on and is multiplicative. This implies that if we know γ for given practical problem, any multiplicative transfer can be realized. For example, assume we know the optimal base learning-rate for the wsd schedule with cooldown fraction [0, 1], and let us denote the tuned value as γ(c). As we have seen, the linear-decay schedule (c = 1) attains the optimal bound; thus, to obtain better model, we might want to retrain with the linear-decay schedule.4 However, we do not yet know γ(1). Can we compute γ(1) from γ(c) based on the theoretical bound? Simulation. In Fig. 11 we show the quantity ln( γ(1) γ(c) ) for (0, 1). We simulate both the linear cooldown (3), and the 1-sqrt cooldown which has the form ηt = 1 T0 (Hagele et al., 2024). Across varying horizons , the results are consistent; for example, knowing γ for 20 % of linear cooldown, we can compute γ(1) e0.7γ(0.2). For Fig. 11, we set = = 1; we checked that the resulting curve looks the same even if we vary or (plots not shown). (cid:113) T0 +1 Experiments. We now analyze the quantity ln( γ(1) γ(c) ) with real data (training 124M Llama-style model for 50k steps), with linear cooldown. We estimate γ(c) from grid of base learning-rates γ and cooldown fractions (see Fig. 12b and Appendix B.5 for details on this step). We plot ln( γ(1) γ(c) ) in Fig. 12a; it matches almost perfectly with the curve predicted from theory in Fig. 11. This implies that knowing the optimal base learning-rate for 20% cooldown, one can immediately transfer the learning-rate to linear-decay (100% cooldown), without any ad4For example, assume that γ(c) has been made public on Github or we obtained it from sweep that used the wsd schedule due to practical constraints. 14 (a) 124M model (b) 210M model Figure 10: Transferring the learning-rate schedule from horizon T1 = 50 000 to T2 [2T1, 4T1]. Decreasing the base learning-rate (green) after 40k steps leads to small improvements in validation loss compared to keeping it the same (grey). We discuss the significance of the difference in loss values of (around 0.01) in Section 5.1 and Appendix B.5. See Fig. 22 for schedules. ditional sweeps; for the setup we consider, the linear-decay run obtains final validation loss of 2.9535 vs. the best run with 20% cooldown obtaining final loss of 2.9660."
        },
        {
            "title": "6 Limitations",
            "content": "We have shown that the empirical performance of various learning-rate schedules for large model training reflects closely the theoretical suboptimality for non-smooth stochastic convex optimization. We want to stress that we can not expect the bound from Theorem 1 to match training curves perfectly: first, it is an upper bound of the loss for convex problems only, and in practice many other factors (e.g., randomness, architecture choices, data mixture) and training techniques (e.g., loss function, weight decay, clipping) will impact convergence and stability of training (Wortsman et al., 2024). The perhaps most glaring limitation of our work is that it is based on theoretical result for SGD, while most of the empirical evidence we use is obtained with Adam(W). More generally, the result in Theorem 1 can not explain any performance differences that stem from the optimization algorithm. However, we believe that this gap can be closed in future work for several reasons: (i) by showing similar theoretical results for the methods used in practice; as first step, we provide proof for mirror descent (an entire family of methods) in Appendix E. It has been shown that for diagonal networks, the iterates of SGD are equivalent to mirror descent on convex problem formulation (Even et al., 2023). (ii) Several recent variants of SGD close the gap to Adam on transformer problems (Kunstner et al., 2023; Xu et al., 2024). (iii) It has been shown that most of the parameters of language models can be equally well trained with SGD (Zhao et al., 2024). The second obvious limitation of Theorem 1 is the convexity assumption, while modern deep learning problems are non-convex. At this point we have no explanation for why 15 Figure 11: Transferring the optimal base learning-rate from cooldown fraction to lineardecay (c = 1): for linear cooldown (left) and 1-sqrt cooldown (right). Dashed lines are fitted polynomial of degree 6. (a) Learning rate transfer (b) Learning rate sweep Figure 12: (Left) Re-analysis of learning-rate transfer  (Fig. 11)  for 124M model. γc denotes the best performing base learning-rate for cooldown fraction c, estimated from sweep (right). We observe that the learning-rate transfer (black line) almost perfectly e0.7γ(0.2)). Note that the maximal matches the predictions by theory (e.g., γ(0.994) cooldown fraction is 0.994 due to warmup and corresponds to full linear schedule. the convex theory is still closely matching (some) real-world observations. However, it has been shown that the landscape of neural network optimization problems might be reasonably close to being convex (Hardt et al., 2018; Liu et al., 2023; Islamov et al., 2024). And, while our experiments are limited to transformer architectures for language model pretraining, the sudden performance increase during cooldown is not restricted to language modeling and has also been reported for images (Zhai et al., 2022) and even for image classification with ResNets trained with SGD (Sandler et al., 2023). Finally, the empirical quantity we compare to the theoretical bound is the test loss. This is limited to situations where the generalization gap between training and test loss is negligible; that being said, the current practice of single-pass training for large models falls within this category (Aitchison, 2024; Xiao, 2024)."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we show that learning-rate schedules in practice behave surprisingly similar to what convex optimization theory predicts. This spans across the necessity and optimal length of the cooldown period at the end of training as well as the optimal learning-rate transfer. Notably, our experiments suggest that the theoretical bounds can be used as testbed for schedule design before training: we have shown that theoretically inspired schedules achieve notable improvements in practical scenarios. More broadly, our results suggest that one key characteristic underlying the observed behavior is gradient norms that do not go to zero; in practice, this could be due to nonsmoothness (of the objective) or due to the problem-inherent gradient noise. We leave it as future work to explain this phenomenon."
        },
        {
            "title": "Acknowledgments",
            "content": "A. Taylor is supported by the European Union (ERC grant CASPER 101162889). U. Simsekli is supported by the European Union (ERC grant DYNASTY 101039676). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency (ERCEA). Neither the European Union nor the granting authority can be held responsible for them. The French government partly funded this work under the management of Agence Nationale de la Recherche as part of the France 2030 program, reference ANR-23-IACL-0008 (PR[AI]RIE-PSAI)."
        },
        {
            "title": "References",
            "content": "Laurence Aitchison. Why you dont overfit, and dont need Bayes if you only train for one epoch. arXiv:2411.14478, November 2024. [Cited on page 17] Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167175, May 2003. [Cited on page 34] Tamay Besiroglu, Ege Erdil, Matthew Barnett, and Josh You. Chinchilla scaling: replication attempt. arXiv:2404.10102, April 2024. [Cited on page 28] Jerˆome Bolte and Edouard Pauwels. Conservative set valued fields, automatic differentiation, stochastic gradient methods and deep learning. Mathematical Programming, 188 (1, Ser. A):1951, 2021. [Cited on page 3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pp. 18771901, 2020. [Cited on page 1] Frank H. Clarke. Optimization and nonsmooth analysis. Canadian Mathematical Society Series of Monographs and Advanced Texts. John Wiley & Sons, Inc., New York, 1983. [Cited on page 3] Aaron Defazio, Ashok Cutkosky, Harsh Mehta, and Konstantin Mishchenko. Optimal linear decay learning rate schedules and further refinements. arXiv:2310.07831, October 2023. [Cited on pages 2, 4, 5, 8, 9, 32, 33, and 36] Yoel Drori and Marc Teboulle. Performance of first-order methods for smooth convex minimization: novel approach. Mathematical Programming, 145(1-2):451482, 2014. [Cited on page 10] Mathieu Even, Scott Pesme, Suriya Gunasekar, and Nicolas Flammarion. (S)GD over diagonal linear networks: Implicit bias, large stepsizes and edge of stability. In Advances in Neural Information Processing Systems, volume 36, pp. 2940629448, 2023. [Cited on page 15] Baptiste Goujaud, Celine Moucer, Francois Glineur, Julien M. Hendrickx, Adrien B. Taylor, and Aymeric Dieuleveut. PEPit: computer-assisted worst-case analyses of firstorder optimization methods in Python. Mathematical Programming Computation, 16 (3):337367, 2024. [Cited on pages 10 and 26] Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour. arXiv:1706.02677, June 2017. [Cited on page 2] 18 Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems. Journal of Machine Learning Research (JMLR), 19(1):10251068, January 2018. [Cited on page 16] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Thomas Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack Rae, and Laurent Sifre. An empirical analysis of compute-optimal large language model training. In Advances in Neural Information Processing Systems, volume 35, pp. 3001630030, 2022. [Cited on pages 1, 2, 26, 27, and 28] Shengding Hu, Yuge Tu, Xu Han, Ganqu Cui, Chaoqun He, Weilin Zhao, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Xinrong Zhang, Zhen Leng Thai, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, dahai li, Zhiyuan Liu, and Maosong Sun. MiniCPM: Unveiling the potential of small language models with scalable training strategies. In First Conference on Language Modeling, 2024. [Cited on pages 1 and 4] Alexander Hagele, Elie Bakouch, Atli Kosson, Loubna Ben Allal, Leandro Von Werra, and Martin Jaggi. Scaling laws and compute-optimal training beyond fixed training durations. arXiv:2405.18392, May 2024. [Cited on pages 1, 2, 3, 4, 6, 7, 9, 12, 14, 23, and 27] Rustem Islamov, Niccol`o Ajroldi, Antonio Orvieto, and Aurelien Lucchi. Loss landscape characterization of neural networks without over-parametrization. arXiv:2410.12455, October 2024. [Cited on page 16] Prateek Jain, Dheeraj M. Nagaraj, and Praneeth Netrapalli. Making the last iterate of sgd information theoretically optimal. SIAM Journal on Optimization, 31(2):1108 1130, January 2021. [Cited on page 4] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In International Conference on Learning Representations, 2015. [Cited on page 3] Frederik Kunstner, Jacques Chen, Jonathan Wilder Lavington, and Mark Schmidt. Noise is not the main factor behind the gap between sgd and adam on transformers, but sign descent might be. In International Conference on Learning Representations, 2023. [Cited on page 15] Xiaoyu Li, Zhenxun Zhuang, and Francesco Orabona. second look at exponential and cosine step sizes: Simplicity, adaptivity, and performance. In Proceedings of the International Conference on Machine Learning, volume 139, pp. 65536564, July 2021. [Cited on page 4] Chaoyue Liu, Dmitriy Drusvyatskiy, Misha Belkin, Damek Davis, and Yian Ma. Aiming In towards the minimizers: Advances in Neural Information Processing Systems, volume 36, pp. 6074860767, 2023. [Cited on page 16] fast convergence of sgd for overparametrized problems. 19 Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. [Cited on pages 1, 2, 4, In International Conference on Learning Representations, 2017. and 26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. [Cited on pages 3 and 27] Francesco Orabona. Last iterate of SGD converges (even in unbounded domains), 2020. [Online; accessed 08-November-2024]. [Cited on page 4] R. Tyrrell Rockafellar. Convex Analysis. Princeton Mathematical Series, No. 28. Princeton University Press, Princeton, N.J., 1970. [Cited on page 3] Mark Sandler, Andrey Zhmoginov, Max Vladymyrov, and Nolan Miller. Training trajectories, mini-batch losses and the curious role of the learning rate. arXiv:2301.02312, January 2023. [Cited on page 16] Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In Proceedings of the International Conference on Machine Learning, volume 28, pp. 7179, June 2013. [Cited on page 4] Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox, and Rameswar Panda. Power scheduler: batch size and token number agnostic learning rate scheduler. arXiv:2408.13359, August 2024. [Cited on page 7] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob Steeves, Joel Hestness, and Nolan Dey. SlimPajama: 627B token cleaned and deduplicated version of RedPajama, 2023. [Cited on pages 12 and 27] Adrien B. Taylor, Julien M. Hendrickx, and Franccois Glineur. Smooth strongly convex interpolation and exact worst-case performance of first-order methods. Mathematical Programming, 161(1-2):307345, 2017a. [Cited on page 10] Adrien B. Taylor, Julien M. Hendrickx, and Francois Glineur. Exact worst-case performance of first-order methods for composite convex optimization. SIAM Journal on Optimization, 27(3):12831313, 2017b. [Cited on page 10] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMa: Open and efficient foundation language models. arXiv:2302.13971, February 2023. [Cited on pages 1, 11, 12, and 27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, (cid:32)Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30, 2017. [Cited on page 11] Kaiyue Wen, Zhiyuan Li, Jason Wang, David Hall, Percy Liang, and Tengyu Ma. Understanding warmup-stable-decay learning rates: river valley loss landscape perspective. arXiv:2410.05192, October 2024. [Cited on page 4] 20 Mitchell Wortsman, Peter J. Liu, Lechao Xiao, Katie E. Everett, Alexander A. Alemi, Ben Adlam, John D. Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl-Dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, and Simon Kornblith. Small-scale proxies for large-scale transformer training instabilities. In International Conference on Learning Representations, 2024. [Cited on page 15] Lechao Xiao. Rethinking conventional wisdom in machine learning: From generalization to scaling. arXiv:2409.15156, September 2024. [Cited on page 17] Minghao Xu, Lichuan Xiang, Xu Cai, and Hongkai Wen. No more Adam: Learning rate [Cited on scaling at initialization is all you need. arXiv:2412.11768, December 2024. page 15] Moslem Zamani and Francois Glineur. Exact convergence rate of the last iterate in subgradient methods. arXiv:2307.11134, July 2023. [Cited on pages 4, 9, and 36] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1210412113, June 2022. [Cited on pages 3, 4, 12, and 16] Tong Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In International Conference on Machine Learning, pp. 116, 2004. [Cited on page 4] Rosie Zhao, Depen Morwani, David Brandfonbrener, Nikhil Vyas, and Sham Kakade. Deconstructing what makes good optimizer for language models. arXiv:2407.07972, July 2024. [Cited on page 15] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the International Conference on Machine Learning, pp. 928935, 2003. [Cited on pages 23 and 32]"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Related Work 3 Convergence Results 3.1 Comparison of cosine and wsd . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Bound for wsd Schedule . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 4 6 7 4 Theoretical Simulations 8 4.1 Cooldown Length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 4.2 Gradient Norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 4.3 Lower Bounds and Convexity . . . . . . . . . . . . . . . . . . . . . . . . . 10 5 Applications 11 5.1 Schedule Construction for Continued Training . . . . . . . . . . . . . . . . 11 5.2 Learning-Rate Transfer Across Schedules . . . . . . . . . . . . . . . . . . . 14 6 Limitations 7 Conclusion 15 17 Ablation: Min-Suboptimality Bounds 23 A.1 Ablation of Section 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 A.2 Ablation of Section 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 Experiments: Supplementary Material 25 B.1 Non-smooth Convex Example . . . . . . . . . . . . . . . . . . . . . . . . . 25 B.2 Schedule Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 B.3 Cosine Cycle Length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 B.4 Details on Lower Bound Computation . . . . . . . . . . . . . . . . . . . . 26 B.5 Details on Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 B.6 Miscellaneous Plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 Auxiliary Lemmas Missing Proofs Mirror Descent Analysis"
        },
        {
            "title": "36\nF.1 wsd Schedule\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\nF.2 Polynomial Decay Schedules . . . . . . . . . . . . . . . . . . . . . . . . . . 39",
            "content": "22 Ablation: Min-Suboptimality Bounds Standard convergence results for the SGD method (2) make statements on the suboptimality of an average iterate, or of the best objective value (in expectation) found up to iteration . We state one of the standard results for the non-smooth (stochastic) convex setting below (Zinkevich, 2003): Rd and define := , s) is convex. Let (xt) be the iterates given by (A2), . Under (A3), we have x1 Theorem 4. Assume that each ( with ηt > 0 and γ > 0. Let E[f (xt) min t=1,...,T (x)] 2γ 1 t=1 ηt (cid:2) The right-hand side of the above is minimized by γ = (cid:80) D2 + γ2 G2 η2 . (10) t=1 (cid:88) (cid:3) t=1 G2 η2 (cid:80)T . Plugging in γ yields min t=1,...,T E[f (xt) (x)] η2 t=1 G2 t=1 ηt (cid:113)(cid:80) . We now repeat the theoretical simulations, but, instead of Ωt from (9), using (cid:80) Ωt = 2γ 1 s=1 ηs as (10) would suggest. (cid:80) A.1 Ablation of Section 3.1 D2 + γ G2 η2 (cid:2) s=1 (cid:88) (cid:3) (11) Figure 13: Same as Fig. 2, but with Ωt from (11) The bound on the best-so-far bound has very different shape of the last-iterate bound. This shows that standard bounds such as in Theorem 4 do not capture the real-world convergence observed in Hagele et al. (2024). A.2 Ablation of Section 4.1 23 (a) Learning-rate sweep (b) Optimal base learning-rate vs. Figure 14: Same as Fig. 3, but with Ωt from (11) (a) Learning-rate sweep (b) Final bound vs. cooldown fraction Figure 15: Same as Fig. 4, but with Ωt from (11) Figure 16: Same as Fig. 5, but with Ωt from (11) 24 Experiments: Supplementary Material B.1 Non-smooth Convex Example Here, we provide details for the non-smooth convex toy example of minx uniformly at random from mentioned in Section 4.3. We set = 2 and pick Rd and set = Ax. We then run gradient descent 1, 1]. We generate an oracle [ (GD) for = 400 iterations with the wsd schedule (cooldown fraction 0.2 and γ = 0.02). As baseline, we plot the constant schedule with γ = 0.02 and cosine schedule with γ = 0.04. We pick zero as starting point, except for the constant schedule, where we pick (10 3) to obtain visually distinguishable path. 3, 10 R20 Ax Rd The objective function and iterate paths are shown in Fig. 17. Figure 17: (Left) Sudden drop of the loss for wsd schedule for convex, non-smooth problem. (Right) Iterate path for the three schedules. For wsd, the cooldown period is indicated with the dashed line. Star marks solution. B.2 Schedule Comparison We compare the upper bound Ωt from (9) for various schedules: wsd with cooldown fraction 0.2, cosine, constant schedule, linear-decay schedule, that is, wsd with cooldown fraction of 1, 1/sqrt schedule, where ηt = 1/t, 1-sqrt schedule, where ηt = 1 1 . We assume = 1, Gt = 1 and set = 400. For each schedule we sweep the base learning-rate γ and plot the bound Ωt for γ = γ obtained from Corollary 2. (cid:113) 25 (a) Convergence (b) Learning-rate sweep Figure 18: Comparison of various learning-rate schedules. Convergence is plotted with the optimal base learning-rate γ (chosen individually for each schedule). B.3 Cosine Cycle Length For the cosine schedule, an important hyperparameter is its cycle length, that is, the amount of training where the schedule first reaches zero. Originally, it was proposed in Loshchilov & Hutter (2017) to use multiple warm restarts (a cycle length less than one). Later, Hoffmann et al. (2022) show empirically that the best performance in language modeling tasks is obtained by setting the cycle length to one (the half-cosine matches exactly the training duration). To the best of our knowledge, this recommendation is based mostly on empirical insights. Using the bound obtained in Theorem 1, our analysis shows that cycle length of one obtains the lowest bound ΩT . Thus, the theoretical bound is in accordance to the empirical conclusion from Hoffmann et al. (2022). Note that Hoffmann et al. (2022) choose the base learning-rate γ equally for all cycle lengths. To match the setting of their experiment, we pick γ for cycle length of one, and use this for all other cycle lengths as well. Picking γ for each cycle length individually yields qualitatively the same result (the optimal cycle length being one), but with slightly less pronounced differences (plots not shown). In contrast to previous simulations, the final value of the schedule is chosen as 0.1 of the peak learning-rate (instead of zero), again in order to match the setting of Hoffmann et al. (2022). B.4 Details on Lower Bound Computation We provide additional details for the simulation in Section 4.3. We compute the lower bounds with the PEPit package (Goujaud et al., 2024). For our purpose, we use the class of convex G-Lipschitz functions and gradient descent (GD) with the step sizes γ ηt. In PEPit, we use the MOSEK solver. As the size of the semidefinite program grows with , we choose rather small = 60, and compute the lower bound of E[f (xt) (x)] for all t+1 that are multiple of 5. Note that the specific worst-case function constructed by PEPit can be different for each (as it maximizes the suboptimality exactly at iteration t). We set γ to γ minimizing the upper bound Ωt (cf. Corollary 2). We set = = 1. 26 Figure 19: Comparison of cycle lengths for the cosine schedule. Compare to Figure A1 in Hoffmann et al. (2022). B.5 Details on Experiments Training details. The loss curves in Fig. 1a are an exact reproduction of the curves in (Hagele et al., 2024, Fig. 3); they are obtained from training 210M Llama-style transformer (Touvron et al., 2023). The base learning-rate of cosine is 0.001, and for wsd it is 0.0005. All of the following applies to the training runs used in the experiments in Section 5: we use exactly the same model architecture as in Hagele et al. (2024), which is Llama-style transformer with 12 (24) layers and 12 attention heads for the 124M (210M) model. The dataset used for training is SlimPajama (Soboleva et al., 2023). Specifically, for runs with 50 000 steps (5B tokens), we use the SlimPajama-6B subset available on Hugging Face (link below). For the extension runs with 100 000 and 200 000 steps (approximately 10B and 20B tokens), we randomly sample 550M documents (roughly 5% of full corpus) from the full SlimPajama-627B to arrive at corpus of 30B tokens. We train for 50 000 steps, where the first 300 steps are reserved for linear warmup. We use AdamW (Loshchilov & Hutter, 2019) with weight decay of 0.1. For all further details we refer to Hagele et al. (2024, App. A.1). Note that all training curves show the validation loss computed over subset of 32 batches, while the final validation loss is computed over approx. 6 000 batches; hence, the final value of the loss curve might not be identical to the final loss computed over the full validation set. One single run over 50 000 steps takes roughly 2 hours on two Nvidia H100 GPUs. The training runs can be reproduced with the following repositories: Training code: https://github.com/epfml/schedules-and-scaling/ Dataset: https://huggingface.co/datasets/DKYoon/SlimPajama-6B Fitting procedure. We execute training runs on grid of base learning-rates γ 0.0005, 0.001, 0.002, 0.003 0.1, 0.2, 0.4, 0.6, 0.8, 0.9, 0.994 . } } { Note that the largest cooldown fraction is slightly smaller than 1 as the remaining 0.6% percent of steps constitute warmup. The final validation set loss (after 50k steps) for all and cooldown fractions { 27 runs is displayed in Fig. 12b (every dot marks one single training run). We then fit, for each cooldown fraction separately, function of the form hc(γ) = Ac γ + Bcγ + Cc, where Ac, Bc, Cc are fittable parameters. The resulting function is plotted as solid line in Fig. 12b. The functional form of hc(γ) is inspired by the bound (8). We then approximate the optimal base learning-rate γ(c) by computing the minimizer of hc(γ). The result of this step is plotted in red in Fig. 12a. Assessing loss differences through scaling laws. In this section, we estimate with scaling laws how much more parameters or training data/steps would be needed to make up loss difference of 0.01 (see end of Section 5.1 for context). The Chinchilla law (Hoffmann et al., 2022) states that the loss L(N, D) for model with parameters after training for tokens can be estimated with L(N, D) = + α + Dβ , (12) where E, A, B, α, β are usually fitted from data. More concretely, assume we have trained model of size N1 for D1 tokens. To arrive at an improvement of δ with new combination of the number of parameters and tokens to (N2, D2), we obtain δ = L(N1, D1) L(N2, D2) = 1 α 1 1 α 2 (cid:19) (cid:18) + 1 Dβ 1 1 Dβ 2 (cid:19) (cid:18) Consequently, we can consider two cases: Case 1: Fix N1 = N2. That is, we fix parameter size and look for the number of tokens by which we need to extend the training to improve the loss by δ. Solving the above equation then gives D2 = 1 Dβ 1 δ (cid:18) 1 β . (cid:19) Case 2: Fix D1 = D2. This is the case where we estimate the size that would achieve the desired loss improvement for the same training data. Similarly, this results in N2 = 1 α 1 δ (cid:18) 1 α . (cid:19) In the settings of our experiments we have N1 { Plugging in the constants by Besiroglu et al. (2024)6 and using δ = 0.01, yields7 124M, 210M } and D1 { 5. 10.24B, 20.48B } 5Batch size 50, two accumulation steps, two GPUs, sequence length 512, 100/200k steps. 6A = 482.01, = 2085.43, = 1.8172, α = 0.3478, β = 0.3658. 7Note that the Chinchilla scaling laws were obtained in different setup. In particular, we do not have access to the exact same dataset and tokenizer, which makes the scaling law not directly transferrable. However, our experiments are comparable in the general dataset composition (webcrawl data extended with other sources) and training task (decoder-only language models). Moreover, with the difference in 103)/ ln(50 vocabulary size (32k vs. 50k), we can scale the loss with the rough approximation of ln(32 103) 0.959 to align the cross-entropy losses. This does not substantially change the results of this analysis. 28 10.88B, 22.16B } 10.24B, 20.48B , respectively. This means that we would need to train } Case 1: Fix N1 = N2. In this case, the scaling law results in D2 { for D1 { the models for 640M or 1.68B more tokens to match the adapted schedule. Case 2: Fix D1 = D2. In this case, we obtain N2 { 124M, 210M { } would approximately result in the same loss after fixing the amount of tokens. for N1 . In other words, increasing the number of parameters by 5M or 10M 129.0M, 220.1M } For both cases, the estimates from the scaling law match our general intuition and would require either noticeably training longer by 6 5%, in line with the argument at the end of Section 5.1. Also note that the (relative) additional cost implied by the Chinchilla law to obtain 0.01 loss improvement grows with the (extended) training length D1. 8% or growing the model by 4 B.6 Miscellaneous Plots Figure 20: (Left) The benefit of cooldown is reflected in the absence of logarithmic terms. Dark grey marks the bound of the constant schedule. (Right) Plotting the individual T2 with γ = γ for the wsd schedule. The sudden drop terms of the bound Ωt = of the bound comes from the term γ T1/γ + γ T2. 29 Figure 21: (Left) Analogous to Fig. 5 (right) with real training curves. We remove cooldown fraction 0.6 as its loss curve shows spike and recovers only late. (Right) Analogous of Fig. 4b with real training data that shows parabola shape for fixed learningrates. Figure 22: (Left) Decreasing factor computed with Corollary 2 for extended schedule up to (where T1 is length of the short run). See Section 5.1, (B2) for details. (Right): Extended schedule for the training runs in Fig. 10; note that this schedule is multiplied by γ = 0.001. 30 Figure 23: (Left) Transferring the 1/sqrt schedule with linear cooldown from horizon [1.5T1, 4T1]. (Right) Adapting the cooldown length has only small T1 = 4000 to T2 benefits. Dashed horizontal lines mark bound for linear-decay schedule with tuned γ. See Section 5.1, (B1) for details. Figure 24: Experiment with adapted cooldown length for 124M model (left) and 210M (right). See Section 5.1, (B1) for details."
        },
        {
            "title": "C Auxiliary Lemmas",
            "content": "Lemma 5 (Lemma 5 from Defazio et al. (2023)). Let (qt) be any sequence, and let (wt) be positive sequence. Then, for any it holds qT = 1 t=1 wt (cid:80) Lemma 6. Let l, + s=1 (cid:88) = 1 2(T + 2 1 wtqt + t=1 (cid:88) and (cid:88)k=1 . It holds (cid:80) l)(T + l), wk t=k+1 wt 1 t=k wt (cid:88)t=k (cid:16) (cid:80) wt(qt qk) . (cid:17) +1 s=1 (cid:88) s2 = 1 6(2T + 3 2l)(T + l)(T + 1 l). Proof. We refer to WolframAlpha: [link to first result], [link to second result]. Lemma 7. Let N. It holds ln(t) ln(t + 1)"
        },
        {
            "title": "D Missing Proofs",
            "content": "t 1 + 1 ds 0 (cid:90) s=1 (cid:88) 1 = Ht 1 + ln(t). The following lemma will be the basic inequality for subsequently proving Theorem 9; it is standard result in the online learning and convex optimization literature (Zinkevich, 2003). Lemma 8. Let 1 T"
        },
        {
            "title": "T and let u",
            "content": "Rd be measurable with respect to xk. It holds ηtE[f (xt) (u)] (cid:88)t=k 1 2 xk 2 + 1 2 η2 gt 2. (cid:88)t=k (13) Proof. From the update rule (2) and property (4) we obtain xt xt gt, xt (xt, st) 2 xt+1 2ηt 2ηt + η2 gt (u, st) 2 = 2 + η2 2. gt Apply conditional expectation (conditioned on t) to obtain (cid:2) 2 Apply total expectation (with respect to = 1, . . . , ) and rearrange to obtain xt+1 xt (xt) gt (u) 2 2ηt 2. (cid:2) (cid:3) (cid:3) + η2 ηtE[f (xt) (u)] Sum from = k, . . . , to obtain the final result: 1 2 xt 2 1 2 xt+1 2 + η2 2 gt 2. ηtE[f (xt) (u)] 1 2 xk 2 + 1 2 (cid:88)t=k (cid:88)t=k η2 gt 2. Theorem 9 (Thm. 10 from Defazio et al. (2023)). Let the iterates (xt) be given by (2) Rd and := with ηt > 0 for = 1, . . . , . Let x x1 t=1 η2 gt E[f (xT ) t=1 ηt 2 1 t=k ηt D2 t=1 ηt 1 ηk t=k+1 ηt . Then, it holds (x)] = gt (cid:80) η2 1 2 (cid:80) (cid:80) + + 2 . 2 2 (cid:88)k=1 (cid:16) (cid:88)t=k (cid:17) Remark 2. (cid:80) (cid:80) (i) Note that the result of Theorem 9 is an anytime result, in the sense that we can evaluate the right-hand side at any without the knowledge of ηt for > . (ii) technical artifact of Theorem 9 is that xT does not depend on ηT by definition, however ηT appears in the bound on the right-hand side. This is standard in the analysis of subgradient methods: in the proof, we bound 0. If one carries through this term to the end, then we obtain multiple terms in the bound that depend on ηT . xT +1 2 Theorem 1 follows from applying Theorem 9 with ˆηt := γηt. We finally prove Theorem 9. (14) (15) (16) Proof. First, apply Lemma 8 with and 1 to obtain Define qt := E[f (xt) t=1 (cid:88) ηtE[f (xt) (x)] 1 2 D2 + 1 η2 2. gt t=1 (cid:88) (x)]. Dividing by 1 t=1 ηt t=1 (cid:88) ηtqt 2 t=1 ηt gives t=1 η2 gt t=1 ηt D2 (cid:80) t=1 ηt + 2 . (cid:80) ηt we need to bound the term (cid:80) In order to apply Lemma 5 with wt (cid:80) (cid:80) ηt(qt qk) = ηtE[f (xt) (cid:88)t=k Thus, apply Lemma 8 with (cid:88)t=k xk to obtain 1 t=k ηt ηt[qt qk] 2 1 t=k ηt (cid:88)t=k (xk)]. (cid:88)t=k η2 2. gt (cid:80) E[f (xT ) Now, combine Lemma 5 with (15) and (16) to get (cid:80) t=1 η2 gt t=1 ηt 2 1 t=k ηt (x)] = qT D2 t=1 ηt 1 ηk t=k+1 ηt (cid:80) 1 2 (cid:80) (cid:80) + + 2 2 (cid:88)k=1 (cid:16) η2 2 gt . (cid:17) (cid:88)t=k (cid:80) (cid:80)"
        },
        {
            "title": "E Mirror Descent Analysis",
            "content": "In this section, we extend the bound from Theorem 9 to the stochastic mirror-descent method. In this section only, we denote with Notation. the rest of the paper, where it denotes the standard Euclidean norm), and let its dual norm, defined by an arbitrary norm (in contrast to denote . be continuously differentiable function that is µ-strongly convex with . Define the Bregman divergence as Let ψ : Rd respect to := supz x, 1 Rd: Bψ(x, y) := ψ(x) ψ(y) y, ψ(y) . It follows Bψ(x, y) following three-point-identity (Beck & Teboulle, 2003, Lem. 4.1): for any x, y, holds 2 from strong convexity of ψ. Further, we will need the Rd it µ 2 Bψ(z, x) + Bψ(x, y) Bψ(z, y) = ψ(y) ψ(x), . (17) Now, the iterates of (stochastic) mirror descent are given by: for ηt > 0 and gt compute (xt, st), xt+1 = arg min Rd gt, ηt xt + Bψ(y, xt). (18) We will now prove mirror descent version of Theorem 9 (in fact, Theorem 9 is special case with Bψ(x, y) = 1 2). To do so, we first follow standard steps in mirrorx 2 descent analysis (Beck & Teboulle, 2003) to obtain the basic inequality in Lemma 10. In contrast to the classical mirror-descent analysis, we use this to prove last-iterate bound in Theorem 11. Lemma 10. Let the iterates (xt) be generated by (18). Let 1 measurable with respect to xk. It holds k"
        },
        {
            "title": "T and let u",
            "content": "Rd be (cid:88)t=k ηtE[f (xt) (u)] E[Bψ(u, xk)] + 1 2µ (cid:88)t=k η2 gt 2 . (19) Proof. For fixed y, we have of (18) are xBψ(x, y) = ψ(x) ψ(y). Thus, optimality conditions 0 = ηtgt + ψ(xt+1) ψ(xt). (20) Then, we have ηt[f (xt, st) = xt+1, (u, st)] ψ(xt) xt ηt ψ(xt+1) :=s1 u, gt ηtgt xt xt+1, ηtgt :=s3 . 34 + xt+1, ψ(xt+1) :=s2 ψ(xt) + From (20), we have s1 = 0. From (17), we have s2 = Bψ(u, xt) Bψ(xt+1, xt). From the (generalized) Cauchy-Schwarz inequality combined with Youngs inequality, we have Bψ(u, xt+1) s3 µ 2 xt+1 2 + xt η2 2µ gt 2 . Using that Bψ(xt+1, xt) µ 2 xt+1 2, we obtain xt ηt[f (xt, st) (u, st)] Bψ(u, xt) Bψ(u, xt+1) + η2 2µ gt . 2 Taking conditional expectation, we have E[f (xt, st) rearrange, take total expectation and sum from = k, . . . , . Using that Bψ(u, xT +1) we obtain (u, st)] = (xt) (u). Finally, 0, (cid:88)t=k ηtE[f (xt) (u)] E[Bψ(u, xk)] + 1 2µ (cid:88)t=k η2 gt 2 . Now, repeating the proof of Theorem 9, but applying Lemma 10 instead of Lemma 8, we obtain the following bound. Theorem 11. Let the iterates (xt) be given by stochastic mirror descent (18) with ηt > 0 for = 1, . . . , . Let Rd. Then, it holds E[f (xT ) (x)] = + E[Bψ(x, x1)] t=1 ηt + 2 t=1 η2 gt t=1 ηt 2µ (cid:80) (cid:80) 1 2µ (cid:88)k=1 ηk t=k+1 ηt (cid:16) 1 (cid:80) t=k ηt η2 E gt 2 . (cid:17) (cid:88)t=k (cid:80) (cid:80)"
        },
        {
            "title": "F Analysis of Schedules",
            "content": "F.1 wsd Schedule In this section, we give further interpretation the bound in Theorem 3 in comparison to the constant and linear-decay schedules. In Section 3.2, we derived that for large and T0 = βT , the bound for wsd is approximately E[f (xT ) (x)] DG 4 1 + β (cid:114) (cid:2) 2 3 + 1+2β 3(1+β) β 3(1+β) + H(1+β)T 2 H(1 β)T +1 . (cid:3) To obtain concrete numbers, plugging in β = 0.8 (that is, 20% cooldown) for wsd, and obtain E[f (xT ) (x)] DG 0.9 + 2.2(H1.8T H0.2T +1). 2 (cid:112) H0.2T +1) For example, if = 105, then 2.2(H1.8T 4.39. In comparison, we have: 1 + HT 2 constant schedule: the bound is DG linear-decay schedule: the same bound from Corollary 2 results in (2 + HT 1 ) DG (Defazio et al., 2023, Thm. 13). However, with different (but less general) proof technique one can show the tighter bound DG (Defazio et al., 2023, Cor. 2), which is actually worst-case optimal for convex, Lipschitz problems (Zamani & Glineur, 2023). +1 1. 2/3 Again for = 105, we have HT 2.0001. In conclusion, for this specific the constant of the bound is roughly twice for wsd compared to linear-decay, and 1/3 compare to constant schedule. 12.09 and (2 + HT 1 1 + 2/3 ) Finally, we prove Theorem 3. Proof of Theorem 3. From Corollary 2, we have E[f (xT ) (x)] 2 T1(η1:T , D, ) T2(η1:T , G1:T , ). Thus, the rest of the proof will compute an upper bound of the right-hand side. First, for 1 , we compute (cid:112) T0 : (cid:88)t=l ηt = 1 +1 T0 (T + (cid:88)t=l t) = 1 +1 T0 +1 = s=1 (cid:88) (T + 2(T + 1 l)(T + 1 T0) l) . Here we made the change of variable + 1 and used Lemma 6. Similarly, we get < T0 : ηt = 1 ηt + (cid:88)t=l = t=T0 (cid:88) (cid:88)t=l 2[T + 2 + T0 ηt = T0 2l]. + t=T0 (cid:88) ηt = T0 + 36 (T + T0)(T + 1 T0) 2(T + 1 T0) Note that this expression is still correct if we would plug in = T0. Next, we compute the sum of squares. We start again with T0 : η2 = (cid:88)t=l And similarly 1 (T + 1 T0)2 +1 s=1 (cid:88) s2 = (2T + 3 2l)(T + 2 6(T + 1 T0) l)(T + 1 l) . < T0 : (cid:88)t=l T0 1 η2 + η2 = η2 = T0 (2T + 3 + 2T0)(T + 2 6(T + 1 T0)(T + 1 T0)2 T0) t=T0 (cid:88) (2T + 2T + 5 6 (cid:88)t=l = T0 = T0 = 1 6 + 2T + 4T0 + 5 6l + 2T0)(T + 2 T0) 6(T + 1 2T0 T0) + 1 6(T + 1 1 (T + T0) T0) . (cid:3) Here, we used that (cid:2) (2T + 3 2T0)(T + 2 = (2T + 5 = (2T + 5 T0) = (2T + 5 2T0)(T + 1 2T0)(T + 1 2T0)(T + 2 T0) + (2T + 5 T0) + 1. T0) 2T0) 2(T + 2 2(T + 2 T0) T0) Again, the expression we obtain for < T0 is correct if we would plug in = T0. Now we N, can try to compute the bound. We start with the easy ones: as Gt = > 0 for all we obtain 1 6(T +1 T0) (21) T1(η1:T , D, ) = 1 t=1 ηt 2 (cid:80) t=1 (cid:88) (cid:0) G2 η2 = D2 + T0 , 2 D2 = 1 t=1 ηt 1 6[2T + 4T0 + 5 G2 (cid:80) 6] + 1 2(T + T0) (cid:1) = G2 6(T + T0) 2T + 4T0 (cid:2) 1 + 1 +1 T0 G2(T + 2T0) 3(T + T0) . (cid:3) . Next, for = 1, . . . , T0 1, we 1 +1 The last inequality is due to T0 < and thus 1 > need to compute Again, as Gt = > 0 for all (cid:80) ηk t=k+1 ηt 1 t=k ηt G2 η2 . (cid:16) (cid:17) N, we omit for now, and start with the case (cid:80) (cid:88)t=k T0: ηk t=k+1 ηt (cid:80) (cid:16) (cid:80)"
        },
        {
            "title": "1\nT\nt=k ηt",
            "content": "T η2 = (cid:88)t=k (cid:17) +1 + T0 (T +2 (2T +3 2k)(T +2 6(T +1 k)(T +1 k)2(T T0)2 k) k)(T +1 4(T +1 T0)2 k) = 2(2T + 3 2k) 3(T + T0)(T . k) Now, if < T0: ηk t=k+1 ηt (cid:80) (cid:16) (cid:80) 1 t=k ηt η2 = (cid:88)t=k (cid:17) = 1 6 1 (T +1 (cid:2) 1 2T + 4T0 + 1 4[T + T0 2T + 4T0 + 5 2 3[T + T0 (cid:2) 6k + 2k][T + 2 + T0 6k + 2k][T + 2 + T0 1 (T +1 . T0) 2k] (cid:3) T0) 2k] Now, compute 1 ηk t=k+1 ηt 1 T0 (cid:80) (cid:88)k=1 (cid:2) 1 t=k ηt 2T + 4T0 + 5 (cid:80) [T + T0 (cid:16) (cid:88)k=1 2 3(cid:34) η2 = (cid:88)t=k (cid:17) 6k + 2k][T + 2 + T0 1 (T +1 T0) 2k] (cid:3) :=Ω1 + 1 (cid:88)k=T (2T + 3 2k) (T + 1 T0)(T k) (cid:35) :=Ω2 (cid:3) =: ( ). Then, it holds [link to proof] 1 Ω2 = (2T + 3 2k) (T + 1 T0)(T (cid:88)k=T0 To simplify this term bit, we estimate (T For Ω1, we can bound the nominator by 2 Ω2 4 3 + 2T = k) 2T0 + 3 T0 T0 + 1 + 3HT 1 . 2 T0)2 + 2HT T0 1 T0 + 1 . 6k + 1 +1 2T + 4T0 + 5 = 3(T + 2 + T0 3(T + 2 + T0 where for the second term we bound T0 1 +1 2k) (T 1 due to T0 2k) (T T0 T0 + 1) + 1 +1 T0), T0 . It holds [link to proof] T0 1 [T + T0 (cid:88)k=1 Therefore, defining Ω3 := (T (T 2k][T + 2 + T0 T0)(T0 1) T0+2)(T +T0) we get 2k] T0 1 T0 + 2)(T + T0) . = (T T0 1 3 + T0 (cid:16) (cid:88)k=1 T0. Altogether, we have (cid:17) 2k Ω3 = 3(HT +T0 2 HT T0+1) Ω3, (Ω1 + Ω2) 2(HT +T0 2 HT T0+1) 2 3 Ω3 + 4 3 + (T 2 T0)2 + 2HT T0 1 T0 + 1 . Multiplying this term with G2 2 , we can plug the result in (21) to finally derive the bound 2 T1(η1:T , D, ) T2(η1:T , G1:T , ) (cid:112) 2DG (cid:118) (cid:117) (cid:117) (cid:116) 1 +T0 (cid:34) +2T0 3(T +T0) + HT +T0 HT 2 T0+1 38 (T T0)(T0 T0+2)(T +T0) + 2 1) 3 + 1 (T T0)2 + HT T01 T0+1 3(T . (cid:35) Ω1 where we used ) = ( 2 3 F.2 Polynomial Decay Schedules In this section, we analyze the schedule ηt = (T + 1 polynomial order α. It holds ηT +1 = 0 and η1 = α. t)α, hence ηt decays to zero with We will approximate the bound in Theorem 1 by approximating the sums in T2(η1:T , G1:T , ) with integrals. First, with the change of variable + 1 and have T1(η1:T , D, ) s, we T +1 +1 ηt = ηt = (T + 1 (cid:88)t=l Similarly (cid:88)t=l (cid:88)t=l k)α = +1 sα s=0 (cid:88) 0 (cid:90) +1 sαds = (T + 1 l)α+1 α + 1 . +1 s2α η2 = (cid:88)t=l Now we approximate s=0 (cid:88) +1 s2αds = 0 (cid:90) (T + 1 l)2α+1 2α + 1 . T1(η1:T , D, ) ="
        },
        {
            "title": "1\nT\nt=1 ηt",
            "content": "D2 D2(α + 1) 2T α+1 , and, if Gt = > 0 for all t, then (approximating (cid:80) t=k+1 ηt with t=k ηt) 1 t=1 ηt G2 2 G2 η2 + (cid:80) 1 1 2 t=1 (cid:88) (cid:0) 1 (T + (cid:88)k=1 (cid:1) k)α (T + 1 (cid:80) 2α + 1 ηk t=k+1 ηt k)2α+1 (cid:80) 1 t=k ηt (cid:88)t=k (α + 1)2 (cid:16) (cid:80) (T + 1 G2 η2 (cid:17) k)2α+2 T2(η1:T , G1:T , ) = 2 G2(α + 1)T 2α+1 (cid:80) 2(2α + 1)T α+1 + = = 1 α + (cid:88)k=1 (α + 1)2 2α + 1 (cid:88)k=1 (α + 1)2 2α + 1 2 (cid:90) (α + 1)2 (T α (2α + 1)α α + α + α + 1 2α + 1 α + 1 2α + 1 α + 1 2α + 1 G2 2 G2 2 G2 2 (cid:104) (cid:104) (cid:104) (T + 1 k)α 1 (cid:105) sα 1ds (cid:105) 2α) (cid:105) G2(α + 1) 2α α. Altogether, the bound is given by 2 T1(η1:T , D, ) T2(η1:T , G1:T , ) DG(α + 1) α+1 α attains its minimum at α = 1 (which is, again, linear decay). αT (cid:112) . The function α (cid:55)"
        }
    ],
    "affiliations": [
        "EPFL, Lausanne, Switzerland",
        "Inria, Departement dInformatique de lEcole Normale Superieure, PSL Research University, Paris, France"
    ]
}