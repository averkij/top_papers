{
    "paper_title": "VisCoder2: Building Multi-Language Visualization Coding Agents",
    "authors": [
        "Yuansheng Ni",
        "Songcheng Cai",
        "Xiangchao Chen",
        "Jiarong Liang",
        "Zhiheng Lyu",
        "Jiaqi Deng",
        "Kai Zou",
        "Ping Nie",
        "Fei Yuan",
        "Xiang Yue",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have recently enabled coding agents capable of generating, executing, and revising visualization code. However, existing models often fail in practical workflows due to limited language coverage, unreliable execution, and lack of iterative correction mechanisms. Progress has been constrained by narrow datasets and benchmarks that emphasize single-round generation and single-language tasks. To address these challenges, we introduce three complementary resources for advancing visualization coding agents. VisCode-Multi-679K is a large-scale, supervised dataset containing 679K validated and executable visualization samples with multi-turn correction dialogues across 12 programming languages. VisPlotBench is a benchmark for systematic evaluation, featuring executable tasks, rendered outputs, and protocols for both initial generation and multi-round self-debug. Finally, we present VisCoder2, a family of multi-language visualization models trained on VisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4.1, with further gains from iterative self-debug, reaching 82.4% overall execution pass rate at the 32B scale, particularly in symbolic or compiler-dependent languages."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 2 4 6 3 2 . 0 1 5 2 : r VISCODER2: BUILDING MULTI-LANGUAGE"
        },
        {
            "title": "VISUALIZATION CODING AGENTS",
            "content": "Yuansheng Ni1, Songcheng Cai1, Xiangchao Chen1, Jiarong Liang1, Zhiheng Lyu1, Jiaqi Deng3, Ping Nie5, Kai Zou4, Fei Yuan5, Xiang Yue2, Wenhu Chen1 1University of Waterloo, 2Carnegie Mellon University, 3Korea Advanced Institute of Science & Technology, 4Netmind.ai, 5Independent Researcher https://tiger-ai-lab.github.io/VisCoder2 Figure 1: Overview of VisCoder2. We present three components: 1) VisCode-Multi-679K: dataset of 679K executable visualization code pairs with multi-round correction dialogues across 12 programming languages; 2)VisPlotBench: spanning 8 languages with natural language instructions, executable code, and rendered outputs; 3)VisCoder2: family of visualization coding agents that iteratively execute, render, and self-debug, approaching the performance of proprietary models."
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) have recently enabled coding agents capable of generating, executing, and revising visualization code. However, existing models often fail in practical workflows due to limited language coverage, unreliable execution, and lack of iterative correction mechanisms. Progress has been constrained by narrow datasets and benchmarks that emphasize single-round generation and single-language tasks. To address these challenges, we introduce three complementary resources for advancing visualization coding agents. VisCodeMulti-679K is large-scale, supervised dataset containing 679K validated and executable visualization samples with multi-turn correction dialogues across 12 programming languages. VisPlotBench is benchmark for systematic evaluation, featuring executable tasks, rendered outputs, and protocols for both initial generation and multi-round self-debug. Finally, we present VisCoder2, family of multi-language visualization models trained on VisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4.1, with further gains from iterative self-debug, reaching 82.4% overall execution pass rate at the 32B scale, particularly in symbolic or compiler-dependent languages."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in large language models (LLMs) have enabled coding agents Jimenez et al. (2023); Yang et al. (2024b) that can generate visualization code, execute it, and even revise their outputs in response to feedback (Robeyns et al., 2025; Li et al., 2025). These agents are increasingly applied 1Core Contributors, Corresponding to {yuansheng.ni, wenhuchen}@uwaterloo.ca 1 to data analysis and reporting workflows, where producing plots and diagrams is central task (Galimzyanov et al., 2024). While existing models can attempt these steps, they often fail in practice: generating code that crashes, produces incorrect visuals, or lacks flexibility across programming languages and libraries (Goswami et al., 2025). Building more reliable visualization coding agents requires resources that go beyond single-round generation, supporting multi-language coverage, runtime validation, and iterative correction through execution feedback (Yang et al., 2023). However, current datasets and benchmarks lack these capabilities, limiting progress toward agents that can effectively assist in real-world visualization workflows (Ni et al., 2025). Visualization presents uniquely valuable setting for advancing these agents. Unlike general-purpose code generation (Li et al., 2022), visualization tasks produce clear and interpretable outputs: the execution process and rendered figure provide an immediate signal of whether the code executed successfully and whether the output aligns with the intended result (Ni et al., 2025). Moreover, visualization requires cross-domain reasoning, combining knowledge of data handling, plotting syntax, and design conventions (Satyanarayan et al., 2016). Crucially, real-world workflows are inherently iterative analysts rarely produce perfect visualizations on the first attempt, instead refining their code based on runtime behavior and visual inspection (Goswami et al., 2025). This natural feedback loop makes visualization tasks especially well-suited for developing agents that can generate and self-correct code (Chen et al., 2023). Despite this potential, existing resources for visualization code generation remain narrow in scope. Most datasets focus on single languages, such as Python or Vega-Lite (Galimzyanov et al., 2024; Luo et al., 2021), and include many snippets that cannot be executed reliably (Ni et al., 2025). They lack validated, executable samples, and they do not provide the multi-turn interactions needed to train models for iterative debugging (Ni et al., 2025). Existing benchmarks also have significant gaps: they emphasize single-round generation and do not support systematic evaluation across languages or multi-round repair scenarios (Yang et al., 2023). As result, current models are tested in settings that fail to capture the complexity of real-world visualization development (Goswami et al., 2025). To address these limitations, we introduce two complementary resources. First, we present VisCode-Multi-679K, large-scale supervised instruction-tuning dataset comprising 679K executable visualization and code-correction samples across twelve programming languages. VisCode-Multi-679K combines validated visualization code extracted from diverse open-source repositories (Lozhkov et al., 2024; Yang et al., 2025a; Rodriguez et al., 2025) with multi-turn dialogues that teach models to revise faulty code based on execution feedback (Zheng et al., 2024). Second, we propose VisPlotBench, benchmark for evaluating visualization coding agents across eight languages. VisPlotBench provides carefully curated, executable tasks with natural language instructions and rendered outputs, along with standardized evaluation protocol for both initial generation and multi-round self-debug (Galimzyanov et al., 2024). Finally, we train VisCoder2, family of multi-language visualization models built on VisCode-Multi-679K. VisCoder2 substantially outperforms size-matched open-source baselines (Hui et al., 2024; Guo et al., 2024a; Ni et al., 2025) and closes much of the performance gap with proprietary models such as GPT-4.1 (Fachada et al., 2025). Experiments show that iterative self-debug yields further improvements, reaching 82.4% at the 32B scale, on par with GPT-4.1 and surpassing GPT-4.1-mini , particularly benefiting symbolic or compiler-dependent languages like LilyPond, LaTeX, and Asymptote. Together, VisCode-Multi-679K, VisPlotBench, and VisCoder2 establish foundation for building and evaluating visualization coding agents that can operate reliably across diverse programming languages and real-world visualization tasks."
        },
        {
            "title": "2 RELATED WORK",
            "content": "LLMs for Visualization Code Generation Large language models have shown promising results in generating visualization code from natural language descriptions (Yang et al., 2024c; Chen et al., 2024; Galimzyanov et al., 2024). Most existing approaches focus on single languages, particularly Python with matplotlib or plotly (Wu et al., 2024; Yang et al., 2024a), while some explore specification-based methods using Vega-Lite (Xie et al., 2024). However, these systems face significant limitations: they typically support only one or two programming languages, lack systematic 2 execution validation, and often generate code that fails to run reliably (Ni et al., 2025). Multilanguage code generation efforts in broader domains (Lozhkov et al., 2024; Muennighoff et al., 2023) provide extensive language coverage but lack the specialized knowledge required for visualization tasks, particularly for domain-specific languages like LaTeX for mathematical plots or LilyPond for musical notation. Our VisCode-Multi-679K dataset addresses these limitations by providing validated, executable visualization samples across twelve programming languages, enabling robust multi-language visualization code generation with systematic quality control and execution verification. Self-Debug and Coding Agents Recent advances in coding agents have emphasized iterative development capabilities, where models can generate, execute, and refine code through multiple rounds of feedback (Jimenez et al., 2023; Yang et al., 2024b). Self-debug approaches leverage execution traces, error messages, and runtime outcomes to guide automatic code correction (Chen et al., 2023; Madaan et al., 2023; Zheng et al., 2024; Zeng et al., 2025). Agent-based systems further extend these capabilities by incorporating planning, tool use, and collaborative debugging workflows (Grishina et al., 2025; Li et al., 2024). While these methods show promise in general programming tasks, their application to visualization remains underexplored. Existing visualization systems like LIDA (Dibia, 2023) incorporate some feedback mechanisms, but lack the systematic multi-turn correction capabilities needed for reliable cross-language deployment. Our work uniquely combines multi-language visualization generation with systematic self-debug, enabling VisCoder2 to iteratively refine code across diverse programming environments, particularly excelling in symbolic languages where execution validation is essential. Visualization Benchmark Existing visualization benchmarks focus predominantly on Python (Galimzyanov et al., 2024; Chen et al., 2024; Yang et al., 2024c; Rahman et al., 2025) or declarative specifications like Vega-Lite (Luo et al., 2021; 2025), limiting their applicability across diverse programming environments used in real-world data analysis. While general code datasets like thestack-v2 (Lozhkov et al., 2024) provide broad language coverage, they lack visualization-specific content and execution validation. Most visualization benchmarks evaluate only single-turn generation, failing to capture the iterative debugging workflows that characterize practical visualization development (Ni et al., 2025; Seo et al., 2025). Without multi-language support and multi-round evaluation, existing benchmarks cannot assess whether models can handle the diverse toolchains and iterative workflows essential for real-world visualization tasks. VisCode-Multi-679K addresses these limitations by providing the first large-scale dataset with execution-validated visualization code across twelve programming languages, while VisPlotBench enables systematic evaluation of both initial generation and multi-turn self-debug capabilities across visualization tasks in multiple programming languages."
        },
        {
            "title": "3 VISCODE-MULTI-679K: AN INSTRUCTION TUNING DATASET FOR\nVISUALIZATION ACROSS TWELVE PROGRAMMING LANGUAGES",
            "content": "Figure 2: Data construction pipeline for VisCode-Multi-679K. We collect code blocks across twelve programming languages from open-source repositories, including large-scale code corpora, synthetic visualization datasets, and domain-specific diagram collections. We validate executability and render outputs through Jupyter-based runtime checks, yielding instructions paired with images. We integrate multi-turn dialogues from Code-Feedback to provide iterative correction supervision. We present VisCode-Multi-679K, supervised instruction tuning dataset for visualization code generation and feedback-driven correction across twelve programming languages. The dataset supports 3 robust multi-language code generation and enables iterative refinement through multi-turn supervision, aligning with the needs of interactive visualization workflows. VisCode-Multi-679K unifies two complementary sources of supervision. The first is large collection of executable visualization code extracted from open source repositories across twelve programming languages, spanning diverse chart types, libraries, and real-world usage patterns. Each sample is validated for runtime execution and paired with its rendered output, ensuring reliable supervision for multi-language code generation. The second source is 66K multi-turn dialogues from the Code Feedback dataset (Zheng et al., 2024), which provide training signals for revising faulty code based on execution feedback. Although these dialogues are not exclusively visualization-oriented, they are essential for modeling realistic self-correction behaviors in iterative workflows. Figure 2 summarizes the construction pipeline of VisCode-Multi-679K, forming the raw material for four-stage process: library-based filtering, code block extraction, runtime validation, and instruction generation. The following subsections detail each stage."
        },
        {
            "title": "3.1 CODE EXTRACTION FROM PUBLIC REPOSITORIES",
            "content": "We construct VisCode-Multi-679K by drawing on three complementary open source corpora: thestack-v21 (Lozhkov et al., 2024), svg-diagrams2 (Rodriguez et al., 2025), and CoSyn-400K3 (Yang et al., 2025b; Deitke et al., 2024). These sources are complementary: the-stack-v2 provides largescale, diverse code across many languages, capturing realistic visualization embedded in general programs; svg-diagrams contributes domain-specific SVG samples focused on diagram rendering; and CoSyn-400K offers synthetic but cleanly structured visualization code spanning multiple languages. Together, they cover both natural and synthetic usage across wide range of languages and visualization styles. From each corpus, we extract code that invokes widely used visualization libraries to capture real-world plotting practices. These sources provide the raw material for pipeline with four stages: library-based filtering for each language, code block extraction, runtime validation, and instruction generation. Filtering and Code Block Extraction. For the-stack-v2 (Lozhkov et al., 2024), which contains approximately 900B tokens of code, we restrict our selection to two filtered subsets: stack-edu4 (Allal et al., 2025) and the-stack-v2-train-smol-ids5. stack-edu was curated from the-stack-v2 using classifier-based filtering strategy that retains only high-quality educational programming content. the-stack-v2-train-smol-ids is near-deduplicated subset further filtered with heuristics and spanning 17 programming languages. We first apply library-based filters on these subsets to identify approximately 5.3M visualization code candidates in Python, JavaScript, C++, TypeScript, HTML, and R. Because most examples are embedded in broader program contexts rather than selfcontained plotting examples, we use GPT-4.1-mini (OpenAI, 2025) to extract standalone plotting blocks for each language. When the original code does not include data, we inject mock inputs so that each block can execute in isolation. This structural cleaning preserves realistic visualization usage while remaining compatible with our runtime pipeline. After filtering and reconstruction, we obtain roughly 900K candidate blocks. For svg-diagrams, which contains 182K domain-specific SVG samples focused on diagrams from star-vector (Rodriguez et al., 2025), we apply regular-expression filtering to remove noisy data that lack width, height, or other essential components. This step retains about 79K candidate blocks. For CoSyn-400K, we select 408K visualization snippets across eight languages, including Python, HTML, LaTeX, SVG, Asymptote, Mermaid, LilyPond, and Vega-Lite. CoSyn-400K provides synthetic but cleanly structured code spanning wide range of styles, with well-rendered outputs and consistent structure. Unlike the-stack-v2, its Python and HTML code store logic and data separately, which requires reconstruction for runtime execution. For languages requiring reconstruction, we rebuild runnable scripts by inserting lightweight annotations such as column headers and 1hf.co/datasets/bigcode/the-stack-v2 2hf.co/datasets/starvector/svg-diagrams 3hf.co/datasets/allenai/CoSyn-400K 4hf.co/datasets/HuggingFaceTB/stack-edu 5hf.co/datasets/bigcode/the-stack-v2-train-smol-ids data row to emulate realistic data loading. When necessary, we append missing plotting function calls to ensure that each language can execute within Jupyter notebook environment. Runtime Validation. To ensure executability, we run each candidate block in isolated Jupyter environments. C++, JavaScript, and are executed in dedicated kernels, while all other languages share the Python kernel. Each block is run with nbconvert using allow-error=False to enforce strict filtering. We apply fixed timeout and terminate runs that hang or enter infinite loops via simulated keyboard interrupt. Only samples that execute successfully and generate valid image files that are non-monochrome and larger than 10KB are retained. This step produces 245K validated plotting scripts from the-stack-v2, 43K from svg-diagrams, and 322K from CoSyn-400K, each paired with its rendered output. The detailed distribution is shown in Table 1. Table 1: Distribution of visualization code samples across languages and sources. The final column reports per-language totals, and the final row reports per-source totals. Language CoSyn-400K the-stack-v2 svg-diagrams"
        },
        {
            "title": "Total",
            "content": "Python HTML LaTeX SVG JavaScript Asymptote C++ Mermaid LilyPond Vega-Lite TypeScript 66,052 75,315 124,039 2,693 - 22,539 - - 13,381 12,093 6,"
        },
        {
            "title": "Total",
            "content": "322,902 120,902 59,915 - - 28,807 - 16,776 13,437 - - - 6,315 246,152 - - - 43,928 - - - - - - - - 43,928 186,954 135,230 124,039 46,621 28,807 22,539 16,776 13,437 13,381 12,093 6,790 6, 612,982 Instruction Generation. To enable models to learn from both structural code features and rendered visual outputs, we generate natural language instructions for each validated example using GPT-4.1 (OpenAI, 2025). This process ensures that supervision captures not only code syntax but also the semantics of the corresponding visualization. To capture both data semantics and visual design, each instruction is structured into five components: (1) brief setup description specifying the programming language and visualization libraries used; (2) description of either the underlying data (for data-driven code) or the visible elements of the figure (for non-data-driven code); (3) data block that either contains copied data-generation line or two-row preview, left empty for non-data-driven cases; (4) high-level output description that conveys the intended visualization conceptually; and (5) style description capturing colors, grid layout, and other visual properties. These components are assembled into fixed template: [Output Description] [Setup] [Data/Visual Description] \"The data is shown below:\" or None [Data] or None [Style Description] This format enforces consistent prompt structure across sources and languages, ensuring that models receive unified description of the visualization target, its data, and its stylistic attributes."
        },
        {
            "title": "3.2 MULTI-TURN INSTRUCTION-FOLLOWING DIALOGUES WITH EXECUTION FEEDBACK",
            "content": "VisCode-Multi-679K further includes over 66K multi-turn dialogues from the Code-Feedback6 dataset (Zheng et al., 2024). These dialogues cover programming tasks in Python, HTML, 6hf.co/datasets/m-a-p/Code-Feedback 5 JavaScript,R, and other languages, with user instructions, model-generated code, and followup turns carrying execution feedback or revision prompts. Although not tailored to visualization, they provide essential supervision for teaching models to revise faulty code based on runtime signals and to reason over iterative interactions. We incorporate these dialogues into the instruction tuning corpus alongside single-turn samples from stack-edu, the-stack-v2, svg-diagrams, and CoSyn-400K. This integration allows models to practice both initial code generation and multi-turn refinement strategies."
        },
        {
            "title": "CODING AGENTS",
            "content": "VisPlotBench is benchmark for evaluating visualization coding agents across eight languages. Unlike prior efforts that focus on single language or specification style, VisPlotBench spans imperative libraries, declarative grammars, markup-based formats, and symbolic notations, providing standardized protocol for assessing both initial code generation and multi-round self-debug. Figure 3: Overview of VisPlotBench. The benchmark covers eight visualization languages and contains 888 diverse visualization tasks, each combining natural language instruction and rendered visual. Tasks are annotated with Visual category and Subtype, spanning 13 categories in total."
        },
        {
            "title": "4.1 OVERVIEW",
            "content": "Existing visualization benchmarks are narrow in scope: most cover single language, few chart families, and no iterative debugging. VisPlotBench fills these gaps with 888 tasks across eight languages and 13 Visual categories (Figure 4). The taxonomy spans common families such as Bars, Lines, and Scatter, while adding rarely represented ones like Hierarchies, Music, and Networks & Flows. Each task combines natural language instruction, executable code, and rendered output, enabling execution-grounded evaluation. With its executerenderscore protocol and multi-round self-debug loop, VisPlotBench provides the first systematic benchmark for assessing visualization coding agents across languages and task types. Table 2 positions VisPlotBench among representative benchmarks across four dimensions: language coverage, visual categories, self-debug support, and dataset size. Earlier resources remain narrowfocusing on Python or Vega-Lite, with limited chart types and no iterative debugging. VisCoder introduced self-debugging for PandasPlotBench, while VisPlotBench generalizes this to 6 Table 2: Comparison with existing benchmarks. VisPlotBench provides executable, multi-language tasks with natural language instructions, rendered outputs, and standardized protocol for both initial code generation and multi-round self-debugging."
        },
        {
            "title": "Coverage",
            "content": "Self-debug Visual Category"
        },
        {
            "title": "Num",
            "content": "VisEval (Chen et al., 2024) MatPlotBench (Yang et al., 2024c) nvBench (Luo et al., 2021) nvBench 2.0 (Luo et al., 2025) Text2Vis (Rahman et al., 2025) PandasPlotBench (Galimzyanov et al., 2024) PandasPlotBench-Enhanced (Ni et al., 2025) VisPlotBench (ours) Python Python VegaLite VegaLite Python Python Python 8 languages 4 11 4 5 10 10 10 13 2,524 100 25,750 7,878 1,985 175 175 888 eight languages, expands coverage to 13 categories, including Hierarchies, Music, and Networks & Flows, and standardizes evaluation for systematic cross-language assessment."
        },
        {
            "title": "4.2 DATA COLLECTION AND CURATION",
            "content": "We assemble 888 executable tasks from publicly available examples, library documentation, and high-quality code snippets across eight programming languages. The tasks span 13 Visual categories and 116 Subtypes, covering common families such as Bars, Lines, and Scatter, as well as underrepresented ones including Hierarchies, Music, and Networks & Flows. Each candidate script is executed in an isolated runtime with language-specific kernels or headless renderers. Tasks are retained only if execution succeeds and valid image is produced. We discard visually trivial outputs (e.g., near-monochrome images) and remove duplicates by hashing rendered outputs and normalizing code. This process yields pool of verified codeimage pairs compatible with our evaluation pipeline. Annotators then review verified pairs, removing low-quality items such as unreadable or degenerate plots. Each remaining task is annotated with Visual category and Subtype from the shared taxonomy shown in Table 7, with library-specific idioms added when appropriate. double-pass review with conflict resolution ensures consistency across languages."
        },
        {
            "title": "4.3 TASK CONSTRUCTION",
            "content": "Each VisPlotBench task extends the verified codeimage pair with structured natural language instruction. To ensure consistency across languages, we adopt five-part schema: Setup Plot Instruct Data Instruct Task Description Style Description. This schema provides unified template that reflects both the semantic intent and the stylistic requirements of each visualization. Setup, Plot Instruct, and Data Instruct are authored separately for each language so that tasks capture real usage, including syntax constraints, runtime notes, and data access conventions. Task Description and Style Description are generated with GPT-4.1 conditioned on the verified code and its rendered visual. The Task Description specifies the semantic intent and structural elements required for correctness, while the Style Description summarizes perceptual attributes such as layout, annotations, label formatting, and color usage. Detailed authoring templates and generation prompts are provided in Appendix A.2 and A.3. The final instruction is the concatenation of the five components, producing unified input format across languages. This design enables coding agents to condition on natural language instructions paired with minimal data previews and generate executable code that satisfies both the semantic and stylistic requirements of the task."
        },
        {
            "title": "4.4 EVALUATION PROTOCOL",
            "content": "VisPlotBench adopts standardized executerenderscore pipeline. Each submission is executed in an isolated runtime with language-specific kernels or headless renderers, subject to strict timeouts 7 and log capture. The process outputs three artifacts: rendered image, execution log and metadata record, supporting execution-grounded and judgment-based evaluation. Evaluation metrics extend those of PandasPlotBench and VisCoder. Execution Pass Rate checks whether the code runs without error and produces valid visualization. Task Score measures instruction compliance using an LLM judge guided by semantic and structural rubrics, and Visual Score assesses perceptual similarity between generated and reference outputs. Both follow the GPTbased judging protocol of PandasPlotBench. To assess iterative refinement, VisPlotBench includes multi-round self-debug protocol. Unresolved tasks are revisited for up to three rounds, where the model receives the instruction, its prior code, and an excerpt of the execution log before producing revision. The final score reflects the best attempt, mirroring real-world correction loops and enabling systematic evaluation of both baseline generation and feedback-driven recovery."
        },
        {
            "title": "5 EXPERIMENT SETUP",
            "content": "Training Setup. We fine-tune Qwen2.5-Coder-Instruct (Hui et al., 2024) at four parameter scales: 3B, 7B, 14B, and 32B. This setup allows us to assess the generalizability of VisCode-Multi-679K across capacities. All models are trained for 3 epochs with learning rate of 5 106, warm-up ratio of 0.05, and cosine scheduler. We perform full-parameter tuning in bfloat16 precision on 8H100 GPUs with total batch size of 64, using the SWIFT infrastructure (Zhao et al., 2024). Evaluation Setup. All evaluations are conducted on VisPlotBench using the standardized protocol in Section 4.4. We report three metrics: Execution Pass Rate, Task Score, and Visual Score, capturing executability, semantic alignment, and perceptual similarity. Models are also tested under the self-debug protocol with up to three rounds of correction based on execution feedback, assessing both baseline generation and recovery through iterative refinement."
        },
        {
            "title": "6 MAIN RESULTS",
            "content": "We evaluate both proprietary and open-source models on VisPlotBench to compare execution reliability across parameter scales, programming languages, and evaluation modes. Proprietary references include GPT-4.1 (OpenAI, 2025) and its lighter variant GPT-4.1-mini (OpenAI, 2025), while open-source baselines include DeepSeek-Coder (Guo et al., 2024b), DeepSeek-CoderV2 (Zhu et al., 2024), Qwen2.5-Coder (Hui et al., 2024), and VisCoder (Ni et al., 2025). Our VisCoder2 models are trained on VisCode-Multi-679K using Qwen2.5-Coder backbones at 3B, 7B, 14B, and 32B scales."
        },
        {
            "title": "6.1 OVERALL COMPARISON",
            "content": "Table 3 summarizes execution pass rates for all models across eight visualization languages and overall averages. The following analysis examines differences between proprietary and open-source models, variation across languages, and the relative advantages of VisCoder2 under both default and self-debug evaluation modes. Proprietary Models Remain Stronger. GPT-4.1 achieves 63.4% overall, the highest among reference models, and GPT-4.1-mini follows closely. Both perform strongly on standardized declarative or markup languages such as Vega-Lite, SVG, and HTML, all above 84%. In contrast, instructiontuned open-source models remain far behind. At the 7B scale, Qwen2.5-Coder reaches only 51.2% overall, with fewer than 30% on LaTeX and just 5.5% on LilyPond. Previous VisCoder variants improve Python performance but fail to generalize across languages. These results underline the substantial gap between proprietary and open-source models. Cross-Language Variation. Performance differs sharply across visualization languages. Vega-Lite and HTML are close to saturation for most models, while Python shows steady gains with scale. By contrast, symbolic and compiler-dependent languages remain the most difficult. Even GPT-4.1 achieves less than 45% on LilyPond and under 25% on Asymptote, and open-source 8 Table 3: Overall execution pass rate (%) of selected models on the VisPlotBench benchmark. The best-performing model in each scale is shown in bold, and the second best is underlined. Model GPT-4.1 GPT-4.1 + Self Debug GPT-4.1-mini GPT-4.1-mini + Self Debug DeepSeek-Coder-1.3B-Ins. Qwen2.5-Coder-3B-Ins. VisCoder-3B VisCoder2-3B VisCoder2-3B + Self Debug DeepSeek-Coder-6.7B-Ins. Qwen2.5-Coder-7B-Ins. VisCoder-7B VisCoder2-7B VisCoder2-7B + Self Debug DeepSeek-Coder-V2-Lite-Ins. Qwen2.5-Coder-14B-Ins. VisCoder2-14B VisCoder2-14B + Self Debug DeepSeek-Coder-33B-Ins. Qwen2.5-Coder-32B-Ins. VisCoder2-32B VisCoder2-32B + Self Debug Exec Pass Overall Python (196) Vega-Lite (129) LilyPond (55) Mermaid (131) SVG (65) LaTeX (112) Asymptote (92) HTML (108) 63.4 82.4 58.9 81.1 32.3 45.8 56.1 67.7 70.0 46.4 51.2 57.2 70.9 76.4 55.3 59. 72.1 78.4 54.3 57.5 73.1 82.4 64.3 84.2 64.8 80.6 29.1 34.2 45.4 56.1 63. 39.3 41.3 58.2 64.8 77.0 47.5 50.0 65.3 78.1 58.2 50.5 65.3 81. 84.5 96.1 84.5 96.9 43.6 63.6 16.4 56.4 3B Scale 53.5 68.2 83.7 83.0 84.5 30.9 3.6 21. 50.9 52.7 7B Scale 79.8 76.0 71.3 83.0 84.5 7.3 5.5 23.6 69.1 72. 14B Scale 75.2 83.0 93.0 94.6 49.1 25.5 54.6 63.6 32B Scale 90.7 83.0 94.6 96.1 30.9 30.9 56.4 69.1 68.7 93.9 51.9 94.7 63.4 74.1 75. 76.3 76.3 91.6 77.9 77.1 78.6 84.7 69.5 74.8 81.7 86.3 87.0 71. 87.0 90.1 95.4 96.9 95.4 96.9 7.7 75.4 76.9 87.7 87.7 96.9 92.3 93.9 96.9 96. 93.9 98.5 89.2 90.8 92.3 93.9 81.5 86.2 31.3 66.1 29.5 58.9 4.5 17.9 23. 36.6 38.4 18.8 25.9 25.9 39.3 42.9 29.5 30.4 42.0 45.5 24.1 29. 42.9 61.6 21.7 46.7 23.9 48.9 13.0 18.5 30.4 62.0 63.0 0.0 13.0 17.4 64.1 70. 20.7 25.0 56.5 66.3 21.7 17.4 58.7 71.7 89.8 97.2 86.1 100.0 36.1 62.0 79. 93.5 94.4 22.2 64.8 75.9 82.4 84.3 64.8 83.3 90.7 94.4 12.0 78. 91.7 93.5 baselines fall much lower. This uneven landscape highlights that progress on symbolic grammars is the key bottleneck for reliable multi-language visualization. VisCoder2 Advantage. Across all scales, VisCoder2 consistently outperforms size-matched open-source baselines. At 32B, it improves overall execution pass rate by approximately 15 points compared with Qwen2.5-Coder and reaches parity with GPT-4.1. The only consistent shortfall is on SVG, where VisCoder2 trails the strongest baseline by over 10 points. Overall, VisCoder2 is the first open-source model to match proprietary reliability on executable visualization tasks. Effect of Self-Debug. Iterative correction consistently improves execution reliability across model families and scales. Proprietary models benefit strongly, and VisCoder2 follows the same trend: at larger scales, overall execution rises by nearly ten points when self-debugging is enabled. The effect is especially pronounced for symbolic and compiler-dependent languages such as LilyPond, LaTeX, and Asymptote, where fragile syntax or compilation errors dominate. Self-debugging enables the model to repair these shallow but frequent failures, allowing models to resolve previously intractable failures into valid outputs. This demonstrates that feedback-driven refinement is not just marginal improvement but critical mechanism for tackling the hardest visualization languages."
        },
        {
            "title": "6.2 TASK AND VISUAL SCORE ANALYSIS",
            "content": "We analyze Task Score and Visual Score on three representative languages that highlight different behaviors, as shown in Table 4: LaTeX illustrates executionsemantics mismatch, LilyPond shows the largest gains on symbolic grammars, and SVG exposes modellibrary sensitivity where semantic and perceptual signals diverge. Results for all languages and scales are provided in Appendix C. LaTeX: ExecutionSemantics Mismatch. Models often capture the intended structure of figure but fail to compile reliably. For example, GPT-4.1 improves from 31.3% to 66.1% execution pass rate with Self-Debug, while task scores remain around 50 even when execution fails. VisCoder2 9 Table 4: Performance of selected languages on the VisPlotbench benchmark. For each model, we report (1) execution pass rate (Exec Pass), (2) mean visual and task scores (Mean), and (3) the proportion of samples scoring at least 75 (Good). The best-performing model in each scale is shown in bold, and the second best is underlined. Model GPT-4.1 GPT-4.1 + Self Debug GPT-4.1-mini GPT-4.1-mini + Self Debug Qwen2.5-Coder-7B-Instruct VisCoder2-7B VisCoder2-7B + Self Debug Qwen2.5-Coder-32B-Instruct VisCoder2-32B VisCoder2-32B + Self Debug Exec Pass 31.3 66.1 29.5 58.9 25.9 39.3 42.9 29.5 42.9 61. LaTeX (112) Mean vis task Good(75) task vis Exec Pass LilyPond (55) Mean vis task Good(75) task vis 18 38 21 35 11 15 16 14 20 26 56 25 50 15 23 24 25 35 45 13% 25% 43.6 25% 51% 63.6 18% 25% 16.4 23% 49% 56.4 6% 5.5 8% 6% 15% 69.1 6% 15% 72.7 9% 27% 30.9 11% 34% 56.4 14% 42% 69. 14 17 2 14 0 16 17 5 14 16 38 54 12 42 3 52 55 22 39 5% 36% 5% 53% 0% 11% 0% 35% 0% 4% 2% 45% 2% 45% 2% 18% 2% 27% 2% 35% SVG (65) Exec Pass Mean vis task Good(75) task vis 95.4 96.9 95.4 96.9 92.3 96.9 96.9 93.9 81.5 86. 45 45 41 42 23 34 34 34 33 34 92 93 86 88 58 73 73 81 68 14% 94% 14% 95% 11% 86% 11% 88% 0% 40% 3% 62% 3% 62% 3% 75% 11% 63% 11% 66% raises execution and task scores compared with baselines, but compilation errors remain frequent. This pattern indicates that semantic alignment does not always translate into successful rendering. LilyPond: Symbolic Grammar Gains. VisCoder2 delivers the clearest advantage on symbolic languages. At 7B, Qwen2.5-Coder executes only 5.5% of tasks, while VisCoder2 reaches 69.1% and further improves with Self-Debug. The proportion of examples with task scores above 75 also increases by more than tenfold. These results show that targeted coverage of symbolic grammars in VisCode-Multi-679K translates directly into reliable generation and semantic adherence. SVG: Sensitivity to Rendering Libraries. Execution success is high across most models, yet visual scores lag behind task scores. For instance, GPT-4.1 with Self-Debug achieves 95.4% execution and task score near 90, but the average visual score is below 50. VisCoder2 performs competitively but trails Qwen2.5 on execution at larger scales (81.5% versus 93.9% at 32B). These discrepancies suggest that evaluation on SVG is strongly influenced by library-specific rendering details rather than semantic understanding alone."
        },
        {
            "title": "6.3 ERROR ANALYSIS",
            "content": "To better understand failure modes across languages, we analyze execution errors before and after self-debug. Many language-specific exceptions, such as FunctionSignatureError in Asymptote or MarkupError in LilyPond, were merged into four broader categories for clarity: Structural Errors (syntax or parsing), Type & Interface Errors (invalid calls or arguments), Semantic / Data Errors (mismatched variables or values), and Runtime / Environment Errors (renderer or package issues). Representative results for VisCoder2-32B are shown in Table 5, with full breakdowns in Appendix E. Table 5: Representative error transitions for VisCoder2-32B across eight visualization languages. Each cell shows error counts from initial failure to the final self-debug round (X Y). dash indicates the error type does not occur for that language."
        },
        {
            "title": "Error Category",
            "content": "Python Vega-Lite LilyPond Mermaid"
        },
        {
            "title": "Asymptote HTML",
            "content": "Structural Errors Type & Interface Semantic / Data Runtime / Env. 1 1 13 3 19 8 - 2 1 2 1 - 2 2 14 10 5 2 - - 12 9 - - - 8 7 - - - 10 4 - 28 23 27 6 9 3 - 15 11 8 6 - - - 3 2 Effective recovery on structural and interface errors. Self-debug reduces shallow errors such as missing tokens or invalid arguments across multiple languages. For example, Python interface errors fall from 13 to 3 (Figure 6), and structural errors in LilyPond decrease from 14 to 10 (Figure 12). Mermaid and Asymptote show the same trend, with syntax and function signature 10 errors shrinking after correction (Figure 15). These cases benefit from explicit diagnostic traces, making them relatively easy to fix through iterative feedback. Persistent failures in semantic and runtime errors. Errors involving semantics or execution In LaTeX, undefined variables decrease only slightly environments remain difficult to resolve. (28 to 23), and Asymptote variable mismatches improve only marginally (15 to 11) (Figure 24). Renderer failures such as Vega-Lite rendering errors (2 to 2) and HTML request failures (3 to 2) often persist across all rounds (Figure 28). These errors require deeper reasoning over symbolic grammars and runtime contexts, which current self-debug protocols cannot fully capture. Symbolic languages and renderer-sensitive environments therefore remain the dominant bottlenecks, pointing to the need for grammar-aware training objectives and more robust runtime integration."
        },
        {
            "title": "6.4 TRAINING DATA ABLATION",
            "content": "To disentangle the contribution of each data source, we conduct controlled ablation study using Qwen2.5-Coder-7B as the base model. Separate models are fine-tuned on individual subsets of The-Stack-V2, CoSyn, StarVector, and Code-Feedback, under the same instructiontuning setup as the full configuration. We report execution pass rates on VisPlotBench in both default and self-debug modes, with comparisons to the untuned Qwen2.5-Coder-7B baseline and the full VisCode-Multi-679K model  (Table 6)  . Table 6: Execution pass rates of Qwen2.5-Coder-7B models trained on individual subsets of VisCode-Multi-679K. Each model is evaluated under both default () and self-debug () modes. Model Self-Debug Overall Python Vega-Lite LilyPond Mermaid SVG LaTeX Asymptote HTML Qwen2.5-Coder-7B-Ins. + The-Stack-V2-246K + CoSyn-323K + StarVector-44K + Code-Feedback-66K + Full VisCode-Multi-679K 51.2 59. 49.0 56.5 59.2 62.2 40.1 44.5 55.2 63.1 70.9 76.4 41.3 61. 47.5 58.2 25.5 31.1 43.4 53.6 47.5 62.2 64.8 77.0 76.0 77. 81.4 83.7 83.7 84.5 72.1 73.6 78.3 80.6 83.0 84.5 5.5 5. 7.3 10.9 65.5 69.1 5.5 7.3 20.0 21.8 69.1 72.7 77.9 79. 69.5 73.3 57.3 61.1 67.9 70.2 81.7 81.7 78.6 84.7 92.3 92. 84.6 84.6 100.0 100.0 16.9 18.5 92.3 92.3 96.9 96.9 25.9 30. 0.9 31.3 36.6 38.4 10.7 13.4 27.7 38.4 39.3 42.9 13.0 20. 17.4 18.5 56.5 62.0 13.0 19.6 17.4 23.9 64.1 70.7 64.8 76. 64.8 65.7 91.7 91.7 47.2 50.0 65.7 83.3 82.4 84.3 Natural vs. Synthetic. Training on The-Stack-V2 alone yields limited improvements and even degrades symbolic languages such as LaTeX, reflecting the sparsity of clean visualization signals in general-purpose code. By contrast, CoSyn delivers large gains on symbolic and grammar-sensitive languages, with execution rates on LilyPond and Asymptote rising by over 60 points compared to the baseline. This contrast shows that large-scale synthetic data provides valuable structural coverage that complements natural code. Domain vs. Multi-turn. The StarVector subset contributes primarily to SVG but is too small to improve overall performance. In contrast, Code-Feedback does not drastically shift baseline pass rates but produces consistent gains under self-debug, lifting overall execution from 55.2% to 63.1%. This demonstrates that multi-turn dialogue data provides critical supervision for recovery through iterative correction, rather than improving one-shot generation. Full Dataset Synergy. Combining all subsets yields the strongest model. With VisCode-Multi679K, the overall pass rate reaches 70.9% in default mode and 76.4% with self-debug, substantially surpassing both the untuned baseline and any single-source variant. These results confirm that the datasets diverse compositionbalancing natural, synthetic, domain-specific, and iterative datais essential for building robust multi-language visualization coding agents."
        },
        {
            "title": "7 CONCLUSION",
            "content": "Reliable visualization coding goes beyond single-pass generation: it requires competence across diverse languages and the ability to refine outputs iteratively in response to execution feedback. Existing datasets and benchmarks lack these capabilities, limiting progress toward practical agents for real-world workflows. We addressed these gaps through three contributions. First, we introduced VisCode-Multi-679K, large-scale instruction tuning dataset that unifies executable visualization code across twelve languages with multi-turn feedback dialogues. Second, we built VisPlotBench, benchmark covering eight visualization languages under standardized executerenderscore protocol, with tasks spanning 13 categories and 116 subtypes. Third, we trained the VisCoder2 model family on these resources, showing that it consistently outperforms open-source baselines and approaches proprietary models in execution reliability. Our experiments highlight two insights. Broad multi-language coverage is essential: symbolic and compiler-dependent languages such as LaTeX, LilyPond, and Asymptote remain challenging, yet progress on them is decisive for true generalization. Iterative refinement further proves indispensable: self-debug delivers large gains across models, especially on languages where structural and semantic errors are common. Taken together, VisCode-Multi-679K, VisPlotBench, and VisCoder2 establish the first systematic framework for building and evaluating visualization coding agents. We believe these resources can accelerate the development of agents that are not only multi-language but also capable of realistic correction loops, pushing toward reliable coding assistants for data analysis, reporting, and beyond."
        },
        {
            "title": "LIMITATION",
            "content": "While VisCode2 represent significant step toward building reliable multi-language visualization coding agents, the VisCode-Multi-679K is still imbalanced: common ecosystems such as Python and Vega-Lite are well represented, whereas symbolic and domain-specific languages have far fewer examples. This uneven distribution may bias models toward dominant languages and limit generalization in low-resource cases. VisPlotBench currently supports eight languages; expanding benchmark coverage to broader set of visualization frameworks would enable more comprehensive evaluation and reduce the risk of overfitting to the existing taxonomy."
        },
        {
            "title": "REFERENCES",
            "content": "Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martın Blazquez, Guilherme Penedo, Lewis Tunstall, Andres Marafioti, Hynek Kydlıˇcek, Agustın Piqueres Lajarın, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clementine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf. Smollm2: When smol goes big data-centric training of small language model, 2025. URL https://arxiv.org/abs/2502.02737. Nan Chen, Yuge Zhang, Jiahang Xu, Kan Ren, and Yuqing Yang. Viseval: benchmark for data visualization in the era of large language models. IEEE Transactions on Visualization and Computer Graphics, 2024. Xinyun Chen, Maxwell Lin, Nathanael Scharli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. ArXiv preprint, abs/2409.17146, 2024. URL https://arxiv.org/abs/2409.17146. Victor Dibia. Lida: tool for automatic generation of grammar-agnostic visualizations and infographics using large language models. ArXiv preprint, abs/2303.02927, 2023. URL https: //arxiv.org/abs/2303.02927. 12 Nuno Fachada, Daniel Fernandes, Carlos M. Fernandes, Bruno D. Ferreira-Saraiva, and Joao P. Matos-Carvalho. Gpt-4.1 sets the standard in automated experiment design using novel python libraries. arXiv preprint arXiv:2508.00033, 2025. Timur Galimzyanov, Sergey Titov, Yaroslav Golubev, and Egor Bogomolov. Drawing pandas: benchmark for llms in generating plotting code. ArXiv preprint, abs/2412.02764, 2024. URL https://arxiv.org/abs/2412.02764. Kanika Goswami, Puneet Mathur, Ryan Rossi, and Franck Dernoncourt. Plotgen: Multi-agent llmbased scientific data visualization via multimodal feedback. arXiv preprint arXiv:2502.00988, 2025. Anastasiia Grishina, Vadim Liventsev, Aki Harma, and Leon Moonen. Fully autonomous programming using iterative multi-agent debugging with large language models. ACM Transactions on Evolutionary Learning, 5(1):137, 2025. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming the rise of code intelligence. ArXiv preprint, abs/2401.14196, 2024a. URL https://arxiv. org/abs/2401.14196. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming the rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024b. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. Qwen2. 5-coder technical report. ArXiv preprint, abs/2409.12186, 2024. URL https://arxiv.org/abs/2409.12186. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. Hao Li, Haoxiang Zhang, and Ahmed Hassan. The rise of ai teammates in software engineering (se) 3.0: How autonomous coding agents are reshaping software engineering. arXiv preprint arXiv:2507.15003, 2025. Jierui Li, Hung Le, Yingbo Zhou, Caiming Xiong, Silvio Savarese, and Doyen Sahoo. Codetree: Agent-guided tree search for code generation with large language models. ArXiv preprint, abs/2411.04329, 2024. URL https://arxiv.org/abs/2411.04329. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024. Tianqi Luo, Chuhan Huang, Leixian Shen, Boyan Li, Shuyu Shen, Wei Zeng, Nan Tang, and Yuyu Luo. nvbench 2.0: benchmark for natural language to visualization under ambiguity. arXiv preprint arXiv:2503.12880, 2025. Yuyu Luo, Jiawei Tang, and Guoliang Li. nvbench: large-scale synthesized dataset for crossdomain natural language to visualization task. arXiv preprint arXiv:2112.12926, 2021. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124, 2023. 13 Yuansheng Ni, Ping Nie, Kai Zou, Xiang Yue, and Wenhu Chen. Viscoder: Fine-tuning llms for executable python visualization code generation. arXiv preprint arXiv:2506.03930, 2025. OpenAI. Introducing gpt-4.1 in the api. https://openai.com/index/gpt-4-1/, 2025. URL https: //openai.com/index/gpt-4-1/. Mizanur Rahman, Md Tahmid Rahman Laskar, Shafiq Joty, and Enamul Hoque. Text2vis: challenging and diverse benchmark for generating multimodal visualizations from text. arXiv preprint arXiv:2507.19969, 2025. Maxime Robeyns, Martin Szummer, and Laurence Aitchison. self-improving coding agent. arXiv preprint arXiv:2504.15228, 2025. Juan A. Rodriguez, Abhay Puri, Shubham Agarwal, Issam H. Laradji, Pau Rodriguez, Sai Rajeswar, David Vazquez, Christopher Pal, and Marco Pedersoli. Starvector: generating scalable vector graphics code from images and text. arXiv preprint arXiv:2312.11556, 2025. Arvind Satyanarayan, Dominik Moritz, Kanit Wongsuphasawat, and Jeffrey Heer. Vega-lite: grammar of interactive graphics. IEEE transactions on visualization and computer graphics, 23 (1):341350, 2016. Wonduk Seo, Seungyong Lee, Daye Kang, Zonghao Yuan, and Seunghyun Lee. Vispath: Automated visualization code synthesis via multi-path reasoning and feedback-driven optimization. ArXiv preprint, abs/2502.11140, 2025. URL https://arxiv.org/abs/2502.11140. Chengyue Wu, Yixiao Ge, Qiushan Guo, Jiahao Wang, Zhixuan Liang, Zeyu Lu, Ying Shan, and Ping Luo. Plot2code: comprehensive benchmark for evaluating multi-modal large language models in code generation from scientific plots. ArXiv preprint, abs/2405.07990, 2024. URL https://arxiv.org/abs/2405.07990. Liwenhan Xie, Chengbo Zheng, Haijun Xia, Huamin Qu, and Chen Zhu-Tian. Waitgpt: Monitoring and steering conversational llm agent in data analysis with on-the-fly code visualization. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology, pp. 114, 2024. Cheng Yang, Chufan Shi, Yaxin Liu, Bo Shui, Junjie Wang, Mohan Jing, Linran Xu, Xinyu Zhu, Siheng Li, Yuxiang Zhang, et al. Chartmimic: Evaluating lmms cross-modal reasoning capability via chart-to-code generation. arXiv preprint arXiv:2406.09961, 2024a. John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing and benchmarking interactive coding with execution feedback. Advances in Neural Information Processing Systems, 36:2382623854, 2023. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024b. Yue Yang, Ajay Patel, Matt Deitke, Tanmay Gupta, Luca Weihs, Andrew Head, Mark Yatskar, Chris Callison-Burch, Ranjay Krishna, Aniruddha Kembhavi, and Christopher Clark. Scaling textrich image understanding via code-guided synthetic multimodal data generation. arXiv preprint arXiv:2502.14846, 2025a. Yue Yang, Ajay Patel, Matt Deitke, Tanmay Gupta, Luca Weihs, Andrew Head, Mark Yatskar, Chris Callison-Burch, Ranjay Krishna, Aniruddha Kembhavi, et al. Scaling text-rich image understanding via code-guided synthetic multimodal data generation. ArXiv preprint, abs/2502.14846, 2025b. URL https://arxiv.org/abs/2502.14846. Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun Yan, Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, et al. Matplotagent: Method and evaluation for llm-based agentic scientific data visualization. ArXiv preprint, abs/2402.11453, 2024c. URL https://arxiv. org/abs/2402.11453. 14 Huaye Zeng, Dongfu Jiang, Haozhe Wang, Ping Nie, Xiaotong Chen, and Wenhu Chen. Acecoder: Acing coder rl via automated test-case synthesis. ArXiv preprint, abs/2502.01718, 2025. URL https://arxiv.org/abs/2502.01718. Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou, and Yingda Chen. Swift:a scalable lightweight infrastructure for fine-tuning, 2024. URL https://arxiv.org/abs/2408.05517. Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. Opencodeinterpreter: Integrating code generation with execution and refinement. ArXiv preprint, abs/2402.14658, 2024. URL https://arxiv.org/abs/2402.14658. Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024."
        },
        {
            "title": "A Prompt Used and Instruct Design",
            "content": "A.1 Prompt Used in VisCode-Multi-679K . . . . . . . . . . . . . . . . . . . . . . . . A.2 Prompt Used in VisPlotBench . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Instruct Design in VisPlotBench Evaluation . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C Breakdown Main Results",
            "content": "C.1 Python, Vega-Lite & Lilypond . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Mermaid, SVG & LaTeX . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Asymptote & HTML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Breakdown Self-Debug Results D.1 Python & Vega-Lite . . D.2 LilyPond & Mermaid . D.3 SVG & LaTeX . . . . D.4 Asymptote & HTML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "E Breakdown Error Type Results",
            "content": "E.1 Python . . . E.2 Vega-Lite . E.3 Lilypond . E.4 Mermaid . E.5 SVG . . E.6 LaTeX . . . . . . . E.7 Asymptote . E.8 HTML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "F Case Study",
            "content": "F.1 Python: Successful Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 Python: Self-Debug Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Python: Self-Debug Failed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.4 Vega-Lite: Successful Generation . . . . . . . . . . . . . . . . . . . . . . . . . . F.5 Vega-Lite: Self-Debug Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . F.6 Vega-Lite: Self-Debug Failed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.7 Lilypond: Successful Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . F.8 Lilypond: Self-Debug Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . F.9 Lilypond: Self-Debug Failed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 18 22 26 28 28 29 31 31 32 33 34 35 35 35 36 36 37 37 38 38 39 41 42 43 44 45 16 F.10 Mermaid: Successful Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . F.11 Mermaid: Self-Debug Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . F.12 Mermaid: Self-Debug Failed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.13 SVG: Successful Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.14 SVG: Self-Debug Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.15 SVG: Self-Debug Failed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.16 LaTeX: Successful Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.17 LaTeX: Self-Debug Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.18 LaTeX: Self-Debug Failed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.19 Asymptote: Successful Generation . . . . . . . . . . . . . . . . . . . . . . . . . . F.20 Asymptote: Self-Debug Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . F.21 Asymptote: Self-Debug Failed . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.22 HTML: Successful Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.23 HTML: Self-Debug Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.24 HTML: Self-Debug Failed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 49 50 51 52 53 55 56 57 58 59"
        },
        {
            "title": "A PROMPT USED AND INSTRUCT DESIGN",
            "content": "In this section, we present the prompts used during the construction of VisCode-Multi-679K and VisPlotBench. A.1 PROMPT USED IN VISCODE-MULTI-679K"
        },
        {
            "title": "Code Extraction Prompt",
            "content": "Model: GPT-4.1-mini # LANGUAGE= [Python, JavaScript, TypeScript, C++, R, HTML] # LANG BULLET = { Python: Write single .py file. No external files or internet access. Use helper libraries only if truly required for the chart. JavaScript: Write single .js file. Add necessary imports for any required libraries. TypeScript: Write single .ts file. Use ES6 module syntax (import/export). The code should not require module bundler. C++: Write single .cpp file with main(). Program must exit automatically. Do not load external assets. If linking is needed, add one build command as comment. R: Write single .R file. Use library(Library). Create mock data if needed. No external files. HTML: Write single HTML file. Include <script> tag and the needed DOM element for the chart. All code runs in the browser. } You are {LANGUAGE} visualization code extraction agent. Given {LANGUAGE} code snippet and the used library, your task is to extract minimal yet runnable {LANGUAGE} snippet that reflects how the library is actually used for visual output. Guidelines: - Keep only the logic needed for the visual output; remove unrelated code. - If the library is not used for rendering/drawing/plotting in Code, return null. - If inputs/assets are missing, create semantically relevant mock data that makes the output meaningful. - Preserve the main intent, API pattern, key parameters, and style** from the original code; simplify when it improves clarity, and avoid adding new wrappers or layers that are not essential. - Make the visual output clear and professional: use appropriate visual cues (titles/labels/legends when applicable); keep layout readable. - Ensure the snippet runs standalone in minimal environment and terminates automatically (no user input required). - {LANG BULLET} - If the library is unused or information is insufficient, return null. Used Library: {used libs} Code: {code} Instruct Generation Prompt: the-stack-v2& svg-diagrams Model: GPT-4.1 # LANGUAGE= [Python, JavaScript, TypeScript, C++, R, HTML, SVG] You are given {LANGUAGE} code snippet that renders an image. rendered image of the resulting output is provided at the end. Your task is to infer and clearly describe the purpose, structure, and style of this image. Break your response into the following five parts: 1. Setup (state the language and rendering context, including any tools or libs implied). 2. Data/Visual Description - If the code is data-driven: summarize the inputs the code relies on and any shaping operations. - If the code is not data-driven: summarize the visible content of the image. 3. Data Generation (the data-generation lines copied verbatim, or None if not applicable). 4. Output Description (omit language constructs; start with Generate... or Create..., and describe the final image conceptually). 5. Style Description (describe appearance and layout without naming language constructs). Each part must start on new line, numbered 1 through 5. Use plain text only; no markdown. Code: {code} Image: Instruct Generation Prompt: CoSyn-400K Model: GPT-4.1 # FOR DATA-DRIVEN LANGUAGES # LANGUAGE= [Python, Vega-Lite, HTML, LilyPond, Mermaid] You are given {LANGUAGE} code snippet that produces rendered visual. rendered image of the resulting output is provided at the end. Your task is to infer and clearly describe the purpose and structure of this visual. Break your response into the following four parts: 1. Setup (state the {LANGUAGE} and its rendering context, including any tools or specification frameworks implied). 2. Data/Content Description (summarize the input fields, entities, or content the code relies on, including any shaping or transformation operations). 3. Output Description (omit library, directive, or element names; start with Generate... or Create..., and describe the visual conceptually). 4. Style Description (describe appearance and layout without naming language constructs). Each part must start on new line, numbered 1 through 4. Use plain text only; no markdown. Code: {code} Image: 19 Instruct Generation Prompt: CoSyn-400K Model: GPT-4.1 # FOR NONE DATA-DRIVEN LANGUAGES # LANGUAGE= [Asymptote, SVG] You are given {LANGUAGE} code snippet that renders an image. rendered image of the resulting output is provided at the end. Your task is to infer and clearly describe the purpose and structure of this image. Break your response into the following four parts: 1. Setup (state the {LANGUAGE} and its rendering context). 2. Visual Elements (summarize the visible components of the image). 3. Output Description (omit language constructs; start with Generate... or Create..., and describe the image conceptually). 4. Style Description (describe appearance and layout without naming language constructs). Each part must start on new line, numbered 1 through 4. Use plain text only; no markdown. Code: {code} Image: 20 A.2 PROMPT USED IN VISPLOTBENCH Task & Style Description Generation Prompt: Model: GPT-4.1 # LANGUAGE= [Python, Vega-Lite, HTML, LilyPond, Mermaid, Asymptote, HTML, SVG] You are given {LANGUAGE} code that produces rendered visual. rendered image of the resulting output is provided at the end. Your task is to infer and clearly describe the purpose and structure of this visual. Break your response into the following two parts: 1. Task Description (omit libraries and specific function names at this part, start with Generate... or Create...). 2. Style Description (describe appearance and layout without using specification keywords). Each part must start on new line, numbered 1 through 2. Use plain text only; no markdown. CODE: code IMAGE: Vis & Task Judge Prompt # Visual Judge You are an excellent judge at evaluating visualization plots between model generated plot and the ground truth. You will be giving scores on how well it matches the ground truth plot. The generated plot will be given to you as the first figure. Another plot will be given to you as the second figure, which is the desired outcome of the user query, meaning it is the ground truth for you to reference. Please compare the two figures head to head and rate them. Suppose the second figure has score of 100, rate the first figure on scale from 0 to 100. Scoring should be carried out in the following aspect: the more Plot correctness: compare closely between the generated plot and the ground truth, resemblance the generated plot has compared to the ground truth, the higher the score. The score should be proportionate to the resemblance between the two plots. Ignore color matching. consider them matching. Capture the resemblance of the main idea of the plot. Only rate the first figure, the second figure is only for reference. After scoring from the above aspect, please give final score. Do not write anything else. The final score is preceded by the [FINAL SCORE] token. For example [FINAL SCORE]: 40 If the plots present the same information but are made in different colors, # Task Judge You are an excellent judge at evaluating visualization plot according to the given task. You will be giving scores on how well plot image matches the task. The generated plot will be given to you as an image. Please score how well plot matches the task. Score it on scale from 0 to 100. Scoring should be carried out in the following aspect: Task adherence: how the plot corresponds to the task given below (begins from [PLOT TASK] token). After scoring from the above aspect, please give final score. Do not write anything else. The final score is preceded by the [FINAL SCORE] token. For example [FINAL SCORE]: 40 21 A."
        },
        {
            "title": "Python Instruct",
            "content": "# SYSTEM PROMPT: You are helpful programming assistant proficient in Python. All answers must be enclosed in block python SOME CODE containing one complete Python code. Example minimal spec: python print(hello world) # SETUP INSTRUCT: Use Python programming language. Import essential libraries. The essential libraries needed are pandas for managing dataframes and [USED LIB] with its subsidiary libraries for plotting. Ensure importing numpy as np and scipy if they are used in program. DO NOT use or import other visualization libraries. # PLOT INSTRUCT: Write code to build plot of dataframe according to following instructions. Write code that returns plot, not just function declaration. Do not write explanations, just code enclosed in codeblock. Important reasoning write in comments to the code. Make sure that all used libraries and functions are imported. # DATA INSTRUCT: Load df dataframe by single line df = pd.read csv(data.csv). DO NOT alter df dataframe columns or add columns. This df dataframe should remain intact. The metadata of the dataframe is following: Vega-Lite Instruct # SYSTEM PROMPT: You are helpful programming assistant proficient in Vega-Lite. All answers must be enclosed in block vegalite SOME CODE containing one complete Vega-Lite specification. Example minimal spec: vegalite { \"$schema\":\"https://vega.github.io/schema/vega-lite/v6.json\", \"data\":{\"values\":[{\"hello\":\"world\"}]}, \"mark\":\"text\", \"encoding\":{\"text\":{\"field\":\"hello\",\"type\":\"nominal\"}} } # SETUP INSTRUCT: Setup. Use the Vega-Lite v6 JSON schema and produce exactly one valid Vega-Lite specification as single top-level JSON object that MUST include the $schema property. Do not output raw Vega specifications, imperative code, language-specific wrappers, or references to other plotting libraries; use only Vega-Lite encodings, transforms, and configuration required by the plot. # PLOT INSTRUCT: Write code to build plot of the dataset according to the following instructions. Return one complete Vega-Lite JSON specification enclosed in single code block, and do not include explanations or comments. Use only the constructs permitted by the setup, ensure that all referenced field names exactly match the dataset metadata, and do not rename or drop columns; only use non-destructive Vega-Lite transforms if required. # DATA INSTRUCT: Data description. Load the dataset by setting data: url: data.csv. Do not create synthetic data or load inline data values. The metadata of the dataset is following:"
        },
        {
            "title": "Mermaid Instruct",
            "content": "# SYSTEM PROMPT: You are helpful programming assistant proficient in Mermaid diagrams. All answers must be enclosed in block mermaid SOME CODE containing one complete Mermaid specification. Example minimal spec: mermaid graph TD A[Start] -rightarrow B{Condition} -rightarrowYes C[Do something] -rightarrowNo D[Stop] # SETUP INSTRUCT: Setup. Use Mermaid syntax only and produce exactly one valid Mermaid diagram definition. Do not output explanations, comments outside the code block, or code in other languages or formats. Do not split the diagram into multiple blocks. Ensure the code can be rendered directly by mermaid-cli (mmdc). # PLOT INSTRUCT: Write diagram according to the following instructions. Do not include explanations or natural language outside the code block. Ensure that the diagram is self-contained, syntactically correct Mermaid code, and does not rely on external data or libraries. Use node and edge labels exactly as provided in the instructions. # DATA INSTRUCT: Data description. The diagram is constructed only from the provided instructions. Do not load external files or datasets."
        },
        {
            "title": "LilyPond Instruct",
            "content": "# SYSTEM PROMPT: You are helpful programming assistant proficient in LilyPond. Always use the version statement version 2.22.1. All answers must be enclosed in block lilypond SOME CODE containing one complete LilyPond score. Example minimal spec: lilypond version \"2.22.1\" score { new Staff { d f } layout { } } # SETUP INSTRUCT: Setup. Use LilyPond syntax only and produce exactly one valid LilyPond music notation definition. Do not output explanations, comments outside the code block, or code in other languages or formats. Do not split the notation into multiple blocks. Ensure the code can be rendered directly by LilyPond. # PLOT INSTRUCT: Write music notation according to the following instructions. Do not include explanations or natural language outside the code block. Ensure that the notation is self-contained, syntactically correct LilyPond code, and does not rely on external data or libraries. Use note names and other musical symbols exactly as provided in the instructions. # DATA INSTRUCT: Data description. The music notation is constructed only from the provided instructions. Do not load external files or datasets."
        },
        {
            "title": "SVG Instruct",
            "content": "# SYSTEM PROMPT: You are helpful programming assistant proficient in SVG. All answers must be enclosed in block svg SOME CODE containing one complete SVG specification. Example minimal spec: svg <svg width=\"100\" height=\"100\" xmlns=\"http://www.w3.org/2000/svg\"> <circle cx=\"50\" cy=\"50\" r=\"40\" stroke=\"black\" stroke-width=\"2\" fill=\"red\" /> </svg> # SETUP INSTRUCT: Setup. Use SVG syntax only and produce exactly one valid SVG definition. Do not output explanations, comments outside the code block, or code in other languages or formats. Do not split the diagram into multiple blocks. Ensure the code can be rendered directly by SVG viewers. # PLOT INSTRUCT: Write an SVG according to the following instructions. Do not include explanations or natural language outside the code block. Ensure that the SVG is self-contained, syntactically correct SVG code, and does not rely on external data or libraries. Use shapes and attributes exactly as provided in the instructions. # DATA INSTRUCT: Data description. The SVG is constructed only from the provided instructions. Do not load external files or datasets."
        },
        {
            "title": "Asymptote Instruct",
            "content": "# SYSTEM PROMPT: You are helpful programming assistant proficient in Asymptote. All answers must be enclosed in block asymptote SOME CODE containing one complete Asymptote specification. Example minimal spec: asymptote import graph; size(100); draw((0,0)--(1,1)); # SETUP INSTRUCT: Setup. Use Asymptote syntax only and produce exactly one valid Asymptote definition. Do not output explanations, comments outside the code block, or code in other languages or formats. Do not split the diagram into multiple blocks. Ensure the code can be rendered directly by Asymptote. # PLOT INSTRUCT: Write an Asymptote according to the following instructions. Do not include explanations or natural language outside the code block. Ensure that the Asymptote is self-contained, syntactically correct Asymptote code, and does not rely on external data or libraries. Use shapes and attributes exactly as provided in the instructions. # DATA INSTRUCT: Data description. Please use the provided data definition code to construct the plot. Do not modify this code or create data by yourself. Use the variables defined in this code directly when building the plot. The data definition code is as follows:"
        },
        {
            "title": "LaTeX Instruct",
            "content": "# SYSTEM PROMPT: You are helpful programming assistant proficient in LaTeX. All answers must be enclosed in block latex SOME CODE containing one complete LaTeX document. Example minimal spec: latex documentclass{standalone} begin{document} Hello end{document} # SETUP INSTRUCT: Setup. Use LaTeX syntax only and produce exactly one valid LaTeX document as single code block. Do not output explanations, comments outside the code block, or code in other languages or formats. Ensure the document can be rendered directly by LaTeX compilers. # PLOT INSTRUCT: Write LaTeX code to build plot of the dataset according to the following instructions. Return exactly one complete LaTeX document enclosed in single code block. Include all required packages. Do not include explanations or comments. Do not create synthetic data or modify the dataset. # DATA INSTRUCT: Data description. Load the dataset by adding pgfplotstablereadlatex.csvdatatable. Do not create synthetic data or modify the dataset. The metadata of the dataset is following:"
        },
        {
            "title": "HTML Instruct",
            "content": "# SYSTEM PROMPT: You are helpful programming assistant proficient in HTML. All answers must be enclosed in block html SOME CODE containing one complete HTML document Example minimal spec: html <!DOCTYPE html> <html> <body>Hello</body> </html> # SETUP INSTRUCT: Setup. Use HTML syntax only and produce exactly one valid HTML document as single code block. Do not output explanations, comments outside the code block, or code in other languages or formats. Ensure the document can be rendered directly by web browsers. # PLOT INSTRUCT: Write an HTML code to build plot of the dataset according to the following instructions. Return exactly one complete HTML document enclosed in single code block. Include all required libraries and scripts. Do not include explanations or comments. Do not create synthetic data or modify the dataset. # DATA INSTRUCT: Data description. Load the dataset by defining: const data = [html.csv]; [html.csv] is placeholder for the parsed CSV rows. Assume that the placeholder will be replaced at runtime by the CSV content converted into JavaScript array of objects (i.e., list of dicts), where each object represents one row with column names as keys and cell values as values. Write the code as if data is already such valid JavaScript array of objects. Do not create synthetic data or modify the dataset. The metadata of the dataset is following:"
        },
        {
            "title": "B TAXONOMY OF VISUAL CATEGORIES AND SUBTYPES",
            "content": "Figure 4: Distribution of fine-grained visualization types in VisPlotBench. Tasks are organized into 13 Visual categories and 116 Subtypes, ensuring broad coverage of both common and underexplored visualization families. [Back to Appendix Contents] 26 Table 7: Taxonomy of Visual Categories and Subtypes [Back to Appendix Contents]"
        },
        {
            "title": "Lines",
            "content": "3D"
        },
        {
            "title": "Diagramming",
            "content": "vertical-bar horizontal-bar grouped-bar normalized-stacked-bar stacked-bar diverging-bar dot-plot lollipop sorted-bar waterfall polar-bar bullet funnel combo-chart missing-bar marimekko single-line multi-line function-line step-line gapped-line band-line slope-chart candlestick surface multi-line scatter point-cloud solid single-line vector-field-map 3d-density-contours connected-scatter isosurface slices sequence-diagram flowchart geometric-figure electrical-circuit-diagram state-machine table uml-class-diagram gantt timeline simple-figure concept-illustration icon block-diagram physics-diagram venn word-cloud mind-map color-palette arrow-annotations Chemical graph sankey"
        },
        {
            "title": "Music",
            "content": "sheet-music 31 23 15 8 7 5 5 3 2 1 1 1 1 1 1 1 45 39 26 10 6 4 3 3 21 3 4 3 3 2 2 2 1 1 1 37 25 20 16 16 15 12 11 11 10 10 10 3 2 2 2 2 1 1 1"
        },
        {
            "title": "Areas",
            "content": "Scatter & Relation"
        },
        {
            "title": "Distribution",
            "content": "Matrix & Heatmaps"
        },
        {
            "title": "Hierarchies",
            "content": "area stacked-area normalized-stacked-area difference-area missing-data-matrix ternary-area streamgraph ridgeline bubble scatter color-scatter regression-ci ternary-line quadrant-chart ellipse-scatter polar-line-scatter splom connected-scatter dumbbell chart box-plot histogram density-contours violin kde-1d hexbin-2d qq-plot rug-plot ridgeline prediction-interval spectrum heatmap calendar-heatmap missing-corr-heatmap adjacency-matrix correlation-heatmap treemap sunburst circle-packing missing-dendrogram tidy-tree indented-tree"
        },
        {
            "title": "Maps",
            "content": "choropleth vector-field-map dot-map proportional-symbol-map Networks & Flows Radial & Polar sankey chord dependency-graph arc-diagram dag-layered force-directed pie radar polar-line-scatter donut radial-bar radial-area wind-rose 17 14 4 4 3 1 1 25 24 20 4 4 3 3 3 2 1 1 17 13 5 5 6 2 2 2 1 1 1 40 5 2 1 1 10 4 3 3 3 1 4 2 2 1 5 2 2 1 1 17 10 10 7 7 3"
        },
        {
            "title": "C BREAKDOWN MAIN RESULTS",
            "content": "In this section, we provide breakdown of model performance in VisPlotBench. For each visualization language,e report (1) execution pass rate (Exec Pass), (2) mean visual and task scores (Mean), and (3) the proportion of samples scoring at least 75 (Good). C.1 PYTHON, VEGA-LITE & LILYPOND Table 8: Performance of selected languages on the VisPlotbench benchmark. For each model, we report (1) execution pass rate (Exec Pass), (2) mean visual and task scores (Mean), and (3) the proportion of samples scoring at least 75 (Good). The best-performing model in each scale is shown in bold, and the second best is underlined. [Back to Appendix Contents] Model GPT-4.1 GPT-4.1 + Self Debug GPT-4.1-mini GPT-4.1-mini + Self Debug DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-3B-Instruct VisCoder-3B VisCoder2-3B VisCoder2-3B + Self Debug DeepSeek-Coder-6.7B-Instruct Qwen2.5-Coder-7B-Instruct VisCoder-7B VisCoder2-7B VisCoder2-7B + Self Debug DeepSeek-Coder-V2-Lite-Instruct Qwen2.5-Coder-14B-Instruct VisCoder2-14B VisCoder2-14B + Self Debug DeepSeek-Coder-33B-Instruct Qwen2.5-Coder-32B-Instruct VisCoder2-32B VisCoder2-32B + Self Debug Exec Pass 64.3 84.2 64.8 80.6 29.1 34.2 45. 56.1 63.3 39.3 41.3 58.2 64.8 77.0 47.5 50.0 65.3 78.1 58.2 50. 65.3 81.6 Python (196) Mean vis task Good(75) task vis Vega-Lite (129) Exec Pass Mean vis task Good(75) task vis Exec Pass LilyPond (55) Mean vis task Good(75) task vis 53 66 53 61 16 23 39 42 25 29 40 44 50 32 35 47 55 40 49 58 61 76 61 71 19 28 39 45 49 29 37 48 54 40 43 56 64 48 43 56 68 51% 61% 84.5 64% 76% 96.1 47% 59% 84.5 56% 67% 96.9 3B Scale 10% 11% 53.5 17% 24% 68.2 26% 35% 83.7 33% 38% 83.0 35% 40% 84.5 7B Scale 19% 23% 79.8 24% 32% 76.0 33% 42% 71.3 37% 49% 83.0 41% 54% 84.5 14B Scale 28% 36% 75.2 28% 39% 83.0 39% 52% 93.0 46% 58% 94.6 32B Scale 34% 41% 90.7 30% 41% 83.0 42% 54% 94.6 46% 62% 96.1 60 64 53 1 25 31 41 43 37 40 39 49 49 36 52 55 52 48 60 62 68 74 63 71 2 34 37 49 50 47 50 58 59 43 61 63 64 61 57 70 72 56% 66% 43.6 60% 72% 63.6 45% 60% 16.4 51% 68% 56. 30.9 0% 0% 13% 22% 3.6 20% 26% 21.8 33% 40% 50.9 34% 41% 52.7 7.3 24% 37% 29% 40% 5.5 31% 43% 23.6 43% 51% 69.1 43% 52% 72.7 27% 33% 49.1 42% 53% 25.5 47% 58% 54.6 47% 60% 63. 40% 51% 30.9 39% 49% 30.9 53% 65% 56.4 54% 67% 69.1 14 17 2 14 2 1 3 10 10 0 0 16 17 9 5 11 12 3 5 14 16 38 54 12 1 2 7 31 32 3 3 11 52 55 28 12 44 11 22 39 48 5% 36% 5% 53% 0% 11% 0% 35% 0% 0% 0% 0% 0% 2% 2% 15% 2% 15% 0% 4% 0% 4% 2% 4% 2% 45% 2% 45% 0% 13% 2% 4% 0% 40% 0% 40% 0% 4% 2% 18% 2% 27% 2% 35% C.2 MERMAID, SVG & LATEX Table 9: Performance of selected languages on the VisPlotbench benchmark. For each model, we report (1) execution pass rate (Exec Pass), (2) mean visual and task scores (Mean), and (3) the proportion of samples scoring at least 75 (Good). The best-performing model in each scale is shown in bold, and the second best is underlined. [Back to Appendix Contents] Model GPT-4.1 GPT-4.1 + Self Debug GPT-4.1-mini GPT-4.1-mini + Self Debug DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-3B-Instruct VisCoder-3B VisCoder2-3B VisCoder2-3B + Self Debug DeepSeek-Coder-6.7B-Instruct Qwen2.5-Coder-7B-Instruct VisCoder-7B VisCoder2-7B VisCoder2-7B + Self Debug DeepSeek-Coder-V2-Lite-Instruct Qwen2.5-Coder-14B-Instruct VisCoder2-14B VisCoder2-14B + Self Debug DeepSeek-Coder-33B-Instruct Qwen2.5-Coder-32B-Instruct VisCoder2-32B VisCoder2-32B + Self Debug Mermaid (131) SVG (65) Exec Pass Mean vis task Good(75) task vis Exec Pass Mean vis task Good(75) task vis Exec Pass LaTeX (112) Mean vis task Good(75) task vis 68.7 93.9 51.9 94.7 63.4 74.1 75.6 76.3 76.3 91.6 77.9 77.1 78.6 84. 69.5 74.8 81.7 86.3 87.0 71.0 87.0 90.1 41 56 33 58 19 30 43 43 40 39 41 43 45 34 39 53 55 44 51 54 57 77 45 79 25 38 40 59 59 50 53 54 59 46 56 67 70 57 56 67 69 22% 56% 95.4 32% 73% 96.9 18% 43% 95.4 26% 74% 96.9 3B Scale 2% 7.7 8% 9% 21% 75.4 12% 21% 76.9 23% 50% 87.7 23% 50% 87.7 7B Scale 11% 28% 96.9 13% 38% 92.3 17% 43% 93.9 20% 53% 96.9 21% 54% 96.9 14B Scale 12% 34% 93.9 15% 48% 98.5 32% 62% 89.2 33% 64% 90.8 32B Scale 15% 40% 92.3 21% 53% 93.9 31% 62% 81.5 34% 63% 86.2 45 45 41 1 18 13 25 25 19 23 23 34 34 23 33 34 23 34 33 34 92 93 86 88 1 39 31 59 59 46 58 73 73 55 80 72 72 58 81 68 71 14% 94% 31.3 14% 95% 66.1 11% 86% 29.5 11% 88% 58. 0% 4.5 0% 2% 28% 17.9 0% 12% 23.2 3% 48% 36.6 3% 48% 38.4 0% 22% 18.8 0% 40% 25.9 2% 32% 25.9 3% 62% 39.3 3% 62% 42.9 2% 34% 29.5 5% 77% 30.4 8% 65% 42.0 8% 65% 45. 0% 43% 24.1 3% 75% 29.5 11% 63% 42.9 11% 66% 61.6 18 38 21 35 2 6 9 14 14 6 11 15 16 10 15 22 24 8 14 20 28 26 56 25 1 9 12 21 23 11 15 15 23 24 16 22 33 14 25 35 45 13% 25% 25% 51% 18% 25% 23% 49% 2% 3% 7% 1% 5% 9% 3% 12% 3% 13% 8% 3% 6% 8% 6% 12% 6% 15% 6% 15% 4% 10% 6% 15% 12% 27% 12% 28% 4% 11% 9% 27% 11% 34% 14% 42% 29 C.3 ASYMPTOTE & HTML Table 10: Performance of selected languages on the VisPlotbench benchmark. For each model, we report (1) execution pass rate (Exec Pass), (2) mean visual and task scores (Mean), and (3) the proportion of samples scoring at least 75 (Good). The best-performing model in each scale is shown in bold, and the second best is underlined. [Back to Appendix Contents]"
        },
        {
            "title": "Model",
            "content": "GPT-4.1 GPT-4.1 + Self Debug GPT-4.1-mini GPT-4.1-mini + Self Debug DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-3B-Instruct VisCoder-3B VisCoder2-3B VisCoder2-3B + Self Debug DeepSeek-Coder-6.7B-Instruct Qwen2.5-Coder-7B-Instruct VisCoder-7B VisCoder2-7B VisCoder2-7B + Self Debug DeepSeek-Coder-V2-Lite-Instruct Qwen2.5-Coder-14B-Instruct VisCoder2-14B VisCoder2-14B + Self Debug DeepSeek-Coder-33B-Instruct Qwen2.5-Coder-32B-Instruct VisCoder2-32B VisCoder2-32B + Self Debug Asymptote (92)"
        },
        {
            "title": "Mean",
            "content": "vis task Good(75) task vis"
        },
        {
            "title": "Exec\nPass",
            "content": "HTML (108) Mean vis task Good(75) task vis 21.7 46.7 23.9 48.9 13.0 18.5 30. 62.0 63.0 0 13.0 17.4 64.1 70.7 20.7 25.0 56.5 66.3 21.7 17. 58.7 71.7 12 22 13 21 20 41 22 40 7% 20% 89.8 9% 39% 97.2 7% 21% 86.1 9% 36% 100 3B Scale 0 8 23 23 0 11 12 36 37 0% 4% 3% 0% 9% 8% 36.1 62.0 79. 7% 26% 93.5 7% 27% 94.4 7B Scale 0 7 7 27 29 0 10 11 43 0% 5% 3% 0% 9% 9% 22.2 64.8 75.9 11% 33% 82.4 11% 35% 84.3 14B Scale 5 27 31 10 17 45 50 1% 64.8 9% 9% 16% 83.3 15% 41% 90.7 16% 45% 94.4 32B Scale 8 9 27 31 14 13 46 53 2% 12.0 9% 5% 12% 78.7 10% 39% 91.7 10% 41% 93. 48 51 36 42 2 16 21 34 34 5 20 20 30 31 21 41 42 4 33 43 44 64 68 53 62 3 19 29 47 8 31 32 46 47 32 50 58 60 6 49 61 21% 50% 22% 52% 11% 34% 12% 42% 0% 1% 6% 7% 9% 17% 8% 23% 8% 23% 1% 3% 6% 13% 5% 16% 7% 19% 7% 21% 4% 18% 9% 31% 12% 36% 13% 37% 0% 0% 11% 32% 18% 48% 18% 49% 30 BREAKDOWN SELF-DEBUG RESULTS In this section, we provide breakdown of model performance under the self-debug setting. For each language, we report execution pass rates across up to three rounds of automatic correction, grouped by model series. D.1 PYTHON & VEGA-LITE Table 11: Execution pass rates (%) in Python and Vega-Lite under the normal and self-debug settings. Models that fail initially are allowed up to three rounds of automatic correction. Left columns show Python results, right columns show Vega-Lite results. [Back to Appendix Contents] Model GPT-4.1 GPT-4.1-mini DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-3B-Instruct VisCoder-3B VisCoder2-3B DeepSeek-Coder-6.7B-Instruct Qwen2.5-Coder-7B-Instruct VisCoder-7B VisCoder2-7B DeepSeek-Coder-V2-Lite-Instruct Qwen2.5-Coder-14B-Instruct VisCoder2-14B DeepSeek-Coder-33B-Instruct Qwen2.5-Coder-32B-Instruct VisCoder2-32B Normal Python Self Debug Round 1 Round 2 Round 3 Normal Vega-Lite Self Debug Round 1 Round 2 Round 64.3 64.8 29.1 34.2 45.4 56.1 39.3 41.3 58.2 64.8 47.5 50. 65.3 58.2 50.5 65.3 75.0 73.5 35.7 39.8 51.0 61. 46.9 53.6 66.8 72.5 54.6 65.3 76.5 67.9 70.9 76. 81.6 79.1 3B Scale 35.7 41.8 52.6 62.8 7B Scale 49.5 60.2 68. 76.0 14B Scale 55.6 72.5 78.1 32B Scale 71.4 78. 80.1 84.2 80.6 35.7 42.9 52.6 63.3 53.1 61.7 71.9 77. 58.7 76.0 78.1 73.0 79.1 81.6 84.5 84.5 53.5 68.2 83. 83.0 79.8 76.0 71.3 83.0 75.2 83.0 93.0 90.7 83. 94.6 95.3 95.3 53.5 68.2 83.7 84.5 81.4 77.5 76.0 84. 78.3 86.8 93.8 92.3 87.6 96.1 96.1 96.9 53.5 69.0 83. 84.5 81.4 77.5 77.5 84.5 79.8 86.8 94.6 92.3 89. 96.1 96.1 96.9 53.5 69.0 83.7 84.5 81.4 77.5 77.5 84. 79.8 86.8 94.6 92.3 89.9 96.1 31 D.2 LILYPOND & MERMAID Table 12: Execution pass rates (%) in LilyPond and Mermaid under the normal and self-debug settings. Models that fail initially are allowed up to three rounds of automatic correction. Left columns show LilyPond results, right columns show Mermaid results. [Back to Appendix Contents] Model GPT-4.1 GPT-4.1-mini DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-3B-Instruct VisCoder-3B VisCoder2-3B DeepSeek-Coder-6.7B-Instruct Qwen2.5-Coder-7B-Instruct VisCoder-7B VisCoder2-7B DeepSeek-Coder-V2-Lite-Instruct Qwen2.5-Coder-14B-Instruct VisCoder2-14B DeepSeek-Coder-33B-Instruct Qwen2.5-Coder-32B-Instruct VisCoder2-32B Normal LilyPond Self Debug Round 1 Round 2 Round 3 Normal Mermaid Self Debug Round 1 Round 2 Round 3 43.6 16. 30.9 3.6 21.8 50.9 7.3 5.5 23.6 69.1 49.1 50.0 54. 30.9 30.9 56.4 54.5 30.9 32.7 5.5 21.8 52.7 9.1 5.5 27. 72.7 52.7 65.3 63.6 40.0 40.0 61.8 63.6 47. 3B Scale 32.7 5.5 21.8 52.7 7B Scale 10.9 5.5 30.9 72. 14B Scale 52.7 72.5 63.6 32B Scale 41.8 43.6 69. 63.6 56.4 32.7 5.5 21.8 52.7 10.9 5.5 30.9 72.7 52.7 76. 63.6 41.8 43.6 69.1 68.7 51.9 63.4 74.1 75.6 76. 91.6 77.9 77.1 78.6 69.5 83.0 81.7 87.0 71.0 87. 84.7 81.7 76.3 76.3 76.3 76.3 93.9 79.4 80.9 84.0 69.5 86. 86.3 87.0 74.8 89.3 93.0 90.1 77.9 76.3 76.3 76. 94.7 79.4 80.9 84.7 69.5 86.8 86.3 87.8 75.6 90. 93.9 94.7 78.6 76.3 76.3 76.3 94.7 79.4 80.9 84.7 71.0 86. 86.3 88.6 76.3 90.1 32 D.3 SVG & LATEX Table 13: Execution pass rates (%) in SVG and LaTeX under the normal and self-debug settings. Models that fail initially are allowed up to three rounds of automatic correction. Left columns show SVG results, right columns show LaTeX results. [Back to Appendix Contents] Model GPT-4.1 GPT-4.1-mini DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-3B-Instruct VisCoder-3B VisCoder2-3B DeepSeek-Coder-6.7B-Instruct Qwen2.5-Coder-7B-Instruct VisCoder-7B VisCoder2-7B DeepSeek-Coder-V2-Lite-Instruct Qwen2.5-Coder-14B-Instruct VisCoder2-14B DeepSeek-Coder-33B-Instruct Qwen2.5-Coder-32B-Instruct VisCoder2-32B Normal SVG Self Debug Round 1 Round 2 Round 3 Normal LaTeX Self Debug Round 1 Round 2 Round 3 95.4 95.4 7.7 75.4 76.9 87. 96.9 92.3 93.9 96.9 93.9 98.5 89.2 92.3 93.9 81. 96.9 96.9 96.9 96.9 3B Scale 95.4 75.4 76.9 87.7 98.5 92.3 93. 96.9 93.9 98.5 90.8 92.3 93.9 84.6 95.4 75.4 76. 87.7 7B Scale 98.5 92.3 93.9 96.9 14B Scale 93.9 98. 90.8 32B Scale 92.3 93.9 86.2 96.9 96.9 95.4 75.4 76. 87.7 98.5 92.3 93.9 96.9 93.9 98.5 90.8 92.3 93. 86.2 31.3 29.5 4.5 17.9 23.2 36.6 18.8 25.9 25.9 39. 29.5 30.4 42.0 24.1 29.5 42.9 53.6 50.9 5.4 17.9 25. 38.4 19.6 28.6 38.4 42.9 33.9 37.5 43.8 28.6 42. 55.4 59.8 55.4 5.4 17.9 25.9 38.4 22.3 30.4 42.0 42. 35.7 38.4 45.5 31.3 50.0 59.8 66.1 58.9 5.4 17.9 25. 38.4 22.3 30.4 43.8 42.9 35.7 38.4 45.5 31.3 51. 61.6 33 D.4 ASYMPTOTE & HTML Table 14: Execution pass rates (%) in Asymptote and HTML under the normal and self-debug settings. Models that fail initially are allowed up to three rounds of automatic correction. Left columns show Asymptote results, right columns show HTML results. [Back to Appendix Contents] Model GPT-4.1 GPT-4.1-mini DeepSeek-Coder-1.3B-Instruct Qwen2.5-Coder-3B-Instruct VisCoder-3B VisCoder2-3B DeepSeek-Coder-6.7B-Instruct Qwen2.5-Coder-7B-Instruct VisCoder-7B VisCoder2-7B DeepSeek-Coder-V2-Lite-Instruct Qwen2.5-Coder-14B-Instruct VisCoder2-14B DeepSeek-Coder-33B-Instruct Qwen2.5-Coder-32B-Instruct VisCoder2-32B Normal Asymptote Self Debug Round 1 Round 2 Round Normal HTML Self Debug Round 1 Round 2 Round 3 21.7 23.9 13.0 18.5 30.4 62.0 0.0 13.0 17. 64.1 20.7 25.0 56.5 21.7 17.4 58.7 35.9 37. 43.5 42.4 3B Scale 17.4 18.5 31.5 63.0 1.1 16.3 26.1 68. 23.9 32.6 64.1 26.1 25.0 68.5 17.4 18.5 32.6 63. 7B Scale 2.2 20.7 26.1 70.7 14B Scale 26.1 39.1 66. 32B Scale 28.3 31.5 71.7 46.7 48.9 17.4 18.5 32.6 63. 2.2 20.7 26.1 70.7 26.1 40.2 66.3 29.4 33.7 71. 89.8 86.1 36.1 62.0 79.6 93.5 22.2 64.8 75.9 82.4 64.8 83. 90.7 12.0 78.7 91.7 96.3 99.1 36.1 65.7 83.3 94. 25.0 75.9 81.5 84.3 76.9 89.8 94.4 14.8 88.9 92. 97.2 99.1 36.1 70.4 83.3 94.4 25.0 76.9 82.4 84.3 79.6 89. 94.4 14.8 89.8 93.5 97.2 100 36.1 70.4 83.3 94. 25.0 76.9 82.4 84.3 79.6 89.8 94.4 14.8 89.8 93."
        },
        {
            "title": "E BREAKDOWN ERROR TYPE RESULTS",
            "content": "In this section, we provide breakdown error type results of execution errors for GPT-4.1 and VisCoder2-32B. For each language, we report error type across up to three rounds of automatic correction. E.1 PYTHON Table 15: Distribution of execution errors for GPT-4.1 and VisCoder2-32B in Python. Each column shows error counts at different self-debugging rounds after initial failure. [Back to Appendix Contents] Normal Round 1 Round 2 Round GPT-4.1 VisCoder2-32B Normal Round 1 Round 2 Round 3 17 - 2 1 - 7 - - - 2 1 20 20 70 12 - 2 1 - 7 - - - 0 1 16 10 9 - 1 1 - 6 - - - 0 1 14 4 36 8 - 0 1 - 6 - - - 0 1 14 1 31 15 1 2 - 3 9 1 2 2 - 1 13 19 12 1 1 - 2 9 1 2 1 - 1 7 10 47 12 1 1 - 1 9 1 2 0 - 1 3 8 39 10 1 1 - 0 9 1 2 0 - 1 3"
        },
        {
            "title": "Total Errors",
            "content": "E.2 VEGA-LITE Table 16: Distribution of execution errors for GPT-4.1 and VisCoder2-32B in Vega-Lite. Each column shows error counts at different self-debugging rounds after initial failure. [Back to Appendix Contents] Normal Round 1 Round 2 Round 3 GPT-4. VisCoder2-32B Normal Round 1 Round 2 Round 3 1 1 8 9 1 20 0 0 2 4 0 6 0 0 2 2 4 0 0 2 2 0 4 - 1 2 2 2 7 - 1 1 1 5 - 1 1 1 2 5 - 1 1 1"
        },
        {
            "title": "Total Errors",
            "content": "E.3 LILYPOND Table 17: Distribution of execution errors for GPT-4.1 and VisCoder2-32B in Lilypond. Each column shows error counts at different self-debugging rounds after initial failure. [Back to Appendix Contents]"
        },
        {
            "title": "Total Errors",
            "content": "Normal Round 1 Round 2 Round 3 GPT-4.1 VisCoder2-32B Normal Round 1 Round 2 Round 3 1 3 25 2 31 1 2 17 25 1 2 12 2 20 1 2 12 2 20 1 4 14 5 24 1 4 12 4 21 1 4 10 2 1 4 10 2 17 E.4 MERMAID Table 18: Distribution of execution errors for GPT-4.1 and VisCoder2-32B in Mermaid. Each column shows error counts at different self-debugging rounds after initial failure. [Back to Appendix Contents] Normal Round 1 Round 2 Round 3 GPT-4.1 VisCoder2-32B Normal Round 1 Round 2 Round 3 2 32 2 4 1 - - - 41 2 16 1 1 0 - - - 20 0 7 1 0 0 - - - 8 0 7 1 0 0 - - - 8 1 12 - 1 - 1 1 17 0 10 - 1 - 1 1 1 14 0 9 - 1 - 1 1 1 0 9 - 1 - 1 1"
        },
        {
            "title": "Total Errors",
            "content": "E.5 SVG Table 19: Distribution of execution errors for GPT-4.1 and VisCoder2-32B in SVG. Each column shows error counts at different self-debugging rounds after initial failure. [Back to Appendix Contents] Normal Round 1 Round 2 Round 3 GPT-4. VisCoder2-32B Normal Round 1 Round 2 Round 3 1 2 3 1 1 2 1 2 1 1 2 4 8 12 2 10 2 7 9 2"
        },
        {
            "title": "Total Errors",
            "content": "E.6 LATEX Table 20: Distribution of execution errors for GPT-4.1 and VisCoder2-32B in LaTeX. Each column shows error counts at different self-debugging rounds after initial failure. [Back to Appendix Contents]"
        },
        {
            "title": "Total Errors",
            "content": "Normal Round 1 Round 2 Round 3 GPT-4.1 VisCoder2-32B Normal Round 1 Round 2 Round 3 16 2 17 3 5 21 64 16 2 9 2 4 50 16 1 7 2 4 15 45 15 1 7 1 4 15 43 5 - 27 6 10 77 5 - 12 3 6 26 52 5 - 9 3 4 24 45 2 - 6 3 4 38 36 E.7 ASYMPTOTE Table 21: Distribution of execution errors for GPT-4.1 and VisCoder2-32B in Asymptote. Each column shows error counts at different self-debugging rounds after initial failure. [Back to Appendix Contents] Normal Round 1 Round 2 Round 3 GPT-4.1 VisCoder2-32B Normal Round 1 Round 2 Round 3 - 1 2 28 16 1 3 21 - 72 - 1 1 20 15 1 2 19 - 59 - 1 1 18 13 1 1 17 - 52 - 1 1 16 13 1 1 16 - 49 1 1 - 9 2 8 - 15 38 1 1 - 4 2 7 - 12 2 29 1 1 - 3 2 6 - 11 2 26 1 1 - 3 2 6 -"
        },
        {
            "title": "Total Errors",
            "content": "E.8 HTML Table 22: Distribution of execution errors for GPT-4.1 and VisCoder2-32B in HTML. Each column shows error counts at different self-debugging rounds after initial failure. [Back to Appendix Contents]"
        },
        {
            "title": "Total Errors",
            "content": "Normal Round 1 Round 2 Round 3 GPT-4.1 VisCoder2-32B Normal Round 1 Round 2 Round 3 1 9 1 11 1 2 4 1 1 1 3 1 1 1 3 3 3 9 2 3 3 8 2 2 3 7 2 2"
        },
        {
            "title": "F CASE STUDY",
            "content": "In this section, we present set of representative examples from VisCoder2-32B to illustrate model behavior across the eight visualization languages. F.1 PYTHON: SUCCESSFUL GENERATION Figure 5: Example of successful generation in Python (ID: 1). The model generates code that executes successfully and produces plot consistent with the ground truth. [Back to Appendix Contents] F.2 PYTHON: SELF-DEBUG RECOVERY Figure 6: Example of failed generation in Python (ID: 69), where the initial code raises ValueError and is resolved in the first round of self-debug, resulting in corrected plot that matches the intended semantics. [Back to Appendix Contents] 39 F.3 PYTHON: SELF-DEBUG FAILED Figure 7: Example of failed generation in Python (ID: 115), where the initial code raises AttributeError and is still failed after three rounds self-debug. [Back to Appendix Contents] 40 F.4 VEGA-LITE: SUCCESSFUL GENERATION Figure 8: Example of successful generation in Vega-Lite (ID: 18). The model generates code that executes successfully and produces plot consistent with the ground truth. [Back to Appendix Contents] 41 F.5 VEGA-LITE: SELF-DEBUG RECOVERY Figure 9: Example of failed generation in Vega-Lite (ID: 14), where the initial code raises TypeError and is resolved in the second round of self-debug, resulting in corrected plot that matches the intended semantics. [Back to Appendix Contents] 42 F.6 VEGA-LITE: SELF-DEBUG FAILED Figure 10: Example of failed generation in Vega-Lite (ID: 50), where the initial code raises TypeError and is still failed after three rounds self-debug. [Back to Appendix Contents] 43 F.7 LILYPOND: SUCCESSFUL GENERATION Figure 11: Example of successful generation in Lilypond (ID: 15). The model generates code that executes successfully and produces plot consistent with the ground truth. [Back to Appendix Contents] F.8 LILYPOND: SELF-DEBUG RECOVERY Figure 12: Example of failed generation in Lilypond (ID: 13), where the initial code raises SyntaxError and is resolved in the first round of self-debug, resulting in corrected plot that matches the intended semantics. [Back to Appendix Contents] 45 F.9 LILYPOND: SELF-DEBUG FAILED Figure 13: Example of failed generation in Lilypond (ID: 48), where the initial code raises TypeError and is still failed after three rounds self-debug. [Back to Appendix Contents] 46 F.10 MERMAID: SUCCESSFUL GENERATION Figure 14: Example of successful generation in Mermaid (ID: 19). The model generates code that executes successfully and produces plot consistent with the ground truth. [Back to Appendix Contents] 47 F.11 MERMAID: SELF-DEBUG RECOVERY Figure 15: Example of failed generation in Mermaid (ID: 88), where the initial code raises SyntaxError and is resolved in the second round of self-debug, resulting in corrected plot that matches the intended semantics. [Back to Appendix Contents] 48 F.12 MERMAID: SELF-DEBUG FAILED Figure 16: Example of failed generation in Mermaid (ID: 80), where the initial code raises AttributeError and is still failed after three rounds self-debug. [Back to Appendix Contents] 49 F.13 SVG: SUCCESSFUL GENERATION Figure 17: Example of successful generation in SVG (ID: 52). The model generates code that executes successfully and produces plot consistent with the ground truth. [Back to Appendix Contents] F.14 SVG: SELF-DEBUG RECOVERY Figure 18: Example of failed generation in SVG (ID: 42), where the initial code raises ExPatError and is resolved in the first round of self-debug, resulting in corrected plot that matches the intended semantics. [Back to Appendix Contents] 51 F.15 SVG: SELF-DEBUG FAILED Figure 19: Example of failed generation in SVG (ID: 12), where the initial code raises ParseError and is still failed after three rounds self-debug. [Back to Appendix Contents] 52 F.16 LATEX: SUCCESSFUL GENERATION Figure 20: Example of successful generation in LaTeX (ID: 33). The model generates code that executes successfully and produces plot consistent with the ground truth. [Back to Appendix Contents] 53 F.17 LATEX: SELF-DEBUG RECOVERY Figure 21: Example of failed generation in LaTeX (ID: 37), where the initial code raises NameError and is resolved in the second round of self-debug, resulting in corrected plot that matches the intended semantics. [Back to Appendix Contents] 54 F.18 LATEX: SELF-DEBUG FAILED Figure 22: Example of failed generation in LaTeX (ID: 97), where the initial code raises NameError and is still failed after three rounds self-debug. [Back to Appendix Contents] 55 F.19 ASYMPTOTE: SUCCESSFUL GENERATION Figure 23: Example of successful generation in Asymptote (ID: 31). The model generates code that executes successfully and produces plot consistent with the ground truth. [Back to Appendix Contents] F.20 ASYMPTOTE: SELF-DEBUG RECOVERY Figure 24: Example of failed generation in Asymptote (ID: 53), where the initial code raises NameError and is resolved in the third round of self-debug, resulting in corrected plot that matches the intended semantics. [Back to Appendix Contents] 57 F.21 ASYMPTOTE: SELF-DEBUG FAILED Figure 25: Example of failed generation in Asymptote (ID: 79), where the initial code raises TypeError and is still failed after three rounds self-debug. [Back to Appendix Contents] 58 F.22 HTML: SUCCESSFUL GENERATION Figure 26: Example of successful generation in HTML (ID: 6). The model generates code that executes successfully and produces plot consistent with the ground truth. [Back to Appendix Contents] 59 F.23 HTML: SELF-DEBUG RECOVERY Figure 27: Example of failed generation in HTML (ID: 9), where the initial code raises ImportError and is resolved in the first round of self-debug, resulting in corrected plot that matches the intended semantics. [Back to Appendix Contents] 60 F.24 HTML: SELF-DEBUG FAILED Figure 28: Example of failed generation in HTML (ID: 85), where the initial code raises TypeError and is still failed after three rounds self-debug. [Back to Appendix Contents]"
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Independent Researcher",
        "Korea Advanced Institute of Science & Technology",
        "Netmind.ai",
        "University of Waterloo"
    ]
}