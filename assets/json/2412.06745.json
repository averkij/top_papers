{
    "paper_title": "ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended Capabilities",
    "authors": [
        "Adhiraj Ghosh",
        "Sebastian Dziadzio",
        "Ameya Prabhu",
        "Vishaal Udandarao",
        "Samuel Albanie",
        "Matthias Bethge"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Traditional fixed test sets fall short in evaluating open-ended capabilities of foundation models. To address this, we propose ONEBench(OpeN-Ended Benchmarking), a new testing paradigm that consolidates individual evaluation datasets into a unified, ever-expanding sample pool. ONEBench allows users to generate custom, open-ended evaluation benchmarks from this pool, corresponding to specific capabilities of interest. By aggregating samples across test sets, ONEBench enables the assessment of diverse capabilities beyond those covered by the original test sets, while mitigating overfitting and dataset bias. Most importantly, it frames model evaluation as a collective process of selecting and aggregating sample-level tests. The shift from task-specific benchmarks to ONEBench introduces two challenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to the aggregation over diverse metrics, while incompleteness describes comparing models evaluated on different data subsets. To address these challenges, we explore algorithms to aggregate sparse measurements into reliable model scores. Our aggregation algorithm ensures identifiability(asymptotically recovering ground-truth scores) and rapid convergence, enabling accurate model ranking with less data. On homogenous datasets, we show our aggregation algorithm provides rankings that highly correlate with those produced by average scores. We also demonstrate robustness to ~95% of measurements missing, reducing evaluation cost by up to 20x with little-to-no change in model rankings. We introduce ONEBench-LLM for language models and ONEBench-LMM for vision-language models, unifying evaluations across these domains. Overall, we present a technique for open-ended evaluation, which can aggregate over incomplete, heterogeneous sample-level measurements to continually grow a benchmark alongside the rapidly developing foundation models."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 ] . [ 1 5 4 7 6 0 . 2 1 4 2 : r ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended Capabilities Adhiraj Ghosh1,2 Sebastian Dziadzio1,2 Ameya Prabhu1,2 Vishaal Udandarao1,2,3 Samuel Albanie Matthias Bethge1,2 1Tubingen AI Center, University of Tubingen 2Open-Ψ (Open-Sci) Collective 3University of Cambridge github.com/bethgelab/onebench ı huggingface.co/datasets/bethgelab/onebench Abstract Traditional fixed test datasets fall short in evaluating the open-ended capabilities of foundation models. To address this limitation, we propose ONEBench (OpeN-Ended Benchmarking), new testing paradigm that consolidates individual evaluation datasets into unified, ever-expanding sample pool. ONEBench allows users to generate custom, open-ended evaluation benchmarks from this sample pool, corresponding to specific capabilities of interest. By aggregating and reusing samples across test sets, ONEBench enables the assessment of diverse capabilities beyond those covered by the original test sets, while mitigating overfitting and dataset bias. Most importantly, it frames model evaluation as collective process of selecting and aggregating sample-level tests. The shift from task-specific benchmarks to ONEBench introduces two key challenges: (1) heterogeneity and (2) incompleteness. Heterogeneity refers to the aggregation over diverse metrics, including binary, numeric, and ordinal data, while incompleteness describes comparing models evaluated on different subsets of testing data. To address these challenges, we explore algorithms to aggregate sparse, unequal measurements into reliable model scores. Our aggregation algorithm ensures identifiability (asymptotically recovering ground-truth scores) and rapid convergence, enabling accurate model comparisons with relatively little data. On homogenous datasets, we show our aggregation algorithm provides rankings that highly correlate with those produced by average scores. Furthermore, we demonstrate robustness to over 95% of measurements missing, reducing evaluation cost by up to 20 with little-to-no change in model rankings. We introduce ONEBench-LLM for language models and ONEBench-LMM for vision-language models, unifying evaluations across these domains, and showcase targeted testing of models over wide range of capabilities. Overall, we present technique for open-ended evaluation of foundation models, which can aggregate over incomplete, heterogeneous sample-level measurements to continually grow benchmark alongside the rapidly developing foundation models. 3.2.3 Sample Efficiency and Handling Incomplete Rankings . . . . . . . . . . . . . . 4 ONEBench: Creation & Capability Querying 9 4.1 Creation of ONEBench-LLM & ONEBench-LMM 9 9 4.1.1 ONEBench-LLM . . . . . . . . . . . . . 10 4.1.2 ONEBench-LMM . . . . . . . . . . . . . 4.2 Capabilities and Concept Probing: Results & . . . . . . . . . . . . . . . . . . . . . . Insights 10 5 Related Works 6 Conclusions and Open Problems 13 15 1 Introduction 2 2 ONEBench: Formulation 4 2.1 Components . . . . . . . . . . . . . . . . . . . . 4 2.2 Querying Capabilities for Personalized Evaluation 3 Aggregation in ONEBench: Theory and Practice 3.1 Theoretical Foundations . . . . . . . . . . . . . 3.2 Translating Theory to Practice . . . . . . . . . Setup . . . . . . . . . . . . . . . . . . . Is Plackett-Luce Good Fit for RealWorld Data? . . . . . . . . . . . . . . . 3.2.1 3.2.2 6 6 7 7 8 equal contribution, random order, core contributors"
        },
        {
            "title": "Introduction",
            "content": "ONEBench to rule them all (evals), ONEBench to find them (samples), ONEBench to test them all (models), and by queried ability rank them. Deep learning has arrived in the post-dataset era1. With the rapidly expanding range of zero-shot capabilities of foundation models, the focus of model evaluation has moved beyond singular, dataset-specific performance measurements obtained by splitting fixed collection of data into training and test sets. Instead, foundation models are employed as general knowledge and reasoning engines across all domains in which they prove to be useful. This creates pressing need to characterize their open-ended set of capabilities across various metrics in zero-shot settings [39], for flexible evaluation over several use-cases. However, traditional static benchmarks, which test generalization on fixed test splits, are unable to effectively probe the ever-evolving set of capabilities of foundation models. This raises an important question: How can benchmarking adapt to measure an open-ended set of capabilities? We propose solution based on dynamic sample-level evaluation, which we call ONEBench for OpeNEnded Benchmarking, where test sets for particular capabilities are generated ad-hoc from large pool of individual annotated data samples. These sample-level evaluations act as atomic units of measurement that can be combined into an exponential variety of aggregations. Due to this flexibility, the sample pool and corresponding annotation metrics can be continuously updated to include new evaluations. Additionally, this approach can reduce dataset biassystematic quirks in the data arising from how it was collected [151, 87, 171]. Finally, by combining samples across test sets, ONEBench can better capture real-world diversity [111]. The most important feature of ONEBench is the potential to democratize evaluation. Unlike traditional benchmarks, typically created by individual groups based on their own criteria for data collection and evaluation procedures [27, 7], ONEBench allow the integration of test sets from multiple sources reflecting wide range of perspectives, use-cases, and objectives. This flexibility allows different interest groups with varying needs to collaboratively define their own evaluations by selecting the most appropriate combination of tests to suit their specific requirements. Moreover, the design of ONEBench challenges the dominant approach of chasing single benchmark scores, which do not indicate the difficulty of individual data instances [32], in favour of plurality of rankings and dynamic, granular, multi-faceted evaluation. Challenges in ONEBench. To build ONEBench, we must address two main challenges: (a) Heterogeneity and (b) Incompleteness. Heterogeneity means that model measurements span different metric types, such as binary (correct/incorrect), numeric (BLEU scores), and ordinal (preference rankings). This diversity makes it difficult to aggregate measurements and standardize comparisons across different models. Incompleteness, on the other hand, arises from models being evaluated on different, unequal subsets of testing data, rendering direct aggregation unfair and inaccurate. Traditional benchmarks use multi-task setting, where each component test set requires evaluating all models over fixed set of samples using single metric, completely sidestepping both these issues. Solution and Theoretical Guarantees. To tackle these challenges, we apply social choice theory, treating data samples as voters who express preferences among models. By converting all measurements into ordinal rankings, we leverage well-established principles to create robust algorithm for aggregating over heterogeneous and incomplete data. Our approach assumes random utility model based on the Plackett-Luce framework, which provides guarantees for accurately recovering ground-truth utility scores from input measurements. This approach ensures that our model rankings are both theoretically sound and practical, with rapid convergence guarantees enabling accurate rankings from relatively small amounts of data. Empirical Validation. We develop two instantiations of ONEBench: ONEBench-LLM for language models 1From talk by Alexei Efros at ICML 2020 2 Figure 1: The ONEBench Framework. Left: ONEBench comprises set of models, pool of data samples spanning multiple test sets, metadata describing models and data samples, and collection of heterogeneous, sample-level measurements. Right: the user formulates query to capture the desired model capability, using mix of structured metadata filters and semantic search. Selected models are then ranked on subset of data samples that meet the specified criteria. and ONEBench-LMM for multimodal(vision and language) models. These benchmarks unify evaluations across their respective domains by aggregating data from diverse sources, from arena-style human preference data [18, 91] to heterogeneous multi-task leaderboards [8, 79, 176, 23]. Our empirical results demonstrate that the Plackett-Luce model [119, 93] is good fit for aggregating real-world benchmarks, showing high correlation with ground-truth score-based rankings over homogeneous datasets. Importantly, we demonstrate that this strong correlation holds even when up to 95% of the data is missing. This robustness allows us to reduce costs by 20 times with little loss in performance by selecting subset of data samples for evaluation. Finally, we compare Plackett-Luce rankings with widely adopted ranking metrics like ELO [30] and Bradley-Terry [15] and show that our method outperforms them on accuracy and robustness to missing information. Personalized Aggregation. Consider the following scenario: you are scientist in biochemistry lab and require an LLM to assist with designing experiments related to antibodies. With ONEBench, you can input query, such as immunology or antibodies, and receive dynamically constructed benchmark that ranks models based on their performance on this specific capability. While the optimal selection of personalized capability sets is an emerging research field, we present proof of concept by distinguishing between tasks (e.g., reading comprehension) and concepts (e.g., Clostridium bacteria). We show how combination of structured filters and flexible semantic search allows users to define their capability of interest with respect to these two dimensions and perform targeted evaluations resulting in personalized model rankings. 3 In essence, ONEBench represents democratized, open-source collection of diverse evaluation samples enriched with detailed metadata, driven by an aggregation method designed to robustly rank models across heterogeneous metrics and incomplete evaluation data. Users can conduct semantic searches and apply structured query filters to dynamically generate benchmark tailored to their specific use case. They can also contribute new evaluation samples and sample-level model measurements, which can then be instantly aggregated, producing personalized rankings. This framework enables lifelong aggregation of arbitrary test sets with unprecedented flexibility and precision."
        },
        {
            "title": "2 ONEBench: Formulation",
            "content": "At the heart of ONEBench is the idea of homogenizing performance evaluation across benchmarks by replacing benchmark-specific metrics with rankings. Importantly, this can be done at the level of individual data samples. In the following, we describe the process of construction and evaluation in detail, together with mathematical guarantees."
        },
        {
            "title": "2.1 Components",
            "content": "The goal of building ONEBench (B) from growing set of benchmarks (Bk)N k=1 is to evaluate collection of models (M) using an ever-expanding test pool of data instances (D) which may be annotated with additional meta-data specifying the capabilities (C) tested. To cope with the diversity of data originating from different benchmarks, sample-level rankings (S) are created for all data instances in the test pool. We provide schematic overview of ONEBench in Fig. 1 and describe each component below: i) Pool of Data. D=((x1, y1), . . . , (xn, yn)) denotes an ordered collection of test data instances xi with annotation yi. An example of data instance xk is the question What was the dominant strain of Flu in 2010? Select among the four choices. with the reference answer H1N1/09 represented by yk. In addition, information about tested capabilities can be provided as metadata, for example as list of keywords such as temporal Q&A, pandemics, history, biology, virology, multiple-choice Q&A, beyond the specific dataset it originates from. Typically, the data samples are obtained via pooling from different benchmarks (Bk)N and we refer to the subset of data instances obtained from benchmark Bk as Dk D. k=1 ii) Models. M=(fbase, f1, . . . , fm) is set of + 1 models, whose capabilities are evaluated with respect k=1 will likely to baseline fbase. An example of fbase is random model. The original benchmarks (Bk)N cover different sets of models MBk for their evaluations. iii) Sample-level Rankings. For each data instance (xj, yj) sample-level ranking sj is created for the subsets of models Mj := MBk(j) where k(j) denotes the index of the benchmark from which the data instance (xj, yj) was collected. Importantly, sample-level rankings are function of the metrics used by the different benchmarks that discard any information about the specifics of the metrics. This is the key to our approach of enabling the aggregation across heterogeneous evaluation paradigms and metrics. More specifically, sj represents an ordinal ranking over the models Mj for sample (xj, yj) represented by permutation σj such that fσj (1) fσj (mj ) where mj = Mj is the number of models compared in the j-th sample-level ranking. In addition, for each we distinguish the case fσ(k1) fσ(k) if fσ(k1) performs better than fσ(k) and fσ(k1) fσ(k) in case of indistinguishable performance. Thus, each sample-level ranking sj can be uniquely determined by mapping σj : {1, . . . , mj} {1, . . . , m} with σj(k) providing the index of the model in that is on the k-th place in the ordering for the j-th sample-level ranking and πj {, }mj 1 defining the corresponding binary sequence of pairwise performance relations. Ordinal Rankings and Information Loss. Using ordinal measurements leads to information loss, which can impede downstream aggregation algorithms due to the data processing inequality ([148], Section 2.8). This principle asserts that any estimation made from processed data cannot outperform estimation based on the original, unprocessed data. However, cardinal measurements frequently suffer from calibration issues, even within single metric [134]. Consequently, in practice, ordinal measurements can paradoxically outperform cardinal ones despite the inherent information loss. 4 iv) Capabilities. To support the selective retrieval of all relevant sample-level rankings in based on user queries, it is possible to endow the sample-level rankings with additional capability C. Of course, modelling the range of capabilities that different evaluators might be interested in is research challenge in itself. Here, we only provide proof of concept, for which we define two categories of capabilities: (1) tasks, like multiple-choice question answering, captioning, and translation, and (2) concepts like makeup, geometry, tarantula, etc. The rationale for this broad interpretation in ONEBench is to explore which capabilities can be reliably evaluated in dynamic, flexible manner through queries. Since the set of capabilities is open-ended, we only tag the data samples with task information. The relevant samples for given concept are retrieved from ONEBench dynamically at test time through semantic search. Lifelong Expansion of ONEBench. The data pool (D) and model IDs (M) are stored as table, while sample-level model measurement (S) are represented as relational database linking these two tables. Constructing lifelong heterogenous benchmark augments D, and with the following operations: B=(D, M, S, insertD, insertM, insertS ). Operations insertD and insertM for expanding the data pool are straightforward: add new samples and new models to the corresponding table. The insertS operation inserts sample-level ranking. Additional metadata is saved to enable retrieval over database rows sharing the same original metric, such as BLEU score, or exact match."
        },
        {
            "title": "2.2 Querying Capabilities for Personalized Evaluation",
            "content": "To evaluate given capability, ONEBench takes dynamic approach. First, we retrieve (retrieveD) samples that match the query. Then, we aggregate (aggregateS,D) the sample-level rankings to produce the overall ranking. Retrieve (retrieveD). In this step, the system selects relevant data instances based on users query. The query language is flexible and allows retrieving data instances that semantically relate to specific topic or match certain criteria. The retrieval is implemented through combination of k-nearest neighbors (kNN) search on dense embeddings using the query as the input and structured queries that take advantage of the unified data schema. We provide extensive empirical analysis to test retrieval quality. Aggregate (AggregateS,D). The sample-level measurements over the retrieved subset of data samples is combined using the random utility modelling approach [163], which defines joint probability distribution over all measurements assuming statistical independence: p(s1, . . . , snγ1, . . . , γm) = n(cid:89) j= p(sj = [.](σj ,πj )γ1, . . . , γm). The Placket-Luce framework assumes the following probability model: (cid:0)sj = [.](σj ,πj ) (cid:1) = γσj (1) mj (cid:88) γσj (k) γσj (2) mj (cid:88) γσj (k) k=1 (cid:124) (cid:123)(cid:122) fσj (1) (cid:125) k=2 (cid:124) (cid:123)(cid:122) fσj (2) (cid:125) γσj (mj 1) γσj (mj 1) + γσj (mj ) (cid:123)(cid:122) (cid:125) (cid:124) fσj (mj ) , defining one parameter γk for each model fk that determines its performance relative to all other models. To aggregate the model performances over all sample-level rankings, we determine the parameters ˆγ1, . . . ˆγm = argmax (γ1,...γm)Rm log p(s1, . . . , snγ1, . . . , γm) with maximum likelihood estimation. The global ranking is given by the permutation σ for which ˆγσ(1) > > ˆγσ(m). The maximum likelihood condition uniquely determines all performance parameters {ˆγk}m k=1, as the likelihood function is strictly concave. The parameters of the Plackett-Luce model are identifiable up to an arbitrary additive constant. Consistency and asymptotic normality can also be shown 5 under certain assumptions about the comparison graph [50]. We refer to the estimated latent variables {ˆγk}m k=1 as model scores. model with higher score is more likely to perform better on randomly picked sample-level task than one with lower score. To fix the arbitrary additive constant, we set the score of the baseline model ˆγbaseline to zero."
        },
        {
            "title": "3 Aggregation in ONEBench: Theory and Practice",
            "content": "We view aggregating sparse ordinal preferences over models through computational social choice lens samples are voters, models are candidates, and the aggregation algorithm is the voting mechanisms [16]. Using established methods, we aggregate ordinal comparisons with partial data to produce global ranking and analyze the properties of this resultant ranking."
        },
        {
            "title": "3.1 Theoretical Foundations",
            "content": "We begin by postulating ground-truth statistical model generating the data, which is converted into ordinal comparisons (S)2. Specifically, we use random-utility model [149], where each model fi is associated with utility distribution Ufi . Preferences between models fi and fj are based on comparing sampled utilities, i.e., fi fj := u(fi) < u(fj), where uf Uf . Since computing maximum likelihood estimates over general random-utility models is computationally hard [163], we focus on the PlackettLuce model [119, 94], the only known exception that allows for tractable maximum likelihood estimates (MLE). Property 1: Identifiability. We first ask: Are the utility distributions for all models recoverable? The Plackett-Luce model allows identifying the utility distribution (up to an arbitrary additive constant) if all models are compared via directed path [57, 163]3. Consistency and asymptotic normality hold under specific assumptions about the comparison graph [50]. Property 2: Sample-Efficient Convergence from Sparse Data. Given that identifiability is asymptotic, we then ask: How sample-efficient is the algorithm for recovering the utility distribution? With partial rankings of size k, the MLE is surprisingly sample efficient while being minmax-optimal [46, 102]. Specifically, sampling model comparisons from the model set independently and uniformly at random for samples induces an expander graph with high probability, which provides guarantees on sample-efficiency of recovery, with = Ω(F )/k samples being necessary, and = Ω(F log )/k samples being sufficient. Efficient algorithms like those in Agarwal et al. [1] and Maystre and Grossglauser [102] achieve these bounds. Rank-breaking techniques, used in our empirical evaluation, also offer near-optimal solutions [141]. Property 3: Active Aggregation. In ONEBench, we can strategically select model comparisons, by framing selection as an online multi-armed bandit problem. Sample efficiency can be improved with PAC guarantees [145, 124, 126], significantly outperforming random comparisons [103]. Property 4: Social Properties. The Plackett-Luce model ensures computational efficiency and recoverability of the underlying ranking. However, to design democratic systems for decision-making, it is essential also to have fair aggregation. Ensuring fairness involves trade-offs[174], because different notions of fairness often conflict, and, depending on the intended application areas, differing or even opposing preferences may be valid [6, 10, 38]. The Plackett-Luce model offers procedural fairness [84] (Section 2.2), satisfying the following three criteria: Anonymity. All voters (samples) are treated equally, ensuring the system does not rely on single vote. The rankings remain unchanged if the input sample set is permuted. Neutrality. The ranking is invariant to the identities of the models, ensuring fairness among alternatives. This means permuting the models similarly permutes the new ranking. 2This contrasts with Zhang and Hardt [174], who view aggregation as classical voting, analysing tradeoffs in aggregating voter preferences rather than uncover an underlying ranking. 3Recall that using the reference model fbase removes the additive ambiguity. 6 Figure 2: Top-10 model ranking changes across different aggregation methods. Our PlackettLuce-based rank aggregation method (Ours) shows the most similarity to the Ground Truth model rankings (GT). However, there is progressive degradation in ranking accuracy for LMArena (LMArena) and Elo (ELO). Comparisons are shown for ONEBench-LLM (top) and ONEBench-LMM (bottom). Our method best preserves the ranking of the top-10 models. Independence from Irrelevant Alternatives (IIA). The relative ranking of two models is unaffected by other alternatives in given sample, as guaranteed by Luces axiom of choice [93]. This provides grounding for incomplete model evaluations."
        },
        {
            "title": "3.2 Translating Theory to Practice",
            "content": "We now empirically validate our framework, aiming to show that: (i) the Plackett-Luce model works well on real-world data, (ii) our aggregation method is sample-efficient, and (iii) it handles high levels of incompleteness. Below, we describe our setup and address these points. 3.2.1 Setup Benchmarks. We conduct experiments using four popular benchmarks with established model rankings based on average scores specific to the benchmark: HELM [79] and Open LLM Leaderboard [8] for LLMs, and VHELM [23] and LMMs-Eval [176] for LMMs. We use the score-based rankings over individual datasets as ground truth and define our sample pool as the sum of the data pools of all consituent datasets of given benchmark. To test the faithfulness of our aggregation strategy we compare the resulting rankings to the original leaderboards. These leaderboards evaluate foundation models across varied tasks with different metrics, serving as good indicators of real-world performance. Ground Truth. The current system of benchmarking involves evaluating models on individual test sets and measuring the mean score per model. This holds even for benchmarks that combine test sets such as HELM [79], VHELM [23], etc. We consider these scores as the ground truth measurement and generate ground truth model ranking from these scores. Since we aggregate benchmarks spanning multiple measurement metrics, we implement min-max normalization of numeric measurements to bring all benchmark samples to the same 0-1 score range. Our final ground truth refers to the model rankings derived from the mean score across all benchmarks. 7 Figure 3: Sample-efficient convergence (top) and robustness to sparsity. Kendall τ between groundtruth ranking and different ranking methods as random individual data samples are dropped (top) and model measurements are randomly removed (bottom). Methods typically remain robust to missing data, with Plackett-Luce consistently achieving higher correlation, even with 95% measurements missing. Methods. We evaluate three model ranking methods: (i) Elo Score [30]: competitive game rating system adapted to rank models through pairwise comparisons, adjusting scores based on wins or losses to reflect win-rate reliability. (ii) LMArena Ranking [18]: method for LLM ranking using the Bradley-Terry model [15], which estimates model rankings through Maximum Likelihood Estimation (MLE) based on pairwise comparisons using an underlying ELO model. (iii) Our Method: Our approach leverages the Plackett-Luce model [102] to aggregate pairwise comparisons using partial rank breaking [141], which achieves significant speed-up for rank estimation methods. Metrics. We compare the rankings generated by each method to the ground-truth from the leaderboards using Kendalls τ , standard correlation metric for rankings. Each method is tested three times, and we report the mean and variance. We additionally check that the top-k models are reliably recovered. 3.2.2 Is Plackett-Luce Good Fit for Real-World Data? Metric HELM Elo Score LMArena Ranking Our Method 0.347 0.132 0.852 0.001 0.877 0.001 Open LLM Leaderboard 0.213 0.065 0.969 0.000 0.997 0.000 VHELM LMMs-Eval 0.639 0.024 0.697 0.000 0.799 0.000 0.332 0.109 0.427 0.000 0.641 0. Table 1: Kendalls τ correlations to ground-truth ranking for different aggregation algorithms. Results show improvements over ELO and LMArena rankings, with notable correlation boosts on ONEBenchLMM leaderboards, including LMMs-Eval (41.65%) and VHELM (14.63%). Q1. Is it good fit? We assess whether the Plackett-Luce model performs well on large-scale benchmark data 8 by comparing our aggregation algorithms rankings to the leaderboard rankings. As shown in Table 1, our algorithm achieves high positive Kendalls τ , indicating strong alignment with the ground truth rankings. Q2. Is it better than current metrics? In addition to evaluating fit, we also compare our method to popular algorithms like Elo and LMArena. Table 1 shows that our algorithm consistently outperforms these methods, demonstrating its superior performance for large real-world datasets. Q3. Are the top-k models preserved? For practitioners, the critical concern is whether the top models are ranked correctly. Figure 2 shows that our algorithm effectively preserves the top-10 model rankings compared to ground truth, while outperforming state-of-the-art methods in maintaining accurate top-k rankings. Conclusion. The Plackett-Luce model fits real-world data well, outperforming other methods in both overall Kendalls τ and top-10 model rankings, proving its effectiveness for large-scale benchmarks. The underlying reason is that we avoid the limitations of Elo-based methods, which rely on assumptions that do not apply to foundation models [13]."
        },
        {
            "title": "3.2.3 Sample Efficiency and Handling Incomplete Rankings",
            "content": "We now empirically test the sample efficiency and robustness to incomplete data of our framework. Q1. Is Our Algorithm Sample-Efficient? We systematically reduce the number of samples and re-rank the models using various methods, calculating Kendalls τ for each. Missing data is simulated from 0% to 99%, with 10% intervals until 90%, followed by 1% increments. As shown in Fig. 3, our method maintains stable performance even with up to 95% samples missing, demonstrating that it can achieve accurate rankings with far fewer data pointsup to 20x less than current benchmarks. Q2. Can our Algorithm Aggregate Highly Sparse Rankings? We evaluate the methods ability to handle highly incomplete data by removing model comparisons for each sample and re-ranking the models. We randomly remove given fraction of model measurements from each sample and re-rank using the three aggregation methods. Again, we simulate data removal from 0% to 99%, with increments as before. As shown in Fig. 3, our method performs well even with 95% fewer model comparisons, proving it can recover accurate rankings with highly sparse data. This is crucial for ONEBench, where models cannot be expected to be evaluated on the entire data pool. Conclusion. Our algorithm provides significant sample efficiency, maintaining accurate rankings with 20x fewer data points, and is robust to highly sparse input rankings."
        },
        {
            "title": "4 ONEBench: Creation & Capability Querying",
            "content": "After evaluating the robustness of our aggregation method across incomplete and heterogeneous measurements, we present the overall system applied to two large-scale, real-world instances of ONEBench for foundation models: LLMs and LMMs. We first outline how these benchmarks were created, then show how to use them to test arbitrary capabilities, and finally highlight key insights gained."
        },
        {
            "title": "4.1 Creation of ONEBench-LLM & ONEBench-LMM",
            "content": "We develop two instantiations of ONEBench: (1) ONEBench-LLM for language models and (2) ONEBenchLMM for multimodal (vision and language) models, as outlined in Fig. 4. 4.1.1 ONEBench-LLM Data Pool D. For ONEBench-LLM (Tab. 2), we source data from the Open LLM Leaderboard [8], HELM [79], and LMArena [18]. Open LLM Leaderboard and HELM aggregate several individual benchmarks (such as MMLU [52] and HellaSwag [170]), while LMArena uses pairwise model comparisons based on user-generated prompts, with user votes determining the superior model. Metrics which are converted to sample-wise ordinal 9 Figure 4: Constituent datasets of ONEBench-LLM (left) and OneBench-LMM (right). We provide task type, metric, and license about each dataset in Tab. 2 and Tab. 3. rankings here include F1-Score, Exact Match (EM), and Quasi-Exact Match (QEM), as well as pairwise preferences from LMArena. Models F. For ONEBench-LLM, we use the 100 most downloaded models from Open LLM Leaderboard and all 79 models from HELM (as of v1.9.0), including both proprietary models like GPT-4o [112] and open-weights ones like LLaMA-3 [105]. full list of evaluated models is provided in Appx. A. 4.1.2 ONEBench-LMM Data Pool D. For ONEBench-LMM (Tab. 3), data is sourced from VHELM, LMMs-Eval, and WildVisionArena. Similar to ONEBench-LLM, VHELM and LMMs-Eval aggregate individual datasets like MMMU [169] and VQAv2 [42], while WildVisionArena uses pairwise tests for LMMs through image-based chats. We convert diverse set of metrics to sample-wise rankings, from binary metrics like EM, QEM, to real-valued scores like ROUGE [81], Perception (P), and Cognition (C) scores from MME [36]. We additionally combine pairwise comparisons from WildVisionArena with LLM-As-A-Judge preferences generated using Prometheus-2 [66], which correlate highly with human judgments. The preference comparisons are sampled randomly from LMMs-Eval while avoiding overlap with cardinal measurements. Models F. For ONEBench-LMM, we use 14 models from LMMs-Eval [176] and 25 models from VHELM [23], including proprietary models like Gemini Pro Vision [146] and open-weights models like LLaVA [85]. complete list of evaluated models is provided in Appx. A."
        },
        {
            "title": "4.2 Capabilities and Concept Probing: Results & Insights",
            "content": "Here, we present empirical results on generating arbitrary test sets and rankings. Our goal is to enable users to make targeted queries within ONEBench and interactively search across the data pool, helping them identify the best models for their needs. To achieve this, we extend our system with flexible mechanism for personalized aggregation, allowing users to (1) retrieve relevant data instances through structured filters and semantic search, and (2) dynamically generate model rankings based on the retrieved samples. Setup. Given query, the system retrieves relevant data samples using combination of flexible semantic 10 Dataset Source Task Size Metric License LegalBench [43] MATH [53] MedQA [59] NarrativeQA [68] NaturalQuestions [71] OpenbookQA [106] WMT 2014 [12] ARC [19] HellaSwag [170] TruthfulQA [82] Winogrande [129] GSM8K [20] MMLU [52] HELM HELM HELM HELM HELM HELM HELM Leaderboard Leaderboard Leaderboard Leaderboard HELM + Leaderboard HELM + Leaderboard Cardinal Legal Maths Medical Openbook QA Search Engine Queries Openbook QA Machine translation General QA Reasoning General QA Reasoning Maths General QA Ordinal 1K 1K 1K 1K 1K 1K 1K 1.1K 10K 817 1.2K 1.3K 13.8K QEM QEM QEM F1 F1 EM BLEU EM EM EM EM QEM EM Unknown MIT MIT Apache-2.0 CC BY-SA 3.0 Apache-2.0 CC-BY-SA-4.0 CC-BY-SA-4.0 MIT Apache-2.0 Apache-2.0 MIT MIT Chatbot Arena [18] Chatbot Arena Pairwise Battles 51K - CC BY 4.0 Table 2: Datasets in ONEBench-LLM. diverse collection of benchmarks testing the abilities of LLMs in areas such as law, medicine, mathematics, question answering, reasoning and instruction following, as well as the performance of LLMs in pairwise battles. search and rigid constraints. This concept querying mechanism provides personalized comparison of foundation model capabilities. We use two querying mechanisms. (i) Semantic search: we perform k-NN lookup in the embedding space of all-MiniLM-L6-v2 [123] for language tasks and SigLIP-B16 [172] for vision-language tasks, using cosine similarity. We retrieve the top-k samples for given concept with tuned cut-off similarity score of 0.3 and 0.7 for ONEBench-LLM and ONEBench-LMM, respectively. (ii) Metadata search: we verify that per-sample metadata satisfies the constraints defined in the query. Some benchmarks, such as MMMU, are equipped with detailed metadata, including categories like image type (diagram, table, etc.), question type (multiple-choice), field (Electronics), and subfield (Analog Electronics). Other benchmarks, such as image captioning datasets (e.g. COCO, NoCaps, Flickr30k), contain little metadata. With these resources, we gather representative samples for individual queries across diverse test sets, and aggregate the ordinal sample-level model rankings using the Plackett-Luce model to produce model ranking for that particular query. Concepts Tested. We curated diverse set of 50 concepts to test the breadth and versatility of ONEBench, ranging from domain-specific knowledge, such as the Coriolis Effect, to broader academic disciplines like Neuroscience, and everyday consumer goods like the Apple iPad. We showcase 6 of them in the main paper and present the rest in Appx. B. We present the results from concept querying in Figure 5 and summarize our insights below: Insight 1. Are the retrieved data samples accurate? Two expert annotators manually reviewed and filtered out incorrect matches 4. To evaluate the quality of the retrieved samples, we report average precision (AP) scores for random subset of queried concepts in Fig. 5, with full list of scores in Appx. B. Aggregating over all tested concepts in Table 4, our mAP over the concepts is 0.84 and 0.73 for ONEBench-LLM and ONEBench-LMM, respectively, demonstrating that we can reliably retrieve samples that match the intended capabilities, although there is substantial scope for improvement in some cases. Please refer to the per-concept AP in Tab. 5 for better indicator of underrepresented concepts in ONEBench. Note that the retrieval mechanism is expected to only improve with better retrieval models, larger test sets that cover more diverse range of capabilities, and the integration of more sophisticated querying mechanisms in ONEBench. 4The inter-annotator agreement, measured by Cohens Kappa, is shown in Table 4, with high values of 0.793 and 0.912 indicating strong consistency between annotators. Dataset Source Task Size Metric License A-OKVQA [133] Bingo [24] Crossmodal-3600 [147] Hateful Memes [64] Mementos [160] MultipanelVQA [33] OODCV-VQA [152] PAIRS [35] Sketchy-VQA [152] AI2D [61] IconQA [88] InfoVQA [101] LLaVA-in-the-Wild [85] ChartQA [99] CMMMU [173] DocVQA [100] MMBench [86] MMVET [167] MP-DocVQA [150] NoCaps [3] OK-VQA [98] RefCOCO [60, 97] ScienceQA [89] TextCaps [139] TextVQA [140] COCO [83] Flickr30k [165] GQA[56] MathVista [90] MME [36] MMMU [169] POPE [78] SEED-Bench [73, 74] VizWiz [44] VQAv2 [42] VHELM VHELM VHELM VHELM VHELM VHELM VHELM VHELM VHELM LMMs-Eval LMMs-Eval LMMs-Eval LMMs-Eval LMMs-Eval LMMs-Eval LMMs-Eval LMMs-Eval LMMs-Eval LMMs-Eval LMMs-Eval LMMs-Eval LMMs-Eval LMMs-Eval LMMs-Eval LMMs-Eval VHELM+LMMs-Eval VHELM+LMMs-Eval VHELM+LMMs-Eval VHELM+LMMs-Eval VHELM+LMMs-Eval VHELM+LMMs-Eval VHELM+LMMs-Eval VHELM+LMMs-Eval VHELM+LMMs-Eval VHELM+LMMs-Eval Cardinal VQA Bias+Hallucination Captioning Hate Speech Sequential Reasoning VQA VQA Bias VQA Maths+Science Docs and Infographics Docs and Infographics Multi-disciplinary Docs and Infographics Multi-disciplinary Docs and Infographics Multi-disciplinary Multi-disciplinary Docs and Infographics Captioning VQA Captioning Maths+Science Captioning VQA Captioning Captioning Scene Understanding Maths+Science Multi-disciplinary Multi-disciplinary Hallucination Multi-disciplinary VQA VQA Ordinal 7.2K 886 1.5K 1K 945 200 1K 508 1K 3.09K 43K 6.1K 60 2.5K 900 10.5K 24K 218 5.2K 4.5K 5.1K 38K 12.6K 3.2K 5K 45.5K 31K 12.6K 1K 2.4K 900 9K 42.5K 4.3K 214K QEM ROUGE ROUGE QEM GPT QEM QEM QEM QEM QEM ANLS ANLS GPT4 QEM QEM ANLS GPT GPT QEM ROUGE ANLS ROUGE EM ROUGE EM ROUGE ROUGE QEM QEM/GPT4 QEM/C+P QEM QEM/EM QEM/EM QEM/EM QEM/EM Apache-2.0 Unknown CC BY-SA 4.0 Custom(Meta) CC-BY-SA-4.0 MIT CC-BY-NC-4.0 Unknown CC-BY-NC-4.0 Apache-2.0 CC BY-SA 4.0 Unknown Apache-2.0 GPL-3.0 CC-BY-4.0 Unknown Apache-2.0 Apache-2.0 MIT MIT Unknown Apache-2.0 CC BY-NC-SA 4.0 CC BY 4.0 CC BY 4.0 CC-BY-4.0 CC-0 Public Domain CC-BY-4.0 CC-BY-SA-4.0 Unknown CC BY-SA 4.0 MIT Apache CC BY 4.0 CC BY 4.0 Vision Arena [91] LMMs-Eval(Prometheus2) [66] - - Pairwise Battles Pairwise Battles 9K 610K - - MIT MIT Table 3: Datasets in ONEBench-LMM: diverse collection of benchmarks testing the abilities of LLMs in tasks such as general VQA, image captioning, hate speech detection, bias and hallucination understanding, maths and science, documents and infographics, scene understanding and sequential reasoning as well as the performance of LMMs in pairwise battles. Benchmark #Concepts Cohen-κ mAP CMC@1 CMC@10 ONEBench-LLM ONEBench-LMM 40 50 0.793 0.912 0.8462 0.7337 0.95 0.94 1.0 0.96 Table 4: Capability Probing (Quantitative): we provide summary of the number of concepts curated for capability probing, along with the inter-annotator agreement and retrieval metrics. Insight 2. Do models perform differently across queries? key check is to verify whether models perform distinctly across different capability queries. If the results are similar regardless of the query, fine-grained querying may be less useful, as the top model from generic leaderboard could be good candidate for any specific capability, as is common practice currently. However, we observe in Fig. 5 and Fig. 6 that different models perform well on different domains and concepts. This suggests that ONEBench returns valid candidate models for arbitrary user queries. 12 Figure 5: Capability Probing (Qualitative): we provide six sample retrieval results for set of queries covering diverse set of topics and report the top-5 models for each query."
        },
        {
            "title": "5 Related Works",
            "content": "Multi-task Benchmarks as Broad Capability Evaluators. Multi-task leaderboards have been the standard for benchmarking foundation models. Examples include GLUE [158], decaNLP [104], SuperGLUE [157], BigBench [142], Dynabench [65], Open LLM Leaderboard [8], CLIP-Benchmark [72], ELEVATOR [75], StableEval [153] and DataComp-38 [37], as well as massive multitask benchmarks like XTREME [138] and ExT5 [5]. However, concerns have arisen regarding the limitations of multi-task benchmarks [14]. Issues include saturation and subsequent discarding of samples [80, 11, 113, 31, 162], susceptibility to dataset selection [25], 13 obscuring progress by evaluation metrics [22, 132], training on test tasks [154, 28, 109, 107, 143, 159], and data contamination [29, 96, 41, 26, 127, 40, 128]. ONEBench tackles these challenges by enabling extensive reuse of samples for broader model comparisons, avoiding task selection bias through democratized sourcing of samples, and using ordinal rankings to avoid evaluation minutia. Sample-level evaluation with sparse inputs also allows selective removal of contaminated data for fairer comparisons. Moreover, by supporting over-ended, evolving evaluation, it makes it harder to train on all test tasks, as opposed to fixed leaderboards that are easier to game. On Aggregation across Benchmarks. The dominant approach to benchmarking has traditionally been multi-task benchmarks, where the most common aggregation strategy is the arithmetic mean of scores across individual tasks. However, this approach assumes that the scoring metrics are homogeneous and scaled correctly, and treat tasks of different complexities equally [108, 117]. In consequence, simple normalization preprocessing influences the rankings [21], and makes them nearly entirely dependent on outlier tasks [2]. Simply changing the aggregation method from arithmetic to geometric or harmonic mean can change the ranking [136]. Similarly, including irrelevant alternative models can change statistical significance or even change the ranking entirely [9, 174]. Mean-aggregation also has significant failure modes in handling missing scores in benchmarks [54]. The benchmarking paradigm is hence shifting towards adopting evaluation principles from other fields, such as non-parametric statistics and social choice theory [16, 125]. We use ordinal rankings instead of scores, similar to LMArena. However, unlike Arena, we use the pairwise variant of the Plackett-Luce model, which has been shown to have advantages both theoretically and empirically [116]. We benefit from some of its theoretical properties like identifiability, sample-efficient convergence, provable robustness to irrelevant alternatives, non-dominance of outliers and empirical robustness across wide range of real-world factors which affect ranking. Moreover, we do not aggregate over benchmarks in the first placeour primary proposal is to avoid monolithic benchmarks and consider aggregation on sample level, needing to tackle incomplete and heterogeneous measurements. We note that several other social-choice theory-based models such as score-based models [125, 137] based on the Condorcet-winner criterion [164] have been proposed, yet they were primarily applied for aggregation on multi-task benchmarks, whereas crucial component of our proposal is to break down the benchmark boundaries and aggregate heterogeneous samples. Dynamic Evaluation and Active Testing. Some previous works like [58, 69, 70, 130, 55, 178] tackle the active testing problem, where the goal is to identify small high-quality test data-subsets, from large pool of uncurated evaluation data. These works typically assume that the cost of unlabeled test data acquisition is low whereas the cost of acquiring per-instance labels is high. However, as pointed out in [122], these assumptions are unrealistic for foundation models, as both the acquisition of test data and label annotations can be tedious in general. Hence, in our work, we tackle broader problem: given large testing data pool, how can we curate and query to produce consistent and targeted set of model rankings? Efficient Evaluation. As evaluation suites have grown, associated inference costs have also increased. Recent research has focused on creating compressed subsets of traditional benchmarks to address this issue [155, 120, 156, 177, 115, 67, 114]. Popular approaches include subsampling benchmarks to preserve correlations with an external source like LMArena [111], sample clustering to gauge sample difficulty and then sub-sampling[156], item-response-theory based methods for informatively sampling subset of samples for evaluation[120], or designing evolving sample-level benchmarks [122]. While the work of Prabhu et al. [122] is similar to us in principle, it requires binary metrics as input and does not handle incomplete input matrices, which is necessary for aggregation over multiple time steps. We precisely address these limitations by showing efficient evaluation while accommodating incomplete data and extending it to ordinal ranks. Democratizing Evaluation. Standard image classification and retrieval benchmarks are collected from platforms like Flickr, which are predominantly Western-centric [4, 135]. This has raised the important question: Progress for whom?, with many seminal works showcasing large disparities in model performance on concepts [51, 110], tasks [47, 48, 49], and even input samples [45, 121, 144] from the Global South. In response, works have developed benchmarks tailored to diverse cultures and demographics to include their voice in measuring progress [92, 110, 121, 118]. Further works have tried to create personalized, task-specific 14 benchmarks for flexibly evaluating models based on user-preferences [17, 77, 131, 168] Zhang et al. [175] created Task-Me-Anything that enables users to input specific queries that then get processed to provide model rankings or responses to the query. However, their system is entirely procedurally generated, thereby not reflecting the real-world use-cases that models are typically subjected to in practice. Further, they are restricted to the fixed set of instances in their task generator pool. We take different approach by creating flexible benchmarks where individuals, and contributing entities, can add their own samples and preferences collected from both real-world benchmarks and live model arenas like LM-Arena, thereby providing users with realistic overview of model rankings on practical scenarios. Further, during capability testing, users can select similar preferences, making ONEBench more inclusive than traditional test sets."
        },
        {
            "title": "6 Conclusions and Open Problems",
            "content": "This work tackles scalable benchmarking of arbitrary capabilities of foundation models, requiring shift from traditional fixed training and test splits, by introducing ONEBench, an open-ended benchmarking framework for foundation models. Our open-source, democratized benchmarking methodology allows diverse evaluation samples and model measurements with detailed metadata. This affords creating customized benchmarks and testing arbitrary capabilities, including using semantic and structured searches. We provide principled aggregation mechanism, that is both theoretically grounded and empirically validated to be robust to incomplete data and heterogeneous measurements across evaluations. We demonstrate the utility of ONEBench in two domains: ONEBench-LLM and ONEBench-LMM, showing how dynamic probing reveals new insights into model performance on specific tasks, domains, or concepts. This combination of theoretical rigour, empirical results, and practical flexibility makes ONEBench valuable tool for comprehensively evaluating foundation models. We provide some promising directions for improvement below: 1. Testing Limits and Scaling Up ONEBench: currently, our prototype comprises less than 100K samples in ONEBench-LLM and under 1M in ONEBench-LMM. These pools can be greatly expanded and diversified by expanding to incorporating all existing LLM and LMM benchmarks. Our retrieval mechanisms are designed to scale efficiently as the test pool grows in size and diversity. 2. Exploring Other Aggregation Algorithms: while we use the Plackett-Luce model for aggregating diverse measurements, there exist other algorithms from computational social choice theory with different trade-offs. comprehensive evaluation of these alternatives could offer new insight for aggregating model performance. 3. Structured Querying and Enhanced Retrieval: One can improve retrieval by better querying mechanisms using models like ColBERT [62] and ColPALI [34], further optimized using DSPy [63]. particularly interesting direction is allowing compositional queries, where users combine multiple queries to test behaviour in foundation models, similar to works like ConceptMix [161] and SkillMix [166]. 4. On the Limits of Capability Probing: While we currently allow broad, open-ended inputs to probe capabilities, some are easier to assess than others [95, 76]. As foundation models become more generalizable, thorough analysis identifying which capabilities can be easily, reliably evaluated, which are possible to evaluate but challenging, and which are in principle impossible to evaluate is neededthis will help improve benchmarking effectiveness."
        },
        {
            "title": "Acknowledgements",
            "content": "The authors would like to thank (in alphabetic order): Nikhil Chandak, Karel DOosterlinck, Shashwat Goel, Shyamgopal Karthik, Balint Mucsanyi and Tim Weiland for their helpful suggestions and feedback. Special thanks to Bo Li for (promptly and consistently) providing sample-level model evaluation logs of LMMs-Eval, Yujie Lu for providing early access to the battle data of Vision Arena and Palzer Lama for creating the logo of ONEBench and helping with figures. The authors would also like to thank Federico DAgostino for recommending the title of the paper and the name of the benchmark. AP and MB acknowledge financial support via the Open Philanthropy Foundation funded by the Good Ventures Foundation. VU and SD thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) and the European Laboratory for Learning and Intelligent Systems (ELLIS) PhD program for support. VU is supported by Google PhD Fellowship in Machine Intelligence. MB acknowledges support via the CRC 1233 on Robust Vision and from the Machine Learning Cluster of Excellence, funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germanys Excellence Strategy EXC number 2064/1 Project number 390727645."
        },
        {
            "title": "References",
            "content": "[1] Arpit Agarwal, Prathamesh Patil, and Shivani Agarwal. Accelerated spectral ranking. In International Conference on Machine Learning, pages 7079. PMLR, 2018. 6 [2] Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc Bellemare. Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34:2930429320, 2021. 14 [3] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In Proceedings of the IEEE/CVF international conference on computer vision, pages 89488957, 2019. 12 [4] Amith Ananthram, Elias Stengel-Eskin, Carl Vondrick, Mohit Bansal, and Kathleen McKeown. See it from my perspective: Diagnosing the western cultural bias of large vision-language models in image understanding. arXiv preprint arXiv:2406.11665, 2024. [5] Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Tran, Dara Bahri, Jianmo Ni, et al. Ext5: Towards extreme multi-task scaling for transfer learning. arXiv preprint arXiv:2111.10952, 2021. 13 [6] Kenneth Arrow. difficulty in the concept of social welfare. Journal of political economy, 58(4):328346, 1950. 6 [7] Hritik Bansal and Pratyush Maini. Peeking behind closed doors: Risks of llm evaluation by private data curators. 2024. URL https://pratyushmaini.github.io/blog/2024/risks-private-evals/. Accessed November 27, 2024. 2 [8] Edward Beeching, Clementine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard. https://huggingface.co/spaces/ HuggingFaceH4/open_llm_leaderboard, 2023. 3, 7, 9, 13, 2 [9] Alessio Benavoli, Giorgio Corani, and Francesca Mangili. Should we really use post-hoc tests based on mean-ranks? The Journal of Machine Learning Research, 17(1):152161, 2016. 14 [10] Jean-Pierre Benoıt. The gibbardsatterthwaite theorem: simple proof. Economics Letters, 69(3):319322, 2000. 6 [11] Lucas Beyer, Olivier Henaff, Alexander Kolesnikov, Xiaohua Zhai, and Aaron van den Oord. Are we done with imagenet? In Conference on Neural Information Processing Systems (NeurIPS), 2021. 13 [12] Ondˇrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the ninth workshop on statistical machine translation, pages 1258, 2014. 16 [13] Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker, and Marzieh Fadaee. Elo uncovered: Robustness and best practices in language model evaluation. arXiv preprint arXiv:2311.17295, 2023. 9 [14] Samuel Bowman and George Dahl. What will it take to fix benchmarking in natural language understanding? arXiv preprint arXiv:2104.02145, 2021. 13 [15] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. 3, 8 [16] Felix Brandt, Vincent Conitzer, Ulle Endriss, Jerˆome Lang, and Ariel Procaccia. Introduction to computational social choice. Handbook of Computational Social Choice, pages 129, 2016. 6, 14 [17] Natasha Butt, Varun Chandrasekaran, Neel Joshi, Besmira Nushi, and Vidhisha Balachandran. Benchagents: Automated benchmark creation with agent interaction. arXiv preprint arXiv:2410.22584, 2024. 15 [18] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132, 2024. 3, 8, 9, [19] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. 11 [20] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 11 [21] Pierre Colombo, Nathan Noiry, Ekhine Irurozki, and Stephan Clemencon. What are the best systems? new perspectives on nlp benchmarking. Advances in Neural Information Processing Systems, 35:2691526932, 2022. 14 [22] Pierre Jean Colombo, Chloe Clavel, and Pablo Piantanida. Infolm: new metric to evaluate summarization & data2text generation. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages 1055410562, 2022. 14 [23] CRFM. The first steps to holistic evaluation of vision-language models. https://crfm.stanford.edu/helm/ vhelm/latest/, 2024. URL https://crfm.stanford.edu/helm/vhelm/latest/. Accessed: 2024-06-15. 3, 7, 10 [24] Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges. arXiv preprint arXiv:2311.03287, 2023. [25] Mostafa Dehghani, Yi Tay, Alexey Gritsenko, Zhe Zhao, Neil Houlsby, Fernando Diaz, Donald Metzler, and Oriol Vinyals. The benchmark lottery. arXiv preprint arXiv:2107.07002, 2021. 13 [26] Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, and Arman Cohan. Investigating data contamination in modern benchmarks for large language models. arXiv preprint arXiv:2311.09783, 2023. 14 [27] Sanchari Dhar and Lior Shamir. Evaluation of the benchmark datasets for testing the efficacy of deep convolutional neural networks. Visual Informatics, 5(3):92101, 2021. [28] Ricardo Dominguez-Olmedo, Florian Dorner, and Moritz Hardt. Training on the test task confounds evaluation and emergence. arXiv preprint arXiv:2407.07890, 2024. 14 [29] Aparna Elangovan, Jiayuan He, and Karin Verspoor. Memorization vs. generalization: Quantifying data leakage in nlp performance evaluation. arXiv preprint arXiv:2102.01818, 2021. 14 [30] Arpad Elo. The proposed uscf rating system, its development, theory, and applications. Chess life, 22(8): 242247, 1967. 3, [31] Kawin Ethayarajh and Dan Jurafsky. Utility is in the eye of the user: critique of nlp leaderboards. arXiv preprint arXiv:2009.13888, 2020. 13 17 [32] Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with v-usable information. In International Conference on Machine Learning (ICML), 2022. 2 [33] Yue Fan, Jing Gu, Kaiwen Zhou, Qianqi Yan, Shan Jiang, Ching-Chen Kuo, Xinze Guan, and Xin Eric Wang. Muffin or chihuahua? challenging large vision-language models with multipanel vqa. arXiv preprint arXiv:2401.15847, 2024. [34] Manuel Faysse, Hugues Sibille, Tony Wu, Gautier Viaud, Celine Hudelot, and Pierre Colombo. Colpali: Efficient document retrieval with vision language models. arXiv preprint arXiv:2407.01449, 2024. 15 [35] Kathleen Fraser and Svetlana Kiritchenko. Examining gender and racial bias in large visionlanguage models using novel dataset of parallel images. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 690713. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.eacl-long.41. 12 [36] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 10, 12 [37] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. In Conference on Neural Information Processing Systems (NeurIPS), 2023. 13 [38] Mark Garman and Morton Kamien. The paradox of voting: Probability calculations. Behavioral Science, (4):306316, 1968. 6 [39] Yingqiang Ge, Wenyue Hua, Kai Mei, Juntao Tan, Shuyuan Xu, Zelong Li, Yongfeng Zhang, et al. Openagi: When llm meets domain experts. Advances in Neural Information Processing Systems, 36, 2024. 2 [40] Shahriar Golchin and Mihai Surdeanu. Data contamination quiz: tool to detect and estimate contamination in large language models. arXiv preprint arXiv:2311.06233, 2023. 14 [41] Shahriar Golchin and Mihai Surdeanu. Time travel in LLMs: Tracing data contamination in large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=2Rwq6c3tvr. [42] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913, 2017. 10, 12 [43] Neel Guha, Julian Nyarko, Daniel Ho, Christopher Re, Adam Chilton, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore, Diego Zambrano, et al. Legalbench: collaboratively built benchmark for measuring legal reasoning in large language models. Advances in Neural Information Processing Systems, 36, 2024. 11 [44] Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36083617, 2018. 12 [45] Laura Gustafson, Megan Richards, Melissa Hall, Caner Hazirbas, Diane Bouchacourt, and Mark Ibrahim. Exploring why object recognition performance degrades across income levels and geographies with factor annotations. Advances in Neural Information Processing Systems, 36, 2024. 14 [46] Bruce Hajek, Sewoong Oh, and Jiaming Xu. Minimax-optimal inference from partial rankings. Advances in Neural Information Processing Systems, 27, 2014. [47] Melissa Hall, Bobbie Chern, Laura Gustafson, Denisse Ventura, Harshad Kulkarni, Candace Ross, and Nicolas Usunier. Towards reliable assessments of demographic disparities in multi-label image classifiers. arXiv preprint arXiv:2302.08572, 2023. 14 18 [48] Melissa Hall, Candace Ross, Adina Williams, Nicolas Carion, Michal Drozdzal, and Adriana Romero Soriano. Dig in: Evaluating disparities in image generations with indicators for geographic diversity. arXiv preprint arXiv:2308.06198, 2023. 14 [49] Melissa Hall, Samuel Bell, Candace Ross, Adina Williams, Michal Drozdzal, and Adriana Romero Soriano. Towards geographic inclusion in the evaluation of text-to-image models. In The 2024 ACM Conference on Fairness, Accountability, and Transparency, pages 585601, 2024. 14 [50] Ruijian Han and Yiming Xu. unified analysis of likelihood-based estimators in the plackettluce model. arXiv preprint arXiv:2306.02821, 2023. [51] Reyhane Askari Hemmat, Melissa Hall, Alicia Sun, Candace Ross, Michal Drozdzal, and Adriana RomeroSoriano. Improving geo-diversity of generated images with contextualized vendi score guidance. arXiv preprint arXiv:2406.04551, 2024. 14 [52] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. International Conference on Learning Representations (ICLR), 2021. 9, 11 [53] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. 11 [54] Anas Himmi, Ekhine Irurozki, Nathan Noiry, Stephan Clemencon, and Pierre Colombo. Towards more robust nlp system evaluation: Handling missing scores in benchmarks. arXiv preprint arXiv:2305.10284, 2023. 14 [55] Yuheng Huang, Jiayang Song, Qiang Hu, Felix Juefei-Xu, and Lei Ma. Active testing of large language model via multi-stage sampling. arXiv preprint arXiv:2408.03573, 2024. 14 [56] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 12 [57] David Hunter. Mm algorithms for generalized bradley-terry models. The annals of statistics, 32(1):384406, 2004. 6 [58] Disi Ji, Robert Logan, Padhraic Smyth, and Mark Steyvers. Active bayesian assessment of black-box classifiers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 79357944, 2021. 14 [59] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021. [60] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. 12 [61] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer, 2016. 12 [62] Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late In Proceedings of the 43rd International ACM SIGIR conference on research and interaction over bert. development in Information Retrieval, pages 3948, 2020. 15 [63] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714, 2023. 15 [64] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. Advances in neural information processing systems, 33:26112624, 2020. 19 [65] Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, et al. Dynabench: Rethinking benchmarking in nlp. North American Chapter of the Association for Computational Linguistics (NAACL), 2021. 13 [66] Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other language models. arXiv preprint arXiv:2405.01535, 2024. 10, 12 [67] Alex Kipnis, Konstantinos Voudouris, Luca Schulze Buschoff, and Eric Schulz. metabencha sparse benchmark to measure general ability in large language models. arXiv preprint arXiv:2407.12844, 2024. 14 [68] Tomaˇs Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328, 2018. 11 [69] Jannik Kossen, Sebastian Farquhar, Yarin Gal, and Tom Rainforth. Active testing: Sample-efficient model evaluation. In International Conference on Machine Learning (ICML), 2021. 14 [70] Jannik Kossen, Sebastian Farquhar, Yarin Gal, and Thomas Rainforth. Active surrogate estimators: An active learning approach to label-efficient model evaluation. Conference on Neural Information Processing Systems (NeurIPS), 2022. 14 [71] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. 11 [72] LAION-AI. Clip benchmark, 2024. URL https://github.com/LAION-AI/CLIP_benchmark. Accessed: 2024-0615. 13 [73] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 12 [74] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. 12 [75] Chunyuan Li, Haotian Liu, Liunian Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, et al. Elevater: benchmark and toolkit for evaluating language-augmented visual models. Advances in Neural Information Processing Systems, 35:92879301, 2022. 13 [76] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. 15 [77] Xiang Lisa Li, Evan Zheran Liu, Percy Liang, and Tatsunori Hashimoto. Autobencher: Creating salient, novel, difficult datasets for language models. arXiv preprint arXiv:2407.08351, 2024. [78] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. 12 [79] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. Transactions on Machine Learning Research, 2023. URL https://openreview.net/forum?id=iO4LZibEqW. 3, 7, 9 [80] Thomas Liao, Rohan Taori, Inioluwa Deborah Raji, and Ludwig Schmidt. Are we learning yet? meta review of evaluation failures across machine learning. In Conference on Neural Information Processing Systems (NeurIPS), 2021. 13 [81] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. 10 [82] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32143252, 2022. 11 [83] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision (ECCV), 2014. 12 [84] Christian List. Social Choice Theory. In Edward N. Zalta and Uri Nodelman, editors, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Winter 2022 edition, 2022. 6 [85] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 10, [86] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 12 [87] Zhuang Liu and Kaiming He. decades battle on dataset bias: Are we there yet? arXiv preprint arXiv:2403.08632, 2024. 2 [88] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: new benchmark for abstract diagram understanding and visual language reasoning. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. 12 [89] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. [90] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=KUNzEQMWU7. 12 [91] Yujie Lu, Dongfu Jiang, Wenhu Chen, William Wang, Yejin Choi, and Bill Yuchen Lin. Wildvision arena: Benchmarking multimodal llms in the wild, February 2024. URL https://huggingface.co/spaces/WildVision/ vision-arena/. 3, 12 [92] Alexandra Sasha Luccioni and David Rolnick. Bugs in the data: How imagenet misrepresents biodiversity. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 1438214390, 2023. 14 [93] Duncan Luce. Individual choice behavior, volume 4. Wiley New York, 1959. 3, 7 [94] Duncan Luce. The choice axiom after twenty years. Journal of mathematical psychology, 15(3):215233, 1977. 6 [95] Netta Madvil, Yonatan Bitton, and Roy Schwartz. Read, look or listen? whats needed for solving multimodal dataset. arXiv preprint arXiv:2307.04532, 2023. 15 [96] Inbal Magar and Roy Schwartz. Data contamination: From memorization to exploitation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 157165, 2022. 14 [97] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1120, 2016. 12 [98] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 31953204, 2019. 21 [99] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, 2022. 12 [100] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 12 [101] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. 12 [102] Lucas Maystre and Matthias Grossglauser. Fast and accurate inference of plackettluce models. Advances in neural information processing systems, 28, 2015. 6, [103] Lucas Maystre and Matthias Grossglauser. Just sort it! simple and effective approach to active preference learning. In International Conference on Machine Learning, pages 23442353. PMLR, 2017. 6 [104] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018. 13 [105] Meta. Introducing meta llama 3: The most capable openly available llm to date, April 2024. URL https: //ai.meta.com/blog/meta-llama-3/. Accessed: 2024-06-15. [106] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23812391, 2018. 11 [107] Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. arXiv preprint arXiv:2410.05229, 2024. 14 [108] Swaroop Mishra and Anjana Arunkumar. How robust are model rankings: leaderboard customization approach for equitable evaluation. In Proceedings of the AAAI conference on Artificial Intelligence, volume 35, pages 1356113569, 2021. 14 [109] Marianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, and Jenia Jitsev. Alice in wonderland: Simple tasks showing complete reasoning breakdown in state-of-the-art large language models. arXiv preprint arXiv:2406.02061, 2024. 14 [110] Thao Nguyen, Matthew Wallingford, Sebastin Santy, Wei-Chiu Ma, Sewoong Oh, Ludwig Schmidt, Pang Wei Koh, and Ranjay Krishna. Multilingual diversity improves vision-language representations. arXiv preprint arXiv:2405.16915, 2024. 14 [111] Jinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, and Yang You. Mixeval: Deriving wisdom of the crowd from llm benchmark mixtures. arXiv preprint arXiv:2406.06565, 2024. 2, [112] OpenAI. Hello gpt-4o, May 2024. URL https://openai.com/index/hello-gpt-4o/. Accessed: 2024-06-15. 10 [113] Simon Ott, Adriano Barbosa-Silva, Kathrin Blagec, Jan Brauner, and Matthias Samwald. Mapping global dynamics of benchmark creation and saturation in artificial intelligence. Nature Communications, 13(1):6793, 2022. 13 [114] Lorenzo Pacchiardi, Lucy Cheke, and Jose Hernandez-Orallo. 100 instances is all you need: predicting the success of new llm on unseen data by testing on few instances. arXiv preprint arXiv:2409.03563, 2024. 14 [115] Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, and Leshem Choshen. Efficient benchmarking (of language models). In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 25192536, 2024. 14 22 [116] Maxime Peyrard, Wei Zhao, Steffen Eger, and Robert West. Better than average: Paired evaluation of nlp systems. arXiv preprint arXiv:2110.10746, 2021. 14 [117] Matuˇs Pikuliak and Marian ˇSimko. Average is not enough: Caveats of multilingual evaluation. arXiv preprint arXiv:2301.01269, 2023. 14 [118] Giada Pistilli, Alina Leidinger, Yacine Jernite, Atoosa Kasirzadeh, Alexandra Sasha Luccioni, and Margaret Mitchell. Civics: Building dataset for examining culturally-informed values in large language models. arXiv preprint arXiv:2405.13974, 2024. 14 [119] Robin Plackett. The analysis of permutations. Journal of the Royal Statistical Society Series C: Applied Statistics, 24(2):193202, 1975. 3, [120] Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, and Mikhail Yurochkin. tinybenchmarks: evaluating llms with fewer examples. arXiv preprint arXiv:2402.14992, 2024. 14 [121] Angeline Pouget, Lucas Beyer, Emanuele Bugliarello, Xiao Wang, Andreas Peter Steiner, Xiaohua Zhai, and Ibrahim Alabdulmohsin. No filter: Cultural and socioeconomic diversityin contrastive vision-language models. arXiv preprint arXiv:2405.13777, 2024. 14 [122] Ameya Prabhu, Vishaal Udandarao, Philip Torr, Matthias Bethge, Adel Bibi, and Samuel Albanie. Lifelong benchmarks: Efficient model evaluation in an era of rapid progress. arXiv preprint arXiv:2402.19472, 2024. 14 [123] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908.10084. [124] Wenbo Ren, Jia Liu, and Ness Shroff. Pac ranking from pairwise and listwise queries: Lower bounds and upper bounds. arXiv preprint arXiv:1806.02970, 2018. 6 [125] Mark Rofin, Vladislav Mikhailov, Mikhail Florinskiy, Andrey Kravchenko, Elena Tutubalina, Tatiana Shavrina, Daniel Karabekyan, and Ekaterina Artemova. Votenrank: Revision of benchmarking with social choice theory. Annual Meeting of the Association for Computational Linguistics (EACL), 2022. 14 [126] Aadirupa Saha and Aditya Gopalan. Pac battling bandits in the plackett-luce model. In Algorithmic Learning Theory, pages 700737. PMLR, 2019. 6 [127] Oscar Sainz, Jon Ander Campos, Iker Garcıa-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. [128] Oscar Sainz, Iker Garcıa-Ferrero, Alon Jacovi, Jon Ander Campos, Yanai Elazar, Eneko Agirre, Yoav Goldberg, Wei-Lin Chen, Jenny Chim, Leshem Choshen, et al. Data contamination report from the 2024 conda shared task. arXiv preprint arXiv:2407.21530, 2024. 14 [129] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. 11 [130] Gayathri Saranathan, Mahammad Parwez Alam, James Lim, Suparna Bhattacharya, Soon Yee Wong, Martin Foltin, and Cong Xu. Dele: Data efficient llm evaluation. In ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models, 2024. 14 [131] Michael Saxon, Ari Holtzman, Peter West, William Yang Wang, and Naomi Saphra. Benchmarks as microscopes: call for model metrology. arXiv preprint arXiv:2407.16711, 2024. [132] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models mirage? Advances in Neural Information Processing Systems, 36, 2023. 14 [133] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world knowledge. In European conference on computer vision, pages 146162. Springer, 2022. 12 23 [134] Nihar Shah, Sivaraman Balakrishnan, Joseph Bradley, Abhay Parekh, Kannan Ramchandran, and Martin Wainwright. When is it better to compare than to score? arXiv preprint arXiv:1406.6618, 2014. [135] Shreya Shankar, Yoni Halpern, Eric Breck, James Atwood, Jimbo Wilson, and Sculley. No classification without representation: Assessing geodiversity issues in open data sets for the developing world. arXiv preprint arXiv:1711.08536, 2017. 14 [136] Tatiana Shavrina and Valentin Malykh. How not to lie with benchmark: Rearranging nlp leaderboards. In (Still) Cant Believe Its Not Better! NeurIPS 2021 Workshop, 2021. 14 [137] Valeriy Shevchenko, Nikita Belousov, Alexey Vasilev, Vladimir Zholobov, Artyom Sosedka, Natalia Semenova, Anna Volodkevich, Andrey Savchenko, and Alexey Zaytsev. From variability to stability: Advancing recsys benchmarking practices. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 57015712, 2024. 14 [138] Aditya Siddhant, Junjie Hu, Melvin Johnson, Orhan Firat, and Sebastian Ruder. Xtreme: massively multilingual multi-task benchmark for evaluating cross-lingual generalization. In Proceedings of the International Conference on Machine Learning 2020, pages 44114421, 2020. 13 [139] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: dataset for image captioning with reading comprehension. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 742758. Springer, 2020. [140] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. 12 [141] Hossein Azari Soufiani, David Parkes, and Lirong Xia. Computing parametric ranking models via rank-breaking. In International Conference on Machine Learning, pages 360368. PMLR, 2014. 6, 8 [142] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. 13 [143] Saurabh Srivastava, Anto PV, Shashank Menon, Ajay Sukumar, Alan Philipose, Stevin Prince, Sooraj Thomas, et al. Functional benchmarks for robust evaluation of reasoning performance, and the reasoning gap. arXiv preprint arXiv:2402.19450, 2024. 14 [144] Abhishek Sureddy, Dishant Padalia, Nandhinee Periyakaruppa, Oindrila Saha, Adina Williams, Adriana RomeroSoriano, Megan Richards, Polina Kirichenko, and Melissa Hall. Decomposed evaluations of geographic disparities in text-to-image models. arXiv preprint arXiv:2406.11988, 2024. [145] Balazs Szorenyi, Robert Busa-Fekete, Adil Paul, and Eyke Hullermeier. Online rank elicitation for plackett-luce: dueling bandits approach. Advances in neural information processing systems, 28, 2015. 6 [146] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 10 [147] Ashish Thapliyal, Jordi Pont Tuset, Xi Chen, and Radu Soricut. Crossmodal-3600: massively multilingual multimodal evaluation dataset. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 715729, 2022. 12 [148] MTCAJ Thomas and Thomas Joy. Elements of information theory. Wiley-Interscience, 2006. 4 [149] Louis Leon Thurstone. Three psychophysical laws. Psychological Review, 34(6):424, 1927. [150] Rub`en Tito, Dimosthenis Karatzas, and Ernest Valveny. Hierarchical multimodal transformers for multipage docvqa. Pattern Recognition, 144:109834, 2023. 12 24 [151] Antonio Torralba and Alexei Efros. Unbiased look at dataset bias. In Conference on Computer Vision and Pattern Recognition (CVPR), 2011. 2 [152] Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou, Huaxiu Yao, and Cihang Xie. How many unicorns are in this image? safety evaluation benchmark for vision llms. arXiv preprint arXiv:2311.16101, 2023. [153] Vishaal Udandarao, Nikhil Parthasarathy, Muhammad Ferjad Naeem, Talfan Evans, Samuel Albanie, Federico Tombari, Yongqin Xian, Alessio Tonioni, and Olivier Henaff. Active data curation effectively distills large-scale multimodal models. arXiv preprint arXiv:2411.18674, 2024. 13 [154] Vishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge. No zero-shot without exponential data: Pretraining concept frequency determines multimodal model performance. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 14 [155] Neeraj Varshney, Swaroop Mishra, and Chitta Baral. Ildae: Instance-level difficulty analysis of evaluation data. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 34123425, 2022. 14 [156] Rajan Vivek, Kawin Ethayarajh, Diyi Yang, and Douwe Kiela. Anchor points: Benchmarking models with much fewer examples. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15761601, 2024. 14 [157] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: stickier benchmark for general-purpose language understanding systems. Conference on Neural Information Processing Systems (NeurIPS), 2019. 13 [158] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rJ4km2R5t7. [159] Siyuan Wang, Zhuohan Long, Zhihao Fan, Zhongyu Wei, and Xuanjing Huang. Benchmark self-evolving: multi-agent framework for dynamic llm evaluation. arXiv preprint arXiv:2402.11443, 2024. 14 [160] Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, et al. Mementos: comprehensive benchmark for multimodal large language model reasoning over image sequences. arXiv preprint arXiv:2401.10529, 2024. 12 [161] Xindi Wu, Dingli Yu, Yangsibo Huang, Olga Russakovsky, and Sanjeev Arora. Conceptmix: compositional image generation benchmark with controllable difficulty. arXiv preprint arXiv:2408.14339, 2024. 15 [162] Chunqiu Steven Xia, Yinlin Deng, and Lingming Zhang. Top leaderboard ranking= top coding proficiency, always? evoeval: Evolving coding benchmarks via llm. arXiv preprint arXiv:2403.19114, 2024. 13 [163] Lirong Xia. Learning and decision-making from rank data. Morgan & Claypool Publishers, 2019. 5, 6 [164] Peyton Young. Condorcets theory of voting. American Political science review, 82(4):12311244, 1988. 14 [165] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:6778, 2014. 12 [166] Dingli Yu, Simran Kaur, Arushi Gupta, Jonah Brown-Cohen, Anirudh Goyal, and Sanjeev Arora. Skill-mix: flexible and expandable family of evaluations for ai models. arXiv preprint arXiv:2310.17567, 2023. 15 [167] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. MM-vet: Evaluating large multimodal models for integrated capabilities. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=KOTutrSR2y. 25 [168] Xiaohan Yuan, Jinfeng Li, Dongxia Wang, Yuefeng Chen, Xiaofeng Mao, Longtao Huang, Hui Xue, Wenhai Wang, Kui Ren, and Jingyi Wang. S-eval: Automatic and adaptive test generation for benchmarking safety evaluation of large language models. arXiv preprint arXiv:2405.14191, 2024. 15 [169] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 10, 12 [170] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, 2019. 9, 11 [171] Boya Zeng, Yida Yin, and Zhuang Liu. Understanding bias in large-scale visual datasets. arXiv preprint arXiv:2412.01876, 2024. [172] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pretraining. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. 11 [173] Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, et al. Cmmmu: chinese massive multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2401.11944, 2024. 12 [174] Guanhua Zhang and Moritz Hardt. Inherent trade-offs between diversity and stability in multi-task benchmarks. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id= fwxnHViGNj. 6, 14 [175] Jieyu Zhang, Weikai Huang, Zixian Ma, Oscar Michel, Dong He, Tanmay Gupta, Wei-Chiu Ma, Ali Farhadi, Aniruddha Kembhavi, and Ranjay Krishna. Task me anything. arXiv preprint arXiv:2406.11775, 2024. 15 [176] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, et al. Lmms-eval: Reality check on the evaluation of large multimodal models. arXiv preprint arXiv:2407.12772, 2024. 3, 7, 10, 5 [177] Lin Zhao, Tianchen Zhao, Zinan Lin, Xuefei Ning, Guohao Dai, Huazhong Yang, and Yu Wang. Flasheval: In Proceedings of the Towards fast and accurate evaluation of text-to-image diffusion generative models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1612216131, 2024. 14 [178] Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, and Xing Xie. Dyval: Graph-informed dynamic evaluation of large language models. arXiv preprint arXiv:2309.17167, 2023."
        },
        {
            "title": "Table of Contents",
            "content": "A Models used in ONEBench:Further Details A.1 ONEBench-LLM: Open LLM Leaderboard . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 ONEBench-LLM: HELM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 ONEBench-LMM: LMMs-Eval A.4 ONEBench-LMM: VHELM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Capability Testing Across Arbitrary Queries B.1 Queries: List and Additional Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2 3 5 5 6 1 Models used in ONEBench:Further Details In this section, we provide deeper insight into the models used in the creation of ONEBench. It is important to note that ONEBench-LLM and ONEBench-LMM have complementary characteristics: while ONEBench-LLM has fewer data samples Dk, they are evaluated on more models Mk, while ONEBench-LMM contains (significantly) more data samples but they are evaluated on less models. A.1 ONEBench-LLM: Open LLM Leaderboard The Open LLM Leaderboard [8] was created to track progress of LLMs in the open-source community by evaluating models on the same data samples and setup for more reproducible results and trustworthy leaderboard where all open-sourced LLMs could be ranked. However, due to the abundance of models found on the leaderboard and the lack of adequate documentation, and therefore reliability, of many of these models being evaluated, we rank the models based on the number of downloads, as metric of adoption of these models by the community. We provide the total list of models as an artefact and list the top 100 models below: 1. 01-ai/Yi-34B-200K 2. AI-Sweden-Models/gpt-sw3-126m 3. BioMistral/BioMistral-7B 4. CohereForAI/c4ai-command-r-plus 5. CohereForAI/c4ai-command-r-v01 6. Deci/DeciLM-7B-instruct 7. EleutherAI/llemma 7b 8. EleutherAI/pythia-410m 9. Felladrin/Llama-160M-Chat-v1 10. Felladrin/Llama-68M-Chat-v1 11. FreedomIntelligence/AceGPT-7B 12. GritLM/GritLM-7B 13. Intel/neural-chat-7b-v3-1 14. JackFram/llama-160m 15. Nexusflow/NexusRaven-V2-13B 16. Nexusflow/Starling-LM-7B-beta 17. NousResearch/Hermes-2-Pro-Mistral-7B 18. NousResearch/Meta-Llama-3-8B-Instruct 19. NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO 31. Qwen/Qwen1.5-4B 32. Qwen/Qwen1.5-4B-Chat 33. Qwen/Qwen1.5-72B-Chat 34. Qwen/Qwen1.5-7B 35. Qwen/Qwen1.5-7B-Chat 36. SeaLLMs/SeaLLM-7B-v2 37. TinyLlama/TinyLlama-1.1B-Chat-v1.0 38. TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T 39. VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct 40. abhishekchohan/mistral-7B-forest-dpo 41. ahxt/LiteLlama-460M-1T 42. ai-forever/mGPT 43. alignment-handbook/zephyr-7b-sft-full 44. augmxnt/shisa-gamma-7b-v1 45. bigcode/starcoder2-15b 46. bigcode/starcoder2-3b 47. bigcode/starcoder2-7b 48. cloudyu/Mixtral 7Bx4 MOE 24B 49. codellama/CodeLlama-70b-Instruct-hf 50. cognitivecomputations/dolphin-2.2.1-mistral-7b 51. cognitivecomputations/dolphin-2.6-mistral-7b-dpo 20. NousResearch/Nous-Hermes-2-SOLAR-10.7B 52. cognitivecomputations/dolphin-2.9-llama3-8b 21. NousResearch/Nous-Hermes-2-Yi-34B 22. OpenPipe/mistral-ft-optimized-1227 23. Qwen/Qwen1.5-0.5B 24. Qwen/Qwen1.5-0.5B-Chat 25. Qwen/Qwen1.5-1.8B 26. Qwen/Qwen1.5-1.8B-Chat 27. Qwen/Qwen1.5-110B-Chat 28. Qwen/Qwen1.5-14B 29. Qwen/Qwen1.5-14B-Chat 30. Qwen/Qwen1.5-32B-Chat 53. daekeun-ml/phi-2-ko-v0.1 54. deepseek-ai/deepseek-coder-1.3b-instruct 55. deepseek-ai/deepseek-coder-6.7b-base 56. deepseek-ai/deepseek-coder-6.7b-instruct 57. deepseek-ai/deepseek-coder-7b-instruct-v1.5 58. deepseek-ai/deepseek-math-7b-base 59. deepseek-ai/deepseek-math-7b-instruct 60. deepseek-ai/deepseek-math-7b-rl 61. google/codegemma-7b-it 62. google/gemma-1.1-7b-it 63. google/gemma-2b 2 64. google/gemma-2b-it 65. google/gemma-7b 66. google/gemma-7b-it 67. google/recurrentgemma-2b-it 68. h2oai/h2o-danube2-1.8b-chat 69. hfl/chinese-alpaca-2-13b 70. ibm/merlinite-7b 71. meta-llama/Meta-Llama-3-70B 72. meta-llama/Meta-Llama-3-70B-Instruct 73. meta-llama/Meta-Llama-3-8B 74. meta-llama/Meta-Llama-3-8B-Instruct 75. meta-math/MetaMath-Mistral-7B 76. microsoft/Orca-2-7b 77. microsoft/phi78. mistral-community/Mistral-7B-v0.2 79. mistral-community/Mixtral-8x22B-v0.1 80. mistralai/Mistral-7B-Instruct-v0.2 81. mistralai/Mixtral-8x22B-Instruct-v0.1 83. mistralai/Mixtral-8x7B-v0.1 84. openai-community/gpt 85. openai-community/gpt2-large 86. openchat/openchat-3.5-0106 87. openchat/openchat-3.5-1210 88. openchat/openchat 3.5 89. sarvamai/OpenHathi-7B-Hi-v0.1-Base 90. speakleash/Bielik-7B-Instruct-v0. 91. speakleash/Bielik-7B-v0.1 92. stabilityai/stablelm-2-1 6b 93. stabilityai/stablelm-2-zephyr-1 6b 94. stabilityai/stablelm-zephyr-3b 95. teknium/OpenHermes-2.5-Mistral-7B 96. tokyotech-llm/Swallow-70b-instruct-hf 97. upstage/SOLAR-10.7B-Instruct-v1.0 98. upstage/SOLAR-10.7B-v1.0 99. wenbopan/Faro-Yi-9B 82. mistralai/Mixtral-8x7B-Instruct-v0.1 100. yanolja/EEVE-Korean-Instruct-10.8B-v1.0 A.2 ONEBench-LLM: HELM Similar to the Open LLM Leaderboard, the goal of HELM was to provide uniform evaluation of language models over vast set of data samples (termed as scenarios in Liang et al. [79]). HELM, however, has broader scope of models used for evaluation, employing open, limited-access, and closed models. All models currently used in ONEBench-LLM is listed below: 1. 01-ai yi-34b 2. 01-ai yi-6b 3. 01-ai yi-large-preview 4. ai21 j2-grande 5. ai21 j2-jumbo 6. ai21 jamba-1.5-large 7. ai21 jamba-1.5-mini 8. ai21 jamba-instruct 18. anthropic claude-3-sonnet-20240229 19. anthropic claude-instant-1.2 20. anthropic claude-instant-v 21. anthropic claude-v1.3 22. cohere command 23. cohere command-light 24. cohere command-r 25. cohere command-r-plus 26. databricks dbrx-instruct 9. AlephAlpha luminous-base 27. deepseek-ai deepseek-llm-67b-chat 10. AlephAlpha luminous-extended 11. AlephAlpha luminous-supreme 12. allenai olmo-7b 13. anthropic claude-2. 14. anthropic claude-2.1 15. anthropic claude-3-5-sonnet-20240620 16. anthropic claude-3-haiku-20240307 28. google gemini-1.0-pro-001 29. google gemini-1.0-pro-002 30. google gemini-1.5-flash31. google gemini-1.5-pro-001 32. google gemini-1.5-pro-preview-0409 33. google gemma-2-9b-it 34. google gemma-2-27b-it 35. google gemma-7b 17. anthropic claude-3-opus36. google text-bison@001 3 37. google text-unicorn@001 59. nvidia nemotron-4-340b-instruct 38. meta llama-2-7b 39. meta llama-2-13b 40. meta llama-2-70b 41. meta llama-3-8b 42. meta llama-3-70b 43. meta llama-3.1-8b-instruct-turbo 44. meta llama-3.1-70b-instruct-turbo 45. meta llama-3.1-405b-instruct-turbo 46. meta llama-65b 47. microsoft phi-2 48. microsoft phi-3-medium-4k-instruct 49. microsoft phi-3-small-8k-instruct 50. mistralai mistral-7b-instruct-v0.3 51. mistralai mistral-7b-v0. 52. mistralai mistral-large-2402 53. mistralai mistral-large-2407 54. mistralai mistral-medium-2312 55. mistralai mistral-small-2402 56. mistralai mixtral-8x7b-32kseqlen 57. mistralai mixtral-8x22b 60. openai gpt-3.5-turbo-0613 61. openai gpt-4-0613 62. openai gpt-4-1106-preview 63. openai gpt-4-turbo-2024-04-09 64. openai gpt-4o-2024-05-13 65. openai gpt-4o-mini-2024-0766. openai text-davinci-002 67. openai text-davinci-003 68. qwen qwen1.5-7b 69. qwen qwen1.5-14b 70. qwen qwen1.5-32b 71. qwen qwen1.5-72b 72. qwen qwen1.5-110b-chat 73. qwen qwen2-72b-instruct 74. snowflake snowflake-arctic-instruct 75. tiiuae falcon-7b 76. tiiuae falcon-40b 77. writer palmyra-x78. writer palmyra-x-v2 58. mistralai open-mistral-nemo-2407 79. writer palmyra-x-v3 4 A.3 ONEBench-LMM: LMMs-Eval LMMs-Eval is the first comprehensive large-scale evaluation benchmark for Large Multimodal models, meant to promote transparent and reproducible evaluations [176]. The models supported by LMMs-Eval are primarily open-sourced and the full list of currently used models are listed below: 1. idefics2-8b 2. internlm-xcomposer2-4khd-7b 3. instructblip-vicuna-7b 6. llava-13b 7. llava-1.6-13b 8. llava-1.6-34b 4. instructblip-vicuna-13b 9. llava-1.6-mistral-7b 11. llava-1.6-vicuna-7b 12. llava-7b 13. llava-next-72b 5. internVL-Chat-V110. llava-1.6-vicuna-13b 14. qwen vl chat A.4 ONEBench-LMM: VHELM Finally, ONEBench-LMM comprises VHELM, an extension of HELM for Vision-Language models. The models currently used by us, spanning open, limited-access, and closed models, are as follows: 1. anthropic claude 3 haiku 20240307 14. llava 1.6 vicuna 13b 2. anthropic claude 3 opus 20240229 3. anthropic claude 3 sonnet 20240229 4. google gemini 1.0 pro vision 001 5. google gemini 1.5 pro preview 0409 6. google gemini pro vision 7. google paligemma 3b mix 8. huggingfacem4 idefics2 8b 9. huggingfacem4 idefics 80b 10. huggingfacem4 idefics 80b instruct 11. huggingfacem4 idefics 9b 12. huggingfacem4 idefics 9b instruct 15. llava 1.6 vicuna 7b 16. microsoft llava 1.5 13b hf 17. microsoft llava 1.5 7b hf 18. mistralai bakllava v1 hf 19. openai gpt 4 1106 vision preview 20. openai gpt 4 vision preview 21. openai gpt 4o 2024 05 22. openflamingo openflamingo 9b vitl mpt7b 23. qwen qwen vl 24. qwen qwen vl chat 13. llava 1.6 mistral 7b 25. writer palmyra vision"
        },
        {
            "title": "B Capability Testing Across Arbitrary Queries",
            "content": "B.1 Queries: List and Additional Results Concept ONEBench-LLM AP ONEBench-LMM AP Common Queries apple ipad architecture beach biochemistry boat botany bus car cell(biology) china tourism cigarette advertisment coffee maker components of bridge decomposition of benzene(organic chemistry) epidemiology kirchoffs law(electrical engineering) food chain game of football german shepherd (dog) gothic style (architecture) law literary classics macroeconomics makeup microwave oven neuroscience components pasta perfume photosynthesis plants political diplomacy python code renaissance painting shareholder report sheet music solar cell battery thermodynamics united states of america vaccines volanic eruption bike leaning against wall child playing baseball coriolis effect dijkstras shortest path algorithm empty bridge overlooking the sea judo wrestling man in suit musical concert sine wave woman holding an umbrella 0.7435 0.7683 0.7152 0.9778 0.7728 0.9876 0.9035 0.9140 0.9937 0.6392 0.7249 0.8426 0.9222 0.6745 0.9316 0.6572 0.5405 0.8221 0.9359 0.7829 0.8566 0.9869 1.0000 1.0000 0.7979 0.9844 0.5678 0.5996 0.9848 1.0000 0.9529 0.8850 0.9270 1.0000 0.8322 0.8853 0.9567 0.8096 0.8572 0.7905 - - - - - - - - - - Queries testing Visual Capabilities 0.1985 0.8981 0.5698 0.7303 0.8829 0.7556 0.9739 0.8477 0.5075 1.0000 0.6590 0.4057 0.5865 0.7623 0.7991 0.4824 1.0000 1.0000 0.3078 1.0000 0.4138 1.0000 0.9570 0.2247 1.0000 0.2854 0.2142 0.6355 0.3665 0.6488 0.9561 0.9444 0.9799 0.8317 0.9750 0.8082 0.8852 0.8642 0.3411 0.9229 0.8271 0.9638 0.7063 0.9135 0.5934 0.6092 0.5611 0.9879 0.4232 0.8821 Table 5: Aggregate Average Precision(AP) for ONEBench-LLM and ONEBench-LMM concepts. 6 Figure 6: Additional qualitative analysis for ONEBenchs capability probing for selected queries."
        }
    ],
    "affiliations": [
        "Open-Ψ (Open-Sci) Collective",
        "Tubingen AI Center, University of Tubingen",
        "University of Cambridge"
    ]
}