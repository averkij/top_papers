{
    "paper_title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement",
    "authors": [
        "Xiyao Wang",
        "Zhengyuan Yang",
        "Chao Feng",
        "Hongjin Lu",
        "Linjie Li",
        "Chung-Ching Lin",
        "Kevin Lin",
        "Furong Huang",
        "Lijuan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at https://github.com/si0wang/ThinkLite-VL."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 4 3 9 7 0 . 4 0 5 2 : r SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement Xiyao Wang1,2, Zhengyuan Yang2, Chao Feng3, Hongjin Lu1 Linjie Li2, Chung-Ching Lin2, Kevin Lin2, Furong Huang1,, Lijuan Wang2, 1University of Maryland, College Park 3University of Michigan xywang@umd.edu Equal advise 2Microsoft Figure 1: Recent Reasoning VLMs studies finetune Base VLMs with extra reasoning training data to improve visual reasoning. This paper presents data-efficient self-improving method for better training reasoning VLMs. (Left) Comparison of VLMs with different parameter sizes on MathVista. Our model ThinkLite-VL-7B achieves the state-of-the-art (SoTA) accuracy of 75.1, surpassing Qwen2.5-VL-72B-Instruct, GPT-4o, O1, and other 7B-level reasoning VLMs. (Right) Comparison of the reasoning training data size used by 7B-level reasoning models. Our model achieves SoTA performance using only 11k data, and without any additional knowledge distillation."
        },
        {
            "title": "Abstract",
            "content": "In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at https://github.com/si0wang/ThinkLite-VL."
        },
        {
            "title": "Introduction",
            "content": "Leveraging long chain-of-thought reasoning with effective reflection during inference, large language models (LLMs) [24, 34] are capable of solving complex reasoning tasks such as math and coding. Recent studies [16] show that large-scale reinforcement fine-tuning (RFT) is critical factor in enhancing models reasoning performance. Notably, substantial reasoning performance improvements can be achieved solely through reinforcement fine-tuning in the post-training stage, even without the standard supervised fine-tuning (SFT) in post-training. Despite the notable successes in enhancing LLM reasoning with large-scale RFT, similar progress in vision-language models (VLMs) remains limited, likely due to the mismatch between the text-focused pre-training and the multimodal nature of VLM post-training tasks. Recent attempts [22, 12, 53, 81] have employed knowledge-distillation via supervised fine-tuning before the RFT stage, to encourage more visual reasoning related responses being generated. Despite the performance improvement, the knowledge distillation stage is cumbersome, and inherently prevents base VLMs from self-improving themselves in achieving stronger intelligence. In this paper, we demonstrate that high-quality, appropriately challenging training data is key factor to enable and self-improve visual reasoning ability. When visual reasoning training data aligns properly with the base VLMs skill level, large-scale RFT alone can significantly enhance VLMs reasoning ability without relying on knowledge distillation for format fine-tuning or base capability enhancement. Based on this insight, We introduce data-efficient training pipeline that results in ThinkLite-VL, reasoning VLM that achieves SoTA visual reasoning performance with less training samples. Figure 2: Performance comparison on 8 visual benchmarks. Our model significantly outperforms Qwen2.5-VL-7b-Instruct and other 7b-level reasoning models. The critical factor to ThinkLite-VLs success is the strategic selection of training samples with suitable difficulty. To achieve this, we repurpose Monte Carlo tree search (MCTS), classic inference-time search algorithm, to accurately quantify the sample difficulty. Specifically, MCTSs explicit tree search enforces sufficient thinking compute in deciding the question difficulty, and provide tight correlation between the question difficulty and the number of MCTS iterations needed to solve it. Our training pipeline begins with collecting 70k open-source samples from three key domains: mathematical reasoning, natural image understanding, and chart comprehension. We then implement MCTS-guided sample selection by applying the VLM itself to perform iterative reasoning on each of the 70k samples, using the number of iterations required to reach the correct solution as difficulty measure. This rigorous filtering process results in set of 11k challenging and high-quality samples tailored specifically for our base model. We then directly perform RFT with these selected samples, avoiding any additional supervised fine-tuning steps. Using the Qwen2.5-VL-7B-Instruct model as our base, we develop our final model, ThinkLite-VL-7B. We evaluate ThinkLite-VL-7B on eight widely used VLM benchmarks. As shown in Figure 2, after RFT with the filtered 11k high-quality data, ThinkLite-VL-7B significantly improves the average performance of Qwen2.5-VL-7B-Instruct from 59.69 to 63.89. It also surpasses the fairly comparable baseline that RFT with the same amount of unfiltered data, from 60.89 to 63.89. Furthermore, compared with the most recent 7B-level reasoning VLMs, ThinkLite-VL-7B consistently demonstrates substantial performance advantages. Notably, on the MathVista benchmark, ThinkLite-VL-7B 2 achieves state-of-the-art (SoTA) accuracy of 75.1 as shown in Figure 1, significantly surpassing other 7B-level models, open-sourced larger models, GPT-4o, and O1."
        },
        {
            "title": "2 Related work",
            "content": "Large language model reasoning. Simulating human-like thinking processes through intermediate reasoning steps has significantly improved the performance of large language models (LLMs) on tasks that require reasoning [24]. One family of methods focuses on explicitly controlling the structure or format of the models outputs, such as by applying Chain-of-Thought (CoT) prompting [75] and Self-Consistency [74]. Related lines of work include more elaborate reasoning strategies like Tree of Thoughts [84] or Graph of Thoughts [4]. Additionally, some approaches involve supervised finetuning (SFT) on curated datasets with reasoning annotations [50, 85]. Researchers have also explored process reward models (PRMs) that encourage systematic thought processes [33, 64, 68, 27, 94, 45]. Others incorporate search techniques, including Monte Carlo Tree Search (MCTS) or beam search, to refine or verify reasoning paths [77, 78, 5, 15, 19, 70]. Recently, large-scale RL with outcome-based reward functions has been leveraged [16] to elicit powerful reasoning capabilities in LLMs. In this paper, we focus on how to use large-scale RL to enhance the reasoning ability of VLMs. Vision language model reasoning. Vision language models [1, 66, 36, 23, 35, 3, 10, 62, 29, 82] can perform vision tasks using language given visual input through vision encoders like [55, 90, 63]. These models demonstrate comprehensive multimodal capabilities across various scenarios [89, 38, 87, 47, 18, 88, 20, 30] and exhibit reasoning capabilities to some extent [41, 73, 39, 92, 67]. Inspired by the success of reasoning in LLMs, researchers have sought to improve the reasoning capabilities of VLMs. For instance, CoT prompting is applied to VLMs [93, 49, 44, 11, 96, 21] and some papers create multimodal datasets [83, 80, 59, 95, 12, 22, 17, 61], using SFT for knowledge distillation to improve reasoning abilities. Some prior works have also explored improving VLM performance through self-improvement strategies [98, 69, 72, 13]. More recently, RL training has emerged as promising approach to further strengthen the reasoning capabilities of VLMs [12, 22, 48, 79]. While recent works explore SFT and RL [12, 22] for VLM reasoning, efficiently utilizing training data and avoiding costly knowledge distillation remains challenge. In this paper, we propose novel approach using MCTS to filter for high-quality training instances based on the difficulty level. We then directly apply RL training to enhance reasoning on this curated data, demonstrating strong performance without requiring any SFT stage. Data filtration. Data filtration aims to identify and retain high-quality, diverse, and task-relevant data while discarding noisy or redundant information to optimize training efficiency and generalization performance. It is important for the pretraining phase [14, 28, 76, 56, 52, 2, 91, 65, 54] and instruction tuning phase [32, 31, 7, 9, 36, 99, 86] of both LLMs and VLMs. In this paper, we specifically focus on filtering training instances to curate data optimally for efficient downstream RL training to improve the reasoning capabilities of VLMs. concurrent work, MM-Eureka [48], also investigates the impact of data filtration on RFT. However, their approach is limited to relatively simple selfconsistency-based difficulty filtering strategy, where all samples with zero accuracy are discarded. In contrast, we propose more principled methodMCTS-based sample selectionwhich enables the identification of truly challenging examples for the VLM. Importantly, our findings reveal that the unsolved samples, which VLMs fail to solve during MCTS, play critical role in enhancing reasoning performance during RFT, rather than being excluded from the training process."
        },
        {
            "title": "3 Training Recipe",
            "content": "In this section, we will introduce the complete training pipeline of ThinkLite-VL. First, in Section 3.1, we describe how we collect our training data that we later sample hard problems from. Then, in Section 3.2, we detail how we employ base model combined with Monte Carlo Tree Search (MCTS) for data filtering to select prompts that are challenging for the base model. Finally, in Section 3.3, we explain how we use these filtered data to train ThinkLite-VL. We note that the proposed data filtering method, introduced in Section 3.2, is the core technical contribution of ThinkLite-VL. Specifically, ThinkLite-VL highlights the importance of difficulty-aware training sample selection in self-improving training, and effectively repurposes MCTS for sample difficulty prediction. 3 Category QA Category Data source Data size Math Reasoning Natural Image Understanding Chart Understanding Open-ended Multi-choice Multi-choice Open-ended Multi-choice Open-ended Open-ended Open-ended Geometry3K GeoQA Geos FigureQA ScienceQA OK-VQA IconQA TabMWP 3001 5010 66 10000 10332 9009 10000 22579 Figure 3: Data statistic of ThinkLite-VL-70k training dataset. We find that converting all answers to open-ended format is critical in reliably assessing question difficulty and effective model training. 3.1 Data Collection We collect total of 70k datas from widely used open-source training datasets as our initial training set, covering three category: multimodel mathematical reasoning (Geometry3K [40], GeoQA [6], Geos [58]), natural image understanding (FigureQA [25], ScienceQA [41], OK-VQA [46]), and chart understanding (IconQA [43], TabMWP [42]). For FigureQA and IconQA, due to the large size of their original training sets, we only randomly sample 10k data points from each as our training set. The overall data distribution is shown in Figure 3. Each training sample is organized into the following format: (Image, id, Prompt, Answer). Furthermore, to prevent the VLM from obtaining correct answers by merely guessing from multiplechoice options, we reformulated IconQA, FigureQA, Geometry3K, TabMWP, and OK-VQA from multiple-choice format to an open-ended format. This modification compels the VLM to derive the correct answer through reasoning rather than selection, thereby increasing the difficulty of the tasks and enhancing the reliability of the data filtering process described in the subsequent section. 3.2 MCTS-based Sample Selection In our work, the collected data primarily originates from commonly used pretraining datasets for existing VLMs, which makes the model susceptible to overfitting on certain samples. Inspired by recent successes of data filtration in LLM SFT [51, 85] and conventional reinforcement learning [57, 71], we propose MCTS-based sample selection mechanism. This approach leverages the VLMs own iterative reasoning process, using the number of iterations required to reach the correct answer as metric to assess the difficulty of each data sample. Consequently, we can selectively filter for those samples that are more challenging for the model during RL training, rather than using the entire dataset. Figure 4: Data difficulty distribution of our 11k training set after MCTS-based data filtration. Unsolved refers to data that VLM cannot solve after 50 MCTS iterations. Specifically, we define the state at step t, denoted as st, to represent the prefix of the reasoning chain. The introduction of new reasoning step, a, transitions the state to st+1, which is formed by concatenating st with a. By leveraging VLM itself as policy model, πθ, we sample candidate steps from the probability distribution πθ(ax, I, st), where denotes the tasks input prompt and represents the input image. The MCTS process starts from the root node, s0, representing the beginning of sentence. It then iteratively proceeds through three key phasesselection, expansion and simulationwhich are described in detail in the subsequent paragraphs. In contrast to previous studies, during the data filtering stage with MCTS, we prioritize computational efficiency and comprehensive exploration of the solution space, with our focus centered on self-rewarding setting. Consequently, throughout the MCTS process, we do not employ any pretrained or separately trained process reward models, thereby simplifying and accelerating the procedure. The prompt used for MCTS is shown in Table 1. 4 Table 1: Prompt used for VLM during MCTS procedure. We provide two examples of in-context learning to force VLM to follow the reasoning format. MCTS Prompt Template: Answer the question **step by step** and provide the final answer at the end, each step should end with **<end>** and put your final answer within . Below are two examples: Question: BoatsRUs built 7 canoes in January of this year and then each subsequent calendar month they built twice the number of canoes they had built the previous month. How many total canoes were built by BoatsRUs by the end of May of this year? ### Step1: To find the result of the total number of canoes built by BoatsRUs by the end of May, need to find the number of canoes built in each month from January to May and then add them up. <end> ### Step2: To find the number of canoes built in each month, need to use the formula for the number of canoes built in given month, which is the number of canoes built in the previous month times 2. <end> ### Step3: So, the number of canoes built in January is 7, the number of canoes built in February is 7 times 2, which is 14, the number of canoes built in March is 14 times 2, which is 28, the number of canoes built in April is 28 times 2, which is 56, and the number of canoes built in May is 56 times 2, which is 112. <end> ### Step4: Now, can add up these numbers to get the total number of canoes built by BoatsRUs by the end of May: 7 plus 14 plus 28 plus 56 plus 112, which is 217. <end> ### Final Answer: The answer is: 217 . Question: Find the number of blue circles in the figure. ### Step 1: To find the result of the number of blue circles, need to interpret the figure. The figure is Venn diagram with two labeled sets: - One set labeled \"blue\" contains all the shapes that are blue in color. - The other set labeled \"circle\" contains all the shapes that are circular in shape. The overlapping region of the Venn diagram contains shapes that are both blue and circular. <end> ### Step 2: The overlapping region contains shapes that meet both criteria: Blue color and Circle shape. From the diagram: - There is **one blue circle** in the overlapping region. <end> ### Final Answer: The answer is: 1 . Remember to answer the question **step by step**! Here is your question: Question: {QUESTION} Selection. In our MCTS procedure, the selection process is only determined by the visitation frequency, denoted as (st), of the current state st. At node st, the subsequent node is selected according to the following formula: st+1 = arg maxst (cid:20) cpuct (st) 1+N (st+1) (cid:21) Expansion. Given current step st, the VLM generates distinct actions based on the prompt and image through temperature decoding. Each of these actions is then combined with the current step to form candidates next steps. The diversity among these actions is regulated by temperature parameter, which is set to 0.5 in our experiments, with configured as 3. Simulation. After selecting node , we directly utilize the policy πθ to generate several reasoning steps until final answer is produced or preset reasoning step limit is reached. Subsequently, we employ the corresponding LLM (in our experiments, the Qwen2.5-VL-7B-Instruct is used, with Qwen2.5-7B-Instruct serving as the critic model) to compare the generated final answer with the ground truth answer, thereby determining the correctness of the response. If the answer is correct, the MCTS process is terminated and the current iteration number is recorded; if the answer is incorrect, the visit count of the selected node is updated and the next iteration commences. Table 2 illustrates the prompt employed for the critic model. Data filtration. We apply this MCTS procedure to the entire collection of 70k data samples and record the iteration number required to solve each problem, using Qwen2.5-VL-7B-Instruct as 5 the policy model. In this process, served as metric for assessing the difficulty of each sample: higher indicates that the VLM requires more extensive exploration to arrive at the correct answer, thereby reflecting greater level of challenge. Ultimately, we select all samples with greater than 5, as well as those that remained unsolved after 50 iterations, resulting in final training set of 11k samples. The data difficulty distribution of this final training set is shown in Figure 4. Table 2: Critic prompt for MCTS simulation results evaluation. Critic Prompt Template: Please help me judge the correctness of the generated answer and the corresponding rationale. Question: {} Ground truth answer: {} Generated rationale and answer: {} Your output should only be one sentence: the generated answer is true or false. 3.3 Visual Reasoning Training Table 3: Visual reasoning training data comparison between ThinkLite-VL and other VLM reasoning models. ALL these reasoning models have distilled knowledge from larger models or closed-source models except for MM-Eureka-Qwen-7B. MM-Eureka-Qwen-7B uses more K12 data (54k) than ours and performs accuracy-based data filtering before training. Here the data size refers to the amount of additional visual reasoning data used to boost the base model for reasoning via SFT or RL training."
        },
        {
            "title": "Reasoning Models",
            "content": "Knowledge Distillation (KD) LLaVA-Cot-11B [80] Mulberry-7B [83] Vision-R1-7B [22] OpenVLThinker-7B [12] MM-EUREKA-Qwen-7B [48] GPT-4o GPT-4o, Qwen2-VL-72B Deepseek-R1 DeepSeek-R1-Distill-Qwen-14B ThinkLite-VL-7B RFT"
        },
        {
            "title": "Data size",
            "content": "100k 260k 200k + 10k 59.2k 54k 11k Unlike previous VLM reasoning studies, which heavily depend on large-scale Chain-of-Thought (CoT) data generated by external models and employ SFT for knowledge distillation to enhance reasoning capabilities (as shown in Table 3), we demonstrate that directly performing reinforcement fine-tuning (RFT) with small amount of high-quality training data can significantly enhance the reasoning ability of VLMs, without the need for extensive external data generation. After conducting MCTS-based sample selection and obtaining filtered set of 11k high-quality training data, we then perform RL fine-tuning on the Qwen2.5-VL-7B-Instruct model using these selected data. Specifically, we employ Group Relative Policy Optimization (GRPO) loss function proposed by [60] for training, with the objective defined as follows: JGRPO(θ) = qP (Q),{oi}G i=1πold θ (Oq) (cid:40) (cid:34) 1 (cid:88) i=1 1 oi oi (cid:88) t=1 min πθ(oi,t q, oi,<t) πold θ (oi,t q, oi,<t) (cid:32) ˆAi,t, clip πθ(oi,t q, oi,<t) πold θ (oi,t q, oi,<t) , 1 ϵ, 1 + ϵ (cid:33) (cid:41) ˆAi,t β DKL(πθ πpre) (cid:35) . (1) We provide the training prompt template during RFT in Table 4. 6 Table 4: Prompt template used for reinforcement learning fine-tuning. Prompt Template: You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE put in ."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Benchmark Evaluation In this subsection, we systematically evaluate ThinkLite-VL on several commonly used multimodal benchmark datasets and perform comprehensive comparisons with existing reasoning models. Through these experiments, we demonstrate the effectiveness and advantages of our model in multimodal reasoning tasks. Baselines and implementation details. We use Qwen2.5-VL-7B-Instruct as the base model and perform RFT on the 11k high-quality data obtained through MCTS-based filtration, resulting in our proposed model, named ThinkLite-VL-7B. We conduct training using Easy-R1 [97] code base and set GRPO rollout number as 32. Our main baselines are as follows: (1) Qwen2.5-VL-7B-Instruct, serving as our base model; (2) ThinkLite-VL-Random11k, trained using RFT on randomly sampled subset of 11k instances from the full dataset. Besides, we report the performance of several recent reasoning VLMs for comparison, including the SFT-based models LLaVA-Cot-11B and Mulberry-7B, as well as the RFT-based models Vision-R1, MM-Eureka-Qwen-7B, and OpenVLThinker-7B. We also include larger open-source models and commercial models as SOTA performance references which include Qwen2.5-VL-72B-Instruct, InternVL2.5-78B, GPT-4o, and O1. Benchmarks. We select eight widely used VLM benchmarks for evaluation, namely MathVista [39], MathVison [67], MathVerse [92], MMMU [89], MMStar [8], MMBench [37], MMVet [87], and AI2D [26]. Among them, MathVista, MathVison, and MathVerse are widely used in VLM research to evaluate mathematical reasoning capabilities, while MMVet also includes significant number of mathematical reasoning tasks. In contrast, MMMU, MMStar, MMBench, and AI2D are primarily utilized to assess VLMs visual perception reasoning and scientific reasoning abilities. SoTA performance over 7B reasoning models. As shown in Table 5, ThinkLite-VL-7B shows significant improvement in average performance across the eight benchmarks compared to the base model Qwen2.5-VL-7B-Instruct, with the average performance increasing from 59.69 to 63.89. Compared to ThinkLite-VL-Random11k, which is trained with the same data size using random sampling, our method shows significant advantages across all benchmarks, indicating the effectiveness and importance of MCTS-based sample selection. Furthermore, ThinkLite-VL-7B also outperforms reasoning models that primarily achieve performance enhancement through extensive knowledge distillation (such as LLaVA-CoT-11B, Mulberry-7B, Vision-R1-7B, and OpenVLThinker-7B) with the closest average performance to GPT-4o. Compared to MM-EUREKA-Qwen-7B, which does not involve SFT knowledge distillation but adopts larger RL training dataset, our model consistently outperforms across all benchmarks, highlighting the importance of high-quality data filtering before training, and the effectiveness of the proposed MCTS-based filtering. From the perspective of individual benchmarks, our method achieves the highest scores among 7B-level models on six out of the eight benchmarks. The only exceptions are the MMMU and MathVerse benchmarks, where we slightly lag behind Mulberry-7B and Vision-R1-7B that focused on narrower range of tasks, respectively. Remarkably, our model achieves the SoTA accuracy of 75.1 on the MathVista benchmark, surpassing larger open-sourced VLMs, GPT-4o, and O1. 4. Importance of MCTS-based Sample Selection In this section, we conduct ablation studies to demonstrate the importance of MCTS-based sample selection. We compare five different training settings of ThinkLite-VL: (1) ThinkLite-VL-Unsolved: 7 Table 5: Comparison of different VLMs on 8 widely used visual benchmarks. The grey sections indicate models with larger parameter sizes and closed-source models. Our model achieves SoTA performance at the 7B level on 6 benchmarks and reaches SoTA performance of 75.1 on MathVista among all VLMs. On average, our model improves performance by 7% compared with Qwen2.5-VL7B-Instruct. We do not evaluate Mulberry-7B on MathVision because Mulberry-7B uses MathVision as training dataset, and for Vision-R1-7B, their model is not open-sourced, so we only refer to the results reported in their paper. Models i a i e Data size i h i e V a n Qwen2.5-VL-72B-Instruct InterVL2.5-78B GPT-4o O1 LLaVA-Cot-11B Mulberry-7B Vision-R1-7B OpenVLThinker-7B MM-EUREKA-Qwen-7B Qwen2.5-VL-7B-Instruct ThinkLite-VL-Random11k ThinkLite-VL-7B 100k 260k 210k 59.2k 54k 11k 11k 74.8 72.3 63.8 73.9 54.8 63.1 73.5 70.2 73.0 67.8 71.9 75.1 39.8 34.9 36.8 16.3 29.6 31.9 23.6 26.1 32.9 57.6 51.7 50.2 33.9 39.6 52.4 47.9 50.3 44.5 47.3 50.7 M 70.2 70.1 69.1 78.2 46.2 55.0 51.9 52.3 50.6 51.7 54.6 S 70.8 69.5 64.7 57.6 61.3 63.2 64. 61.7 62.7 65.0 e 88.6 88.3 83.4 75.0 79.2 81.3 79.3 80.7 81.1 81.4 - V 76.2 72.3 69.1 60.3 63.7 66.9 64.9 66.0 65.5 67.8 2 88.5 89.1 84.6 78.7 80.1 82.7 81.4 82.6 80.9 83.6 Avg. 70.81 68.53 65.21 52.85 61.71 62.15 59.69 60.89 63. Trained using only the 5.6k samples that could not be solved by MCTS, representing the most difficult subset. (2) ThinkLite-VL-Iter5Only: Trained on the subset of data that VLM is able to solve via MCTS, but required more than 5 iterations. This set, combined with the unsolved samples, forms the full 11k training set used in ThinkLite-VL. (3) ThinkLite-VL-Random11k: Trained on randomly sampled 11k subset from the full 70k dataset, matching the size of the ThinkLite-VL training set. (4) ThinkLite-VL-SelfConsistency: Trained on 23k samples selected based on self-consistency difficulty measure. Specifically, for each prompt, we perform 50 rollouts using Qwen2.5-VL-7BInstruct and compute answer accuracy using Qwen2.5-7B-Instruct. Samples with accuracy lower than 0.2 are selected for RFT. (5) ThinkLite-VL-Fullset: Trained on the complete 70k dataset without any filtering. We report the evaluation results of all five settings across the eight VLM benchmarks, as shown in Table 6. We observe that ThinkLite-VL-7B, trained using 11k samples via MCTS-guided sample selection, achieves the highest average performance (63.89) among all settings. It outperforms not only the random sampling baseline (ThinkLite-VL-Random11k, 60.89) but also models trained on the full dataset (ThinkLite-VL-Fullset, 63.13) and self-consistency-based filtering (ThinkLite-VL-SelfConsistency, 63.15), despite using significantly fewer training samples. This highlights the effectiveness of our difficulty-aware data selection strategy. Further analysis reveals that models trained on subsets derived solely from unsolved samples (ThinkLite-VL-Unsolved, 62.04) or samples requiring more than five iterations (ThinkLite-VL-Iter5Only, 62.38) also show decent performance, suggesting that hard and medium-difficulty samples contribute meaningfully to reasoning ability. However, neither subset alone is sufficient. The combination of both unsolved and medium-difficulty samples yields the strongest and most effective training signal. Besides, we compare the reward curves during RFT of ThinkLite-VL-Random11k, ThinkLite-VLFullset, ThinkLite-VL-Iter5Only, and ThinkLite-VL, as shown in Figure 5. Although ThinkLiteVL-Random11k and ThinkLite-VL-Fullset achieve higher rewards during training, their actual benchmark performances are inferior to ThinkLite-VL. This observation suggests that incorporating large number of easy samples into training rapidly improves rewards but fails to enhance the models reasoning ability. Moreover, ThinkLite-VL exhibits notably lower rewards compared to ThinkLite-VL-Iter5Only, indicating that the unsolved data identified by our MCTS-based sample selection strategy indeed pose significant challenges to the VLM. By progressively learning to solve these challenging problems during trainingeven if not all are solved completelythe reasoning capabilities of VLMs can be substantially improved. 8 Table 6: Comparison with models trained on data sampled using different selection strategies, ThinkLite-VL achieves significantly better performance, highlighting the effectiveness and superiority of our proposed MCTS-based sample selection method. Models ThinkLite-VL-7B ThinkLite-VL-Unsolved ThinkLite-VL-Iter5Only ThinkLite-VL-Random11k ThinkLite-VL-SelfConsistency ThinkLite-VL-Fullset i a m t Data size i h M m e a i 11k 5.6k 5.4k 11k 23k 70k 75.1 73.6 73.5 71.9 74.6 74.3 32.9 26.9 27.5 26.1 30.9 29.9 50. 49.4 50.2 47.3 50.1 52.2 M 54.6 52.1 52.5 51.7 53.8 53.1 S 65. 62.7 64.2 62.7 64.1 63.7 e 81.4 81.1 80.9 81.1 81.3 81.6 - M 67.8 67.0 66.9 65.5 67.1 67.2 2 83.6 83.5 83.3 80.9 83.3 83.0 Avg. 63.89 62.04 62.38 60.89 63.15 63.13 Figure 5: Comparison of reward curves of models trained with different data during RFT. Iter5+Unsolved 11k dataset presents the most challenging learning setting for VLM, highlighting the difficulty of the samples selected by MCTS-based sample selection. 4.3 Ablation Study of Data Difficulty In this section, we investigate how training data difficulty affects model performance. We present the average performance of models trained using different difficulty data in Table 7. Notably, the model trained with the Iter5+Unsolved subset achieves the highest average score of 63.89, outperforming all other settings. When expanding the difficulty threshold (e.g., Iter10, Iter20, Iter30, and Iter40), the model performance consistently declines, suggesting that medium-difficulty samples are important for improving model reasoning ability. As the difficulty of the training data decreases, the models performance also declines. This trend suggests that the inclusion of an excessive number of easy samples may weaken the training signal during RFT and ultimately hurt the models reasoning ability."
        },
        {
            "title": "5 Case Studies",
            "content": "In this section, we present samples of varying difficulty levels selected by the MCTS-based sample selection method across different datasets, as shown in Tables 13 through 12. The difficulty levels are determined based on the number of reasoning iterations required by the VLM to arrive at the correct answer during the MCTS process, providing reference examples for understanding how the method distinguishes between easy and challenging samples. 9 Table 7: ThinkLite-VL performance under different training data difficulty settings. Iter5+Unsolved achieves the best performance."
        },
        {
            "title": "Difficulty level",
            "content": "Data size Avg. score Fullset Iter1+Unsolved Iter5+Unsolved Iter10+Unsolved Iter20+Unsolved Iter30+Unsolved Iter40+Unsolved Unsolved 70k 18k 11k 8k 6.8k 6.1k 5.8k 5.6k 63.13 63.29 63.89 62.65 62.61 62.39 62.26 62.04 Example 3: Different difficulty samples from FigureQA Iter Question: Is Medium Blue less than Dark Orchid? Ground Truth Answer: Yes. Iter29 Question: Does Dodger Blue intersect Dark Slate? Ground Truth Answer: Yes. Unsolved Question: Does Violet Red have the maximum area under the curve? Ground Truth Answer: No. Table 8: Example of samples with different difficulties decided by MCTS-based sample selection from FigureQA."
        },
        {
            "title": "6 Conclusion",
            "content": "We have introduced an effective self-improvement approach to enhance the reasoning capabilities of VLMs, eliminating the need for external supervision or knowledge distillation. Our key insight highlights the critical importance of selecting genuinely challenging examples for Reinforcement Fine-Tuning (RFT). We find that when training data quality is sufficiently high, even modest dataset can substantially enhance visual reasoning performance without resorting to knowledge distillation 10 Example 4: Different difficulty samples from ScienceQA Iter0 Question: Think about the magnetic force between the magnets in each pair. Which of the following statements is true? Choices: (A) The magnitude of the magnetic force is greater in Pair 2. (B) The magnitude of the magnetic force is greater in Pair 1. (C) The magnitude of the magnetic force is the same in both pairs. Ground Truth Answer: A. Iter Question: Which solution has higher concentration of purple particles? Choices: (A) neither; their concentrations are the same (B) Solution (C) Solution Ground Truth Answer: B. Unsolved Question: What is the direction of this push? Choices: (A) away from the hockey stick (B) toward the hockey stick Ground Truth Answer: A. Table 9: Example of samples with different difficulties decided by MCTS-based sample selection from ScienceQA. methods. Building on this insight, we propose novel data selection technique, MCTS-based sample selection, which identifies and retains challenging samples by quantifying the number of reasoning iterations required by the VLM to resolve each problem using MCTS. Applying our method to curated initial set of 70k VLM training samples, we obtain high-quality subset comprising 11k challenging samples. This curated dataset is then used to fine-tune the Qwen2.5-VL-7B-Instruct model via RFT, resulting in reasoning VLM named ThinkLite-VL. Our model demonstrates significant improvements across multiple visual reasoning benchmarks, and notably achieves new SoTA accuracy of 75.1 on MathVista. We hope that our findings on the difficulty-based selection of RFT training data can provide insights for training more effective reasoning VLMs."
        },
        {
            "title": "Acknowledgment",
            "content": "Wang and Huang are supported by DARPA Transfer from Imprecise and Abstract Models to Autonomous Technologies (TIAMAT) 80321, National Science Foundation NSF-IIS-2147276 FAI, National Science Foundation NAIRR240045, DOD-AFOSR-Air Force Office of Scientific Research under award number FA9550-23-1-0048. 11 Example 5: Different difficulty samples from OK-VQA Iter0 Question: What food group is pictured here? Ground Truth Answer: fruit. Iter Question: What is the length of the surfboard the man in the black shorts at the back of the line of people is holding? Ground Truth Answer: 7 feet. Unsolved Question: What is this guys profession? Ground Truth Answer: security. Table 10: Example of samples with different difficulties decided by MCTS-based sample selection from OK-VQA."
        },
        {
            "title": "References",
            "content": "[1] Gpt-4v(ision) system card. 2023. [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning. ArXiv, abs/2204.14198, 2022. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1768217690, 2024. [5] Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: process supervision without process. arXiv preprint arXiv:2405.03553, 2024. 12 Example 6: Different difficulty samples from IconQA Iter0 Question: How many flowers are there? Ground Truth Answer: 56. Iter10 Question: How many dots are there? Ground Truth Answer: 40. Unsolved Question: How many stars are there? Ground Truth Answer: 19. Table 11: Example of samples with different difficulties decided by MCTS-based sample selection from IconQA. [6] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric P. Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning, 2022. [7] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. Alpagasus: Training better alpaca with fewer data. In The Twelfth International Conference on Learning Representations, 2024. [8] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [9] Ruibo Chen, Yihan Wu, Lichang Chen, Guodong Liu, Qi He, Tianyi Xiong, Chenxi Liu, Junfeng Guo, and Heng Huang. Your vision-language model itself is strong filter: Towards high-quality instruction tuning with data selection. ArXiv, abs/2402.12501, 2024. [10] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. 13 Example 7: Different difficulty samples from TabMWP Iter0 Question: Adriana wants to buy 3 pounds of silver confetti. How much will she spend? Ground Truth Answer: 36. Iter22 Question: game show viewer monitors how often wheel numbered 1 through 5 stops at each number. How many people are there in all? Ground Truth Answer: 29. Unsolved Question: The employee at the department store counted the number of ties on each tie rack. How many racks have at least 30 ties but fewer than 70 ties? Ground Truth Answer: 15. Table 12: Example of samples with different difficulties decided by MCTS-based sample selection from TabMWP. [11] Zhenfang Chen, Qinhong Zhou, Yikang Shen, Yining Hong, Hao Zhang, and Chuang Gan. See, think, confirm: Interactive prompting between vision and language models for knowledge-based visual reasoning. arXiv preprint arXiv:2301.05226, 2023. [12] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative selfimprovement, 2025. [13] Yihe Deng, Pan Lu, Fan Yin, Ziniu Hu, Sheng Shen, Quanquan Gu, James Zou, Kai-Wei Chang, and Wei Wang. Enhancing large vision language models with self-training on image comprehension. Advances in Neural Information Processing Systems, 37:131369131397, 2024. [14] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. ArXiv, abs/2101.00027, 2020. [15] Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, and Lijie Wen. Interpretable contrastive monte carlo tree search reasoning. arXiv preprint arXiv:2410.01707, 2024. 14 Example 1: Different difficulty samples from Geometry3K Iter0 Question: Find so that the quadrilateral is parallelogram. Ground Truth Answer: 9. Iter16 Question: Use parallelogram R to find y. Ground Truth Answer: 6.45. Unsolved Question: Find the area of the parallelogram. Round to the nearest tenth if necessary. Ground Truth Answer: 315. Table 13: Example of samples with different difficulties decided by MCTS-based sample selection from GeoQA. [16] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [17] Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, and Xiang Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. arXiv preprint arXiv:2412.05237, 2024. [18] Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36083617, 2018. [19] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023. [20] Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025. [21] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024. 15 Example 2: Different difficulty samples from Geos Iter0 Question: What is the area of the following square, if the length of BD is 2 (A) 1 (B) 2 (C) 3 (D) 4 (E) 5. Ground Truth Answer: D. 2? Choices: Iter7 Question: Given the circle at the right with diameter AB, find x. Choices: (A) 30 degrees (B) 45 degrees (C) 60 degrees (D) 90 degrees (E) None Ground Truth Answer: D. Unsolved Question: In the diagram at the right, lines and are parallel, and lines and are parallel. = 75. What is the value of + z? Choices: (A) 75 (B) 105 (C) 150 (D) 180 (E) None Ground Truth Answer: D. Table 14: Example of samples with different difficulties decided by MCTS-based sample selection from Geos. [22] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models, 2025. [23] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [24] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [25] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning, 2018. [26] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images, 2016. [27] Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Stepdpo: Step-wise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629, 2024. 16 [28] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In Annual Meeting of the Association for Computational Linguistics, 2021. [29] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, et al. Multimodal foundation models: From specialists to general-purpose assistants. Foundations and Trends in Computer Graphics and Vision, 16(1-2):1214, 2024. [30] Ming Li, Ruiyi Zhang, Jian Chen, Jiuxiang Gu, Yufan Zhou, Franck Dernoncourt, Wanrong Zhu, Tianyi Zhou, and Tong Sun. Towards visual text grounding of multimodal large language model, 2025. [31] Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, and Tianyi Zhou. Superfiltering: Weak-to-strong data filtering for fast instruction-tuning. ArXiv, abs/2402.00530, 2024. [32] Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. From quantity to quality: Boosting llm performance with selfguided data selection for instruction tuning. In North American Chapter of the Association for Computational Linguistics, 2023. [33] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [34] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [35] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [36] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [37] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? arXiv:2307.06281, 2023. [38] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. [39] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. [40] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning, 2021. [41] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. [42] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning, 2023. 17 [43] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: new benchmark for abstract diagram understanding and visual language reasoning, 2022. [44] Bozhi Luan, Hao Feng, Hong Chen, Yonghui Wang, Wengang Zhou, and Houqiang Li. Textcot: Zoom in for enhanced multimodal text-rich image understanding. arXiv preprint arXiv:2404.09797, 2024. [45] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2, 2024. [46] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge, 2019. [47] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [48] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [49] Chancharik Mitra, Brandon Huang, Trevor Darrell, and Roei Herzig. Compositional chain-ofthought prompting for large multimodal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1442014431, 2024. [50] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [51] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. [52] Guilherme Penedo, Hynek Kydlícek, Loubna Ben Allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale. ArXiv, abs/2406.17557, 2024. [53] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. [54] Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon Vandenhende, Yash J. Patel, Yi Wen, Vignesh Ramanathan, and Dhruv Kumar Mahajan. Filtering, distillation, and hard negatives for vision-language pre-training. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 69676977, 2023. [55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [56] Laura Ruis, Maximilian Mozes, Juhan Bae, Siddhartha Rao Kamalakara, Dwarak Talupuru, Acyr F. Locatelli, Robert Kirk, Tim Rocktaschel, Edward Grefenstette, and Max Bartolo. Procedural knowledge in pretraining drives reasoning in large language models. ArXiv, abs/2411.12580, 2024. [57] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay, 2016. 18 [58] Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. Solving geometry problems: Combining text and diagram interpretation. In Lluís Màrquez, Chris Callison-Burch, and Jian Su, editors, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 14661476, Lisbon, Portugal, September 2015. Association for Computational Linguistics. [59] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. Advances in Neural Information Processing Systems, 37:86128642, 2024. [60] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. [61] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. Llamav-o1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. [62] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. [63] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. [64] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. [65] Alex Wang, Kevin Lin, David Junhao Zhang, Stan Weixian Lei, and Mike Zheng Shou. Too large; data reduction for vision-language pre-training. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 31243134, 2023. [66] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022. [67] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [68] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. [69] Xiyao Wang, Jiuhai Chen, Zhaoyang Wang, Yuhang Zhou, Yiyang Zhou, Huaxiu Yao, Tianyi Zhou, Tom Goldstein, Parminder Bhatia, Furong Huang, et al. Enhancing visual-language modality alignment in large vision language models via self-improvement. arXiv preprint arXiv:2405.15973, 2024. [70] Xiyao Wang, Linfeng Song, Ye Tian, Dian Yu, Baolin Peng, Haitao Mi, Furong Huang, and Dong Yu. Towards self-improvement of llms via mcts: Leveraging stepwise knowledge with curriculum preference learning. arXiv preprint arXiv:2410.06508, 2024. [71] Xiyao Wang, Wichayaporn Wongkamjan, Ruonan Jia, and Furong Huang. Live in the moment: Learning dynamics model adapted to evolving policy. In International Conference on Machine Learning, pages 3647036493. PMLR, 2023. 19 [72] Xiyao Wang, Zhengyuan Yang, Linjie Li, Hongjin Lu, Yuancheng Xu, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Scaling inference-time search with vision value model for improved visual comprehension. arXiv preprint arXiv:2412.03704, 2024. [73] Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, et al. Mementos: comprehensive benchmark for multimodal large language model reasoning over image sequences. arXiv preprint arXiv:2401.10529, 2024. [74] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [75] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [76] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via importance resampling. ArXiv, abs/2302.03169, 2023. [77] Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy Lillicrap, Kenji Kawaguchi, and Michael Shieh. Monte carlo tree search boosts reasoning via iterative preference learning. arXiv preprint arXiv:2405.00451, 2024. [78] Huajian Xin, ZZ Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, et al. Deepseek-prover-v1. 5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. arXiv preprint arXiv:2408.08152, 2024. [79] Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llava-critic: Learning to evaluate multimodal models. arXiv preprint arXiv:2410.02712, 2024. [80] Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step, 2025. [81] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, and Wei Chen. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [82] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9(1):1, 2023. [83] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, and Dacheng Tao. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search, 2024. [84] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. [85] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning, 2025. [86] Qifan Yu, Zhebei Shen, Zhongqi Yue, Yang Wu, Wenqiao Zhang, Yunfei Li, Juncheng Li, Siliang Tang, and Yueting Zhuang. Mastering collaborative multi-modal data selection: focus on informativeness, uniqueness, and representativeness. ArXiv, abs/2412.06293, 2024. [87] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities, 2024. 20 [88] Weihao Yu, Zhengyuan Yang, Lingfeng Ren, Linjie Li, Jianfeng Wang, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, and Xinchao Wang. Mm-vet v2: challenging benchmark to evaluate large multimodal models for integrated capabilities. arXiv preprint arXiv:2408.00765, 2024. [89] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. [90] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. [91] Lei Zhang, Fangxun Shu, Tianyang Liu, Sucheng Ren, Hao Jiang, and Cihang Xie. Filter&align: Leveraging human knowledge to curate image-text data. 2023. [92] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024. [93] Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, and Yiming Yang. Improve vision language model chain-of-thought reasoning. arXiv preprint arXiv:2410.16198, 2024. [94] Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025. [95] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. [96] Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei Yang. Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models. Advances in Neural Information Processing Systems, 36:51685191, 2023. [97] Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github. com/hiyouga/EasyR1, 2025. [98] Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. Calibrated self-rewarding vision language models. arXiv preprint arXiv:2405.14622, 2024. [99] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023."
        }
    ],
    "affiliations": [
        "Microsoft",
        "University of Maryland, College Park",
        "University of Michigan"
    ]
}