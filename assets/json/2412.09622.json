{
    "paper_title": "LoRACLR: Contrastive Adaptation for Customization of Diffusion Models",
    "authors": [
        "Enis Simsar",
        "Thomas Hofmann",
        "Federico Tombari",
        "Pinar Yanardag"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in text-to-image customization have enabled high-fidelity, context-rich generation of personalized images, allowing specific concepts to appear in a variety of scenarios. However, current methods struggle with combining multiple personalized models, often leading to attribute entanglement or requiring separate training to preserve concept distinctiveness. We present LoRACLR, a novel approach for multi-concept image generation that merges multiple LoRA models, each fine-tuned for a distinct concept, into a single, unified model without additional individual fine-tuning. LoRACLR uses a contrastive objective to align and merge the weight spaces of these models, ensuring compatibility while minimizing interference. By enforcing distinct yet cohesive representations for each concept, LoRACLR enables efficient, scalable model composition for high-quality, multi-concept image synthesis. Our results highlight the effectiveness of LoRACLR in accurately merging multiple concepts, advancing the capabilities of personalized image generation."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 1 ] . [ 1 2 2 6 9 0 . 2 1 4 2 : r LoRACLR: Contrastive Adaptation for Customization of Diffusion Models Enis Simsar Thomas Hofmann1 Federico Tombari2,3 Pinar Yanardag4 1ETH Zurich 2TU Munich 3Google 4Virginia Tech https://loraclr.github.io/ Figure 1. High-Fidelity Multi-Concept Image Generation. Examples illustrating LoRACLRs ability to generate unified scenes with multiple distinct characters and styles across diverse settings. Each scene demonstrates LoRACLRs capability to combine varied concepts seamlessly, preserving the original identities of each character, as seen in concepts."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Recent advances in text-to-image customization have enabled high-fidelity, context-rich generation of personalized images, allowing specific concepts to appear in variety of scenarios. However, current methods struggle with combining multiple personalized models, often leading to attribute entanglement or requiring separate training to preserve concept distinctiveness. We present LoRACLR, novel approach for multi-concept image generation that merges multiple LoRA models, each fine-tuned for distinct concept, into single, unified model without additional individual fine-tuning. LoRACLR uses contrastive objective to align and merge the weight spaces of these models, ensuring compatibility while minimizing interference. By enforcing distinct yet cohesive representations for each concept, LoRACLR enables efficient, scalable model composition for high-quality, multi-concept image synthesis. Our results highlight the effectiveness of LoRACLR in accurately merging multiple concepts, advancing the capabilities of personalized image generation. Diffusion models for text-to-image generation [11] have revolutionized image synthesis from textual prompts, as demonstrated by major advancements forward from Stable Diffusion [29], Imagen [33], and DALL-E 2 [27]. Customization techniques for these models have further amplified their versatility, enabling the personalized image generation of specific concepts such as characters, objects, or artistic styles. Low-Rank Adaptation (LoRA) [12] has emerged as powerful tool for customizing pre-trained models with minimal retraining, allowing for flexible, efficient personalization. By combining LoRA with advanced customization methods like DreamBooth [30], users can generate images that not only retain high-fidelity but also capture their unique creative vision. However, combining multiple LoRA models to create single composition remains significant challenge. Current multi-concept models often struggle to maintain the quality of individual concepts, require simultaneous training on multiple concepts [20], or need per-image optimization [21]. Alternative methods face specific limitations: some can only merge style and content LoRAs [35], while 1 others become unstable as the number of combined LoRAs grows [13]. Methods like Mix-of-Show [9] require specialized LoRA variants, such as Embedding-Decomposed LoRAs (EDLoRAs), which diverge from the standard LoRA formats widely used in the community. More recent approaches, like OMG [19], employ segmentation methods to isolate subjects during generation, but these approaches rely heavily on the accuracy of the segmentation models used. These challenges limit the broader applicability of text-toimage models, particularly when multiple distinct concepts must coexist in single output image. To address these challenges, we propose LoRACLR, novel approach that combines multiple LoRA models into single model capable of accurately generating multiple concepts simultaneously. Our method introduces novel contrastive objective that aligns each models weight space, prevents interference, and preserves fidelity by ensuring that each model represents its respective concept distinctly within the joint composition. Importantly, our approach allows for the use of pre-existing LoRA models without the need for retraining or accessing original training data. By employing contrastive learning, LoRACLR achieves scalable model composition, enabling high-quality multiconcept image generation without requiring additional finetuning or computational overhead. Our extensive evaluations reveal that LoRACLR achieves significant improvements in both visual quality and compositional coherence over baseline methods. Through qualitative and quantitative experiments, we demonstrate that our approach consistently preserves the fidelity and identity of each concept while avoiding the common issue of feature interference, even as complexity increases with additional concepts. LoRACLR provides practical and scalable solution to compositional customization in generative models, with promising implications for applications like virtual content creation, personalized storytelling, and digital art. 2. Related Work Text-Conditioned Image Synthesis. Text-conditioned image synthesis has seen significant advances through the development of both GANs [2, 8, 1618] and diffusion models [5, 10, 11, 22, 29, 38]. Early GAN-based methods focused on generating images conditioned on classes [2, 14, 16] or text attributes [1, 7, 23, 28]. More recently, the focus has shifted towards large-scale text-to-image diffusion models [27, 29, 33, 42], trained on large-scale datasets [34], enabling more nuanced and accurate image synthesis. Personalized Image Generation and Customization. Personalized image generation aims to embed user-specific concepts that can be reused across different contexts, from distinct characters to unique styles. Early methods like Textual Inversion (TI) [6] and DreamBooth (DB) [30] laid the groundwork by learning representations from limited set of images. TI optimizes text embedding to reconstruct target images using diffusion-based loss, allowing for flexible and personalized image synthesis. DB, on the other hand, fine-tunes model weights to learn unique concept representations, using rare tokens to encode custom features for reliable reproduction. Subsequent works, such as P+ [39], build upon Texture Inversion by incorporating more expressive token representation, enhancing subject alignment and fidelity in generation. Further developments have aimed to improve the scalability and efficiency of customization. Custom Diffusion [20] advanced this goal by fine-tuning only the cross-attention layers, balancing customization precision and computational efficiency. Building on these foundations, DB-LoRA [32] introduces LoRA [12] to DB to enable more parameterefficient tuning, reducing the need for extensive retraining. Recent approaches, such as StyleDrop [37], HyperDreamBooth [31], and variety of feed-forward network-based techniques [15, 36, 40, 41], have further minimized computational demands by predicting adaptation parameters directly from data. Merging Multiple Concepts. Combining LoRAs for style and subject control remains an open research challenge. Current methods for multi-concept synthesis often face notable limitations. Weighted summation [32] is simple approach but suffers from feature interference. Mix-ofShow [9] requires specialized Embedding-Decomposed LoRAs (ED-LoRAs) for each concept, which limits compatibility with standard LoRAs. ZipLoRA [35] can merge style and content LoRAs but struggles when multiple content LoRAs are needed. OMG [19] depends on off-the-shelf segmentation to isolate subjects, making its performance highly dependent on segmentation accuracy and the models ability to generate multiple objects. Recent advancements have focused on merging multiple specialized models into unified generative framework. Mix-of-Show [9] effectively merges models using ED-LoRAs but requires access to the original training data, which limits compatibility with community LoRAs, such as those available on platforms like civit.ai [3]. Orthogonal Adaptation [24] introduces constraints to separate attributes across LoRAs, reducing interference; however, it increases training complexity by directly modifying the fine-tuning process, which also requires access to the original data. Our approach, LoRACLR, uses contrastive objective to align the weight spaces of specialized LoRA models, enabling coherent multi-concept compositions with minimal interference. Unlike prior methods, LoRACLR combines pre-existing LoRA models without retraining, preserving each models distinct attributes for scalable, high-fidelity multi-concept image synthesis. 2 Figure 2. Framework Overview. The framework comprises two main stages: (a) generating concept-specific representations with individual pre-trained LoRA models and (b) merging these representations into unified model using novel contrastive objective. In (a), each LoRA model produces input-output pairs (Xi, Yi) for distinct concepts (V1, V2, . . . , Vn), establishing positive pairs (aligned concepts) and negative pairs (unrelated concepts). In (b), these representations are combined into single model, , to enable multi-concept synthesis. LoRACLR aligns attracting positive pairs to ensure identity retention and repelling negative pairs to prevent cross-concept interference. 3. Method Our proposed approach, LoRACLR, enables seamless multi-concept synthesis by merging independently trained Instead of modiLoRA models in post-training phase. fying or retraining each model for compatibility, LoRACLR uses an optimization-based merging that adapts pre-existing LoRA models to function cohesively within shared model, capable of generating all the target concepts. Leveraging contrastive learning, our method aligns the weight spaces of the models, ensuring each concept retains high-fidelity while remaining compatible in joint compositions. An overview of LoRACLR is shown in Fig. 2. 3.1. Low-Rank Adaptation Models LoRA [12] fine-tunes large models by adding low-rank matrices Win and Wout to frozen base layers, allowing adaptation with minimal computation. Applied to crossattention layers in Stable Diffusion [29], LoRA updates weights as = + WinWout, where Win and Wout are significantly smaller than , reducing model storage to just 15-100 MB compared to the full models 3.44 GB. This fine-tuning embeds new styles or concepts efficiently within the diffusion models latent space. 3.2. LoRACLR The core of LoRACLRs merging process is contrastive loss objective designed to ensure compatibility across independently trained LoRA models within unified model. Given pre-trained LoRA Vi, we first create pairs of input and output features, denoted as Xi and Yi, respectively, see Fig. 2 (a). Meanwhile, predicted features ˆYi represent the merged models output for the same input features, Xi, i.e., ˆYi = (W + )Xi. The intuition behind our contrastive objective is as follows: positive pairs generated by the same LoRA models should attract, while negative pairs generated by the different LoRAs should repel, see Fig. 2 (b). The contrastive loss objective is defined as: Lcontrastive ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:0)d2 p,i + max(0, dn,i)2(cid:1) , (1) where dp,i represents the positive distance for each pair, dn,i represents the negative distance, is the margin parameter, which defines the minimum allowable distance for negative pairs to enforce separation and prevent feature overlap, and is the number of concepts being combined. The positive and negative components are defined as: dp,i = Yi ˆYi2, where dp,i is the distance between the output features Yi of the original LoRA model and the predicted features ˆYi from the merged model for the same concept. (2) dn,i = min j=i Yi ˆYj2, (3) where dn,i is the negative distance, computed as the minimum distance between the output features Yi for given concept and the predicted features ˆYj of unrelated concepts, with = i. This contrastive objective keeps unrelated concepts distinct while aligning each concepts output features with the merged models predicted features, enabling LoRACLR to perform coherent multi-concept synthesis with minimal interference. 3 Delta-Based Merging. LoRACLR uses an additive delta, , to merge LoRA models without altering the base weights directly. Initialized to zero, we learn to adjust the pre-trained weights, preserving the integrity of each model while ensuring compatibility. An L2 regularization term is applied to to limit its magnitude, ensuring sparsity and minimal adjustments. The optimization objective combines the contrastive merging loss, Lcontrastive and Ldelta: Ldelta = λdeltaW 2, (4) where λdelta controls the trade-off between effective merging of concepts and maintaining sparsity in . The total objective is: Ltotal = Lcontrastive + Ldelta. (5) At each step, is updated to minimize Ltotal using gradient descent. Positive and negative samples contribute to the contrastive pairs, reinforcing distinct boundaries for each concept while ensuring cohesive alignment among merged concepts. This iterative process converges to an optimized weight configuration. By restricting updates to rather than altering base weights directly, LoRACLR achieves balanced adaptation, preserving the integrity of each concepts features while enabling seamless multi-concept merging within unified model. 4. Experiments Implementation Details. In all our experiments, we use the Stable Diffusion model [29] with the ChilloutMix checkpoint1 for its high-quality image generation capability. Our approach leverages pre-trained LoRA models, eliminating the need to train individual LoRA models from scratch and significantly reducing computational overhead. During the merging process, we use learning rate of 1e4, margin parameter of 0.5 to control separation between positive and negative pairs during contrastive learning, and regularization coefficient λdelta of 0.001 to ensure sparsity in the learned delta. All experiments, including model outputs, and processing, were conducted on an NVIDIA A100 GPU at Virginia Tech. Combining 12 concepts is highly efficient, typically taking about 5 minutes, which makes our approach scalable and practical for real-world applications that require seamless integration of multiple concepts. For our experiments, we utilize pre-trained single-concept LoRA and/or ED-LoRA models as the starting point. Baselines. We evaluate our approach against several stateof-the-art baselines in multi subject customization, including: DB-LoRA [30], P+ [39], Custom Diffusion [20], Mixof-Show [9], and Orthogonal Adaptation [24]. Each method employs different strategy for merging fine-tuned models. For DB-LoRA, Federated Averaging (FedAvg) is used to 1https://huggingface.co/windwhinny/chilloutmix merge models. Custom Diffusion utilizes its optimizationbased method for merging, while Mix-of-Show merges using gradient fusion. Orthogonal Adaptation introduces orthogonal transformations to mitigate concept interference during merging. In the case of P+, no fine-tuning of model weights is performed; instead, merging is achieved by directly querying each concepts token embedding. Datasets & Metrics. For all evaluations, we use the same experimental setup and dataset proposed by [24], which includes 12 concept identities, each represented by 16 distinct images of the target concept across various contexts. Following prior work [9, 24], we evaluate our method using three key metrics: Text Alignment, Image Alignment, and Identity Alignment. Text Alignment measures text-image similarity using the CLIP [26] model to ensure that generated images match the input prompts. Image Alignment evaluates the similarity between generated and reference images in the CLIP feature space. Identity Alignment uses the ArcFace [4] model to assess how accurately the target human identity is preserved in the generated images. 4.1. Qualitative Results We demonstrate the qualitative results of our method for both single and multiple subjects. Given set of subjects, each represented by personalized modelfor instance, celebrities like Margot Robbie and Taylor Swift (refer to Fig. 3)and text prompt such as <Margot> and <Taylor> in an ancient library with towering shelves, our objective is to generate composite image that integrates these subjects according to the provided text prompt. Using 12 subjects identified by [24], we first merge them into unified model through our novel contrastive-based objective, applied consistently across all experiments. Unlike methods that require individual fine-tuning for each concept [24], our approach can work with pre-trained models such as LoRA or ED-LoRA models. Single Concepts. We first demonstrate our methods ability to preserve individual identities. Figure 3 (bottom row) confirms that our approach maintains the integrity of each identity for single concepts. This capability extends to diverse settings. For example, the <CONCEPT> Cyberpunk style shows subjects portrayed as game-like characters, as seen in the fourth images of the bottom row in Fig. 3. Multiple Concepts. Figure 3 showcases images generated with our method using varying numbers of concepts6, 5, 4, 3, and 2. These visuals demonstrate that our method not only accurately captures each individual identity but also generates composite images guided by the text prompts. Notably, our approach excels by utilizing single merged model to generate wide range of concepts across diverse prompts, eliminating the need to train separate models for different concept counts. We also note that the original LoRA models for individual subjects are trained on vari4 Figure 3. Qualitative Results. LoRACLR effectively combines different numbers of unique concepts across wide range of scenes, producing high-fidelity compositions that capture the complexity and nuance of multi-concept prompts in diverse environments. LoRACLR preserves the identity of each concept, ensuring accurate representation in composite scenes while also maintaining fidelity in singleconcept generation, as demonstrated in the last row. Real images from the original concepts are shown on the left for reference. 5 Figure 4. Multi-Concept Comparison. Composite images generated by our method (LoRACLR) and competing methods (Orthogonal Adaptation [24], Mix-of-Show [9], Prompt+ [39]) for multi-concept prompts. Each row depicts different scene defined by the text prompts. Our method consistently preserves individual identities, while others struggle with identity preservation and concept interference. ous images of celebrities who may have different hairstyles or colors. As result, variations such as different hairstyles or colors can appear even within generations of the same celebrity model (e.g., different generations of Taylor Swift LoRA might show varying hair colors or styles, reflecting her real-life changes). These variations are not inconsistencies in our model but rather reflect the diversity inherent Importantly, despite these in the original LoRA models. 6 Table 1. Quantitative Results. Comparison of LoRACLR against state-of-the-art models, evaluated before and after merging. LoRACLR achieves competitive performance across all metrics, with notable improvements in image and identity alignment post-merging."
        },
        {
            "title": "Method",
            "content": "P+ [39] Custom Diffusion [20] DB-LoRA (FedAvg) [32] MoS (FedAvg) [9] MoS (Grad Fusion) [9] Orthogonal Adaptation [24] LoRACLR (Ours) Text Alignment Image Alignment Identity Alignment Single Merged .643 .643 .668 .673 .613 .682 .625 .621 .625 .631 .624 .644 .668 . +.005 +.069 -.004 +.006 +.020 -.003 Single Merged .683 .683 .648 .623 .744 .531 .745 .735 .745 .729 .748 .741 .766 .776 -.025 -.213 -.010 -.016 -. +.010 Single Merged .515 .515 .504 .408 .683 .098 .728 .706 .728 .717 .740 .745 .799 .828 -.096 -.585 -.022 -.011 +.005 +.029 Figure 5. Quantitative Results on Number of Concepts. Text alignment, image alignment, and identity preservation scores as the number of merged concepts increases. Our method achieves high scores across all metrics, maintaining identity and prompt adherence. Dots represent the baseline metrics for each LoRA model before merging, serving as reference for performance comparisons. variations, our model maintains fidelity and identity to the celebrities faces and ensures that details like hairstyles do not mix across different subjects. Qualitative Comparisons. In Fig. 4, we visually compare our method against several state-of-the-art approaches, including Orthogonal Adaptation, MoS, and P+, across various concepts. Figure 4 illustrate that our method successfully preserves individual identities without crossovers, whereas other methods encounter issues. For instance, Orthogonal Adaptation [24] inadvertently transfers features, such as the hair of female characters, to others, while MoS [9] and P+ [39] struggle with precise identity depiction. Additionally, while these models perform adequately with two or three concepts, their ability to accurately represent more concepts diminishes as the concept count increases. For instance, in scenes with six concepts, refer to Fig. 4 first section, methods other than LoRACLR fail to preserve the identity of Lionel Messi, highlighting their limitations as the number of concepts increases. Style LoRA Integration To showcase the flexibility of our approach, we integrate style-specific LoRA models to generate scenes combining conceptual and stylistic elements, enabling outputs in styles like comic art or oil painting. As shown in Fig. 6, prompts such as ...in city, in comic style capture the vibrant aesthetic of comic art, while ...in garden, holding flowers, in oil painting style reflect the textured quality of oil painting. Complex scenes like ...in castle, signing papers, in oil painting style illustrate the models ability to maintain content coherence alongside stylistic adaptation. These results demonstrate our methods efficiency in preserving content accuracy and achieving high stylistic fidelity, making it practical for creative and artistic workflows. Figure 6. Style LoRA Integration. Our method combines styles like comic art and oil painting into multi-subject scenes, ensuring stylistic fidelity and content coherence, showcasing its flexibility. Figure 7. Non-human subject generation. Our method effectively combines diverse concepts such as animals, objects (e.g., tables, chairs, vases), and monuments (e.g., pyramids, rocks) into cohesive and visually appealing scenes. Non-Human Examples. We present examples involving non-human concepts, including animals, objects, and monuments. Our model generalizes seamlessly to these scenarios while maintaining high-quality outputs and stylistic consistency. As shown in Fig. 7, our method effectively integrates diverse concepts into cohesive scenes. It excels with objects like tables, chairs, and vases, accurately capturing their distinct textures. Additionally, it demonstrates robustness with monumental elements such as pyramids and rocks, blending them seamlessly with other concepts. These results highlight the adaptability of our method in generating coherent and visually engaging compositions across wide range of non-human concepts, including animals, objects, and landmarks. This versatility makes it suitable for both creative and practical applications, such as wildlife illustration, interior design, and architectural visualization. 4.2. Quantitative Results In Tab. 1, we present the results for each approach before and after identity merging to illustrate their impact on the metrics. Our approach achieves the highest scores for image and identity alignment compared to other methods, as demonstrated by the qualitative examples in Fig. 4. While Custom Diffusion and DB-LoRA show superior Text Alignment, they fall short in Image and Identity Alignment, underscoring the versatility and balanced performance of our approach across all key aspects. Effect of # Concepts. Unlike other methods, which struggle to maintain text alignment, image alignment, and identity preservation as the number of combined concepts increases, LoRACLR maintains these metrics, see 5. User Study. To supplement the findings in Tab. 1 and Fig. 5, we conduct user study involving 50 participants on Prolific.com [25]. The study consisted of 40 questions where participants are shown reference images of individual concepts alongside composite images, with varying numbers of concepts, generated by our method and competing methods, presented in randomized order. Participants are asked to evaluate each image pair based on: Identity Alignment: Given the reference images on the left, how well does the image on the right capture the identity of these concepts? (Scale: 1 = Not at all, 5 = Very much). Our method achieves significantly higher ratings for identity alignment compared to other methods, indicating its superior ability to maintain concept identity in composite images, see Tab. 2. Table 2. User Study Results. Our method achieves the highest average score for identity alignment, indicating superior preservation of concept identities compared to competing methods."
        },
        {
            "title": "Identity Alignment",
            "content": "Ours Orthogonal Adaptation Mix-of-Show Prompt+ 3.42 2.41 2.21 2.01 Merging Time. In terms of time efficiency, our method demonstrates significant advantages. It takes only 5 minutes to combine 12 LoRA models. In contrast, [24] requires finetuning each LoRA model from scratch, each taking approximately 10-15 minutes. While the actual merging process of [24] takes only 1 second, it requires prior fine-tuning, which adds up to total of 120 minutes to merge them. Meanwhile, Mix-of-Show requires 15 minutes to merge the models. After merging, generating the images takes approximately 10 seconds for all methods. This comparison clearly shows that our method is substantially faster than other methods in merging models. Once LoRACLR merges LoRA models into unified model, it can generate composite images without any further need for retraining the individual concepts or access to original training data. 4.3. Ablation Study We conduct ablation studies to evaluate the impact of key parameters. By exploring the effects of margin, λdelta, and concept count, we identify optimal settings for robust identity preservation and coherence in complex compositions. Impact of Margin and λdelta. As shown in Fig. 8, we explore different values of margin and λdelta. The results demonstrate that our method achieves robust identity preservation and visual coherence with margin values around 0.250.5 and λdelta set at 0.001. Higher values for either parameter lead to diminished performance in maintaining individual identity and prompt coherence. Effect of # Concepts. Qualitative results in Fig. 8 further demonstrate that our model maintains identity and visual coherence even with complex multi-concept compositions, highlighting its scalability and robustness. 5. Limitation and Societal Impact Our method uses novel contrastive learning objective to merge pre-trained LoRA models and achieves successful results in multi-subject image synthesis. However, similar to related work [9, 24], its performance is inherently tied to the capabilities of the underlying LoRA models provided as input. Therefore, the success of LoRACLR in generating Figure 8. Ablation Study on Margin, λdelta, and Concept Count. Effect of varying margin, λdelta, and number of concepts (2, 5, 8, 12) on identity preservation and visual coherence. coherent and high-quality images depends on the robustness and adaptability of these initial models. This dependency highlights the importance of using well-trained, versatile LoRA models to ensure optimal outcomes. Moreover, given that our method enables sophisticated composition capabilities, it is crucial to consider the potential for misuse, such as in creating deepfakes. Thus, we advocate for the careful use of our method to prevent such applications, promote ethical use, and ensure that advancements in image synthesis contribute positively to technology and society. 6. Conclusion We introduce LoRACLR, novel approach that merges multiple LoRA models using contrastive learning objective. Our method preserves the distinct identities of concepts while enabling the creation of composite images from multiple subjects. LoRACLR is designed to be compatible with any existing LoRA model, ensuring seamless integration with future, more advanced models. Operating as post-training method, LoRACLR requires only one-time merging process and can be used with any prompts, utilizing community-available LoRA models. Unlike existing methods that require individual fine-tuning of each LoRA model, our approach offers more efficient and flexible solution. The effectiveness of LoRACLR is demonstrated through extensive testing, where it shows superior visual quality and coherence compared to other methods."
        },
        {
            "title": "References",
            "content": "[1] Rameen Abdal, Peihao Zhu, John Femiani, Niloy Mitra, and Peter Wonka. Clip2stylegan: Unsupervised extraction of stylegan edit directions. In ACM SIGGRAPH 2022 conference proceedings, pages 19, 2022. 2 [2] Andrew Brock. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. 2 [3] Civitai. https://civitai.com, 2020. 2 [4] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR, pages 46904699, 2019. 4 [5] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 34:87808794, 2021. 2 [6] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 2 [7] Rinon Gal, Or Patashnik, Haggai Maron, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clipguided domain adaptation of image generators. ACM Transactions on Graphics (TOG), 41(4):113, 2022. 2 [8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. [9] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et al. Mix-of-show: Decentralized lowrank adaptation for multi-concept customization of diffusion models. arXiv preprint arXiv:2305.18292, 2023. 2, 4, 6, 7, 9, 12 [10] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2 [11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. 1, 2 [12] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 1, 2, 3 [13] Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. Lorahub: Efficient cross-task generalization via dynamic lora composition. arXiv preprint arXiv:2307.13269, 2023. 2 [14] Xun Huang, Arun Mallya, Ting-Chun Wang, and Ming-Yu Liu. Multimodal conditional image synthesis with productof-experts gans. In European Conference on Computer Vision, pages 91109. Springer, 2022. 2 [15] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and Yu-Chuan Su. Taming encoder for zero fine-tuning image customization with text-to-image diffusion models. arXiv preprint arXiv:2304.02642, 2023. [16] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44014410, 2019. 2 [17] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improvIn Proceedings of ing the image quality of stylegan. the IEEE/CVF conference on computer vision and pattern recognition, pages 81108119, 2020. [18] Tero Karras, Miika Aittala, Samuli Laine, Erik Harkonen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. In Proc. NeurIPS, 2021. 2 [19] Zhe Kong, Yong Zhang, Tianyu Yang, Tao Wang, Kaihao Zhang, Bizhu Wu, Guanying Chen, Wei Liu, and Wenhan Luo. Omg: Occlusion-friendly personalized multiarXiv preprint concept generation in diffusion models. arXiv:2403.10983, 2024. 2, 12 [20] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. 1, 2, 4, 7 [21] Tuna Han Salih Meral, Enis Simsar, Federico Tombari, and Pinar Yanardag. Clora: contrastive approach to compose multiple lora models. arXiv preprint arXiv:2403.19776, 2024. 1 [22] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML, 2021. 2 [23] Gaurav Parmar, Yijun Li, Jingwan Lu, Richard Zhang, JunYan Zhu, and Krishna Kumar Singh. Spatially-adaptive multilayer selection for gan inversion and editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1139911409, 2022. 2 [24] Ryan Po, Guandao Yang, Kfir Aberman, and Gordon Wetzstein. Orthogonal adaptation for modular customization of In Proceedings of the IEEE/CVF Condiffusion models. ference on Computer Vision and Pattern Recognition, pages 79647973, 2024. 2, 4, 6, 7, 9, [25] Prolific. Prolific. https://www.prolific.com/, 2024. Accessed: 2024-02-28. 8 [26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 4 [27] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1, 2 [28] Daniel Roich, Ron Mokady, Amit Bermano, and Daniel Cohen-Or. Pivotal tuning for latent-based editing of real images. ACM Transactions on graphics (TOG), 42(1):113, 2022. 2 [29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. 10 fei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 2 the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2, 3, 4 [30] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 22500 22510, 2023. 1, 2, 4 [31] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65276536, 2024. 2 [32] Simo Ryu. Low-rank adaptation for fast text-to-image diffusion fine-tuning, 2023. 2, 7 [33] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022. 1, 2 [34] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022. 2 [35] Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani. Ziplora: Any subject in any style by effectively merging loras. arXiv preprint arXiv:2311.13600, 2023. 1, 2 [36] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without testtime finetuning. ArXiv, abs/2304.03411, 2023. 2 [37] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image generation in any style. arXiv preprint arXiv:2306.00983, 2023. [38] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 2 and Stefano Ermon. arXiv preprint [39] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual conditioning in text-toimage generation. arXiv preprint arXiv:2303.09522, 2023. 2, 4, 6, 7, 12 [40] Zhouxia Wang, Xintao Wang, Liangbin Xie, Zhongang Qi, Ying Shan, Wenping Wang, and Ping Luo. Styleadapter: single-pass lora-free model for stylized image generation. ArXiv, abs/2309.01770, 2023. 2 [41] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. [42] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin11 LoRACLR: Contrastive Adaptation for Customization of Diffusion Models"
        },
        {
            "title": "Supplementary Material",
            "content": "7. User Study Details We conducted user study to evaluate identity preservation and composition quality in generated images. Participants were shown reference images alongside generated scenes  (Fig. 9)  and asked to rate identity similarity on scale of 1 (does not look similar) to 5 (looks very similar). Figure 9. User Study Interface. Participants rated identity similarity between reference images and generated scenes, focusing on accuracy and realism. 8. More Comparison In addition to the comparisons presented in the main paper, this section highlights further evaluations to emphasize the robustness of our method. 8.1. Comparison with OMG OMG [19] relies on two-step process for scene generation. First, it generates layout that structures the composition of the scene. Next, it populates this layout by placing the subjects in their respective positions. This dependency on intermediate layout generation introduces notable limitations. Errors in the layout creation stage often propagate, resulting in inconsistencies in the final output. Additionally, OMG struggles with scenarios involving subjects that share similar attributes, such as two individuals of the same gender (e.g., two women). This limitation leads to reduced quality and coherence in the generated images. Furthermore, since OMG operates in two stages, it requires approximately twice the inference time compared to single-stage approaches, e.g., ours, Mix-of-show and Orthogonal Adaptation, making it less efficient for real-time or large-scale applications. 12 In contrast, our method bypasses the need for intermediate layouts, directly producing coherent and visually appealing compositions. As shown in Fig. 10, our approach excels in creating realistic and contextually aligned scenes, such as ...on the street, drinking coffee and ...in cool restaurant, delicious meals on the table. These examples highlight the superior fidelity and contextual understanding achieved by our method compared to OMG [19]. 8.2. More Qualitative Comparison This subsection provides additional qualitative results to highlight the strengths of our approach in generating multiconcept scenes, from 2 concepts to 6 concepts. Compared to existing methods such as Orthogonal Adaptation [24], Mixof-Show [9], and P+ [39], our method excels in producing coherent, contextually accurate, and visually appealing compositions, even in complex scenarios involving multiple concepts and intricate stylistic requirements. Figure 11 showcases examples such as ...working in bustling kitchen preparing dish with steam rising from pots and pans. Our method accurately captures the dynamic nature of the scene, ensuring proper interactions between concepts and retaining their distinct identities. In ...inside futuristic spaceship, sci-fi realism, the futuristic aesthetics and intricate details are vividly rendered, demonstrating the superiority of our approach in handling complex compositions compared to baselines, which often introduce artifacts or fail to maintain consistency. Figure 12 further highlights the versatility of our method with scenes such as ...performing surgery together in an operating room. Our model not only preserves the realism of the surgical environment but also ensures that all concepts are seamlessly integrated into the scene. In another example, ...investigating crime scene in noir detective style, our method faithfully reproduces the intended stylistic elements while maintaining accurate subject interactionsa challenge for baseline methods that struggle to balance style and coherence. Finally, Fig. 13 presents challenging scenarios like ...in an ancient grand library with towering shelves. Our method captures the details of the setting while ensuring the concepts interact naturally within the environment. In ...inside futuristic spaceship, sci-fi realism, the vivid rendering of the scenes futuristic details once again underscores the robustness of LoRACLR compared to baselines that exhibit inconsistencies in subject placement and stylistic alignment. Figure 10. Comparison between our method and OMG for generating multi-concept scenes. OMG struggles with intermediate layout dependence and compositional errors, particularly with same-gender concepts, while our method achieves seamless and accurate results. 13 Figure 11. Qualitative comparison of multi-concept scenes. Our method effectively captures dynamic interactions and complex stylistic elements, as seen in examples such as bustling kitchens and futuristic spaceships. It surpasses Orthogonal Adaptation, Mix-of-Show and P+ in coherence and realism. Figure 12. Additional multi-concept image generation examples. Our method demonstrates superior integration of concepts and themes in diverse scenarios, such as operating rooms and detective noir settings, while maintaining stylistic fidelity. 15 Figure 13. Extended qualitative results for multi-concept image generation. It showcases our methods ability to generate intricate compositions, such as ancient libraries and sci-fi interiors. These results emphasize the robustness of our approach in maintaining style, subject integrity, and contextual relevance."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "Google",
        "TU Munich",
        "Virginia Tech"
    ]
}