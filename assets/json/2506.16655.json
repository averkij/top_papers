{
    "paper_title": "Arch-Router: Aligning LLM Routing with Human Preferences",
    "authors": [
        "Co Tran",
        "Salman Paracha",
        "Adil Hafeez",
        "Shuguang Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rapid proliferation of large language models (LLMs) -- each optimized for different strengths, style, or latency/cost profile -- routing has become an essential technique to operationalize the use of different models. However, existing LLM routing approaches are limited in two key ways: they evaluate performance using benchmarks that often fail to capture human preferences driven by subjective evaluation criteria, and they typically select from a limited pool of models. In this work, we propose a preference-aligned routing framework that guides model selection by matching queries to user-defined domains (e.g., travel) or action types (e.g., image editing) -- offering a practical mechanism to encode preferences in routing decisions. Specifically, we introduce \\textbf{Arch-Router}, a compact 1.5B model that learns to map queries to domain-action preferences for model routing decisions. Our approach also supports seamlessly adding new models for routing without requiring retraining or architectural modifications. Experiments on conversational datasets demonstrate that our approach achieves state-of-the-art (SOTA) results in matching queries with human preferences, outperforming top proprietary models. Our approach captures subjective evaluation criteria and makes routing decisions more transparent and flexible. Our model is available at: \\texttt{https://huggingface.co/katanemo/Arch-Router-1.5B}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 5 5 6 6 1 . 6 0 5 2 : r Arch-Router: Aligning LLM Routing with Human Preferences Co Tran Katanemo Labs, Inc. co-tran@katanemo.com Salman Paracha Katanemo Labs, Inc. salman@katanemo.com Adil Hafeez Katanemo Labs, Inc. adil@katanemo.com Shuguang Chen Katanemo Labs, Inc. shuguang@katanemo.com"
        },
        {
            "title": "Abstract",
            "content": "With the rapid proliferation of large language models (LLMs) each optimized for different strengths, style, or latency/cost profile routing has become an essential technique to operationalize the use of different models. However, existing LLM routing approaches are limited in two key ways: they evaluate performance using benchmarks that often fail to capture human preferences driven by subjective evaluation criteria, and they typically select from limited pool of models. In this work, we propose preference-aligned routing framework that guides model selection by matching queries to user-defined domains (e.g., travel) or action types (e.g., image editing) offering practical mechanism to encode preferences in routing decisions. Specifically, we introduce Arch-Router, compact 1.5B model that learns to map queries to domain-action preferences for model routing decisions. Our approach also supports seamlessly adding new models for routing without requiring retraining or architectural modifications. Experiments on conversational datasets demonstrate that our approach achieves state-of-theart (SOTA) results in matching queries with human preferences, outperforming top proprietary models. Our approach captures subjective evaluation criteria and makes routing decisions more transparent and flexible. Our model is available at: https://huggingface.co/katanemo/Arch-Router-1.5B."
        },
        {
            "title": "Introduction",
            "content": "As new models continue to emerge rapidly [2, 21], users are shifting from single-model setups to multi-model systems to leverage the unique strengths of each LLM for tasks like text summarization, code generation, image editing [9, 24]. LLM routing [6, 10] has emerged as an effective way to build and deploy such systems by processing each user query through router model that selects the most suitable LLM. However, existing routing approaches have limitations in real-world use. They typically optimize for benchmark performance while neglecting human preferences driven by subjective evaluation criteria [5, 14]. For instance, some routers are trained to achieve optimal performance on benchmarks like MMLU [8] or GPQA [29], which dont reflect the subjective and task-specific judgments that users often make in practice. These approaches are also less flexible because they are typically trained on limited pool of models [45], and usually require retraining and architectural modifications to support new models or use cases. This exposes fundamental gap: the need for routing systems that align with subjective human preferences, offer more transparency, and remain easily adaptable as models and use cases evolve. To address this, we propose preference-aligned routing frameworka principled approach to matching queries with routing policies based on user-defined preferences. In our framework, users define routing policies using Domain-Action Taxonomy (e.g., healthcare, code explanation) expressed in natural language. Each policy is associated with preferred model, enabling human-aligned control over routing decisions that capture subjective evaluation criteria grounded in real-world use. At the core of this framework is Arch-Router 1, compact 1.5B language model that matches user queries to routing policies with high accuracy. We also introduce complementary data creation pipeline that produces high-quality, labeled conversationscapturing nuanced intents and complex dialogue patternsto train and evaluate preference-aligned routing effectively. Given set of natural language policy descriptions, Arch-Router makes accurate routing decisions without requiring retraining or architectural changesmaking our framework highly adaptable as new routes or models are added. Trained on rich conversational data, Arch-Router handles diverse conversations and multi-turn interactions more effectively than static embedding-based methods [4, 12]. Furthermore, our experiments show that Arch-Router outperforms top proprietary LLMs by 7.71% on average. To summarize, our work makes the following key contributions: 1. preference-aligned routing framework comprising Domain-Action Taxonomy and 1.5B model that maps queries to user-defined routing policies with high accuracy. 2. Our approach aligns with subjective human preferences, enabling more practical and userrelevant routing decisions. 3. It offers transparency and flexibility in model routing, reflecting how LLMs are evaluated, integrated, and applied in real-world scenarios."
        },
        {
            "title": "2 Related Work",
            "content": "Recent LLM routing work is focused on two broad categories: task-specific routing or performancebased routing. We provide an overview of these methods and highlight the gaps that motivate our work in guiding routing decisions with human preferences. Task-Based Routing. This approach focuses on directing queries to models or systems specialized for predefined task. These strategies range from using BERT-based classifiers for domain classification [32] to more sophisticated methods involving k-NN layers and entropy-based classification [11]. For example, OrchestraLLM [17] implements retrieval-based router: at inference it finds the kk most similar dialogue exemplars via embedding similarity and routes to small or large expert model by majority vote. HuggingGPT uses LLMs for model selection based on model descriptions [31]. This principle has also been extended to Retrieval-Augmented Generation (RAG) scenarios, where routers dynamically select the appropriate data modality for retrieval to improve response accuracy [18, 26, 42]. While effective for well-defined and clearly separable tasks, these approaches struggle in settings where task boundaries blur or overlap. They are particularly brittle in multi-turn interactions, where the users intent may evolve or drift, requiring adaptive reclassification that most static routers are not designed to handle without retraining or manual intervention. Performance-Based Routing. The most prevalent research in LLM routing is around performancebased approaches that optimize for cost-performance trade-offs in routing between different LLMs. This approach typically uses scoring function to predict which model will yield the best possible performance for given query. They aim to automatically select the model most likely to produce high-quality response, often by training router to decide whether query can be handled by \"weak,\" cheaper model or must be escalated to \"strong,\" more expensive ones. Examples are RouteLLM[22] and HybridLLM[6]. Or FrugalLLM that expands further by choosing from larger pool of LLMs with specific budget constraints [3]. Training methods range from leveraging historical performance statistics [7, 39] to semantic similarity [4, 12] and ELO-based ranking [43]. While achieving the best possible performance is the right end goal, current work suffers from limitations that hinder its use in practical real-world settings. First, these routers are often brittle 1Available at https://huggingface.co/katanemo/Arch-Router-1.5B. and rigid. Trained on small, limited set of models (typically 3-10) and specific task domains (e.g., coding, math) using benchmarks like RouterBench [10] or MT-Bench [45]. Their performance degrades on out-of-domain queries and they cannot adapt to new models without retraining [15]. Second and more fundamentally, current work treats quality as an objective measurement, neglecting human preferences driven by subjective evaluation criteria. As consequence, performance-based routing is more suitable in controlled environments with stable model sets and objective tasks, but ill-suited for real world scenarios where subjective evaluation of response quality matter. Human Preferences. Preference modeling and Reinforcement Learning from Human Feedback (RLHF) have led to notable gains in LLM performance on well-defined tasks like question answering and summarization [23, 30, 33, 38]. However, these methods are less effective in scenarios where quality is shaped by subjective factorssuch as tone, style, or task-specific expectationsthat cannot be reliably captured by well-defined benchmarks [5]. Studies consistently show disconnect between LLM-based evaluators and human judgments in such settings [14, 34, 41]. While LLMs can mimic human-like fluency, they often overlook subtle signals in user intent and encode their own systemic biases [41]. These limitations undermine routing methods that rely on automatic quality scores for model selection. Such systems are typically trained on LLM-labeled preference datasets and evaluated on benchmarks that reflect aggregate rather than individual preferences, such as Chatbot Arena [45] and RouterBench [10]. In contrast, our work reframes LLM routing as preference-alignment problem, where model selection is guided by matching queries to user-defined policies expressed via Domain (e.g., finance) or Action taxonomy (e.g., image search). This offers practical mechanism to encode subjective preferences directly into routing decisions, making them more interpretable, flexible, and aligned with realworld usage."
        },
        {
            "title": "3 Problem Formulation",
            "content": "3.1 Preliminary The main goal of LLM routing is to choose the appropriate model for given query from pool of models, creating system that is more capable and effective than any single model. LLM Routing is commonly defined as function : (q, P) that maps user query to an appropriate model from model pool under routing objectives such as optimal performance and low cost. 3.2 Preference-aligned Routing To incorporate human preferences as routing objectives, we introduce routing framework that decouples route selection from model assignment. We define set of route policies = {c1, . . . , ck}, where each route policy ci = (ni, di) is tuple consisting of unique route identifier ni paired with natural language description di. Additionally, we define mapping : that associates each route policy with specific model. DomainAction Taxonomy In this work, we focus on modeling LLM routing to align with human preferences driven by subjective evaluation criteria. To incorporate human preferences as routing objectives, we adopt DomainAction taxonomy, two-level hierarchy that mirrors how people typically describe tasksstarting with general topic and narrowing to specific action. Domain (e.g., legal and finance) captures the high-level thematic context of query while Action (e.g., summarization and code generation) denotes the specific operation requested. This taxonomy serves as mental model to help users define clear and structured routing policies. Separating Domain and Action strikes balance between expressiveness and simplicity: it avoids an unwieldy flat list of composite labels (e.g., finance summarization and legal advice) and introduces natural fallback. If query is too vague to match an Action, the router can still resolve the Domainmaintaining robustness and reducing semantic ambiguity. Routing Mechanism As shown in Fig. 1, the routing process includes two stages: first, preferencealigned router : (q, C) takes user query and the complete set of route policies C, then selects the most appropriate based on the policy descriptions. Second, maps the selected 3 Figure 1: Preference-Aligned Routing Mechanism. The routes policies and user conversation is provided to the router to select the appropriate policy and corresponding LLM. Example of usage in coding application is shown in the right. route policy to its associated LLM model: = (c) 2 encodes human preferences in both the construction and description of route policies in C, and the associations between route policies and models defined in . Thus, best is defined directly by human preference rather than by an estimated performance score. Because the LLM routing process is decoupled to = F, model selection is completely delegated to . Models can therefore be added, removed, or swapped by editing alone, without retraining or modifying the router. This decoupling provides the flexibility required in practical deployments where model usage evolves continuously. Practical Benefits This preference-aligned routing design offers several key advantages over traditional routing approaches. First, the decoupling of route selection from model assignment enables dynamic reconfiguration without retraining the routerusers can update model mapping in to accommodate new models, changed requirements, or performance optimizations while preserving the learned routing logic in F. Second, the natural language descriptions di in route policies make the system interpretable and auditable, allowing users to understand routing decisions and validate that policies align with intended preferences. Third, the Domain-Action taxonomy provides both flexibility and structure: it supports fine-grained control when specific domain-action combinations are defined, while gracefully degrading to domain-level routing when actions are ambiguous or undefined. Finally, this framework naturally accommodates human-in-the-loop refinement, as route policies can be iteratively adjusted without requiring architectural changes or model retraining, making the system both maintainable and adaptable to evolving preferences."
        },
        {
            "title": "4 Methodology",
            "content": "We introduce Arch-Router, compact 1.5B generative language model designed for our framework. Given query and complete policy set C, Arch-Router generates the identifier of the best-matching policy. Because the full set of policy descriptions is included in the prompt, the model can adopt new or modified routes at inference time without retraining, aligning seamlessly with the frameworks modular design. Furthermore, we present novel, two-phase data creation pipeline to model the routing framework. Our data pipeline (i) builds rich corpus of route policies and realistic multi-turn dialogues and (ii) injects real-world scenariostopic shifts, irrelevant turns, and noise. 4.1 Data Creation Framework To support Arch-Router, we generate corpus that represents the operating environment of the routing function in two phases (see Fig. 2 for full illustration). Phase 1 focuses on generating clean, structurally correct conversations grounded in verified set of route policies. Phase 2 then systematically introduces real-world complexities to augment these conversations and policy set, reproducing the ambiguity the router must resolve in practice. This two-phase separation is crucial: it allows us to first ensure the correctness and quality of the core conversational data, and then 2An open-source implementation is available at github.com/katanemo/archgw. 4 independently layer on challenges to improve data robustness. Every data sample contains (i) the conversation, (ii) the full set as route policies, and (iii) the ground truth policy, yielding data set that represents the preference-aligned routing task as it occurs during inference time. Data Generation. Phase 1 produces clean, labeled conversations through structured two-step process. First, we generate route policies by constructing diverse topic pool from industry classifications [20], academic benchmarks such as MMLU [8], and real-world API documentation [19]. An LLM is used to generate candidate route policies from this pool. These candidates are then validated and refined by second LLM to ensure clarity, appropriate granularity (as defined by the Domain-Action Taxonomy 3.2), and semantic coherence. Second, we synthesize conversations using the curated set of route policies. For each policy, an LLM generates specific conversational intent, which is then passed to another LLM to produce full dialogue. To ensure data quality, final LLM verifies the alignment between the conversation and the intended routing policy. Conversations that fail this check are regenerated. Data Augmentation. Phase 2 enhances the dataset by systematically incorporating real-world complexity to improve the robustness of the routing model. We apply three augmentation techniques to diversify conversational patterns: 1) Irrelevance injection introduces noise by adding off-topic user messages or removing the ground-truth route policy from sample, simulating ambiguity in user intent, 2) Policy modification alters the candidate set of route policies by including irrelevant or misleading options, creating more challenging decision boundaries, and 3) Scenario mixing enriches the data by combining segments from different conversations to create longer dialogues with abrupt topic shifts, follow-up questions, and abandoned intents. These augmentations yield more representative and challenging dataset, essential for training routing model that generalizes well to real-world usage. 4.2 Arch-Router We construct Arch-Router - FArch as generative language model where it is trained to generate the route identifier of the target route policy. FArch is provided with structured prompt (for full prompt template see 3) that contains both the user query and the set of all possible route policies C. The model FArch processes this input prompt to generate route identifier. The training objective is maximizing the generated output being the correct route policy, ctrue, which represented as minimizing the cross-entropy loss over (x, ctrue) pair [25]: arg min FArch L(FArch(x), ctrue) (1) Design Benefits. The decision to implement Arch-Router as generative language model offers advantages over other approaches like classifier or semantic matching. Unlike multi-class classifier, which is architecturally bound to fixed set of output classes, generative model is inherently flexible. By providing the available route policies as part of the input prompt, Arch-Router can adapt to new policies at inference time without retraining. Moreover, this approach allows Arch-Router to leverage its foundation models pre-trained knowledge, improving semantic understanding of both the query and the policy descriptions. Furthermore, this approach allows the model to process the entire conversation history together with the policy descriptions simultaneously. This is distinct advantage over semantic search methods or classifier that embed the query and routes separately."
        },
        {
            "title": "5 Experiment",
            "content": "Based on the data creation framework, we curated set of 43k samples to train the Arch-Router model. In this section, we lay out the details and results of our training and evaluation, consequently demonstrating the robust design of data creation framework and the effectiveness of preferencealigned routing. 5.1 Experiment Setup Evaluation Corpora. We evaluate our model on four public datasets described in below. For all the dataset, we randomly sample portion from each dataset for eval, the full details are shown in 5 Figure 2: Overview of the data creation for Arch-Router framework. Phase 1 generates route configurations through an LLM process with feedback loops. Phase 2 generates conversations from generated intent. Phase 3 augments the conversations to get diverse scenarios and irrelevance. Table 4. To adapt the evaluation sets into our preference-aligned routing framework, we augmented each dataset. Additionally, all route descriptions are generated using an LLM and went through verification step for consistency with the training dataset. CLINC-150 [16] convers broad range of user needs in task-oriented dialogue systems. Since the dataset doesnt have the hierarchy between action and domain, we leverage LLM to cluster them by intent name semantic. MANtIS [27] is multi-domain, human-human dialogue dataset focused on informationseeking tasks. We only use the domains provided from this dataset to measure performance. SGD [28] (Schema-Guided Dialogue) is dialogue state tracking dataset. Similarly to CLINC-150, we cluster action level route policies by LLM to create domains ground truth. LMSYS-1M [44] is large-scale evaluation dataset with over one million conversations. For our evaluation, we annotated each conversation with route policy labels and configurations using both LLM and human. Comparison Pool. We compare the performance of our trained model Arch-Router against stateof-the-art proprietary model familes such as OpenAI (GPT-4o-mini, GPT-4o); Anthropic (Claude3.5-haiku, Claude-3.7-sonnet); Gemini (Gemini-2.0-flash, Gemini-2.0-flash-lite), and the original Qwen2.5-1.5B. Base Model. Considering the balance between speed and performance for practical deployment, we use models under 2B in size from the following families: Qwen 2.5 [1], Qwen 3 [40], Llama 3.2 [37], Gemma 2[36], and Gemma 3[35]. The best performing model based on validation split is chosen to be Arch-Router, which based on Qwen 2.5 - 1.5B. Training Details. We adopt Supervised fine-tuning (SFT) [25], carried out with the Llama-Factory library [46], using full-parameter updates in bfloat16 precision, the AdamW optimizer, and maximum of four epochs on single NVIDIA A100 GPU. We train over 43, 000 pairs of samples with training/validation split as 90/10. Full training data details are shown in Table 4. Metrics. We measure model performance at the following three levels of accuracy 1. Turn: the fraction of single turns for which the predicted route matches the ground-truth. 2. Span: the fraction of contiguous spans of identical route labels in which all turns are predicted correctly; labels immediately outside the span may differ. 3. Conversation: the fraction of conversations whose every turn is correctly predicted. For more comprehensive evaluation, we also include these three scenarios: 6 Models Turn Span Conv. Overall Qwen2.5-1.5B 34.37 16.09 11. GPT-4o GPT-4o-mini Claude-sonnet-3.7 Claude-haiku-3.5 Gemini-2.0-flash Gemini-2.0-flash-lite 93.96 87.49 96.24 88.21 89.19 85.13 90.74 80.89 94.70 81.46 85.05 72.61 84.52 80.00 87.45 83.72 85.79 74.76 Arch-Router 96. 94.98 88.48 20.69 89.74 82.79 92.79 84.96 85.63 76.69 93.17 Table 1: Overall routing performance across different evaluation granularities. Each metric is an aggregated score averaged over all test datasets and scenarios (fQfA, fQcA, Irrelevance) from Tables 5,6 ,7 ,8 in Appendix A. The table shows how accuracy is maintained as the evaluation scope expands from single turn, to multi-turn span, to the entire conversation. Models fQfA fQcA Irrelevance Turn Overall Qwen2.5-1.5B 51.74 40.89 GPT-4o GPT-4o-mini Claude-sonnet-3.7 Claude-haiku-3.5 Gemini-2.0-flash Gemini-2.0-flash-lite 97.09 95.77 96.87 93.85 93.74 92.93 91.85 86.03 94.63 90.27 88.72 87.61 Arch-Router 98.11 93.56 10. 92.94 80.67 97.21 80.51 85.11 74.86 96.49 34.37 93.96 87.49 96.24 88.21 89.19 85.13 96.05 Table 2: Detailed breakdown of turn-level performance across three evaluation scenarios which also aggregated from Tables 5,6 ,7 ,8 in A. The scores show model accuracy on the action-level task (fQfA), on domain-level when action is not available (fQcA), and irrelevant queries (Irrelevance). The Turn Overall column corresponds to the Turn column in Table 1. 1. Fine-grained Query Fine-grained Answer (fQfA). the models ability to match the exact action route policy. 2. Fine-grained Query Coarse-grained Answer (fQcA). the models ability to match the domain route policy where there is no exact action route policy present. 3. Irrelevance. the models ability detect irrelevant request or request is complete. 5.2 Results Arch-Router records the highest overall routing score of 93.17%  (Table 1)  , surpassing every other candidate model on average by 7.71%. Its margin widens with context length: per-turn accuracy is competitive, yet span-level and full-conversation accuracies rise to the top 94.98% and 88.48%, respectivelyevidence that the model can follow multi-turn context better than other candidate models. scenario-wise breakdown  (Table 2)  sharpens this picture. On the fine-grained fQfA benchmark Arch-Router attains 98.11, confirming its ability to map queries to specific action routes. Performance on the other tasks fQcA and irrelevancematches Claude-sonnet-3.7, the strongest proprietary model in our candidate set, indicating that the generative, preference-aligned design remains robust even when the query cues are coarse or out-of-distribution."
        },
        {
            "title": "6 Analysis",
            "content": "While the aggregated performance scores in the previous section demonstrate the effectiveness of our approach, this section uncovers the qualitative and practical implications of our framework. We begin 7 Figure 3: Comparison of failure distributions for Arch-Router (left) and Claude-Sonnet-3.7 (right) on SGD dataset 7. Each panel combines heat-map and histogram. The heatmaps illustrate at which turn multi-turn conversational spans tend to fail and how often different span lengths are affected. The histograms show the count distribution of the first fail turn index. by examining the specific error patterns to understand in which scenarios Arch-Router succeeds and fails. Then, we contextualize our work by comparing the preference-aligned to performance-based routing and discussing the inherent limitations of our approach. 6.1 Analysis on Model Behavior In Fig. 3, we provide an analysis on error patterns for both Arch-Router and Claude-Sonnet-3.7 closer look at the error patterns reveals distinct failure profiles. In general, both models exhibit the highest frequency of failures in spans of moderate length (3-4 turns), as shown in the heatmaps. However, the distribution of when these failures first occur highlights crucial behavioral difference. Arch-Router fails most often on the first turn. The bar chart shows large spike in first-failures at turn 1, which then steadily decreases. This suggests that Arch-Router is most vulnerable to initial query ambiguity; if it correctly identifies the users intent from the start, it is highly robust and unlikely to fail on subsequent follow-up turns. In contrast, Claude-Sonnet-3.7s failures are more evenly distributed throughout the span. Its bar chart shows that the highest number of first-failures occurs mid-span at turn 3, with significant failures also happening at turns 1. This pattern suggests that Claude-Sonnet-3.7 is less sensitive to ambiguity in the first turn but more likely to fail on later follow-up requests. After correctly handling the first turn, it can drift off-context or misread nuanced follow-up, leading to errors later in the span. The error pattern analysis further reinforce our observation on the strong ability of Arch-Router on tracking continuous intents. 8 6.2 Preference-Aligned V.S. Performance-Based Routing Preference-aligned and performance-based routing represent two fundamentally different approaches for routing LLMs, distinguished by their routing objectives and mechanisms. Preference-aligned routing frames as multi-class classification over user-defined route policies. In contrast, performancebased routing treats it as constrained optimization, seeking to select model that maximizes predicted numerical quality score while adhering to budget, as formulated in [13]. This reflects fundamental difference in where the judgment of quality resides. In preference-aligned routing, the user makes the quality judgment, defining set of route policies that serve as categorical proxy. The routers task is not to guess what LLM is best for each task, but to faithfully map user query to the user-defined policies. Conversely, performance-based routing delegates the quality judgment to the model, using score function as direct numerical proxy to predict the best outcome. This leads to natural difference in usage: Consequently, preference-aligned routing is the method of choice when the quality is subjective and driven by humanthat is, when success is determined by user-defined criteria rather than by an automatic metricwhereas performance-based routing best suits cost-sensitive settings equipped with reliable, model-predicted quality scores. 6.3 Limitations Preference-based routing offers clear advantages in transparency and control, but this design introduces two limitations. First, routing accuracy is constrained by the precision of the policy set; ambiguous or overlapping policy descriptions directly degrade routing performance. For instance, if one policy is defined as review contract clauses and another as analyze legal documents, query about non-disclosure agreements could reasonably match both descriptions, leading the router to make arbitrary routing decisions that may not align with user intent. Second, overall routing effectiveness is inherently constrained by user model selection. If users assign inappropriate models to their route policies, even accurate routing will result in suboptimal outcomes."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we introduce preference-aligned routing framework that lets users encode human preferences as explicit route policies and decouple those policies from model selection enabling routing decisions that reflect subjective evaluation criteria while improving transparency and flexibility. Within this framework, we presented Arch-Router, compact 1.5B generative language model and two-phase data-creation pipeline. Empirical results on multi-turn conversations benchmarks show that Arch-Router surpasses the best proprietary models. From our results and analysis, we highlight the effectiveness and practical use of human preferences as primary routing objectives. Future work includes exploring these following directions: 1) hybrid frameworks that combine preferencealigned routing with precise performance objectives, and 2) human preference modeling to support wider range of routing policies. Addressing these challenges will broaden the applicability of preference-aligned routing and reinforce its role as practical approach for LLM routing."
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [2] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. survey on evaluation of large language models. ACM transactions on intelligent systems and technology, 15(3):145, 2024. [3] Lingjiao Chen, Matei Zaharia, and James Zou. FrugalGPT: How to use large language models while reducing cost and improving performance. Transactions on Machine Learning Research, 2024. [4] Shuhao Chen, Weisen Jiang, Baijiong Lin, James Kwok, and Yu Zhang. Routerdc: Query-based router by dual contrastive learning for assembling large language models. Advances in Neural Information Processing Systems, 37:6630566328, 2024. 9 [5] Rendi Chevi, Kentaro Inui, Thamar Solorio, and Alham Fikri Aji. How individual traits and language styles shape preferences in open-ended user-llm interaction: preliminary study. arXiv preprint arXiv:2504.17083, 2025. [6] Dujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee, Victor Rühle, Laks V. S. Lakshmanan, and Ahmed Hassan Awadallah. Hybrid LLM: Cost-efficient and qualityaware query routing. In The Twelfth International Conference on Learning Representations, 2024. [7] Tao Feng, Yanzhen Shen, and Jiaxuan You. Graphrouter: graph-based router for LLM selections. In The Thirteenth International Conference on Learning Representations, 2025. [8] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and In International Jacob Steinhardt. Measuring massive multitask language understanding. Conference on Learning Representations, 2021. [9] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, 2024. [10] Qitian Jason Hu, Jacob Bieker, Xiuyu Li, Nan Jiang, Benjamin Keigwin, Gaurav Ranganath, Kurt Keutzer, and Shriyash Kaustubh Upadhyay. Routerbench: benchmark for multi-llm routing system. arXiv preprint arXiv:2403.12031, 2024. [11] Swayambhoo Jain, Ravi Raju, Bo Li, Zoltan Csaki, Jonathan Li, Kaizhao Liang, Guoyao Feng, Urmish Thakkar, Anand Sampat, Raghu Prabhakar, et al. Composition of experts: modular compound ai system leveraging large language models. arXiv preprint arXiv:2412.01868, 2024. [12] Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Exploring the benefits of training expert language models over instruction tuning. In International Conference on Machine Learning, pages 1470214729. PMLR, 2023. [13] Wittawat Jitkrittum, Harikrishna Narasimhan, Ankit Singh Rawat, Jeevesh Juneja, Zifeng Wang, Chen-Yu Lee, Pradeep Shenoy, Rina Panigrahy, Aditya Krishna Menon, and Sanjiv Kumar. Universal model routing for efficient llm inference. arXiv preprint arXiv:2502.08773, 2025. [14] Erik Jones, Arjun Patrawala, and Jacob Steinhardt. Uncovering gaps in how humans and LLMs interpret subjective language. In The Thirteenth International Conference on Learning Representations, 2025. [15] Aly Kassem, Bernhard Schölkopf, and Zhijing Jin. How robust are router-llms? analysis of the fragility of llm routing capabilities. arXiv preprint arXiv:2504.07113, 2025. [16] Stefan Larson, Anish Mahendran, Joseph J. Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K. Kummerfeld, Kevin Leach, Michael A. Laurenzano, Lingjia Tang, and Jason Mars. An evaluation dataset for intent classification and out-of-scope prediction. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 13111316, Hong Kong, China, November 2019. Association for Computational Linguistics. [17] Chia-Hsuan Lee, Hao Cheng, and Mari Ostendorf. OrchestraLLM: Efficient orchestration of language models for dialogue state tracking. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 14341445, Mexico City, Mexico, June 2024. Association for Computational Linguistics. [18] Yangning Li, Yinghui Li, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao Zheng, Fei Huang, Jingren Zhou, and Philip S. Yu. Benchmarking multimodal retrieval augmented generation with dynamic VQA dataset and self-adaptive planning agent. In The Thirteenth International Conference on Learning Representations, 2025. 10 [19] Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh RN, et al. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. Advances in Neural Information Processing Systems, 37:5446354482, 2024. [20] Morningstar, Inc."
        },
        {
            "title": "Equity",
            "content": "research methodology. http://corporate. morningstar.com/us/documents/methodologydocuments/methodologypapers/ equityclassmethodology.pdf, n.d. Accessed: 2025-06-11. [21] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. comprehensive overview of large language models. arXiv preprint arXiv:2307.06435, 2023. [22] Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph Gonzalez, Waleed Kadous, and Ion Stoica. Routellm: Learning to route llms from preference data. In The Thirteenth International Conference on Learning Representations, 2024. [23] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [24] Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122, 2023. [25] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. [26] Chunyi Peng, Zhipeng Xu, Zhenghao Liu, Yishan Li, Yukun Yan, Shuo Wang, Zhiyuan Liu, Yu Gu, Minghe Yu, Ge Yu, et al. Learning to route queries across knowledge bases for step-wise retrieval-augmented reasoning. arXiv preprint arXiv:2505.22095, 2025. [27] Gustavo Penha, Alexandru Balan, and Claudia Hauff. Introducing mantis: novel multi-domain information seeking dialogues dataset. arXiv preprint arXiv:1912.04639, 2019. [28] Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. Schemaguided dialogue state tracking task at dstc8. arXiv preprint arXiv:2002.01359, 2020. [29] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [30] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [31] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36:3815438180, 2023. [32] Toby Simonds, Kemal Kurniawan, and Jey Han Lau. MoDEM: Mixture of domain expert models. In Tim Baldwin, Sergio José Rodríguez Méndez, and Nicholas Kuo, editors, Proceedings of the 22nd Annual Workshop of the Australasian Language Technology Association, pages 7588, Canberra, Australia, December 2024. Association for Computational Linguistics. [33] Bingqing Song, Boran Han, Shuai Zhang, Hao Wang, Haoyang Fang, Bonan Min, Yuyang Wang, and Mingyi Hong. Effectively steer llm to follow preference via building confident directions. arXiv preprint arXiv:2503.02989, 2025. [34] Annalisa Szymanski, Noah Ziems, Heather Eicher-Miller, Toby Jia-Jun Li, Meng Jiang, and Ronald Metoyer. Limitations of the llm-as-a-judge approach for evaluating llm outputs in expert knowledge tasks. In Proceedings of the 30th International Conference on Intelligent User Interfaces, pages 952966, 2025. 11 [35] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [36] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. [37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [38] Binghai Wang, Runji Lin, Keming Lu, Le Yu, Zhenru Zhang, Fei Huang, Chujie Zheng, Kai Dang, Yang Fan, Xingzhang Ren, et al. Worldpm: Scaling human preference modeling. arXiv preprint arXiv:2505.10527, 2025. [39] Yu Xia, Fang Kong, Tong Yu, Liya Guo, Ryan Rossi, Sungchul Kim, and Shuai Li. Which llm to play? convergence-aware online model selection with time-increasing bandits. In Proceedings of the ACM Web Conference 2024, pages 40594070, 2024. [40] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [41] Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, Nitesh Chawla, and Xiangliang Zhang. Justice or prejudice? quantifying biases in LLM-as-a-judge. In The Thirteenth International Conference on Learning Representations, 2025. [42] Woongyeong Yeo, Kangsan Kim, Soyeong Jeong, Jinheon Baek, and Sung Ju Hwang. Universalrag: Retrieval-augmented generation over multiple corpora with diverse modalities and granularities. arXiv preprint arXiv:2504.20734, 2025. [43] Zesen Zhao, Shuowei Jin, and Morley Mao. Eagle: Efficient training-free router for multi-llm inference. arXiv preprint arXiv:2409.15518, 2024. [44] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, et al. Lmsys-chat-1m: large-scale real-world llm conversation dataset. arXiv preprint arXiv:2309.11998, 2023. [45] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. [46] Zheng, Zhang, Zhang, Ye, Luo, Feng, and Llamafactory Ma. Unified efficient fine-tuning of 100+ language models. arxiv 2024. arXiv preprint arXiv:2403.13372, 2024."
        },
        {
            "title": "A Datasets Performance",
            "content": "Prompt element Exact text TASK You are helpful assistant designed to find the best suited route. You are provided with route description within <routes></routes> XML tags: <routes> n{routes}n </routes> <conversation> n{conversation}n </conversation>Your task is to decide which route is best suit with user intent on the conversation in <conversation></conversation> XML tags. Follow the instruction: 1. If the latest intent from user is irrelevant or user intent is full filled, respond with other route {\"route\": \"other\"}. 2. Analyze the route descriptions and find the best match route for user latest intent. 3. Respond only with the route name that best matches the users request, using the exact name in the <routes> block. Based on your analysis, provide your response in the following JSON format if you decide to match any route: {\"route\": \"route_name\"} Table 3: Routing Prompt This section provides the detailed performance breakdown for each model across our four evaluation datasets. The datasets are organized to test models on spectrum of complexity, from context-free single-turn queries to intricate, real-world multi-turn dialogues. The prefixes STand MTin the table headers distinguish between these evaluation contexts: ST (Single-Turn): Refers to evaluations on isolated queries where no conversation history is provided. This context is used for the CLINC150 dataset  (Table 5)  . MT (Multi-Turn): Refers to evaluations on queries within dialogue, where understanding prior context is essential for correct routing. This context applies to the MANtIS, SGD, and LMSYS datasets (Tables 6, 7, and 8). Dataset # Domains # Actions # Conversations # Spans # Turns Training CLINC150 MANtIS SGD LMSYS-1M 981 8 14 12 258 3 156 80 N/A 252 390 5 194 N/A N/A 2 500 123 N/A N/A N/A 2 000 662 43 442 1 000 1 000 5 141 Table 4: Dataset statistics"
        },
        {
            "title": "B Latency Analysis",
            "content": "The result clearly shows Arch-Routers main advantage: its incredible efficiency with near instance latency. Arch-Router achieves better accuracy as the best commercial model, Claude-sonnet-3.7 13 Model ST-fQfA ST-fQcA Irrelevance Qwen2.5-1.5B GPT-4o GPT-4o-mini Claude-Sonnet-3.7 Claude-Haiku-3.5 Gemini 2.0 Flash Gemini 2.0 Flash-Lite Arch-Router"
        },
        {
            "title": "Turn",
            "content": "67.80 97.20 94.80 95.10 91.00 91.00 94.80 97."
        },
        {
            "title": "Turn",
            "content": "50.30 94.80 93.50 96.20 95.80 95.80 94.10 94."
        },
        {
            "title": "Turn",
            "content": "9.40 96.20 95.40 95.60 87.20 87.20 85.00 95.20 Table 5: Single-turn routing accuracy - CLINC"
        },
        {
            "title": "Models",
            "content": "MT-fQcA Turn Qwen2.5-1.5B GPT-4o GPT-4o-mini Claude-Sonnet-3.7 Claude-Haiku-3.5 Gemini 2.0 Flash Gemini 2.0 Flash-Lite Arch-Router 40.40 88.40 83.50 91.30 87.50 83.90 82. 90.10 Table 6: Multi-turn, single route in conversation history - MANtIS (93.17% vs. 92.79%) while also has massive gap in latency. Arch-Router is over 28 times faster than its closest competitor. Arch-Router offers unique solution by delivering the accuracy of large, premium model with the latency of small, efficient one, making it ideally suited for industry applications. Latency Benchmark. We measured the end-to-end completion latency, representing the total time from request to full response for each model. The benchmark was performed on 2,000 routing prompts randomly selected from our training dataset. Commercial model latencies were measured via the OpenRouter API, while Arch-Router was benchmarked on its self-hosted AWS L40S instance to reflect its practical deployment speed. Disclaimer. It is important to note that the latency presented for commercial models are based on snapshots from the OpenRouter API at the time of our evaluation and are subject to change. The performance of these third-party services can vary based on server load, network conditions, and evolving pricing structures. However, while the exact figures may fluctuate, the fundamental conclusion of our analysis remains robust. The orders-of-magnitude difference in speed between compact 1.5B model like Arch-Router and large, general-purpose APIs is structural advantage. Case Study: Routing in Coding Session C.1 Conversation and Routes multi-turn coding interaction (11 user turns) was routed by Arch-Router using the prompt template in Table 3 and RouteLLM. Table 11 lists each user request, the route policy chosen by the router, and whether that choice matched the ground-truth intent. The route policies is shown in Table 14 Models MT-fQfA MT-fQcA Irrelevance"
        },
        {
            "title": "Turn",
            "content": "Span Conv."
        },
        {
            "title": "Turn",
            "content": "Span Conv."
        },
        {
            "title": "Turn",
            "content": "Qwen2.5-1.5B 63.07 34.95 34.20 30.67 22. 6.63 GPT-4o GPT-4o-mini Claude-Sonnet-3.7 Claude-Haiku-3.5 Gemini 2.0 Flash Gemini 2.0 Flash-Lite 99.81 99.81 99.41 98.59 97.53 93.60 92.90 92.20 97.40 95.40 91.20 86.30 92.25 92.25 93.40 91.25 91.00 88.40 95.63 83.09 95.63 92.52 90.96 89. 88.66 80.15 89.04 87.80 83.44 77.88 87.29 75.70 87.29 82.87 80.56 68.51 12.90 83.54 51.33 97.40 63.00 76.80 67."
        },
        {
            "title": "Span",
            "content": "8.40 84.50 46.94 96.40 62.10 76.50 62.30 Arch-Router 99.20 97.00 94. 96.26 91.68 90.05 95.40 95.20 Table 7: Multi-turn, multi-route in history - SGD Models MT-fQfA MT-fQcA Irrelevance"
        },
        {
            "title": "Turn",
            "content": "Span Conv."
        },
        {
            "title": "Turn",
            "content": "Span Conv. Qwen2.5-1.5B 24.36 18.12 4.80 GPT-4o GPT-4o-mini Claude-Sonnet-3.7 Claude-Haiku-3.5 Gemini 2.0 Flash Gemini 2.0 Flash-Lite 94.28 92.69 96.10 91.96 92.69 90.38 93.25 90.75 95.27 88.97 91.33 90.08 81.30 78.86 84.55 80.49 80.49 65.87 12.18 88.58 84.04 94.76 85.26 84.23 73.08 6. 0.80 87.13 80.97 91.99 80.82 83.13 61.93 77.24 73.17 84.55 77.24 75.61 57."
        },
        {
            "title": "Turn",
            "content": "9.13 99.08 95.89 98.63 91.32 91.32 71."
        },
        {
            "title": "Span",
            "content": "6.37 98.73 93.63 98.09 87.26 89.17 70.06 Arch-Router 97.93 96.83 86. 93.79 90.63 82.29 98.87 98.54 Table 8: Multi-turn, multi-route with diverse conversation - LMSYS 10 which provide policy name, description, and corresponding LLM. The model choice is purely based on each our preferences. C.2 Analysis Across the conversation Arch-Router perfectly routed 8 / 8 user turns. Its generative architecture digests the entire conversation plus the natural language descriptions, so it can interpret elliptical follow-ups. After producing code in Turn 2, it recognized this doesnt work, try again as bug_fixing requesteven though no keywords such as bug or fix appearedand later mapped run too slow to performance_optimization. Because new routes are just additional text in the routing policy, the same mechanism works if product team inserts, say, security_hardening or accessibility_audit without retraining. RouteLLMs policy is trained to predict answer quality scores from benchmarks, then pick weak or strong model accordingly. That shortcut works when each query is self-contained, but it breaks as soon as meaning depends on prior turns. In our conversation, RouteLLM downgraded three context-dependent requests (Turns 3, 5, 7) to cheap model, weaker model. The mis-routes illustrate structural limitation: it is uncertain whether follow-up will yield costly code generation, quick bug fix, or simply thank you response. 15 Model Latency (ms) Performance (%) GPT-4o GPT-4o-mini Claude-sonnet-3.7 Claude-haiku-3.5 Gemini-2.0-flash Gemini-2.0-flash-lite Arch-Router 836 239 737 164 1450 385 1249 352 581 101 510 82 51 12 89.74 82.79 92.79 84.96 85.63 76. 93.17 Table 9: Performance and Latency Analysis of Router Models. The table compares latency (average standard deviation in milliseconds) benchmarked from OpenRouter ), and overall routing performance. The cost for Arch-Router is estimated in hosting the model on AWS L40S instance. Route Policy Description code_generation code_explanation Generate new code snippets, functions, or boiler-plate from user requirements. Explain what piece of code does, including logic and edge cases. Identify and fix errors or bugs in user-supplied code. bug_fixing performance_optimization Suggest changes to make code faster, more scalable, or api_help programming_question default more readable. Assist with understanding or integrating external APIs and libraries. Answer general programming theory or best-practice questions. Table 10: Route policy for the coding application. LLM Claude-sonnet-3.7 GPT-4o GPT-4o GPT-4o GPT-4o-mini GPT-4o-mini Qwen2.5-4B Turn User request 1 2 3 4 6 7 8 Hi Write function to visualize an dataframe that has error column, the visualization the accuracy aggregation over all the rows This doesnt work Okay, now try the same thing for dataframe with label, output column, please check the correctness of the output column compared to the label, note that there is different incorrect types: relevance, irrelevance, and incorrect format The code runs too slow, any way can simplify it to make it faster? What are the functions that can be replaced from seaborn Any other ones? Thats all, thank you RouteLLM weak strong Arch-Router general code_generation weak strong bug_fixing code_generation weak strong weak weak performance_optimization api_help api_help general Table 11: Comparison of routing predictions from RouteLLM and Arch-Router for each user turn in coding conversation."
        }
    ],
    "affiliations": [
        "Katanemo Labs, Inc."
    ]
}