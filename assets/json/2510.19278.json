{
    "paper_title": "D2D: Detector-to-Differentiable Critic for Improved Numeracy in Text-to-Image Generation",
    "authors": [
        "Nobline Yoo",
        "Olga Russakovsky",
        "Ye Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-image (T2I) diffusion models have achieved strong performance in semantic alignment, yet they still struggle with generating the correct number of objects specified in prompts. Existing approaches typically incorporate auxiliary counting networks as external critics to enhance numeracy. However, since these critics must provide gradient guidance during generation, they are restricted to regression-based models that are inherently differentiable, thus excluding detector-based models with superior counting ability, whose count-via-enumeration nature is non-differentiable. To overcome this limitation, we propose Detector-to-Differentiable (D2D), a novel framework that transforms non-differentiable detection models into differentiable critics, thereby leveraging their superior counting ability to guide numeracy generation. Specifically, we design custom activation functions to convert detector logits into soft binary indicators, which are then used to optimize the noise prior at inference time with pre-trained T2I models. Our extensive experiments on SDXL-Turbo, SD-Turbo, and Pixart-DMD across four benchmarks of varying complexity (low-density, high-density, and multi-object scenarios) demonstrate consistent and substantial improvements in object counting accuracy (e.g., boosting up to 13.7% on D2D-Small, a 400-prompt, low-density benchmark), with minimal degradation in overall image quality and computational overhead."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 8 7 2 9 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "D2D: DETECTOR-TO-DIFFERENTIABLE CRITIC FOR IMPROVED NUMERACY IN TEXT-TO-IMAGE GENERATION Nobline Yoo1, Olga Russakovsky1, Ye Zhu 1,2 1Department of Computer Science, Princeton University 2LIX, École Polytechnique, IP Paris {nobliney,olgarus}@cs.princeton.edu,ye.zhu@polytechnique.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Text-to-image (T2I) diffusion models have achieved strong performance in semantic alignment, yet they still struggle with generating the correct number of objects specified in prompts. Existing approaches typically incorporate auxiliary counting networks as external critics to enhance numeracy. However, since these critics must provide gradient guidance during generation, they are restricted to regression-based models that are inherently differentiable, thus excluding detector-based models with superior counting ability, whose count-via-enumeration nature is non-differentiable. To overcome this limitation, we propose Detector-to-Differentiable (D2D), novel framework that transforms non-differentiable detection models into differentiable critics, thereby leveraging their superior counting ability to guide numeracy generation. Specifically, we design custom activation functions to convert detector logits into soft binary indicators, which are then used to optimize the noise prior at inference time with pre-trained T2I models. Our extensive experiments on SDXLTurbo, SD-Turbo, and Pixart-DMD across four benchmarks of varying complexity (low-density, high-density, and multi-object scenarios) demonstrate consistent and substantial improvements in object counting accuracy (e.g., boosting up to 13.7% on D2D-Small, 400-prompt, low-density benchmark), with minimal degradation in overall image quality and computational overhead."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion-based text-to-image generative models (Podell et al., 2024; Rombach et al., 2022; Sauer et al., 2025; Chen et al., 2024; 2025b) have achieved promising performance in semantic alignment between the synthesized images and text prompts, particularly with recent post-enhancement techniques such as fine-tuning (Clark et al., 2024; Chen et al., 2025a; Yang et al., 2024; Wallace et al., 2024; Black et al., 2024; Xu et al., 2023; Fan et al., 2023) or sampling-based, training-free strategies (Wallace et al., 2023; Eyring et al., 2024; Chung et al., 2024; Chefer et al., 2023). However, even with those advanced alignment techniques, T2I diffusion models continue to struggle at generating exact numbers of objects. As illustrated in Fig. 1, recent semantic alignment methods, such as ReNO (Eyring et al., 2024), which enhances generic image alignment with user intent via human preference rewards, shows limited ability to synthesize images with the exact number of objects specified in the text input. Motivated by this observation, we tackle the challenge of accurate numeracy generation in this work. Since vanilla T2I models are not explicitly trained to count, existing methods (Kang et al., 2025; Zafar et al., 2024) introduce auxiliary counting critics to provide additional supervision during generation. These correction signals are propagated to the generative backbone through gradients from the external critics, which restricts current approaches to differentiable, regression-based models such as RCC (Hobley & Prisacariu, 2022) and CLIP-Count (Jiang et al., 2023). However, this design inherently excludes detector-based models, which perform counting via bounding box enumeration. Despite being non-differentiable, such detectors (e.g., OWLv2 (Minderer et al., 2023), YOLOv9 Wang Work mainly completed when YZ was postdoc at Princeton University. 1 Figure 1: Qualitative examples illustrating the count-correction ability of our detector-based critic on variety of objects, counts 1-10. SDXL-Turbo (Sauer et al., 2025) is base model with no post-enhancement. ReNO (Eyring et al., 2024) is generic semantic alignment method that exhibits limited performance in this setting. More recent methods, like Make It Count (Binyamin et al., 2025) and Counting Guidance (Kang et al., 2025), explicitly address count-correction. Our method proposes new and effective way to leverage detectors for this challenging task. Prompt template: realistic photo of scene with [count] [object class]. et al. (2024)) often outperform regression-based counterparts (e.g., RCC (Hobley & Prisacariu, 2022), CLIP-Count (Jiang et al., 2023), CounTR (Chang et al., 2022)) in low-density object scenarios due to their more advanced object localization ability, as illustrated in Fig. 2b. To this end, we propose resolving this bottleneck by converting existing object detectors into differentiable critics, thereby allowing T2I diffusion models to benefit from stronger counting models for improved numeracy. Our Detector-to-Differentiable (D2D) framework builds on two key insights that set it apart from existing numeracy-enhancement methods (Kang et al., 2025; Zafar et al., 2024; Binyamin et al., 2025). First, rather than relying on the conventional non-differentiable count-via-enumeration mechanism, we design high-curvature activation function that converts bounding box logits outputted from detectors into soft binary indicators, thereby making them gradient-friendly for count optimization. Second, to leverage our count-via-summation gradient, unlike prior approaches that intervene at intermediate states or denoised predictions along the sampling trajectory, we instead optimize the initial noise using test-time tunable module, the Latent Modifier Network. This backbone-agnostic design enables broader generalization of our method across diverse diffusion-based T2I architectures, including U-Net (Ronneberger et al., 2015) and DiT (Peebles & Xie, 2023). We demonstrate the effectiveness of D2D via comprehensive experiments using various generative backbones (i.e., SDXL-Turbo (Sauer et al., 2025), SD-Turbo (Sauer et al., 2025), Pixart-DMD Chen et al. (2025b)) and multiple benchmarks (i.e., CoCoCount (Binyamin et al., 2025), D2D-Small, D2D-Multi, D2D-Large), covering diverse numeracy generation scenarios, including single and multiple objects. D2D yields the highest numeracy across all multi-step and one-step baselines and benchmarks. In particular, on base model SDXL-Turbo, D2D effectively corrects 42% of under-generations (i.e., where the initial generation contains fewer than requested objects) and 40% of over-generations, nearly or more than 2x ReNO (Eyring et al., 2024) and Token Optimization (TokenOpt)s (Zafar et al., 2024) correction rate. In summary, our contributions are as follows: We highlight the importance of accurate numeracy in T2I generation and propose framework to convert robust object detectors into differentiable critics for count-correction with newly designed activation function, addressing the bottleneck of having to rely on existing regression-based methods. 2 (a) Low and high-density examples with incorrect numeracy, generated by SDXL-Turbo. (b) Error by ground truth count on TallyQA (Acharya et al., 2019) and FSC147 (Ranjan et al., 2021). Figure 2: The low-density setting is where incorrect numeracy is most noticeable and also where detectors count better than regression-based methods. But detectors are not differentiable, which precludes them from being used as critics for count correction. We reposition the count-correction problem within the initial noise optimization framework, motivated by the presence of structural priors that exhibit cross-model consistency. Our method D2D outperforms previous one-step and multi-step count-correction methods by up to 13.7% points (from 30% with Make It Count to 43.7% with D2D on D2D-Small), with minimal degradation in image quality  (Fig. 1)  . On single-object prompts with counts 10, our method introduces less or comparable computational overhead than baselines."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Generic alignment-enhancement methods. As noted in the literature (Black et al., 2024; Chen et al., 2025a), the base log-likelihood objective of diffusion models is insufficient on its own to achieve state-of-the-art alignment. To address this, prior works optimize human preference scores via post-enhancement strategies ranging from fine-tuning the U-Net or text encoder (Clark et al., 2024; Xu et al., 2023; Yang et al., 2024; Wallace et al., 2024; Black et al., 2024; Fan et al., 2023; Chen et al., 2025a) to inference-time, training-free strategies that update the intermediate latents (Chung et al., 2024; Chefer et al., 2023). promising recent line of work (Wallace et al., 2023; Eyring et al., 2024) proposes inference-time alignment via initial noise selection, motivated by the presence of semantic/structural priors in the initial noise (Wang et al., 2025) that influence the semantics/structure of the generated output consistently across diffusion models even with different backbones. But regardless of whether the specific approach is to fine-tune model components or update latents, the problem remains that generic alignment objectives like human-preference scores are insufficient to solve numeracy, as we find there remains significant gap relative to state-of-the-art count-correction methods like Binyamin et al. (2025). In our work, we specifically address the challenge of improving numeracy with new formulation for the objective, as well as adopt initial noise optimization as the method of learning, for the ease with which it can be applied across different backbones and the ability to leverage optimized seeds to complement existing methods, as we demonstrate in experiments. Numeracy correction methods. Existing count-correction methods leverage two main mechanisms at inference-time to correct count: (1) apply the gradient of external counting models to correct tunable portion of the generation process, like Counting Guidance (Kang et al., 2025) and TokenOpt (Zafar et al., 2024), or (2) use attention to control the layout of generated instances, like Make It Count (Binyamin et al., 2025). Counting Guidance uses the RCC counting model (Hobley & Prisacariu, 2022) to optimize the predicted noises, and TokenOpt uses CLIP-Count (Jiang et al., 2023) to optimize the embedding of count token injected into the prompt as well as detector to scale down CLIP-Counts overestimates, which increases the computational overhead (about 2-6 times longer than D2D on average). Make It Count (Binyamin et al., 2025) is an SDXL-specific (Podell et al., 2024) method that uses self-attention features of the U-Net to extract masks of generated instances and cross-attention to enforce corrected set of masks. These works are either limited by 3 the need to rely on regression-based counters or manner in which they enforce structure at the cost of image quality, phenomenon documented in Dinh et al. (2023); Zafar et al. (2024); Patel & Serkh (2025) and noted in our experiments. Instead, D2D leverages more robust detector-based critic that enables more effective correction in the low-density setting. Regression vs. detector-based counting models. Regression-based counting methods take an input image and estimate count on continuous scale. Different variations allow for (1) exemplar-based (i.e., count the instances that look similar to the user-provided example), (2) zero-shot (i.e., count the most salient object), and (3) text-prompted counting (i.e., count the text-specified object). Designed to help count high-density images, where continuous-scale predictions are appropriate, they exhibit limited performance in low-density images (Zhang et al., 2025), as illustrated in Fig. 2b. On the other hand, our D2D critic is derived from detectors which show robust performance given low-density images, which is critical to the generative setting  (Fig. 2)  . Furthermore, our critic can be used to generate objects in the open set by leveraging open-vocabulary detectors, like OWLv2 (Minderer et al., 2023), with minimal modification to detector architecture. In our work, we compare our critic against three regression-based counting methods: RCC (Hobley & Prisacariu, 2022) (zero-shot), CLIP-Count (Jiang et al., 2023) (text-specified), and CounTR (zero-shot) (Chang et al., 2022)."
        },
        {
            "title": "3 THE D2D FRAMEWORK",
            "content": "Problem statement. Given pre-trained, one-step T2I model Gθ and prompt requesting counts of an object of class C, our goal is to generate an image with exactly counts of C. Summary of approach. We propose detector-based count critic that provides more effective gradient signal. We then design method to use that signal to influence the generation process, by leveraging the structural priors in the initial latent which we modify to align with the gradient. 3.1 DETECTOR-TO-DIFFERENTIABLE CRITIC Detector takes as inputs an object class and image and outputs set of bboxes {Bi1 n} and logits = {zi1 n}. standard sigmoid σ(zi) = 1 1+ezi converts the logits into confidence scores between 0 and 1, with the most salient bboxes filtered using threshold τ , as follows: = {Biσ(zi) τ } = {Bizi τz}, where τz = σ1(τ ). The final count is B. Our goal is to derive gradient from that can effectively increase or decrease as needed. Our approach is to first, define differentiable function : Rn (cid:55) that can extract the count from the logits z, and second, transform so its gradient is more amenable to convergence, arriving at critic LD2D. Extract the count via . Counting is discrete in nature, but we convert it into continuous, differentiable one by borrowing techniques from logistic regression for binary classification, which optimizes the steepness and transition threshold of sigmoid-curve for discrete 0/1 prediction. We convert each logit into an approximate binary indicator of whether to count the corresponding bbox, by applying to each logit steep sigmoid curve with transition threshold τz and steepness coefficient β, with the final differentiable count formulated as sum of sigmoids (Eq. 1). The next challenge is to make this gradient-friendly. Transform to effectively handle over/under-generation. An effective critic provides strong gradient signal above/below τz (our domain of interest) to push logits below or beyond the threshold as needed to erase/add objects in response to over/under-generation. However, by nature of its sigmoidal shape, has significant plateauing (i.e., weak gradient signals) above and below τ . To improve the gradient steepness in our domain of interest, we scale each sigmoid output by the corresponding logit (Eq. 2), arriving at LD2D. At inference-time, we use LD2D to optimize the generated image. fβ,τz (z) = (cid:88) i=1 σ(β (zi τz)). LD2D = (cid:26)(cid:80)n (cid:80)n i=1 σ(β (zi τz)) (zi τz), i=1 σ(β (τz zi)) (τz zi), if fβ,τz > (i.e., over-generation) if fβ,τz < (i.e., under-generation) 1Unless otherwise noted, we use to perform early-stopping once the requested count is met. (1) (2) Figure 3: The D2D pipeline for improving T2I numeracy. D2D consists of two main components that work together to improve numeracy: our detector-based count critic guides the Latent Modifier Network (LMN) on how to transform the original initial noise xT into more optimal . Our count critic uses sigmoid-based activation functions to convert logits into gradient signals, which are then backpropagated through the frozen generator to update the weights of the LMN. Extension to multiple classes. The main consideration in extending D2D to prompts with > 1 object classes {Cj1 m}, is that every bbox comes with logits, the maximum of which determines its class label. To extend D2D, we update Eq. 2 to correct each bboxs largest logit, while minimizing all others. Details in Appendix E. 3.2 THE LATENT MODIFIER NETWORK (LMN) Given our proposed count critic, we now turn to the learning method used to optimize this objective. Motivated by the presence of meaningful priors in the initial noise, previous works (Eyring et al., 2024; Wang et al., 2025) have used various generic alignment metrics to tune the initial noise directly. Building on this motivation, we propose the Latent Modifier Network (LMN), test-time tunable module whose output is mixed with the original noise to determine the optimal initial noise and whose weights are updated using our critic LD2D. Given initial noise xT (0, I), xT Rd and prompt that requests counts of an object of class C, one-step T2I model Gθ generates image I. Our goal is to find an optimal that produces an image with exactly of the specified object. To achieve this, we introduce tunable Latent Modifier Network (LMN) Mϕ: small, 3-layer perceptron, between the initial random latent and T2I model  (Fig. 3)  , with input/output dimensions equal to that of the initial latent and whose output dictates how to update xT . As shown in Eq. 3, the new latent is weighted sum of xT and Mϕ(xT ), with mixing weight = 0.2. Compared to tuning the initial latent directly, the LMN composes relatively larger parameter space and enforces more incremental updates that preserve portion of the original latent even through all iterations. At inference-time, we tune ϕ using LD2D with the goal of correcting the initial noise, and thereby the numeracy, as described in the following section. = xT + (1 w) Mϕ(xT ). (3) 3.3 OPTIMIZATION The goal is to find the optimal set of parameters ϕ that minimizes the error between the generated and requested count, as seen in Eq. 4. Since detector is non-differentiable, we leverage LD2D to optimize ϕ iteratively, rendering our final update rule (Eq. 5), with regularization term Lreg, learning rate η, and weights α and λ. We adaptively rescale the loss to address exploding gradients that we may encounter due to the large number of tunable parameters. During numeracy optimization, we apply variant of the regularization term used in ReNO (Eyring et al., 2024), using the negative log-likelihood of the norm of xT as follows: ). We use Lreg = (aL 2/2 (d 1) log(x reg = reg + c)10, with scaling coefficient and shift constant c. ϕ = arg min D(Gθ(x )) . ϕ (4) (5) ϕ ϕ η(αLD2D + λLreg). 5 ϕ initialization. To give Mϕ good starting point (i.e., initialize the networks initial output distribution to Gaussian), we propose short, pre-inference alignment stage to be performed one time per base model using only the regularization term. Specifically, we train Mϕ on 100 different randomly sampled latents (xT ) for 200 epochs each (Algorithm 1 in the appendix). At inference-time, given new, randomly sampled xT the network has never seen before, we introduce 0.2-second calibration phase to allow the network to adapt to the new input, using only the regularization term. Afterward, we leverage both D2D and regularization terms, according to Eq. 5. The full algorithm is detailed in Algorithm 2 in the appendix."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "Benchmarks. Our main experimental setting of single-object, low-density prompts leverages two benchmarks, CoCoCount (Binyamin et al., 2025) and D2D-Small. D2D-Small is set of 400 prompts created using 40 countable objects from COCO (Lin et al., 2014) with counts ranging from 1-10 and prompt template adapted from Lian et al. (2024): realistic photo of scene with [count] [object]. CoCoCount consists of 200 prompts from 20 COCO classes and requested counts roughly equally split among 2, 3, 4, 5, 7, and 10. Experiments on multi-object or high-density prompts were performed on D2D-Multi (400 prompts with two objects sampled from 40 countable COCO classes, with N1, N2 < 10, and following the template: realistic photo of scene with [count] [object] and [count] [object]) and D2D-Large (similarly constructed with counts 11-20). Base models. We apply D2D to three one-step models: SDXL-Turbo (Sauer et al., 2025), SDTurbo (Sauer et al., 2025), and Pixart-DMD (Chen et al., 2025b). SDXL-Turbo and SD-Turbo, respectively distilled from SDXL (Podell et al., 2024) and SD2.1 (Rombach et al., 2022), have U-Net backbones. Pixart-DMD, distilled from Pixart-α (Chen et al., 2024), has Transformer backbone. Comparison of numeracy enhancement methods. We compare D2D against count-correction baselines (1) Make It Count (Binyamin et al., 2025) (multi-step), which uses attention-based mechanisms to identify and correct object layout via updates to the intermediate latents, (2) Counting Guidance (Kang et al., 2025) (multi-step), which uses the auxiliary counting network RCC (Hobley & Prisacariu, 2022) to correct the predicted noises, and (3) TokenOpt (Zafar et al., 2024), one-step method which injects count token into the prompt and tunes it using CLIP-Count (Jiang et al., 2023). We run each baseline following its original experiment setup. Importantly, Make It Count is an SDXL-based method; Counting Guidance (original experiments and codebase) is primarily centered on SD1.4; and TokenOpt is built on SDXL-Turbo, so we evaluate Make It Count on SDXL, Counting Guidance on SD1.4, and TokenOpt on SDXL-Turbo. Furthermore, Make It Count addresses the lowdensity, single-object setting and TokenOpt addresses the single-object setting, so we only evaluate Make It Count on CoCoCount and D2D-Small and TokenOpt on CoCoCount and D2D-Small/Large. Comparison with generic prompt-alignment method. The most relevant prior initial noise optimization work is ReNO (Eyring et al., 2024), framework for one-step T2I models that uses the combined gradient of multiple image quality and prompt-image alignment metrics (ImageReward (Xu et al., 2023), PickScore (Kirstain et al., 2023), HPSv2 (Wu et al., 2023), and CLIPScore (Hessel et al., 2021)) to optimize semantic alignment and image quality. Instead of tuning an LMN, ReNO directly tunes the initial latent over 20-50 iterations, with regularization to keep the noise within the initial distribution and gradient clipping to prevent gradient explosion. Though its use of human-preference reward models does improve numeracy relative to the base model, there remains gap between using such generic objectives and our count-tailored critic (Tab. 1). key difference between our method and ReNOs is our introduction of the LMN, which expands the tunable parameter space while preserving portion of the original initial noise throughout the optimization process. To assess the impact of introducing the LMN, we compare our initial noise optimization method with ReNOs, controlling for the loss by swapping out ReNOs human-preference models for our D2D critic. Count critic. We demonstrate D2D on detectors OWLv2 (Minderer et al., 2023) (open-vocabulary, robust) and YOLOv9 (Wang et al., 2024) (high-throughput and trained on COCO (Lin et al., 2014) objects). We expect small accuracy-cost tradeoff, where OWLv2 enables superior numeracy with greater computational overhead, while YOLOv9 yields slightly lower numeracy but faster inference. Table 1: Quantitative results. D2D outperforms all baselines across all four benchmarks (CoCoCount (Binyamin et al., 2025) and D2D-Small/Multi/Large), even generalizing across detector variants OWLv2 (Minderer et al., 2023) and YOLOv9 (Wang et al., 2024). D2D with YOLOv9 on base model SDXL-Turbo is in bold italics to show that while it outperforms all baselines, it is second to using OWLv2. The higher-performing OWLv2 detector is used in all subsequent experiments on SD-Turbo and Pixart-DMD. Standard deviations indicate the significance of our findings. Base models with no post-enhancement highlighted in gray. Avg. over four seeds. Method SDXL (Podell et al., 2024) + Make It Count (Binyamin et al., 2025) SDXL-Turbo (Sauer et al., 2025) + ReNO (Eyring et al., 2024) + TokenOpt (Zafar et al., 2024) + D2D w/ YOLOv9 (Ours) + D2D w/ OWLv2 (Ours) SD2.1 (Rombach et al., 2022) SD1.4 (Rombach et al., 2022) + Counting Guidance (Kang et al., 2025) SD-Turbo (Rombach et al., 2022) + ReNO (Eyring et al., 2024) + D2D w/ OWLv2 (Ours) Pixart-α (Rombach et al., 2022) Pixart-DMD (Chen et al., 2025b) + ReNO (Eyring et al., 2024) + D2D w/ OWLv2 (Ours) CoCoCount D2D-Small D2D-Multi 2.44 0.59 16.06 1.86 24.88 1.70 30.00 1.93 46.75 2.10 2.12 0.83 20.31 1.95 27.38 2.69 5.31 0.38 27.50 0.68 41.88 1.03 23.31 1.66 35.12 0.75 6.25 1.77 36.69 2.40 52.75 1.55 9.81 0.97 43.69 2.36 55.62 2.72 4.81 1.23 24.75 2.85 32.75 1.32 2.81 0.31 16.69 2.59 27.62 4.11 3.38 1.16 17.12 1.69 28.38 1.11 2.56 0.83 15.31 0.87 20.88 3.07 8.94 1.76 32.06 0.99 43.38 3.47 10.75 1.06 39.44 2.37 48.38 3.09 1.31 0.75 14.00 1.08 19.62 1.03 6.25 0.46 27.88 1.51 38.12 2.32 9.44 0.75 37.25 1.70 44.75 1.44 13.31 1.36 41.25 2.81 53.25 2.40 D2D-Large 1.44 0.38 2.56 0.55 4.69 1.25 3.94 0.72 7.50 1.06 9.94 1.57 2.94 0.75 2.12 0.32 1.88 0.60 3.19 1.18 4.25 1.14 11.44 1.98 1.81 0.66 3.19 0.62 4.75 0.74 7.62 1.18 Evaluation. Following similar evaluation protocols (Binyamin et al., 2025; Kang et al., 2025; Zafar et al., 2024), we use CountGD (Amini-Naieni et al., 2024), state-of-the-art counting model built on detector GroundingDINO (Liu et al., 2025), to extract the count of generated objects and compute the proportion of correctly-generated images (see Appendix for CountGDs counting accuracy compared to other regression/detector-based methods). Like Eyring et al. (2024), we analyze imagequality/prompt alignment with human-preference-trained models (ImageReward (Xu et al., 2023), PickScore (Kirstain et al., 2023), HPSv2 (Wu et al., 2023)), and CLIPScore (Hessel et al., 2021). Implementation details. Our main experiments were completed on an L40. For Make It Count (Binyamin et al., 2025) which requires > 50 GB, we used an A100. Additional details in Appendix D. 4.2 NUMERACY IMPROVEMENTS Tab. 1 shows our main D2D-to-baseline comparisons. Baseline evaluations illustrate that though the prompt setting is relatively simple, generating accurate counts remains challenging. On numeracy, D2D consistently outperforms baselines across low-density, single-object, multi-object, and high-density prompts, across base models with U-Net and DiT backbones. On SDXL-Turbo, we demonstrate that performance boosts from D2D generalize across OWLv2 and YOLOv9 detector backbones (i.e., the detector used to compute LD2D), with small accuracy-cost tradeoff as expected (Fig. 6b). The robust OWLv2 detector yields higher numeracy with slightly more overhead, while the real-time YOLOv9 detector yields slightly lower (but still high) numeracy with faster inference (in all other experiments, we use the higher-performing OWLv2 backbone unless otherwise noted). Additionally, D2D effectively complements baselines, boosting numeracy across all four benchmarks when used in combination with TokenOpt or ReNO (Tab. 6 in appendix). For example, applying D2D-optimized seeds to TokenOpt improves numeracy by 13.63% points, relative to TokenOpts baseline performance (from 35.12% to 48.75%) on CoCoCount. Improved numeracy on multi-object/high-density prompts. D2D maintains relative improvement over baselines even in the more challenging multi-object/high-density settings. Nevertheless, the 7 Figure 4: D2D improves numeracy on the majority of the 41 objects in CoCoCount and D2DSmall. Evaluated against ReNO (Eyring et al., 2024) and TokenOpt (Zafar et al., 2024) on base SDXL-Turbo. Avg. over four seeds. Figure 5: D2D effectively corrects over and under-generation. The initial generation contains six more dogs/one fewer cup than requested, which our method iteratively corrects, arriving at an image of 10 dogs/four cups, as requested. Table 2: Given the same initial conditions, D2D is effective at correcting both over and undergeneration. We report the correction rate of initial over/under-generations, as well as the proportion of correct generations that were maintained. On SDXL-Turbo, across CoCoCount and D2DSmall benchmarks. Avg. over four seeds. Table 3: Ablation study on key hyperparameters τ and β. Detector threshold τ = 0.2 is optimal. lower τ (which counts low-confidence bboxes) and higher τ (which potentially discards actually-legitimate bboxes) results in drops in numeracy. Steepness coefficient β = 300 is optimal. Tested on CoCoCount, seed=0. Numeracy of initial generation TokenOpt (Zafar et al., 2024) ReNO (Eyring et al., 2024) D2D w/ OWLv2 Over 13.28 23.32 40.13 Under Correct 25.24 25.11 41.83 69.92 62.19 72.57 Hyperparameters τ β 0. 0.2 0.5 0.8 1 10 300 400 CountGD 51.50 55.50 43.50 32.50 43.00 40.00 52.00 55.50 52.50 accuracy drop from low-density benchmarks to D2D-Large illustrates the remaining challenge of correctly generating large counts (e.g., base SDXL-Turbo: from 43.69% on D2D-Small to 9.94% on D2D-Large). Unsurprisingly, upon parsing D2D-Multi results, we see this holds within multi-object prompts as well (Tab. 7 in appendix). For example, the accuracy of SDXL-Turbo + D2D w/ OWLv2 on D2D-Multi prompts with low total-density (Ntot = N1 + N2 10) is 12.08%, which drops to 3.00% for prompts with higher Ntot (though both are still higher than all baseline scores). LD2D effectively boosts numeracy across all classes. Fig. 4 shows LD2D improves numeracy across all 41 object categories in CoCoCount and D2D-Small, spanning large variety (e.g., apples, elephants, cars, etc.) Upon applying D2D to SDXL-Turbo, umbrella and vase are the two classes that see the most improvement, each jumping from 2.50% (SDXL-Turbo base) to 52.50% (D2D) accuracy. Wine glass and bottle, both of which are (semi)transparent objects, are among the classes that see the least improvement (47.50% to 52.50% and 25.00% to 30.00% accuracy, respectively), which may suggest future direction where detectors are fine-tuned on more difficult classes, or similar, with the purpose of generating highly-tailored scenes of objects. D2D best handles over and under-generation. Tab. 2 breaks down results by the numeracy of the initial generation I, illustrating how well different methods are able to correct over/undergeneration while maintaining the numeracy of already-correct images. Specifically, we compare TokenOpt, ReNO, and D2D on base model SDXL-Turbo, across benchmarks CoCoCount and D2DSmall. D2D has the highest correction rate, correcting 40.13% of over-generations and 41.83% of under-generations, which is at least 16% points over the baselines, while maintaining 72.57% of already-correct generations, which is also more than both TokenOpt and ReNO. Fig. 5 illustrates D2Ds iterative correction process on two sample prompts, going from 16 dogs to the requested 10 dogs and from three cups to the requested four. Additional qualitative examples in Appendix J. 4.3 ADDITIONAL ANALYSIS AND ABLATIONS Impact of hyperparameters. We report our key hyperparameter studies on values for τ (detector threshold) and β (steepness coefficient). Studies of hyperparameters were conducted using base model SDXL-Turbo on benchmark CoCoCount (seed=0) on an A6000/L40. Results (Tab. 3) show that τ = 0.2, β = 300 are optimal; we use these values in all other experiments. 8 Table 4: Among count critics, LD2D is the most effective. On SDXL-Turbo. Avg. over four seeds. Count Critic RCC (Hobley & Prisacariu, 2022) CounTR (Chang et al., 2022) CLIP-Count (Jiang et al., 2023) (OWLv2) LD2D (OWLv2) CoCoCount 37.75 38.38 40.00 32.00 55.62 D2D-Small 26.38 25.62 25.88 20.75 43.69 D2D-Multi 05.19 03.06 09.81 D2D-Large 04.25 05.31 06.38 03.38 09. Table 5: The LMN boosts numeracy. We compare D2D against ReNO (Eyring et al., 2024) using LD2D and reg for both, controlling for the number of iterations tuned. We note boosts in numeracy, with comparable image quality. On SDXL-Turbo. Avg. over four seeds. Method CountGD ImageReward PickScore HPSv2 CLIPScore CoCoCount D2D-Small CoCoCount D2D-Small CoCoCount D2D-Small CoCoCount D2D-Small CoCoCount D2D-Small ReNO w/ LD2D, D2D w/ LD2D, reg reg 43.25 53. 32.00 42.44 1.04 1.08 0.45 0.52 23.25 23.28 21.98 21.99 0.296 0. 0.281 0.282 32.81 32.77 31.79 31.71 (a) Image quality/alignment scores (ImageReward (Xu et al., 2023), PickScore (Kirstain et al., 2023), HPSv2 (Wu et al., 2023), CLIPScore (Hessel et al., 2021)) by method. Aside from ReNO (Eyring et al., 2024), which often scores highest (it specifically optimizes those metrics), D2D is comparable to counting baselines. Min-max normalized. (b) Numeracy vs. inference cost by method. Across base models (SDXL-Turbo, SD-Turbo, Pixart-DMD) and detectors (OWLv2 (Minderer et al., 2023), YOLOv9 (Wang et al., 2024)), D2D scores in the top left (i.e. it is both high-numeracy and low-cost). D2D w/ YOLOv9 is even more compute-efficient than w/ OWLv2. Base model/detector noted in gray. Figure 6: D2D yields image quality/alignment comparable to counting baselines, with minimal addition to computational overhead. Comparisons against counting baselines (Make It Count (Binyamin et al., 2025), Counting Guidance (Kang et al., 2025), TokenOpt (Zafar et al., 2024)) and generic alignment method ReNO. On CoCoCount and D2D-Small. Avg. over four seeds. D2D vs. regression-based counters. Tab. 4 compares the effectiveness of our critic against existing regression-based ones and additionally shows that the formulation LD2D is indeed more convergence-friendly than fβ,τz . Across all four benchmarks, our detector-based critic outperforms regression-based methods RCC, CLIP-Count, and CounTR on numeracy (e.g., ours reaches 55.62% when the max score reached by any regression-based model is 40% on CoCoCount). Notably, LD2D outperforms the others even on the high-density benchmark D2D-Large, though regression-based methods outperform detectors in the non-generative, counting setting (Fig. 2b). Furthermore, not only does LD2D, which produces stronger gradient signal, outperform fβ,τz on numeracy; fβ,τz yields the lowest numeracy, which indicates that though it composes the mathematical backbone of LD2D, fβ,τz itself is not suitable critic, as expected (Tab. 4). The Latent Modifier Network Mϕ. Next, we assess the impact of introducing the LMN, module whose output is mixed with the original noise to arrive at the optimal noise, by comparing our method with ReNOs, controlling for the optimization objectives used (LD2D, reg) and number of iterations tuned. Tab. 5 shows the LMN generally improves numeracy while maintaining image quality; numeracy jumps 10% points on CoCoCount and D2D-Small from 43.25% to 53.88% and from 32.00% to 42.44%, respectively. Fig. 13 in the appendix, which breaks down the numeracy by the absolute error between the requested and generated count in the initial generation, shows the LMN boosts the correction rate across initial generations. 9 Impact on image quality and computational overhead. ImageReward, PickScore, HPSv2, and CLIPScore metrics in Fig. 6a show D2Ds image quality and overall prompt alignment is comparable to counting baselines and even surpasses multi-step baselines in many cases, including the layout control-based method, Make It Count (MIC). For example, SDXL-Turbo + D2D (OWLv2) yields ImageReward 0.51 (MIC: 0.30), PickScore 21.98 (MIC: 21.48), and HPSv2 0.28 (MIC: 0.26) on D2D-Small. D2D does not add significantly to inference cost, averaging between 11 and 21 seconds, compared to counting baselines, which average upwards of 28 to 100 seconds (Fig. 6b)."
        },
        {
            "title": "5 CONCLUSION AND DISCUSSION\nIn this work, we address the challenge of correcting numeracy in generation. We identify a central\nlimitation of previous methods, specifically their reliance on differentiable, regression-based counting\nmodels as critics. We propose D2D, novel way to convert more robust detectors into differentiable\ncount critics, which we then use to optimize the initial noise at inference-time to improve numeracy.\nOur method yields the highest numeracy across prompt scenarios, including low-density, single-\nobject, multi-object, high-density settings, effectively correcting both over and under-generation,\nwith minimal additions to temporal overhead and minimal degradation in image quality.",
            "content": "Limitation and future directions. While our method exhibits significant improvements in numeracy, high-density scenarios remain challenging. Given regression-based methods are more appropriate in this setting, future direction may explore how to adapt them into the generative setting. D2D is not layout-control approach and so is limited in more fine-grained control (e.g., object placement). But future directions may explore using D2D to perform more complex tasks, such as object positioning and attribute binding, by leveraging detectors that can robustly work with prompts specifying objects and associated attributes. REPRODUCIBILITY STATEMENT The paper and appendix, along with code which we will release, contain the details for reproducibility. ACKNOWLEDGMENTS This research was partially supported by an Amazon Research Award. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of Amazon. We are also grateful to Princeton Research Computing for compute resources."
        },
        {
            "title": "REFERENCES",
            "content": "Manoj Acharya, Kushal Kafle, and Christopher Kanan. Tallyqa: Answering complex counting questions. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):80768084, Jul. 2019. doi: 10.1609/aaai.v33i01.33018076. Niki Amini-Naieni, Tengda Han, and Andrew Zisserman. Countgd: Multi-modal open-world counting. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 4881048837. Curran Associates, Inc., 2024. Lital Binyamin, Yoad Tewel, Hilit Segev, Eran Hirsch, Royi Rassin, and Gal Chechik. Make it count: Text-to-image generation with an accurate number of objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1324213251, June 2025. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024. Liu Chang, Zhong Yujie, Zisserman Andrew, and Xie Weidi. Countr: Transformer-based generalised visual counting. In British Machine Vision Conference (BMVC), 2022. Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Trans. Graph., 42(4), July 2023. ISSN 0730-0301. doi: 10.1145/3592116. 10 Chaofeng Chen, Annan Wang, Haoning Wu, Liang Liao, Wenxiu Sun, Qiong Yan, and Weisi Lin. Enhancing diffusion models with text-encoder reinforcement learning. In Aleš Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gül Varol (eds.), Computer Vision ECCV 2024, pp. 182198, Cham, 2025a. Springer Nature Switzerland. ISBN 978-3-031-72698-9. Junsong Chen, Jincheng YU, Chongjian GE, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for In The Twelfth International Conference on Learning photorealistic text-to-image synthesis. Representations, 2024. Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-Σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In Aleš Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gül Varol (eds.), Computer Vision ECCV 2024, pp. 7491, Cham, 2025b. Springer Nature Switzerland. ISBN 978-3-031-73411-3. Hyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin Nam, and Jong Chul Ye. Cfg++: Manifoldconstrained classifier free guidance for diffusion models. CoRR, abs/2406.08070, 2024. Kevin Clark, Paul Vicol, Kevin Swersky, and David J. Fleet. Directly fine-tuning diffusion models on differentiable rewards. In The Twelfth International Conference on Learning Representations, 2024. Anh-Dung Dinh, Daochang Liu, and Chang Xu. Rethinking conditional diffusion sampling with progressive guidance. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 4228542297. Curran Associates, Inc., 2023. Luca Eyring, Shyamgopal Karthik, Karsten Roth, Alexey Dosovitskiy, and Zeynep Akata. Reno: Enhancing one-step text-to-image models through reward-based noise optimization. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 125487125519. Curran Associates, Inc., 2024. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 7985879885. Curran Associates, Inc., 2023. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: reference-free evaluation metric for image captioning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 75147528, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. emnlp-main.595. Michael Hobley and Victor Prisacariu. Learning to count anything: Reference-less class-agnostic counting with weak supervision. arXiv preprint arXiv:2205.10203, 2022. Ruixiang Jiang, Lingbo Liu, and Changwen Chen. Clip-count: Towards text-guided zero-shot object counting. In Proceedings of the 31st ACM International Conference on Multimedia, MM 23, pp. 45354545, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701085. doi: 10.1145/3581783.3611789. Wonjun Kang, Kevin Galim, Hyung Il Koo, and Nam Ik Cho. Counting guidance for high fidelity text-to-image synthesis. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pp. 899908, 2025. doi: 10.1109/WACV61041.2025.00097. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Picka-pic: An open dataset of user preferences for text-to-image generation. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 3665236663. Curran Associates, Inc., 2023. Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. LLM-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. Featured Certification. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), Computer Vision ECCV 2014, pp. 740755, Cham, 2014. Springer International Publishing. ISBN 978-3-319-10602-1. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and Lei Zhang. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In Computer Vision ECCV 2024, pp. 3855, 2025. Matthias Minderer, Alexey Gritsenko, and Neil Houlsby. Scaling open-vocabulary object detection. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 7298373007. Curran Associates, Inc., 2023. Zakaria Patel and Kirill Serkh. Enhancing image layout control with loss-guided diffusion models. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pp. 39163924, 2025. doi: 10.1109/WACV61041.2025.00385. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 41954205, October 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. Viresh Ranjan, Udbhav Sharma, Thu Nguyen, and Minh Hoai. Learning to count everything. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 33943403, June 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1068410695, June 2022. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F. Frangi (eds.), Medical Image Computing and Computer-Assisted Intervention MICCAI 2015, pp. 234241, Cham, 2015. Springer International Publishing. ISBN 978-3-319-24574-4. Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In Aleš Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gül Varol (eds.), Computer Vision ECCV 2024, pp. 87103, Cham, 2025. Springer Nature Switzerland. ISBN 978-3-031-73016-0. Bram Wallace, Akash Gokul, Stefano Ermon, and Nikhil Naik. End-to-end diffusion latent optimization improves classifier guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 72807290, October 2023. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 82288238, June 2024. Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. Yolov9: Learning what you want to learn using programmable gradient information. In Computer Vision ECCV 2024: 18th European Conference, Milan, Italy, September 29October 4, 2024, Proceedings, Part XXXI, pp. 121, Berlin, Heidelberg, 2024. Springer-Verlag. ISBN 978-3-031-72750-4. doi: 10.1007/978-3-031-72751-1_ 1. Ruoyu Wang, Huayang Huang, Ye Zhu, Olga Russakovsky, and Yu Wu. The silent assistant: Noisequery as implicit guidance for goal-driven image generation. In ICCV, 2025. 12 Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 1590315935. Curran Associates, Inc., 2023. Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 89418951, June 2024. Oz Zafar, Lior Wolf, and Idan Schwartz. Iterative object count optimization for text-to-image diffusion models. arXiv preprint arXiv:2408.11721, 2024. Ruisu Zhang, Yicong Chen, and Kangwook Lee. Improving CLIP counting accuracy via parameterefficient fine-tuning. Transactions on Machine Learning Research, 2025. ISSN 2835-8856."
        },
        {
            "title": "APPENDIX",
            "content": "In Appendix A, we discuss the broader impacts of our work in downstream uses. In Appendix B, we note our use of LLMs as assistant. Appendix details the algorithms for pre-inference alignment of the Latent Modifier Network (LMN) and inference-time, numeracy optimization algorithms. Appendix details hyperparameters used, including for the LMN architecture (Appendix D.1), core D2D formulation (Appendix D.2), and optimization (Appendix D.3). Appendix discusses the implementation details of extending D2D to multi-object scenarios. Appendix shows the performance improvements that result from using D2D to complement existing baselines. Appendix shows breakdown of numeracy by total density on D2D-Multi. Appendix shows CountGDs baseline counting performance, relative to other regression/detector-based counters. Appendix compares D2Ds image quality to base model SDXL-Turbos. Additional qualitative results are illustrated in Appendix J. Appendix includes results from additional ablation experiments, including studies on the value of mixing weight used to mix the LMN output with the original noise (Appendix K.1); numeracy breakdown by requested count using LD2D vs. regression-based count critics (Appendix K.2); correction rate by numeracy of initial generation with the introduction of the LMN (Appendix K.3); the necessity of inference-time calibration of the LMN (Appendix K.4), and the regularization formulation used (Appendix K.5)."
        },
        {
            "title": "A BROADER IMPACTS",
            "content": "As text-to-image pipeline, our method has many practical downstream uses, so it is necessary to exercise caution in deployment. Our method offers the advantage of stronger numeracy, which may be desirable in applications where users need to generate specific counts of objects. Our model may inherit biases of pre-trained diffusion base models and detectors. We suggest using strict NSFW filters and building more robust detectors. USE OF LARGE LANGUAGE MODELS (LLMS) We used LLMs to help with word choice for clarity and debugging."
        },
        {
            "title": "C ALGORITHMS",
            "content": "Algorithm 1 Pre-inference alignment stage (done once per model) Input: Latent Modifier Network Mϕ, initialized with random weights; latent dimension d; weight w; learning rate η, loss weight λ. Output: Pre-trained Mϕ with output aligned to Gaussian distribution. Set seed = 1. repeat 100 times Sample xT Rd (0, I) for 1 epoch 200 do = xT + (1 w) Mϕ(xT ) Compute = λL reg ϕ ϕ ηL end for Algorithm 2 Inference Input: Prompt specifying of object of class C; pre-trained Latent Modifier Network Mϕ; latent dimension d; weight w, diffusion model Gθ; minimum number of calibration iterations tmin; threshold value specifying good enough regularization τreg; counter fβ,τz and critic LD2D; Stage 1 (Calibration) learning rate ηcalib and loss weight λcalib; Stage 2 numeracy optimization learning rate η and loss weights α and λ; number of tuning steps K. Stage 1: Calibrate Mϕ to newly sampled xT . Done aligning in iterations. Stage 2: Optimize numeracy. if is optimal, stop early Output: Optimal noise . resample True while resample do Sample xT Rd (0, I) for 1 do = xT + (1 w) Mϕ(xT ) Compute = λcalibL if tmin and <= τreg then reg resample False break else ϕ ϕ ηcalibL end if end for end while for epoch do Compute Lreg = Gθ(x , p) Compute fβ,τz and LD2D return if fβ,τz = ϕ ϕ η(α LD2D + λ Lreg) = xT + (1 w) Mϕ(xT ) end for"
        },
        {
            "title": "D HYPERPARAMETERS",
            "content": "D.1 ARCHITECTURE OF THE LATENT MODIFIER NETWORK The LMN is three-layer perceptron, with input/output layers of size (dimension of initial latent, xT Rd), and two hidden layers of size 100 each. On base models SDXL-Turbo, SD-Turbo, and Pixart-DMD, the LMN has 3,303,384 tunable parameters. D.2 CORE D2D HYPERPARAMETERS The core D2D hyperparameters are the detector threshold τ and steepness coefficient β, which determine the transition threshold and curvature of the sigmoid underlying LD2D, and the mixing weight w, which determines the ratio with which to combine the LMN output and original noise to obtain the optimal noise. We use τ = 0.2, β = 300, and = 0.2. Studies on hyperparameters τ and β are reported in Tab. 3 in Sec. 4.3 of the main text. Experiments on are reported in Tab. 9 in Appendix K.1. D.3 OPTIMIZATION During pre-inference alignment and inference-time calibration, when we optimize only the regularization term, we use reg. At inference-time, during numeracy optimization, we use Lreg in conjunction with LD2D. Appendix K.5 reports ablation studies on using reg vs. Lreg during numeracy optimization. In the pre-inference alignment stage, we use learning rate η = 104 and loss weight λ = 0.01. At inference-time (stage 1: calibration), we use minimum number of iterations tmin = 70; good enough threshold for determining when calibration is done τreg = 712.8, learning rate ηcalib = 103, and loss weight λcalib = 0.01. During stage 2 (numeracy optimization), we use learning rate η = 5 104 and loss weights α = 5 and λ = 104. During this stage, we use loss-based, adaptive learning rate scheduling of η to ease steady convergence when the generated count approximates the requested count; we also adaptively rescale (i.e., increase) λ to counteract larger deviations from Gaussian (i.e., divergence). For D2D experimental results with Pixart-DMD on benchmark D2D-Large, we use the base detector OWLv2, instead of fβ,τx , to perform early-stopping. For single-object scenarios, we set = 200, except for D2D with Pixart-DMD on D2D-Large, for which we use = 400. For the multi-object scenario, we use = 400. MULTI-OBJECT LD2D In extending D2D to prompts with > 1 object classes {Cj, 1 m}, we have to note that , z(2) every predicted bbox Bi now comes with corresponding scores z(1) , the max of which, zmax , indicates its corresponding label. Only the bboxes where zmax τz are counted. To correct over/under-generation, our approach then is to focus on appropriately adjusting the largest score zmax of each bbox, while minimizing all other (i.e. non-max) scores. This means minimizing the zmax of those that correspond to under/correctly generated classes (to increase/maintain the count). We use roughly the same formulation as Eq. 2 in the main text, with zi replaced by the specific per-class, per-bbox logits zj that need to be minimized or maximized (Eq. 6). If is the set of classes that are under-generated or correctly generated, then of bboxes that correspond to over-generated classes and maximizing the zmax , ..., z(m) i Sbbox = {(i, j)1 = (arg max zj ) C} refers to the set of the bboxes corresponding to the under/correctly generated classes, along with their max logits. We compute the loss LD2D like so: Lmulti D2D = (cid:88) (i,j)Sbbox σ(β (τz zj )) (τz zj ) + (cid:88) (i,j) /Sbbox 16 σ(β (zj τz)) (zj τz) (6) D2D YIELDS BOOSTS IN NUMERACY, IN COMPLEMENT WITH BASELINES Table 6: D2D additionally yields improvements, in combination with T2I counting/enhancement baselines. On base models SDXL-Turbo, SD-Turbo, and Pixart-DMD; on benchmarks CoCoCount (Binyamin et al., 2025) and D2D-Small/Multi/Large. Avg. over four seeds. Base Model Method CoCoCount D2D-Small D2D-Multi D2D-Large SDXL-Turbo SD-Turbo Pixart-DMD TokenOpt (Zafar et al., 2024) TokenOpt (Zafar et al., 2024) + D2D ReNO (Eyring et al., 2024) ReNO (Eyring et al., 2024) + D2D ReNO (Eyring et al., 2024) ReNO (Eyring et al., 2024) + D2D ReNO (Eyring et al., 2024) ReNO (Eyring et al., 2024) + D2D 35.12 48.75 41.88 54.38 43.38 53.62 44.75 52.50 23.31 34.00 27.50 41.25 32.06 42. 37.25 40.62 5.31 9.69 8.94 11.62 9.44 12.00 3.94 8.56 4.69 10.38 4.25 10. 4.75 8.12 DENSITY-BASED DIFFERENTIAL PERFORMANCE PERSISTS IN D2D-MULTI Table 7: D2D yields higher performance on low total-density prompts than high total-density ones. Avg. over four seeds. Method SDXL (Podell et al., 2024) Make It Count (Binyamin et al., 2025) SDXL-Turbo (Sauer et al., 2025) ReNO (Eyring et al., 2024) TokenOpt (Zafar et al., 2024) D2D w/ OWLv2 (Ours) D2D w/ YOLOv9 (Ours) SD2.1 (Rombach et al., 2022) SD1.4 (Rombach et al., 2022) Counting Guidance (Kang et al., 2025) SD-Turbo (Rombach et al., 2022) ReNO (Eyring et al., 2024) D2D w/ OWLv2 (Ours) Pixart-α (Rombach et al., 2022) Pixart-DMD (Chen et al., 2025b) ReNO (Eyring et al., 2024) D2D w/ OWLv2 (Ours) D2D-Multi Low Total Density 3.08 2.75 6.67 12.08 7.83 6.17 3.75 4.42 3.00 11.58 13.42 1.67 8.00 12.25 17.17 High Total Density 0.50 0.25 1.25 3.00 1.50 0.75 0.00 0.25 1.25 1.00 2.75 0.25 1.00 1.00 1."
        },
        {
            "title": "H COUNTGD IN COMPARISON WITH OTHER COUNTERS",
            "content": "GroundingDINO (Liu et al., 2025)-based counter CountGD (Amini-Naieni et al., 2024) performs well in low and high-density settings. Figure 7: CountGD is state-of-the-art counter, based on GroundingDINO (Liu et al., 2025). D2D EXHIBITS MINIMAL DEGRADATION IN IMAGE QUALITY, COMPARED TO LAYOUT-CONTROL METHOD MAKE IT COUNT Tab. 8 reports the image quality of layout control-based, count-correction method Make It Count (Binyamin et al., 2025), its base model SDXL, and D2D, along with base model SDXLTurbo. Make It Count exhibits some tradeoffs between numeracy and image quality, yielding slightly lower PickScore and HPSv2, compared to its base model SDXL (e.g., on D2D-Small: PickScore: 21.70 (SDXL), 21.48 (MIC); HPSv2: 0.272 (SDXL), 0.264 (MIC)). D2D offers minimal image quality degradation (and sometimes better image quality) than base model SDXL-Turbo (e.g., on D2D-Small: ImageReward: 0.40 (SDXL-Turbo), 0.51 (D2D w/ OWLv2); HPSv2: 0.279 (SDXL-Turbo), 0.282 (D2D w/ OWLv2)). Table 8: D2D yields minimal degradation in image quality and alignment, with minimal computational overhead, compared to layout control-based, multi-step method Make It Count. Base models with no post-enhancement highlighted in gray. On CoCoCount and D2D-Small. Avg. over four seeds. Method ImageReward PickScore HPSv2 CLIPScore CoCoCount D2D-Small CoCoCount D2D-Small CoCoCount D2D-Small CoCoCount D2D-Small SDXL (Podell et al., 2024) + Make It Count (Binyamin et al., 2025) SDXL-Turbo (Sauer et al., 2025) + D2D (w/ OWLv2) + D2D (w/ YOLOv9) 0.87 0. 0.96 1.06 1.06 0.31 0.30 0.40 0.51 0.50 22.99 22.83 23.12 23.30 23.33 21.70 21. 21.98 21.98 22.04 0.290 0.286 0.293 0.300 0.300 0.272 0.264 0.279 0.282 0.282 32.91 32. 32.80 32.84 32.88 32.58 32.96 31.89 31.69 31.90 Inference Time (s) 7.69 37.16 0.16 19.42 11."
        },
        {
            "title": "J ADDITIONAL QUALITATIVE RESULTS",
            "content": "Additional qualitative results on prompts from benchmarks CoCoCount (Binyamin et al., 2025) and D2D-Small shown. Figure 8: Our method D2D effectively corrects numeracy mistakes. Qualitative examples from count correction/alignment methods D2D, ReNO (Eyring et al., 2024), and TokenOpt (Zafar et al., 2024) on base model SDXL-Turbo (Sauer et al., 2025), and Make It Count (Binyamin et al., 2025) on base SDXL (Podell et al., 2024). 19 Figure 9: Our method D2D effectively corrects numeracy mistakes. Qualitative examples from count correction/alignment methods D2D and ReNO (Eyring et al., 2024) on base model SD-Turbo, and Counting Guidance (Kang et al., 2025) on base SD1.4 (Rombach et al., 2022), along with SD2.1 (Rombach et al., 2022) shown. 20 Figure 10: Our method D2D effectively corrects numeracy mistakes. Qualitative examples from count correction/alignment methods D2D and ReNO (Eyring et al., 2024) on base model Pixart-DMD, along with Pixart-α (Chen et al., 2024)."
        },
        {
            "title": "K ADDITIONAL ABLATIONS",
            "content": "K.1 = 0.2 IS OPTIMAL FOR NUMERACY AND QUALITY is the weight used to determine how to mix the LMN output with the original noise. = 0 is numeracy-wise the best, followed by = 0.2, but results in patchy visual artifacts, relative to = 0.2  (Fig. 11)  . Hence, the hyperparameter value we use in our main experiments = 0.2. Table 9: In terms of just numeracy, = 0 is optimal, followed by = 0.2. Base model SDXL-Turbo, on CoCoCount. Seed = 0. 0.0 0. 0.5 0."
        },
        {
            "title": "CountGD",
            "content": "62.50 55.50 37.50 48.00 21 Figure 11: = 0 results in patchy visual artifacts, relative to = 0.2. So, we use = 0.2. K.2 ACROSS REQUESTED COUNTS, OUR COUNT CRITIC LD2D IS MORE EFFECTIVE THAN REGRESSION-BASED METHODS Fig. 12 reports the numeracy breakdown per requested count, comparing the performance of our count critic LD2D against regression-based critics, RCC (Hobley & Prisacariu, 2022), CounTR (Chang et al., 2022), and CLIP-Count (Jiang et al., 2023), as well as , which is the mathematical backbone of LD2D. Across all requested counts, our critic achieves the highest numeracy, peaking at 77% for = 2, whereas the highest score achieved by any regression-based method is 68%. scores lower, often even lower than regression-based methods, as expected, as it is not amenable to convergence. Figure 12: Our detector-based critic LD2D is more effective than regression-based count critics. Given the same initial seed, compared to regression-based methods RCC (Hobley & Prisacariu, 2022), CounTR (Chang et al., 2022), and CLIP-Count (Jiang et al., 2023), our method yields the highest numeracy across requested counts. On SDXL-Turbo on benchmarks CoCoCount and D2D-Small. Avg. over four seeds. 22 K.3 THE LATENT MODIFIER NETWORK EFFECTIVELY CORRECTS NUMERACY Fig. 13 reports the numeracy breakdown by absolute error between the requested and generated count in the initial generation, comparing the performance of D2D to ReNO (Eyring et al., 2024), using LD2D and reg for both. The introduction of the LMN boosts numeracy across all initial absolute errors depicted. Figure 13: The LMN effectively corrects numeracy. Breakdown of numeracy by absolute error between requested and generated count in the initial generation. Depicted on the x-axis are the subset of absolute errors in the range 1-10. Given the same initial seed, the addition of the LMN yields additional boosts in numeracy, compared to ReNO (Eyring et al., 2024). On SDXL-Turbo on benchmarks CoCoCount and D2D-Small. Avg. over four seeds. K. INFERENCE-TIME ALIGNMENT OF Mϕ At inference-time, given new xT , we allow few iterations to calibrate Mϕs output to be close to Gaussian. Fig. 14 shows this calibration stage is crucial to maintaining high image quality. (a) Without alignment. (b) With alignment. Figure 14: Inference-time calibration of Mϕ is necessary. Without calibration, multi-color, patchy visual artifacts result. Prompt: realistic photo of scene with one backpack. K.5 REGULARIZATION Our regularization term (Eq. 8) is variant of the one ReNO uses (Eq. 7) that penalizes larger deviations from Gaussian more and smaller deviations less, to allow enough flexibility in the vicinity of the initial distribution to accommodate updates specified by the count critic. We scale reg by coefficient and shift by constant c, which we take to the power of 10. reg = 2 2 (d 1) log(x ), (7) 23 Lreg = (aL reg + c)10. (8) We set coefficient to 0.03. Shift constant is set to 2139 to offset the optimal value of aL we find to be -2139 in practice. reg, which Our ablation analysis confirms that this variant of the regularization leads to overall numeracy improvements across both CoCoCount and D2D-Small (Tab. 10), indicating our regularization objective allows for effective numeracy correction, even while enforcing heavier penalty on nonGaussian initial noises. Table 10: Our variant of regularization is beneficial to numeracy optimization. Lreg, which penalizes larger deviations from Gaussian more and and smaller deviations less, yields higher numeracy on CoCoCount and D2D-Small benchmarks. On SDXL-Turbo. Avg. over four seeds. Method D2D w/ reg D2D w/ Lreg CoCoCount D2D-Small 53.88 55.62 42.44 43."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Princeton University",
        "LIX, École Polytechnique, IP Paris"
    ]
}