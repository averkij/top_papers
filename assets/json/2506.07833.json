{
    "paper_title": "Improving large language models with concept-aware fine-tuning",
    "authors": [
        "Michael K. Chen",
        "Xikun Zhang",
        "Jiaxing Huang",
        "Dacheng Tao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have become the cornerstone of modern AI. However, the existing paradigm of next-token prediction fundamentally limits their ability to form coherent, high-level concepts, making it a critical barrier to human-like understanding and reasoning. Take the phrase \"ribonucleic acid\" as an example: an LLM will first decompose it into tokens, i.e., artificial text fragments (\"rib\", \"on\", ...), then learn each token sequentially, rather than grasping the phrase as a unified, coherent semantic entity. This fragmented representation hinders deeper conceptual understanding and, ultimately, the development of truly intelligent systems. In response, we introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method that redefines how LLMs are fine-tuned. By enabling the learning of sequences that span multiple tokens, this method fosters stronger concept-aware learning. Our experiments demonstrate significant improvements compared to conventional next-token finetuning methods across diverse tasks, including traditional applications like text summarization and domain-specific ones like de novo protein design. Multi-token prediction was previously only possible in the prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first to bring the multi-token setting to the post-training phase, thus effectively democratizing its benefits for the broader community of practitioners and researchers. Finally, the unexpected effectiveness of our proposed method suggests wider implications for the machine learning research community. All code and data are available at https://github.com/michaelchen-lab/caft-llm"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 3 3 8 7 0 . 6 0 5 2 : r Improving large language models with concept-aware fine-tuning Michael K. Chen Nanyang Technological University Singapore michaelchenkj@gmail.com Jiaxing Huang Nanyang Technological University Singapore jiaxing.huang@ntu.edu.sg Xikun Zhang Nanyang Technological University Singapore xikun.zhang@ntu.edu.sg Dacheng Tao Nanyang Technological University Singapore dacheng.tao@ntu.edu.sg"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have become the cornerstone of modern AI. However, the existing paradigm of next-token prediction fundamentally limits their ability to form coherent, high-level concepts, making it critical barrier to human-like understanding and reasoning. Take the phrase \"ribonucleic acid\" as an example: an LLM will first decompose it into tokens, i.e., artificial text fragments (\"rib\" \"on\" . . . ), then learn each token sequentially, rather than grasping the phrase as unified, coherent semantic entity. This fragmented representation hinders deeper conceptual understanding and, ultimately, the development of truly intelligent systems. In response, we introduce Concept-Aware Fine-Tuning (CAFT), novel multi-token training method that redefines how LLMs are fine-tuned. By enabling the learning of sequences that span multiple tokens, this method fosters stronger concept-aware learning. Our experiments demonstrate significant improvements compared to conventional next-token fine-tuning methods across diverse tasks, including traditional applications like text summarization and domain-specific ones like de novo protein design. Multitoken prediction was previously only possible in the prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first to bring the multi-token setting to the post-training phase, thus effectively democratizing its benefits for the broader community of practitioners and researchers. Finally, the unexpected effectiveness of our proposed method suggests wider implications for the machine learning research community. All code and data are available at https://github.com/michaelchen-lab/caft-llm"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have advanced tremendously in recent years. They owe much of their success to the effectiveness of the LLM development pipeline (Li et al., 2024b), which can be described as such: first, in the pre-training phase, models are trained on large-scale unsupervised text corpus in order to teach general knowledge and language understanding. Next, in the post-training phase, models are fine-tuned on downstream supervised datasets to respond to diverse tasks in specific formats and to prevent dangerous model behaviors through myriad of techniques. This is done via variety of techniques, including instruction tuning, reinforcement learning from human feedback (RLHF), and more (Lambert et al., 2024). This training paradigm has fueled the growth of language models in both AI research and commercial adoption. Importantly, this training paradigm conforms to seemingly unassailable training objective: next-token prediction. vocabulary of tokens, or text fragments, is first created using tokenization algorithms, most commonly byte-pair encoding (BPE) (Sennrich et al., 2015), which forms word/subword tokens based on their frequency in the training corpus. After tokenizing the texts using this vocabulary, the tokens are fed into the model to predict the next token autoregressively. For example, as shown in Figure 1(a,b), if Llama 3 model (Grattafiori et al., 2024) is tasked to predict ribonucleic acid as part of given question, the phrase is first deconstructed, i.e., tokenized, into rib, on, ucle, ic, and acid. Then, the model is trained to predict single token in each forward pass sequentially, starting from rib. Preprint. Under review. Figure 1: (a, b) Next-token vs. multi-token training. Language models are typically trained using the next-token objective (left), where each token is an artificial text fragment. At every forward pass, models are tasked to predict the next immediate token. However, in the multi-token setting (right), models are tasked to predict the next tokens in parallel in each forward pass, thus facilitating conceptual understanding across tokens. In this illustration, the relevant concept is ribonucleic acid\", which is segmented into 5 tokens by the Llama 3 tokenizer. (c) Concept-aware fine-tuning (CAFT) architecture. Next-token models are modified into multi-token ones by training task-agnostic auxiliary heads Fhk (blue) that predict the next k-th token (Section 2.2) for = 5. During multi-token fine-tuning, the auxiliary heads augment the cross-entropy loss to enable the multi-token training objective (Section 2.3). Finally, only the original model layers (in yellow) are used for inference. (d) Examples of multi-token concepts. Our proposed method increases coherence across domains and modalities, including text, code, and mathematical expressions, given the prevalence of multi-token concepts. 2 However, this training objective is suboptimal: tokens are artificial text fragments that do not represent coherent concepts or entities. At each forward pass, models have no access to the succeeding tokens. For example, when predicting rib as part of ribonucleic acid, the latter portion -onucleic acid is hidden. Intuitively, learning single token that is part of larger concept, in isolation, fails to capture the actual underlying information at hand. The growing literature on the effects of tokenization on language model performance supports this assertion. Tokenizers with better compression, i.e., the ability to discretize text into longer words and subwords, lead to better model performances than those with worse compression (Goldman et al., 2024). Additionally, the specific implementation of tokenization affects how numbers and mathematical expressions are segmented, ultimately having undue influence on arithmetic ability (Singh, Strouse, 2024). Training models to predict the next immediate token handicaps their learning process. Instead, models should be trained to predict concepts, often spanning multiple tokens, as shown in Figure 1d. Along these lines, several methods leveraging multi-token prediction have been proposed (Gloeckle et al., 2024; Liu et al., 2024a). Specifically, at each position in the training corpus, models are trained to predict the following tokens using output heads. However, these methods are restricted to the pretraining phase, which results in prohibitive costs and diminished effectiveness. First, the pretraining phase is inherently orders of magnitudes more computationally expensive than post-training, making existing multi-token methods unfeasible for all but select group of well-resourced labs. Second, the pretraining phase teaches models general knowledge and language modeling skills, while the post-training phase teaches specific, relevant skills. Thus, existing methods do not adequately learn domain-specific, multi-token concepts: they exhibit only incremental gains compared to their next-token counterparts on downstream tasks. Naturally, one would expect multi-token prediction to be applied to fine-tuning instead. However, to the best of our knowledge, current research in this direction has been unsuccessful, finding that fine-tuning with multi-token prediction leads to similar or worse performance (Gloeckle et al., 2024; Cai et al., 2024). Incorporating the multi-token setting into the post-training phase is extremely challenging because the multi-token setting represents dramatic distribution shift. Given that post-training is much shorter than pretraining, models fail to adapt, leading to degradation. In response, we introduce Concept-Aware Fine-Tuning (CAFT), novel multi-token fine-tuning method for nexttoken models. First, auxiliary heads that predict token positions beyond the next immediate token are trained using an instruction-tuning mixture, where the ground truth responses are self-distilled from the model itself. We provide trained task-agnostic auxiliary heads for range of popular open-source models, allowing practitioners to focus on their task-specific MTP fine-tuning, as illustrated in Figure 1c. On top of full or Low-Rank Adaptation (LoRA) fine-tuning on the base model, the auxiliary heads and multi-token loss function are added. We empirically demonstrate CAFTs effectiveness and applicability to diverse domains, including traditional ones like text summarization and domain-specific ones like de novo protein design. It achieves superior performance to its next-token full and LoRA fine-tuning counterparts. The magnitudes of gains are similar or better than existing MTP pretraining methods despite using only fraction of the computational cost. Additionally, we find that CAFT LoRA often outperforms next-token full fine-tuning, suggesting that models learn more effectively in multi-token setting. In settings where multi-token prediction is highly advantageous, multi-fold increase in model performance can be observed. Importantly, CAFT presents significant implications for the scientific community. First, by introducing multi-token prediction into the post-training phase, our method democratizes the benefits of MTP to the broader community of practitioners and researchers. This builds the foundation for future works in this nascent frontier. Second, LLMs ability to plan beyond the next token is still hotly debated (Lindsey et al., 2025). The unreasonable effectiveness of CAFT demonstrates that the models do not adequately learn and plan ahead of the next immediate token; an explicit multi-token objective is more effective. Our empirical evidence serves as crucial step towards understanding the internal mechanisms of language models."
        },
        {
            "title": "2 Concept-Aware Fine-Tuning (CAFT)",
            "content": "Auxiliary heads are first trained in order to facilitate multi-token fine-tuning, which we describe in Section 2.2. This only needs to be trained once for given model and can be provided by third-party, so practitioners need only focus on the next step: multi-token fine-tuning on their specific task, which is described in Section 2.3. To better illustrate the multi-token setting, we first briefly describe the canonical next-token training method. 2.1 Background on Next-token Prediction Conventionally, language models are trained autoregressively on large text corpus using next-token prediction task, as illustrated in Figure 1a. Given the inputs x1,...xt, models are tasked to predict xt+1 with the objective of minimizing 3 Figure 2: (a) Sample code implementation. Building on top of the industry-standard Transformers library (Wolf et al., 2020), researchers and practitioners can incorporate CAFT into their existing Transformers training pipelines with just few lines of code using our open-source library caft. (b) Downstream Tasks. These tasks empirically underscore the effectiveness and broad applicability of CAFT. The examples are adapted from the HumanEval, MATH-500, MIMIC-IV-BHC, L+M-24, and Mol-Instructions datasets respectively, which are used for the evaluations in Section 3.2. the following cross-entropy loss: L1 = log pt(yt+1) (1) where yt+1 is the ground truth token at position + 1. This core objective dominates both pretraining and fine-tuning (which is applied in the post-training phase). In this work, we reshape this ubiquitous training objective to predict the next tokens instead, as shown in Figure 1b. The challenge of multi-token fine-tuning. Incorporating the multi-token setting into the post-training phase is not new idea; however, existing attempts have been unsuccessful. For example, Gloeckle et al. (2024) finds that next-token prediction performs better during fine-tuningeven for models pretrained under the multi-token setting! Naively repurposing multi-token pretraining methods to fine-tune next-token models creates several problems: First, introducing the multi-token objective represents dramatic distribution shift, which models often do not recover from, leading to worse performances than the base models. Second, given the naturally higher loss of auxiliary heads (due to having further token position), models tend to optimize for the auxiliary loss at the expense of L1, which is ultimately what matters at inference time. Lastly, the post-training phase is dramatically shorter than pretraining; using existing methods, models do not have sufficient compute to take advantage of the multi-token setting during fine-tuning. Our proposed method, CAFT, introduces series of novel techniques and applications designed to address these previously unsolved challenges, making it the first to enable multi-token fine-tuning. 2.2 Setting the stage: Training auxiliary heads Before CAFT can be applied to next-token models, they must be adapted to predict future tokens at once. As such, auxiliary heads that predict the k-th token for = 1, 2, .., are trained. Importantly, these heads are task-agnostic and can be used on variety of downstream fine-tuning tasks. 1 auxiliary heads are added to predict the next tokens. The architecture consists of (i) an independent hidden layer Fhk , whose weights are initialized identically to those of the last hidden layer Fh1 of the original model, and (ii) shared unembedding layer Fu from the original model. In Figure 1c, layers Fhk are illustrated in blue, while Fu is below them. Layer Fu is shared due to the prohibitively large vocabulary sizes of existing LLMs. Given the token context x1:t = x1, ..., xt, each heads input is the hidden representation z1:t from the shared transformer layers Fs of the original model (which excludes Fh1). Formally, to output pt+k, the k-th head is defined as: pt+k = softmax(Fu(Fhk (z1:t))) (2) Full fine-tuning is used to train layers Fhk for > 1, while all other layers are frozen, including the unembedding layer Fu. This prevents the performance of layer Fh1 from degrading while simultaneously reducing compute costs. The cross-entropy loss for the next future tokens is: Ln = (cid:88) k=2 αk2 log pt+k(yt+k) (3) where yt+k is the ground truth token at position + and αk1 is geometric decay that lowers the loss of auxiliary heads at later token positions. The higher the future token position, the greater the loss due to its inherent unpredictability. αk1 scales the respective losses to promote more stable training. In the absence of the original training recipe of most open-source models, we construct an instruction-tuning dataset of 100,000 samples, sourced from the ShareGPT dataset1 and Tulu 3 SFT mixture (Lambert et al., 2024). It encompasses broad spectrum of tasks to ensure that the auxiliary heads are task-agnostic; full breakdown can be found in Table X. Importantly, to match the output distribution of the 1st (original) head, the datasets ground truth responses are self-distilled from the original head Fh1 . In other words, only the questions from the dataset are externally sourced. 2.3 Concept-aware fine-tuning using auxiliary heads After adding the auxiliary heads as trained and defined in Equation 2, task-specific CAFT can be executed. Generally, only parameters that are part of the original model are fine-tuned. For example, for LoRA fine-tuning, all layers are targeted except for layers Fhk for > 1 to reduce memory footprint and except for the unembedding layer Lu to improve training stability. Other fine-tuning methods besides full and LoRA fine-tuning can also be used in theory, but are beyond the scope of this work. 1Collected from https://sharegpt.com/. 5 Ultimately, the primary objective is to minimize the 1st heads loss L1, considering only the 1st head will be used for inference; the losses of all subsequent heads are purely auxiliary. Guided by this motivation, the cross-entropy loss for CAFT is calculated as: Ln = (cid:88) k=1 αk1βγ log pt+k(yt+k) (4) where β adjusts the weightage of all auxiliary head losses and γ adjusts their weightage over iterations. In practice, we find that models tend to optimize for the auxiliary losses at the expense of the first heads loss L1 due to the formers relatively higher loss. β = 0.01 ensures that the training continues to prioritize the latter, while decaying sine schedule for γ incentivises models to pay greater attention to the auxiliary losses at the start, but ultimately optimize for L1. Importantly, the effectiveness of the multi-token setting is directly correlated with the auxiliary heads adaptation to the given task. Using the method described in Section 2.2, the heads are broadly effective for general conversation, coding, and math tasks. However, for tasks with diverse, unpredictable vocabularies, e.g., story writing, and those with unknown formats, e.g., protein sequences, it is helpful to finetune the auxiliary heads specifically as described in Equation 3 for 1 epoch on the given tasks training set. The compute cost is minimal, but vastly improves CAFTs effectiveness. After training, the auxiliary heads are discarded, such that only the base model remains. Thus, there is no necessary additional computational cost or code modifications for model inference. 2.4 Practical Implementation Concept-aware fine-tuning is straightforward to implement for virtually all language models. In practice, the auxiliary heads of popular models will be trained and open-sourced by various research labs and model providers. Building upon the industry-standard Transformers library (Wolf et al., 2020), practitioners need only augment their fine-tuning script with several additional lines of code with our open-source library caft. An example implementation in shown in Figure 2a. Some tips for practitioners: first, it is best to monitor both L1 and Ln as defined in Equation 3 (without β or γ). We should expect that (i) Ln decreases over epochs, which shows that the model has optimized for the auxiliary losses, and that (i) Ln is ultimately lower than that of conventional fine-tuning, which shows that the multi-token objective is beneficial. Second, in practice, we find that when L2 > 4.0, the auxiliary heads are too unreliable to be useful; in which case, the aforementioned head fine-tuning strategy should be used."
        },
        {
            "title": "3 Experiments",
            "content": "We demonstrate the wide-ranging effectiveness of concept-aware fine-tuning (CAFT) on diverse tasks. Beyond traditional LLM tasks like coding, math, and text generation, we also evaluate its performance on scientific tasks, such as molecular generation and de novo protein design. The Llama3-8B-Instruct model is used as our primary case study, considering its ubiquity in fine-tuning tasks. After training its auxiliary heads, the model is adapted using CAFT with full or LoRA settings on the aforementioned tasks. We find that CAFT methods consistently outperform their next-token counterparts. 3.1 Training Auxiliary Heads We train four auxiliary heads Fh2:5 for Llama3-8B-Instruct using the method and architecture described in Section 2.2, with sequence length of 4096 and for up to 4 epochs. = 5 is chosen as balance between the extent of multi-token representation and the additional compute required. Other studies have empirically found that training four additional heads is optimal (Gloeckle et al., 2024; Cai et al., 2024). As shown in Figure 3, the training leads to significant reduction in model perplexities across all auxiliary heads. It also suggests that four epochs are sufficient to achieve optimal performance. As expected, heads become more inaccurate as the token position increases. Without knowing the predictions of prior heads, subsequent heads will inevitably observe higher losses. Note that the perplexity of the original head Fh1 is shown only for reference; it is not trained at this stage. These task-agnostic auxiliary heads are used to apply CAFT to the downstream tasks below. Note that, in practice, users do not need to train the auxiliary heads and only need to fine-tune on their specific tasks; trained auxiliary heads will be readily available on the internet. 6 Figure 3: Perplexities of auxiliary heads over four epochs. Note that head Fh1 is not adapted and is only displayed for reference. 3.2 Downstream Tasks We present the results on five downstream tasks of different domains to demonstrate the superior performance and broad applicability of concept-aware fine-tuning. Example questions from every evaluation dataset can be found in Figure 2b. The general experimental setup is as follows: using task-specific training datasets, we fine-tune four separate models using full CAFT, LoRA CAFT, and their next-token versions. The latter two serve as baseline models for comparison. They are trained for up to 5 epochs with early stoppage and for sequence lengths ranging from 512 to 2048, depending on the specific task. All results shown are the average of 5 independent evaluation runs. From Task 3 onwards, the auxiliary heads are pretrained on the training set for 1 epoch before proceeding to the actual fine-tuning. 3.2.1 Downstream Task 1: Coding Code is an intuitive application of CAFT. Programming languages have vastly different vocabularies and distributions from natural languages. For example, punctuations like brackets and colons convey vastly different semantic meanings. Given the dominance of natural language in pretraining corpora, modern tokenizers do not effectively encode programming-specific texts. Thus, coherent \"words\" in code, such as __name__ in Python, are often deconstructed into two or more unintuitive fragments. For this experiment, we first construct Python training dataset that consists of combined 10,000 examples from CodeAlpaca (Chaudhary, 2023), MagiCoder (Xu et al., 2024), and the Mostly Basic Python Programming (MBPP) training set (Austin et al., 2021). The HumanEval test set (Chen et al., 2021) is used to evaluate the models; an example question is shown in Figure 2b. Models tested under the pass@1 setting, i.e., one response is sampled per question, which is only deemed correct if it passes all test cases. Table 1: Model Performances on HumanEval. Task Method Base HumanEval LoRA Fine-tuning Full Fine-tuning Accuracy (%) 38.9 40.9 45.1 40.5 49.3 Next-token CAFT Next-token CAFT As shown in Table 1, CAFT dramatically improves Python coding performance: LoRA and full CAFT lead to 4.2% and 8.8% improvement in accuracy respectively. The magnitude of these gains is surprising given the relatively small training sample size and short training time. This reinforces our aforementioned hypotheses that the arbitrary parsing of code greatly hinders model learning, suggesting that the current next-token paradigm suppresses the potential of LLMs to complete coding tasks. 7 3.2.2 Downstream Task 2: Mathematics Similar to code, mathematical expressions are segmented by tokenizers. Most tokenizers have vocabulary size of 30-120 thousand, which is vastly insufficient to encompass the space of numbers and symbols. multi-token setting could reassemble these tokens into the relevant expressions. The math training set consists of 10,000 random samples from MetaMathQA (Yu et al., 2023), which are math questions augmented from the training sets of GSM8K (Cobbe et al., 2021) and MATH (Lightman et al., 2023). Each answer contains the answer steps in the form of chain-of-thought. Models are evaluated on the MATH-500 test set, based on whether their final answer exactly matches the ground truth answer. Table 2: Model Performances on MATH-500."
        },
        {
            "title": "Method\nBase",
            "content": "MATH-500 LoRA Fine-tuning Full Fine-tuning Accuracy (%) 19.1 22.9 24.6 23.7 25.2 Next-token CAFT Next-token CAFT Importantly, this particular experimental setting is highly unfavorable to CAFT. First, the MetaMathQA dataset primarily features natural language reasoning, rather than long, recurring mathematical expressions; the latter, however, benefits the most from our proposed method. Additionally, math problems, unlike the rest of the downstream tasks, are evaluated entirely on their final answers. Thus, CAFT primarily enhances the intermediate chain-of-thought reasoning, thereby only indirectly influencing model accuracy. Nonetheless, as shown in Table 2, substantial accuracy gains are observed when CAFT is used. 3.2.3 Downstream Task 3: Text Generation In this task, we explore the use of CAFT for domain-specific text generation, i.e., clinical text summarization. Brief Hospital Course (BHC) summaries are short summaries that describe patients hospital stay. By referencing often voluminous clinical notes, BHC summaries are tediously written by clinicians. This significant time burden could be alleviated by leveraging LLMs to create these summaries. To evaluate the effectiveness of CAFT for this task, we use MIMIC-IV-BHC (Aali et al., 2025), pre-processed dataset extracted from raw MIMIC-IV notes (Johnson et al., 2023). In line with the existing literature on text summarization, we evaluate the models using the ROUGE metrics (Lin, Hovy, 2003): ROUGE-N measures the n-gram overlap between the model generation and ground truth summaries, ROUGE-L measures the Longest Common Subsequence (LCS) to captures sentence-level structure and word order, and ROUGE-Lsum calculates the LCS of each sentence for more granularity. Table 3: Model Performances on CNN-DailyMail and WritingPrompts. Method Base LoRA Fine-tuning Full Fine-tuning Next-token CAFT Next-token CAFT ROUGE-1 ROUGE-2 ROUGE-L ROUGE-Lsum 29.17 42.31 44.16 44.57 45.93 6.64 20.56 22.3 22.94 24.44 15.46 29.86 31.62 32.17 33. 27.49 40.44 42.37 42.75 44.04 As shown in Table 3, CAFT methods consistently outperform their next-token counterparts. Unlike tasks like math and coding, text generation in uncontrolled; it exhibits significantly larger linguistic diversity, ranging from natural language text to domain-specific acronyms. Given the relative sparsity of multi-token concepts, there was concern that the model would fail to learn these representations. However, the clear improvement in performance shows that the auxiliary heads effectively capture ideas spanning multiple tokens, even when they are not as prevalent. 8 3.2.4 Downstream Task 4: Molecular Generation The de novo molecule generation task is defined as follows: Given set of desired molecular functions and characteristics, generate the corresponding molecular structure. The task of formulating novel molecules has important downstream applications in areas such as drug discovery and materials design, and conventionally relies on the intuition of chemists (Meyers et al., 2021). While there have been many attempts to use generative models to accelerate the process, they still have significant room for improvement, given the unique grammar of SMILES sequences and the inherent difficulty of associating molecular properties with the given representation. For example, molecular structures often contain functional groups, which are coherent subsequences within the SMILES representation. CAFT presents unique opportunity to better understand its idiosyncratic syntax and exploit these domain-specific patterns. For this case study, the train and test sets are drawn from the challenging L+M-24 dataset (Edwards et al., 2024). The input is natural language description of the desired molecular properties, and the output is the associated molecule in the form of SMILES sequence. The following metrics are used: (i) exact match (EM), representing the proportion of generated SMILES sequence that exactly matches the gold truth, (ii) Levenshtein distance (Miller et al., 2009), measuring the statistical distance between the model generation and ground truth, (iii) Morgan Fingerprint Tanimoto Similarity (FTS) (Rogers, Hahn, 2010), measuring the structural similarity between SMILES strings, and finally (iv) validity, representing the proportion of generations that conform to SMILES grammar and chemical rules. Table 4: Model Performances on L+M-24. Task Method EM (%) Levenshtein Morgan FTS Validity (%) L+M-24 LoRA Fine-tuning Full Fine-tuning Base Next-token CAFT Next-token CAFT 0.00 0.00 0.00 0.14 0.54 139.19 64.14 58.69 47.59 41.23 4.71 62.42 63.20 65.34 67.79 68.70 91.84 93.46 92.38 97. Scientific LLMs typically rely on full fine-tuning, rather than LoRA, due to the complexity of biomedical tasks and lack of exposure to relevant content in their original training corpus. As such, we focus on the results of full CAFT in Table 4, which offers significant gains over the baseline. This is especially true for exact match accuracy, which improved several-fold, and the percentage of valid SMILES strings, which improved by 4.76%. These results suggest that CAFT is effective for tasks with usual, out-of-distribution targets. 3.2.5 Downstream Task 5: De Novo Protein Design The goal of de novo protein design is to generate proteins that exhibit the desired functions and characteristics. In conventional protein design, protein backbone structure at the atomic level is first defined, followed by finding sequence consistent with that structure. De novo protein design, on the other hand, generates the protein from scratch, without relying on existing sequences or other starting points. This method holds immense promise for protein engineering, potentially paving the way for creating proteins with novel architectures and functions, and enabling precise control over the proteins functions and characteristics (Kortemme, 2024). However, de novo design is deeply challenging given the immense space of potential sequences and unintuitive grammar of protein sequences. To evaluate CAFTs effectiveness in de novo protein design, we leverage user requirement-protein sequence pair dataset using features from the UniProt knowledgebase, curated by Mol-Instructions (Fang et al., 2023). The protein design requirements, expressed in natural language, consist of the proteins general functions, associated metabolic pathways, co-factors, and 17 other commonly targeted features. Due to the significant difficulty of protein sequences, LoRA methods fail to achieve substantial improvements; only fine-tuning methods are reported. After fine-tuning on 10,000 samples, models are evaluated along two axes: sequence and structural similarity. For the former, we use (i) sequence identity (Rost, 1999), which measures the proportion of exact matches between the model generated and ground truth sequences, and (ii) sequence alignment, which measures the degree of similarity between sequences using the BLOSUM62 substitution matrix for protein alignments (Henikoff, Henikoff, 1992) and the Needleman-Wunsch global pairwise alignment algorithm (Needleman, Wunsch, 1970). For structural similarity, the relevant metrics are (i) predicted local distance difference test (pLDDT) (Jumper et al., 2021), which measures models confidence in the protein structure generated from given sequence, and (ii) TM-score (Zhang, Skolnick, 2004), which measures the similarity between the model generated and ground truth structures. Note that for the structural metrics, we leverage ColabFold (Mirdita et al., 2022), fast, popular implementation of AlphaFold2 (Jumper et al., 2021), to generate the protein structure of the model-generated sequences. pLDDT is important because it measures whether given sequence can lead to plausible protein structure. Table 5: Model Performances on Mol-Instructions De Novo Protein Design."
        },
        {
            "title": "Method\nBase",
            "content": "Full Fine-tuning Next-token CAFT Identity (%) Alignment pLDDT TM-score (%) 12.06 20.32 22.14 -71.59 -16.01 3. 22.28 52.6 54.3 6.80 33.07 35.12 As shown in Table 5, CAFT consistently outperforms the baseline across both axes. In particular, multi-token training leads to significantly higher sequence alignment. Additionally, the proportion of structurally similar protein generations has greatly increased. 25.0% of generated sequences have high pLDDT score (typically defined as >70.0) for the CAFT model, compared to the 20.0% of the next-token model; similarly, 20.0% have high TM-score (typically defined as >50.0) as compared to 15.8%. 3.3 CAFT improves performance by learning multi-token concepts. We theorize that CAFTs outperformance can be attributed to better conceptual understanding across tokens. An authoritative investigation into the internal mechanisms of CAFT models is, however, unfeasibly difficult. Thus, we support our hypothesis by empirically studying the heterogeneous effects of concepts on CAFTs performance gains. First, we identify proxies of concepts in two tasks, coding and molecular generation, using the full CAFT and next-token models from Section 3.2.1 and 3.2.4. For Python code, we define concepts as coherent snippets of code that span multiple tokens. In practice, we use Python parser to extract expressions contained within brackets, quotation marks, etc, and methods separated by periods. For SMILES strings, we define concepts as complex functional groups. This analysis focuses on benzene, amide, and carboxylic acid groups, which are complex yet well-represented in the dataset. Figure 4a illustrates two examples from the HumanEval and L+M-24 model evaluations, which show how concepts are extracted from these domains, and how CAFT enables better conceptual generation. The analysis is conducted as follows: For HumanEval, questions are split into two bins, conceptual or non-conceptual, based on whether their number of concepts is above or below the dataset average. CAFTs performance improvement over next-token fine-tuning across both bins is then measured. For L+M-24, we compare the CAFT and next-token fine-tuned models ability to correctly generate functional groups. Besides the proportion of matches, to account for false positives, we also report the F1 score. HumanEvals results, as reported in Figure 4(left), show that CAFTs performance gains are more pronounced in highly conceptual questions (+11.67%) as opposed to less conceptual ones (+7.59%). L+M-24s results, as shown in Figure 4(right), show that CAFT substantially outperforms next-token fine-tuning in learning concepts, i.e., functional groups. The empirical evidence is consistent with our hypotheses that CAFT leads to better multi-token conceptual understanding, but further exploration is required for conclusive prove."
        },
        {
            "title": "4 Related Works",
            "content": "Large language model optimization. LLMs are generally trained to predict the next token autoregressively: Given sequence x1:t, predict the next token xt+1. The foundations of this approach were first introduced by the seminal work of Shannon (1948), and have since grown to form the core of modern LLMs (Devlin et al., 2019; Brown et al., 2020). The mechanism for model optimization is as follows: the models raw outputs are passed through softmax activation function to compute its probability distribution for the predicted xt+1. It is then compared against the ground truth one-hot probability distribution to compute the cross-entropy loss, which is used to optimize the model weights through backpropagation. Modern LLMs are constructed based on the decoder-only Transformer architecture (Vaswani et al., 2017; Radford et al., 2018). LLM training pipeline. There are generally two main phases of LLM training (Li et al., 2024b): pretraining and post-training. During pretraining, models are trained on massive text corpus in an unsupervised fashion. The goal is to teach language modeling skills and general knowledge. In practice, this phase is responsible for the majority of total compute used. The resulting models, such as DeepSeek V3 (Liu et al., 2024a) and Llama3-8B-Base (Grattafiori et al., 2024), serve as \"base\" models for the next phase: post-training, where they are further trained on supervised datasets to learn specific skills and output formats. This is typically done via supervised fine-tuning and reinforcement learning (Ouyang et al., 2022), where the former consumes the majority of the compute cost in this phase. 10 Figure 4: (a) Examples of CAFTs concept-informed generation. comparison of the ground truth, next-token fine-tuned model generation, and CAFT model generation for two questions from the HumanEval (top) and L+M-24 (bottom) datasets. The red boxes show the relevant proxy concepts, derived using the method described in Section 3.3, and how CAFT is able to identify them. (b) Coding (HumanEval) Ablation. The full CAFT model performs disproportionately better on coding questions with high number of concepts. (c) Molecular Generation (L+P-24) Ablation. The full CAFT model is substantially better at generating the relevant functional groups when required. Downstream Fine-tuning. Importantly, foundational models are often further trained by industry practitioners and researchers to perform domain-specific tasks, such as reasoning (Guo et al., 2025), math (Liu et al., 2024b), and even molecular generation (Fang et al., 2023). However, because fine-tuning all model parameters may be too computationally expensive, parameter-efficient fine-tuning methods such as LoRA (Hu et al., 2022) and QLoRA (Dettmers et al., 2023) were introduced. Multi-token prediction. growing body of literature finds that the next-token training paradigm performs poorly on lookahead tasks (Bachmann, Nagarajan, 2024) and compositional tasks (Dziri et al., 2023), and is highly inefficient relative to human children (Frank, 2023). In response, two existing works introduce multi-token training to the pretraining phase (Gloeckle et al., 2024; Liu et al., 2024a). Prior to our work, multi-token training in the post-training phase saw worse performance than the conventional next-token setting (Cai et al., 2024; Gloeckle et al., 2024). Orthogonal to these works, several speculative decoding methods leverage multi-token prediction to serve as drafts for future tokens (Stern et al., 2018; Cai et al., 2024). Their primary purpose is to improve inference speed and generally observe minor performance degradation. Our proposed method, CAFT, builds upon both bodies of literature, especially Cai et al. (2024) and Gloeckle et al. (2024), to unlock multi-token predictions potential for fine-tuning."
        },
        {
            "title": "5 Discussion",
            "content": "5.1 The Advantages of Concept-Aware Fine-Tuning (CAFT) Concept-aware fine-tuning (CAFT) addresses LLMs fragmented conceptual understanding, caused by the artificial nature of tokenizations text parsing. Specifically, CAFT is the first method to introduce multi-token prediction to model fine-tuning. This was previously thought to be unfeasible due to the brevity of the post-training phase. Empirically, CAFT leads to substantial performance gains by improving concept formation. Our proposed method is designed to be broadly applicable and straightforward to implement. (1) CAFT can be applied to any generation task, including those requiring modalities beyond natural language, such as protein sequences. It is therefore useful for both industry practitioners and researchers. (2) CAFT is cost-efficient. Existing multi-token pretraining methods are prohibitively expensive and can only be adopted by AI hyperscalers, such as Meta and DeepSeek. For the first time, CAFT democratizes the multi-token setting for any company or research lab. (3) CAFT is easy to implement. Using our open-source library caft, only few additional lines of code must be added to ones existing fine-tuning script. 5.2 Broader Implications: Rethinking the next-token objective LLMs ability to plan beyond the next token has long been controversial. Learning across tokens is crucial for coherent text generation and accomplishing broad spectrum of tasks. The broad success of next-token LLMs, starting from GPT-3 (Brown et al., 2020), strongly suggests that existing models can learn coherent concepts across tokens to some extent. Other empirical works confirm this: for one, Lindsey et al. (2025) find that LLMs plan beyond the next word when writing poems. However, on the flip side, the multi-token objective has been empirically shown to improve model performance: Gloeckle et al. (2024) and Liu et al. (2024a) have done so for the pretraining phase; our work is the first to do so for the post-training phase. If the next-token setting already adequately incentivises lookahead, planning, and multi-token conceptual understanding, these results should not be observed. The unreasonable effectiveness of CAFT suggests that it addresses significant limitation in the next-token setting, i.e., the weak conceptual understanding. Thus far, multi-token training has been held back by being restricted to the pretraining phase and by its prohibitive computation costs. Given that CAFT directly addresses these factors, we hope the introduction of CAFT will drive the adoption of the multi-token setting during post-training, potentially becoming the de facto training objective for model fine-tuning."
        },
        {
            "title": "6 Conclusion",
            "content": "Tokenization in LLMs segments text into artificial, incoherent fragments. Given the conventional next-token training objective, models fail to adequately learn multi-token concepts, thus constraining their performance. In response, we introduce concept-aware fine-tuning (CAFT), which enables better conceptual understanding by introducing the multi-token setting to the post-training phase. Empirically, CAFT leads to substantially better model performance across diverse tasks, including coding, text summarization, and de novo protein design."
        },
        {
            "title": "References",
            "content": "Aali Asad, Van Veen Dave, Arefeen Yamin Ishraq, Hom Jason, Bluethgen Christian, Reis Eduardo Pontes, Gatidis Sergios, Clifford Namuun, Daws Joseph, Tehrani Arash S, others . dataset and benchmark for hospital course summarization with adapted large language models // Journal of the American Medical Informatics Association. 2025. 32, 3. 470479. Austin Jacob, Odena Augustus, Nye Maxwell, Bosma Maarten, Michalewski Henryk, Dohan David, Jiang Ellen, Cai Carrie, Terry Michael, Le Quoc, others . Program synthesis with large language models // arXiv preprint arXiv:2108.07732. 2021. Bachmann Gregor, Nagarajan Vaishnavh. The pitfalls of next-token prediction // arXiv preprint arXiv:2403.06963. 2024. Brown Tom, Mann Benjamin, Ryder Nick, Subbiah Melanie, Kaplan Jared D, Dhariwal Prafulla, Neelakantan Arvind, Shyam Pranav, Sastry Girish, Askell Amanda, others . Language models are few-shot learners // Advances in neural information processing systems. 2020. 33. 18771901. Cai Tianle, Li Yuhong, Geng Zhengyang, Peng Hongwu, Lee Jason D, Chen Deming, Dao Tri. Medusa: Simple llm inference acceleration framework with multiple decoding heads // arXiv preprint arXiv:2401.10774. 2024. Chaudhary Sahil. Code Alpaca: An Instruction-following LLaMA model for code generation. 2023. Chen Mark, Tworek Jerry, Jun Heewoo, Yuan Qiming, Pinto Henrique Ponde De Oliveira, Kaplan Jared, Edwards Harri, Burda Yuri, Joseph Nicholas, Brockman Greg, others . Evaluating large language models trained on code // arXiv preprint arXiv:2107.03374. 2021. Cobbe Karl, Kosaraju Vineet, Bavarian Mohammad, Chen Mark, Jun Heewoo, Kaiser Lukasz, Plappert Matthias, Tworek Jerry, Hilton Jacob, Nakano Reiichiro, others . Training verifiers to solve math word problems // arXiv preprint arXiv:2110.14168. 2021. Dettmers Tim, Pagnoni Artidoro, Holtzman Ari, Zettlemoyer Luke. Qlora: Efficient finetuning of quantized llms // Advances in neural information processing systems. 2023. 36. 1008810115. Devlin Jacob, Chang Ming-Wei, Lee Kenton, Toutanova Kristina. Bert: Pre-training of deep bidirectional transformers for language understanding // Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers). 2019. 41714186. Dziri Nouha, Lu Ximing, Sclar Melanie, Li Xiang Lorraine, Jiang Liwei, Lin Bill Yuchen, Welleck Sean, West Peter, Bhagavatula Chandra, Le Bras Ronan, others . Faith and fate: Limits of transformers on compositionality // Advances in Neural Information Processing Systems. 2023. 36. 7029370332. Edwards Carl, Wang Qingyun, Zhao Lawrence, Ji Heng. L+ m-24: Building dataset for language+ molecules@ acl 2024 // arXiv preprint arXiv:2403.00791. 2024. Fang Yin, Liang Xiaozhuan, Zhang Ningyu, Liu Kangwei, Huang Rui, Chen Zhuo, Fan Xiaohui, Chen Huajun. // arXiv preprint Mol-instructions: large-scale biomolecular instruction dataset for large language models arXiv:2306.08018. 2023. Frank Michael C. Bridging the data gap between children and large language models // Trends in Cognitive Sciences. 2023. 27, 11. 990992. Gloeckle Fabian, Idrissi Badr Youbi, Rozière Baptiste, Lopez-Paz David, Synnaeve Gabriel. Better & faster large language models via multi-token prediction // arXiv preprint arXiv:2404.19737. 2024. Goldman Omer, Caciularu Avi, Eyal Matan, Cao Kris, Szpektor Idan, Tsarfaty Reut. Unpacking tokenization: Evaluating text compression and its correlation with model performance // arXiv preprint arXiv:2403.06265. 2024. Grattafiori Aaron, Dubey Abhimanyu, Jauhri Abhinav, Pandey Abhinav, Kadian Abhishek, Al-Dahle Ahmad, Letman Aiesha, Mathur Akhil, Schelten Alan, Vaughan Alex, others . The llama 3 herd of models // arXiv preprint arXiv:2407.21783. 2024. Guo Daya, Yang Dejian, Zhang Haowei, Song Junxiao, Zhang Ruoyu, Xu Runxin, Zhu Qihao, Ma Shirong, Wang Peiyi, Bi Xiao, others . Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning // arXiv preprint arXiv:2501.12948. 2025. 13 Henikoff Steven, Henikoff Jorja G. Amino acid substitution matrices from protein blocks. // Proceedings of the National Academy of Sciences. 1992. 89, 22. 1091510919. Hu Edward J, Shen Yelong, Wallis Phillip, Allen-Zhu Zeyuan, Li Yuanzhi, Wang Shean, Wang Lu, Chen Weizhu, others . Lora: Low-rank adaptation of large language models. // ICLR. 2022. 1, 2. 3. Johnson Alistair EW, Bulgarelli Lucas, Shen Lu, Gayles Alvin, Shammout Ayad, Horng Steven, Pollard Tom J, Hao Sicheng, Moody Benjamin, Gow Brian, others . MIMIC-IV, freely accessible electronic health record dataset // Scientific data. 2023. 10, 1. 1. Jumper John, Evans Richard, Pritzel Alexander, Green Tim, Figurnov Michael, Ronneberger Olaf, Tunyasuvunakool Kathryn, Bates Russ, Žídek Augustin, Potapenko Anna, others . Highly accurate protein structure prediction with AlphaFold // nature. 2021. 596, 7873. 583589. Kortemme Tanja. De novo protein designFrom new structures to programmable functions // Cell. 2024. 187, 3. 526544. Lambert Nathan, Morrison Jacob, Pyatkin Valentina, Huang Shengyi, Ivison Hamish, Brahman Faeze, Miranda Lester James V, Liu Alisa, Dziri Nouha, Lyu Shane, others . T\" ulu 3: Pushing frontiers in open language model post-training // arXiv preprint arXiv:2411.15124. 2024. Li Jia, Beeching Edward, Tunstall Lewis, Lipkin Ben, Soletskyi Roman, Huang Shengyi, Rasul Kashif, Yu Longhui, Jiang Albert Q, Shen Ziju, others . Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions // Hugging Face repository. 2024a. 13. 9. Li Junyi, Tang Tianyi, Zhao Wayne Xin, Nie Jian-Yun, Wen Ji-Rong. Pre-trained language models for text generation: survey // ACM Computing Surveys. 2024b. 56, 9. 139. Li Peng, He Yeye, Yashar Dror, Cui Weiwei, Ge Song, Zhang Haidong, Fainman Danielle Rifinski, Zhang Dongmei, Chaudhuri Surajit. Table-gpt: Table-tuned gpt for diverse table tasks // arXiv preprint arXiv:2310.09263. 2023. Lightman Hunter, Kosaraju Vineet, Burda Yuri, Edwards Harrison, Baker Bowen, Lee Teddy, Leike Jan, Schulman John, Sutskever Ilya, Cobbe Karl. Lets verify step by step // The Twelfth International Conference on Learning Representations. 2023. Lin Chin-Yew, Hovy Eduard. Automatic evaluation of summaries using n-gram co-occurrence statistics // Proceedings of the 2003 human language technology conference of the North American chapter of the association for computational linguistics. 2003. 150157. Lindsey Jack, Gurnee Wes, Ameisen Emmanuel, Chen Brian, Pearce Adam, Turner Nicholas L., Citro Craig, Abrahams David, Carter Shan, Hosmer Basil, Marcus Jonathan, Sklar Michael, Templeton Adly, Bricken Trenton, McDougall Callum, Cunningham Hoagy, Henighan Thomas, Jermyn Adam, Jones Andy, Persic Andrew, Qi Zhenyi, Thompson T. Ben, Zimmerman Sam, Rivoire Kelley, Conerly Thomas, Olah Chris, Batson Joshua. On the Biology of Large Language Model // Transformer Circuits Thread. 2025. Liu Aixin, Feng Bei, Xue Bing, Wang Bingxuan, Wu Bochao, Lu Chengda, Zhao Chenggang, Deng Chengqi, Zhang Chenyu, Ruan Chong, others . Deepseek-v3 technical report // arXiv preprint arXiv:2412.19437. 2024a. Liu Zihan, Chen Yang, Shoeybi Mohammad, Catanzaro Bryan, Ping Wei. Acemath: Advancing frontier math reasoning with post-training and reward modeling // arXiv preprint arXiv:2412.15084. 2024b. Longpre Shayne, Hou Le, Vu Tu, Webson Albert, Chung Hyung Won, Tay Yi, Zhou Denny, Le Quoc V, Zoph Barret, Wei Jason, others . The flan collection: Designing data and methods for effective instruction tuning // International Conference on Machine Learning. 2023. 2263122648. Luo Ziyang, Xu Can, Zhao Pu, Sun Qingfeng, Geng Xiubo, Hu Wenxiang, Tao Chongyang, Ma Jing, Lin Qingwei, Jiang Daxin. WizardCoder: Empowering Code Large Language Models with Evol-Instruct. 2023. Meyers Joshua, Fabian Benedek, Brown Nathan. De novo molecular design and generative models // Drug discovery today. 2021. 26, 11. 27072715. Miller Frederic P, Vandome Agnes F, McBrewster John. Levenshtein distance: Information theory, computer science, string (computer science), string metric, damerau? Levenshtein distance, spell checker, hamming distance. 2009. Mirdita Milot, Schütze Konstantin, Moriwaki Yoshitaka, Heo Lim, Ovchinnikov Sergey, Steinegger Martin. ColabFold: making protein folding accessible to all // Nature methods. 2022. 19, 6. 679682. Needleman Saul B, Wunsch Christian D. general method applicable to the search for similarities in the amino acid sequence of two proteins // Journal of molecular biology. 1970. 48, 3. 443453. Ouyang Long, Wu Jeffrey, Jiang Xu, Almeida Diogo, Wainwright Carroll, Mishkin Pamela, Zhang Chong, Agarwal Sandhini, Slama Katarina, Ray Alex, others . Training language models to follow instructions with human feedback // Advances in neural information processing systems. 2022. 35. 2773027744. Radford Alec, Narasimhan Karthik, Salimans Tim, Sutskever Ilya, others . Improving language understanding by generative pre-training.(2018). 2018. Rogers David, Hahn Mathew. Extended-connectivity fingerprints // Journal of chemical information and modeling. 2010. 50, 5. 742754. Rost Burkhard. Twilight zone of protein sequence alignments // Protein engineering. 1999. 12, 2. 8594. Sennrich Rico, Haddow Barry, Birch Alexandra. Neural machine translation of rare words with subword units // arXiv preprint arXiv:1508.07909. 2015. Shannon Claude E. mathematical theory of communication // The Bell system technical journal. 1948. 27, 3. 379423. Singh Aaditya K, Strouse DJ. Tokenization counts: the impact of tokenization on arithmetic in frontier llms // arXiv preprint arXiv:2402.14903. 2024. Stern Mitchell, Shazeer Noam, Uszkoreit Jakob. Blockwise parallel decoding for deep autoregressive models // Advances in Neural Information Processing Systems. 2018. 31. Toshniwal Shubham, Du Wei, Moshkov Ivan, Kisacanin Branislav, Ayrapetyan Alexan, Gitman Igor. OpenMathInstruct2: Accelerating AI for Math with Massive Open-Source Instruction Data // arXiv preprint arXiv:2410.01560. 2024. Vaswani Ashish, Shazeer Noam, Parmar Niki, Uszkoreit Jakob, Jones Llion, Gomez Aidan N, Kaiser Łukasz, Polosukhin Illia. Attention is all you need // Advances in neural information processing systems. 2017. 30. Wadden David, Shi Kejian, Morrison Jacob, Naik Aakanksha, Singh Shruti, Barzilay Nitzan, Lo Kyle, Hope Tom, Soldaini Luca, Shen Shannon Zejiang, others . Sciriff: resource to enhance language model instruction-following over scientific literature // arXiv preprint arXiv:2406.07835. 2024. Wolf Thomas, Debut Lysandre, Sanh Victor, Chaumond Julien, Delangue Clement, Moi Anthony, Cistac Pierric, Rault Tim, Louf Rémi, Funtowicz Morgan, others . Transformers: State-of-the-art natural language processing // Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations. 2020. 3845. Xu Can, Sun Qingfeng, Zheng Kai, Geng Xiubo, Zhao Pu, Feng Jiazhan, Tao Chongyang, Lin Qingwei, Jiang Daxin. WizardLM: Empowering large pre-trained language models to follow complex instructions // The Twelfth International Conference on Learning Representations. 2024. Yu Longhui, Jiang Weisen, Shi Han, Yu Jincheng, Liu Zhengying, Zhang Yu, Kwok James T, Li Zhenguo, Weller Adrian, Liu Weiyang. MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models // arXiv preprint arXiv:2309.12284. 2023. Zhang Yang, Skolnick Jeffrey. Scoring function for automated assessment of protein structure template quality // Proteins: Structure, Function, and Bioinformatics. 2004. 57, 4. 702710. Zhao Wenting, Ren Xiang, Hessel Jack, Cardie Claire, Choi Yejin, Deng Yuntian. Wildchat: 1m chatgpt interaction logs in the wild // arXiv preprint arXiv:2405.01470. 2024. Zheng Lianmin, Chiang Wei-Lin, Sheng Ying, Zhuang Siyuan, Wu Zhanghao, Zhuang Yonghao, Lin Zi, Li Zhuohan, Li Dacheng, Xing Eric. P, Zhang Hao, Gonzalez Joseph E., Stoica Ion. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. 2023."
        },
        {
            "title": "A Auxiliary head training dataset details",
            "content": "In the absence of the original training recipe of most open-source models, we construct an instruction-tuning dataset of 100,000 samples, sourced from the ShareGPT dataset and the Tulu 3 SFT mixture (Lambert et al., 2024). The latter is combination of many data sources, which are broken down in Table 6. This dataset aims to encompass broad spectrum of tasks and response formats that the base model has already been trained on, such as multi-turn conversations, mathematics, coding, and instruction following. The ShareGPT dataset is leveraged for its extensive multi-turn conversations, while the Tulu 3 SFT mixture is used for its diverse tasks and proven effectiveness in posttraining. Note that only the questions from these datasets are used; the \"ground truth\" responses are distilled from the given original model to prevent large distribution shifts. Table 6: Sources for Auxiliary Head Training Dataset. Source Zheng et al. (2023) Dataset ShareGPT Tulu 3 Persona MATH Lambert et al. (2024) Evol CodeAlpaca WildChat GPT-4 FLAN v2 NuminaMath-TIR Tulu 3 Persona GSM OpenMathInstruct-2 Tulu 3 Persona Python Lambert et al. (2024) Lambert et al. (2024) Tulu 3 Persona IF Wadden et al. (2024) SciRIFF Li et al. (2023) TableGPT Total Luo et al. (2023) Zhao et al. (2024) Longpre et al. (2023) Li et al. (2024a) Lambert et al. (2024) Toshniwal et al. (2024) Samples Used 42,239 12,550 8,933 8,339 7,425 5,311 4,226 4,175 2,939 2,501 823 429 100, Importantly, this post-training phase for the auxiliary heads is relatively computationally inexpensive. For reference, the full Tulu 3 SFT mixture contains 939,344 samples. Additionally, given that only the auxiliary heads are adapted during training, training time and memory usage are further reduced."
        },
        {
            "title": "B Understanding the CAFT hyperparameters",
            "content": "The introduction of the CAFT hyperparameters α, β, and γ is key to CAFTs effectiveness. Specifically, they ensure that models leverage the additional information provided by the auxiliary heads, while ensuring they focus on optimizing L1. Figure 5 visualizes how they scale auxiliary losses over iterations, assuming α = 0.8, β = 0.01, and γ is defined by the reflected sine schedule, which is formally: γt = sin((1 ) π 2 ) (5) where is the current step and is the global number of steps. The influence of the auxiliary losses peaks at the start of training and declines over iterations. Additionally, the auxiliary losses are much smaller than the primary L1, as evidenced by the scaling range of 0.01 to 0. These help ensure that the influence of Ln does not overpower that of L1."
        },
        {
            "title": "C Experiment Settings",
            "content": "C.1 Auxiliary Head Training We train four auxiliary heads for Llama-3-8B-Instruct using the aforementioned training dataset for 4 epochs, with maximum sequence length of 4096 and batch size of 64. For optimization, the 8-bit AdamW optimizer is used with peak learning rate of 1e-4. To improve training dynamics, 300 warmup steps and cosine scheduler are implemented. 16 Figure 5: Scaling of training losses of auxiliary heads for iterations = 1000, α = 0.8, β = 0.01, and γ. C.2 Full and LoRA Concept-Aware Fine-Tuning (CAFT) Model Training. All models are trained for 5 epochs with early stoppage and using the AdamW optimizer with cosine scheduler. 50 warm-up steps are used. Several hyperparameters are task-specific: For sequence length, 512 is used for mathematics, coding, molecular generation, and protein design; 2048 for BHC summarization. Additionally, for BHC summarization, molecular generation, and protein design, the auxiliary heads are specifically trained for 1 epoch before the main fine-tuning. In general, CAFT is more robust to hyperparameters than conventional fine-tuning. Because CAFT affords more information during training, there is lower risk of overfitting: Higher learning rates and LoRA rank & dropout can be used for better performance. For example, full fine-tunings optimal learning rate is lower than full CAFTs. In practice, we recommend that practitioners start with the same hyperparameters as conventional fine-tuning, and then adjust accordingly if underfitting is observed. Table 7: Hyperparameters for Full and LoRA concept-aware vs. conventional fine-tuning. LoRA FT LoRA CAFT Full FT Full CAFT Epochs Peak LR Batch Size LoRA Rank LoRA Alpha LoRA Dropout CAFT α CAFT β 5 1e-5 32 8 16 0.10 - - 5 1e-5 32 8 16 0.10 0.8 0. 5 5e-6 32 - - - - - 5 1e-5 32 - - - 0.8 0.01 For the CAFT-specific hyperparameters, the objective is to incentivize models to leverage the auxiliary losses and ultimately optimize for L1. We conducted an extensive hyperparameter search and found the following settings to be robust across all tasks. γ is defined using reflected sine schedule. Empirically, we find that this performs better than constant or sine schedule. Other hyperparameters are shown in Table 7. Additionally, when doing hyperparameter search, we find that α = 0.8 performs better than 0.7 and 0.9, while β = 0.01 performs better than 0.05 and 0.10. In practice, these values do not need to be further tuned. Datasets. Every dataset contains 10,000 training samples. The test set sizes are as follows: 164 for HumanEval, 500 for MATH-500, 500 for BHC summarization, 1000 for L+P-24, and 500 for Mol-Instructions protein design. If the original dataset contains more samples than mentioned above, random subset is extracted. Additionally, small number of samples are filtered out from the datasets if they do not fit into their respective maximum sequence lengths reported above. 17 C.3 Evaluation All base, next-token fine-tuned, and CAFT models are provided zero-shot prompt that gives explicit instructions on the desired output format. Models are asked to generate the relevant code, text, or sequence only, except for MATH-500, where models are asked to think step-by-step\". To control for noise, 5 independent runs are done for every dataset; their average metrics are reported. Given that early stoppage is used, models are also evaluated on earlier checkpoints, and the top-performing one is reported. All the downstream tasks are deterministic, where precision is more important than diversity. Thus, temperature of 0.1 is used as the default sampling strategy. Only the protein design evaluation deviates from this: all models tend to become repetitive. As such, temperature of 0.3 and repetition penalty of 1.1 are used for this task."
        }
    ],
    "affiliations": [
        "Nanyang Technological University Singapore"
    ]
}