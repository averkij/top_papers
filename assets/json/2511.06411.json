{
    "paper_title": "SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization",
    "authors": [
        "Zhi Zheng",
        "Wee Sun Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The soft-thinking paradigm for Large Language Model (LLM) reasoning can outperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in some scenarios, underscoring its research and application value. However, while the discrete-token CoT reasoning pattern can be reinforced through policy optimization algorithms such as group relative policy optimization (GRPO), extending the soft-thinking pattern with Reinforcement Learning (RL) remains challenging. This difficulty stems from the complexities of injecting stochasticity into soft-thinking tokens and updating soft-thinking policies accordingly. As a result, previous attempts to combine soft-thinking with GRPO typically underperform their discrete-token GRPO counterparts. To fully unlock the potential of soft-thinking, this paper presents a novel policy optimization algorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning pattern. SofT-GRPO injects the Gumbel noise into logits, employs the Gumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained embedding space, and leverages the reparameterization trick in policy gradient. We conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and results demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly outperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while exhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes and weights are available on https://github.com/zz1358m/SofT-GRPO-master"
        },
        {
            "title": "Start",
            "content": "SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization Zhi Zheng 1 Wee Sun Lee 1 5 2 0 2 9 ] . [ 1 1 1 4 6 0 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The soft-thinking paradigm for Large Language Model (LLM) reasoning can outperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in some scenarios, underscoring its research and application value. However, while the discrete-token CoT reasoning pattern can be reinforced through policy optimization algorithms such as group relative policy optimization (GRPO), extending the soft-thinking pattern with Reinforcement Learning (RL) remains challenging. This difficulty stems from the complexities of injecting stochasticity into soft-thinking tokens and updating soft-thinking policies accordingly. As result, previous attempts to combine soft-thinking with GRPO typically underperform their discrete-token GRPO counterparts. To fully unlock the potential of soft-thinking, this paper presents novel policy optimization algorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning pattern. SofT-GRPO injects the Gumbel noise into logits, employs the Gumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained embedding space, and leverages the reparameterization trick in policy gradient. We conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and results demonstrate that SofT-GRPO enables softthinking LLMs to slightly outperform discretetoken GRPO on Pass@1 (+0.13% on average accuracy), while exhibiting substantial uplift on Pass@32 (+2.19% on average accuracy). 2 (a) Reasoning with Discrete-Token CoT (b) Reasoning with the Soft-Thinking Pattern (c) GRPO (Shao et al., 2024) for Discrete-Token CoT (d) Existing Work of Soft-Thinking + GRPO (Butt et al., 2025) 1. Introduction Reasoning with large language models (LLMs) has demonstrated impressive versatility across diverse domains 1School of Computing, National University of SingaCorrespondence to: Wee Sun Lee pore, Singapore. <leews@comp.nus.edu.sg>. 2Codes and weights are available on https://github. com/zz1358m/SofT-GRPO-master (e) SofT-GRPO (Ours) for Reinforcing Soft-Thinking Figure 1. The soft-thinking pattern (b) passes the expectation of embeddings to the next LLM step (Zhang et al., 2025b), which can surpass the conventional discrete-token CoT (a) without any finetuning. However, employing the GRPO algorithm (c) will boost the performance of discrete-token CoT, but existing attempts (d) of applying RLVR to soft-thinking derive inferior performances. The proposed SofT-GRPO (e) provides the first valid RLVR algorithm, which can outperform the discrete-token CoT with GRPO. 1 Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization (Sprague et al., 2024). However, most existing reasoning methods rely on generating discrete tokens, which may limit their ability to represent certain abstract concepts (Zhang et al., 2025b). In pursuit of better expressing abstract ideas and even implementing possible latent search tree (Wu et al., 2025a), Zhang et al. (2025b) presents the soft-thinking reasoning pattern. The soft-thinking reasoning pattern replaces each discrete token in the chain-of-thought (CoT) with continuous representation: weighted sum of ddimensional token embeddings, computed from their output probabilities, which is then input to the next LLM decoding step. With suitable sampling techniquessuch as the Gumbel-Softmax technique (Wu et al., 2025a) or the Dirichlet resampling technique (Zhuang et al., 2025)softthinking can outperform conventional discrete-token CoT on wide range of tasks, without requiring any fine-tuning (Wu et al., 2025a). Recently, growing body of research has focused on Reinforcement Learning with Verifiable Rewards (RLVR) approaches to further enhance the effectiveness of discretetoken CoT reasoning (Wang et al., 2024; Liu et al., 2025a; Yu et al., 2025). Among these approaches, Group Relative Policy Optimization (GRPO) (Shao et al., 2024) has emerged as particularly compelling framework. GRPO operates by sampling groups of CoT trajectories for each query and updating the policy to favor higher-reward trajectories, yielding substantial improvements on various benchmarks. Notably, discrete-token GRPO can consistently outperform the possible advantage brought about by applying the soft-thinking pattern on original LLMs. In contrast, initial attempts to synergize soft-thinking with GRPO typically underperform in comparison to their discrete-token counterparts (Butt et al., 2025). As shown in 1 (b), different from discrete-token reasoning processes, tokens in soft-thinking reasoning are deterministic without randomness. So, this shortfall can be attributed to challenges of 1) introducing controllable stochasticity within soft-thinking tokens for reasoning path exploration, as well as 2): effectively updating the soft-thinking policies by exploiting high-quality samples. These challenges highlight the need for specialized policy optimization algorithms that fully unlock the latent potential of soft-thinking reasoning in LLMs. To address these challenges and unlock the promise of softthinking, we propose novel policy optimization algorithm, SofT-GRPO, specifically designed to reinforce the softthinking reasoning ability in LLMs. As shown in Figure 1(e), in the rollout process, SofT-GRPO samples groups of soft-thinking reasoning paths by injecting sampled Gumbel noises into the output probabilities and employs the GumbelSoftmax technique to avoid invalid inputs outside the pretrained discrete-token embedding space. For the following soft-thinking policy updates, SofT-GRPO leverages the reparameterization trick on the Gumbel distribution. It can achieve accurate gradient estimation and accurately attribute the reward improvement to the output probability of LLMs. We conduct thorough evaluations of SofT-GRPO on three representative LLM architecturesDeepSeek-R1-DistillQwen-1.5B, LLaMA-3.2-3B-Instruct, and DeepSeek-R1Distill-Qwen-7Bacross five numerical reasoning benchmarks. Experimental results consistently show that SofTGRPO-enhanced soft-thinking not only slightly surpasses discrete-token GRPO on Pass@1, but also yields substantial gains on Pass@32, highlighting the practical advantages of robust policy optimization for soft-thinking reasoning. Our contributions can be summarized as follows: We introduce SofT-GRPO, novel and powerful policy optimization algorithm designed for reinforcing the soft-thinking reasoning paradigm in LLMs. It integrates the Gumbel-Softmax technique into the group rollout process, actively obtaining diverse but valid soft-thinking reasoning paths. We propose an innovative gradient estimation approach via Gumbel reparameterization, enabling precise attribution of improvements to the LLMs output probability distributions in policy optimization. We conduct comprehensive experiments across LLMs of 1.5B7B parameters on five in-domain benchmarks and three out-of-domain benchmarks, demonstrating that SofT-GRPO consistently outperforms the discrete-token GRPO baselines, especially at higher sample rates (Pass@16 or Pass@32). 2. Preliminaries 2.1. Discrete-Token CoT Reasoning Discrete-token CoT reasoning processes seek to solve Q-token question = (q1, . . . , qQ) by generating reasoning CoT tokens = (r1, . . . , rR) before outputting the answer prediction = (a1, . . . , aA) (Guo et al., 2025; Sprague et al., 2024). All these tokens are in the discrete language domain, i.e., Q, R, , where represents the set of language tokens. Language reasoning tokens and answer tokens are generated with the next-token prediction (NTP) policy of LLM πθ as follows: p(R, AQ) = (cid:89) t=1 πθ(rt[Q, (r1, . . . , rt1)]) (cid:89) t=1 πθ(at[Q, R, (a1, . . . , at1)]), (1) where [, , ] denotes concatenation. Supervised Fine-tuning (SFT) for Discrete-Token CoT LLM Reasoning. As straightforward fine-tuning methods 2 (2) (3) Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization JGRPO(θ) ="
        },
        {
            "title": "1\nG",
            "content": "E QD,{R}G g=1,{A}G g=1p(,Q) (cid:34) (cid:88) g= 1 Rg + Ag Rg+Ag (cid:88) (cid:16) (cid:16) min pg,t ˆAg, clip(pg,t, 1 ϵ, 1 + ϵ) ˆAg (cid:17) (cid:35) βDKL(πθπθref) t=1 ˆAg = (At) mean(f (A))G std(f (A))G g=1 g=1 , pg,t = (cid:40) πθ(ag,t[Q,Rg,(ag,1,...,ag,t1)]) πθold (ag,t[Q,R,(ag,1,...,ag,t1)]) πθ(rg,t[Q,(rg,1,...,rg,t1)]) πθold (rg,t[Q,(rg,t,...,rg,t1)]) if > Rg if Rg, for LLM reasoning, SFT methods (Wu et al., 2025b; Zheng & Lee, 2025) first collect high-quality CoT labels [R, A] for each question Q. Then LLMs are fine-tuned for correct predictions on each of the + tokens. However, SFT methods for LLM reasoning highly rely on the quality of CoT labels, which are difficult to obtain for complex datasets. Moreover, there are also concerns about the outof-domain generalization ability of SFT (Chu et al., 2025). RLVR Fine-tuning for Discrete-Token Reasoning. RLVR fine-tuning methods such as GRPO (Liu et al., 2024), Dr. GRPO (Liu et al., 2025a), DAPO (Yu et al., 2025), and Lite PPO (Liu et al., 2025b) sample several CoTs [R, A] and assign reward to each of them based on the quality of the answers A. The standard discrete-token GRPO algorithm (Shao et al., 2024) samples CoTs for each question and optimizes the reasoning policy towards the CoTs with higher rewards. The loss function of GRPO is shown in Eq. (2), where ˆAg represents the advantage function for the g-th CoT, the reward function (At) = 1 if and only if the answer At is correct, ϵ is the clipping hyperparameter, πθold is the policy before the update, and DKL represents the KL-divergence of policy πθ and reference model policy πθref. Employing RLVR methods, the average performance of LLMs in numerical reasoning problems can be improved without the need for labels (Yue et al., 2025). 2.2. Soft-Thinking Paradigm Conventional discrete-token reasoning is constrained to selecting single token from the token set at each of the CoT reasoning steps. This approach may hinder the models ability to express certain abstract concepts that cannot be easily represented by single deterministic token (Zhang et al., 2025b). To enhance the models capability to represent abstract concepts, Zhang et al. (2025b) presents another paradigm called soft-thinking. The proposed soft-thinking paradigm replaces discrete-token reasoning steps with soft-thinking reasoning path = (s1, . . . , sS). Each token si Rd is real-valued vector, which is calculated as the weighted sum of token embeddings with their output probabilities. The weighted sum embeddings within the vector space of token embeddings are then fed into the next LLM step as follows: pt πθ(Q, (s1, . . . , st1)])), st = (cid:88) i=1 pi ei, where pi [0, 1] is the predicted probability of token in pt and ei Rd is the LLM embeddings of token i. The soft-thinking paradigm may inherently implement multithread latent search tree in the serial decoding process (Wu et al., 2025a). Based on Eq. 3, Wu et al. (2025a) find that most pre-trained LLMs tend to be single-thread, and Wu et al. (2025a) propose to introduce sampling strategies in the soft-thinking reasoning process for randomness, employing methods such as the Gumbel-Softmax technique (Jang et al., 2016) based on the output probability pt as follows: exp(gi/τg) i=1 exp(gi/τg) (cid:80)T , (4) gi = log pi + ϵi, yi = st = (cid:88) i=1 yi ei, where ϵi is scaler noise sampled from the Gumbel distribution Gumbel(0, 1), and τg is the temperature of GumbelSoftmax. Besides using Gumbel-Softmax. Wu et al. (2025a) also tries to use the Dirichlet resampling technique as follows: 1 B(α pt) (cid:89) xαpi1 (5) (x1, . . . , xT ) (pt) = st = (cid:88) i=1 xi ei, where α is scaling parameter. Empirically, cooperated with the Gumbel-Softmax technique for randomness, the soft-thinking pattern can outperform conventional discretetoken CoT on broad range of tasks, including numerical reasoning, code reasoning, and scientific reasoning, without requiring any fine-tuning. (Wu et al., 2025a). Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization Figure 2. The pipeline of the proposed SofT-GRPO algorithm. In training with Query Q, the SofT-GRPO first generates group of soft-thinking reasoning paths with Gumbel noises and the Gumbel-Softmax technique (Jang et al., 2016). We transmit the value and for the loss calculation afterward. Then, we reconstruct the soft-thinking input. Finally, we update the soft-thinking policy with the off-policy REINFORCE (Williams, 1992) algorithm, optimizing the soft-thinking reasoning tokens with Gumbel reparameterization. 2.3. Attempts of RLVR on Soft-Thinking However, after being boosted with the RLVR fine-tuning, discrete-token CoT will turn to clearly outperform the softthinking reasoning. So, Butt et al. (2025) tries to similarly improve the performance of soft-thinking using GRPO. Butt et al. (2025) adds Gaussian noise on the input st in Eq. (3) as follows: ˆst = st + (0, σ2Id). (6) In deriving the off-policy soft-thinking probability in GRPO, they restore the value of ˆst in the rollout and calculate the log probability for the soft-thinking reasoning path with the Gaussian reparameterization trick as follows, keeping the updating process of the answering part unchanged: (7) log p(ˆst) = 2 + Constant. 1 2σ2 ˆst st2 However, we observe that there are two drawbacks to this method. 1): Although it may help avoid inputs outside the pre-trained discrete-token embedding space, adding noise to inputs instead of logits is not direct and may theoretically mismatch the LLM predictions. As analyzed in Appendix C.1, the embeddings of each token may be linearly dependent, so it becomes hard to attribute which probability pi may contribute to effectiveness. Moreover, the added noise may even be impossible to represent by the embeddings. 2): The methods in (Butt et al., 2025) do not use any advanced sampling methods for effectiveness in training (e.g., incorporating Dirichlet resampling or Gumbel Softmax technique (Wu et al., 2025a)), which will undermine the performance of sampled soft-thinking reasoning paths. So empirically, although observing improvements compared to discrete-token GRPO fine-tuning on the Pass@32 metrics (i.e., the pass rate with 32 attempts), there is severe degradation on the average accuracy (Butt et al., 2025). As result, existing attempts of applying GRPO to soft-thinking will not keep its advantage over the discrete-token CoT under the no-finetune setting. 3. SofT-GRPO: Reinforcing Soft-Thinking Policy with Gumbel Reparameterization To fully unlock the potential of the soft-thinking paradigm and establish reliable framework to reinforce it, we present an effective RLVR algorithm for soft-thinking policies, SofT-GRPO. As shown in Figure 2, the proposed SofTGRPO first samples soft-thinking CoTs with controllable randomness using the Gumbel-Softmax technique (Wu et al., 2025a; Jang et al., 2016). In the following policy update stage, we propose novel SofT-GRPO loss function using the Gumbel reparameterization trick. 3.1. Group Rollout with Gumbel Noise As an off-policy RLVR algorithm, for each Query Q, SofTGRPO samples and restores group of soft-thinking CoTs in the rollout stage in parallel. As discussed in Section 2.2, advanced sampling methods will bring performance imSurpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization JSofT-GRPO(θ) ="
        },
        {
            "title": "1\nG",
            "content": "E QD,{S}G g=1,{A}G g=1p(,Q) (cid:34) (cid:88) g= 1 St + At St+At (cid:88) (cid:16) (cid:16) min pg,t ˆAg, clip(pg,t, 1 ϵ, 1 + ϵ) ˆAg (cid:17) βDKL(πθπθref ) (cid:35) t=1 (8) log pg,t = (cid:40) log πθ(ag,t[Q,S,(ag,1,...,ag,t1)]) πθold (ag,t[Q,S,(ag,1,...,ag,t1)]) (cid:80)T i=1 log pi) exp(g (cid:0) (g + log pi)(cid:1) (cid:0) ϵi exp(ϵi)(cid:1) if > Sg if Sg, provement together with stochasticity. So, to explore diverse and powerful soft-thinking reasoning paths, we introduce the Gumbel-Softmax resampling technique (shown in Eq. (4)) in the rollout process as follows: (p1, . . . , pT ) = πθold ([Q, (s1, . . . , st1)]), where st is built from Eq. (9). When doing the on-policy sampling in SofT-GRPO, we reconstruct the group of softthinking reasoning paths = (s1, . . . , sS) from the restored values in the rollout process in Eq. (9). Then, the log prob is generated with the restored values as follows: exp(g i/τg) i=1 exp(g (cid:80)T i/τg) , (9) = log pi + ϵi, = st = (cid:88) i=1 ei, where ei is the d-dimensional embedding of the i-th token, πθold is the old LLM policy used in rollout, and ϵi is the Gumbel noise. Using the inverse transform sampling, we can sample ϵi by computing = log( log(u)) where Uniform(0, 1) (Jang et al., 2016). Different from adding noise in other distributions, the Gumbel Softmax technique can ensure the stability of the added Gumbel Noise. As shown in Theorem 3.1 (Refer to Appendix C.2 for proof), the Gumbel Softmax will preserve the distribution of LLMs output probabilities after adding noise for stochasticity. So, it may simulate the multinomial sampling and reduce the risk of making st drop outside the pre-trained discrete-token embedding space. Theorem 3.1 (Gumbel-max Trick). Let (p1, . . . , pn) be nonnegative, and ϵ1, . . . , ϵn independent samples from Gumbel(0, 1) (Maddison et al., 2016), (cid:16) Pr = arg max (ϵi + log pi) (cid:17) = pj (cid:80)n i=1 pi . (10) 3.2. Gumbel Reparameterization for Loss Function When fine-tuning with the conventional discrete-token GRPO, as represented in Eq. (1), the probability of sampled trajectories can be easily obtained from the output Categorical distributions. However, calculating such probability is more difficult in soft-thinking (Jain & Rappazzo, 2025). In the rollout process, we estimate the probability with the reparameterization trick over Gumbel noises as follows: log p(st[Q, (s1, . . . , st1)], θold) = (cid:88) i=1 ϵi exp(ϵi), (11) 5 log p(st[Q, (s1, . . . , st1)], θ) = (cid:88) (g log pi) exp((g log pi)), (12) where i=1 (p1, . . . , pT ) = πθ([Q, (s1, . . . , st1)]). Finally, we can represent the total loss of the proposed SofTGRPO as shown in Eq. (8) 3.3. Detailed Settings We follow the general setting in soft-thinking works (Wu et al., 2025a), enabling both the top-p and top-k sampling strategies. In both training and inference, we set top-p as 0.95 and top-k as 5, the temperature of LLMs τ = 0.6, and the temperature in Gumbel Softmax τg = 0.1. 4. Experiments In this section, we implement the proposed SofT-GRPO algorithm to reinforce the soft-thinking reasoning of three LLMs, including DeepSeek-R1-Distill-Qwen-1.5B, LLaMA3.2-3B-Instruct, and DeepSeek-R1-Distill-Qwen-7B. 4.1. Implementation Detail Training & Testing Settings We employ DeepScaler (Luo et al., 2025) as the training dataset, which contains In implementing SofT-GRPO, we use 40,315 queries. SGLang (Zheng et al., 2024)1 for the rollout process and the verl-0.4.x framework (Sheng et al., 2024) for policy update. We involve five famous numerical benchmarks as test sets, i.e., AIME2024, AIME2025, AMC23, MATH-500, and GSM8K. The maximum generation length is confined to 8192 in training & validating, and 32768 in testing. Test answers are verified using the Math Verify package (Kydlıˇcek, 1Specifically, we modify the rollout process based on the SGLang implementation in https://github.com/ eric-ai-lab/Soft-Thinking Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization Table 1. Experiment results of baselines and the proposed Latent-RPC on five numerical reasoning benchmarks. We cover 3 base LLMs from 1.5B to 7B and two reasoning patterns, i.e., discrete-token CoT and soft-thinking reasoning. @1 metrics denote the Mean@32 values, where we run each method 32 times on the dataset for average Pass@1 accuracies. @16 and @32 denote the Pass@16 and Pass@32 values on the dataset, respectively. We multiply all the results by 100 to highlight the differences between results. The best result on each metric and dataset is underlined, the best average result is bolded, and the second-best average result is shaded. Dataset Metrics AIME2024 AIME2025 AMC23 MATH-500 GSM8K Average @1 @16 @32 @1 @16 @32 @1 @16 @32 @1 @16 @32 @1 @16 @32 @1 @16 @32 No-Finetune + GRPO 30.6 70.0 31.8 66.7 73.3 23.0 46.7 76.7 25.3 46.7 97.8 81.5 95.8 97.8 84.9 95.1 96.7 58.09 80.54 83.23 95.8 61.28 80.16 82. DeepSeek-R1-Distill-Qwen-1.5B Base LLM Discrete-Token CoT Reasoning Pattern 95.0 84.6 97.8 53.3 70.7 92.5 95.0 87.1 97.4 46.7 77.3 95.0 27.3 66.7 No-Finetune 29.2 70.0 + GRPO + SofT-GRPO 32.6 76.7 70.0 23.8 46.7 73.3 25.4 46.7 80.0 26.1 50.0 53.3 69.9 95.0 53.3 75.8 95.0 53.3 76.4 97.5 95.0 79.4 93.2 95.0 86.3 96.8 97.5 86.3 97.4 96.6 81.0 94.6 98.2 84.9 95.6 98.0 85.5 96. 97.1 56.28 79.23 82.41 96.4 60.31 80.81 83.26 97.0 61.39 83.54 85.18 Soft-Thinking Reasoning Pattern No-Finetune + GRPO 4.4 7.3 20.0 23.3 26.7 26. 3.4 No-Finetune + GRPO 8.0 + SofT-GRPO 7.7 16.7 20.0 23.3 16.7 23.3 26.7 0.3 0.5 0.2 0.7 0.3 LLaMA-3.2-3B-Instruct Base LLM Discrete-Token CoT Reasoning Pattern 75.0 38.1 75.6 1.0 67.5 48.3 77.2 3. 18.3 65.0 27.3 62.5 Soft-Thinking Reasoning Pattern 0.3 3.3 84.0 67.9 92.1 82.6 79.6 95.4 94.6 25.79 50.61 56.26 96.5 32.60 52.35 55.32 6.7 3.3 10. 6.7 17.6 70.0 10.0 27.3 70.0 10.0 31.3 67.5 77.5 36.7 76.0 75.0 47.8 76.8 67.5 47.2 77.6 81.4 66.9 91.6 81.8 79.2 94.8 83.4 77.6 96.4 94.7 24.96 52.18 55.39 96.3 32.60 53.00 57.28 97.7 32.83 54.96 57.06 No-Finetune + GRPO 55.7 80.0 54.0 80. 80.0 39.4 66.7 80.0 40.4 66.7 DeepSeek-R1-Distill-Qwen-7B Base LLM Discrete-Token CoT Reasoning Pattern 97.5 93.6 98.8 66.7 89.9 97.5 95.0 93.5 98.4 70.0 89.2 95.0 Soft-Thinking Reasoning Pattern 99.2 89.5 96.7 99.0 91.4 96.1 97.0 73.62 87.94 88.07 96.6 73.69 87.24 88.12 55.3 80.0 No-Finetune + GRPO 55.5 80.0 + SofT-GRPO 53.2 80. 83.3 39.2 66.7 80.0 37.8 63.3 83.3 40.4 60.0 70.0 90.2 95.0 66.7 90.1 97.5 73.3 89.6 97.5 97.5 93.3 98.8 97.5 93.6 98.6 97.5 93.3 98.6 99.0 89.3 96.6 99.0 91.8 96.7 99.0 92.1 97.2 97.0 73.44 87.41 89.38 96.9 73.76 87.23 88.01 97.7 73.74 86.66 90.18 2025). All experiments in this paper are implemented on node of 8 NVIDIA H200 GPUs (141 GB VRAM each). With the 64 batch size and 1e-6 learning rate, running SofT-GRPO on 1.5B LLM will take approximately 45 hours. Baselines We include discrete-token GRPO trained on the same dataset and base LLMs without fine-tuning as baselines. In comparing the results of baselines and the proposed SofT-GRPO, we cover two reasoning patterns, i.e., conventional discrete-token CoT reasoning and soft-thinking reasoning. For baselines trained with discrete tokens, we implement their soft-thinking reasoning pattern with the effective Gumbel-Softmax technique (the temperature τg = 0.5) proposed in Wu et al. (2025a). Under both patterns, we follow the hyperparameters in (Wu et al., 2025a), setting the temperature τ of LLMs to 0.6 (we discuss this setting in Appendix D.2), top-p to 0.95, and top-k to 30. Metrics In evaluating the performances of LLMs, we cover the general metrics of Mean@32, Pass@16, and 6 Table 2. Comparison of the proposed SofT-GRPO to the RLVR fine-tuning method proposed in (Butt et al., 2025). Due to the difficulty in reproduction, we compare to their reported results under the same training dataset. GRPO* represents the reported result of GRPO in its paper, and Soft Tokens* represents its reported results fine-tuned under the soft-thinking pattern. Dataset Metrics MATH-500 @ @32 GSM8K @1 @32 LLaMA-3.2-3B-Instruct Base LLM Discrete-Token CoT Reasoning Pattern No-Finetune + GRPO + GRPO* + Soft Tokens* + Soft Tokens* + SofT-GRPO 38.1 48.3 48.3 37.4 84.0 82.6 78.1 80.6 67.9 79.6 79.5 71.0 Soft-Thinking Reasoning Pattern 75.5 77.6 (+2.1) 41.3 47.2 (+5.9) 77.9 83.4 (+5.5) 94.6 96.5 96.6 98.0 95.2 97.7 (+2.5) Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization Table 3. Average accuracies on out-of-domain datasets. We cover GPQA Diamond, HumanEval, and MBPP. @1 metrics denote the Mean@32 values, where we run the methods 32 times on the dataset for average Pass@1 accuracies. @8, @16, and @32 denote the Pass@8, Pass@16, and Pass@32 values on the dataset, respectively. Dataset Metrics GPQA Diamond HumanEval @1 @8 @16 @32 @1 @8 @16 @32 @1 @8 @16 @32 MBPP Average @1 @ @16 @32 DeepSeek-R1-Distill-Qwen-1.5B Base LLM Discrete-Token CoT Reasoning Pattern No-Finetune + GRPO No-Finetune + GRPO + SofT-GRPO 36.7 35.4 36.0 36.5 37. 84.3 77.8 83.8 81.3 82.8 92.4 88.4 91.9 91.4 89.9 96.0 93.4 97.0 94.4 95. 68.1 72.2 67.2 71.8 71.2 65.5 68.1 90.2 92.7 93.9 94.5 87.2 90.9 Soft-Thinking Reasoning Pattern 89.6 89.0 88. 92.7 95.1 94.5 91.5 92.7 91.5 64.7 68.1 68.8 84.8 85.2 84.4 84.8 88.7 89.1 87. 86.0 87.9 90.3 91.1 90.0 88.7 90.3 91.4 56.77 58.56 85.45 84.62 90.59 89. 93.64 92.65 55.98 58.82 59.08 85.97 85.05 86.65 89.79 90.68 90.54 92.79 93.28 93.80 Table 4. Experiments on using majority voting to boost the performances on AIME2024, AIME2025, AMC23 and GSM8K. @1 represents the Pass@1 result from Table 1, which is averaged over 32 runs. M@16 and M@32 represent Major@16 and Major@32, respectively. Dataset Metrics AIME2024 AMC23 @1 M@16 M@32 @1 M@16 M@32 @1 M@16 M@32 @1 M@16 M@32 @1 M@16 M@32 AIME2025 GSM8K Average No Fine-tune + GRPO 30.6 31.8 27.3 No Fine-tune + GRPO 29.2 + SofT-GRPO 32.6 56.7 53.3 56.7 43.3 60.0 60.0 50. 60.0 46.7 63.3 23.0 25.3 23.8 25.4 26.1 40.0 30.0 DeepSeek-R1-Distill-Qwen-1.5B Base LLM Discrete-Token CoT Reasoning Pattern 70.7 36.7 77.3 36.7 Soft-Thinking Reasoning Pattern 69.9 75.8 76.4 33.3 30.0 36. 33.3 30.0 33.3 92.5 92.5 95.0 95.0 92.5 92.5 95.0 92.5 90.0 92.5 81.5 84. 81.0 84.9 85.5 89.5 90.1 89.3 90.5 90.6 89.5 90.4 89.3 90.8 90.5 51.5 54. 50.5 53.8 55.2 68.2 68.1 68.6 64.1 69.1 71.1 65.7 68.8 65.0 71.4 Pass@32. Mean@32 judges the average Pass@1 accuracy over 32 runs on the datasets. Pass@16 and Pass@32 measure the average probability of covering the accurate answer within 16 and 32 runs, respectively. 4.2. Main Result The main result of SofT-GRPO is shown in Table 1. SofTGRPO can lead to clear and consistent improvement from the No-Finetune results under the soft-thinking reasoning pattern. Compared to GRPO under the discrete-token CoT, the proposed SofT-GRPO exhibits slight but stable leads (+0.13% on average) on the Mean@32 (i.e., Pass@1, @1 in the Table) metrics over LLMs of three sizes. Notably, the SofT-GRPO can lead to clear improvement in the Pass@16 (+1.80% on average) and Pass@32 (+2.19%) metrics. Moreover, this observation will still hold when adopting the soft-thinking pattern on the GRPO, demonstrating the significance of SofT-GRPO in reinforcing the soft-thinking performance. 4.3. Comparison to Butt et al. (2025) In this subsection, we conduct comparison experiments with an existing RLVR algorithm for the soft-thinking pattern, the method in Butt et al. (2025). We note this method as Soft Tokens in Table 2, where the proposed SofT-GRPO can demonstrate clear update compared to the results reported in Butt et al. (2025). Compared to SofT-GRPO, the algorithm in Butt et al. (2025) requires transitioning the d-dimensional vector between rollout workers and RLVR workers, improving the difficulty of implementation. 4.4. Comparison in Out-of-Domain Datasets Besides numerical reasoning, we conduct out-of-domain experiments to evaluate the general reasoning ability of SofT-GRPO. As shown in Table 3, the LLMs fine-tuned with SofT-GRPO on numerical queries can still demonstrate advantages from Pass@1 to Pass@32 on scientific reasoning benchmark (GPQA Diamond) and two code benchmarks (HumanEval and MBPP). 4.5. Comparison in the Token Efficiency In addition to the accuracy, the token efficiency of LLMs is also crucial metric. In Table 6 and Appendix D.1, we compare the token efficiency of baselines and SofTGRPO. Compared to the No-Finetune results, fine-tuning with SofT-GRPO will effectively reduce the thinking length. Compared to the discrete-token GRPO, SofT-GRPO will not severely increase the number of tokens. Specifically, there is clear reduction in the thinking length of the LLaMA-3.23B-Instruct model, which demonstrates the effectiveness of 7 Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization Table 5. Ablation studies on the noise added in the proposed SofT-GRPO. We highlight the performance differences on the ablation variants (i.e., adding Dirichlet noises or Gaussian noises). Dataset Metrics AIME2024 AIME2025 AMC23 MATH-500 GSM8K Average @1 @16 @32 @1 @16 @32 @1 @16 @32 @1 @16 @32 @1 @16 @32 @1 @16 @32 No-Finetune + GRPO + SofT-GRPO (Original, Gumbel) + SofT-GRPO (Dirichlet Noise) + SofT-GRPO (Gaussian Noise) DeepSeek-R1-Distill-Qwen-1.5B Base LLM Soft-Thinking Reasoning Pattern 46.7 46.7 50.0 56. 70.0 23.8 73.3 25.4 80.0 26.1 70.0 22.9 53.3 66.7 27.3 53.3 70.0 29.2 53.3 76.7 32.6 63.3 60.0 25.4 (-16.7) (-10) (-3.2) (+6.7) (+10) (-7.2) 43.3 21.9 60.0 (-10) (-10.7) (-16.7) 73.0 20.0 (-6.1) (-7) 40.0 (-10) 69.9 75.8 76.4 68.4 (-8) 65.6 (-10.8) 93.2 96.8 97.4 97. 96.6 98.2 98.0 97.8 95.0 95.0 79.4 95.0 95.0 86.3 97.5 97.5 86.3 97.5 97.5 83.6 (0) (0) 97.5 97.5 79.9 (0) (0) 82.4 94.6 83.3 95.6 85.2 96.1 85.3 97.0 (-2.7) (-0.4) (-0.2) (-1.8) (+0.9) (+0.8) (-4.6) (-1.9) (+0.1) 81.2 96.6 94.5 (-4) (-6.5) (-2.2) (-1.4) (-3.7) (-1.7) 53.8 77.4 (-7.6) (-6.1) 81.0 84.9 85.5 83.6 56.3 60.3 61.4 56. 97.1 96.4 97.0 97.8 79.2 80.8 83.5 81.6 95.5 (-1.5) 81.8 95.2 (a) Training reward curve of variants on added noises (b) Validation reward curve of different hyperparameters (c) Training reward curve of different hyperparameters Figure 3. Smoothed training or validation curves of ablation studies (the dashed background contains the actual data points). (a) discusses the setting of adding Gumbel noise in SofT-GRPO. (b) discusses the setting of top-p=0.95 and the Gumbel-Softmax temperature τg = 0.1 in SofT-GRPO. SofT-GRPO in saving computational consumption. 4.6. Boosting SofT-GRPO with Majority Voting As demonstrated in Table 1 and Table 3, SofT-GRPO can lead to clear advantages on Pass@32. To further exploit this advantage, in this subsection, we design to boost SofTGRPO with majority voting (Chen et al., 2024). As shown in Table 4, SofT-GRPO with majority-voting can outperform No-Finetune LLM and LLM fine-tuned with discrete-token GRPO across Major@16 (the accuracy of the most common answer in 16 runs) and Major@32. Results exhibit the potential to strengthen the SofT-GRPO-fine-tuned LLMs for better soft-thinking solvers. 5. Discussion 5.1. Ablation on Noise Added In SofT-GRPO, we employ the Gumbel Softmax technique for controllable stochasticity. As discussed in Section 2.2, adding Dirichlet noise to predicted probabilities can be another choice, so we compare the performance of the original SofT-GRPO and two of its variants, adding Dirichlet noise or Gaussian noise to probabilities in training. The reward curve in training is shown in Figure 3(a), the validation curve is shown in Figure 3(b), and the final performance of variants is shown in Table 5. LLMs can not learn clear refinement from the added Dirichlet noise, and adding the Gaussian noise will cause poorer initial performance. So, these variants cannot perform as well as integrating the original Gumbel noises. 5.2. Ablation on Hyper-Parameters SofT-GRPO sets the top-p as 0.95 and the Gumbel temperature as τg = 0.1. To demonstrate the reason for these settings, in Figure 3(c), we compare the training reward curve of the original SofT-GRPO with two variants (varying top-p to 1.0 or τg to 0.25). Results show that both variants will cause collapse in the training process. As analyzed in Appendix D.3, the two variants will cause substantial improvement in the KL divergence between πθ and πθref . So, we attribute this kind of collapse to the fact that fine-tuning under the soft-thinking pattern may lead to inputs outside the pre-trained discrete-token embedding space. 8 Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization 6. Conclusion This paper presents powerful RLVR algorithm, SofTGRPO, to reinforce LLMs under the soft-thinking reasoning pattern. It integrates controllable stochasticity with Gumbel-Softmax and reinforces the soft-thinking policy with Gumbel reparameterization. The proposed SofT-GRPO can demonstrate better numerical, scientific, and code reasoning ability compared to the conventional discrete-token GRPO on Pass@1, especially Pass@16 and Pass@32. It can also be boosted to superior solver with majority voting. This article emphasizes the prospects of soft-thinking in LLM reasoning. In the future, we will try building effective tricks over SofT-GRPO and apply it to border fields such as Vision Language Models."
        },
        {
            "title": "References",
            "content": "Arora, D. and Zanette, A. Training language models to reason efficiently. arXiv preprint arXiv:2502.04463, 2025. Butt, N., Kwiatkowski, A., Labiad, I., Kempe, J., and Ollivier, Y. Soft tokens, hard truths. arXiv preprint arXiv:2509.19170, 2025. Chen, L., Davis, J. Q., Hanin, B., Bailis, P., Stoica, I., Zaharia, M., and Zou, J. Are more llm calls all you need? towards scaling laws of compound inference systems. arXiv preprint arXiv:2403.02419, 2024. Chen, X., Zhao, A., Xia, H., Lu, X., Wang, H., Chen, Y., Zhang, W., Wang, J., Li, W., and Shen, X. Reasoning beyond language: comprehensive survey on latent chain-of-thought reasoning, 2025. URL https: //arxiv.org/abs/2505.16782. Cheng, J. and Van Durme, B. Compressed chain of thought: Efficient reasoning through dense representations. arXiv preprint arXiv:2412.13171, 2024. Goyal, S., Ji, Z., Rawat, A. S., Menon, A. K., Kumar, S., and Nagarajan, V. Think before you speak: Training language models with pause tokens. In ICLR, 2024. URL https: //openreview.net/forum?id=k5E1Yw5u3Q. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hao, S., Sukhbaatar, S., Su, D., Li, X., Hu, Z., Weston, J., and Tian, Y. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. Hao, Z., Wang, H., Liu, H., Luo, J., Yu, J., Dong, H., Lin, Q., Wang, C., and Chen, J. Rethinking entropy interventions in rlvr: An entropy change perspective. arXiv preprint arXiv:2510.10150, 2025. Jain, A. and Rappazzo, B. Learning to reason with mixture of tokens. arXiv preprint arXiv:2509.21482, 2025. Jang, E., Gu, S., and Poole, B. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. Kydlıˇcek, H. Math-Verify: Math Verification Library, 2025. https://github.com/huggingface/ URL math-verify. Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Liu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025a. Chu, T., Zhai, Y., Yang, J., Tong, S., Xie, S., Schuurmans, D., Le, Q. V., Levine, S., and Ma, Y. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. Liu, Z., Liu, J., He, Y., Wang, W., Liu, J., Pan, L., Hu, X., Xiong, S., Huang, J., Hu, J., et al. Part i: Tricks or traps? deep dive into rl for llm reasoning. arXiv preprint arXiv:2508.08221, 2025b. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Luo, M., Tan, S., Wong, J., Shi, X., Tang, W. Y., Roongta, M., Cai, C., Luo, J., Li, L. E., Popa, R. A., and Stoica, I. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. Dai, M., Yang, C., and Si, Q. S-grpo: Early exit via reinforcement learning in reasoning models. arXiv preprint arXiv:2505.07686, 2025. Maddison, C. J., Mnih, A., and Teh, Y. W. The concrete distribution: continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. Feng, S., Fang, G., Ma, X., and Wang, X. Efficient reasoning models: survey. arXiv preprint arXiv:2504.10903, 2025. Qi, P., Liu, Z., Pang, T., Du, C., Lee, W. S., and Lin, M. Optimizing anytime reasoning via budget relative policy optimization. arXiv preprint arXiv:2505.13438, 2025a. 9 Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization Qi, P., Liu, Z., Zhou, X., Pang, T., Du, C., Lee, W. S., and Lin, M. Defeating the training-inference mismatch via fp16. arXiv preprint arXiv:2510.26788, 2025b. Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229256, 1992. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Shen, Z., Yan, H., Zhang, L., Hu, Z., Du, Y., and He, Y. Codi: Compressing chain-of-thought into continuous space via self-distillation. arXiv preprint arXiv:2502.21074, 2025. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Sprague, Z., Yin, F., Rodriguez, J. D., Jiang, D., Wadhwa, M., Singhal, P., Zhao, X., Ye, X., Mahowald, K., and Durrett, G. To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning. arXiv preprint arXiv:2409.12183, 2024. Tan, W., Li, J., Ju, J., Luo, Z., Luan, J., and Song, R. Think silently, think fast: Dynamic latent compression of llm reasoning chains. arXiv preprint arXiv:2505.16552, 2025. Wang, C., Li, Z., Bai, J., Zhang, Y., Cui, S., Zhao, Z., and Wang, Y. Arbitrary entropy policy optimization: Entropy is controllable in reinforcement finetuning. arXiv preprint arXiv:2510.08141, 2025a. Wang, J., Wu, Z., Lai, F., Lian, S., and Zeng, Z. Synadapt: Learning adaptive reasoning in large language models via synthetic continuous chain-of-thought. arXiv preprint arXiv:2508.00574, 2025b. Wang, S., Zhang, S., Zhang, J., Hu, R., Li, X., Zhang, T., Li, J., Wu, F., Wang, G., and Hovy, E. Reinforcement learning enhanced llms: survey. arXiv preprint arXiv:2412.10400, 2024. Wu, C., Lu, J., Ren, Z., Hu, G., Wu, Z., Dai, D., and Wu, H. Llms are single-threaded reasoners: Demystifying the working mechanism of soft thinking. arXiv preprint arXiv:2508.03440, 2025a. Wu, X.-K., Chen, M., Li, W., Wang, R., Lu, L., Liu, J., Hwang, K., Hao, Y., Pan, Y., Meng, Q., et al. Llm finetuning: Concepts, opportunities, and challenges. Big Data and Cognitive Computing, 9(4):87, 2025b. Xu, Y., Guo, X., Zeng, Z., and Miao, C. Softcot: Soft chain-of-thought for efficient reasoning with llms, 2025a. Xu, Y., Guo, X., Zeng, Z., and Miao, C. Softcot++: Testtime scaling with soft chain-of-thought reasoning. arXiv preprint arXiv:2505.11484, 2025b. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Zelikman, E., Harik, G. R., Shao, Y., Jayasiri, V., Haber, N., and Goodman, N. Quiet-star: Language models can teach themselves to think before speaking. In First Conference on Language Modeling, 2024. URL https://arxiv. org/abs/2403.09629. Zhang, J., Zhu, Y., Sun, M., Luo, Y., Qiao, S., Du, L., Zheng, D., Chen, H., and Zhang, N. Lightthinker: Thinking stepby-step compression. arXiv preprint arXiv:2502.15589, 2025a. Zhang, Z., He, X., Yan, W., Shen, A., Zhao, C., Wang, S., Shen, Y., and Wang, X. E. Soft thinking: Unlocking the reasoning potential of llms in continuous concept space. arXiv preprint arXiv:2505.15778, 2025b. Zheng, L., Yin, L., Xie, Z., Sun, C. L., Huang, J., Yu, C. H., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., et al. Sglang: Efficient execution of structured language model programs. Advances in neural information processing systems, 37:6255762583, 2024. Zheng, Z. and Lee, W. S. Reasoning-cv: Fine-tuning powerful reasoning llms for knowledge-assisted claim verification. arXiv preprint arXiv:2505.12348, 2025. Wei, X., Liu, X., Zang, Y., Dong, X., Cao, Y., Wang, J., Qiu, X., and Lin, D. Sim-cot: Supervised implicit chain-ofthought. arXiv preprint arXiv:2509.20317, 2025. Zhuang, Y., Liu, L., Singh, C., Shang, J., and Gao, J. Text generation beyond discrete token sampling. arXiv preprint arXiv:2505.14827, 2025. 10 Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization A. Related Work In this section, we will discuss recent developments on discrete-token RLVR and similar domain of soft-thinking, the latent reasoning methods. A.1. RLVR on Discrete-Token CoT Reasoning The Deepseek-R1-zero (Guo et al., 2025) has demonstrated remarkable performance with discrete-token RLVR. And there has been wide collection of RLVR methods for discrete-token CoT policy optimization, e.g., GRPO (Shao et al., 2024), Dr. GRPO (Liu et al., 2025a), DAPO (Yu et al., 2025), and Lite PPO (Liu et al., 2025b). To further improve these algorithms towards some specific goals, recent works focus on developing RLVR methods for efficient responses (Feng et al., 2025) or controlling the entropy in CoT generation for better exploration-exploitation balance (Hao et al., 2025). To generate concise discrete-token CoTs, Arora & Zanette (2025) modifies the reward function by adding penalties on the generation length. Qi et al. (2025a) and Dai et al. (2025) turn to formulate the fine-tuning process for concise language CoT as multi-objective optimization task with the trade-off between token efficiency and accuracy. They truncate the generation of discrete-token CoT at several thinking budgets and optimize the overall performance over them. RLVR methods with entropy control try to make better balance between exploration and exploitation. They try to control the entropy in the training process to stimulate the exploration within the groups of CoTs in GRPO Wang et al. (2025a). In the future, we plan to have comprehensive investigation on whether these modifications can also boost the proposed SofT-GRPO. A.2. Latent Reasoning Similar to the soft-thinking pattern, latent reasoning methods pass continuous vectors between LLM steps. These methods fully decouple the reasoning process from explicit natural language (which soft-thinking does not do) and perform inference in the hidden space of the model. Generally, latent CoT methods are often diverse from each other and can be mainly divided into token-wise auto-aggressive methods and auxiliary strategies (Chen et al., 2025; Tan et al., 2025). Token-wise auto-aggressive methods transform the reasoning process into soft thoughts with dense latent embeddings (Hao et al., 2024) or specialized tokens (e.g., pause (Goyal et al., 2024; Zelikman et al., 2024)). These methods focus on transferring the original reasoning policy in the language domain to latent embedding space, including curriculum learning (e.g., Coconut (Hao et al., 2024), LightThinker (Zhang et al., 2025a), SIM-COT (Wei et al., 2025)), self-distillation (e.g., CODI (Shen et al., 2025)), and one-shot compression (e.g., CoLaR (Tan et al., 2025), SynAdapt (Wang et al., 2025b)). Auxiliary strategies (e.g., SoftCoT (Xu et al., 2025a)) generate latent embeddings from an auxiliary module and inject them into the frozen main model (Cheng & Van Durme, 2024; Xu et al., 2025b). Due to the existing token-wise auto-aggressive methods completely treating the language CoTs as the label, these methods can hardly surpass or even reach the performance level of language LRMs. So empirically, these methods can effectively improve the token efficiency compared to language CoT, but there is clear performance drop. Auxiliary strategies, instead, can effectively prompt the performance of the original LLM, sacrificing the running efficiency. Compared to latent reasoning methods mainly aiming at better efficiency, the proposed SofT-GRPO aims at reinforcing the accuracy of the soft-thinking pattern to surpass discrete-token CoT with GRPO on general reasoning tasks. Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization B. Prompt of Language Reasoning and Latent Generation In this part, we show the prompt we adopt for reasoning problems, including the in-domain numerical reasoning, out-ofdomain GPQA reasoning, and code reasoning. We inherit the prompts in Zhang et al. (2025b) for out-of-domain benchmarks. We use the same prompt for both the soft-thinking reasoning pattern and the discrete-token reasoning pattern. Prompt for Numerical Reasoning. user {Question} Lets think step by step and output the final answer within boxed{} Prompt for GPQA Reasoning. user Please solve the following multiple-choice question. Please show your choice in the answer field with only the choice letter, e.g.,answer: C. {Question} Prompt for Code Reasoning (HumanEval). user Please solve the programming task below in Python. Code should be wrapped in markdown code block. python {Question} Prompt for Code Reasoning (MBPP). user Please solve the programming task with test cases below in Python. Make sure your code satisfies the following requirements: 1. The function name and signature must match exactly as specified in the test cases. 2. Your code should be wrapped in markdown code block without including any test cases. Task: {Question} Test Cases: python {TestCases} The blue part represents the specific question (query Q), the brown part represents the possible test cases provided in the MBPP code reasoning benchmark. We use the code provided in Zhang et al. (2025b) in the verification process of the responses. 12 Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization C. Motivation and Theoretical Proof C.1. Motivation: Mismatch in Butt et al. (2025) Butt et al. (2025) adds Gaussian noise on the soft-token inputs as follows: st = (cid:88) i=1 pi ei, ˆst = st + (0, σ2Id) (13) and calculates the log probability as follows: 1 2σ2 ˆst st2 Let RT be the embedding matrix, and each token probability vector 1 corresponds to the soft input = Ep Rd. 2 + Constant. log p(ˆst) = (14) Assume the observed noisy soft input is ˆs, and we define likelihood log p(ˆs p) 1 2σ2 ˆs Ep2 2. (15) Suppose we want to regard this as likelihood on p. In general, the mapping (cid:55) Ep is many-to-one: since > d, the kernel of is nontrivial, so p1 = p2, with Ep1 = Ep2 Thus, the same may correspond to infinitely many p. This means that, under this Gaussian model, two different token mixtures can lead to the exact same log-probability value. The information about the original token distribution is partially lost in the embedding projection, unless is invertible (which it is not). Thus, the use of Gaussian noise model on the embedding space gives mismatch to the true simplex-based probability geometry. (16) In summary of Drawback 1: Due to the non-injectivity (non-invertibility) of the embedding transformation, the model log p(ˆst) ˆst st2 does not define true likelihood on the simplex of token probabilities. The above mismatch is not only due to the non-invertibility of the embedding matrix. Even if we restrict to be sparse (nonzero only on top-k set, which is general setting of LLMs or common nature of LLM predictions), and even if the corresponding submatrix EK is invertible, the process of adding Gaussian noise in the d-dimensional embedding space fundamentally breaks the connection to sparse token distributions. More specifically, after adding Gaussian noise, with general top-k setting (e.g., k=10 to 30, and << d) the perturbed embedding ˆst = st +ϵ (with ϵ (0, σ2I d)) will almost surely not lie in the convex hull of any set of token embeddings. In other words, ˆst, for almost all ϵ, k-sparse such that ˆst = Ep. (17) The set of all top-k soft-token embeddings forms low-dimensional union of simplices in Rd, which is measure-zero subset of the space. The probability of randomly perturbed embedding ˆst coinciding with legal top-k mixture is thus zero. Therefore, defining the likelihood p(ˆst) as simple Gaussian on embedding space cannot be interpreted as likelihood on the space of top-k soft tokensnot only due to non-invertibility or nonlinearity, but more fundamentally because most ˆst produced by noise are not realizable by any top-k soft-token distribution. In summary of Drawback 2: Under the general top-k setting, the likelihood p(ˆst) as simple Gaussian on the embedding space cannot be interpreted as likelihood on the space of top-k embeddings. Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization C.2. Proof for Theorem 3.1 Theorem 3.1 (Gumbel-max Trick) Let (p1, . . . , pn) be nonnegative real numbers, not all zero. Let g1, . . . , gn be independent samples from Gumbel(0, 1). Then, (cid:16) Pr = arg max (gi + log pi) (cid:17) = pj (cid:80)n i=1 pi . (18) Proof. For any {1, . . . , n}, (cid:16) Pr = arg max (gi + log pi) (cid:17) = Pr (gj + log pj gi + log pi, = j) = (cid:90) + (cid:89) i=j Pr (gi gj + log pj log pi) fgj (gj) dgj, where fgj (g) = egexp(g) is the PDF of the standard Gumbel distribution. Pr(gi t) = FGumbel(t) = exp (et), so Pr (gi gj + log pj log pi) (cid:89) i=j (cid:16) e(gj +log pj log pi)(cid:17) exp = (cid:89) i=j = exp (cid:88) i=j e(gj +log pj log pi) = exp egj (cid:88) i=j . pi pj (cid:90) + egj eegj exp egj (cid:88) i=j dgj pi pj egj exp egj 1 + = (cid:90) + dgj. (cid:88) i=j pi pj The total probability is Let = 1 + (cid:80) pi pj = (cid:80)n i=1 pi pj i=j . Substitute = egj , dgj = dy , (0, +), (cid:90) 0 = y=+ (cid:18) exp(yS) (cid:19) dy = (cid:90) + 0 exp(yS)dy = 1 = pj (cid:80)n i=1 pi . Thus, combining all the above equations, (cid:16) Pr = arg max (gi + log pi) (cid:17) = pj (cid:80)n i=1 pi . 14 Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization D. Supplementary of Experiments Table 6. Experiments on the token efficiency for baselines and the proposed SofT-GRPO. #Token values in Table represent the number of tokens across all queries, and the #Token values represent the number of tokens across correct queries. Dataset Metrics AIME AIME2025 AMC23 MATH-500 GSM8K Average #Token #Token #Token #Token #Token #Token #Token #Token #Token #Token #Token #Token No-Finetune + GRPO 16241.6 8927.6 14997.8 8535.6 16416.3 8039.4 13448.8 6414.2 10052.2 5001.3 9394.2 4672. 5616.6 3106.1 5368.1 2958.5 1839.3 1417.1 1772.5 1370.6 10033.2 5298.3 8996.3 4790. Soft-Thinking Reasoning Pattern DeepSeek-R1-Distill-Qwen-1.5B Base LLM Discrete-Token CoT Reasoning Pattern 17857.8 No-Finetune + GRPO 9383.2 + SofT-GRPO 11039.6 16191.3 8934.5 10756.1 17569.4 8007.2 10519.6 14582.4 7325.8 7831. 10482.0 4894.4 5630.4 11269.9 5203.7 5900.2 4015.9 3233.6 3549.5 LLaMA-3.2-3B-Instruct Base LLM Discrete-Token CoT Reasoning Pattern 3888.3 3131.7 3399.2 1699.3 1385.5 1577.6 1649.9 1349.9 1542. 10482.5 5442.7 6517.3 9358.8 5127.2 5831.9 No-Finetune + GRPO No-Finetune + GRPO + SofT-GRPO 5010.5 8991.8 4859.2 9749.0 829. 3334.1 6368.9 2727.7 7627.8 862.9 4297.6 8443.8 2430.9 11088.3 2790.7 4998.1 2398.1 3978. 1850.1 3334.8 1494.2 2463.0 Soft-Thinking Reasoning Pattern 4934.8 8498.9 893.1 3735.2 10041.4 879.6 3259.9 5389.9 911. 2624.0 4987.3 927.9 2086.1 3701.9 632.8 1494.1 2764.6 575.7 236.6 502.0 243.5 541.9 294.6 224.6 453. 227.8 505.8 292.9 DeepSeek-R1-Distill-Qwen-7B Base LLM Discrete-Token CoT Reasoning Pattern No-Finetune + GRPO 13120.7 7795.9 11511.4 7116.1 14347.7 8003. 11750.7 7369.8 6346.0 4050.9 5987.1 3719.7 3998.9 2473.3 3939.0 2405.5 1061.1 1146. 1032.0 1118.7 13017.4 No-Finetune + GRPO 7464.9 + SofT-GRPO 8035.8 11888.6 6531.4 7556.8 14116.8 8291.6 8381.7 11507.4 7160.4 7873.7 6346.7 3931.7 4008. 6043.6 3736.7 3843.5 3947.4 2473.2 2630.2 3858.6 2423.0 2583.5 996.3 1117.0 1293.2 962.2 1104.5 1276.4 Soft-Thinking Reasoning Pattern 2837.1 5254.1 3076.7 5576.3 712.3 7774.9 4694.0 7684.9 4655.7 4869.8 1976.4 4870.5 2161.8 5185.4 707. 6844.0 4346.0 6852.1 4191.2 4626.8 D.1. Supplementary of Token Efficiency As shown in Section 4.5, besides the performance, the token efficiency of LLMs is also an important metric. In this section, we compare the token efficiency of baselines and SofT-GRPO in Table 6. Compared to No-Finetune variants, SofT-GRPO can demonstrate clear refinement in both the token efficiency across all queries and the token efficiency across correct queries. Compared to discrete-token GRPO, SofT-GRPO will not cause severe token improvement. Specifically, we observe severe reduction in the thinking length of the LLaMA-3.2-3B-Instruct model. As shown in Figure 4, unlike GRPO, SofT-GRPO maintains and even enhances the token efficiency compared to the base LLM when training progresses, demonstrating the effectiveness of SofT-GRPO in reducing computational consumption. Figure 4. Token consumption curve on LLaMA-3.2-3B-Instruct Base LLM during training. Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization D.2. Experiments under Different Temperatures In Section 4, we adopt the setting of τ = 0.6 for discrete-token CoT. To investigate whether other temperature settings will break our observations in our main experiments, similar to Yue et al. (2025), we try temperatures from τ = 0.6, τ = 0.8, τ = 1.0, τ = 1.2, and τ = 1.4. As shown in Figure 5, we conduct experiments on the 1.5B LLMs (i.e., DeepSeek-R1-Distill-Qwen-1.5B Base LLM) for their average Pass@k accuracies across five numerical reasoning benchmarks (i.e., AIME2024, AIME2025, AMC23, MATH-500, GSM8K), where the proposed SofT-GRPO can demonstrate outstanding results compared to GRPO and No-Finetune variants with various temperatures, from Pass@1 to Pass@32. Figure 5. Running discrete-token CoT methods (GRPO and No-finetune) with more temperature options on DeepSeek-R1-Distill-Qwen1.5B Base LLM. Pass@k represents the pass rate within at most runs, and Pass@1 is additionally averaged from 32 runs. Experiments are run on the five datasets in Table 1 for the average. Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization D.3. Supplementary of Ablation on Hyper-Parameters (a) Training KL Divergence curve between πθref and πθ (b) Training Proximal Policy Optimization (PPO) (Schulman et al., 2017) KL Divergence curve between πθold and πθ Figure 6. KL Divergence curves in SofT-GRPO. In this subsection, we have further investigation into the hyperparameter settings. As briefly shown in Section 5.2, adopting higher top-p or τg will cause collapses in training. We attribute these collapses to the case that some soft-thinking tokens may become incomprehensible for LLMs. As shown in Figure 6(a), the variants (varying top-p to 1.0 or varying τg to 0.25) will demonstrate higher divergence between the fixed pre-trained πθref and the current policy, which can be an indicator of the inputs outside the pre-trained embedding space. Recently, Qi et al. (2025b) provides excellent insight into the collapse situation in the RLVR fine-tuning. When collapse is caused by precision issue, they observe super high KL divergence between πθold and πθ (Refer to Figure 3 in (Qi et al., 2025b), 103 even higher). However, in the variants shown in Figure 6(b), we find that their KL divergence in PPO policies is less than 105, indicating that the variants of SofT-GRPO are less likely to meet precision issues. 17 Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization E. Baselines & Datasets & Licenses In our experiments, we evaluate and compare the following model baselines and datasets. For each, we detail the official website and usage license. E.1. Baselines We mainly include No-Finetune base LLMs, Discrete-Token GRPO, and the method in Butt et al. (2025) (noted Soft Token) as baselines. Base LLMs This paper includes DeepSeek-R1-Distill-Qwen-1.5B, LLaMA-3.2-3B-Instruct, and DeepSeek-R1-DistillQwen-7B as base LLMs. Discrete-Token GRPO We utilize the latest verl (Sheng et al., 2024) https://github.com/volcengine/verl/ tree/main and vLLM rollout to implement discrete-token GRPO with default parameters. Butt et al. (2025) Soft Token Due to the requirement of passing d-dimensional inputs ˆs between the rollout workers and the verl policy optimization workers, implementing this algorithm requires high amount of communication between the rollout workers and the policy update workers. So, we report the results in (Butt et al., 2025) for comparison in Table 2. E.2. Datasets This paper covers five in-domain numerical reasoning datasets (i.e., AIME2024, AIME2025, AMC23, MATH-500, and GSM8K (Cobbe et al., 2021)), one out-of-domain scientific reasoning dataset GPQA-Diamond (Rein et al., 2024), and two out-of-domain code reasoning datasets (i.e., HumanEval and MBPP). These datasets are provided in https://github. com/eric-ai-lab/Soft-Thinking. Specifically, due to their answer can be correct with various equivalent forms, we exclude the MATH-500 benchmark for Table 4. E.3. Licenses For Base LLM, Dataset, and frameworks, we list their Licenses in Table 7. Resources Type License URL Table 7. summary of licenses. DeepSeek-R1-Distill-Qwen-1.5B Base LLM Base LLM LLaMA-3.2-3B-Instruct Base LLM DeepSeek-R1-Distill-Qwen-7B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B MIT License Llama 3.2 License https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct MIT License https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B verl verl-0.4.x SGLang for soft-thinking RL-framework RL-framework Inference-framework MIT License Apache-2.0 license https://github.com/volcengine/verl Apache-2.0 license https://github.com/volcengine/verl/tree/v0.4.x https://github.com/eric-ai-lab/Soft-Thinking AIME2024, AIME2025, GSM8K Dataset Dataset AMC23, MATH-500 Dataset HumanEval, GPQA-Diamond Dataset MBPP https://github.com/eric-ai-lab/Soft-Thinking MIT License https://github.com/eric-ai-lab/Soft-Thinking Available Online https://github.com/eric-ai-lab/Soft-Thinking MIT License Apache-2.0 license https://github.com/eric-ai-lab/Soft-Thinking"
        }
    ],
    "affiliations": [
        "School of Computing, National University of Singapore"
    ]
}