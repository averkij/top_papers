{
    "paper_title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training",
    "authors": [
        "Zhenting Wang",
        "Guofeng Cui",
        "Kun Wan",
        "Wentian Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: https://github.com/ZhentingWang/DUMP."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 0 1 7 9 0 . 4 0 5 2 : r DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training Zhenting Wang1 Guofeng Cui1 Kun Wan2 Wentian Zhao2 1Rutgers University 2Adobe Inc."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as unified whole, overlooking the fact that modern LLM training often involves mixture of data from diverse distributionsvarying in both source and difficulty. This heterogeneity introduces key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much model can still benefit from further training on given distribution. Based on this, we propose distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distributionaware curriculum strategies in LLM post-training. Code: https://github.com/ ZhentingWang/DUMP."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning (RL)-based post-training has emerged as powerful approach for enhancing the capabilities of large language models (LLMs), particularly in areas requiring structured reasoning, multi-step inference, and task-specific generalization [15]. By leveraging reward signals derived from task performance, human feedback, or domain-specific metrics, RL provides flexible alternative to supervised fine-tuning. Unlike imitation-based methods that merely mimic reference outputs, RL-based approaches allow models to optimize directly toward behavioral objectives, making them especially effective for boosting model performance on complex reasoning and agentic tasks. While RL-based post-training has become key technique for enhancing LLM capabilities in reasoning, alignment, and coding, one foundational challenge remains underexplored: how to dynamically schedule training across heterogeneous data distributions. In practice, LLMs are posttrained on datasets drawn from wide variety of sourcesranging from factual QA to math problems and coding taskseach differing in knowledge/capability relevance, and learning difficulty [6 *Corresponding author. Email: kuwan@adobe.com, wezhao@adobe.com. Pre-print with preliminary results, work in progress. 8]. This heterogeneity is evident in large-scale post-training datasets such as Tülu 3 [8], where prompts span general dialogue, logic puzzles, STEM problems, and multilingual instructions, with widely varying counts, formats, and alignment objectives. More recently, next-generation posttraining pipelines (e.g., Seed-Thinking v1.5 [9]) have shifted toward synthetic data generation with controllable parameterse.g., configuring logical puzzle difficulty. This allows fine-grained control over the data distribution, making distribution-level curriculum learning both feasible and increasingly important. Despite this, most RL-based pipelines still treat all data distributions equallyuniformly sampling tasks throughout training or relying on static, hand-designed curricula. This static treatment ignores the models evolving learning needs and underutilizes the training budget. As reinforcement learning becomes increasingly used in post-training and training costs continue to rise, data-driven curriculum mechanism that dynamically prioritizes learnable distributions is not just desirable, but necessary. This motivates the need for automated distribution-level curriculum learning: dynamic strategy that adjusts sampling probabilities across data distributions throughout training. While prior work has explored instance-level curricula based on sample difficulty [10], and static/heuristic multi-stage schedules have been applied in LLM post-training [11, 12], little attention has been paid to automated, distribution-level schedulingespecially in the context of RL for capability-oriented post-training. The central challenge lies in identifying signals that reflect the current learnability of each distribution and in designing algorithms that can stably and efficiently leverage these signals to guide sampling. In this paper, we present DUMP (Automated Distribution-level cUrriculuM learning for RL-based LLM Post-training), simple but theoretically grounded approach to address this challenge. Our central insight is that the magnitude of policy advantagesthe expected absolute difference between models predicted return and its baseline valueserves as natural proxy for distribution-level learnability. High advantages on specific data distribution indicate underfitting and high potential for improvement on it, while low advantages suggest diminishing returns. Moreover, the statistical reliability of these advantage estimates improves with the number of samples drawn from each distribution. DUMP operationalizes this insight by using bandit-style Upper Confidence Bound (UCB) scores to schedule distribution sampling. It maintains sliding window of recent advantage magnitudes for each distribution and computes score that balances exploitation (high advantage) and exploration (low visitation). These scores are normalized via softmax to form sampling weights, which are then used to generate training batches. Unlike fixed or heuristic curricula, DUMP adapts throughout training based on empirical signals, and can be seamlessly integrated into standard LLM RL pipelines. We instantiate DUMP with GRPO [3], but the method is compatible with any advantage-based RL algorithm. We evaluate DUMP on logic reasoning corpora. Our experiments show that DUMP significantly accelerates convergence and yields stronger performance compared to uniform sampling. Furthermore, we provide theoretical analysis that supports the use of absolute advantages as surrogate for distribution-level learnability, formalizing its connection to sample efficiency and regret minimization. We summarize our contributions as follows. ① We highlight the underexplored challenge of curriculum learning at the distribution level for RL-based post-training aimed at capability enhancement. ② We propose DUMP, theoretically grounded framework that leverages advantage-based UCB scores to adaptively guide training over data distributions. ③ We demonstrate DUMPs effectiveness through empirical results and theoretical analysis, showing that it enables faster, more efficient improvement on LLM capabilities."
        },
        {
            "title": "2 Background",
            "content": "RL-based LLM Post-training. Reinforcement learning (RL) plays central role in post-training large language models (LLMs), especially for tasks involving reasoning, subjective preference, or long-horizon control. The RLHF framework [1, 1316] laid the foundation by aligning models using reward signals derived from human preferences. Beyond preference alignment, recent RL-based post-training approaches have notably enhanced LLMs capabilities in complex reasoning tasks, particularly coding and mathematics. For instance, RL post-trained model OpenAI o1 [17], o3 [18, 5], DeepSeek-R1 [4] significantly outperform LLMs without RL post-training such as GPT-4o [19] and DeepSeek-V3 [20] on challenging mathematics and coding benchmarks (e.g., AIME [21] and Codeforces [22]). Proximal Policy Optimization (PPO) [23] is widely used in post-training due to its clipped objective, which stabilizes training by preventing large policy updates. PPO remains strong 2 baseline in many LLM alignment settings. Direct Preference Optimization (DPO) [2] simplifies the pipeline by replacing RL rollouts with classification-style loss derived from KL-constrained reward maximization objective. While DPO works well on pairwise preference data, it does not naturally support group-wise or comparative feedback. Group Relative Policy Optimization (GRPO) [3] addresses this limitation by leveraging group-based feedback. For each input prompt x, GRPO samples group of candidate outputs {o1, . . . , oG} πref(x) from frozen reference policy πref. Each output oi is assigned reward ri, and the advantage of oi is computed by normalizing its reward relative to others in the group: ˆAi = ri mean({r1, . . . , rG}) std({r1, . . . , rG}) + ϵ , (1) where ϵ > 0 is small constant for numerical stability. These normalized advantages capture the relative quality of outputs within the group. The model policy πθ is then updated by maximizing the following clipped surrogate objective: JGRPO(θ) = Ex,{oi} (cid:34) 1 (cid:88) min i=1 (cid:18) πθ(oix) πref(oix) ˆAi, clip (cid:18) πθ(oix) πref(oix) , 1 ϵ, 1 + ϵ (cid:19) (cid:19) ˆAi (cid:35) β DKL(πθπref) , (2) where πθ(oix) is the probability assigned by the current model to output oi, and πref(oix) is the same under the reference model. The first term inside the summation is clipped policy ratio scaled by ˆAi, similar to PPO [23], which prevents overly large updates. The outer expectation is taken over prompts and their sampled output groups {oi}. The second term is KL divergence penalty that regularizes the updated policy πθ to stay close to πref, weighted by hyperparameter β. This formulation eliminates the need for an explicit value baseline and stabilizes training by comparing outputs within local groups. Curriculum Learning for RL. Curriculum learning [24, 25] is training paradigm that defines meaningful progression over training samples, typically guiding models to learn from easier examples before progressing to harder ones. In reinforcement learning (RL), task-specific curricula are often used to gradually increase the complexity of the environment [2628], aiming to improve the generalization and transferability of learned policies. To reduce reliance on expert-defined task hierarchies, teacher-guided curricula have been proposed [29, 30], where teacher agent adaptively selects tasks for student agent, with task selection formalized as partially observable Markov decision process (POMDP). With the adoption of RL in LLM post-training, curriculum learning has shown potential for improving both training efficiency and model effectiveness. For example, RORL [31] improves alignment by removing the easiest and hardest samples from the training set, keeping only examples of intermediate difficulty. Several recent studies [1012, 32] have incorporated curriculum strategies into LLM training. Curri-DPO [10] constructs instance-level curricula by ranking preference pairs based on the score gap between preferred and dispreferred responses, introducing harder pairs gradually during DPO fine-tuning. Kimi k1.5 [11] and LogicRL [12], on the other hand, use manually defined heuristic curricula with fixed training stages, e.g., models are first trained on easy samples for pre-specified number of steps, then switched to hard samples. These strategies rely on static schedules and heuristic difficulty labels, without adapting to the models learning progress. R3 [32] addresses sparse reward credit assignment by gradually increasing the generation difficultyrequiring the model to regenerate longer and longer suffixes of demonstration trajectories. While these works demonstrate the benefit of curriculum learning in LLM post-training, most existing approaches focus on instance-level difficulty or use static, manually designed strategies. In contrast, automatic curriculum learning at the distribution level, especially in RL-based post-training, remains underexplored. In this paper, we propose DUMP to fill this gap by adaptively scheduling training over distributions using advantage-based learnability signals."
        },
        {
            "title": "3 Method",
            "content": "In this section, we introduce DUMP, distribution-level curriculum learning framework for RLbased LLM post-training. We first introduce expected absolute advantage as proxy for learnability, and formalize the scheduling problem as multi-armed bandit. We then describe UCB-based strategy to guide distribution selection, followed by the full implementation of DUMP. 3 3.1 Measuring Learnability via Absolute Advantage We aim to dynamically assess the usefulness of different data distributions during LLM reinforcement learning post-training. Intuitively, distribution is more useful (or learnable) if the model can gain more from training on its samples. To help understand and measure the learnability of the data samples from different distributions, we provide the following theorem: Theorem 3.1 (Expected Advantage Magnitude Reflects Learnability). Given policy πθ and data distribution d, the expected absolute advantage Exd serves as proxy for how much that distribution can help the model improve, where the distribution consisting of prompts d, each prompt has group of sampled outputs {o1, . . . , on}, and ˆAi denotes the advantage of output oi. (cid:104) Eoiπθ(x) ˆAi (cid:105)(cid:105) (cid:104) Intuitively, if training on distribution results in larger expected advantage magnitude, then that distribution is considered more learnable. The advantage function measures the deviation between an actions predicted value and its actual return; large advantageeither positive or negativeindicates that the models current policy is still far from optimal on those samples but has large potential to improve. small advantage magnitude does not necessarily imply masteryit may also occur when task is too difficult or noisy for the model to learn from effectively, resulting in weak or unstable learning signals. To capture this deviation in both directions, we take the absolute value of the advantage. Without this, positive and negative advantages within batch may cancel out, masking the true extent of the models uncertainty or suboptimality. By averaging the absolute advantage over multiple sampled outputs and prompts, we obtain robust estimate of how much learning signal remains in given distribution. This expected absolute advantage thus acts as practical proxy for distribution-level learnability: it reflects how much the model can benefit from training on that distribution. It also has the strength of being lightweight to compute in RL pipelines, as advantage estimates are already generated during rollout. 3.2 Formalizing Distribution-Level Curriculum Learning as Multi-armed Bandit We aim to design curriculum learning strategy that dynamically allocates training focus across multiple data distributions to maximize overall model improvement. Let = {d1, . . . , dN } be set of data distributions. At each training step, we sample batch of examples Bt by drawing prompts from these distributions according to learnable sampling policy, and use the batch to update model parameters θ via reinforcement learning. The goal is to assign higher sampling probabilities to distributions that offer greater learning potential, thereby maximizing cumulative capability gain. (cid:104) (cid:105)(cid:105) (cid:104) ˆA(o) As motivated in Theorem 3.1, we quantify the learning potential of distribution via its expected absolute advantage, defined as L(d) = Exd Eoπθ(x) . Our objective is to dynamically adjust the sampling distribution over such that, over the training horizon , we approximately maximize the total expected learnability gain (cid:80)T EdPt[L(d)], where Pt is the sampling distribut=1 tion at step t. This setup resembles multi-armed bandit (MAB) problem, where each distribution acts as an arm and its reward corresponds to its learnability. In this setting, the central challenge is to estimate and balance each distributions potential: exploiting those with high observed advantage while still exploring under-sampled ones that may offer long-term benefit. To this end, we adopt the classic Upper Confidence Bound (UCB) principle [33], which provides theoretical guarantees for balancing exploration and exploitation in bandit problems. Specifically, UCB-based algorithms achieve sublinear regret compared to the optimal fixed-arm strategy, and we show in Appendix that applying UCB on empirical advantage statistics yields near-optimal schedule under mild assumptions. To allow smoother allocation of sampling probabilities without hard cutoffs and reducing variance in learning, we adopt soft-selection mechanism: instead of choosing one distribution at each step, we compute UCB score for every distribution and normalize the scores with softmax function to obtain sampling distribution. This soft-selection formulation preserves the spirit of UCBhigher scoring distributions are sampled morebut enables partial exploration of all arms, and it is easier to integrate into LLM training pipelines. The resulting sampling distribution provides convex mixture over data sources, where each distribution dj is selected with probability. Each training batch is then composed by drawing examples from multiple distributions in proportion to their scores. To estimate learnability in practice, we maintain sliding window Aw of recent absolute dj advantages for each distribution dj, and define its empirical reward as the mean absolute advantage: 4 Algorithm 1 Automated Distribution-Level Curriculum Learning with UCB Sampling Input: Dataset = {d1, . . . , dN }; pre-trained model parameters θ Output: Post-trained model parameters θ 1: function DUMP(D, θ) 2: 3: 4: 5: 6: Initialize distribution-level statistics for each dj do Aw dj ndj 0 (dj) 1 Sliding window for absolute advantages Total samples seen from dj Equal initial weights [ ] 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: for training step = 1, 2, . . . , do Sample batch Bt from according to (dj) Compute advantages ˆA(o) for all Bt via model rollout for each dj with samples in Bt do ndj ndj + Bt,dj Aw dj Aw dj Aw dj Aw dj [k :] (cid:110) ˆA(o) Bt,dj , πθ(x) Update sample count; Bt,dj : subset of batch from dj Append new advantages from dj k: Window Size; Keep last elements (cid:111) Compute UCB scores for each distribution ntotal (cid:80) dj ndj for each dj do ˆL(dj) 1 Aw dj (cid:80) aAw dj (cid:114) UCB(dj) ˆL(dj) + 2 log(ntotal+1) ndj +1 Update sampling distribution (dj) exp(UCB(dj )/τ ) (cid:80)N Update θ using Bt with an RL algorithm (e.g., GRPO) j=1 exp(UCB(dj )/τ ) dj return θ Mean of absolute advantages Eq. 3 τ : temperature (cid:80) ˆL(dj) = 1 aAw Aw dj dj ndj , and the global sample count ntotal = (cid:80) a. We also track the total number of samples drawn from each distribution ndj . The UCB score for each distribution is: UCB(dj) = ˆL(dj) + (cid:115) 2 log(ntotal + 1) ndj + 1 (3) The first term encourages exploitation of distributions with high observed advantages, while the second term ensures sufficient exploration of rarely sampled distributions. To obtain the final sampling weights, we apply softmax over the UCB scores. Specifically, the probability of selecting distribution dj is computed as: (dj) = exp(UCB(dj )/τ ) j=1 exp(UCB(dj )/τ ) , where τ > 0 is temperature hyperparameter that controls the sharpness of the sampling distribution. lower τ results in more peaked selection around the top-scoring distributions, while higher τ leads to smoother, more exploratory curriculum. This bandit-based formulation provides lightweight, adaptive, and reward-sensitive curriculum learning mechanism. It balances the need to focus on learnable distributions while avoiding premature neglect of underexplored ones. In the next section, we present the complete algorithmic implementation of DUMP, including its integration with rollout procedures and online statistics tracking. (cid:80)N 3.3 Algorithm The detailed curriculum learning procedure is illustrated in Algorithm 1. The algorithm takes as input dataset = {d1, . . . , dN } composed of multiple distributions and returns the optimized model parameters θ through reinforcement learning loop. In lines 36, we initialize per-distribution statistics: each distribution dj is associated with an empty sliding window Aw to store recent dj absolute advantages, counter ndj for tracking the number of samples drawn from dj, and an initial sampling probability (dj) = 1 indicating uniform sampling. At each training step (line 8), 5 Figure 1: Example of puzzle and prompt used in Knights and Knaves (K&K) puzzle dataset. batch Bt is sampled according to the current distribution weights (dj). Advantages ˆA(o) are then computed via model rollouts for each sampled output Bt (line 9). For every distribution dj that contributes samples in the current batch, we update its sample count ndj (line 11), append the corresponding advantages to its sliding window Aw (line 12), and truncate the window to retain dj only the most recent entries (300 by default) in line 13. This ensures that our estimate of perdistribution learnability remains up-to-date and robust to noise. In lines 1518, we compute the Upper Confidence Bound (UCB) score UCB(dj) for each distribution. The score consists of two terms: the empirical mean absolute advantage ˆL(dj) over the sliding window Aw , and an exploration bonus dj inversely proportional to the square root of the number of samples ndj . This balances prioritization of distributions that are either highly learnable or underexplored. In line 20, the sampling probabilities (dj) are updated by applying softmax over the UCB scores with temperature parameter τ (0.1 by default). Lower values of τ result in sharper distributions that concentrate more heavily on top-ranked distributions, while higher τ values induce smoother, more exploratory curriculum. Finally, in line 21, the model parameters θ are updated using the current batch Bt with reinforcement learning algorithm such as GRPO. After steps, the algorithm returns the post-trained model θ, which has been adaptively guided to learn from the most informative distributions."
        },
        {
            "title": "4 Experiments and Results",
            "content": "In this section, we first introduce our experiments setup including used models datasets and more implementation details. We then demonstrate the results for the effectiveness of our method DUMP. 4.1 Experiments Setup RL Algorithm and LLM Models. We use GRPO [3] as the underlying RL algorithm in our experiments, which is commonly-used in capability-oriented LLM post-training [4]. We use Qwen2.57B-Instruct-1M [34] in our experiments. This model is 7 billion parameter instruction-tuned language model from the Qwen series, which has demonstrated strong long-context capabilities. Datasets. We evaluate our method on the Knights and Knaves (K&K) puzzle dataset, originally introduced by Xie et al. [35], and widely adopted in RL-based LLM post-training research [12]. This dataset consists of procedurally generated logic puzzles, where each character is either knight (who always tells the truth) or knave (who always lies), and the goal is to determine the identity of each character based on their statements. These puzzles require multi-step logical reasoning and are designed to evaluate models capacity for deduction rather than surface-level pattern matching or memorization. An illustration of sample puzzles and the prompting format used during training [12] is provided in Figure 1. The dataset allows precise control over difficulty by varying the number of characters in each puzzle. In our experiments, we synthesize puzzles with character counts ranging from 3 to 14. We treat puzzles with the same number of characters as belonging to the same distribution, resulting in 12 distinct distributions. Each distribution contains 900 training samples and 100 test samples. Reward Implementation. We adopt the rule-based reward mechanism Shao et al. [3] to provide stable and hack-resistant training signals during RL-based post-training and follow the detailed reward implemetation in Logic-RL [12]. Specifically, each model response is expected to follow (a) 3 Characters (b) 4 Characters (c) 5 Characters (d) 6 Characters (e) 7 Characters (f) 8 Characters (g) 9 Characters (h) 10 Characters (i) 11 Characters (j) 12 Characters (k) 13 Characters (l) 14 Characters Figure 2: Effectiveness of DUMP on the K&K puzzle dataset mixed with 12 distributions defined by the number of characters in each puzzle. DUMP consistently achieves higher answer reward on test dataset compared to baseline. structured format with the reasoning process enclosed in <think> tags and the final answer enclosed in <answer> tags. The reward system consists of two components: Format Reward. binary reward is assigned based on whether the output strictly adheres to the expected format. If the model includes exactly one well-formed <think> and one <answer> section in the correct order, it receives reward of +1; otherwise, it receives penalty of 1. Answer Reward. Conditional on the format being correct, we further evaluate the semantic correctness of the final answer. If the predicted identities fully match the ground truth, the model receives reward of +2; if the answer is incorrect, 1.5; and if the answer is missing or unparsable, 2. Other Implementation Details. All experiments are conducted on servers equipped with 8 Nvidia A100 GPUs. Our method is implemented with VeRL [36] LLM Reinforcement Learning framework. We use GRPO [3] as the training algorithm and follow standard practice for actor rollout and optimization. The actor learning rate is set to 1e6, training batch size is set to 128, and the PPO (a) 3 Characters (b) 4 Characters (c) 5 Characters (d) 6 Characters (e) 7 Characters (f) 8 Characters (g) 9 Characters (h) 10 Characters (i) 11 Characters (j) 12 Characters (k) 13 Characters (l) 14 Characters Figure 3: Curriculum (sample counts) induced by DUMP across 12 distributions with increasing difficulty (defined by the number of characters in each puzzle). Simpler distributions are automatically prioritized in early training, while more complex ones are progressively emphasizedboth in an entirely automated mannerdemonstrating automated distribution scheduling. mini-batch size is 32. KL divergence regularization is applied to encourage alignment with the reference policy, with KL loss coefficient of 0.001. Each rollout batch contains 16 responses, and we allow for maximum response length of 20480 tokens during training. The window size and the temperature τ in our curriculum learning framework is set to 300 and 0.1, respectively. 4.2 Effectiveness of DUMP To evaluate the effectiveness of DUMP in improving post-training efficiency and performance, we compare it against uniform distribution sampling baseline across 12 distinct data distributions in the K&K puzzle dataset. Each distribution corresponds to fixed number of characters in the puzzle, ranging from 3 to 14. Figure 2 plots the test answer reward over training steps for each distribution, with and without DUMP. Across all distributions, DUMP consistently outperforms the baseline, achieving faster convergence and higher test performance. The gains are particularly notable in midto high-difficulty distributions (e.g., 6 to 12 characters), where uniform sampling 8 tends to struggle due to data underutilization. For example, in the 9-character distribution (Figure 2g), the model trained with DUMP achieves reward of over 0.5, whereas the baseline remains below 0.0. These results validate the core intuition of DUMP: dynamically adjusting the sampling focus toward high-learnability distributions accelerates policy improvement while avoiding wasted effort on over-saturated or low-signal data. Notably, the improvement is achieved without any curriculum heuristics or manual data orderingonly by observing advantage signals and adapting online. 4.3 Analyzing the Automated Curriculum by DUMP To understand how DUMP dynamically allocates training effort across data distributions, we analyze the sampling patterns induced by its UCB-based curriculum mechanism. Figure 3 shows the cumulative number of samples drawn from each distribution (3 to 14 characters) over the course of training. We observe clear curriculum-like progression: distributions corresponding to simpler puzzles (e.g., 35 characters) are heavily sampled in the early stages of training, while more complex distributions (e.g., 1014 characters) are gradually introduced and increasingly prioritized as training progresses. This pattern aligns with the models evolving capacityearly training favors distributions with high initial advantage magnitudes, and as the model saturates on those, DUMP shifts focus to underexplored but learnable distributions. Importantly, this adaptive sampling behavior emerges automatically from empirical advantage signals without requiring manual specification of curriculum order. These results highlight DUMPs ability to construct an implicit, data-driven curriculum that mirrors traditional easy-to-hard strategies, while remaining responsive to online training dynamics."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduce distribution-level curriculum learning framework for RL-based posttraining of large language models. DUMP leverages the expected absolute advantage as learnability signal to adaptively allocate training focus across heterogeneous distributions. By formalizing scheduling as multi-armed bandit and adopting UCB-based sampling strategy, DUMP balances exploitation and exploration in principled way. Experiments demonstrate that DUMP consistently improves convergence and final performance over baselines. These results highlight the value of distribution-aware curriculum learning in LLM RL post-training."
        },
        {
            "title": "References",
            "content": "[1] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [2] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [3] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [4] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [5] Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaiev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, et al. Competitive programming with large reasoning models. arXiv preprint arXiv:2502.06807, 2025. [6] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods In International Conference on Machine Learning, pages for effective instruction tuning. 2263122648. PMLR, 2023. 9 [7] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, et al. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv e-prints, pages arXiv2309, 2023. [8] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. T\" ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. [9] ByteDance Seed. Seed-thinking-v1.5: Advancing superb reasoning models with reinforcement learning. Technical report, ByteDance, 2025. URL https://github.com/ ByteDance-Seed/Seed-Thinking-v1.5. [10] Pulkit Pattnaik, Rishabh Maheshwary, Kelechi Ogueji, Vikas Yadav, and Sathwik Tejaswi Madhusudhan. Enhancing alignment using curriculum learning & ranked preferences. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1289112907, 2024. [11] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [12] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. [13] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. [14] Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. [15] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [16] Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022. [17] OpenAI. Learning to reason with llms. Technical report, OpenAI, 2024. URL https: //openai.com/index/learning-to-reason-with-llms/. [18] OpenAI. Openai o3-mini. Technical report, OpenAI, 2025. URL https://openai.com/ index/openai-o3-mini/. [19] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [20] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [21] Aime_1983_2024 (revision 6283828), 2025. URL https://huggingface.co/datasets/ di-zhang-fdu/AIME_1983_2024. [22] Mikhail Mirzayanov. Codeforces. https://codeforces.com/. Accessed: 2025-04-13. [23] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [24] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 4148, 2009. [25] Alex Graves, Marc Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated curriculum learning for neural networks. In international conference on machine learning, pages 13111320. Pmlr, 2017. [26] Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Julian Togelius, and Sebastian Risi. Illuminating generalization in deep reinforcement learning through procedural level generation. arXiv preprint arXiv:1806.10729, 2018. [27] Rui Wang, Joel Lehman, Jeff Clune, and Kenneth Stanley. Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions. arXiv preprint arXiv:1901.01753, 2019. [28] Richard Li, Allan Jabri, Trevor Darrell, and Pulkit Agrawal. Towards practical multi-object manipulation using relational reinforcement learning. In 2020 ieee international conference on robotics and automation (icra), pages 40514058. IEEE, 2020. [29] Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacherstudent curriculum learning. IEEE transactions on neural networks and learning systems, 31(9):37323740, 2019. [30] Rémy Portelas, Cédric Colas, Katja Hofmann, and Pierre-Yves Oudeyer. Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments. In Conference on Robot Learning, pages 835853. PMLR, 2020. [31] Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, and Donghyun Kwak. Online difficulty filtering for reasoning oriented reinforcement learning. arXiv preprint arXiv:2504.03380, 2025. [32] Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, et al. Training large language models for reasoning through reverse curriculum reinforcement learning. arXiv preprint arXiv:2402.05808, 2024. [33] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47:235256, 2002. [34] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [35] Chulin Xie, Yangsibo Huang, Chiyuan Zhang, Da Yu, Xinyun Chen, Bill Yuchen Lin, Bo Li, Badih Ghazi, and Ravi Kumar. On memorization of large language models in logical reasoning. arXiv preprint arXiv:2410.23123, 2024. [36] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. 11 Proof for Theorem 3.1 Theorem A.1 (Expected Advantage Magnitude Reflects Learnability). Given policy πθ and data distribution d, the expected absolute advantage Exd serves as proxy for how much that distribution can help the model improve, where the distribution consisting of prompts d, each prompt has group of sampled outputs {o1, . . . , on}, and ˆAi denotes the advantage of output oi. (cid:104) Eoiπθ(x) ˆAi (cid:105)(cid:105) (cid:104) Proof. Let πθ be the current model policy. Consider data distribution d, where are prompts and {o1, . . . , on} πθ(x) are sampled outputs. For each output oi, the advantage is estimated as ˆAi = ri b(x), where ri is the reward assigned to oi and b(x) is baseline (e.g., the mean reward over the group). The policy gradient under common policy-gradient methods (e.g., PPO or GRPO) can be written as: θJ (θ) = Exd (cid:104) Eoiπθ(x) (cid:104) ˆAi θ log πθ(oi x) (cid:105)(cid:105) . Now consider the magnitude of the gradient vector. The strength of the training signal from depends on the expected norm of the gradient, which is bounded below by: θJ (θ) Exd (cid:104) Eoiπθ(x) (cid:104) ˆAi θ log πθ(oi x) (cid:105)(cid:105) . Assuming that θ log πθ(oi x) is bounded and varies slowly across d, the dominant term affecting the gradient norm is: Exd (cid:104) Eoiπθ(x) (cid:104) ˆAi (cid:105)(cid:105) . Thus, the expected absolute advantage serves as proxy for the learning signal magnitude contributed by distribution d. The expected absolute advantage reflects how much training on distribution can improve the model parameters, making it suitable signal for curriculum scheduling. Theoretical Justification for UCB-Based Distribution Scheduling We provide theoretical justification for using Upper Confidence Bound (UCB) as strategy for scheduling training over data distributions in RL-based post-training. Our objective is to maximize the cumulative learnability gain over training steps, defined as: max {dt}T t=1 (cid:88) t=1 L(dt), where L(d) = Exd (cid:104) Eoπθ(x) (cid:104) ˆA(o) (cid:105)(cid:105) . This setting can be viewed as stochastic multi-armed bandit (MAB) problem, where each data distribution dj corresponds to an arm with unknown reward L(dj), interpreted as the expected absolute advantage from training on samples from dj. At each training step t, the learner selects distribution dt and obtains an empirical reward ˆL(dt) by averaging the absolute advantages observed in the batch. We define the regret as the gap between the cumulative learnability gain of the best fixed distribution = arg maxd L(d) and that of the learners actual selections: Regret(T ) = (cid:88) t=1 L(d) (cid:88) t=1 L(dt). To analyze this regret, we make the following assumptions: 1. For each distribution dj, the per-output absolute advantages ˆA(o), where πθ(x), are i.i.d. and bounded in [0, C] for some constant > 0. 2. The true expected advantage L(dj) remains approximately stationary over local training window, enabling meaningful online adaptation. Note: In practice, we can clip or normalize ˆA(o) to satisfy the boundedness condition. The introduction of the constant only scales the regret by constant factor and does not affect the asymptotic rate of convergence. Under these assumptions, the following regret bound holds: Theorem B.1. Let = {d1, . . . , dN } be set of data distributions with fixed expected rewards L(dj) [0, C]. Then, applying the UCB1 algorithm to the empirical reward observations yields the regret bound: Regret(T ) C (cid:88) j:j >0 log , where = L(d) L(dj). Proof. This result is direct application of the classical UCB1 regret bound [33], extended to the case where reward values lie in [0, C]. Let = arg maxd L(d) be the optimal distribution, and let = L(d) L(dj) denote the suboptimality gap for each arm dj. At each time step t, UCB1 selects the distribution dj with the highest upper confidence bound: UCB(dj) = ˆL(dj) + (cid:115) 2C 2 log nj , where nj is the number of times distribution dj has been sampled so far, and ˆL(dj) is the empirical mean of observed rewards (mean absolute advantages). Under the assumptions that rewards are i.i.d. and bounded in [0, C], the Hoeffding inequality guarantees that with high probability the empirical mean concentrates around the true mean L(dj), and the UCB selection mechanism will only pick suboptimal arms logarithmic number of times. Based on UCB1 regret bound [33], The cumulative regret is therefore bounded by: Regret(T ) (cid:88) j:j >0 (cid:18) 8C 2 log (cid:19) + O(j) , which simplifies to the stated asymptotic bound: Regret(T ) = . log (cid:88) j:j >0 This result shows that our distribution-level scheduling strategy, when driven by UCB over empirical advantage rewards, is provably efficient. It dynamically concentrates training on distributions with high estimated learnability while ensuring sufficient exploration, with regret that scales logarithmically in and linearly in 1/j."
        }
    ],
    "affiliations": [
        "Adobe Inc.",
        "Rutgers University"
    ]
}