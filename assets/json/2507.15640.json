{
    "paper_title": "Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training",
    "authors": [
        "Kailai Yang",
        "Xiao Liu",
        "Lei Ji",
        "Hao Li",
        "Yeyun Gong",
        "Peng Cheng",
        "Mao Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Continual pre-training on small-scale task-specific data is an effective method for improving large language models in new target fields, yet it risks catastrophic forgetting of their original capabilities. A common solution is to re-weight training data mixtures from source and target fields on a domain space to achieve balanced performance. Previous domain reweighting strategies rely on manual designation with certain heuristics based on human intuition or empirical results. In this work, we prove that more general heuristics can be parameterized by proposing Data Mixing Agent, the first model-based, end-to-end framework that learns to re-weight domains. The agent learns generalizable heuristics through reinforcement learning on large quantities of data mixing trajectories with corresponding feedback from an evaluation environment. Experiments in continual pre-training on math reasoning show that Data Mixing Agent outperforms strong baselines in achieving balanced performance across source and target field benchmarks. Furthermore, it generalizes well across unseen source fields, target models, and domain spaces without retraining. Direct application to the code generation field also indicates its adaptability across target domains. Further analysis showcases the agents' well-aligned heuristics with human intuitions and their efficiency in achieving superior model performance with less source-field data."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 0 4 6 5 1 . 7 0 5 2 : r Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training Kailai Yang1* Xiao Liu2 Lei Ji2 Hao Li1 Yeyun Gong2 Peng Cheng2 Mao Yang2 1 The University of Manchester 2 Microsoft Research {kailai.yang,hao.li-2}@manchester.ac.uk {xiaoliu2,leiji,yegong,pengc,maoyang}@microsoft.com"
        },
        {
            "title": "Abstract",
            "content": "Continual pre-training on small-scale task-specific data is an effective method for improving large language models in new target fields, yet it risks catastrophic forgetting of their original capabilities. common solution is to re-weight training data mixtures from source and target fields on domain space to achieve balanced performance. Previous domain reweighting strategies rely on manual designation with certain heuristics based on human intuition or empirical results. In this work, we prove that more general heuristics can be parameterized by proposing Data Mixing Agent, the first model-based, end-to-end framework that learns to reweight domains. The agent learns generalizable heuristics through reinforcement learning on large quantities of data mixing trajectories with corresponding feedback from an evaluation environment. Experiments in continual pre-training on math reasoning show that Data Mixing Agent outperforms strong baselines in achieving balanced performance across source and target field benchmarks. Furthermore, it generalizes well across unseen source fields, target models, and domain spaces without retraining. Direct application to the code generation field also indicates its adaptability across target domains. Further analysis showcases the agents wellaligned heuristics with human intuitions and their efficiency in achieving superior model performance with less source-field data."
        },
        {
            "title": "Introduction",
            "content": "Modern Large Language Models (LLMs) are usually pre-trained with trillion-token large-scale general domain datasets (Yang et al., 2025; Liu et al., 2024a). Despite with strong generalization capabilities, these foundation models often require further enhancement in certain knowledge-intensive fields, such as applications in math problem solving (Yang et al., 2024; Shao et al., 2024) or code generation (Guo et al., 2024; Hui et al., 2024). Due to the formidable cost of pre-training from scratch, adapting foundation models to new knowledge/capabilities is usually achieved via continual pre-training on smaller-scale, high-quality data in the target field. However, directly adapting to the target field data can lead to catastrophic forgetting (Dyer et al., 2022) of source data and collapse on existing model capabilities (Lin et al., 2025), normally due to the significant distribution shift between source and target fields. popular solution is to curate data mixtures of the source and target fields to achieve balanced performance (Shi et al., 2024). Existing methods mainly organize data mixtures defined by metaattributes such as data sources and focus, known as domains (Du et al., 2022; Luo et al., 2024). *Work done during his internship at Microsoft Research. Corresponding authors. Preprint. Under review. During training, the data mixture is allocated through distribution in the domain space, which reflects the ratio of data allocated from each domain by reweighting. The distribution can be adjusted after several training steps when necessary, leading to data mixing trajectory (Luo et al., 2024; Xia et al., 2023) along the domain reweighting steps. Data mixing trajectories are proven to significantly influence model performance (OLMo et al., 2024; Grattafiori et al., 2024; Li et al., 2024), which leads previous works to explore various data mixing algorithms for determining the optimal trajectory for various tasks (Liu et al., 2024b; Ye et al., 2024; Xie et al., 2023; Luo et al., 2024; Xia et al., 2023). commonality of these data mixing methods is that their designations are all based on certain general heuristics, such as \"data mixtures that provide balanced evaluation loss lead to desired downstream performance\". Another indication of these heuristics is the various empirical conclusions drawn from training practices. For example, Wettig et al. (2025) concluded that \"Data from Science domain heavily promote model performance on MMLU (Hendrycks et al., 2020), while the Home domain improves HellaSwag (Zellers et al., 2019) performance\". In Fig. 1, we further provide an average of distributions along 20 randomly generated data mixing trajectories, separated by increasing/decreasing performance on the MMLU and MATH (Hendrycks et al., 2021) benchmarks. The results show an explicit gap between data distributions that increase/decrease model performance. For example, in MMLU, increasing the DCLM data from the Science and Home&Garden domains improves benchmark performance. In MATH, increasing Dolmino-math data from the Hobbies&Leisure and Real estate domains while keeping balanced mix of DCLM data is likely to improve benchmark performance. These results prove the existence of more general heuristics for data mixing in continual pre-training. Figure 1: Four averaged distributions drawn from 20 randomly generated data mixing trajectories. Each distribution in the trajectories is first categorized by whether it increases/decreases the performance of 50M target model on the MMLU/MATH benchmarks within one re-weighting step. Each category is then averaged to obtain the corresponding distribution in the figure. The models are trained on 52-dimensional space (more details in Sec. 3.1), mixing the DCLM (Li et al., 2024) and the math split of the Dolmino-mix-1124 (OLMo et al., 2024) dataset. 2 The above arguments reveal potentially rich heuristic space for domain reweighting. We believe these modeland data-agnostic heuristics can be unified into small agent model to guide the data mixing trajectories in an end-to-end manner. Based on this intuition, we propose Data Mixing Agent, the first model-based method that learns to re-weight domains for continual pre-training. We start by randomly sampling large quantities of data mixing trajectories, each with fixed domain re-weighting steps. We then train small proxy models on all trajectories, obtaining model checkpoints on each re-weighting step. All checkpoints are then evaluated on light-weight but accurate evaluation environment to assess targeted capabilities. The sampled trajectories and corresponding environment feedback are expected to empirically enclose considerable range of heuristics. Data Mixing Agent is then trained via random sampling on these collected data, and optimized in an off-policy reinforcement learning manner using the Conservative Q-Learning (CQL) algorithm (Kumar et al., 2020). During continual pre-training on the target model, Data Mixing Agent directly predicts the domain distribution for the next data reweighting step on the fly, considering previous states in the data mixing trajectory and the environment feedback. We apply Data Mixing Agent for continual pre-training on the math reasoning target field while preserving performance in the general field. Evaluation on in-distribution source field data shows that Data Mixing Agent significantly outperforms the strong RegMix (Liu et al., 2024b) baseline, achieving an average improvement of 3.02% across 8 general benchmarks and 4 math reasoning benchmarks. The agents generalization ability is demonstrated by its balanced performance across 3 unseen source fields, 4 target models, and 2 domain spaces, all without retraining. We further use the trained agent on the math reasoning field directly to the unseen code generation target field, showing that its learned heuristics partially generalize across target domains. Additional analysis confirms that these heuristics align well with human intuitions, and Data Mixing Agent can efficiently leverage the data mixture to achieve superior continual pre-training performance with less source-field data. In summary, this work makes the following contributions: We propose the framework of Data Mixing Agent, the first model-based, lightweight domain reweighting method for continual pre-training, guiding the training recipe for the target model in an end-to-end manner; Extensive experiments prove the effectiveness of data mixing agents in alleviating catastrophic forgetting in continual pre-training and achieving balanced performance across model capabilities; Data mixing agent learns heuristics that generalize across source and target fields, target models, and domain spaces, enabling its application in multiple scenarios."
        },
        {
            "title": "2 Domain Re-weighting as Markov Decision Process",
            "content": "In this section, we formally state domain re-weighting as Markov Decision Process (MDP), defined as tuple (S, A, f, r, ρs, ρe). We describe each element as follows: State Space The state space is continuous space consisting of all data distributions from previous domain reweighting steps. Specifically, for step t, the state st has dimension st RtN , where is the dimension of the action space, determined by the definition of the domains. Action Space The action space is continuous space denoting the data distribution in the current domain reweighting step. At step t, the action at is probability distribution over the domain space: at RN and (cid:80)N = 1, ai 0. i=1 ai Policy and Reward The policy function denotes the model-based agent for guiding domain reweighting, which directly determines the action at the current step based on previous states and environment feedback. The feedback are modeled by the reward function r, determined by the target fields for continual pre-training and the evaluation environment design. With domain re-weighting modeled as MDP, the data mixing agent can be optimized via reinforcement learning (Kumar et al., 2020). Start and Terminate State The start state ρs depends on the target model checkpoint, reflecting the domain distribution of its pre-training data. The terminate state ρs depends on manual setting or 3 data scale, as the training process can often end with predefined token budgets or exhaustion of target field data."
        },
        {
            "title": "3 Data Mixing Agent",
            "content": "In this section, we introduce the methodology of Data Mixing Agent, mainly including three procedures: 1) modeling the heuristic space by randomly sampling data mixing trajectories and collecting feedback from an evaluation environment; 2) parameterizing the heuristic space by training the model-based agent on the collected trajectories and feedback via conservative Q-learning; 3) utilizing the data mixing agent to guide the domain reweighting for the target models on the fly to achieve balanced performance. An overview of the pipeline is shown in Fig. 2. Figure 2: An overview of the training and domain reweighting pipeline of the data mixing agent. We first sample large quantities of data mixing trajectories and train small proxy models on them. Each model checkpoint obtains feedback from the evaluation environment. Secondly, the data mixing agent is optimized on these trajectories and feedback via supervised fine-tuning and off-policy reinforcement learning with CQL-based function. During guiding the training of the target model, the agent directly determines the distribution for the next domain re-weighting step on the fly. 3.1 Modeling the Heuristic Space with Trajectory Sampling Action Space Definition We start by defining the action space A, which is essential for trajectory sampling. While most methods define the space via data sources (Xia et al., 2023; Luo et al., 2024; OLMo et al., 2024), recent work has emphasized the drawbacks of data overlap across domains (Xi et al., 2025) and the unstructured nature (Wettig et al., 2025) of source-based data clustering. Inspired by Wettig et al. (2025), we construct domains with the Nvidia domain classifier 1, which classifies the data from the source and target fields, each into 26 domains, leading to 54-dimensional data distribution space. The full domain definitions can be found in Fig. 1. Start State Estimation The start state ρs can easily be determined when data from the source field are available. We randomly sample 1B tokens from the training data and utilize the same Nvidia domain classifier to organize the data into the defined domains. The start state is then estimated as the normalization of the sample numbers in each domain. In scenarios where the data from the source field are unavailable (Grattafiori et al., 2024; Liu et al., 2024a), we explore randomly sampled data from the target model as estimates for the start state. To prove the viability of this method, we experiment on five Pythia models (Biderman et al., 2023), as they are trained on the same open-source Pile dataset (Gao et al., 2020), where the ground-truth start state can be calculated. Specifically, we 1https://huggingface.co/nvidia/domain-classifier 4 Figure 3: The KL divergence between the estimated start state by sampled data from the target model and the ground-truth distribution obtained from the Pile dataset. The results are averages of 5 random runs. sample tokens simply by pre-pending the start-of-sentence token to start generation with default temperature of 1.0. The generated data are then passed through the domain classifier to estimate the start state. We calculate the KL divergence between the estimated start state and the ground-truth distribution obtained from the Pile dataset, and the results are presented in Fig. 3. As shown, sampling over 2,000 data points leads to KL divergence of less than 0.1 on 4 out of 5 Pythia models. The estimated distribution converges on most models with over 3,000 samples. Larger models also show more accurate estimates and faster convergence rate, possibly due to the higher quality of their generated data. The above results prove the effectiveness of using random samples from the target model as estimates for their start states. Data Mixing Trajectory Sampling We randomly sample data mixing trajectories as the foundation of the training data for modeling the heuristic space. The random sampling process is based on the following principle: The data mixing trajectories should be well-distributed across the defined action space, ensuring coverage of actions that both enhance and degrade model performance. To ensure this principle, we design inductive scoring algorithms to rate each sampled distribution, guiding the next-action selection process. The detailed algorithm for the sampling process is provided in Algorithm 1. The function CalculateInductiveScores describes the scoring algorithm. This function is designed based on three inductive biases that denote potentially good distribution: The data re-weighting distribution at the current step should not deviate significantly from that of the previous step; As the data re-weighting progresses, the distribution at each step should gradually align more closely with the target distribution; The distribution at the current step should differ from those at the same step in previously sampled trajectories to encourage diversity. The target distribution is defined as the complement of the start state: probabilities for source-field domains are set to zero, while those for target-field domains are estimated based on their empirical distribution in the target field data. The target distribution encourages trajectories to gradually reduce reliance on source-field data and increase the coverage of target-field data, thereby accelerating the continual pre-training process. 5 Algorithm 1: Data Mixing Trajectory Sampling with Top-K Inductive Biases Input: Source field Data S, Target field data , Path sampling number , Max data reweighting steps , Reweighting sample number per step R, Inductive threshold Output: Sampled trajectories 1 GetDomainConfig() ; 2 ρs GetStartState(S, D) ; 3 ρt GetTargetState(T , D) ; 4 TM ; 5 [ ] ; 6 for 1 to do 0, 0, ρ ρs, τ [ρs] ; while < do [ ] ; for 1 to 20000; do // Load the domain space based on definitions. // Estimate start state from source data S. // Estimate target state from source data S. // Max data samples for each trajectory. // Initialize empty trajectory list. // Initialize the current trajectory. // Reset candidate list. // Repeat the sampling 20,000 times. // Randomly sample distribution. ρ RandomProbability(D) ; CalculateInductiveScores(d, ρ, , τ , ρ, ρt) ; Append (ρ, s) to ; ˆρ RandomTopK(C, K) ; // Randomly select from top-K candidates with lowest inductive scores. Append ˆρ to τ ; ρ ˆρ, + 1 ; c+ TargetSamplesCovered(ˆρ, R) ; number. if then break ; // Early stopping if target data is fully covered. // Track covered target sample // Update current trajectory. Append τ to ; 21 22 Function CalculateInductiveScores(d, ρ, , τ , ρ, ρt): // Store the current trajectory. sc KL(ρ ρ) ; // KL divergence between the current and last action. st KL(ρt ρ) ; // KL divergence between the current action and target state. sd 0 ; if > 0 then [ ] ; foreach τ do if < τ then // set to store the similarities. {KL(τ [d] ρ)} ; // Calculate similarities to states in previous trajectories. if > 0 then (cid:80) sd 1 return α sc + β σ (cid:0) xS ; // Average similarity to previous states (cid:1) st γ sd ; // Final inductive score 6 9 10 11 12 13 15 16 17 18 19 24 25 26 27 28 30 31 32 33 During implementation, we use 100B random tokens from the DCLM (Li et al., 2024) as the source field data S, and the math split (about 10B tokens) of the Dolmino-mix-1124 (OLMo et al., 2024) dataset as the target field data . The max data reweighting steps = 80, and the reweighting sample number per step is set to 8K. To ensure inclusion of both high-quality and low-quality trajectories, we run Algorithm 1 four times, each with the path sampling number = 96 and the threshold set to 1, 100, 1000, and 10,000, leading to the trajectory set with subsets Ttop1, Ttop100, Ttop1000, and Ttop10000, with 384 trajectories in total. Evaluation Environment Design and Feedback Collection The evaluation environment is manually curated to assess model checkpoints. It is designed to be lightweight, yet accurately reflect target capabilities, providing effective supervision signals while minimizing computational overhead. Specifically, we select small high-quality evaluation set Di that well represents the i-th target field: {qj, rj}Di j=1. For the model checkpoint M, we compute the average per-token log probability on all question-answer pairs to reflect model performance on the i-th target field: Score(M, Di) = 1 Di (cid:88) (qj ,rj )Di 1 rj log PM(rj qj) The final environment feedback returns vector-style assessment for model M: reward(M) = (cid:2)Score(M, D1), Score(M, D2), ..., Score(M, DD)(cid:3) (1) (2) During implementation, the environment assesses the general capability of the checkpoints via the validation set of the MMLU dataset 2 with 1,531 high-quality general-domain questions and answers. The math reasoning capability is evaluated with 1,500 random samples from the training split of the MATH dataset 3. Leveraging this evaluation environment, we collect feedback data by training small proxy model Mp with the LLaMA3 structure and 50M parameters on each sampled trajectory from scratch. The model checkpoint is evaluated on this environment at each data reweighting step, resulting in 27,266 feedbacks. Formally, for the i-th data mixing distribution ρi τ , we obtain tuple (ρi, reward(Mi p) denotes the environment feedback for the model checkpoint Mi after training on the i-th domain reweighting step. Notably, the feedback at the start state is obtained with the initialized base proxy model. p)), where reward(Mi 3.2 Parameterize the Heuristic Space with Reinforcement Learning We expect the sampled data mixing trajectories and the feedback from the environment to well represent the heuristic space for domain reweighting. We further parameterize these heuristics by training model-based agent on these trajectories in reinforcement learning-based paradigm. Agent Model Structure We determine the model structure for the data mixing agent with the following principles: The structure should be designed to effectively model temporal sequences and support long-range interactions between data mixing distributions; The data mixing agent should be lightweight to enable fast, low-cost inference and prevent unacceptable latency during target model training. Based on the above principles, we utilize the Transformer (Vaswani et al., 2017) decoder architecture, which is widely applied to time series forecasting tasks (Zhang et al., 2024; Li et al., 2025) and facilitates long-range interactions between data points with its dot-product attention mechanism. To ensure fast inference, we stack two layers of Transformer, followed by linear layer and Softmax to project the representations to the action space, with merely 2.1M parameters. Formally, at data reweighting step t, the agent predicts the domain distribution with the previous trajectory and environment feedback as follows: ρt = (ρ0, ρ1, ..., ρt1) ρi = [ρi; reward(Mi)] , = 0, ..., 1 where ; denotes concatenation, ρi RN +D denotes the input feature in the data reweighting step i, and ρt denotes the agents output action at step t. (3) 2https://huggingface.co/datasets/cais/mmlu 3https://huggingface.co/datasets/EleutherAI/hendrycks_math SFT-based Warming Up We first perform Supervised Fine-Tuning (SFT) to reduce the parameter searching space in the reinforcement learning phase. We train the agent from scratch on the highquality Ttop1 trajectories with simple MSE loss. At the data reweighting step t, the agent is optimized as follows: LSF = (cid:88) (ˆρt (ρ0, ρ1, ..., ρt1))2 (4) where ˆρt denotes the ground-truth distribution in step t. Notably, before the SFT process, we standardize the environment feedback on each target field across data reweighting steps within all trajectories by forcing their mean value to 0 and standard deviation to 1. This is to regularize the reward space for the agent and avoid out-of-distribution rewards from unseen target models. The feedback for later reinforcement learning and agent inference processes also utilizes this standardization procedure. Off-policy Optimization with Conservative Q-Learning Based on the warmed-up agent model, we further parameterize the heuristic space via reinforcement learning, where the algorithm selection is based on the following two principles: The agent model is trained in an offline, off-policy setting using data collected from proxy models, without access to the evaluation environment during training; The agents actions are probability distributions sampled from continuous domain space. Following the above principles, we select Conservative Q-Learning (CQL) (Kumar et al., 2020) as the optimization algorithm. CQL prevents overestimation of Q-values for out-of-distribution actions by encouraging the learned Q-function to be conservative. Specifically, CQL introduces conservative penalty for the Q-function optimization process: LCQL(Q) = E(s,a,r,s)D (cid:124) (cid:20)(cid:16) Q(s, a) (cid:16) + γ max Q(s, a) (cid:17)(cid:17)2(cid:21) (cid:125) (cid:123)(cid:122) Bellman error (cid:32) (cid:34) + α EsD log (cid:124) (cid:88) exp(Q(s, a)) E(s,a)D[Q(s, a)] (cid:123)(cid:122) Conservative penalty (cid:125) (cid:35) (cid:33) (5) CQL is then trained in an actor-critic (Sutton et al., 1999) structure, where the data agent acts as the actor model, and another neural network is initialized from scratch as the critic model (Q-function). During implementation, we randomly sample fragments τ (dont have to be full trajectories) from the data mixing trajectory set . At domain re-weighting step t, = [ρ0, ρ1, ..., ρt1], = ρt, and = [ρ0, ρ1, ..., ρt]. The scalar reward value is obtained as the gain of linear combination of environment feedback reward(Mt p) compared to that of the last step: = (cid:88) i= λiScore(Mt p, Di) (cid:88) i=1 λiScore(Mt1 , Di) (6) During implementation, we set all coefficients to be equal: λi = 1 . The critic model is parameterized by another single-layer Transformer decoder, followed by linear layer and sigmoid function to project the representations into Q-value scalar, with the following inference function: Q(s, a) = (ρ0, ρ1, ..., ρt) (7) The actor and critic models are trained in this function until convergence. 3.3 Domain Reweighting with Data Mixing Agent The mechanism of the domain reweighting process with Data Mixing Agent is described in Algorithm 2. During continual pre-training, the agent directly determines the distribution for the next domain re-weighting step on the fly, considering the previous states in the data mixing trajectory and the corresponding environment feedback. This MDP continues until the target data is fully leveraged or predetermined computation budget is reached. We expect the agent to optimally curate the training recipe by balancing performance across all target fields while minimizing the use of source-field data 8 Algorithm 2: Continal Pre-training with Data Mixing Agent Input: domains of source data {S1, S2, ..., SN } and target data {T1, T2, ..., TN }, the agent , Max data reweighting steps Mtgt, Reweighting sample number per step Rtgt, the target model Mtgt, the evluation environment E. Output: The continually pretrained target model checkpoint ˆMtgt // Load the domain space based on definitions. // Estimate start state from source data S. // Initialize the current model checkpoint. // Initialize trajectory list. // Initialize feedback list. 1 GetDomainConfig() ; 2 ρs GetStartState(S, D) ; 3 ˆMtgt Mtgt ; 4 Ttgt [ρs] ; 5 rewardtgt ; 6 0 7 for 1 to Mtgt do // Get environment feedback for // Update current feedback list. // Standardize the current feedback list. // Concatenate each trajectory with the // Obtain the data reweighting distribution from // Sample batch from the domain data reward( ˆMtgt) GetEnvFeedback( ˆMtgt, E) ; the current model checkpoint. Append reward( ˆMtgt) to rewardtgt ; rewards tgt std(rewardtgt); {ρ} concat(Ttgt, rewards tgt); corresponding feedback. ρt (ρ1, ρ2, ..., ρt1); the data mixing agent. Bt sample({Si}N according to the current distribution. Update weights for ˆMtgt with the training loss L( ˆMtgt, Bt); Append ρt to Ttgt; c+ TargetSamplesCovered(ˆρ, R) ; if then break ; 1 ,{Ti}N 1 ,ρt); 9 10 11 12 14 15 16 17 18 // Track covered target sample number. // Early stopping if target data is fully covered. tokens to reduce computational cost. We also expect the agents learned heuristics to generalize to unseen target models, data mixtures, and even target domains. This generalization is crucial to avoid repeated trajectory sampling and agent retraining when adapting to new continual pre-training scenarios, thereby significantly reducing overall computational cost."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we evaluate the performance of Data Mixing Agent in improving the math reasoning and code generation capabilities of target models via continual pretraining. We comprehensively compare the performance of the agent to strong baseline methods across 4 target models on 8 general benchmarks, 4 math reasoning benchmarks, and 2 code generation benchmarks. 4.1 Experimental Settings Target Models We aim to rigorously evaluate domain reweighting methods on target models that do not possess math or coding capabilities. Since most open-source models have been optimized on large-scale data from the math reasoning or code generation field, we pre-train three models from scratch, with the same LLaMA3 model architecture (Grattafiori et al., 2024) of 32 Transformer layers and 3B model parameters, on 100B randomly sampled tokens from the DCLM (Li et al., 2024), Fineweb-Edu (Penedo et al., 2024), and Nemotron-CC (Su et al., 2024) dataset, resulting in three target models: LLaMA-3B-DCLM-100B, LLaMA-3B-FWE-100B, LLaMA-3B-Nemotron-100B. We also include the Pythia-1.4B model (Biderman et al., 2023) to evaluate performance on existing open-source models and scenarios when data from the source field is not directly available. Baseline Methods We compare Data Mixing Agent (DataAgentRL) with the following baseline methods: 9 Base Model: direct evaluation of the target models on the benchmarks, reflecting model capabilities before the continual pertaining phase; Naive Training: continually training the base model on data from the target field without curating any data mixtures from source-field data; RegMix (Liu et al., 2024b): one of the state-of-the-art domain re-weighting methods. It trains large quantities of 1B-sized small proxy models (512 models in our implementation) on random domain distributions, then evaluates these models on the target benchmarks. The best data mixing recipe is determined by fitting regression model to the feedback and selecting distributions that lead to the highest scores; DataAgentSF : the data mixing agent model without the reinforcement learning process. The model mostly provides heuristically appropriate trajectories because its only fine-tuned on the Ttop1 dataset. We include this baseline method to assess the effectiveness of off-policy optimization with CQL. Target Model Training Data For data from the source field, the self-pretrained LLaMA-3B models utilize their corresponding pre-training data, each with 100B tokens. We use randomly sampled data from the Pythia-1.4B model as the source field data for itself, applying the agent in scenarios where the data from the source field is not directly available. Following the method in Sec. 3.1, we sample 10B tokens by pre-pending the start-of-sentence token to start generation with default temperature of 1.0. The generated data are then filtered through the Nvidia text quality classifier 4, where all data within the \"Low quality\" class are discarded, resulting in source field dataset with around 7.7B tokens. For data from the math reasoning field, we select the math split (10B tokens) of the Dolmino-mix-1124 dataset 5, which was used for the mid-training process of the OLMo2 model series (OLMo et al., 2024), including data sources such as TuluMath (Ivison et al., 2024), MathCoder (Wang et al., 2023), and Metamath (Yu et al., 2023). For data from the code generation field, we select the GitHub training split 6 of the SlimPajama-DC dataset (Shen et al., 2023) with 30B tokens. Evaluation Benchmarks We evaluate target models general capabilities by evaluating with the lm_eval evaluation library 7 on the MMLU (Hendrycks et al., 2020), HellaSwag (Zellers et al., 2019) (Hella.), OpenBookQA (Mihaylov et al., 2018) (OBQA), Winogrande (Sakaguchi et al., 2021) (Wino.), ARC-Challenge (Clark et al., 2018) (ARC-C), PiQA (Bisk et al., 2020), SciQ (Welbl et al., 2017), and LogiQA (Liu et al., 2020) benchmarks. We evaluate the math reasoning capabilities using the math_lm_eval library 8 on the GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), Minerva (Lewkowycz et al., 2022), and MathQA (Amini et al., 2019) benchmarks. We evaluate the code generation capabilities using the eval_plus library 9 on the HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) benchmarks. The MMLU and GSM8K benchmarks are evaluated with 5-shot setting, the Minerva benchmark is evaluated in 4-shot setting. Other benchmarks are evaluated with zero-shot setting. Target Model Training Setting Firstly, the agent is trained based on 26-dimensional domain definition, leading to 52-dimensional domain reweighting space (shown in Sec. 3.1). To evaluate on different domain spaces, we further employ the data mixing agent on the original 2-dimensional space based on data sources (source and target). Note that this action does not require the agent to retrain, as its action can be directly converted by summing the 26 probabilities for source/target fields into single dimension, still preserving probability distribution. During training, we set the number of reweighting samples per step Rtgt to 64K and the maximum data reweighting steps Mtgt to 80. We use the same evaluation environment as the Data Mixing Agent training when the target field is math reasoning, and change the MATH validation set to 1,000 random samples from the GitHub validation split of the SlimPajama-DC dataset. Due to resource limits, we only train on the 2-dimensional data reweighting space for code generation. 4https://huggingface.co/nvidia/quality-classifier-deberta 5https://huggingface.co/datasets/allenai/dolmino-mix-1124 6https://huggingface.co/datasets/MBZUAI-LLM/SlimPajama-627B-DC 7https://github.com/EleutherAI/lm-evaluation-harness 8https://github.com/ZubinGou/math-evaluation-harness 9https://github.com/evalplus/evalplus 10 Implementation We continually pre-train the target model in distributed manner on 8 nodes with total of 64 Nvidia A100 GPUs with 40GB of memory. The code for training the target model with data mixing agents is built upon the Megatron-LM framework (Shoeybi et al., 2019). The SFT-based warm-up stage is conducted on the OpenRLHF library (Hu et al., 2024). The CQL-based off-policy reinforcement learning framework is built on the d3rlpy (Seno and Imai, 2022) library with further modifications to support training on Huggingface Transformer models. Table 1: The evaluation results of continual pretraining on the math reasoning target field, reflected on 12 benchmarks. We also separately report the average results on general benchmarks, math reasoning benchmarks, and all benchmarks. (a) Model performances on the 2-dimensional data reweighting space based on data sources. Method Avg. MMLU Hella. OBQA Wino. ARC-C PiQA SciQ LogiQA Avg. GSM8K Minerva MATH MathQA Avg. General Benchmarks Math Benchmarks Base Model Naive Training RegMix DataAgentSF DataAgentRL Base Model Naive Training RegMix DataAgentSF DataAgentRL Base Model Naive Training RegMix DataAgentSF DataAgentRL Base Model Naive Training RegMix DataAgentSF DataAgentRL 38.15 38.51 44.01 44.95 47.03 37.65 38.51 43.83 45.23 45. 38.22 37.86 44.13 44.12 45.8 33.47 31.06 34.07 33.6 35.12 34.5 27.11 30.42 33.81 34.06 34.47 27.25 32.83 34.65 33.78 34.22 27.06 35.03 34.06 34.27 30.74 25.8 29.37 30.66 30. 64.5 37.0 59.72 60.23 63.38 60.52 37.03 60.2 60.83 60.44 64.51 37.4 65.15 64.25 63.95 52.0 26.76 48.3 45.23 48.5 37.0 28.2 36.6 34.2 42.14 37.8 28.4 36.45 38.28 38. 37.6 28.0 35.9 39.04 39.8 33.2 18.8 33.4 26.6 30.6 LLaMA-3B-DCLM-100B 84.2 68.7 85.1 87.3 87.89 36.69 28.58 34.73 36.26 36.92 75.84 60.28 73.88 73.28 74.85 LLaMA-3B-FWE-100B 85.4 69.6 86.5 85.6 84. 74.21 61.1 71.72 74.6 73.12 40.53 26.96 37.17 40.8 38.89 61.56 54.22 61.72 60.43 62.35 57.77 53.51 54.51 59.3 59.59 LLaMA-3B-Nemotron-100B 59.12 52.88 59.83 60.4 61. 57.3 41.93 43.88 43.83 41.17 36.26 27.05 36.45 38.1 38.74 75.57 59.74 72.85 74.17 74.49 Pythia-1.4B 28.33 21.81 21.16 22.25 25.66 70.89 52.95 65.7 61.94 65. 88.4 68.6 86.2 86.75 86.9 79.3 62.2 64.46 72.2 75.8 28.11 26.88 28.73 29.33 30.29 28.26 28.11 28.0 26.96 27.49 26.73 25.19 28.88 29.16 29.95 27.5 21.11 25.2 26.88 24. 52.8 41.37 51.36 51.86 54.04 52.37 41.5 50.92 52.63 52.13 52.8 40.74 52.54 53.24 53.71 47.41 33.92 41.43 41.2 42.72 2.55 59.21 55.87 57.84 59.24 2.5 58.91 55.9 56.26 58. 2.5 59.05 49.39 47.81 54.28 1.67 48.98 40.2 37.1 40.26 4.1 16.16 11.5 12.7 14.8 2.52 12.58 12.2 12.19 13.46 4.85 12.3 10.7 9.07 10.83 4.39 10.16 6.92 7.19 7. 4.22 22.85 17.7 21.3 22.75 4.06 24.7 20.35 21.92 23.96 5.3 24.52 19.23 17.8 22.94 2.1 14.64 7.93 7.85 8.75 24.52 32.96 32.16 32.73 35.3 23.72 33.94 30.1 31.32 33. 23.62 32.53 30.0 28.86 31.85 14.16 27.52 22.28 21.45 22.94 8.85 32.80 29.31 31.14 33.02 8.2 32.53 29.64 30.42 32.19 9.07 32.1 27.33 25.89 29.98 5.58 25.33 19.33 18.4 19. (b) Model performances on the 52-dimensional data reweighting space based on the Nvidia domain classifier. Method Avg. MMLU Hella. OBQA Wino. ARC-C PiQA SciQ LogiQA Avg. GSM8K Minerva MATH MathQA Avg. General Benchmarks Math Benchmarks Base Model Naive Training RegMix DataAgentSF DataAgentRL Base Model Naive Training RegMix DataAgentSF DataAgentRL Base Model Naive Training RegMix DataAgentSF DataAgentRL Base Model Naive Training RegMix DataAgentSF DataAgentRL 38.15 38.51 44.67 45.75 46.84 37.65 38.51 43.78 45.21 45. 38.22 37.86 44.86 45.16 45.9 33.47 31.06 34.64 34.01 35.25 34.5 27.11 34.38 34.47 32.99 34.47 27.25 35.64 34.27 32.55 34.22 27.06 35.68 34.8 33.89 30.74 25.8 29.6 30.95 28. 64.5 37.0 62.17 63.36 62.64 60.52 37.03 57.64 61.2 58.64 64.51 37.4 66.4 64.01 62.7 52.0 26.76 47.02 46.93 46.97 37.0 28.2 38.2 40.6 41.6 37.8 28.4 39.4 37.95 35. 37.6 28.0 41.4 38.2 42.6 33.2 18.8 34.78 32.2 28.8 LLaMA-3B-DCLM-100B 84.2 68.7 87.5 89.2 89.1 75.84 60.28 74.97 74.32 73.5 36.69 28.58 36.95 35.87 36.64 LLaMA-3B-FWE-100B 85.4 69.6 85.03 85.8 87. 40.53 26.96 38.2 40.28 39.76 74.21 61.1 67.4 75.17 73.34 61.56 54.22 61.93 62.35 63.98 57.77 53.51 57.56 58.57 59.42 LLaMA-3B-Nemotron-100B 59.12 52.88 61.56 60.73 59. 57.3 41.93 46.32 43.93 43.61 36.26 27.05 35.82 36.9 40.0 75.57 59.74 72.4 73.92 74.86 Pythia-1.4B 28.33 21.81 24.76 21.23 25.91 70.89 52.95 62.83 62.2 65. 88.4 68.6 87.53 87.3 84.34 79.3 62.2 69.3 68.8 71.31 28.11 26.88 29.49 29.96 31.5 28.26 28.11 29.34 28.23 29.35 26.73 25.19 29.34 29.37 28.29 27.5 21.11 27.03 27.88 23. 52.8 41.37 53.19 53.77 53.99 52.37 41.5 51.28 52.68 52.01 52.8 40.74 53.77 53.15 53.31 47.41 33.92 42.71 41.77 41.76 2.55 59.21 55.78 56.77 59.04 2.5 58.91 53.68 54.5 58. 2.5 59.05 48.17 52.5 56.78 1.67 48.98 38.28 38.84 45.44 4.1 16.16 10.36 11.12 16.48 2.52 12.58 10.84 13.27 14.68 4.85 12.3 10.91 10.7 11.82 4.39 10.16 7.24 7.19 8. 4.22 22.85 15.75 18.76 22.9 4.06 24.7 20.35 22.17 23.62 5.3 24.52 19.03 20.77 23.61 2.1 14.64 7.51 7.3 10.01 24.52 32.96 28.67 32.3 31.72 23.72 33.94 30.0 31.1 33. 23.62 32.53 30.04 32.71 32.03 14.16 27.52 21.1 20.68 25.38 8.85 32.80 27.64 29.74 32.54 8.2 32.53 28.72 30.26 32.65 9.07 32.1 27.04 29.17 31.06 5.58 25.33 18.53 18.5 22. 4.2 Evaluation Results on Math Reasoning The evaluation results of continual pretraining on the math reasoning target field are shown in Table 1. According to the results, we have the following observations: Naive training significantly improves target model performance on the target field but leads to drastic collapse on the capabilities of the source field. Compared to the base model, the average math reasoning performance increases by an average of 22.77% on the four target models, indicating the effectiveness of training on high-quality in-distribution data for the target field. However, the 11 Table 2: The evaluation results of continual pretraining on the code generation target field, reflected on 10 benchmarks. The data is reweighted based on the 2-dimensional domain space. Method Avg. MMLU Hella. OBQA Wino. ARC-C PiQA SciQ LogiQA Avg. HumanEval MBPP Avg. General Benchmarks Code Benchmarks Base Model Naive Training RegMix DataAgentSF DataAgentRL Base Model Naive Training RegMix DataAgentSF DataAgentRL 44.52 40.08 44.85 45.07 46.3 38.91 35.14 38.96 40.82 41.63 34.5 27.6 31.22 32.69 33.84 30.74 26.03 28.2 30.1 29.9 64.5 42.96 57.13 63.43 63.79 52.0 38.48 47.66 47.93 46. 37.0 29.37 33.4 35.0 37.8 33.2 24.4 28.6 30.44 31.8 LLaMA-3B-DCLM-100B 61.56 53.1 57.43 49.88 57.3 57.3 46.61 52.09 54.46 56.9 36.69 24.76 29.05 33.46 35. 75.84 70.5 76.1 72.85 73.2 Pythia-1.4B 28.33 21.59 26.76 26.76 27.35 70.89 59.87 64.3 69.87 69.2 84.2 62.95 82.09 78.66 78.57 79.3 56.73 69.49 73.11 78.26 28.11 24.46 29.33 29.61 27. 27.5 20.58 24.78 24.87 23.27 52.8 41.96 49.47 49.45 50.86 47.41 36.79 42.74 44.69 45.37 8.6 27.3 21.1 22.4 22.0 4.9 24.7 20.7 30.1 23.3 14.2 37.8 31.6 32.8 34. 4.9 32.4 27.0 29.1 30.0 11.4 32.55 26.35 27.6 28.05 4.9 28.55 23.85 25.35 26.65 performance on general benchmarks drops by an average of 11.96%, showing significant degradation in the source-field model capability. These results further highlight the existence of catastrophic forgetting problems in continual pre-training scenarios, motivating exploration in data mixture and domain reweighting algorithms. Domain reweighting algorithms such as RegMix can achieve balanced performance across fields. According to the results, the RegMix method exhibits trade-off effect across domains. On the 2-dimensional data reweighting space, it outperforms the base model on math reasoning by an average of 18.47%, while largely preserving general capabilities with mere 2.28% degradation on the corresponding benchmarks. RegMix also outperforms the naive training by 5.03% on the overall average performance. Similar conclusions can be drawn from the results on the 52-dimensional domain space in Fig.1b. These results show that the catastrophic forgetting problem can be considerably alleviated by carefully curating data mixtures of source and target fields. Data Mixing Agent significantly outperforms other methods in balanced performance across fields. In Fig. 1a, for the in-distribution LLaMA-3B-DCLM-100B target model, DataAgentRL outperforms the RegMix results on 7 out of 8 general benchmarks and all 4 math benchmarks. It achieves the best average performance 54.04% and 33.02% on general/math benchmarks, even outperforming the base model in general ability and the naively trained model on math reasoning. These results prove that DataAgentRL can effectively curate the data mixture to improve both general and math reasoning capabilities. With careful domain reweighting, increasing capability on the target field can further enhance performance on the source field. Overall, DataAgentRL achieves 47.03% on average, surpassing RegMix by 3.02% and the base model by 8.88%. DataAgentRL also outperforms DataAgentSF by large margin of 2.08%. This advantage shows that the empirical guidance presented in Algorithm 1 is trivial compared to heuristics derived from the broader sampling of data mixing trajectories, proving reinforcement learning with CQL as crucial step towards capable agents. The capabilities of data mixing agents can generalize across target models, source-field data, and domain spaces without retraining. Though our data mixing agent is trained on the 52-dimensional data reweighting space with trajectories sampled with the DCLM data, it effectively guides domain reweighting for four target models across 2 domain definitions. For example, in Fig. 1a, DataAgentRL outperforms RegMix by an average of 1.66% on the two unseen target models: LLaMA-3B-FWE100B and LLaMA-3B-Nemotron-100B, based on the 2-dimensional domain space. In Fig. 1b, DataAgentRL outperforms RegMix by an average of 1.41% based on the 52-dimensional domain space. These results indicate that Data Mixing Agent learns dataand model-agnostic heuristics from the sampled trajectories that can guide domain reweighting on multiple source-field data distributions, which is crucial to the efficiency of this algorithm, as the feedback collection for sampled data trajectories (Sec. 3.1) requires considerable computations. With these generalization capabilities, the agent is still expected to perform well in applications to new target models and source-field data without re-training. Data mixing agent is effective in guiding continual pre-training on estimated start state and data mixtures with synthetic source-field data. We prove this by reweighting domains on the Pythia-1.4B target model with the estimated start state obtained as in Sec. 3.1 and source-field data obtained as described in Sec. 4.1. In math reasoning, DataAgentRL also outperforms RegMix 12 by 0.59% in Table 1a and 3.7% in Table 1b. However, the preservation on general capabilities significantly drops, with 4.69% and 5.65% gap on 2-dimensional and 52-dimensional domain spaces. Overall, DataAgentRL still significantly improves average performance compared to the base model, and outperforms RegMix by 1.05% in Table 1a and 0.61% in Table 1b. 4.3 Evaluation Results on Code Generation We evaluate the Data Mixing Agents generalization to unseen target fields by directly utilizing the agent trained on the math reasoning field to guide domain reweighting for the code generation field. The results are shown in Table 2. We have the following observations: The capabilities of Data Mixing Agent can partially generalize across target fields without retraining. DataAgentRL achieves the best average performance of 46.3% and 41.63% on the LLaMA-3B-DCLM-100B and Pythia-1.4B target models, outperforming the RegMix method by 1.45% and 2.67%. These results prove that heuristics learned in the math reasoning field can be partially transferred to the code generation field without modifying the weights of the agent. However, we observe degradation in DataAgentRLs advantage over the baseline methods in code generation. For example, DataAgentRL outperforms naive training by 6.22%, while in Table 1a, the advantage is 8.52%. This is mainly due to that applying DataAgentRL to code generation leads to major 3.18% drop on general benchmarks compared to math reasoning, which indicates the existence of heuristics that are dependent on the target field and the potential misalignment when converting them to new target field. The Data Mixing Agent still demonstrates strong generalization to synthetic source-field data and unseen target fields. This is validated through continual pre-training in the code generation domain using synthetic data from the Pythia-1.4B model. DataAgentRL outperforms RegMix by 2.63% on general benchmarks, 2.8% on code benchmarks, and 2.67% on average. These results highlight the agents ability to generalize effectively, enabling its application to scenarios where the source-field data is unavailable and the target model is trained on previously unseen target fields. Figure 4: The two data mixing agents output domain reweighting trajectories based on the 2dimensional domain space, training on the LLaMA-3B-DCLM-100B model and the math reasoning field. The dashed line denotes the optimal domain distributions determined by RegMix. 4.4 Analysis on Domain Reweighting Trajectories In this section, we showcase the domain reweighting process guided by Data Mixing Agent to train the LLaMA-3B-DCLM-100B model on the math reasoning field, aiming to provide more intuitions on its actions based on the heuristics and feedback. The trajectories on the 2-dimensional and Figure 5: DataAgentSF domain reweighting trajectories based on the 52-dimensional domain space, training on the LLaMA-3B-DCLM-100B model and the math reasoning field. The legends within each sub-figure are the same as those of Fig. 4. 52-dimensional domain spaces are provided in Fig. 4, Fig. 5, and Fig. 6. We have the following observations: Data Mixing Agents follow less-to-more trend when adapting the target field data along the data mixing trajectory, but DataAgentRL adopts more fine-grained approach to achieve superior performance. In Fig. 4, both the DataAgentRL and DataAgentSF models show an overall trend to increase data from the target field and decrease data from the source field, but with different strategies. DataAgentSF shows radical trend towards more target field data, increasing the DCLM data ratio almost monotonically from about 45% to over 60% during continual pre-training. DataAgentRL adopts more conservative three-stage strategy: Early warm-up stage: the agent prioritizes source field data to stabilize training; Mid-training stage: the agent rapidly increases the use of target field data to enhance performance on the target capability; Final stage: the agent gradually reintroduces more source field data, with the data distribution stabilizing around the optimal weights identified by RegMix. 14 Figure 6: DataAgentRLs domain reweighting trajectories based on the 52-dimensional domain space, training on the LLaMA-3B-DCLM-100B model and the math reasoning field. The legends within each sub-figure are the same as those of Fig. 4. As shown in Table 1a, the superior performance of DataAgentRL on both general and math reasoning benchmarks proves the advantage of its subtle domain reweighting strategy. This performance gap between Data Mixing Agents is mainly due to the comprehensive modeling of the heuristic space during reinforcement learning. DataAgentSF is only fine-tuned on the Ttop1 trajectories, which mostly model the inductive biases from the CalculateInductiveScores function. DataAgentRL is further optimized on broad range of trajectories via reinforcement learning, including Ttop1, Ttop100, Ttop1000, and Ttop10000, with contrastive supervision signals to increase probabilities of actions that improve overall performance and avoid actions that hurt performance measured by the environment feedback. The visualization of the 52-dimensional domain reweighting trajectories further strengthens the above arguments. In Fig. 5, the DataAgentSF organizes the target field data from about 60% of the domains to be almost monotonically increasing along the domain reweighting trajectory, while in Fig. 6, the DataAgentRL model introduces more complicated reweighting strategies on about 80% of the domains. Data Mixing Agents learn heuristics and perform actions that correspond to human intuitions on the target capabilities. Our work uses the MMLU evaluation set to represent the general capabilities in the environment. Wettig et al. (2025) summarized the top-3 domains that benefit the 15 MMLU performance: Science&T ech., Health, and olitics. In Fig. 6, we observe significant uplift of the target data distributions in the corresponding domains compared to the RegMix domain distributions: Science, Health, and eople&Society. Wettig et al. (2025) also enumerated domains that can hurt performance on MMLU, such as ashion&Beauty, while DataAgentRL also conveys an explicit down-sampling process in the Beauty&F itness domain. These observations further ensure the effectiveness of the learned heuristics, encouraging the discovery of more heuristics via the agents trajectories. For example, DataAgentRL continuously reduces data from both source and target fields in the ets&Animals domain, possibly indicating its lack of importance in enhancing either general or math reasoning capabilities. Figure 7: The performance dynamics of the target model on the evaluation environment with increasing training data (measured in Billion tokens) on the corresponding field. We set total training budget of 10.5B tokens, but DataAgentRL triggers an early stopping at 9.96B tokens, and DataAgentSF triggers an early stopping at 9.43B tokens. 4.5 Data Efficiency We explore how efficiently the Data Mixing Agents leverage the source and target field data to improve or preserve model capabilities in the corresponding fields. Training on the mixture of DCLM-100B and the math split of Dolmino-mix-1124 datasets, we record the performance dynamics of the LLaMA-3B-DCLM-100B target model on the general/math evaluation environment with increasing training data (measured in Billion tokens) on the general/math reasoning field. The results are shown in Fig. 7. We have the following observations: Data Mixing Agents leverage general field data more efficiently than RegMix, better preserving model capabilities in the source field. According to the visualization in the general field, the agent methods obtain higher general feedback values from the environment at most token budgets for the source field. The capability measurement for RegMix fluctuates around -2.6, while both data mixing agent models maintain the feedback over -2.575. DataAgentRL further outperforms DataAgentSF in most cases, with feedback values fluctuating around -2.525, which provides evidence for the heuristics learned during reinforcement learning in preserving the general capabilities. Notably, DataAgentRL shows significantly higher variance in feedback values along the domain reweighting trajectory than both DataAgentSF and RegMix, reflecting its more active strategies in adjusting domain reweighting distributions to improve source field capabilities. Its final superior performance on MMLU and the average of general benchmarks (as shown in Table 1a) further indicates the effectiveness of such strategies. Data Mixing Agents leverage data from the math reasoning field more efficiently than RegMix, resulting in greater improvements in the target field performance. As presented in the visualization of the math reasoning field, though all methods show logarithmic-scale improvements from the math reasoning environment, the data mixing agent methods show faster momentum in increasing general feedback values from the environment at most token budgets for the target field. RegMix performance stabilizes around -1.2 while both data mixing agent methods achieve performance over -1.1. These results show that our method can better arrange the continual pre-training data to improve model capability in the target field. DataAgentRL also outperforms DataAgentSF with the optimized feedback values over -1.0. The leading performance of DataAgentRL on both general 16 and math reasoning fields proves its effectiveness in coordinating the source and target field data to improve performance on multiple target capabilities. Data Mixing Agents achieve balanced continual pre-training performance with less reliance on data from the source field. As described in Fig. 7, while we set total training budget of 21B tokens, DataAgentRL triggers an early stopping at 19.92B tokens, and DataAgentSF triggers an early stopping at 18.86B tokens, due to the exhaustion of the target field data. These results show that the data mixing agent can achieve superior performance than RegMix on both the general and math reasoning fields while relying on 2.14B fewer tokens in the source field, further proving the efficiency of their domain reweighting process."
        },
        {
            "title": "5 Related Work",
            "content": "5.1 Continual Pre-training Continual pre-training is an effective and efficient method for adapting LLMs to new target fields where the pre-training data do not align well, such as knowledge-intensive and complex-reasoning tasks. In math reasoning, DeepSeekMath (Shao et al., 2024) was initialized with the DeepSeekCoder (Guo et al., 2024) models and continually trained on 500B tokens of high-quality math-related data. In code generation, the Qwen2.5-Coder (Hui et al., 2024) is based on the Qwen2.5 foundation model and continuously trained on 3.64T tokens of data in the code field. Continual pre-training is also used in other fields such as finance (Xie et al., 2024), system research (Lin et al., 2025), and medicine (Tu et al., 2024). The catastrophic forgetting problem is widely encountered in continual pre-training works (Hui et al., 2024; Lin et al., 2025; Luo et al., 2023; Yang et al., 2024). Existing works mostly curate mixtures of data from the target field and data from the original field to obtain balanced performance. For example, Qwen2.5-Coder manually determined an optimal data mixing recipe of 7:2:1 in code data, text data, and math data for the Qwen2.5-Coder training dataset, leading to over 20% improvement in average performance on multiple fields compared to training solely on code data. 5.2 Data Re-weighting in Pre-training Domain reweighting is an emerging research field that aims to develop an optimal data mixing strategy for the fixed data mixture to achieve the best possible performance on the target model (Xie et al., 2023; Liu et al., 2024b; Xia et al., 2023; Luo et al., 2024). Doremi (Xie et al., 2023) trains reference model based on initial domain weights, which is used to guide the training of another proxy model with the group DRO (Sagawa et al., 2019) algorithm to determine the optimal domain weights for the target model. RegMix (Liu et al., 2024b) trained large quantities of small proxy models on random domain distributions, then evaluates these models on the target benchmarks. The best data mixing recipe is determined by fitting regression model to these data and selecting distributions that lead to the highest scores. Other works focus on balancing the loss of multiple target fields to achieve balanced optimization (Xia et al., 2023; Luo et al., 2024). For example, Xia et al. (2023) proposed batch loading algorithm that loads training data from each domain in proportion to its corresponding rate of loss reduction, which increases the future domain distributions for domains that have slow loss reduction. Recent works have also explored the effect of domain space definition on data reweighting performance (Wettig et al., 2025; Rukhovich et al., 2025; Diao et al., 2025; Xi et al., 2025), strengthening the importance of carefully defined domains. For example, previous data mixing methods mostly utilized the default domain space defined by data sources. Wettig et al. (2025) carefully defined 24-dimensional domain space from both the topic (e.g., Science&Tech, Fashion&Beauty) and format (e.g., Academic writing, Content listing) perspectives, and re-organized the training data into these domain spaces. Extensive data mixing experiments on these novel domain spaces showed their effectiveness in improving model training performances compared to the source-based domain space. Inspired by their success, we also train the Data Mixing Agent based on these superior ways of domain space definition."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we propose the Data Mixing Agent, the first model-based domain reweighting method for continual pre-training, which learns general heuristics for balancing model capabilities on multiple target fields via randomly sampled data mixing trajectories and feedback from an evaluation environment. Extensive experiments show that the agent significantly outperforms strong baseline methods in overall results on 12 general and math reasoning benchmarks. The learned heuristics also generalize well across source-field data, target models, domain spaces, and new target fields such as code generation, without retraining the agent model. Further analysis showcases the data mixing agents well-aligned heuristics with human intuitions and their efficiency in achieving superior performance in the target fields with less source-field data."
        },
        {
            "title": "References",
            "content": "Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. arXiv preprint arXiv:1905.13319 (2019). Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732 (2021). Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning. PMLR, 23972430. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, Vol. 34. 74327439. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021). Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457 (2018). Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 (2021). Shizhe Diao, Yu Yang, Yonggan Fu, Xin Dong, Dan Su, Markus Kliegl, Zijia Chen, Peter Belcak, Yoshi Suhara, Hongxu Yin, et al. 2025. CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training. arXiv preprint arXiv:2504.13161 (2025). Nan Du, Yanping Huang, Andrew Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022. Glam: Efficient scaling of language models with mixture-of-experts. In International conference on machine learning. PMLR, 5547 5569. Ethan Dyer, Aitor Lewkowycz, and Vinay Ramasesh. 2022. Effect of scale on catastrophic forgetting in neural networks. In International Conference on Learning Representations. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 (2020). Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. 2024. The llama 3 herd of models. arXiv e-prints (2024), arXiv2407. 18 Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. 2024. DeepSeek-Coder: When the Large Language Model Meets ProgrammingThe Rise of Code Intelligence. arXiv preprint arXiv:2401.14196 (2024). Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 (2020). Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874 (2021). Jian Hu, Xibin Wu, Zilin Zhu, Weixun Wang, Dehao Zhang, Yu Cao, et al. 2024. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143 (2024). Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186 (2024). Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah Smith, Yejin Choi, and Hanna Hajishirzi. 2024. Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback. Advances in neural information processing systems 37 (2024), 3660236633. Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. 2020. Conservative q-learning for offline reinforcement learning. Advances in neural information processing systems 33 (2020), 11791191. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems 35 (2022), 38433857. Hao Li, Bowen Deng, Chang Xu, Zhiyuan Feng, Viktor Schlegel, Yu-Hao Huang, Yizheng Sun, Jingyuan Sun, Kailai Yang, Yiyao Yu, et al. 2025. MIRA: Medical Time Series Foundation Model for Real-World Health Data. arXiv preprint arXiv:2506.07584 (2025). Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, et al. 2024. Datacomp-lm: In search of the next generation of training sets for language models. Advances in Neural Information Processing Systems 37 (2024), 1420014282. Zhenghao Lin, Zihao Tang, Xiao Liu, Yeyun Gong, Yi Cheng, Qi Chen, Hang Li, Ying Xin, Ziyue Yang, Kailai Yang, et al. 2025. Sigma: Differential Rescaling of Query, Key and Value for Efficient Language Models. arXiv preprint arXiv:2501.13629 (2025). Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024a. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 (2024). Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. arXiv preprint arXiv:2007.08124 (2020). Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin. 2024b. Regmix: Data mixture as regression for language model pre-training. arXiv preprint arXiv:2407.01492 (2024). Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2023. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747 (2023). Zheheng Luo, Xin Zhang, Xiao Liu, Haoling Li, Yeyun Gong, Chen Qi, and Peng Cheng. 2024. Velocitune: Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training. arXiv preprint arXiv:2411.14318 (2024). Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789 (2018). Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, et al. 2024. 2 OLMo 2 Furious. arXiv preprint arXiv:2501.00656 (2024). Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. 2024. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems 37 (2024), 3081130849. Alexey Rukhovich, Alexander Podolskiy, and Irina Piontkovskaya. 2025. Commute Your Domains: Trajectory Optimality Criterion for Multi-Domain Learning. arXiv preprint arXiv:2501.15556 (2025). Shiori Sagawa, Pang Wei Koh, Tatsunori Hashimoto, and Percy Liang. 2019. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731 (2019). Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Commun. ACM 64, 9 (2021), 99106. Takuma Seno and Michita Imai. 2022. d3rlpy: An Offline Deep Reinforcement Learning Library. Journal of Machine Learning Research 23, 315 (2022), 120. http://jmlr.org/papers/v23/ 22-0017.html Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 (2024). Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Zhengzhong Liu, Hongyi Wang, Bowen Tan, Joel Hestness, Natalia Vassilieva, Daria Soboleva, et al. 2023. Slimpajama-dc: Understanding data combinations for llm training. arXiv preprint arXiv:2309.10818 (2023). Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, Zifeng Wang, Sayna Ebrahimi, and Hao Wang. 2024. Continual learning of large language models: comprehensive survey. Comput. Surveys (2024). Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053 (2019). Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. 2024. Nemotron-CC: Transforming Common Crawl into Refined Long-Horizon Pretraining Dataset. arXiv preprint arXiv:2412.02595 (2024). Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. 1999. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems 12 (1999). Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, et al. 2024. Towards generalist biomedical AI. Nejm Ai 1, 3 (2024), AIoa2300138. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. 2023. Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. arXiv preprint arXiv:2310.03731 (2023). Johannes Welbl, Nelson Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. arXiv preprint arXiv:1707.06209 (2017). Alexander Wettig, Kyle Lo, Sewon Min, Hannaneh Hajishirzi, Danqi Chen, and Luca Soldaini. 2025. Organize the Web: Constructing Domains Enhances Pre-Training Data Curation. arXiv preprint arXiv:2502.10341 (2025). Xiangyu Xi, Deyang Kong, Jian Yang, Jiawei Yang, Zhengyu Chen, Wei Wang, Jingang Wang, Xunliang Cai, Shikun Zhang, and Wei Ye. 2025. SampleMix: Sample-wise Pre-training Data Mixing Strategey by Coordinating Data Quality and Diversity. arXiv preprint arXiv:2503.01506 (2025). Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 2023. Sheared llama: Accelerating language model pre-training via structured pruning. arXiv preprint arXiv:2310.06694 (2023). Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, et al. 2024. Finben: holistic financial benchmark for large language models. Advances in Neural Information Processing Systems 37 (2024), 9571695743. Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc Le, Tengyu Ma, and Adams Wei Yu. 2023. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems 36 (2023), 6979869818. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388 (2025). An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. 2024. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122 (2024). Jiasheng Ye, Peiju Liu, Tianxiang Sun, Jun Zhan, Yunhua Zhou, and Xipeng Qiu. 2024. Data mixing laws: Optimizing data mixtures by predicting language modeling performance. arXiv preprint arXiv:2403.16952 (2024). Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284 (2023). Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830 (2019). Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh Gupta, and Jingbo Shang. 2024. Large language models for time series: survey. arXiv preprint arXiv:2402.01801 (2024)."
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "The University of Manchester"
    ]
}