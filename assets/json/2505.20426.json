{
    "paper_title": "MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness",
    "authors": [
        "Yunlong Tang",
        "Pinxin Liu",
        "Mingqian Feng",
        "Zhangyun Tan",
        "Rui Mao",
        "Chao Huang",
        "Jing Bi",
        "Yunzhong Xiao",
        "Susan Liang",
        "Hang Hua",
        "Ali Vosoughi",
        "Luchuan Song",
        "Zeliang Zhang",
        "Chenliang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding perspective is fundamental to human visual perception, yet the extent to which multimodal large language models (MLLMs) internalize perspective geometry remains unclear. We introduce MMPerspective, the first benchmark specifically designed to systematically evaluate MLLMs' understanding of perspective through 10 carefully crafted tasks across three complementary dimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark comprises 2,711 real-world and synthetic image instances with 5,083 question-answer pairs that probe key capabilities, such as vanishing point perception and counting, perspective type reasoning, line relationship understanding in 3D space, invariance to perspective-preserving transformations, etc. Through a comprehensive evaluation of 43 state-of-the-art MLLMs, we uncover significant limitations: while models demonstrate competence on surface-level perceptual tasks, they struggle with compositional reasoning and maintaining spatial consistency under perturbations. Our analysis further reveals intriguing patterns between model architecture, scale, and perspective capabilities, highlighting both robustness bottlenecks and the benefits of chain-of-thought prompting. MMPerspective establishes a valuable testbed for diagnosing and advancing spatial understanding in vision-language systems. Resources available at: https://yunlong10.github.io/MMPerspective/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 6 2 4 0 2 . 5 0 5 2 : r MMPerspective: Do MLLMs Understand Perspective? Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness Yunlong Tang1,, Pinxin Liu1,, Mingqian Feng1,, Zhangyun Tan1,, Rui Mao1,, Chao Huang1, Jing Bi1, Yunzhong Xiao2, Susan Liang1, Hang Hua1, Ali Vosoughi1, Luchuan Song1, Zeliang Zhang1, Chenliang Xu1 1University of Rochester, 2Carnegie Mellon University {yunlong.tang, mingqian.feng, jing.bi, chenliang.xu}@rochester.edu, {pliu23, rmao6, lsong11, zzh136}@ur.rochester.edu, ztan12@u.rochester.edu, {chuang65, sliang22, hhua2}@cs.rochester.edu, avosoughi@ece.rochester.edu, yunzhonx@andrew.cmu.edu Figure 1: MMPerspective benchmark overview. We introduce 10 tasks spanning 3 complementary dimensions of perspective understanding: Perspective Perception, Reasoning, and Robustness."
        },
        {
            "title": "Abstract",
            "content": "Understanding perspective is fundamental to human visual perception, yet the extent to which multimodal large language models (MLLMs) internalize perspective geometry remains unclear. We introduce MMPerspective, the first benchmark specifically designed to systematically evaluate MLLMs understanding of perspective through 10 carefully crafted tasks across three complementary dimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark comprises 2,711 real-world and synthetic image instances with 5,083 question-answer pairs that probe key capabilities, such as vanishing point perception and counting, perspective type reasoning, line relationship understanding in 3D space, invariance to Core contributor. Preprint. Under review. perspective-preserving transformations, etc. Through comprehensive evaluation of 43 state-of-the-art MLLMs, we uncover significant limitations: while models demonstrate competence on surface-level perceptual tasks, they struggle with compositional reasoning and maintaining spatial consistency under perturbations. Our analysis further reveals intriguing patterns between model architecture, scale, and perspective capabilities, highlighting both robustness bottlenecks and the benefits of chain-of-thought prompting. MMPerspective establishes valuable testbed for diagnosing and advancing spatial understanding in vision-language systems. Resources available at: https://yunlong10.github.io/MMPerspective/"
        },
        {
            "title": "Introduction",
            "content": "Perspective is nothing more than rational demonstration applied to the consideration of how objects in front of the eye transmit their image to it. Leonardo da Vinci, The Notebooks of Leonardo da Vinci [Da Vinci, 2012] From the chalked strings of Renaissance artists to the calibrated optics of modern cameras, perspective has long served as cornerstone for representing three-dimensional reality on two-dimensional surfaces [Kemp et al., 1990, Neher, 2005]. Based on the geometry of the pinhole camera model, perspective projection enables humans to infer spatial structure, depth, and layout from flat images, capability central to artistic creation, scientific visualization, and machine perception [Hartley, 2003, Hecht, 2012]. For instance, artists employ perspective to enhance realism, guide viewer attention, manipulate spatial illusion, and convey narrative depth [Robertson and Bertling, 2013, Panofsky, 2020]. In scientific visualization, perspective projections are used to render complex 3D structures, such as molecular surfaces and anatomical forms [Ware, 2019]. In computer vision, some methods based on the perspective principle have been developed to analyze, edit images, and fix distortions [Criminisi et al., 2002, Carroll et al., 2010, Carroll, 2013, Song et al., 2024, 2021, Liang et al., 2023, 2024]. Therefore, perspective understanding plays foundational role in visual cognition and spatial representation. However, current work [Bharadwaj et al., 2025, Coudert et al., 2022, Zhao et al., 2021] is still primarily focused on using perspective principles to implement applications, with relatively little research on the ability of intelligent systems themselves to understand perspective. Although some studies have already attempted to enable models to locate vanishing points [Bharadwaj et al., 2025], detect key lines in space [Coudert et al., 2022, Zhao et al., 2021], etc., these models either rely on precise mathematical models or learn from specialized datasets, being hard to capture perspective-related semantics or apply learned understanding of perspective to more general tasks. On the other hand, recent multimodal large language models (MLLMs) such as GPT-4o [Achiam et al., 2023] and Gemini [Reid et al., 2024] have demonstrated powerful human-like visual perception and reasoning capabilities Bi et al. [2025a,b], Liu et al. [2024c, 2025a] through large-scale training, but their ability to understand perspective has not yet been tested. Given its foundational role in visual cognition and spatial representation, an important open question is: Do MLLMs understand perspective? These models have shown remarkable performance across broad range of high-level vision-language tasks, including visual captioning [Wang et al., 2023, Tang et al., 2025] and visual question answering [Liu et al., 2024a, Achiam et al., 2023, Reid et al., 2024, Chen et al., 2024b, Wang et al., 2024b, Zhang et al., 2024, Song et al., 2023]. However, existing benchmarks rarely evaluate their capacity for geometric reasoning. In particular, it remains unclear whether MLLMs can identify vanishing points, understand the convergence of parallel lines, reason about spatial relationships induced by perspective, or maintain consistent spatial interpretations across different viewpoints. These are fundamental aspects of human visual understanding and have been systematically studied in both art history and computational vision [Robertson and Bertling, 2013], yet they are largely absent from current evaluation protocols [Yu et al., 2024b, Liu et al., 2025b, Li et al., 2024c, Hua et al., 2024, Wang et al., 2024d, Tang et al., 2024, Bi et al., 2024] for MLLMs. To bridge this gap, we introduce MMPerspective, the first benchmark specifically designed to evaluate perspective understanding in MLLMs. As shown in Figure 1, our benchmark comprises 10 tasks divided across three dimensions: Perspective Perception, Perspective Reasoning, and Perspective Robustness. Perception tasks probe the ability to identify geometric cues such as vanishing points and critical lines. Reasoning tasks examine models ability to interpret 3D structure, assess scene composition, and predict off-canvas geometry. Robustness task evaluates spatial consistency under appearance-preserving transformations, such as flipping and cropping. Our benchmark comprises 2,711 image instances and 5,083 question-answer pairs, each framed as multiple-choice question grounded in real-world imagery rich with architectural, urban, and indoor perspective cues, such as vanishing lines, orthogonal edges, and depth gradients. Tasks are organized to increase in difficulty across perceptual, reasoning, and robustness dimensions, requiring progressively deeper spatial abstraction. We evaluate 43 state-of-the-art MLLMs, ranging from lightweight open-source models to proprietary systems like GPT-4o and Gemini. While many models perform competitively on surface-level perception tasks, they exhibit clear performance drops on reasoning and robustness tasks. For instance, models often fail to maintain consistent predictions under simple geometric-preserving edits, such as horizontal flipping or partial occlusion of key cues, revealing their limited internalization of spatial priors and geometric constraints. Figure 2: Left: MMPerspective benchmark consists of 2,711 instances and 5,083 QA pairs, hierarchically organized into 3 core categories (Perspective Perception, Reasoning, and Robustness). Right: The accuracy of 8 representative MLLMs on 10 tasks of MMPerspective across the 3 categories. In short, our contributions are three-fold: We introduce MMPerspective, the first dedicated benchmark for evaluating perspective understanding in MLLMs, spanning 10 tasks across three dimensions, consisting of 2,711 instances and 5,083 QA pairs. We conduct comprehensive evaluation of 43 representative MLLMs and reveal key limitations in perspective perception, reasoning, and robustness. We offer new insights into current model bottlenecks and provide guidance toward building geometry-aware, spatially grounded multimodal systems."
        },
        {
            "title": "2 MMPerspective",
            "content": "2.1 Preliminary Understanding the key elements of perspective geometry is essential for interpreting spatial relationships in 2D images. In this section, we introduce foundational terms used, following classical principles of linear perspective as described in drawing literature [Robertson and Bertling, 2013]. As shown in the Figure 4, the Ground Plane (GP) is the surface upon which objects rest and from which vertical height is measured. The Station Point (SP) represents the viewers position in space, typically aligned with the eye or camera origin. The Figure 3: Perspective illustration with terminology. The figure is adapted from [Robertson and Bertling, 2013]. 3 Line of Sight (LS) defines the direction in which the observer is looking; when this is parallel to GP, vertical lines in the scene remain vertical in the image, as seen in oneor two-point perspectives. Tilting the LS results in three-point perspective, where verticals also converge. The Picture Plane (PP) refers to an imaginary plane perpendicular to the LS where the visual projection occurs. It is often conceptualized as transparent sheet placed between the observer and the scene, capturing the intersections of visual rays from the Station Point to the object. The Vision Rays (VRs) are the lines extending from the eye through each point on the object to the PP. The Horizon Line (HL) corresponds to the viewers eye level and is the projection of the GP onto the PP. Vanishing Point (VP) is the point at which set of parallel lines appears to converge. In 1-point perspective, single set of lines converges to one VP. In 2-point perspective, two sets of lines converge to separate VPs on the HL. In 3-point perspective, an additional VP is used for vertical convergence, located either above or below the HL, depending on whether the observer is looking up or down. 2.2 Taxonomy The MMPerspective benchmark is designed to evaluate perspective understanding in MLLMs across three complementary and hierarchically structured dimensions: Perspective Perception, Perspective Reasoning, and Perspective Robustness. These dimensions reflect progression from low-level visual recognition to high-level spatial inference and consistency under image transformations. Perspective Perception (PPercep) focuses on models ability to detect and interpret explicit perspective-related cues directly visible in the image. It includes the following tasks: Vanishing Point Perception (VPP) evaluates whether model can correctly locate VP or determine its presence within given region. Critical Line Perception (CLP) assesses the identification of the HL from set of candidate lines, based on perspective convergence. Lens Distortion Perception (LDP) requires the model to distinguish regions in the image that are free from curved-line distortion. View Angle Perception (VAP) asks the model to infer the LS direction (e.g., upward, downward, or horizontal) using visible spatial cues. All tasks in this category are grounded in localized, directly observable visual evidence and require minimal reasoning beyond geometric feature detection. Perspective Reasoning (PReason) tests whether the model can integrate multiple spatial cues and apply geometric reasoning to infer high-level relationships in the 3D structure of the scene. The tasks include: Perspective Type Reasoning (PTR), which involves classifying the underlying perspective structure of the image (e.g., 1-point, 2-point, 3-point, or non-linear). Line Relationship Reasoning (LRR), which asks the model to determine whether two lines in the 3D space are parallel, perpendicular, or intersecting. Perspective Transformation Spotting (PTS), which requires detecting changes in perspective type across paired images. Vanishing Point Counting (VPC), which involves estimating the number of identifiable VPs present in the scene. Out-of-View Reasoning (OVR), which challenges the model to infer the quadrant in which VP lies when it is not explicitly shown in the image. These tasks demand combination of compositional reasoning, global geometric understanding, and spatial abstraction beyond direct visual perception. Perspective Robustness (PRobust) assesses the models ability to produce consistent and geometryaware predictions under controlled, appearance-preserving transformations of the input image. Each original image-question pair is augmented with perturbed versions through perspective-invariant operations such as cropping, flipping, and masking. While these transformations do not alter the scenes underlying geometry, they may obscure or de-emphasize key visual cues. model is considered robust if it provides the same, correct answer across all such transformed variants. This consistency serves as direct measure of its geometric grounding, separating genuine perspective understanding from brittle reliance on surface-level visual patterns. 2.3 Data Curation Data Collection. To support the construction of these tasks, we curated diverse set of perspectiverich images from multiple sources. Images are sourced from four streams. First, we collect unlabeled examples from the web, primarily architectural and indoor scenes with strong perspective cues. Second, we shoot real-world perspective images in life scenarios with both linear perspectives and curvilinear perspectives (fish-eye perspectives). For one scene, we shoot multiple images with different views to form perspective image pairs. Third, we incorporate data from the open-source RPVP datasets [Bharadwaj et al., 2025]. In this dataset, perspective cues come from the recurrence 4 Figure 4: Data Curation Pipeline for MMPerspective. pattern rather than lines at object edges. Fourth, we utilize Blender to create images with ground-truth VP coordinates. Specifically, we first employ Claude 3.7 Sonnet to create 3D models based on scene descriptions, empowered by Blender-MCP. For each scene, we render multiple images with different camera transform and lens parameters. From these parameters, we calculate the ground-truth VP coordinates for each image. We provide more details of this approach in Appendix. Annotation. We annotate each image with task-specific metadata using hybrid pipeline. For PTS, we manually annotate the perspective changes in the image pairs that we shoot. For LDP, we combine fish-eye perspective images and regular linear perspective images randomly and record the corresponding option. For PTR, LRR, VAP, CLP, and VPC, we use images collected from the web and manually annotate the right answers for the questions and hints on the images. For VPP, we use both images from the web and Blender. The VP annotations of the former are manually created, while the latter are born with ground-truth VP coordinates. For OVR, we use the annotation from the RPVP datasets [Bharadwaj et al., 2025]. Quality Control. Quality assurance is carried out via multi-stage review process. All automatically generated annotations are verified manually. For subjective tasks involving spatial reasoning, at least two annotators independently label each sample, with disagreements resolved through discussion and consensus. We exclude any examples where ambiguity could not be resolved, and the final benchmark comprises only unambiguous, perspective-defining scenes. We also manually check and filter all unsafe images we collect. 2.4 Evaluation Metrics For all tasks of PPercep and PReason in MMPerspective, we use accuracy as the main evaluation metric, with each question having one correct answer. For PRobust, we assess consistency across perturbed image variants. The robustness score is: RobustM = 1 (cid:88) (cid:34) M(Is, q) = (Is,q,a)S (cid:35) M(Ii, q) = , (1) (cid:94) i=1 where 1[] is the indicator function, is the set of seed examples, each with an image-question pair (Is, q) and ground truth answer a, and I1, . . . , In are the perturbed variants. model is robust if it answers correctly on both the original image and all perturbed versions. 5 Table 1: Performance of MLLMs on MMPerspective. Models are grouped by size and ranked by overall accuracy. Best scores in each group are bolded. Model VPP CLP VAP LDP PTR LRR OVR PTS VPC Acc Acc Overall PRobust Perspective Perception Perspective Reasoning PPercep & PReason Robustness InternVL2.5-2B Qwen2.5-VL-3B InternVL2.5-4B InternVL3-2B InternVL2-4B Qwen2-VL-2B InternVL3-1B InternVL2-1B LLaVA-OV-1B InternVL2-2B InternVL2.5-1B InternVL2.5-8B Qwen2.5-VL-7B Qwen2-VL-7B InternVL3-9B InternVL3-8B LLaVA-OV-7B Eagle-X4-8B InternVL2-8B LLaVA-Next-m-7B Eagle-X5-7B LLaVA-Next-v-7B InternVL2.5-26B InternVL3-14B InternVL2-26B Eagle-X4-13B LLaVA-Next-13B InternVL2.5-38B InternVL3-38B Qwen2.5-VL-32B Eagle-X5-34B InternVL2-40B InternVL3-78B InternVL2.5-72B Qwen2.5-VL-72B Qwen2-VL-72B LLaVA-OV-72B LLaVA-Next-72B InternVL2-72B Gemini-2-flash (CoT) GPT-4o (CoT) Gemini-2-flash GPT-4o Gemini-1.5-flash (CoT) GPT-4o-mini Gemini-1.5-flash 47.4 27.6 32.1 22.4 26.9 12.2 19.9 20.5 13.5 26.9 14.7 38.5 35.3 34.6 37.2 42.3 34.0 39.1 33.3 35.9 25.0 16.7 41.7 39.1 28.2 42.3 7.7 46.8 45.5 35.9 36.5 26.3 43.6 47.4 41.7 34.6 25.6 21.8 26.9 69.2 45.5 64.7 42.9 30.1 35.3 26. 22.8 22.8 26.0 28.5 12.2 19.5 13.0 20.3 14.6 26.0 23.6 17.9 29.3 25.2 33.3 27.6 33.3 17.1 19.5 21.1 26.0 20.3 35.0 26.0 35.0 26.8 17.1 36.6 35.0 22.8 28.5 22.0 39.8 30.1 31.7 18.7 26.0 21.1 18.7 49.6 46.3 35.0 35.0 28.5 24.4 25. 13.0 56.8 59.3 50.0 54.3 49.4 53.7 15.4 35.8 3.1 0.6 53.1 70.4 63.0 63.0 67.9 51.2 46.9 59.3 35.2 24.7 40.7 55.6 73.5 61.1 41.4 54.3 67.9 71.0 68.5 60.5 66.0 69.8 67.3 67.9 70.4 75.9 66.0 57.4 72.8 70.4 73.5 66.0 66.7 43.2 59. 65.3 55.1 64.2 44.6 60.4 35.8 20.7 24.2 24.2 36.8 33.0 75.4 73.7 64.2 77.5 81.8 57.9 47.7 73.3 50.5 34.7 39.6 81.8 73.3 74.0 44.6 34.7 89.5 90.9 73.7 79.6 76.1 89.1 89.5 82.1 82.5 81.1 32.3 56.8 87.4 88.8 87.0 86.0 79.3 71.6 70. MLLMs: < 7B 62.2 32.3 28.2 43.1 18.0 23.3 16.3 24.1 15.2 18.8 20.1 31.8 32.5 30.5 31.1 40.4 24.5 8.6 11.3 19.2 12.6 11.3 16.6 15.9 10.7 34.4 18.8 28.9 23.7 24.0 19.5 23.1 13.3 MLLMs: 7B - 9B 40.8 42.4 57.1 30.7 38.1 44.9 65.3 27.1 17.7 22.1 16. 48.3 44.4 49.0 53.0 46.4 53.0 37.1 36.4 37.7 46.4 44.4 34.7 32.1 27.3 27.9 20.8 19.8 18.2 42.5 15.6 15.6 19.8 MLLMs: 10B - 30B 65.5 36.5 50.7 65.8 66.7 46.4 34.4 41.7 20.5 24.5 43.5 54.5 28.9 28.2 13. MLLMs: 30B - 70B 58.4 37.3 62.0 19.5 43.2 51.7 43.0 37.7 51.0 55.0 38.3 56.8 33.8 24.0 27.3 MLLMs: > 70B 55.9 65.2 65.3 68.8 81.4 65.7 56. 57.6 53.6 38.4 52.3 55.6 49.7 47.0 40.3 41.9 39.9 38.6 22.4 22.4 24.7 MLLMs: Proprietary 78.7 81.4 71.3 82.0 51.0 43.1 26.4 32.5 47.0 34.4 41.7 39.7 29.8 27.8 40.9 34.4 29.9 29.9 20.1 14.6 18. 30.0 39.4 37.1 25.4 24.4 32.9 21.6 22.1 22.1 21.1 34.7 24.9 28.6 31.0 23.9 23.9 35.2 32.9 22.1 27.2 20.7 16.4 34.3 28.2 28.6 31.0 26.8 44.1 37.6 35.2 39.0 25.8 38.0 32.4 39.0 35.2 28.2 27.2 24.4 39.9 37.6 40.8 33.8 31.5 31.0 26. 50.0 44.7 36.8 43.0 45.6 47.4 47.4 44.7 40.4 34.2 45.6 67.5 44.7 46.5 43.9 32.5 49.1 68.4 48.2 46.5 42.1 7.0 46.5 54.4 43.0 57.9 43.9 44.7 43.0 45.6 63.2 47.4 42.1 37.7 38.6 42.1 31.6 30.7 7.9 43.9 34.2 41.2 32.5 35.1 45.6 22. 37.1 40.6 45.4 36.4 38.4 29.2 26.8 20.1 22.0 23.2 18.0 46.2 52.1 46.7 52.8 54.9 44.1 37.7 46.4 35.7 27.6 29.3 53.5 53.0 49.6 38.8 28.5 60.2 60.6 50.2 51.3 47.6 60.6 58.6 55.8 51.5 52.2 35.3 40.0 69.8 62.7 65.0 57.5 51.1 43.6 45. 38.1 33.0 28.7 35.4 29.4 31.4 23.5 25.2 23.3 22.0 25.0 43.3 38.5 42.2 35.9 32.3 40.4 44.4 35.3 28.9 29.4 20.8 47.2 41.6 38.6 40.7 35.0 47.5 43.5 42.9 39.3 39.7 46.8 46.2 44.3 47.4 43.8 39.1 32.0 47.2 46.9 43.5 44.0 35.5 32.8 24. 37.7 36.3 36.1 35.8 33.4 30.4 25.0 23.0 22.7 22.5 21.9 44.6 44.5 44.2 43.4 42.4 42.0 41.4 40.2 31.9 28.6 24.6 50.0 46.7 43.5 39.8 32.1 53.1 51.1 46.1 44.6 43.2 52.9 51.7 49.4 49.2 47.5 37.4 35.6 57.2 54.0 53.1 50.0 42.4 37.6 33. 46.5 6.4 20.6 23.9 7.9 4.7 13.8 6.7 7.8 12.3 18.2 22.3 15.3 25.5 7.3 15.9 15.9 55.3 7.9 16.4 15.9 16.4 33.7 13.5 26.5 53.8 51.1 19.1 9.1 25.5 16.0 12.6 25.5 29.7 24.3 25.0 53.1 33.2 22.9 45.9 49.9 30.7 49.9 15.3 10.8 10."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Experiment Setup We select 20 representative models, including both open-source and proprietary models, covering broad spectrum of model scales and architecture types. These include GPT-4o [Hurst et al., 2024], Gemini-2 [DeepMind, 2025], LLaVA-OV [Li et al., 2024b], LLaVA-Next [Liu et al., 2024b], InternVL2 [Chen et al., 2024b], InternVL2.5 [Chen et al., 2024b], InternVL3 [Zhu et al., 2025], Qwen2-VL [Wang et al., 2024c], Qwen2.5-VL [Bai et al., 2025], and Eagle-X [Shi et al., 2024]. To ensure consistency, all open-source models under 14B are evaluated using single NVIDIA A6000 48GB GPU. Models larger than 14B and up to 70B are evaluated using single NVIDIA H100 80GB GPU. Larger models (>70B) are run on multiple NVIDIA A100 80G GPUs (at least 4). Proprietary 6 models are executed via APIs. Each model is evaluated under the same test conditions, with identical multiple-choice question formats across all tasks. 3.2 Main Results Table 1 presents the performances of various MLLMs on our MMPERSPECTIVE benchmark. In general, larger models tend to perform better, with GPT-4o and Gemini-2-flash achieving the highest overall accuracy (57.7% and 57.6%, respectively). Perspective Perception. For VPP, Gemini-2-flash (CoT) achieves the highest accuracy (69.8%), while many smaller models struggle with this fundamental task. In CLP, all models perform poorly, with even GPT-4o (CoT) only reaching 46.3%, indicating general limitation in detecting HLs. Most larger models exceed 60% on VAP, with InternVL3-14B leading at 73.5%. For LDP, InternVL3-38B demonstrates the strongest performance (90.9%), surpassing even proprietary models. Perspective Reasoning. In PTR, GPT-4o achieves the highest score (82.0%), with LLaVA-OV-72B close behind (81.4%). LRR shows less correlation with model size, with InternVL3-78B leading at 57.6%. For OVR, InternVL3-38B significantly outperforms all others (56.8%), suggesting unique architectural advantages. In VPC, the Eagle-X4 family demonstrates superior performance (68.4% for 8B), indicating specialized capabilities for identifying multiple VPs. Perspective Robustness. PRobust scores reveal surprising patterns, with Eagle-X4-8B achieving great performance (55.3%) despite modest size. LLaVA-OV-72B (53.1%) and Eagle-X4-13B (53.8%) also present strong robustness. Notably, many large models with high accuracy perform poorly on robustness, with InternVL3-38B showing excellent perception (67.2%) but poor robustness (9.1%). 3.3 Further Findings Finding 1. Our analysis reveals that perspective understanding scales strongly with total model size but only weakly with vision encoder size, with robustness showing particularly limited correlation to encoder scaling. Our analysis of model scaling reveals important insights into how different architectural components influence perspective understanding capabilities in MLLMs. In Fig. 5, there is clear progression of performance within model families as model size increases, with deeper blue coloration indicating higher accuracy and robustness for larger variants. The scatter plots in Fig. 6 quantify these relationships more precisely, demonstrating strong positive correlation between model size and perspective understanding accuracy (r = 0.81), while robustness shows weaker correlation (r = 0.34). This disparity suggests that while general perspective understanding capabilities scale reliably with language model size, robustness to perspective-preserving transformations follows different pattern. For instance, models like Eagle-X4 achieve high perspective robustness even at moderate sizes (8B and 13B), suggesting their architecture may have inherent advantages for maintaining consistent geometric interpretations across image variations. When examining vision encoder scaling specifically (Fig. 6c-d), we observe moderate correlation with overall perspective accuracy (r = 0.51) but notably weak correlation with perspective robustness (r = 0.15). This suggests that vision encoders play more limited role in ensuring consistent geometric interpretations across transformations than in enabling basic perspective understanding. The data indicates that while increasing vision encoder capacity may help models better recognize perspective features initially, it does not necessarily translate to more stable geometric interpretations when those features are partially obscured or repositioned. The limited range of encoder sizes currently employed across model families (mostly 300-500M parameters) makes it difficult to draw definitive conclusions about vision encoder scaling laws for perspective understanding. This represents gap in our understanding of how to optimally design MLLMs for spatial reasoning tasks that require both accurate perspective perception and consistent geometric interpretations under varying conditions. Finding 2. Chain-of-thought (CoT) prompting modestly improves model performance and robustness on perspective-related tasks by encouraging stepwise deduction. 7 Table 2: Chain of Thought (CoT) prompting improves MLLM performance on perspective tasks. Accuracy changes due to CoT prompting across perception and reasoning tasks. Perspective Perception Perspective Reasoning PPercep & PReason Robustness VPP CLP VAP LDP PTR LRR OVR PTS VPC Acc Acc Overall PRobust GPT-4o Gemini-1.5-flash Gemini-2-flash Average +2.56 +3.21 +4. +3.42 +11.38 +3.25 +14.63 +4.32 +7.41 -0.62 +2.81 +8.77 +0.35 -0.66 +24.59 +7.43 +5.30 +11.92 -1. +4.55 +1.95 +11.04 +9.76 +3.70 +3.98 +10.45 +5. +5.84 +3.76 +4.69 -0.94 +2.50 +1.75 +12.28 +2.63 +5.56 +5.27 +5.66 +4. +5.21 +2.94 +11.09 +3.63 +5.89 +3.97 +8.67 +4.11 +5.59 +0.00 +4.72 +15. +6.63 As shown in Table 2, CoT prompting leads to consistent performance gains across nearly all perspective-related tasks. All three evaluated models, GPT-4o, Gemini-1.5-flash, and Gemini-2-flash, experience improvements in both perception and reasoning sub-tasks when CoT is applied. Notably, no single sub-task exhibits degradation in performance for more than one model, suggesting that CoT prompting is broadly beneficial and rarely harmful within this domain. The overall accuracy and robustness metrics also trend upward with CoT, reinforcing its value not only in structured reasoning but also in enhancing the models resilience to perspective-related perturbations. For instance, the average gain in P&R Overall Accuracy is +5.59%, and in Robustness is +6.63%, indicating that step-by-step reasoning contributes to more confident and stable outputs. While the benefits are widespread, few failures still emerge. In Appendix, we analyze three representative failure cases to better understand CoTs limitations. These include GPT-4o on Perspective Type Reasoning, and Gemini-2-flash on Line Relationship and Perspective Transformation Spotting. Overall, our findings suggest that while CoT prompting is not silver bullet, it provides meaningful and reliable improvements in most perspective tasks. This points toward the promise of integrating structured reasoning strategies with visual understanding, especially for tasks where spatial interpretation and viewpoint deduction are required. Figure 5: Heatmaps illustrating the relationship between model size and performance, measured by P&R Overall Accuracy and Robustness. Darker colors indicate higher performance. Each line represents model family, with sizes increasing from left to right. Figure 6: Correlation analysis between performance and size across MLLM families: (a) Overall accuracy vs. model size (r = 0.81), (b) Robustness vs. model size (r = 0.34), (c) Overall accuracy vs. encoder size (r = 0.51), (d) Robustness vs. encoder size (r = 0.15). Total model scaling strongly impacts perspective understanding, while vision encoder size has limited influence on robustness. Finding 3. Error pattern analysis reveals that while architectural/training choices strongly influence perspective perception biases, some spatial reasoning challenges present consistent difficulties across all model families. 8 Figure 7: Error pattern analysis across model families: (a) Cumulative distribution of phi coefficients shows significantly higher correlations within families than across families (Cohens = 0.33, < 0.001). (b) Task-wise breakdown reveals perception tasks (VAP, CLP) exhibit the strongest family-specific patterns, while reasoning tasks (VPC, LRR) show weaker family effects. Error correlations reveal that model architecture/training strongly influences perspective understanding failure modes. Fig. 7a demonstrates models from the same family exhibit significantly more similar error patterns than models from different families (Cohens = 0.33, < 0.001), indicating architectural biases systematically affect perspective interpretation. The task-wise analysis in Fig. 7b reveals this family effect varies markedly across the perspective hierarchy: low-level perception tasks show the strongest architecture/training-specific biases, with VAP and CLP exhibiting the largest within-family versus across-family differences (p < 0.001). Notably, some tasks maintain relatively high correlation coefficients even in cross-family comparisons, particularly for VAP (0.41) and VPC (0.31). This suggests certain perspective challenges present universal difficulties that transcend architectural/training differences, especially tasks requiring complex spatial judgment (VAP) or precise counting of geometric features (VPC). In contrast, tasks like CLP show much larger gaps between within-family and cross-family correlations, indicating these capabilities are more sensitive to architectural design or training choices. These patterns reveal that while architecture significantly shapes perspective understanding biases, some fundamental spatial reasoning challenges remain consistently difficult across model designs."
        },
        {
            "title": "4 Related Work",
            "content": "Perspective Understanding. Perspective is cornerstone of visual realism, dictating how objects in 2D image are perceived as three-dimensional. The theory of perspective can be traced back to Renaissance art, where principles such as VPs and HLs were formalized [Elkins, 1994, Haley, 2018]. In computer graphics and vision, perspective projection [Huang et al., 2023] ensures that parallel lines in the real world converge at VP on the image plane [Hartley and Zisserman, 2003]. Multiple VPs, depending on the orientation of objects, define 1-point, 2-point, or 3-point perspectives. Efficient and accurate VP detection has been critical area of research, facilitating tasks like scene reconstruction [Lee et al., 2009, Hedau et al., 2009] and camera calibration [Zhang, 2000]. Techniques such as the Hough Transform [Duda and Hart, 1972] and its extensions [Candès et al., 2011] enable robust line detection, while Gaussian sphere mapping [Barnard, 1983] provides framework for detecting intersections representing VPs. Classical methods often detect VPs through line segment intersections [Quan and Mohr, 1989, Lutton et al., 1994], followed by clustering approaches [McLean and Kotturi, 1995] or specialized voting schemes [Gamba et al., 1996]. Recent works leverage deep learning, with methods like NeurVPS [Zhou et al., 2019] that employ conic convolution operators and the Deep Hough Transform [Lin et al., 2022] to improve accuracy in VP detection across diverse datasets. Evaluation Benchmarks for MLLMs. With the rapid advancement of MLLMs [Fei et al., 2024], numerous benchmarks have emerged to systematically evaluate diverse capabilities [Li et al., 2025b]. These benchmarks generally assess two dimensions: text-centric evaluations measuring commonsense knowledge and reasoning (MMMU [Yue et al., 2024], NaturalBench [Li et al., 2024a]), and visioncentric assessments focusing on perception and robustness (MMBench [Liu et al., 2024d], MME [Fu et al., 2023], Grit [Gupta et al., 2022]). Specialized visual tasks are evaluated through benchmarks for spatial relationship comprehension (SEED-Bench [Li et al., 2023a], MM-Vet [Yu et al., 2024a]), chart understanding (MMSTAR [Chen et al., 2024a], MuirBench [Wang et al., 2024a]), visual grounding 9 (Flickr30k [Plummer et al., 2015], TRIG [Li et al., 2025a]), and hallucination detection (POPE [Li et al., 2023b], HallusionBench [Guan et al., 2024]). Common evaluation approaches include image captioning [Lin et al., 2014, Onoe et al., 2024], Visual Question Answering [Antol et al., 2015, Marino et al., 2019, Mathew et al., 2020], and visual reasoning [Johnson et al., 2017, Suhr et al., 2017, Hua et al., 2025, Bi et al., 2025a]. However, while certain benchmarks incorporate deeper assessments of perspective understanding remains limited [Thrush et al., 2022, Hua et al., 2024]."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduce MMPerspective, the first benchmark to systematically evaluate perspective understanding in MLLMs. Through 10 tasks across perception, reasoning, and robustness, we reveal that while current models demonstrate basic geometric awareness, they fall short in compositional reasoning and maintaining consistency under perspective-preserving transformations. Our large-scale evaluation of 43 models uncovers clear performance trends and architectural limitations, pointing to the need for stronger spatial priors and geometry-aware design. MMPerspective provides foundation for diagnosing perspective-related weaknesses and guiding the development of more spatially grounded vision-language systems."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 24252433, 2015. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Stephen T. Barnard. Interpreting perspective images. Artificial Intelligence, 21(4):435462, 1983. Skanda Bharadwaj, Robert Collins, and Yanxi Liu. Recurrence-based vanishing point detection. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 89278936. IEEE, 2025. Jing Bi, Junjia Guo, Yunlong Tang, Lianggong Bruce Wen, Zhang Liu, and Chenliang Xu. Unveiling visual perception in language models: An attention head analysis approach, 2024. URL https: //arxiv.org/abs/2412.18108. Jing Bi, Junjia Guo, Susan Liang, Guangyu Sun, Luchuan Song, Yunlong Tang, Jinxi He, Jiarui Wu, Ali Vosoughi, Chen Chen, and Chenliang Xu. Verify: benchmark of visual explanation and reasoning for investigating multimodal reasoning fidelity, 2025a. Jing Bi, Susan Liang, Xiaofei Zhou, Pinxin Liu, Junjia Guo, Yunlong Tang, Luchuan Song, Chao Huang, Guangyu Sun, Jinxi He, et al. Why reasoning matters? survey of advancements in multimodal reasoning (v1). arXiv preprint arXiv:2504.03151, 2025b. Emmanuel Candès, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis? Journal of the ACM (JACM), 58(3):137, 2011. Robert Carroll, Aseem Agarwala, and Maneesh Agrawala. Image warps for artistic perspective manipulation. In ACM SIGGRAPH 2010 papers, pages 19. 2010. Robert Evan Carroll. warping framework for wide-angle imaging and perspective manipulation. PhD thesis, Citeseer, 2013. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024a. 10 Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024b. Yoann Coudert, Elmar Eisemann, Ricardo Marroquim, et al. Semi-automatic perspective lines from paintings. In GCH 2022-Eurographics Workshop on Graphics and Cultural Heritage, 2022. Antonio Criminisi, Martin Kemp, and Andrew Zisserman. Bringing pictorial space to life: computer techniques for the analysis of paintings. Digital art history: subject in transition, A. BentkowskaKafel, T. Cashen, and H. Gardner, eds, pages 77100, 2002. Leonardo Da Vinci. The notebooks of Leonardo da Vinci, volume 1. Courier Corporation, 2012. Google DeepMind. Gemini 2.0 flash, 2025. URL https://deepmind.google/technologies/ gemini/flash/. Richard Duda and Peter Hart. ªuse of the hough transform to detect lines and curves in pictures, º comm. ACM, 1972. James Elkins. The poetics of perspective. Cornell University Press, 1994. Hao Fei, Yuan Yao, Zhuosheng Zhang, Fuxiao Liu, Ao Zhang, and Tat-Seng Chua. From multimodal llm to human-level ai: Modality, instruction, reasoning, efficiency and beyond. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024): Tutorial Summaries, pages 18, 2024. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. Paolo Gamba, Alessandro Mecocci, and Salvatore. Vanishing point detection by voting scheme. In Proceedings of 3rd IEEE International Conference on Image Processing, volume 2, pages 301304. IEEE, 1996. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. Tanmay Gupta, Ryan Marten, Aniruddha Kembhavi, and Derek Hoiem. Grit: General robust image task benchmark. arXiv preprint arXiv:2204.13653, 2022. Sarah Haley. Perspective drawing. Tempe Digital, 2018. Richard Hartley. Multiple view geometry in computer vision, volume 665. Cambridge university press, 2003. Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003. Eugene Hecht. Optics. Pearson Education India, 2012. Varsha Hedau, Derek Hoiem, and David Forsyth. Recovering the spatial layout of cluttered rooms. In 2009 IEEE 12th international conference on computer vision, pages 18491856. IEEE, 2009. Hang Hua, Yunlong Tang, Ziyun Zeng, Liangliang Cao, Zhengyuan Yang, Hangfeng He, Chenliang Xu, and Jiebo Luo. Mmcomposition: Revisiting the compositionality of pre-trained vision-language models. arXiv preprint arXiv:2410.09733, 2024. Hang Hua, Jing Shi, Kushal Kafle, Simon Jenni, Daoan Zhang, John Collomosse, Scott Cohen, and Jiebo Luo. Finematch: Aspect-based fine-grained image and text mismatch detection and correction. In European Conference on Computer Vision, pages 474491. Springer, 2025. 11 Chao Huang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. Egocentric audio-visual object In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern localization. Recognition (CVPR), pages 2291022921, June 2023. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2017. Martin Kemp et al. The science of art: Optical themes in western art from brunelleschi to seurat. 1990. David Lee, Martial Hebert, and Takeo Kanade. Geometric reasoning for single image structure recovery. In 2009 IEEE conference on computer vision and pattern recognition, pages 21362143. IEEE, 2009. Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, and Deva Ramanan. Naturalbench: Evaluating visionlanguage models on natural adversarial samples. arXiv preprint arXiv:2410.14669, 2024a. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024b. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023a. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024c. Ming Li, Ruiyi Zhang, Jian Chen, Jiuxiang Gu, Yufan Zhou, Franck Dernoncourt, Wanrong Zhu, Tianyi Zhou, and Tong Sun. Towards visual text grounding of multimodal large language model. arXiv preprint arXiv:2504.04974, 2025a. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023b. Zongxia Li, Xiyang Wu, Hongyang Du, Huy Nghiem, and Guangyao Shi. Benchmark evaluations, applications, and challenges of large vision language models: survey. arXiv preprint arXiv:2501.02189, 2025b. Susan Liang, Chao Huang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. Av-nerf: Learning neural fields for real-world audio-visual scene synthesis. Advances in Neural Information Processing Systems, 36:3747237490, 2023. Susan Liang, Chao Huang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. Language-guided joint audio-visual editing via one-shot adaptation. arXiv preprint arXiv:2410.07463, 2024. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. Yancong Lin, Ruben Wiersma, Silvia Pintea, Klaus Hildebrandt, Elmar Eisemann, and Jan van Gemert. Deep vanishing point detection: Geometric priors make dataset variations vanish. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 61036113, 2022. 12 Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge (january 2024). URL https://llava-vl. github. io/blog/2024-01-30-llava-next, 1(8), 2024a. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024b. URL https:// llava-vl.github.io/blog/2024-01-30-llava-next/. Pinxin Liu, Haiyang Liu, Luchuan Song, and Chenliang Xu. Intentional gesture: Deliver your intentions with gestures for speech. arXiv preprint arXiv:2505.15197, 2025a. Xinyi Liu, Pinxin Liu, and Hangfeng He. An empirical analysis on large language models in debate evaluation. arXiv preprint arXiv:2406.00050, 2024c. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024d. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European Conference on Computer Vision, pages 216233. Springer, 2025b. Evelyne Lutton, Henri Maitre, and Jaime Lopez-Krahe. Contribution to the determination of vanishing points using hough transform. IEEE transactions on pattern analysis and machine intelligence, 16 (4):430438, 1994. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 31953204, 2019. Minesh Mathew, Dimosthenis Karatzas, R. Manmatha, and C. V. Jawahar. Docvqa: dataset for vqa on document images. 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 21992208, 2020. URL https://api.semanticscholar.org/CorpusID:220280200. Gerard McLean and Kotturi. Vanishing point detection by line clustering. IEEE Transactions on pattern analysis and machine intelligence, 17(11):10901095, 1995. Allister Neher. How perspective could be symbolic form. The Journal of aesthetics and art criticism, 63(4):359373, 2005. Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer, Su Wang, and Jason Baldridge. DOCCI: Descriptions of Connected and Contrasting Images. In arXiv:2404.19753, 2024. Erwin Panofsky. Perspective as symbolic form. Princeton University Press, 2020. Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 26412649, 2015. Long Quan and Roger Mohr. Determining perspective structures using hierarchical hough transform. Pattern Recognition Letters, 9(4):279286, 1989. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Alayrac, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Scott Robertson and Thomas Bertling. How to Draw: drawing and sketching objects and environments from your imagination. Designstudio Press, 2013. Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024. 13 Luchuan Song, Bin Liu, Guojun Yin, Xiaoyi Dong, Yufei Zhang, and Jia-Xuan Bai. Tacr-net: editing on deep video and voice portraits. In Proceedings of the 29th ACM International Conference on Multimedia, pages 478486, 2021. Luchuan Song, Guojun Yin, Zhenchao Jin, Xiaoyi Dong, and Chenliang Xu. Emotional listener portrait: Realistic listener motion simulation in conversation. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 2078220792. IEEE, 2023. Luchuan Song, Lele Chen, Celong Liu, Pinxin Liu, and Chenliang Xu. Texttoon: Real-time text toonify head avatar from single video. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. corpus of natural language for visual reasoning. In Annual Meeting of the Association for Computational Linguistics, 2017. URL https://api.semanticscholar.org/CorpusID:19435386. Yunlong Tang, Junjia Guo, Hang Hua, Susan Liang, Mingqian Feng, Xinyang Li, Rui Mao, Chao Huang, Jing Bi, Zeliang Zhang, et al. Vidcomposition: Can mllms analyze compositions in compiled videos? arXiv preprint arXiv:2411.10979, 2024. Yunlong Tang, Jing Bi, Chao Huang, Susan Liang, Daiki Shimada, Hang Hua, Yunzhong Xiao, Yizhi Song, Pinxin Liu, Mingqian Feng, et al. Caption anything in video: Fine-grained object-centric captioning via spatiotemporal multimodal prompting. arXiv preprint arXiv:2504.05541, 2025. Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 52385248, 2022. Fei Wang, Xingyu Fu, James Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, et al. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv preprint arXiv:2406.09411, 2024a. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024b. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024c. Teng Wang, Jinrui Zhang, Junjie Fei, Hao Zheng, Yunlong Tang, Zhe Li, Mingqi Gao, and Shanshan Zhao. Caption anything: Interactive image description with diverse multimodal controls. arXiv preprint arXiv:2305.02677, 2023. Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024d. Colin Ware. Information visualization: perception for design. Morgan Kaufmann, 2019. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In International conference on machine learning. PMLR, 2024a. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In International conference on machine learning. PMLR, 2024b. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 14 Pengfei Zhang, Pinxin Liu, Hyeongwoo Kim, Pablo Garrido, and Bindita Chaudhuri. Kinmo: Kinematic-aware human motion understanding and generation. arXiv preprint arXiv:2411.15472, 2024. Zhengyou Zhang. flexible new technique for camera calibration. IEEE Transactions on pattern analysis and machine intelligence, 22(11):13301334, 2000. Kai Zhao, Qi Han, Chang-Bin Zhang, Jun Xu, and Ming-Ming Cheng. Deep hough transform for semantic line detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9): 47934806, 2021. Yichao Zhou, Haozhi Qi, Jingwei Huang, and Yi Ma. Neurvps: Neural vanishing point scanning via conic convolution. Advances in Neural Information Processing Systems, 32, 2019. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A Task Definitions",
            "content": "Table 3 outlines the and reasoning tasks included in the MMPerspective benchmark. Sample cases and representative questions are included to illustrate the task format and input style. We also show examples of perspective-invariant image operations for robustness evaluation in Figure 17, including cropping, masking, flipping, and rotation. Table 3: Task and question definition in MMPerspective. Task Vanishing Point Perception (VPP) # Sample Case 156 Figure 8 Description Sample Questions Identify the region the that contains in vanishing point the image. Where is the vanishing point in this image? Which line highlighted in the image is the Horizon Line? What direction is the Line of Sight in this image? Which region shows no curved-line distortion? What is the perspective type of this image? What is the relationship between these two highlighted lines in the 3D space? What changes occur from the left image to the right image? How many vanishing points can you identify within the image? In which quadrant might the vanishing point be located? t r v e e i a v e e Line Critical Perception (CLP) View Angle Perception (VAP) 162 Lens Distortion Perception (LDP) 285 Perspective Type Reasoning (PTR) Line Relationship Reasoning (LRR) Perspective Transformation Spotting (PTS) Vanishing Point Counting (VPC) Out-of-View Reasoning (OVR) 606 213 114 308 Figure 9 Determine which of the highlighted lines is the horizon line. Figure 10 Infer the cameras line of sight direction from spatial cues. Figure 11 Identify the region without curved-line distortion in the image. Figure 12 Classify the perspective type used in the image (e.g., onepoint, two-point). Figure 13 Determine the spatial relationship between two lines in 3D (e.g., parallel, perpendicular). Figure 14 Identify the change in perspective type between two images. Figure 15 Count the number of vanishing points present in the image. Figure 16 Infer the quadrant location of an unseen vanishing point based on scene geometry. Figure 8: Examples of Vanishing Point Perception. Figure 9: Examples of Critical Line Perception. Figure 10: Examples of View Angle Perception. 17 Figure 11: Examples of Line Relationship Reasoning. Figure 12: Examples of Perspective Type Reasoning. 18 Figure 13: Examples of Line Relationship Reasoning. Figure 14: Examples of Perspective Transformation Spotting. Figure 15: Examples of Vanishing Point Counting. 19 Figure 16: Examples of Out-of-View Reasoning. Figure 17: Examples of Perspective-Invariant Image Operations for Robustness Evaluation."
        },
        {
            "title": "B More Terminology of Perspective",
            "content": "Figure 18 illustrates the key distinction between the Line of Sight (LS) and Horizon Line (HL) in perspective drawing. HL represents the viewers eye level, while LS indicates the exact direction the viewer is looking. When LS is parallel to the ground, it aligns with HL, resulting in typical 2-point perspective with verticals remaining straight. But when LS tilts upward or downward, it separates from HL, introducing vertical convergence and shifting the drawing into 3-point perspective. Importantly, the relative position of LS and HL also determines the view angle. If LS is above HL, the viewer is looking up (upward view); if its below, the viewer is looking down (downward view). This shift changes what parts of an object are emphasized, more base or more top, and impacts how space is perceived."
        },
        {
            "title": "C More Visualization",
            "content": "C.1 Model Size & Performance for Each Task In Figure 20 to 28, we present heatmaps for the 10 tasks in our Perspective Perception, Perspective Reasoning. The figures show the correlations between the sizes of model parameters and the metrics. Deeper color represents better performance. Each row represents model family with the sizes growing from small to large. Most tasks clearly exhibit the correlation between model sizes and performance, i.e., larger model leads to higher metrics. However, Figure 27 shows that models with median size have better performance than smaller and larger models in Perspective Transformation Spotting (PTS). Moreover, in Vanishing Point Counting (VPC), we observe reversed correlation where larger models lead to worse performance. C.2 Effect of Chain-of-Thought Figures 29 to 32 are examples that demonstrate how Chain-of-Thought (CoT) can generally enhance the models performance. Despite the general enhancement, few failures still emerge. Figures 33, 34, and 35 show three representative failure cases, including GPT-4o on Perspective Type Reasoning, and Gemini-2-flash on Line Relationship and Perspective Transformation Spotting. In these three cases, we all observed that the models made direct factual errors when analyzing the information in the images, rather than logical errors during the CoT process. This indicates that what limits the performance of the model is the ability to understand images. C.3 Performance for Each Model Family Figure 36 and Figure 37 show task performance across various models within the same model families. Generally, models that are larger usually excel in most tasks. C.4 Question Difficulty Distribution Figure 38 presents the question difficulty distribution based on average model accuracy. Each question is categorized into four difficulty levels, Easy, Medium, Hard, and Super Hard, based on the proportion of models that answered it correctly. The top two charts show the overall and type-level distributions, while the bottom figure provides fine-grained view across tasks."
        },
        {
            "title": "D Annotation Tool",
            "content": "We develop dedicated annotation tool (see Figure 39) to support the systematic construction of multiple-choice questions in our benchmark. Designed specifically for perspective understanding, the tool enables annotators to load image pairs, formulate perspective-related questions, and select answers from predefined list of geometric transformations (e.g., 1-point to 3-point perspective, 2point to 1-point perspective). This standardization ensures consistent labeling across the dataset. The interface integrates suite of carefully designed features to facilitate precise annotation. Annotators can draw lines and circles to mark vanishing directions, orthogonal structures, or other relevant cues. 21 Figure 18: The relationship between Station Point (SP), Picture Plane (PP), Line of Sight (LS), and Horizon Line (HL) in perspective drawing. They demonstrate how viewing objects from different heights and angles affects spatial representation, emphasizing the critical distinction between LS and HL for accurate perspective construction. Figures are adapted from [Robertson and Bertling, 2013]. 22 Figure 19: Word clouds of questions (left) and answer choices (right) in the MMPerspective Benchmark, illustrating the distribution of key terms related to perspective understanding. Figure 20: The heatmap for Vanishing Point Perception. Figure 21: The heatmap for Critical Line Perception. Figure 22: The heatmap for View Angle Perception. Figure 23: The heatmap for Lens Distortion Perception. Figure 24: The heatmap for Perspective Type Reasoning. Figure 25: The heatmap for Line Relationship Reasoning. Figure 26: The heatmap for Out of View Reasoning. Figure 27: The heatmap for Perspective Transformation Spotting. Figure 28: The heatmap for Vanishing Point Counting. Adjustable line width, zoom controls, and undo/redo functionality support detailed inspection and flexible editing. The tool also provides step-wise navigation through image sets and supports saving both visual annotations and structured Q&A data. By tailoring the design to the specific needs of perspective-based reasoning, the tool enables the efficient generation of high-quality, semantically grounded tasks. It plays central role in ensuring the accuracy, consistency, and scalability of our benchmark construction."
        },
        {
            "title": "E Limitations",
            "content": "While MMPerspective provides comprehensive benchmark for evaluating perspective understanding in MLLMs, several limitations should be acknowledged. First, the benchmark primarily focuses on static images and multiple-choice question formats, which may not fully capture the depth of 23 Figure 29: Examples of Chain-of-Thought Reasoning. Figure 30: Examples of Chain-of-Thought Reasoning. 24 Figure 31: Examples of Chain-of-Thought Reasoning. Figure 32: Examples of Chain-of-Thought Reasoning. Figure 33: Examples of GPT-4o with Chain-of-Thought Reasoning in Perspective Type Reasoning. 25 Figure 34: Examples of Gemini-2-flash with Chain-of-Thought Reasoning in Perspective Transformation Spotting. Figure 35: Examples of Gemini-2-flash with Chain-of-Thought Reasoning in Line Relationship Reasoning. Figure 36: Task performance of models within each family (part 1). 27 Figure 37: Task performance of models within each family (part 2). spatial reasoning required in dynamic or open-ended tasks. Real-world applications often demand free-form generation, spatial manipulation, or multi-turn interactions that extend beyond our current evaluation scope. Second, although we curated diverse set of real and synthetic images, the dataset still exhibits bias toward architectural and indoor scenes, which may limit generalizability to natural environments or abstract visual contexts. Third, despite our efforts to standardize evaluation, some tasks inevitably contain ambiguous visual cues, and model errors may stem from subjective interpretations rather than lack of geometric understanding. Lastly, our benchmark assumes that all correct answers are equally accessible across models without considering differences in input modalities, prompting formats, or underlying vision-language alignment strategies. Future work could address these limitations by incorporating more open-ended tasks, expanding domain diversity, and developing adaptive evaluation protocols that account for model-specific reasoning pathways. 28 Figure 38: Distribution of question difficulty across task types. Figure 39: Annotation interface developed for constructing perspective-based multiple-choice questions. The tool integrates geometric drawing utilities, structured answer selection, and image navigation to support precise and consistent labeling."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "University of Rochester"
    ]
}