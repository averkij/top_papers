{
    "paper_title": "Neural LightRig: Unlocking Accurate Object Normal and Material Estimation with Multi-Light Diffusion",
    "authors": [
        "Zexin He",
        "Tengfei Wang",
        "Xin Huang",
        "Xingang Pan",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recovering the geometry and materials of objects from a single image is challenging due to its under-constrained nature. In this paper, we present Neural LightRig, a novel framework that boosts intrinsic estimation by leveraging auxiliary multi-lighting conditions from 2D diffusion priors. Specifically, 1) we first leverage illumination priors from large-scale diffusion models to build our multi-light diffusion model on a synthetic relighting dataset with dedicated designs. This diffusion model generates multiple consistent images, each illuminated by point light sources in different directions. 2) By using these varied lighting images to reduce estimation uncertainty, we train a large G-buffer model with a U-Net backbone to accurately predict surface normals and materials. Extensive experiments validate that our approach significantly outperforms state-of-the-art methods, enabling accurate surface normal and PBR material estimation with vivid relighting effects. Code and dataset are available on our project page at https://projects.zxhezexin.com/neural-lightrig."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 1 ] . [ 1 3 9 5 9 0 . 2 1 4 2 : r Neural LightRig: Unlocking Accurate Object Normal and Material Estimation with Multi-Light Diffusion Zexin He1*, Tengfei Wang2*, Xin Huang2, Xingang Pan3, Ziwei Liu3 1The Chinese University of Hong Kong, 2Shanghai AI Lab, 3Nanyang Technological University Figure 1. Neural LightRig takes an image as input and generates multi-light images to assist the estimation of high-quality normal and PBR materials, which can be used to render realistic relit images under various environment lighting."
        },
        {
            "title": "Abstract",
            "content": "Recovering the geometry and materials of objects from single image is challenging due to its under-constrained nature. In this paper, we present Neural LightRig, novel framework that boosts intrinsic estimation by leveraging auxiliary multi-lighting conditions from 2D diffusion priors. Specifically, 1) we first leverage illumination priors from large-scale diffusion models to build our multi-light diffusion model on synthetic relighting dataset with dedicated designs. This diffusion model generates multiple consistent images, each illuminated by point light sources in different directions. 2) By using these varied lighting images to reduce estimation uncertainty, we train large G-buffer model with U-Net backbone to accurately predict surface normals and materials. Extensive experiments validate that our approach significantly outperforms state-of-the-art methods, enabling accurate surface normal and PBR material estimation with vivid relighting effects. Code and dataset are available on our project page at https://projects.zxhezexin.com/neural-lightrig. 1. Introduction * Equal contribution. Work done during Zexin Hes internship at Shanghai AI Lab. Recovering the geometry and physically-based rendering (PBR) materials of real-world objects from images is piv1 otal problem in graphics and computer vision. This task, also known as inverse rendering, facilitates wide range of applications, such as video gaming, augmented and virtual reality, and robotics. In this paper, we proposed datadriven approach for jointly estimating the surface normal and PBR materials of objects from single image. Due to the complex interaction among geometry, materials, and environmental lighting, this ill-posed problem remains particularly challenging. Prior research [6, 17] has predominantly focused on optimization-based generation through differentiable rendering, which compares forward-rendered images with input images to refine normals and PBR materials. However, these methods are often time-consuming and heavily reliant on the capabilities of the differentiable renderer [27]. Though some works explored feed-forward estimation [34, 54, 57], their quality and generalizability still remain challenging, due to the inherently ill-posed nature of inferring geometry and materials from single image. For precise normal and material acquisition, photometric stereo techniques [51] are widely employed, as they mitigate ambiguity by capturing multiple images from the same viewpoint with various lighting. These images are illuminated by different point light sources, which provide variations in surface reflectance to enrich information. However, such methods [10, 13, 28] often require complex capture systems with sophisticated cameras or lighting setups, which can be costly and impractical for in-the-wild images. Given the promising advances in image diffusion models, we ask the question: can we develop multi-light diffusion model to simulate images illuminated by different directional light sources, thereby improving surface normal and material estimation (as shown in Fig. 1)? Our motivation arises from recent advances in 3D generation, which employ diffusion models [30, 43] to generate multi-view images and train reconstruction models [21] for 3D reconstruction. These multi-view diffusion models have demonstrated the potential to manipulate camera views of pre-trained image diffusion models such as Stable Diffusion [38]. Similarly, we aim to expand the use of pre-trained diffusion models for multi-light image generation. In this work, we present Neural LightRig for joint normal and material estimation of objects from monocular images, which consists of multi-light diffusion model and large prediction model. Given an input image, the multilight diffusion model produces consistent and high-quality relit images under various point light sources (as shown in Fig. 4). To achieve this, we create synthetic relighting dataset for training with Blender [9]. With dedicated architecture and training design, our diffusion model enables the multi-light generation of objects from arbitrary categories. The large G-buffer model then processes the generated multi-light images to produce surface normals and PBR materials, such as albedo, roughness, and metallic. We employ UNet architecture for efficient and high-resolution prediction, with end-to-end supervision at the pixel level. To bridge the domain gap between multi-light images rendered from 3D objects and those generated by diffusion models, we further design series of data augmentation strategies for domain alignment. Taken together, the proposed framework demonstrates remarkable performance on both synthetic and real-world images. Extensive qualitative and quantitative evaluations show that Neural LightRig surpasses existing approaches in surface normal estimation, PBR material estimation, and single-image relighting. Comprehensive visual results are provided in the appendix and on our project page. Our key contributions are as follows: We propose novel approach for object normal and PBR estimation from monocular images, reformulating this illposed problem by simulating multi-lighting conditions. We construct synthetic dataset for multi-light image generation and surface property estimation. With this dataset, we demonstrate the capability to manipulate diffusion models for consistent multi-light generation. Extensive experiments validate the effectiveness of our method, establishing new state-of-the-art results. 2. Related Works Diffusion Models. Well-trained diffusion models [38, 49] have shown promising potential in providing essential priors for under-determined tasks. Recent works showcase the utility of image diffusion models in novel-view synthesis [32, 33, 35, 43, 44, 50], which combines with reconstruction models [18, 21, 46] to achieve high-quality 3D generation. Similarly, some recent works attempt to leverage the learned priors in diffusion models to simulate lighting variations [23, 56], but they do not account for the consistency of multi-light generation. In contrast, we aim to generate multiple images under different lighting sources that facilitate object surface property estimation. Monocular Normal Estimation. Estimating surface normals from single image is classic yet under-determined problem. Early works often relied on photometric cues or handcrafted features [15, 19, 20], while later works adopted deep learning to improve accuracy [4, 12, 26, 29, 37, 48, 52, 62]. More recently, large-scale datasets [11, 14] have further advanced regression-based methods [2, 3, 5]. Despite promising results, they struggle with complex details due to inherent ambiguity. Diffusion-based methods [16, 25, 53], turn to generative priors [38] to help address such ambiguity but often fall short in accurately aligning with ground truth, leading to deviations in finer geometric details crucial for downstream tasks. Material Estimation. Material estimation aims to recover intrinsic properties from images, which is an ill-posed prob2 Figure 2. Framework Overview. Multi-light diffusion generates multi-light images from an input image. These images with corresponding lighting orientations are then used to predict surface normals and PBR materials with regression U-Net. lem, as multiple combinations of materials and lighting conditions could lead to the same appearance, Traditional methods attempted to employ photometric stereos [13, 51] to disambiguate this problem under controlled lighting conditions [10, 28]. Some works [7, 17, 45, 58] optimize neural representation with multi-view images. Later, the emergence of large-scale synthetic datasets [11, 47] has advanced data-driven approaches [31, 34, 41, 42, 54, 55], but they still contend with under-determination. Recently, diffusion-based methods [8, 22, 36, 57] have emerged as promising alternative, but often suffer from domain shift between material images and natural images. 3. Approach Given an image I, we aim to estimate both its surface normal and PBR materials (albedo a, roughness r, and metallic m), where n, RHW 3 and r, RHW 1. These surface properties, commonly known as G-buffers in graphics, are collectively denoted as = {n, a, r, m}. However, interpreting these properties from single lighting condition is challenging due to the the underconstrained nature of the problem. To address this, we propose Neural LightRig, as illustrated in Fig. 2. Our approach leverages multi-light diffusion model (Sec. 3.1) to generate multi-light images from the input, which then act as enriched conditions to alleviate the inherent ambiguity in G-buffer prediction model (Sec. 3.2). We further describe the construction of our synthetic dataset, LightProp, which supports both stages of our framework, in Sec. 3.3. 3.1. Multi-Light Diffusion To obtain surface reflectance variations that increase contextual information for accurate G-buffer estimation, we learn diffusion model g() to generate multi-light images from the input image I: {xi = 1, 2, . . . , L} = g(I). (1) In particular, we set = 9 to balance performance and efficiency, covering diverse range of lighting variations  (Fig. 4)  without excessive overhead. Generating Multi-Light Images. Collecting such training pairs is challenging due to the limited availability of 3D objects with PBR [11, 47] and the high cost of real-world capturing in photometric stereos [24]. Fortunately, diffusion models trained on massive internet images have shown an inherent ability to model complex 3D shapes and textures, which have been applied for novel view synthesis [43] and relighting [23, 56]. We thus leverage the prior from well-trained image diffusion model and fine-tune it for multi-light generation, arguing that such well-trained image generation model possesses the capacity to simulate diverse lighting conditions. Rather than generating each-light image xi separately, we arrange nine-light images in 3 3 grid layout to form single image x, allowing the simultaneous generation for them. This simple configuration facilitates efficient cross-image context communication, thereby enhancing the consistency of generated multi-light images. Conditioning Strategy. To incorporate the input image into the diffusion model, we employ hybrid conditioning method, as illustrated in Fig. 3. As the input images are pixel-wise aligned with the multi-light images, we naturally apply channel-wise concatenation. This straightforward concatenation effectively captures the variations between the input and each multi-light image, which is essential for generating accurate lighting effects. However, we found this simple concatenation alone is inadequate for generating high-fidelity multi-light images, leading to discrepancies in color tone and texture relative to the input. To address this, we further adopt reference attention [43, 59], where self-attention layers in the denoising U-Net also attend to keys and values obtained from the input image. This is represented as Attn(Q, [K, Kcond], [V, Vcond]), in which Q, K, are the query, key, and value tokens from the denoising stream, and the subscript cond denotes tokens from the input image. This combined approach manages to preserve desired textures from in the input and is crucial for generating high-quality and realistic multi-light images. Tuning Scheme. We build our model on Stable Diffusion Figure 3. Hybrid condition in multi-light diffusion. Input images are incorporated via concatenation with noise latents and enhanced through reference attention, where queries in the denoise stream attend to keys and values from both streams. Figure 4. Visualization of multi-light setup in LightProp. Camera and point lights are positioned on sphere around the object. θ, φ are spherical coordinates to determine each lights orientation relative to the object. v-version model [1, 40]. Let αt, σt be the controlling factors in the diffusion process, and define ground-truth velocity as = αtϵ + σtx and predicted velocity as vθ(). The training target can be denoted as: = Ex,I,ϵ,t (cid:2)v vθ(zt, t, I)2(cid:3) , (2) where zt is the noisy latent of at timestep t, and is the input image. To fully leverage the capacity of diffusion model, we adopt two-phase training scheme. Initially, we freeze most parameters except for the first convolution layer and all attention layers to warm up the weights. This stabilizes early training, allowing for smooth transition without severely disrupting the pre-trained model. Afterwards, we fine-tune the entire model at considerably lower learning rate, facilitating careful adaptation for multi-light generation while retaining as much prior knowledge as possible. 3.2. Large G-Buffer Model Next, we learn regression model () to predict normals and PBR maps with the auxiliary multi-light images. Prediction Model. Since the input image, multi-light images, and G-buffer maps are pixel-wise aligned, we opt for U-Net architecture thanks to its efficiency in high-resolution prediction. Also, U-Net provides inductive bias for learning spatial relations, making it well-suited for our task. The model takes channel-wise concatenated input and multilight images, and outputs an 8-channel G-buffer, containing 3-channel and maps, and 1-channel and maps. This multi-light-enhanced G-buffer prediction is represented as: = (cid:0)I, (cid:8)(xi, θi, φi) = 1, 2, . . . , L(cid:9)(cid:1) , (3) where each novel-light image xi is associated with the light source poses θi and φi, which indicate spherical coordinates of the light source relative to the object (see Fig. 4). Conditioning on these poses allows () to explicitly correlate shading variations with their respective light sources, enhancing surface estimation. Training Objectives. To train the model () for G-buffer prediction, we apply loss functions to each of the G-buffer properties. We employ cosine similarity loss for normals, enforcing the model to capture precise surface orientations. To stabilize the training, we also include an MSE term as regularization: (cid:18) Lnormal = 1 (cid:19) ˆn nˆn + λ1n ˆn2, (4) where ˆn and are the predicted and ground-truth normals. For the predicted albedo ˆa, roughness ˆr, and metallic ˆm, we simply use MSE losses as: LPBR = ˆa2 + ˆr2 + ˆm2. (5) The overall loss is the weighted sum of the two losses. Augmentations. We train our prediction model using ground-truth rendered multi-light images, but for inference, we rely on generated images from diffusion models. In our earlier experiments, we observed domain gap between the generated and rendered multi-light images in sharpness and brightness. This gap would introduce discrepancies between training and inference, causing degraded performances. To bridge this gap, we apply series of augmentations to multi-light images during training, including: (a) Random Degradation, such as resizing and grid distortion that simulate small misalignments; (b) Random Intensity that adjusts brightness in HSV space, simulating brightness variations of multi-light images; (c) Random Orientation perturbs {θi, φi} to account for potential disparities, encouraging () to be robust to inaccurate lighting cues; and (d) Data Mixing, where we mix generated multi-light images into the training data to further mitigate this gap. 3.3. LightProp Dataset To train our model, we need to collect paired multi-light images and corresponding normal and PBR material maps. 4 Table 1. Quantitative comparison on surface normal estimation. We report mean and median angular errors, as well as accuracies within different angular thresholds from 3 to 30. Method RGBX [57] DSINE [2] GeoWizard [16] Marigold [25] StableNormal [53] Ours Mean Median 14.847 9.161 8.455 8.652 8.034 6.413 13.704 7.457 6.926 7.078 6.568 4. 3 11.676 23.565 22.245 25.219 21.393 38.656 5 23.073 41.751 40.993 42.289 43.917 56. 7.5 35.196 57.596 58.457 58.062 63.740 70.938 11.25 22.5 49.829 72.003 74.916 72.873 78. 82.853 75.777 90.294 93.315 92.326 93.671 95.412 30 86.348 95.297 97.162 96.742 96.785 98. Table 2. Quantitative comparison on PBR materials estimation and single-image relighting. Method Albedo Roughness Metallic PSNR RMSE PSNR RMSE PSNR RMSE PSNR Relighting SSIM LPIPS Latency Average Time RGBX [57] Yi. et al [54] IntrinsicAnything [8] DiLightNet [56] IC-Light [60] Ours 16.26 21.10 23.88 - - 26.62 0.176 0.106 0.078 - - 0.054 19.21 16.88 17.25 - - 23.44 0.134 0.180 0.172 - - 0.085 16.65 20.30 22.00 - - 26.23 0.199 0.144 0.134 - - 0.109 20.78 26.47 27.98 22.68 20. 30.12 0.8927 0.9316 0.9474 0.8751 0.9027 0.9601 0.0781 0.0691 0.0490 0.0981 0.0638 0.0371 15s 5s 2min 30s 1min 5s However, capturing such pairs in the real world requires specialized photometric equipments and controlled lighting, which is impractical for large-scale collection, while internet images typically lack access to their underlying 3D data, making it infeasible to derive ground-truth surface properties. Therefore, we construct synthetic dataset LightProp, where we curate 80k objects from Objaverse [11], filtering out those of low-quality or without PBR materials. LightProp provides multi-light images and G-buffer maps for every object. Each object is rendered at 5 random views, and for each view, we simulate 5 images under random lighting conditions, including point light, area light, and HDR environment maps. Each view also provides full set of surface normal and PBR materials, along with multilight images rendered under known directional lighting. As shown in Fig. 4, we position the camera and point lights on sphere around the object, where θ determines the vertical position of the lights relative to the overhead direction, and φ controls the rotation relative to the camera. In practice, the positions of light sources are fixed during the training of multi-light diffusion model g() and the inference of Gbuffer prediction model (), while randomized light positions are applied for training () to encourage generalization. More details on dataset construction can be found in the appendix. 4. Experiments We evaluate our method across various tasks. For normal estimation, we benchmark against regression-based method DSINE [2] and diffusion-based methods GeoWizard [16], Marigold [25] and StableNormal [53]. For PBR material prediction, we compare our method with data-driven method by Yi et al. [54], an optimization method IntrinsicAnything [8], and diffusion-based model RGBX [57]. For image relighting, we use groundtruth normal maps and predicted PBR materials from baselines [8, 54, 57] to render relit images, serving as relighting baselines. We also compare our method with diffusion-based image relighting models DiLightNet [56] and IC-Light [60], using captioning model [39] to generate prompts. 4.1. Quantitative Evaluation We calculate metrics on held-out subset of LightProp, consisting of 1, 000 randomly selected, unseen objects. Normal. Following prior works [16, 53], we report the comparison results in mean and median angular errors, and accuracy within various angular thresholds. Since we observe promising accuracy within the commonly used thresholds from 5to 30, we further report the accuracy under finer threshold of 3. As shown in Tab. 1, our method outperforms baselines across all metrics, particularly under finer thresholds, clearly showing the effectiveness. Materials and Relighting. Following previous works, we calculate PSNR and RMSE for albedo, roughness, and metallic maps, and evaluate relit images using PSNR, SSIM, and LPIPS [61]. We also report the average time per frame, calculated by measuring the total time to render 120 relit frames from single input image and dividing by the number of frames. As shown in Tab. 2, our method shows clear improvement over baselines. These results demonstrate the effectiveness and efficiency of our approach in predicting accurate material properties and rendering faithful relighting images. 4.2. Qualitative Evaluation We present qualitative comparison results on both the unseen Objaverse subset and in-the-wild images. More visual Figure 5. Qualitative comparison on surface normal estimation. Ground truth normals (G.T.) are provided for input images rendered from available 3D objects (the last two rows) and are omitted for in-the-wild images (the first two rows). Figure 6. Qualitative comparison on single-image relighting. results are given in appendix. Normal. As shown in Fig. 5, our method produces sharp, coherent normal maps while preserving surface details. For instance, in the cow case, our method accurately captures the normal variations around the ears. In the robot example, other methods tend to produce over-smoothed or inaccurate normal, while ours demonstrates clear advantage in capturing complex surface geometries. Please refer to Fig. 16 6 Figure 7. Qualitative comparison on PBR material estimation. Ground truth materials (G.T.) are provided for input images rendered from available 3D objects (the right column) and are omitted for in-the-wild images (the left column). Table 3. Effects of condition strategies in multi-light diffusion. PSNR SSIM LPIPS Concatenation Reference Attention Concatenation + Reference Attention 19.32 19.87 20. 0.8597 0.8691 0.8718 0.0909 0.0829 0.0815 for more examples. PBR Materials. As shown in Fig. 7, our approach generates more accurate PBR materials than baselines. Baseline methods fail to remove highlights in their albedo maps, while our approach produces smooth base colors regardless of the illumination conditions of input images. Also, our method is more robust at distinguishing metal and nonmetal materials, while baselines are prone to reflective parts or fail to locate the metallic regions. More examples can be found in Figs. 17 and 18. Image Relighting. As shown in Fig. 6, our approach generates realistic lighting effects and retains details such as Chinese characters in the last example. In contrast, without underlying physical properties, DiLightNet and IC-Light tend to generate over-saturated images, while others are limited in eliminating highlights and shadows from the input image. Video comparisons are provided in our project page. In the appendix, we provide more relighting comparisons in Fig. 19 and more relighting results of our method in Figs. 14 and 15. Figure 8. Visualization of different conditioning strategies in multi-light diffusion. Concat stands for concatenation. RA stands for reference attention. 4.3. Ablation Study Due to the expensive training cost of the full model, we use smaller models for the following ablation experiments. Conditioning Strategy for Multi-Light Diffusion. We ex7 Table 4. Effect of the number of multi-light images on the performance of the large G-buffer model. Number of Light Images Albedo Roughness Metallic PSNR RMSE PSNR RMSE PSNR RMSE MAE 0 3 6 9 22.22 23.72 23.82 23.90 0.082 0.068 0.068 0.067 20.99 23.89 24.19 24.36 0.104 0.075 0.072 0.069 18.56 20.66 20.64 20. 0.136 0.106 0.106 0.105 7.563 4.763 4.275 4.059 5 45.846 68.344 72.777 74.720 Normal 7.5 61.425 80.896 83.997 85. 11.25 22.5 76.948 89.959 91.730 92.330 95.488 97.928 98.312 98.431 Table 5. Effect of augmentation strategy on the large G-buffer model. Albedo Roughness Metallic PSNR RMSE PSNR RMSE PSNR RMSE MAE w/o augmentation w/ augmentation 21.69 22.36 0.087 0.081 20.46 21. 0.110 0.099 16.61 18.81 0.179 0.135 7.080 6.342 5 52.235 55. Normal 7.5 11.25 22.5 67.032 70.326 80.115 82.848 94.802 96. Figure 9. Visualization of using different numbers of multi-light images. We evaluate the G-Buffer prediction model with different numbers of novel-light images (0, 3, 6, and 9) as conditions. plore three different settings, concatenation, reference attention (RA), and our hybrid approach. The quantitative analyses are given in Tab. 3. As shown in Fig. 8, while Concat captures correct highlights and shadows, it often results in over-saturated colors or inaccurately rendered surface textures, as seen in the excessive brightness on the vase and inconsistent color tones on the chess piece. RA, on the other hand, fails to reflect faithful lighting effects. In contrast, the hybrid approach yields the best qualitative and quantitative performances. Number of Multi-Light Images for Prediction. To examine how multi-light images affect performance, we evaluate the large G-buffer model with varying numbers of rendered light images (0, 3, 6, and 9). As shown in Tab. 4, the performances improve sharply from 0 to 3 images by reducing ambiguity, and steadily improve with more provided images. The same conclusion is also observed in Fig. 9, where leveraging multi-light images yields sharper normal and better PBR maps. Figure 10. Visualization of the augmentation strategy. Effects of Augmentation Strategy. We examine the impact of data augmentation on enhancing the robustness and generalization of the G-buffer prediction model. As shown in Tab. 5 and Fig. 10, the proposed augmentation strategy improves the models ability to produce consistent and accurate outputs, demonstrating increased invariance to artifacts introduced by the multi-light diffusion model. This augmentation effectively bridges the gap caused by noise, color inconsistencies, and other disturbances. 5. Conclusion In this work, we present Neural LightRig, framework capable of estimating accurate surface normals and PBR materials from single image. Leveraging multi-light diffusion model, we generated consistent relit images under various directional light sources. These generated images significantly reduce the inherent ambiguity when estimating surface properties, serving as enriched conditions for the G-Buffer prediction model. Extensive experiments demon8 strate that our method achieves significant improvements in both quality and generalizability. Future work will focus on extending this approach to more complex scenes and integrating it with 3D reconstruction systems."
        },
        {
            "title": "References",
            "content": "[1] Stability AI. https : / / huggingface . co / stabilityai / stable - diffusion-2-1, 2023. 4 Stable diffusion v2.1. [2] Gwangbin Bae and Andrew J. Davison. Rethinking inductive biases for surface normal estimation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, 5 [3] Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla. Estimating and exploiting the aleatoric uncertainty in surface normal estimation. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), page 1311713126. IEEE, 2021. 2 [4] Aayush Bansal, Bryan Russell, and Abhinav Gupta. Marr revisited: 2d-3d alignment via surface normal prediction. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), page 59655974. IEEE, 2016. 2 [5] Manel Baradad, Yuanzhen Li, Forrester Cole, Michael Rubinstein, Antonio Torralba, William T. Freeman, and Varun Jampani. Background prompting for improved object depth, 2023. [6] Jonathan Barron and Jitendra Malik. Shape, albedo, and illumination from single image of an unknown object. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 334341. IEEE, 2012. 2 [7] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu, and Hendrik P.A. Lensch. Nerd: Neural reflectance decomposition from image collections. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV). IEEE, 2021. 3 [8] Xi Chen, Sida Peng, Dongchen Yang, Yuan Liu, Bowen Pan, Chengfei Lv, and Xiaowei Zhou. Intrinsicanything: Learning diffusion priors for inverse rendering under unknown illumination, 2024. 3, 5 [9] Blender Online Community. Blender - 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. 2, 12 [10] Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter Duiker, Westley Sarokin, and Mark Sagar. Acquiring the In Proceedings of the reflectance field of human face. 27th Annual Conference on Computer Graphics and Interactive Techniques, page 145156, USA, 2000. ACM Press/Addison-Wesley Publishing Co. 2, 3 [11] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. 2, 3, 5, [12] Tien Do, Khiem Vuong, Stergios I. Roumeliotis, and Hyun Soo Park. Surface normal estimation of tilted images 9 via spatial rectifier. In Proc. of the European Conference on Computer Vision, Virtual Conference, 2020. 2 [13] O. Drbohlav and M. Chaniler. Can two specular pixels calIn Tenth IEEE International ibrate photometric stereo? Conference on Computer Vision (ICCV05) Volume 1, pages 18501857 Vol. 2, 2005. 2, 3 [14] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: scalable pipeline for making multi-task mid-level vision datasets from 3d scans. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), page 1076610776. IEEE, 2021. 2 [15] David F. Fouhey, Abhinav Gupta, and Martial Hebert. Datadriven 3d primitives for single image understanding. In 2013 IEEE International Conference on Computer Vision, pages 33923399, 2013. [16] Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image. In ECCV, 2024. 2, 5 [17] Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg. Shape, light, and material decomposition from images usIn Proceedings ing monte carlo rendering and denoising. of the 36th International Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2024. Curran Associates Inc. 2, 3 [18] Zexin He and Tengfei Wang. Openlrm: Open-source large reconstruction models. https://github.com/ 3DTopia/OpenLRM, 2023. 2 [19] Derek Hoiem, Alexei A. Efros, and Martial Hebert. Automatic photo pop-up. ACM Trans. Graph., 24(3):577584, 2005. 2 [20] Derek Hoiem, Alexei A. Efros, and Martial Hebert. RecovInternational Journal ering surface layout from an image. of Computer Vision: Special Issue on Celebrating Kanades Vision, 75(1):151 172, 2007. 2 [21] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. LRM: Large reconstruction model for single image to In The Twelfth International Conference on Learning 3d. Representations, 2024. [22] Xin Huang, Tengfei Wang, Ziwei Liu, and Qing Wang. Material anything: Generating materials for any 3d object via diffusion. arXiv, 2024. 3 [23] Haian Jin, Yuan Li, Fujun Luan, Yuanbo Xiangli, Sai Bi, Kai Zhang, Zexiang Xu, Jin Sun, and Noah Snavely. Neural gaffer: Relighting any object via diffusion. In Advances in Neural Information Processing Systems, 2024. 2, 3 [24] Berk Kaya, Suryansh Kumar, Carlos Oliveira, Vittorio Ferrari, and Luc Van Gool. Multi-view photometric stereo revisited. In 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), page 31253134. IEEE, 2023. 3 [25] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, 5 [26] Lubor Ladicky, Bernhard Zeisl, and Marc Pollefeys. Discriminatively trained dense surface normal estimation. In ECCV, pages 468484. Springer International Publishing, 2014. 2 [27] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila. Modular primitives for high-performance differentiable rendering. ACM Transactions on Graphics, 39(6), 2020. [28] Marc Levoy, Kari Pulli, Brian Curless, James Davis, Szymon Rusinkiewicz, David Koller, Lucas Pereira, Matt Ginzton, Sean Anderson, Jeremy Ginsberg, Jonathan Shade, and Duane Fulk. The digital michelangelo In Proceedings of project: 3d scanning of large statues. the 27th Annual Conference on Computer Graphics and Interactive Techniques, page 131144, USA, 2000. ACM Press/Addison-Wesley Publishing Co. 2, 3 [29] Bo Li, Chunhua Shen, Yuchao Dai, Anton van den Hengel, and Mingyi He. Depth and surface normal estimation from monocular images using regression on deep features In 2015 IEEE Conference on Comand hierarchical crfs. puter Vision and Pattern Recognition (CVPR), pages 1119 1127, 2015. 2 [30] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. In The Twelfth International Conference on Learning Representations, 2024. 2, 13 [31] Daniel Lichy, Jiaye Wu, Soumyadip Sengupta, and David W. In 2021 Jacobs. Shape and material capture at home. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 61196129. IEEE, 2021. 3 [32] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 1007210083. IEEE, 2024. [33] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: In 2023 IEEE/CVF InZero-shot one image to 3d object. ternational Conference on Computer Vision (ICCV), page 92649275. IEEE, 2023. 2 [34] Yunfei Liu, Yu Li, Shaodi You, and Feng Lu. Unsupervised learning for intrinsic image decomposition from single image. In CVPR, 2020. 2, 3 [35] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. In The Twelfth International Conference on Learning Representations, 2024. 2 [36] Linjie Lyu, Ayush Tewari, Marc Habermann, Shunsuke Saito, Michael Zollhofer, Thomas Leimkuehler, and Christian Theobalt. Diffusion posterior illumination for ambiguity-aware inverse rendering. ACM Transactions on Graphics, 42(6), 2023. 3 10 [37] Xiaojuan Qi, Zhengzhe Liu, Renjie Liao, Philip H. S. Torr, Raquel Urtasun, and Jiaya Jia. Geonet++: Iterative geometric neural network with edge-aware refinement for joint depth and surface normal estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(2):969984, 2022. [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 2 [39] Salesforce. Blip-2, opt-2.7b, pre-trained only. https: //huggingface.co/Salesforce/blip2-opt-2. 7b, 2023. 5 [40] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. 4 [41] Shen Sang and M. Chandraker. Single-shot neural relighting and svbrdf estimation. In ECCV, 2020. 3 [42] Jian Shi, Yue Dong, Hao Su, and Stella X. Yu. Learning non-lambertian object intrinsics across shapenet categories. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 58445853, 2017. [43] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model, 2023. 2, 3 [44] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. MVDream: Multi-view diffusion for 3d generation. In The Twelfth International Conference on Learning Representations, 2024. 2 [45] Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, and Jonathan T. Barron. Nerv: Neural reflectance and visibility fields for relightIn 2021 IEEE/CVF Conference ing and view synthesis. on Computer Vision and Pattern Recognition (CVPR), page 74917500. IEEE, 2021. 3 [46] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In Computer Vision ECCV 2024: 18th European Conference, Milan, Italy, September 29October 4, 2024, Proceedings, Part IV, page 118, Berlin, Heidelberg, 2024. Springer-Verlag. 2 [47] Giuseppe Vecchio and Valentin Deschaintre. Matsynth: In Proceedings of the modern pbr materials dataset. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 3 [48] Rui Wang, David Geraghty, Kevin Matzen, Richard Szeliski, and Jan-Michael Frahm. Vplnet: Deep single view norIn 2020 mal estimation with vanishing points and lines. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 686695, 2020. 2 [49] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong Chen, Qifeng Chen, and Fang Wen. Pretraining is all you need for image-to-image translation. In arXiv, 2022. 2 [50] Zhenwei Wang, Tengfei Wang, Zexin He, Gerhard Hancke, Ziwei Liu, and Rynson WH Lau. Phidias: generative model for creating 3d content from text, image, and 3d conditions with reference-augmented diffusion. arXiv preprint arXiv:2409.11406, 2024. 2 [51] Robert J. Woodham. Photometric method for determining surface orientation from multiple images, page 513531. MIT Press, Cambridge, MA, USA, 1989. 2, 3 [52] Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, and Chunhua Shen. What matters when repurposing diffusion models for general dense perception tasks?, 2024. 2 [53] Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang Xiu, and Xiaoguang Han. Stablenormal: Reducing diffusion variance for stable and sharp normal. ACM Transactions on Graphics (TOG), 2024. 2,"
        },
        {
            "title": "In Proceedings of",
            "content": "[54] Renjiao Yi, Chenyang Zhu, and Kai Xu. Weakly-supervised the single-view image relighting. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 84028411, 2023. 2, 3, 5 [55] Ye Yu and William A. P. Smith. Inverserendernet: Learning single image inverse rendering. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 31503159. IEEE, 2019. 3 [56] Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong, Hongzhi Wu, and Xin Tong. Dilightnet: Fine-grained lightIn ACM ing control for diffusion-based image generation. SIGGRAPH 2024 Conference Papers, 2024. 2, 3, 5 [57] Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, Ling-Qi Yan, and Miloˇs Haˇsan. Rgbx: Image decomposition and synthesis using materialand lighting-aware diffusion models. In ACM SIGGRAPH 2024 Conference Papers, New York, NY, USA, 2024. Association for Computing Machinery. 2, 3, 5 [58] Jingyang Zhang, Yao Yao, Shiwei Li, Jingbo Liu, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. Neilf++: Inter-reflectable light fields for geometry and material esIn 2023 IEEE/CVF International Conference on timation. Computer Vision (ICCV). IEEE, 2023. 3 [59] Lyumin Zhang. https : / / github.com/Mikubill/sd-webui-controlnet/ discussions/1236, 2023. Reference-only control. [60] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Ic-light github page, 2024. 5 [61] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 5 [62] Zhenyu Zhang, Zhen Cui, Chunyan Xu, Yan Yan, Nicu Sebe, and Jian Yang. Pattern-affinitive propagation across depth, surface normal and semantic segmentation. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 41014110. IEEE, 2019."
        },
        {
            "title": "Appendix",
            "content": "A. Dataset Details In the main paper, we provided an overview of the LightProp dataset, designed specifically to address the challenges of learning robust multi-light image generation and geometry-material estimation. Here, we detail the data curation and rendering configurations. A.1. Data Curation Objaverse [11] originally contains around 800, 000 synthetic objects across various categories and styles. To ensure high-quality content for LightProp, we implemented rigorous curation process. First, we filtered out objects with extreme thinness or unbalanced proportions, such as objects with large surface areas but minimal thickness or depth, which often distort lighting interactions and hinder effective learning. Additionally, we excluded objects that originated from 3D scans or those representing entire scenes, as these typically contain irrelevant environmental details that are less suitable for our framework. Finally, objects lacking essential PBR material maps (albedo, roughness, and metallic maps) were removed to ensure comprehensive material data for training. This selection process resulted in refined subset of around 80, 000 high-quality objects for LightProp. A.2. Rendering Setup The LightProp dataset is created using the Cycles rendering engine in Blender [9], with each image generated at 128 samples per pixel and accelerated using CUDA. To introduce diversity in object orientation and perspective, each object is rendered from five distinct viewpoints: front view, right view, top view, and two random views sampled on surrounding sphere. For each viewpoint, we apply five distinct lighting conditions, comprising point light, an area light, and three HDR environment maps randomly selected from 25 high-quality maps. To set up our directional lighting, we position eight lights around the camera and place one additional light directly at the cameras position. The lighting orientations are parameterized by spherical coordinates θ and φ, specifically configured as: θi = π 4 φi = {1, 2, 1, 2, 1, 2, 1, 2, 0} for = 0, 1, . . . , 8, π 6 (6) (7) . This arrangement ensures diverse lighting directions to enhance shading and reflectance variations in multi-light images, which are essential for accurate geometry and material estimation. In addition to the multi-light images, each object view is paired with ground-truth G-buffer maps, including surface normals, albedo, roughness, and metallic maps. These G-buffers, rendered via Blenders physically-based 12 pipeline, provide the necessary supervision for training in surface normal and PBR material prediction. B. Implementation Details B.1. Multi-Light Diffusion We build our multi-light diffusion model on top of Stable Diffusion v2-1. As discussed in the main paper, we adopt two-phase training scheme to adapt this pre-trained model for multi-light image generation. In the initial phase, we tune the first convolution layer, all parameters in the selfattention layers, and only the key and value parameters in the cross-attention layers. This phase runs for 80, 000 steps with peak learning rate of 1 104 and total batch size of 128, following cosine annealing schedule with 2, 000 warm-up steps. We use the AdamW optimizer with β1 = 0.9, β2 = 0.999, and weight decay of 0.01, and enable bf16 mixed precision to accelerate the training. Additionally, we apply gradient clipping with maximum norm of 1.0 to stabilize training and incorporate classifierfree guidance, with probability of dropping the conditioning set to 0.1. In the following phase, we further fine-tune the full model for another 80, 000 steps at significantly lower peak learning rate of 5 106 with the same training particulars. Both of the two phases are trained with an input image resolution of 256 256, and multi-light output of 768 768. In total, the complete training process of our multi-light diffusion model takes approximately 2.5 days on 32 NVIDIA A100 (80G) GPUs. B.2. Large G-Buffer Prediction Model Architecture. Our large G-buffer prediction model takes as input single image with 4 channels (including alpha), combined with multi-light images comprising 9 lighting conditions, each with 3 channels, resulting in total of 4+93 = 31 input channels. The output consists of 8 channels, representing the surface normals, albedo, roughness, and metallic maps (3, 3, 1, and 1 channel, respectively). The regression U-Net architecture comprises four downsampling blocks with progressively increasing channels of 224, 448, 672, and 896, followed by bottleneck block with 896 channels, and then four up-sampling blocks with correspondingly decreasing channels of 896, 672, 448, and 224. Each block contains two residual layers with Group Normalization (using 32 groups), and SiLU activation. Attention mechanisms, implemented in pre-norm style , are applied in all but the first down-sampling block and the last up-sampling block, using an attention head dimension of 8. Within each block, up-sampling and down-sampling are performed via convolutional layer placed after the two residual layers. To encode the spherical coordinates {θi, φi} associated with each lighting condition, we employ https://huggingface.co/stabilityai/stable-diffusion-2-1 sinusoidal embeddings. Each scalar θ or φ is projected to higher dimension of dscalar = 224 and we concatenate these projected vectors into single 9 2 224 = 4032 dimensional vector, which is subsequently embedded by 2-layer MLP, producing an illumination embedding with final dimensionality of demb = 896. This embedding is modulated to each block in the U-Net with adaptive group normalization. For the smaller models in our ablation study, we use U-Net with down-sampling blocks at 128, 256, 384, and 512 channels, mirrored in the up-sampling blocks, along with 512-channel bottleneck block. Training Details. We apply weighted loss contributions to balance Lnormal and LPBR. Specifically, we set 4 : 1 ratio for surface normals relative to PBR materials. Additionally, we apply stabilization factor of λ1 = 0.25 for the MSE term in Lnormal, as outlined in the main paper. Given the computational demands of high-resolution feature maps, especially with attention layers, we employ two-phase training strategy, gradually transitioning from low to high resolutions. In the initial phase, we train at resolution of 256 256 to establish core feature representations, running for 60, 000 steps with batch size of 128. This phase includes 1, 500 warm-up steps, peak learning rate of 1 104, and weight decay of 0.01, using cosine annealing schedule and the AdamW optimizer with β1 = 0.9 and β2 = 0.999. Training on 32 NVIDIA A100 (80G) GPUs, this phase completes in approximately 20 hours. Following this foundational phase, we move to higher resolution of 512 512, allowing the model to capture finer details essential for precise geometry and material predictions. This fine-tuning phase involves reduced learning rate of 2 105 and runs for an additional 30, 000 steps on the same setup of 32 NVIDIA A100 (80G) GPUs, completing in approximately 7 days. All other training parameters are kept consistent with the initial phase. Augmentation Details. In the main paper, we introduced the augmentations to bridge the gap between our multi-light diffusion model and the large G-buffer prediction model. For Random Degradation, we down-sample each multilight image to lower resolution uniformly sampled from U(128, 256) and then up-sample it back to the original resolution of 256. Following this, we apply grid distortion with perturbation strength sampled from U(0.15, 0.3) to simulate geometrical misalignments. For Random Intensity, we convert the multi-light images to HSV format and adjust the brightness channel using an image-level scaling factor from U(0.9, 1.3). Additionally, we apply pixel-level noise by scaling each pixel independently with factor sampled from (1, 0.05). The input image receives separate brightness adjustment factor sampled from U(0.9, 1.1). For Random Orientation, all spherical coordinates are perturbed by an angular gaussian noise in radians. θi receive noise sampled from (0, 0.1) and are wrapped with modulus 2π. Figure 11. Failure case. φi are perturbed with noise from (0, 0.02) and clamped within [0, π 2 ]. The above three augmentations are triggered independently with probability of 0.6. For Data Mixing, this augmentation is applied with probability of 0.3. We generate multi-light images from our diffusion model with classifier-free guidance scale of 2.0 over 75 inference steps. Additionally, inspired by prior work on multi-view reconstruction [30], we shuffle the order of the multi-light images during training with probability of 0.5 to encourage robustness in learning features across varied lighting sequences, thereby reducing dependency on any specific lighting arrangement. C. Limitations While our approach demonstrates strong performance, several limitations remain. First, for input images with extreme highlights or shadow areas, our method struggles to fully remove illumination effects in the predicted albedo maps, as shown in Fig. 11. Additionally, the resolution of the backbone multi-light diffusion model (256 256) limits the level of detail achievable in the generated multi-light images, subsequently constraining the final normal and material predictions. Increasing the models resolution could enhance the quality of the predicted surface properties. Finally, our method is currently designed for objects rather than full scenes, limiting its applicability in complex, multiobject environments. D. Additional Results D.1. Our Results Figs. 12 and 13 present examples of our full pipeline output, including input images, generated multi-light images, estimated surface normals, PBR materials, and relit images under various environment maps. These results showcase the robustness of our approach in generating consistent geometry and material estimates and realistic relighting effects across different lighting conditions. Additionally, Figs. 14 and 15 showcase extended single-image relighting results of our method under an even broader range of environment maps, further highlighting the models ability to generate high-quality, adaptable relit images across diverse lighting setups. These results illustrate the robustness in managing various lighting conditions and further demonstrate the efficacy of our approach. 13 D.2. Comparison Results In Fig. 16, Fig. 17, Fig. 18, and Fig. 19 we offer more comparison results for surface normal estimation, PBR material estimation, and single-image relighting. These comparisons further demonstrate the advantages of our method over baseline approaches in accurately capturing surface details, material properties, and producing realistic relit images under diverse lighting conditions. Figure 12. More results of our method. 15 Figure 13. More results of our method. 16 Figure 14. More single-image relighting results of our method. Figure 15. More single-image relighting results of our method. 18 Figure 16. More comparisons on surface normal estimation. 19 Figure 17. More comparisons on PBR material estimation. Figure 18. More comparisons on PBR material estimation. 21 Figure 19. More comparisons on single-image relighting."
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "Shanghai AI Lab",
        "The Chinese University of Hong Kong"
    ]
}