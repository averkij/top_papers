{
    "paper_title": "Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration",
    "authors": [
        "Yuhang Han",
        "Xuyang Liu",
        "Pengxiang Ding",
        "Donglin Wang",
        "Honggang Chen",
        "Qingsen Yan",
        "Siteng Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "To accelerate the inference of heavy Multimodal Large Language Models (MLLMs), this study rethinks the current landscape of training-free token reduction research. We regret to find that the critical components of existing methods are tightly intertwined, with their interconnections and effects remaining unclear for comparison, transfer, and expansion. Therefore, we propose a unified ''filter-correlate-compress'' paradigm that decomposes the token reduction into three distinct stages within a pipeline, maintaining consistent design objectives and elements while allowing for unique implementations. We additionally demystify the popular works and subsume them into our paradigm to showcase its universality. Finally, we offer a suite of methods grounded in the paradigm, striking a balance between speed and accuracy throughout different phases of the inference. Experimental results across 10 benchmarks indicate that our methods can achieve up to an 82.4% reduction in FLOPs with a minimal impact on performance, simultaneously surpassing state-of-the-art training-free methods. Our project page is at https://ficoco-accelerate.github.io/."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 2 ] . [ 1 6 8 6 7 1 . 1 1 4 2 : r Rethinking Token Reduction in MLLMs: Towards Unified Paradigm for Training-Free Acceleration Yuhang Han1, Xuyang Liu2, Pengxiang Ding3, Donglin Wang3, Honggang Chen2, Qingsen Yan1, Siteng Huang4(cid:12) 1Northwestern Polytechnical University 2Sichuan University 3Westlake University 4DAMO Academy, Alibaba Group Figure 1. (Left) Schematic diagram of our unified filter-correlate-compress paradigm for training-free token reduction in MLLMs. (Right) Performance comparison on TextVQA benchmark [32]."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction To accelerate the inference of heavy Multimodal Large Language Models (MLLMs), this study rethinks the current landscape of training-free token reduction research. We regret to find that the critical components of existing methods are tightly intertwined, with their interconnections and effects remaining unclear for comparison, transfer, and expansion. Therefore, we propose unified filtercorrelate-compress paradigm that decomposes the token reduction into three distinct stages within pipeline, maintaining consistent design objectives and elements while allowing for unique implementations. We additionally demystify the popular works and subsume them into our paradigm to showcase its universality. Finally, we offer suite of methods grounded in the paradigm, striking balance between speed and accuracy throughout different phases of the inference. Experimental results across 10 benchmarks indicate that our methods can achieve up to an 82.4% reduction in FLOPs with minimal impact on performance, simultaneously surpassing state-of-the-art trainingfree methods. Our project page is at https://ficocoaccelerate.github.io/. Equal contribution. (cid:12) Corresponding author. Email: siteng.huang@gmail.com 1 Multimodal Large Language Models (MLLMs) [2, 7, 22 24, 43], which extract visual features and integrate them with textual inputs to form mixed-modality instructions, have successfully harnessed the advanced emergent capabilities of pre-trained Large Language Model (LLM) [1, 28, 34] decoders. However, the quadratic complexity that scales with sequence length poses challenge as the increasing length of multimodal contexts results in prohibitive computational and memory demands, limiting the practical deployment of MLLMs. As result, improving their inference efficiency is priority for both academia and industry. Natural vision signals, such as images and videos, inherently possess higher degree of information redundancy compared to human-generated languages [10, 13]. However, in modality-mixed instructions, the number of visual tokens typically exceeds that of textual tokens by significant margin. Consequently, recent efforts [4, 6, 18, 31, 39, 44] have aimed to accelerate the inference of MLLMs by reducing the quantity of visual tokens while maintaining In this work, we first investithe necessary information. gate the current state of training-free token reduction methods [3, 5, 21, 31, 42], as these plug-and-play techniques avoid the additional computational and resource burden introduced by re-training. We provide discussion with examples in Sec. 2.2, where we determine that the core components of these existing methods are tightly intertwined, and the connections between them are still unclear. Furthermore, the lack of design flexibility may result in suboptimal performance and hinder the expansion to new approaches. In this study, we introduce novel filter-correlatecompress paradigm, offering unified viewpoint to handle the common issues. As illustrated in Fig. 1 (left), the interpretable paradigm distinctly decomposes various methods into three key stages within pipeline, maintaining consistent design objectives and abstract elements in each stage while providing sufficient space for unique implementations. Then, we subsume the recent popular works into our paradigm and explain their mechanisms with clearer formulas. Additionally, we provide empirical evidence to show that popular token reduction approaches have their equivalent counterparts under the unified paradigm. Thus, the unified paradigm exhibits decomposability, understandability, and flexibility, while facilitating the transfer of design choices for the development of new methods. On top of the paradigm, we further present FiCoCo, trio of complementary variants designed to reduce tokens at different phases of MLLM inference, and variant is meticulously crafted to implement targeted strategies. During the forward inference of the MLLM, FiCoCo fully leverages the intermediate products to perform token reduction, thus achieving compromising theoretical reduction in FLOPs. To evaluate their effectiveness and efficiency, we conduct extensive experiments across 10 multimodal benchmarks. Empirical results demonstrate that all three variants of our FiCoCo significantly outperform most training-free token reduction methods across nearly all benchmarks and even surpass some training-based methods on certain benchmarks using LLaVA-1.5-7B/13B. In particular, our FiCoCo series achieves comparable performance with only 17.6% of the computational cost and requires approximately 67.6% of the GPU memory compared to LLaVA-1.5-7B in practical applications. As illustrated in Fig. 1 (right), the results highlight that all FiCoCo variants significantly outperform popular methods with the same FLOPs, especially when the FLOPs are lower, indicating that our FiCoCo achieves an optimal balance between efficiency and accuracy in MLLMs. 2. Unified Paradigm of Token Reduction In this section, we delve into the exploration on the possibility of unifying training-free token reduction in MLLMs. We first revisit the core of MLLMs to set the stage for subsequent discussions (Sec. 2.1). Then, by analyzing popular methods, we rethink the current state of token reduction and identify the issues within this research field (Sec. 2.2). Finally, we present unified filter-correlatecompress paradigm and show how the paradigm encompasses the methods with both theoretical and empirical evidence (Sec. 2.3). An overview is illustrated in Fig. 1 (left). 2.1. Preliminaries: Revisiting MLLMs Inference. Given the input image and the textual instructions, the inference of MLLM generates responses that interpret the image content based on the provided instruction. To fully leverage the capabilities of the pre-trained LLM decoder, common practice is to devide the forward pass of the MLLM into two phases. In the multimodal instruction encoding phase, visual encoder first converts the input image into sequence of visual tokens Xv. Then, an additional visual projector maps visual tokens to the input space of the LLM decoder, forming multimodal instruction by combining with the embeddings of textual instrucIn the second response decoding phase, the LLM tions. decoder generates the instruction-following response in an autoregressive manner, which can be formulated as (cid:0)Y Xv, Xt(cid:1) = (cid:89) i=1 (cid:0)yi Xv, Xt, Y1:i1 (cid:1) , (1) i=1 denotes the generated response tokens, where = {yi}N Xv and Xt respectively denote visual and textual tokens. Self-Attention. The self-attention mechanism [35] is the most essential modeling operation in transformer-based visual encoder and LLM decoder. Given the input 1D sequence of length , the self-attention layer produces self-attention map RN to globally model the dependence relationships between tokens, formulated as = Attention (Q, K) = Softmax (cid:16) QK/ (cid:17) , (2) where denotes the transpose of the matrix, the query and key matrices Q, RN are obtained by projecting with learnable parameter matrices. 2.2. Rethinking Token Reduction When investigating the current state of research on trainingfree token reduction, we select three popular methods as representatives to gain insight while ensuring the generality and diversity. Note that the following introduction closely adheres to the phrasing of the original paper for fidelity. ToMe [3] performs token merging between the attention layer and the feed-forward layer within each block of the Vision Transformer (ViT). Specifically, the visual tokens are randomly divided into two sets and of roughly equal size. Each token in is connected to its most similar token in B, where the similarity is defined as the cosine similarity between the keys of each token. Then, only the most similar edges are retained, and tokens that remain connected are merged through feature averaging. Finally, the two sets are concatenated back together. EViT [21] also merges the tokens in the ViT. Given the visual tokens, EViT computes the average attention value between each token and the [CLS] token of all attention 2 heads. Then, the tokens with the largest attention value are preserved, and the other tokens are merged into new token with weighted average operation. FastV [5] is token pruning method happening in the It simply computes the average attention LLM decoder. value one token received from all other tokens, and prunes out the last tokens after ranking. From the investigation of the representative methods, we can observe the following common issues: (1) The majority of methods rely on textual descriptions to illustrate their processes, with notable absence of formulas that would clarify the operations at each step. (2) The overall design of these methods is driven by intuition rather than unifying guiding principle, resulting in excessive coupling. Therefore, we are limited to evaluating the performance of algorithms in their entirety and struggle to isolate the effect of their specific design elements. (3) Similarly, it is challenging to make targeted modifications and adaptations, or to alter the design in response to the MLLM phases at which token reduction occurs. (4) Most importantly, the difficulty in deconstructing existing methods hinders inspiration for the development of subsequent methods. 2.3. One Paradigm Unifies Current Methods To tackle the aforementioned issues, we propose unified filter-correlate-compress paradigm for training-free token reduction, which offers several distinct benefits: (1) Decomposability: The paradigm unfolds the entangled token reduction into structured pipeline with three key stages, each with standardized input and output interfaces. (2) Understandability: Each stage within the paradigm is characterized by well-defined design objective and clearly specifies the intermediate elements to be implemented. (3) Flexibility: The implementation of the intermediate elements is not restricted, allowing the paradigm to accommodate existing methods and facilitate further expansion. We now proceed to detailed introduction to each stage and show how they integrate existing methods seamlessly."
        },
        {
            "title": "2.3.1 Stage One: Filter",
            "content": "As detailed in Sec. 2.2, existing methods display ambiguity regarding early token selection, particularly concerning whether to select tokens for retention or deletion. To achieve clarity, the filter stage within our paradigm addresses the question, What token should be discarded? Given input visual tokens, this stage first defines scoring vector RP that quantifies the redundancy of tokens, where = or < . In the latter case, token reduction occurs only on pre-determined subset of input tokens, directly preserving specific tokens. Then, the scores can be ranked, and tokens with higher scores are expected to be discarded. Therefore, source set that contains the indices of discarded tokens can be identified, typically through topK operation on the scores. In this way, the stage ensures unified filtering operation while leaving space for flexibly designing the range and calculation of the redundancy scores in each method. And only the source set proceeds to the next stage with the visual tokens. ToMe [3] treats the set as the pre-determined subset (i.e., = N/2) and calculates the redundancy scores as si = max 1jN/2 KA KB KB KA . (3) EViT [21] treats all patch tokens as the pre-determined subset and calculates the redundancy scores as si = aCLS = exp (qCLSK / i=1 exp (qCLSK / D) (cid:80)N , D) (4) where qCLS is the query projection of the [CLS] token. FastV [5] treats all patch tokens as the pre-determined subset and calculates the redundancy scores as si = (cid:88) j=1 Ai,j. (5)"
        },
        {
            "title": "2.3.2 Stage Two: Correlate",
            "content": "The correlate stage starts to unify token merging and token reduction methods from the view of information. While token reduction techniques directly discards the information in redundant tokens, token merging techniques advocate that the information should be appropriately retained. Therefore, our second stage addresses the query, Where should discarded information be preserved? Specifically, target set T, comprising the indices of candidate tokens, should be defined initially. Then, correlation matrix RN SN is computed to evaluate the relationships between each discarded token in and all tokens in T. This matrix facilitates the tracking of the information propagation from each discarded token to the candidate tokens. In summary, the stage allows the customization of the target set and the calculation of correlation matrix C, and feeds and with into the next stage. ToMe [3] sets = and computes the matrix as Ci,j = KA KB KA KB . (6) EViT [21] uniquely identifies an extra vector filled with 0 as the only element of the target set, i.e., = {0} and = 1, while calculating the correlation matrix as Ci,j = aCLS . (7) FastV [5] directly prunes the discarded tokens. Therefore, 3 we can denote = and Ci,j = 0."
        },
        {
            "title": "2.3.3 Stage Three: Compress",
            "content": "Following the correlate stage, the final compress stage aims to handle the question, How to fuse the tokens to preserve information? Given the tokens in the target set XT, the tokens in the source set XS, and the correlation matrix C, we can update XT with function (), formulated as XT (XT, XS, C). (8) While the updating function can be customized, common consideration is that information from each discarded token may not be relevant for propagation to all target tokens, as it may introduce noise for some of them. Therefore, methods can apply topK operation on each row of to limit the selection of correlated tokens from that each token in should be merged into, where > 0 for merging methods and = 0 for pruning methods. Conversely, the j-th token in obtains an index set Ij, which specifies the features from discarded tokens that will be utilized to update itself. ToMe [3] implements the function () as"
        },
        {
            "title": "XS\ni",
            "content": "XT + (cid:80) iIj 1 + Ij XT , where (9) Ij ={i and Ci,j = max kT Ci,k}. It can be seen as each discarded token finding correlated token through topK operation with = 1. EViT [21] implements the function () as XT Ci,jXS . + XT (10) (cid:88) iS FastV [5] can represent the function () as XT while in practice XT does not require an update. XT , Note that for clarity, our formula calculations are designed to target individual elements within vectors or matrices. However, these operations can be tensorized in the practical implementation to facilitate batched inference."
        },
        {
            "title": "2.3.4 Empirical Equivalency of Paradigm",
            "content": "After deconstructing the popular methods according to the proposed paradigm, Tab. 1 provides empirical evidence to illustrate the equivalence between the original methods and our deconstructed versions. We conduct the comparison on TextVQA [32] and SQA [27] datasets with FLOPs=3.3T, leveraging LLaVA-1.5-7B [24]. Across all scenarios, we observe that the performance discrepancy between the original and our deconstructed implementations is within reasonable range (0.03). This indicates that our paradigm can encompass existing token reduction methods effortlessly. Method Original Deconstructed ToMe [3] EViT [21] FastV [5] ToMe [3] EViT [21] FastV [5] 65.43 65.21 66.98 52.14 51.72 52. SQA 65.42 65.18 66.99 TextVQA 52.14 51.74 52.82 0.01 0.03 -0.01 0.00 -0.02 0.01 Table 1. Performance discrepancy of original and deconstructed methods on SQA and TextVQA benchmarks. 3. Methodology: FiCoCo In this section, we present series of methods based on the proposed paradigm, which includes FiCoCo-V (reducing tokens in the visual encoder), FiCoCo-L (reducing tokens in the LLM decoder), and FiCoCo-VL (reducing tokens in both phases). We provide detailed introduction to the methodological design of each stage within the paradigm. An overview is illustrated in Fig. 2. 3.1. FiCoCo-V Filter stage. We calculate redundancy scores for all input visual tokens by assessing redundancy from both local and task perspectives. Regarding local redundancy, tokens that draw significant information from others at the attention layer are more likely to be replaceable in later processing stages. Thus, the attention weights Av 1 in the visual encoder can, to some degree, measure token redundancy. For task redundancy, patch tokens must convey sufficient global semantic information for multimodal understanding. Early reduction of tokens with dense semantic content may result in significant performance decline. As the [CLS] token represents the global image representation, its attention weights aCLS can quantify the semantic content of patch tokens. Therefore, we compute the redundancy scores as sv = λ"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) j=1 Av i,j (1 λ)aCLS , (11) where λ is scalar hyperparameter that balances the factors. The same applies to β and γ in the following paragraphs. concern is that tokens discarded in one layer might concentrate in certain area of the image, potentially resulting in spatial-centralized information loss. Therefore, we develop local penalty strategy to guarantee that the discarded tokens are uniformly distributed across the spatial domain. Specifically, we can represent the scoring vector sv back to 2D grid and partition it into non-overlapped windows with an equal size of . For the blanks belonging 1In our FiCoCo series introduction, comprises elements from computations with patch tokens as queries and keys, excluding [CLS] token. 4 Figure 2. An overview of the proposed FiCoCo method series. During different phases of MLLM inference, FiCoCo-V and FiCoCo-L provide distinct solutions across three stages. to previously discarded tokens, we apply padding to maintain the 2D information. Finally, we apply scaling coefficient to the maximum score within each window, enhancing positive scores and diminishing negative ones. This effectively suppresses the global prominence of other large scores within the windows. Empirically, we have observed that any coefficient not less than 2 yields similar results. Correlate stage. After ranking the redundancy scores sv, we can obtain the source set that is expected to be discarded, and consider all the preserved visual tokens as the target set T. In the visual encoder, attention weights inherently represent the flow of information during feature updates. Therefore, we conduct the correlation matrix as Cv i,j = Av i,j. (12) Compress stage. Given the correlation matrix Cv, we employ topK operation to find correlated tokens for each discarded token. However, different from ToMe that applies fixed value as 1, we apply token-adaptive K. Specifically, we compute the ε-th quantiles of each row in the correlation matrix to determine token-wise threshold for each discarded token. This threshold τi is re-applied to the matrix to identify the target tokens correlated to the ith discarded token. This approach enables multiple target tokens to receive information from the same discarded token when required. Finally, we update the correlated tokens with weighted compression, formulated as 5 XT αij = XT + (cid:80) iIj 1 + (cid:80) iIj Ci,j (cid:80) jJi Ci,j αijXS αij , where Ij = {i and Ci,j τi}, , where Ji = {j and Ci,j τi}. (13) The weight αij represents the proportion of information from the i-th discarded token that is allocated to the j-th correlated token. 3.2. FiCoCo-L Filter stage. In the LLM decoder, we borrow the local redundancy from FiCoCo-V. However, more straightforward approach exists for measuring the task redundancy of visual tokens. As textual tokens directly encode task instructions, the attention weights visual tokens received from textual tokens indicate their task relevance. Given textual tokens, we compute the redundancy scores as sl = β"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) j=1 Al i,j (1 β) +M (cid:88) k=N + Al i,k. (14) Correlate stage. We maintain the way to split the source set and the target set T, and continue to regard attention weights as measure of direct correlation. However, we explore an additional form of indirect semantic correlation, which leverages textual tokens as bridge. Specifically, when measuring the association between the i-th token and the j-th token, we sum the products of the attention weights from the i-th token to all textual tokens and from all textual tokens to the j-th token. If the peak attention weights of the i-th token and the j-th token are concentrated on the same textual tokens, then the computed correlation between them is higher. In summary, we have Cl i,j = γAl i,j + (1 γ) +M (cid:88) k=N +1 Al i,k Al k,j. (15) Compress stage. Due to the universality of the paradigm and the minimal coupling between stages, FiCoCo-L can effortlessly continue the compression process from FiCoCoV, as illustrated in Eq. (13). We provide theoretical estimation of the computing cost in the supplementary materials. While maintaining consistent FLOPs, the following points in FiCoCo series deserve highlighting: FiCoCo-VL. Naturally, we can integrate the designs of FiCoCo-V and FiCoCo-L to perform token reduction during both phases of MLLM inference. We refer to this approach as FiCoCo-VL. Starting Layer. Attention Sink behavior [36], which indicates the fact that attention can be divergent in the very early layers, has been observed in both ViTs [9] and LLMs [5]. Since the effectiveness of our FiCoCo is based on the reliability of attention mechanisms, we delay the token reduction until the attention converges to stability. 4. Experiments 4.1. Comparisons with State-of-the-art Methods Benchmarks. To validate the effectiveness of FiCoCo, we conduct evaluations on 10 widely adopted multimodal benchmarks: ScienceQA (SQA) [27], TextVQA (VQAT) [32], POPE [19], VizWiz [12], MM-Vet [40], MMBench-CN (MMBCN) [26], GQA [15], LLaVAW [23], MMBench (MMB) [26] and VQAv2 [11]. All experiments follow the default settings and evaluation metrics of these benchmarks. Comparison Details. For the multimodal evaluation on images, we validate FiCoCo using the LLaVA-1.57B/13B [24]. During inference, we strictly adhere to the default settings of LLaVA-1.5 for consistency in experimental conditions. Additionally, for comprehensive and fair comparison with other state-of-the-art results, we follow the FLOPs settings used in related works [5, 6, 31, 44]. For studies where FLOPs are not explicitly recorded, we use [41] to theoretically estimate the FLOPs based on the number of tokens in these models. Ultimately, we obtain four key FLOPs points (1.5T, 2.4T, 3.3T, 4.2T), which perfectly cover the corresponding FLOPs range of existing state-of-the-art methods. All experiments are conducted on single A800 80GB GPU."
        },
        {
            "title": "Compress",
            "content": "Method SQA TextVQA 68.37 FiCoCo-V w/o local redundancy 67.81 64.67 w/o task redundancy 68.12 w/o local penalty 67.82 fixed K=0 67.43 fixed K=1 fixed K=2 67.21 average compression 67.92 55.46 52.51 48.74 53.24 53.56 46.97 51.36 53.34 Table 2. Ablation results of FiCoCo-V. Main Results. Tab. 3 presents the performance of FiCoCo across 10 benchmarks based on LLaVA-1.5-7B, where several highlights can be observed: (1) FiCoCo-V, FiCoCoL, and FiCoCo-VL generally outperform existing training- (2) FiCoCo-L demonstrates superior perfree methods. formance over both FiCoCo-V and FiCoCo-VL. This indicates that supplying comprehensive visual information to LLMs and reducing visual tokens within LLMs can more effectively maintain task performance. (3) FiCoCo series even achieves comparable accuracy to the latest trainingbased methods on certain benchmarks. For instance, when FLOPs=1.5T, FiCoCo-L improves the accuracy by 1.7% over IVTP [14] on the SQA dataset, while FiCoCo-V shows 4.5% accuracy gain relative to IVTP on the VizWiz benchmark. We also report LLaVA-1.5-13B results in the supplementary materials to show superiority. 4.2. Ablation Study To further validate the effectiveness of the design at each stage, we conduct extensive ablation studies on the SQA and TextVQA benchmarks with FLOPs=1.5T. In Tab. 2, we ablate both filter and compress stages for FiCoCo-V: Filter. Both local and task redundancy improve the identification of discarded tokens. Notably, task redundancy has more significant impact on the final performance. This indicates that token reduction within the visual encoder should prioritize the retention of tokens rich in global semantic information. Additionally, we observe that by promoting spatially uniform distribution of discarded tokens, the local penalty strategy aids in preserving visual information. Compress. We evaluate the impact of fixing different values, including K=0 (pruning), K=1 (merging into single token), and K=2 (merging into multiple tokens). Although our findings indicate that the token-adaptive Kvalue strategy outperforms these fixed alternatives, counterintuitive observation is that setting to 0 yields superior results compared to the other two settings. We believe this occurs because fixing small value reduces the information sources available for updating correlated tokens, which potentially lead to the over-dilution of the information contained within correlated tokens by small number of discarded tokens, and even introduce excessive noise. Consequently, their performance is inferior to direct pruning. We Method LLaVA-1.5 [24] TFLOPs=4.2 FitPrune [38] FiCoCo-V FiCoCo-L FiCoCo-VL TFLOPs=3.3 SparseVLM [44] FastV [5] ToMe [3] FiCoCo-V FiCoCo-L FiCoCo-VL TFLOPs=2.4 TRIM [33] SparseVLM [44] FastV [5] ToMe [3] FiCoCo-V FiCoCo-L FiCoCo-VL TFLOPs=1.5 Honeybee [4] LLaMA-VID [20] Qwen-VL [2] IVTP [14] PyramidDrop [37] SparseVLM [44] Random Sampling [14] TopK [14] Spatial Pooling [14] EViT [21] FastV [5] ToMe [3] LLaVA-PruMerge [31] Recoverable Compression [6] FiCoCo-V FiCoCo-L FiCoCo-VL Training-free TFLOPs SQA VQAT POPE Vizwiz MM-Vet MMBCN GQA LLAVA-W MMB VQAv2 8.5 4.4 4.2 4.2 4.2 3.3 3.3 3.3 3.3 3.3 3.3 2.4 2.5 2.5 2.5 2.4 2.4 2.4 1.6 1.6 1.6 1.6 1.8 1.5 1.6 1.6 1.6 1.6 1.6 1.6 1.5 1.5 1.5 1.5 1. 69.5 58.2 86.4 50.0 31.6 59. 62.5 63.7 66.1 79.1 67.8 67.9 69.2 68.1 69.1 67.3 65.2 67.8 69.6 68. 69.1 67.1 60.2 59.6 68.3 69.4 68.2 67.8 67.9 68.1 67.8 - 62.2 67.2 66.9 67.7 67.7 51.1 50.0 67.9 69.0 68.4 69.5 68.1 58.2 55.9 57.4 55.7 56.1 52.5 52.1 55.7 56.6 55.1 53.7 54.9 50.6 49.1 55.6 56.3 54.9 50.9 51.4 54.4 58.2 - 51.8 48.5 52.4 52.5 54.7 47.8 45.3 53.3 55.3 55.5 55.7 54. 86.5 84.3 84.7 84.7 83.6 64.8 72.4 82.5 84.6 84.7 85.3 80.5 59.6 62.8 82.2 84.4 79.5 84.0 83.1 83.4 85.7 86.0 75.1 82.5 83.8 82.3 82.8 48.0 52.5 76.3 72.0 79.8 84.1 79.3 50.4 51.1 49.1 50.2 - - - 51.5 48.7 50. 48.1 - - - 49.4 48.4 48.9 47.2 46.8 47.3 47.9 - - 37.9 47.0 46.5 47.0 - - - - 52.4 48.2 49.7 32.8 30.2 30.3 29.7 - - - 29.7 31.4 28.4 28.0 - - - 28.2 30.1 28.1 27.1 29.7 27.2 30.5 - - 23.6 26.5 28.3 27.3 - - - - 26.8 27.4 29. 58.4 55.9 53.9 56.5 - - - 55.3 53.6 56.2 54.9 - - - 54.3 53.5 55.5 55.2 55.4 55.0 57.4 58.5 - 48.0 55.2 53.3 55.7 - - - - 53.0 53.3 54.4 61.5 58.6 61.2 58.7 57.6 52.7 54.3 58.5 61.1 58. 61.4 56.0 49.6 52.4 57.6 60.6 57.7 59.0 59.2 58.9 60.4 - 52.4 57.1 58.1 59.6 59.4 46.1 48.6 - - 57.4 60.0 57.4 - 58.8 61.9 58.4 - - - 60.4 60.3 55.7 58.7 - - - 56.6 59.4 57.6 59.4 58.9 59.2 62.8 - - 55.8 59.2 59.7 60.0 - - - - 58.6 57.3 56. 64.6 62.7 65.0 62.5 62.5 61.2 60.5 62.3 64.6 63.7 67.4 60.0 56.1 53.3 61.1 64.4 61.9 57.8 57.0 57.4 66.1 66.1 56.2 55.4 55.2 56.6 57.8 48.0 43.7 56.8 57.9 60.2 64.0 60.2 78.3 76.6 77.4 76.8 75.6 67.1 68.0 74.4 76.8 74. 76.4 73.8 61.8 63.0 73.1 76.4 73.9 74.8 74.3 74.9 77.8 - 68.2 69.0 72.4 73.9 74.1 61.8 57.1 65.9 70.4 74.8 75.6 75.3 Table 3. Comparison results on MLLMs with 7B LLM. For baselines, we reference results reported in other papers, which may exhibit slight discrepancies from the experimental results presented earlier. Our methods are primarily compared with training-free approaches."
        },
        {
            "title": "Compress",
            "content": "SQA TextVQA Method 69.46 FiCoCo-L 69.16 w/o local redundancy 68.22 w/o task redundancy w/ local penalty 68.79 w/o indirect correlation 68.89 68.45 w/o direct correlation 68.96 fixed K=0 68.57 fixed K=1 68.32 fixed K=2 68.32 average compression 55.72 55.43 55.64 55.38 54.78 55.45 50.33 50.11 50.18 54.66 Table 4. Ablation results of FiCoCo-L. also note that our weighted compression outperforms directly averaging the features, indicating that the calculated weights can effectively regulate the contribution of information sources in the updates of correlated tokens. In Tab. 4, we ablate all three stages for FiCoCo-L: Filter. Although both local redundancy and task redundancy continue to contribute to an accurate assessment of redundancy, we find that neither dominates. This could be attributed to the fact that the attention mechanism within LLMs can detect more stable token dependencies, thereby diminishing the necessity for redundancy measurement to rely heavily on semantic factors. Additionally, we find that persisting with the local penalty strategy in FiCoCo-L results in slight decrease in performance. We attribute the result to the enforcement of spatial uniformity in token retention within LLMs when visual features are fully present, which disrupts the redundancy assessments previously established by attention mechanisms. Correlate. Compared to FiCoCo-V, FiCoCo-L incorporates both the direct correlations of visual tokens and the indirect correlations that leverage textual tokens as bridge. It is observed that both two correlations contribute to accurately identifying correlated tokens, thereby leading to improved performance across both datasets. 7 5. Related Work Multimodal large language models (MLLMs). To acquire visual comprehension and reasoning capabilities, MLLMs [2, 7, 17, 23] first use pre-trained vision encoder (e.g., from CLIP [29]) to extract visual features, which are then directly projected into the input embedding space of the LLM decoder via visual projector. The LLM then processes these visual embeddings alongside user instructions to understand the images and craft suitable responses. For example, BLIP-2 [17] effectively employs frozen FlanT5 model for multimodal understanding by training Q-Former as the visual projector to bridge the modality gap. InstructBLIP [8] incorporates academic-task-oriented VQA datasets to further enhance the zero-shot generalization ability of the original BLIP-2. LLaVA [23] introduces high-quality visual instruction tuning dataset to fine-tune simple linear projector and LLM in two-stage process, facilitating alignment between vision and language spaces. LLaVA-1.5 [24] further improves the vision encoder to handle higher resolutions and replaces the linear projector with multi-layer perceptron (MLP). As the trend moves towards larger model sizes and longer context lengths, the inference speed and memory of MLLMs become the bottlenecks in their application. Token reduction for acceleration. Token reduction approaches can be broadly categorized into two dominant techniques: token pruning and token merging. Token pruning directly eliminates less important tokens, with token importance assessed either by trainable modules [30] or by the significance of attention [25]. Conversely, token merging [3, 21] attempts to compress tokens into smaller set of more compact units, predicated on the assumption that such strategy minimizes information loss. However, previous studies have predominantly concentrated on ViTs. To accelerate the inference of MLLM, recent trainingbased methods [4, 14, 18] involve training of learnable components either individually or with the base model, which incurs unaffordable computation and time costs. In contrast, training-free methods [5, 31, 44] can be directly applied to off-the-shelf MLLMs without the need for retraining, offering more practical efficiency. For instance, LLaVAPruMerge [31] dynamically selects and retains the most crucial visual tokens by utilizing the sparse distribution of attention scores within the visual encoder. FastV [5] prunes unnecessary visual tokens based on the ranking of attention scores derived from the self-attention mechanism in the LLM. SparseVLM [44] adaptively prunes visual tokens in the LLM based on their attention scores with text tokens. 6. Conclusion In this paper, we rethink the current landscape of trainingfree token reduction research and propose clear and flexiFigure 3. Visualizations of token reduction by (a) FiCoCo-V and (b) FiCoCo-L. The red box indicates the traced patch token, while the green box shows where the traced token is merged. Compress. Similar to FiCoCo-V, employing tokenadaptive to identify correlated tokens and updating these tokens with weighted average of information from discarded tokens constitute the optimal strategy. 4.3. Qualitative Analysis We visualize the discarded tokens of FiCoCo-V (see Fig. 3 (a)) and FiCoCo-L (see Fig. 3 (b)) across multiple compression levels in different VQA scenarios. We highlight the tokens in the images that are highly relevant to the answer based on the question (i.e., the patch tokens with the red bounding boxes), allowing us to track how these key tokens change within FiCoCo-L and FiCoCo-V. visual token associated with 2 is traced in Fig. 3 (a), while token associated with GAMES is tracked in Fig. 3 (b). In both instances, we note consistent trend: at FLOPs=4.2T, the number of discarded tokens is relatively small, and these tracked tokens are preserved to provide critical information during decoding. However, when FLOPs=1.5T, considerable number of tokens must be discarded, including those we are tracking. We further trace their information propagation during the token reduction, indicated by red arrows. And the green boxes frames their correlated tokens, where varying levels of transparency denote the proportion of the original tokens information retained in these correlated tokens. We discover that these correlated tokens, which have received crucial information, are also important for answering questions and are ultimately preserved in token reduction. Moreover, the discarded information can be received by multiple correlated tokens to enhance the understanding of the essential region (see Fig. 3 (b)). This qualitatively proves the effectiveness of our methodological design. 8 ble paradigm to unify prevailing methodologies. By deconstructing existing methods into standardized stages within the paradigm, we facilitate the comparison and potential transfer of distinctive design elements across methods. Building upon the paradigm, we further develop suite of methods, collectively referred to as FiCoCo, which incorporates three invariants designed to accelerate the inference of MLLMs. And extensive experimental results show that all three approaches significantly reduce the FLOPs while effectively preserving the performance. We hope our discoveries can contribute to further advancements in the acceleration of multimodal foundation models."
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 1 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1, 7, 8, 2 [3] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your ViT but faster. In Proceedings of the International Conference on Learning Representations, 2023. 1, 2, 3, 4, 7, 8 [4] Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh. Honeybee: Locality-enhanced projector for In Proceedings of the IEEE/CVF Conmultimodal LLM. ference on Computer Vision and Pattern Recognition, pages 1381713827, 2024. 1, 7, 8, 2 [5] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In Proceedings of the European Conference on Computer Vision, 2024. 1, 3, 4, 6, 7, [6] Yi Chen, Jian Xu, Xu-Yao Zhang, Wen-Zhuo Liu, YangYang Liu, and Cheng-Lin Liu. Recoverable compression: multimodal vision token recovery mechanism guided by text information. arXiv preprint arXiv:2409.01179, 2024. 1, 6, 7 [7] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng InternVL: Scaling up vision foundation models and Dai. aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 1, 8 [8] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. InstructBLIP: Towards generalpurpose vision-language models with instruction tuning. In Proceedings of the Advances in Neural Information Processing Systems, 2023. 8 [9] Timothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. In Proceedings of the International Conference on Learning Representations, 2024. 6 [10] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaiming He. Masked autoencoders as spatiotemporal learners. In Proceedings of the Advances in Neural Information Processing Systems, pages 3594635958, 2022. [11] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in VQA matter: Elevating the role of image understanding in visual question In Proceedings of the IEEE/CVF Conference answering. on Computer Vision and Pattern Recognition, pages 6325 6334, 2017. 6 [12] Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P. Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3608 3617, 2018. 6 [13] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross B. Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1597915988, 2022. 1 [14] Kai Huang, Hao Zou, Ye Xi, BoChen Wang, Zhen Xie, and Liang Yu. Ivtp: Instruction-guided visual token pruning for large vision-language models. In Proceedings of the European Conference on Computer Vision, 2024. 6, 7, 8, 2 [15] Drew A. Hudson and Christopher D. Manning. GQA: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67006709, 2019. 6 [16] Chen Ju, Haicheng Wang, Haozhe Cheng, Xu Chen, Zhonghua Zhai, Weilin Huang, Jinsong Lan, Shuai Xiao, and Bo Zheng. Turbo: Informativity-driven acceleration plugin for vision-language large models. In Proceedings of the European Conference on Computer Vision, pages 436455, 2024. 1 [17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the International Conference on Machine Learning, pages 1973019742, 2023. [18] Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jianke Zhu, and Lei Zhang. TokenPacker: Efficient visual projector for multimodal LLM. arXiv preprint arXiv:2407.02392, 2024. 1, 8 9 [19] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 292305, 2023. 6 [20] Yanwei Li, Chengyao Wang, and Jiaya Jia. LLaMA-VID: An image is worth 2 tokens in large language models. In Proceedings of the European Conference on Computer Vision, pages 323340, 2024. 7, 2 [21] Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie. Not all patches are what you need: Expediting vision transformers via token reorganizations. In Proceedings of the International Conference on Learning Representations, 2022. 1, 2, 3, 4, 7, 8 [22] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. VILA: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2667926689, 2024. [23] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Proceedings of the Advances in Neural Information Processing Systems, 2023. 6, 8, 1 [24] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2628626296, 2024. 1, 4, 6, 7, 8, 2 [25] Xiangcheng Liu, Tianyi Wu, and Guodong Guo. Adaptive sparse vit: Towards learnable adaptive token pruning by fully exploiting self-attention. In Proceedings of the International Joint Conference on Artificial Intelligence, pages 12221230, 2023. 8 [26] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. MMBench: Is your multi-modal model an all-around player? In Proceedings of the European Conference on Computer Vision, pages 216 233, 2024. 6 [27] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In Proceedings of the Advances in Neural Information Processing Systems, pages 25072521, 2022. 4, 6 [28] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning, pages 87488763, 2021. 8 [30] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. DynamicViT: Efficient vision transformers with dynamic token sparsification. In Proceedings of the Advances in Neural Information Processing Systems, pages 1393713949, 2021. 8 [31] Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. LLaVA-PruMerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. 1, 6, 7, [32] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards VQA models that can read. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 83178326, 2019. 1, 4, 6 [33] Dingjie Song, Wenjun Wang, Shunian Chen, Xidong Wang, Michael Guan, and Benyou Wang. Less is more: simple yet effective token reduction method for efficient multimodal llms. arXiv preprint arXiv:2409.10994, 2024. 7, 2 [34] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1 [35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the Advances in Neural Information Processing Systems, pages 59986008, 2017. 2 [36] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In Proceedings of the International Conference on Learning Representations, 2024. 6 [37] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, and Dahua Lin. PyramidDrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. arXiv preprint arXiv:2410.17247, 2024. 7 [38] Weihao Ye, Qiong Wu, Wenhao Lin, and Yiyi Zhou. Fit token pruning arXiv preprint and prune: Fast and training-free visual for multi-modal large language models. arXiv:2409.10197, 2024. [39] Xubing Ye, Yukang Gan, Xiaoke Huang, Yixiao Ge, Ying Shan, and Yansong Tang. VoCo-LLaMA: Towards vision compression with large language models. arXiv preprint arXiv:2406.12275, 2024. 1 [40] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In Proceedings of the International Conference on Machine Learning, 2024. 6 [41] Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, Beidi Chen, Guangyu Sun, and Kurt Keutzer. LLM inference unveiled: Survey and roofline model insights. arXiv preprint arXiv:2402.16363, 2024. 6, 3 [42] Zheng Zhan, Yushu Wu, Zhenglun Kong, Changdi Yang, Yifan Gong, Xuan Shen, Xue Lin, Pu Zhao, and Yanzhi Wang. Rethinking token reduction for state space models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2024. 1 10 [43] Hang Zhang, Xin Li, and Lidong Bing. Video-LLaMA: An instruction-tuned audio-visual language model for video understanding. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 543553, 2023. [44] Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, and Shanghang Zhang. SparseVLM: Visual token sparsification for efficient visionlanguage model inference. arXiv preprint arXiv:2410.04417, 2024. 1, 6, 7, 8 11 Rethinking Token Reduction in MLLMs: Towards Unified Paradigm for Training-Free Acceleration"
        },
        {
            "title": "Supplementary Material",
            "content": "In the appendix, we provide our main contributions in Sec. 7, comparison with recent work in Sec. 8, theoretical FLOPs calculation in Sec. 9, more implementation details in Sec. 10, more additional experiments and analysis in Sec. 11, and detailed explanation of our methods in Sec. 12. 7. Contribution Summarization The main contributions of our work are four-fold: We propose novel filter-correlate-compress paradigm for token reduction, distinctly decomposes various methods into three key stages within pipeline, thereby ensuring the unity of design objectives and elements in each stage. We conduct empirical studies to show that the paradigm can encompass existing token reduction methods while being flexible enough to derive new approaches. Based on the paradigm, we develop series of methods named FiCoCo that efficiently reduce the amount of visual token without re-training. We validate the effectiveness of FiCoCo on wide range of vision-language tasks across different MLLMs with thorough ablation studies. 8. Comparison with Recent Work Similar to our FiCoCo-V, there is recent work Turbo [16] that also detects redundant tokens by considering their relationships with other patch tokens and the [CLS] token. However, distinct differences are evident, particularly in our correlate and compress stages. Different from ours, Turbo inherits the design of ToMe [3], employing bipartite soft matching with maximum cosine similarity to merge tokens. Our work goes beyond Turbo in the following aspects. Firstly, we propose unified filter-correlate-compress paradigm for training-free token reduction, which systematically decomposes existing pruning and merging techniques into standardized stages with consistent elements. We regard this as the greatest contribution of our work, which provides substantial inspiration or advancing the field and for the formulation of future methodologies. Secondly, we also address the unification of token reduction across the two phases of MLLM inference and propose the FiCoCo-L variant. This method optimally leverages the semantic and task information embedded within textual tokens, thereby achieving more effective compression of task-irrelevant redundant visual tokens during LLM decoding, as demonstrated empirically."
        },
        {
            "title": "Considering that Turbo did not provide results for",
            "content": "LLaVA series [23], the predominant base models utilized in our study and associated research, and given the unavailability of its source code at the time of our submission, we were unable to include it in our experimental comparisons. Integrating Turbo into our unified paradigm and conducting empirical comparisons with our methods will be part of our future work. 9. Theoretical FLOPs Calculation Here we consider hypothetical scenario to analyze the changes in FLOPs before and after applying FiCoCo-V and FiCoCo-L. In this context, the hidden state dimension in single transformer layer is denoted as D, while the feedforward layer dimension is represented by H. The total number of visual tokens is represented by , with denoting the number of compressed visual tokens per layer. Additionally, represents the number of text tokens. To simplify the equations, we define: = S, = + M, = + M. Here, represents the total number of visual and text tokens before compression, while represents the total tokens after compression. Finally, for FiCoCo-V, we have: FLOPsbefore = 4N D2 + 2N 2D + 2N DH, FLOPsafter =4N D2 + 2(N )2D + 2N DH, = 4N SD2 + 2 (cid:0)N (N S)2(cid:1) + 2N SDH. (16) For FiCoCo-L, we have: FLOPsbefore = 4P D2 + 2P 2D + 2P DH, FLOPsafter =4P D2 + 2(P )2D + 2P DH, = 4N SD2 + 2 (cid:0)2N (N S)2(cid:1) + 2N SDH. (17) We now analyze the additional FLOPs introduced by the internal operations of FiCoCo-V and FiCoCo-L. As described in Sec. 12, the primary computational overhead for FiCoCo-V stems from the redundancy score calculation, the determination of token-adaptive values, and the token updating process. In comparison, FiCoCo-L incorporates similar steps but introduces an additional interaction with the indirect text matrix during the correlate phase, resulting in higher computational complexity. The variable represents the number of target tokens. However, since both FiCoCo-V and FiCoCo-L only operate on visual tokens, 1 Method LLaVA-1.5 [24] TFlops=15.4 TRIM [33] Honeybee [4] LLaMA-VID [20] Qwen-VL [2] IVTP [14] Random Sampling [14] TopK [14] Spatial Pooling [14] EViT [21] ToMe [3] FiCoCo-V FiCoCo-L FiCoCo-VL Training-free TFLOPs SQA VQAT POPE VizWiz MM-Vet MMBCN GQA LLAVA-W MMB VQAv2 28.6 71.4 61.3 86.2 54.1 36. 63.2 63.4 70.1 68.0 80.0 16.4 15.4 15.4 15.4 15.4 15.4 15.4 15.4 15.4 15.4 15.4 15.4 15. 72.8 70.5 70.4 70.8 70.1 68.0 68.9 69.5 70.1 70.1 72.1 72.4 72.0 54.8 59.7 57.2 56.4 60.0 51.5 54.2 55.0 57.9 57.1 57.2 58.3 57.2 86.3 83.5 83.3 84.0 85.4 83.3 84.5 84.8 84.6 85.3 82.3 83.1 82.1 53.2 46.6 50.8 51.1 53.4 52.9 53.1 54.1 50.0 - 53.0 53.9 53.2 30.3 24.6 26.5 27.4 28.6 32.7 30.1 33.5 24.4 - 32.6 34.2 33.1 58.3 54.8 58.0 54.9 55.4 55.4 56.1 57.3 52.4 - 60.7 61.1 60. 59.0 59.2 61.7 61.2 62.3 56.7 59.2 59.7 60.2 61.4 59.2 60.1 59.4 57.0 58.8 62.8 64.2 64.6 66.0 65.3 68.8 45.5 - 62.3 67.9 65.9 69.2 60.3 60.5 61.7 66.7 58.0 58.3 60.2 61.0 61.2 63.1 65.2 64.6 75.4 74.8 76.5 77.3 78.4 72.3 74.8 75.1 77.2 76.9 76.8 77.6 77.3 Table 5. Comparison results on MLLMs with 13B LLM. For baselines, we reference results reported in other papers. Our methods are primarily compared with training-free approaches. their FLOPs calculations are nearly identical. For FiCoCoV, we have: FLOPs = 2 + 2N + S(N + 2D + 1) + D. (18) For FiCoCo-L, we have: FLOPs = 2(N 2 + 2N ) + S(N + 2D + 1) + D. (19) Based on the above analysis, the additional FLOPs introduced by FiCoCo-V and FiCoCo-L are negligible compared to the significant reduction in FLOPs ( ) achieved through token compression. Specifically, while grows quadratically with the hidden state dimension D, the additional FLOPs primarily grow linearly, making their impact inconsequential in practical scenarios. 10. More Implementation Details For FiCoCo, we adopt the LLaVA-1.5-7B/13B models [24] and employ the following settings: (1) λ = 0.35 in filter stage of FiCoCo-V, (2) β = 0.6 in filter stage of FiCoCoL, (3) γ = 0.6 in correlate stage of FiCoCo-L, (4) scaling coefficient=2 in local penalty strategy, (5) ε = 0.998 to determine the token-wise threshold in compress stage. We provide sensitivity analyses of these hyperparameters in Sec. 11.2. For the local penalty strategy, we fix 2 2 window across all layers. In addition, as discussed in Sec. 3.2, we delay the token reduction until the attention converges to stability. Specifically, in FiCoCo-V, the token compression starts at the 12-th layer of the vision encoder, while in FiCoCo-L, it starts at the 4-th layer of the LLM. 11. More Experiments and Analysis 11.1. Comparisons on LLaVA-1.5 with 13B LLM Tab. 5 reports the comparison results, where our methods still demonstrates competitiveness. FiCoCo-V FiCoCo-L ε SQA TextVQA SQA TextVQA 0.998 68.37 0.996 68.33 0.994 68.21 0.992 68.47 55.46 53.15 52.05 52.29 69.46 69.51 69.32 69.36 55.72 55.62 55.42 55. Table 6. Hyperparameter sensitivity analysis of ε on TextVQA and SQA benchmarks. scaling coefficient FiCoCo-V in local penalty strategy SQA TextVQA 1 2 3 4 68.12 68.37 68.21 68. 53.24 55.46 55.04 55.49 Table 7. Hyperparameter sensitivity analysis of scaling coefficient in local penalty strategy on TextVQA and SQA benchmarks. 11.2. Sensitivity Analysis of Hyperparameters We explore the hyperparameter configurations of FiCoCo, performing sensitivity analysis on individual parameters to assess their impact. The experiments are conducted on both TextVQA and SQA benchmarks, with FLOPs at 1.5. Trade-off hyperparameters. It is observed that: (1) The hyperparameter λ = 0.35 is the optimal setting. Under this configuration, both FiCoCo-V and FiCoCo-L variants achieve relatively optimal accuracy. This indicates that when λ = 0.35, FiCoCo effectively balances the local information conveyed by patch tokens with the global information carried by the [CLS] token, thereby enhancing the integration of visual features and the completeness of in- (2) The hyperparameter β = 0.6 is the optiformation. mal setting. For the SQA dataset, FiCoCo-L demonstrates clear upward trend between β = 0.4 and β = 0.6, with similar trend observed on the TextVQA dataset. This finding suggests that, under this parameter setting, an effective balance is achieved between textual information and the information conveyed by patch tokens. (3) The hyperparam2 Figure 4. Hyperparameter sensitivity analysis of λ, β and γ on TextVQA and SQA benchmarks. eter γ = 0.6 is the optimal setting. Fig. 4 clearly shows that FiCoCo-V and FiCoCo-L both reach their performance peaks at γ = 0.6 across the two benchmarks. This result suggests that incorporating semantic similarity more effectively guides the selection of the target set during the compress stage, thereby optimizing overall performance. ε hyperparameter. Tab. 6 compares the impact of different quantile thresholds ε-th. Experimental results demonstrate that setting ε to 0.998 yields optimal performance on both the TextVQA and SQA benchmarks. However, as ε-th decreases, the information of single token gets distributed across more tokens, which leads to noticeable performance drop in both benchmarks due to the excessive information fusion. Scaling coefficient hyperparameter in local penalty strategy. Tab. 7 shows that when the scaling coefficient exceeds 2, the performance stably closes to optimal. Therefore, to balance design simplicity and performance stability, we opt to fix the punishment coefficient at 2. 11.3. Efficiency Analysis Utilizing the tools provided by [41], we conduct detailed analysis of the theoretical efficiency of our FiCoCo. In Tab. 8, we assume the number of textual tokens is 60 for LLaVA-1.5-7B. And in Tab. 9, we assume the number of textual tokens is 512 for LLaVA-1.5-13B. The results demonstrate that, compared to the baseline models of LLaVA-1.5-7B/13B, our FiCoCo series achieve significant improvements in both computational efficiency and GPU memory utilization. Specifically, our FiCoCo series reduces computational overhead by nearly 80%, GPU memory usage by approximately 40%, and KV-Cache storage by around 80%, all while achieving performance comparable to LLaVA-1.5-7B. Notably, this is accomplished without requiring any additional training, highlighting the efficiency and flexibility of our FiCoCo series. 11.4. Further Experiments on LLaVA-NeXT We apply our FiCoCo series to the LLaVA-NeXT model (i.e., Open-LLaVA-NeXT-7B) to evaluate its extensibility in greater depth. Unlike LLaVA-1.5, LLaVA-NeXT incorporates the anyres technique, which increases the number of visual tokens fed into the LLM. While this enhances performance, it also introduces more pronounced computational bottleneck. Therefore, common practice is to use FlashAttention tool for acceleration. We provide detailed analysis of both the flexibility and the limitations of our proposed approach in Tab. 10, where exist the following observations: (1) FiCoCo-V does not require calculating attention scores within the LLM, thus allowing the smooth utilization of FlashAttention. Compared to Open-LLaVA-NeXT-7B, the time consumption on the SQA and MMB benchmarks is reduced by 28.6% and 35.7%, respectively, while the accuracy degradation is limited to 0.2% and 1.0%, respectively. (2) Our FiCoCo-L and FiCoCo-VL require explicit access to the attention weights within the LLM, which prevents the use of FlashAttention in the LLM. Tab. 10 shows that when FlashAttention is disabled across all methods, both FiCoCo-L and FiCoCo-VL significantly reduce inference time while keeping accuracy loss on the SQA and MMB 3 Method LLaVA-1.5 FiCoCo-V FiCoCo-L FiCoCo-VL LLaVA-1.5 FiCoCo-V FiCoCo-L FiCoCo-VL LLaVA-1.5 FiCoCo-V FiCoCo-L FiCoCo-VL Method LLaVA-1.5 FiCoCo-V FiCoCo-L FiCoCo-VL LLaVA-1.5 FiCoCo-V FiCoCo-L FiCoCo-VL LLaVA-1.5 FiCoCo-V FiCoCo-L FiCoCo-VL LLM Backbone Quantization TFLOPs Total Memory (GB) KV-Cache (MB) Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B FP16 FP16 FP16 FP16 INT8 INT8 INT8 INT8 INT4 INT4 INT4 INT 8.5 1.5 (82%) 1.5 (82%) 1.5 (82%) 4.3 0.8 (81%) 0.8 (81%) 0.7 (84%) 2.1 0.4 (81%) 0.4 (81%) 0.4 (81%) 22.4 14.4 (36%) 14.3 (36%) 13.0 (42%) 11.2 7.8 (30%) 7.2 (36%) 6.5 (42%) 6.2 4.4 (29%) 3.3 (47%) 3.3 (47%) 333 65.0 (80%) 64.2 (81%) 60.8 (82%) 167 32.5 (81%) 32.1 (81%) 30.4 (82%) 83.4 16.3 (81%) 16.1 (81%) 15.2 (82%) Table 8. Efficiency analysis of methods based on LLaVA-1.5-7B. LLM Backbone Quantization TFLOPs Total Memory (GB) KV-Cache (MB) Vicuna-13B Vicuna-13B Vicuna-13B Vicuna-13B Vicuna-13B Vicuna-13B Vicuna-13B Vicuna-13B Vicuna-13B Vicuna-13B Vicuna-13B Vicuna-13B FP16 FP16 FP16 FP16 INT8 INT8 INT8 INT8 INT4 INT4 INT4 INT4 28.6 15.4 (46%) 15.4 (46%) 15.4 (46%) 14.3 7.7 (46%) 7.7 (46%) 7.6 (47%) 7.6 3.9 (46%) 3.9 (49%) 3.8 (50%) 56.1 38.6 (31%) 38.4 (32%) 38.3 (32%) 28 19.3 (31%) 19.2 (31%) 19.2 (31%) 14 9.6 (32%) 9.5 (32%) 9.5 (32%) 891 488 (43%) 485 (46%) 482 (46%) 446 244 (45%) 242 (46%) 241 (46%) 223 122 (49%) 121 (46%) 120 (46%) Table 9. Efficiency analysis of methods based on LLaVA-1.5-13B. benchmarks within an acceptable range. Specifically, on the SQA benchmark, FiCoCo-VL reduces inference time by 36.8% while improving accuracy by 0.25%. These results indicate that our FiCoCo series can effectively reduce the computational cost and inference time of Open-LLaVANeXT while maintaining strong performance, further highlighting the flexibility of the FiCoCo series. 11.5. Analysis of Failure Cases FiCoCo maintains substantial performance even when compressing significant number of visual tokens. However, the inevitable loss of visual information during the token reduction still causes failure cases. We show two cases in Fig. 5 where the answers generated by LLaVA-1.5 are consistent with the ground truth, while FiCoCo-L and FiCoCoV fail to answer correctly. By analyzing the erroneous responses generated by FiCoCo-L and FiCoCo-V, it can be observed that FiCoCo-L produces answers more closely aligned with the questions, guided by the token selection process involving textual information. For instance, in Fig. 5(a), the prompts top and yellow sticker jointly indicate the yellow region at the top of the refrigerator, leading FiCoCo-L to search for the answer in this specific region. However, FiCoCo-V fails to attend to the crucial information regarding top. Moreover, in Fig. 5(b), the cues 3 letter word and left of casa jointly guide the answer towards tua. Although the generated answer of FiCoCo-L is mal, Figure 5. Failure cases of FiCoCo, where FiCoCo-L produces answers more closely aligned with the questions. it more effectively considers these two cues. In contrast, FiCoCo-V fails to adequately track the critical information pertaining to 3 letter word. 12. Algorithm Illustration We provide detailed explanation of our FiCoCo-V and FiCoCo-L processes in Algorithm 1 and Algorithm 2, respectively, to facilitate clearer understanding of the unified filter-correlate-compress paradigm we propose. 4 Method TFLOPs FlashAttn Open-LLaVA-NeXT-7B FiCoCo-V Open-LLaVA-NeXT-7B FiCoCo-L FiCoCo-VL 20.8 9.5 (54.3%) 20.8 9.5 (54.3%) 9.5 (54.3%) SQA Time 12m01s 8m35s (28.6%) 17m34s 13m23s (23.8%) 11m06s (36.8%) Acc 69.06 68.86 69.01 68.21 69.26 MMB Time 22m47s 14m39s (35.7%) 34m02s 25m13s (25.9%) 21m45s (36.1%) Acc 66.07 65.03 66.07 64.67 65.30 Table 10. Comparisons based on Open-LLaVA-NeXT-7B. We categorize the methods based on the availability of FlashAttention and provide FLOPs and time measurements to demonstrate that our methods can effectively accelerate across different scenarios. Algorithm 1 FiCoCo-V Require: Input tokens RN D, attention score tensor Av RN , [CLS] attention score vector aCLS RN , reduction factor R, number of visual tokens R, hyperparameters λ, ε [0, 1] Ensure: Output tokens R(N S)D 1: Step 1: Filter 2: Compute redundancy scores for all visual tokens: (cid:88) Av i,j (1 λ)aCLS sv = λ"
        },
        {
            "title": "1\nN",
            "content": "j=1 3: Partition sv into windows and apply local penalty 4: Identify source set = topK(sv, S) that contains the indices of discarded visual tokens 5: Identify target set that contains the indices of (N S) remaining visual tokens 6: Step 2: Correlate 7: Construct correlation matrix: Cv i,j = Av i,j, S, 8: Step 3: Compress 9: Apply token-wise quantile-based thresholding: τi = quantile(Cv i,:, ε) 10: Compute token-adaptive topK correlations: Ij = {i and Cv i,j τi}, Ji = {j and Cv i,j τi} 11: Compute compression weights: Cv i,j αij = (cid:80) jJi Cv i,j 12: Update correlated tokens: XT XT + (cid:80) 1 + (cid:80) iIj iIj αijXS αij 13: Output tokens: 14: return XS Algorithm 2 FiCoCo-L Require: Input tokens R(N +M )D, attention score tensor Al R(N +M )(N +M ), reduction factor R, number of visual tokens R, number of textual tokens R, hyperparameters β, γ, ε [0, 1] Ensure: Output tokens R(N +M S)D 1: Step 1: Filter 2: Compute redundancy scores for all visual tokens: sl = β"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) j=1 Al i,j (1 β) +M (cid:88) k=N + Al i,k 3: Identify source set = topK(sv, S) that contains the indices of discarded visual tokens 4: Identify target set that contains the indices of (N S) remaining visual tokens 5: Step 2: Correlate 6: Compute direct and indirect correlations: Cl i,j = γAl i,j + (1 γ) +M (cid:88) k=N + Al i,k Al k,j 7: Step 3: Compress 8: Apply token-wise quantile-based thresholding: τi = quantile(Cl i,:, ε) 9: Compute token-adaptive topK correlations: Ij = {i and Cl i,j τi}, Ji = {j and Cl i,j τi} 10: Compute compression weights: Cl αij = i,j (cid:80) jJi Cl i,j 11: Update correlated tokens: XT XT + (cid:80) 1 + (cid:80) iIj iIj αijXS αij 12: Output tokens: 13: return XS"
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Northwestern Polytechnical University",
        "Sichuan University",
        "Westlake University"
    ]
}