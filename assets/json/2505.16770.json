{
    "paper_title": "RBench-V: A Primary Assessment for Visual Reasoning Models with Multi-modal Outputs",
    "authors": [
        "Meng-Hao Guo",
        "Xuanyu Chu",
        "Qianrui Yang",
        "Zhe-Han Mo",
        "Yiqing Shen",
        "Pei-lin Li",
        "Xinjie Lin",
        "Jinnian Zhang",
        "Xin-Sheng Chen",
        "Yi Zhang",
        "Kiyohiro Nakayama",
        "Zhengyang Geng",
        "Houwen Peng",
        "Han Hu",
        "Shi-Min Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancement of native multi-modal models and omni-models, exemplified by GPT-4o, Gemini, and o3, with their capability to process and generate content across modalities such as text and images, marks a significant milestone in the evolution of intelligence. Systematic evaluation of their multi-modal output capabilities in visual thinking processes (also known as multi-modal chain of thought, M-CoT) becomes critically important. However, existing benchmarks for evaluating multi-modal models primarily focus on assessing multi-modal inputs and text-only reasoning while neglecting the importance of reasoning through multi-modal outputs. In this paper, we present a benchmark, dubbed RBench-V, designed to assess models' vision-indispensable reasoning abilities. To construct RBench-V, we carefully hand-pick 803 questions covering math, physics, counting, and games. Unlike previous benchmarks that typically specify certain input modalities, RBench-V presents problems centered on multi-modal outputs, which require image manipulation such as generating novel images and constructing auxiliary lines to support the reasoning process. We evaluate numerous open- and closed-source models on RBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the best-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below the human score of 82.3%, highlighting that current models struggle to leverage multi-modal reasoning. Data and code are available at https://evalmodels.github.io/rbenchv"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 2 0 7 7 6 1 . 5 0 5 2 : r Bench-V: Primary Assessment for Visual Reasoning Models with Multi-modal Outputs Meng-Hao Guo1, Xuanyu Chu1, Qianrui Yang1, Zhe-Han Mo1, Yiqing Shen1 Pei-Lin Li1, Xinjie Lin1, Jinnian Zhang2, Xin-Sheng Chen1, Yi Zhang1 Kiyohiro Nakayama3, Zhengyang Geng4, Houwen Peng2, Han Hu2, Shi-Min Hu1 1 Tsinghua University, 2 Tencent Hunyuan X, 3 Stanford University, 4 Carnegie Mellon University"
        },
        {
            "title": "Abstract",
            "content": "The rapid advancement of native multi-modal models and omni-models, exemplified by GPT-4o, Gemini and o3 with their capability to process and generate content across modalities such as text and images, marks significant milestone in the evolution of intelligence. Systematic evaluation of their multi-modal output capabilities in visual thinking process (a.k.a., multi-modal chain of thought, M-CoT) becomes critically important. However, existing benchmarks for evaluating multi-modal models primarily focus on assessing multi-modal inputs and text-only reasoning process while neglecting the importance of reasoning through multi-modal outputs. In this paper, we present benchmark, dubbed as RBench-V, designed to assess models vision-indispensable reasoning. To conduct RBench-V, we carefully hand-pick 803 questions covering math, physics, counting and games. Unlike problems in previous benchmarks, which typically specify certain input modalities, RBench-V presents problems centered on multi-modal outputs, which require image manipulation, such as generating novel images and constructing auxiliary lines to support reasoning process. We evaluate numerous openand closed-source models on RBench-V, including o3, Gemini 2.5 pro, Qwen2.5VL, etc. Even the best-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below the human score of 82.3%, which shows current models struggle to leverage multi-modal reasoning. Data and code are available at https://evalmodels.github.io/rbenchv."
        },
        {
            "title": "Introduction",
            "content": "What cannot create, do not understand. Richard Feynman. Whether adults or children, when faced with complex problems, they sometimes turn to drawing or diagramming to organize their thoughts, support reasoning, and seek solutions. As highlighted by the quote 1 and findings in neuroscience Goldschmidt [1991], Pylyshyn [2001], Edwards [2012], Fan et al. [2023], the capability to draw during problem-solving is not only hallmark of cognitive development but also an expression of human intelligence. But what about intelligent models? Can they also learn to draw in order to reason and solve problems? Recently, researchers have made great progress toward equipping foundation models with above capability, and the landscape of foundation models has undergone profound transformation, driven Shi-Min Hu is the corresponding author. E-mail: shimin@tsinghua.edu.cn. Preprint. Figure 1: The comparison between open-source models, closed-source models and human experts on RBench-V. It reveals there remains significant gap between models and human experts in visual reasoning with multi-modal outputs. by two major converging trends. First, modal convergence reflects the evolution from single-modality language models, such as ChatGPT OpenAI [2022], to omni-modal systems capable of both multimodal input and output, exemplified by GPT-4o OpenAI [2024a]. Second, cognitive convergence captures the transition from chat-oriented models to reasoning-driven models, as evidenced by the progression from ChatGPT OpenAI [2022] to more advanced systems such as OpenAI o1/o3 OpenAI [2024b] and DeepSeek R1 DeepSeek [2025]. As model input and output modalities converge, the evaluation frameworks for these leading foundation models must also evolve accordingly. Existing benchmarks such as MMMU Yue et al. [2024a] and MMLU Hendrycks et al. [2021], have played key role in advancing the field by providing frameworks to evaluate model capabilities. However, these benchmarks are predominantly input-oriented, focusing on the models ability to interpret, understand, and reason over multi-modal inputs, while overlooking an equally critical aspect: the modality of outputs. This refers to the models ability to generate contextually appropriate multi-modal responses during the problem-solving process, whether through language, visual content, or other formats. In this paper, we present RBench-V, an early exploration for multi-modal output-oriented reasoning benchmark. To build RBench-V, we carefully and strictly hand-pick and design 803 question-answer pairs, covering math, physics, counting, and games. In Fig 3, we clearly illustrate the differences between RBench-V and other classic language model benchmarks, MMLU Hendrycks et al. [2021], and the multi-modal input-oriented benchmark, MMMU Yue et al. [2024a]. It can be seen that the main difference from previous benchmarks is that in RBench-V, each question requires the model to produce multi-modal outputs during the reasoning process, particularly modifications on the images, such as drawing images, adding auxiliary lines, and so on. We evaluate wide range of openand closed-source multi-modal large language models (MLLMs) and omni models on the RBench-V, including GPT-4o OpenAI [2024a], Gemini Google et al. [2023], Qwen2.5VL Bai et al. [2025], Claude3.5 Anthropic [2024], DeepSeek-VL2 Wu et al. [2024], etc. Besides, we also organize human to conduct tests on RBench-V. Our main observations and findings from the experiments are highlighted as follows. For more details, please refer to the experiment section. If models, such as the InternVL or Qwen-VL series, lack multi-modal CoT, merely increasing their model sizes will not effectively resolve the challenges of vision-indisperential reasoning. Figure 2: The motivation of RBench-V. Left: An illustration showing both humans and the GPT4o model being asked game-related question from RBench-V. Right: This part shows common benchmarks such as MMLU, MMMU, and Rench focus on multi-modal inputs and textual outputs, whereas RBench-V emphasizes not only multi-modal inputs but also multi-modal outputs. It might be necessary to explore new paradigms, potentially incorporating M-CoT or agentbased reasoning frameworks, to solve visual-necessary complex problems. Despite the diverse capabilities of these models, even the highest-performing one, i.e., o3 OpenAI [2025c], achieves only 25.8% accuracy on RBench-V, which is significantly lower than the human score of 82.3%. This stark contrast highlights that, while impressive in many areas, the models still struggle to generate and integrate appropriate multi-modal responses in the visual thinking process. Besides, the o3 model has achieved significant progress in visual reasoning, outperforming previous state-of-the-art models by substantial margin (+4.9%). RBench-V effectively captures this advancement and offers an automated framework for evaluating multi-modal output capabilities in visual reasoning. The reasoning thought of models differ from that of human experts in math. We find that while models perform well on math questions, this does not necessarily indicate that they have learned to multi-modal reasoning. Instead, models often convert certain geometry problems into algebraic ones and solving them through textual reasoning. In contrast, human experts tend to prefer geometric solutions. This highlights fundamental difference between the intelligence exhibited by current models and that of humans."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Foundation Models Foundation models have evolved rapidly along two axes: the expansion from language understanding to omni-models, and the progression from chat models to advanced reasoning models. From language models to omni-models: It represents progress along the modality axis. Early foundation modelssuch as ChatGPT OpenAI [2022], LLaMA Touvron et al. [2023], Qwen Bai et al. [2023], Mistral Jiang et al. [2023], GLM Zeng et al. [2022], etc, are limited to text-based dialogue. Researchers then begin exploring models with multi-modal inputs, including GPT-4V OpenAI [2023], LLaVA Liu et al. [2024], miniGPT-4 Zhu et al. [2023], Claude3.5 Anthropic [2024], etc. Recently, attention has shifted toward omni-models, which not only receive multi-modal inputs but also generate flexible multi-modal outputs (e.g., GPT-4o OpenAI [2024a], Gemini 2.5 Pro Google [2025], Qwen2.5-Omni Xu et al. [2025] and Emu3 Wang et al. [2024b]). From chat models to reasoning models: Another critical axis of advancement lies in reasoning capabilities. Early chat-based foundation models OpenAI [2022], Touvron et al. [2023], Abdin et al. [2024] primarily focus on fluent and context-aware dialogue generation. Recently, researchers have begun to push the boundaries of model reasoning OpenAI [2024b], DeepSeek [2025], Google [2025], 3 aiming to equip models with the ability to synthesize existing knowledge and solve complex, novel problems. For more, readers are referred to this survey Wang et al. [2025]. 2.2 Benchmarking Foundation Models Benchmarks serve as an essential tool for evaluating foundation models and providing guiding light for their further development. Existing benchmarks primarily focus on multi-modal inputs such as text Hendrycks et al. [2021], Guo et al. [2025], Wang et al. [2024c], Chen et al. [2021], Contributors [2023] and multimodality Yue et al. [2024a,b], Lu et al. [2023], Wang et al. [2024a], Lu et al. [2023], Gao et al. [2024], but their outputs are all textual, overlooking the evaluation of models multi-modal output capabilities. Besides, some studies Heusel et al. [2017], Kastryulin et al. [2022], You et al. [2024] have also focused on image generation quality, but they primarily emphasize aesthetic metrics. As mentioned in Sec. 2.1, we believe that the next generation of powerful models are omni-models with strong textual and visual reasoning capabilities. Thus, in this paper, we present RBench-V, benchmark for omni reasoning models. To the best of our knowledge, this is the first attempt to design benchmark to assess models multi-modal generation capability in the visual thinking process."
        },
        {
            "title": "3 RBench-V",
            "content": "3.1 Data Collection of RBench-V The central challenge in developing RBench-V lies in designing and curating questions that assess models ability to generate multi-modal outputs during visual reasoning. Clear examples are shown in Fig. 3, solving problems in RBench-V requires producing outputs beyond text, such as drawing geometric figures (top-right example) or tracing path through maze (bottom-right example). To build RBench-V, our principle in designing or collecting questions is that their solution should involve creating new visual content, such as creating images or modification of existing images, during the problem-solving process. We can imagine that numerous real-world scenarios, such as GUI operation and drawing, rely on multi-modal outputs for visual reasoning in daily life. In this work, RBench-V primarily focuses on math, physics, counting, and games. To curate high-quality questions in math and physics, we collaborate with domain experts. For counting and games, we conduct rigorous rule to create, collect, and filter questions. The collection criteria for different domains are detailed as follows. Math: For math, we primarily focus on geometric and graph theory problems, including transformation geometry, planar geometry, solid geometry, etc. Transformation Geometry: The problems in RBench-V mainly involve translations, reflections and rotations, requiring the model to draw the resulting figures after applying these transformations. in order to arrive at the correct answer. Planar Geometry: These problems evaluate whether the model can construct appropriate auxiliary lines to aid in reasoning. Solid Geometry: The solid geometry tasks in RBench-V assess models to assemble 3D shapes from 2D components according to specific rules and to draw the resulting solid before answering the question. Graph Theory: RBench-V requires models to first complete the graph based on given constraints, mark the leaf nodes, and then reason to arrive at the correct answer. Physics: We primarily focus on optics, mechanics, electromagnetism, and thermodynamics. Not all above problems meet our criteria and we specifically select those that require visual reasoning. Optics: Tasks emphasize geometric optics, requiring models to trace light trajectories involving reflection, refraction, and diffraction. Precise visualization of light paths is essential for deriving optical principles. Mechanics: This category includes statics, kinematics, and dynamics, involving complex physical constraints. Models must interpret and construct geometric relationships using freebody diagrams and motion trajectories to analyze force interactions, motion paths, and equilibrium conditions. Electromagnetism: This area comprises two subcategories. Circuit analysis tasks require models to identify current paths and simplify circuit diagrams in complex scenarios. Dynamic problems combine electromagnetic fields with mechanics, necessitating the visualization of electric and magnetic field lines to analyze particle motion. Thermodynamics: Tasks primarily involve fluid force analysis, where models must visually represent dynamic changes in liquid surfaces and force distributions to solve problems related to surface tension, hydrostatic pressure, and buoyancy. Figure 3: visual comparison with MMLU, MMMU and RBench-V. It shows that solving problems in MMLU and MMMU mainly requires understanding multi-modal inputs and generating textual outputs, whereas solving problems in RBench-V demands not only understanding multi-modal inputs but also generating multi-modal outputs. The red lines shown in the figure are not part of the original questions and represent the multi-modal reasoning process when solving problems in RBench-V, such as drawing geometric shapes or tracing paths through maze. Counting: Counting problems typically require no image modification. For instance, identifying two people in clearly depicted photo needs no interaction. The counting problems in RBench-V differ from the simple example above and can be broadly categorized into the following three types: Firstly, problems require drawing geometric shapes based on descriptions or connecting lines within the image before answering questions such as how many triangles are present (an example is shown in the top-right corner of Fig. 3). Secondly, questions involve images with lots of targets and chaotic backgrounds. Solving such problems requires models to carefully check the images, mark the targets it has counted, and then reason to answer the total. (examples are provided in the supplementary materials.) Thirdly, problems demand an understanding of spatial relationships and imagination. Models need to mentally manipulate or move 3D objects and visualize the resulting state after movement, in order to complete the counting task. Games: We primarily focus on several types of games that require multi-modal outputs in the visual reasoning process: Connect-the-dots: Models need to connect sequence of dots to reveal an image and identify what object in the image. Mazes: Models should trace the correct path through the maze and answer questions based on the trajectory. Dart-and-balloon, and Gold Miner: These require models to accurately draw the trajectory of darts and hooks, and determine their intersection with the target objects. Puzzles: The task involves moving different pieces to complete the full puzzle. Ball-and-brick: It requires drawing the trajectory of the ball, which may collide and bounce against the wall multiple times. 5 Table 2: Models and experts scores of multi-modal output requirements across different benchmarks. (a) RBench-V vs. MMLU on text-only questions. (b) RBench-V vs. MMMU on multi-modal questions."
        },
        {
            "title": "Model\nHuman",
            "content": "RBench-V win MMLU win Tie 3.8 9.1 92.4 86.4 3.8 4."
        },
        {
            "title": "Model\nHuman",
            "content": "RBench-V win MMMU win Tie 0.0 10.0 94.1 83.4 5.9 6.6 3.2 Statistics of RBench-V We conduct statistical analysis on RBench-V with the results presented in Tab. 1. It presents that RBench-V includes 803 questions across four areas, with 176 math questions, 157 physics questions, 195 counting problems, and 275 gamerelated questions, comprising 356 multiple-choice questions (We categorize questions with clearly limited answer choices as multiple-choice questions, such as the maze problem in the bottom right of Fig. 3.) and 447 open-ended questions. It is worth noting that since we primarily focus on multi-modal outputs rather than inputs, so RBench-V includes both textonly and multi-modal input questions. categorized as 40 and 763, respectively. As an early exploration into visual reasoning and multi-modal outputs, this paper focuses on text and image modalities, aiming to offer insights for foundation models. As for more modalities such as video and audio outputs, we expect more work to be done in the future. Table 1: Statistics on RBench-V. MC denotes multiple-choice. Statistics"
        },
        {
            "title": "Math Questions\nPhysics Questions\nCounting Questions\nGames Questions",
            "content": "MC Questions Open Questions Text-only Questions Multi-modal Questions Number 803 176 157 195 275 356"
        },
        {
            "title": "4 Experiments",
            "content": "After developing RBench-V, we comprehensively evaluate many openand closed-source MLLMs (e.g., Qwen2.5VL Bai et al. [2025], Claude3.7 Anthropic [2024], LLaVA-OneVision Li et al. [2024], etc) , omni-models (e.g., GPT-4o OpenAI [2024a], Gemini2.5pro Google et al. [2024], Qwen2.5Omni Xu et al. [2025], etc) , thinking models (e.g., OpenAI o3 OpenAI [2025c] and Doubao-1.5thinking-pro-m Seed [2025b]) and humam experts on RBench-V. We list all the tested models in Tab. 3. By default, all evaluations are conducted in zero-shot setting. Besides, since RBench-V includes both multiple-choice and open-ended questions, we adopt unified LLM-as-a-Judge framework, with the judge model being GPT-4o. We report Top-1 accuracy (%) as our default evaluation metric. 4.1 Comparison with other benchmarks on multi-modal output capability requirements Here, we compare RBench-V with other benchmarks (MMLU, MMMU) in terms of multi-modal output evaluation. As we know, it is challenging to find quantitative method to assess the property for multi-modal outputs in existing benchmarks. Therefore, we construct expert scores and model scores to measure their multi-modal reasoning property. Specifically, we randomly sample 30 examples from each of RBench-V, MMLU, and MMMU, and instruct either human experts or the models (o3 and Doubao-1.5-thinking-m) to compare whether requires drawing during the thinking process. We summarize the win rates in Table 2. Results from both human experts and models consistently show that RBench-V imposes significantly higher requirement for multi-modal outputs during the reasoning process compared to MMLU and MMMU. This highlights that RBench-V is specifically designed to assess multi-modal output capabilities and visual reasoning skills. 4.2 Evaluating visual reasoning and multi-modal outputs of different models We assess various openand closed-source models, along with human experts, on RBench-V. The specific models are listed in Tab. 3. For open-source models, we use vLLM Kwon et al. [2023] and VLMEvalKit Duan et al. [2024] for deployment, setting the temperature as 0 while leaving all other 6 Table 3: Performance (%) of different models and human experts on RBench-V. means long chain thinking model. represents the omni-model. The highest scores are highlighted in red, and the second-highest scores are highlighted in blue."
        },
        {
            "title": "Name",
            "content": "Overall w/o Math Math Physics Counting Games Open-source models Qwen2.5-Omni-7B Xu et al. [2025] InternVL-3-14B Zhu et al. [2025] InternVL-3-8B Zhu et al. [2025] Qwen2.5VL-7B Bai et al. [2025] LLaVA-OneVision-7B Li et al. [2024] DeepSeek-VL2 Wu et al. [2024] LLaVA-OneVision-72B Li et al. [2024] MiniCPM-2.6-V Yao et al. [2024] Llama4-Scout (109B MoE) Meta [2025] MiniCPM-2.6-o Yao et al. [2024] Qwen2.5VL-32B Bai et al. [2025] InternVL-3-38B Zhu et al. [2025] Qwen2.5VL-72B Bai et al. [2025] 7.7 8.0 8.2 8.3 8.5 9.0 9.0 9.1 9.5 9.7 10.0 10.0 10.6 4.5 7.0 6.0 7.0 6.8 7.0 8.9 7.2 6.9 7.5 6.4 7.2 9.2 Closed-source models 11.0 QVQ-Max Qwen [2025] 11.5 Claude-3.7-sonnet Anthropic [2025] 12.6 OpenAI GPT-4.5 OpenAI [2025b] Step-R1-V-Mini StepFun [2025] 13.2 13.6 OpenAI GPT-4.1 OpenAI [2025a] OpenAI GPT-4o-20250327 OpenAI [2024a] 14.1 15.6 Doubao-1.5-vision-pro Seed [2025a] OpenAI o1 OpenAI [2024b] 16.2 Doubao-1.5-thinking-pro-m Seed [2025b] 17.1 Gemini 2.5 pro-preview-0506 Google [2025] 20.2 OpenAI o4-mini OpenAI [2025c] 20.9 OpenAI o3 OpenAI [2025c] 25.8 8.1 9.1 11.0 8.8 11.7 11.2 11.5 11.0 11.0 13.9 14.6 19.5 Human Experts 11.4 11.4 15.9 13.1 14.2 15.9 9.1 15.9 18.8 17.6 22.7 20.5 15.3 21.0 19.9 18.2 29.0 20.5 24.4 30.1 34.7 38.6 42.6 43.2 48.3 1.9 1.3 1.9 2.5 2.5 0.6 4.5 1.3 3.2 1.3 2.5 0.6 3. 5.7 3.8 2.5 6.4 5.7 3.2 8.9 5.7 13.4 9.6 12.7 20.4 2.1 5.1 5.6 3.6 4.6 5.6 4.6 6.2 4.1 3.6 4.1 5.1 6.2 6.2 8.7 11.8 10.3 11.3 13.3 12.8 12.3 9.7 19.0 17.4 22.1 7.7 11.6 8.7 12.0 10.9 11.6 14.5 11.3 10.9 13.8 10.2 12.4 14.5 10.9 12.4 15.3 9.1 15.3 14.2 12.0 13.1 10.5 12.7 13.8 17.1 Human Experts Score 82.3 81.7 84.7 69.4 81.0 89. parameters at their default values. The above experiments are conducted on 8 NVIDIA H20 GPUs. For closed-source models, we follow the official API usage guidelines provided for each model. If the official API allows setting the temperature parameter, we set it to 0; all other parameters are kept as recommended by the official documentation. For the human expert score, we invite senior undergraduate students to serve as human experts. For physics and math, we recruit some senior undergraduates for each major, assigning them different sets of questions. For games and counting tasks, we similarly invite some senior undergraduates, without restricting their academic backgrounds. The experimental results are summarized in Tab. 3, and comprehensive analysis is presented in Sec. 4.4. 4.3 Visualization Here, we visualize correct example and an incorrect example: the correct case is from planar geometry in math, while the incorrect case is from games. The answer comes from the representative model, o3, the results are shown in Fig. 4. From the results, we observe that although o3 arrives at the correct answer for the planar geometry question, its solution is based on an algebraic approach by establishing coordinate system, rather than using typical geometric method commonly adopted by humans. This suggests that the model tends to favor algebraic solutions with text-only reasoning thought over multi-modal geometric reasoning path when both approaches are available. It indicates that improvements in mathematical 7 Figure 4: Examples of o3s responses to math and game questions in RBench-V. Left: o3 correctly answers math question in RBench-V by transforming the geometry problem into an algebraic one using coordinate system, whereas humans typically solve it using geometric methods. Right: o3 fails to answer game question correctly. The blue highlights indicate the cause of the error and the key issue is that the model fails to follow the instructions to draw the required connections. performance do not necessarily reflect genuine advancements in multimodal reasoning ability, but rather suggest that models have learned certain multi-modal reasoning shortcuts. Experts in mathematics have validated this hypothesis, pointing out that most geometry problems can be solved using algebraic methods. In contrast, counting, and games do not exhibit such multimodal reasoning shortcuts. Therefore, we also report the performance under the w/o math setting in Table 3, which may serve as better indicator of models true multimodal reasoning capability. In the second question, derived from connect-the-dots task in the games category, o3 fails to generate correct answer. Analysis reveals that the errors here mainly stem from o3 merely attempting to describe the points in the diagram, rather than actually connecting them as required by the question. Due to space limitations, we are unable to present more examples, but our analysis shows that the majority of model failures are caused by this limitation. 4.4 Observation and findings If models, such as the InternVL or Qwen-VL series, lack multi-modal CoT, merely increasing their model sizes will not effectively resolve the challenge of visual reasoning. As shown in Tab. 3, increasing the parameter size of the Qwen2.5VL model from 7B to 72B does not result in clear performance improvement on RBench-V. similar phenomenon is also observed in the InternVL and LLaVA-OneVision series. It suggests that the scaling law may be insufficient to address the challenges of multi-modal output in visual reasoning. Furthermore, we question whether foundation models trained primarily via next-token prediction are inherently limited in their ability to handle such tasks. While this training paradigm is well-suited for text generation, it may be fundamentally inadequate for detailed and precise multi-modal generation and understanding such as precisely tracing curve trajectory in mazes. 8 Omni-models and long text-only CoT approaches also do not show significant improvements on this task. As shown in Table 3, comparison between Qwen2.5VL-7B and Qwen2.5VL-Omni-7B, as well as between MiniCPM-V-2.6 and MiniCPM-o-2.6, indicates that simply incorporating images during output decoding, as done in omni-models, fails to effectively address the multimodal reasoning challenge. In addition, typical long text-only thinking models also show only marginal gains on RBench-V, as evidenced by the comparison between Double1.5-vision-pro and Doubao1.5-thinkingpro-m. Combining our analysis on scaling laws, omni-models, and long text-only CoT approaches, indicating that for current foundation models, novel techniques such as M-CoT and agents can be required to effectively solve visual reasoning tasks involving precise multimodal outputs. Foundation models still fall well short of human expert performance in generating multi-modal outputs during visual reasoning. As shown in Table 3, even the best-performing model to date, o3, achieves only an overall accuracy of 25.8%, which remains significantly behind the human expert score of 82.3%. This substantial performance gap underscores the limitations of current foundation models in handling tasks that demand precise multi-modal outputs in the visual reasoning process. This phenomenon is clearly illustrated by the bar chart in Fig. 1. The results emphasize that, despite recent progress, there is still considerable room for advancement in multimodal reasoning. The methods used by human experts and models to solve problems are not consistent. As shown in the Tab. 3, various models tend to perform best on the mathematics subject. We analyze and present representative examples from mathematics in Fig. 4, revealing that models often convert geometric problems into algebraic ones by constructing coordinate systems. This approach differs significantly from that of human experts, who typically solve such problems using geometric reasoning. It suggests that the intelligence of current models differs from that of humans. Therefore, to avoid such multimodal reasoning shortcuts, we also report the accuracy after removing math-related questions in Tab 3. The results show that excluding math further amplifies the performance gap between models and human experts. OpenAI o3 has made substantial progress in visual reasoning with multi-modal output. The release of o3 attracted widespread attention, largely due to its impressive ability to handle complex visual reasoning tasksa capability that has been challenging for previous models. We observed the same phenomenon in our proposed RBench-V, where o3 significantly outperformed all other models in tasks that require accurate and coherent multimodal outputs. This performance lead suggests that o3 has undergone deliberate and effective enhancements specifically aimed at improving its visual reasoning and output alignment capabilities. Notably, the results also validate the design of RBench-V itself. It demonstrates RBench-V can serve as reliable benchmark for evaluating progress and tracking how models are evolving toward human-level multimodal reasoning. Open-source models still lag far behind closed-source models. Although open-source models such as Qwen2.5VL Bai et al. [2025] and LLaMA 4 Meta [2025] are making continuous progress, there remains noticeable gap (10.6% vs. 25.8%) between open-source and closed-source models in visual reasoning tasks that require multi-modal outputs. In addition, we find that the performance of current open-source models is quite similar, with low accuracy rates mostly ranging between 8% and 10%. It suggests that current open-source models exhibit only minimal capability in multi-modal reasoning. We hope the community will develop new techniques based on open-source models to enhance their multi-modal output capabilities and ultimately close the gap with closed-source models on RBench-V."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we carefully hand-pick 803 questions across 4 topics and propose RBench-V, benchmark specifically designed to evaluate models multimodal output capabilities in the visual reasoning process. It systematically assesses the current performance of models, highlights the progress made by the o3 model in this domain, and reveals the significant gap between current intelligent models and human experts. Besides, according to our observation, the current technologies such as scaling law, long text-only CoT and joint text-visual decoding, fail to effectively address the challenges posed by RBench-V. Looking ahead, we hope that foundation models will evolve towards the direction of omni reasoning models and achieve better performance on RBench-V. In addition, we will also try to improve the foundation models from the perspective of M-CoT and the agent."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Anthropic. Introducing claude 3.5 sonnet. https://www.anthropic.com/news/claude-3-5-sonnet, 2024. Anthropic. Claude 3.7 sonnet. https://www.anthropic.com/claude/sonnet, 2025. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https://github.com/open-compass/opencompass, 2023. DeepSeek. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM international conference on multimedia, pages 1119811201, 2024. Betty Edwards. Drawing on the right side of the brain: The definitive. Penguin, 2012. Judith Fan, Wilma Bainbridge, Rebecca Chamberlain, and Jeffrey Wammes. Drawing as versatile cognitive tool. Nature Reviews Psychology, 2(9):556568, 2023. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. Gabriela Goldschmidt. The dialectics of sketching. Creativity research journal, 4(2):123143, 1991. Google. Gemini 2.5: Our most intelligent ai model. https://blog.google/technology/googledeepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking, 2025. Google, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Google, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Meng-Hao Guo, Jiajun Xu, Yi Zhang, Jiaxi Song, Haoyang Peng, Yi-Xuan Deng, Xinzhi Dong, Kiyohiro Nakayama, Zhengyang Geng, Chen Wang, Bolin Ni, Guo-Wei Yang, Yongming Rao, Houwen Peng, Han Hu, Gordon Wetzstein, and Shi min Hu. R-bench: Graduate-level multidisciplinary benchmarks for llm & mllm complex reasoning evaluation, 2025. URL https: //arxiv.org/abs/2505.02018. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 10 Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Sergey Kastryulin, Jamil Zakirov, Denis Prokopenko, and Dmitry Dylov. Pytorch image quality: Metrics for image quality assessment. arXiv preprint arXiv:2208.14818, 2022. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Meta. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. https://ai.meta.com/blog/llama-4-multimodal-intelligence/, 2025. OpenAI. Introducing chatgpt. https://openai.com/index/chatgpt/, 2022. OpenAI. Gpt-4v(ision) system card. https://openai.com/index/gpt-4v-system-card/, 2023. OpenAI. Gpt-4o system card. https://openai.com/index/gpt-4o-system-card/, 2024a. OpenAI. Learning to reason with llms. https://openai.com/index/learning-to-reason-with-llms/, 2024b. OpenAI. Introducing gpt-4.1 in the api. https://openai.com/index/gpt-4-1/, 2025a. OpenAI. Introducing gpt 4.5. https://openai.com/index/introducing-gpt-4-5/, 2025b. OpenAI. Openai o3 and o4-mini system card. https://openai.com/index/o3-o4-mini-system-card/, 2025c. Zenon Pylyshyn. Visual indexes, preconceptual objects, and situated vision. Cognition, 80(1-2): 127158, 2001. Qwen. Qvq-max: Think with evidence. https://qwenlm.github.io/blog/qvq-max-preview/, 2025. Seed. Doubao-1.5-pro. https: // seed. bytedance. com/ en/ special/ doubao _1 _5 _pro/ , 2025a. Seed. Seed1.5-thinking: Advancing superb reasoning models with reinforcement learning, 2025b. URL https://arxiv.org/abs/2504.13914. StepFun. Step-r1-v-mini. https://www.stepfun.com/chats/new, 2025. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804, 2024a. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024b. 11 Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao Fei. Multimodal chain-of-thought reasoning: comprehensive survey. arXiv preprint arXiv:2503.12605, 2025. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024c. Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Zhiyuan You, Jinjin Gu, Zheyuan Li, Xin Cai, Kaiwen Zhu, Chao Dong, and Tianfan Xue. Descriptive image quality assessment in the wild. arXiv preprint arXiv:2405.18842, 2024. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024a. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024b. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Stanford University",
        "Tencent Hunyuan X",
        "Tsinghua University"
    ]
}