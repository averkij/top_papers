{
    "paper_title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model",
    "authors": [
        "Qingyu Shi",
        "Jinbin Bai",
        "Zhuoran Zhao",
        "Wenhao Chai",
        "Kaidong Yu",
        "Jianzong Wu",
        "Shuangyong Song",
        "Yunhai Tong",
        "Xiangtai Li",
        "Xuelong Li",
        "Shuicheng Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unified generation models aim to handle diverse tasks across modalities -- such as text generation, image generation, and vision-language reasoning -- within a single architecture and decoding paradigm. Autoregressive unified models suffer from slow inference due to sequential decoding, and non-autoregressive unified models suffer from weak generalization due to limited pretrained backbones. We introduce Muddit, a unified discrete diffusion transformer that enables fast and parallel generation across both text and image modalities. Unlike prior unified diffusion models trained from scratch, Muddit integrates strong visual priors from a pretrained text-to-image backbone with a lightweight text decoder, enabling flexible and high-quality multimodal generation under a unified architecture. Empirical results show that Muddit achieves competitive or superior performance compared to significantly larger autoregressive models in both quality and efficiency. The work highlights the potential of purely discrete diffusion, when equipped with strong visual priors, as a scalable and effective backbone for unified generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 6 0 6 3 2 . 5 0 5 2 : r Muddit: Liberating Generation Beyond Text-to-Image with Unified Discrete Diffusion Model Qingyu Shi1,2, Jinbin Bai2,3, Zhuoran Zhao3, Wenhao Chai4, Kaidong Yu2, Jianzong Wu1, Shuangyong Song2, Yunhai Tong1, Xiangtai Li1, Xuelong Li2, Shuicheng Yan3 Equal Contribution, Project Lead, Corresponding Authors"
        },
        {
            "title": "Abstract",
            "content": "Unified generation models aim to handle diverse tasks across modalitiessuch as text generation, image generation, and vision-language reasoningwithin single architecture and decoding paradigm. Autoregressive unified models suffer from slow inference due to sequential decoding, and non-autoregressive unified models suffer from weak generalization due to limited pretrained backbones. We introduce Muddit, unified discrete diffusion transformer that enables fast and parallel generation across both text and image modalities. Unlike prior unified diffusion models trained from scratch, Muddit integrates strong visual priors from pretrained text-to-image backbone with lightweight text decoder, enabling flexible and high-quality multimodal generation under unified architecture. Empirical results show that Muddit achieves competitive or superior performance compared to significantly larger autoregressive models in both quality and efficiency. The work highlights the potential of purely discrete diffusion, when equipped with strong visual priors, as scalable and effective backbone for unified generation. The code and model are available at https://github.com/M-E-AGI-Lab/Muddit."
        },
        {
            "title": "Introduction",
            "content": "Multimodal generative models capable of handling both text and images have rapidly advanced, typically relying on large autoregressive (AR) Transformers, also known as large language models (LLMs) [52]. These unified models represent both modalities as token sequences and generate outputs in left-to-right autoregressive manner. However, this sequential decoding imposes major inference bottleneck. For instance, in early unified transformers [46], as illustrated in Fig. 1(a), generating single image requires sampling thousands of visual tokens one at time. Despite strong correlation among adjacent image tokens, each token prediction triggers full network forward, resulting in significant redundant computation. As result, inference becomes extremely slow and compute-intensive. We refer to this as the first dark cloud over current unified generative models. Moreover, AR decoding enforces rigid generation order. This prevents speed-quality trade-offs or flexible conditional generation like inpainting without fine-tuning, which severely limits practical applicability in interactive or real-time scenarios. To mitigate these limitations, some hybrid approaches [9, 11, 41], adopt AR language models paired with diffusion-based image synthesis heads (Fig. 1(b)). However, these glue architectures fall short of true unification, as they lack shared generative modeling paradigm across modalities. Recent work like Dual-Diffusion [29] (Fig. 1(c)) claims to unify modalities under discrete diffusion, but it ultimately relies on continuous diffusion for image generation via Stable Diffusion 3, 1Peking University, 2TeleAI, China Telecom, 3National University of Singapore, 4Princeton University (cid:66): jinbin.bai@u.nus.edu Preprint. Under review. Figure 1: Four types of unified generative models. More details can be found in Sec. 2. continuous diffusion paradigm. This fundamental mismatch in generative principles undermines its claim of true unification. UniDisc [48](Fig. 1(d)), takes more promising step by applying discrete diffusion1 over unified token spaces. This allows parallel refinement of text and image tokens, improving inference efficiency and enabling more flexible conditioning. However, the overall generation quality of UniDisc remains far from satisfactory. For example, it struggles to produce high-resolution 1024 1024 images, fails to match the fidelity of early diffusion models such as Stable Diffusion 1.5, and lacks support for vision-language reasoning tasks such as visual question answering (VQA). These limitations expose the second dark cloud: the absence of strong pretrained discrete diffusion backbone models: Unlike established unified autoregressive models that leverage powerful pretrained large language models, current unified discrete diffusion models are typically trained from scratch on mixed-modality tokens, which limits both their generative fidelity and transferability. Without modular components carrying rich pixel-level priors, these models face generalization and scalability bottlenecks. Taken together, the two dark clouds: inefficient autoregressive sampling and the lack of strong pretrained foundations, highlight the need for new generation of unified models. In this work, we present Muddit, MaskGIT-style unified discrete diffusion transformer equipped with lightweight text decoder. By combining the strengths of parallel discrete diffusion and semantically rich image priors from pre-trained Meissonic text-to-image backbone [5], Muddit enables scalable, efficient, and flexible sampling while significantly improving alignment and quality across modalities and various tasks such as high-resolution text-to-image synthesis, image-to-text synthesis, and visual question answering. We systematically detail the training objective of unified discrete diffusion models, the masking strategy, and the shared inference sampling strategy across three tasks. Finally, we conduct comprehensive evaluations with current popular unified models on several benchmarks, including GenEval, CIDEr, VQAv2, MME, and GQA, demonstrating Muddits superior performance and efficiency, validating that the unexplored purely discrete diffusion approach can rival, or even surpass, much larger autoregressive-based unified models. While concurrent unified generation models [57] often build upon language modeling priorleveraging pretrained dLLMs as the backbonewe instead take visual-first approach. Muddit is built upon an image generation prior, offering new path toward unifying vision and language tasks within discrete diffusion framework. We hope that this work inspires new trend for unified generative modeling, grounded in discrete diffusion, beyond the boundaries of traditional text-to-image synthesis [5] and text synthesis [25, 39]."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Unified Models For Generation and Understanding The success of LLMs in language modeling has inspired efforts to extend unified generation to multimodal domains. However, the divergence between autoregressive and diffusion-based paradigms presents fundamental architectural trade-offs. Autoregressive models naturally handle language, and several works [11, 16, 20, 47, 51, 53] extend this by connecting vision modules to LLMs via adapters or instruction tuning, with LLMs serving as planning modules that produce intermediate representations for image generation. While effective to some extent, these paradigms often exhibit limited interaction between text and image modalities and struggle with content consistency, particularly in image-to-image generation and complex instruction-based synthesis. To address these limitations, 1MaskGIT, MaskAR, RandomAR, and Discrete Diffusion share significant conceptual and practical overlaps, often differing only in decoding order or architectural nuances. We elaborate on their connections in the next section. While Meissonic [5] follows the naming convention of MaskGIT [8], we standardize terminology in this paper by referring to all such models under the umbrella of Discrete Diffusion. 2 recent research explores unified generation models that integrate understanding and generation within single architecture, We categorize these into four major paradigms (see Fig. 1): Fully Autoregressive: Both text and image are tokenized into discrete sequences and modeled with an AR Transformer [13, 22, 32, 34, 50, 54, 55, 59]. These models achieve strong cross-modal generation but suffer from high latency due to sequential decoding. Text AR, Image Diffusion: LLMs generate text tokens while image synthesis is delegated to pretrained continuous diffusion backbones [38, 58, 60] or discrete diffusion [56]. Though visually strong, these models are not truly unified, as they rely on separate architectures and token spaces. Image Diffusion, Text Discrete Diffusion: Emerging models experiment with discrete diffusion for text and images [29], though many, like Dual-Diffusion, still use continuous diffusion for image synthesis, failing to realize true modality symmetry. Fully Discrete Diffusion: Recent work like UniDisc [48] pioneers full-token discrete diffusion over shared Transformer backbones. These models support parallel sampling and native integration, but currently lag behind in generation fidelity and scale. Among these, the GPT-4o [40] model represents significant advance as unified multimodal generative system. However, its closed-source nature obscures critical architectural and training details, and its success may be largely attributable to scale rather than architectural novelty [12]. 2.2 Masked Image Modeling Masked Image Modeling (MIM) has emerged as powerful self-supervised learning paradigm in computer vision, drawing inspiration from the success of Masked Language Modeling (MLM) in NLP, notably BERT [15]. The fundamental principle of MIM involves obscuring portions of an image, which could be raw pixels (MAE [23]), latent patches of pixels, or even discrete latent tokens (BEiT [6], MaskGIT [8]), and training model, typically an autoencoder, to predict or reconstruct this missing information by leveraging the context provided by the visible parts. MaskGIT [8] introduced parallel decoding via iterative token refinement, inspiring discrete diffusion models. Recent work such as RandomAR [18] and MAR [28] formalize this as random-order or masked autoregressive generation, blending AR and MIM principles. The major conceptual difference between RandomAR/MAR and MaskGIT is in the scanning order at inference time. This class of techniques forms the conceptual foundation of discrete diffusion over tokenized spaces and plays critical role in modern unified models. We will introduce discrete diffusion in the next section."
        },
        {
            "title": "3 Method",
            "content": "3.1 Discrete Diffusion with Unified Image and Text Perspective In discrete diffusion, sample is treated as one-hot vector x, where = {1, . . . , }. For language models, equals the vocabulary size. While for image models, is the number of discrete imagetoken IDs obtained from tokenizer or VQ-codebook. At each diffusion step, we stochastically corrupt the tokens, gradually transforming the data distribution into maximally entropic categorical prior; the generative model then learns to invert this corruption. Following recent works [5, 36] that cast token corruption as continuoustime Markov chain (CTMC) over the finite alphabet , we let pt dt = Qt pt, (1) where pt RN +1 is the distribution of xt, the timedependent matrix Qt transports the data distribution p0 pdata to the maximally entropic noise distribution p1 = pstationary. We adopt the absorbing-state (masked) diffusion variant that has proved particularly effective in text modelling: , 1) but never leaves it, i.e. is an every symbol can jump to dedicated mask token = (0, . . . , 0 (cid:124) (cid:123)(cid:122) (cid:125) absorbing class. 3 Forward posterior. Marginalising gives q(xt x) = Cat(cid:0)xt αtx + (1 αt)m(cid:1). Cat() denotes categorical distribution; it returns one-hot token sampled from the probability vector inside the parentheses. αt [0, 1] is the survival probability, i.e. the probability that an individual token has not yet been masked by time t. Thus xt equals the original clean token with probability αt and equals the mask token with probability 1 αt. (2) Reverse process. For any 0 < < < 1, the CTMC induces an analytic posterior q(xs xt, x) = Cat(xs xt), xt = m, (cid:16) Cat xs (1 αs)m + (αs αt)x 1 αt (cid:17) , xt = m, (3) xt and xs are the corrupted tokens at times and (s < t). If xt is already real vocabulary token (xt = m) it stays unchanged going backwards; otherwise, when xt = m, the distribution over xs is convex combination of the mask and the clean token x, weighted by their respective survival probabilities αs and αt. Training Objective. We employ masked-token predictor xθ(xt, αt) x, which leads to the continuous-time negative ELBO LNELBO = Eq(xtx) (cid:104)(cid:90) 1 0 α 1 αt (cid:105) log(cid:0)xθ(xt, αt)x(cid:1) dt , (4) = dαt dt and is the one-hot vector of ground truth. xθ(xt, αt) RN +1 is the models where α predicted categorical probability vector for the clean token given the corrupted input (xt, αt); is the one-hot ground-truth clean token. During generation, we start from an all-mask sequence (t = 1) and integrate the reverse CTMC towards = 0, repeatedly replacing every masked position with the models categorical prediction. Because the corruption schedule and objective are identical for any discrete alphabet , the same diffusion backbone unifies text and image generation. In the following section, we present Muddit, unified framework that leverages discrete diffusion to model the generation tasks for both text and image jointly. 3.2 Muddit 3.2.1 Unified Architecture As shown in Fig. 2, our architecture comprises text encoder Etxt, image encoder Eimg, transformer generator G, sampler S, text decoder Dtxt, and image decoder Dimg. The generator is single MM-DiT model, following the dual-/single-stream design of FLUX [26]. Importantly, the generator is initialized from the Meissonic [5], which has been extensively trained for high-resolution text-toimage generation. This initialization brings in strong pretrained image prior, capturing rich spatial structures and semantic correlations across image and text tokens, which significantly enhances sample quality and accelerates convergence in the multimodal setting. Consequently, the same MM-DiT predicts the masked tokens for both modalities, which produces shared generator for text and image synthesis. To reduce the computational cost of high-resolution imagery and lengthy captions, we quantize both modalities into compact discrete space. pre-trained VQ-VAE acts as the image encoder Eimg, mapping pixels to codebook indices, while the CLIP text model, as Etxt, provides the text token embeddings. The MM-DiT predicts clean tokens in this shared space, which lightweight linear head Dtxt converts back to text tokens. 3.2.2 Unified Training Masking Strategy. We model the forward posterior in Eq. 2 of both modalities using time-dependent hyperparameters αt, with the mask ratio defined as γt = 1 αt. While BERT [15] employs fixed mask ratio of 15%, this setting is suitable for token completion but insufficient for generation. To support generative tasks, the design of γt must satisfy the following criteria: Figure 2: The training and inference architecture of Muddit. (a) During training, we randomly mask tokens from one of the two modalities. MM-DiT is trained to predict the masked tokens using re-weighted cross-entropy loss, which jointly optimizes both the MM-DiT backbone and lightweight text decoder. (b) In text-to-image inference, we initialize the image latent features using all-masked tokens and iteratively predict each latent token via MM-DiT. (c) In image-to-text inference, we similarly initialize all text tokens as masked and generate the text through the same iterative decoding process. Specifically for VQA tasks, we append mask token IDs to the end of the question and predict all masked token IDs as the final answer. 1. γt must be continuous function, bounded between 0 and 1, for [0, 1]. 2. γt should monotonically decrease with respect to t, with boundary conditions γ0 0 (initially clean data) and γ1 1 (masking all tokens). Several strategies for masking and sampling have been proposed to meet these criteria [8]. We adopt cosine scheduling strategy. During training, timestep [0, 1] is sampled from truncated arccos distribution, with the density function: γt = 2 π (1 (1 t)2) 1 2 . (5) During training, mask ratio γt [0, 1) is randomly sampled for each modality x0 (either image or text tokens), and the forward process (Eq. 2) is applied by randomly replacing clean tokens with mask tokens to obtain xt. Unified Training Objective. Let denote the conditioning: the text embedding when synthesizing an image, or the image embedding when generating caption. We randomly sample mask ratio by Eq. 5. Then we corrupt the target sequence x0 (image or text tokens) with the CTMC described in Eq. 1 and train single masked-token predictor G(xt, αt, c) to reconstruct x0. Both directionstext image and image textshare the identical continuous-time negative ELBO Lunified = Eq(xtx) (cid:104)(cid:90) 1 0 α 1 αt (cid:105) log(cid:0)G(xt, αt, c)x(cid:1) dt , (6) where all symbols are as in Eq. 4 but the now receives the cross-modal condition as an additional input. Key point: switching from text image to image text merely changes the conditioning signal c; the loss Eq. 6 itself is unchanged. This symmetry keeps optimization identical across tasks and allows us to train single parameter set jointly for both generation directions. During inference we again start from an all-mask sequence (t=1) and integrate the reverse CTMC towards t=0, feeding in the desired condition to obtain either an image or sentence from the same diffusion backbone. 5 3.2.3 Unified Inference Sampling Strategy. During inference, we apply the time-reversed posterior as defined in Eq. 3. S(G, xt, t) = pθ(xs xt) = Cat(xs xt), xt = m, (cid:16) Cat xs (1 αs)m + (αs αt)G(xt, αt, c) 1 αt (cid:17) , xt = m, (7) where θ denotes the parameters of G, is the multimodal condition, and αt in Eq. 5 is applied , . . . , 1 sequentially with taking values 1, 1 , where is the total number of reverse steps. At each timestep t, Muddit predicts fraction γt+ 1 γt of the masked tokens by and update the masked tokens xt by , continuing iteratively until all masked tokens are recovered. This dynamic approach offers several advantages over autoregressive methods, which require the model to learn conditional probabilities (xi x<i) based on fixed token ordering. In contrast, random masking with variable ratio enables the model to learn (xi xΛ), where Λ denotes an arbitrary subset of observed tokens. This flexibility is essential for parallel sampling, allowing multiple tokens to be predicted simultaneously rather than sequentially. Our Muddit supports three tasks with single generator and sampler S: (i) text image, (ii) image text (captioning), and (iii) visualquestion answering (VQA). The only change across tasks is the conditioning source provided to G; the diffusion process and guidance logic are shared. (i) Text image. Given text prompt tp , the text encoder Etxt produces text token embedding ctxt = Etxt(tp). Starting from fully masked sequence x1, the generator produces logits lt = G(xt, αt, ctxt), xt 1 = S(lt, xt, t), (8) , . . . 1 . After steps we obtain visual tokens x0, which the image decoder Dimg for = 1, 1 converts to pixel-space image = Dimg(x0). (ii) Image text. For captioning, an input image is tokenized by the image encoder Eimg: cimg = Eimg(I). The generator now conditions on the visual tokens while progressively decoding text: yielding text token sequence x0, which Dtxt maps to caption caption = Detokenize(Dtxt(x0)). lt = G(xt, αt, cimg), tt = S(lt, xt, t), (9) (iii) Image + question answer (VQA). For visualquestion answering we supply both an image and question: cimg = Eimg(I) and ctxt = Etxt(q). They are concatenated and fed to the generator, which outputs logits over answer tokens xk: lt = G(xt, αt, [cimg, ctxt]), xt = S(lt, xt, t), (10) until the full answer is produced and decoded by = Detokenize(Dtxt(x0)). Classifier-free guidance. At each decoding step, we apply the same guidance rule, independent of modality: lk G(zk, αk, c) + λ(cid:2)G(zk, αk, c) G(zk, αk, cneg)(cid:3), (11) where zk (image or text tokens) is the partial target sequence, is the positive condition (prompt, image, or image +question), cneg is the corresponding negative condition, and λ is the guidance scale. Because the loss, decoding schedule, and guidance operator are identical in all three scenariosonly the conditioning signal changesour framework realises genuinely unified multimodal generator."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Experimental Setup Implementation details. We build Muddit on top of the open-sourced Meissonic models [5]. The MM-DiT backbone is initialized with pretrained weights, and lightweight linear head is added as text decoder. Following Meissonic, we adopt the CLIP tokenizer and encoder [43], as well as the VQ-VAE, keeping them entirely frozen throughout all experiments, including the mask token 6 Text Gen Arch Image Gen Arch Params (B) Overall Model PixArt-α [10] SD 2.1 [45] DALL-E 2 [44] SDXL [42] DALL-E 3 [7] SD 3 [17] LWM [35] SEED-X [20] Chameleon [50] Show-O [56] Transfusion [60] D-DiT [30] - - - - - - Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion AR AR AR AR AR AR AR AR Discrete Diff. Diffusion Discrete Diff. Diffusion Monetico (512 512) [5] Discrete Diff. Meissonic (1024 1024) [5] Discrete Diff. UniDisc (512 512) [49] Discrete Diff. Discrete Diff. Muddit (512 512) Discrete Diff. Discrete Diff. - - Objects Single Two 0.98 0.50 0.98 0.51 0.94 0.66 0.98 0.74 0.96 0.87 0.98 0.74 0.93 0.41 0.97 0.58 - - 0.98 0.80 - - 0.97 0.80 0.92 0.48 0.99 0.66 0.92 0.47 0.98 0.72 Counting Colors Position Color Attribution 0.44 0.44 0.49 0.39 0.47 0. 0.46 0.26 - 0.66 - 0.54 0.26 0.42 0.15 0.54 0.80 0.85 0.77 0.85 0.83 0.67 0.79 0.80 - 0.84 - 0.76 0.78 0.86 0.67 0.82 0.08 0.07 0.10 0.15 0.43 0. 0.09 0.19 - 0.31 - 0.32 0.06 0.10 0.13 0.19 0.07 0.17 0.19 0.23 0.45 0.36 0.15 0.14 - 0.50 - 0.50 0.13 0.22 0.19 0.41 0.6 0.9 6.5 2.6 - 7 17 7 1.3 8 2 1 1 1.4 1 0.48 0.50 0.52 0.55 0.67 0.62 0.47 0.49 0.39 0.68 0.67 0.65 0.44 0.54 0.42 0.61 Table 1: Evaluation of text-to-image generation performance on the GenEval [21]. embedding in CLIP. To support discrete denoising, we append special <mask> token to CLIPs vocabulary for text masking, while the image mask token is inherited directly from Meissonics initialization. During training, we use constant learning rate of 1 104 and weight decay of 1 102. Gradient accumulation is applied in both pretraining and supervised fine-tuning, resulting in an effective batch size of 1024. During inference, we adopt the default Meissonic configuration, using cosine masking scheduling, 64 sampling steps, and classifier-free guidance (CFG) scale of 9.0 for both text-to-image and image-to-text generation. Training Data. We train Muddit in two stages using mix of publicly available and internal datasets, comprising approximately 3.5 million imagetext pairs. Both stages are optimized using the unified training objective defined in Eq. 6. Below, we detail the datasets and settings for each stage: 1. Pretraining. We pretrain Muddit for 70K steps with batch size of 1024, using the unified objective across both modalities. Text inputs are truncated to maximum of 77 tokens, and images are resized to 512 512. The pretraining corpus consists of 2 million imagetext pairs, re-captioned using Qwen2.5-VL-3B for improved consistency. Each batch is evenly split between text-to-image and image-to-text samples to enable joint training in both directions. 2. Supervised Fine-tuning. After pretraining, we fine-tune the model on combination of instruction-following datasets, including LLaVA-Instruct-150K and the MG-LLaVA tuning set. During this stage, only the answer portion of each prompt is masked. Additionally, we construct curated dataset of 500K high-quality imagetext pairs to support multi-task training on VQA and image generation. Following the task instructions embedded in each sample, Muddit learns to produce long-form answers, concise replies, and image captions via task-specific prompting. We present both quantitative and qualitative results for T2I and I2T tasks in the following sections. Additional experiments and ablation studies are provided in the Appendix. 4.2 Text-to-Image Generation Quantitive Results. Following prior work, we evaluate our 512 512 model on the GenEval [21] benchmark after supervised fine-tuning, measuring its ability to generate images aligned with textual prompts. As shown in Tab. 4.2, Muddit achieves strong overall accuracy of 0.61, outperforming previous discrete diffusion models such as Monetico (0.44) and Meissonic (0.54), and approaching the performance of Stable Diffusion 3 (0.62), despite using only 1B parameters. Muddit exhibits strong compositional reasoning, scoring 0.72 on the \"Two Objects\" subset and 0.54 on \"Counting\". Notably, we observe that joint training across modalities significantly enhances the text-to-image generation capabilities of the Meissonic backbone. These results highlight the potential of Muddit as the first unified model to adopt pure discrete diffusion framework for both text and image modalities, achieving competitive quality with compact, scalable architecture. Qualitative Results. We present diverse generations from our model conditioned on rich textual prompts in Fig. 3. The outputs exhibit strong text-image alignment, capturing fine details in both 7 Figure 3: Samples of Text-to-Image Generation by Muddit. Figure 4: Samples of Visual Question Answering by Muddit. realistic and imaginative scenes. Our model effectively renders complex structures, lighting, and textures across various domains. 4.3 Image-to-Text Generation We present comprehensive comparison of our model Muddit against other multimodal models across four benchmarks: MS-COCO (image captioning) [31], VQAv2 [2], MME [19], and GQA [24] in Tab. 4.3. Notably, Muddit is the first unified model to employ discrete diffusion for both image and text generation, demonstrating that this approach is not only viable but also highly competitive. 8 Figure 5: Samples of Image-to-Text Generation by Muddit. Model InternVL-2.0 [14] LLaVA-Next [33] BLIP-2 [27] QWEN-VL [4] OpenFlamingo [3] Flamingo [1] Chameleon [50] LWM [34] Show-O (256256) [56] Show-O (512512) [56] Transfusion [60] D-DiT (256256) [29] D-DiT (512512) [29] UniDisc [49] Muddit (512512) Params (B) 8 13 13 7 9 9 7 7 1.3 1.3 7 2 2 0.33 1 Text Gen Arch AR AR AR AR AR AR AR AR AR AR AR Discrete Diff. Discrete Diff. Discrete Diff. Discrete Diff. Discrete Diff. Discrete Diff. Image Gen MS-COCO VQAv2 MME GQA Acc. Acc. 61.0 1648.1 65.4 1575.0 41.0 1293.8 57.5 1487.5 - - - - - - 44.8 - 54.2 1014.9 58.0 1097.2 - - 55.1 897.5 59.2 1124.7 - - 57.1 1104.6 Arch - - - - - - AR AR Discrete Diff. Discrete Diff. Diffusion Diffusion Diffusion CIDEr - - - - 65.5 79.4 18.0 - - - 29.0 - 56.2 46.8 59. Acc. - 82.8 65.0 78.2 43.5 51.8 - 55.8 64.7 69.4 - 59.5 60.1 - 67.7 Table 2: Evaluation of image-to-text generation and visual question answering. Quantitive Comparison. Despite having only 1B parameterssubstantially fewer than most competing modelsMuddit achieves strong performance across both image captioning and visual question answering tasks. It obtains CIDEr score of 59.7 on MS-COCO, surpassing larger models such as Show-O and D-DiT. On the VQAv2 benchmark, it reaches 67.7%, outperforming other diffusion-based models like D-DiT (512512) and approaching the performance of much larger autoregressive models such as LLaVA-Next (13B). Muddit also demonstrates competitive results on MME and GQA (1104.6 and 57.1 accuracy, respectively), highlighting its capability as unified model without compromising task-specific quality. Qualitative Results. We present example captions generated by our model across diverse scenarios in Fig. 5, including humans, animals, vehicles, and natural landscapes. The model demonstrates strong visual grounding and fine-grained descriptive ability, accurately capturing attributes such as 9 Metric GenEval MS-COCO CIDEr VQAv2 0.2 60.1 50.2 62.1 0.4 60.5 51.2 65.8 0. 61.6 58.4 67.8 0.8 60.8 58.3 67.9 Metric w/o joint training w/ joint training GenEval MS-COCO CIDEr VQAv2 28.3 59.4 69.2 61.6 58.4 67.8 Table 3: Impact of text loss weight. Table 4: Effect of joint training. Timestep T=8 T=16 T=24 T=32 T=40 T=50 T=64 GenEval MS-COCO CIDEr VQAv2 51.4 43.4 68.3 58.1 58.5 68.4 59.2 58.6 68.5 61.6 58.4 67. 61.5 59.2 67.5 61.4 60.0 67.6 60.8 59.7 67.7 Table 5: Performance across different diffusion timesteps. clothing, expressions, background context, and object relationships. Fig. 4 illustrates our models ability to accurately answer visual questions across various domains, including object counting, color recognition, material identification, and compositional reasoning. 4.4 Ablation Study and Analysis Analysis of the inference timesteps. As shown in Tab. 5, increasing the number of diffusion steps generally improves performance, with most metrics plateauing around = 3250. In particular, GenEval and CIDEr scores improve substantially from = 8 to = 32, though the marginal gains diminish thereafter. VQAv2 remains largely stable across timesteps, suggesting that fewer steps suffice for discriminative tasks. Overall, moderate number of steps offers favorable trade-off between accuracy and efficiency. Analysis of the text loss weight. As shown in Tab. 3, moderate text loss weights (approximately 0.6) yield the best overall performance. Both CIDEr and GenEval scores peak near this value, indicating that placing either too little or too much emphasis on text can impair generation quality. Notably, VQAv2 performance continues to improve with increased text supervision, but begins to plateau beyond 0.6. These observations suggest that while stronger textual guidance benefits discriminative tasks, generative tasks require balanced integration of visual and textual signalsunderscoring the notion that effective multimodal models must not only learn language, but also learn to ground it. Analysis of joint training. With joint training\" denotes the use of cross-entropy loss on both image token prediction and text token prediction, whereas without joint training\" refers to applying the loss only on text token prediction. As shown in Tab. 4, removing joint training results in dramatic drop in GenEval performancefrom 61.6 to 28.3highlighting more than two-fold decrease that exceeds any other variation. Meanwhile, CIDEr remains nearly unchanged (59.4 58.38), suggesting that language quality is preserved, and VQAv2 declines only marginally (69.2 67.8), representing minimal cost relative to the gains in cross-modal alignment. This ablation underscores that decoupling the training objectives significantly impairs the models ability to integrate vision and language, reinforcing the necessity of unified optimization for multimodal coherence. 4.5 Inference Time Analysis As shown in Fig. 6, autoregressive multimodal models are inherently limited by token-by-token decoding, which constrains their inference speed. Muddit overcomes this bottleneck with parallel discrete diffusion decoder, reducing average latency to just 1.49 seconds, achieving 4 to 11 speedup over competitive baselines (4.2 faster than Qwen-2.5-VL, 5.6 than Show-o, 8.1 than BLIP-2, and 10.9 than LLaVA-1.6). Besides, we present detailed FLOPs comparison between Autoregressive and Discrete Diffusion. Autoregressive (AR) without KV Cache: At step t, the model attends over previous tokens. Per-step attention FLOPs: O(t2D). 10 Figure 6: Inference speed comparison. We use 32 inference steps for Muddit and fix the sequence length to 77 across all models. Total FLOPs: (cid:88) t=1 (cid:32) O(t2D) = (cid:33) (cid:88) t=1 t2 (cid:18) = D L(L + 1)(2L + 1) 6 (cid:19) = O(L3D) Autoregressive (AR) with KV Cache: At step t, is computed for 1 token, and attends to K/V keys. Per-step attention FLOPs: O(tD). Total FLOPs: (cid:88) t=1 (cid:32) O(tD) = (cid:33) (cid:18) = (cid:19) L(L + 1) (cid:88) t=1 = O(L2D) Discrete Diffusion: Each step updates the full sequence (length L) in parallel. Per-step attention FLOPs: O(L2D). Total FLOPs: O(L2D) = O(T L2D), While discrete diffusion may appear less efficient than autoregressive (AR) models with KV caching in terms of theoretical FLOPs, it offers significant advantage over AR without cachingachieving an L/T speedup by updating the full token sequence in parallel over iterations. In practice, the higher degree of parallelism leads to competitive, and often faster, inference speed compared to AR models, especially when considering real-world GPU throughput. As KV cache techniques for discrete diffusion are rapidly evolving [37], we expect further acceleration in the near future, narrowing the theoretical speed gap even with KV-cache AR baselines. 4.6 Generated Results Step by Step Muddit frames text generation as reverse discrete diffusion over fixed-length sequence of 77 token indices. At inference time, the model performs 16 32 denoising steps, starting from maximally entropic prior where every token is masked. At each step t, parameter-shared transformer predicts categorical distribution over all positions in parallel, and sampler selects the next sequence: (12) where xt V77 is the token sequence at step t, and denotes conditioning inputs. The logits can be tempered or top-k filtered before sampling each token independently. The resulting sequence xt1 seeds the next step, enabling fast, parallel decoding without autoregressive constraints. xt1 = S(G(xt, c, t), xt, t), = T, . . . , 1, 11 Because all positions are updated in parallel, Muddit preserves global syntactic and semantic structure throughout the reverse diffusion processunlike left-to-right autoregressive models, which can only condition on past predictions. Empirically, as few as 16 32 steps are sufficient to approximate the natural language distribution with high fidelity. Thus, Muddit unifies diffusion generation with parallel decoding, effectively overcoming the serial bottleneck that limits conventional autoregressive multimodal models. We present 2 examples in Fig. 11 and Fig. 12."
        },
        {
            "title": "5 Discussion",
            "content": "5.1 Limitations While Muddit advances discrete diffusion for unified multimodal generation, it still presents several limitations. First, due to its token-level discrete representation, the model may underperform continuous diffusion models in generating photorealistic or high-resolution images. Second, Muddit is initialized from pretrained text-to-image foundation model, which offers strong visual priors but limits its capacity for rich text understanding and generation compared to the latest large language models. This makes it less suitable for tasks that require long-form understanding and generation or deep linguistic reasoning. 5.2 Broader Impacts Muddit explores new paradigm in multimodal generation by leveraging strong visual prior as the backbone, in contrast to the prevailing trend of scaling large language models. This offers complementary path toward efficient, grounded multimodal generation, particularly in vision-centric applications. The models ability to generate aligned visual and textual outputs in fast, parallel manner could benefit downstream tasks, especially in completion-based scenarios such as masked captioning, image editing, and code implementation. However, as with all generative models, there remains risk of misuse in synthetic content creation. 5.3 Additional Qualitative Results Image-to-text Generation. We present more examples for image-to-text generation in Fig. 7. Text-to-image Generation. We present more examples for text-to-image generation in Fig. 8. Visual Question Answering. We present more examples for visual question answering in Fig. 9. Muddit reliably identifies fine-grained attributes (e.g., blonde hair), object categories (e.g., beagle), and physical affordances (e.g., answering No to crossing at red light). Notably, it also handles commonsense reasoning and spatial localization, such as inferring traffic legality or locating vehicles on the street. Image-guided text editing. Zero-shot text-guided image editing performance is already verified and presented in Meissonic [5]. As the successor to Meissonic, we present Muddits performance on the image-guided text editing task, where the model completes masked sentence based on the input image. As shown in Fig. 10, given partially masked caption and an image, Muddit fills in the blanks with semantically and visually grounded phrases."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we present Muddit, unified generative framework that employs discrete diffusion to bridge text and image modalities. By unifying image and text generation within single model, Muddit demonstrates strong performance across text-to-image, image-to-text, and VQA tasks. Notably, it outperforms or matches the capabilities of significantly larger autoregressive models, while enabling fast, parallel inference. Our results validate the effectiveness of discrete denoising as general-purpose modeling strategy and highlight its potential to serve as scalable backbone for future multimodal systems. 13 Figure 7: Image-to-text generated results. 14 Figure 8: Text-to-image generation results. Figure 9: Visual question answering results. Figure 10: Image-guided text editing results. 16 Figure 11: Image-to-text generated results in each step. 17 Figure 12: Image-to-text generated results in each step."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 9 [2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 24252433, 2015. 8 [3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework for training large autoregressive vision-language models, 2023. 9 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. 9 [5] Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Qing-Guo Chen, Xiangtai Li, Zhen Dong, Lei Zhu, and Shuicheng Yan. Meissonic: Revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis. ICLR, 2025. 2, 3, 4, 6, 7, 13 [6] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021. 3 [7] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science., 2:3, 2023. 7 [8] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1131511325, 2022. 2, 3, 5 [9] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, and Ran Xu. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset, 2025. 1 [10] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR, 2024. 7 [11] Liang Chen, Shuai Bai, Wenhao Chai, Weichu Xie, Haozhe Zhao, Leon Vinci, Junyang Lin, and Baobao Chang. Multimodal representation alignment for image generation: Text-image interleaved control is easier than you think. arXiv preprint arXiv:2502.20172, 2025. 1, [12] Sixiang Chen, Jinbin Bai, Zhuoran Zhao, Tian Ye, Qingyu Shi, Donghao Zhou, Wenhao Chai, Xin Lin, Jianzong Wu, Chao Tang, et al. An empirical study of gpt-4o image generation capabilities. arXiv preprint arXiv:2504.05979, 2025. 3 [13] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 3 [14] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, 2025. 9 [15] Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 3, 4 [16] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023. [17] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 7 [18] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. 3 [19] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 8 [20] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. 2, 7 [21] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. 7 [22] Jianyuan Guo, Hanting Chen, Chengcheng Wang, Kai Han, Chang Xu, and Yunhe Wang. Vision superalignment: Weak-to-strong generalization for vision foundation models. arXiv preprint arXiv:2402.03749, 2024. 3 [23] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 1600016009, 2022. 3 [24] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 8 [25] Inception. Inception labs. https://www.inceptionlabs.ai/, Feb 2025. Accessed: 2025-05-16. 2 [26] Black Forest Labs. https://blackforestlabs.ai/ Announcing black forest labs, 2024. announcing-black-forest-labs/. 4 [27] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. 9 [28] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024. 3 [29] Zijie Li, Henry Li, Yichun Shi, Amir Barati Farimani, Yuval Kluger, Linjie Yang, and Peng Wang. Dual diffusion for unified image generation and understanding. arXiv preprint arXiv:2501.00289, 2024. 1, 3, 9 [30] Zijie Li, Henry Li, Yichun Shi, Amir Barati Farimani, Yuval Kluger, Linjie Yang, and Peng Wang. Dual diffusion for unified image generation and understanding, 2024. 7 [31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECV, 2014. 8 [32] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining, 2024. 3 [33] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. 9 [34] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv preprint arXiv:2402.08268, 2024. 3, 9 [35] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv preprint, 2024. 7 [36] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. 3 [37] Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang. dkv-cache: The cache for diffusion language models. arXiv preprint arXiv:2505.15781, 2025. 11 [38] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv preprint arXiv:2411.07975, 2024. 3 [39] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. 2 [40] OpenAI. Addendum to gpt-4o system card: 4o image generation, 2025. Accessed: 2025-04-02. 3 [41] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. 1 [42] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 6 [44] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 7 [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 7 [46] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [47] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222, 2023. 2 [48] Alexander Swerdlow, Mihir Prabhudesai, Siddharth Gandhi, Deepak Pathak, and Katerina Fragkiadaki. Unified multimodal discrete diffusion. arXiv preprint arXiv:2503.20853, 2025. 2, 3 [49] Alexander Swerdlow, Mihir Prabhudesai, Siddharth Gandhi, Deepak Pathak, and Katerina Fragkiadaki. Unified multimodal discrete diffusion. arXiv preprint arXiv:2503.20853, 2025. 7, 9 [50] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 3, 7, 9 [51] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. 2 [52] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1 [53] Chunwei Wang, Guansong Lu, Junwei Yang, Runhui Huang, Jianhua Han, Lu Hou, Wei Zhang, and Hang Xu. Illume: Illuminating your llms to see, draw, and self-enhance. arXiv preprint arXiv:2412.06673, 2024. [54] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 3 [55] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. 3 [56] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 3, 7, 9 [57] Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. 2 [58] Chuyang Zhao, Yuxing Song, Wenhao Wang, Haocheng Feng, Errui Ding, Yifan Sun, Xinyan Xiao, and Jingdong Wang. Monoformer: One transformer for both diffusion and autoregression. arXiv preprint arXiv:2409.16280, 2024. [59] Mengyu Zheng, Yehui Tang, Zhiwei Hao, Kai Han, Yunhe Wang, and Chang Xu. Adapt without forgetting: Distill proximity from dual teachers in vision-language models. In European Conference on Computer Vision, pages 109125. Springer, 2024. 3 [60] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 3, 7,"
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Peking University",
        "Princeton University",
        "TeleAI, China Telecom"
    ]
}