{
    "paper_title": "Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling",
    "authors": [
        "Ruijie Ye",
        "Jiayi Zhang",
        "Zhuoxin Liu",
        "Zihao Zhu",
        "Siyuan Yang",
        "Li Li",
        "Tianfu Fu",
        "Franck Dernoncourt",
        "Yue Zhao",
        "Jiacheng Zhu",
        "Ryan Rossi",
        "Wenhao Chai",
        "Zhengzhong Tu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We study instruction-based image editing under professional workflows and identify three persistent challenges: (i) editors often over-edit, modifying content beyond the user's intent; (ii) existing models are largely single-turn, while multi-turn edits can alter object faithfulness; and (iii) evaluation at around 1K resolution is misaligned with real workflows that often operate on ultra high-definition images (e.g., 4K). We propose Agent Banana, a hierarchical agentic planner-executor framework for high-fidelity, object-aware, deliberative editing. Agent Banana introduces two key mechanisms: (1) Context Folding, which compresses long interaction histories into structured memory for stable long-horizon control; and (2) Image Layer Decomposition, which performs localized layer-based edits to preserve non-target regions while enabling native-resolution outputs. To support rigorous evaluation, we build HDD-Bench, a high-definition, dialogue-based benchmark featuring verifiable stepwise targets and native 4K images (11.8M pixels) for diagnosing long-horizon failures. On HDD-Bench, Agent Banana achieves the best multi-turn consistency and background fidelity (e.g., IC 0.871, SSIM-OM 0.84, LPIPS-OM 0.12) while remaining competitive on instruction following, and also attains strong performance on standard single-turn editing benchmarks. We hope this work advances reliable, professional-grade agentic image editing and its integration into real workflows."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 ] . [ 1 4 8 0 9 0 . 2 0 6 2 : r Agent Banana: High-Fidelity Image Editing with"
        },
        {
            "title": "Agentic Thinking and Tooling",
            "content": "Ruijie Ye1,2, Jiayi Zhang1,3, Zhuoxin Liu4, Zihao Zhu1, Siyuan Yang1, Li Li5, Tianfu Fu6, Franck Dernoncourt7, Yue Zhao5, Jiacheng Zhu8, Ryan Rossi7, Wenhao Chai9, Zhengzhong Tu1 1TAMU 2Brown University 3UW-Madison 4UCSD 5USC 6xAI 7Adobe Research 8Meta AI 9Princeton University Corresponding Author: tzz@tamu.edu. Equal contributions. Work done outside of Meta. Project Website: agent-banana.github.io Figure 1: We present Agent Banana, an agentic editing system that enables high-fidelity, native-resolution image editing through reasoning-based natural-language interaction, where each edit is context-aware, logically dependent, and locally precise. In this example, the user provides vague yet complex editing prompt, and Agent Banana iteratively refines scene in native high resolution (5460 3640)from stylistic replacement (Turn 1), to attribute decoupling that preserves non-target dynamics (changing the bottle color without affecting the pouring liquid; Turn 2), and finally to retrieving prior state and adding fine details (Turn 3). The result is professional-style workflow that resists over-editing and background drift, while faithfully preserving what should remain unchanged."
        },
        {
            "title": "Abstract",
            "content": "We study instruction-based image editing under professional workflows and identify three persistent challenges: (i) editors often over-edit, modifying content beyond the users intent; (ii) existing models are largely single-turn, while multi-turn edits can alter object faithfulness; and (iii) evaluation at around 1K resolution is misaligned with real workflows that often operate on ultra high-definition images (e.g., 4K). We propose Agent Banana, hierarchical agentic plannerexecutor framework for high-fidelity, object-aware, thinking with editing. Agent Banana introduces two key mechanisms: ❶ Context Folding, which compresses long interaction histories into structured memory for stable long-horizon control, and ❷ Image Preprint. Under review. Layer Decomposition, which performs localized layer-based edits to preserve non-target regions while enabling native-resolution outputs. To support rigorous evaluation, we build HDD-Bench, high-definition, dialogue-based benchmark featuring verifiable stepwise targets and native 4K images (11.8M pixels) for diagnosing long-horizon failures. On HDD-Bench, Agent Banana achieves the best multi-turn consistency and background fidelity (e.g., IC 0.871, SSIMOM 0.84, LPIPSOM 0.12) while remaining competitive on instruction following, and also attains strong performance on standard single-turn editing benchmarks. We hope this work advances reliable, professional-grade agentic image editing and its integration into real workflows."
        },
        {
            "title": "Introduction",
            "content": "Instruction-based image editing [3, 54, 40, 10, 18, 4, 33, 45, 27] enables users to modify images via natural-language commands and has become core capability of modern generative vision systems. Recent advances in foundation modelsparticularly diffusion [14, 26] and autoregressive transformers [42]have substantially improved both photorealism and instruction following, powering practical editing experiences in commercial systems (e.g., GPT-4o [33], Gemini 2.5 Flash Image [8]) and strong open-source models (e.g., Flux-1 [20], Qwen-Image-Edit [44]). Despite this rapid progress, substantial gap remains between current generative editors [44, 27, 17] and the requirements of professional workflows. In high-stakes settings such as photography [16], graphic design [28], visual effects (VFX), and filmmaking [56], users typically work on native high-resolution assets (often 4K or higher) and demand precise, localized modifications that preserve all non-target content [17]. By contrast, todays models often operate at reduced resolution or rely on downsampling, making it difficult to maintain fine textures and sharp boundaries. Moreover, they frequently exhibit over-editing effects, unintentionally altering regions outside the users intent or degrading global semantic coherence. Lastly, they struggle with complex requests that are multi-goal or sequential [59], where success requires decomposing the instruction, verifying intermediate results, and revising earlier decisions across turns. We argue that to bridge this gap, next-generation editing tools must satisfy four core capabilities: 1 Intent understanding and decomposition of complex requests into atomic sub-edits; 2 Accurate localized editing to ensure edits are precisely applied while maintaining the rest of the content unchanged, on native resolution; 3 State tracking and rollback to retain intermediate steps across multi-turn interactions so that users (or intelligent agents) can easily revert to previous step and re-plan the remaining steps; and 4 High-resolution native editing to operate directly on native 4K images, preserving fine-grained textures and sharp boundaries while avoiding downsampling. To this end, we introduce Agent Banana, an agentic, layer-aware image editing framework that couples high-level reasoning and planning with tool-use capabilities, benefiting from the rapid progress of Vision-Language Models (VLMs) in image understanding, reasoning, and tool invocation [15, 36, 46, 47, 34, 1]. Agent Banana decomposes vibe-type prompts into discrete, single-goal steps, executes these steps using Photoshop-style layer isolation, masking, and local edits. Agent Banana also includes self-reflection mechanism [50, 38], allowing it to retry, rollback, and replan at inference time. Crucially, Agent Banana is built around two mechanisms tailored for long-horizon, high-resolution editing: Context Folding, which compresses long interaction histories into structured memory for stable state tracking across turns, and Image Layer Decomposition, which performs edits on isolated high-resolution layers to preserve non-target content and prevent drift across iterations. To evaluate multi-turn, high-definition editing under realistic stepwise dependencies, we build HDDBench, High-Definition and Dialogue-based benchmark designed to simulate professional editing workflows. Unlike prior benchmarks that are predominantly single-turn or weakly dependent across turns [10, 18, 4, 45, 27, 51], HDD-Bench features logically dependent instruction chains where each turn induces well-defined state transition and can be verified step by step. HDD-Bench benchmarks instruction adherence, edit locality, multi-turn consistency, and overall visual fidelity at native resolution. To reduce evaluation ambiguity, we further introduce graph-based evaluation protocol that tracks object-state transitions across turns, complementing global perceptual metrics with localized, turn-level checks of whether the intended edits are applied and whether non-target regions remain preserved. Figure 2: Overview of the Agent Banana Framework. The system operates in multi-turn loop (Left), comprising two core agents: Planner that decomposes user queries into executable editing plans, and an Executor that selects tools via the MCP Server. Crucially, the Executor incorporates self-correction mechanism (Quality Test), reiterating the editing process if the quality check fails before presenting the result to the user. (Right) Our Evaluator assesses performance by analyzing the transition between Turn 1 and Turn n, utilizing instruction adherence checks and state tracking (JSON) to derive the final score."
        },
        {
            "title": "2.1 Problem Setup and Motivation",
            "content": "We consider multi-turn instruction-based image editing task, where the user provides sequence of natural-language instructions = {q1, q2, . . . , qT } and an initial image I0. The system responds by executing trajectory of editing steps τ = {(a1, o1), (a2, o2), . . . , (aT , oT )}, where each ai denotes the i-th action (comprising reasoning and tool invocation), and oi is the resulting image state. The environment dynamics can be abstracted as transition operator such that oi = E(oi1, ai). Following the ReAct-style paradigm [50], the agent incrementally selects actions based on the full interaction history: Pθ(τ q) (cid:89) i=1 πθ (ai q, a<i, o<i) . (1) While conceptually simple, this formulation introduces two major challenges in practice: ❶ Longhorizon context overflow. As the number of editing steps increases, the agent must repeatedly condition on the entire interaction history, both textual and visual. This leads to severe token inefficiency, quickly exceeding the LLMs context length, and introduces irrelevant noise that impairs reasoning and planning in later steps. ❷ Full-image detail degradation Existing editing tools often operate by resampling the entire image at each step, regardless of the locality of the edit. This not only wastes computation on unchanged regions, but also causes subtle degradation of fine details over timeespecially in backgrounds or fixed objectsleading to accumulation of visual artifacts across turns."
        },
        {
            "title": "2.2 Overview of Agent Banana",
            "content": "To address the challenges of context overflow and iterative degradation, we introduce Agent Banana, hierarchical multi-agent editing framework designed for high-fidelity, multi-turn image editing at native resolution, as shown in Figure 2. The framework explicitly separates global task reasoning from low-level execution via two specialized agents: Planner: Performs global intent interpretation, decomposes complex user instructions into executable sub-goals, and monitors overall progress. Executor: Carries out atomic editing operations, invokes tools on localized image regions, and handles intermediate validation and error recovery. This division of roles enables the system to both reason over long-horizon objectives and execute fine-grained visual edits in scalable and interpretable manner. Agent Banana is built around two key mechanisms that mitigate the core bottlenecks identified earlier: 3 Context Folding: hierarchical memory abstraction that compresses the growing interaction history into structured representations, enabling long-horizon planning without exceeding context limits. Image Layer Decomposition (ILD): localized execution strategy that performs edits on cropped high-resolution patches (layers), preserving pixel-level fidelity in unedited regions and naturally supporting ultra-HD editing workflows. During each interaction round, the Planner receives the user instruction and current image state, decomposes the task into sub-goals, and delegates them to the Executor. The Executor generates intermediate candidates via ILD-based editing and returns feedback. The Planner verifies whether the updated image meets the instruction goal; if not, it can replan or rollback using the maintained image state graph. This closed-loop process continues until the users intent is satisfied or predefined turn limit is reached."
        },
        {
            "title": "2.3 Context Folding",
            "content": "To effectively mitigate the exponential explosion of context in long-horizon tasks, we introduce the Context Folding mechanism. The core idea is to \"fold\" the raw, high-dimensional interaction history into compact semantic representation through hierarchical abstraction and selective memory. Specifically, we decouple context information into three schemas of varying granularity: the Asset Level, the Execution Level, and the Planning Level. Asset Level: ImageContext. This is the fundamental data unit of the system, constructed by the Executor after each image generation. Instead of directly embedding high-dimensional image tokens, ImageContext abstracts the image into lightweight semantic node, containing unique identifier (URI), VLM-generated textual description of the content, its parent URI, and the transformation type leading to this state change. Through this text-based graph representation, the agent can track the full image evolution history with minimal context overhead while preserving the topological relationships between image states. Execution Level: ToolContext. This serves as the Transient Working Memory used by the Executor during single-step reasoning. It details the microscopic operations required to complete an atomic instruction, including tool selection, parameter configuration, the intermediate reasoning process (Thought), and references to relevant ImageContexts. ToolContext primarily facilitates error recovery and state backtracking within the current step. Once the current sub-task is completed, these trivial trial-and-error details are \"folded\" and do not enter the long-term global memory, thereby preventing irrelevant execution noise from interfering with the Planner. Planning Level: ActionContext. This forms the Persistent Memory established after each round of user interaction. When the Planner confirms that series of operations successfully meets the users requirements, it constructs an ActionContext. This context retains only the verified effective editing path: the final intention determined by the Planner and the corresponding sequence of key ImageContexts. ActionContext essentially acts as semantic compression of ToolContext, discarding procedural tool invocation details and preserving only high-level task semantics and result states. This ensures that the agent maintains clear cognitive grasp of the global task state even after dozens of interaction turns, without being overwhelmed by excessive token sequences. 2."
        },
        {
            "title": "Image Layer Decomposition",
            "content": "To resolve the issues of detail loss and resolution limitations inherent in full-image generation, we propose the Image Layer Decomposition (ILD) mechanism. Traditional end-to-end editing models often resample the entire image, causing unintended pixel drift in unedited regions (such as the background or irrelevant objects). The ILD mechanism abandons this global operation in favor of \"decompose-edit-fuse\" local processing paradigm. Specifically, this mechanism utilizes dynamic object-aware mask to precisely localize the target region, losslessly cropping it from the original high-resolution image into an independent layer patch. All generative editing is performed solely within the local coordinate system of this patch, thereby freezing the pixel state of the background region and substantially reducing degradation in non-target regions by avoiding full-image resampling. Upon completion of editing, the system seamlessly blends 4 Table 1: Comparison of existing image editing datasets vs. our HDD-Bench. We compare key features including support for multi-turn interaction, high-resolution images, object-level editing granularity, reasoning capabilities, and ground-truth verification. HDD-Bench is the only benchmark encompassing all these capabilities, bridging the gap for professional-grade editing evaluation."
        },
        {
            "title": "Dataset",
            "content": "#Turn Res. (pxs) Obj Scale Reason. Verifi. AnyEdit [52] GEdit-Bench [27] SEED-DataEdit [12] ImgEdit-Bench [51] HDD-Bench (ours) 1.0 1.0 3.8 1.0 3.0 0.4M 3.4M 1.1M 1.1M 11.8M"
        },
        {
            "title": "Large\nLarge\nMedium\nLarge\nSmall",
            "content": "the updated patch back into the original image using Gaussian blending algorithms. Furthermore, since it only processes local patches, this mechanism naturally supports ultra-high-definition image editing beyond the models native resolution limits. Based on the ILD mechanism, we define an Action Space of five atomic instructions that cover common editing needs: replace: Substitutes the content of the target layer with new object using inpainting techniques while maintaining edge consistency. remove: Eliminates the target layer and fills the void using background completion algorithms. add: Generates new layer at specified location and performs layer superposition. adjust: Applies attribute transformations (e.g., color correction, style transfer) to the target layer without altering its geometry. undo: Rapidly rolls back to the previous image state node based on the state graph maintained in Context Folding. These five atomic operations form the foundational capability set for Agent Banana, enabling the Planner to execute complex, composite edits by composing these primitives."
        },
        {
            "title": "3 HDD-Bench: High-Definition, Dialogue-based image editing benchmark",
            "content": "Recent generative editors are increasingly interactive and agentic, yet rigorous evaluation for professional-grade editing remains underdeveloped. Existing benchmarks typically fall short along at least one key dimension: (i) single-turn interactions that fail to capture the stepwise dependencies inherent to real editing sessions; (ii) low-resolution formats that cannot meet the fidelity and locality requirements of native 4K workflows; and (iii) human-in-the-loop processes that, while enabling richer interactions, act as significant bottleneck restricting dataset scale and diversity. More importantly, most benchmarks provide only an end result, without verifiable intermediate interface. Without turn-by-turn targets, it is hard to diagnose long-horizon failures such as error accumulation, over-editing of non-target regions, or semantic drift across turns. This motivates benchmark that (i) supports multi-turn, logically dependent instruction chains; (ii) evaluates at native high resolution to ensure fidelity; and (iii) provides structured intermediate supervision, enabling precise failure attribution without the scalability constraints of human oversight."
        },
        {
            "title": "3.1 A Scalable Data Pipeline for Multi-turn Editing",
            "content": "To enable verifiable multi-turn supervision without expensive pixel-level annotation, we propose scalable symbolic data engine that synthesizes editing trajectories in an attribute-level state space. For each input image, we construct an initial scene state s0 that represents salient objects and their attributes, including name, color, size, material, and shape. Each editing turn is specified by set of canonical edit commands ct, which are applied deterministically to update the state: st+1 = (st, ct), where is deterministic transition operator that modifies only the targeted object attributes. This design decouples interaction synthesis from image generation: we can generate consistent and checkable intermediate targets {s1, s2, . . . } without rendering images during data construction. 5 Figure 3: Scalable Data Pipeline for Multi-turn Editing. This diagram illustrates the process of generating aligned (State, Instruction) pairs from HD images. To mimic real user behavior, language agent paraphrases the canonical command set ct into single natural-language instruction qt, optionally mixing multiple intents (e.g., adding an object while changing another objects color). Importantly, any ambiguity is introduced only in the surface phrasing qt, while the underlying ct and target state st+1 are preserved as the internal ground truth. To ensure reliability, we incorporate human verification at the entry point of the pipeline: the initial scene graph and extracted attributes used to form s0 are manually inspected and corrected. Since subsequent turns are produced by deterministic transitions, this guarantees the correctness of the entire multi-turn state chain and provides principled, verifiable interface for evaluation."
        },
        {
            "title": "3.2 Constructing the HDD-Bench",
            "content": "Built on top of the data engine, we construct HDD-Bench, High-Definition, Dialogue-based benchmark that targets professional editing requirements. HDD-Bench is designed to jointly stress (i) multi-turn dependency, where later instructions build on earlier edits; (ii) high-resolution fidelity, where fine textures and sharp boundaries must be preserved at native resolution; and (iii) object-level compositionality, where instructions may involve multiple objects and mixed intents. Each sample in HDD-Bench is three-turn editing session. At turn t, the benchmark provides natural-language instruction qt (often combining multiple edit intents into single request) and corresponding target symbolic state st for verifiable evaluation. We adopt three-turn interactions to control difficulty and simplify comparisons across methods, while still capturing stepwise dependency and error accumulation; notably, our engine can generate longer sessions without changing the evaluation interface. HDD-Bench contains 96 curated sessions selected from the synthesis pipeline. The selected samples emphasize scenes with multiple salient objects and non-trivial edit chains, and cover diverse set of atomic actions (e.g., add, remove, replace, adjust, undo) as well as hybrid instructions that require composing multiple actions within turn."
        },
        {
            "title": "3.3 Evaluation Protocol",
            "content": "HDD-Bench evaluates editing quality from two complementary perspectives: (i) semantic correctness of the intended edits, and (ii) visual preservation of non-target regions. The first aspect is assessed in verifiable, object-centric manner using the symbolic state representation; the second is assessed at the pixel/perceptual level to quantify background fidelity. State-based metrics: Instruction Following and Image Consistency. Given generated image at turn t, we map it to predicted post-edit state ˆst using the same perception pipeline used to construct s0. We then compare ˆst against the ground-truth target state st to compute two scores: Instruction Following (IF) measures whether the attributes of targeted objects match the requested edits, while Image Consistency (IC) measures whether non-target objects remain unchanged across turns. Both 6 Figure 4: Qualitative Comparison of Editing Fidelity. We utilize the instruction \"...And change that little bright blue cooler under the shelter to softer sea-foam green with creamy top ...\" to guide the editing process. While the prompt solely targets color modification, baseline models exhibit significant limitations: they often suffer from reduced resolution, introduce unwanted structural changes (modifying shape or position), or fail to apply the target color change. By leveraging our agents superior interpretation capabilities, our method accurately captures the instructions focus while preserving the integrity of the original image. scores are computed by averaging attribute-level correctness over objects: sIF or sIC ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i="
        },
        {
            "title": "1\nMi",
            "content": "Mi(cid:88) j=1 si,j , where is the number of evaluated objects (edited or preserved), Mi is the number of attributes for object i, and si,j is the correctness score for the j-th attribute. Otsu-masked background fidelity. Global full-reference metrics such as PSNR, SSIM, and LPIPS can be misleading for editing, since they penalize valid foreground changes and unwanted background corruption equally. To isolate preservation quality, we compute Otsu-Masked PSNR/SSIM/LPIPS [35, 43, 55], denoted as PSNROM, SSIMOM, and LPIPSOM. Concretely, we form pixel-wise difference map between the pre-edit and post-edit images, apply Otsus method to obtain an adaptive threshold by maximizing inter-class variance, = arg max 1k<L and construct background mask Mbg by selecting pixels whose differences fall below k. We then compute metrics only on the masked background region. This provides targeted measure of whether the model preserves non-edited context while performing the intended semantic edits. σ2 B(k),"
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "In our experiments, we employ GPT-5-mini as the foundational Large Language Model (LLM) powering both the Planner and Executor agents. To endow the agents with robust visual generation and editing capabilities, we construct comprehensive toolset integrating state-of-the-art visual models, including both open-source and private models for high-quality generation and editing, complemented by GPT-5-mini for visual verification. To ensure fair comparison, when Nano Banana Pro is used as the underlying image model, our gains reflect agentic scaffolding (decomposition, masking, verification) rather than changes to the generator weights; instead, we compare against the Nano Banana Pro and other baseline models operating without our multi-step workflow. 7 Table 2: Quantitative Comparison of Image Editing Performance. We evaluate models on HDDBench focusing on image fidelity (PSNROM, SSIMOM, LPIPSOM), instruction adherence (InstructFollowing, Image Consistency), and support for high-resolution (4K) editing. Agent Banana achieves state-of-the-art performance, balancing precise instruction execution with high visual fidelity, and is natively capable of processing at 4K resolution. Model ICEdit [57] Qwen-Image [44] OmniGen2 [45] BAGEL [11] Step1X-Edit [27] FLUX.1 Kontext [Pro] [19] GPT Image 1 [High] [32] Nano Banana Pro [9] Agent Banana (ours) PSNROM SSIMOM LPIPSOM IF IC Add Adj. Repl. Rem. HDD Bench ImgEdit 29.21 23.62 23.59 26.93 25.82 25.98 19.20 26.62 28.40 0.80 0.80 0.72 0.79 0.77 0.74 0.54 0.72 0. 0.14 0.14 0.23 0.17 0.19 0.17 0.33 0.14 0.12 0.595 0.845 0.545 0.676 0.808 0.845 0.882 0.911 0.687 0.807 0.655 0.723 0. 0.702 0.727 0.861 0.849 0.871 3.58 4.38 3.57 3.56 3.88 4.25 4.61 4.58 4. 3.39 4.16 3.06 3.31 3.14 4.15 4.33 4.56 4.59 3.15 4.66 3.74 3.3 3.40 4.56 4.35 4.55 4. 2.93 4.14 3.20 2.62 2.41 3.57 3.66 4.39 4.60 4K Res"
        },
        {
            "title": "4.2 Performance on Multi-turn Editing",
            "content": "To comprehensively evaluate the performance of Agent Banana, we benchmark it against representative image editing models, including the closed-source commercial model Flux.1 Kontext [19], Nano Banana Pro [9], and GPT-Image-1 [High] [32]. We adopt the standard metrics defined by HDD-Bench, covering editing accuracy (sinstruction following), Otsu-Masked PSNR (sPSNROM), and the final composite score (sfinal). Detailed quantitative comparisons are presented in Table 2. Given that this benchmark focuses on multi-turn sequential editing tasks, we report the average score across all interaction turns as the final performance metric. Notably, our dataset consists entirely of 4K-resolution images, posing significant challenge to the high-resolution processing capabilities of the models. For baselines that downsample inputs during processing, we explicitly denote their maximum supported resolution in the results table and evaluate them after upsampling the output back to 4K. The results indicate that Agent Banana not only achieves competitive scores against the baselines but, crucially, is one of only two models capable of maintaining high fidelity at 4K native resolution. This validates the effectiveness of our proposed Image Layer Decomposition mechanism in preventing detail loss during high-resolution editing."
        },
        {
            "title": "4.3 Performance on Single-turn Editing",
            "content": "In addition to evaluating long-horizon multi-turn capabilities, we assess the foundational performance of Agent Banana on single-turn editing tasks using the ImgEdit-Bench. This experiment aims to verify that our agent architecture, despite being designed for complex planning, maintains SOTA precision when handling atomic editing instructions. We compare Agent Banana with mainstream single-turn editing models. As shown in Table 2, our method achieves leading or comparable results across all metrics. This is primarily attributed to the Executors precise control over tool parameters and the self-verification mechanism provided by the Quality Test modules."
        },
        {
            "title": "4.4 Ablation Study",
            "content": "Impact of Foundational LLM Capabilities. We first investigate the sensitivity of system performance to the capabilities of the base model. Given that the base model directly dictates instruction understanding, task planning, and the accuracy of tool invocation, we experimented by replacing the kernels of the Planner and Executor with the smaller-scale Qwen-3-8B. Observations reveal that weaker base model exhibits significant performance degradation when handling ambiguous instructions and long-sequence planning, frequently generating unparseable tool parameters or erroneous dependencies, leading to workflow interruptions. This confirms that robust reasoning capability is prerequisite for agents handling complex multi-turn editing tasks. 8 Figure 5: Qualitative Comparison of Unedited Region Consistency. Although the editing instruction does not target the sofa cushion, Nano Banana Pro distorts the original details due to global editing. In contrast, our method successfully maintains the visual consistency of the unedited regions. Figure 6: Metric Comparison across Sequential Turns. Agent Banana (red line) exhibits relatively better performance and consistent stability across all evaluated metrics. Compared to several other models, demonstrating its effectiveness in preserving image quality throughout the multi-turn process."
        },
        {
            "title": "4.5 Native-Resolution Editing Analysis",
            "content": "A significant advantage of Agent Banana is its capability for native-resolution editing. Unlike existing baselines (e.g., FLUX.1 Kontext or Qwen-Image) that typically force inputs to be downsampled to 1k resolution, our method avoids this loss through layered processing mechanism. As illustrated in Figure 5, for high-resolution input of 2716 4060, baseline models lose substantial texture detail during the downsampling-upsampling process, whereas our method perfectly preserves the high-frequency information of the original image. However, baseline models exhibit significant limitations: they often suffer from reduced resolution, introduce unwanted structural changes (e.g., modifying object shape or position), or fail to apply the target color change. By leveraging our agents superior interpretation capabilities, our method accurately captures the instructions focus while preserving the integrity of the original image. This minimal-loss characteristic positions Agent Banana as viable solution for professional-grade image editing tasks."
        },
        {
            "title": "4.6 On the Prior-Induced Editing Drift (PIED)",
            "content": "We observe subtle but important failure mode in multi-turn editing using generative editors: even when each turn appears highly realisticsometimes indistinguishable to the eyethe purported non-edited regions (which are, in practice, repeatedly re-generated) can gradually drift toward the generators preferred texture and style statistics as turns accumulate. We term this effect PriorInduced Editing Drift (PIED). Figure 6 shows that several baselines exhibit steady increase in PSNROM on non-edited regions across turns, which can be misleading. We hypothesize that PIED games this metric: repeated re-synthesis slightly re-renders the whole image, shrinking Otsu-partitioned background changes (thus inflating PSNROM) while faithfulness to the original input 9 still degrades. In contrast, Agent Banana keeps PSNROM nearly constant across turns, matching qualitative observations of reduced accumulated artifacts and better preservation of high-frequency details and style in non-edited regions. Overall, PIED suggests that per-turn visual fidelity can decouple from long-horizon faithfulness, and drift accumulation should be explicitly measured in evaluating multi-turn editors."
        },
        {
            "title": "5 Related Work",
            "content": "5.1 Instruction-based Image Editing Recent progress in instruction-based image editing has been driven by diffusion and autoregressive foundation models, such as GLIDE [31], InstructPix2Pix [3], MagicBrush [53], Prompt-toPrompt [13], and UltraEdit [58]. Beyond these single-turn editors, emerging interactive systems (e.g., GPT-Image-1 [33] and Nano Banana [8]) indicate shift toward multi-turn, context-aware interaction. To further strengthen fine-grained control, follow-up work explores attention manipulation [13], mask-based inpainting [2], and automatic region detection [6]; additionally, several methods decompose scenes into object-specific layers for more precise localized editing [30, 48, 41]."
        },
        {
            "title": "5.2 Agentic Systems for Image Editing",
            "content": "The exceptional reasoning and language capabilities of large language models (LLMs) have catalyzed rapid advances in agentic systems for interaction and task solving in complex environments. Paradigms exemplified by ReAct [49] establish foundational framework by alternating reasoning and atomic actions within an iterative thinkact loop. Meanwhile, Anthropics Model Context Protocol (MCP) [29] unifies the communication interface between LLMs and external tools, substantially improving the standardization and scalability of tool orchestration. Agentic perceptiondecisionaction paradigms have long been explored in vision and learning via closed-loop or adaptive frameworks [37, 7, 39, 61, 24, 5], with VLMs increasingly serving as planners. For image/video restoration, AgenticIR and MoA-VR independently introduce VLM-integrated multi-agent repair paradigms [60, 25]. In creative photo retouching and task-oriented restoration, intelligent tool-invocation workflows such as JarvisIR, JarvisArt, 4KAgent, and JarvisEvo further demonstrate the effectiveness of agentic pipelines for restoration and editing [21, 22, 62, 23]."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce Agent Banana, multi-agent, layer-aware framework for instruction-based image editing, together with HDD-Bench, high-resolution multi-turn benchmark aligned with professional workflows. By coupling LLM planning, VLM perception, and layer-aware tool use, Agent Banana performs precise, rollback-safe edits on 4K images while preserving non-target regions, and consistently improves instruction following, edit locality, and multi-turn stability over strong non-agentic baselines. Beyond single system and benchmark, we model editing as explicit state transitions on object-level graphs, enabling stepwise, verifiable evaluation and natural support for undo, branching, and long-horizon correction. Our scalable data engine further decouples state transitions from pixel rendering, making it practical to synthesize large-scale visionlanguage reasoning traces and edit histories. Impact Statement. This work advances instruction-based image editing toward professional workflows by emphasizing two properties that matter in real deployments: native high-resolution fidelity and multi-turn reliability. In particular, our benchmark and evaluation protocol provide stepwise, verifiable checks of what changed and what must remain invariant across turns, helping the community move beyond single-turn demos and toward diagnosing long-horizon failure modes such as over-editing, drift, and irreversible degradation. At the same time, stronger editing capabilities can be misused to create misleading visual content or facilitate non-consensual manipulation. We therefore emphasize evaluation and auditing: our contributions are designed to measure controllability and detect failure accumulation rather than to optimize for unconstrained manipulation, and we encourage future systems built on this line of work to adopt provenance, consent, and disclosure mechanisms when applied to real-world media."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Claude 3.7 sonnet system card. https://www.anthropic.com/news/ claude-3-7-sonnet-system-card, 2025. Accessed: 2025-10-29. [2] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1820818218, 2022. [3] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. [4] Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, et al. Hidream-i1: high-efficient image generative foundation model with sparse diffusion transformer. arXiv preprint arXiv:2505.22705, 2025. [5] Haoyu Chen, Wenbo Li, Jinjin Gu, Jingjing Ren, Sixiang Chen, Tian Ye, Renjing Pei, Kaiwen Zhou, Fenglong Song, and Lei Zhu. Restoreagent: Autonomous image restoration agent via multimodal large language models, 2024. [6] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427, 2022. [7] Ekin Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 113123, 2019. [8] Google DeepMind. Gemini 2.5 flash image (nano banana). https://aistudio.google.com/ models/gemini-2-5-flash-image, 2025. Accessed: 2025-10-29. [9] Google DeepMind. Gemini 3 image preview (nano banana pro). https://deepmind.google/ models/gemini-image/pro/, 2025. Accessed: 2026-1-28. [10] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [11] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [12] Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024. [13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-toprompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [15] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428114290, 2024. [16] Wenyi Hu, Zhihong Lin, Malcolm Clark, Jacqueline Henwood, Xianwen Shang, Ruiye Chen, Katerina Kiburg, Lei Zhang, Zongyuan Ge, Peter van Wijngaarden, et al. Real-world feasibility, accuracy and acceptability of automated retinal photography and ai-based cardiovascular disease risk assessment in australian primary care settings: pragmatic trial. NPJ Digital Medicine, 8(1):122, 2025. [17] Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. Hq-edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024. [18] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. 11 [19] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. [20] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Müller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. [21] Yunlong Lin, Zixu Lin, Haoyu Chen, Panwang Pan, Chenxin Li, Sixiang Chen, Kairun Wen, Yeying Jin, Wenbo Li, and Xinghao Ding. Jarvisir: Elevating autonomous driving perception with intelligent image restoration. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2236922380, 2025. [22] Yunlong Lin, Zixu Lin, Kunjie Lin, Jinbin Bai, Panwang Pan, Chenxin Li, Haoyu Chen, Zhongdao Wang, Xinghao Ding, Wenbo Li, et al. Jarvisart: Liberating human artistic creativity via an intelligent photo retouching agent. arXiv preprint arXiv:2506.17612, 2025. [23] Yunlong Lin, Linqing Wang, Kunjie Lin, Zixu Lin, Kaixiong Gong, Wenbo Li, Bin Lin, Zhenxi Li, Shiyi Zhang, Yuyang Peng, et al. Jarvisevo: Towards self-evolving photo editing agent with synergistic editor-evaluator optimization. arXiv preprint arXiv:2511.23002, 2025. [24] Huaping Liu, Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, and Hanbo Zhang. Towards generalist robot policies: What matters in building vision-languageaction models. 2025. [25] Lu Liu, Chunlei Cai, Shaocheng Shen, Jianfeng Liang, Weimin Ouyang, Tianxiao Ye, Jian Mao, Huiyu Duan, Jiangchao Yao, Xiaoyun Zhang, Qiang Hu, and Guangtao Zhai. Moa-vr: mixture-of-agents system towards all-in-one video restoration, 2025. [26] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua Tenenbaum. Compositional visual generation with composable diffusion models. In European conference on computer vision, pages 423439. Springer, 2022. [27] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. [28] Vahid Mirzaei. The impact of artificial intelligence on creativity in graphic design. Available at SSRN 5292032, 2025. [29] Model Context Protocol. Model context protocol (mcp) specification (version 2025-11-25). https: //modelcontextprotocol.io/specification/2025-11-25, November 2025. Accessed: 2026-01-29. [30] Tom Monnier, Elliot Vincent, Jean Ponce, and Mathieu Aubry. Unsupervised layered image decomposition into object prototypes. In Proceedings of the IEEE/CVF international conference on computer vision, pages 86408650, 2021. [31] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. [32] OpenAI. Gpt-image-1, 2025. [33] OpenAI."
        },
        {
            "title": "Introducing",
            "content": "4o image generation. https://openai.com/index/ introducing-4o-image-generation/, 2025. Accessed: 2025-10-29. [34] OpenAI. Operator. https://openai.com/index/introducing-operator/, 2025. Accessed: 2025-10-29. [35] Nobuyuki Otsu et al. threshold selection method from gray-level histograms. Automatica, 11(285296):2327, 1975. [36] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. 12 [37] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. \" grabcut\" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3):309314, 2004. [38] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. [39] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. [40] Peng Wang, Yichun Shi, Xiaochen Lian, Zhonghua Zhai, Xin Xia, Xuefeng Xiao, Weilin Huang, and Jianchao Yang. Seededit 3.0: Fast and high-quality generative image editing. arXiv preprint arXiv:2506.05083, 2025. [41] Zhenyu Wang, Jianxi Huang, Zhida Sun, Yuanhao Gong, Daniel Cohen-Or, and Min Lu. Layered image vectorization via semantic simplification. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 77287738, 2025. [42] Zhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu. Genartist: Multimodal llm as an agent for unified image generation and editing. Advances in Neural Information Processing Systems, 37:128374128395, 2024. [43] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [44] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. [45] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. [46] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. [47] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. [48] Jinrui Yang, Qing Liu, Yijun Li, Soo Ye Kim, Daniil Pakhomov, Mengwei Ren, Jianming Zhang, Zhe Lin, Cihang Xie, and Yuyin Zhou. Generative image layer decomposition with visual effects. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 76437653, 2025. [49] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [50] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations, 2022. [51] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. [52] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2612526135, 2025. [53] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. [54] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang Zhang, Shuangrui Ding, et al. Internlm-xcomposer: vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023. [55] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 13 [56] Ruihan Zhang, Borou Yu, Jiajian Min, Yetong Xin, Zheng Wei, Juncheng Nemo Shi, Mingzhen Huang, Xianghao Kong, Nix Liu Xin, Shanshan Jiang, et al. Generative ai for film creation: survey of recent advances. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 62676279, 2025. [57] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with in-context generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. [58] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. [59] Zijun Zhou, Yingying Deng, Xiangyu He, Weiming Dong, and Fan Tang. Multi-turn consistent image editing. arXiv preprint arXiv:2505.04320, 2025. [60] Kaiwen Zhu, Jinjin Gu, Zhiyuan You, Yu Qiao, and Chao Dong. An intelligent agentic system for complex image restoration problems. In The Thirteenth International Conference on Learning Representations, 2025. [61] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. arXiv preprint arXiv:2305.17144, 2023. [62] Yushen Zuo, Qi Zheng, Mingyang Wu, Xinrui Jiang, Renjie Li, Jian Wang, Yide Zhang, Gengchen Mai, Lihong Wang, James Zou, et al. 4kagent: agentic any image to 4k super-resolution. arXiv preprint arXiv:2507.07105, 2025."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Brown University",
        "Meta AI",
        "Princeton University",
        "TAMU",
        "UCSD",
        "USC",
        "UW-Madison",
        "xAI"
    ]
}