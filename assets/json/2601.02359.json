{
    "paper_title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors",
    "authors": [
        "Kaede Shiohara",
        "Toshihiko Yamasaki",
        "Vladislav Golyanik"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection."
        },
        {
            "title": "Start",
            "content": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors Kaede Shiohara1 1The University of Tokyo Toshihiko Yamasaki1 Vladislav Golyanik2 2Max Planck Institute for Informatics, SIC 6 2 0 2 J 5 ] . [ 1 9 5 3 2 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, fully self-supervised approach based on diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection1. 1. Introduction The recent advances in generative models such as generative adversarial networks [33, 42, 72] and diffusion models [39, 43, 76, 78, 84] enable the generation of photorealistic images and videos of human faces, which poses risk of misuse for creating deceptive or malicious content, commonly referred to as deepfakes. It can undermine public trust, violate privacy, and spread misinformation; therefore, detection of deepfake media is an increasingly important research area. Recent studies have tackled this prob1Project page: ExposeAnyonePage/ https : / / mapooon . github . io / Figure 1. Our self-supervised face forgery detection approach: We pre-train our audio-to-expression diffusion model on largescale, unlabeled video collection. Then, we personalize our pretrained model on the reference videos of person of interest (POI) by inserting subject-specific adapter. Finally, we authenticate suspected videos of POI by the diffusion reconstruction distance. lem by reducing models dependencies on manipulationspecific artifacts [35, 99, 105, 112], synthesizing pseudofake samples that reproduce general artifacts observed in various deepfakes [54, 56, 81, 111], transferring large vision and language models [20, 37, 107], and leveraging audio information for robust detection [36, 61, 114]. However, in general, it is well-known that these methods, which rely on learning forgery patterns from fake or pseudo-fake samples in training sets, inevitably become biased toward the seen manipulations and often fail to generalize to unseen ones [16, 27, 46, 104]. To address the overfitting problem, some work stands 1 on the perspective that detectors should be trained only on pristine data. They quantify the plausibility of input videos based on reconstruction [44], audio-visual consistency [31, 59, 74], and identity consistency with reference sets of specific subjects [17, 18]. However, they still struggle with learning discriminable representations only from self-supervision. As result, the performance of selfsupervised approaches is very limited compared to state-ofthe-art supervised and pseudo-supervised approaches. In this paper, we propose ExposeAnyone, new paradigm for face forgery detection that is independent of any actual fake or pseudo-fake samples yet detects face forgeries effectively. Our key idea is that diffusion models trained to generate person-specific facial expressions distinguish the real videos from fake videos that mimic the subject. We start by curating large-scale audio-expression dataset, reaching 445 hours of data, with our proposed expression feature extraction strategy that disentangles face expression from face shape effectively. Then, as shown in Fig. 1, we pre-train our simple diffusion model called ExposeAnyone Model (EXAM in short) based on Diffusion Transformer [68] with several modifications for audio-toexpression generation. After pre-training, we personalize EXAM to subject by inserting subject-specific adapter to learn the talking identity. Finally, we predict the fakeness of suspected videos, where the personalized subject appears, using our proposed content-agnostic authentication mechanism, which emphasizes the identity discrepancy. Our method achieves 95.22% in the average AUC on the traditional deepfake benchmarks [24, 50, 51, 103] and outperforms the previous state-of-the-art approaches [9, 20, 26, 3537, 54, 61, 63, 81, 88, 99, 105, 112]. Also, we introduce Sora2 Cameo Forensics Preview, the first dataset for Sora2-generated facial video detection. Our method is capable of detecting even Sora2-generated videos, while the previous methods perform poorly. Moreover, we show that our method is highly robust to corruptions, especially severe video compression rate, where the state-of-the-art AltFreezing method [99] drops performance by 36.71 percentage points in AUC, and ours drops just by 2.0 percentage points. Our contributions are summarized as follows: 1. We propose ExposeAnyone, fully self-supervised face forgery detection framework based on audio-toexpression diffusion models. Our model is personalized to specific subjects to learn person-specific talking identity, which enables the model to expose fake videos with state-of-the-art detection performance on not only traditional deepfake videos but also Sora2generated videos. 2. We introduce new 3DMM extraction strategy and content-agnostic authentication in our framework. We validate in ablation studies that both are necessary for accurate detection. Method PWL [1] WTW [3] A&B [2] ID-Reveal [17] POI-Forensics [18] STIDNet [30] Ours Self-Supervision General Prior (w/ ID labels) (w/ ID labels) (w/ ID labels) (w/ ID labels) (w/o ID labels) Personalization Table 1. Concept-level comparison with previous referencebased methods. Our method leverages general prior for personalization in self-supervised fashion. 3. According to our extensive experiments, ExposeAnyone is the only self-supervised method that achieves competitive performance with the previous state-of-the-art approaches, suggesting promising research direction for self-supervised face forgery detection. 2. Related Work Face Forgery Detection. Recent work tries to address the generalization problem revealed by early studies [16, 27, 46, 104]. Some work tackles this problem by introducing general face boundary representation [54], preventing overfitting to spatial information [99, 112], leveraging pre-trained high-level semantics encoders [20, 3537, 107], and combining audio cues [61, 66, 97, 114]. Other work focuses on training data synthesis, such as blended images [12, 54, 111] and self-blended images [40, 81, 92] that reproduce artifacts generally seen in face swapping [13, 22, 29, 53, 82] and face reenactment [90, 91]. Although they significantly improve baselines on existing benchmarks [24, 57, 77], the forgery types are limited within predefined patterns in training data, making it difficult to maintain detection performance on future manipulations. Recent work focuses more on fully self-supervised methods that aim to achieve complete independence of overfitting to specific forgery patterns. OC-FakeDect [44] proposes reconstruction-based quantification of fakeness scores using variational auto-encoders [48]. SpeechForensics [80] detects deepfake videos by computing the semantic similarity between voice signals and lip motions. However, these methods still struggle to generalize due to the vast and unconstrained search space inherent to their self-supervised objectives, e.g., single audio clip can correspond to various plausible visual patterns and vice versa, which makes it difficult to capture the audio-visual inconsistency on highly synchronized deepfake videos. In contrast, we focus on the discrepancy of talking identity between real and fake subjects, which is manipulationinvariant and robust even on high-fidelity deepfake videos. Reference Assistance. Another research direction is reference-based approaches [13, 17, 18] that utilize refi.e., reliable pristine sources of subjects erence videos, appearing in suspected videos. We categorize previous 2 Figure 2. ExposeAnyone framework for face forgery detection. (a) We pre-train an audio-to-expression diffusion model to predict the added noise sequence ϵ1:L from noisy expression sequence z1:L . Then, we personalize the pre-trained model on specific subject by inserting an adapter token sequence c1:K . (b) After personalization, our model can authenticate videos by computing two reconstruction distances w/ and w/o the adapter c1:K . (c) Our model is trained in self-supervised fashion during both pre-training and personalization. work in Table 1 with three important aspects: 1) SelfSupervision, meaning training only on real videos. 2) General Prior, referring to the knowledge of generic facial dynamics learned from large-scale datasets consisting of nonperson-of-interest individuals. 3) Personalization, meaning tuning on person-of-interest individuals. From these aspects, WTW [3] is trained with fake samples to classify whether input videos belong to subject or not; therefore, it can be biased towards the seen manipulations. PWL [1] and WTW [3] are trained from scratch on each subject; therefore, they require very long duration of reference videos due to the lack of general prior, e.g., PWL [1] requires 1 hour for each subject for accurate disentanglement. Other studies [2, 17, 18, 30] train models to extract identity features from videos and detect deepfakes by comparing identity similarity between suspected and reference videos, without personalization; however, their discriminative ability is limited by the small number of identitylabeled subjects in existing video datasets, e.g., the widely used VoxCeleb2 [14] provides only about 6K identities. In contrast, our approach satisfies all three concepts as summarized in Table 1: 1) It does not rely on any fake samples during training. 2) It learns general prior from large-scale video collections without ID labels. 3) It is personalized to specific subjects, enabling the capture of more fine-grained identity-related facial dynamics. 3. Proposed Framework: ExposeAnyone We give the overview of our framework in Fig. 2. We first build simple diffusion model, called ExposeAnyone Model (EXAM in short), that generates facial expressions from audio (Sec. 3.1). To train EXAM, we collect 200K videos with audio channels from publicly available datasets [10, 14, 28] and extract 3D facial expressions [55] by introducing feed-forward initialization and iterative refinement (Sec. 3.2). We pre-train the model on the curated dataset (Sec. 3.3). Then, we personalize our model on specific subjects to learn their talking identities (Sec. 3.4). After that, our model can be used to authenticate suspected videos by comparing two reconstruction distances with and without identity information (Sec. 3.5). 3.1. ExposeAnyone Model (EXAM) We introduce simple diffusion model, called ExposeAnyone Model (EXAM in short), that takes speech signal and generates the corresponding facial expression sequence. Face Representation. We adopt FLAME [55], kind of 3D morphable models, that is widely incorporated in facial animation synthesis [19, 21, 71, 115]. FLAME represents face as set of compressed coefficients ω = (α, β, γ) Rα+β+γ for the face shape α, expression β, and pose γ. EXAM aims to generate coefficients related to inner face 3 motion, specifically, 50 principal expression parameters and three jaw pose parameters, denoted as R53. Generator. We adopt denoising diffusion probabilistic models (DDPMs) [39], class of state-of-the-art generative models [33, 42, 48, 65, 76]. The overview of our model is shown in Fig. 2(a). Because there is no public implementation of diffusion models to generate FLAME parameters solely from audio, we train simple model based on Diffusion Transformer (DiT) [68], which is equipped mainly with Transformer [94] encoder blocks consisting of multi-head self-attention (MHSA), layer normalization (LN) [4], and feed-forward network (FFN). To encode audio information, we employ Wav2Vec 2.0 [5]. It is worth noting that we simply extend the feature-wise linear modulation (FiLM) [69] to sequential modeling, which we name timeand featurewise linear modulation (TiLM). Specifically, for an input sequence x1:L = (x1, x2, ..., xL) RLC and condition sequence a1:L = (a1, a2, ..., aL) RLD, TiLM stylizes each xl with the corresponding al by regressing the mean and scale factors in FiLMs manner: yl = xl s(al) + m(al), (1) where denotes the element-wise multiplication and each function of and consists of Mish [62] activation and linear layer. Compared to Cross-Attention-based conditioning [86, 89, 102] with time complexity of O(L2) for the sequence length L, TiLMs time complexity is O(L). 3.2. Audio-4D Data Curation with Refinement Because the traditional audio-4D datasets [19, 100] provide limited numbers of videos, we extract 3D facial information from 2D videos, similarly to ID-Reveal [17]. First, we collect videos from three public datasets, VoxCeleb2 [14], AVSpeech [28], and Acappella [10]. We filter out inappropriate videos according to the following criteria: 1) The video duration should be longer than eight seconds, i.e., sufficient to observe consecutive facial performances for training; 2) The video should surpass the threshold of 40 in the image quality assessment score by the HyperIQA [87]. After the filtering process, we obtain 200,359 videos of eight seconds each and for total of 445 hours. Then, we extract 3D information from the collected videos as follows. For each set of video frames with length of (= 200), we initialize the FLAME parameter sequence including shapes α1:L RLα, expressions β1:L RLβ, and poses γ1:L RLγ by the pretrained SPECTRE [32]. To disentangle the face shape space from the expression space, we assign the mean shape across frames to the entire video clip. We denote it as α. Next, we optimize the parameters ω1:L = (α, β1:L, γ1:L) by the same objectives as SPECTRE but with the single shape: ˆω1:L = argmin Lem + Llr + Lc, (2) ω1:L 4 where Lem, Llr, and Lc are the emotion loss, lip-reading loss, and geometric constraints, respectively, used in SPECTRE [32]. We optimize the parameters for 10 iterations with learning rate of 5 105 using the Adam [47] optimizer. More details on our data curation can be found in App. B. 3.3. Pre-training We pre-train our diffusion model on our curated dataset (see Sec. 3.2). In this stage, we train all the parameters but the TCN of Wav2Vec. We add noise sequence ϵ1:L to the clean coefficients z1:L by the diffusion forward process [39]: z1:L = αtz1:L + 1 αtϵ1:L, (3) where αt = (cid:81)t s=1 1 βs and βt is constant noise scale at diffusion timestep t. Given the noisy coefficients z1:L , diffusion timestep t, and waveform w1:H , the model predicts the added noise sequence ϵ1:L. We adopt the simple diffusion loss [39] as follows: L1 = (z1:L,w1:H )D, tU [1,T ], ϵ1:LN (µt,σt) (cid:2)ϵ1:L ϵθ1(z1:L , t, w1:H )2 2 (cid:3) , (4) where and ϵθ1 are the pre-training dataset and the denoising network with weight set θ1, respectively. We denote the optimized θ1 as ˆθ1 = argmin L1. It is noteθ worthy that, differently from the previous reference-assisted approaches [2, 17, 18] that perform large-scale pretraining on identity-annotated datasets such as VoxCeleb2 [14], our model is pre-trained only on unlabeled videos, which enables further extension on larger video collections. 3.4. Personalization to Specific Subjects After the pre-training stage, our model works as strong prior to predict facial expressions from audio. Here, we leverage it to learn specific talking identities. Inspired by LLaMA-Adapter [110], we assign learnable token sequence c1:N RN to each subject. c1:N is projected to be inserted into the self-attention layers by two additional linear layers of Wk RCC and Wv RCC. This process is formulated as: = Attention(cid:0)q1:L h1:L , [k1:L where k1:N = Wkc1:N , , k1:N ], [v1:L , v1:N ](cid:1), v1:N = Wvc1:N , (5) , k1:L , and v1:L where q1:L are the original query, key, and value sequences of the i-th Transformer block, respectively. We concatenate k1:N and v1:N with k1:L , respectively, to insert identity information. We train only parameters of c1:N , Wk, and Wv in this stage. This minimal personalization strategy enables us not only to maintain the original knowledge of the pre-trained model without catastrophic forgetting [49], but also to save spatial memory by and v1:L not needing to store the base model for different subjects and only storing 528,384 parameters (where = 512 and = 8) for each subject. Considering the face forgery detection task, where the model should be personalized to lot of subjects, this property is more suitable compared to full fine-tuning. Our objective function is formulated as: L2 = (z1:L,w1:H ) (cid:101)D, tU [1,T ], ϵ1:LN (µt,σt) (cid:2)ϵ1:L ϵθ(z1:L , t, w1:H , c1:N )2 (cid:3) , (6) where (cid:101)D represents reference set of specific subject, and θ = {ˆθ1, θ2} with weight set θ2 = {c1:N , Wk, Wv} for the adapter. We denote the optimized parameter set as ˆθ = {ˆθ1, ˆθ2} with the optimized adapter ˆθ2 = argmin L2. θ2 3.5. Video Authentication with Content-Agnosticity Once the model is personalized, it can represent subjectspecific expression distribution. Here, we expose deepfakes with our personalized model by verifying whether the talking identity of an input video is the same as that of the personalized subject. One of the straightforward approaches to predict the identity distance is to use the reconstruction loss Eq. (6) as an authentication criterion as in DiffusionClassifiers [11, 52], where smaller values may be considered closer talking identities to the subject. However, we found that this approach does not perform well in our case because the values in Eq. (6) significantly vary depending on their contents, i.e., what and how people talk in videos. Therefore, we introduce content-agnostic authentication. For each input sequence set of (z1:L, w1:H ), our authentication score is formulated as: blur the difference between the numerator and denominator in Eq. (7), i.e., prediction errors with and without identity conditioning. Therefore, we empirically set tstart and tend in Eq. (7) to 201 and 800, respectively, which is explored in Table 7 in the supplementary material. We also discuss the effect of different numbers of sampled noise sequences ϵ1:L in Fig. 5(b). For thresholding to make real/fake decision on our unbounded authentication score, please refer to App. D. 3.6. Implementation Details We implement our model in PyTorch [67]. All the experiments are conducted with single NVIDIA A100 GPU. For pre-training, we optimize our model for 100 epochs. We use the ADAN optimizer [101], and the batch size and learning rate are set to 256 and 104, respectively. Pretraining finishes within day. For personalization, we extract expression parameters from reference sets in similar manner to Eq. (2) but use single shape parameters shared across videos belonging to the same subjects. We insert eight adapter tokens for each subject. Because the reference sets are relatively small, we over-sample the videos by the batch size (=256), which enables us to personalize our model on large batch size for stabilized diffusion training. As result, the number of iterations for each subject is (#videos)(#epochs), where we set the number of epochs to 100. Personalization finishes within 15 minutes for each subject with eight videos. During authentication, the diffusion timesteps are sampled at 60 equally spaced points in the range [201, 800], and on each t, we sample 64 different noise sequences ϵ1:L. Inference, excluding feature extraction of z1:L and w1:H , for single video with eight seconds takes 25 seconds. More details can be found in App. C.1 = tU [tstart,tend), ϵ1:LN (µt,σt) tU [tstart,tend], ϵ1:LN (µt,σt) (cid:2)ϵ1:L ϵˆθ(z1:L , t, w1:H , c1:N )2 2 (cid:3) 4. Experiments (cid:104) ϵ1:L ϵˆθ (z1:L , t, w1:H )2 2 , (cid:105) (7) where the denominator works as an adaptive scaling value to cancel the variances of the video contents, which enables evaluating the purer identity distance. Intuitively, when the identity of an input video is the same as the personalized subject, the numerator is smaller than the denominator. notable point in Eq. (7) is which diffusion timesteps should be sampled for authentication. We observe that removing timesteps at both sides, e.g., from 1 to 200 and from 801 to 1000, improves the discriminability of our model. This is because 1) at small timesteps, the data is minimally perturbed by noise, rendering the noise prediction task trivially easy as the original data structure remains largely intact [39], and 2) at large timesteps, the data is almost entirely dominated by noise, making the prediction excessively challenging [84]. These extreme situations 4.1. Problem Setting The objective of face forgery detection is to determine whether given video contains manipulated facial content. We focus on the person-of-interest scenario introduced by Agarwal et al. [1], where the detector can access set of pristine reference videos per subject. This setting is practical in real-world applications, as reference videos can often be collected online. Thanks to the proliferation of social media platforms, obtaining such reference videos has become increasingly feasible, also for non-experts. 4.2. Setup Baselines. We refer to 20 state-of-the-art methods that can be categorized into three groups based on the learning types: 1) Supervised methods [9, 3537, 61, 88, 95, 99, 105, 107, 112] are trained on actual fake samples from FaceForensics++ (FF++) [77] except one [61] trained on Wav2Lip-modified LRS3 [61]. 2) Pseudo-supervised meth5 Learning Type Method Venue Modality Reference Test Set AUC (%) DF-TIMIT DFDCP KoDF IDForge Avg EfficientNet-b4 [88] LipForensics [35] FTCN [112] RECCE [9] RealForensics [36] AltFreezing [99] UCF [105] LipFD [61] FSFM [95] DFD-FCG [37] EFFORT [107] Face X-ray [54] SBI [81] ICT [26] LAA-Net w/ SBI [63] ForensicsAdapter [20] ID-Reveal [17] AVAD [31] POI-Forensics [18] SpeechForensics [59] Ours w/ VoxCeleb2 Ours ICML19 CVPR21 ICCV21 CVPR22 CVPR22 CVPR23 ICCV23 Frame Video Video Frame Video Video Frame NeurIPS24 Audio & Video CVPR25 CVPR25 ICML25 Frame Video Frame CVPR20 CVPR22 CVPR22 CVPR24 CVPR25 Frame Frame Frame Frame Frame Video ICCV21 CVPR23 Audio & Video CVPRW23 Audio & Video NeurIPS24 Audio & Video Audio & Video Audio & Video - - Supervised Pseudo-Supervised Self-Supervised 94.67 96.74 99.91 68.42 97.74 99.82 89.19 54.78 90.56 99.02 94.96 76.01 84.71 77.35 78.51 97.32 60.41 77.39 85.65 71.35 99.49 99.72 60.15 69.89 61.19 49.76 72.23 69.33 77.10 56.13 86.94 90.04 92.89 67.77 88.51 74.59 88.96 93.98 83.84 45.58 85.34 63.61 93.06 93. 70.21 95.98 86.06 53.13 72.56 96.41 62.71 50.76 74.67 97.40 85.22 48.84 87.84 50.53 82.25 86.10 61.64 61.41 64.91 82.98 87.73 95.31 75.88 93.05 93.52 56.19 78.19 95.79 81.46 64.33 87.59 77.56 85.79 54.76 80.94 59.46 79.90 83.92 74.77 55.78 72.60 93.66 93.25 92. 75.23 88.92 85.17 56.88 80.18 90.34 77.12 56.50 84.94 91.00 89.72 61.35 85.50 65.48 82.41 90.33 70.17 60.04 77.13 77.90 93.38 95.22 Table 2. Generalization ability to unseen forgeries on DF-TIMIT, DFDCP, KoDF, and IDForge datasets. The best value for each learning type on each test set is highlighted in bold. Our method achieves 95.22% in terms of the average AUC, resulting in 5% increase over the previous best methods DFD-FCG and ForensicsAdapter (CVPR25). ods [20, 26, 54, 63, 81]2 are trained on synthetic fake samples generated to cover variety of artifacts seen in deep3) Self-supervised methods [17, 18, 31, 59] are fakes. trained without any fake samples, aiming at independence from overfitting to specific forgery patterns. All the models are reproduced using official or third-party implementations except EfficientNet-b4 and Face X-ray, which we re-implement. We also adopt our model pre-trained only on VoxCeleb2 [14] to compare our method with the previous methods [17, 18] on the same training set. More details can be found in App. C.2. Evaluation Datasets. We refer to four audio-visual deepfake detection datasets for evaluation, including DeepfakeTIMIT (DF-TIMIT) [50], Deepfake Detection Challenge Preview (DFDCP) [24], Korean Deepfake Detection (KoDF) [51], and Identity-Driven Multimedia Forgery Detection (IDForge) [103] datasets, where the numbers of subjects are 32, 39, 67, and 53, respectively. Note that we do not adopt some conventional datasets due to the absence of audio channels [15, 57, 58, 77, 113, 116] and identity annotations [25]. Also, we do not adopt the datasets [7, 8, 45] built on VoxCeleb2 [14] because of the overlap with the training data of our method, ID-Reveal, and POI-Forensics. See App. for further details on the evaluation datasets. Sora2 Cameo Forensics Preview (S2CFP) Dataset. To 2We do not adopt ICT-Ref [26] because it assumes dataset-level reference sets, which is totally different from our problem setting. evaluate the model generality not only on traditional deepfake datasets but also on the most recent video generative model Sora2 [85] with Cameo (i.e., personalized video generation) feature, we introduce new dataset, called Sora2 Cameo Forensics Preview (S2CFP) dataset which serves as starting point for face forensics on Sora2. The dataset provides, for each subject, reference set that consists of 40 real videos and test set that consists of 12 real videos and 12 fake ones. For more details, please refer to App. A. Evaluation Metric. We adopt the video-level area under the ROC curve (AUC). For the frame-level methods, we follow the official or third-party inference strategies. If there is no instruction, we simply average the predictions over frames. We also report the average AUC denoted as Avg to evaluate the generalization ability across the datasets. 4.3. Comparison with Previous Methods Table 2 shows the generalization ability to unseen forgeries. We also report for each method the learning type, input modality, and usage of reference. Overall, our method achieves an average AUC of 95.22% on DF-TIMIT, DFDCP, KoDF, and IDForge, demonstrating the highest generalization ability over datasets. The supervised methods often suffer from the domain gap between training and test sets (e.g., 69.33 by AltFreezing on DFDCP and 77.56 by DFD-FCG on IDForge). The pseudo-supervised methods, such as SBI [81] and ForensicsAdapter [20] stabilize detection performance by pseudo-fake augmentation; how6 Detector Type Method Test Set AUC (%) on Each Subject @ijustine @mcuban @sama Avg Face Forgery Detector Diffusion-Generated Image Detector LipForensics AltFreezing DFD-FCG EFFORT ForensicsAdapter POI-Forensics SpeechForensics Ours DIRE AEROBLADE B-Free 48.61 27.78 36.11 85.42 38.89 41.67 54.86 98.61 11.11 43.75 65.97 56.94 38.19 53.47 56.94 82.64 43.75 64.58 84.72 56.25 52.08 71. 33.33 15.97 56.25 61.03 62.50 72.22 63.89 100.00 43.75 91.67 76.39 46.29 27.31 48.61 67.80 61.34 52.55 61.11 94.44 37.04 62.50 71.30 Table 3. Detection capability on Sora2-generated videos. ever, they fail to generalize on KoDF and IDForge datasets. The previous self-supervised approaches [17, 18, 31] struggle to learn effective features only from self-supervision, resulting in poor performance, e.g., the average AUC of 77.13 by POI-Forensics. The result indicates that our method suggests promising research direction for addressing the generalization problem only with self-supervision. Moreover, our model trained only on VoxCeleb2 [14] still outperforms the previous state-of-the-art methods. We can see that this variant drops performance on KoDF from 95.31 to 87.73 because KoDF focuses on the Korean language, which hardly appears in VoxCeleb2; our additional curated training data, especially AVSpeech [28] that includes variety of languages, improves AUC on nonEnglish videos. 4.4. Comparison on Sora2-Generated Videos Here, we evaluate the detectors on our S2CFP dataset described in Sec. 4.2. We give the result on three subjects, i.e., @ijustine, @mcuban, and @sama in Table 3 with stronger or more important baselines from Table 2. In addition, we refer to the state-of-the-art diffusion-generated image detectors [34, 75, 98] for further comparison. We observe that the previous methods of both detector types fail to generalize to Sora2, while our method achieves the average AUC of 94.44%, demonstrating the generalization ability of our model. The comprehensive result is found in App. D. 4.5. Robustness to Perturbations Next, we evaluate models on corrupted videos, which detectors sometimes encounter in real-world scenarios. Following DeeperForensics [41], we adopt seven perturbations, including color saturation, color contrast, block-wise, Gaussian noise, Gaussian blur, JPEG compression, and video compression with five different severity levels {1,2,3,4,5}. In Fig. 3 where severity = 0 means no corruption, we give the result of our method in comparison with LipForensics, AltFreezing, SBI, and ForensicsAdapter that achieve better generalization than the other previous approaches as shown in Table 2, except DFD-FCG and EFFORT, which we do not adopt because of the insufficient AUC compared to the other supervised methods on IDForge. Our method Figure 3. Robustness to common corruptions on IDForge. Severity levels are defined in DeeperForensics [41]. Our method is highly consistent on the perturbations especially compression that detectors encounter frequently in real-world scenarios. (a) d1 (AUC = 58.88) (b) d2 (AUC = 61.34) Figure 4. Effect of the content-agnostic authentication. The direct use of objectives for (a) pre-training and (b) personalization does not work for authentication, while (c) their quotient significantly distinguishes fake samples from real ones. (c) Ours (AUC = 93.45) is highly robust to the corruptions, especially block-wise, Gaussian blur and compression. We observe in the Average figure that although LipForensics and AltFreezing slightly perform better than ours when no corruption is added, they immediately drop performance when severity > 0. In contrast, our method maintains the consistent AUC, demonstrating the applicability in real-world scenarios. 4.6. Framework Analysis 3DMM Extraction Strategy. We investigate the effect of our 3DMM extraction strategy, including feed-forward initialization by SPECTRE [32] and refinement processes described in Sec. 3.1. We compare our model with variance trained on expression sequences directly extracted by SPECTRE without our refinement process. We observe that the model without refinement results in 46.82% in AUC on the DFDCP dataset. This is because SPECTRE predicts different face shape parameters for different frames; there is crucial entanglement between face shape and expression. In contrast, our refinement process, which singularizes the face shape and iteratively optimizes the error in Eq. (2), disentangles expression parameters from the face shape parameter and enables our model to distinguish real expressions from fake ones, achieving 93.45% on DFDCP. Authentication Strategy. We compare our contentagnostic authentication Eq. (7) with two other possibilities in Fig. 4: (a) the distance used in pre-training, i.e., Figure 5. Analyses on ExposeAnyone. (a) Our model achieves higher AUC on different durations of reference videos than the previous methods. (b) Detection accuracy converges well with 64 noise sequences. (c) The adapter with eight tokens per subject brings the most accurate results. (z1:L , t, w1:H )2 d1 = ϵ1:L ϵˆθ1 2 and (b) one in personalization, i.e., d2 = ϵ1:L ϵˆθ(z1:L , t, w1:H , c1:N )2 2. As shown in the figure, the direct use of the objectives (Figs. 4(a) and 4(b)) fails to distinguish fake samples from real ones, while our content-agnostic authentication works as strong indicator for face forgery detection (Fig. 4(c)). Duration of Reference Videos. It is important to examine the effect of the size of the reference set for personalization. Here, we personalize our model on different reference video durations, {15, 30, 60, 120}; see Fig. 5(a). We also evaluate the previous reference-assisted approaches, ID-Reveal [17] and POI-Forensics [18]. Note that we could not obtain the result of POI-Forensics on duration of 15 seconds corresponding to single reference video because the method requires at least two videos to normalize predicted values. We see that our method outperforms the previous methods in all cases with consistent margins. Moreover, given longer durations, the result of our model greatly improves, while those of the previous approaches show marginal improvements. The result also implies that our method could be further improved with additional reference videos. Number of Sampled Noise Sequences. Our model can better estimate the authentication score by sampling more noise sequences, as can be seen from Eq. (7). Hence, we investigate the effect of the number of sampled sequences on the models performance. We plot the results with different numbers of noise sequences, i.e., {1, 4, 16, 64}, in Fig. 5(b). Though more noise slightly improves the result, our model maintains the performance even with single noise sequence compared to 64 sequences (93.25% vs 93.45%), which shows the robustness of our method to the randomness of added noise sequences. We observe that the authentication values converge well with 64 sequences. Number of Adapter Tokens. We also explore the optimal number of adapter tokens inserted for personalization. We show the results with different numbers of tokens, i.e., {1, 2, 4, 8, 16}, in Fig. 5(c). We can see that adding some toFigure 6. Temporal visualization of the authentication score. The solid blue and red lines represent the authentication scores over the frame index of real video and Sora2-generated video mimicking the subject, respectively. The dotted lines are the averaged values, which are statistically lower for real videos than for the corresponding fake videos. kens improves the results (from 91.79% with single token to 93.45% with eight tokens); however, excessive tokens slightly worsen them due to over-fitting to reference data (from 93.45% with eight tokens to 93.37% with 16 tokens). We recommend using eight tokens by default. Temporal Visualization. In Fig. 6, we visualize our authentication score on real video and the corresponding Sora2-generated video of SCFP by unfolding Eq. 7 in the temporal dimension. We average each point with window size of 15 for better viewability and present their average values as dotted lines. We can see that the scores for the real video are lower than those for the fake one, indicating that our model is capable of distinguishing real and fake videos. More examples are included in App. D. 5. Limitations We observe some limitations: 1) Our model depends on the capabilities of the off-the-shelf models, i.e., FLAME [55] for face representation, SPECTRE [32] for feed-forward initialization, and Wav2Vec 2.0 [5] for audio encoding. 2) Our model has computational overhead during inference mainly due to the iterative optimization for 3DMM extraction and diffusion reconstruction over multiple timesteps and noise sequences, which we discuss further in App. D. 6. Conclusion We present ExposeAnyone, self-supervised face forgery detection framework based on an audio-to-expression diffusion model. Our most important observation is that personalizing audio-to-expression diffusion models to specific subjects enables person-of-interest face forgery detection without any prior knowledge of face forgeries. Extensive experiments demonstrate that our approach significantly outperforms the previous state-of-the-art methods in generalization to unseen manipulations, including not only traditional deepfakes but also Sora2 and in robustness to common corruptions, suggesting promising research direction for real-world face forgery detection. ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors"
        },
        {
            "title": "Supplementary Material",
            "content": "In this document, we provide information about (A) the evaluation data setup, (B) the curated dataset for pretraining, (C) the implementation details of our model and baselines, and (D) additional experimental results. A. Evaluation Dataset Most existing deepfake datasets do not provide reference sets explicitly. Therefore, we carefully construct evaluation sets for the experiments. To compare our model with previous methods, datasets should satisfy the following requirements: datasets should include (1) an audio channel, (2) identity labels, and (3) multiple clips for each subject. (4) We also exclude datasets [7, 8, 45] built on VoxCeleb2 [14] because some methods [17, 18] and ours are trained on the dataset. We report the attributes of existing deepfake video datasets in Table 4. Through the evaluation datasets, we exclude videos that are not well face-tracked from our experiments, following the convention (e.g., seen in the LipForensics paper [35]). We describe the construction of our evaluation datasets below: DF-TIMIT [50]. This dataset is built on the VidTIMIT [79] dataset. It includes short-term videos of 32 subjects. Each subject has 10 videos with different sentences. They are split into three sessions according to when they were recorded. We use six videos of Session 1 for reference and four remaining videos of Sessions 2 and 3 for testing. Also, though this dataset provides two different qualities of deepfake videos, we adopt the higher (i.e., more challenging) one as the lower one is easy to detect for current state-ofthe-art methods. DFDCP [24]. This dataset was released as preview for Kaggle competition on deepfake detection3. It contains two types of face-swapped deepfake videos. We uniformly sample eight videos for reference and use the remaining videos for evaluation, ensuring that the same scene indexes do not overlap between the reference and test sets. The evaluation set contains 39 subjects, and its test set consists of 128 real and fake videos. We uniformly sample real and fake videos for the test set, keeping the ratio of real/fake as close to 1 as possible. KoDF [51]. This dataset focuses on the Korean language. We exclude face-swapped videos (i.e., DeepFakeFaceSwap [22], DeepFaceLab [60], and FSGAN [64]) and only adopt face-reenacted videos (i.e., Audio-driven methods [70, 108] and First Order Motion Model [83]) for eval3https://www.kaggle.com/competitions/deepfakedetection-challenge Dataset DF-TIMIT [50] UADFV [109] FF++ [77] DFD [15] Celeb-DFv2 [57] DFDCP [24] DFDC [25] DeeperForensics [41] FFIW [113] KoDF [51] FakeAVCeleb [45] LAV-DF [7] AV-Deepfake1M [8] IDForge [103] Celeb-DF++ [58] Audio channel Identity label Multiple clips for each subject Independent from VoxCeleb2 Table 4. Attributes of deepfake video datasets. In the main paper, we adopt DF-TIMIT, DFDCP, KoDF, and IDForge datasets that satisfy the requirements for fair evaluation. Dataset #ID Reference Set Test Set #Real Duration[s] #Real #Fake Duration[s] DF-TIMIT DFDCP KoDF IDForge S2CFP 32 39 67 53 3 192 312 4280 2880 120 4.42 15.08 8.00 6.89 8. 128 366 268 187 36 128 378 268 184 36 4.00 8.00 8.00 7.08 8.00 Table 5. Statistics of our evaluation datasets. #ID, #Real, #Fake, and Duration represent the number of identities, the number of real and fake videos, and the mean duration (in seconds) per video, respectively. uation. This is because this dataset includes only single scene (e.g., background and cloth) for each subject; reference-assisted methods can easily spot face-swapped videos by just comparing such irrelevant attributes of input videos with those of reference ones, not based on the talking identities. The evaluation set includes 67 subjects. We use 64 videos at most for each reference set with duration of eight seconds. We uniformly sample at most four respective real and fake videos for each subject; we exclude videos whose faces are not tracked well, and exclude subjects who do not have one or more well-tracked videos for real and fake classes. IDForge [103]. This dataset provides large-scale video collection for reference-assisted face forgery detection. It pre-defines the reference and test sets for each subject; therefore, we follow the official split. The evaluation set includes 67 subjects. We use 64 videos at most for each reference set, with duration of eight seconds at most. We uniformly sample at most four respective real and fake videos for each subject; we exclude videos whose faces are not Figure 7. Pre-training dataset statistics. (a) We collect monocular videos from three sources: VoxCeleb2, AVSpeech, and Acappella datasets. (b) We visualize the ratio of languages spoken in our dataset. (c) All the sequences of our dataset are clipped into eight seconds. (d) We also visualize the Word Cloud for English. tracked well, and exclude subjects who do not have one or more well-tracked videos for real and fake classes. S2CFP. We create new dataset to evaluate detection models on the most recent video generation model Sora2 [85]. It provides the Cameo feature that enables us to generate specific subjects, although Sora2 does not allow us to generate specific subjects directly from text prompts. Because we can generate only those who make their Cameo avatars public, we collect only three subjects at the moment who are famous and easy to collect real videos on the Web. We will enlarge our dataset so that it includes more subjects in the future. For each subject, we collect eight real videos of different scenes, then we split them into four videos each for reference and testing. For each video of the reference set, we manually split it into 10 eight-second clips, ensuring that each clip includes only the portions in which the subject is actually speaking and that the subjects face can be detected to extract facial landmarks [6] on all the frames. Regarding the test set, we split it into three eight-second clips in the same manner as the reference set. Then, we generate fake videos with Sora2. To produce high-quality dataset reducing biases towards video contents, we tried to generate videos whose contents are similar to the original videos. To this end, we used GPT-5 to generate the caption of frame of each video and used Whisper [73] to generate the transcription of each speech. After that, we input the captions and transcriptions into Sora2 to generate videos corresponding to the original videos, specifying the subjects by the Cameo feature. Note that we replace some sensitive words (i.e., someones name) in transcriptions with an abstract word such as man because Sora2 refuses such sensitive prompts. The S2CFP dataset is available on our project page. B. Curated Dataset for Pre-training We describe the details of curation of our pre-training dataset. The overview of the dataset is shown in Fig. 7. Hyperparameter Value Optimizer Learning Rate Diffusion Steps β schedule Video Duration Video FPS Expression Dimension Transformer Dimension MLP Dimension Num Heads Num Layers Dropout Classifier-Free Dropout Adapter Dimension Adapter Length Adan [101] 4e-4 1000 Linear 8 seconds 25 53 512 1024 8 8 0.1 0.25 512 8 Table 6. Hyperparameters for EXAM. Note that we use Whisper [73] to detect language spoken in videos. The specific data sources and pre-processing are as follows: VoxCeleb2 [14]. This dataset is one of the large-scale datasets of talking head videos from YouTube. The videos are already cropped into small region around the face. To improve the quality of the samples from this dataset, we only adopt identity-consistent videos, ensuring that single identity appears in each video. We compute the identity similarity by ArcFace [23] between all the pairs of frames and then exclude videos that include one or more frames with lower identity similarity than 0.4. As result, we obtain 140, 194 videos. AVSpeech [28]. This dataset is also video dataset of talking people from YouTube. We clip the talking parts using the provided annotations. Because the videos are provided without cropping, we track the faces by checking the overlap of the bounding boxes of the adjacent frames. As result, we obtain 45, 706 videos. Acappella [10]. This dataset provides videos of singing people from YouTube. Each video contains not only singing parts but also talking parts; we adopt both parts to enlarge 10 our dataset. We track and crop the videos in the same manner as the pre-processing for the AVSpeech dataset. As result, we obtain 14, 459 videos. C. Implementation Details C.1. ExposeAnyone Model Network Architecture. We adopt similar network with DiT [68] and EDGE [93], but newly introduce TiLM layers for sequential conditioning described in Sec. 3.1. The hyperparameters of our model are shown in Table 6. Condition Guidance. Similar to classifier-free guidance (CFG) [38], we train our model with learnable unconditional vectors for both audio and identity conditions to control the strength of conditions during inference. Importantly, we empirically find that, in contrast to generative tasks (e.g., text-to-image synthesis [76]) where models aim to emphasize conditions, the large strength of guidance harms the detection performance. We set the strengths sa and sc for audio and identity conditions to 0.5 and 0.25, respectively: ϵsa ˆθ (z1:L , t, w1:H ) = ϵˆθ1 (z1:L , t) + saδa, δa = ϵˆθ1 (z1:L , t, w1:H ) ϵˆθ1 (z1:L , t) (8) (9) ϵ{sa,sc} ˆθ (z1:L , t, w1:H , c) = ϵsa ˆθ1 (z1:L , t, w1:H ) + scδc, δc = ϵˆθ(z1:L , t, w1:H , c) ϵˆθ1 (z1:L , t, w1:H ) (10) (11) (z1:L (z1:L (z1:L , t, w1:H ) and ϵ{sa,sc} , t, w1:H , c) ˆθ , t, w1:H , c) in Eq. (7) in We denote ϵsa ˆθ1 , t, w1:H ) and ϵˆθ(z1:L as ϵˆθ1 the main paper for simplicity, respectively. 3DMM Extraction. For pre-training, we assign single shape shared within each clip, assuming that different videos have different face shapes, although some identities overlap with each other. For personalization and authentication, we extract single shape shared between videos considered to belong to an identical subject. This is achieved by using the shape extracted from the first reference video as fixture in optimizations for all other videos. C.2. Baselines EfficientNet-b4. We train the model on the FF++ [77] dataset, including real videos and their manipulated ones by Deepfakes [22], Face2Face2 [90], FaceSwap [29], and NeuralTextures [91] using the same pre-process, augmentations, and inference strategy as SBI [81]. Face X-ray. Because there is no official implementation, we re-implement it. We strictly follow the training setting of the original paper; we freeze the backbone [96] for the first 50K iterations and then update all the layers for the remaining 150K iterations on real and blended images. We 11 Setting DF-TIMIT DFDCP KoDF IDForge Avg U[1, 1000] U[101, 900] U[201, 800] U[301, 700] 99.94 99.84 99.72 99.33 91.68 92.84 93.45 93.48 96.02 96.14 95.31 94.07 90.63 91.96 92.40 92.41 94.57 95.20 95.22 94. Table 7. Study on timestep sampling. Excluding early and late timesteps where identity information is not effective for denoising improves the results. Our default setting is highlighted in gray . Method Threshold KoDF Method #Params Time[s] AltFreezing DFD-FCG Ours Ours Ours 0.5 0.5 µ + σ µ + 2σ µ + 3σ 86.75 86.57 89.73 90.85 86.38 LipForensics AltFreezing SBI ForensicsAdapter Ours 36M 27M 18M 435M 0.67 3.56 0.82 0. 31M + 36M 22.2 + 23.6 Avg 88.92 90.34 85.50 90.33 95.22 Table 8. ACC on KoDF. Table 9. Model complexity analysis. generate the blended images using the authors unofficial implementation4. UCF. We use the DeepfakeBenchs implementation [106]. Others. We directly adopt their official implementations. D. Additional Experiments Timestep Sampling. We explore the sampling strategy of diffusion timesteps during authentication described in Sec. 3.5. We evaluate variants of our method with different sets of timesteps, i.e., [1, 1000], [101, 900], [201, 800], and [301, 700] in Table 7. It can be observed that excluding both sides of timesteps from authentication helps our model detect deepfakes more accurately (94.57% by [1, 1000] vs. 95.22% by [201, 800] in the average AUC). However, the excessive exclusion harms the detection performance (95.22% by [201, 800] vs. 95.20% by [301, 700] in the average AUC). We recommend using the range [201, 800] by default for stable performance on different datasets. Thresholding. In the main paper, we focus on the potential discriminative ability by evaluating models with thresholding-free metric, i.e., AUC. Here, we describe how to decide whether videos are real or fake. We assume that the prediction scores from each subject follow subjectspecific Gaussian distribution. Therefore, we compute the mean µ and the unbiased standard deviation σ from validation set including eight real videos of the subject. Then, we set threshold to divide the real and fake classes. To evaluate the effectiveness, we compute the accuracy (ACC) on KoDF in Table 8 with thresholds µ + σ, µ + 2σ, and µ + 3σ. We also show the results of AltFreezing and DFDFCG, which perform best on KoDF as shown in Table 2, with the threshold simply set to 0.5. Our method achieves consistent accuracy with diverse thresholds, which indicates that our model accurately distinguishes real videos from deepfakes. Note that it is difficult to perform this experiment on the DF-TIMIT and DFDCP datasets because they 4https://github.com/AlgoHunt/Face-Xray Method EfficientNet-b4 LipForensics FTCN RECCE RealForensics AltFreezing UCF LipFD FSFM DFD-FCG EFFORT Face X-ray SBI ICT LAA-Net w/ SBI ForensicsAdapter ID-Reveal AVAD POI-Forensics SpeechForensics DIRE AEROBLADE B-Free Ours Test Set AUC (%) on Each Subject @ijustine @mcuban @sama Avg 65.97 48.61 34.03 46.53 45.83 27.78 36.11 17.36 40.97 36.11 85.42 4.86 45.14 71.53 22.22 38.89 83.33 9.03 41.67 54.86 11.11 43.75 65.97 98.61 41.67 56.94 56.94 66.67 58.33 38.19 12.50 51.39 76.39 53.47 56.94 45.83 45.83 78.47 37.50 82.64 87.50 00.00 43.75 64.58 56.25 52.08 71.53 84.72 35.42 33.33 29.86 49.31 51.39 15.97 28.47 50.69 70.14 56.25 61.03 18.06 50.69 34.72 70.83 62.50 75.69 15.97 72.22 63.89 43.75 91.67 76.39 100.00 47.69 46.29 40.28 54.17 51.85 27.31 25.69 39.81 62.50 48.61 67.80 22.92 47.22 61.57 43.52 61.34 81.02 8.33 52.55 61.11 37.04 62.50 71.30 94. Table 11. Comprehensive comparison on S2CFP. the best result with large margin. Additional Visualizations. We show additional examples of temporal authentication scores in Fig. 8. Overall, our model performs well on wide range of subjects and scenes and is robust to variety of manipulations. We obtain some important observations from the figures: 1) The authentication scores are slightly unstable at silent frames e.g., the beginning and ending of videos. 2) Our model distinguishes real samples from fake ones even in cases where deepfakes appearances are much closer to real ones, as seen on KoDF and S2CFP, which indicates that our method focuses on highly semantic talking identities for face forgery detection. Setting DF-TIMIT DFDCP KoDF IDForge Avg w/o Audio Ours 99.46 99.72 90.76 93.45 93.41 95.31 91.24 92.40 93.72 95. Table 10. Effect of audio conditioning. have limited number of videos for each subject to prepare validation sets. Complexity Analysis. We compare our model with the state-of-the-art methods in terms of the model complexity in Table 9. We compute the number of parameters and the inference time, excluding data loading, per video with eight seconds on single NVIDIA A100 GPU. Note that we follow the official inference strategy for AltFreezing, which can take more time than straightforward inference. Our model has 47M parameters consisting of 31M of SPECTRE and 36M of our diffusion model, which is much smaller than ForensicsAdapter. Our inference takes 22.2 and 23.6 seconds for 3DMM extraction and diffusion authentication, respectively, which is slower than the previous methods. We do not focus on the optimization of inference time in this paper, and it could be improved as follows in future: First, because our 3DMM extraction strategy performs iterative refinement, taking long time, developing feed-forward extraction model that directly predicts disentangled FLAME parameters can drastically reduce the overhead. Second, we can reduce the diffusion costs by using smaller number of noise sequences; we observe in Fig. 5(b) that using quarter of our default number of noise sequences achieves the same AUC on DFDCP. Effect of Audio Conditioning. We demonstrate in our framework that conditioning reconstruction on audio helps detection accuracy. To this end, we perform inference by replacing audio conditions with the learned unconditional vector (see App. C.1 and CFG [38] for the unconditional vector). As shown in Table 10, our model without audio conditions drops the AUCs on all the test sets. This result indicates that audio conditioning helps the prediction of the expression coefficients and thus improves detection performance. Notably, our method without audio still achieves the state-of-the-art generalization ability in Table 2. Comprehensive Comparison on S2CFP. We show the result on S2CFP with all the baselines from Table 2 in Table 11. In addition, we refer to the state-of-the-art diffusiongenerated image detectors [34, 75, 98]. DIRE [98] argues that diffusion models reconstruct diffusion-generated images more precisely than real images, which enables general diffusion-generated image detection. AEROBLADE [75] applies VAE reconstruction to expose latentdiffusion-generated images. B-Free [34] carefully curates its training set so that there is no content difference between real and fake classes to mitigate biases towards image contents. Even compared with these methods, specialized in diffusion-generated image detection, our method achieves 12 (a) On DF-TIMIT (b) On DFDCP (c) On KoDF (d) On IDForge (e) On S2CFP Figure 8. Additional visualizations. The solid blue and red lines represent the authentication scores over the frame index of real videos and those of deepfakes mimicking the subjects, respectively; the dotted lines denote the corresponding averages."
        },
        {
            "title": "References",
            "content": "[1] Shruti Agarwal, Hany Farid, Yuming Gu, Mingming He, Koki Nagano, and Hao Li. Protecting world leaders against deep fakes. In CVPR Workshop, 2019. 2, 3, 5 [2] Shruti Agarwal, Hany Farid, Tarek El-Gaaly, and Ser-Nam Lim. Detecting deep-fake videos from appearance and behavior. In WIFS, 2020. 2, 3, 4 [3] Shruti Agarwal, Liwen Hu, Evonne Ng, Trevor Darrell, Hao Li, and Anna Rohrbach. Watch those words: Video falsification detection using word-conditioned facial motion. In WACV, 2023. 2, 3 [4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv:1607.06450, 2016. 4 [5] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for selfsupervised learning of speech representations. In NeurIPS, 2020. 4, [6] Adrian Bulat and Georgios Tzimiropoulos. How far are we from solving the 2d & 3d face alignment problem? (and dataset of 230,000 3d facial landmarks). In ICCV, 2017. 10 [7] Zhixi Cai, Kalin Stefanov, Abhinav Dhall, and Munawar Hayat. Do you really mean that? content driven audiovisual deepfake dataset and multimodal method for temporal forgery localization. In DICTA, 2022. 6, 9 [8] Zhixi Cai, Shreya Ghosh, Aman Pankaj Adatia, Munawar Hayat, Abhinav Dhall, Tom Gedeon, and Kalin Stefanov. Av-deepfake1m: large-scale llm-driven audiovisual deepfake dataset. In MM, 2024. 6, 9 [9] Junyi Cao, Chao Ma, Taiping Yao, Shen Chen, Shouhong Ding, and Xiaokang Yang. End-to-end reconstructionclassification learning for face forgery detection. In CVPR, 2022. 2, 5, 6 [10] cappella: Audio-visual Singing Voice Separation. 3d face model for pose and illumination invariant face recognition. In BMVC, 2021. 3, 4, 10 [11] Huanran Chen, Yinpeng Dong, Zhengyi Wang, Xiao Yang, Chengqi Duan, Hang Su, and Jun Zhu. Robust classification via single diffusion model. In ICML, 2024. 5 [12] Liang Chen, Yong Zhang, Yibing Song, Lingqiao Liu, and Jue Wang. Self-supervised learning of adversarial examples: Towards good generalizations for deepfake detections. In CVPR, 2022. [13] Renwang Chen, Xuanhong Chen, Bingbing Ni, and Yanhao Ge. SimSwap: An Efficient Framework For High Fidelity Face Swapping. In MM, 2020. 2 [14] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. Voxceleb2: Deep speaker recognition. In INTERSPEECH, 2018. 3, 4, 6, 7, 9, 10 [15] Contributing to data deepfake research. https : / / ai . googleblog . com / 2019 / 09 / contributing - data - to - deepfake - detection.html. Accessed: 2025-11-4. 6, 9 detection [16] Davide Cozzolino, Justus Thies, Andreas Rossler, Christian Riess, Matthias Nießner, and Luisa Verdoliva. Forensictransfer: Weakly-supervised domain adaptation for forgery detection. arXiv:1812.02510, 2018. 1, 2 14 [17] Davide Cozzolino, Andreas Rossler, Justus Thies, Matthias Id-reveal: Identity-aware In ICCV, 2021. 2, 3, 4, 6, 7, Nießner, and Luisa Verdoliva. deepfake video detection. 8, [18] Davide Cozzolino, Alessandro Pianese, Matthias Nießner, and Luisa Verdoliva. Audio-visual person-of-interest deepfake detection. In CVPR Workshop, 2023. 2, 3, 4, 6, 7, 8, 9 [19] Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw, Anurag Ranjan, and Michael Black. Capture, learning, and synthesis of 3D speaking styles. In CVPR, 2019. 3, 4 [20] Xinjie Cui, Yuezun Li, Ao Luo, Jiaran Zhou, and Junyu Dong. Forensics adapter: Adapting clip for generalizable face forgery detection. In CVPR, 2025. 1, 2, 6 [21] Radek Danˇeˇcek, Kiran Chhatre, Shashank Tripathi, Yandong Wen, Michael Black, and Timo Bolkart. Emotional speech-driven animation with content-emotion disentanglement. In SIGGRAPH Asia, 2023. 3 [22] Deepfakes. https://github.com/deepfakes/ faceswap. Accessed: 2025-11-4. 2, 9, [23] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR, 2019. 10 [24] Brian Dolhansky, Russ Howes, Ben Pflaum, Nicole Baram, and Cristian Canton Ferrer. The deepfake detection challenge (dfdc) preview dataset. arXiv:1910.08854, 2019. 2, 6, 9 [25] Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Canton Ferrer. The deepfake detection challenge dataset. arXiv:2006.07397, 2020. 6, 9 [26] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Ting Zhang, Weiming Zhang, Nenghai Yu, Dong Chen, Fang Wen, and Baining Guo. Protecting celebrities from deepfake with identity consistency transformer. In CVPR, 2022. 2, 6 [27] Mengnan Du, Shiva Pentyala, Yuening Li, and Xia Hu. Towards generalizable deepfake detection with locality-aware autoencoder. In CIKM, 2020. 1, [28] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William Freeman, and Michael Rubinstein. Looking to listen at the cocktail party: speaker-independent audio-visual model for speech separation. SIGGRAPH, 2018. 3, 4, 7, 10 [29] FaceSwap. MarekKowalski / FaceSwap/. 11-4. 2, 11 https : / / github . com / 2025Accessed: [30] Mingqi Fang, Lingyun Yu, Hongtao Xie, Qingfeng Tan, Zhiyuan Tan, Amir Hussain, Zezheng Wang, Jiahong Li, and Zhihong Tian. Stidnet: Identity-aware face forgery deIEEE tection with spatiotemporal knowledge distillation. Transactions on Computational Social Systems, 2024. 2, 3 Selfsupervised video forensics by audio-visual anomaly detection. In CVPR, 2023. 2, 6, [31] Chao Feng, Ziyang Chen, and Andrew Owens. [32] Panagiotis P. Filntisis, George Retsinas, Foivos ParaperasPapantoniou, Athanasios Katsamanis, Anastasios RousVisual speech-aware persos, and Petros Maragos. ceptual 3d facial expression reconstruction from videos. arXiv:2207.11094, 2022. 4, 7, 8 [33] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014. 1, 4 [34] Fabrizio Guillaro, Giada Zingarini, Ben Usman, Avneesh Sud, Davide Cozzolino, and Luisa Verdoliva. bias-free training paradigm for more general ai-generated image detection. In CVPR, 2025. 7, 12 [35] Alexandros Haliassos, Konstantinos Vougioukas, Stavros Petridis, and Maja Pantic. Lips dont lie: generalisable In CVPR, and robust approach to face forgery detection. 2021. 1, 2, 5, 6, [36] Alexandros Haliassos, Rodrigo Mira, Stavros Petridis, and Maja Pantic. talking faces via selfsupervision for robust forgery detection. In CVPR, 2022. 1, 6 Leveraging real [37] Yue-Hua Han, Tai-Ming Huang, Kai-Lung Hua, and JunCheng Chen. Towards more general video-based deepfake detection through facial component guided adaptation for foundation model. In CVPR, 2025. 1, 2, 5, 6 [38] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshop, 2021. 11, 12 [39] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 1, 4, 5 [40] Po-Han Huang, Yue-Hua Han, Ernie Chu, Jun-Cheng Chen, and Kai-Lung Hua. Multi-task self-blended images for face forgery detection. In MM Asia, 2024. [41] Liming Jiang, Ren Li, Wayne Wu, Chen Qian, and Chen Change Loy. DeeperForensics-1.0: large-scale In CVPR, dataset for real-world face forgery detection. 2020. 7, 9 [42] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In CVPR, 2019. 1, 4 [43] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In NeurIPS, 2022. 1 [44] Hasam Khalid and Simon S. Woo. Oc-fakedect: Classifying deepfakes using one-class variational autoencoder. In CVPRW, 2020. 2 [45] Hasam Khalid, Minha Kim, Shahroz Tariq, and Simon S. Woo. Evaluation of an audio-video multimodal deepfake dataset using unimodal and multimodal detectors. In MM Workshop, 2021. 6, 9 [46] Ali Khodabakhsh, Raghavendra Ramachandra, Kiran Raja, Pankaj Wasnik, and Christoph Busch. Fake face detection methods: Can they be generalized? In BIOSIG, 2018. 1, 2 [47] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. In ICLR, 2015. 4 [48] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv:1312.6114, 2013. 2, 4 [49] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka GrabskaBarwinska, et al. Overcoming catastrophic forgetting in neural networks. PNAS, 2017. 4 [50] Pavel Korshunov and Sebastien Marcel. Deepfakes: new threat to face recognition? assessment and detection. arXiv:1812.08685, 2018. 2, 6, 9 [51] Patrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo Park, and Gyeongsu Chae. Kodf: large-scale korean deepfake detection dataset. In ICCV, 2021. 2, 6, [52] Alexander C. Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is secretly zero-shot classifier. In ICCV, 2023. 5 [53] Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, and Fang Wen. Advancing high fidelity identity swapping for forgery detection. In CVPR, 2020. 2 [54] Lingzhi Li, Jianmin Bao, Ting Zhang, Hao Yang, Dong Chen, Fang Wen, and Baining Guo. Face x-ray for more general face forgery detection. In CVPR, 2020. 1, 2, 6 [55] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and Javier Romero. Learning model of facial shape and expression from 4D scans. SIGGRAPH Asia, 2017. 3, 8 [56] Yuezun Li and Siwei Lyu. Exposing deepfake videos by detecting face warping artifacts. In CVPR Workshop, 2019. 1 [57] Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei Lyu. Celeb-df: large-scale challenging dataset for deepfake forensics. In CVPR, 2020. 2, 6, 9 [58] Yuezun Li, Delong Zhu, Xinjie Cui, and Siwei Lyu. Celebdf++: large-scale challenging video deepfake benchmark for generalizable forensics. arXiv:2507.18015, 2025. 6, 9 [59] Yachao Liang, Min Yu, Gang Li, Jianguo Jiang, Boquan Li, Feng Yu, Ning Zhang, Xiang Meng, and Weiqing Huang. Speechforensics: Audio-visual speech representation learning for face forgery detection. In NeurIPS, 2024. 2, 6 [60] Kunlin Liu, Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Wenbo Zhou, and Weiming Zhang. Deepfacelab: Integrated, flexible and extensible face-swapping framework. Pattern Recogn., 2023. 9 [61] Weifeng Liu, Tianyi She, Jiawei Liu, Boheng Li, Dongyu Yao, Ziyou Liang, and Run Wang. Lips are lying: Spotting the temporal inconsistency between audio and visual in lipsyncing deepfakes. In NeurIPS, 2024. 1, 2, 5, [62] Diganta Misra. Mish: self regularized non-monotonic activation function. In BMVC, 2020. 4 [63] Dat Nguyen, Nesryne Mejri, Inder Pal Singh, Polina Kuleshova, Marcella Astrid, Anis Kacem, Enjie Ghorbel, and Djamila Aouada. Laa-net: Localized artifact attention network for quality-agnostic and generalizable deepfake detection. In CVPR, 2024. 2, 6 [64] Yuval Nirkin, Yosi Keller, and Tal Hassner. Fsgan: Subject agnostic face swapping and reenactment. In ICCV, 2019. 9 and Koray Kavukcuoglu. Neural discrete representation learning. In NeurIPS, 2017. 4 [65] Aaron van den Oord, Oriol Vinyals, [66] Trevine Oorloff, Surya Koppisetti, Nicol`o Bonettini, Divyaraj Solanki, Ben Colman, Yaser Yacoob, Ali Shahriyari, and Gaurav Bharaj. Avff: Audio-visual feature fusion for video deepfake detection. In CVPR, 2024. 2 [67] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming 15 Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. NeurIPS, 2019. 5 [84] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 1, [85] Sora2. https://sora.chatgpt.com/. Accessed: [68] William Peebles and Saining Xie. Scalable diffusion mod2025-11-4. 6, 10 els with transformers. In ICCV, 2023. 2, 4, 11 [69] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with general conditioning layer. In AAAI, 2018. 4 [70] KR Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, and CV Jawahar. lip sync expert is all you need for speech to lip generation in the wild. In MM, 2020. 9 [71] Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, and Matthias Nießner. Gaussianavatars: Photorealistic head avatars with rigged 3d gaussians. In CVPR, 2024. [72] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv:1511.06434, 2015. 1 [73] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech In Internarecognition via large-scale weak supervision. tional conference on machine learning. PMLR, 2023. 10 [74] Tal Reiss, Bar Cavia, and Yedid Hoshen. Detecting deepfakes without seeing any. arXiv:2311.01458, 2023. 2 [75] Jonas Ricker, Denis Lukovnikov, and Asja Fischer. Aeroblade: Training-free detection of latent diffusion images using autoencoder reconstruction error. In CVPR, 2024. 7, 12 [76] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 1, 4, 11 [77] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Niessner. Faceforensics++: Learning to detect manipulated facial images. In ICCV, 2019. 2, 5, 6, 9, 11 [78] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Photorealistic text-to-image diffusion models Norouzi. with deep language understanding. In NeurIPS, 2022. [79] Conrad Sanderson. The VidTIMIT Database. Idiap-Com Idiap-Com-06-2002, IDIAP, 2002. 9 [80] Sahibzada Adil Shahzad, Ammarah Hashmi, Yan-Tsung Peng, Yu Tsao, and Hsin-Min Wang. Av-lip-sync+: Leveraging av-hubert to exploit multimodal inconsistency for video deepfake detection. arXiv:2311.02733, 2023. 2 [81] Kaede Shiohara and Toshihiko Yamasaki. Detecting deepfakes with self-blended images. In CVPR, 2022. 1, 2, 6, 11 [82] Kaede Shiohara, Xingchao Yang, and Takafumi Taketomi. Blendface: Re-designing identity encoders for faceswapping. In ICCV, 2023. 2 [83] Aliaksandr Siarohin, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. NeurIPS, 2019. Stephane Lathuili`ere, [86] Stefan Stan, Kazi Injamamul Haque, and Zerrin Yumak. Facediffuser: Speech-driven 3d facial animation synthesis using diffusion. In SIGGRAPH, 2023. 4 [87] Shaolin Su, Qingsen Yan, Yu Zhu, Cheng Zhang, Xin Ge, Jinqiu Sun, and Yanning Zhang. Blindly assess image quality in the wild guided by self-adaptive hyper network. In CVPR, 2020. 4 [88] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In ICML, 2019. 2, 5, 6 [89] Balamurugan Thambiraja, Ikhsanul Habibie, Sadegh Aliakbarian, Darren Cosker, Christian Theobalt, and Justus Thies. Imitator: Personalized speech-driven 3d facial animation. In ICCV, 2023. 4 [90] Justus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, and Matthias Nießner. Face2face: Real-time face capture and reenactment of rgb videos. In CVPR, 2016. 2, [91] Justus Thies, Michael Zollhofer, and Matthias Nießner. Deferred neural rendering: Image synthesis using neural textures. TOG, 2019. 2, 11 [92] Jiahe Tian, Cai Yu, Xi Wang, Peng Chen, Zihao Xiao, Jiao Dai, Jizhong Han, and Yesheng Chai. Real appearance modeling for more general deepfake detection. In ECCV, 2024. 2 [93] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge: Editable dance generation from music. In CVPR, 2023. 11 [94] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 4 [95] Gaojian Wang, Feng Lin, Tong Wu, Zhenguang Liu, Zhongjie Ba, and Kui Ren. Fsfm: generalizable face security foundation model via self-supervised facial representation learning. In CVPR, 2025. 5, 6 [96] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. TPAMI, 2020. 11 [97] Yifan Wang, Xuecheng Wu, Jia Zhang, Mohan Jing, Keda Lu, Jun Yu, Wen Su, Fang Gao, Qingsong Liu, Jianqing Sun, and Jiaen Liang. Building robust video-level deepfake detection via audio-visual local-global interactions. In MM, 2024. 2 [98] Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen, and Houqiang Li. Dire for diffusion-generated image detection. In ICCV, 2023. 7, [99] Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, and Houqiang Li. Altfreezing for more general video face forgery detection. In CVPR, 2023. 1, 2, 5, 6 [100] Cheng-hsin Wuu, Ningyuan Zheng, Scott Ardisson, Rohan Bali, Danielle Belko, Eric Brockmeyer, Lucas Evans, Timothy Godisart, Hyowon Ha, Xuhua Huang, et al. Multiface: 16 dataset for neural face rendering. arXiv:2207.11243, 2022. 4 [101] Xingyu Xie, Pan Zhou, Huan Li, Zhouchen Lin, and Shuicheng Yan. Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models. TPAMI, 2024. 5, 10 [102] Jinbo Xing, Menghan Xia, Yuechen Zhang, Xiaodong Cun, Jue Wang, and Tien-Tsin Wong. Codetalker: Speech-driven 3d facial animation with discrete motion prior. In CVPR, 2023. [103] Junhao Xu, Jingjing Chen, Xue Song, Feng Han, Haijun Shan, and Yu-Gang Jiang. Identity-driven multimedia forgery detection via reference assistance. In MM, 2024. 2, 6, 9 [104] Xinsheng Xuan, Bo Peng, Wei Wang, and Jing Dong. On the generalization of gan image forensics. In CCBR, 2019. 1, 2 [105] Zhiyuan Yan, Yong Zhang, Yanbo Fan, and Baoyuan Wu. Ucf: Uncovering common features for generalizable deepfake detection. In ICCV, 2023. 1, 2, 5, 6 [106] Zhiyuan Yan, Yong Zhang, Xinhang Yuan, Siwei Lyu, and Baoyuan Wu. Deepfakebench: comprehensive benchmark of deepfake detection. In NeurIPS, 2023. 11 [107] Zhiyuan Yan, Jiangming Wang, Peng Jin, Ke-Yue Zhang, Chengchun Liu, Shen Chen, Taiping Yao, Shouhong Ding, Baoyuan Wu, and Li Yuan. Orthogonal subspace decomposition for generalizable AI-generated image detection. In ICML, 2025. 1, 2, 5, 6 [108] Ran Yi, Zipeng Ye, Juyong Zhang, Hujun Bao, and YongJin Liu. Audio-driven talking face video generation with learning-based personalized head pose. arXiv:2002.10137, 2020. 9 [109] Ming-ching Chang Yuezun Li and Siwei Lyu. In ictu oculi: Exposing ai generated fake face videos by detecting eye blinking. In WIFS, 2018. [110] Renrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu, Yu Qiao, Hongsheng Li, and Peng Gao. LLaMAadapter: Efficient fine-tuning of large language models with zero-initialized attention. In ICLR, 2024. 4 [111] Tianchen Zhao, Xiang Xu, Mingze Xu, Hui Ding, Yuanjun Xiong, and Wei Xia. Learning self-consistency for deepfake detection. In ICCV, 2021. 1, 2 [112] Yinglin Zheng, Jianmin Bao, Dong Chen, Ming Zeng, and Fang Wen. Exploring temporal coherence for more general video face forgery detection. In ICCV, 2021. 1, 2, 5, 6 [113] Tianfei Zhou, Wenguan Wang, Zhiyuan Liang, and Jianbing Shen. Face forensics in the wild. In CVPR, 2021. 6, 9 [114] Yipin Zhou and Ser-Nam Lim. Joint audio-visual deepfake detection. In ICCV, 2021. 1, 2 [115] Zhenglin Zhou, Fan Ma, Hehe Fan, Zongxin Yang, and Yi Yang. Headstudio: Text to animatable head avatars with 3d gaussian splatting. In ECCV, 2024. [116] Bojia Zi, Minghao Chang, Jingjing Chen, Xingjun Ma, and Yu-Gang Jiang. Wilddeepfake: challenging real-world dataset for deepfake detection. In MM, 2020."
        }
    ],
    "affiliations": [
        "Max Planck Institute for Informatics",
        "The University of Tokyo"
    ]
}