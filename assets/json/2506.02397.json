{
    "paper_title": "OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for Over-Reasoning Mitigation",
    "authors": [
        "Shengjia Zhang",
        "Junjie Wu",
        "Jiawei Chen",
        "Changwang Zhang",
        "Xingyu Lou",
        "Wangchunshu Zhou",
        "Sheng Zhou",
        "Can Wang",
        "Jun Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advanced large reasoning models (LRMs) leverage extended chain-of-thought (CoT) reasoning to solve complex tasks, achieving state-of-the-art performance. Despite their success, we identify a critical issue: a substantial portion of simple tasks solved by LRMs can also be addressed by non-reasoning LLMs using significantly fewer tokens, indicating the complex reasoning may not always be necessary. To address this, we systematically analyze the reasoning trajectories of LRMs and present a method utilizing identified paradigms and LLM-Judge to classify these trajectories as either Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1, a method that prunes redundant reasoning steps while preserving logical validity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking) for straightforward problems while engaging in deliberate thinking (slow-thinking) for complex problems. Experiments across mathematical and question-answering tasks demonstrate that OThink-R1 reduces reasoning redundancy by almost 23\\% on average without compromising accuracy, offering practical guidelines for efficient reasoning models. The code is available at https://github.com/AgenticIR-Lab/OThink-R1."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 7 9 3 2 0 . 6 0 5 2 : r OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for Over-Reasoning Mitigation Shengjia Zhang1 Junjie Wu2 Jiawei Chen1 Changwang Zhang2 Xingyu Lou2 Wangchunshu Zhou2 Sheng Zhou1 Can Wang1 1 Zhejiang University, Hangzhou, China 2 OPPO Research Institute, Shenzhen, China shengjia.zhang@zju.edu.cn, wujunjie1@oppo.com, sleepyhunt@zju.edu.cn, changwangzhang@foxmail.com, louxingyu@oppo.com, zhouwangchunshu@oppo.com, zhousheng_zju@zju.edu.cn, wcan@zju.edu.cn, junwang.lu@gmail.com Jun Wang"
        },
        {
            "title": "Abstract",
            "content": "Recent advanced large reasoning models (LRMs) leverage extended chain-ofthought (CoT) reasoning to solve complex tasks, achieving state-of-the-art performance. Despite their success, we identify critical issue: substantial portion of simple tasks solved by LRMs can also be addressed by non-reasoning LLMs using significantly fewer tokens, indicating the complex reasoning may not always be necessary. To address this, we systematically analyze the reasoning trajectories of LRMs and present method utilizing identified paradigms and LLM-Judge to classify these trajectories as either Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1, method that prunes redundant reasoning steps while preserving logical validity. OThink-R1 dynamically employs the nonthinking mode (fast-thinking) for straightforward problems while engaging in deliberate thinking (slow-thinking) for complex problems. Experiments across mathematical and question-answering tasks demonstrate that OThink-R1 reduces reasoning redundancy by almost 23% on average without compromising accuracy, offering practical guidelines for efficient reasoning models. The code is available at https://github.com/AgenticIR-Lab/OThink-R1."
        },
        {
            "title": "Introduction",
            "content": "Human cognitive processes are often divided into two complementary modes: fast, intuitive thinking (System 1) and slow, deliberate reasoning (System 2) [1]. Fast thinking is instinctive and effortless, allowing for quick decisions but often leading to cognitive biases in complex situations. In contrast, slow thinking is thoughtful and analytical, employing logical reasoning to achieve more accurate problem-solving. Traditional large language models (LLMs) primarily mimic fast thinking, relying on heuristics from vast data patterns, which can limit their effectiveness in handling intricate tasks. Recent advancements in language reasoning models (LRMs), such as OpenAI o1 [2], OpenAI o3mini, and DeepSeek-R1 [3], emulate slow thinking by generating explicit reasoning chains (CoT) [4], transforming raw predictions into more structured, step-by-step solutions. This approach helps to mitigate the biases of fast thinking, significantly enhancing their reasoning capabilities for tackling complex problems. Equal contribution. Work done during the internship of Zhang at OPPO Research Institute. Corresponding authors. Preprint. Under review. Figure 1: Schematic diagram of our proposed OThink-R1. Despite the impressive performance of LRMs in complex tasks, they inevitably generate longer output sequences, which results in significantly higher computational overhead. For example, when comparing the typical LRMs (e.g. DeepSeek-R1-Distill-Qwen series) to the non-reasoning LLMs (e.g. Qwen2.5-Instruct series), we observe that the LRMs produce over 9.78 times more tokens than non-reasoning LLMs on both question answering (QA) and math tasks for the same correctly answered questions  (Table 1)  . These substantial discrepancies in computational costs pose significant challenges to the widespread adoption of LRMs, raising concerns about inference latency and economic efficiency. Recent efforts have aimed to mitigate these issues through reinforcement learning techniques that control reasoning length [57] or prompts that eliminate reasoning steps [8]. However, these approaches confine models to single static reasoning patterns, either consistently compressed or entirely bypassed. This inflexibility prevents adaptive reasoning, where straightforward tasks should receive immediate responses while complex problems require thorough analysis, creating an intrinsic limitation in balancing computational efficiency with reasoning completeness. To address this issue, we advocate for new hybrid paradigm that integrates both fast-thinking and slow-thinking modes within single model, enabling autonomous selection of the appropriate mode for different problems. This concept is inspired by cognitive theories [9], which postulates that humans possess both fast and slow thinking capacities and can flexibly switch between them as needed. Typically, fast thinking is used for multitude of simple tasks, providing responses swiftly; while slow thinking is reserved for complex problems that demand detailed logical reasoning. We seek to transfer this paradigm to LRMs, allowing them to dynamically adapt their reasoning strategies. Our empirical results  (Table 1)  also indicate that slow thinking is not always necessary, as vanilla non-reasoning LLMs can solve substantial portion of tasks with significantly fewer tokens. However, implementing adaptive hybrid reasoning in LRMs presents two key challenges: 1) How to identify which scenarios that fast thinking is sufficient? 2) How to empower LRMs to autonomously select the appropriate reasoning mode? While few pioneering works, such as Dualformer [10] and Qwen33, have explored hybrid modes, their methods still require manual selection of the reasoning mode and lack automatic adaptation. In this work, we propose novel framework, OThink-R1, which enables LRMs to automatically switch between fast and slow thinking modes. As shown in Figure 1, we systematically analyze the reasoning trajectories of LRMs across various tasks and reveal that certain slow-thinking trajectories are redundant and can be pruned to address the first challenge. Specifically, we identify three characteristic patterns in which LRMs generate less-essential output after arriving at the correct solution. Conversely, we also identify three essential reasoning patterns that significantly contribute to correct answers. These salient features are then formulated as prompts to instruct another LLM (e.g., GPT-4o) to serve as judge and adaptively classify reasoning as either redundant or essential. 3https://github.com/QwenLM/Qwen3 2 To further enhance the ability of LRMs for automatic hybrid reasoning, we explicitly construct supervised fine-tuning (SFT) dataset. For tasks where fast-thinking LLMs yield correct answers and the reasoning trajectories of LRMs are judged redundant, we remove the redundant reasoning trajectories, retaining only the immediate responses. These curated problem-response pairs are then used to fine-tune LRMs, thereby promoting their adaptive use of fast or slow thinking depending on the problem. Additionally, we propose novel loss function that incorporates dual-reference KL-divergence constraint, leveraging guidance from both fast and slow thinking variants of the same model architecture. This strengthens the models dynamic switching ability between the two modes while simultaneously learning the output distributions of both reference models. In our experiments, we implement our OThink-R1 into the representative LRMs, Deepseek-R1Distill-Qwen series, with varying model sizes. Results show that OThink-R1 reduces generated tokens by an average of 23.4% on four representative QA and math datasets, while maintaining or even surpassing the performance of the original baselines. Further analysis reveals that the enhanced model employs fast thinking on more than 27.3% of problems. Our contributions are as follows: We propose OThink-R1, novel large reasoning model that automatically switches between fast and slow thinking modes. This addresses significant gap in the dynamic reasoning capabilities of existing models. We systematically analyze the reasoning trajectories of LRMs and present method utilizing identified paradigms and LLM-Judge to classify these trajectories as either Redundant Reasoning (Multi-Solution Exploration, Repeated Self-Validation, Defensive Assumptions) or Essential Reasoning (Key-word Identification, Misunderstanding Prevention, Premise Omission Avoidance). We propose dual reference KL-divergence loss function Lhybrid, incorporating guidance from fast and slow thinking models during supervised fine-tuning for LRMs. Lhybrid enables the integration of knowledge from multiple reference models into unified reasoning model. We validated the effectiveness of the proposed OThink-R1 through systematic experiments on four public datasets. The experimental results demonstrate consistent improvements in performance with reduced token usage compared to the latest relevant baselines."
        },
        {
            "title": "2 Preliminaries",
            "content": "Supervised Fine-Tuning. Let θ be the parameters of an LLM that predicts output sequences = [y1, ..., ym] given input prompts = [x1, ..., xn]. SFT aims to refine the output style of pretrained LLMs, enabling them to generate target responses. Specifically, the pretrained model πθ is finetuned on curated dataset DSFT = {x(i), y(i)}N min θ L(θ, DSFT) = E(x,y)DSFT i=1, via maximum likelihood estimation [11]: (cid:105) log πθ(yx) (cid:104) (1) where πθ(yx) denotes the likelihood of the generated text, typically modeled auto-regressively as: πθ(yx) = (cid:89) j=1 πθ(yjx, y<j). (2) Thinking Mode. Cognitive processes in humans typically follow two modes: fast, intuitive thinking and slow, deliberate reasoning. This duality informs our understanding of the thinking modes employed by LRMs. Slow-Thinking: This mode represents the traditional generation process, where models such as Deepseek-R1 and the Deepseek-R1-Distill-Qwen series engage in thorough analytical reasoning, leveraging structured logical processes. The slow-thinking mode consists of three key components: 1) detailed reasoning process encapsulated within the <think> and </think> tags; 2) step-by-step presentation of intermediate solutions; and 3) the final answer. Fast-Thinking: In contrast to slow thinking, this mode emulates human instinctive cognition. For LRMs, the fast-thinking mode involves rapidly generating intermediate solutions and final answers while bypassing explicit reasoning steps, characterized by the empty content within the <think> tag. 3 Adaptive Hybrid-Thinking: To fully capture human cognitive flexibility, we introduce an Adaptive Hybrid-Thinking mode for LRMs. This mode empowers LRMs to autonomously switch between fast and slow thinking modes in response to task demands, optimizing performance based on task complexity. Prior research has investigated hybrid approaches that integrate both fast and slow thinking modes. However, existing implementations generally require manual mode selection and show constrained adaptive behavior, which excludes them from the Adaptive Hybrid-Thinking mode."
        },
        {
            "title": "3 Methodology",
            "content": "To enable LRMs to utilize an Adaptive Hybrid-Thinking mode, we propose OThink-R1, model that dynamically switches between fast and slow thinking modes through the following three steps: 3.1 Identifying Paradigms of Complex Reasoning in LRMs While LRMs demonstrate impressive reasoning abilities, our empirical experiments in Table 1 reveal that both non-reasoning LLMs (e.g., Qwen2.5 series models) and LRMs can accurately solve substantial portion of simple tasks (30% - 90% overlap in correct solutions), with non-reasoning LLMs requiring significantly fewer tokens. Crucially, this overlap motivates deeper investigation: Do the complex reasoning steps of LRMs matter when simpler non-reasoning LLMs can effectively solve the same tasks? To address this, we systematically analyze the reasoning trajectories of LRMs on overlapping problems, and identify two distinct reasoning paradigms: Redundant Reasoning: We observe three characteristic patterns of redundant reasoning (Appendix A) where LRMs produce non-essential output after determining correct solution: 1. Multi-Solution Exploration: Persistent exploration of alternative solutions despite determining correct answer. 2. Repeated Self-Validation: Excessive re-validation of every intermediate steps in final solutions. 3. Defensive Assumptions: Being overly cautious, take extraneous hypotheses into consideration based on internal knowledge, rather than problem-specific constraints. Essential Reasoning: Building upon the pitfalls of redundant reasoning, we propose three fundamental principles for effective reasoning in LRMs based on observations (Appendix B). These principles address the limitations identified in redundant reasoning by emphasizing that reasoning steps should: 1) highlight critical problem elements; 2) resolve potential ambiguities; 3) strictly adhere to given constraints. Specifically: 1. Key-word Identification: Extracting and emphasizing critical problem components forms the basis for solution derivation. 2. Misunderstanding Prevention: Eliminating misunderstandings in the problem statement prevents errors from incorrect assumptions. 3. Premise Omission Avoidance: Comprehensive coverage of all given premises and conditions ensures solution validity. 3.2 SFT Dataset Construction with Curated Trajectories As analyzed in Section 3.1, our observations indicate that reasoning is not always essential for solving certain problems. Both LRMs and non-reasoning LLMs can provide correct answers, with the latter doing so using significantly fewer tokens. Building on these findings, we focus on refining the training process by pruning redundant reasoning trajectories. The goal is to construct an SFT dataset that not only solves problems effectively but also enhances the models ability to balance fast and slow thinking. The construction of the SFT dataset begins with query questions = {x(i)}N i=1 and their corresponding responses = {y(i)}N i=1 generated by an LRM πθ. Since the generated responses may inherently contain incorrect examples, directly incorporating them into training data risks 4 Table 1: Comparison of average generated tokens and overlap ratios between R1 (DeepSeek-R1Distill-Qwen) and Qwen (Qwen2.5-Instruct). 1) Qwen: the average generated tokens of Qwen2.5Instruct series. 2) R1: the average generated tokens of DeepSeek-R1-Distill-Qwen series. 2) Overlap: Percentage of problems that both R1 and Qwen can solve in training data (a/b), where denotes the number of problems that both R1 and Qwen can solve, denotes the number of training samples. Dataset Qwen Overlap Qwen R1 Overlap Qwen Overlap 1.5B 7B 14B OpenBookQA CommonsenseQA ASDIV GSM8K 45 36 100 802 638 309 1025 34.98% (1734/4957) 35.21% (2744/7793) 86.04% (826/960) 71.27% (4261/5979) 50 104 276 638 741 691 60.92% (3020/4957) 51.20% (3990/7793) 90.10% (865/960) 87.36% (5223/5979) 89 65 104 559 572 316 649 73.01% (3619/4957) 61.62% (4802/7793) 88.75% (852/960) 89.31% (5340/5979) reinforcing flawed reasoning patterns, which could derail the models optimization trajectory. To ensure dataset quality, we filter the responses using verification function V(), e.g., Math-Verify4, retaining only question-response pairs (x(i), y(i)) that are verified as correct: = {(x(i), y(i)) x(i) Q, y(i) R, V(y(i)) is True}. (3) Next, we use non-reasoning LLM of the same architecture to categorize the responses in into two groups: (1) cases where both the LRM and the LLM provide correct answers, and (2) cases where only the LRM produces correct answers while the LLM fails. For the first group, we employ an auxiliary verifier (LLM-Judge, implemented with GPT-4o) to identify reasoning paradigms. The LLM-Judge evaluates reasoning trajectories based on two criteria: (1) the presence of redundant reasoning patterns and (2) adherence to predefined essential reasoning principles. Detailed system prompt for identifying redundant or essential reasoning is provided in Appendix C. If an LRM response is identified as exhibiting redundant reasoning, its reasoning trajectory, wrapped in <think>, is pruned during the construction of the SFT dataset and included as fast-thinking sample. Otherwise, it is retained as slow-thinking sample. Specifically, when constructing fastthinking samples, we retain the <think> tags in the responses. This is because we have observed that the outputs of LRMs are highly dependent on <think> tokens (even if these tokens are removed from the chat template). Directly eliminating the <think> related tags can lead to fine-tuning failures. Notably, for responses in the second group, we retain their full reasoning trajectories during finetuning. This preserves the models capacity for complex reasoning while ensuring comprehensive SFT dataset. The final SFT dataset combines pruned fast-thinking samples with unmodified slowthinking trajectories, achieving an optimal balance between efficiency and robust reasoning ability. 3.3 SFT with Dual Reference KL-Divergence Loss We have constructed the SFT dataset to guide LRMs in balancing their responses between fast and slow thinking modes. Traditional SFT that relies solely on maximum likelihood estimation often results in preference for one predominant mode within the dataset, such as generating only fast or slow thinking responses. By explicitly leveraging the dual-mode characteristics within the dataset, we designed specific loss function that enables the model to adopt the cautious reasoning style of LRMs during slow thinking while retaining the efficient generation capabilities of base LLMs during fast thinking. Our approach encompasses two primary goals: (1) preventing excessive deviation from the original distribution of the base model during fine-tuning, and (2) ensuring that the model can dynamically adapt to the probability distributions of different thinking modes. To achieve this, we propose dual-reference KL-divergence loss function: Lhybrid = E(x,y)DSFT + β1KL(πθπLRM) + β2KL(πθπLLM) (cid:105) log πθ(yx) (4) (cid:104) 4https://github.com/huggingface/Math-Verify 5 The hyperparameters β1 and β2 regulate the retention level of the slow thinking mode relative to the reference distribution of LRMs, and the alignment strength of the fast thinking mode with the distribution of non-reasoning LLMs, respectively. Through the joint optimization of the supervised learning objective and the dual regularization terms, OThink-R1 gains the ability to autonomously select the optimal response mode based on context, thereby facilitating dynamic balance between fast and slow thinking."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setting OThink-R1s mode-switching efficacy was evaluated on simple tasks where: (1) fast thinking maintains performance with reduced token usage (unlike difficult tasks where it causes performance drops), and (2) the switching benefits are most demonstrable. Datasets and Models. We evaluated our approach using relatively simple datasets from two tasks: question answering with OpenBookQA [12] and CommonsenseQA [13], and mathematical reasoning with ASDIV [14] and GSM8K [15]. Detailed descriptions of these datasets are provided in the Appendix D. The DeepSeek-R1-Distill-Qwen series (1.5B/7B/14B variants) served as foundational LRMs in our comparative experiments. SFT Datasets Construction. We constructed two SFT datasets for training: QA dataset combining OpenBookQA and CommonsenseQA and MATH dataset merging ASDIV and GSM8K examples, with sample pruning ratios fully specified in Table 3a. All original training instances were processed through DeepSeek-R1-Distill-Qwen (τ = 0.9, top-p = 0.95) to generate responses. The validation sets comprised 20% held-out samples from each dataset except for OpenBookQA which has predefined 500-sample validation set. Hyperparameter Tuning. The models are trained for up to 4 epochs with learning rates lr [3e-5, 4e-5, 5e-5]. We use the AdamW optimizer [16] with default hyperparameters, and the gradient accumulation step is set to 2. Training is conducted on 8 80GB NVIDIA A100 GPUs. The details of the optimal hyperparameters are provided in Appendix E. Baselines. We compare two baselines: 1) NoThinking [8], which directly bypass the reasoning process with explicit prompting; 2) DualFormer [10], which randomly drop the intermediate reasoning steps. 4.2 Main Results Table 2: Performance comparison across multiple datasets, showing that OThink-R1 reduces the average number of generated tokens by 23.4% on QA and math tasks. (: fewer tokens, better efficiency; : higher accuracy, better performance). Results for OThink-R1 are highlighted in blue , with bolded values indicating superior accuracy. Model DeepSeek-R1-Distill-Qwen-1.5B NoThinking-R1-1.5B [8] DualFormer-R1-1.5B [10] OThink-R1-1.5B DeepSeek-R1-Distill-Qwen-7B NoThinking-R1-7B [8] DualFormer-R1-7B [10] OThink-R1-7B DeepSeek-R1-Distill-Qwen-14B NoThinking-R1-14B [8] DualFormer-R1-14B [10] OThink-R1-14B OpenBookQA CommonsenseQA ASDIV GSM8K Tokens ACC Tokens ACC Tokens ACC Tokens ACC 43.80% 37.70% 45.20% 48.40% 64.70% 54.30% 66.70% 66.90% 81.70% 78.90% 79.80% 81.80% 348 179 203 352 138 223 270 319 197 223 412 95.02% 79.10% 86.70% 96.00% 97.00% 88.00% 96.00% 98.00% 97.00% 94.40% 95.70% 98.00% 1111 404 558 719 258 460 488 657 458 482 791 76.12% 69.70% 55.20% 72.00% 86.10% 77.50% 81.30% 86.70% 91.20% 88.60% 86.40% 89.80% 642 180 399 783 130 723 667 522 296 1688 421 50.00% 37.80% 49.00% 51.80% 76.40% 56.60% 75.20% 76.80% 92.80% 87.80% 91.40% 93.40% 528 163 377 730 106 701 634 569 373 2003 435 6 Superior Efficiency-Effectiveness Trade-off in QA Tasks. As shown in Table 2, the OThink-R1 models demonstrate significant advantages over baseline LRM models across various parameter scales (1.5B, 7B, and 14B). Specifically, average accuracy improves by 3.2%, 1.3%, and 0.35%, while the average number of generated tokens decreases substantially by 68.2%, 13.9%, and 21.4%, respectively. Further analysis in Table 3b indicates that the model effectively activates the fastslow thinking switching mechanism during QA processes, with this mechanism having more pronounced impact in small parameter models. Comparative experimental results show that although the NoThinking generates the fewest tokens, it suffers from significant degradation in accuracy. Additionally, the DualFormer approach fails to deliver the anticipated performance improvements. In contrast, OThink-R1 maintains superior generation efficiency while consistently outperforming other comparative methods in accuracy, demonstrating the advantages of its dynamic fast-slow thinking switching capability. Improved Performance in Simple Mathematical Tasks. In reasoning-intensive mathematical tasks, OThink-R1 consistently outperforms baseline LRM models on the ASDIV dataset  (Table 2)  , achieving the highest accuracy across all model scales. When tackling the more challenging GSM8K dataset, the model effectively activates the fast-slow thinking switching mechanism  (Table 3)  , with the 7B model generating fewer tokens while achieving higher accuracy. Notably, OThink-R1 surpasses both comparative methods in accuracy, demonstrating its ability to maintain excellent performance while enhancing efficiencyan advantage that extends even to reasoning-intensive tasks. Table 3: Prune/Fast-Thinking Ratios Across Model Scales and Datasets. a. Prune Ratio (Training): Pruning ratio of redundant reasoning trajectories within <think> during training (a/b), where denotes the number of training examples whose reasoning trajectory was pruned, denotes the training examples that R1 derive the correct answer. b. Fast-thinking Ratio (Test): Percentage of fast thinking in testing set, average 27.3%. a. Prune Ratio (Training) b. Fast-thinking Ratio (Test) Dataset OpenBookQA CommonsenseQA ASDIV GSM8K Prune Ratio (Training) 1.5B (%) 7B (%) 14B (%) 69.63 (1685/2420) 76.31 (2631/3448) 30.15 (278/922) 36.51 (1818/4980) 75.18 (2845/3784) 74.89 (3684/4919) 32.51 (303/932) 32.11 (1734/5400) 67.36 (3021/4485) 62.70 (3870/6172) 31.45 (295/938) 31.65 (1738/5492) Dataset Fast-thinking Ratio (Test) 1.5B (%) 7B (%) 14B (%) OpenBookQA 80.00 CommonsenseQA 80.34 ASDIV GSM8K 18.27 20.77 6.40 8.76 13. 9.93 36.80 35.70 7.97 8.56 4.3 Ablation Study In Table 4, we evaluate various variants of OThink-R1: 1) w/o prune, which fine-tunes LRMs on filtered correct question-response pairs, without pruning any reasoning trajectories. 2) w/o LLMJudge, where the LLM-Judge process is removed. This variation brute force prune the reasoning trajectories in cases where both the LRM and the LLM provide correct answers. 3) w/o KL-constraint employs the vanilla SFT loss (cf. Eq.(1)), without introducing dual KL-divergence constraint. 4) β1 = 0, where the KL-constraint on reference LRM is removed. 5) β2 = 0, where the KL-constraint on reference LLM is removed. We observe that OThink-R1 outperforms other variants in most cases. We highlight the following key findings: 1) The only difference between w/o prune variant and OThink-R1 lies in the former retains all reasoning trajectories, which allows it to learn the external knowledge in the dataset. OThink-R1 outperforms w/o prune variant except in OpenBookQA on 14B series, demonstrating that the performance improvement stems from dynamical fast/slow thinking switching rather than dataset knowledge. 2) Compared to w/o LLM-Judge variant, OThink-R1 retains reasoning trajectories that is determined as essential. The result demonstrate that our systematic analyze on reasoning trajectories is indeed, as blindly removing essential reasoning trajectories leads to performance drop. 3) OThink-R1 outperforms w/o KL-constraint on most cases, necessitating the utilization of dual KL-constraint. Moreover, we conduct fine-grained analysis by only remove one reference model. If we set β1 = 0, i.e., remove LRM reference model constraint, the model suffers from severe 7 Table 4: Ablation study across multiple datasets. (: lower is better, : higher is better; Our proposed OThink-R1 is highlighted in blue , with bolded results indicating superior accuracy. ) Model w/o prune w/o LLM-Judge w/o KL-constraint β1 = 0 β2 = 0 OThink-R1-1.5B w/o prune w/o LLM-Judge w/o KL-constraint β1 = 0 β2 = 0 OThink-R1-7B w/o prune w/o LLM-Judge w/o KL-constraint β1 = 0 β2 = 0 OThink-R1-14B OpenBookQA CommonsenseQA ASDIV GSM8K Tokens ACC Tokens ACC Tokens ACC Tokens ACC 734 177 151 1776 172 206 997 997 328 4364 649 1676 4353 3731 10599 253 421 51.40% 52.40% 51.60% 47.40% 47.80% 51.80% 75.40% 69.40% 68.80% 72.20% 76.40% 76.80% 94.60% 93.20% 94.80% 89.00% 91.40% 93.40% 621 213 182 1355 161 166 775 776 268 4320 617 1455 5038 3001 11516 409 435 46.40% 49.10% 47.10% 48.70% 47.60% 48.40% 66.10% 65.70% 64.00% 69.40% 64.20% 66.90% 80.70% 81.50% 81.60% 80.80% 81.20% 81.80% 298 192 264 939 261 347 249 171 292 2144 267 305 196 281 8566 282 412 93.40% 94.70% 94.00% 89.40% 94.00% 94.00% 98.00% 97.00% 97.30% 97.70% 97.30% 98.00% 96.70% 98.00% 97.00% 89.70% 96.30% 98.00% 991 406 564 1506 621 757 432 279 588 2073 480 629 300 338 4807 475 791 75.30% 69.10% 71.90% 69.40% 70.10% 71.40% 86.40% 84.20% 86.40% 84.20% 85.50% 86.70% 91.70% 88.70% 91.10% 82.70% 88.30% 89.80% overthinking issues, significantly improve the average generated tokens. If we remove LLM reference model constraint, the performance drops. These results necessate the dual KL-divergence constraint. 4.4 Case Study We present the case study in Figure 2. OThink-R1 utilize fast-thinking mode, and solve the problem correctly. While the reasoning model exhibits overthinking phenomenon, significantly improve the generated tokens, while even derive the incorrect answer. It demonstrate that OThink-R1 can dynamically switch between fast and slow thinking. Moreover, the reasoning process may not always be necessary, which would even influence the model performance."
        },
        {
            "title": "5 Related work",
            "content": "5.1 Efficient Reasoning Recent advances in efficient reasoning for LRMs have attracted growing attention. Current approaches can be broadly categorized into training and training-free strategies. Training Strategies. prominent direction involves reinforcement learning with explicit lengthbased rewards [6, 1719]. For instance, DAST [20] fine-tunes LRMs via length-preference dataset governed by an adaptive token-length budget. Similarly, L1 [5] enforces length constraints by adding \"Think for tokens\" instructions in training samples before policy optimization. Other efforts including training LLMs to learn latent representations [2125], explicitly instruct LLMs to generate fewer reasoning steps [2630] (e.g., setting token budget in prompts), or dynamically route inputs based on task difficulty to control reasoning complexity [31, 32]. However, these approaches share critical limitation: they employ fixed compression strategies that lack the capacity for dynamic switching between different reasoning modes. Training-free Strategies. Several works achieve efficiency through inference-time adjustments. NoThinking [8] suppresses intermediate reasoning steps via specialized prompting, while Dynam8 Figure 2: Comparison of generated responses on CommonsenseQA. icExit [33] employs pattern detection (e.g., hesitation markers like \"Wait\" or \"Hmm\") to terminate reasoning early. However, these works cannot adaptively choose between fast and slow thinking. 5.2 Variable-Length Reasoning Recent work explores supervised fine-tuning (SFT) on variable-length reasoning data to balance efficiency and accuracy of LRMs. C3oT [34] compresses extended CoT trajectories via GPT-4 distillation, retaining critical steps while shortening reasoning length. CoT-Valve [35] dynamically adjusts reasoning steps by interpolating parameters from long-reasoning and non-reasoning models. DualFormal [10] randomly prunes reasoning trajectories during training to reduce step redundancy. The most similar work is Yu et al. [36], which distill reasoning models into non-reasoning ones by fine-tuning solely on final answers. In contrast, OThink-R1selectively prunes reasoning trajectories while enabling LRMs dynamic switching between fast and slow thinking. Other efforts including TokenSkip [37] and iterative self-compression [38] similarly prioritize step reduction via semantic importance estimation or self-distillation. While these methods enhance efficiency through shortened reasoning trajectories, they primarily focus on static compression and lack explicit control over thinking modes, which OThink-R1overcomes. 5.3 Overthinking Issue Recent work has explored the phenomenon of overthinking in LRMs, where models engage in redundant and unnecessary reasoning steps. Cuadron et al. [39] formalize this issue in code generation, demonstrating how LRMs prioritize prolonged internal reasoning over essential environmental interactions. Subsequent studies classify distinct types of overthinking: Fu et al. [40] identify selfdoubt behaviors such as Reassessment and Perspective Bridging, while Qu et al. [41] analyzes underlying causes of redundant reasoning, including Circular Reasoning and Inefficient Loopback. Notably, Yi and Wang [42] propose taxonomy including Continued Exploration of Alternative Strategies and Model Collapse, highlighting trade-offs between reasoning depth and output stability. Another study by Yang et al. [33] evaluates confidence in intermediate steps by detecting uncertainty markers (e.g.,\"Hmm\", \"Wait\" and others), offering dynamic termination criterion. While existing works focus on characterizing causes or categorizing types of overthinking, our work moves beyond the issue itself, identifying the principles that enable LRMs to reliably derive solutions."
        },
        {
            "title": "6 Limitations",
            "content": "Although our proposed OThink-R1 endows large reasoning models adaptively switch between fastthinking and slowing, while significantly reduce the average generated tokens, we utilize LLM-Judge 9 for determining redundant reasoning trajectories. In the future, how to end-to-end identify the redundant reasoning, and design more specified criteria is promising direction."
        },
        {
            "title": "7 Conclusion",
            "content": "We propose OThink-R1, novel large reasoning model that dynamically switches between fast and slow thinking modes, improving both efficiency and performance. By pruning redundant reasoning trajectories and introducing dual-reference KL-divergence loss, OThink-R1 enhances hybrid reasoning capabilities. Experimental results demonstrate its effectiveness in advancing adaptive and efficient AI reasoning across diverse tasks. These results highlight the frameworks potential for efficient and scalable AI reasoning, with future work exploring extensions to multimodal reasoning and broader model architectures to further advance adaptive AI systems."
        },
        {
            "title": "References",
            "content": "[1] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419, 2025. [2] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [3] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [4] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [5] Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. [6] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint arXiv:2501.12570, 2025. [7] Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning. arXiv preprint arXiv:2504.01296, 2025. [8] Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. Reasoning models can be effective without thinking. arXiv preprint arXiv:2504.09858, 2025. [9] Daniel Kahneman. Thinking, fast and slow. macmillan, 2011. [10] DiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng. Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=bmbRCRiNDu. [11] Jian-Xin Pan, Kai-Tai Fang, Jian-Xin Pan, and Kai-Tai Fang. Maximum likelihood estimation. Growth curve models and statistical diagnostics, pages 77158, 2002. [12] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In EMNLP, 2018. [13] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421/. [14] Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. diverse corpus for evaluating and developing english math word problem solvers. arXiv preprint arXiv:2106.15772, 2021. 10 [15] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/abs/2110.14168. [16] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [17] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-ofthought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. [18] Daman Arora and Andrea Zanette. Training language models to reason efficiently. arXiv preprint arXiv:2502.04463, 2025. [19] Yuxiao Qu, Matthew YR Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching, Ruslan Salakhutdinov, and Aviral Kumar. Optimizing test-time compute via meta reinforcement fine-tuning. arXiv preprint arXiv:2503.07572, 2025. [20] Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, and Shiguo Lian. Dast: Difficulty-adaptive slow-thinking for large reasoning models. arXiv preprint arXiv:2503.04472, 2025. [21] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. [22] Jeffrey Cheng and Benjamin Van Durme. Compressed chain of thought: Efficient reasoning through dense representations. arXiv preprint arXiv:2412.13171, 2024. [23] Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, and Jiuxiang Gu. Efficient reasoning with hidden thinking. arXiv preprint arXiv:2501.19201, 2025. [24] Yige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. Softcot: Soft chain-of-thought for efficient reasoning with llms. arXiv preprint arXiv:2502.12134, 2025. [25] Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, and Sashank Reddi. Reasoning with latent thoughts: On the power of looped transformers. arXiv preprint arXiv:2502.17416, 2025. [26] Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. Token-budgetaware llm reasoning. arXiv preprint arXiv:2412.18547, 2024. [27] Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600, 2025. [28] Ayeong Lee, Ethan Che, and Tianyi Peng. How well do llms compress their own chain-of-thought? token complexity approach. arXiv preprint arXiv:2503.01141, 2025. [29] Matthew Renze and Erhan Guven. The benefits of concise chain of thought on problem-solving in large language models. In 2024 2nd International Conference on Foundation and Large Language Models (FLLM), pages 476483. IEEE, 2024. [30] Tong Wu, Chong Xiang, Jiachen Wang, and Prateek Mittal. Effectively controlling reasoning models through thinking intervention. arXiv preprint arXiv:2503.24370, 2025. [31] Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph Gonzalez, Waleed Kadous, and Ion Stoica. Routellm: Learning to route llms from preference data. In The Thirteenth International Conference on Learning Representations, 2024. [32] Yu-Neng Chuang, Leisheng Yu, Guanchu Wang, Lizhe Zhang, Zirui Liu, Xuanting Cai, Yang Sui, Vladimir Braverman, and Xia Hu. Confident or seek stronger: Exploring uncertainty-based on-device llm routing from benchmarking to generalization. arXiv preprint arXiv:2502.04428, 2025. [33] Chenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Zheng Lin, Li Cao, and Weiping Wang. Dynamic early exit in reasoning models. arXiv preprint arXiv:2504.15895, 2025. [34] Yu Kang, Xianghui Sun, Liangyu Chen, and Wei Zou. C3ot: Generating shorter chain-of-thought without compromising effectiveness. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2431224320, 2025. [35] Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang. Cot-valve: Lengthcompressible chain-of-thought tuning. arXiv preprint arXiv:2502.09601, 2025. 11 [36] Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023, 2024. [37] Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, and Wenjie Li. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067, 2025. [38] Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, and Zheng Zhang. Can language models learn to skip steps? arXiv preprint arXiv:2411.01855, 2024. [39] Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, et al. The danger of overthinking: Examining the reasoning-action dilemma in agentic tasks. arXiv preprint arXiv:2502.08235, 2025. [40] Yichao Fu, Junda Chen, Yonghao Zhuang, Zheyu Fu, Ion Stoica, and Hao Zhang. Reasoning without selfdoubt: More efficient chain-of-thought through certainty probing. In ICLR 2025 Workshop on Foundation Models in the Wild, 2025. [41] Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian Liang, Junxian He, et al. survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond. arXiv preprint arXiv:2503.21614, 2025. [42] Jingyang Yi and Jiazheng Wang. Shorterbetter: Guiding reasoning models to find optimal inference length for efficient reasoning, 2025. URL https://arxiv.org/abs/2504.21370. [43] Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, and Jian Li. Learning to break the loop: Analyzing and mitigating repetitions for neural text generation. Advances in Neural Information Processing Systems, 35:30823095, 2022."
        },
        {
            "title": "A Case Study on Redundant Reasoning",
            "content": "In this section, we present three redundant reasoning cases as follows: Repeated Self-Validation: As shown in Figure 3, the LRM double-checks its own reasoning steps. It does this by going back over its previous conclusions, using phrases like double-checked it multiple ways and let me recap one more time. While this repeated checking helps ensure answers stay consistent, it also uses extra time and computational resources. Furthermore, once the model has already derived the correct answer, these extra checks are unnecessary. Joy can read 8 pages of book in 20 minutes. How many hours will it take her to read 120 pages? Question DeepSeek-R1-Distill-Qwen-7B (Generated Reasoning Tokens: 1416) <think> </think> . . . think Ive verified it enough ways. So, its 5 hours. But just to recap, because sometimes its easy to get confused with rates and time. . . . . . dont think made any mistakes here. double-checked it multiple ways, which is helpful because sometimes when you do math. . . . . . Just to make sure, let me recap one more time:. . . Figure 3: Repeated Self-Validation on GSM8K. Defensive Assumptions: As shown in Figure 4, the LRM acts too cautiously by exploring multiple hypotheses. Specifically, it questions its initial understanding by expressing maybe she is asking or if theres another interpretation. Since the problem directly asks, How many ounces of tea does she need?a straightforward question requiring the total number, these additional hypotheses are unnecessary. It wastes additional computational resources, reducing the inference efficiency. Question Artemis is making tea for party. She knows her mom drinks an 8-ounce cup of tea and uses one ounce of tea. She will use this same ratio for the party. The party has 12 people there and each of them wants 6-ounce cup of tea. How many ounces of tea does she need? DeepSeek-R1-Distill-Qwen-7B (Generated Reasoning Tokens: 1316) <think> </think> . . . Alternatively, maybe she is asking how much tea she needs to add, not how much total tea is made?. . . . . . Wait, no, the question is, how many ounces of tea does she need? . . . . . . But let me think if theres another interpretation. Maybe the 8-ounce cup is the total tea, including the tea itself and water?. . . Figure 4: Defensive Assumptions on GSM8K. Multi-Solution Exploration: As illustrated in Figure 5, the model continues to explore alternative solutions even after deriving the correct answer. This behavior is evidenced by recurring phrases such as another way Alternatively and Another approach. Since the model has derived the correct answer, exploring other solutions is redundant and wasting computational resources. Tim has 30 less apples than Martha, and Harry has half as many apples as Tim. If Martha has 68 apples, how many apples does Harry have? Question DeepSeek-R1-Distill-Qwen-7B (Generated Reasoning Tokens: 818) <think> </think> . . . Martha has 68, Tim has 30 less, so 68 - 30 = 38. Harry has half of Tims apples, so 38 divided by 2 is 19. Yeah, that seems correct . . . . . . Is there another way to approach this problem? Maybe by using algebraic expressions. Let me set variables for each persons apples . . . . . . Alternatively, maybe can visualize this with diagram or something . . . . . . Another way to think about it is by using equations . . . Figure 5: Multi-Solution Exploration on GSM8K."
        },
        {
            "title": "B Case Study on Essential Reasoning",
            "content": "In this section, we present three essential reasoning examples as follows: Premise Omission Avoidance: The problem contains critical premises, and omitting any of these may lead to incorrect conclusions. As demonstrated in Figure 6, the model systematically identifies and incorporates each premise during reasoning. Specifically, it explicitly catches: 1) the weight of each item, and 2) What Tonys washing items includes. Question Tonys dad is very strict about the washing machine and family members are only allowed to wash 50 total ounces of clothing at time. Tony doesnt want to break the rules, so he weighs his clothes and finds that pair of socks weighs 2 ounces, underwear weighs 4 ounces, shirt weighs 5 ounces, shorts weigh 8 ounces, and pants weigh 10 ounces. Tony is washing pair of pants, 2 shirts, pair of shorts, and 3 pairs of socks. How many more pairs of underwear can he add to the wash and not break the rule? DeepSeek-R1-Distill-Qwen-7B (Generated Reasoning Tokens: 214) <think> </think> . . . pair of pants weighs 10 ounces, each shirt weighs 5 ounces, shorts weigh 8 ounces, and each pair of socks weighs 2 ounces . . . . . . Tony is washing one pair of pants, two shirts, one pair of shorts, and three pairs of socks . . . Figure 6: Premise Omission Avoidance on GSM8K. Key-word Identification: The model performs reasoning to identify and analyze critical keywords within the problem statement. In Figure 7, the model explicitly recognizes 5 less than 20 and one barnyard owl makes 5 hoots per minute, which enables it to derive the correct solution. One barnyard owl makes 5 hoot sounds per minute. If 5 less than 20 hoots per minute are heard coming out of the barn, how many Barnyard owls would this noise be coming from? Question DeepSeek-R1-Distill-Qwen-7B (Generated Reasoning Tokens: 154) <think> </think> . . . First, need to determine the total number of hoots per minute that are being heard coming out of the barn. . . . . . . The problem states that this number is 5 less than 20. . . . . . . Next, know that one barnyard owl makes 5 hoots per minute . . . Figure 7: Key-word Identification on GSM8K. Misunderstanding Prevention: The model explicitly verifies task requirements to avoid misunderstanding. As shown in Figure 8, when asked to calculate temperature decrease, the model recognizes this, rather than simply report the final temperature. It specifically highlights the phrases lose one-fourth of its initial temperature and find the decrease, demonstrating awareness that the task requires computing the temperature change rather than deriving the resultant Addison mountains temperature. In one hour, Addison mountains temperature will decrease to 3/4 of its temperature. If the current temperature of the mountain is 84 degrees, what will the temperature decrease by? Question DeepSeek-R1-Distill-Qwen-7B (Generated Reasoning Tokens: 94) <think> </think> . . . To determine the temperature decrease, first recognize that the temperature decreases to three-fourths of its current value after one hour . . . . . . This means the temperature will lose one-fourth of its initial temperature . . . . . . Given that the current temperature is 84 degrees, calculate one-fourth of this value to find the decrease . . . Figure 8: Misunderstanding Prevention on GSM8K."
        },
        {
            "title": "C System Prompt",
            "content": "OThink-R1 employs an auxiliary verifier (LLM-Judge, implemented using GPT-4o) to distinguish between redundant reasoning and essential reasoning. To systematically categorize reasoning paradigms, given our observed redundant reasoning patterns and established essential reasoning principles, we construct the following LLM-Judge prompt: Figure 9: System Prompt"
        },
        {
            "title": "D Experiment Datasets",
            "content": "We evaluated our approach using the following four datasets: OpenBookQA [12]: OpenBookQA provides the additional open-book knowledge for solving the multiple-choice questions. It consists of 4.96K training pairs with 500 validation and test examples. In this study, we intentionally exclude this open-book information during training and test, using only question-answer pairs. This design choice isolates the evaluation of OThink-R1, as incorporating external knowledge risks conflating performance gains between retrieved knowledge and the method itself. CommonsenseQA [13]: CommonsenseQA consists of multiple-choice questions designed to evaluate models ability to apply diverse forms of commonsense knowledge in reasoning tasks. The dataset is split into 9.4K training, 1.22K validation and 1.14K test sets. As there is no answer key in the test set, the validation set is utilized to evaluate the performance of our proposed OThink-R1. ASDIV [14]: dataset of elementary math word problems comprising 1,200 training and 301 test problems. The test set includes diverse problem types designed to assess basic arithmetic operations, such as aggregation, addition, and others. GSM8K [15]: GSM8K is linguistically diverse dataset of 8.5K grade school math word problemscomprising 7.5K training and 1K test examplesthat is designed to evaluate multi-step reasoning."
        },
        {
            "title": "E Optimal Hyperparameters for Tuning",
            "content": "We use the open-source TRL library 5 to train our models. The learning rate is searched in the range lr {3 105, 4 105, 5 105}, β1, β2 {103, 104}, gas (gradient accumulation steps) is searched in the range {1, 4}, batch (batch size in each GPU device) is set as 2. The maximum padding length for SFT is set as 3000. For all experiments, we train for 4 epochs, saving checkpoint after each epoch. Final reported performance corresponds to the model checkpoint (selected by epoch) and hyperparameters that achieve the highest validation accuracy. The optimal hyperparameters for each method, across all datasets and model scales (1.5B, 7B, 14B), are listed in Tables 6 and 7, following the order of hyperparameters specified in Table 5. Table 5: Hyperparameters to be searched for each method. gas denotes the gradient accumulation steps; batch denotes the batch size in each GPU device. Method Hyperparameters DualFormer OThink-R1 lr, gas, batch lr, gas, batch, β1, β Table 6: Optimal hyperparameters and best-performing epoch checkpoint (selected by validation accuracy) on QA. Model Size Checkpoint Epoch Hyperparameters Method 1.5B 7B 14B DualFormer OThink-R1 DualFormer OThink-R1 DualFormer OThink-R1 4 105 5 105 3 105 3 105 3 105 3 105 4 4 4 4 4 2 2 2 2 2 2 10 104 103 103 104 104 3 2 1 2 4 Table 7: Optimal hyperparameters and best-performing epoch checkpoint (selected by validation accuracy) on MATH. Model Size Checkpoint Epoch Hyperparameters Method 1.5B 7B 14B DualFormer OThink-R1 DualFormer OThink-R1 DualFormer OThink-R 5 105 4 105 4 105 3 105 3 105 3 105 4 1 4 1 4 1 2 2 2 2 2 103 104 103 104 10 104 4 4 2 2 4 3 For inference, we utilize the setting τ = 0.9, top-p = 0.95 for 1.5B/7B/14B models. As 1.5B series occasionally enter sentence-level loops [43], we additionally set the repetition_penalty as 1.1. The LLM inference is conducted via the open-source vLLM library 6. 5https://github.com/huggingface/trl 6https://github.com/vllm-project/vllm"
        },
        {
            "title": "F Societal Impacts",
            "content": "Modern large reasoning models like DeepSeek-R1 demonstrate exceptional performance in complex reasoning tasks. However, their reliance on hybrid reasoning paradigms (i.e., slow-thinking mechanisms) incurs significant computational overhead for simple tasks that non-reasoning models can solve with far fewer generated tokens. This inefficiency raises concerns about resource consumption, particularly given the growing societal reliance on AI systems. Our proposed OThink-R1addresses this challenge by enabling dynamic switching between fastand slow-thinking modes. This adaptive approach reduces unnecessary computational expenditure while preserving performance on complex tasks. By optimizing resource utilization, our method contributes to: Environmental sustainability: Lower energy consumption mitigates the carbon footprint of large-scale AI deployments. Maintenance efficiency: OThink-R1 improve the inference efficiency, which decreases human labor costs in monitoring and maintaining computational resources."
        }
    ],
    "affiliations": [
        "OPPO Research Institute, Shenzhen, China",
        "Zhejiang University, Hangzhou, China"
    ]
}