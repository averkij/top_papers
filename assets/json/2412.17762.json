{
    "paper_title": "The Superposition of Diffusion Models Using the Itô Density Estimator",
    "authors": [
        "Marta Skreta",
        "Lazar Atanackovic",
        "Avishek Joey Bose",
        "Alexander Tong",
        "Kirill Neklyudov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Cambrian explosion of easily accessible pre-trained diffusion models suggests a demand for methods that combine multiple different pre-trained diffusion models without incurring the significant computational burden of re-training a larger combined model. In this paper, we cast the problem of combining multiple pre-trained diffusion models at the generation stage under a novel proposed framework termed superposition. Theoretically, we derive superposition from rigorous first principles stemming from the celebrated continuity equation and design two novel algorithms tailor-made for combining diffusion models in SuperDiff. SuperDiff leverages a new scalable It\\^o density estimator for the log likelihood of the diffusion SDE which incurs no additional overhead compared to the well-known Hutchinson's estimator needed for divergence calculations. We demonstrate that SuperDiff is scalable to large pre-trained diffusion models as superposition is performed solely through composition during inference, and also enjoys painless implementation as it combines different pre-trained vector fields through an automated re-weighting scheme. Notably, we show that SuperDiff is efficient during inference time, and mimics traditional composition operators such as the logical OR and the logical AND. We empirically demonstrate the utility of using SuperDiff for generating more diverse images on CIFAR-10, more faithful prompt conditioned image editing using Stable Diffusion, and improved unconditional de novo structure design of proteins. https://github.com/necludov/super-diffusion"
        },
        {
            "title": "Start",
            "content": "THE SUPERPOSITION OF DIFFUSION MODELS USING THE ITÔ DENSITY ESTIMATOR Marta Skreta,1,2 Lazar Atanackovic,1,2 Avishek Joey Bose3,4 Alexander Tong4,5 Kirill Neklyudov4,5, 1University of Toronto 4Mila - Quebec AI Institute 5Université de Montréal 3University of Oxford 2Vector Institute 4 2 0 D 3 2 ] . [ 1 2 6 7 7 1 . 2 1 4 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "The Cambrian explosion of easily accessible pre-trained diffusion models suggests demand for methods that combine multiple different pre-trained diffusion models without incurring the significant computational burden of re-training larger combined model. In this paper, we cast the problem of combining multiple pre-trained diffusion models at the generation stage under novel proposed framework termed superposition. Theoretically, we derive superposition from rigorous first principles stemming from the celebrated continuity equation and design two novel algorithms tailor-made for combining diffusion models in SUPERDIFF. SUPERDIFF leverages new scalable Itô density estimator for the log likelihood of the diffusion SDE which incurs no additional overhead compared to the well-known Hutchinsons estimator needed for divergence calculations. We demonstrate that SUPERDIFF is scalable to large pre-trained diffusion models as superposition is performed solely through composition during inference, and also enjoys painless implementation as it combines different pre-trained vector fields through an automated re-weighting scheme. Notably, we show that SUPERDIFF is efficient during inference time, and mimics traditional composition operators such as the logical OR and the logical AND. We empirically demonstrate the utility of using SUPERDIFF for generating more diverse images on CIFAR-10, more faithful prompt conditioned image editing using Stable Diffusion, and improved unconditional de novo structure design of proteins. https://github.com/necludov/super-diffusion"
        },
        {
            "title": "INTRODUCTION",
            "content": "The design and application of generative models at scale are arguably one of the fastest-growing use cases of machine learning, with generational leaps in performance that often exceed expert expectations (Steinhardt, 2022). few of the central facilitators of this rapid progress are the availability of high-quality training data and large computing hardware (Kaplan et al., 2020); which in tandem provide tried and trusted recipe to scale generative models in variety of data modalities such as video generation (Brooks et al., 2024), natural language understanding (OpenAI, 2023; Achiam et al., 2023; Dong et al., 2022), and other challenging domains like mathematical reasoning (Trinh et al., 2024), or code assistance (Bubeck et al., 2023). As result, it is not surprising that driving force behind current generative modeling research is centered around developing open-source tooling (Dao et al., 2022; Kwon et al., 2023) to enable further scaling and understanding emergent behavior of such models (Schaeffer et al., 2023), including probing current limitations (Dziri et al., 2024). Indeed, the rapid escalation of generative model development has also induced democratizing effect, given the easy access to large pre-trained in the current AI climate (Stability AI, 2023; Midjourney, 2023; Ramesh et al., 2021). Furthermore, with the rise of open-source models, it is now easier than ever to host and deploy fine-tuned models. However, the current pace of progress also makes it infeasible to easily scale further models without confronting practical challenges. For instance, for continuous domains such as natural images current pre-trained diffusion models already exhaust all public data, with growing proportion of the web already populated with synthetic data (Schuhmann et al., 2022). Compounding these challenges is the tremendous cost of pre-training these large Authors contributed equally. Correspondence to k.necludov@gmail.com"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Concept interpolations via different methods: SUPERDIFF (top row), the averaging of outputs with different prompts (middle row), and joint prompting with standard Stable Diffusion (SD) (bottom row) for six different prompt combinations. Here we use SUPERDIFF with the AND operation (sampling equal densities). diffusion models, which at present makes it computationally unattractive for individuals to build large pre-trained models on different datasets without re-training larger combined model. compelling alternative to training ever larger models is to consider the efficacy of maximizing the utility of existing pre-trained models. In particular, it is interesting to consider the compositional benefits of combining pre-trained models at the generation stage in place of training single monolithic model (Du & Kaelbling, 2024). For diffusion models, which are the current de facto modeling paradigm over continuous domains, compositional generation can be framed as modifying the inference mechanism through either guidance (Dhariwal & Nichol, 2021; Ho & Salimans, 2022) or applying complex MCMC correction schemes (Du et al., 2023). Despite advancements, methods that utilize guidance lack firm theoretical underpinning (Bradley & Nakkiran, 2024) and MCMC techniques are prone to scaling issues, and more importantly, remain unproven at combining existing large pre-trained diffusion models. This motivates the following timely research question: Can we combine pre-trained diffusion models solely at inference in theoretically sound and efficient manner? Present work. In this paper, we cast the problem of combining and composing multiple pre-trained diffusion models under novel joint inference framework we term superposition. Intuitively we develop our framework by starting from the well-known principle of superposition in physical systems which summarizes the net response to multiple inputs in linear system as the sum of individual inputs. Applying the superposition principle, even in the simplest case of mixture model, requires constructing the resultant superimposed vector field; which can be built analytically by reweighting using the marginal density of generated samples along each diffusion models inference trajectory. We introduce SUPERDIFF for scalable superposition by proposing novel estimator of the density of generated samples along an SDE trajectory, which we expect to be of independent interest beyond superposition. In particular, our (Itô) density estimator in Thm. 1 is constructed by exploiting Itôs lemma and is exact, is general as it is not restricted to the vector field under which the trajectories are generated, and requires no additional computation during inference. This is in stark contrast to all prior density estimation approaches that involve computing expensive and high variance divergence estimates of drift vector field associated with the probability flow ODE. Armed with our new Itô density estimator we propose to combine pre-trained diffusion models by guiding the joint inference process through fine-grained control of the relative superposition weights of the model outputs. More precisely, in Sec. 3.2, we propose algorithms for two specific instantiations, illustrated in  (Fig. 1)  of the superposition principle: (i) generating samples from mixture of densities (an OR operation over models) or (ii) generating samples that are equally likely under all densities (an AND operation over models). We test the applicability of our approach SUPERDIFF using the two proposed superimposition strategies for image and protein generation tasks. For images, we first demonstrate the ability to combine the outputs of two models trained on disjoint datasets such that they yield better performance"
        },
        {
            "title": "Preprint",
            "content": "than the model trained on both of the datasets (see Sec. 4.1). In addition, we demonstrate the ability to interpolate between densities which correspond to concept interpolation in the image space (see Sec. 4.2). For proteins, we demonstrate that combining two different generative models leads to improvements in designability and novelty generation (see Sec. 4.3). Across all our experimental settings, we find that combining pre-trained diffusion models using SUPERDIFF leads to higher fidelity generated samples that better match task specification in text-conditioned image generation and also produce more diverse generated protein structures than comparable composition strategies."
        },
        {
            "title": "2 PRELIMARIES\nGenerative models learn to approximate a target data distribution pdata ∈ P(Rd) defined over Rd\nusing a parametric model qθ with learnable parameters θ. In the conventional problem definition, the\ndata distribution is realized as an empirical distribution that is provided as a training set of samples\nD = {µi}m\ni=1. Whilst there are multiple generative model families to choose from, we restrict our\nattention to diffusion models (Song et al., 2020; Ho et al., 2020) which are arguably the most popular\nmodeling family driving current application domains. We next review the basics of the continuous\ntime formulation of diffusion models before casting them within our superposition framework.",
            "content": "2.1 CONTINUOUS-TIME DIFFUSION MODELS x0 q0(x0). dxt = ft(x)dt + gtdWt, diffusion model can be cast as the solution to the Stochastic Differential Equation (Øksendal, 2003), (1) In the Itô SDE literature, the function ft : Rd Rd is known as the drift coefficient while gt : is real-valued function called the diffusion coefficient and Wt is the standard Wiener process. The subscript index [0, 1] indicates the time-valued nature of the stochastic process. Specifically, we fix the starting time = 0 to correspond to the data distribution pdata := q0(x0) and set = 1 as the terminal time = 1 to an easy to sample prior such as standard Normal distribution pnoise := q1(x1) = (x10, I). As such the diffusion SDE, also called the forward process, can be seen as progressively corrupting the data distribution and ultimately hitting terminal distribution devoid of any structure. To generate samples from the marginal density qt(xt) induced by the diffusion SDE in equation 1 we leverage the reverse-time SDE with the same marginal density as demonstrated below. Proposition 1. [Reverse-time SDEs/ODE] Marginal densities qt(x) induced by Eq. (1) correspond to the densities induced by the following SDE that goes back in time (τ = 1 t) with the corresponding initial condition (cid:18) dxτ = ft(xτ ) + (cid:18) g2 2 (cid:19) + ξτ log qt(xτ ) (cid:19) dτ + (cid:112)2ξτ dW τ , xτ =0 q1(x0) , (2) where τ is the standard Wiener process in time τ , and ξτ is any positive schedule. See proof in App. A.1. The reverse SDE flows backward in time τ = 1 and is linked to the diffusion SDE through the score log qτ (x), and dW τ is another Weiner process. As result, parametric model log qτ (x; θ) may directly learn to approximate this score function for every point in time and then draw samples by simulating the reverse SDE in equation 2 by plugging back in the learned score. Notably, for ξt 0, the SDE becomes an ODE which defines smooth change of measure corresponding to pnoise into the measure corresponding to pdata. In practice, for generative modelling, the forward SDE from Eq. (1) is chosen to be so simple that it can be integrated in time analytically without simulating the SDE itself. This is equivalent to choosing the noising schedule first, and then deriving the SDE that corresponds to this schedule. Namely, for every training sample µi, we can define the density of the corrupted µi as normal density with the mean scaled according to αt and the variance σ2 , then the density of the entire corrupted dataset is simply mixture over all training samples µi, i.e. t(x) = (x αtµi, σ2 qi I) , qt(x) = 1 (cid:88) i=1 qi t(x) . (3) Clearly, choosing αt, σt such that α0 = 1, σ0 = 0 and α1 = 0, σ1 = 1, we guarantee pdata := q0(x0) and pnoise := q1(x1) = (x10, I). This perturbation of the data distribution using Gaussian kernel offers specific forms for the drift ft, and diffusion coefficient gt as described next."
        },
        {
            "title": "Preprint",
            "content": "Proposition 2. [OrnsteinUhlenbeck SDE] The time-dependent densities in Eq. (3) correspond to the marginal densities of the following SDE, with the corresponding initial condition dxt = log αt (cid:123)(cid:122) ft(xt) (cid:124) xt (cid:125) (cid:114) dt + 2σ2 (cid:124) log σt αt (cid:125) (cid:123)(cid:122) gt dWt , x0 q0(x0) . (4) See proof in App. A.2. We highlight the simplicity of the drift term, linear scaling, that allows us to simulate efficiently the reverse SDE and is crucial for the proposed density estimators in Sec. 3.1. Altogether, the derivations of this section allow us to go from the noise schedules of samples in Eq. (3) used during the training of given diffusion model to the corresponding forward SDE in Prop. 2, and finally to the reverse SDEs or ODE used in Prop. 1."
        },
        {
            "title": "2.2 SUPERPOSITION OF ODES AND SDES",
            "content": "In this section, we introduce the superposition of multiple time-dependent densities that correspond to different stochastic processes. suggestive view of these densities is as processes corresponding to different training data (e.g. different datasets), different conditions (e.g. different text prompts), or simply differently trained diffusion models. Namely, we consider forward noising process {qi i=1 (e.g. different datasets). Assume that we know the individual vector fields vi t(x) that define the change of corresponding densities qi i=1 that possibly start from different initial distributions {qi t=0(x)}N t(x)}N t(x) via the state-space ODEs and continuity equations, t(x)vi t(x) = (cid:10)x, qi qi t(xt) = = vi dxt dt t(x)(cid:11) , [N ]. i=1 is the mixture of corresponding densities: The superposition of the noising processes {qi t(x)}N qmix (x) := (cid:88) j=1 ωjqj (x), (cid:88) i=1 ωi = 1 , ωi 0 , (5) (6) where ωj is mixing coefficient. Note that superimposed qmix for the superposition of the vector fields vi t(x) as demonstrated in the following proposition. (x) also satisfies the continuity equation Proposition 3. [Superposition of ODEs (Liu, 2022)] The mixture density in Eq. (6) follows the continuity equation with the superposed vector fields from Eq. (5), i.e. qmix (x) = (cid:10)x, qmix (x)vt(x)(cid:11) , vt(x) = 1 j=1 ωjqj (cid:80)N (x) (cid:88) i=1 ωiqi t(x)vi t(x) . (7) We reproduce the proof for Prop. 3 in the context of our superposition in App. A.3. The superposition principle is the core principle that allows for efficient simulation-free learning of the flow-based models (Liu et al., 2022b; Lipman et al., 2022; Albergo et al., 2023) and diffusion models (Song et al., 2020). We discuss how these frameworks are derived from the superposition principle in App. B. The superposition principle straightforwardly extends to the marginal densities of SDEs. That is, consider marginals densities qi τ (x) generated by the following SDEs τ (xτ )dτ + gτ dW τ , xτ =0 = qi (8) where one has to note the same diffusion coefficient for all the SDEs. Then the mixture of densities from Eq. (6) can be simulated by the SDE from the following proposition. dxτ = ui τ =0(x0) , Proposition 4. [Superposition of SDEs] The mixture qmix marginals {qi i=1 induced by SDEs from Eq. (8) corresponds to the following SDE i=1 ωiqi t(x)}N t(x) of density (x) := (cid:80)N dxτ = uτ (xτ )dτ + gτ dW τ , ut(x) = 1 j=1 ωjqj (cid:80)N (x) (cid:88) i=1 ωiqi t(x)ui t(x) . (9) See App. A.4 for the proof. Both Prop. 3 and Prop. 4 can be easily extended to the families of densities parameterized with continuous variable, but this is beyond the scope of the current paper."
        },
        {
            "title": "Preprint",
            "content": "(a) Training datasets (b) Averaging the vector fields (c) Sampling the mixture of densities (ours) (d) Sampling equal densities (ours) Figure 2: An intuitive illustration of using model superposition for improving inference performance. We show an example of two disjoint datasets and train model for each set. Each individual model learns to generate samples only from their respective datasets. Using model superposition enables sampling from both densities."
        },
        {
            "title": "3 SUPERPOSITION OF DIFFERENT MODELS",
            "content": "We now introduce our method for combining pre-trained diffusion models using the principle of superposition. The result of this is novel inference time algorithm SUPERDIFF which can be easily applied without further fine-tuning or post-processing of any of the pre-trained diffusion weights. Our proposed approach SUPERDIFF can be instantiated in two distinct ways that allow for the composition diffusion models that can be informally interpreted as logical composition operators in the logical AND and the logical OR. More precisely, given two pre-trained diffusion models that are trained on datasets and inference using SUPERDIFF can be done by either sampling from the mixture of the two learned densities, i.e. logical AND Fig. 2c, or sampling from the equal density locus (logical OR Fig. 2d). In such manner, superposition using AND leads to generated samples that are equally likely under both pre-trained diffusion models while superposition using OR creates samples that are preferentially generated by either pre-trained modeland thus mimics the empirical distributions or B. Method overview. SUPERDIFF is applicable in settings where the modeler has access to pre-trained diffusion models, along with its learned score function log qi t(x). Each of the pre-trained models follows marginal density qi t(x) and as result must admit corresponding vector field vi t(x) that satisfies the continuity equation in Equation 5. The key idea of our approach is an adaptive re-weighting scheme of the pre-trained models vector fields that relies on the likelihood of sample under different models. naive approach to estimating each marginal density during generation immediately presents several technical challenges as it requires the estimation of the divergence of superimposed vector fields. In particular, these challenges can be stated as follows: (C1) The marginal superpositioned vector field differs from the vector fields of either of the models. (C2) The divergence operation requires backpropagation through the network and is computationally expensive even with Hutchinsons trace estimator (Hutchinson, 1989). Our proposed approach SUPERDIFF overcomes these computational challenges by introducing novel density estimator in Sec. 3.1. Crucially, this new estimator does not require divergence estimation and enjoys having the same variance as the computationally expensive Hutchinsons trace estimator, making it favorable choice when generating using large pre-trained diffusion models. In section Sec. 3.2 we exploit this new density estimator to formally present our algorithm SUPERDIFF and derive connections to the composition operators that intuitively resemble logical AND and OR. 3.1 EVALUATING THE DENSITIES ON THE FLY In this section, we introduce novel method for evaluating the marginal density of diffusion model during the inference process. The conventional way to evaluate the density uses the continuity equation and solves the same ODE that is used for generating samples. This, however, is not easily possible in the case of our superposition of vector fields framework as outlined in Prop. 3. To solve this, we present the following proposition that disentangles the vector field generating the sample (ut(x) in the proposition) and the vector fields corresponding to different generative models vi t(x)."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1: SUPERDIFF pseudocode (for OR and AND operations) Input :M pre-trained score models log qi t(x), the parameters of the schedule αt, σt, stepsize dτ > 0, temperature parameter , bias parameter ℓ, and initial noise (0, I). for τ = 0, . . . , 1 do = 1 τ , ε (0, I) κi τ softmax(T log qi t(xτ ) + ℓ) // for OR according to Prop. 3 solve Linear Equations // for AND according to Prop. 6 i=1 κi ut(x) (cid:80)M τ log qi dxτ (cid:0)f1τ (xτ ) + g2 xτ +dτ xτ + dxτ log q1τ (xτ ) = (cid:10)dxτ , log q1τ (xτ )(cid:11) + t(xτ ) 1τ ut(x)(cid:1)dτ + g1τ dW τ // using Prop. 1 (cid:18) (cid:10), f1τ (xτ )(cid:11)+ (cid:28) + f1τ (xτ ) g2 1τ 2 log q1τ (xτ ), log q1τ (xτ ) (cid:29)(cid:19) dτ // using Thm. 1 return Proposition 5. [Smooth density estimator] For the integral curve x(t) solving dx/dt = ut(xt), t(x)(cid:11), the and the density qi log-density along the curve changes according to the following ODE t(x(t)) satisfying the continuity equation t(x) = (cid:10)x, qi t(x)vi qi dt log qi t(x(t)) = (cid:10)x, vi t(x)(cid:11) (cid:10)x log qi t(x), vi t(x) ut(x)(cid:11) . (10) See proof in App. C. We use this proposition for our experiments conducted in Sec. 4.1. However, as outlined previously, evaluating the marginal density via the continuity equation is restricted to small-scale models due to the computational challenges associated with efficiently estimating the divergence of the associated vector field. As result, common line of attack assumes constructing stochastic unbiased estimator that trades for increased speed by introducing bit of variance into the divergence estimate. This approach is known as Hutchinsons estimator and requires computing Jacobian-vector product at every step of the inference. Instead, we propose new way to estimate density that allows for efficient computation while integrating the backward SDE from Prop. 1 with specific choice of the diffusion coefficient. Theorem 1. [Itô density estimator] Consider time-dependent density qt(x) induced by the marginals of the following forward process dxt = ft(xt)dt + gtdWt , xt=0 q0(x) , [0, 1] , (11) where dWt is the Wiener process. Then, for the following backward SDE (with τ = 1 t) dxτ = uτ (xτ )dτ + g1τ dW τ , τ [0, 1] , (12) the change of the log-density log qτ (xτ ) follows the following SDE (cid:18) (cid:10), f1τ (xτ )(cid:11)+ log q1τ (xτ ) = (cid:10)dxτ , log q1τ (xτ )(cid:11) + (cid:28) + f1τ (xτ ) g2 1τ 2 log q1τ (xτ ), log q1τ (xτ ) dτ . (cid:29)(cid:19) (13) We provide the proof in App. C. Notably, the SDE Eq. (13) that keeps track of the change of log-density includes only the divergence of the forward SDE drift (cid:10), f1τ (xτ )(cid:11). However, in practice, when using the Ornstein-Uhlenbeck SDE, this divergence is simply constant due to linear drift scaling. In App. D, we derive the same estimator but in discrete time using the detailed balance condition. 3.2 SUPERDIFF: SUPERPOSING PRE-TRAINED DIFFUSION MODELS Mixture of densities (logical OR). For mixture of the densities, superposition of the models follows directly from the propositions in Sec. 2.2. That is, for every qi t(x), we assume that it can be generated from SDEs or an ODE from Prop. 1 and we assume the knowledge of scores log qi t(x)."
        },
        {
            "title": "Preprint",
            "content": "Then, for the ODE simulation, according to Prop. 3, we can sample from the mixture of densities qmix (x) := 1/M (cid:80)M i=1 qi t(x) using the following vector field g2 1τ 2 (cid:88) vτ (x) = f1τ (x) + (cid:80) qi t(x) qj (x) log qi 1τ (x) , (14) i=1 starting from samples xτ =0 q1(x0). The densities are estimated along the trajectory using Prop. 5. We highlight this analogously applies for simulation using the SDE dxτ = uτ (xτ )dτ + g1τ dW τ . Namely, according to Prop. 4, we use the following vector field: uτ (x) = f1τ (x) + g2 1τ (cid:88) qi t(x) qj (x) (cid:80) log qi 1τ (x) , (15) i=1 starting from the samples xτ =0 q1(x0). The densities are estimated along the trajectory using Thm. 1. We provide the pseudocode in Algorithm 1. Sampling equal densities (logical AND). To produce the sample that have equal densities under different models we rely on our formula for the density update (see Thm. 1) to find the optimal weights for the vector fields. Indeed, for diffusion models, we have system of equations: the equal change of density for every model and the normalization constraint for model weights, which is linear system w.r.t. vector field weights as we show in the following proposition. Proposition 6. [Density control] For the SDE dxτ = (cid:88) j=1 κjuj τ (xτ )dτ + g1τ dW τ , where κ are the weights of different models and (cid:80) log qi by solving system of linear equations w.r.t. κ. 1τ (xτ ) = log qj κj = 1, one can find κ that satisfies 1τ (xτ ) , i, [M ] , (16) (17) We provide the proof and the formulas for the system of linear equations in App. C.1. This proposition allows us to find κ that controls the densities to stay the same for all the models as described in Algorithm 1. This approach can also be straightforwardly extended to the case of diffusion models for satisfying different prescribed density ratios, i.e. 1τ (xτ ) + ℓi . log 1τ (xτ ) = log qi (18)"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 JOINING MODELS WITH DISJOINT TRAINING DATA We validate the proposed Algorithm 1 for the generation of the mixture of the distributions (OR setting). We split CIFAR-10 into two disjoint training sets of equal size (first 5 labels and last 5 labels), train two diffusion models on each part, and generate the samples jointly using both models. Namely, the stochastic inference is the OR implementation of Algorithm 1, whereas the deterministic setting is the integration of the ODEs (see Prop. 1) and the estimation of the log-density according to Prop. 5. For the choice of hyperparameters, architecture, and data preprocessing, we follow (Song et al., 2020). In Table 1, we demonstrate that the performance of SUPERDIFF drastically outperforms the performance of the individual models and performs even better than the model trained on the union of both parts of the dataset. For comparison, we evaluate conventional image quality metrics (Frechet inception distance (FID), Inception score (IS), and feature likelihood divergence (FLD) (Jiralerspong et al., 2023), which takes into account the generalization abilities of the model. 4.2 CONCEPT INTERPOLATION AND SELECTION WITH SUPERDIFF AND STABLE DIFFUSION Next, we evaluate the ability of SUPERDIFF to interpolate (logical AND) or select (logical OR) different concepts using prompt-conditioned Stable Diffusion (SD). In this setup, we generate images from SD by conditioning it on prompt using classifier-free guidance. We define two models using two separate prompts that represent concepts e.g. \"a sunflower\" and \"a lemon\". We can thus consider each prompt-conditioned model as separate diffusion model and apply Algorithm 1 to gen-"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Unconditional image generation performance for CIFAR-10 with models trained on two disjoint partitions of the training data (labeled and B). We compare SUPERDIFF (OR) with the respective models with the model that is trained on the full dataset (modelAB) and random choice between two models (modelA OR B). ODE inference SDE inference FID () IS () FLD () FID () IS () FLD () modelA modelB modelAB modelA OR SUPERDIFF (OR) SUPERDIFF =100 (OR) 14.00 13.09 6.00 4.28 4.41 4.11 8.73 7.89 8.95 9.14 9.12 9. 15.50 0.17 18.90 0.18 8.06 0.12 5.96 0.11 6.10 0.11 5.89 0.11 15.33 13.50 3.50 3.99 4.00 4.00 7.98 7.98 9.14 9.36 9.36 9.48 15.47 0.18 18.54 0.23 7.51 0.11 5.29 0.14 5.33 0.05 5.20 0.11 Table 2: Quantativate evaluation of SD-generated images. For logical AND, we compare SUPERDIFF (AND), joint prompting, and averaging of outputs for two concept prompts. We report the minimum CLIP, ImageReward, and TIFA scores from the two concept prompts, which gives measure of how well each method represents both concepts. For logical OR, we compare SUPERDIFF (OR), joint prompting, and an upper bound of randomly selecting single concept prompt (PromptA OR B). We report the maximum scores from each prompt pair, as well as the absolute difference between the maximum and minimum scores (). These metrics reflect how well method can select one concept to generate. Min. CLIP() Min. ImageReward () Min. TIFA () Joint prompting Average of scores (Liu et al., 2022a) SUPERDIFF (AND) 23.87 24.23 24.79 1.62 1.57 1.39 27.58 32.48 39. PromptA OR (uncorrelated random choice) Joint prompting SUPERDIFF (OR) /Max. CLIP () /Max. ImageReward () /Max. TIFA () 9.13/29. 7.20/29.80 8.58/29.87 2.87/0.70 2.47/0.59 2.76/0.64 88.21/97.58 79.46/97.92 84.10/95.83 erate images sampled with proportionally equal likelihood with respect to both prompt-conditioned models (logical AND), or to generate from the mixture of distributions (logical OR).1 We generate 20 images for 20 different concept pairs (tasks) for our model and baselines (c.f. App. H)). Baselines for concept interpolation (AND). We consider two approaches for composing images as baselines. The first is simple averaging of SD outputs based on the approach in (Liu et al., 2022a); we set κ = 0.5. The second guides SD generation with single joint prompt that encourages SD to interpolate concepts. The prompts are constructed by joining two input concepts with the linking term \"that looks like\". For example, given the concepts \"a sunflower\" and \"a lemon\", the prompt is \"a sunflower that looks like lemon\". For fairness, we also flip the order of the prompt and keep the image with the higher score for all metrics listed below (Luo et al., 2024). Baselines for concept selection (OR). For the OR setting, our baseline is prompting SD that prompts the model to select between two concepts: \"a sunflower or lemon\". As with AND, we also flip the prompt and keep the better image for each metric. As an upper bound, we generate images from SD by prompting it with random choice between the two concepts (PromptA OR B). SUPERDIFF qualitatively generates images with better concept interpolations and selections. We plot sample generated images for SUPERDIFF (AND) in Fig. 1. For the complete set of generated images, see App. (Figs. A3A22). We observe that SUPERDIFF (AND) can interpolate concepts while also maintaining high perceptual quality. In contrast, the averaging baseline either fails to interpolate concepts from both prompts fully or yields images with lower perceptual quality. We observe that SD using single prompt struggles to interpolate both concepts. For concept selection (OR), we find that SUPERDIFF (OR) can faithfully generate images with single concept. The joint prompting baseline can sometimes generate images that combine fragments of both concepts, but other times it also generates images of single concept, typically the first concept in the joint prompt (this also underscores why this method struggles with concept interpolation). We show examples of these cases in Fig. A23. SUPERDIFF outperforms baselines on three image evaluation metrics. To quantitatively evaluate the generated images, we consider three metrics: CLIP Score (Radford et al., 2021), ImageReward (Xu et al., 2024), and TIFA (Hu et al., 2023). CLIP Score measures the cosine similarity between an image 1In Table 2, we report results for inference only using SDEs. For the ODE setting, evaluating densities takes about 1082 7 seconds per image with SD, whereas it only takes 209 2 with our density estimator in the SDE setting (over 5-fold decrease!), while also requiring almost 30% less memory."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Evaluation of SUPERDIFF and baseline models, Proteus & FrameDiff, for unconditional protein generation. We show results for three categories of metrics: designability, novelty, and diversity. We also include baseline of simple averaging of scores (κ = 0.5). We use the parameter ℓ (see Eq. (18)) to control and bias model superposition towards Proteus, i.e. (ℓ > 0) or FrameDiff (ℓ < 0). We use temperature values = 1 for all variants of SUPERDIFF. For each type of model composition (averaging or SUPERDIFF), we mark each metric with () if it is better than both Proteus and FrameDiff on their own, and with () if it better than either one of them. Designability < 2Å scRMSD () scRMSD () FrameDiff Proteus Average of scores (Liu et al., 2022a) 0.392 0.03 0.928 0.02 0.740 0.03 4.315 0.25 1.014 0.07 1.960 0.14 < 0.5 scTM () 0.152 0.02 0.360 0.03 0.360 0.03 SUPERDIFFℓ=0(OR) SUPERDIFFℓ=1(OR) SUPERDIFFℓ=0(AND) 0.752 0.03 0.976 0.01 0.752 0.03 1.940 0.14 0.929 0.05 2.079 0.16 0.276 0.03 0.396 0.03 0.296 0.03 0.008 0.01 0.024 0.01 0.040 0.01 0.547 0.01 0.528 0.01 0.521 0.01 Novelty < 0.3 scTM () Max. scTM () Frac. β () Pairwise scTM () Max. cluster () Diversity 0.016 0.01 0.020 0.01 0.024 0.01 0.570 0.02 0.536 0.01 0.511 0.01 0.175 0.01 0.119 0.01 0.139 0.01 0.147 0.01 0.127 0.01 0.141 0.01 0.337 0.02 0.312 0.01 0.310 0.01 0.309 0.02 0.307 0.02 0.306 0.01 0.326 0.05 0.217 0.02 0.253 0.01 0.268 0.02 0.246 0.03 0.256 0.01 embedding and text prompt embedding. ImageReward evaluates generated images by assigning score that reflects how closely they align with human preferences, including aesthetic quality and prompt adherence. TIFA generates several question-answer pairs using Large Language Model for given prompt and assigns score by answering the questions based on the image with visual questionanswering model. We report these metrics for SUPERDIFF and all baselines in Table 2. For the logical AND setting, we evaluate the image against each concept prompt separately (i.e., \"a sunflower\" as one prompt and \"a lemon\" as the other) and take the minimum score for each metric. This is so that we can measure how well both concepts are represented. We find that SUPERDIFF (AND) obtains the highest scores across all metrics, indicating that our method can better represent both concepts in the images, while the baseline methods typically only represent one concept or (especially in the case of averaging outputs), generate compositions with lower fidelity. For the logical OR setting, we again evaluate the image against each concept prompt separately and take both the maximum score and the absolute difference between both scores for each metric. This is so that we can measure how well one concept is represented. The upper bound for this setting is randomly prompting SD with either of the prompts; we find that we are almost able to match this setting across all scores, indicating that our method is able to faithfully select single concept. SD with joint prompting does not perform as well, as nothing prevents it from combining components from both concepts. 4.3 PROTEIN GENERATION Finally, we apply our method in the setting of unconditional de novo protein generation. Protein generation has critical implications in drug discovery (Abramson et al., 2024). good understanding of the protein landscape is important to rationally find novel proteins. We evaluate proteins generated by the superposition of two existing protein diffusion models, Proteus (Wang et al., 2024) and FrameDiff (Yim et al., 2023b), in terms of their designability, novelty, and diversity.We report the results of our best model in Table 3, as well as results for each model individually and simply averaging them. All results are averages over 50 generated proteins at lengths {100, 150, 200, 250, 300} for 500 timesteps. Metrics for evaluations. We consider designability, novelty, and diversity metrics for evaluation of unconditional generation of protein structures. Protein designability refers to the insilico agreement between generated structures and refolded structures as computed using purpose-built folding model e.g. ESMFold (Lin et al., 2022), which is positively correlated with the synthesizability of the protein in wet-lab setting. Generally, if the root-mean-square distance between the generated and refolded proteins (scRMSD) is less than 2Å, it is considered to be designable. We compute several novelty metrics based on the similarity of generated proteins to those from the set of known proteins (the training set), which are called scTM scores; the lower the score, the less similar the generated protein is to the training data. Lastly, we use diversity to assess the degree of heterogeneity present in the set of generated proteins. This is done by clustering the generated proteins in terms of Figure 3: UMAP visualization of protein structures showing cluster archetypes where structure diversity is maintained with SUPERDIFF ℓ=0 (OR)."
        },
        {
            "title": "Preprint",
            "content": "sequence overlap, and measuring the fraction of proteins with challenging-to-generate secondary structures such as β sheets (Bose et al., 2024). We provide details of all metrics in this section in App. G.2. SUPERDIFF improves structure generation. By combining two protein diffusion models, SUPERDIFF is able to outperform both of them. This is somewhat surprising as FrameDiff is substantially less designable than Proteus (0.392 vs. 0.928 scRMSD) (see Table 3). Nevertheless, by using SUPERDIFF (OR), we can increase designability, novelty, and maintain diversity. We also find that SUPERDIFF (OR) substantially outperforms simple averaging, which simply mixes the performance of the two models, and does not improve over either one. We further investigate the composition made by SUPERDIFF (OR) in Fig. 3. Here, we see few modes (particularly on the plots left-hand side) that Proteus does not generate. We find that SUPERDIFF (OR) can maintain knowledge of these clusters (although to lesser extent) while maintaining designability. We also find that SUPERDIFF (AND) outperforms averaging in designability and diversity. Perhaps more impressively, SUPERDIFF (AND) can generate the most proteins that are furthest away from the set of known proteins (scTM score < 0.3) by almost two times more than the next best method. We visualize these proteins in Fig. A2 and explore composition in Fig. A1. This motivates the utility of applying our method in novel discovery settings."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Compositional diffusion models. Combining multiple density models into one model with better properties is classical subject in machine learning (Hinton, 2002). For diffusion models, the most straightforward combination is via averaging their respective learned score functions (Liu et al., 2022a; Kong et al., 2024), which can also be viewed as form of guidance (Ho et al., 2020; Dhariwal & Nichol, 2021; Bansal et al., 2023). Another way to combine diffusion models comes from their connections to energy-based models (Du et al., 2023; 2020; Nie et al., 2021; Ajay et al., 2024). This, however, comes under strong assumption that the marginal densities of the noising process are given, which is not the case in most of the modern applications (e.g. (Rombach et al., 2022)); our method resolves this. Density estimation, also, is the main bottleneck for the continual learning of diffusion models, e.g. Golatkar et al. (2023) proposes to learn separate model for the densities in order to simulate the vector fields from Prop. 3. Finally, Zhong et al. (2024); Biggs et al. (2024) propose to combine models by averaging their weights which, however, does not allow for merging of models with different architectures or different conditionsboth resolved by SUPERDIFF in Sec. 4.3 and Sec. 4.2 respectively. Protein generation. Structure-based de novo protein design using deep generative models has recently seen surge in interest, with particular emphasis on diffusion-based approaches (Watson et al., 2023; Yim et al., 2023b), and also flow matching methods (Bose et al., 2024; Yim et al., 2023a; 2024; Huguet et al., 2024). Moreover, building on the initial SE(3) equivariant diffusion paradigm multiple recent approaches have sought to increase the performance of the methods through architectural innovations (Wang et al., 2024), conditioning on auxiliary modalities such as sequence or sidechains (Ingraham et al., 2023; Lin et al., 2024). Finally, recent approaches tackle the problem of co-generation which seeks to define joint inference procedure over both structure and sequences (Campbell et al., 2024; Ren et al., 2024; Lisanza et al., 2023), but remains distinct from the setting of this work which attempts to combine different pre-trained models using superposition."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Despite the ubiquity of diffusion models, many possible ways of performing generation remain unexplored, with classifier-free guidance being the only practical option (Ho & Salimans, 2022). In this paper, we address this shortcoming by proposing two novel methods for combining different models (or the same model with different condition variables) using SUPERDIFF for joint generation. Limitations. While computationally efficient SUPERDIFF is still limited by the computational budget required to produce the outputs of each model. In particular, the combination at the level of model outputs cannot be simply done via cheap combinations of pre-trained model weights. This, however, invites us to develop more principledarchitecture and training-agnosticmethod that does not require any assumptions prior assumptions which makes SUPERDIFF general purpose method. Future Work. Our method unlocks novel research directions by allowing for the principled generation of novel samples that were not previously possible to generate easily Sec. 4.2. Furthermore, we argue that the proposed way to estimate the density during the generation enables numerous new potential ways to control the generation process by providing information about the likelihood."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "To facilitate reproducibility of our empirical results and findings, we intend to make our code publicly available in the final version. We describe all mathematical and algorithmic details necessary In Sec. 3 we outline the theoretical basis and to reproduce our results throughout this paper. mathematical framework for our method. Furthermore, we provide pseudocode for our method in Algorithm 1. For our theoretical contributions, we provide detailed proofs for all theorems and propositions in App. A, App. and App. C. We provide experimental details for the CIFAR-10 image experiment results in Sec. 4.1. Details regarding experiments for concept interpolation via Stable Diffusion are discussed in Sec. 4.2 and App. H. Experimental details for unconditional protein generation are described in Sec. 4.3 and App. G.1."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "The authors thank Rob Brekelmans for providing the feedback significantly improving the paper. The research was enabled in part by by the Province of Ontario and companies sponsoring the Vector Institute (http://vectorinstitute.ai/partners/), the computational resources provided by the Digital Research Alliance of Canada (https://alliancecan.ca), and Mila (https://mila.quebec). KN is supported by IVADO. AJB is partially supported by an NSERC Post-doc fellowship. This research is partially supported by EPSRC Turing AI World-Leading Research Fellowship No. EP/X040062/1. REFERENCES Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew Ballard, Joshua Bambrick, et al. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, pp. 13, 2024. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Anurag Ajay, Seungwook Han, Yilun Du, Shuang Li, Abhi Gupta, Tommi Jaakkola, Josh Tenenbaum, Leslie Kaelbling, Akash Srivastava, and Pulkit Agrawal. Compositional foundation models for hierarchical planning. Advances in Neural Information Processing Systems, 36, 2024. Michael Albergo, Nicholas Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 843852, 2023. Helen Berman, John Westbrook, Zukang Feng, Gary Gilliland, Talapady Bhat, Helge Weissig, Ilya Shindyalov, and Philip Bourne. The protein data bank. Nucleic acids research, 28(1): 235242, 2000. Benjamin Biggs, Arjun Seshadri, Yang Zou, Achin Jain, Aditya Golatkar, Yusheng Xie, Alessandro Achille, Ashwin Swaminathan, and Stefano Soatto. Diffusion soup: Model merging for text-toimage diffusion models. arXiv preprint arXiv:2406.08431, 2024. Avishek Joey Bose, Tara Akhound-Sadegh, Guillaume Huguet, Killian Fatras, Jarrid Rector-Brooks, Cheng-Hao Liu, Andrei Cristian Nica, Maksym Korablyov, Michael Bronstein, and Alexander Tong. Se(3)-stochastic flow matching for protein backbone generation. In The International Conference on Learning Representations (ICLR), 2024. Arwen Bradley and Preetum Nakkiran. Classifier-free guidance is predictor-corrector. arXiv preprint arXiv:2408.09000, 2024. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators."
        },
        {
            "title": "Preprint",
            "content": "Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. arXiv preprint arXiv:2402.04997, 2024. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert Ragotte, Lukas Milles, Basile IM Wicky, Alexis Courbet, Rob de Haas, Neville Bethel, et al. Robust deep learningbased protein sequence design using proteinmpnn. Science, 378(6615):4956, 2022. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Q. Dong, Lei L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui. survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022. Yilun Du and Leslie Kaelbling. Compositional generative modeling: single model is not all you need. arXiv preprint arXiv:2402.01103, 2024. Yilun Du, Shuang Li, and Igor Mordatch. Compositional visual generation with energy based models. Advances in Neural Information Processing Systems, 33:66376647, 2020. Yilun Du, Conor Durkan, Robin Strudel, Joshua Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc. In International conference on machine learning, pp. 84898510. PMLR, 2023. Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, et al. Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems, 36, 2024. Aditya Golatkar, Alessandro Achille, Ashwin Swaminathan, and Stefano Soatto. Training data protection with compositional diffusion models. arXiv preprint arXiv:2308.01937, 2023. Herbert and Sternberg. Maxcluster: tool for protein structure comparison and clustering, 2008. Geoffrey Hinton. Training products of experts by minimizing contrastive divergence. Neural computation, 14(8):17711800, 2002. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A. Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 2040620417, October 2023. Guillaume Huguet, James Vuckovic, Kilian Fatras, Eric Thibodeau-Laufer, Pablo Lemos, Riashat Islam, Cheng-Hao Liu, Jarrid Rector-Brooks, Tara Akhound-Sadegh, Michael Bronstein, et al. Sequence-augmented se (3)-flow matching for conditional protein backbone generation. arXiv preprint arXiv:2405.20313, 2024. Michael Hutchinson. stochastic estimator of the trace of the influence matrix for laplacian smoothing splines. Communications in Statistics-Simulation and Computation, 18(3):10591076, 1989."
        },
        {
            "title": "Preprint",
            "content": "John Ingraham, Max Baranov, Zak Costello, Karl Barber, Wujie Wang, Ahmed Ismail, Vincent Frappier, Dana Lord, Christopher Ng-Thow-Hing, Erik Van Vlack, et al. Illuminating protein space with programmable generative model. Nature, 623(7989):10701078, 2023. Marco Jiralerspong, Avishek Joey Bose, Ian Gemp, Chongli Qin, Yoram Bachrach, and Gauthier Gidel. Feature likelihood score: Evaluating generalization of generative models using samples, 2023. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Lingkai Kong, Yuanqi Du, Wenhao Mu, Kirill Neklyudov, Valentin De Bortol, Haorui Wang, Dongxia Wu, Aaron Ferber, Yi-An Ma, Carla Gomes, et al. Diffusion models as constrained samplers for optimization with unknown constraints. arXiv preprint arXiv:2402.18012, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model In Proceedings of the 29th Symposium on Operating Systems serving with pagedattention. Principles, pp. 611626, 2023. Yeqing Lin, Minji Lee, Zhao Zhang, and Mohammed AlQuraishi. Out of many, one: Designing and scaffolding proteins at the scale of the structural universe with genie 2. arXiv preprint arXiv:2405.15489, 2024. Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, et al. Language models of protein sequences at the scale of evolution enable accurate structure prediction. bioRxiv, 2022. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. International Conference on Learning Representations, 2022. Sidney Lyayuga Lisanza, Jake Merle Gershon, Sam Tipps, Lucas Arnoldt, Samuel Hendel, Jeremiah Nelson Sims, Xinting Li, and David Baker. Joint generation of protein sequence and structure with rosettafold sequence space diffusion. bioRxiv, pp. 202305, 2023. Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua Tenenbaum. Compositional visual generation with composable diffusion models. In European Conference on Computer Vision, pp. 423439. Springer, 2022a. Qiang Liu. Rectified flow: marginal preserving approach to optimal transport. arXiv preprint arXiv:2209.14577, 2022. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. International Conference on Learning Representations, 2022b. Michael Luo, Justin Wong, Brandon Trabucco, Yanping Huang, Joseph E. Gonzalez, Zhifeng Chen, Ruslan Salakhutdinov, and Ion Stoica. Stylus: Automatic adapter selection for diffusion models. Advances in Neural Information Processing Systems, 2024. Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction, 2018. Midjourney. https://www.midjourney.com/home/, 2023. Accessed: 2023-09-09. Weili Nie, Arash Vahdat, and Anima Anandkumar. Controllable and compositional generation with latent-space energy-based models. Advances in Neural Information Processing Systems, 34: 1349713510, 2021. Bernt Øksendal. Stochastic differential equations. In Stochastic differential equations, pp. 6584. Springer, 2003. OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023."
        },
        {
            "title": "Preprint",
            "content": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning. PMLR, 2021. Milong Ren, Tian Zhu, and Haicang Zhang. Carbonnovo: Joint design of protein structure and sequence using unified energy-based model. In Forty-first International Conference on Machine Learning, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1068410695, June 2022. Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models mirage? arxiv. arXiv preprint arXiv:2304.15004, 2023. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 2022. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2020. Stability AI. https://stability.ai/stablediffusion, 2023. Accessed: 2023-09-09. Jacob Steinhardt. AI Forecasting: One Year In . https://bounded-regret.ghost.io/ ai-forecasting-one-year-in/, 2022. Accessed: 2023-09-09. Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476482, 2024. Brian L. Trippe, Jason Yim, Doug Tischer, David Baker, Tamara Broderick, Regina Barzilay, and Tommi S. Jaakkola. Diffusion probabilistic modeling of protein backbones in 3d for the motifscaffolding problem. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=6TxBxqNME1Y. Michel Van Kempen, Stephanie Kim, Charlotte Tumescheit, Milot Mirdita, Jeongjae Lee, Cameron LM Gilchrist, Johannes Söding, and Martin Steinegger. Fast and accurate protein structure search with foldseek. Nature biotechnology, 42(2):243246, 2024. Pascal Vincent. connection between score matching and denoising autoencoders. Neural computation, 23(7):16611674, 2011. Chentong Wang, Yannan Qu, Zhangzhi Peng, Yukai Wang, Hongli Zhu, Dachuan Chen, and Longxing Cao. Proteus: exploring protein structure generation for enhanced designability and efficiency. bioRxiv, pp. 202402, 2024. Joseph L. Watson, David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, Andrew J. Borst, Robert J. Ragotte, Lukas F. Milles, Basile I. M. Wicky, Nikita Hanikel, Samuel J. Pellock, Alexis Courbet, William Sheffler, Jue Wang, Preetham Venkatesh, Isaac Sappington, Susana Vázquez Torres, Anna Lauko, Valentin De Bortoli, Emile Mathieu, Sergey Ovchinnikov, Regina Barzilay, Tommi S. Jaakkola, Frank DiMaio, Minkyung Baek, and David Baker. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976):10891100, 2023. ISSN 1476-4687. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024."
        },
        {
            "title": "Preprint",
            "content": "Jason Yim, Andrew Campbell, Andrew Y. K. Foong, Michael Gastegger, José Jiménez-Luna, Sarah Lewis, Victor Garcia Satorras, Bastiaan S. Veeling, Regina Barzilay, Tommi Jaakkola, and Frank Noé. Fast protein backbone generation with se(3) flow matching, 2023a. Jason Yim, Brian Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and Tommi Jaakkola. Se (3) diffusion model with application to protein backbone generation. arXiv preprint arXiv:2302.02277, 2023b. Jason Yim, Andrew Campbell, Emile Mathieu, Andrew YK Foong, Michael Gastegger, José JiménezImproved Luna, Sarah Lewis, Victor Garcia Satorras, Bastiaan Veeling, Frank Noé, et al. motif-scaffolding with se (3) flow matching. arXiv preprint arXiv:2401.04082, 2024. Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, and Weizhu Chen. Multi-lora composition for image generation. arXiv preprint arXiv:2402.16843, 2024."
        },
        {
            "title": "APPENDIX",
            "content": "A PROOFS FOR SEC. 2 A.1 PROOF OF PROP. 1 Proposition 1. [Reverse-time SDEs/ODE] Marginal densities qt(x) induced by Eq. (1) correspond to the densities induced by the following SDE that goes back in time (τ = 1 t) with the corresponding initial condition (cid:18) dxτ = ft(xτ ) + (cid:18) g2 2 (cid:19) + ξτ log qt(xτ ) (cid:19) dτ + (cid:112)2ξτ dW τ , xτ =0 q1(x0) , (2) where τ is the standard Wiener process in time τ , and ξτ is any positive schedule. For the forward SDE we can write the corresponding Fokker-Planck equation qt(x) = (cid:10), qt(x)ft(x)(cid:11) + g2 2 qt(x) , τ then for the inverse time τ = 1 t, we have g2 q1τ (x) = (cid:10), q1τ (x)f1τ (x)(cid:11) 1τ 2 g2 1τ 2 , q1τ (x) f1τ (x) = (cid:28) (cid:18) q1τ (x) (cid:19)(cid:29) log q1τ (x) (19) (20) (21) (cid:28) (cid:18) = , q1τ (x) f1τ (x) + (cid:18) g2 1τ 2 (cid:19) (cid:19)(cid:29) + ξτ log q1τ (x) + ξτ q1τ (x) , which corresponds to the SDE (cid:18) dxτ = dτ f1τ (xτ ) + (cid:18) g2 1τ 2 (cid:19) (cid:19) + ξτ log q1τ (xτ ) + (cid:112)2ξτ dW τ . (22) (23) A.2 PROOF OF PROP. 2 Proposition 2. [OrnsteinUhlenbeck SDE] The time-dependent densities in Eq. (3) correspond to the marginal densities of the following SDE, with the corresponding initial condition dxt = log αt (cid:123)(cid:122) ft(xt) (cid:124) xt (cid:125) (cid:114) dt + 2σ2 (cid:124) log σt αt (cid:125) (cid:123)(cid:122) gt dWt , x0 q0(x0) . (4) Proof. For individual components qi continuity equation, which we can rewrite as t(x) = (cid:10)x, vi log qi t(x)(cid:11) (cid:10)x log qi t(x), vi t(x)(cid:11) . from Eq. (3), we have to find the vector field satisfying the Using the formula for the density of the normal distribution qi t(x) = (x αtµi, σ ), we have log qi t(x) = (x αtµi) , 1 σ2 t log qi t(x) = log σt + 1 σ3 σt (cid:13) (cid:13)x αtµi(cid:13) 2 (cid:13) (cid:42) + 1 σ2 (cid:10)x αtµi, µi(cid:11) αt (cid:28) = x, log σt (cid:29) log qi t(x), log σt (x αtµi) + (cid:123)(cid:122) vi t(x) αt (cid:43) µi (cid:125) = (cid:10)x, vi t(x)(cid:11) (cid:10)x log qi t(x), vi (cid:124) t(x)(cid:11) . 16 (24) (25) (26) (27) (28)"
        },
        {
            "title": "Preprint",
            "content": "For the mixture of densities qt from Eq. (3), we can use Prop. 3 and write (cid:88) qi t(x) (cid:20) log σt (x αtµi) + (cid:21) µi αt vt(x) = 1 qt(x)N = log σt t"
        },
        {
            "title": "At the same time",
            "content": "i=1 + (cid:20) αt log σt αt (cid:21) 1 qt(x)N (cid:88) i=1 t(x)µi . qi log qt(x) = 1 qt(x)N (cid:88) i=1 qi t(x) (cid:20) 1 σ2 (cid:21) (x αtµi) , 1 qt(x)N (cid:88) i= t(x)µi = qi 1 αt (cid:2)σ2 log qt(x) + x(cid:3) . (29) (30) (31) (32) vt(x) = Using this formula in Eq. (30), we have (cid:20) log σt log αt αt t σ2 1 αt log + = σt αt log σt (cid:21) + (cid:20) αt log σt αt (cid:21) σ2 αt log qt(x) (33) log qt(x) . (34) Hence, we have qt(x) = (cid:10), qt(x)vt(x)(cid:11) = (cid:28) , qt(x) (cid:19)(cid:29) (cid:18) log αt + σ2 log σt αt qt(x) , (35) which corresponds to the SDE in the statement. A.3 PROOF OF PROP. 3 Proposition 3. [Superposition of ODEs (Liu, 2022)] The mixture density in Eq. (6) follows the continuity equation with the superposed vector fields from Eq. (5), i.e. qmix (x) = (cid:10)x, qmix (x)vt(x)(cid:11) , vt(x) = 1 j=1 ωjqj (cid:80)N (x) (cid:88) i=1 ωiqi t(x)vi t(x) . Proof. By the straightforward substitution, we have (cid:88) (cid:88) qmix (x) = ωi qi t(x) = ωi(cid:10)x, qi t(x)vi t(x)(cid:11) i=1 (cid:42) = x, i=1 ωiqi t(x)vi t(x) (cid:43) (cid:42) = x, (cid:88) i=1 (cid:42) = x, qmix (x) 1 j=1 ωjqj (cid:80)m (x) (cid:88) i=1 (cid:123)(cid:122) vt(x) ωiqi t(x)vi t(x) . (cid:125) (cid:43) ωiqi t(x)vi t(x) qmix qmix (x) (x) (cid:88) i=1 (cid:43) (cid:124) A.4 PROOF OF PROP. Proposition 4. [Superposition of SDEs] The mixture qmix marginals {qi i=1 induced by SDEs from Eq. (8) corresponds to the following SDE i=1 ωiqi t(x)}N t(x) of density (x) := (cid:80)N dxτ = uτ (xτ )dτ + gτ dW τ , ut(x) = 17 1 j=1 ωjqj (cid:80)N (x) (cid:88) i=1 ωiqi t(x)ui t(x) . (9) (7) (36) (37) (38)"
        },
        {
            "title": "Preprint",
            "content": "Proof. By the straightforward substitution, we have (cid:18) qmix (x) = (cid:88) i= ωi (cid:88) qi t(x) = ωi (cid:10)x, qi t(x)ui t(x)(cid:11) + The first term is analogous to Prop. 3, i.e. (cid:42) ωi(cid:0)(cid:10)x, qi t(x)ui t(x)(cid:11)(cid:1) = (cid:88) i="
        },
        {
            "title": "The second term is",
            "content": "i=1 x, qmix (x) 1 j=1 ωjqj (cid:80)m (x) (cid:124) (cid:88) i=1 (cid:123)(cid:122) ut(x) (cid:88) i= ωi g2 2 qi t(x) = g2 2 (cid:88) i=1 ωiqi t(x) = g2 2 qmix (x) . (cid:19) . qi t(x) g2 2 (39) (cid:43) ωiqi t(x)ui t(x) (cid:125) . (40) (41)"
        },
        {
            "title": "This results in the following PDE",
            "content": "(cid:42) qmix (x) = x, qmix (x) (cid:88) (cid:43) ωiqi t(x)ui t(x) + g2 2 qmix (x) . (42) 1 j=1 ωjqj (cid:80)m (x) (cid:124) i=1 (cid:123)(cid:122) ut(x) (cid:125)"
        },
        {
            "title": "B FLOWS AND DIFFUSION AS A SUPERPOSITION OF ELEMENTARY VECTOR",
            "content": "FIELDS From Prop. 3, one can immediately get the main principle for the simulation-free training of generative flow models, as demonstrated in the following proposition. Proposition 7. [Superposition of L2-losses] For the parametric vector field model vt(x; θ) with parameters θ, the L2-loss for the vector field from Prop. 3 can be decomposed into the losses for the vector fields from Eq. (5), i.e. (cid:90) 1 (cid:88) (cid:90) 1 dt qi t(x) (cid:13) (cid:13)vi t(x) vt(x; θ)(cid:13) 2 (cid:13) + constant , dt qmix (x)vt(x) vt(x; θ)2 = 0 ωi i=1 0 where the constant does not depend on the parameters θ. Proof. By the straightforward calculation, we have (cid:90) (cid:90) 1 dt (x)vt(x) vt(x; θ)2 = qmix dt qmix (x) 0 0 (cid:104) vt(x)2 2(cid:10)vt(x), vt(x; θ)(cid:11) + vt(x; θ)2(cid:105) , where the first term is constant w.r.t. θ and the last term is amenable for straightforward estimation. The middle term, according to Prop. 3, is (cid:90) dx qmix (x)(cid:10)vt(x), vt(x; θ)(cid:11) = (cid:90) dx (cid:88) i=1 ωiqi t(x)(cid:10)vi t(x), vt(x; θ)(cid:11) . Thus, we have (cid:90) 1 0 (cid:90) 0 = dt qmix (x) (cid:20) vt(x)2 2(cid:10)vt(x), vt(x; θ)(cid:11) + vt(x; θ)2 (cid:21) (cid:20) dt (x)vt(x)2 qmix (cid:88) ωiE qi t(x) (cid:13) (cid:13)vi t(x)(cid:13) 2 (cid:13) + (cid:88) i=1 ωiE qi t(x) (cid:13) (cid:13)vi t(x)(cid:13) 2 (cid:13) i=1 (cid:123)(cid:122) constant (cid:125) (cid:124) 2 (cid:88) i= ωiE qi t(x) (cid:10)vi t(x), vt(x; θ)(cid:11) + (cid:88) i= 18 ωiE t(x)vt(x; θ)2 qi (cid:21) (43) (44) (45) (46)"
        },
        {
            "title": "Preprint",
            "content": "= (cid:90) 1 0 dt (cid:88) i= ωiE qi t(x) (cid:13) (cid:13)vi t(x) vt(x; θ)(cid:13) 2 (cid:13) + constant . (47) The proof for proposition follows simple extension of Liu (2022). Consequently, the generative modeling problem explicitly boils down to learning time-dependent vector field vt(x, θ). Moreover, for the generative modeling task, the noising process is required to satisfy the following boundary conditions qi 1(x) = pnoise(x) , D. Generating samples that resemble pdata(µ) during inference then simply amounts to solving the reverse-time ODE with the learned model vt(x; θ) starting from noisy sample x1 pnoise(x). Diffusion models can be incorporated into the flow-based framework by selecting Gaussian forward process which gives the marginal density at timestep as the mixture of corresponding Gaussians, 0(x) = δ(x µi) and qi t(x) = (x αtµi, σ2 qi ) , qt(x) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 qi t(x) . (48) Under this Gaussian diffusion framework, we can express the vector fields analytically using the score, which is formalized by the following proposition. Proposition 8. [Diffusion processes] The time-dependent densities in Eq. (48) satisfy the following continuity equations t(x) = (cid:10)x, qi qi t(x)(cid:11) , vi t(x)vi t(x) = (49) log σt log αt (x αtµi) + (cid:18) σ2 log µi , (cid:19) αt σt αt log qt(x) . (50) qt(x) = (cid:10)x, qt(x)vt(x)(cid:11) , vt(x) = t The proof repeats the proof for Prop. 2 in App. A.2 since Eq. (50) corresponds to the OrnsteinUhlenbeck process: dxt = log αt xtdt + (cid:115) (cid:18) 2 σ2 log σt αt (cid:19) dWt. (51) Diffusion models using Gaussian perturbation kernel enjoy the benefit of giving an exact expression for the Stein score of the perturbed data which leads to the celebrated denoising score matching objective (Vincent, 2011; Ho et al., 2020). Analogously to Prop. 7 we can write the denoising score matching objective as the superposition of the scores as demonstrated in the following proposition. Proposition 9. [Denoising Score Matching (Vincent, 2011)] For the parametric score model log qt(x; θ) with parameters θ, the score matching objective can be decomposed into the corresponding objectives for the individual scores, i.e. (cid:90) 1 0 (cid:90) 1 (cid:88) i=1 = 1 dt Eqt(x) log qt(x) log qt(x; θ)2 = dt qi t(x) (cid:13) (cid:13) log qi t(x) log qt(x; θ)(cid:13) 2 (cid:13) + constant , where the constant does not depend on the parameters θ and log qi t(x) = 1 σ2 (x αtµi). Proof. From Prop. 8, we have log σt log σt vi t(x) = = (x αtµi) + (x (x + σ2 t(x))) + αt 1 αt (x + σ2 log qi t(x)) (52) (53) µi αt log qi"
        },
        {
            "title": "Preprint",
            "content": "= vt(x) = log αt log αt + σ2 + σ2 t log log αt σt αt σt log qi t(x) log qt(x) ."
        },
        {
            "title": "Applying the same change of variables to the parametric model",
            "content": "vt(x; θ) = log αt + σ2 log αt σt log qt(x; θ) , (54) (55) (56) and using Prop. 7, we have dt Eqt(x)vt(x) vt(x; θ)2 = (cid:88) i="
        },
        {
            "title": "1\nN",
            "content": "(cid:90) 1 0 dt Eqt(x)x log qt(x) log qt(x; θ)2 (cid:90) 1 0 (cid:90) 1 = (cid:88) i="
        },
        {
            "title": "1\nN",
            "content": "(cid:90) 1 0 dt qi t(x) (cid:13) (cid:13)x log qi t(x) log qt(x; θ)(cid:13) 2 (cid:13) + (cid:16) which concludes the proof."
        },
        {
            "title": "C DENSITY ESTIMATORS",
            "content": "dt qi t(x) (cid:13) (cid:13)vi t(x) vt(x; θ)(cid:13) 2 (cid:13) + constant , (57) (58) (59) (cid:17) , constant log αt σt σ2 Proposition 5. [Smooth density estimator] For the integral curve x(t) solving dx/dt = ut(xt), t(x)(cid:11), the and the density qi log-density along the curve changes according to the following ODE t(x(t)) satisfying the continuity equation t(x) = (cid:10)x, qi t(x)vi qi dt log qi t(x(t)) = (cid:10)x, vi t(x)(cid:11) (cid:10)x log qi t(x), vi t(x) ut(x)(cid:11) . Proof. By straightforward computation, we have dt log qi t(x(t)) = (cid:28) log qi t(x) + t(x) = (cid:10)x, qi t(x)vi and using the continuity equation t qi t(x) = (cid:10)x, vi log qi t(x)(cid:11) (cid:10)x log qi (cid:29) , dx log qi t(x), dt t(x)(cid:11), we have t(x)(cid:11) , t(x), vi dt log qi t(x(t)) = (cid:10)x, vi t(x)(cid:11) (cid:10)x log qi t(x), vi t(x) ut(x)(cid:11) . (10) (60) (61) (62) Theorem 1. [Itô density estimator] Consider time-dependent density qt(x) induced by the marginals of the following forward process dxt = ft(xt)dt + gtdWt , xt=0 q0(x) , [0, 1] , (11) where dWt is the Wiener process. Then, for the following backward SDE (with τ = 1 t) dxτ = uτ (xτ )dτ + g1τ dW τ , τ [0, 1] , (12) the change of the log-density log qτ (xτ ) follows the following SDE (cid:18) (cid:10), f1τ (xτ )(cid:11)+ log q1τ (xτ ) = (cid:10)dxτ , log q1τ (xτ )(cid:11) + (cid:28) + f1τ (xτ ) g2 1τ 2 log q1τ (xτ ), log q1τ (xτ ) dτ . (cid:29)(cid:19) (13) Proof. The Fokker-Planck equation describing the evolution of the marginals for the forward process is qt(x) = (cid:10), ft(x)qt(x)(cid:11) + g2 2 qt(x) , (63) 20 (cid:18) τ g2 1τ 2 +"
        },
        {
            "title": "Preprint",
            "content": "hence, for the inverse time τ = 1 t, we have q1τ (x) = (cid:10), f1τ (x)q1τ (x)(cid:11) τ log q1τ (x) = (cid:10), f1τ (x)(cid:11) + (cid:10) log q1τ (x), f1τ (x)(cid:11) q1τ (x) g2 1τ 2 τ g2 1τ 2 log p1τ (x) g2 1τ 2 log q1τ (x)2 . For the following reverse-time SDE dxτ = uτ (xτ )dτ + g1τ dW τ , using Itôs lemma for log q1τ (xτ ), we have log q1τ (xτ ) = log q1τ (xτ ) + (cid:10) log q1τ (xτ ), uτ (xτ )(cid:11)+ (cid:19) log qτ (xτ ) dτ + g1τ (cid:10) log q1τ (xτ ), dW τ (cid:11) . Using Eq. (65), we have (cid:18)(cid:28) log q1τ (xτ ) = log q1τ (xτ ), f1τ (xτ ) (cid:29) log q1τ (xτ ) + g2 1τ 2 + (cid:10), f1τ (xτ )(cid:11) (cid:42) (cid:19) dτ + log q1τ (xτ ), uτ (xτ )dτ + g1τ dW τ (cid:125) (cid:124) (cid:123)(cid:122) dxτ (cid:43) C.1 PROOF OF PROP. 6 Proposition 6. [Density control] For the SDE dxτ = (cid:88) j=1 κjuj τ (xτ )dτ + g1τ dW τ , where κ are the weights of different models and (cid:80) log qi by solving system of linear equations w.r.t. κ. 1τ (xτ ) = log qj κj = 1, one can find κ that satisfies 1τ (xτ ) , i, [M ] , Proof. Using the result of Thm. 1, for the density change of every model, we have log qi 1τ (xτ ) = (cid:10)dxτ , log qi 1τ (xτ )(cid:11) + (cid:18) (cid:10), f1τ (xτ )(cid:11)+ (cid:28) + f1τ (xτ ) g2 1τ 2 log qi 1τ (xτ ), log qi 1τ (xτ ) (cid:29)(cid:19) dτ (64) (65) (66) (67) (68) (69) . (70) (16) (17) (71) (72) (73) = (cid:88) j= (cid:18) (cid:28) + g1τ dW τ + f1τ (xτ ) κjdτ (cid:10)uj τ (xτ ), log qi 1τ (xτ )(cid:11) + (cid:18) (cid:10), f1τ (xτ )(cid:11)+ g2 1τ 2 log qi 1τ (xτ ) (cid:19) dτ, log qi 1τ (xτ ) . (74) (cid:29)(cid:19) Let us denote aij = dτ (cid:10)uj 1τ (xτ )(cid:11) , τ (xτ ), log qi (cid:28) (cid:18) bi = (cid:10), f1τ (xτ )(cid:11) + g1τ dW τ + f1τ (xτ ) 21 g2 1τ 2 log qi 1τ (xτ ) (cid:19) dτ, log qi 1τ (xτ ) . (75) (cid:29) (76)"
        },
        {
            "title": "Preprint",
            "content": "Then the change of densities can be written as the following linear transformation log q1 1τ (xτ ) . . . 1τ (xτ ) . . . log qM 1τ (xτ ) = Aκ + . log qi"
        },
        {
            "title": "Solving",
            "content": "Aκ + = 1 . . . 1 . . . 1 κj = 1, we get the solution. , and normalizing kappas so we have (cid:80) (77) (78) DISCRETE-TIME PERSPECTIVE ON THM. 1 In this section we derive Thm. 1 from another perspective by operating with discrete time transition kernels and the detailed balance. Namely, we aim to compute the marginal density by starting from the detailed balance equation, which states the equivalence of the following joint densities: qt(y)kt(x y) = qt+t(x)rt(y x). (79) The detailed balance condition simply states the pair (x, y) can be sampled in two equivalent ways: 1.) sampling from the marginal qt(y) and then sampling via the noising kernel kt(x y), or 2.) sampling from qt+t(y) and then denoising it via rt(y x). Indeed, there are infinitely many valid kernels that may satisfy the detailed balance; we make specific choice informed by the following principle. We aim to construct universal noising kernel that is independent of the data distribution or other densities, i.e. pdata(), qt(), qt+t(). Remarkably, this principle results in unique kernel choice, which we formalize in the following proposition. Proposition 10. [Universal noising kernel] For any data density pdata(µ), for the continuous noising process qt(x) = (cid:82) dµ (x αtµ, σ2 )pdata(µ), there exists unique noising kernel kt(x y) = , qt+t(x) = dy kt(y x)qt(y), (80) (cid:19) αt+t αt y, S2 t+t (cid:18) (cid:12) (cid:12) (cid:12) (cid:12) α2 (cid:90) where S2 t+t = σ2 t+t σ2 t+t α2 and it is independent of the densities pdata(), qt(), qt+t(). The proof for this proposition is presented in App. D.1. Note that the proposition holds exactly for any ti.e. there are no any assumptions on the scale of or any approximations. Once the noising kernel is fixed, the detailed balance Eq. (79) uniquely defines the denoising kernel rt(y x) that propagates samples back in time. However, the analytic form of this kernel depends on the densities qt+t(x) and qt(y), which are unavailable for the modern large-scale diffusion models that are served using limited API endpoints. Therefore, we consider the Gaussian approximation of the reverse kernel and justify its applicability in the following theorem. Theorem 2. [Denoising kernel] For the universal noising kernel kt(x y), the Gaussian approximation of the corresponding reverse kernel rt(y x) = kt(x y)qt(y)/qt+t(x) is (cid:32) rt(y x) = y (cid:12) (cid:12) (cid:12) (cid:12) αt αt+t + αt αt+t t+t log qt+t(x), (cid:18) αt αt+t (cid:19)2(cid:33) St+t , (81) which corresponds to single step of the Euler discretization of the following SDE σt αt log αtx 2σ2 log qt(x) (cid:18) dxτ = dτ + σt αt t 2σ2 log log (cid:114) (cid:19) dWτ , and DKL(rt(y x)rt(y x)) = o(t) . 22 (82) (83)"
        },
        {
            "title": "Preprint",
            "content": "See proof in App. D.3. Given the noising kernel from Prop. 10 and the approximation of the reverse kernel from Thm. 2, we propose to estimate the log marginal density of the current sample using the detailed balance as follows, log qt(y) log qt+t(x) = log kt(x y) + log rt(y x) + log rt(y x) rt(y x) , (84) where the last term involves the unknown reversed kernel. This term can typically be approximated by taking the divergence of the score (see Lemma 1), but due to the computational challenge of estimating the divergence we opt for different strategy. Instead, in the following theorem, we study the distribution of the last term for different samples generated from and argue that this term can be ignored. Theorem 3. [Detailed balance density estimator error] For = + + gt+t log σt where gt+t = αt estimator is unbiased w.r.t. the generated samples y, i.e. and ε is standard normal random variable, the following 2σ2 (cid:113) tε, (cid:20) log Eε (cid:21) rt(y x) rt(y x) = o(t) , (85) and the variance is (cid:20) IDε log where 2 log qt+t(x) x2 (cid:21) rt(y x) rt(y x) = t2 g4 t+t (cid:18) IDε εT 2 log qt(x) x2 (cid:19) ε + o(t2) , (86) is the Hessian matrix of the log-density. We postpone the proof of this theorem until App. D.4. Note that the last term in Eq. (84) has zero expectation and the same variance as Hutchinsons trace estimator. Thus, we argue that the following estimator is as efficient as the conventional divergence estimator with Hutchinsons trick log qt(y) log qt+t(x) log kt(x y) + log rt(y x) . (87) We further highlight that since the generation via the inference process happens in many steps with small t, we approximate the relation Eq. (87) up to the second order terms in t. Proposition 11. [Recurrence relation for the log-density] For = + + gt+t 2σ2 where gt+t = from Eq. (87) can be expanded as follows and ε is standard normal random variable, the estimator log σt αt (cid:113) tε, log kt(x y) + log rt(y x) = dt (cid:28) + + gt+t tε + t log αt+t g2 t+t 2 log qt+t(x)2+ (cid:29) log αt+tx, log qt+t(x) + o(t) (88) The proof for this proposition is included in App. D.5. For practical use cases we note that Eq. (88) is the final form of our density estimator. D.1 PROOF OF PROP. 10 Proposition 10. [Universal noising kernel] For any data density pdata(µ), for the continuous noising process qt(x) = (cid:82) dµ (x αtµ, σ2 )pdata(µ), there exists unique noising kernel kt(x y) = , qt+t(x) = dy kt(y x)qt(y), (80) (cid:19) αt+t αt y, S2 t+t (cid:18) (cid:12) (cid:12) (cid:12) (cid:12) α2 (cid:90) where S2 t+t = σ t+t σ2 t+t α2 and it is independent of the densities pdata(), qt(), qt+t(). Proof. (cid:90) qt(x) = dµ (x αtµ, σ2 )pdata(µ) (89)"
        },
        {
            "title": "Preprint",
            "content": "Lets derive the incremental kernel kt(x y) from this formula, i.e. qt+t(x) = (cid:90) dy kt(x y)qt(y) (90) (cid:90) dµ (x αt+tµ, σ2 (cid:18) (cid:90) t+t)pdata(µ) = dµ (x αt+tµ, σ2 t+t) (cid:90) (cid:90) (cid:90) dy kt(x y) dµ (y αtµ, σ2 )pdata(µ) (91) dy kt(x y)N (y αtµ, σ2 ) (cid:19) pdata(µ) = 0 (92) (x αt+tµ, σ t+t) = (cid:90) dy kt(x y)N (y αtµ, σ2 ) = αt+tµ + αtµ σt + Rε , where (y αtµ, σ ) , and ε (ε 0, 1) S2 + R2 = σ2 , then the kernel is t+t αt+t αt To make the kernel independent of µ we have to choose = σt kt(x y) = αt+t αt D.2 LEMMA 1 . y, σ2 t+t σ2 (cid:123)(cid:122) t+t S2 (cid:124) α2 t+t α2 (cid:125) (93) (94) (95) (96) Lemma 1. [Reverse kernel lemma] For the universal noising kernel kt(x y), the corresponding reverse kernel rt(y x) = kt(x y)qt(y)/qt+t(x), and its Gaussian approximation (cid:32) rt(y x) = (cid:12) (cid:12) (cid:12) (cid:12) αt αt+t + αt αt+t S2 t+t log qt+t(x), (cid:18) αt αt+t (cid:19)2(cid:33) St+t , (97) we have log rt(x + + gt+t rt(x + + gt+t (cid:18) tε x) tε x) log qt+t(x) εT 2 log qt(x) = = x2 g2 t+t 2 log σt+t αt+t . where gt+t = (cid:113) 2σ2 t+t Proof. From the detailed balance equation, we have log rt(y x) = log kt(x y) + log qt(y) log qt+t(x) (cid:19) ε + o(t) , (98) (99) (100) = 1 (cid:18) 2S2 t+t αt+t αt (cid:19)2 2 log 2πS2 t+t + log qt(y) log qt+t(x) , (101) where we use the definition of the forward kernel. Thus, the difference between kernels is (cid:19)2 (cid:18) log rt(y x) rt(y x) = 1 2S2 t+t αt+t αt log 2πS2 t+t + log qt(y) log qt+t(x)+ + + 1 2(St+t αt αt+t (cid:18) )2 log 2πS2 t+t + 2 log αt αt+t (cid:18) αt (cid:19) . αt αt+t S2 t+t log qt+t(x) αt+t 24 (102) (cid:19)2 + (103) (104)"
        },
        {
            "title": "Preprint",
            "content": "(107) (108) (cid:19) Opening the brackets, we have log rt(y x) rt(y x) = (cid:18) (cid:19)2 1 2S (cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24) (cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24) αt+t αt (cid:18) αt+t αt t+t 1 2S2 (cid:19)2 + t+t + S2 t+t 2 log qt+t(x)2 +"
        },
        {
            "title": "The constants can be estimated as follows",
            "content": "+ log qt(y) log qt+t(x)+ x, log qt+t(x) (cid:28) αt+t αt (cid:18) αt log αt+t (cid:19)2 . (105) + (106) (cid:29) t+t = σ2 S2 t+t σ2 α2 t+t α2 = σ t+t (σ2 t+t 2dtσt+t σt+t + o(t)) α2 t+t α2 = σ2 t+t (σ2 t+t 2dtσt+t σt+t (cid:18) + o(t)) 1 + 2dt = 2dtσ2 t+t log σt+t αt+t + o(t) , and α2 α3 t+t t+t αt+t + o(t) (109) (110) αt+t αt = 1 αt+t α t+t αt+t (t) + o(t) = 1 + log αt+t + o(t) (111) 2 log (cid:18) αt (cid:19)2 αt+t = dtd log αt+t + o(t) . Thus, we have log rt(y x) rt(y x) = log qt(y) log qt+t(x) + tσ t+t dtd (cid:28) log αt+t (1 + log αt+t From Prop. 8, the time-derivative of the density is (112) log σt+t αt+t log qt+t(x)2 (113) (cid:29) )y x, log qt+t(x) + o(t) . (114) log qt(x) = (cid:10), vt(x)(cid:11) (cid:10) log qt(x), vt(x)(cid:11) = log αt log αt (cid:18) + log σ2 σt αt (cid:10) log qt(x), x(cid:11) + (cid:19) log qt(x) (cid:18) σ2 t log σt αt (cid:19) log qt(x)2 . Hence, we have log qt(x) log qt+t(x) = t log qt+t(x) + o(t) = dtd log αt+t (cid:18) σ2 t+t (cid:19) log σt+t αt+t log qt+t(x)+ (115) (116) (117) (118) (119) +t log αt+t (cid:10) log qt+t(x), x(cid:11) (cid:18) σ2 t+t (cid:19) log σt+t αt+t log qt+t(x)2 . (120) Using it in Eq. (114), we have log rt(y x) rt(y x) = log qt(y) log qt(x) (cid:18) σ2 t+t (cid:19) log σt+t αt+t log qt+t(x) (121) (1 + log αt+t )(cid:10)y x, log qt+t(x)(cid:11) + o(t) . (122)"
        },
        {
            "title": "Preprint",
            "content": "Finally, we consider = + + gt+t tε. Then we can estimate log qt(y) = log qt(x) + (cid:10) log qt(x), x(cid:11) + 1 2 (y x)T 2 log qt(x) x2 (y x) + o(y x2) = log qt(x) + + g2 t+t (cid:68) log qt(x), + gt+t εT 2 log qt(x) x2 ε + o(t) , (cid:69) tε where 2 log qt(x) we have x2 denotes the Hessian of the log-density. Thus, for = + + gt+t log rt(y x) rt(y x) = g2 t+t 2 εT 2 log qt(x) x2 ε (cid:18) σ2 t+t log σt+t αt+t (cid:19) (cid:68) log qt(x) log qt+t(x), + gt+t + (123) (124) (125) (126) tε, log qt+t(x)+ (127) tε + o(t) . (128) (cid:69) t+t log σt+t αt+t , we g2 t+t 2 = σ2 log rt(x + + gt+t rt(x + + gt+t log qt+t(x) εT 2 log qt(x) tε x) tε x) (cid:19) ε + o(t) . x2 = (129) (130) The last term is o(t) by expanding the scores in time, hence, for have (cid:18) σ2 t+t = (cid:19)(cid:18) log σt+t αt+t D.3 PROOF OF THM. 2 Theorem 2. [Denoising kernel] For the universal noising kernel kt(x y), the Gaussian approximation of the corresponding reverse kernel rt(y x) = kt(x y)qt(y)/qt+t(x) is (cid:32) rt(y x) = (cid:12) (cid:12) (cid:12) (cid:12) αt αt+t + αt αt+t S2 t+t log qt+t(x), (cid:18) αt αt+t (cid:19)2(cid:33) St+t , (81) which corresponds to single step of the Euler discretization of the following SDE σt αt log αtx 2σ2 log qt(x) (cid:18) dxτ = dτ + σt αt t 2σ2 log log (cid:114) (cid:19) dWτ , and DKL(rt(y x)rt(y x)) = o(t) . (82) (83) Proof. To find the reverse the kernel, we first use Tweedies formula to find the expectation, i.e. (cid:90) (cid:19) (cid:18) log qt+t(x) = 1 qt+t(x) dy αt+t αt kt(x y)qt(y) (131) αt αt+t S2 t+t log qt+t(x) = 1 qt+t(x) (cid:90) + αt αt+t αt αt+t Thus, we are going to approximate S2 t+t log qt+t(x) = dy αt+t kt(x y)qt(y) qt+t(x) (cid:18) αt (cid:19) kt(x y)qt(y) (132) (cid:90) = dy yrt(y x) . (133) 1 S2 t+t (cid:90) dy (cid:32) rt(y x) = (cid:12) (cid:12) (cid:12) (cid:12) αt αt+t + αt αt+t S2 t+t log qt+t(x), (cid:19)2(cid:33) (cid:18) αt αt+t St+t (134)"
        },
        {
            "title": "Preprint",
            "content": "(cid:16) αt St+t where estimated as follows αt+t (cid:17)2 is chosen to match the leading term of kt(x y). The constants can be t+t = σ2 t+t σ2 α2 t+t α2 = σ2 t+t (σ2 t+t 2dtσt+t σt+t + o(t)) α2 t+t α2 = σ2 t+t (σ t+t 2dtσt+t σt+t (cid:18) + o(t)) 1 + 2dt = 2dtσ t+t log σt+t αt+t + o(t) , and α2 α3 t+t t+t αt+t (135) (cid:19) + o(t) (136) (137) αt αt+t 1 αt+t Thus, can be generated as = 1 + αt+t (t) + o(t) = 1 log αt+t + o(t) . (138) = log αt+t + 2dtσ2 t+t σt+t αt+t log (cid:115) log qt+t(x)+ (139) + 2dtσ2 t+t log σt+t αt+t ε , (140) where ε is standard normal random variable. This corresponds to the single step of the Euler discretization of the following SDE (cid:18) log αtx 2σ2 log qt(x) dxt = + dWt . t 2σ2 (141) log log (cid:114) (cid:19) σt αt σt αt For the KL-divergence between the reverse kernel and its Gaussian approximation, we have DKL(rt(y x)rt(y x)) = Eε log rt(x + + gt+t rt(x + + gt+t tε x) tε x) + o(t) , (142) where = (cid:18) From Lemma 1, we have log αtx 2σ2 log σt αt (cid:19) log qt(x) , and gt+t = (cid:114) 2σ2 log σt αt . DKL (rt(y x)rt(y x)) = (143) (144) (cid:18) σ2 t+t = t log σt+t αt+t (cid:18) (cid:19) Eε εT 2 log qt(x) x2 ε log qt+t(x) + o(t) (145) (cid:19) = o(t) . D.4 PROOF OF THM. 3 (146) tε, Theorem 3. [Detailed balance density estimator error] For = + + gt+t log σt where gt+t = αt estimator is unbiased w.r.t. the generated samples y, i.e. and ε is standard normal random variable, the following 2σ2 (cid:113) (cid:20) log Eε (cid:21) rt(y x) rt(y x) = o(t) , (85) and the variance is (cid:20) IDε log where 2 log qt+t(x) x2 (cid:21) rt(y x) rt(y x) = t2 g4 t+t 4 (cid:18) IDε εT 2 log qt(x) x2 (cid:19) ε + o(t2) , (86) is the Hessian matrix of the log-density."
        },
        {
            "title": "Preprint",
            "content": "Proof. From Lemma 1, we have g2 rt(y x) t+t 2 rt(y x) (cid:18) = Eεt Eε log (cid:18) log qt+t(x) εT 2 log qt(x) x2 (cid:19) ε + o(t) (147) = g2 t+t 2 = g2 t+t 2 = o(t) . (cid:20) εT 2 log qt(x) log qt+t(x) Eε Tr x2 (cid:20) 2 log qt(x) x2 log qt+t(x) Eε Tr (cid:21)(cid:19) (cid:18) (cid:21)(cid:19) ε + o(t) (148) + o(t) (149) (150) For the variance, we have IDε log rt(y x) rt(y x) = t2 g4 t+t 4 = t2 g4 t+t (cid:18) (cid:18) Eε Eε log qt+t(x) εT 2 log qt(x) (cid:19)2 ε + o(t2) (151) Eη (cid:20) ηT 2 log qt(x) x2 (cid:21) η εT 2 log qt(x) x2 (cid:19)2 ε + o(t2) εT 2 log qt(x) x2 where we used standard normal random variable η. = t2 g4 t+t 4 IDε (cid:18) (cid:19) ε + o(t2) , D.5 PROOF OF PROP. 11 (152) (153) tε, Proposition 11. [Recurrence relation for the log-density] For = + + gt+t 2σ2 where gt+t = from Eq. (87) can be expanded as follows and ε is standard normal random variable, the estimator log σt αt (cid:113) log kt(x y) + log rt(y x) = dt (cid:28) + + gt+t tε + t log αt+t g2 t+t 2 log qt+t(x)2+ (cid:29) log αt+tx, log qt+t(x) + o(t) Proof. Indeed, rt(y x) kt(x y) log = 1 2B2 t+t (cid:18) (cid:19)2 αt+t αt + log 2πS2 t+t (88) (154) (cid:18) )2 αt αt+t S2 t+t log qt+t(x) (cid:19) (155) 1 2(St+t 2 log 2πS2 αt αt+t t+t log (cid:28) αt+t αt + αt αt+t αt αt+t αt αt+t S2 t+t log qt+t(x)2 = log x, log qt+t(x) (cid:29) (156) (157) (158) Expanding the time around + t, we have (cid:28) = dt log αt+t + (1 + log rt(y x) kt(x y) (cid:18) σ2 t+t log αt+t)y x, log qt+t(x) (159) (cid:29) (cid:19) log σt+t αt+t log qt+t(x)2 + o(t) . (160)"
        },
        {
            "title": "Preprint",
            "content": "For = + + tgt+tε, gt+t = (cid:113) 2σ2 t+t log σt+t αt+t , we have log rt(y x) kt(x y) (cid:28) = dt + + gt+t log αt+t g2 t+t log qt+t(x)2+ (161) tε + t log αt+tx, log qt+t(x) + o(t) . (162) (cid:29)"
        },
        {
            "title": "E BROADER IMPACTS",
            "content": "In this paper, we present theoretical results and demonstrate use cases in generation tasks such as image generation and unconditional protein generation. Because of the theoretical nature of our contributions, this work carries little societal impact. SUPERDIFF can be used to generated protein structure from composition of existing protein diffusion models. Better protein generation methods can potentially lead to negative use in generating bio-hazardous molecules and proteins. We do not perceive this as great risk at the current stage of these models. ADDITIONAL RESULTS FOR CIFAR-10 Table A1: Image generation performance for CIFAR-10 with conditional models trained on two random partitions of the training data (labeled and B). We compare SUPERDIFF (OR) with the respective models (modelA and modelB) with the model that is trained on the full dataset (modelAB) and random choice between two models (modelA OR B). ODE inference SDE inference FID () IS () FLD () FID () IS () FLD () modelA modelB modelAB modelA OR SUPERDIFF (OR) SUPERDIFF =100 (OR) 4.75 4.78 5.30 4.75 4.74 4.63 8.98 8.97 9.04 8.94 9.00 8.98 6.95 0.12 6.86 0.15 6.82 0.09 6.86 0.15 6.98 0.10 6.81 0.19 4.66 4.36 2.83 4.41 4.46 4.23 9.35 9.45 9.44 9.40 9.40 9.42 6.39 0.13 6.20 0.14 6.26 0.11 6.3 0.18 6.22 0.15 6.27 0."
        },
        {
            "title": "G PROTEIN GENERATION",
            "content": "G.1 EXPERIMENTAL DETAILS In our setting, we consider two state-of-the-art diffusion models for protein generation: Proteus (Wang et al., 2024) and FrameDiff (Yim et al., 2023b), which were trained on protein structures from Protein Data Bank (PDB, (Berman et al., 2000)) to estimate the special Euclidean group (SE(3)) equivariant score over multiple diffusion steps. The models outputs predict the coordinates of monomeric protein backbone. We use pre-trained checkpoints from Proteus2 and FrameDiff3. During protein generation at inference, we separately combine scores for translations and rotations from both models using Algorithm 1. We investigate the use of different temperature (T ) settings to scale κ for controlling the densities. We also found that adding small bias (ω) towards Proteus log densities improved designability. G.2 METRICS FOR EVALUATING GENERATED PROTEINS Designability. We assess designability using the self-consistency evaluation from Trippe et al. (2023), where given generated backbone, we predict its scaffold using sequence prediction model (we use ProteinMPNN (Dauparas et al., 2022)) and re-fold the sequence using structure prediction model (we use ESMFold (Lin et al., 2022)). We then compare the re-folded protein to the original generated backbone by computing their template modeling score (scTM) and root-mean-square-distance (scRMSD). protein is considered to be designable if its scRMSD is < 2Å to the refolded structure. For each protein, we repeat this process 8 times and keep the sequence with the lowest scRMSD. 2https://github.com/Wangchentong/Proteus 3https://github.com/jasonkyuyim/se3_diffusion"
        },
        {
            "title": "Preprint",
            "content": "Figure A1: UMAP visualization of protein structures with (a) SUPERDIFF (AND) and (b) averaging of outputs. We report the fraction of designable proteins for each method in Table 3 as well as the scRMSD mean of the resulting designable proteins. Novelty. significant impetus for generative modelling in biology and chemistry is to propose compounds that have not been previously identified (i.e., different from the training data), but are also possible to make. We compute three novelty metrics: the fraction of designable proteins with scTM score < 0.5 (higher is better), the fraction of designable proteins with scTM score < 0.3, and the mean scTM score between generated proteins and the proteins from PDB that the original diffusion models were trained on, which represents the collective human knowledge of protein structures; lower score is indicates higher distance from the training set and is desirable since it shows generalization ability. Diversity. To measure how diverse the generated proteins are, we compute their mean pairwise scTM score (lower is better), and also report the fraction of unique clusters formed after clustering them with MaxCluster (Herbert & Sternberg, 2008) (higher is better). Finally, we report the fraction of proteins that contain β-sheet secondary structures, as it has been found that these structures are typically more rare to generate (Bose et al., 2024). G.3 PROTEIN DIVERSITY EXPLORATION To embed and cluster the generated protein backbones we use the Foldseek (Van Kempen et al., 2024) package to compute pairwise aligned TM-scores. We then use the UMAP (McInnes et al., 2018) package to compute 2D embedding. Where proteins are represented as points that are close to each other if they are structurally similar (by aligned TM-Score). We then clustered the proteins again with the Foldseek tool to find representative structures. Finally we used KMeans to explore the space and narrow down which protein structures belong where on the protein structure manifold. In Fig. A1, we show UMAP visualizations of proteins generated with SUPERDIFF (AND) and averaging of outputs (Liu et al., 2022a). G.4 PROTEIN NOVELTY EXPLORATION In Fig. A2, we visualize the proteins generated by SUPERDIFF (AND) that are furthest away from the set of known proteins (scTM score < 0.3)."
        },
        {
            "title": "H GENERATING IMAGE COMPOSITIONS WITH STABLE DIFFUSION",
            "content": "For our concept interpolation experiments, we use publicly-available pre-trained weights, models, and schedulers from Stable Diffusion v1-4 https://huggingface.co/CompVis/ stable-diffusion-v1-4. In Figs. A3A22, we show examples of image compositions generated by SUPERDIFF (AND), averaging of outputs, and joint prompting. Prompts are shown in each image caption. We show images generated by the first 6 seeds (uniform sampling), as well as our favourite images generated from 20 seeds. For joint prompting, we generate prompts from two concepts using the linking term \"that looks like\". For example, given the concepts \"a lemon\" and \"a sunflower\","
        },
        {
            "title": "Preprint",
            "content": "Figure A2: Proteins generated by SUPERDIFF (AND) with scTM score < 0.3. the resulting prompt would be \"a sunflower that looks like lemon\". We also generate images with the reversed prompt (\"a lemon that looks like sunflower\"), and keep the images generated by the prompt resulting in the higher mean score for each metric. The order of the concepts in each image caption reflects the ordering that obtained the higher TIFA score. In Fig. A23, we show examples of image compositions generated by SUPERDIFF (OR), joint prompting, and an upper bound of randomly selecting prompt of one concept. Again, we keep the prompt order that resulted in higher scores for the baseline."
        },
        {
            "title": "Preprint",
            "content": "Figure A3: Image compositions generated using SUPERDIFF (AND), averaging of outputs, or joint prompting for the concepts \"an airplane\" and \"an eagle\"."
        },
        {
            "title": "Preprint",
            "content": "Figure A4: Image compositions generated using SUPERDIFF (AND), averaging of outputs, or joint prompting for the concepts \"bottle cap\" and \"chess pawn\"."
        },
        {
            "title": "Preprint",
            "content": "Figure A5: Image compositions generated using SUPERDIFF (AND), averaging of outputs, or joint prompting for the concepts \"a bicycle wheel\" and \"a spider web\"."
        },
        {
            "title": "Preprint",
            "content": "Figure A6: Image compositions generated using SUPERDIFF (AND), averaging of outputs, or joint prompting for the concepts \"a chair\" and \"an avocado\"."
        },
        {
            "title": "Preprint",
            "content": "Figure A7: Image compositions generated using SUPERDIFF (AND), averaging of outputs, or joint prompting for the concepts \"a cinnamon roll\" and \"a snail\"."
        },
        {
            "title": "Preprint",
            "content": "Figure A8: Image compositions generated using SUPERDIFF (AND), averaging of outputs, or joint prompting for the concepts \"cookie\" and \"moon\"."
        },
        {
            "title": "Preprint",
            "content": "Figure A9: Image compositions generated using SUPERDIFF (AND), averaging of outputs, or joint prompting for the concepts \"a dog\" and \"a cat\"."
        },
        {
            "title": "Preprint",
            "content": "Figure A10: Image compositions generated using SUPERDIFF (AND), averaging of outputs, or joint prompting for the concepts \"a donut\" and \"a map\"."
        },
        {
            "title": "Preprint",
            "content": "Figure A11: Image compositions generated using SUPERDIFF (AND), averaging of outputs, or joint prompting for the concepts \"duck\" and \"otter\"."
        },
        {
            "title": "Preprint",
            "content": "Figure A12: Image compositions generated using SUPERDIFF (AND), averaging of outputs, or joint prompting for the concepts \"fireworks\" and \"dandelion\"."
        },
        {
            "title": "Preprint",
            "content": "Figure A13: Image compositions generated using SUPERDIFF (AND), averaging of outputs, or joint prompting for the concepts \"a flamingo\" and \"a candy cane\"."
        },
        {
            "title": "Preprint",
            "content": "Figure A14: Image compositions generated using SUPERDIFF (AND), averaging of outputs, or joint prompting for the concepts \"a helicopter\" and \"a dragonfly\"."
        },
        {
            "title": "Preprint",
            "content": "Figure A15: Image compositions generated using SUPERDIFF (AND), averaging of outputs, or joint prompting for the concepts \"pebbles on beach\" and \"a turtle\"."
        },
        {
            "title": "Preprint",
            "content": "Figure A16: Image compositions generated using SUPERDIFF (AND), averaging of outputs, or joint prompting for the concepts \"a pineapple\" and \"a beehive\"."
        },
        {
            "title": "Preprint",
            "content": "Figure A17: Image compositions generated using SUPERDIFF (AND), averaging of outputs, or joint prompting for the concepts \"a rocket\" and \"a cactus\"."
        },
        {
            "title": "Preprint",
            "content": "Figure A18: Image compositions generated using SUPERDIFF (AND), averaging of outputs, or joint prompting for the concepts \"a silhouette of dog\" and \"a mountain landscape\"."
        },
        {
            "title": "Preprint",
            "content": "Figure A19: Image compositions generated using SUPERDIFF (AND), averaging of outputs, or joint prompting for the concepts \"a sunflower\" and \"a lemon\"."
        },
        {
            "title": "Preprint",
            "content": "Figure A20: Image compositions generated using SUPERDIFF (AND), averaging of outputs, or joint prompting for the concepts \"teddy bear\" and \"panda\"."
        },
        {
            "title": "Preprint",
            "content": "Figure A21: Image compositions generated using SUPERDIFF (AND), averaging of outputs, or joint prompting for the concepts \"a waffle cone\" and \"a volcano\"."
        },
        {
            "title": "Preprint",
            "content": "Figure A22: Image compositions generated using SUPERDIFF (AND), averaging of outputs, or joint prompting for the concepts \"zebra\" and \"barcode\"."
        },
        {
            "title": "Preprint",
            "content": "Figure A23: Concept selections using SUPERDIFF (OR), joint prompting, and randomly selecting prompt of one concept. The top subfigure shows generated images for the concepts \"a candy cane\" or \"a flamingo\". The bottom subfigure shows generated images for the concepts \"a pineapple\" or \"a beehive\"."
        }
    ],
    "affiliations": [
        "Mila - Quebec AI Institute",
        "University of Oxford",
        "University of Toronto",
        "Université de Montréal",
        "Vector Institute"
    ]
}