{
    "paper_title": "MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs",
    "authors": [
        "Sixun Dong",
        "Juhua Hu",
        "Mian Zhang",
        "Ming Yin",
        "Yanjie Fu",
        "Qi Qian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) demonstrate impressive performance in understanding visual content with language instruction by converting visual input to vision tokens. However, redundancy in vision tokens results in the degenerated inference efficiency of VLMs. While many algorithms have been proposed to reduce the number of vision tokens, most of them apply only unimodal information (i.e., vision/text) for pruning and ignore the inherent multimodal property of vision-language tasks. Moreover, it lacks a generic criterion that can be applied to different modalities. To mitigate this limitation, in this work, we propose to leverage both vision and text tokens to select informative vision tokens by the criterion of coverage. We first formulate the subset selection problem as a maximum coverage problem. Afterward, a subset of vision tokens is optimized to cover the text tokens and the original set of vision tokens, simultaneously. Finally, a VLM agent can be adopted to further improve the quality of text tokens for guiding vision pruning. The proposed method MMTok is extensively evaluated on benchmark datasets with different VLMs. The comparison illustrates that vision and text information are complementary, and combining multimodal information can surpass the unimodal baseline with a clear margin. Moreover, under the maximum coverage criterion on the POPE dataset, our method achieves a 1.87x speedup while maintaining 98.7% of the original performance on LLaVA-NeXT-13B. Furthermore, with only four vision tokens, it still preserves 87.7% of the original performance on LLaVA-1.5-7B. These results highlight the effectiveness of coverage in token selection."
        },
        {
            "title": "Start",
            "content": "MMTOK: MULTIMODAL COVERAGE MAXIMIZA-"
        },
        {
            "title": "TION FOR EFFICIENT INFERENCE OF VLMS",
            "content": "Sixun Dong1 1Arizona State University Juhua Hu3 Mian Zhang4 Ming Yin5 Yanjie Fu1 2Zoom Communications Qi Qian2(cid:66) 3University of Washington 4University of Texas at Dallas 5Duke University sixundong@asu.edu,juhuah@uw.edu,yanjie.fu@asu.edu,qianq.mail@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Vision-Language Models (VLMs) demonstrate impressive performance in understanding visual content with language instruction by converting visual input to vision tokens. However, redundancy in vision tokens results in the degenerated inference efficiency of VLMs. While many algorithms have been proposed to reduce the number of vision tokens, most of them apply only unimodal information (i.e., vision/text) for pruning and ignore the inherent multimodal property of visionlanguage tasks. Moreover, it lacks generic criterion that can be applied to different modalities. To mitigate this limitation, in this work, we propose to leverage both vision and text tokens to select informative vision tokens by the criterion of coverage. We first formulate the subset selection problem as maximum coverage problem. Afterward, subset of vision tokens is optimized to cover the text tokens and the original set of vision tokens, simultaneously. Finally, VLM agent can be adopted to further improve the quality of text tokens for guiding vision pruning. The proposed method MMTok is extensively evaluated on benchmark datasets with different VLMs. The comparison illustrates that vision and text information are complementary, and combining multimodal information can surpass the unimodal baseline with clear margin. Moreover, under the maximum coverage criterion on the POPE dataset, our method achieves 1.87 speedup while maintaining 98.7% of the original performance on LLaVA-NeXT-13B. Furthermore, with only four vision tokens, it still preserves 87.7% of the original performance on LLaVA-1.5-7B. These results highlight the effectiveness of coverage in token selection. 5 2 0 2 5 2 ] . [ 1 4 6 2 8 1 . 8 0 5 2 : r Figure 1: MMTok achieves better performance across multiple benchmarks. Work done during internship at Zoom Communications. (cid:66) Corresponding author."
        },
        {
            "title": "INTRODUCTION",
            "content": "By converting the visual input to vision tokens, Vision-Language Models (VLMs) can leverage powerful pre-trained Large Language Models (LLMs) to understand visual content as text (Liu et al., 2024b; Li et al., 2024b; Team et al., 2023). Unlike discrete text tokens, where the information is highly compressed, current vision encoders extract vision tokens directly from the original input patches, which are redundant according to previous studies (Bolya et al., 2022; He et al., 2022) and their count can far exceed that of text tokens. For example, given Describe the image with less than 10 text tokens, 2, 880 vision tokens can be obtained from single image in LLaVA-NeXT (Liu et al., 2024a). Since LLMs are built on self-attention layers (Vaswani et al., 2017) that have quadratic computational cost to the total number of tokens, the large volume of vision tokens can significantly challenge the inference efficiency of VLMs. To accelerate inference, many works have been proposed to sample subset of vision tokens for inference with LLMs without compromising performance. While some work adopts an additional training process to enable vision token selection, in this work, we will focus on the training-free paradigm to reduce optimization efforts. Our experiments also confirm that the proposed training-free method can even outperform baselines with fine-tuning. Despite different architectures of VLMs (Team, 2024; Bai et al., 2025; Guo et al., 2025), the leading performance is from the one containing separated vision encoder to obtain vision tokens (Bai et al., 2025). In that architecture, both vision tokens and text tokens are available for token selection before applying LLMs. However, most of the existing work relies on unimodality for pruning while the multimodal information has not been explored sufficiently (Zhang et al., 2024; Yang et al., 2025; Alvar et al., 2025). For example, SparseVLM (Zhang et al., 2024) mainly considers text tokens from language instruction to guide the pruning of vision tokens, while VisionZip (Yang et al., 2025) heavily depends on the [CLS] vision token to select informative vision tokens. By investigating visionlanguage tasks, we find that given the same image, the answers can be different due to user-specific text queries, while the same text instruction can be applied for different images, i.e., caption tasks. Therefore, unimodal method is hard to capture sufficient information about target tasks, implying suboptimal performance for token selection. In order to leverage both vision and text information to obtain informative vision tokens, in this work, we propose multimodal strategy for efficient inference. First, we formulate the token selection problem as maximum coverage problem, which aims to cover the target tokens with subset of source tokens. While the source tokens are vision-only, the target ones can come from either text or vision, respectively. Therefore, the framework can explicitly combine the information from different modalities. Then, we optimize the coverage problem by maximizing submodular function defined on the similarity between target and source tokens. Although the original problem is NP-hard (Khuller et al., 1999), simple greedy algorithm can observe an approximate solution that is not worse than (1 1/e) of the optimal solution (Nemhauser et al., 1978). Finally, lightweight agent can be applied to enrich text information optionally. The main contributions of this work are summarized as follows. We introduce the coverage maximum problem for vision token selection. The problem can be formulated as maximizing submodular function, which has an efficient algorithm to obtain near-optimal solution with theoretical guarantee. We apply the coverage criterion to cover both the text tokens and the entire set of vision tokens with subset of selected vision tokens. The text-vision and vision-vision coverage explicitly help explore multimodal information for selection. Experiments are conducted on benchmark datasets with diverse VLMs. The superior performance of the proposed method demonstrates the effectiveness of the proposed coverage criterion for the subset selection of vision tokens. For example, the proposed MMTok can achieve overall best performance under different settings as illustrated in Figure 1 (b) and shows the potential to compress to an extremely small number of vision tokens as in Figure 1 (a)."
        },
        {
            "title": "2 RELATED WORK",
            "content": "VLMs, such as LLaVA (Liu et al., 2023), InstructBLIP (Dai et al., 2023), and Qwen (Bai et al., 2025), have become cornerstone for multimodal understanding by integrating large-scale vision encoders 2 Figure 2: Overview of MMTok framework. Our method optimizes two maximum coverage problems simultaneously to leverage text-vision and vision-vision similarity for vision token selections. Meanwhile, an optional lightweight agentic model can be applied to enhance the text semantics as denoted by the red-dotted line. (e.g., CLIP-ViT (Radford et al., 2021b)) with pre-trained language models. These models achieve strong performance by representing images as sequences of visual tokens, but their inference cost grows quadratically with token count, highlighting the need for more efficient processing. Many vision token selection methods have been proposed recently, but most of them only rely on unimodal information for pruning (Yang et al., 2025; Chen et al., 2024; Zhang et al., 2024; Alvar et al., 2025). For example, VisionZip (Yang et al., 2025) and FastV (Chen et al., 2024) prune tokens using pre-trained attention signals, either ranking by [CLS] token attention (VisionZip) or discarding low-attention vision tokens in deeper layers (FastV). Besides ranking, DivPrune (Alvar et al., 2025) uses diversity-based criterion but only has vision tokens to maximize the intra-set diversity. These methods solely rely on vision information and may miss query-related semantics (Jain & Wallace, 2019; Wiegreffe & Pinter, 2019). SparseVLM (Zhang et al., 2024) instead uses text-to-vision attention for scoring, yet ignores the information from the whole image. To mitigate the gap between existing unimodal algorithms and target multimodal tasks, we propose coverage-based criterion to leverage both vision and text information sufficiently to select vision tokens effectively."
        },
        {
            "title": "3 THE PROPOSED METHOD",
            "content": "To leverage the power of pre-trained models, many existing VLMs adopt pre-trained vision encoder to extract vision tokens from images and then concatenate them with text tokens as input for the pretrained LLMs. Although the simple architecture demonstrates promising performance, the inference efficiency can be challenging. Concretely, given an image, pre-defined number of vision tokens will be obtained as {v1, . . . , vn}. Even for small 336 336 image, is 576 with the ViT-L-336px from CLIP (Radford et al., 2021a), which is much larger than the text tokens from the text query (Liu et al., 2023). The large set of vision tokens will significantly slow down the inference of LLMs, which relies on the self-attention operations, and the complexity is quadratic to the total number of tokens. To accelerate the inference of VLMs, we propose to select an informative subset of vision tokens {vs}sS to reduce the number of input tokens for LLM in VLM, where = {1, . . . , n}, , and n. Figure 2 illustrates the framework of our method, and we will elaborate it as follows."
        },
        {
            "title": "3.1 VISION TOKEN SELECTION BY COVERAGE MAXIMIZATION",
            "content": "Unlike most of the existing work, we apply coverage as the main criterion for token selection. Given similarity matrix Rm,n defined between target tokens and source tokens, where denotes the number of target tokens and is the number of source tokens, subset will be sampled to maximize similarity as follows: 1 max Mi,S ; = arg max (S; ) = (S; ) (cid:88) (1) i=1 The objective is to maximize the similarity between the target and selected tokens, a.k.a. covering the target tokens by an appropriate subset of source tokens. To maximize the objective, we first find that Eq. 1 is popular submodular function Leskovec et al. (2007). Proposition 1. (Leskovec et al., 2007) For all subsets and B, (A {s}) (A) (B {s}) (B) Maximizing submodular functions in general is NP-hard (Khuller et al., 1999), but simple greedy algorithm can achieve good approximation. Proposition 2. then we have (Nemhauser et al., 1978) Let denote the subset obtained by the greedy algorithm, (S) (1 1/e) max A:A=S (A) We elaborate on how to apply the coverage function for token selections in the following subsections."
        },
        {
            "title": "3.1.1 MAXIMUM TEXT-VISION COVERAGE",
            "content": "First, we consider covering the semantics from text tokens with source vision tokens, which aims to find the vision tokens related to the text input (e.g., query). Let {t1, . . . , tm} denote the text tokens from the query. similarity matrix between text and vision tokens can be obtained as i,j = where tv Rmn and i, j, ti2 = vj2 = 1. To align the semantic similarity between text and vision, we adopt the vision tokens after the projection layer, which will be concatenated with text tokens as input for LLMs."
        },
        {
            "title": "M tv",
            "content": "i vj After obtaining the appropriate similarity matrix, subset of vision tokens can be gathered to maximize the similarity between all text tokens and selected vision tokens for coverage as = arg max (S; tv) According to Proposition 2, greedy algorithm can approximate the optimal solution. We summarize the simple algorithm in Alg. 1. Algorithm 1 Greedy Algorithm to Cover Text Input with Vision Tokens 1: Input: Similarity Matrix tv, 2: Initialize = 3: for = 1, , do for do 4: 5: 6: 7: 8: 9: end for 10: return end for Obtain si = arg maxs g(s) = si Compute g(s) = (S s; tv) , , vv Algorithm 2 MMToK: Greedy Algorithm for Multimodal Coverage 1: Input: Similarity Matrices tv 2: Initialize = 3: for = 1, , do for do 4: 5: 6: 7: 8: 9: end for 10: return end for Obtain si = arg maxs g(s) = si Compute g(s) = (S s; tv , vv ) The proposed Alg. 1 contains only simple operations (e.g., addition, matrix multiplication, etc.) and is efficient for implementation."
        },
        {
            "title": "3.1.2 MAXIMUM VISION-VISION COVERAGE",
            "content": "Although text-vision coverage can explore vision information according to text, it may be insufficient due to vague text, e.g., Please describe the image. Therefore, we propose to cover all vision information with limited number of vision tokens. Concretely, vision-vision similarity matrix can be generated as"
        },
        {
            "title": "M vv",
            "content": "i,j = where vv Rnn. Unlike tv that adopts vision tokens after the projection layer to align with text tokens, the feature before projection is more appropriate to capture similarity between vision tokens without mixing text information. We have to distinguish it from the token after projection, i.e., v. Then, we can apply the greedy algorithm to select subset of vision tokens to cover the main information implied by the whole set of vision tokens. Obviously, vision-vision coverage is complementary to text-vision coverage, which is also confirmed by our ablation study. The remaining challenge is to combine the two maximum coverage problems, which is described in the next subsection."
        },
        {
            "title": "3.1.3 MAXIMUM MULTIMODAL COVERAGE",
            "content": "The maximum coverage problem can be applied to the original text and vision tokens simultaneously, while tv and vv have different shapes and similarity measurements. Their values must be aligned before fusion. To calibrate the similarity between different modalities, the score for each row, i.e., that for target tokens, is first normalized by softmax operation as tv i,j = exp(M tv i,j/τt) j=1 exp(M tv i,j/τt) (cid:80)n ; vv i,j = exp(M vv i,j /τv) j=1 exp(M vv i,j /τv) (cid:80)n where the softmax operation normalizes each row to distribution over all vision tokens. τt and τv aim to further normalize the distribution shape for text-vision and vision-vision, respectively. After calibration, the final objective for multimodal coverage can be written as (S; tv , vv ) = (S; tv ) + αf (S; vv ) (2) where α is used to weigh the importance of vision-vision coverage. Incorporating text-vision coverage with vision-vision coverage, the function in Eqn. 2 is still submodular function as follows. Corollary 1. The sum of two submodular functions is submodular function. Proof. It comes from the addition property of inequalities directly. With Corollary 1, we can apply similar greedy algorithm to obtain the near-optimal solution for the multimodal scenario efficiently. The detailed algorithm is summarized in Alg. 2."
        },
        {
            "title": "3.2 AN AGENTIC APPROACH TO ENRICH TEXT COVERAGE",
            "content": "Compared with vision tokens, text input may carry limited semantic information as target for vision tokens to cover. To enrich the text, lightweight agent VLM can be leveraged to provide additional meaningful text tokens to guide the selection of the vision tokens. Let Agent denote tiny agent VLM, and let and be the input text query and image, respectively. The text output tokens can be obtained as {t 1, . . . , o} = Agent(T, I) With the preliminary response from the agent, the answer tokens will be concatenated with the input text tokens for coverage as {t1, . . . , tm, 1, . . . , o} 5 which leads to the similarity matrix tv R(m+o)n. Compared with the original text, the rough answer from the agent can provide more relevant guide for vision. Moreover, efficiency will not be compromised if the agent can return the result before the inference stage of the LLM in the original VLM. Although the agent can be helpful in general, the improvement depends on the type of queries and datasets. We provide further analysis in the experiments to demonstrate."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "To evaluate the performance of the proposed method, MMTok , we conduct experiments on diverse benchmark datasets and VLMs with different architectures. For fair comparison, we conduct experiments on the datasets adopted in VisionZip (Yang et al., 2025), which contains GQA (Hudson & Manning, 2019), MMBench (Liu et al., 2024c), MME (Fu et al., 2023), POPE (Li et al., 2023b), ScienceQA(IMG) (Lu et al., 2022), VQAv2-Test-Dev (Goyal et al., 2017), TextVQA (Singh et al., 2019), MMMU (Yue et al., 2024), and SeedBench (Li et al., 2023a). Meanwhile, five VLMs are applied for comparison, that is, LLaVA-1.5-7B (Liu et al., 2023), LLaVA-1.5-13B (Liu et al., 2023), LLaVA-NeXT-7B (Liu et al., 2024a), LLaVA-NeXT-13B (Liu et al., 2024a), and recent model Qwen-2.5-VL-7B (Bai et al., 2025). Finally, we compare our method with state-of-the-art vision token pruning algorithms, including FastV (Chen et al., 2024) (a vision-based method), SparseVLM (Zhang et al., 2024) (a language-based method), VisionZip (Yang et al., 2025) (a [CLS]-importance-based method), and DivPrune (Alvar et al., 2025) (a diversity-based method). We also include fine-tuningbased method, VisionZip , in the comparison. We obtain the result of DivPrune through its official code, and that for other baselines is directly from (Yang et al., 2025). Evaluation is implemented within the Lmms-eval framework (Li et al., 2024a) with the details elaborated as follows. Implementation Details The proposed method relies on an appropriate similarity for coverage optimization. Since different layers may demonstrate different similarity measurements (Liu et al., 2023), we have the vision tokens before the projection layer to compute vision-vision similarity, while those after the projection layer are for text-vision coverage. It is because the latter layer aligns better with text. We find that our method is not sensitive to hyperparameters, as shown in the ablation study. Therefore, we fix τt = 0.02, τv = 0.2, and α = 0.5 for all experiments if not specified. Finally, SmolVLM2-256M-Video-Instruct (Marafioti et al., 2025) is used as the lightweight agent to provide preliminary answers for our agentic version, MMTokAgent ."
        },
        {
            "title": "4.1 COMPARISON ON DIVERSE TASKS",
            "content": "Given nine datasets in experiments, we compare the performance of token selection algorithms on different VLMs, respectively. LLaVA-1.5-7B First, we compare our method with baselines using LLaVA-1.5-7B, which is popular benchmark for vision token selection. The model has fixed number of vision tokens for arbitrary visual inputs. As shown in Table 1, given the original 576 tokens, MMTok achieves the best performance (preserving on average 98.7/97.9/96.5% original performance of LLaVA-1.5-7B), when retaining only 192/128/64 tokens (reducing by 67/78/89% of tokens compared to 576), respectively. Specifically, our method outperforms DivPrune by 1.7% when using budget of 64 tokens. Although the gap decreases with more tokens as expected, MMTok still surpasses all baselines without fine-tuning by at least 0.7% with 192 tokens. In addition, compared to the fine-tuning method, the proposed method is 1.5% better than VisionZip with 64 tokens, which shows the potential of the training-free strategy. Finally, with an agent model, MMTokAgent achieves an overall better performance with 128/196 tokens while the improvement varies on different tasks. For example, the agent helps the performance on VQAv2 and MME, but may degenerate the results on MMB under 64 tokens. This is because some tasks are multi-choice Q&A (e.g., Whats in the image? A. Cat, B. Dog, C. Bird.), and thus agents response (e.g. A) may not be informative for token selection. Since VisionZip and DivPrune show much better performance than FastV and SparseVLM, we will include only them for comparison in the following experiments. LLaVA-1.5-13B Here, we report the averaged performance over all benchmark datasets and token budgets in Table 2, while detailed results can be found in Appendix Table 11. Although the model"
        },
        {
            "title": "GQA MMB MME",
            "content": "POPE SQA VQAV2 VQAText MMMU SEED Avg. LLaVA-1.5-7B 61.90 64.70 1862.00 85. 69.50 78.50 58.20 36.30 58.60 100% Total 576 Tokens"
        },
        {
            "title": "MMTokAgent",
            "content": "52.70 57.60 59.30 59.97 60.10 60.07 60.03 49.60 56.00 57.60 59.25 58.90 59.29 59. 46.10 52.70 55.10 57.78 57.00 58.29 58.51 61.20 62.50 63.00 62.54 63.40 1612.00 1721.00 1782.60 1762.23 1834.00 Retain 192 Tokens 67% 67.10 75.60 76.80 76.87 77. 64.80 83.60 85.30 87.00 84.90 67.30 69.10 68.90 68.91 68.20 63.40 1773.86 63.40 1789. 86.42 86.44 68.96 68.77 77.11 77. 56.10 60.00 62.00 62.03 62.60 1490.00 1696.00 1761.70 1718.22 1823.00 Retain 128 Tokens 78% 61.80 73.80 75.60 75.96 76.60 60.20 67.10 68.90 68.96 68.30 59.60 80.50 83.20 86.72 83.70 62. 1779.14 62.46 1780.79 86.25 86.28 69. 68.82 76.35 76.47 Retain 64 Tokens 89% 48.00 56.20 60.10 59.28 61.50 1256.00 1505.00 1690.00 1674.40 1756. 61.17 1715.33 60.91 1722.19 48.00 75.10 77.00 85.56 80.90 85. 85.85 51.10 62.20 69.00 68.17 68.80 68.86 68.42 55.00 68.20 72.40 74.11 74.20 75. 75.44 52.50 56.10 57.30 56.97 57.80 57.68 57.74 50.60 54.90 56.80 56.06 57.00 57. 57.05 47.80 51.80 55.50 54.69 56.00 56.01 56.01 34.30 33.80 36.60 35.44 36.20 36. 36.44 34.90 33.80 37.90 35.56 37.30 35.67 35.89 34.00 32.70 36.20 35.56 35.60 36. 36.11 57.10 55.80 56.40 58.71 57.10 59.21 59.17 55.90 53.40 54.90 56.98 55.80 58. 58.56 51.90 51.10 52.20 55.13 53.40 57.15 57.20 89.6% 95.5% 97.9% 98.0% 98.4% 98.7% 98.8% 84.4% 92.9% 96.8% 97.0% 97.7% 97.9% 98.0% 75.6% 86.9% 93.2% 94.8% 95.0% 96.5% 96.5% Table 1: Performance Comparison on LLaVA-1.5-7B. More details in Appendix Table 10. is larger, the observation is similar to the above 7B counterpart, where our method consistently outperforms the baselines with clear margin. LLaVA-NeXT 7B and 13B In addition to models that have fixed number of vision tokens, we further evaluate our method on LLaVA-NeXT (Liu et al., 2024a), which dynamically samples up to five images and processes them individually, resulting in up to 2880 vision tokens. To align the comparison with real applications, we keep the dynamic settings as VisionZip (Yang et al., 2025) and have token selection performed in fixed ratio. For example, with maximum budget of 160 tokens, we retain 32 tokens per image in up to five images (32 5 = 160). The retained number of tokens becomes 128 if only four images are sampled by the VLMs according to the ratio of 160/2, 880. The same setting is used for all baselines as fair comparison. As shown in Table 2, our method retains more than 95% of the original performance using only 5.5% of the tokens with budget of 160 tokens, indicating substantial redundancy in vision tokens and the effectiveness of the proposed strategy. Detailed results can be found in Appendix Tables 12 and 13. Method LLaVA-1.5-7B (2023) LLaVA-1.5-13B (2023) LLaVA-NeXT-7B (2024a) LLaVA-NeXT-13B (2024a) 576 tokens 576 tokens Upper(Up.) 2880 tokens Upper(Up.) 2880 tokens Compress Ratio 67% 78% 89% 67% 78% 89% 78% 89% 94% 78% 89% 94% Remain Token 192 64 192 128 64 Up. 640 Up. 320 Up. 160 Up. 640 Up. 320 Up. 160 VisionZip DivPrune VisionZip 97.9% 96.8% 93.2% 97.9% 97.0% 93.7% 98.0% 97.0% 94.8% 98.1% 96.9% 95.4% 98.4% 97.7% 95.0% 98.7% 97.4% 94.8% 97.5% 97.1% 98.9% 94.5% 95.1% 97.6% 90.4% 92.4% 95.0% 97.7% 97.1% 98.8% 94.7% 94.6% 97.8% 91.4% 92.1% 94.6% MMTok 98.7% 97.9% 96.5% 98.6% 97.5% 96.3% 98.7% 97.2% 95.1% 98.3% 96.4% 95.1% MMTokAgent 98.8% 98.0% 96.5% 98.4% 97.4% 96.2% 98.8% 97.4% 95.8% 98.3% 96.6% 95.3% Table 2: Comparison on LLaVA-1.5 and LLaVA-NeXT. Details are in Appendix Tables 10 to 13. Qwen-2.5-VL-7B Finally, we compare different algorithms on more advanced VLM, that is, Qwen-2.5-VL-7B. Unlike previous work, it adopts dynamic resolution and token-merging layer."
        },
        {
            "title": "Method",
            "content": "GQA MMB MME POPE VQAText Acc. P+C Acc. Acc. F1 SQA OCRBench Acc. Acc. Avg. Dynamic Resolution (MinPix = 256 28 28, MaxPix = 2048 28 28), Upper Bound (100%) Avg. Tokens Qwen-2.5-VL-7B 83.80 60.48 83.25 86.16 77.72 76. 2327 359.6 976.5 652.8 867.6 323. 276.9 358.5 100% Fixed Resolution (MinPix = MaxPix = 2048 28 28), Upper Bound (100%) Qwen-2.5-VL-7B Retain 20% VisionZip DivPrune MMTok Retain 10% VisionZip DivPrune MMTok Retain 5% VisionZip DivPrune MMTok 58. 71.7 56.80 56.70 58.09 35.9 52.47 53.43 55.09 17.9 46.28 49.01 50.66 83.59 55.4 80.33 76.98 79.30 27.7 75.60 72.85 74.74 13.8 67.53 65.89 65.89 2339 173.5 2174 2163 2217 86.8 2003 1957 2051 43.4 1677 1739 1796 86. 71.9 83.38 80.59 82.38 36.0 78.90 74.99 78.75 18.0 66.38 68.45 71.35 76.64 195.3 70.43 65.86 70.49 97.7 63.78 59.59 63.90 48.8 54.49 52.02 55.95 73.67 64.6 76.30 70.55 71.94 32.3 74.07 69.21 71.69 16.2 72.63 67.67 70.10 76. 130.6 59.50 48.10 59.60 65.3 36.90 37.30 43.60 32.6 19.70 24.90 30.70 99.3% 80% 94.2% 91.5% 94.6% 90% 87.5% 84.7% 88.5% 95% 75.4% 76.3% 79.0% Qwen-2.5-VL-7B 31.84 20."
        },
        {
            "title": "0 Token ↓ 100%\n0.00∗",
            "content": "935 38.93 65.25 1.80 33.8% Table 3: Comparison on Qwen-2.5-VL-7B. Avg. are computed over 5 datasets. *When no visual tokens are provided, Qwen-2.5-VL outputs \"No\" for all questions, leading to 0% F1. More detailed results are in Appendix Table 14. Those strategies help reduce the total number of vision tokens while demonstrating better performance. For example, on POPE the average number of input tokens is only 359.6 in Qwen, significantly less than 2880 tokens in LLaVA-NeXT. Therefore, it is more challenging to apply the token selection algorithm in this stronger model. Following experiments for LLaVA-NeXT, we conduct the evaluation under dynamic resolution for all methods. Due to distinct image pre-processing strategies in Qwen, we include 7 image datasets in this comparison. Since ScienceQA(SQA) is low-IC dataset that will be discussed in Section 4.2 and all baselines perform poorly on OCRBench, the average performance is computed across the remaining 5 datasets. For MMTok , we reduce τt to 0.01 for all datasets while other parameters remained. First, we compare the dynamic resolution to the fixed number of tokens in Qwen as shown in the first two rows of Table 3. Although the model can use fixed number of about 2,048 vision tokens for different tasks, the performance is worse than that of the dynamic strategy, which has much fewer tokens. It shows that vision tokens are quite redundant for VLM tasks, and the sophisticated strategies in Qwen already compress the number to hundreds, providing even better performance. Based on the challenging dynamic setting, we further investigate whether token selection is still valuable. From Table 3, we can find that our MMTok can preserve nearly 95% of the original performance while further reducing the number of vision tokens to 20%. This observation demonstrates that even for models with token compression, the remaining tokens can still be redundant. The proposed method MMTok can effectively explore the most informative tokens and further reduce the number of vision tokens from hundreds to tens. Furthermore, our method is better than VisionZip with different budgets, which confirms the efficacy of our proposed multimodal coverage strategy. Finally, we can observe that even without any vision tokens, Qwens performance on SQA is still close to its version with all tokens. This reminds us to investigate the contribution of vision to vision-language tasks in the next subsection, which can help to better evaluate the performance of token selection methods."
        },
        {
            "title": "4.2 COMPARISON ON HIGH IC TASKS WITH LIMITED VISION TOKENS",
            "content": "Although multimodal tasks rely on images for answers, the contribution of vision varies. Table 4 summarizes the performance with/without vision tokens on different datasets. It is interesting to observe that even without any vision tokens, LLaVA-1.5 still preserves 92% of the original performance on MMMU and 82% on ScienceQA. Those tasks may fail to help adequately assess the efficacy of vision token selection. To mitigate the issue, we introduce Image Contribution (IC) 8 to quantify the relative performance gain from all vision tokens, IC = (PerfAll Perf0)/Perf0 and summarize IC values in Table 4. According to the table, we can identify 5 and 6 high-IC datasets for LLaVA and LLaVA-NeXT, respectively. Then, we compare different algorithms on those datasets in Table 5. To evaluate the performance with an extremely aggressive compression ratio, we extend the experiments from 64 tokens to 2 tokens. The comparison shows that our method can substantially preserve the informative vision tokens for VL tasks. Moreover, we illustrate the performance ratio compared to the original result in Figure 1. On POPE, our method maintains about 80% original performance with only 2 vision tokens, showing the importance of appropriate vision tokens. More results can be found in Appendix Tables 15 and 16. Dataset MMB POPE MME SEED-I GQA LLaVA-1.5-7B LLaVA-NeXT-7B All / Zero IC All / Zero IC 64.7/19.33 85.9/44.64 1862/970.89 66.14/37.03 61.9/37.65 2.347 0.924 0.918 0.786 0.644 67.9/17.87 86.4/25.84 1842/867 70.2/37.43 64.2/38.23 2.801 2.344 1.125 0.875 0.679 TextVQA 58.2/41.66 0.397 61.3/37.77 0.623 SQA MMMU 69.5/56.92 36.3/33. 0.221 0.089 70.2/63.91 35.1/31.56 0.098 0.112 Model/Method Different Token Budgets 32 16 8 4 2 LLaVA-1.5-7B (MMB, POPE, MME, SEED, GQA) VisionZip DivPrune MMTok (Ours) 90.0% 83.5% 69.7% 48.9% 43.8% 43.0% 93.1% 89.6% 81.4% 68.4% 54.3% 50.1% 94.7% 91.0% 86.4% 79.8% 71.4% 62.1% LLaVA-NeXT-7B (MMB, POPE, MME, SEED, GQA, TextVQA) VisionZip DivPrune MMTok (Ours) 94.5% 91.7% 82.7% 61.2% 51.5% 50.8% 96.2% 94.1% 91.1% 85.4% 77.6% 67.3% 97.2% 96.8% 94.5% 91.3% 86.4% 79.9% Table 4: Demonstration of Image Contribution (IC). Table 5: Comparison on high-IC tasks with different token budgets."
        },
        {
            "title": "4.3 ABLATION STUDY",
            "content": "We conduct comprehensive ablation studies to demonstrate each component in MMTok . All experiments are performed on LLaVA-1.5-7B with 64 tokens unless otherwise specified."
        },
        {
            "title": "Multimodal\nCoverage",
            "content": "GQA MMB MME P+C Acc. Acc. POPE F1 SQA Acc. VQAText MMMU SEED Acc. Acc. Acc. Avg. Total 576 Tokens (100%) LLaVA-1.5-7B 61.9 64.7 1862 85.9 69. 58.2 36.3 58.6 100% T-V (M tv) V-V (M vv) Softmax T-V (M tv ) Softmax V-V (M vv ) + vv MMTok (M tv 56.82 58.14 56.66 57.97 Retain 64 Tokens 88.9% 59.62 59.88 58.85 60.31 1632.47 1662.34 1674.11 1684.33 83.56 83.43 83.69 84.31 67.87 68.07 67.82 68. ) 58.29 61.17 1715.33 85.77 68. 51.97 53.93 52.01 55.90 56.01 35.33 35.33 35.33 35.89 36.11 56.36 56.90 56.37 56.88 93.7% 94.7% 93.8% 95.7% 57.15 96.6% Table 6: Ablation on multimodal coverage in MMTok. The best performance with token selection is highlighted in bold and the second-best is underlined. Unimodal Coverage v.s. Multimodal Coverage The proposed method contains both text-vision coverage (T-V) and vision-vision coverage (V-V). We evaluate each component separately in Table 6. Compared with the original similarity matrix, the softmax variant can obtain similar or even better performance, which shows that calibration on similarity matrices will not hurt the performance. Then, combining coverage optimization on different modalities shows an improvement of more than 5% over unimodal coverage, which demonstrates the complementarity between T-V and V-V coverages for diverse tasks. More ablation experiments can be found in Appendix. Inference Efficiency All of the above experiments show that our method can substantially reduce the number of vision tokens. Here, we examine efficiency in real scenarios. To mimic real applications, we report the total running time on different datasets in Table 7. 9 Model Upper Token Total Infer T(s) POPE Infer T(s) GPU Util. Memory (+25.42 GB) POPE SEED TextVQA MME MMB GQA Acc. Acc. Acc. Acc. P+C F1 Avg. (%) H100 Single GPU Performance, Upper 2880 Tokens LLaVA-NeXT-13B VisionZip DivPrune MMTok 2880 Upper 160 Upper Upper 160 15204 7551 8186 7768 1705 866 1060 913 86.7% 52.4% 50.9% 58.0% 4.59 1.92 1.23 1.78 86.22 76.32 82.16 85.11 71.89 61.18 63. 65.45 64.33 58.33 54.65 55.91 1900.86 1738.24 1699.83 69.16 64.78 64.78 65.38 57.77 59. 100.0 89.6 90.5 1811.35 65.89 61.94 93.7 Table 7: Comparison of Inference Efficiency. All results are reproduced under the same hardware and evaluation settings. The initial memory usage for loading the model is 25.42GB. First, we profile the computational cost on POPE. Obviously, all token selection methods help reduce the utility percentage of the GPU by about 30%, which shows that pruning is helpful for inference. Then, with fixed memory cost of 25.42GB for model loading, these methods can also help reduce the usage of running-time memory by more than 58.2% compared to the baseline. This reduction in computation and memory helps significantly improve the inference time on POPE, where both VisionZip and our method can reduce the running time by about 50%. DivPrune runs little bit slower due to different strategy for handling multiple crops in its original code. Although our method introduces two subproblems, that is, T-V and V-V to optimize, the running time is almost the same as the fast unimodal method, i.e., VisionZip, which confirms the efficiency of MMTok . The running time accumulated over 6 tasks demonstrates similar phenomenon, where the performance of MMTok is better than VisionZip by 4.1%. This further demonstrates the efficacy and efficiency of our proposal."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we propose multimodal coverage framework, MMTok, to guide vision token selection to accelerate the inference of VLMs in training-free manner. Extensive experiments on benchmark datasets and representative VLMs demonstrate that our method outperforms the unimodal baselines without compromising efficiency. While our current strategy is only applied to the input tokens for the LLM in VLM, the proposal is not limited to this scenario, and exploring informative tokens during inference in the LLM stage can be our future work."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We thank Yebowen Hu and Kaiqiang Song for their valuable discussions. Dr. Yanjie Fu is supported by the National Science Foundation (NSF) via the grant numbers: 2426340, 2416727, 2421864, 2421865, 2421803, and National academy of engineering Grainger Foundation Frontiers of Engineering Grants."
        },
        {
            "title": "REFERENCES",
            "content": "Saeed Ranjbar Alvar, Gursimran Singh, Mohammad Akbari, and Yong Zhang. Divprune: Diversitybased visual token pruning for large multimodal models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 93929401, 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large visionlanguage models. In European Conference on Computer Vision, pp. 1935. Springer, 2024. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in neural information processing systems, 36:4925049267, 2023. 10 Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. MME: comprehensive evaluation benchmark for multimodal large language models. arXiv:2306.13394, 2023. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 69046913, 2017. Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked In Proceedings of the IEEE/CVF Conference on autoencoders are scalable vision learners. Computer Vision and Pattern Recognition, pp. 1600016009, 2022. Drew Hudson and Christopher Manning. GQA: new dataset for real-world visual reasoning and compositional question answering. Conference on Computer Vision and Pattern Recognition (CVPR), 2019. Sarthak Jain and Byron Wallace. Attention is not explanation. arXiv preprint arXiv:1902.10186, 2019. Samir Khuller, Anna Moss, and Joseph Naor. The budgeted maximum coverage problem. Inf. Process. Lett., 70(1):3945, 1999. Jure Leskovec, Andreas Krause, Carlos Guestrin, Christos Faloutsos, Jeanne M. VanBriesen, and Natalie S. Glance. Cost-effective outbreak detection in networks. In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Jose, California, USA, August 12-15, 2007, pp. 420429. ACM, 2007. Bo Li, Peiyuan Zhang, Kaichen Zhang, Fanyi Pu, Xinrun Du, Yuhao Dong, Haotian Liu, Yuanhan Zhang, Ge Zhang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Accelerating the development of large multimodal models. https://github.com/EvolvingLMMs-Lab/lmms-eval, March 2024a. Version v0.1.0, Zenodo. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023a. Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv:2403.18814, 2024b. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv:2305.10355, 2023b. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv:2310.03744, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024a. URL https: //llava-vl.github.io/blog/2024-01-30-llava-next/. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems, 2024b. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pp. 216233. Springer, 2024c. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. 11 Andres Marafioti, Orr Zohar, Miquel Farre, Merve Noyan, Elie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, Vaibhav Srivastav, Joshua Lochner, Hugo Larcher, Mathieu Morlon, Lewis Tunstall, Leandro von Werra, and Thomas Wolf. Smolvlm: Redefining small and efficient multimodal models. arXiv preprint arXiv:2504.05299, 2025. George Nemhauser, Laurence Wolsey, and Marshall Fisher. An analysis of approximations for maximizing submodular set functionsi. Mathematical programming, 14(1):265294, 1978. Qi Qian, Yuanhong Xu, and Juhua Hu. Intra-modal proxy learning for zero-shot visual categorization with CLIP. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 87488763. PMLR, 2021a. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021b. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 83178326, 2019. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv:2312.11805, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 59986008, 2017. Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. arXiv preprint arXiv:1908.04626, 2019. Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. Visionzip: Longer is better but not necessary in vision language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1979219802, 2025. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, et al. Sparsevlm: Visual token sparsification for efficient vision-language model inference. arXiv preprint arXiv:2410.04417, 2024."
        },
        {
            "title": "A EXPERIMENTS",
            "content": "A.1 ADAPTIVE TEMPERATURE τ To further improve the calibration between tv , an adaptive visual temperature can be applied for each example. Concretely, when fixing τt, the maximal similarity between the target text tokens and the whole set of vision tokens can be obtained as (N ; tv ), letting = . The desired temperature τv should lead to similar magnitude for the vision-vision similarity. The optimization problem can be cast as and vv (N ; tv ) (N ; vv ) min τ For the default , it is monotone to τ the diagonal elements in vv the issue, the k-th largest value is applied to search for the temperature as , which can be solved efficiently by bisection search. However, can mislead the optimization due to their fixed value of 1. To mitigate (N ; tv ) fk(N ; vv ); fk(N ; vv ) = min τ 1 (cid:88) i= vv i,: max Moreover, fk is not guaranteed to be monotone function to τ , and we can search the value in (τt, τv] as suggested in (Qian et al., 2023), where it shows that the temperature between vision-vision should be higher than that between text-vision due to the modality gap. We perform the evaluation on high IC tasks in Table 8. As discussed above, the second largest value is adopted for searching the temperature in the set of {0.05, 0.1, 0.15, 0.2}. While the variant with adaptive temperature, i.e., MMTokAdapt, shows slightly better performance with budget of 16 tokens, the results over different tasks are almost the same, demonstrating that our method is insensitive to hyperparameters."
        },
        {
            "title": "Method",
            "content": "GQA MMB POPE MME P+C Acc. Acc. F1 SEED-I Acc. Avg. Upper Bound: LLaVA-1.5 7B (576 Tokens) LLaVA-1.5 7B 61.9 64.7 85.9 1862 58.6 100% Retain 16 Tokens 97.2%"
        },
        {
            "title": "MMTok\nMMTokAdapt",
            "content": "53.31 53.31 54.30 54.30 79.79 79.83 1550.65 1565.10 56.67 56.66 88.6% 88.7% Table 8: Fixed v.s. Adaptive Temperature. Evaluation on LLaVA-1.5 7B across three token budgets. Adaptive temperature τ {0.05, 0.1, 0.15, 0.2}. A.2 POOLING STRATEGY FOR TEXT Given an LLM, each word can be tokenized into multiple tokens. To recover the semantic information of words, we may aggregate tokens from the same word. In this experiment, we explore different pooling strategies when computing T-V similarity. Concretely, we consider the pooling process either before or after computing the similarity matrix, where Max-pooling selects the token with the maximum feature value or similarity, Mean-pooling averages similarity over all tokens, and First-pooling simply retains the first token of word. As shown in Table 9, there is no pooling strategy that consistently yields the best performance across all eight datasets. Therefore, our method does not apply word pooling for simplicity."
        },
        {
            "title": "B COMPLETE EMPIRICAL RESULTS",
            "content": "This section shows per-dataset results for all models and token budgets, including LLaVA-1.5 (7B/13B) (Tables 10 and 11 ), LLaVA-NeXT (7B/13B) (Tables 12 and 13), and Qwen-2.5-VL (7B Table 14). We report both raw scores and percentage retention relative to the full-token setting. We also report results with an extremely low number of tokens on LLaVA-1.5-7B and LLaVA-NeXT-7B (Tables 15 and 16)."
        },
        {
            "title": "Position",
            "content": "GQA MMB MME P+C Acc. Acc. POPE F1 SQA Acc. VQAText MMMU SEED Acc. Acc. Acc. Avg."
        },
        {
            "title": "None",
            "content": "- 58.29 61.17 1715 85.77 68. 56.01 36.11 57.15 100.0% MMTok on LLaVA-1.5-7B with 64 Tokens (Baseline)"
        },
        {
            "title": "Post\nPost\nPost",
            "content": "58.01 58.26 58.39 58.20 58.36 58.39 Pre-Pooling (Before Similarity Calculation) 55.73 61.00 55.82 61.17 61.34 55.76 68.62 68.47 68.22 85.75 85.64 85.67 1703 1704 Post-Pooling (After Similarity Calculation) 55.77 55.68 55.76 68.37 68.47 68.22 85.67 85.61 85.67 1690 1711 1709 61.00 61.00 61.34 36.00 35.89 36. 36.22 36.22 36.11 57.13 56.94 56.90 57.16 57.04 56.90 99.3% 99.1% 99.6% 99.2% 99.4% 99.6% Table 9: Word token pooling strategies for token selection on LLaVA-1.5-7B. Pre-pooling aggregates subword tokens before similarity computation, while post-pooling applies pooling afterward. We evaluate three methods: Mean (average pooling), Max (maximum pooling), and First (first subword). The baseline applies no pooling. The best is in bold and the second-best is underlined."
        },
        {
            "title": "Method",
            "content": "GQA MMB MME P+C Acc. Acc. POPE F1 SQA Acc. Acc. VQAV2 VQAText MMMU SEED Acc. Acc. Acc. Avg. LLaVA-1.5 Vanilla 7B 64.7 61.9 100% 100% Upper Bound, 576 Tokens (100%) 69.5 100% 78.5 100% 85.9 100% 1862 100% FastV (2024) 52.7 1612 61.2 85.1% 94.6% 86.6% Retain 192 Tokens 66.7% 64.8 67.1 67.3 75.4% 96.8% 85.5% SparseVLM (2024) 1721 62.5 57.6 93.1% 96.6% 92.4% 75.6 69.1 83.6 97.3% 99.4% 96.3% VisionZip (2025) DivPrune (2025) VisionZip (2025) MMTok (Ours) 59.3 1782.6 63.0 95.8% 97.4% 95.7% 85.3 76.8 68.9 99.3% 99.1% 97.8% 76.87 59.97 96.9% 96.7% 94.6% 101.3% 99.2% 97.9% 1762.23 68.91 62.54 87. 60.1 1834 63.4 97.1% 98.0% 98.5% 84.9 77.4 68.2 98.8% 98.1% 98.6% 60.07 77.11 97.0% 98.0% 95.3% 100.6% 99.2% 98.2% 1773.86 68.96 86. 63.40 FastV (2024) 49.6 1490 56.1 80.1% 86.7% 80.0% Retain 128 Tokens 77.8% 59.6 61.8 60.2 69.4% 86.6% 78.7% SparseVLM (2024) 56.0 1696 60.0 90.5% 92.7% 91.1% 80.5 73.8 67.1 93.7% 96.5% 94.0% VisionZip (2025) DivPrune (2025) VisionZip (2025) MMTok (Ours) 57.6 1761.7 62.0 93.1% 95.8% 94.6% 83.2 75.6 68.9 96.9% 99.1% 96.3% 59.25 75.96 95.7% 95.9% 92.3% 101.0% 99.2% 96.8% 1718.22 68.96 86.72 62. 58.9 1823 62.6 95.2% 96.8% 97.9% 83.7 76.6 68.3 97.4% 98.3% 97.6% 59.29 76.35 95.8% 96.3% 95.5% 100.4% 99.3% 97.3% 1779.14 86.25 62. 69.01 FastV (2024) 46.1 1256 48.0 74.5% 74.2% 67.5% Retain 64 Tokens 88.9% 48.0 55.0 51.1 55.9% 73.5% 70.1% SparseVLM (2024) 52.7 1505 56.2 85.1% 86.9% 80.8% 75.1 68.2 62.2 87.4% 89.4% 86.9% VisionZip (2025) DivPrune (2025) VisionZip (2025) MMTok (Ours) 55.1 1690 60.1 89.0% 92.9% 90.8% 77.0 72.4 69.0 89.6% 99.3% 92.2% 57.78 1674.4 59.28 93.3% 91.6% 89.9% 85.56 74.11 68.17 99.6% 98.1% 94.4% 1756 61.5 57.0 92.1% 95.1% 94.3% 74.2 68.8 80.9 94.2% 99.0% 94.5% 58.29 1715.33 61.17 94.2% 94.5% 92.1% 85.77 75.20 68.86 99.9% 99.1% 95.8% 58.2 100% 52.5 90.2% 56.1 96.4% 57.3 98.5% 56.97 97.9% 57.8 99.3% 57.68 99.1% 50.6 86.9% 54.9 94.3% 56.8 97.6% 56.06 96.3% 57.0 97.9% 57.03 98.0% 47.8 82.1% 51.8 89.0% 55.5 95.4% 54.69 94.0% 56.0 96.2% 56.01 96.3% 36.3 100% 58.6 100% 100% 34.3 94.5% 33.8 93.1% 36.6 100.8% 35.44 97.6% 36.2 99.7% 57.1 97.4% 55.8 95.2% 56.4 96.2% 58.71 100.2% 57.1 97.4% 89.6% 95.5% 97.9% 98.0% 98.4% 36.33 59.21 100.1% 101.0% 98.7% 34.9 96.1% 33.8 93.1% 37.9 104.4% 35.56 98.0% 37.3 102.8% 35.67 98.3% 34.0 93.7% 32.7 90.1% 36.2 99.7% 35.56 98.0% 35.6 98.1% 36.11 99.5% 55.9 95.4% 53.4 91.1% 54.9 93.7% 56.98 97.3% 55.8 95.2% 84.4% 92.9% 96.8% 97.0% 97.7% 58.59 100.0% 97.9% 51.9 88.6% 51.1 87.2% 52.2 89.1% 55.13 94.1% 53.4 91.1% 57.15 97.5% 75.6% 86.9% 93.2% 94.8% 95.0% 96.5% Table 10: Performance Comparison on LLaVA-1.5-7B. The vanilla number of visual tokens is 576. The first line of each method shows the raw benchmark accuracy, and the second line is the proportion relative to the upper limit. The last column is the average value."
        },
        {
            "title": "Method",
            "content": "GQA MMB MME P+C Acc. Acc. POPE F1 SQA Acc. VQAV2 VQAText MMMU SEED-I Acc. Acc. Acc. Acc. SEED Acc. Avg. LLaVA-1.5 Vanilla 13B 63.2 100% 67.7 100% 1818 100% Upper Bound, 576 Tokens (100%) 85.9 100% 80.0 100% 72.8 100% 61.3 100% VisionZip (2025) DivPrune (2025) 59.1 66.9 93.5% 98.8% 1754 96.5% 85.1 78.1 73.5 99.1% 101.0% 97.6% Retain 192 Tokens 66.7% 59.42 66.58 94.0% 98.3% 1781.50 77.98 98.0% 101.0% 100.1% 97.5% 72.88 86.76 VisionZip (2025) 61.6 67.1 97.5% 99.1% 1790 98.5% 84.5 98.4% 72.7 99.9% 78.6 98.3% MMTok (Ours) VisionZip (2025) DivPrune (2025) 59.67 78.30 94.4% 100.0% 98.1% 100.3% 100.4% 97.9% 1784.16 86.15 73.08 67.70 57.9 66.7 91.6% 98.5% 1743 95.9% 85.2 76.8 74.0 99.2% 101.6% 96.0% Retain 128 Tokens 77.8% 58.89 66.07 93.2% 97.6% 1748.56 77.10 96.2% 100.7% 100.0% 96.4% 72.83 86. VisionZip (2025) 60.1 67.6 95.1% 99.9% 1736 95.5% 83.8 77.6 73.0 97.6% 100.3% 97.0% MMTok (Ours) 58.98 67.18 93.3% 99.2% 1756.20 77.57 96.6% 100.4% 100.9% 97.0% 86.22 73.43 56.2 64.9 88.9% 95.9% 1676 92.2% 76.0 73.7 74.4 88.5% 102.2% 92.1% Retain 64 Tokens 88.9% 57.66 64.60 91.2% 95.4% 1777.93 97.8% VisionZip (2025) 58.1 65.6 91.9% 96.9% 1671 91.9% MMTok (Ours) 58.42 65.72 92.4% 97.1% 1763.39 97.0% 84.80 98.7% 81.6 95.0% 84.39 98.2% 71.34 98.0% 72.3 99.3% 72.53 99.6% 75.20 94.0% 75.2 94.0% 76.55 95.7% VisionZip (2025) DivPrune (2025) 59.5 97.1% 58.46 95.4% 59.9 97.7% 59.64 97.3% 58.7 95.8% 58.17 94.9% 59.2 96.6% 59.22 96.6% 57.4 93.6% 57.11 93.2% 58.5 95.4% 58.40 95.3% 36.4 100% 66.9 100% 61.6 100% 100% 36.4 100% 36.56 100.4% 36.4 100% 36.78 101.0% 36.1 99.2% 35.56 97.7% 35.4 97.3% 35.44 97.4% 36.4 100% 35.22 96.8% 35.3 97.0% 35.22 96.8% 65.2 97.5% 65.72 98.2% 66.1 98.8% 65.49 97.9% 63.8 95.4% 64.22 96.0% 64.9 97.0% 64.26 96.1% 60.4 90.3% 62.44 93.3% 61.4 91.8% 63.39 94.8% 61.20 99.4% 60.83 98.8% 61.17 99.3% 59.74 97.0% 59.49 96.6% 60.11 97.6% 57.13 92.7% 57.70 93.7% 59.51 96.6% 97.9% 98.1% 98.7% 98.6% 97.0% 96.9% 97.4% 97.5% 93.7% 95.4% 94.8% 96.3% Table 11: Performance Comparison on LLaVA-1.5-13B. The vanilla number of visual tokens is 576. The first line of each method shows the raw benchmark accuracy, and the second line is the proportion relative to the upper limit. SEED-I represents SEED-IMG, SEED represents SEED-ALL. Following (Yang et al., 2025), Avg. is based on SEED-I instead of SEED. 16 Method GQA MMB MME P+C Acc. Acc. POPE F1 SQA Acc. VQAV2 VQAText MMMU SEED-I Acc. Acc. Acc. Acc. Avg. Images Avg. Tokens (n 576) 4.90 2822.4 4.12 2373. 4.53 2609.28 4.90 2822.40 3.85 2217.60 4.98 2868.48 4.98 2868.48 4.07 2344. 4.72 2718.72 Upper Bound: 2880 (5 576) Tokens (100%) LLaVA-NeXT Vanilla 7B Upper: 5 128 = 640 SparseVLM (2024) VisionZip (2025) DivPrune (2025) VisionZip (2025) MMTok (Ours) Upper: 5 64 = 320 SparseVLM (2024) VisionZip (2025) DivPrune (2025) VisionZip (2025) MMTok (Ours) Upper: 5 32 = 160 SparseVLM (2024) VisionZip (2025) DivPrune (2025) VisionZip (2025) MMTok (Ours) 64.2 100% 67.9 100% 527 627 60.3 65.7 93.9% 96.8% 61.3 66.3 95.5% 97.6% 1842 100% 580 1772 96.2% 1787 97.0% 61.58 65.38 95.9% 96.3% 1773.04 96.3% 86.4 100% 627 86.3 99.9% 85.51 99.0% 70.2 100% 493 67.7 96.4% 68.1 97.0% 67.82 96.6% 62.4 65.9 97.2% 97.1% 67.9 87.6 1778 96.5% 101.4% 96.7% 62.27 65.29 97.0% 96.2% 1829.28 68.47 86.74 99.3% 100.4% 97.5% 264 314 57.7 64.3 89.9% 94.7% 59.3 63.1 92.4% 92.9% 290 1694 92.0% 1702 92.4% 59.63 63.66 92.9% 93.7% 1731.04 94.0% 61.0 64.4 95.0% 94.8% 1770 96.1% 60.96 64.35 95.0% 94.8% 1799.33 97.7% 132 157 51.2 63.1 79.8% 92.9% 55.5 60.1 86.4% 88.5% 145 1542 83.7% 1630 88.5% 57.79 62.29 90.0% 91.7% 1658.25 90.0% 58.2 63.9 90.7% 94.1% 1699 92.2% 60.05 62.97 93.5% 92.7% 1715.54 93.1% 314 82.1 95.0% 83.47 96.6% 86.2 99.8% 85.76 99.3% 157 74.8 86.6% 79.36 91.9% 83.4 96.5% 83.87 97.1% 246 67.3 95.9% 67.3 95.9% 67.82 96.6% 67.5 96.2% 67.23 95.8% 123 67.5 96.2% 68.3 97.3% 68.02 96.9% 67.5 96.2% 67.82 96.6% 80.1 100% 638 77.1 96.3% 79.1 98.8% 78.94 98.6% 79.9 99.8% 79.31 99.0% 319 73.4 91.6% 76.2 95.1% 76.64 95.7% 78.4 97.9% 77.68 97.0% 159 66.3 82.8% 71.4 89.1% 73.92 92.3% 75.6 94.4% 75.62 94.4% 61.3 100% 638 57.8 94.3% 60.2 98.2% 55.41 90.4% 60.8 99.2% 58.97 96.2% 319 55.9 91.2% 58.9 96.1% 53.84 87.8% 59.3 96.7% 56.93 92.9% 159 46.4 75.7% 56.2 91.7% 52.42 85.5% 57.3 93.5% 54.17 88.4% 35.1 100% 521 34.6 98.6% 34.7 98.9% 36.89 105.1% 37.2 106.0% 37.22 106.0% 261 34.4 98.0% 35.3 100.6% 37.11 105.7% 38.0 108.3% 38.00 108.3% 130 32.8 93.4% 36.1 102.8% 36.44 103.8% 37.7 107.4% 37.89 107.9% 70.2 100% 604 66.7 95.0% 67.56 96.2% 67.8 96.6% 67.74 96.5% 302 63.4 90.3% 65.35 93.1% 65.9 93.9% 66.29 94.4% 151 58.3 83.0% 62.54 89.1% 62.9 89.6% 64.54 91.9% Avg. 100% 77.8% - 97.5% 97.1% 98.9% 98.7% 88.9% - 94.5% 95.1% 97.6% 97.2% 94.4% - 90.4% 92.4% 95.0% 95.1% Table 12: Performance Comparison on LLaVA-NeXT-7B. The vanilla number of visual tokens varies by dataset due to dynamic image processing (max 2880 for 5 images). - means performance not available in the original paper. 17 Method GQA MMB MME P+C Acc. Acc. POPE F1 SQA Acc. VQAV2 VQAText MMMU SEED-I Acc. Acc. Acc. Acc. Avg. Images Avg. Tokens (n 576) 4.90 2822. 4.12 2373.12 4.53 2609.28 4.90 2822.40 3.85 2217.60 4.98 2868.48 4.98 2868. 4.07 2344.32 4.72 2718.72 Upper Bound: 2880 (5 576) Tokens (100%) LLaVA-NeXT Vanilla 13B Upper: 5 128 = 640 VisionZip (2025) DivPrune (2025) VisionZip (2025) MMTok (Ours) 65.4 100% 70.0 100% 527 627 63.0 68.6 96.3% 98.0% 1901 100% 580 1871 98.4% 62.82 66.84 96.1% 95.5% 1832.76 96.4% 86.2 100% 627 85.7 99.4% 86.17 99.9% 73.5 100% 493 71.2 96.9% 72.04 98.0% 63.7 66.6 97.4% 95.1% 73.2 86.3 1829 96.2% 100.1% 99.6% 63.71 67.44 97.4% 96.3% 1874.63 72.48 86.72 98.6% 100.6% 98.6% Upper: 5 64 = 320 VisionZip (2025) 264 314 60.7 67.2 92.8% 96.0% 290 1805 95.0% DivPrune (2025) VisionZip (2025) MMTok (Ours) 61.03 65.46 93.3% 93.5% 1802.79 94.8% 62.5 66.9 95.6% 95.6% 1861 97.9% 62.95 65.55 96.3% 93.6% 1840.10 96.8% Upper: 5 32 = 160 VisionZip (2025) 132 157 57.8 64.9 88.4% 92.7% 145 1739 91.5% DivPrune (2025) VisionZip (2025) MMTok (Ours) 59.34 64.78 90.7% 92.5% 1699.83 89.4% 59.7 65.3 91.3% 93.3% 1766 92.9% 61.94 65.89 94.7% 94.1% 1811.35 95.3% 314 82.0 95.1% 84.86 98.4% 85.7 99.4% 85.88 99.6% 157 76.6 88.9% 82.16 95.3% 84.0 97.4% 85.11 98.7% 246 70.3 95.6% 71.49 97.3% 72.7 98.9% 72.38 98.5% 123 69.3 94.3% 71.10 96.7% 72.0 98.0% 72.24 98.3% 81.8 100% 638 79.7 97.4% 79.87 97.6% 81.2 99.3% 80.55 98.5% 319 76.8 93.9% 77.6 94.9% 80.0 97.8% 78.79 96.3% 159 72.4 88.5% 74.72 91.3% 77.6 94.9% 76.8 93.9% 64.3 100% 638 62.2 96.7% 57.54 89.5% 64.4 36.2 100% 521 36.4 100.5% 37.78 104.4% 38.1 100.2% 105.2% 61.06 95.0% 319 60.9 94.7% 55.75 86.7% 63.2 98.3% 58.88 91.6% 159 58.4 90.8% 54.65 85.0% 60.8 94.6% 55.91 87.0% 37.11 102.5% 261 35.6 98.3% 36.00 99.4% 36.9 101.9% 36.33 100.4% 130 37.0 102.2% 35.89 99.1% 36.0 99.4% 37.11 102.5% 71.9 100% 604 68.8 95.7% 69.38 96.5% 69.2 96.2% 69.61 96.8% 302 65.2 90.7% 66.75 92.8% 67.9 94.4% 67.81 94.3% 151 61.1 85.0% 63.80 88.7% 64.4 89.6% 65.45 91.0% Avg. 100% 77.8% 97.7% 97.1% 98.8% 98.3% 88.9% 94.7% 94.6% 97.8% 96.4% 94.4% 91.4% 92.1% 94.6% 95.1% Table 13: Performance Comparison on LLaVA-NeXT-13B. The vanilla upper number of visual tokens is 2880. SEED-I represents SEED-IMG."
        },
        {
            "title": "Method",
            "content": "GQA MMB MME P+C Acc. Acc. POPE VQAText Acc. F1 SQA OCRBench Acc. Acc. Avg. Dynamic Resolution (MinPix = 256 28 28, MaxPix = 2048 28 28), Upper Bound (100%) Avg. Tokens Qwen-2.5-VL-7B Dynamic Res. 77.72 100% 86.16 100% 76.65 100% 2327 100% 83.80 100% 60.48 100% 83.25 100% 359.6 976.5 652.8 358.5 323. 867.6 276.9 100% Qwen-2.5-VL-7B Fixed Res. Fixed Resolution (MinPix = MaxPix = 2048 28 28), Upper Bound (100%) 76.60 91.4% 58.59 86.09 96.9% 100.4% 100.5% 99.9% 73.67 96.1% 76.64 98.6% 83.59 2339 Retain 20% VisionZip (2025) DivPrune (2025) MMTok (Ours) Retain 10% VisionZip (2025) DivPrune (2025) MMTok (Ours) Retain 5% VisionZip (2025) DivPrune (2025) MMTok (Ours) 71.7 55.4 173.5 71.9 56.80 80.33 93.9% 96.5% 2174 83.38 93.4% 96.8% 56.70 76.98 93.8% 92.5% 2163 80.59 93.0% 93.5% 58.09 79.30 96.0% 95.3% 2217 82.38 95.3% 95.7% 35. 27.7 86.8 36.0 52.47 75.60 86.8% 90.8% 2003 78.90 86.1% 91.6% 53.43 72.85 88.3% 87.5% 1957 74.99 84.1% 87.0% 55.09 74.74 91.1% 89.8% 2051 78.75 88.1% 91.4% 17.9 13.8 43. 18.0 67.53 46.28 76.5% 81.1% 66.38 1677 72.1% 77.1% 49.01 65.89 81.0% 79.1% 1739 68.45 74.7% 79.4% 65.89 50.66 83.8% 79.2% 71.35 1796 77.2% 82.8% 195.3 70.43 90.6% 65.86 84.7% 70.49 90.7% 97. 63.78 82.1% 59.59 76.7% 63.90 82.2% 48.8 54.49 70.1% 52.02 66.9% 55.95 72.0% 64.6 76.30 99.5% 70.55 92.1% 71.94 93.8% 32. 74.07 96.6% 69.21 90.3% 71.69 93.5% 16.2 72.63 94.8% 67.67 88.3% 70.10 91.4% Qwen-2.5-VL 7B Text-Only 31.84 20.10 54.3% 24.0%"
        },
        {
            "title": "0 Token ↓ 100%\n935",
            "content": "0.00 40.0% 0.0%* 38.93 50.8% 65.25 88.6% 130.6 59.50 71.0% 48.10 57.4% 59.60 71.1% 65.3 36.90 44.0% 37.30 44.5% 43.60 52.1% 32. 19.70 23.5% 24.90 29.7% 30.70 36.6% 1.80 2.1% 99.3% 80% 94.2% 91.5% 94.6% 90% 87.5% 84.7% 88.5% 95% 75.4% 76.3% 79.0% 33.8% Table 14: Performance Comparison on Qwen-2.5-VL-7B-Instruct. Avg. is the average performance over the 5 datasets: GQA, MMB, MME, POPE, and VQAText. The first line of each method shows the raw benchmark accuracy, and the second line is the proportion relative to the upper limit. *Qwen-2.5-VL outputs \"No\" for all POPE questions when no visual tokens are provided, resulting in 0% F1 score. 19 Method GQA MMB MME P+C Acc. Acc. POPE F1 SQA Acc. VQAText MMMU SEED-I* Avg@8 Avg@5 90% 80% Acc. Acc. Acc. /8 /8 LLaVA-1.5-7B 90% Threshold 80% Threshold VisionZip DivPrune MMTok VisionZip DivPrune MMTok VisionZip DivPrune MMTok VisionZip DivPrune MMTok VisionZip DivPrune MMTok VisionZip DivPrune MMTok 61.9 55.71 49.52 55.1 57.78 58.29 51.78 55.11 55.95 46.72 51.10 53. 39.47 46.09 49.06 36.57 40.67 43.93 35.94 38.58 40.58 64.7 58.23 51.76 60.1 59.28 61.17 57.22 58.93 58. 45.70 53.09 54.30 24.40 43.13 49.06 18.30 28.61 36.94 16.84 21.48 25.69 1862 1675.80 1489.60 1690 1674.40 1715. 1580.43 1600 1624.72 1326.89 1518 1550.65 1069.94 1294 1355.31 923.57 1134 1290.31 890.28 991 1122.42 85.9 77.31 68. 77.0 85.56 85.77 68.88 82.06 82.95 51.84 69.56 79.79 23.66 52.10 78.46 24.48 33.33 74.84 26.48 37.60 68. Vanilla Baseline (576 tokens) 36.3 58.2 32.67 52.38 29.04 46.56 69.5 62.55 55.60 69.0 68.17 68.86 68.12 68.57 67.38 67.13 69.21 65.54 63.41 67.77 63. 61.48 65.10 61.33 59.84 64.60 60.29 64 Tokens 55.5 54.69 56.01 32 Tokens 53.23 53.20 53.70 16 Tokens 49.74 50.01 50.04 8 Tokens 44.62 45.21 45. 4 Tokens 40.82 42.54 43.52 2 Tokens 39.55 42.16 42.42 0 Tokens 41.66 36.2 35.56 36.11 35.11 35.33 35.33 35.00 35.44 34. 33.67 34.00 34.11 33.78 33.33 34.00 33.78 33.44 32.67 66.14 59.53 52.91 57.84 60.21 61.29 53.28 57.08 59. 46.66 52.72 56.67 38.46 46.68 52.74 35.34 40.99 48.10 34.62 38.43 42.89 100.0% 100.0% 90.0% 90.0% 80.0% 80.0% 93.0% 94.4% 96.0% 88.0% 91.9% 92.7% 78.3% 86.2% 88.3% 63.2% 76.3% 82.9% 58.8% 66.3% 76.7% 57.8% 63.5% 70.0% 90.0% 93.1% 94.7% 83.5% 89.6% 91.0% 69.7% 81.4% 86.4% 48.9% 68.4% 79.8% 43.8% 54.3% 71.4% 43.0% 50.1% 62.1% 8/8 5/8 7/8 8/8 3/8 5/8 7/8 2/8 2/8 3/8 2/8 2/8 3/8 1/8 2/8 1/8 1/8 2/8 1/ 8/8 8/8 8/8 8/8 8/8 8/8 8/8 3/8 7/8 8/8 2/8 2/8 3/8 2/8 2/8 3/ 2/8 2/8 3/8 Baseline 37.65 19.33 970.89 44. 56.92 33.33 37.03 62.0% 50.2% 1/ 2/8 Table 15: Extended Performance Comparison with Extremely Less Token Budgets on LLaVA1.5-7B. *SEED-I indicts SEEDBench-Image. Avg@8 is across all 8 datasets, while Avg@5 is on 5 High-IC datasets. 20 Method GQA MMB MME P+C Acc. Acc. POPE F1 SQA Acc. VQAText MMMU SEED-I Avg@8 Avg@6 90% 80% Acc. Acc. Acc. /8 /8 LLaVA-NeXT-7B 90% Threshold 80% Threshold VisionZip DivPrune MMTok VisionZip DivPrune MMTok VisionZip DivPrune MMTok VisionZip DivPrune MMTok VisionZip DivPrune MMTok 64.2 57.78 51.36 55.5 57.79 60.05 50.80 55.73 58.23 41.87 52.87 54. 36.56 49.57 49.60 36.17 45.19 45.72 67.9 61.11 54.32 60.1 62.29 62.97 50.69 59.97 62.54 28.35 55.76 59. 18.38 48.54 51.20 17.96 37.11 38.75 1842 1657.80 1473.60 1630 1658 1716 1431 1575 1681 999 1462 814 1324 1457 823 1134 1283 Vanilla Baseline (Upper 2880 tokens) 86.4 77.76 69.12 70.2 63.18 56.16 35.1 31.59 28.08 61.3 55.17 49. Upper 325 Tokens (160 Tokens) 5.6% 74.8 79.36 83.87 36.1 36.44 37.89 56.2 52.42 54.17 68.3 68.02 67.82 Upper 165 Tokens (80 Tokens) 2.8% 61.82 74.74 81.89 34.44 36.56 36. 51.65 50.35 49.56 66.73 66.83 67.08 Upper 85 Tokens (40 Tokens) 1.4% 31.44 33.44 35.00 42.85 48.02 45.77 64.40 66.78 67.18 21.22 67.49 81. Upper 45 Tokens (20 Tokens) 0.7% 31.56 0.40 31.78 52.14 33.67 82.41 35.36 44.06 42.33 63.56 65.94 66.83 Upper 25 Tokens (10 Tokens) 0.3% 30.56 0.80 33.22 25.48 33.78 79.62 32.84 40.33 39.77 62.82 65.25 65. 70.2 63.18 56.16 58.3 62.54 64.54 51.77 59.48 61.86 41.93 55.40 59.28 34.98 51.25 55.34 34.31 45.54 49. 100.0% 100.0% 90.0% 90.0% 80.0% 80.0% 90.6% 92.4% 95.2% 81.8% 89.2% 92.0% 62.1% 83.7% 88.3% 52.1% 76.3% 83.3% 50.9% 66.8% 76.9% 91.7% 94.1% 96.8% 82.7% 91.1% 94.5% 61.2% 85.4% 91.3% 51.5% 77.6% 86.4% 50.8% 67.3% 79.9% 8/8 3/8 6/8 7/8 2/8 2/8 6/8 1/8 2/8 3/8 1/8 2/8 3/8 0/8 2/8 3/8 8/8 8/8 8/8 8/8 3/8 8/8 8/8 2/8 4/8 7/8 2/8 2/8 3/8 2/8 2/8 3/8 Baseline 38.23 17.87 867 25.84 63.91 37. 31.56 37.43 57.3% 57.3% 1/8 2/ 0 Tokens Table 16: Extended Performance Comparison with Extremely Less Token Budgets on LLaVANeXT-7B. Avg@8 is across all 8 datasets, while Avg@6 is across 6 High-IC datasets. The 5 notation indicates maximum sampling to 5 images. Average percentages are calculated relative to the vanilla baseline for each metric."
        }
    ],
    "affiliations": [
        "Arizona State University",
        "Duke University",
        "University of Texas at Dallas",
        "University of Washington",
        "Zoom Communications"
    ]
}