{
    "paper_title": "ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition",
    "authors": [
        "Muyang Zhao",
        "Qi Qi",
        "Hao Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) can achieve strong reasoning performance with sufficient computation, but they do not inherently know how much computation a task requires. We study budgeted inference-time reasoning for multiple tasks under a strict global token constraint and formalize it as a Ordered Stochastic Multiple-Choice Knapsack Problem(OS-MCKP). This perspective highlights a meta-cognitive requirement -- anticipating task difficulty, estimating return over investment (ROI), and allocating computation strategically. We propose ROI-Reasoning, a two-stage framework that endows LLMs with intrinsic, budget-aware rationality. In the first stage, Meta-Cognitive Fine-Tuning teaches models to predict reasoning cost and expected utility before generation, enabling explicit solve-or-skip decisions. Next, Rationality-Aware Reinforcement Learning optimizes sequential decision making under a hard token budget, allowing models to learn long-horizon allocation strategies. Across budgeted mathematical reasoning benchmarks, ROI-Reasoning consistently improves overall score while substantially reducing regret under tight computation budgets."
        },
        {
            "title": "Start",
            "content": "ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition Muyang Zhao1, Qi Qi1*, Hao Sun1* 1Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China {myzhao13, qi.qi, haosun}@ruc.edu.cn 6 2 0 2 7 ] A . [ 1 2 2 8 3 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) can achieve strong reasoning performance with sufficient computation, but they do not inherently know how much computation task requires. We study budgeted inference-time reasoning for multiple tasks under strict global token constraint and formalize it as Ordered Stochastic Multiple-Choice Knapsack Problem(OSMCKP). This perspective highlights metacognitive requirementanticipating task difficulty, estimating return over investment (ROI), and allocating computation strategically. We propose ROI-Reasoning, two-stage framework that endows LLMs with intrinsic, budgetaware rationality. In the first stage, MetaCognitive Fine-Tuning teaches models to predict reasoning cost and expected utility before generation, enabling explicit solve-or-skip decisions. Next, Rationality-Aware Reinforcement Learning optimizes sequential decision making under hard token budget, allowing models to learn long-horizon allocation strategies. Across budgeted mathematical reasoning benchmarks, ROI-Reasoning consistently improves overall score while substantially reducing regret under tight computation budgets."
        },
        {
            "title": "Introduction",
            "content": "People often face situations where they must plan ahead despite uncertainty about the cost and benefit of future actions. Consider Bob, rational test-taker during timed exam. Bob can see all problems at once, but he does not know how long each problem will take or whether it is even solvable under the allotted time. Spending too long on deceptively difficult question risks wasting minutes that could have been invested in easier ones; spending too little may lead to avoidable mistakes. rational Bob therefore distributes his limited time across questions so as to maximize his final exam score. *Corresponding authors: Qi Qi and Hao Sun 1 Figure 1: LLM is taking multi-question test under fixed token budget. Crucially, success in such settings requires more than raw problem-solving ability. Beyond being able to solve individual questions, Bob must possess meta-cognitive planning: the ability to reason about how limited effort should be allocated across multiple problems. This includes anticipating problem difficulty, estimating the potential return of additional effort, and deciding when to persist, skip, or move on. This form of planning operates at higher level than local problem solving and directly governs how computation is spent. Large language models (LLMs) lack this form of meta-cognitive planning. While recent approaches such as Chain-of-Thought (Wei et al., 2022; Kojima et al., 2022), self-reflection (Shinn et al., 2023; Madaan et al., 2023), and test-time scaling (Snell et al., 2024; Muennighoff et al., 2025) demonstrate that additional reasoning can improve performance, they rely on heuristics rather than on principled allocation of reasoning effort. In particular, models do not plan ahead how much computation to invest across multiple problems, nor do they revise such plans as partial reasoning unfolds. In contrast, human test-takers naturally engage in precomputation meta-cognition: before solving any question, they survey the exam and form high-level allocation guidelines, which are then continuously adjusted based on observed effort and progress. To bridge this gap, we model large language model inference under global token budget as an Ordered Stochastic Multiple-Choice Knapsack Problem (OS-MCKP). In this formulation, each problem corresponds to class, and different levels of reasoning effort correspond to items within that class, each associated with computational cost (token usage) and reward (answer quality). The total token budget serves as the knapsack capacity. The problem is stochastic in that rewards are only revealed after generation, and ordered in that problems must be processed sequentially, such that early decisions irreversibly constrain the remaining budget. This abstraction captures the core challenge of inference-time decision making in LLMs: allocating limited computation across problems to maximize overall performance under uncertainty. To illustrate this challenge, we evaluate DeepSeek-V3.2 (Liu et al., 2025a) on multiquestion exam-style tasks and visualize its inference-time behavior in Figure 1. As shown in the figure, the model often spends too much effort on early questions, leaving insufficient budget for later ones, which reflects lack of global planning. Building on this formulation, we introduce ROIReasoning, meta-cognitive reasoning framework that performs Rational Optimization for Inference, where ROI naturally corresponds to Return over Investment. It explicitly models the costbenefit tradeoff of reasoning and trains models to reason about this tradeoff during inference. Our approach consists of two complementary stages. We first introduce Meta-Cognitive FineTuning, which leverages rejection sampling finetuning (Touvron et al., 2023) to enable precomputation: before generating any solution, the model learns to anticipate problem difficulty and the expected utility of additional reasoning. We then propose Rationality-Aware Reinforcement Learning, which trains the model to plan and allocate computation across multiple problems under hard global token constraint. Together, these two stages endow LLMs with both anticipatory planning and adaptive decision making, allowing inference-time computation to be allocated in rational, ROI-aware manner."
        },
        {
            "title": "2 Related Work",
            "content": "Chain-of-thought (CoT) prompting has been shown to improve the reasoning performance of large language models on tasks involving multi-step logical or mathematical inference (Wei et al., 2022; Kojima et al., 2022). Also, reinforcement learning has also demonstrated strong empirical effectiveness in shaping and improving reasoning behaviors in large language models (Guo et al., 2025; OpenAI, 2024; Team et al., 2025; Yang et al., 2025). However, these gains often come at the cost of significantly longer outputs and increased inference-time computation. Motivated by this trade-off, recent work has focused on efficient reasoning and explicit control of computational. Zhang et al. (2025) proposes to adaptively switch between thinking and no-thinking modes via reinforcement learning, reducing token usage without sacrificing accuracy, while Qi et al. (2025) introduces anytime reasoning objectives that enable valid predictions under arbitrary truncations of the reasoning process. From more structured perspective, Lin et al. (2025) decomposes complex reasoning into subproblems and allocates token budgets based on predicted uncertainty. Other approaches regulate reasoning length through adaptive computation controllers (Huang et al., 2025), learned generationlength policies (Li et al., 2025), or explicit conditioning on predicted token budgets at inference time (Han et al., 2025). While effective, these methods primarily aim to calibrate model behavior on individual tasks, rather than learning to allocate limited reasoning capacity across multiple tasks under shared budget. Beyond per-instance control, budget allocation can be viewed as an optimization problem under uncertainty. In online or delayed-feedback settings, common solution is the predict-then-optimize paradigm, which first estimates quantities such as costs or rewards and then solves downstream optimization problem based on these predictions (Mandi et al., 2020; Mulamba et al., 2020). Finally, our work relates to reinforcement learning for combinatorial optimization. Prior surveys highlight the effectiveness of reinforcement learning in problems with large discrete action spaces and delayed rewards, where explicit modeling of costs and benefits is challenging (Mazyavkina et al., 2021; Yang and Whinston, 2023). By learning policies directly through interaction, reinforcement learning is well suited to budgeted sequential decision-making, where rewards are revealed only after sequence of irreversible allocation decisions."
        },
        {
            "title": "3 Problem Formulation",
            "content": "We study budgeted reasoning problem motivated by scenarios such as test-taker answering multiple questions under strict time limit. Let de2 Figure 2: Overview of the ROI-Reasoning framework, including Meta-Cognitive Fine-Tuning and Rationality-Aware Reinforcement Learning. note distribution over problem instances. The model is presented with set of problems {x1, x2, . . . , xN }, all revealed upfront within single prompt. Although the full problem set is observable, the model must generate answers sequentially in fixed order, completing its response to xi before proceeding to xi+1, without revising earlier decisions. The model follows policy πθ, which maps the current problem xi together with an internal state siinduced by previous actions and observationsto an action πθ(si, xi). An action may correspond to producing solution or strategically abstaining from answering. Each action yields non-negative reward r(πθ(si, xi)), determined by scoring function, and incurs computational cost c(πθ(si, xi)), measured in generated tokens. While token consumption is directly observable during generation, the realized reward is only revealed after evaluation."
        },
        {
            "title": "The objective is to maximize the expected total",
            "content": "reward subject to global computation budget: max πθ ExX , πθ (cid:35) r(πθ(si, xi)) (cid:34) (cid:88) i=1 s.t. (cid:88) i=1 c(πθ(si, xi)) B, (1) where the expectation is taken over both problem instances and the stochasticity of the policy. Knapsack Problems. From an optimization perspective, our formulation can be viewed as generalization of the Multiple-Choice Knapsack Problem (MCKP) (Kellerer et al., 2004). In classical MCKP, items are grouped into disjoint classes, and at most one item is selected from each class, with deterministic values and weights. In our setting, each problem xi corresponds to class, while different actions (e.g., solving, partially attempting, or abstaining) correspond to items within that class. The reward r() and computational cost c() play the roles of value and weight, respectively, and the total token budget corresponds to the knapsack capacity. Unlike classical MCKP, the reward and cost of an action are not known at decision time and must be estimated before execution. This places our problem closer to the Stochastic Multiple-Choice Knapsack Problem (S-MCKP) (Tran-Thanh et al., 2015), where item values and weights are random variables revealed only after selection. Such stochasticity naturally arises in LLM reasoning, as both answer correctness and required computation depend on the problem instance and the generation process. Our formulation also differs from standard SMCKP by introducing an explicit ordering constraint. Although all problems are observable upfront, they must be processed in fixed order. This design choice serves two purposes. First, it enables consistent and reproducible evaluation. Second, it highlights the models meta-cognitive ability: the"
        },
        {
            "title": "FULL INPUT PROMPT OVERVIEW",
            "content": "BLOCK 1: Instruction The following text contains three mathematics problems. Please solve them sequentially. Constraint: You have strict limit of 1024 tokens. Your goal is to answer as many questions correctly as possible within this limit. Step 1: Predict Length & Difficulty Before answering, estimate token length and difficulty for every problem: 1. Level-0: < 256 tokens 2. Level-1: 256 512 tokens 3. Level-2: > 512 tokens 4. Level-3: Too difficult (Skip reasoning, Answer NA) Step 2: Solve & Box If Level 0-2: Provide step-by-step reasoning. You MUST wrap your final answer in boxed{}. If Level 3: Do NOT provide reasoning. Directly output boxed{NA}. Strict Format Requirements: Start with: <predicted_level>Level-X</predicted_level> for each problem Content: (Normal) [Reasoning]... boxed{...} or (Give up) boxed{NA} (followed by) BLOCK 2: Mathematics Problems Problem 1: [Text content of Problem 1...] Problem 2: [Text content of Problem 2...] Problem 3: [Text content of Problem 3...] Figure 3: Structure of the ROI-Reasoning input prompt. model must decide how much computation to invest in each problem while considering the impact on later ones. Taken together, our problem can be interpreted as an Ordered Stochastic Multiple-Choice Knapsack Problem (OS-MCKP). Decisions are made following fixed order under uncertainty, costs are observable but irreversible, and rewards are revealed only after evaluation. Directly optimizing such behavior end-to-end is difficult because supervision is sparse and delayed, making it hard to separate the effects of answer quality, ordering decisions, and overall budget usage. This motivates decomposition that separates meta-cognitive assessment at the problem level from global budget allocation across the full problem order."
        },
        {
            "title": "4.1 Overview",
            "content": "Our goal is to transform LLMs from passive executors into rational agents capable of ROI-Reasoning, 4 i.e., reasoning under explicit computational constraints to maximize return over investment. We draw inspiration from exam-taking scenarios where rational test-taker must decide not only how to solve problem, but also whether it is worth attempting given limited time. We propose twostage training pipeline. First, Meta-Cognitive Fine-Tuning (MFT) cold-starts cost awareness and structured decision-making, teaching the model to anticipate difficulty, estimate cost, and strategically abstain from low-ROI attempts. Second, Rationality-Aware Reinforcement Learning (RARL) places the model in simulated multiproblem exams with strict budgets, enabling it to improve both problem-solving and global budget allocation through trial and error, thereby maximizing overall ROI. An overview of the overall training pipeline and decision process is illustrated in Figure 2. At inference time, these capabilities are realized through structured input prompt that explicitly exposes the global budget, enforces prereasoning difficulty prediction, and constrains the models output format. An overview of this prompt structuretogether with the sequential decision process it inducesis shown in Figure 3."
        },
        {
            "title": "4.2 Meta-Cognitive Fine-Tuning",
            "content": "Meta-Cognitive Fine-Tuning (MFT) aims to equip the model with explicit cost awareness and metacognitive control prior to detailed reasoning. The core objective is to train the model to think before thinking: before generating any solution, it must first assess the anticipated reasoning cost and decide whether attempting the problem is worthwhile under limited budget. To make such cost awareness explicit and controllable, we introduce structured difficulty tag that must appear before any reasoning: <predicted_level>Level-k</predicted_level> The discrete level serves as coarse proxy for the expected computational cost. In practice, each level corresponds to predefined range of token consumption, enabling the model to reason about anticipated effort at categorical level rather than relying on exact token counts. For example, Level-0 denotes short solutions (e.g., within 256 tokens), Level-1 corresponds to moderate reasoning budgets (e.g., 256512 tokens), and higher levels indicate increasingly complex reasoning requirements. MFT is carried out through two conceptually distinct stages: Tag Alignment and Refusal Learning. Tag Alignment teaches the model to accurately associate problems with appropriate cost levels, while Refusal Learning trains the model to strategically abstain from low-ROI problems when the expected cost outweighs the potential benefit. During Tag Alignment, the model is trained to follow standardized reasoning template and to output calibrated <predicted_level> tags. We first perform fine-tuning on single-problem instances, allowing the model to learn atomic mappings between problem difficulty and cost levels. We then extend this supervision to short multiproblem sequences, which encourages the model to make cost predictions in the presence of competing tasks and limited shared budget. Refusal Learning further augments this capability by introducing explicit supervision for strategic abstention. When problem is consistently unsolved across multiple attempts or is estimated to require excessive computation, the model is allowed to output standardized box{ NA } token as its final answer. This mechanism provides clear training signal for recognizing low-ROI situations and avoiding unproductive computation. Together, these two stages transform MFT from simple template imitation into form of metacognitive training: the model learns not only how to reason, but also when reasoning is worth the cost. The complete training procedure alternates between single-problem and multi-problem instances, gradually bridging atomic difficulty assessment and contextual decision making. Detailed dataset construction, rejection sampling rules, and intermediate model variants are described in Appendix A."
        },
        {
            "title": "Learning",
            "content": "While MFT provides structured demonstrations of ROI-aware behavior, it remains imitation-based and cannot fully capture the long-horizon credit assignment required for global budget allocation across multiple problems. We therefore introduce Rationality-Aware Reinforcement Learning (RARL), which enables the model to learn budget-aware planning through interaction in simulated exam environments with an explicit total token budget B. In each training sample, the model is presented with problems at once, forming single exam instance. The problems are ordered and must be processed sequentially, following the same fixed order throughout generation. hard global constraint is imposed: the total number of generated tokens across the entire exam instance must not exceed B. This constraint is enforced in two ways. First, the budget is explicitly provided in the prompt, encouraging the model to reason with resource allocation across the problem sequence. Second, generation is terminated once the token limit is reached, ensuring that all sampled trajectories strictly satisfy the budget constraint. Under this formulation, the objective is to maximize the total return under global budget constraint. The reward is defined at the exam-instance level to directly reflect global return over investment. For each problem i, the model receives unit reward if and only if two conditions are simultaneously satisfied: (i) the final answer is correct, and (ii) the predicted cost level matches the posterior token consumption. Let Toki denote the actual number of tokens consumed for problem i, and let Level() map token counts to discrete cost levels. The reward for problem is defined as (cid:104) correcti Level(Toki) = (cid:91)Leveli ri = , (2) where (cid:91)Leveli is the level predicted by the model before reasoning. The total reward for an exam instance is then (cid:105) = (cid:88) i=1 ri, (3) which is bounded above by . No intermediate or per-token rewards are used during reinforcement learning; instead, the computational constraint is entirely captured by the hard budget B, forcing the model to internalize the trade-off between solving more problems and spending more computation. We optimize this objective using Dr. Group Relative Policy Optimization (Dr. GRPO), debiased variant of Group Relative Policy Optimization (GRPO) (Shao et al., 2024; Liu et al., 2025b). For each exam instance q, we sample group of rollout trajectories {o1, . . . , oG} from the old policy πθold under the same budget constraint. Here, denotes the full ordered problem set {x1, . . . , xN }, and each rollout oi is complete auto-regressive response spanning all problems. This sequence-level notation aligns naturally with language model generation, while the (xi, si) formulation in Section 3 makes the underlying sequential decision process explicit. 5 Table 1: Performance comparison under different difficulty levels and token budgets. We report the Score (maximum = 3), Scoreeasy, and Regret on Medium and Hard test papers, each consisting of three questions, under 1024-token and 512-token constraints. The best and second-best results are highlighted in bold and underline, respectively. Method 512 1024 512 Medium Hard Score Scoreeasy Regret Score Scoreeasy Regret Score Scoreeasy Regret Score Scoreeasy Regret DeepSeek-V3.2 (685B) GPT-4o-mini Llama-3.1-8B-Instruct Qwen2.5-Math-7B-Instruct Qwen2.5-7B-Instruct Llama-3.2-3B-Instruct Qwen2.5-Math-1.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct + Plan-and-Solve Prompting + Least-to-Most Prompting + MFT + MFT + Greedy Knapsack + MFT + RARL (Ours) 1.33 1.41 0.74 1.07 1.27 0.41 1.05 0. 0.40 0.52 0.98 0.97 1.13 1.85 1.53 1.03 1.60 1.55 0.66 1.42 0.72 0.61 0.65 1.01 1.01 1.15 0.39 0.09 0.39 0.50 0.22 0.61 0.35 0. 0.53 0.25 0.03 0.04 0.02 Large-Scale Models (>100B) 1.53 1.19 0.88 0.65 0.74 0.83 Open-Source Models (1.5B8B) 0.96 1.09 1.24 0.63 1.01 0.55 0.54 0.51 0.64 0.33 0.49 0. 0.78 1.14 0.94 0.91 1.06 0.67 Meta-Cognitive Methods 0.15 0.28 0.82 0.83 0.97 0.35 0.44 0.90 0.98 1.08 1.33 0.57 0.10 0.18 0.11 0.87 1. 0.55 0.69 1.07 0.30 0.79 0.49 0.28 0.38 0.95 0.96 1.12 1.83 1.54 1.03 1.59 1.54 0.66 1.41 0.68 0.60 0.64 0.99 1.00 1.14 1.10 0. 0.84 1.30 0.44 1.20 0.78 0.37 1.14 0.68 0.04 0.04 0.02 0.49 0.32 0.28 0.18 0.29 0.17 0.19 0.15 0.05 0.13 0.81 0.83 0.93 1.53 1. 0.96 1.07 1.24 0.63 1.01 0.56 0.33 0.45 0.90 0.89 1.08 2.12 2.69 2.42 4.94 3.28 2.71 4.32 2.73 5.60 2.46 0.11 0.07 0.16 The policy πθ is updated by maximizing J(θ) = q,{oi}G i=1πθold (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) oi (cid:88) (cid:16) min i=1 t= (cid:0)ri,t(θ) ˆAi,t, clip(ri,t(θ), 1 ϵ, 1 + ϵ) ˆAi,t (cid:1) (cid:17) (cid:35) , πθold (4) where ri,t(θ) = πθ(oi,tq,oi,<t) (oi,tq,oi,<t) is the importance sampling ratio at token position t, ϵ controls the clipping range. The advantage ˆAi,t is computed from the group-level rewards {R1, . . . , RG} as ˆAi,t = Ri mean({R1, . . . , RG}). Through RARL, the model jointly improves its problem-solving competence and its planning ability, learning when to invest computation in highROI problems and when to conserve budget for subsequent ones. As result, the model acquires long-horizon, globally rational behavior that cannot be fully induced by supervised meta-cognitive fine-tuning alone. (5)"
        },
        {
            "title": "5 Experiments",
            "content": "token constraints. Our goal We evaluate our approach on budgeted multiproblem mathematical reasoning tasks under explicit is to assess whether Meta-Cognitive Fine-Tuning (MFT) and Rationality-Aware Reinforcement Learning (RARL) improve meta-cognitive computation allocation across problems, rather than merely enhancing single-problem accuracy. 6 Data. The is drawn from training data et and 2021) al., the GSM8K (Cobbe training al., 2021) et MATH (Hendrycks sets, together with AIME problems from years 20222024. Evaluation is conducted on the corresponding GSM8K and MATH test sets, as well as AIME25 (Zhang and Math-AI, 2025). Test Paper Construction and Budgets. Instead of evaluating individual problems in isolation, we group three problems into single test paper, which constitutes one query to the model. We assume an expected difficulty ordering of AIME > MATH > GSM8K, and construct each test paper by sampling one problem from each dataset and arranging them in different orders. We consider two settings: Medium papers, which interleave problems of different difficulty, and Hard papers, which place more challenging problems earlier in the sequence. Detailed construction procedures are provided in Section B. During training, all test papers are answered under fixed budget of 1024 tokens. At evaluation time, we consider both 1024-token and 512-token budgets. The 1024-token setting typically allows the model to fully solve two problems with limited remaining budget for the third, inducing nontrivial trade-offs between continuing reasoning and reserving computation for subsequent problems. The 512-token setting imposes much tighter constraint, where early stopping and selective abstention become critical. (a) Medium with 512 Budget (b) Medium with 1024 Budget (c) Hard with 512 Budget (d) Hard with 1024 Budget Figure 4: Token length distributions across varying difficulties and constraints. The histograms compare the reasoning length patterns of Qwen2.5-1.5B-Instruct, Qwen2.5-1.5B-Instruct + MFT, and Qwen2.5-1.5B-Instruct + MFT + RARL under four distinct settings. Models and Baselines. We consider three categories of models and baselines in our experiments, corresponding to the grouped results in Table 1. attempts problems in ascending order of estimated cost until the budget is exhausted. Implementation details are provided in Section C. Large-scale models. The first category consists of advanced large-scale proprietary models, represented by GPT-4o-mini (Hurst et al., 2024) and DeepSeek-V3.2 (Liu et al., 2025a). These models are used to assess whether model scale and general reasoning ability alone induce meta-cognitive behavior under explicit budget constraints. Open-source models. The second category includes widely used open-source instruction-tuned models with parameter sizes ranging from 1.5B to 8B, such as the Qwen2.5 family (Qwen et al., 2025) and the Llama 3 series (Dubey et al., 2024). For fair comparison, all models are evaluated using simple and explicit prompting strategy, which we treat as the default prompt (Section C.1). Meta-cognitive methods. The third category focuses on meta-cognitive approaches built upon the same base model, Qwen2.5-1.5B-Instruct. We equip this model with structured multi-step reasoning prompts, including Plan-and-Solve (Wang et al., 2023) and Least-to-Most (Zhou et al., 2022), to evaluate whether explicit reasoning scaffolds can induce better budget-aware behavior. In addition, inspired by the predict-thenoptimize paradigm studied in (Muppidi et al., 2025), where problem ordering and computation allocation are solved externally, we construct greedy knapsack baseline. This method uses the MFT model to estimate the difficulty (expected token usage) of each problem and then greedily Finally, we include ablation variants with MetaCognitive Fine-Tuning (MFT) only and with the full MFT+RARL pipeline. These ablations allow us to disentangle the respective contributions of explicit meta-cognitive supervision and reinforcement learning to budget-aware reasoning performance. Evaluation Metrics. We evaluate models on Medium and Hard test papers using two metrics. The primary metric is the Score, defined as the average total score over the three problems in each test paper. To capture the performance loss induced by suboptimal answering order under fixed token budget, we measure Regret. For given problem set {x1, . . . , xN } and policy π, let s(π; x1:N ) denote the score obtained under the given order. The optimal achievable score over all ! permutations is defined as s(π) = max σP s(π; σ(x1:N ))."
        },
        {
            "title": "The regret is then given by",
            "content": "Regret(π) = s(π) s(π; x1:N ) s(π) . In practice, computing s(π) exactly requires evaluating all ! possible orderings, which incurs substantial computational overhead. We therefore adopt an approximate regret measure. Specifically, 7 models to internalize meta-cognitive decision making without targeted supervision. To further analyze model behavior, Figure 4 compares token usage patterns for three variants of the same backbone model. While the base model exhibits rigid token consumption, MFT introduces coarse adaptation, and MFT+RARL demonstrates clear budget-aware behavior by shortening reasoning or abstaining earlier on harder problems. Finally, Figure 5 shows that ROI-Reasoning achieves high accuracy in difficulty prediction, indicating reliable meta-cognitive self-reflection under budget constraints."
        },
        {
            "title": "6 Conclusion",
            "content": "We studied budgeted inference-time reasoning for multiple tasks under strict global token constraint, motivated by the classical test-taking scenario where rational test-taker must decide not only how to solve, but also whether problem is worth the time. This perspective underscores the importance of meta-cognitive planning: anticipating difficulty, estimating return over investment (ROI), and allocating limited computation across questions to maximize the final score. We proposed ROI-Reasoning, two-stage framework that endows LLMs with intrinsic, budget-aware rationality. Meta-Cognitive FineTuning (MFT) teaches models to predict cost levels and enable explicit solve-or-skip decisions, while Rationality-Aware Reinforcement Learning (RARL) further trains long-horizon planning under hard global budget. Experiments on budgeted mathematical reasoning benchmarks show that combining MFT and RARL consistently improves overall score and substantially reduces regret under tight budgets."
        },
        {
            "title": "7 Use of AI Assistants",
            "content": "ChatGPT was used solely for proofreading and language refinement of the manuscript. The use of AI assistance did not influence the scientific content, experimental results, or conclusions presented in this paper."
        },
        {
            "title": "8 Limitations",
            "content": "Our study focuses on budgeted mathematical reasoning with fixed prompt structure (three problems per test paper) and uses token count as proxy 1https://chatgpt.com/ Figure 5: Composition of predicted-level errors (correst, under-, and over-estimated) under different budgets on Medium and Hard test papers, comparing RFT and RFT+RARL. for each test paper, we construct reference ordering by sorting the same set of problems from lower to higher expected difficulty, yielding relatively easier sequence. Let Score denote the models score on the original test paper and Scoreeasy the score obtained on the reordered paper. The approximate regret is defined as (cid:94)Regret = Scoreeasy Score Scoreeasy . Results. Table 1 summarizes the main results. Overall, the full MFT+RARL model consistently achieves higher scores and lower regret across both Medium and Hard settings. Notably, even advanced large-scale models such as GPT-4o-mini and DeepSeek-V3.2 do not exhibit clear advantages, echoing the motivating example in Figure 1 and suggesting that strong general reasoning capability alone is insufficient when models must anticipate difficulty and allocate computation under strict global budget. Among open-source models, parameter scale does not directly reflect meta-cognitive ability. For instance, Qwen2.5-Math-7B-Instruct does not consistently outperform its 1.5B counterpart, and Qwen2.5-7B-Instruct often performs better than Qwen2.5-Math-7B-Instruct despite lacking specialized mathematical pretraining. These observations indicate that stronger mathematical capability alone does not translate into stronger meta-cognitive reasoning under budget constraints. We also find that simple and explicit prompting strategy yields competitive performance in several settings, highlighting the difficulty for language 8 for computational cost. While tokens provide convenient and measurable notion of investment, they do not fully capture other practical costs (e.g., wall-clock latency, tool-calling overhead, verification cost, or memory/context usage). In addition, our difficulty supervision relies on coarse discrete levels and specific refusal protocol (e.g., outputting NA), which may not transfer directly to settings that require finer-grained or domain-specific abstention and calibration. More broadly, meta-cognition is much larger problem than allocating tokens for math solutions. Real-world LLM deployments increasingly resemble agents operating under budgets and constraints, where the model must plan across heterogeneous subtasks: writing and editing, code generation (Yang et al., 2024b,a) and debugging, tool use (search, database queries, execution) (Singh et al., 2025; Dong et al., 2025; Chen et al., 2025), and workflow automation. Extending ROI-Reasoning to these settings introduces additional challenges, including non-stationary and delayed rewards, stochastic tool outcomes, safety constraints, and evaluation that measures task success rather than answer exact-match. We view our formulation and results as step toward general-purpose, budgetaware meta-cognitive control, and leave broader agentic extensions and richer cost models to future work."
        },
        {
            "title": "9 Ethics",
            "content": "This work studies inference-time reasoning and computational budget allocation in large language models from an algorithmic and optimization perspective. The proposed methods do not involve human subjects, personal data, or sensitive information, and all experiments are conducted on publicly available benchmark datasets. We do not identify any significant ethical concerns specific to this work beyond those generally associated with large language models. The goal of improving budget-aware reasoning is to enable more efficient and controllable use of computation, and we encourage responsible deployment practices when such techniques are applied in realworld systems."
        },
        {
            "title": "References",
            "content": "Yifei Chen, Guanting Dong, and Zhicheng Dou. 2025. Toward effective tool-integrated reasoning via self-evolved preference learning. arXiv preprint arXiv:2509.23285. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, and 1 others. 2025. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. 2025. Tokenbudget-aware llm reasoning. In Findings of the Association for Computational Linguistics: ACL 2025, pages 2484224855. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. NeurIPS. Shijue Huang, Hongru Wang, Wanjun Zhong, Zhaochen Su, Jiazhan Feng, Bowen Cao, and Yi Fung. 2025. Adactrl: Towards adaptive and controllable reasoning via difficulty-aware budgeting. arXiv preprint arXiv:2505.18822. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Hans Kellerer, Ulrich Pferschy, and David Pisinger. 2004. The multiple-choice knapsack problem. In Knapsack problems, pages 317347. Springer. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199 22213. Junyan Li, Wenshuo Zhao, Yang Zhang, and Chuang Gan. 2025. Steering llm thinking with budget guidance. arXiv preprint arXiv:2506.13752. 9 Junhong Lin, Xinyue Zeng, Jie Zhu, Song Wang, Julian Shun, Jun Wu, and Dawei Zhou. 2025. Plan and budget: Effective and efficient test-time scaling on large language model reasoning. arXiv preprint arXiv:2505.16122. Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, and 1 others. 2025a. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. 2025b. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, and 1 others. 2023. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594. Jayanta Mandi, Peter Stuckey, Tias Guns, and 1 others. 2020. Smart predict-and-optimize for hard combinatorial optimization problems. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 16031610. Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev. 2021. Reinforcement learning for combinatorial optimization: survey. Computers & Operations Research, 134:105400. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2028620332. Maxime Mulamba, Jayanta Mandi, Michelangelo Diligenti, Michele Lombardi, Victor Bucarey, and Tias Guns. 2020. Contrastive losses and solution caching for predict-and-optimize. arXiv preprint arXiv:2011.05354. Aneesh Muppidi, Katrina Brown, and Rana Shahout. 2025. Predictive scheduling for efficient inferenceIn EStime reasoning in large language models. FoMo III: 3rd Workshop on Efficient Systems for Foundation Models. OpenAI. 2024. language models. learning-to-reason-with-llms/. blog. Learning to reason with large https://openai.com/index/ Technical Penghui Qi, Zichen Liu, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. 2025. Optimizing anytime reasoning via budget relative policy optimization. arXiv preprint arXiv:2505.13438. 10 Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, and 25 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652. Joykirat Singh, Raghav Magazine, Yash Pandya, and Akshay Nambi. 2025. Agentic reasoning and tool integration for llms via reinforcement learning. arXiv preprint arXiv:2505.01441. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, and 1 others. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, and 1 others. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Long Tran-Thanh, Yingce Xia, Tao Qin, and Nicholas Jennings. 2015. Efficient algorithms with performance guarantees for the stochastic multiple-choice knapsack problem. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Planand-solve prompting: Improving zero-shot chain-ofthought reasoning by large language models. arXiv preprint arXiv:2305.04091. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024a. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:50528 50652. Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, and 1 others. 2024b. If llm is the wizard, then code is the wand: survey on how code empowers large language models to serve as intelligent agents. arXiv preprint arXiv:2401.00812. Yunhao Yang and Andrew Whinston. 2023. survey on reinforcement learning for combinatorial optimization. In 2023 IEEE World Conference on Applied Intelligence and Computing (AIC), pages 131136. IEEE. Jiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, and Juanzi Li. 2025. Adaptthink: Reasoning modarXiv preprint els can learn when to think. arXiv:2505.13417. Yifan Zhang and Team Math-AI. 2025. American invitational mathematics examination (aime) 2025. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and 1 others. 2022. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625. Meta-Cognitive Fine-Tuning This appendix provides implementation details of Meta-Cognitive Fine-Tuning (MFT). The procedure is designed to enhance the models metacognitive capabilities through two stages: Tag Alignment and Refusal Learning. Training alternates between single-problem instances and short multi-problem sequences to progressively bridge atomic cost estimation and contextual decision making. Stage I: Tag Alignment The goal of Tag Alignment is to train the model to correctly predict difficulty levels using the <predicted_level> tag and to adhere to the standardized reasoning template. Single-Problem Adaptation. We first construct dataset of single-problem instances from GSM8K and MATH. Starting from the base model, we apply Rejection Sampling Fine-Tuning (RFT) and retain only samples that both produce correct final answers and strictly adhere to the required output format, including valid <predicted_level> tag. For retained samples, difficulty tags are calibrated based on empirical token consumption and difficulty statistics. The resulting model is denoted as M1-Q-v1. Sequential Adaptation. To expose the model to contextual budgeting, we construct three-problem sequences by concatenating GSM8K and MATH instances. Initializing from M1-Q-v1, we perform RFT and retain sequences where all three problems are solved correctly and the <predicted_level> tags follow the required format. This produces M3-Q-v1. Stage II: Refusal Learning Refusal Learning introduces explicit supervision for strategic abstention by allowing the model to output standardized NA token when problems are consistently unsolved. Single-Problem Refusal. We extend the dataset to include harder problems from AIME 20222024. Starting from M3-Q-v1, we perform RFT with sampled generations per problem. If none of the samples yield correct answer, the target output is labeled as: <predicted_level>Level-3</predicted_level> box{ NA } Otherwise, correct samples are retained and their difficulty tags are recalibrated. The resulting model is denoted as M1-Q-v2. Sequential Refusal. Finally, we construct threeproblem sequences drawn from GSM8K, MATH, and AIME. Initializing from M1-Q-v2, we perform RFT with samples per sequence. For fully correct sequences, difficulty tags are calibrated as before. For sequences containing persistent failures, the corresponding problem is labeled as: <predicted_level>Level-3</predicted_level> box{ NA } The final model, M3-Q-v2, serves as our MetaCognitive Fine-Tuned model."
        },
        {
            "title": "B Test Paper Construction",
            "content": "In this section, we describe how test papers are constructed from individual problems. Each test paper consists of exactly three mathematics problems and is treated as single query to the model. The instruction text specifies strict global token budget and standardized answer format; since different baselines use different instructions, we focus here on the construction of problem sets and their ordering. 11 Problem Sources. Each test paper is formed by sampling problems from three standard datasets with increasing expected difficulty: GSM8K (easy), MATH (medium), and AIME (hard). All problems are drawn from the official test splits of the corresponding datasets. For each dataset, we shuffle the problem pool once and then cycle through it sequentially to ensure balanced coverage across test papers. Paper Length and Composition. Every test paper contains exactly three problems. Problems are sampled according to predefined templates that specify the dataset source of each position in the sequence. This allows us to systematically control the relative ordering of problem difficulties while ensuring that each paper contains mixture of easy, medium, and hard problems. For each setting (Medium or Hard), we construct test set consisting of 5,000 test papers. Medium Test Papers. Medium papers are designed to interleave problems of varying difficulty. We define set of templates that mix GSM8K, MATH, and AIME problems across the three positions, for example: (MATH, GSM8K, MATH), (MATH, GSM8K, AIME), and (AIME, MATH, GSM8K). For each test paper, one template is selected uniformly at random, and problems are sampled accordingly. As result, Medium papers typically contain both easier and harder problems, without strong bias toward placing the most difficult problem at the beginning of the sequence. Hard Test Papers. Hard papers place more challenging problems earlier in the sequence, increasing the opportunity cost of early reasoning. To this end, we use templates that favor AIME and MATH problems in the first and second positions, such as (AIME, MATH, GSM8K) and (AIME, MATH, MATH). GSM8K problems rarely appear in the first position. This construction makes early decisions more critical under tight global budget, as allocating excessive computation to the first problem may prevent the model from solving later ones. Easy Variants and scoreeasy. For each test paper, we additionally construct an easy variant using the same set of three problems but different ordering. Specifically, problems are reordered from easiest to hardest according to the fixed priority GSM8K MATH AIME. This easy-to-hard ordering serves as reference sequence that is easier to solve under limited budget. We report the resulting performance as scoreeasy, which is further used to compute regret by comparing it with the score obtained under the original ordering. Importantly, the original and easy variants share identical problem content and differ only in order. Budgets. All test papers are evaluated under fixed global token budget. During training, the budget is set to 1024 tokens. At evaluation time, we consider both 1024-token setting and more restrictive 512-token setting. The latter regime strongly penalizes inefficient early reasoning and highlights the importance of selective skipping and early stopping."
        },
        {
            "title": "C Baselines",
            "content": "This appendix documents the baselines used in our experiments, with an emphasis on prompt-based controls under strict global token budget. Unless otherwise stated, all baselines share the same evaluation protocol and decoding configuration; they differ only in the mechanism used to elicit budget awareness (e.g., whether the model explicitly plans, whether it must execute sequentially, and how skipping is triggered). For reproducibility, we report the prompts verbatim. C.1 Simple and Explicit Prompt This baseline provides minimal, explicit instruction: solve the three problems sequentially under strict global budget, and output box{ NA } when choosing to give up. It evaluates the models implicit ability to trade off depth of reasoning against the opportunity cost of consuming budget on earlier questions, without any dedicated planning stage."
        },
        {
            "title": "Simple and Explicit Prompt",
            "content": "The following text contains three mathematics problems. Please solve them sequentially. Constraint: You have strict limit of 1024 tokens. Your goal is to answer as many questions correctly as possible within this limit. For each question, please follow these steps to answer: Provide step-by-step reasoning. You MUST wrap your final answer in boxed{} (e.g., boxed{42}). If you think the question is too difficult: Do NOT provide reasoning. Directly output boxed{NA}. Content: (Normal): [Reasoning] ... The final answer is boxed{...}. 12 Table 2: Per-problem accuracy and overall score under different token budgets. We report accuracy for each problem position (P1P3) and the resulting total score on Medium and Hard test papers."
        },
        {
            "title": "Hard",
            "content": "P1 P2 P"
        },
        {
            "title": "Score",
            "content": "P1 P2 P"
        },
        {
            "title": "Score",
            "content": "DeepSeek-V3.2 (685B) GPT-4o-mini 1024 512 1024 512 Qwen2.5-7B-Instruct Llama-3.1-8B-Instruct Qwen2.5-Math-7B-Instruct 1024 512 1024 512 1024 512 1024 512 Qwen2.5-Math-1.5B-Instruct 1024 512 1024 512 Qwen2.5-1.5B-Instruct Llama-3.2-3B-Instruct Qwen2.5-1.5B-Instruct + Plan-and-Solve Prompting 1024 + Least-to-Most Prompting + MFT + MFT + Greedy Knapsack + MFT + RARL (Ours) 512 1024 512 1024 512 1024 512 1024 512 0.60 0.45 0.28 0.48 0.29 0.11 0.44 0.54 0.43 0.38 0.23 0.04 0.26 0.27 0.21 0.24 0.20 0.10 0.50 0.41 0.16 0.39 0.12 0.00 0.45 0.50 0.32 0.39 0.22 0.03 0.20 0.12 0.09 0.19 0.10 0.04 0.45 0.39 0.21 0.35 0.13 0.01 0.24 0.21 0.15 0.21 0.10 0. 0.20 0.12 0.08 0.11 0.03 0.01 0.21 0.18 0.13 0.17 0.08 0.03 0.26 0.36 0.36 0.25 0.31 0.26 0.26 0.36 0.35 0.25 0.31 0.27 0.29 0.43 0.41 0.29 0.39 0.29 1.33 0.88 1.41 0.65 0.74 0.54 1.07 0.51 0.27 0.64 0.41 0.33 1.05 0.49 0.60 0.33 0.40 0.15 0.52 0.28 0.98 0.82 0.97 0.83 1.13 0.97 0.32 0.29 0.26 0.19 0.17 0.13 0.18 0.54 0.55 0.14 0.15 0.03 0.09 0.20 0.26 0.08 0.11 0.09 0.19 0.31 0.19 0.13 0.05 0.00 0.17 0.49 0.41 0.14 0.12 0.03 0.06 0.11 0.13 0.06 0.06 0.05 0.17 0.37 0.25 0.12 0.06 0.01 0.07 0.22 0.20 0.07 0.06 0. 0.07 0.10 0.11 0.03 0.01 0.01 0.07 0.15 0.16 0.05 0.05 0.03 0.09 0.35 0.51 0.10 0.33 0.38 0.09 0.35 0.52 0.08 0.34 0.41 0.10 0.41 0.61 0.11 0.39 0.43 0.87 0.49 1.27 0.32 0.55 0.28 0.69 0.18 1.07 0.29 0.30 0.17 0.79 0.19 0.49 0.15 0.28 0.05 0.38 0.13 0.95 0.81 0.96 0.83 1.12 0.93 (Give up): boxed{NA} - Ensure you strictly follow this format. C.2 Plan-and-Solve Prompt This baseline augments the above instruction with an explicit global meta-plan that must be produced before any detailed reasoning. The model is required to (i) scan all problems, (ii) estimate whether the budget suffices, and (iii) prioritize attempts based on qualitative ROI notion. In contrast to the simple prompt, the key difference is that budgeting decisions are made explicitly and upfront, rather than emerging implicitly during generation. Plan-and-Solve Prompt The following text contains three mathematics problems. You are taking timed exam with strict global limit of 1024 tokens. Your Goal: Maximize the total number of correct answers within this budget. (Quality > Quantity, but dont waste time on impossible tasks). Strategy: Plan-First, Then Solve Before answering any specific question, you MUST perform Global Meta-Cognitive Assessment: 1. Scan: Briefly assess the difficulty of all three problems. 2. Budget: Estimate if you have enough tokens to solve all of them. 1024 tokens is very tight! 3. Prioritize: Decide which problems yield the highest ROI (Return on Investment). Strategically Skip hard/lengthy problems to save tokens for easier ones. Response Format: Step 1: Output your plan. <Meta-Plan> Analysis: [Briefly assess P1, P2, P3 difficulty and estimated cost] Strategy: [e.g., Attempt P1, Skip P2 (too long), Attempt P3] </Meta-Plan> Step 2: Solve sequentially based on your strategy. For each question: 13 If your strategy is Attempt: Provide concise step-by-step reasoning. You MUST wrap your final answer in boxed{}. If your strategy is Skip: Do NOT generate any reasoning. Immediately output boxed{NA} to conserve tokens. C.3 Least-to-Most Prompt This baseline enforces strict sequential execution while still allowing limited look-ahead assessment. The model must answer problems in the given order, but is encouraged to skip an early highcost problem to preserve budget for later ones. Relative to Plan-and-Solve, this baseline stresses the irreversibility of sequential allocation: even with global awareness, the execution order is fixed. Least-to-Most Prompt The following text contains three mathematics problems. You are taking timed exam with strict global limit of 1024 tokens. Your Goal: Maximize the total number of correct answers within this budget. Constraint: Strict Sequential Execution You MUST answer the problems strictly in the order they are presented: Problem 1 Problem 2 Problem 3. You cannot change the order. However, you can (and should) strategically SKIP earlier problems if they are too costly, to save tokens for easier problems later. Step 1: Global ROI Assessment (Look-Ahead Planning) Before answering Problem 1, analyze ALL three problems to form plan: 1. Assess Cost/Reward: Which problems are Easy (Low Cost), Medium, or Hard (High Cost)? 2. Knapsack Decision: Given global tokens, if Problem 1 is very hard but Problem 2 and 3 are easy, the rational choice is to SKIP Problem 1 (Output NA) to ensure you have budget for P2 and P3. Step 2: Sequential Execution Execute your plan strictly in order (1, 2, 3). For each question: Check Plan & Budget: Based on your Step 1 plan and remaining tokens. Action: Attempt: Provide concise step-by-step reasoning. MUST wrap final answer in boxed{}. Skip: Do NOT provide reasoning. Immediately output boxed{NA} to conserve tokens. C.4 Greedy Knapsack Baseline Reference Baseline (Predict-then-Optimize). conventional approach to resource-constrained multi-task reasoning follows the predict-thenoptimize paradigm. Under this framework, the model first predicts, for each problem, its anticipated difficulty or computational cost, and then allocates fixed budget across problems by solving knapsack-style optimization problem. This idea has been explored in prior work as general strategy for computation allocation (Muppidi et al., 2025). Following this paradigm, we include Greedy Knapsack baseline as simple reference. Rather than learning budget-aware generation end-to-end, this baseline decouples prediction and execution: it uses predicted difficulty signals to guide post-hoc greedy selection of problems under fixed token budget. We treat this baseline as lightweight heuristic for cost-based allocation. Concretely, the model first generates candidate solutions for all problems in test paper. greedy knapsack-style procedure is then applied to retain only subset of problems whose estimated costs fit within the budget; rejected problems are replaced by boxed{NA}. Unlike our method, this baseline does not regulate reasoning length during generation and relies on external selection after generation. Cost Estimation. We interpret the discrete difficulty tags predicted by the MFT model as proxies for computational cost. For greedy sorting, each predicted tag Level-k is mapped to scalar expected token usage ˆc(k): ˆc(k) = 0+256 2 = 128 256+512 2 512+1048 2 = 384 = 768 if = 0, if = 1, if = 2. (6) These values correspond to the approximate centroids of the token ranges used in our difficulty categorization. If predicted tag is missing or malformed, we assign default cost of ˆc = 384. Problem Extraction and Reordering. Because the model processes all three problems as single sequence, we allow it to generate responses without hard cutoff and then parse the raw output into perproblem segments. We use the boxed{} delimiter to identify boundaries: the text segment ending at the i-th valid boxed closure is treated as the 14 solution to the i-th problem. This yields set of candidate solutions that can be reordered based on their estimated costs. Selection Algorithm. Let = {p1, . . . , pN } denote the problems in test paper and let be the global token budget (e.g., = 1024). The greedy selection procedure is as follows: 1. Estimate. For each problem pi, extract its predicted difficulty tag and assign an estimated cost ˆci. 2. Sort. Reorder the problems to obtain permutation π such that ˆcπ(1) ˆcπ(2) ˆcπ(N ). 3. Select. Iteratively include problems in sorted order, retaining pπ(j) if (cid:80)j k=1 ˆcπ(k) B. Case Study: Meta-cognitive Resource"
        },
        {
            "title": "Allocation",
            "content": "This case study illustrates the ROI-Reasonings ability to perform intrinsic difficulty estimation and strategic decision-making. Faced with complex geometry problem (Problem 1) and two simpler tasks (Problem 2 & 3), the model opts to skip the former to stay within the strict 512-token budget. Full Input Prompt (Constraint: 512 Tokens) The following text contains three mathematics problems. Please solve them sequentially. Constraint: You have strict limit of 512 tokens. Your goal is to answer as many questions correctly as possible within this limit. For each question, please follow these steps to answer: Step 1: Predict Length & Difficulty Before answering, estimate the token length and difficulty for every single problem: 4. Fill. For selected problems, the full generated solution is kept; for rejected problems, the output is replaced by boxed{NA}. 1. Level-0: < 256 tokens 2. Level-1: 256 - 512 tokens 3. Level-2: > 512 tokens Hard Constraint Enforcement. Finally, to ensure consistency with strict-budget evaluation, the reassembled output is tokenized. If its total length exceeds the budget B, hard truncation is applied. This penalizes cases where the actual generation length significantly exceeds the estimated cost ˆc(k), potentially resulting in incomplete solutions at the end of the sequence."
        },
        {
            "title": "D Extra Experiment",
            "content": "In this section, we conduct an additional analysis on per-problem accuracy. While the main experiments evaluate performance at the test-paper level under global budget constraint, it is also informative to examine how accuracy is distributed across individual problem positions within paper. Specifically, for each test paper, we record whether the model correctly solves the first, second, and third problem (denoted as P1, P2, and P3). This allows us to disentangle overall performance gains from positional effects induced by budget allocation and ordering. In particular, we are interested in whether improvements stem from better handling of later problems under tight budgets, rather than uniformly increasing accuracy across all positions. Table 2 reports per-problem accuracy under both Medium and Hard test papers, evaluated with 1024token and 512-token budgets. 15 4. Level-3: Too difficult (Skip reasoning, Answer NA) Step 2: Solve & Box - If Level 0-2: Provide step-by-step reasoning. You MUST wrap your final answer in boxed{} (e.g., boxed{42}). - If Level 3: Do NOT provide reasoning. Directly output boxed{NA}. Strict Format Requirements: - Start with: <predicted_level>Level-X</predicted_level> for each problem - Content: (Normal): [Reasoning] ... The final answer is boxed{...}. (Give up): boxed{NA} - Ensure you strictly follow this XML format. Problem 1: In ABC points and lie on AB so that AD < AE < AB, while points and lie on AC so that AF < AG < AC. Suppose AD = 4, DE = 16, EB = 8, AF = 13, = 52, and GC = 26. Let be the reflection of through , and let be the reflection of through E. The area of quadrilateral DEGF is 288. Find the area of heptagon AF BCEM . Problem 2: Compute arcsin (cid:0) 1 answer in radians. Problem 3: The rainstorm flooded the wetlands and washed Phineas Frog 200 yards away from his home in the swamp. To return home, he will need to hop and swim his way back. If he hops on land at speed of 20 yards per minute, and swims through water at speed of 10 yards per minute, how long will it take Phineas, in minutes, to return home if half of the distance is on land and the other half is in water? (cid:1) . Express your"
        },
        {
            "title": "Model Prediction",
            "content": "2 2 Solution to Problem 1: <predicted_level>Level-3</predicted_level> NA Solution to Problem 2: <predicted_level>Level-0</predicted_level> We need to compute arcsin (cid:0) 1 (cid:1). The value of arcsin (cid:0) 1 (cid:1) is π 6 radians. The answer is π 6 . Solution to Problem 3: <predicted_level>Level-0</predicted_level> Phineas needs to hop and swim back to his home. The distance to be covered is 200 yards. Half of the distance is 100 yards on land and half is 100 yards in water. The time to hop back on land is 100 20 = 5 minutes. The time to swim back in water is 100 10 = 10 minutes. The total time is 5 + 10 = 15 minutes. The answer is 15 ."
        },
        {
            "title": "Ground Truth",
            "content": "Problem 1: 588 Problem 2: π/6 Problem 3: 15 Observation. The model demonstrates an effective predict-then-reason behavior. Problem 1 is an AIME-level geometry problem that typically requires extensive coordinate geometry or area-ratio calculations, likely exceeding the remaining token budget. By correctly assigning Level-3, the model bypasses the complex reasoning and successfully solves the subsequent two problems within the 512token limit, achieving total score of 2."
        }
    ],
    "affiliations": [
        "Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China"
    ]
}