{
    "paper_title": "Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning",
    "authors": [
        "Yuting Li",
        "Lai Wei",
        "Kaipeng Zheng",
        "Jingyuan Huang",
        "Linghe Kong",
        "Lichao Sun",
        "Weiran Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the rapid progress of multimodal large language models (MLLMs), they have largely overlooked the importance of visual processing. In a simple yet revealing experiment, we interestingly find that language-only models, when provided with image captions, can achieve comparable or even better performance than MLLMs that consume raw visual inputs. This suggests that current MLLMs may generate accurate visual descriptions but fail to effectively integrate them during reasoning. Motivated by this, we propose a simple visual perturbation framework that enhances perceptual robustness without requiring algorithmic modifications or additional training data. Our approach introduces three targeted perturbations: distractor concatenation, dominance-preserving mixup, and random rotation, that can be easily integrated into existing post-training pipelines including SFT, DPO, and GRPO. Through extensive experiments across multiple datasets, we demonstrate consistent improvements in mathematical reasoning performance, with gains comparable to those achieved through algorithmic changes. Additionally, we achieve competitive performance among open-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual perturbation. Through comprehensive ablation studies, we analyze the effectiveness of different perturbation strategies, revealing that each perturbation type contributes uniquely to different aspects of visual reasoning. Our findings highlight the critical role of visual perturbation in multimodal mathematical reasoning: better reasoning begins with better seeing. Our code is available at https://github.com/YutingLi0606/Vision-Matters."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 6 3 7 9 0 . 6 0 5 2 : r Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning Yuting Li1 Lai Wei1,3 Kaipeng Zheng1,2 Jingyuan Huang1,4 Linghe Kong1 Lichao Sun5 Weiran Huang1,2,4, 1 School of Computer Science, Shanghai Jiao Tong University 3 Zhongguancun Academy 2 Shanghai Innovation Institute 4 State Key Laboratory of General Artificial Intelligence, BIGAI 5 Lehigh University"
        },
        {
            "title": "Abstract",
            "content": "Despite the rapid progress of multimodal large language models (MLLMs), they have largely overlooked the importance of visual processing. In simple yet revealing experiment, we interestingly find that language-only models, when provided with image captions, can achieve comparable or even better performance than MLLMs that consume raw visual inputs. This suggests that current MLLMs may generate accurate visual descriptions but fail to effectively integrate them during reasoning. Motivated by this, we propose simple visual perturbation framework that enhances perceptual robustness without requiring algorithmic modifications or additional training data. Our approach introduces three targeted perturbations: distractor concatenation, dominance-preserving mixup, and random rotation, that can be easily integrated into existing post-training pipelines including SFT, DPO, and GRPO. Through extensive experiments across multiple datasets, we demonstrate consistent improvements in mathematical reasoning performance, with gains comparable to those achieved through algorithmic changes. Additionally, we achieve competitive performance among open-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual perturbation. Through comprehensive ablation studies, we analyze the effectiveness of different perturbation strategies, revealing that each perturbation type contributes uniquely to different aspects of visual reasoning. Our findings highlight the critical role of visual perturbation in multimodal mathematical reasoning: better reasoning begins with better seeing. Our code is available at https://github.com/YutingLi0606/Vision-Matters."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in multimodal large language models (MLLMs) have demonstrated impressive capabilities in understanding and reasoning about visual information [22, 23, 42, 44, 59]. However, their performance on complex mathematical reasoning tasks remains suboptimal, particularly when visual elements play crucial role in problem-solving. While significant progress has been made in developing specialized datasets [6, 8, 57] and training approaches [5, 30, 46, 47, 53] for multimodal mathematical reasoning, there has been relatively little focus on how visual information is processed [14] and integrated during reasoning. Our work begins with simple yet revealing observation: language-only models with image captions can sometimes outperform multimodal models on mathematical reasoning tasks. On MathVision [40], the language model Qwen2.5-7B [42] achieves accuracy comparable to its multimodal counterpart Correspondence to Weiran Huang (weiran.huang@outlook.com). Qwen2.5-VL-7B. When given image captions, its accuracy improves, slightly outperforming the original multimodal model. This pattern holds at larger scales too: the language model QwQ32B-Preview [37] with captions reaches higher accuracy than the bigger multimodal model QvQ72B-Preview [36]. Based on this observation, we hypothesize that caption-augmented language model should establish natural lower bound for the performance of an ideal multimodal model with comparable language understanding capabilities, since image captions contain less information than the original images. When MLLMs fail to outperform such caption-based LLMs, it indicates they may not effectively use their visual capabilities for mathematical reasoning. While they can generate accurate visual representations, they struggle to integrate this information into their reasoning process. Motivated by this observation, we propose simple visual perturbation framework that enhances perceptual robustness without requiring architectural changes or additional training data. Our approach introduces three targeted perturbationsdistractor concatenation, dominance-preserving mixup, and In particular, distractor concatenation adds irrelevant visual elements to test random rotation. the models ability to focus on relevant information. Dominance-preserving mixup combines multiple images while maintaining the prominence of the primary content. Random rotation challenges the model to reason about geometrical properties regardless of orientation. These perturbations can be easily integrated into existing post-training pipelines including SFT [38], DPO [33], and GRPO [10]. Figure 1: Caption-augmented LLM can outperform MLLM on MathVision [40]. We conducted extensive experiments across multiple datasets and evaluated our approach on four benchmarks: MathVision [40], MathVista [27], MathVerse [56], and WeMath [32]. Despite its simplicity, our method achieves consistent improvements across various settings. We trained fifteen models across five datasets using three different training pipelines (SFT, DPO, and GRPO). The average performance across the four benchmarks improved consistently by approximately 2 percentage points compared to their respective baselines. Notably, on the GEOQA dataset [8], our approach combining GRPO with visual perturbation is comparable to current advanced models [5, 30, 39, 43] that use several times more training data or rely on algorithmic improvements. Through comprehensive ablation studies, we analyze the effectiveness of different perturbation strategies, revealing that each perturbation type contributes uniquely to different aspects of visual reasoning. By training Qwen2.5-VL-7B with our visual perturbation framework, we achieve competitive performance among open-source 7B RL-tuned models. Our experiments show that rotation-based perturbations significantly improve geometry problem performance by enhancing spatial reasoning capabilities. For scientific problems with complex visual elements, distractor concatenation proves most effective by strengthening the models ability to focus on relevant information. Dominancepreserving mixup particularly benefits problems requiring integration of multiple visual components. These findings demonstrate that visual perturbation plays critical role in multimodal mathematical reasoning - better reasoning fundamentally depends on better visual understanding. The key contributions of this work are as follows: We identify critical insight that caption-augmented LLMs can match or even surpass MLLMs on mathematical reasoning tasks, highlighting fundamental limitations in how current MLLMs process and integrate visual information; We propose simple yet effective visual perturbation framework that can be easily integrated into existing training pipelines, leading to consistent improvements in math reasoning performance across different datasets; We provide empirical evidence and analysis of how different perturbation strategies affect reasoning performance across various problem types, revealing insights into the specific visual processing challenges in multimodal mathematical reasoning."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal Mathematical Reasoning. Recently, the mathematical reasoning abilities of MLLMs have become central focus of research [6, 7, 11, 13, 15, 21, 50, 60]. In contrast to traditional LLM-based mathematical reasoning [28, 54], which primarily relies on text, multimodal approaches must also process and interpret visual inputs, significantly increasing the complexity of tasks such as geometric problem-solving and chart interpretation [2]. Recent work in this field has sought to refine visual mathematical inputs through specialized encoders [4], and MAVIS [57] introduced automated generation of math-centric visual data to produce aligned vision-language pairs with explicit reasoning steps. Another significant development focuses on synthesizing varied and large-scale training data. For instance, Math-LLaVA [34] offers the MathV360K dataset, which sorts images by complexity and augments them with relevant questions. Multimath [31] gathers high-quality reasoning data from K-12 textbooks and utilizes GPT-4 for both chain-of-thought data generation and validation. In addition, LLaVA-CoT-100k [48] and Mulberry-260k [51] have been developed to encourage stepby-step reasoning across diverse cross-modal patterns. These data-synthesis practices have gained traction in both academia and industry, given their demonstrated efficiency [16, 29, 35]. Unlike these approaches that primarily focus on synthesizing large scale of training data or constructing specialized encoders, our work tackles the visual processing limitations of existing MLLMs through simple perturbation strategies based on existing datasets, requiring neither newly collected training data nor architectural changes to enhance multimodal reasoning performance. Multimodal Data Augmentation. Data augmentation techniques aim to enhance both the quantity and quality of training data by introducing minor modifications to existing samples or generating synthetic data from raw inputs. Multimodal data augmentation extends these ideas to multi-input settings, particularly vision-language tasks. Notable methods include MixGen [14], RobustMixGen [18], XTRA [12], and LEMDA [25]. Among them, MixGen [14] is widely adopted approach that augments image-text pairs by mixing images while preserving their corresponding textual descriptions. Unlike traditional Mixup [55], which operates within single modality, MixGen combines visual content across different samples without altering the associated text. RobustMixGen [18] further improves upon MixGen by selecting image-text pairs based on semantic factors such as object and background classes, thereby enhancing the coherence and relevance of the augmented samples and demonstrating stronger robustness in multimodal scenarios. While our work aligns with the general philosophy of these augmentation methods, existing approaches do not specifically target the emerging class of multimodal reasoning tasks for MLLMs. In contrast, we design tailored visual perturbation strategies explicitly for such reasoning tasks and demonstrate their effectiveness when integrated with various MLLM alignment techniques, including SFT [38], DPO [33], and GRPO [9]. Recently, concurrent work Noisyrollout [24] propose simple yet effective data augmentation method that mixes trajectories from both clean and moderately distorted images during RL training."
        },
        {
            "title": "3 Method",
            "content": "In this section, we first present our key observation that language models with image captions can sometimes outperform multimodal models on mathematical reasoning tasks, suggesting ineffective visual information utilization. We then introduce our visual perturbation framework with three strategiesdistractor concatenation, dominance-preserving mixup, and random rotationdesigned to enhance perceptual robustness. Finally, we demonstrate how these perturbations can be seamlessly integrated into various post-training pipelines including SFT, DPO, and GRPO. 3.1 Observation Before introducing our proposed visual perturbation strategies, we conduct simple yet revealing experiment to examine how current MLLMs utilize visual information in mathematical reasoning. We observe an interesting pattern in Table 1: pure language models, when provided with image captions, can sometimes achieve comparable or even better performance than multimodal models that process raw visual inputs. On MathVision [41], the 7B language model Qwen2.5-7B [49] achieves score of 24.3, similar to its multimodal counterpart Qwen2.5-VL-7B [1] at 25.4. When given image captions generated by Qwen2.5-VL-7B, Qwen2.5-7Bs accuracy improves to 28.5, slightly outperforming the original multimodal model Qwen2.5-VL-7B. This pattern exists at larger scales as well: the 32B 3 Table 1: Performance of LLMs and MLLMs on MathVision, MathVista, MathVerse, and We-Math benchmarks. Star symbol () denotes that LLMs are prompted with image captions generated by Qwen-VL for each question. Models Size MathVision [41] MathVista [27] MathVerse [56] We-Math [32] Qwen2.5-7B [49] Qwen2.5-7B [49] QwQ-Preview [49] QwQ-Preview [49] Qwen2.5-VL-7B [1] QvQ-Preview [1] 7B 7B 32B 32B 7B 72B 24.3 28.5 37.7 42.7 25.4 35.6 26.5 56.7 30.7 61.6 65.4 69.2 28.0 41.5 34.5 54.9 42.7 52. 37.5 57.3 38.4 63.3 64.8 65.3 language reasoning model QwQ-Preview [37] with captions reaches score of 42.7, while the larger 72B multimodal reasoning model QvQ-Preview [36] achieves 35.6. Similar trends can be observed on another benchmark MathVerse [56]. Through this simple exploratory experiment, it suggests that current MLLMs might not effectively integrate their visual capabilities into mathematical reasoning. We hypothesize that captionaugmented language model establishes natural lower bound for the performance of an ideal visionlanguage model on visual reasoning tasks, under the assumption that both models possess comparable language understanding capabilities. Since image captions are compressed representations of visual content, they inherently contain less information than the original images. Thus, well-aligned and effective MLLM, which can directly access and process raw visual inputs, should in principle outperform or at least match language model that only relies on generated captions. When this expectation is not met, it suggests that the MLLM may be underutilizing visual information or that its vision-language alignment is suboptimal. This observation motivates our investigation into strategies that can improve how MLLMs leverage and integrate visual inputs for mathematical reasoning. 3.2 Visual Perturbation Strategies Building upon our earlier observation (see Section 3.1) that MLLMs often underutilize visual information in mathematical reasoning, we propose lightweight visual perturbation framework aimed at encouraging stronger visual grounding and improving perceptual robustness. Our method involves applying controlled perturbations to input images that preserve core semantics while introducing visual variations. These perturbations are designed to challenge the models ability to localize, extract, and reason over relevant visual information in the presence of noise, ambiguity, or structural shifts. In particular, we introduce three perturbation strategies at the image level, each targeting different aspect of visual perception and reasoning. During training or inference, one of the three perturbations is randomly applied to each image unless otherwise specified. Distractor Concatenation. Given an input image I, we horizontally concatenate randomly sampled, semantically irrelevant distractor image , forming [I; ]. This strategy challenges whether the model can localize and attend to the relevant subregion of the visual input while ignoring irrelevant content. It mimics real-world settings where important information may appear alongside clutter, noise, or unrelated visual elements. Robust models should learn to suppress spurious visual signals and focus on the region aligned with the textual question. Dominance-Preserving Mixup. Inspired by classic mixup [55], we combine the original image and distractor using skewed alpha-blending: Imix = λI + (1 λ)I , where λ [0.8, 0.95]. Unlike standard mixup, our formulation preserves the dominant visual features of the original image while injecting low-level noise from an unrelated scene. This encourages the model to learn more invariant and robust visual features, focusing on the dominant structures relevant for mathematical reasoning rather than overfitting to low-level image textures or noise patterns. Random Rotation. We randomly rotate the input image by small angle (e.g., 15) to simulate geometric transformations commonly encountered in real-world diagrams and figures. This perturbation is particularly valuable for geometry-centric problems, testing the models spatial invariance and 4 Figure 2: Our visual perturbation framework consists of three strategies: (1) distractor concatenation that horizontally combines the input image with random distractor, (2) dominance-preserving mixup that blends the input with distractor using skewed alpha values, and (3) random rotation that applies small angle rotations. During training, these perturbations are applied across multiple alignment pipelines including SFT, DPO, and GRPO to enhance the models perceptual robustness and reasoning consistency. its ability to parse rotated structures or symbols. Together, these perturbations introduce controlled variations in the input space of images without altering their semantic structures necessary to solve the problem. 3.3 Visual Perturbation at Training Stage Our visual perturbation strategies can be easily integrated in various post-training pipelines. Let denote the space of input images, and : represent our visual perturbation function. For each multimodal input = (I, q) where is the image and is the corresponding question in the training dataset D, we apply to to obtain the perturbed input P(x) = (P(I), q). The resulting perturbed input P(x) is then used to post-train the model πθ across multiple pipelines. SFT. For standard supervised fine-tuning (SFT), we collect the correct response o+ via rejection sampling [38]. The SFT objective with visual perturbations can be formulated as: JSFT(θ) = E(x,o+)D (cid:104) log πθ(o+P(x)) (cid:105) . DPO. For DPO [33] training, we select correct response as the positive sample o+ and an incorrect response as the negative sample by rejection sampling. The DPO objective with visual perturbations can be written as: JDPO(θ) = E(x,o+,o)D (cid:104) log σ(β log πθ(o+P(x)) πref(o+P(x)) β log πθ(oP(x)) πref(oP(x)) (cid:105) ) , where σ is the sigmoid function, and πref is the reference model. GRPO. The GRPO algorithm [9] is widely used in the post-training stage of multimodal LLMs. For each multimodal input = (I, q), we first apply visual perturbation to obtain P(x). GRPO then samples group of outputs = {o1, o2, . . . , oG} from the old policy πold and optimizes the policy model by maximizing: JGRPO(θ) = xD,{oi}G i=1πθold (OP(x)) 1 (cid:88) i=1 1 oi (cid:40) oi (cid:88) (cid:34) γi,t(θ) ˆAi,t, clip (γi,t(θ), 1 ϵ, 1 + ϵ) ˆAi,t (cid:35) min (cid:104) πθπref (cid:105) (cid:41) , βDKL t=1 where γi,t(θ) = πθ(oi,tP(x),oi,<t) πθold (oi,tP(x),oi,<t) , πref represents the reference model, and the term DKL introduces KL divergence constraint. The advantage estimate ˆAi is computed using group-normalized rewards {r1, r2, . . . , rG} for the responses in set O: ˆAi = rimean({r1,r2,...,rG}) . std({r1,r2,...,rG})"
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we present comprehensive experiments to evaluate the effectiveness of our proposed visual perturbation approach for enhancing mathematical reasoning in multimodal large language models. We first describe our experimental setup, including implementation details and evaluation benchmarks. Then, we present our main results comparing different training methods with and without visual perturbations. We also conduct ablation studies to analyze the impact of different perturbation types and their combinations. Finally, we provide qualitative examples to illustrate how visual perturbations help models overcome visual challenges in mathematical reasoning tasks. 4.1 Experimental Setup Implementation Details. We conduct experiments using Qwen2.5-VL-7B-Instruct [1] as our base model. For SFT and DPO training, we adopt the MS-Swift [58] framework, while for GRPO training we use the EasyR1 [52] framework. For SFT and DPO, we first perform rejection sampling by generating 16 responses from Qwen2.5-VL-7B-Instruct for each instruction. The responses are evaluated for correctness by comparing the extracted answers with ground truth using Qwen2.5-32BInstruct as the evaluator. For SFT, we select the longest correct response as the positive sample, training for 3 epochs with learning rate of 1e-4 and weight decay of 0.1. For DPO, we choose both the longest correct response as the positive sample and the shortest incorrect response as the negative sample, training for 1 epoch with learning rate of 5e-5, weight decay of 0.1, and warmup ratio of 0.05. For GRPO training, we follow the default hyperparameters in EasyR1, setting training episodes to 15, using AdamW optimizer with learning rate of 1 106, weight decay of 1 102, and gradient clipping at maximum norm of 1.0. We set the number of rollouts per episode to 5 for GRPO training. The vision tower of Qwen2.5-VL-7B is fine-tuned without freezing, and the GRPO objective incorporates KL divergence penalty with coefficient of 0.01 to stabilize training. During training, we adopt simple accuracy-based reward function that assigns +1 for correct final answers and 0 for incorrect ones. Evaluation Benchmarks. We evaluate the MLLMs on several multimodal mathematical reasoning benchmarks: MathVision [40] is challenging benchmark containing 3040 mathematical problems with visual contexts from real-world math competitions across 12 grades. It covers 16 subjects over 5 difficulty levels, including specialized topics like Analytic Geometry, Combinatorial Geometry, and Topology. MathVista [27] is comprehensive benchmark for evaluating mathematical reasoning in visual contexts. It contains 1000 questions featuring diverse problem types including geometry, charts, and tables. MathVerse [56] is an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs. The test set contains 3940 multi-subject math problems with diagrams from publicly available sources, focusing on Plane Geometry and Solid Geometry. We-Math [32] meticulously collect and categorize 1740 visual math problems in the test set, spanning 67 hierarchical knowledge concepts and 5 layers of knowledge granularity. For all benchmarks, we prompt the models to place their final answers within designated box format. We then employ Qwen2.5-32B-Instruct [49] to evaluate answer correctness by comparing the extracted responses with ground truth answers, which often contain complex mathematical expressions. Note that our reported benchmark scores may differ from those in the original papers due to variations in evaluation protocols. 4.2 Main Results Effectiveness Across Training Pipelines. Table 2 demonstrates the effectiveness of our visual perturbation framework across five diverse datasets (Geometry3K [26], TQA [17], GeoQA [2], 6 Table 2: Performance comparison of different training methods (SFT, DPO, GRPO) with and without visual perturbations (VP) across various training datasets. Results are evaluated on MathVision, MathVista, MathVerse, and We-Math benchmarks. All values represent accuracy percentages (%). Model and Methods Training Data MathVision MathVista MathVerse We-Math Average Qwen2.5-VL-7B SFT SFT + VP DPO DPO + VP GRPO GRPO + VP SFT SFT + VP DPO DPO + VP GRPO GRPO + VP SFT SFT + VP DPO DPO + VP GRPO GRPO + VP SFT SFT + VP DPO DPO + VP GRPO GRPO + VP SFT SFT + VP DPO DPO + VP GRPO GRPO + VP Geometry3K Geometry3K Geometry3K Geometry3K Geometry3K Geometry3K TQA TQA TQA TQA TQA TQA GeoQA GeoQA GeoQA GeoQA GeoQA GeoQA Math8K Math8K Math8K Math8K Math8K Math8K M3CoT M3CoT M3CoT M3CoT M3CoT M3CoT 25.4 25.9 25.9 27.8 27.2 28.3 28. 24.9 26.3 26.2 27.5 25.5 25.9 25.7 26.1 26.7 27.8 27.5 29.0 25.7 25.5 27.1 27.6 27.0 26.8 24.8 25.4 26.7 27.9 26.8 26.8 65.4 65.9 66.5 64.2 65.2 69.3 71. 67.1 67.2 69.7 71.1 68.9 70.7 65.4 67.7 67.0 69.4 69.6 71.0 65.7 66.3 64.1 67.3 65.0 66.9 64.4 66.4 67.1 69.7 67.8 68.2 42.7 43.7 44.9 44.8 46.2 46.4 46. 43.5 44.1 46.3 45.3 47.0 46.7 44.7 45.9 46.7 46.5 47.3 47.9 44.2 45.1 46.5 47.1 45.2 45.6 43.4 44.9 45.9 46.5 44.5 47.2 64.8 64.9 65.4 62.6 65.2 68.9 70. 64.8 65.6 63.5 64.7 63.6 63.7 65.1 65.3 64.1 66.1 68.7 69.8 64.8 65.0 63.3 65.5 65.9 67.1 63.2 63.9 65.2 66.5 64.4 63.8 49.6 50.1 50.7 (1.2%) 49.9 51.0 (2.2%) 53.2 54.2 (1.9%) 50.1 50.8 (1.4%) 51.4 52.2 (1.6%) 51.3 51.8 (1.0%) 50.2 51.3 (2.2%) 51.1 52.5 (2.7%) 53.3 54.4 (2.1%) 50.1 50.5 (0.8%) 50.3 51.9 (3.2%) 50.8 51.6 (1.6%) 49.0 50.2 (2.4%) 51.2 51.7 (1.0%) 50.9 51.5 (1.2%) Math8K [19], M3CoT [3]) using various training pipelines. key finding is that, despite the simplicity of our approach, visual perturbations yield performance gains comparable to algorithmic improvements - for instance, the enhancement from DPO to DPO+VP (1.0%-3.2% improvement) is comparable to the improvement from SFT to DPO. Across fifteen models trained on five datasets using three different training pipelines (SFT, DPO, and GRPO), we observe consistent performance improvements. Visual perturbations enhance SFT performance by 0.8%-2.4%, DPO by 1.0%-3.2%, and GRPO by 1.0%-2.1% across different datasets. On Geometry3K, GRPO+VP achieves the highest average score of 54.2% (a 1.9% improvement over GRPO), with significant gains on MathVista (71.4%, 2.1% increase over baseline). For GeoQA, GRPO+VP reaches 54.4% average accuracy (a 2.1% improvement), with notable enhancements on MathVision (29.0%) and We-Math (69.8%). The Math8K dataset shows DPO+VP outperforming baseline DPO by 3.2%, with particularly strong gains on MathVista (67.3%, 5.0% relative increase). These consistent improvements across different training methods and datasets validate the effectiveness of our visual perturbation framework in enhancing multimodal mathematical reasoning capabilities. These results strongly support our central thesis that better reasoning begins with better seeing, as improved visual processing directly translates to enhanced mathematical reasoning performance. Table 3: Performance of visual perturbations (VP) on advanced models across benchmarks. Results show consistent improvements when applying our visual perturbation framework (VP) to different base models. Models Training Data MathVision MathVista MathVerse WeMath Average MM-eureka-Qwen-7B [30] + VP ThinkLite-7B [43] + VP MMR1-7B [20] + VP MMK12-16k MMK12-16k ThinkLite-hard-11k ThinkLite-hard-11k MMR1-Math-7k MMR1-Math-7k Qwen2.5-VL-7B (Ours) + VP GEOQA-8k GEOQA-8k 28.0 29. 27.3 29.0 32.3 31.2 27.5 29.0 70.3 71.2 72.2 71.8 71.2 72. 69.6 71.0 50.0 49.2 48.8 49.9 47.9 50.4 47.3 47.9 65.3 66. 66.6 66.8 70.3 69.4 68.7 69.8 53.4 54.0 (1.1%) 53.8 54.4 (1.1%) 55.4 55.8 (0.7%) 53.3 54.4 (2.1%) Table 4: Effective rank and Diff-eRank [45] of our best model before and after our visual perturbation framework (VP) on GEOQA-8k [2] across four multimodal reasoning benchmarks. Effective Rank MathVision MathVerse MathVista We-Math Qwen2.5-VL-7B [1] Qwen2.5-VL-7B + VP Difference of Effective Rank (Diff-eRank [45]) 73.6 85.7 +12. 62.4 77.6 +15.2 63.9 68.4 +4.5 60.1 71.0 +10.9 Complementary to Advanced Models. We investigate whether visual perturbations can improve performance when applied to more advanced base models, as shown in Table 3. Our experiments with several well-trained models reveal that applying VP to the same training datasets they were originally trained on and continuing training yields consistent performance gains without requiring additional data. For MM-eureka-Qwen-7B [30], which was trained on the comprehensive MMK12 [30] dataset (15.6K samples), applying VP during continued training improves performance by 3.9% on MathVision (from 28.0% to 29.1%) and by 1.3% on MathVista (from 70.3% to 71.2%), with an overall improvement of 1.1 percentage points on average. Similarly, ThinkLite-7B [43], which was trained directly on the ThinkLite-hard-11K [43] dataset, shows significant gains when we apply VP and continue training, resulting in improvements of 6.2% on MathVision and 2.3% on MathVerse, for 1.1 percentage point increase in average performance. The most substantial improvements are observed with MMR1-7B [20], which was trained on the MMR1-Math-RL-7K [20] dataset. When we apply VP to this same dataset and continue training, performance increases by 0.7 percentage points on average, with notable gains of 1.1% on MathVista and 5.2% on MathVerse. Notably, on the GEOQA dataset [8], our approach combining GRPO with visual perturbation achieves performance comparable to current advanced models [5, 30, 39, 43]. In addition, we also calculate the effective rank and the difference (eRank) [45] for our best model before and after visual perturbation in Table 4. On fix-sized model, the value of effective rank usually correlates with the amount of knowledge the model comprehends. After post-training (GRPO), extra knowledge is injected into the model, which leads to consistent increase in effective rank on different benchmarks. These results demonstrate that our approach provides natural way to enhance even advanced base models by introducing controlled visual variations during continued training, without requiring any additional training examples or algorithmic modifications. 4.3 Ablation Studies We conduct comprehensive analysis of different perturbation strategies to understand their impact on mathematical reasoning performance on GEOQA [2] with DPO [33] training. To systematically evaluate this impact, we categorize perturbations into two distinct types: (1) those that preserve original image information while adding slight perturbations or noise, and (2) those that deliberately degrade original image information to test MLLMs dependence on visual details. Table 6 shows that our three proposed perturbations (distractor concatenation, dominance-preserving mixup, and random rotation) all preserve essential visual information while introducing controlled variations. Specifically, distractor concatenation improves the average score by 0.3 percentage points, dominance-preserving mixup achieves 0.6 percentage point improvement, and random 8 Table 5: Performance comparison of different perturbation types with SFT training across mathematical benchmarks. All models are trained on the GeoQA dataset. Perturbation Type MathVision MathVista MathVerse WeMath Average None (Baseline) Gaussian noise Gaussian Blur Random Cropping Distractor Concatenation Dominance-Preserving Mixup Random Rotation Ours 25.7 26.2 24.8 25.2 27.2 25.2 26.6 26.1 65.4 62.2 60.8 64.7 64.5 65.0 65.5 67.7 44.7 43.1 40.5 45.8 45.2 47.2 45.7 45.9 65.1 66.8 63.4 64.6 65.8 66.2 64.0 65.3 50.2 49.6 (1.2%) 47.4 (5.7%) 50.1 (0.03%) 50.7 (0.9%) 50.9 (1.4%) 50.5 (0.5%) 51.3 (2.2%) Table 6: Performance comparison of different perturbation types on GEOQA with DPO training across mathematical benchmarks. Perturbation Type MathVision MathVista MathVerse WeMath Average None (Baseline) Gaussian noise Gaussian Blur Random Cropping Distractor Concatenation Dominance-Preserving Mixup Random Rotation Ours 26.7 26.8 24.0 24.5 27.6 26.4 26.9 27. 67.0 65.2 52.8 63.7 66.5 66.4 64.5 69.4 46.7 42.1 31.5 41.8 46.2 47.7 45.7 46.5 64.1 66.1 53.4 60.6 65.0 66.2 65.3 66.1 51.1 50.1 (2.0%) 40.4 (21.0%) 47.7 (6.7%) 51.3 (0.4%) 51.7 (1.2%) 50.6 (1.0%) 52.5 (2.7%) rotation shows 0.2 percentage point gain. In contrast, perturbations that significantly degrade visual informationsuch as Gaussian noise with std=300, Gaussian blur with kernel size=50, and random cropping of 50% of the original imagelead to performance drops of 0.6-1.0 percentage points. As shown in the table, Gaussian blur has the most severe negative impact with dramatic 21.0% decrease in average performance, while random cropping also significantly hurts performance with 6.7% drop. Interestingly, we observe that even with some degree of information degradation, performance still improves on certain benchmarks, suggesting varying levels of visual information dependency across different reasoning tasks. The combination of our three information-preserving perturbations achieves the best overall performance with 1.4 percentage point improvement in average score, demonstrating their complementary nature. Its worth noting that we only explored several simple and easy-to-implement visual perturbation methods. Future work could further investigate more complex or semantically meaningful perturbation strategies. 4.4 Qualitative Analysis Our analysis reveals interesting patterns in how different visual perturbations affect reasoning across various problem types. We extracted four major problem categories from public benchmarksgeometry (1,245), counting (1,183), table (956), and science (1,810)and analyzed the impact of each perturbation strategy. We note that our analysis focuses only on these major categories and does not cover all problem types present in the datasets, such as algebraic expressions, word problems, and other specialized mathematical tasks. Our detailed analysis reveals that different problem categories benefit from specific perturbation types, while some perturbations may actually harm performance on certain problems. Geometry problems show the greatest improvement with rotation-based perturbations (428/1,245443/1,245, +3.5%), as these challenge the model to reason about spatial relationships regardless of orientation. However, rotation degrades performance on other problem types, particularly table problems (402/956396/956, -1.5%) and counting problems (379/1,183375/1,183, -1.1%). For all problem categories, distractor concatenation proves consistently effective, with the strongest gains in science problems (724/1,810745/1,810, +2.9%) and counting problems (379/1,183389/1,183, +2.6%), as it trains the model to identify and focus on relevant information while filtering out distractions. Dominance-preserving mixup shows benefits for geometry (428/1,245435/1,245, +1.6%), counting (379/1,183385/1,183, +1.6%), and science problems (724/1,810739/1,810, +2.1%), but hurts 9 Table 7: Performance comparison of different perturbation types with GRPO training across mathematical benchmarks. All models are trained on the GeoQA [2]dataset. Perturbation Type MathVision MathVista MathVerse WeMath Average None (Baseline) Gaussian noise Gaussian Blur Random Cropping Distractor Concatenation Dominance-Preserving Mixup Random Rotation Ours 27.5 27.9 27.7 26.7 28.5 27.6 27.7 29.0 69.6 71.4 68.8 68.8 70.7 71.8 70.4 71.0 47.3 46.8 46.0 46.1 48.1 45.7 46.7 47.9 68.7 69.4 66.6 67.7 69.1 69.3 68.5 69. 53.3 53.9 (1.0%) 52.3 (2.0%) 52.3 (2.0%) 54.1 (1.5%) 53.6 (0.5%) 53.3 (0.0%) 54.4 (2.1%) Table 8: Performance change with different perturbation strategies across problem types extracted from all benchmarks. Values show accuracy before/after applying perturbation and percentage change. Perturbation Type Geometry Counting Table Science Baseline Distractor Concatenation Dominance-Preserving Mixup Random Rotation 428/1245 436/1245 (1.9%) 435/1245 (1.6%) 443/1245 (3.5%) 379/1183 389/1183 (2.6%) 385/1183 (1.6%) 375/1183 (1.1%) 402/956 408/956 (1.5%) 398/956 (1.0%) 396/956 (1.5%) 724/1810 745/1810 (2.9%) 739/1810 (2.1%) 720/1810 (0.6%) performance on table problems (402/956398/956, -1.0%). These findings provide valuable insights into how visual information is processed during mathematical reasoning, suggesting that the bottleneck in multimodal math reasoning lies not just in alignment, but in visual attention, structure, and clarity."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work we present simple yet effective visual perturbation framework for enhancing multimodal mathematical reasoning. Our approach introduces three targeted perturbations: distractor concatenation, dominance-preserving mixup, and random rotation. These techniques improve perceptual robustness without requiring architectural changes or additional training data. Through extensive experiments we show consistent performance gains across multiple datasets with improvements comparable to those achieved through algorithmic modifications. Our findings reveal fundamental insight: better reasoning begins with better seeing. We demonstrate that the bottleneck in current MLLMs lies not only in cross-modal alignment but also in how visual information is perceived, attended to, and structured during reasoning. Our main contribution is providing the community with new perspective on multimodal reasoning. One that shifts focus away from language-centric improvements and toward the critical role of visual processing. We hope this work inspires future research to place greater emphasis on the image side of the pipeline and to develop more effective visual understanding techniques in multimodal systems. Looking forward we encourage researchers to explore wider variety of visual perturbations. And to design task-specific transformations that better capture the structure of visual reasoning problems."
        },
        {
            "title": "Acknowledgement",
            "content": "This project is supported by the National Natural Science Foundation of China (No. 62406192), Opening Project of the State Key Laboratory of General Artificial Intelligence (No. SKLAGI2024OP12), Tencent WeChat Rhino-Bird Focused Research Program, and Doubao LLM Fund."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 10 [2] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 513523, 2021. [3] Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. M3cot: novel benchmark for multi-domain multi-step multi-modal chain-of-thought. arXiv preprint arXiv:2405.16473, 2024. [4] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. [5] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement. arXiv preprint arXiv:2503.17352, 2025. [6] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. arXiv preprint arXiv:2411.14432, 2024. [7] Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql empowered by large language models: benchmark evaluation. arXiv preprint arXiv:2308.15363, 2023. [8] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. [9] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [11] Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, and Xiang Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. arXiv preprint arXiv:2412.05237, 2024. [12] Shir Gur, Natalia Neverova, Chris Stauffer, Ser-Nam Lim, Douwe Kiela, and Austin Reiter. Cross-modal retrieval augmentation for multi-modal classification. arXiv preprint arXiv:2104.08108, 2021. [13] Xiaotian Han, Yiren Jian, Xuefeng Hu, Haogeng Liu, Yiqi Wang, Qihang Fan, Yuang Ai, Huaibo Huang, Ran He, Zhenheng Yang, et al. Infimm-webmath-40b: Advancing multimodal pre-training for enhanced mathematical reasoning. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. [14] Xiaoshuai Hao, Yi Zhu, Srikar Appalaraju, Aston Zhang, Wanqian Zhang, Bo Li, and Mu Li. Mixgen: new multi-modal data augmentation. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 379389, 2023. [15] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024. [16] Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, and Weizhu Chen. Key-pointdriven data synthesis with its enhancement on mathematical reasoning. arXiv preprint arXiv:2403.02333, 2024. [17] Daesik Kim, Seonhoon Kim, and Nojun Kwak. Textbook question answering with multi-modal context graph understanding and self-supervised open-set comprehension. arXiv preprint arXiv:1811.00232, 2018. [18] Sunwoo Kim, Hun Im, Woojun Lee, Seonggye Lee, and Pilsung Kang. Robustmixgen: Data augmentation for enhancing robustness of visuallanguage models in the presence of distribution shift. Neurocomputing, 619:129167, 2025. [19] EvolvingLMMs Lab. Multimodal open r1, 2025. URL https://github.com/EvolvingLMMs-Lab/ open-r1-multimodal. Accessed: 2025-02-28. [20] Sicong Leng. Mmr1: Advancing the frontiers of multimodal reasoning. https://github.com/ LengSicong/MMR1, 2025. 11 [21] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [23] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. [24] Xiangyan Liu, Jinjie Ni, Zijian Wu, Chao Du, Longxu Dou, Haonan Wang, Tianyu Pang, and Michael Qizhe Shieh. Noisyrollout: Reinforcing visual reasoning with data augmentation. arXiv preprint arXiv:2504.13055, 2025. [25] Liu, Tang, Shi, Zhang, Li, Shrivastava, and AG Wilson. Learning multimodal data augmentation in feature space. arxiv, 2022. [26] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Intergps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. [27] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [28] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. [29] Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, and Mohammad Rastegari. OpenELM: An Efficient Language Model Family with Open Training and Inference Framework. arXiv.org, April 2024. URL https://arxiv.org/abs/2404.14619v1. [30] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning, 2025. URL https://github.com/ModalMinds/MM-EUREKA. [31] Shuai Peng, Di Fu, Liangcai Gao, Xiuqin Zhong, Hongguang Fu, and Zhi Tang. Multimath: Bridging visual and mathematical reasoning for large language models. arXiv preprint arXiv:2409.00147, 2024. [32] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. [33] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. [34] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024. [35] Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Durrett. To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning. arXiv preprint arXiv:2409.12183, 2024. [36] Qwen Team. Qvq: To see the world with wisdom, December 2024. URL https://qwenlm.github.io/ blog/qvq-72b-preview/. [37] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. URL https: //qwenlm.github.io/blog/qwq-32b-preview/. [38] Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving. Advances in Neural Information Processing Systems, 37:78217846, 2024. 12 [39] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. [40] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2025. [41] Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, 2024. [42] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [43] Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement. arXiv preprint arXiv:2504.07934, 2025. [44] Lai Wei, Zihao Jiang, Weiran Huang, and Lichao Sun. Instructiongpt-4: 200-instruction paradigm for fine-tuning minigpt-4. arXiv preprint arXiv:2308.12067, 2023. [45] Lai Wei, Zhiquan Tan, Chenghai Li, Jindong Wang, and Weiran Huang. Diff-erank: novel rank-based metric for evaluating large language models. arXiv preprint arXiv:2401.17139, 2024. [46] Lai Wei, Yuting Li, Chen Wang, Yue Wang, Linghe Kong, Weiran Huang, and Lichao Sun. Unsupervised post-training for multi-modal llm reasoning via grpo. arXiv preprint arXiv:2505.22453, 2025. [47] Lai Wei, Yuting Li, Kaipeng Zheng, Chen Wang, Yue Wang, Linghe Kong, Lichao Sun, and Weiran Huang. Advancing multimodal reasoning via reinforcement learning with cold start. arXiv preprint arXiv:2505.22334, 2025. [48] Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step. CoRR, abs/2411.10440, 2024. [49] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [50] Zhen Yang, Jinhao Chen, Zhengxiao Du, Wenmeng Yu, Weihan Wang, Wenyi Hong, Zhihuan Jiang, Bin Xu, Yuxiao Dong, and Jie Tang. Mathglm-vision: Solving mathematical problems with multi-modal large language model. arXiv preprint arXiv:2409.13729, 2024. [51] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. [52] Zheng Yaowei, Lu Junting, Wang Shenzhi, Feng Zhangchi, Kuang Dongdong, and Xiong Yuwen. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/hiyouga/EasyR1, 2025. [53] Peng Yingzhe, Zhang Gongrui, Zhang Miaosen, You Zhiyuan, Liu Jie, Zhu Qipeng, Yang Kai, Xu Xingzhong, Geng Xin, and Yang Xu. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl, 2025. [54] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. [55] Hongyi Zhang, Moustapha Cisse, Yann Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. [56] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [57] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Ziyu Guo, Shicheng Li, Yichi Zhang, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, et al. Mavis: Mathematical visual instruction tuning with an automatic data engine. arXiv preprint arXiv:2407.08739, 2024. 13 [58] Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou, and Yingda Chen. Swift:a scalable lightweight infrastructure for fine-tuning, 2024. URL https://arxiv.org/abs/2408.05517. [59] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [60] Wenwen Zhuang, Xin Huang, Xiantao Zhang, and Jin Zeng. Math-puma: Progressive upward multimodal alignment to enhance mathematical reasoning. arXiv preprint arXiv:2408.08640, 2024."
        }
    ],
    "affiliations": [
        "Lehigh University",
        "School of Computer Science, Shanghai Jiao Tong University",
        "Shanghai Innovation Institute",
        "State Key Laboratory of General Artificial Intelligence, BIGAI",
        "Zhongguancun Academy"
    ]
}