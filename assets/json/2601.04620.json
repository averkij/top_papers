{
    "paper_title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
    "authors": [
        "Di Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \\textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \\textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development."
        },
        {
            "title": "Start",
            "content": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering Di Zhang Fudan University di.zhang@ustc.edu 6 2 0 2 8 ] A . [ 1 0 2 6 4 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hardto-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into regressionaware release pipeline. We introduce AgentDevel, release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes passfail regressions and failpass fixes as first-class evidence. Unlike populationbased search or in-agent self-refinement, AgentDevel maintains single canonical version line and emphasizes non-regression as primary objective. Experiments on executionheavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides practical development discipline for building, debugging, and releasing LLM agents as software development."
        },
        {
            "title": "Introduction",
            "content": "Large language model (LLM)based agents are no longer just chatbots. Today, many agents can take actions in the world: they browse websites, 1 call external tools and APIs, write code, and solve tasks that require multiple steps and decisions. For agents deployed in production workflows, unreliable improvement can be more damaging than no improvement at all. As these agents move closer to real applications, we face very practical question: how do we improve an agent reliably over time when it repeatedly fails? natural idea is to let the agent improve itself. Recent work follows this direction in several ways. Some approaches ask the agent to reflect on feedback and store reflections in memory so it can do better next time (Reflexion (Shinn et al., 2023a)). Others iterate loop where the model critiques its own output and rewrites it (Self-Refine (Madaan et al., 2023)). Still others treat prompts as objects to be searched or evolved (PromptBreeder (Fernando et al., 2023b)). These methods can indeed raise benchmark scores, but they often come with familiar frustration for anyone who has built real systems: improvements can feel brittle and difficult to control. After change, it may be unclear whether we introduced regression, whether the improvement can be reproduced, or why certain failures suddenly appear. In other words, we may get better averages while losing confidence in the systems stability. This paper argues that the root issue is partly matter of framing. Many self-improvement methods treat improvement as something that lives inside the agentas if the agent should become self-evolving organism. We take different view: in real software development, systems rarely improve because the program reflects. They improve because developers build an external workflow around them: we collect logs, run tests, diagnose failures, and release new versions only after they pass checks. If you have used continuous integration (CI), you already know key lesson: average performance is not enough. What often matters most is how individual test cases change across versions. In particular, we pay close attention to passfail (PF) flips (a previously working case now breaks, i.e., regression) and failpass (FP) flips (a previously failing case now works, i.e., fix). This flip view gives us concrete and intuitive way to think about agent improvement: we should improve an agent like we improve softwareby controlling regressions and making progress visible and auditable. Based on this thesis, we propose AgentDevel, factory-style development pipeline that reframes agent improvement as release engineering. AgentDevel maintains single canonical agent line, rather than creating many competing variants. Each iteration produces exactly one release candidate (RC); only if the RC passes acceptance checks is it promoted to the next official version. To start an iteration, we run the current agent on development set (TrainSet) and record structured execution traces: what actions the agent took, which tools it called, what it observed, what errors occurred, and what final output it produced. When available, we also run programmatic scorersnon-LLM checks such as unit tests, schema validation, and format checkingto obtain deterministic pass/fail signals. Next, we introduce an independent LLM critic to provide higher-level, human-like quality characterization. The critic is intentionally designed to be implementation-blind: it does not see the agents internal design, which we refer to as the agents blueprintthe agents prompt, code, and tooling internals (i.e., how it is constructed). Instead, the critic only receives (i) the rubric (what counts as correct), (ii) the execution traces, and (iii) optionally the programmatic scoring results. Importantly, the critic does not perform causal attribution and does not propose repairs. Its job is simply to describe what went wrong at the surface level and to group failures into symptom-like categories such as missing required step, wrong order of actions, or invalid tool arguments. The set of symptom categories is not fixed; it can evolve over time as new failure appearances emerge. With traces and symptom descriptions in hand, AgentDevel then generates and executes diagnostic scripts (e.g., Python) that summarize dominant failure appearances, typical triggering conditions, representative examples, and how frequently each issue occurs. These scripts are regenerated each iteration, using the previous iterations scripts as soft references, which enables bootstrapped diagnosis process rather than rigid template system. Based on this diagnosis, AgentDevel synthesizes one RC that may modify prompts, code, or tool wrappers in the blueprint. The RC is then evaluated on the same TrainSet under flip-centered gate: instead of focusing only on aggregate metrics, we emphasize example-level flips, especially PF (regressions) and FP (fixes). In release engineering terms, PF cases are the most alarmingbecause they correspond to breaking something that used to work. RCs are promoted only when improvements are concentrated on the targeted symptom categories and unacceptable PF regressions are limited. Iteration continues until marginal gains are exhausted and further changes begin to show clear overfitting signals; only then do we evaluate the final version once on held-out TestSet. AgentDevel is designed to be benchmarkagnostic: the same release workflow can be applied to domains with very different failure surfaces, including verifiable software engineering tasks, longhorizon web interaction, and tool/API composition. Across such regimes, our goal is to show that single externalized, release-driven pipeline can produce agent improvements that are more stable, easier to audit, and less regression-prone than approaches that place self-improvement inside the agent or rely on multi-variant evolutionary search. Contributions. We introduce AgentDevel, releaseengineering paradigm and pipeline that externalizes agent improvement into single canonical development line with RC-based promotion. We formalize an implementation-blind, symptom-level critic that consumes only rubrics, traces, and optional programmatic scoring resultsseparating surface characterization from causal diagnosis and repair. We propose flip-centered gating (PF/FP) together with executable diagnostic scripting for regression-aware, auditable agent improvement and principled stopping based on diminishing returns and overfitting risk."
        },
        {
            "title": "2 Method",
            "content": "2.1 Setup & Overview (What we build) We treat an LLM agent as maintainable software artifact rather than self-evolving organism. Concretely, we refer to the agents internal designits 2 Figure 1: Main pipeline of AgentDevel prompt, code, and tooling internalsas the agent blueprint. Throughout the paper, improving an agent means proposing and validating changes to this blueprint. Objects and data. We assume task comes with development set and held-out test set: Dtrain = {xi}N i=1, Dtest = {xj}M j=1. All iterative development is performed on Dtrain, while Dtest is used only once for final post-hoc reporting. blueprint induces concrete agent Ab. Running the agent on an input produces both an output and structured execution record: (ˆy, τ ) Ab(x), where τ is trace that records execution steps such as actions, tool calls, observations, errors, and the final output. We use traces in the same spirit as observability in software systems: they capture what happened during an execution and support downstream diagnosis and analysis. To evaluate behavior, we assume rubric that specifies grading criteria. When available, we also use deterministic programmatic scorers (e.g., unit tests, schema or format checks), denoted by s(ˆy, τ ; R) g, where is structured grading signal (e.g., pass/fail and/or an error code). Single canonical line and release candidates. AgentDevel maintains single canonical version line of the agent. At iteration t, the current blueprint is bt (with agent Abt). The pipeline produces exactly one release candidate (RC) blueprint bRC . If the RC passes gating, it is promoted to the next official version bt+1; otherwise, it is discarded. Figure 1 provides one-pass view of the pipeline: Run Score Critic Diagnose(script) RC Gate(PF / FP) Promote. Why we care about flips. key lesson from regression testing is that after change, we must ensure we did not break previously working behavior. Let pt(x) {0, 1} indicate whether example passes under blueprint bt. Comparing the current 3 version and candidate t+1, we define P2Ft = {x Dtrain : pt(x) = 1, pt+1(x) = 0} (1) F2Pt = {x Dtrain : pt(x) = 0, pt+1(x) = 1}. (2) P2Ft corresponds to passfail flips (regressions), while F2Pt corresponds to failpass flips (fixes). These flips form the core lens for gating decisions in AgentDevel. 2.2 Running the Agent & Producing Quality Signals (What We Observe) Each AgentDevel iteration begins by running the current build and converting its behavior into auditable quality signals. At iteration t, we have an agent blueprint bt and its instantiated agent Abt. For every development case Dtrain, we execute the agent and collect both the produced output and structured execution trace: (ˆyt(x), τt(x)) Abt(x). (3) Here τt(x) is trace that records the step-by-step execution (e.g., actions, tool calls, observations, errors, and the final output), serving as an observability artifact for downstream diagnosis. Programmatic scoring (hard signals). When available, AgentDevel first applies deterministic, non-LLM programmatic scorers such as unit tests or schema/format validators. We denote generic scorer as gt(x) s(ˆyt(x), τt(x); R) , (4) where is the rubric and gt(x) is structured grading result (e.g., pass/fail and error codes). These signals are reproducible and provide hard, objective checks. Implementation-blind LLM critic (soft signals). We then introduce an implementation-blind LLM critic that observes only the rubric R, the trace τt(x), and (optionally) the programmatic scorer output gt(x): (pt(x), ℓt(x), dt(x)) c(R, τt(x), gt(x)) . (5) The critic returns pass/fail judgment pt(x) {0, 1}, symptom label ℓt(x), and short symptom description dt(x). We assume is deterministic given its inputs (or evaluated with fixed random seed). The critic does not observe the agent blueprint bt (prompt, code, or tooling internals), performs no causal attribution, and proposes no repairs; it is restricted to symptom-level characterization. This separation mirrors standard quality inspection practices in manufacturing, where inspectors report observable defects rather than hypothesized root causes. Final pass indicator. Because deterministic checks are preferred when available, we define the final per-example pass indicator as pt(x) = Pass(gt(x)) , if gt available, pt(x), otherwise. (6) Open-ended symptom space. Symptom labels are task-local and allowed to expand over iterations: ℓt(x) Lt, Lt Lt+1. (7) Per-example quality record. Each example yields an implementation-agnostic record rt(x) = (cid:0)ˆyt(x), τt(x), gt(x), pt(x), pt(x), ℓt(x), dt(x)(cid:1), (8) where gt(x) may be absent if no programmatic scorer exists. These records constitute the quality signals consumed by the executable diagnosis stage. These records are the sole inputs to the executable diagnosis stage. 2.3 Executable Diagnosis & RC Synthesis (How We Propose Changes) The previous stage converts raw executions into structured quality records that describe how the current agent behaves and what kinds of failures appear on the surface. However, describing failures is not the same as fixing them. In this stage, AgentDevel turns these observations into executable diagnoses and then synthesizes concrete release candidate (RC)a single, auditable proposal for changing the agent blueprint. From symptom records to executable diagnosis. At iteration t, AgentDevel collects all per-example quality records into Rt = { rt(x) Dtrain }, (9) where each record rt(x) contains the execution trace τt(x), the programmatic scoring result gt(x) (if available), and the critics symptom-level outputs (ℓt(x), dt(x)). Instead of producing informal textual summaries, AgentDevel generates and executes diagnostic 4 scripts (e.g., in Python) that operate directly on Rt. These scripts aggregate failure appearances by symptom labels, identify frequent patterns in traces, and surface representative failure cases. The key idea is that diagnosis is executable: it is performed by running code, not by relying on free-form narrative summaries. Diagnostic scripts are regenerated at each iteration, but they are allowed to reference scripts from the previous iteration. This creates bootstrapped diagnosis process in which the system gradually refines how it inspects and summarizes failures, similar to how debugging pipelines in software systems evolve across releases. Diagnosis as an engineering specification. Executing the diagnostic scripts yields structured diagnosis report Dt. This report plays the role of an engineering specification that summarizes: (i) which symptom classes currently dominate, (ii) what trace patterns tend to trigger them, (iii) representative failing examples, and (iv) the affected surface, i.e., which portions of the training set are impacted by each major failure appearance. Rather than being model output, Dt functions like bug triage report in software engineering: it provides concrete, auditable evidence about what is broken, where, and how often. Synthesizing single release candidate. Based on Dt, AgentDevel synthesizes exactly one release candidate blueprint, bRC = Φ(bt, Dt), (10) which proposes updates to the current agent blueprint bt. These updates may involve any part of the blueprintprompt, code, or tool wrappersand are bundled into single, unified change package. Importantly, AgentDevel does not generate multiple competing variants. This reflects our central design philosophy: improvement is treated as release engineering, not as search over many alternatives. Each RC also carries short change intent describing which symptom classes it primarily aims to address, directly grounded in Dt. This intent does not claim causal correctness; it simply documents the alignment between observed failure appearances and the proposed changes. Conceptually, the RC is release packagea concrete, versionable proposal that can be audited, evaluated, and either promoted or rejected by the flip-centered gate described in the next section. 2.4 flip-centered Gating, Promotion, and Stopping (How We Decide & When We Stop) Once AgentDevel synthesizes release candidate (RC) blueprint bRC , the next question is the most practical one: should we promote it? This subsection describes our release-style acceptance policy (gating), how we promote or discard RCs, and when we stop iterating. Gating as release acceptance on the same TrainSet. We evaluate the RC on the same development set Dtrain used for iteration. Concretely, we run the agent instantiated by the RC blueprint and compute per-example pass indicators in exactly the same way as for the current version (cf. Sec. 2.2). This yields, for each Dtrain, pRC (x) {0, 1}, (11) where pRC (x) = 1 indicates that the RC passes on (using programmatic scorer when available, otherwise falling back to the critic-based judgment), and pRC (x) = 0 indicates failure. Example-level flips (PF and FP). core lesson from continuous integration is that aggregate scores can be misleading: change may improve averages while silently breaking previously working cases. We therefore center gating on examplelevel flips between the current version and the RC. Using the current-version pass indicator pt(x), we define: P2Ft = { Dtrain pt(x) = 1 pRC F2Pt = { Dtrain pt(x) = 0 pRC (x) = 0 }, (12) (x) = 1 }. (13) P2Ft (passfail) captures regressions (bad flips), while F2Pt (failpass) captures fixes (good flips). For scale-aware reporting, we also track flip rates: ρP2F = ρF2P = P2Ft {x Dtrain : pt(x) = 1} + ϵ F2Pt {x Dtrain : pt(x) = 0} + ϵ , , (14) (15) where ϵ > 0 avoids division by zero. Release gate decision. We view gating as release acceptance decision: an RC should be promoted only if it delivers meaningful fixes while 5 keeping regressions under control. We write this abstractly as binary decision Acceptt = G(cid:0)Rt, RRC , P2Ft, F2Pt, It (cid:1) {0, 1}, (16) where Rt = {rt(x)} and RRC (x)} are the quality records for the current version and the RC (Sec. 2.2), and It is the RCs change intent, i.e., which symptom classes the RC claims to target. = {rRC In practice, G() follows three principles: (i) PF is high-priority risk: large P2Ft indicates that the RC breaks previously working cases and is treated as release accident; (ii) FP is fix evidence: F2Pt indicates that the RC resolves failures; (iii) alignment with intent: fixes should primarily concentrate on the symptom classes stated in It, while newly introduced failures should be limited and scrutinized. We emphasize that AgentDevel does not prescribe universal set of thresholds; rather, it provides the evidence (flip sets, rates, and symptomaligned summaries) that the gate uses to make release-style decision. Promotion or discard. becomes the next official version: If the RC is accepted, it bt+1 bRC if Acceptt = 1. (17) Otherwise, we discard the RC and keep the current blueprint as the official line: RCs are repeatedly rejected by the gate multiple times, or clear overfitting signals emerge (e.g., gains concentrate on shrinking subset of cases). Intuitively, we continue iterating as long as the pipeline can produce RCs that turn failures into passes without creating unacceptable passfail regressions. The held-out test set Dtest is not used in any gating or stopping decision; it is reserved for single final evaluation after development is complete."
        },
        {
            "title": "3 Result",
            "content": "3.1 Experiment Settings We implement AgentDevel using Claude Code and use Claude-Sonnet-4.5 as the underlying LLM throughout all experiments. Claude Code is responsible for generating and executing diagnostic scripts, summarizing symptom patterns, and proposing blueprint diffs, with all generated artifacts (diagnostic scripts, RC diffs, critic outputs, and flip lists) saved and versioned for auditability. To avoid confounding factors, we keep the Devel engine (model and tooling) fixed across iterations; all iterative decisions are based exclusively on TrainSet signals under flip-centered gating, while the TestSet is reserved for single, final evaluation. bt+1 bt if Acceptt = 0. (18) 3.2 Main Results This promote-or-discard discipline keeps AgentDevel on single canonical version line and avoids proliferating variants. Stopping criterion. We stop iterating when additional changes no longer provide meaningful benefits and begin to increase regression or overfitting risk. Formally, we define stopping predicate Stopt {0, 1} that may depend on flip behavior and acceptance outcomes, e.g., Stopt = I[ Stopping conditions are met ] , (19) We consider stopping when one or more of the following conditions hold: the size of F2Pt becomes consistently small over successive iterations, the regression rate ρP2F shows sustained increase, Table 1 reports the primary-metric results of AgentDevel across four execution-heavy benchmarks. On all benchmarks, AgentDevel substantially improves over the same initial blueprint b0, indicating that the release-engineering pipeline can reliably drive large end-to-end gains without changing the underlying model. On SWE-bench Lite, AgentDevel doubles the resolved rate from 11.0% to 22.0%, surpassing the reported SWE-agent baseline. On the more stringent SWE-bench Verified subset, AgentDevel again doubles performance (15.0% 30.0%), approaching scaffolded GPT-4o system reported in prior work. These results are notable because SWEbench evaluation is itself release-stylerequiring patches to pass repository testsmaking it natural fit for AgentDevels RC-based development and flip-centered gating. We observe similarly large gains in interactive and tool-use environments. On WebArena, 6 Benchmark Method Primary metric (%) SWE-bench Lite (Resolved ) Base agent (b0) AgentDevel (final) SWE-agent SWE-bench Verified (Resolved ) WebArena Base agent (b0) AgentDevel (final) GPT-4o (scaffolded) (Success ) Base agent (b0) AgentDevel (final) CER_hybrid StableToolBench (SoWR ) Base agent (b0) AgentDevel (final) DFS 11.00 22.00 18.00 15.00 30.00 33.20 17.00 35.50 36.70 54.00 73.50 70. Reported numbers from prior work (not rerun under our exact setup/budget). Table 1: Main results (primary metric only). Base agent and AgentDevel start from the same initial blueprint b0. AgentDevel more than doubles the task success rate (17.0% 35.5%), approaching the reported CER_hybrid system. On StableToolBench, which emphasizes stable and reproducible tool-use evaluation, AgentDevel improves SoWR by nearly 20 points (54.0% 73.5%), outperforming DFS baseline reported in prior work. Together, these results suggest that AgentDevels improvements are not confined to code-centric settings, but generalize to realistic web and tool-use environments. or search in-agent Importantly, these gains are achieved through disciplined release process rather than through selfpopulation-based refinement. In the following sections, we show that these improvements come with substantially fewer regressions, as quantified by example-level passfail flips, and that the improvements are driven by executable diagnosis rather than ad-hoc tuning. 3.3 Case Study Table 2 illustrates how AgentDevel makes release decisions based on example-level flips rather than aggregate scores. Across accepted iterations (e.g., 1, 2, 46, 810), the pipeline consistently yields many failpass fixes with very low regression rates (ρP2F 0.7%) and high hit rates, indicating that realized improvements are well aligned with the RCs stated symptom intent. In contrast, rejected iterations (3, 7, 11) exhibit markedly higher 7 Table 2: Flip-centered gate summary across iterations on StableToolBench. Iter. Gate F2Pt P2Ft ρP2F hit rate FTP / P2P 0 1 Acc. 2 Acc. 3 Rej. 4 Acc. 5 Acc. 6 Acc. 7 Rej. 8 Acc. 9 Acc. 10 Acc. 11 Rej. 38 30 42 25 18 12 9 8 6 5 2 4 0.006 5 0.007 28 0.040 3 0.004 4 0.005 3 0.004 15 0.021 2 0.003 2 0.003 2 0.003 3 0. 0.74 0.12 / 0.98 0.78 0.20 / 0.979 0.41 0.28 / 0.93 0.81 0.32 / 0.978 0.83 0.36 / 0.977 0.86 0.39 / 0.977 0.52 0.41 / 0.955 0.88 0.44 / 0.976 0.90 0.46 / 0.975 0.92 0.47 / 0.974 0.48 / 0.97 0.67 passfail regressions (up to 4.0%) and substantially lower hit rates, and are therefore filtered out despite sometimes achieving non-trivial fixes. The steady increase in FTP (from 0.12 to 0.48) alongside consistently high P2P ( 0.970.98) shows that AgentDevel accumulates fixes while preserving previously working behavior. Overall, the table concretely demonstrates flip-centered gating as release acceptance policy that prioritizes non-regression over raw aggregate gains. 3.4 Ablation Study Table 3 isolates the contributions of the three core components of AgentDevel to release stability and regression control. The full pipeline achieves strong final test metric while maintaining low regression rate (3.1%) and zero bad releases, indicating that stable improvements can be accumulated without sacrificing non-regression. Removing the flip-centered gate yields the highest final train/test scores but causes dramatic increase in regressions (PF rate 14.8%) and leads to four bad releases. This confirms that aggregate score improvements alone are insufficient for safe promotion and that flip-centered gating is essential for preventing release accidents. Removing executable diagnosis reduces both fix yield (FP) and final performance, suggesting that script-based, trace-grounded diagnosis is key driver of effective and targeted improvements rather than ad-hoc tuning. Allowing the critic to see the blueprint increases apparent training performance but more than doubles the regression rate (6.7%), indicating tighter entanglement between evaluation and implementation leads to overfitting and less stable releases. ToTable 3: Ablations on release stability and regression control on WebArena. Setting AgentDevel (full) w/o flip gate w/o executable diagnosis critic not blind (critic sees blueprint) Final Test metric 34.2 35.0 31.8 32.5 Final Train pass 78.5 81.0 74.0 83.5 Total FP 214 230 150 205 Total PF 18 95 22 40 PF rate 3.1% 14.8% 3.9% 6.7% Gate reject rate 42% N/A 63% 58% Bad release count 0 4 0 0 Notes. All settings start from the same initial blueprint b0 with the same data split and budget. FP and PF are computed on Dtrain by comparing each promoted release to its evaluated RC (use single consistent protocol). Bad release count is the number of promoted updates whose regressions exceed preset threshold. For w/o flip gate, reject rate is N/A by design. gether, these ablations show that AgentDevels core designs are not interchangeable components but jointly enforce stable, regression-aware improvement."
        },
        {
            "title": "4 Conclusion",
            "content": "This work introduces release-engineering perspective on agent improvement. Rather than treating improvement as an internal cognitive process or as search over many competing variants, we frame agents as shippable software artifacts whose evolution should be managed through auditable, regression-aware release pipelines. AgentDevel operationalizes this perspective by externalizing improvement into structured workflow: implementation-blind, symptom-level quality signals provide observability; executable diagnosis produces concrete engineering specifications; single release candidate is synthesized per iteration; and promotion decisions are made using flipcentered gating that treats passfail regressions as first-class risks. This reframing addresses fundamental engineering challenges that arise as agents move from demos to systems: guaranteeing non-regression, enabling reproducibility, and making improvement trajectories auditable across versions. By explicitly modeling example-level flips and preserving versioned artifacts such as diagnostic scripts, blueprint diffs, and critic outputs, AgentDevel transforms agent development from an ad-hoc tuning process into disciplined release practice. Empirically, this discipline yields stable improvements with substantially fewer regressions in execution-heavy environments. Looking forward, AgentDevel opens path toward CI-like automation for agent development. Future work may extend this paradigm to multiagent systems and repository-scale deployments, enrich symptom taxonomies as shared diagnostic vocabularies across tasks, and integrate human-inthe-loop review into the promotion gate. More broadly, we hope this work encourages the community to adopt release engineering as first-class principle for building, debugging, and deploying LLM agents."
        },
        {
            "title": "References",
            "content": "Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B. Hashimoto. 2024. Length-controlled alpacaeval: simple way to debias automatic evaluators. Preprint, arXiv:2404.04475. Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktäschel. 2023a. Promptbreeder: Self-referential self-improvement via prompt evolution. Preprint, arXiv:2309.16797. Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktäschel. 2023b. Promptbreeder: Self-referential self-improvement via prompt evolution. Preprint, arXiv:2309.16797. Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and Yang Liu. 2024. StableToolBench: Towards stable large-scale benchmarking on tool learning of large In Findings of the Association language models. for Computational Linguistics: ACL 2024, pages 1114311156, Bangkok, Thailand. Association for Computational Linguistics. Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. Swe-bench: Can language modPreprint, els resolve real-world github issues? arXiv:2310.06770. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpt-4 with better human alignIn Proceedings of the 2023 Conference on ment. Empirical Methods in Natural Language Processing, 8 pages 25112522, Singapore. Association for Computational Linguistics. Aman Madaan and at al. 2023. Iterative refinement with self-feedback. Preprint, arXiv:2303.17651. Self-refine: Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. Preprint, arXiv:2303.17651. Y. Qin and at al. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. Preprint, arXiv:2307.16789. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023a. Reflexion: Language agents with verbal reinforcement learning. Preprint, arXiv:2303.11366. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023b. Reflexion: Language agents with verbal reinforcement learning. Preprint, arXiv:2303.11366. tatsu-lab. 2023. AlpacaEval: An automatic evaluator for instruction-following language models. GitHub repository. P. Xia and at al. 2025. Live-swe-agent. Preprint, arXiv:2511.13646. Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. 2023. On the tool manipulation capability of open-source large language models. Preprint, arXiv:2305.16504. John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024. Swe-agent: Agent-computer interfaces enable automated software engineering. Preprint, arXiv:2405.15793. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Preprint, arXiv:2305.10601. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. ReAct: Synergizing reasoning and acting in language models. Preprint, arXiv:2210.03629. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Preprint, arXiv:2306.05685. S. Zhou and at al. 2023. Webarena: realistic web environment for building autonomous agents. Preprint, arXiv:2307.13854."
        },
        {
            "title": "A Limitations",
            "content": "AgentDevel introduces release-engineering discipline for improving LLM agents, but it also has limitations. First, the pipeline incurs non-trivial overhead: iterative execution, trace collection, diagnostic script generation, and gating require additional compute and wall-clock time compared to single-pass or in-agent refinement methods. While this overhead is acceptable in development settings where stability and auditability are critical, it may be less suitable for rapid prototyping or highly latency-sensitive scenarios. our Second, although implementationblind critic is designed to reduce evaluationimplementation entanglement, it remains an LLM-based evaluator and can inherit biases and inconsistencies of the underlying model. Programmatic scorers mitigate this when available, but tasks without strong automatic checks still depend on rubric-based judgments. Third, our current experiments focus on singleagent, single-repository (or single-environment) settings. Scaling AgentDevel to multi-agent systems, very large codebases, or continuously changing environments may require additional coordination mechanisms and more sophisticated stopping and gating policies. Finally, our stopping criteria and gating policies are configurable rather than universal; while this flexibility is practical, it also means that different deployments may choose different thresholds and trade-offs between fix yield and regression risk. Future work is needed to formalize these policies and study their robustness across broader classes of tasks."
        },
        {
            "title": "B Related works",
            "content": "Jenny Zhang, Shengran Hu, Cong Lu, Robert Lange, and Jeff Clune. 2025. Darwin godel machine: Openended evolution of self-improving agents. Preprint, arXiv:2505.22954. Recent work on LLM agents has made rapid progress in improving performance, but much of this progress is guided by sharedoften implicitassumption about where improvement lives. 9 Broadly, prior work tends to treat improvement as something that happens inside the agent, as search over population of variants, or as optimization of aggregate scores. AgentDevel departs from all three framings by externalizing improvement into regression-aware release engineering pipeline. Improvement as cognition: reflection and selffeedback inside the agent. prominent line of work treats improvement as an internal cognitive process: if an agent can reflect on its mistakes, store feedback, or revise its own outputs, it can gradually become more capable. Reflexion introduces verbal reinforcement learning by storing feedback into memory and conditioning later attempts on that memory (Shinn et al., 2023b). Self-Refine shows that single LLM can iteratively improve its outputs by alternating between feedback generation and refinement (Madaan and at al., 2023). Related work on trajectory-centric agents such as ReAct emphasizes reasoningaction traces as basis for interpretability and improvement (Yao et al., 2022). These approaches implicitly frame improvement as part of the agents cognition: evaluation, feedback, and modification are intertwined with the agents internal prompt, memory, or control loop. While effective in many settings, this framing makes improvement trajectories difficult to audit, reproduce, and reason about across versions. AgentDevel rejects the improvement-ascognition framing and instead treats the agent as shippable artifact whose evolution is managed by an external release pipeline. Improvement as search: multi-variant exploration and evolution. Another major framing casts improvement as search problem over space of prompts, thoughts, or scaffolds. Tree of Thoughts explores multiple reasoning branches and selects among them to improve task performance (Yao et al., 2023). Promptbreeder evolves prompts using population-based mutation and selection (Fernando et al., 2023a). In software engineering contexts, recent systems attempt to self-modify agent scaffolds and maintain archives of evolving variants, such as the Darwin Gödel Machine (Zhang et al., 2025) and Live-SWE-Agent (Xia and at al., 2025). These methods conceptualize progress as selecting better candidate from many competing variants. While powerful, this paradigm leads to variant proliferation and opaque selection decisions, and it often relies on aggregate metrics that can mask regressions. AgentDevel replaces 10 improvement-as-search with release discipline: it maintains single canonical version line and produces exactly one release candidate per iteration, emphasizing auditable diffs and regressionaware promotion. Improvement as aggregate-score optimization: judges and leaderboards. third framing equates improvement with increases in aggregate scores reported by automatic judges and leaderboards. LLM-as-a-judge frameworks such as MTBench and Chatbot Arena provide scalable rubricbased evaluation, while also documenting systematic biases (e.g., position and verbosity biases) in such judges (Zheng et al., 2023). G-Eval introduces rubric-structured prompting to improve the reliability of LLM-based evaluation (Liu et al., 2023). AlpacaEval further popularizes automatic preference evaluation and exposes additional confounders (tatsu-lab, 2023; Dubois et al., 2024). These evaluation-centric practices implicitly encourage optimizing averages rather than managing regressions. In contrast, AgentDevel adopts flipcentered view inspired by software release engineering: passfail flips are treated as first-class regression risks, while failpass flips serve as fix evidence. This reframing makes non-regression an explicit objective rather than an incidental side effect of score maximization. Why this reframing matters in realistic agent environments. The need for release-style improvement becomes particularly clear in execution-heavy environments, where failures manifest as distinct behavioral breakdowns rather than single incorrect answer. Benchmarks for tool use and realistic interaction highlight this complexity, including ToolBench (Xu et al., 2023), ToolLLM (Qin and at al., 2023), and the stability-focused StableToolBench (Guo et al., 2024). Realistic web interaction is captured by WebArena (Zhou and at al., 2023), and software engineering tasks are represented by SWE-bench (Jimenez et al., 2023) and SWE-agent (Yang et al., 2024). calls, invalid tool trace-grounded Across these settings, many failures have recognizable symptoms in execution tracesmissing incorrect ordersteps, ingmaking and regression control central to reliable development. AgentDevel directly targets regime by organizing improvement around symptom-level characterization, and flip-centered release gating. executable diagnosis, diagnosis this Pseudo-Code of AgentDevel Algorithm 1: AgentDevel (release-engineering improvement) Input: initial blueprint b0, TrainSet Dtrain, rubric Output: final promoted blueprint b0 π for = 0, . . . , 1 do (current promoted version) (previous diagnosis script) (symptom taxonomy) Run current agent and collect records for each Dtrain: (ˆy, τ ) Ab(x) (p, ℓ, d) c(R, τ ) (implementation-blind critic) {rt(x)} end Executable diagnosis π GenDiagScript(R, π, L) Run(π, R) Synthesize RC (bRC Gate RC compute P2Ft, F2Pt if G(D, P2Ft, F2Pt, It) = 1 then bRC else discard bRC if Stop(P2Ft, F2Pt) then break , It) Φ(b, D) end for return (TestSet used only once at final evaluation)"
        }
    ],
    "affiliations": [
        "Fudan University"
    ]
}