{
    "paper_title": "BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities",
    "authors": [
        "Sahal Shaji Mullappilly",
        "Mohammed Irfan Kurpath",
        "Sara Pieri",
        "Saeed Yahya Alseiari",
        "Shanavas Cholakkal",
        "Khaled Aldahmani",
        "Fahad Khan",
        "Rao Anwer",
        "Salman Khan",
        "Timothy Baldwin",
        "Hisham Cholakkal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper introduces BiMediX2, a bilingual (Arabic-English) Bio-Medical EXpert Large Multimodal Model (LMM) with a unified architecture that integrates text and visual modalities, enabling advanced image understanding and medical applications. BiMediX2 leverages the Llama3.1 architecture and integrates text and visual capabilities to facilitate seamless interactions in both English and Arabic, supporting text-based inputs and multi-turn conversations involving medical images. The model is trained on an extensive bilingual healthcare dataset consisting of 1.6M samples of diverse medical interactions for both text and image modalities, mixed in Arabic and English. We also propose the first bilingual GPT-4o based medical LMM benchmark named BiMed-MBench. BiMediX2 is benchmarked on both text-based and image-based tasks, achieving state-of-the-art performance across several medical benchmarks. It outperforms recent state-of-the-art models in medical LLM evaluation benchmarks. Our model also sets a new benchmark in multimodal medical evaluations with over 9% improvement in English and over 20% in Arabic evaluations. Additionally, it surpasses GPT-4 by around 9% in UPHILL factual accuracy evaluations and excels in various medical Visual Question Answering, Report Generation, and Report Summarization tasks. The project page including source code and the trained model, is available at https://github.com/mbzuai-oryx/BiMediX2."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 1 ] . [ 1 9 6 7 7 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Technical Report",
            "content": "BIMEDIX2 : BIO-MEDICAL EXPERT LMM FOR DIVERSE MEDICAL MODALITIES Sahal Shaji Mullappilly1, Mohammed Irfan Kurpath1, Sara Pieri1, Saeed Yahya Alseiari5 Shanavas Cholakkal6, Khaled Aldahmani3,4, Fahad Khan1,2, Rao Anwer1, Salman Khan1 Timothy Baldwin1, Hisham Cholakkal1 1Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), 2Linkoping University 3Shaikh Tahnoon bin Mohammed Medical City (STMC), 4Tawam Hospital 5Sheikh Shakhbout Medical City (SSMC), 6Govt Medical College Kozhikode"
        },
        {
            "title": "ABSTRACT",
            "content": "This paper introduces BiMediX2, bilingual (Arabic-English) Bio-Medical EXpert Large Multimodal Model (LMM) with unified architecture that integrates text and visual modalities, enabling advanced image understanding and medical applications. BiMediX2 leverages the Llama3.1 architecture and integrates text and visual capabilities to facilitate seamless interactions in both English and Arabic, supporting text-based inputs and multi-turn conversations involving medical images. The model is trained on an extensive bilingual healthcare dataset consisting of 1.6M samples of diverse medical interactions for both text and image modalities, mixed in Arabic and English. We also propose the first bilingual GPT-4o based medical LMM benchmark named BiMed-MBench. BiMediX2 is benchmarked on both text-based and image-based tasks, achieving state-of-the-art performance across several medical benchmarks. It outperforms recent state-ofthe-art models in medical LLM evaluation benchmarks. Our model also sets new benchmark in multimodal medical evaluations with over 9% improvement in English and over 20% in Arabic evaluations. Additionally, it surpasses GPT-4 by around 9% in UPHILL factual accuracy evaluations and excels in various medical Visual Question Answering, Report Generation, and Report Summarization tasks. The project page, including source code and the trained model, is available at https://github.com/mbzuai-oryx/BiMediX2."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advancements in healthcare AI, including medical large language models (LLMs) and large multimodal models (LMMs), have shown great potential in improving access to quality medical advice. However, these advancements remain predominantly English-centric, leaving significant gaps in addressing the healthcare needs of non-English-speaking populations. This language bias limits the accessibility and utility of AI-driven medical solutions in regions where languages like Arabic are widely spoken, creating pressing need for more inclusive and multilingual models. Furthermore, conventional medical LMMs (Zhang et al. (2024); Li et al. (2023); Chen et al. (2024) see Fig 3) generally compromise their advanced medical text-based understanding when integrating multimodal capabilities. Our project, BiMediX2, strives to address the aforementioned limitations of existing medical LMMs by introducing Bio-Medical EXpert Large Multimodal Model (LMM) built on the Llama3.1 architecture. It achieves state-of-the-art performance in medical image understanding while also excelling in text-based medical LLM evaluations. BiMediX2 facilitates seamless user interactions in both English and Arabic, leveraging our extensive, purpose-built bilingual multimodal healthcare dataset BiMed-V. BiMediX2 facilitates medical image understanding along with textual-queries, enabling multi-turn conversations about user-provided medical images. It supports wide range of imaging modalities, Equal Contribution"
        },
        {
            "title": "Technical Report",
            "content": "Figure 1: Model Performance Comparison on BiMed-MBench: These comparisons are made across different categories, including CT, MRI, CXR, Histology, Gross, and their Arabic counterparts (CT ar, MRI ar, CXR ar, Histology ar, Gross ar). The models compared are LLaVA-pp, LLaVA-Med, BiMediX2, Dragonfly-Med, MiniGPT-Med, and BiomedGPT. Each axis represents the performance score in specific category, allowing for visual comparison of how each model performs in bilingual medical contexts. including chest X-rays, CT scans, MRIs, histology slides, and gross pathology images. This multimodal approach ensures comprehensive and accessible medical assistance in bilingual setting. Recent advancements in medical LLMs and vision-language models (VLMs) have shown progress in tasks such as multi-turn conversations (MTC), report generation (RG), and specialized domains like radiology (Rad) and pathology (Path) as shown in Tab 1. While models like LLaVa-pp (Rasheed et al. (2024)) and LLaVa-Med (Li et al. (2023)) enable seamless multi-turn conversations, others, such as MiniGPT-Med (Alkhaldi et al. (2024)), are limited to specific modalities like chest X-rays, and BiomedGPT (Zhang et al. (2024)) requires fine-tuned checkpoints for each downstream task. BiMediXv1 (Pieri et al. (2024)) enables multi-turn conversational capabilities and report summarization within bilingual framework, setting it apart from other medical LLMs. comprehensive comparison of state-of-the-art models in Clinical LLM evaluations is shown in Fig 3. Building on this, BiMediX2 expands its functionality to cover broader spectrum of medical tasks, including Multi-turn Conversations (MTC), Report Summarization (RS), Report Generation (RG), Radiology (Rad), Ophthalmology (Oph), Pathology (Path), Microscopic Analysis (Micro), in Unified Model (UM) with LLM and VLM capabilities (LLM+VLM). Fig 1 illustrates the performance of state-of-the-art medical LMMs on our BiMed-MBench evaluation in bilinIts robust bilingual support for Arabic addresses the healthcare needs of Arabicgual context. speaking regions, providing comprehensive and inclusive solution for diverse, multilingual, and multimodal healthcare applications. CONTRIBUTIONS In this work, we present BiMediX2, bilingual multimodal medical LMM with the following key contributions: (i) We introduce the first bilingual medical LMM that achieves state-of-the-art results on VLM evaluation benchmarks across various medical image modalities, while also excelling on medical LLM evaluation benchmarks. (ii) We curated comprehensive Arabic-English multimodal bilingual instruction set named BiMed-V comprising over 1.6M instructions."
        },
        {
            "title": "Technical Report",
            "content": "Model MTC RS RG Rad Oph Path Micro LLM+VLM Bil (Ar) Meditron (Chen et al. (2023)) Med42 (Christophe et al. (2024)) OpenBioLLM (Ankit Pal (2024)) Llama3.1 (Meta (2024)) BiMediXv1 (Pieri et al. (2024)) Model MTC RS RG Rad Oph Path Micro UM LLM+VLM Bil (Ar) LLaVA-pp (Rasheed et al. (2024)) MiniGPT-Med (Alkhaldi et al. (2024)) BioMedGPT (Zhang et al. (2024)) LLaVA-Med (Li et al. (2023)) Dragonfly VLM (Chen et al. (2024)) BiMediX Table 1: Comparison of Recent Medical LLMs and VLMs. Abbreviations: MTC (Multiturn conversation), RS (Report Summarization), RG (Report Generation), Rad (Radiology), Oph (Ophthalmology), Path (Pathology), Micro (Microscopic), UM (Unified Model: Single model checkpoint for all downstream tasks), LLM+VLM (Unified LLM + VLM), Bil (Ar) (Bilingual Arabic capabilities). (iii) We introduce the first bilingual GPT-4o-based medical LMM benchmark named BiMedMBench, consisting of 286 medical queries in English and Arabic across various medical image modalities, fully verified by medical experts. (iv) Our BiMediX2 LLM outperforms GPT-4 by more than 8% on the USMLE benchmark and by more than 9% in UPHILL factual accuracy evaluations. (v) Our BiMediX2 LMM achieves state-of-the-art results on BiMed-MBench, with over 9% improvement in English evaluations and more than 20% improvement in Arabic evaluations. Furthermore, it excels in medical Visual Question Answering, Report Generation, and Report Summarization tasks."
        },
        {
            "title": "2 BIMEDIX2",
            "content": "Figure 2: BiMediX2: Overall Architecture Our model is designed for medical image analysis and bilingual multi-turn conversations. Medical images are processed through Vision Encoder and aligned with Projector, while the text inputs are tokenized using the default tokenizer. The resulting tokens are then passed into the language model (Meta Llama 3.1) to generate responses in the prompted language. We only train the language model using LoRA adapters, while the projector is finetuned for medical image-text alignment. robust data generation framework translates an English data corpus into Arabic using GPT-4o, with verification by medical expert to ensure accurate and contextually appropriate translations. This approach supports effective training and benchmarking in bilingual context."
        },
        {
            "title": "Technical Report",
            "content": "The architecture of BiMediX2 is meticulously designed to facilitate seamless integration of medical image analysis and bilingual multi-turn conversations. At its core, (see Fig 2) the model employs Vision Encoder (Radford et al. (2021)) to process diverse array of medical imaging modalities, including chest X-rays, CT scans, MRIs, histology slides, and gross pathology images. This visual data is aligned with textual inputs through dedicated Projector, ensuring accurate and contextually rich medical image-text mapping. Text inputs are processed using standard tokenizer, transforming them into the language embedding space of Llama 3.1 (Meta (2024)). This design enables BiMediX2 to generate precise and context-aware responses in either English or Arabic, depending on the user prompt, ensuring flexibility and inclusivity. Key to BiMediX2s performance is its modular and efficient training approach. LoRA adapters (Hu et al. (2021)) are utilized to fine-tune the language model while maintaining computational efficiency and minimizing resource demands. The projector is simultaneously fine-tuned to optimize image-text alignment in medical context. Furthermore, the system is supported by robust data generation framework, where comprehensive English data corpus is translated into Arabic using GPT-4o. random subset of this translation is meticulously verified by bilingual medical experts to ensure clinical relevance and linguistic accuracy. This pipeline enables BiMediX2 to excel in wide range of medical tasks, including report generation, radiology analysis, pathology insights, and ophthalmological assessments, all within unified, bilingual, and multimodal framework. Figure 3: State of the art comparison of models in Clinical LLM Benchmarks BIMED-V BILINGUAL AND MULTIMODAL DATASET CREATION The BiMed-V dataset is comprehensive bilingual and multimodal instruction set comprising of 1.6M samples, developed to enhance medical image-text alignment and multimodal understanding. It incorporates diverse range of publicly available datasets, such as PMC-OA (Lin et al. (2023)), Rad-VQA (Lau et al. (2018)), Path-VQA (He et al. (2020)) and SLAKE (Liu et al. (2021)) complemented by custom-curated data. We also curated 163k VQA samples by repurposing the LLaVAMed (Li et al. (2023)) 60K-IM dataset, which were tailored to align with real-world medical queries. Furthermore, over 10k samples from the LLaVA-Med pretraining dataset were reformatted into interactive conversations using the Llama 3.1 70B model. subset of the PMC-OA dataset with short question-answer pairs and multiple-choice questions were added to enhance the datasets diversity. Training splits of Rad-VQA, Path-VQA and SLAKE, which typically feature concise answers, were restructured into more detailed responses using the same Llama 3.1 70B model, enhancing the datasets depth and usability for complex tasks. unique feature of BiMed-V is its bilingual support, facilitated by multimodal instruction set comprising 326k samples across various medical imaging modalities. This includes 163k Arabiclanguage samples generated via comprehensive translation framework (see Fig. 2). English datasets were translated into Arabic using GPT-4o, with rigorous verification of random subset by bilingual medical experts to ensure clinical relevance and linguistic precision. This hybrid approach balances automation and expert validation, significantly reducing reliance on human medical domain experts while maintaining data quality. Additionally, the inclusion of text-based clinical data from BiMediXv1 (Pieri et al. (2024)) ensures the dataset retains robust language understanding capabilities while expanding its multimodal medical proficiency. This extensive dataset forms the foundation for advanced medical image-text alignment and conversational multimodal applications."
        },
        {
            "title": "BILINGUAL AND MULTIMODAL MEDICAL INSTRUCTION TUNING",
            "content": "To enable BiMediX2s robust capabilities in both bilingual and multimodal medical tasks, we employ two-stage training process that ensures precise alignment of visual and textual representations while adapting the language model for complex medical instruction tasks. Stage 1 Training: Medical Concept Alignment In the first stage, we finetune the Projector alone to align visual embeddings to the language embedding space. The training utilizes comprehensive dataset of 467k image-caption pairs sourced from the LLaVA-Med (Li et al. (2023)) pretraining dataset. These pairs span diverse medical imaging modalities and captions that describe clinically relevant features. Stage 2 Training: Multimodal Medical Instruction Alignment The second stage finetunes the LoRA(Hu et al. (2021)) adapters within the language model, enhancing its ability to process and generate multimodal medical instructions. For this, we utilize our BiMed-V 1.6M bilingual multimodal instruction set, which comprises meticulously crafted English and Arabic prompts paired with corresponding visual and textual responses. This dataset enables the model to learn nuanced instructions across wide array of medical domains, from radiology to pathology, in bilingual context. Through these two stages, BiMediX2 achieves seamless integration of bilingual and multimodal capabilities, enabling it to deliver accurate and context-aware medical insights in both English and Arabic, tailored to variety of clinical scenarios."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "In the literature, evaluating medical language models predominantly involves multiple-choice question-answering tasks, with accuracy as the performance metric. We employed the EleutherAI (Gao et al. (2024)) evaluation framework for evaluating text-only medical tasks following (Pieri et al. (2024)). For our bilingual and multimodal evaluations, we utilized several benchmarks and tools. We leveraged our BiMed-MBench for assessing the correctness, helpfulness, relevance and accuracy of the model in bilingual medical multimodal context. Additionally, we evaluated the model on VQA using datasets such as Path-VQA, SLAKE, and Rad-VQA. For report generation, we used the MIMIC-CXR (Johnson et al. (2019)) dataset, and for report summarization, we used the MIMIC-III (Johnson et al. (2016)) dataset. These evaluations were conducted using the MultiMedEval (Royer et al. (2024)) toolkit, ensuring comprehensive assessment of the models capabilities in both textual and visual medical tasks. MEDICAL LLM BENCHMARKS Model BioMedGPT-LM-7B BiMediX2 4B LLaVA-Med Dragonfly-Med GPT 3.5 Meditron 70B BiMediX2 8B GPT 4 Llama3-Med42-70B OpenBioLLM-70B Llama 3.1 70B BiMediX2 70B MMLU MedMCQA MedQA USMLE PubmedQA Average Cli-KG C-Bio C-Med Med-Gen Pro-Med Ana 49.4 55.1 59.6 65.6 69.8 68.3 77.7 86.0 84.2 92.5 83.4 86.8 43.1 63.9 59.7 69.4 72.2 77.8 79.2 95.1 93.1 93.8 95.1 95.1 41.4 47.4 50.9 56.6 61.3 63.6 68.8 76.9 79.8 85.6 79.2 79.8 45.0 55.0 59.0 69.0 70.0 75.0 82.0 91.0 91.0 93.0 93.0 94.0 51.0 36.0 51.5 58.4 70.2 74.6 74.3 93.0 90.1 93.4 91.5 91. 45.2 52.6 51.9 57.0 56.3 56.3 65.9 80.0 80.7 83.7 80.7 82.2 34.8 38.1 44.5 49.9 50.1 48.4 58.0 69.5 72.5 74.1 71.7 70.5 33.2 37.9 35.7 42.8 50.8 53.1 57.0 78.9 73.8 68.9 73.8 74.3 31.7 47.1 36.9 46.1 49.1 55.4 68.6 83.8 84.3 72.0 92.0 92.3 74.0 72.2 74.0 75.4 71.6 76.2 72.4 75.2 80.6 78.0 77.6 79.0 44.9 50.5 52.4 59.0 62.1 64.9 70.4 82.9 83.0 83.5 83.8 84. Table 2: Clinical LLM Evaluation Benchmark PubMedQA (Jin et al. (2019)) is question-answering dataset derived from biomedical research papers on PubMed. Given question sourced from the title of research paper and context from the abstract, the task is to provide an answer in the form of yes, no, or maybe, mirroring the conclusion of the paper. Building on prior research, our examination focuses on the PQAL subset of PubMedQA, consisting of 500 manually annotated QA pairs sourced from PubMed. Answering these questions with accuracy requires thorough reasoning over the biomedical contexts and quantitative data analysis."
        },
        {
            "title": "Technical Report",
            "content": "MedMCQA (Pal et al. (2022)) is multiple-choice dataset constructed from questions featured in the Indian AIIMS and NEET PG medical entrance exams. It covers broad spectrum of medical subjects, evaluating both professional domain knowledge and language comprehension. The dataset includes test set of 4,183 questions, each with four options. MedQA (Jin et al. (2021)) is dataset comprising multiple-choice questions from medical board examinations in the US, Mainland China, and Taiwan. It features two types of questions: concise sentences to assess specific knowledge and extended paragraphs that detail patients condition. Our analysis concentrates on the English portion (USMLE), sourced directly from the National Board of Medical Examiners (NBME), responsible for the assessments required for medical licensure in the U.S., containing 1,273 samples for testing purposes. Engaging with this benchmark involves multiple stages of reasoning and evidence retrieval. USMLE (Han et al. (2023)) USMLE Self-assessment Test, derived from the United States Medical Licensing Examination (USMLE) self-assessment for Step 1, Step 2, and Step 3, is composed of multiple-choice questions. We employ the version provided by the authors of MedAlpaca, which specifically omits image-based questions and comprises 325 test samples. The Medical MMLU (Hendrycks et al. (2020)) (Massive Multitask Language Understanding) is group of six datasets that compiles 1,089 test questions with four options related to different medical subjects. The six MMLU datasets are Clinical Knowledge (Cli-KG), College Biology (CBio), College Medicine (C-Med), Medical Genetics (Med-Gen), Professional Medicine (Pro-Med) & Anatomy (Ana). UPHILL OpenQA (Kaur et al. (2023)) The UPHILL dataset is designed to evaluate the factual accuracy of LLMs in handling health-related queries in English embedded with varying degrees of presuppositions. Our analysis specifically targeted the zero-shot models abilities to process and respond to false claims (factually incorrect or debunked by reliable sources) across spectrum of presupposition levels, crucial test given the potential real-world implications of misinformation and inaccuracies in the health domain. In this context, the accuracy refers to the models effectiveness in accurately refuting false health-related claims at different presupposition levels. MEDICAL VLM BENCHMARKS BiMed-MBench: We introduced the first bilingual GPT-4o-based medical LMM benchmark, consisting of 286 medical queries spanning various medical imaging modalities. The test dataset includes conversational interactions and detailed descriptions for modalities such as chest X-rays (CXR), MRI, histology, gross and CT scans derived from LLaVA-Med (Li et al. (2023)). GPT4o evaluates the correctness of model responses based on the provided image context and caption. The reference prediction, serving as the upper-bound answer for the teacher model, is generated using GPT-4 (Achiam et al. (2023)). For Arabic evaluations, the ground truth is derived from the upper-bound, translated by GPT-4o, and all test samples are comprehensively verified by medical experts for quality assurance. GPT-4o evaluates the responses by comparing the candidate LMM against the upper-bound for English queries and against the Arabic ground truth for Arabic queries. The evaluation assesses the helpfulness, relevance, accuracy, and level of detail of the responses while maintaining the user language. The scoring is conducted on scale of 1 to 10, with higher scores indicating better performance. Additionally, GPT-4o provides detailed explanations of the evaluation to offer deeper insights into the models performance. Finally, we compute and compare the relative scores of the two candidates. Visual Question Answering (VQA): To evaluate models performance in Visual Question Answering, three datasets are used: Path-VQA, SLAKE, and Rad-VQA. These datasets include mix of open-ended and close-ended questions, designed to test the models ability to interpret and respond to queries based on medical images. This evaluation ensures the models effectiveness in handling variety of visual and textual inputs. Report Generation: The MIMIC-CXR (Johnson et al. (2019)) dataset, which includes de-identified radiology reports associated with chest X-rays, is utilized to assess the models performance in generating the findings section of medical reports. For this task, the input consists of one or more radiology images related to case, followed by prompt such as, <image> <image> Please caption this scan with findings and impressions. This setup evaluates the models ability to generate accurate and coherent medical reports based on visual inputs."
        },
        {
            "title": "Technical Report",
            "content": "Model Conversation Description CXR MRI Histology Gross CT Overall BiomedGPT LLaVA-pp MiniGPT-Med LLaVA-Med Dragonfly-Med BiMediX2 8B 15.3 34.3 37.5 55.6 59.2 64.9 13.3 36.6 29.6 43.3 34.2 54.5 16.4 44.7 47.6 59.5 67.0 71. 13.0 33.3 32.5 43.4 51.2 56.8 14.1 34.7 36.3 54.4 53.7 62.5 14.9 30.2 31.8 53.9 42.6 61.4 15.8 31.5 29.1 51.0 48.3 58.9 14.8 34.9 35.4 52.4 52.7 62.2 Table 3: BiMed-MBench English Evaluation Model Conversation Description CXR MRI Histology Gross CT Overall BiomedGPT MiniGPT-Med LLaVA-Med LLaVA-pp Dragonfly-Med BiMediX2 8B 11.1 21.6 23.9 29.0 32.8 54.3 11.2 12.6 29.4 27.8 19.9 36.2 11.4 23.7 31.2 33.2 31.9 61. 10.8 12.7 25.3 25.0 25.7 44.6 11.5 32.0 24.8 33.0 33.0 51.5 11.3 15.8 23.4 25.8 24.0 43.5 11.1 14.9 26.4 25.8 31.7 50.8 11.2 20.2 26.2 28.7 29.5 50.5 Table 4: BiMed-MBench Arabic Evaluation Report Summarization: We conduct an evaluation of report summarization using the MIMIC-III (Johnson et al. (2016)) dataset. The task involves generating the impressions section of radiology report based on the findings section. To achieve this, the model is provided with the free-text findings along with task prompt, Summarize the findings. This assessment focuses on the models ability to distill comprehensive medical information into concise and precise summary, which is essential for clear and effective medical communication."
        },
        {
            "title": "4 RESULTS",
            "content": "LLM EVALUATION The performance of BiMediX2 and other models on various medical benchmarks is presented in Fig 3 and Tab 2. Our BiMediX2 70B achieved the highest average score of 84.6%, outperforming other models such as GPT-4 (82.9%) and Llama-3-Med42-70B (83.0%). BiMediX2 70B exhibited superior average performance across multiple datasets, including Medical MMLU, MedMCQA, MedQA, USMLE, and PubMedQA, demonstrating its strong understanding of medical contexts. UPHILL OPENQA EVALUATION Fig 4 illustrates the performance comparison on the UPHILL OpenQA benchmark. BiMediX2 70B achieved the highest overall factual accuracy of 60.6%, and the second highest being BiMediX2 8B (56.1%), surpassing other models such as GPT-4 (51.5%), Meditron 70B (49.6%), and Med42 (53.5%). This highlights BiMediX2s effectiveness in discerning and correcting misinformation in medical contexts. Figure 4: Performance comparison on UPHILL OpenQA (Kaur et al. (2023)), assessing the models ability to address false medical claims at different presupposition levels."
        },
        {
            "title": "Technical Report",
            "content": "Dataset Metric RadFM LLaVA Med BioMedGPT MiniGPT-Med Phi-3.5 BiMediX2 4B BiMediX2 8B Rad-VQA Slake-VQA Path-VQA BLEU-1 closed accuracy open recall recall open accuracy F1 BLEU-1 closed accuracy open recall recall open accuracy F1 BLEU-1 closed accuracy open recall recall open accuracy F1 Average 0.475 0.577 0.407 0.438 0.335 0.442 0.746 0.752 0.758 0.695 0.725 0.714 0.257 0.505 0.020 0.221 0.005 0.232 0.461 0.033 0.545 0.246 0.372 0.140 0. 0.036 0.512 0.429 0.443 0.362 0.075 0.021 0.512 0.116 0.287 0.053 0.052 0.239 0.044 0.203 0.199 0.199 0.150 0.064 0.175 0.248 0.293 0.260 0.259 0.192 0.145 0.260 0.093 0.176 0.077 0. 0.177 0.662 0.829 0.546 0.703 0.490 0.675 0.337 0.572 0.308 0.396 0.278 0.349 0.296 0.581 0.040 0.311 0.019 0.299 0.427 0.377 0.618 0.295 0.475 0.200 0. 0.089 0.535 0.377 0.404 0.329 0.129 0.283 0.553 0.063 0.308 0.027 0.287 0.319 0.501 0.685 0.292 0.511 0.225 0.516 0.625 0.744 0.624 0.664 0.567 0.641 0.469 0.708 0.239 0.474 0.210 0. 0.509 0.552 0.725 0.363 0.565 0.305 0.569 0.778 0.831 0.763 0.786 0.729 0.787 0.587 0.872 0.314 0.593 0.282 0.595 0.611 Table 5: Medical VQA Benchmark (MultiMedEval Royer et al. (2024)) BIMED-MBENCH EVALUATION Tables 3 and 4 present the evaluation results of BiMediX2 and other medical LMMs on the English and Arabic BiMed-MBench benchmark, respectively. BiMediX2 8B demonstrated superior performance in both English (overall score of 62.2%) and Arabic (overall score of 50.5%) evaluations, outperforming other models. Fig 1 compares the performance of state-of-the-art medical LMMs on our BiMed-MBench evaluation in bilingual context. This indicates BiMediX2s strong bilingual capabilities and its effectiveness in handling medical conversations and descriptions across different imaging modalities. MEDICAL VQA BENCHMARK Tab 5 shows the performance of BiMediX2 and other models on the Medical VQA benchmark using the MultiMedEval toolkit. Our BiMediX2 8B achieves the highest average score of 0.611, outperforming other models across datasets such as Rad-VQA, Slake-VQA, and Path-VQA. This demonstrates BiMediX2s proficiency in visual question answering, critical task in medical diagnostics. REPORT SUMMARIZATION Tab 6 presents the report summarization performance on the MIMIC-III dataset. BiMediX2 8B achieved the highest average score of 0.416, surpassing other models like LLaVA-Med (0.398) and Dragonfly-Med (0.110). This average score is derived as unified metric by re-scaling BLUE-4* and RadCliQ* metrics. This indicates BiMediX2s effectiveness in generating concise and accurate summaries of medical reports, vital task for efficient healthcare communication. REPORT GENERATION Tab 7 shows the report generation performance on the MIMIC-CXR dataset. BiMediX2 8B achieved the highest average score of 0.235, outperforming other models like LLaVA-Med (0.192) and BioMedGPT (0.145). The average score is derived as unified metric by re-scaling BLUE-4* and RadCliQ* metrics. This highlights BiMediX2s capability in generating detailed and accurate medical reports from radiology images, crucial task for diagnostic purposes. QUALITATIVE EXAMPLES Medical Image Understanding in Conversational Context: Fig 5 illustrates the capabilities of the BiMediX2 framework in analyzing medical images and providing detailed, context-aware responses. The top section showcases conversation involving sagittal CT scan of the lumbar spine. The model identifies the scan type and explains that it is vertical slice of the body focusing on the lumbar spine. When asked about abnormalities, the model accurately identifies fracture in"
        },
        {
            "title": "Technical Report",
            "content": "Dataset MIMIC-III Metric ROUGE-L BLEU-1 BLEU-4* F1-RadGraph RadCliQ* CheXbert vector METEOR Average LLaVA Med Dragonfly-Med BiMediX2 4B BiMediX2 8B 0.185 0.192 0.520 0.232 0.753 0.600 0. 0.398 0.072 0.062 0.000 0.000 0.247 0.326 0.060 0.110 0.209 0.153 0.410 0.222 0.923 0.633 0.264 0.402 0.205 0.178 0.449 0.230 0.918 0.593 0. 0.416 Table 6: Report Summarization (MultiMedEval Royer et al. (2024)) Dataset MIMIC-CXR Report Generation Metric F1-RadGraph BLEU-1 BLEU-4* ROUGE-L RadCliQ* CheXbert vector METEOR Average RadFM LLaVA Med BioMedGPT BiMediX2 4B BiMediX2 8B 0.042 0.006 0.000 0.065 0.655 0.197 0.053 0.145 0.048 0.163 0.060 0.125 0.660 0.150 0.137 0.192 0.000 0.003 0.000 0.012 0.827 0.153 0. 0.145 0.083 0.046 0.042 0.131 0.865 0.205 0.107 0.211 0.098 0.155 0.016 0.153 0.860 0.189 0.174 0.235 Table 7: Report Generation (MultiMedEval Royer et al. (2024)) the L4 vertebra, providing an explanation of the condition and its potential causes, such as trauma or stress. The bottom section features color Doppler ultrasound scan of the left ovary, identifying potential abnormality left ovarian cyst with solid component. Our model explains the imaging technique, names the organ, and discusses the detected abnormality, emphasizing the need for further evaluation. These examples demonstrate BiMediX2s ability to interpret complex medical images and provide informative responses to assist in clinical decision-making. Medical Image Understanding in Conversational and Bilingual (Arabic) Setting: Fig 6 illustrates BiMediX2s ability to understand medical imagery and converse in Arabic, showcasing its bilingual capabilities. In the top section, our model accurately identifies the organ and the type of scan, providing clear and precise information that can assist medical professionals in diagnosing and treating liver-related conditions. The middle section shows that our model identifies the body part and the type of scan. And in the bottom section, BiMediX2 accurately identifies the organ, explains the imaging technique, and detects abnormalities, providing valuable insights that can aid in the diagnosis and treatment of conditions related to the female reproductive system. This particular example showcases BiMediX2s capability to converse in both English and Arabic simultaneously. Medical Image Understanding of our BiMediX2 in Multidomain: Fig 7 showcases BiMediX2s versatility and accuracy across various medical imaging modalities. In the top section, it identifies key structures in scanning electron micrograph of mosquito head. In subsequent sections, it correctly interprets an MRI of parotid tumor, histology slide of adipose tissue (including stain type), chest X-ray with pneumothorax, and CT scan of the abdomen, pinpointing adrenal abnormalities. These examples demonstrate BiMediX2s capability to analyze and diagnose diverse imaging types, making it valuable tool across medical specialties."
        },
        {
            "title": "5 CONCLUSION",
            "content": "BiMediX2 represents leap forward in bilingual, multimodal medical AI, addressing the global need for accessible and inclusive healthcare solutions in both English and Arabic. By integrating text and visual modalities within unified architecture, it enables seamless multi-turn interactions for diverse medical tasks, including medical image analysis and complex medical conversations. Key contributions include the comprehensive bilingual dataset, BiMed-V, which provides diverse multimodal medical instructions tailored for both languages, and the introduction of BiMed-MBench, the first bilingual GPT-4o-based medical benchmark, which showcases the models ability to excel in wide range of expert-verified medical scenarios. BiMediX2 paves the way for inclusive, multilingual, and multimodal healthcare applications, significantly enhancing the accessibility and quality of medical assistance worldwide."
        },
        {
            "title": "Technical Report",
            "content": "Figure 5: Qualitative Examples of our BiMediX2 for Medical Image Understanding in Conversational Context."
        },
        {
            "title": "6 LIMITATIONS",
            "content": "Despite its overall improvement, BiMediX2, like other generative language models, may experience issues such as hallucinations, toxicity, and stereotypes. These issues stem from both the inherited limitations of the base models and the nature of the pretraining data. While we have conducted automatic and qualitative evaluations with medical professionals, we acknowledge that our models medical diagnoses and recommendations may not always be accurate. Extensive human evaluation is more reliable but expensive and time-consuming. The exploration of alternative solutions remains an important focus for ongoing research. Currently, our models lack explicit mechanisms to curb undesirable behaviors. Future research will focus on enhancing security and alignment strategies. On brighter note, we believe that releasing our model weights could contribute to investigating and mitigating risks tied to application uses."
        },
        {
            "title": "7 SAFETY AND ETHICAL IMPLICATIONS",
            "content": "We recognize the significant societal impact of BiMediX2 and emphasize the importance of ethical considerations and transparency. This release is intended for research purposes only and is not ready for clinical or commercial use. Ensuring the models accuracy and reliability is crucial, as incorrect medical advice could have serious health consequences. Robust validation and quality control measures are essential to minimize errors."
        },
        {
            "title": "Technical Report",
            "content": "Figure 6: Qualitative Examples of our BiMediX2 for Medical Image Understanding in Conversational and Bilingual (Arabic) Setting: The top section translates to; User: What is the organ shown in the image? BiMediX2: The image shows the liver, which is large and vital organ in the human body. User: What type of scan is this? BiMediX2: This is computed tomography (CT) scan of the abdomen. The middle section translates to; User: What type of imaging technique is used in this image? BiMediX2: The imaging technique used in this image is magnetic resonance imaging (MRI). User: What part of the body is shown in the image? BiMediX2: The image shows the head and neck area, with particular focus on the pituitary gland. And the bottom section translates to; User: What is the organ shown in the image? BiMediX2: The organ shown in the image is the ovary, which is part of the female reproductive system. User: What type of scan is this? BiMediX2: This is an ultrasound image, which is non-invasive imaging technique that uses high-frequency sound waves to create detailed images of the bodys internal structures. User: What is the abnormality in this ultrasound image? BiMediX2: The abnormality in this ultrasound image is the presence of multilocular ovarian cyst. Ethical considerations include protecting patient privacy and ensuring the confidentiality of medical data. The model must comply with relevant data protection regulations and ethical guidelines."
        },
        {
            "title": "Technical Report",
            "content": "Figure 7: Qualitative Examples for Medical Image Understanding of our BiMediX2 in Multidomain: Capability of BiMediX2 to understand and interpret various medical imaging modalities and provide accurate responses. The examples include describing the anatomy of mosquitos head in scanning electron micrograph, detecting left-sided parotid tumor in an MRI scan, recognizing mature adipose tissue in histology slide, identifying right-sided pneumothorax in chest X-ray, and identifying bilateral adrenal hyperplasia in CT scan of the abdomen. These examples highlight BiMediX2s versatility and effectiveness in medical image analysis and diagnosis."
        },
        {
            "title": "Technical Report",
            "content": "Addressing potential biases in the models outputs is also critical for ensuring fair and equitable healthcare outcomes. Collaboration with patients, medical professionals, and ethicists is essential for ethical oversight and further research to ensure safety and accuracy in clinical settings. By acknowledging and addressing these considerations, we can continue to refine BiMediX2 for safe and effective use in healthcare."
        },
        {
            "title": "8 ACKNOWLEDGEMENT",
            "content": "We would like to thank Dr. Omair Mohammed, Dr. Mohammed Zidan and Dr. Vishal Thomas Oommen for their contribution in verification of medical responses. The computations were enabled by resources provided by LUMI hosted by CSC (Finland) and LUMI consortium, and by Berzelius resource provided by the Knut and Alice Wallenberg Foundation at the NSC. We are grateful to the Meta Llama Impact Innovation Awards for recognizing BiMediX2, as one of the winners awarded in October 2024. This recognition highlights our commitment to advancing AI-driven healthcare solutions."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Asma Alkhaldi, Raneem Alnajim, Layan Alabdullatef, Rawan Alyahya, Jun Chen, Deyao Zhu, Ahmed Alsinan, and Mohamed Elhoseiny. Minigpt-med: Large language model as general interface for radiology diagnosis. arXiv preprint arXiv:2407.04106, 2024. Malaikannan Sankarasubbu Ankit Pal. Openbiollms: Advancing open-source large language models for healthcare and life sciences. https://huggingface.co/aaditya/ OpenBioLLM-Llama3-70B, 2024. Kezhen Chen, Rahul Thapa, Rahul Chalamala, Ben Athiwaratkun, Shuaiwen Leon Song, and James Zou. Dragonfly: Multi-resolution zoom supercharges large visual-language model, 2024. URL https://arxiv.org/abs/2406.00977. Zeming Chen, Alejandro Hernandez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Kopf, Amirkeivan Mohtashami, et al. Meditron-70b: Scaling medical pretraining for large language models. arXiv preprint arXiv:2311.16079, 2023. Clement Christophe, Praveen Kanithi, Prateek Munjal, Tathagata Raha, Nasir Hayat, Ronnie Rajan, Ahmed Al-Mahrooqi, Avani Gupta, Muhammad Umar Salman, Gurpreet Gosal, Bhargav Kanakiya, Charles Chen, Natalia Vassilieva, Boulbaba Ben Amor, Marco AF Pimentel, and Shadab Khan. Med42 evaluating fine-tuning strategies for medical llms: Full-parameter vs. parameter-efficient approaches, 2024. URL https://arxiv.org/abs/2404.14779. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/ 12608602. Tianyu Han, Lisa Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander Loser, Daniel Truhn, and Keno Bressem. Medalpacaan open-source collection of medical conversational ai models and training data. arXiv preprint arXiv:2304.08247, 2023. Xuehai He, Zhuo Cai, Wenlan Wei, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathological visual question answering. arXiv preprint arXiv:2010.12435, 2020. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, arXiv preprint and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv:2106.09685, 2021. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: dataset for biomedical research question answering. arXiv preprint arXiv:1909.06146, 2019. Alistair EW Johnson, Tom Pollard, Lu Shen, Li-wei Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger Mark. Mimic-iii, freely accessible critical care database. Scientific data, 3(1):19, 2016."
        },
        {
            "title": "Technical Report",
            "content": "Alistair EW Johnson, Tom Pollard, Seth Berkowitz, Nathaniel Greenbaum, Matthew Lungren, Chih-ying Deng, Roger Mark, and Steven Horng. Mimic-cxr, de-identified publicly available database of chest radiographs with free-text reports. Scientific data, 6(1):317, 2019. Navreet Kaur, Monojit Choudhury, and Danish Pruthi. Evaluating large language models for healthrelated queries with presuppositions. arXiv preprint arXiv:2312.08800, 2023. Jason Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):110, 2018. Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large language-and-vision assistant for biomedicine in one day. arXiv preprint arXiv:2306.00890, 2023. Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi In InXie. Pmc-clip: Contrastive language-image pre-training using biomedical documents. ternational Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 525536. Springer, 2023. Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: semanticallylabeled knowledge-enhanced dataset for medical visual question answering. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), pp. 16501654. IEEE, 2021. Meta. Meta llama 3.1. https://huggingface.co/meta-llama/Meta-Llama-3. 1-8B-Instruct, 2024. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: large-scale In Conference on multi-subject multi-choice dataset for medical domain question answering. Health, Inference, and Learning, pp. 248260. PMLR, 2022. Sara Pieri, Sahal Shaji Mullappilly, Fahad Shahbaz Khan, Rao Muhammad Anwer, Salman Khan, Timothy Baldwin, and Hisham Cholakkal. Bimedix: Bilingual medical mixture of experts llm, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad S. Khan. Llava++: Extending visual capabilities with llama-3 and phi-3, 2024. URL https://github.com/mbzuai-oryx/ LLaVA-pp. Corentin Royer, Bjoern Menze, and Anjany Sekuboyina. Multimedeval: benchmark and toolkit for evaluating medical vision-language models, 2024. Kai Zhang, Rong Zhou, Eashan Adhikarla, Zhiling Yan, Yixin Liu, Jun Yu, Zhengliang Liu, Xun Chen, Brian Davison, Hui Ren, et al. generalist visionlanguage foundation model for diverse biomedical tasks. Nature Medicine, pp. 113, 2024."
        }
    ],
    "affiliations": [
        "Govt Medical College Kozhikode",
        "Linkoping University",
        "Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI)",
        "Shaikh Tahnoon bin Mohammed Medical City (STMC)",
        "Sheikh Shakhbout Medical City (SSMC)",
        "Tawam Hospital"
    ]
}