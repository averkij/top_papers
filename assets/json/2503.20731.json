{
    "paper_title": "RecTable: Fast Modeling Tabular Data with Rectified Flow",
    "authors": [
        "Masane Fuchi",
        "Tomohiro Takagi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Score-based or diffusion models generate high-quality tabular data, surpassing GAN-based and VAE-based models. However, these methods require substantial training time. In this paper, we introduce RecTable, which uses the rectified flow modeling, applied in such as text-to-image generation and text-to-video generation. RecTable features a simple architecture consisting of a few stacked gated linear unit blocks. Additionally, our training strategies are also simple, incorporating a mixed-type noise distribution and a logit-normal timestep distribution. Our experiments demonstrate that RecTable achieves competitive performance compared to the several state-of-the-art diffusion and score-based models while reducing the required training time. Our code is available at https://github.com/fmp453/rectable."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 1 3 7 0 2 . 3 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "RECTABLE: FAST MODELING TABULAR DATA WITH RECTIFIED FLOW Masane Fuchi & Tomohiro Takagi Meiji University ce235031@meiji.ac.jp"
        },
        {
            "title": "ABSTRACT",
            "content": "Score-based or diffusion models generate high-quality tabular data, surpassing GAN-based and VAE-based models. However, these methods require substantial training time. In this paper, we introduce RecTable, which uses the rectified flow modeling, applied in such as text-to-image generation and text-tovideo generation. RecTable features simple architecture consisting of few stacked gated linear unit blocks. Additionally, our training strategies are also simple, incorporating mixed-type noise distribution and logit-normal timestep distribution. Our experiments demonstrate that RecTable achieves competitive performance compared to the several state-of-the-art diffusion and score-based models while reducing the required training time. Our code is available at https://github.com/fmp453/rectable."
        },
        {
            "title": "INTRODUCTION",
            "content": "Tabular data is fundamental data format utilized across various domains, including science, finance Li et al. (2021), medicine Wang et al. (2024b), healthcare Ghasemi & Amyot (2016), and e-commerce Nederstigt et al. (2014). Its characteristics encompass aspects such as dataset size, the diversity of categorical features, the distributional properties of numerical data, and the presence or absence of privacy-sensitive information. Tabular data is widely applied in practical scenarios, including data analysis, missing value imputation Zheng & Charoenphakdee (2022), data augmentation Fonseca & Bacao (2023), anomaly detection Shenkar & Wolf (2022), and simulation. In machine learning, constructing highly accurate models requires diverse and sufficiently large datasets. However, real-world data is often inaccessible due to challenges such as privacy regulations, high data acquisition costs, data imbalance, and the presence of missing values. 0.95 Shorter training and high performance TabSyn 0.9 RecTable (Ours) TabDiff GReaT STaSy TabDDPM CoDi A 0.85 0.8 103 GOGGLE 103.5 104 Training Time (sec) Figure 1: Training time and Machine Learning Efficiency score on the adult dataset. Our proposed method, RecTable, maintains the high performance in downstream task and shorten training time. To address these challenges, research has been conducted in recent years on methods for generating high-quality tabular data. Traditionally, data augmentation techniques such as SMOTE Chawla et al. (2002) and its variants have been widely employed Mukherjee & Khushi (2021); Han et al. (2005); Bunkhumpornpat et al. (2009). However, these methods struggle to accurately model data distributions and are inherently limited in the diversity of generated data due to their deterministic generation processes. In contrast, the advancement of deep learning has driven substantial progress in tabular data generation through generative models. Early studies introduced approaches based on generative adversarial networks (GANs) Goodfellow et al. (2014) and variational autoencoders (VAEs) Kingma & Welling (2022), which enabled more flexible and higher-quality data generation compared to traditional techniques Chow & Liu (1968);"
        },
        {
            "title": "Preprint",
            "content": "Srivastava et al. (2017). More recently, methods leveraging the success of foundation models Bommasani et al. (2022), including large language models (LLMs) and diffusion models Ho et al. (2020); Sohl-Dickstein et al. (2015), have gained widespread adoption Kotelnikov et al. (2023); Shi et al. (2025); Zhang et al. (2024a); Kim et al. (2023); Lee et al. (2023). These approaches offer significant advantages in learning complex tabular data distributions and generating high-quality synthetic datasets. However, methods based on LLMs and diffusion models Sohl-Dickstein et al. (2015); Ho et al. (2020) face significant challenges related to computational costs. LLMs generate data sequentially at the token level, requiring numerous inference steps, which increases generation time. Similarly, diffusion models produce data through an iterative denoising process, necessitating multiple inference steps. These factors not only lead to high computational costs during inference but also demand substantial time and computational resources for model training. Addressing these challenges is crucial for the practical application of tabular data generation. In this paper, we introduce RecTable, tabular data generation method that offers faster training compared to methods based on LLMs or diffusion models. RecTable uses rectified flow Liu et al. (2023b) employed in models such as Stable Diffusion 3Esser et al. (2024) and Frieren Wang et al. (2024a). It features simple architecture and an ℓ2 loss function, enabling efficient training. To further reduce training time, RecTable avoids transformer-based architectures, as reducing the number of parameters is crucial for computational efficiency. Instead, it incorporates Gated Linear Units (GLU) Dauphin et al. (2017); Narang et al. (2021) to enhance accuracy. Additionally, we introduce three modifications to the standard rectified flow training process. Specifically, we adopt the logitnormal timestep distribution ATCHISON & SHEN (1980), as used in Stable Diffusion 3, and adjust the noise distribution to better accommodate mixed-type data. We demonstrate that RecTable achieves competitive performance compared to diffusion-based methods while outperforming GANs and VAEs. On the adult dataset (containing 32,561 training samples), as shown in Figure 1, RecTable achieves shorter training time than existing methods while maintaining the quality of generated data. Furthermore, on some datasets, RecTable surpasses stateof-the-art methods in terms of both the diversity and quality of generated samples."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "2.1 TABULAR DATA GENERATION WITH DEEP LEARNING The application of generative models to tabular data is crucial for real-world scenarios. Tabular data is inherently complex, as it consists of both numerical and categorical features. Categorical features are often imbalanced, posing challenges for effective modeling. To address this, CTGAN and TVAE, introduced by Xu et al. (2019), use GANs and VAEs, respectively, and remain de facto standards for tabular data generation. GOGGLE Liu et al. (2023a) was proposed as an encoderdecoder model based on VAEs. It explicitly captures dependency relationships between columns by representing them as graph structure. Inspired by advancements in natural language processing, GReaT Borisov et al. (2023) applies language modeling approach to tabular data by fine-tuning GPT-2 Radford et al. (2019), treating each column as sequence of natural language tokens. Denoising diffusion models Sohl-Dickstein et al. (2015); Ho et al. (2020), which have demonstrated remarkable success in image generation Dhariwal & Nichol (2021); Saharia et al. (2022), have been extensively studied for tabular data generation. Several methods, including TabDDPM Kotelnikov et al. (2023), STaSy Kim et al. (2023), CoDi Lee et al. (2023), CDTD Mueller et al. (2025), and TabDiff Shi et al. (2025), apply diffusion models directly in data space. In image generation, Latent Diffusion Models Rombach et al. (2022) have emerged as the mainstream approach, significantly reducing computational complexity by projecting data into latent space. TabSyn Zhang et al. (2024a) extends this idea to tabular data, achieving comparable performance to conventional diffusion models while requiring lower computational cost."
        },
        {
            "title": "2.2 RECTIFIED FLOW AND ITS APPLICATION",
            "content": "Diffusion models, which transform distributions using stochastic differential equations, are known to be inefficient for both training and generation. In contrast, Flow Matching Lipman et al. (2023), an extension of Continuous Normalizing Flow Chen et al. (2018), employs ordinary differential equations for distribution transformation, offering more efficient alternative. Flow Matching, which learns vector fields to map between distributions, has been improved in various ways within the framework of optimal transport. Among these advancements, Rectified Flow Albergo & VandenEijnden (2023); Liu et al. (2023b), which utilizes linear interpolations, is particularly well-suited for high-dimensional distributions such as images due to its simplicity and scalability. Notably, such as Stable Diffusion 3 Esser et al. (2024) and InstaFlow Liu et al. (2024) use rectified flow to enable faster generation while maintaining or even improving output quality. Beyond image generation, the application of rectified flow is being actively explored in other domains, including video generation Wang et al. (2024a), audio generation Guan et al. (2024); Guo et al. (2024), and language modeling Zhang et al. (2024b)."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 PRELIMINARIES: RECTIFIED FLOW Rectified Flow learns transport map : Rd Rd that transforms data point z0 Rd sampled from the source distribution π0 to corresponding data point z1 Rd sampled from the target distribution π1. In generative modeling, π0 is typically known noise distributioncommonly gaussian distribution (0, 1)while π1 represents the unknown data distribution to be learned. Rectified Flow trains velocity field vθ, parameterized by deep neural network, using the following loss function: = E(z0,z1)(π0,π1),tpt[vθ(zt, t) (z1 z0)2 2] (1) where zt is an interpolated state defined as zt = tz1 + (1 t)z0, and pt denotes the timestep distribution. To generate samples, we solve the following ordinary differential equation from = 1 to = dzt = vθ(zt, t)dt. (2) 3.2 NETWORK ARCHITECTURE We replace the MLP blocks in the TabDDPM architecture Kotelnikov et al. (2023) with Gated Linear Unit (GLU) blocks Dauphin et al. (2017); Narang et al. (2021) to better capture nonlinear relationships between features. The GLU block used in RecTable is described as: GLU(x) = Dropout ((xW1 + b1) Sigmoid (xW2 + b2)) (3) where W1, W2 are the weight matrices of the linear layers, and denotes the Hadamard product (element-wise product). Compared to the attention mechanism Vaswani et al. (2017), GLU requires fewer parameters, which contributes to reduction in training time. Following TabDDPM, we embed the timestep information using sinusoidal time embedding, as proposed in Ho et al. (2020) and Dhariwal & Nichol (2021). emb = Linear(SiLU(Linear(SinTimeEmb(t)))) (4) All linear layers in timestep embedding have 128 dimensions."
        },
        {
            "title": "3.3 TRAINING STRATEGIES",
            "content": "We introduce three modifications to the standard training strategies of rectified flow."
        },
        {
            "title": "3.3.1 TIMESTEP DISTRIBUTION",
            "content": "In Stable Diffusion 3 Esser et al. (2024), various timestep distribution settings are considered. We use the best timestep distribution in their experiments, logit-normal distribution ATCHISON & SHEN (1980). It is represented as πln(t; m, s) = 1 2π 1 t(1 t) (cid:18) exp (logit(t) m)2 2s2 (cid:19) (5) where logit(t) = log , and are hyperparameters respectively. We use = 0, = 1. This 1 is the best setting in their experiments. 3.3.2 NOISE DISTRIBUTION In general, gaussian distribution is commonly used as the distribution π0. However, tabular data consists of mixed-type features: numerical and categorical. While numerical data typically follows gaussian distribution, categorical data follows categorical distribution. Let = [xnum, xcat1 , . . . , xcatC ] be tabular data sample with Nnum numerical features and catgeorical features. Each categorical feature xcati has Ki cardinalities and is represented using one-hot encoding xohe {0, 1}Ki. We assume that the numerical features follow gaussian distribution, cati xnum (0, 1), while the categorical features follow categorical distributions, xcati Cati. To match these distributions during training, we adopt hybrid noise model: gaussian distribution for numerical features and uniform distribution for categorical features. The initial noise z0 π0 is thus defined as: z0 = [znum, zcat1, . . . , zcatC ], znum (0, 1), zcati U(1, Ki) (6) 3.3.3 REFLOW Reflow Liu et al. (2023b) is rectified flow process that utilizes real noise and corresponding synthetic data. This process generates ODE straight trajectories after being repeated times to enable fast generation. However, maintaining strict trajectory straightness is not prerequisite for fast generation Wang et al. (2025). Based on this insight, we do not execute the reflow process."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETTINGS Datasets. Following Zhang et al. (2024a), we use six real-world tabular datasets: Adult Becker & Kohavi (1996), Default Yeh (2009), Shoppers Sakar & Kastro (2018), Magic Bock (2004), Beijing Chen (2015), and News Fernandes & Sernadela (2015). Each dataset has numerical and categorical column. More details are shown in Appendix A. Baselines. We compare the proposed RecTable with nine methods. GAN-based model: CTGAN Xu et al. (2019). VAE-based models: TVAE Xu et al. (2019) and GOGGLE Liu et al. (2023a). Autoregressive language model: GReaT Borisov et al. (2023). Diffusion-based models: TabDDPM Kotelnikov et al. (2023), CoDi Lee et al. (2023), STaSy Kim et al. (2023), TabSyn Zhang et al. (2024a), and TabDiff Shi et al. (2025). Evaluation Methods. Following previous studies Zhang et al. (2024a); Shi et al. (2025), We evaluate the generated data from two aspects. 1) Fidelity: Shape, Trend, detection score (C2ST) LopezPaz & Oquab (2017), α-Precision Alaa et al. (2022) and β-Recall Alaa et al. (2022) assess how well"
        },
        {
            "title": "Preprint",
            "content": "Method CTGAN1 TVAE1 GOGGLE1 GReaT1 STaSy1 CoDi1 TabDDPM1 TabSyn1 TabDiff1 Table 1: Performance comparison on the error rates (%) of Shape. Adult Default Shoppers Magic Beijing News Average 16.84 0.03 14.220.08 16.97 12.120.04 11.290.06 21.380.06 1.750.03 0.810.05 0.630.05 16.830.04 10.170.05 17.02 19.940.06 5.770.06 15.77 0.07 1.57 0.08 1.010.08 1.240.07 21.150.10 24.510.06 22.33 14.510.12 9.370.09 31.840.05 2.720.13 1.440.07 1.280. 9.810.08 8.250.06 1.90 16.160.09 6.290.13 11.560.26 1.010.09 1.030.14 0.780.08 21.390.05 19.160.06 16.93 8.250.12 6.710.03 16.940.02 1.300.03 1.260.05 1.030.05 16.090.02 16.620.03 25.32 N/A2 6.890.03 32.270.04 78.750.01 2.060.04 2.350.03 17.02 15.49 16.75 14.20 7.72 21.63 14.53 1.27 1.22 3.79 RecTable 3.630.07 1.740.04 4.440.17 1.210.12 5.420.10 6.280. 1 The results of all baselines are taken from Shi et al. (2025). 2 The N/A represents the results are not provided in Shi et al. (2025) and Zhang et al. (2024a)due to outof-memory. the generated data can faithfully recover the ground-truth data distribution. 2): Machine Learning Efficiency (MLE): To evaluate the utility of the generated data for downstream machine learning tasks, we first split the real data into training, validation, and test set. The generative models are trained on the training set and then used to generate synthetic data of the same size as the original training set. We train classification or regression model using XGBoost Chen & Guestrin (2016) on the synthetic data and evaluate their performance on the test set. MLE performance is measured using the AUC score for classification tasks and RMSE for regression tasks. We report the mean and standard deviation of the AUC and RMSE score over 20 independent experiments. 4.2 IMPLEMENTATION DETAILS We implement RecTable using PyTorch 2.3.1 Paszke et al. (2019). RecTable has four GLU Blocks and one MLP head, the hidden sizes are 1024, 2048, 1024, and 1024, respectively. The model is optimized using Adam optimizer Kingma & Ba (2017) with learning rate of 2 104 and β1 = 0.9. RecTable is trained for 30,000 iterations with batch size of 4096 using the ℓ2 loss function. All experiments except GReaT is conducted on an NVIDIA RTX A5000 GPU with 24GB memory (GReaT is trained on four NVIDIA RTX A5000 GPUs). To satisfy xnum (0, 1), we use QuantileTransformer1 from Scikit-learn Pedregosa et al. (2011) for numerical features as preprocessing. For one-hot encoding categorical features, we also use scikit-learn. We use the Runge-Kutta method of order 5(4) from Scipy Virtanen et al. (2020) for generation following Liu et al. (2023b). 4.3 RESULTS OF DATA FIDELITY The results for Shape and Trend are presented in Tables 1 and 2, respectively. In both metrics, RecTable performs competitively, ranking just below TabDiff and TabSyn. Since RecTable lags behind state-of-the-art methods in Shape, we consider that capturing complex column-to-column relationships remains challenge. Notably, when compared to TabDDPMwhich shares similar architectureTabDDPMs simple MLP-based design excels on low-dimensional datasets. However, as dimensionality increases (e.g., in the news dataset), the performance of MLP-based models declines, whereas RecTables GLU-based architecture effectively mitigates this deterioration. In contrast to Shape, RecTable achieves state-of-the-art performance on certain datasets in Trend evaluation as shown in Table 2. Similar to Shape, it also demonstrates robustness against performance degradation in high-dimensional datasets such as the news. Moreover, comparison with Table 6 indicates that the scores in Tables 1 and 2 do not always directly correlate with downstream task performance. Following Zhang et al. (2024a) and Shi et al. (2025), we calculate α-Precision, which measures generation quality, and β-Recall, which measures how well the synthetic data covers the real data 1https://scikit-learn.org/stable/modules/generated/sklearn. preprocessing.QuantileTransformer.html"
        },
        {
            "title": "Preprint",
            "content": "Method CTGAN1 TVAE1 GOGGLE1 GReaT1 STaSy1 CoDi1 TabDDPM1 TabSyn1 TabDiff1 Table 2: Performance comparison on the error rates (%) of Trend. Adult Default Shoppers Magic Beijing News Average 20.231.20 14.150.88 45.29 17.590.22 14.510.25 22.490.08 3.010.25 1.930.07 1.490.16 26.950.93 19.500.95 21.94 70.020.12 5.960.26 68.410.05 4.890.10 2.810.48 2.550.75 13.080.16 18.670.38 23.90 45.160.18 8.490.15 17.780.11 6.610.16 2.130.10 1.740. 7.000.19 5.820.49 9.47 10.230.40 6.610.53 6.530.25 1.700.22 0.880.18 0.760.12 22.950.08 18.010.08 45.94 59.600.55 8.000.10 7.070.15 2.710.09 3.130.34 2.590.15 5.370.05 6.170.09 23.19 N/A2 3.070.04 11.100.01 13.160.11 1.520.03 1.280.04 15.93 13.72 28.29 44.24 7.77 23.23 5.35 2.07 1.73 2.82 RecTable 6.040.07 1.130.03 3.510.37 1.800.55 2.410.07 2.030. 1 The results of all baselines are taken from Shi et al. (2025). 2 The N/A represents the results are not provided in Shi et al. (2025) and Zhang et al. (2024a) due to out-of-memory. distribution. Tables 3 and 4 are presented the results of α-Precision and β-Recall, respectively. We observe similar trend to Shape results in Table 3. However, RecTable achives the highest β-Recall scores in Table 4, indicating that it can generate diverse samples. In the news dataset, which TabDDPM cannot generate meaningful samples according to Zhang et al. (2024a), RecTable successfully generates meaningful samples with high-quality. We consider that GLU contributes to complement nonlinear relationships between features. Table 3: Comparison of α-Precision scores. Higher scores indicate better performance. Adult Default Shoppers Magic Beijing News Average Ranking Methods CTGAN1 TVAE1 GOGGLE1 GReaT1 STaSy1 CoDi1 TabDDPM1 TabSyn1 TabDiff Methods CTGAN1 TVAE1 GOGGLE1 GReaT1 STaSy1 CoDi1 TabDDPM1 TabSyn1 TabDiff1 77.740.15 98.170.17 50.68 55.790.03 82.870.26 77.580.45 96.360.20 99.390.18 99.020.20 62.080.08 85.570.34 68.89 85.900.17 90.480.11 82.380.15 97.590.36 98.650.23 98.490.28 76.970.39 58.190.26 86.95 78.880.13 89.650.25 94.950.35 88.550.68 98.360.52 99.110.34 86.900.22 86.190.48 90.88 85.460.54 86.560.19 85.010.36 98.590.17 99.420.28 99.470.21 96.270.14 97.200.10 88.81 98.320.22 89.160.12 98.130.38 97.930.30 97.510.24 98.060. 96.960.17 86.410.17 86.41 N/A2 94.760.33 87.150.12 0.000.00 95.050.30 97.360.17 RecTable 86.700.26 97.610.27 93.240.61 99.200. 97.960.26 95.360.27 1 The results of all baselines are taken from Shi et al. (2025). 2 The N/A represents the results are not provided in Shi et al. (2025) and Zhang et al. (2024a)due to out-of-memory. Table 4: Comparison of β-Recall scores. Higher scores indicate better results. Adult Default Shoppers Magic Beijing News Average Ranking 30.800.20 38.870.31 8.80 49.120.18 29.210.34 9.200.15 47.050.25 47.920.23 51.640. 18.220.17 23.130.11 14.38 42.040.19 39.310.39 19.940.22 47.830.35 46.450.35 51.090.25 31.800.350 19.780.10 9.79 44.900.17 37.240.45 20.820.23 47.790.25 49.100.60 49.750.64 11.750.20 32.440.35 9.88 34.910.28 53.970.57 50.560.31 48.460.42 48.030.50 48.010.31 34.800.10 28.450.08 19.87 43.340.31 54.790.18 52.190.12 56.920.13 59.150.22 59.630.23 24.970.29 29.660.21 2.03 N/A2 39.420.32 34.400.31 0.000.00 43.010.28 42.100.32 RecTable 41.750.27 48.040.46 53.720.49 57.710.33 54.700.25 57.550. 1 The results of all baselines are taken from Shi et al. (2025). 2 The N/A represents the results are not provided in Shi et al. (2025) and Zhang et al. (2024a)due to out-of-memory. We further investigate the similarity between synthetic and real data using C2ST test Lopez-Paz & Oquab (2017), which measures the difficulty of distinguishing synthetic data from real data. We present the results in Table 5. We observe similar trend to the Shape and the Trend results. We 6 82.82 85.29 78.77 80.87 88.91 87.53 79.84 98.06 98.59 95.01 6 8 10 7 4 5 9 2 3 25.39 28.72 10.79 43.34 42.32 38.19 41.34 48.94 50.37 52.25 9 8 10 4 5 7 6 3"
        },
        {
            "title": "Preprint",
            "content": "believe that C2ST scores are due to the poor Shape score and the failure of the generated data to reproduce the relationships between columns in the real data. Table 5: Detection score (C2ST) using logistic regression classifier. Higher scores indicate superior performance. Adult Default Shoppers Magic Beijing News Average Ranking Method CTGAN1 TVAE1 GOGGLE1 GReaT1 STaSy1 CoDi1 TabDDPM1 TabSyn1 TabDiff1 0.5949 0.6315 0.1114 0.5376 0.4054 0.2077 0.9755 0.9910 0.9950 0.4875 0.6547 0.5163 0.4710 0.6814 0.4595 0.9712 0.9826 0.9774 RecTable 0. 0.9057 0.7488 0.2962 0.1418 0.4285 0.5482 0.2784 0.8349 0.9662 0.9843 0.8358 0.6728 0.7706 0.9526 0.4326 0.6939 0.7206 0.9998 0.9960 0.9989 0.7531 0.8659 0.4779 0.6893 0.7922 0.7177 0.9513 0.9528 0.9781 0.6947 0.4076 0.0745 N/A2 0.5287 0.0201 0.0002 0.9255 0. 0.6586 0.6044 0.3791 0.5118 0.6083 0.4007 0.7888 0.9690 0.9774 0.9726 0.7152 0.7829 0.8366 5 7 10 8 6 9 4 2 3 1 The results of all baselines are taken from Shi et al. (2025). 2 The N/A represents the results are not provided in Shi et al. (2025) and Zhang et al. (2024a)due to out-of-memory. 4.4 RESULTS OF MACHINE LEARNING EFFICIENCY We present the MLE results for each dataset in Table 6. In this evaluation, an XGBoost classifier or regressor is trained using the generated samples, and its performance is measured on the test data. If the quality of the generated data is close to that of real data, the scores of the classifier or regressor is expected to achive high scores. From the results in Table 6, RecTable achieves state-of-the-art performance on half of the datasets, which indicates that high-quality generation is feasible. Overall, its performance is comparable to that of the state-of-the-art method, TabDiff. Even for the datasets where the scores are low in Tables 1 and 2, MLE demonstrates high performance. Remarkably, in the news dataset, RecTable achives results that surpass those obtained with real data. 4.5 TRAINING TIME We present the measured training time on the adult dataset in Table 7. RecTable achieves the fastest training time while maintaining high-quality data generation. Although its performance is competitive with state-of-the-art methods, it has the shortest training among them. We attribute this efficiency to RecTables simple architecture, loss function, and training strategy. Furthermore, RecTable achieves high performance with fewer updates than other state-of-the-art methods. For example, TabDiff, one of the leading approaches, is trained for 8000 epochs. By contrast, RecTable is trained for 30k iterations, which means approximately 4313 epochs with batch size of 4096. This suggests that RecTable can achieve high performance even with relatively small number of updates."
        },
        {
            "title": "5 ABLATION STUDIES",
            "content": "We use the adult dataset for ablation studies. 5.1 BACKBONE ARCHITECTURE We compare four backbone architectures; TabDDPM (MLP-based), STaSy (ConcatSquashLinearbased), TabDiff (Transfomrers and MLP-based), and RecTable. As shown in Table 8, TabDDPM is competitive results with RecTable. However, as demonstrated in Table 6, RecTable outperforms TabDDPM on the news dataset, suggests that the GLU-based architecture is more effective than the MLP-based. STaSy, whose architecture is inspired by Grathwohl et al. (2019), does not achieve high performance. TabDiff, consisting of Transfomrers and MLPs, requires long training time due to the"
        },
        {
            "title": "Preprint",
            "content": "Table 6: Evaluation of MLE (Machine Learning Efficiency): AUC and RMSE are used for classification and regression tasks, respectively. Default Shoppers Magic Beijing News Average Gap Methods Real1 CTGAN1 TVAE1 GOGGLE1 GReaT1 STaSy1 CoDi1 TabDDPM1 TabSyn1 TabDiff1 Adult AUC AUC AUC AUC RMSE RMSE .927.000 .770.005 .926. .946.001 .423.003 .842.002 .886.002 .878.004 .778.012 .913.003 .906.001 .871.006 .907.001 .909.001 .912.002 .696.005 .724.005 .584.005 .755.006 .752.006 .525.006 .758.004 .763.002 .763.005 .875.009 .871.006 .658.052 .902.005 .914.005 .865.006 .918.005 .914.004 .921. .855.006 .887.003 .654.024 .888.008 .934.003 .932.003 .935.003 .937.002 .936.003 .902.019 .770.011 1.09.025 .653.013 .656.014 .818.021 .592.011 .580.009 .555.013 .880.016 1.01.016 .877.002 N/A2 .871.002 1.21.005 4.863.04 .862.024 .866.021 % 0.0 24.5 20.9 43.6 13.3 10.9 30.5 9.14 7.43 6. 6.40 RecTable .906.002 .754.004 .904.010 .939. .555.011 .840.004 1 The results of all baselines are taken from Shi et al. (2025). 2 The N/A represents the results are not provided in Shi et al. (2025) and Zhang et al. (2024a)due to out-of-memory. Table 7: Training time comparison. We use the implementation of the baselines provided by Zhang et al. (2024a) and Shi et al. (2025). Methods GOGGLE STaSy GReaT CoDi TabDDPM TabSyn TabDiff RecTable Training Time (sec) 12752 6445 13773 25910 2393 6231 1800 Table 8: Results of MLE with different backbone architectures. Other settings are same as RecTables. Architecture TabDDPM STaSy TabDiff RecTable AUC Training Time (sec) .904.002 840 .487.046 1425 .905.001 .906.002 1800 high computational cost of the attention mechanism. Moreover, despite the long training time, its AUC score is not best of them."
        },
        {
            "title": "5.2 DETAILED SETTINGS",
            "content": "We compare the detailed settings in RecTable. We conduct two configurations: (config A) using gaussian distribution as the noise distribution π0 and logit-normal timestep distribution, and (config B) using combination of gaussian distribution and uniform distribution as π0 along with uniform timestep distribution. The results are shown in Table 9. The combination of gaussian distribution and uniform distribution is effective in improving the quality of generated samples. Although the application of logit-normal timestep distribution has slight effect on the quality of the generated data, its impact is not as significant as observed in Stable Diffusion 3 Esser et al. (2024). In addition, sampling timestep increases training time, which may be unnecessary if faster training is desired."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we presented RecTable for tabular data synthesis. RecTable incorporates rectified flow without reflow, simpler training strategies compared to diffusion models, combination of gaussian distribution for numerical data and uniform distribution for categorical data, gated linear unit-based architecture for fast training and high-quality generation, and logit-normal timestep dis-"
        },
        {
            "title": "Preprint",
            "content": "Table 9: Results of MLE with different detailed settings."
        },
        {
            "title": "Configuration",
            "content": "config A3 config B4 RecTable AUC Training Time (sec) .890.005 409 .905.001 600 .906.002 3 RecTable without combined distribution. 4 RecTable without logit-normal timestep distribution. tribution. We evaluated RecTable against several state-of-the-art baselines on six real-world datasets and confirmed that it maintains or outperforms these baselines in machine learning efficiency and diverse generation. We believe that the rectified flow framework has potential for high-quality tabular data synthesis. Despite RecTables simple architecture and training strategies, it achieves competitive results with state-of-the-art methods. This indicates that with more sophisticated architecture and training strategies, rectified flow could surpass denoising diffusion models."
        },
        {
            "title": "REFERENCES",
            "content": "Ahmed Alaa, Boris Van Breugel, Evgeny S. Saveliev, and Mihaela van der Schaar. How faithful is your synthetic data? Sample-level metrics for evaluating and auditing generative models. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 290306. PMLR, 1723 Jul 2022. URL https: //proceedings.mlr.press/v162/alaa22a.html. Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=li7qeBbCR1t. J. ATCHISON and S.M. SHEN. Logistic-normal distributions:some properties and uses. Biometrika, 67(2):261272, 08 1980. ISSN 0006-3444. doi: 10.1093/biomet/67.2.261. URL https:// doi.org/10.1093/biomet/67.2.261. Barry Becker and Ronny Kohavi. Adult. UCI Machine Learning Repository, 1996. DOI: https://doi.org/10.24432/C5XW20. R. Bock. MAGIC Gamma Telescope. UCI Machine Learning Repository, 2004. DOI: https://doi.org/10.24432/C52C8B. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Re, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tram`er, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models, 2022. URL https://arxiv.org/abs/2108.07258."
        },
        {
            "title": "Preprint",
            "content": "Vadim Borisov, Kathrin Sessler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. LanIn The Eleventh International Conferguage models are realistic tabular data generators. ence on Learning Representations, 2023. URL https://openreview.net/forum?id= cEygmQNOeI. Chumphol Bunkhumpornpat, Krung Sinapiromsaran, and Chidchanok Lursinsap. Safe-level-smote: Safe-level-synthetic minority over-sampling technique for handling the class imbalanced problem. In Thanaruk Theeramunkong, Boonserm Kijsirikul, Nick Cercone, and Tu-Bao Ho (eds.), Advances in Knowledge Discovery and Data Mining, pp. 475482, Berlin, Heidelberg, 2009. Springer Berlin Heidelberg. ISBN 978-3-642-01307-2. Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. J. Artif. Int. Res., 16(1):321357, June 2002. ISSN 1076-9757. Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/ paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf. Song Chen. Beijing PM2.5. https://doi.org/10.24432/C5JS49. UCI Machine Learning Repository, 2015. DOI: Tianqi Chen and Carlos Guestrin. Xgboost: scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 16, pp. 785794, New York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450342322. doi: 10.1145/2939672.2939785. URL https://doi.org/10. 1145/2939672.2939785. C. Chow and C. Liu. Approximating discrete probability distributions with dependence trees. IEEE Transactions on Information Theory, 14(3):462467, 1968. doi: 10.1109/TIT.1968.1054142. Synthetic Data Metrics. DataCebo, Inc., 2023. Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th convolutional networks. International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 933941. PMLR, 0611 Aug 2017. URL https://proceedings.mlr. press/v70/dauphin17a.html. Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id= AAWuCvzaVt. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 1260612633. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/v235/esser24a.html. Vinagre Pedro Cortez Paulo Fernandes, Kelwin and Pedro Sernadela. Online News Popularity. UCI Machine Learning Repository, 2015. DOI: https://doi.org/10.24432/C5NS3V. Joao Fonseca and Fernando Bacao. Tabular and latent space synthetic data generation: literISSN 2196-1115. doi: 10.1186/ ature review. s40537-023-00792-7. URL https://doi.org/10.1186/s40537-023-00792-7. Journal of Big Data, 10(1):115, Jul 2023. Mahdi Ghasemi and Daniel Amyot. Process mining in healthcare: systematised literature reInternational Journal of Electronic Healthcare, 9(1):6088, 2016. doi: 10.1504/IJEH. view. 2016.078745. URL https://www.inderscienceonline.com/doi/abs/10.1504/ IJEH.2016.078745."
        },
        {
            "title": "Preprint",
            "content": "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2014/ 2014. file/f033ed80deb0234979a61f95710dbe25-Paper.pdf. Generative adversarial nets. Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, and David Duvenaud. Scalable reversible generative models with free-form continuous dynamics. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rJxgknCcK7. Wenhao Guan, Qi Su, Haodong Zhou, Shiyu Miao, Xingjia Xie, Lin Li, and Qingyang Hong. Reflow-tts: rectified flow model for high-fidelity text-to-speech. In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 10501 10505, 2024. doi: 10.1109/ICASSP48485.2024.10447822. Yiwei Guo, Chenpeng Du, Ziyang Ma, Xie Chen, and Kai Yu. Voiceflow: Efficient text-toIn ICASSP 2024 - 2024 IEEE International Conferdoi: speech with rectified flow matching. ence on Acoustics, Speech and Signal Processing (ICASSP), pp. 1112111125, 2024. 10.1109/ICASSP48485.2024.10445948. Hui Han, Wen-Yuan Wang, and Bing-Huan Mao. Borderline-smote: new over-sampling method in imbalanced data sets learning. In De-Shuang Huang, Xiao-Ping Zhang, and Guang-Bin Huang (eds.), Advances in Intelligent Computing, pp. 878887, Berlin, Heidelberg, 2005. Springer Berlin Heidelberg. ISBN 978-3-540-31902-3. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 68406851. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2020/ 2020. file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf. Jayoung Kim, Chaejeong Lee, and Noseong Park. STasy: Score-based tabular data synthesis. In The Eleventh International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=1mNssCWt_v. Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization, 2017. URL https://arxiv.org/abs/1412.6980. Diederik Kingma and Max Welling. Auto-encoding variational bayes, 2022. URL https: //arxiv.org/abs/1312.6114. Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. TabDDPM: Modelling tabular data with diffusion models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1756417579. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/ v202/kotelnikov23a.html. Chaejeong Lee, Jayoung Kim, and Noseong Park. CoDi: Co-evolving contrastive diffusion models for mixed-type tabular synthesis. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1894018956. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/ v202/lee23i.html. Yiren Li, Zheng Huang, Junchi Yan, Yi Zhou, Fan Ye, and Xianhui Liu. Gfte: Graph-based financial table extraction. In Alberto Del Bimbo, Rita Cucchiara, Stan Sclaroff, Giovanni Maria Farinella, Tao Mei, Marco Bertini, Hugo Jair Escalante, and Roberto Vezzani (eds.), Pattern Recognition. ICPR International Workshops and Challenges, pp. 644658, Cham, 2021. Springer International Publishing. ISBN 978-3-030-68790-8."
        },
        {
            "title": "Preprint",
            "content": "Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t. Tennison Liu, Zhaozhi Qian, Jeroen Berrevoets, and Mihaela van der Schaar. GOGGLE: Generative modelling for tabular data by learning relational structure. In The Eleventh International Conference on Learning Representations, 2023a. URL https://openreview.net/forum?id= fPVRcJqspu. Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023b. URL https://openreview.net/forum?id=XVjTT1nw5z. Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and qiang liu. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= 1k4yZbbDqX. David Lopez-Paz and Maxime Oquab. Revisiting classifier two-sample tests. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum? id=SJkXfE5xx. Markus Mueller, Kathrin Gruber, and Dennis Fok. Continuous diffusion for mixed-type tabular In The Thirteenth International Conference on Learning Representations, 2025. URL data. https://openreview.net/forum?id=QPtoBPn4lZ. Mimi Mukherjee and Matloob Khushi. Smote-enc: novel smote-based method to generate synthetic data for nominal and continuous features. Applied System Innovation, 4(1):18, March 2021. ISSN 2571-5577. doi: 10.3390/asi4010018. URL http://dx.doi.org/10.3390/ asi4010018. Sharan Narang, Hyung Won Chung, Yi Tay, Liam Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou, Wei Li, Nan Ding, Jake Marcus, Adam Roberts, and Colin Raffel. Do transformer modifications transfer across implementations and applications? In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 57585773, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.465. URL https://aclanthology.org/2021.emnlp-main.465/. Lennart J. Nederstigt, Steven S. Aanen, Damir Vandic, and Flavius Frasincar. Floppies: framework for large-scale ontology population of product information from tabular data in e-commerce stores. Decision Support Systems, 59:296311, 2014. doi: https://doi. org/10.1016/j.dss.2014.01.001. URL https://www.sciencedirect.com/science/ article/pii/S0167923614000025. ISSN 0167-9236. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, deep learning library. and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/ paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf. Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. Scikit-learn: Machine learning in python. Journal of Machine Learning Research, 12(85): 28252830, 2011. URL http://jmlr.org/papers/v12/pedregosa11a.html. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019."
        },
        {
            "title": "Preprint",
            "content": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1068410695, June 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=08Yk-n5l2Al. C. Sakar and Yomi Kastro. Online Shoppers Purchasing Intention Dataset. UCI Machine Learning Repository, 2018. DOI: https://doi.org/10.24432/C5F88Q. Tom Shenkar and Lior Wolf. Anomaly detection for tabular data with internal contrastive learning. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=_hszZbt46bT. Juntong Shi, Minkai Xu, Harper Hua, Hengrui Zhang, Stefano Ermon, and Jure Leskovec. Tabdiff: mixed-type diffusion model for tabular data generation. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id= swvURjrt8z. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised In Francis Bach and David Blei (eds.), Prolearning using nonequilibrium thermodynamics. ceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 22562265, Lille, France, 0709 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/sohl-dickstein15.html. Akash Srivastava, Lazar Valkov, Chris Russell, Michael U. Gutmann, and Charles Sutton. Veegan: Reducing mode collapse in gans using implicit variational learning. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2017/ 2017. file/44a2e0804995faf8d2e3b084a1e2db1d-Paper.pdf. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2017/ 2017. file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C. J. Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antˆonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, Aditya Vijaykumar, Alessandro Pietro Bardelli, Alex Rothberg, Andreas Hilboll, Andreas Kloeckner, Anthony Scopatz, Antony Lee, Ariel Rokem, C. Nathan Woods, Chad Fulton, Charles Masson, Christian Haggstrom, Clark Fitzgerald, David A. Nicholson, David R. Hagen, Dmitrii V. Pasechnik, Emanuele Olivetti, Eric Martin, Eric Wieser, Fabrice Silva, Felix Lenders, Florian Wilhelm, G. Young, Gavin A. Price, Gert-Ludwig Ingold, Gregory E. Allen, Gregory R. Lee, Herve Audren, Irvin Probst, Jorg P. Dietrich, Jacob Silterra, James T. Webber, Janko Slaviˇc, Joel Nothman, Johannes Buchner, Johannes Kulick, Johannes L. Schonberger, Jose Vinıcius de Miranda Cardoso, Joscha Reimer, Joseph Harrington, Juan Luis Cano Rodrıguez, Juan Nunez-Iglesias, Justin Kuczynski, Kevin Tritz, Martin Thoma, Matthew Newville, Matthias Kummerer, Maximilian Bolingbroke, Michael Tartre, Mikhail Pak, Nathaniel J. Smith, Nikolai Nowaczyk, Nikolay Shebanov, Oleksandr Pavlyk, Per A. Brodtkorb, Perry Lee, Robert T. McGibbon, Roman Feldbauer, Sam Lewis, Sam Tygier, Scott Sievert, Sebastiano Vigna, Stefan Peterson, Surhud More, Tadeusz Pudlik, Takuya Oshima, Thomas J. Pingel, Thomas P."
        },
        {
            "title": "Preprint",
            "content": "Robitaille, Thomas Spura, Thouis R. Jones, Tim Cera, Tim Leslie, Tiziano Zito, Tom Krauss, Utkarsh Upadhyay, Yaroslav O. Halchenko, Yoshiki Vazquez-Baeza, and SciPy 1.0 Contributors. Scipy 1.0: fundamental algorithms for scientific computing in python. Nature Methods, 17(3):261272, Mar 2020. ISSN 1548-7105. doi: 10.1038/s41592-019-0686-2. URL https://doi.org/10.1038/s41592-019-0686-2. Fu-Yun Wang, Ling Yang, Zhaoyang Huang, Mengdi Wang, and Hongsheng Li. Rectified diffuIn The Thirteenth International Confersion: Straightness is not your need in rectified flow. ence on Learning Representations, 2025. URL https://openreview.net/forum?id= nEDToD1R8M. Yongqi Wang, Wenxiang Guo, Rongjie Huang, Jiawei Huang, Zehan Wang, Fuming You, Ruiqi Li, and Zhou Zhao. Frieren: Efficient video-to-audio generation network with rectified flow matching. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024a. URL https://openreview.net/forum?id=prXfM5X2Db. Zifeng Wang, Chufan Gao, Cao Xiao, and Jimeng Sun. Meditab: scaling medical tabular data predictors via data consolidation, enrichment, and refinement. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI 24, 2024b. ISBN 978-1-95679204-1. doi: 10.24963/ijcai.2024/670. URL https://doi.org/10.24963/ijcai.2024/ 670. Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Modeling tabular data using conditional gan. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/ paper/2019/file/254ed7d2de3b23ab10936522dd547b78-Paper.pdf. I-Cheng Yeh. Default of Credit Card Clients. UCI Machine Learning Repository, 2009. DOI: https://doi.org/10.24432/C55S3H. Hengrui Zhang, Jiani Zhang, Zhengyuan Shen, Balasubramaniam Srinivasan, Xiao Qin, Christos Faloutsos, Huzefa Rangwala, and George Karypis. Mixed-type tabular data synthesis with scorebased diffusion in latent space. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id=4Ay23yeuz0. Shujian Zhang, Lemeng Wu, Chengyue Gong, and Xingchao Liu. LanguageFlow: Advancing diffusion language generation with probabilistic flows. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 38933905, Mexico City, Mexico, June 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.215. URL https://aclanthology.org/2024. naacl-long.215/. Shuhan Zheng and Nontawat Charoenphakdee. Diffusion models for missing value imputation in tabular data. In NeurIPS Table Representation Learning (TRL) Workshop, 2022."
        },
        {
            "title": "A DATASETS",
            "content": "We use six real-world tabular datasets: Adult, Default, Shoppers, Magic, Faults, Beijing, and News from UCI Machine Learning Repository2. The statistics of the datasets are presented in Table 10. Table 10: Statistics of datasets. # Num stands for the number of numerical columns, and # Cat stands for the number of categorical columns."
        },
        {
            "title": "Dataset",
            "content": "# Rows # Num # Cat # Train # Validation # Test"
        },
        {
            "title": "Adult\nDefault\nShoppers\nMagic\nBeijing\nNews",
            "content": "48, 842 30, 000 12, 330 19, 019 43, 824 39, 644 6 14 10 10 7 46 9 11 8 1 5 2 28, 943 24, 000 9, 864 15, 215 35, 058 31, 714 3, 618 3, 000 1, 233 1, 902 4, 383 3, 965 16, 281 Classification 3, 000 Classification 1, 233 Classification 1, 902 Classification 4, 383 Regression 3, 965 Regression"
        },
        {
            "title": "B METRICS",
            "content": "B.1 SHAPE Shape is proposed by SDMetrics Dat (2023)3. Shape calculates the column-wise density. Kolmogorov-Smirnov Test (KST)is used for numerical data and the Total Variation Distance (TVD) is used for categorical data. KST. Given two continuous distributions pr(x) and ps(x), KST quantifies the distance between the two distributions using the upper bound of the discrepancy between two corresponding Cumulative Distribution Functions (CDFs): KST = sup Fr(x) Fs(x) where Fr(x) and Fs(x) are the CDFs of pr(x) and ps(x), respectively: (x) = (cid:90) p(x)dx (7) (8) TVD. TVD computes the frequency of each category value and expresses it as probability. Then, the TVD score is the average difference between the probabilities of the categories: TVD = 1 2 (cid:88) ωΩ R(ω) S(ω) (9) where ω represents all possible categories in column Ω. R() and S() denotes the real and synthetic frequencies of these categories. B.2 TREND Trend is also proposed by SDMetrics. It calculates pair-wise column correlation. Pearson correlation is used for numerical data and contingency similarity is used for categorical data. 2https://archive.ics.uci.edu/datasets 3https://docs.sdv.dev/sdmetrics"
        },
        {
            "title": "Preprint",
            "content": "Pearson Correlation. The Pearson correlation measures whether two continuous distributions are linearly correlated and is computed as ρx,y = Cov(x, y) ρxρy (10) where x, are two continuous columns, Cov is the covariance, and ρ is the standard deviation. The performance of correlation estimation is measured by the average differences between the real datas correlations and the synthetic datas corrections Pearson = 1 2 Ex,yρR(x, y) ρS(x, y) (11) where ρR(x, y), ρS(x, y) represent the Pearson correlation between column and of the data or R, respectively. Contingency similarity. For pair of categorical columns and B, the contingency similarity score computes the difference between the contingency tables using the TVD. The process is summarized by the formula below Contingency = 1 2 (cid:88) (cid:88) αA βB Rα,β Sα,β (12) where α, β represent all the possible categories in column A, B, respectively. Rα,β, Sα,β are the joint frequency of α and β in the data and S, respectively."
        },
        {
            "title": "C VISUALIZATIONS OF SYNTHETIC DATA",
            "content": "We present visualizations of the distributions of synthetic data generated by RecTable alongside real data."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Visualizations of the generated and real adult dataset. Figure 3: Visualizations of the generated and real default dataset."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Visualizations of the generated and real shoppers dataset. Figure 5: Visualizations of the generated and real magic dataset. Figure 6: Visualizations of the generated and real beijing dataset."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Visualizations of the generated and real news dataset."
        }
    ],
    "affiliations": [
        "Meiji University"
    ]
}