{
    "paper_title": "The Serial Scaling Hypothesis",
    "authors": [
        "Yuxi Liu",
        "Konpat Preechakul",
        "Kananart Kuwaranancharoen",
        "Yutong Bai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While machine learning has advanced through massive parallelization, we identify a critical blind spot: some problems are fundamentally sequential. These \"inherently serial\" problems-from mathematical reasoning to physical simulations to sequential decision-making-require dependent computational steps that cannot be parallelized. Drawing from complexity theory, we formalize this distinction and demonstrate that current parallel-centric architectures face fundamental limitations on such tasks. We argue that recognizing the serial nature of computation holds profound implications on machine learning, model design, hardware development. As AI tackles increasingly complex reasoning, deliberately scaling serial computation-not just parallel computation-is essential for continued progress."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 9 4 5 2 1 . 7 0 5 2 : r a"
        },
        {
            "title": "The Serial Scaling Hypothesis",
            "content": "Yuxi Liu1 Konpat Preechakul1 Kananart Kuwaranancharoen2 Yutong Bai1 1UC Berkeley 2Independent Researcher"
        },
        {
            "title": "Abstract",
            "content": "While machine learning has advanced through massive parallelization, we identify critical blind spot: some problems are fundamentally sequential. These \"inherently serial\" problemsfrom mathematical reasoning to physical simulations to sequential decision-makingrequire dependent computational steps that cannot be parallelized. Drawing from complexity theory, we formalize this distinction and demonstrate that current parallel-centric architectures face fundamental limitations on such tasks. We argue that recognizing the serial nature of computation holds profound implications on machine learning, model design, hardware development. As AI tackles increasingly complex reasoning, deliberately scaling serial computationnot just parallel computationis essential for continued progress."
        },
        {
            "title": "Introduction",
            "content": "Scaling up machine learning has driven remarkable progress [1, 44, 26, 49, 55], but not all scaling is created equal. Although the field has focused on increasing parallel computation [23, 17, 87, 102, 46]with bigger GPUs, wider models, and more datasome problems stubbornly resist these advances [2, 51, 59, 71, 69, 16]. For class of tasks, only increasing serial computationallowing models to perform more sequential stepsyields further progress. The recognition that some problems fundamentally require deep or sequential models has remained mostly theoretical curiosity [115, 72, 13]. Only recently has there been growing recognition of scaling test-time computation, in contrast to train-time computation [105, 82, 128]. Yet this dichotomy still overlooks the role of parallel vs. serial computation which is central to this paper. In practice, both parameter counts and computation FLOPs are largely treated as single numbers, with no distinction between width (parallel) and depth (serial computation) [49, 44, 82, 105]. Consider Sudokua number-placement puzzle requiring each number 19 to appear once per row, column, and subgridas parable (Figure 1). Easy puzzles can be solved by filling in many blanks independently, in parallel. Hard puzzles, however, require long chain of dependent reasoning: each blank depends on the others. No algorithm can shortcut the process. This distinctionbetween problems that are wide (parallel) and those that are deep (inherently serial)is fundamental; it dictates how we design efficient models, yet it is underappreciated in machine learning. We propose the Serial Scaling Hypothesis: For many important ML problems, especially those involving complex reasoning, planning, or the evolution of interacting systems, increasing parallel computation alone is insufficient. Progress requires scaling the amount of serial computation. This hypothesis is motivated by both theory and practice. Complexity theory shows that some problems cannot be efficiently parallelized. Empirically, tasks such as hard Sudoku, mathematical reasoning, and sequential decision-making benefit disproportionately from models that can perform more serial steps (e.g., deeper networks, longer chains of thought, more search iterations). Equal contribution. Preprint. Why does this matter for machine learning? As we push toward more challenging tasksadvanced reasoning, physical simulations, planning, and scientific discoverywe encounter problems that parallel architectures (like Transformers and diffusion models) cannot efficiently solve. Recognizing the limits of parallel scaling and the necessity of serial computation has several implications: Model design: Should we revisit architectures that allow for deeper or more sequential computation? Hardware: Is it time to invest in faster, lower-latency processors, not just more parallel ones? Evaluation: Should we measure and report serial compute separately from total compute? Contributions. We (i) formulate the Serial Scaling Hypothesis and connect it to complexity theory, (ii) survey range of problemscellular automata, many-body physics, sequential decision-making, and advanced reasoningwhere serial computation is provably or empirically essential, (iii) analyze the limitations of modern parallel-centric machine learning models on these tasks, and (iv) discuss implications for future model and hardware design, with practical recommendations. 1.1 Parable: Easy vs. Hard Sudoku Not all hard problems are hard in the same way. An easy Sudoku puzzle can be solved by filling in many blanks independentlyeach can be completed in parallel. In contrast, hard Sudoku requires long chain of dependent reasoning: each blank depends on the others, and the solution must be built step by step. No algorithm exists that can shortcut this process; only more sequential computationmore \"depth\"helps. While large number of easy puzzles may take the same amount of computation as single hard puzzle, yet one can be significantly accelerated by more processors, while the other cannot. This distinction between wide (parallelizable) and deep (inherently serial) computation is central to our thesis. Many tasks in machine learning, from advanced reasoning to planning, share this serial character: they require sequence of dependent steps, not just more parallel computation. 1.2 Brief History of Scaling The success of modern machine learning has been driven by scalingbigger models, more data, and especially more parallel compute. Hardware has shifted from CPUs to massively parallel GPUs; architectures have moved from RNNs to highly parallelizable Transformers; and algorithms increasingly exploit parallelism for efficiency. Figure 1: The four circled blanks in the easy Sudoku have no interdependence and can be filled in parallel. The blanks in hard Sudoku puzzle are densely interdependent, such that one blank cannot be filled conclusively until one has worked out long chain of consequences for many others. Scaling Architecture Algorithm Hardware Serial Depth RNN RL CPU Parallel Width Transformer Imitation learn. GPU However, this focus on parallel scaling has blind spot: it assumes that parallel and serial computation are interchangeable. In reality, for many problems, only increasing serial computeallowing models to perform more sequential stepsyields further progress. This is especially true for tasks with inherent temporal or causal dependencies. As we show in this paper, recognizing the limits of parallel scaling and the necessity of serial computation is crucial for the next phase of progress in machine learning. Table 1: Examples of serial and parallel approaches."
        },
        {
            "title": "2 Machine Learning from the Serial Perspective",
            "content": "We first formalize inherently serial problems (Section 2.1), drawing from complexity theory to show they are not only theoretically valid but also pervasive in machine learning domains (Section 2.2). We then examine why predominant ML models struggle with these tasks (Section 2.3). Finally, we discuss broader implications for both software and hardware design (Section 2.4). 2 Figure 2: . (A) decision problem has variable-size input and fixed-size output (e.g., yes/no). (B) serial problem requires deeper or more steps as the problem size grows. Examples of serial problems are: (C) Cellular automaton: takes the initial state as input and outputs discrete value of the row at cell for {1, . . . , 2N 1}. (D) Many-body mechanics: takes initial positions and momenta of each particle with time as inputs and outputs the particle locations at time in limited-precision space. (E) Math QA: takes question as input and outputs the answer autoregressively, with each output from fixed set of possibilities. 2.1 Complexity Framework for Inherently Serial Problems We focus our attention on decision problems which take in input tokens and output either yes or no as depicted in Figure 2(A). While this might seem limited, it is incredibly general. Any problem with discrete output can be framed as decision problem by asking multiple yes/no questions to cumulatively reconstruct the final output. Figure 2 (CE) show how cellular automata, many-body mechanics, and open-ended question answering can be represented in this way. As our focus is on recent machine learning developments, i.e., deep neural networks, we adopt the complexity class TC (Threshold Circuits) which has strong connections to neural networks to formally distinguish between serial and parallel problems. Definition 2.1 (Informal, see Appendix A). problem belongs to the class (L-uniform) TCi if it can be solved by family of Boolean circuitsone for each problem size with poly(N ) width and O(logi ) depth, using gates such as AND, OR, NOT, and MAJORITY. Here, L-uniformity means that there exists single program that would output the schematics of the -th circuit using only O(log ) space during the computation. The class TC is defined as the union of all such TCi for 0. Figure 3: The complexity classes are nested as TC0 TC1 TC P. Each containment is widely believed to be strict. Problems in TC are considered parallel, while problems outside are inherently serial. Intuitively, problem belongs to TC if and only if it can be solved by an MLP with polynomial width and polylog depth, where polylog denotes poly(log N) [85]. problem belongs to TC0 if and only if it can be solved by constant-depth MLP. We consider TC problems as parallel problems since there are sublinear-depth MLPs that solve them. 3 While that seems contradictory with the universal approximation theorem [22, 45] which argues that 3-layer MLP with an unbounded width may approximate any continuous function, it does not. When the width is bounded as polynomial, the problems that the MLP can solve are much more restricted to only in TC0, class of problems that fixed-depth MLPs can solve. It is common phenomenon in computational complexity that one can make neural network shallower at the cost of exponentially larger width [118, 40, 101, 124, 81, 28, 20, 115, 72]. However, such exponential-sized networks are intractable in both memory and computation. We consider them as inefficient solutions in this paper. This exponential depth-width trade-off underlines the importance of characterizing neural networks in terms of depth, which motivates the Serial Scaling Hypothesis. To formally characterize the problems in terms of depth, we have the following definition: Definition 2.2. problem is parallel if TC; otherwise, it is inherently serial. Assumption 2.3. We adopt the widely held belief that TC [34, Ch. 5]that is, some polynomialtime problems, e.g., P-complete ones, are inherently serial [34, Ch. 8]. Importantly, labeling problem inherently serial does not preclude the use of parallel computation. It is possible that serial problem might have most of its problem instances that are parallelizable, and parallel solutions, such as sublinear-depth MLPs, may obtain respectable average accuracy. However, solving such problem exactly would still require genuinely serial solution, such as linear-depth MLPs or more. 2.2 Real-world problems are likely inherently serial The existence of inherently serial problems does not automatically imply their practical relevance. natural question arises: Are these problems likely to appear in real-world applications? The answer is: Yesparticularly if one works in domains involving the need for physical simulation or reinforcement learning. But even outside these areas, it remains highly likely that one will encounter inherently serial problems. Surprisingly, even tasks that appear simple and for which we have polynomial-time computer program can exhibit inherent seriality that resists parallelization.2 Drawing from both empirical observations and theoretical arguments presented in the following Section 4, we propose the following complexity-theoretic hypothesis: Hypothesis 1 The following problemscellular automata evolution, many-body mechanics, sequential decision problems, mathematical question answeringare inherently serial (i.e., not contained in the complexity class TC). Problems outside the class TC (see Definition 2.1) exhibit fundamental computational dependencies: the outcome of intermediate steps directly influences subsequent steps in ways that cannot be shortcut without compromising correctness. This intrinsic structure precludes flattening the execution into parallel form without loss of fidelity. 2.3 Are our models capable of solving inherently serial problems? Knowing how relevant inherently serial problems are, the crucial question is: Are our models also serial? The answer is they may not be. The predominant computational architectures adopted in our community excel in parallel computation efficiency. Transformers and state-space models (SSM) [36, 35] are themselves sequence models, yet their computation is parallelizable. Unless used with autoregressive inference, which limits their throughput to one token at time, Transformer or SSM layer can ingest the whole input tokens concurrently and produce the output. Based on theoretical evidence provided in Appendix B, this seeming efficiency comes at cost: 2Certain problems are believed to be inherently serial despite being solvable in polynomial time, due to the strong belief that TC = [34, Ch. 5]. 4 Theorem 2.4 (Informal [73, 15, 14, 74]). Any computation performed by standard feedforward (non-recurrent) network, fixed-precision Transformer, or linear state-space model of polynomial size and constant depth can be simulated by uniform threshold circuit of constant depthi.e., TC0. Membership in TC0 means models computation can be executed in constant number of parallel steps, with each layer flattened into threshold circuit, allowing all input tokens to be processed simultaneouslynot relying on long chains of sequential computation. In contrast, truly serial modelssuch as recurrent neural networks (RNNs), repeating layers, and autoregressive Chain-of-Thought (CoT)fall outside TC, reflecting their ability to capture dependencies that unfold over the input tokens. It is important to note that parallel architecture doesnt necessarily mean parallel model. As model is combination of architecture and inference method, parallel architecture such as Transformer, when used with autoregressive Chain-of-Thought inference, becomes serial model. Section 3.1 provides the discussion in more detail. Diffusion models cannot solve inherently serial problems. The surprising finding that is first proven here is that diffusion modelswhich appear non-parallelizable due to their iterative natureare incapable of solving inherently serial problems. We demonstrate theoretically in Section 5 that diffusion models [42, 107, 106] with fixed-depth TC0 backbone network still lie in TC0, i.e., cannot solve inherently serial problems, even with infinite diffusion steps. We provide summary of models theoretical capabilities in Table 2. Under the Hypothesis 1, we conclude result that we believe is particularly relevant and accessible to practitioners in the machine learning community: Method Parallel? MLPs Transformers SSMs and Mamba RNNs Repeating layers Chain-of-Thought Diffusion models (TC0 backbone) 3 Solve serial problem? Table 2: Parallelizable models are limited to parallel problems. Only non-parallelizable models may solve inherently serial problems. Key Limitation of Modern Machine Learning Models Theorem 2.5 (Informal). Transformer-based, structured state-space models (SSMs), and diffusion models with TC0 backbones are provably incapable of solving general instances of problems such as cellular automata evolution, many-body mechanics, sequential decisionmaking, and mathematical question answering. By connecting the models computation characteristics with the inherently serial nature of the key problems, this theorem highlights the fundamental insufficiency of the predominant non-serial models we use and the inherently serial problems we aim to solve. Limitations. (1) Some instances of inherently serial problems may be parallelizable or even trivial; only in the general case do they exhibit seriality. (2) For exceedingly hard problems in NP and beyond, serial computation may not be the dominant bottleneck due to the overwhelming exponential cost. Therefore, our position is most applicable to problems of practical real-world difficulty. 2.4 Implications of The Serial Scaling Hypothesis For machine learning practitioners. Many important real-world problemssuch as cellular automata, many-body mechanics, and sequential decision-makingare inherently serial. Solving them requires sufficient serial computation unless model is exponentially wide. But in practice, we can afford neither exponentially wide models nor exponentially large datasets. This mismatchsolving inherently serial problems using shallow or parallel modelsmay help explain why many ML models only work well on examples similar to their training data [117, 135, 89, 60, 68, 136]. 3While diffusion models are not yet parallelizable, given their TC0 capabilities, there might exist parallelization algorithm for diffusion models. 5 We hypothesize that models during training learn many algorithms for each problem. Some take constant time to run, essentially memorization and shallow pattern-matching. Some take short time to run and correspond to fast heuristics. Some take longer to run but produce correct, general solutions. However, at inference time, if the model lacks enough serial computation, it defaults to the fastest routinesthose that fail to generalize beyond the training distribution. Recent evidence suggests that pretrained LLMs are already capable of both short-form and long-form solutions. However, only after RL fine-tuning is longer and often more general solution elicited [123, 134, 100]. We thus advise that for models to generalize, they should be given sufficient serial computation at inference time. For model designers. To solve these challenging real-world problems efficiently, future models may need to incorporate recurrent structures to increase their serial computation, in addition to the current predominantly parallel designs. It is important to note that such designs may come with trainability concerns. For instance, adding recurrence or depth often increases the gradient variance [8, 86] and the L-Lipschitzness of the model [7, 30], which cumulatively make the model harder to train. This further requires improvements in training techniques. For hardware designers. The need for more serial computation is fundamental and cannot be substituted by parallel computation. Future progress in machine learning depends on improving low-latency, sequential processing. This is call to potentially revive interest in CPUs or specialized processors that tightly integrate memory and compute to reduce data movement overhead [48, 50]while still leveraging parallelism within each step, as in GPUs."
        },
        {
            "title": "3 Potential Misconceptions",
            "content": "3.1 Architecture vs. Inference Methods One may be confused by the statement that Transformers are not serial, followed by Chain of Thought is serial. This appears to be contradiction. It is not, though it may appear so due to our omission of words. The first statement really should be If you fix Transformers parameters and give it problems of size O(n), but only run it for O(1) forward passes, then you cannot solve inherently serial problem-families. Similarly, the second statement really should be If you fix Transformers parameters and give it problems of size O(n), and also run it for O(n) forward passes, then you can solve inherently serial problem-families. This is special case of general phenomenon: people confuse architecture and inference methods, meshing them all together like two lumps of clay gently melding under the desert sun. This is unfortunate but understandable because, in practice, architecture and inference couple together strongly, and this confusion is also encouraged by things like architecturedeployment co-design. The fact is that the abstractions leak. How you use model for inference depends on its architecture. How you design new model architecture depends on what you expect its inference method to be. This is good thing, but it may cause people to confuse architecture and inference. Fundamentally, we consider an architecture to be class, and its instances are atoms of computation, and this is the intuition that the reader should keep in mind. An atom has mass and volume. An atom can be copied and put into many molecules. What an atom cannot do is be divided or changed. Similarly, an instance of the Transformer architecture is an atomic Transformer. This atomic Transformer does have subatomic structures, such as its individual layers. However, since we are not performing complicated model surgeries, such as taking off its head, reading out the tensor from its middle layers, etc., we can assume it is atomic. The least you can do with this atomic Transformer is perform one single forward pass, and this has definite computational cost. It may be 48 layers or 96 layers, but the cost is definite. Still, even though the atomic Transformer is unchanging, it can be used in many molecules, and the molecules can change. It can be used in Chain of Thought molecule, and this molecule would grow like polymer, taking on length of O(n) in reaction to length O(n) input. In short, single architecture may be used on its own in fixed number of forward passes, or number of forward passes growing linearly with the problem size, or in tree search, beam search, etc. Inference methods are separate from model architecture. In this paper, we discuss both architectures and inference methods. We have found that Transformers, SSMs, and diffusion models are all not 6 serial when the inference method is just single forward pass, but can become serial when the inference method allows for more serial compute, such as Chain of Thought inference. 3.2 SSM is not RNN We class SSMs as not inherently serial, while RNNs as inherently serial. This is not mistake, and in fact, very valuable illustration of how an architecture can appear serial but is not. First, in what sense is RNN serial, while Transformer is not serial? Recall the title of the original paper on Transformers: Attention is all you need [119]. What did they mean by all you need? What was removed? Back in 2017, the standard sequence transduction model was pair of RNNs connected in the middle by the attention mechanism. The encoder RNN processes the input sequence one token at time, then the decoder RNN produces the output sequence, again one token at time. The key problem is that, while both an RNN and Transformer must decode tokens one-by-one, an RNN must encode tokens also one-by-one, whereas Transformer can encode tokens all-in-one-go. The animating spirit of the Transformer architecture was to remove everything that stands in the way of all-in-one-go, but preserve everything that allows RNNs to work so well. So they removed recurrence and preserved attention. Their title meant attention is all you need (and recurrence is not). This is exactly what our paper is arguing against: attention is not all you need, and inherently serial processing is necessary. This may be performed by recurrence or some other architecture, but it cannot be avoided. So now we come to SSM. SSM is not RNN because it is not recurrent in an inherently serial way. It was designed with the same spirit as the Transformer and suffers the same issue. Concretely, consider the problem of ingesting an input sequence: x1, x2, . . . , xn, and the task is to output the next token xn+1. For Transformer with 96 layers to accomplish the task, it does so by 96 sequential steps, each step is very wide but not deep. In contrast, an RNN with 96 layers must process x1 first, then x2, and so on. It requires 96n sequential steps that cannot be done in parallel. Suppose one attempts to do them in parallel, then there is an immediate problem: To run the RNN on x2, one needs the internal states of the RNN just after it has processed x3, and to do that, it needs to have processed x2, etc. Abstractly, the operation of an RNN looks like: where s0 is the initial internal state. fθ(xn, fθ(xn1, . . . , fθ(x2, fθ(x1, s0)))), In short, because the internal states of an RNN change nonlinearly and unpredictably, one cannot skip the steps. The animating idea of SSM is precisely to remove the inherent seriality of RNN. The idea is that if the internal states change linearly and predictably, then one can skip the steps. Concretely, the operation of an SSM is of the form: outputk = fθ(M k1x1 + k2x2 + + 1xk1, xk) since its internal states change linearly in data-independent way. This means that to run an SSM on input x4, one does not need to know the internal state of the SSM after processing x3. This allows it to be just as parallel as Transformer, and thus just as lacking in seriality. 3.3 Training vs. Inference Throughout most of the paper, the main contrast is the parallel vs. serial computation contrast. We do not mean this to be contrast between the parallel phase of training vs. the autoregressive phase of inference, which is how modern GPT-like transformers are produced and used. Concretely, consider initializing language model with 96 layers. The training algorithm samples chunk of text (32,768 tokens long) from the training corpus. The model then performs single forward inference simultaneously on all tokens, such that it only takes 96 steps of timeone step per layer. Now, during inference, the model must then generate one token at time, so the same 32,768 tokens would take 3,145,728 steps of time, which is vast expansion. Yet, this is not the main focus of the paper. The paper mostly talks about the theoretical and empirical results concerning learned and frozen models. The paper does talk about training, but only occasionally and informally, by sketching out intuitive analogies with inference, without rigorous theoretical justification. This is not due to an intention to mislead. Indeed, we recognize that there is parallel training vs. autoregressive inference contrast, and we have attempted to get some theoretical handle on this contrast. We failed. Because learning theory is extremely difficult, we could neither find theorems proven by people that came before us, nor prove theorems ourselves. Therefore, we stayed within inference. Fortunately, the parallel vs. serial contrast is already sharp even within inference, with both clear theoretical and empirical results, allowing us to write the paper. We believe strongly that in the future, the parallel vs. serial contrast will also be shown for training, and leave this as work for the near-posterity."
        },
        {
            "title": "4 Serial Problems",
            "content": "In this section, we introduce inherently serial problems. We begin by showing that even simple evolving system (Section 4.1) exhibits this property. We then demonstrate that system governed by physical laws (Section 4.2) also resists parallelization. We then show that inherently serial problems are quite generic, appearing throughout computational complexity theory (Section 4.3). Finally, we describe how sequential decision making (Section 4.4) and math QA (Section 4.5) are bottlenecked by serial computation. 4.1 Cellular Automata We begin with the problem of predicting the outcome of cellular automata (CA) [95, 19], shown in Figure 2(C), as an example of simple problem that is inherently serial. CA consists of regular grid of cells, each of which can be in one of finitely many states. The system evolves from an initial state in discrete time steps according to local update rule that determines the new state of each cell based on the states of its neighboring cells. Despite their simplicity, CA can exhibit wide spectrum of behaviors, ranging from completely predictable to extremely complex and seemingly random dynamics such as Rule 110 in Figure 4. The unpredictability is the first hint at the inherent seriality of this problem. Cellular automata problem is inherently serial. The Rule 110 cellular automaton has been proven to be Turing-complete [127], meaning that there is an algorithm which converts Turing machine to an initial state (top row) of the CA such that steps of the Turing machine are performed by iterations of the CA. Due to Turingcompleteness, the problem of computing the state of particular cell xi at row of Rule 110 CA is only solvable by simply working out the state evolution, one after anotherwithout shortcuts [34, p. 58]. This is not unique for Rule 110. Many CA problems are known to be P-complete [76], i.e., not efficiently parallelizable. Figure 4: Unpredictable patterns of Rule 110 cellular automaton and its rules. Given the top row, CA evolves to the bottom row, one by one, according to the rules. One might attempt to reduce serial computation by evolving across rows at once, as in the linear speedup theorem [84, Sec. 2.4]. However, this requires examining 2k + 1 adjacent cells, leading to 22k+1 possible rules that must be stored and looked upmuch larger than the original 8. While this reduces serial compute linearly, it increases parallel cost exponentially. This exponential depth-width tradeoff is common phenomenon in computational complexity [118, 40, 101, 124, 81, 28, 20, 115, 72]. See [47, Sec. 11.10] for review. What does this mean? simple system evolving under 8 rules is sufficient to eliminate computational shortcuts, requiring serial solutions. Reducing the serial computation often comes at an exponential cost in parallel computation. Serial and parallel computations are not interchangeable. 4.2 Many-Body Mechanics The previous section shows that inherent seriality arises from simple evolution of system governed by just 8 rules. Here, we examine more realistic case: many-body system under Newtonian 8 mechanics. Consider particles moving in multi-dimensional space under force field or hard collisions. As shown in Figure 2(D), given the starting positions and momentum of particles at time = 0, predict the positions of the particles at some later time = in space with limited precision.4 Many-body mechanics are inherently serial. The key argument is that Newtonian physics is rich enough to simulate any computation. Fredkin & Toffoli [32] recreate any Turing machine using nothing but billiard balls elastically reflecting off each other and wallscalled billiard-ball computer. Moore [75] proposes similar recreation using smooth particle motion in potential field in R3. Since the problem of simulating general Turing machines is inherently serial [34, p. 58], an algorithm that accurately simulates general physical systems can only be done step by stepexcept the special cases where the underlying Turing machine configurations are parallelizable by design. Physical systems are often described as ODEs, which raises the question: Can ordinary differential equations (ODEs) be parallelized over time? The answer is: it depends. As case study: Parareal algorithm [62] parallelizes the integration by applying coarse solver to divide the ODE into multiple parts [0, t1], [t1, t2], . . . , [tN 1, ]where each segment can be solved in parallelfollowed by iterative corrections to reconcile the trajectories. Parareal may yield speedup if the coarse solver is sufficiently accuratei.e., local gradient information reflects the global behaviorand may offer no wall-clock speed up otherwise [67, 33, Rmk. 4.7]. This leads to an intuitive rule of thumb: slow-changing ODEs tend to be parallelizable; fast-changing ODEs tend to be inherently serial. Such seriality induced by object interactions is not exclusive to scientific simulations but also rooted in video prediction, where the task is to predict the next frame given sequence of video frames. This setup is used to train large-scale models for content creation [133, 12, 130, 122, 114] and decision making [6, 29, 129]. Video prediction is inherently serial. This is because video frames are still rich enough to capture the mechanics of 2D billiard ball computer. It takes two consecutive frames to infer the momentum of each ball, assuming unit mass. To predict the particles positions at time , we mask the intermediate frames until time , then the output is the frame at time . Similarly in real-world videos, objects frequently move out of view due to camera motion or occlusions, effectively acting as masked frames, as illustrated in Figure 5. Thus, video predictionin theory or practicerequires predicting object positions many steps into the future, which is inherently serial whenever complex object interaction occurs with any regularity. Figure 5: To predict the next time , frame at the input frames are given, however, the intermediate frames are effectively masked due to camera motion or occlusions. Inherent seriality calls for model that tracks the evolving state of all objectsoccluded ones included. If the model loses track of any object, it must serially compute its current position from the last known location. Not only taking long wall-clock time, it also makes the task unsuitable for parallel models such as Transformers. What does this mean? Tasks involving the modeling of future dynamics of system, including physical simulations and video prediction, likely require serial models to be efficiently solved. For video prediction, the video model may also need to maintain an evolving world state to keep the task tractable. 4.3 P-complete Problems The inherently serial problems given previously have the following commonality: Given problem statement of size O(n), it is immediately clear how to construct linear computational graph of length O(n), such that each node of the graph only depends on the previous one, and each step takes constant time. This is quite intuitive since this ought to be what being serial means. In computational complexity theory, there is large class of problems of this form, or reducible to problem of this form: P-complete problems. They have been studied in the theory of parallel 4Limited precision is an assumption used in practical scientific settings. With unlimited precision, the problem becomes PSPACE-hard, i.e. intractable. computation. They serve to formalize the intuition that certain problems are efficient to solve on single processor but impossible to solve much more efficiently with polynomially-many processors. The precise definition is in Appendix A. Of the P-complete problems, the most prototypical ones involve the evaluation of Boolean circuits, called the Circuit Value Problem (CVP) [56]. The CVP is quite robust, in that many variations on it are still P-complete [34]. The CVP problem is: Given Boolean circuit with single output bit, specified as list of logic gates and how they connect to each other, and an input binary string, what is the output bit? The obvious solution is to perform topological sort on the graph of the logic gates so that each gate depends only on the output values of the previous gates, then evaluate them one after another. Since topological sorting takes O(n) time on graph with nodes, the CVP takes O(n) time to solve on single processor. The CVP embodies the intuition of what an inherently serial problem should be, which is that each step easily depends on the previous step in an easy way, but skipping steps is difficult. As it turns out, any problem is efficiently reducible to the CVP, thus making it P-complete. This gives us an intuitive rule: If the problem appears to have linear computational graph similar to the CVP, then it is likely inherently serial. Indeed, this intuition guided us in discovering that some problems in Section 4.4 are inherently serial. Furthermore, it is widely believed that P-complete problems are outside of TC, and there is an extensive list of such problems in [34], thus suggesting that inherently serial problems are common and likely to be encountered by an AI. 4.4 Sequential Decision Problems (a) MCTS in Hex board game: Performance improves with more MTCS expansion nodes across all training regimes. Perfect play is only possible with test-time MCTS. Data from Villalobos & Atkinson [120]. (b) Actor & critic networks depth vs. width in locomotion & manipulation. 2 deeper network (8 layers, width 256) outperforms 16 wider network (4 layers, width 4,096) in Humanoid, locomotion task with the largest observation & action spaces among the three. Data from Kevin et al. [53]. Figure 6: Empirical serial scaling on (a) Hex board game and (b) locomotion & manipulation. As we have seen previously, inherent seriality arises from chains of non-trivial computations whether rule-based logic or physical interactionsthat cannot be compressed into fewer steps. Here, we discuss that the sequential nature of sequential decision problemscentral to robotics and optimizationalso imposes similar requirements which favor serial solutions over parallel ones. The goal in sequential decision problems is to obtain an optimal policy π(s) = arg maxπ J(π), where J(π) is the expected return under policy π in Markov Decision Process (MDP) with finite horizon , and discount factor γ [0, 1]. Therefore, given state s, the policy outputs an optimal action from limited possibilities (discrete action) or with finite precision (continuous action). Sequential decision problems are inherently serial. In theory, it can be shown that there exists an MDP for which computing the optimal policy is inherently serial. As detailed in Appendix D, the proof relies on problem that is inherently serial to even approximate, and shows that selecting an optimal action is effectively approximating the problem, which is inherently serial by design. However, in practice, it remains unclear whether this problem is likely serial. Below, we offer some intuition suggesting that it is. Consider policy gradient methods [113, 125], including popular variants like PPO [98], widely used in applications such as LLM fine-tuning [83]. The gradient is given by θJ(πθ) = 1 (cid:88) t=0 γt E(st,at)dπ [θ log πθ(atst) Rt] , is the distribution of (st, at) under policy πθ at time t, and Rt = (cid:80)N 1 where dπ the return from time to the episodes end. k=t γktrk denotes Given sufficiently expressive model θ, convergence to the optimal policy via policy gradient requires the return estimate Rt to be unbiased5. As we will argue, this estimation step represents key inherently serial bottleneck of sequential decision problems. Computing the return is likely inherently serial. To compute returns in parallel, not linearly dependent on the trajectory length, it requires accessing state (and hence obtaining reward ri) in fewer than steps. This is unlikely for real-world MDPs given that CAs 8-rule transition tables (Section 4.1) or physical interactions (Section 4.2) are sufficient to prevent this. While an exponential-compute parallel algorithm exists6, more efficient way is go through each state in the trajectory one by one serially. Thus, although scaling parallel computationvia aggregating multiple trajectoriescan reduce variance in gradient estimation and accelerate convergence [112, p. 93], it cannot replace the accurate return estimation only enabled by serial computation. This inherent seriality motivates model-based reinforcement learning, where the return is computed by unrolling an internal model step-by-step. For instance, Monte Carlo Tree Search (MCTS)which increases serial computation via tree expansion and reduces return estimation biashas achieved superhuman-level performance in diverse board games [103, 104, 97]. Villalobos & Atkinson [120] demonstrate consistent improvement in the Hex board game from increasing MCTS expansion nodes (see Figure 6a). The benefits of increased serial computation are not limited to model-based methods. In model-free settings, Kevin et al. [53] show that deeper (more serial) networks significantly outperform wider (more parallel) ones on locomotion tasks such as Humanoid (see Figure 6b). What does this mean? The roles of serial and parallel computation in reinforcement learning are distinct. Without sufficient serial computation, regardless of the amount of parallel computation, RL will not converge to the optimal policy. 4.5 Math Question Answering Instead of planning through complex MDP as in sequential decision problems, advanced math question answering exhibits inherent seriality through logical step-by-step reasoning. As shown in Figure 2(E), given question as input tokens, the model outputs the solution autoregressivelygenerating each token from limited set of possibilities. Note that the inherent seriality does not stem from the autoregressive output format; rather, it is property of the problem that demands serial computation, regardless of the number of output tokens. Math QA is likely inherently serial. Solving gradeschool mathematics, GSM8K [18], which has been used to benchmark reasoning capabilities in LLMs, can be formalized as dependency graphs [131]. solution is to traverse the graph in topological order and performing necessary arithmetic operations sequentially [131]. This resembles the Arithmetic Circuit Value Problem [34, p. 124], generalization of the standard CVP to arithmetic operations. Like CVP, this problem is P-completethat is, inherently serial. Since the inherent seriality pervades even in relatively simple math QA, it likely extends to more challenging competition-level problems, such as those in AMC, MATH, AIME, and Olympiad-bench. Figure 7: Longer reasoning chains (shown in different colors) vs. majority voting (shown as dots along each line) macro avg. over AMC, MATH, AIME, and Olympiad-bench. Data from Aggarwal & Welleck [2]. 5Convergence to suboptimal policies is still possible with biased return estimates [77, 116]. 6It is possible to random trajectory and check for its consistence with the current MDP and policy. While this is fast and parallelizable, the trajectory possibilities grow exponentially with the trajectory length n. 11 In mathematics QA, as shown in Figure 7, Aggarwal & Welleck [2] demonstrate that sequential scaling with longer reasoning chains consistently outperforms parallel scaling, which aggregates many shorter chains via majority voting. This pattern holds across mathematical datasets of varying difficulty, including AMC, MATH, AIME, and Olympiad-bench, controlled for the same token budget. Similarly, science QA also appears to exhibit inherent seriality. Muennighoff et al. [78] report that in GPQA Diamond [90], PhD-level Google-proof science question set, sequential scaling yields consistent accuracy improvements, only limited by the models context window, and is significantly more efficient than parallel scaling (via majority voting), which plateaus. What does this mean? certain aspect of complex question answering requires building the answer step-by-step over computation graph. This aspect, likely shared among math and science QA, requires serial computation."
        },
        {
            "title": "5 Diffusion model’s computation is not serial",
            "content": "The first part of this paper examined inherently serial problems, which require models capable of serial computation. This section turns to modern ML models and their computational capacity. We provide elaborations on MLPs, Transformers, SSMs, RNNs, and Chain-of-Thought in Appendix B. Here, we focus on diffusion models [42, 108, 107]. These models are widely used in image/video generation [24, 12], other vision tasks such as depth estimation [96, 27, 52], and language modeling tasks [58, 80, 4]. Figure 8: The backbone network takes as input sequence of tokens, and single noisy output token. This is repeated until the output token is fully denoised. Consider problem with input x1, . . . , xN and fixed output xN +1 (as in Figure 2). diffusion model uses backbone neural network θ which takes the inputs and noisy version of xN +1, denoising it over steps until it becomes the output (see Figure 8). This models the conditional distribution ptruth(xN +1 x1, . . . , xN ) or concisely ptruth. While the number of denoising steps is scalablemore denoising steps for finer approximation of ptruth, in practice, diffusion models converge rapidly [66]. For instance, image generation plateaus at 300 steps [79], depth estimation shows little difference between 5 and 100 steps [88], and language modeling yields similar perplexity for 32 vs. 1024 steps [5]. With distillation, 14 steps suffice without much loss [132, 64, 109, 61, 93]. Such rapid convergence suggests that the effective computation depth of diffusion models is low. It would be surprising if the underlying computation were truly serial. Diffusion models with TC0 backbone can only solve problems in TC0. Relying on the results that the backward diffusion converges to p0 at the rate = O(d/T ) [57, Thm. 1], where is the total variation between p0 and the denoising pθ,0, and is the dimension of xN +1. For precision ϵ > 0 required by task, there exists constant such that, for any input size , the process converges in total variation to within ϵ at step effectively solving the task. We arrive at the following theorem: Theorem 5.1 (Informal). If problem can be solved by diffusion model with TC0 backbone with high probability with infinite diffusion steps, then the problem itself is in the parallelizable class TC0. The formal statement and proof of this theorem are provided in Appendix C. It has two parts. The first part uses the fast convergence rate of the EulerMaruyama method for solving the diffusion equation to show that we do not need many steps to approximately and correctly sample from the diffusion model. The second part uses standard derandomization argument to remove the high probability qualifier and obtain deterministic solution. The idea is, for each length n, we replicate the network k(n) times. Each network must take one seed. For each choice of k(n) seeds s1, . . . , sk(n), we have particular deterministic model: (cid:55) majority(f (x, s1), (x, s2), . . . , (x, sk(n))). 12 If is randomly sampled, then let the probability that (x, s) is wrong be upper-bounded by constant < 1/2. By Hoeffdings inequality [43], the probability that the majority vote is correct is 1 e2k(n)(p1/2)2 . Sample k(n) random seeds, and fix them. This provides deterministic model. Now, we try this deterministic model on every single possible input of length n. There are only #vocabn of them. If we set k(n) ln(#vocab) 2(p1/2)2 , then by the union bound, the probability that the majority vote is correct on all inputs of length is nonzero. Thus, there exists specific choice of random seeds (s1, . . . , sk(n)) that makes the compound model correct on all inputs of length n. This construction is nonuniform precisely in the part where we have only shown the choice of seeds exists. To actually find these seeds may take exponential time. What does this mean? Diffusion models only provide constant amount of additional serial computation. The above theorem precludes the use of diffusion models as scalable means of increasing serial computation. Unlike Chain-of-Thought (CoT), which genuinely adds serial compute (see Appendix B), diffusion models do not. Limitation. The theorem doesnt apply: (1) If the dimension of xn+1 is increasing. However, diffusion language modeling, as output length increases, has yet to show promising results [5, 65, 37, 92]. (2) If the backbone is either poorly trained or trained under different objective. This leaves open the possibility of serial-compute-scalable generative models beyond score-based diffusion."
        },
        {
            "title": "6 Related Works",
            "content": "Recent work on the formal theory of neural networks highlights recurring observation: many models that are parallelizable struggle to solve inherently serial problems [41, 111, 9, 39, 137, 94]. This has led to the parallelism tradeoff hypothesisthat all parallelizable models, irrespective of design, must necessarily fail to solve inherently serial problems [70, 71]. Our position builds on the classical depth-width tradeoff in the theory of parallel algorithms [121], which distinguishes total compute from serial compute via the notions of work and depth by Blelloch [10]. This distinction underlies Amdahls law [3, 38], as inherent seriality imposes lower bounds on achievable speedups. We also draw from the concepts of P-completeness [34, Ch. 8] and computational irreducibility [126], which characterize systems whose behavior cannot be efficiently reduced. While these ideas are wellestablished in complexity theory, our work brings them into the machine learning contextextending them to real-world tasks such as sequential decision-making and question answering, and highlighting the gap between these tasks and the limited serial capabilities of modern machine learning models."
        },
        {
            "title": "7 Limitations",
            "content": "Our conclusions rely on the widely-held but unproven assumption that TC (Assumption 2.3). If this assumption is overturned, the serial/parallel dichotomy would be invalid. Moreover, our theoretical arguments apply only to the general case; although the intuition may extend to practical settings, such generalization is not guaranteed. In particular, the average-case complexity [11] of many problems could differ or even allow parallel solutions. For especially hard problems (e.g., NP and beyond), exponential costrather than serial depthmay dominate as the primary bottleneck. Thus, our focus is on real-world problems of practical difficulty. Our theorem on diffusion models applies only when the output dimension remains fixed. If it grows with problem size, the result may not holdthough current empirical evidence in language modeling does not suggest strong serial scaling. The theorem also assumes well-trained backbone using score-matching objective, and may not apply to poorly trained models, especially early in training. Finally, beyond theoretical and circumstantial evidence, more empirical work is needed to quantify the degree of seriality in practiceparticularly the benefits of increased serial compute. promising direction is to benchmark real-world tasks under varying ratios of serial and parallel computation."
        },
        {
            "title": "8 Acknowledgements",
            "content": "We thank William Merrill and Anant Sahai for their insightful feedback and technical corrections, and Yossi Gandelsman and Yuatyong Chaichana for feedback that helped improve the presentation of this paper."
        },
        {
            "title": "References",
            "content": "[1] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Aggarwal, P. and Welleck, S. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv [cs.CL], 6 March 2025. URL http://arxiv.org/abs/2503. 04697. [3] Amdahl, G. M. Validity of the single processor approach to achieving large scale computing capabilities. In Proceedings of the April 18-20, 1967, spring joint computer conference, pp. 483485, 1967. [4] Arriola, M., Gokaslan, A., Chiu, J. T., Yang, Z., Qi, Z., Han, J., Sahoo, S. S., and Kuleshov, V. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025. [5] Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van Den Berg, R. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. [6] Bar, A., Zhou, G., Tran, D., Darrell, T., and LeCun, Y. Navigation world models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [7] Bartlett, P., Foster, D. J., and Telgarsky, M. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, 26 June 2017. [8] Bengio, Y., Simard, P., and Frasconi, P. Learning long-term dependencies with gradient descent is difficult. IEEE Trans. Neural Netw., 5(2):157166, 1994. URL http://dx.doi.org/10. 1109/72.279181. [9] Bhattamishra, S., Ahuja, K., and Goyal, N. On the ability and limitations of transformers to recognize formal languages. arXiv preprint arXiv:2009.11264, 2020. [10] Blelloch, G. E. Programming parallel algorithms. Communications of the ACM, 39(3):8597, 1996. [11] Bogdanov, A., Trevisan, L., et al. Average-case complexity. Foundations and Trends in Theoretical Computer Science, 2(1):1106, 2006. [12] Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., et al. Video generation models as world simulators. OpenAI Blog, 1: 8, 2024. [13] Chen, L., Peng, B., and Wu, H. Theoretical limitations of multi-layer transformer. arXiv preprint arXiv:2412.02975, 2024. [14] Chen, Y., Li, X., Liang, Y., Shi, Z., and Song, Z. The computational limits of state-space models and mamba via the lens of circuit complexity. arXiv preprint arXiv:2412.06148, 2024. [15] Chiang, D. Transformers in uniform TC0. arXiv preprint arXiv:2409.13629, 2024. [16] Chollet, F. On the measure of intelligence. arXiv [cs.AI], 4 November 2019. URL http: //arxiv.org/abs/1911.01547. [17] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. [18] Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv [cs.LG], 27 October 2021. URL http://arxiv.org/abs/2110.14168. 14 [19] Codd, E. F. Cellular automata. Academic press, 2014. [20] Cohen, N., Sharir, O., and Shashua, A. On the expressive power of deep learning: tensor analysis. In Conference on learning theory, pp. 698728. PMLR, 2016. [21] Cook, S. A. taxonomy of problems with fast parallel algorithms. Information and Control, 64(13):222, 1985. [22] Cybenko, G. Approximation by superpositions of sigmoidal function. Math. Control Signals Systems, 2(4):303314, December 1989. URL https://web.njit.edu/usman/courses/ cs675_fall18/10.1.1.441.7873.pdf. [23] Dao, T. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. [24] Dhariwal, P. and Nichol, A. Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems, pp. 87808794, 11 May 2021. URL http: //arxiv.org/abs/2105.05233. [25] Díaz, J., Serna, M., Spirakis, P., and Torán, J. Paradigms for Fast Parallel Approximability. Cambridge University Press, Cambridge, 1st edition, 1997. [26] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021. [27] Duan, Y., Guo, X., and Zhu, Z. Diffusiondepth: Diffusion denoising approach for monocular depth estimation. In European Conference on Computer Vision, pp. 432449. Springer, 2024. [28] Eldan, R. and Shamir, O. The power of depth for feedforward neural networks. In Conference on learning theory, pp. 907940. PMLR, 2016. [29] Escontrela, A., Adeniji, A., Yan, W., Jain, A., Peng, X. B., Goldberg, K., Lee, Y., Hafner, D., and Abbeel, P. Video prediction models as rewards for reinforcement learning. Neural Inf Process Syst, abs/2305.14343:6876068783, 23 May 2023. URL http://dx.doi.org/10. 48550/arXiv.2305.14343. [30] Fazlyab, M., Robey, A., Hassani, H., Morari, M., and Pappas, G. J. Efficient and accurate estimation of lipschitz constants for deep neural networks. In Advances in Neural Information Processing Systems, 11 June 2019. [31] Feng, G., Zhang, B., Gu, Y., Ye, H., He, D., and Wang, L. Towards revealing the mystery behind chain of thought: theoretical perspective. Advances in Neural Information Processing Systems, 36:7075770798, 2023. [32] Fredkin, E. and Toffoli, T. Conservative logic. Int. J. Theor. Phys., 21(3-4):219253, April 1982. URL http://dx.doi.org/10.1007/BF01857727. [33] Gander, M. J. and Vandewalle, S. Analysis of the parareal time-parallel time-integration method. SIAM Journal on Scientific Computing, 29(2):556578, 2007. [34] Greenlaw, R., Hoover, H. J., and Ruzzo, W. L. Limits to Parallel Computation: P-Completeness Theory. Oxford University Press, New York, 1995. ISBN 0-19-508591-4. [35] Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [36] Gu, A., Goel, K., and Ré, C. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [37] Gulrajani, I. and Hashimoto, T. B. Likelihood-based diffusion language models. In Advances in Neural Information Processing Systems, 30 May 2023. [38] Gustafson, J. L. Reevaluating amdahls law. Communications of the ACM, 31(5):532533, 1988. [39] Hahn, M. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156171, 2020. [40] Hajnal, A., Maass, W., Pudlák, P., Szegedy, M., and Turán, G. Threshold circuits of bounded depth. Journal of Computer and System Sciences, 46(2):129154, 1993. 15 [41] Hao, Y., Angluin, D., and Frank, R. Formal language recognition by hard attention transformers: Perspectives from circuit complexity. Transactions of the Association for Computational Linguistics, 10:800810, 2022. [42] Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 19 June 2020. URL http://arxiv.org/abs/2006. 11239. [43] Hoeffding, W. Probability inequalities for sums of bounded random variables. Journal of the American statistical association, 58(301):1330, 1963. [44] Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [45] Hornik, K., Stinchcombe, M., and White, H. Multilayer feedforward networks are universal approximators. Neural Netw., 2(5):359366, January 1989. URL https://cognitivemedium. com/magic_paper/assets/Hornik.pdf. [46] Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers, A., Boyle, R., Cantin, P.-L., Chao, C., Clark, C., Coriell, J., Daley, M., Dau, M., Dean, J., Gelb, B., Ghaemmaghami, T. V., Gottipati, R., Gulland, W., Hagmann, R., Ho, C. R., Hogberg, D., Hu, J., Hundt, R., Hurt, D., Ibarz, J., Jaffey, A., Jaworski, A., Kaplan, A., Khaitan, H., Koch, A., Kumar, N., Lacy, S., Laudon, J., Law, J., Le, D., Leary, C., Liu, Z., Lucke, K., Lundin, A., MacKean, G., Maggiore, A., Mahony, M., Miller, K., Nagarajan, R., Narayanaswami, R., Ni, R., Nix, K., Norrie, T., Omernick, M., Penukonda, N., Phelps, A., Ross, J., Ross, M., Salek, A., Samadiani, E., Severn, C., Sizikov, G., Snelham, M., Souter, J., Steinberg, D., Swing, A., Tan, M., Thorson, G., Tian, B., Toma, H., Tuttle, E., Vasudevan, V., Walter, R., Wang, W., Wilcox, E., and Yoon, D. H. In-datacenter performance analysis of tensor processing unit. In Proceedings of the 44th Annual International Symposium on Computer Architecture, 16 April 2017. [47] Jukna, S. et al. Boolean function complexity: advances and frontiers, volume 27. Springer, 2012. [48] Kang, H., Gibbons, P. B., Blelloch, G. E., Dhulipala, L., Gu, Y., and McGuffey, C. The processing-in-memory model. In Proceedings of the 33rd ACM Symposium on Parallelism in Algorithms and Architectures, volume 1, pp. 295306, New York, NY, USA, 6 July 2021. ACM. URL http://dx.doi.org/10.1145/3409964.3461816. [49] Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [50] Kaur, R., Asad, A., and Mohammadi, F. comprehensive review of processing-in-memory architectures for deep neural networks. Computers, 13(7):174, 16 July 2024. URL http: //dx.doi.org/10.3390/computers13070174. [51] Kazemi, M., Fatemi, B., Bansal, H., Palowitch, J., Anastasiou, C., Mehta, S. V., Jain, L. K., Aglietti, V., Jindal, D., Chen, P., Dikkala, N., Tyen, G., Liu, X., Shalit, U., Chiappa, S., Olszewska, K., Tay, Y., Tran, V. Q., Le, Q. V., and Firat, O. BIG-bench extra hard. arXiv [cs.CL], 26 February 2025. URL http://dx.doi.org/10.48550/arXiv.2502.19187. [52] Ke, B., Obukhov, A., Huang, S., Metzger, N., Daudt, R. C., and Schindler, K. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [53] Kevin, W., Ishaan, J., Michał, B., Tomasz, T., and Benjamin, E. 1000 layer networks for self-supervised RL: Scaling depth can enable new goal-reaching capabilities. arXiv [cs.LG], 19 March 2025. URL http://arxiv.org/abs/2503.14858. [54] Kirousis, L. M. and Spirakis, P. Probabilistic log-space reductions and problems probabilistically hard for P. In SWAT 88: 1st Scandinavian Workshop on Algorithm Theory, Halmstad, Sweden, July 58, 1988 Proceedings. Springer Berlin Heidelberg, 1988. [55] Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. 16 [56] Ladner, R. E. The circuit value problem is log space complete for p. ACM Sigact News, 7(1): 1820, 1975. [57] Li, G. and Yan, Y. O(d/T) convergence theory for diffusion probabilistic models under minimal assumptions. arXiv preprint arXiv:2409.18959, 2024. [58] Li, X., Thickstun, J., Gulrajani, I., Liang, P. S., and Hashimoto, T. B. Diffusion-LM improves controllable text generation. Advances in neural information processing systems, 35:4328 4343, 2022. [59] Li, Z., Liu, H., Zhou, D., and Ma, T. Chain of thought empowers transformers to solve inherently serial problems. arXiv preprint arXiv:2402.12875, 2024. [60] Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., Newman, B., Yuan, B., Yan, B., Zhang, C., Cosgrove, C. A., Manning, C. D., Re, C., Acosta-Navas, D., Hudson, D. A., Zelikman, E., Durmus, E., Ladhak, F., Rong, F., Ren, H., Yao, H., Wang, J., Santhanam, K., Orr, L., Zheng, L., Yuksekgonul, M., Suzgun, M., Kim, N., Guha, N., Chatterji, N. S., Khattab, O., Henderson, P., Huang, Q., Chi, R. A., Xie, S. M., Santurkar, S., Ganguli, S., Hashimoto, T., Icard, T., Zhang, T., Chaudhary, V., Wang, W., Li, X., Mai, Y., Zhang, Y., and Koreeda, Y. Holistic evaluation of language models. Transactions on Machine Learning Research, 2023. URL https: //openreview.net/forum?id=iO4LZibEqW. [61] Lin, S., Wang, A., and Yang, X. SDXL-lightning: Progressive adversarial diffusion distillation. arXiv [cs.CV], 21 February 2024. URL http://arxiv.org/abs/2402.13929. [62] Lions, J.-L., Maday, Y., and Turinici, G. Résolution dEDP par un schéma en temps pararéel . C.R. Acad. Sci. (Ser. 1) (Math./Math.), 332(7):661668, 1 April 2001. URL http://dx. doi.org/10.1016/S0764-4442(00)01793-6. [63] Liu, B., Ash, J. T., Goel, S., Krishnamurthy, A., and Zhang, C. Transformers learn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2022. [64] Liu, X., Zhang, X., Ma, J., Peng, J., and Liu, Q. InstaFlow: One step is enough for high-quality diffusion-based text-to-image generation. In The Twelfth International Conference on Learning Representations, 2024. URL http://arxiv.org/abs/2309.06380. [65] Lou, A., Meng, C., and Ermon, S. Discrete diffusion modeling by estimating the ratios of the data distribution. In Proceedings of the 41st International Conference on Machine Learning (ICML), 2024. [66] Ma, N., Tong, S., Jia, H., Hu, H., Su, Y.-C., Zhang, M., Yang, X., Li, Y., Jaakkola, T., Jia, X., and Xie, S. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv [cs.CV], 16 January 2025. URL http://arxiv.org/abs/2501.09732. [67] Maday, Y. and Mula, O. An adaptive parareal algorithm. J. Comput. Appl. Math., 377(112915): 112915, 15 October 2020. URL http://dx.doi.org/10.1016/j.cam.2020.112915. [68] Mancoridis, M., Weeks, B., Vafa, K., and Mullainathan, S. Potemkin understanding in large language models. arXiv [cs.CL], 26 June 2025. URL http://arxiv.org/abs/2506. 21521. [69] Marcus, G. Deep learning: critical appraisal. arXiv [cs.AI], 2 January 2018. URL http://arxiv.org/abs/1801.00631. [70] Merrill, W. and Sabharwal, A. The expresssive power of transformers with chain of thought. arXiv preprint arXiv:2310.07923, 2023. [71] Merrill, W. and Sabharwal, A. The parallelism tradeoff: Limitations of log-precision transformers. Transactions of the Association for Computational Linguistics, 11:531545, 2023. [72] Merrill, W. and Sabharwal, A. little depth goes long way: The expressive power of log-depth transformers. arXiv preprint arXiv:2503.03961, 2025. [73] Merrill, W., Sabharwal, A., and Smith, N. A. Saturated transformers are constant-depth threshold circuits. Transactions of the Association for Computational Linguistics, 10:843856, 2022. [74] Merrill, W., Petty, J., and Sabharwal, A. The illusion of state in state-space models. arXiv preprint arXiv:2404.08819, 2024. 17 [75] Moore, C. Unpredictability and undecidability in dynamical systems. Physical Review Letters, 64(20):2354, 1990. [76] Moore, C. Majority-vote cellular automata, ising dynamics, and p-completeness. Journal of Statistical Physics, 88:795805, 1997. [77] Mu, S. and Klabjan, D. On the second-order convergence of biased policy gradient algorithms. In Proceedings of the 41st International Conference on Machine Learning, 2024. [78] Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Candès, E., and Hashimoto, T. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [79] Nichol, A. Q. and Dhariwal, P. Improved denoising diffusion probabilistic models. In International conference on machine learning, pp. 81628171. PMLR, 2021. [80] Nie, S., Zhu, F., You, Z., Zhang, X., Ou, J., Hu, J., Zhou, J., Lin, Y., Wen, J.-R., and Li, C. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. [81] Oliveira, I. C. and Santhanam, R. Majority is incompressible by AC0[p] circuits. In 30th Conference on Computational Complexity (CCC 2015), pp. 124157. Schloss DagstuhlLeibniz-Zentrum fuer Informatik, Germany, 2015. [82] OpenAI. Learning to reason with LLMs. https://openai.com/index/ learning-to-reason-with-llms/. Accessed: 2025-4-16. [83] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, 4 March 2022. [84] Papadimitriou, C. H. Computational Complexity. Pearson, 1st edition, 1993. ISBN 9780201530827. [85] Parberry, I. and Schnitger, G. Parallel computation with threshold functions. J. Comput. Syst. Sci., 36(3):278302, 1 June 1988. URL http://dx.doi.org/10.1016/0022-0000(88) 90030-X. [86] Pascanu, R., Mikolov, T., and Bengio, Y. On the difficulty of training recurrent neural networks. In International Conference on Machine Learning, pp. 13101318, 13 February 2013. URL http://www.jmlr.org/proceedings/papers/v28/pascanu13.html. [87] Rajbhandari, S., Ruwase, O., Rasley, J., Smith, S., and He, Y. ZeRO-infinity: Breaking the GPU memory wall for extreme scale deep learning. Int. Conf. High Perform. Comput. Netw. Storage Anal., pp. 115, 16 April 2021. URL https://dl.acm.org/doi/10.1145/ 3458817.3476205. [88] Ravishankar, R., Patel, Z., Rajasegaran, J., and Malik, J. Scaling properties of diffusion models for perceptual tasks. arXiv preprint arXiv:2411.08034, 2024. [89] Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do ImageNet classifiers generalize to ImageNet? In Proceedings of the 36th International Conference on Machine Learning, 13 February 2019. [90] Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. GPQA: graduate-level google-proof Q&A benchmark. In Proceedings of the Conference on Language Modeling (COLM), 2024. [91] Sahni, S. and Gonzalez, T. P-complete approximation problems. Journal of the ACM (JACM), 23(3):555565, 1976. [92] Sahoo, S. S., Arriola, M., Schiff, Y., Gokaslan, A., Marroquin, E., Chiu, J. T., Rush, A., and Kuleshov, V. Simple and effective masked diffusion language models. In Advances in Neural Information Processing Systems (NeurIPS) 2024, 11 June 2024. [93] Salimans, T. and Ho, J. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 1 February 2022. URL http:// arxiv.org/abs/2202.00512. 18 [94] Sanford, C., Fatemi, B., Hall, E., Tsitsulin, A., Kazemi, M., Halcrow, J., Perozzi, B., and Mirrokni, V. Understanding transformer reasoning capabilities via graph algorithms. Advances in Neural Information Processing Systems, 37:7832078370, 2024. [95] Sarkar, P. brief history of cellular automata. Acm computing surveys (csur), 32(1):80107, 2000. [96] Saxena, S., Herrmann, C., Hur, J., Kar, A., Norouzi, M., Sun, D., and Fleet, D. J. The surprising effectiveness of diffusion models for optical flow and monocular depth estimation. Advances in Neural Information Processing Systems, 36:3944339469, 2023. [97] Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., Lillicrap, T., and Silver, D. Mastering atari, go, chess and shogi by planning with learned model. Nature, 2020. URL http://arxiv.org/abs/ 1911.08265. [98] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv [cs.LG], pp. 112, 20 July 2017. URL http://arxiv.org/abs/ 1707.06347. [99] Serna, M. Approximating linear programming is log-space complete for P. Information Processing Letters, 37(4):233236, 1991. [100] Shao, R., Li, S. S., Xin, R., Geng, S., Wang, Y., Oh, S., Du, S. S., Lambert, N., Min, S., Krishna, R., Tsvetkov, Y., Hajishirzi, H., Koh, P. W., and Zettlemoyer, L. Spurious rewards: Rethinking training signals in RLVR. [101] Sherstov, A. A. Separating AC0 from depth-2 majority circuits. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pp. 294301, 2007. [102] Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-LM: Training multi-billion parameter language models using model parallelism. arXiv [cs.CL], 17 September 2019. URL http://arxiv.org/abs/1909.08053. [103] Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489, 2016. [104] Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al. Mastering the game of go without human knowledge. nature, 550(7676):354359, 2017. [105] Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [106] Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, pp. 22562265, 12 March 2015. URL http://arxiv. org/abs/1503.03585. [107] Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. [108] Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. URL http://arxiv.org/abs/2011.13456. [109] Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. In Proceedings of the 40th International Conference on Machine Learning (ICML), 2 March 2023. [110] Strobl, L., Merrill, W., Weiss, G., Chiang, D., and Angluin, D. Transformers as recognizers of formal languages: survey on expressivity. arXiv preprint arXiv:2311.00208, 2023. [111] Strobl, L., Merrill, W., Weiss, G., Chiang, D., and Angluin, D. What formal languages can transformers express? survey. Transactions of the Association for Computational Linguistics, 12:543561, 2024. [112] Sutton, R. S., Barto, A. G., et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. 19 [113] Sutton, R. S., Mcallester, D., Singh, S., and Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems 12, pp. 10571063, 1999. URL http://dx.doi.org/10.1.1.37.9714. [114] Team, G. Mochi 1, 2024. URL https://github.com/genmoai/models. [115] Telgarsky, M. Benefits of depth in neural networks. In Conference on learning theory, pp. 15171539. PMLR, 2016. [116] Tian, H., Olshevsky, A., and Paschalidis, Y. Convergence of actor-critic with 2023. https://proceedings.neurips.cc/paper_files/paper/2023/hash/ multi-layer neural networks. URL 1dc9fbdb6b4d9955ad377cb983232c9f-Abstract-Conference.html. Inf Process Syst, 36:92799321,"
        },
        {
            "title": "Neural",
            "content": "[117] Torralba, A. and Efros, A. A. Unbiased look at dataset bias. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, June 2011. [118] Valiant, L. G. Exponential lower bounds for restricted monotone circuits. In Proceedings of the fifteenth annual ACM symposium on Theory of computing, pp. 110117, 1983. [119] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. [120] Villalobos, P. and Atkinson, D. Trading off compute in training and inference, 2023. URL https://epoch.ai/blog/trading-off-compute-in-training-and-inference. Accessed: 2025-04-05. [121] Vishkin, U. and Wigderson, A. Trade-offs between depth and width in parallel computation. SIAM Journal on Computing, 14(2):303314, 1985. [122] Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [123] Wang, Y., Yang, Q., Zeng, Z., Ren, L., Liu, L., Peng, B., Cheng, H., He, X., Wang, K., Gao, J., Chen, W., Wang, S., Du, S. S., and Shen, Y. Reinforcement learning for reasoning in large language models with one training example. arXiv [cs.LG], 29 April 2025. URL http://arxiv.org/abs/2504.20571. [124] Williams, R. Nonuniform acc circuit lower bounds. Journal of the ACM (JACM), 61(1):132, 2014. [125] Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach. Learn., 8(3-4):229256, 1992. URL https://link.springer.com/ article/10.1007/BF00992696. [126] Wolfram, S. New Kind of Science. Wolfram Media, Inc., 1st edition, May 2002. ISBN 1579550088. 1st Edition. [127] Woods, D. and Neary, T. The complexity of small universal turing machines: survey. Theoretical Computer Science, 410(4-5):443450, 2009. [128] Wu, Y., Sun, Z., Li, S., Welleck, S., and Yang, Y. Inference scaling laws: An empirical analysis of compute-optimal inference for LLM problem-solving. In International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=VNckp7JEHn. [129] Yang, S., Walker, J. C., Parker-Holder, J., Du, Y., Bruce, J., Barreto, A., Abbeel, P., and Schuurmans, D. Position: Video as the new language for real-world decision making. In International Conference on Machine Learning, pp. 5646556484. PMLR, 8 July 2024. URL https://proceedings.mlr.press/v235/yang24z.html. [130] Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., Yin, D., Gu, X., Zhang, Y., Wang, W., Cheng, Y., Liu, T., Xu, B., Dong, Y., and Tang, J. CogVideoX: Text-to-video diffusion models with an expert transformer. In International Conference on Learning Representations (ICLR), 2025. [131] Ye, T., Xu, Z., Li, Y., and Allen-Zhu, Z. Physics of language models: Part 2.1, grade-school In The Thirteenth International Conference on math and the hidden reasoning process. Learning Representations, 2024. 20 [132] Yin, T., Gharbi, M., Park, T., Zhang, R., Shechtman, E., Durand, F., and Freeman, W. T. Improved distribution matching distillation for fast image synthesis. In Advances in Neural Information Processing Systems (NeurIPS), 23 May 2024. [133] Yin, T., Zhang, Q., Zhang, R., Freeman, W. T., Durand, F., Shechtman, E., and Huang, X. From slow bidirectional to fast causal video generators. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [134] Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Yue, Y., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning capacity in LLMs beyond the base model? arXiv [cs.AI], 18 April 2025. URL http://arxiv.org/abs/2504.13837. [135] Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning requires rethinking generalization. In Proceedings of the 5th International Conference on Learning Representations (ICLR 2017), pp. 114, 2017. [136] Zhang, Y., Ma, Y., Gu, Y., Yang, Z., Zhuang, Y., Wang, F., Huang, Z., Wang, Y., Huang, C., Song, B., Lin, C., and Zhao, J. ABench-physics: Benchmarking physical reasoning in LLMs via high-difficulty and dynamic physics problems. arXiv [cs.LG], 7 July 2025. URL http://arxiv.org/abs/2507.04766. [137] Zhou, H., Bradley, A., Littwin, E., Razin, N., Saremi, O., Susskind, J., Bengio, S., and Nakkiran, P. What algorithms can transformers learn? study in length generalization. arXiv preprint arXiv:2310.16028, 2023. TC0 and TC Classes Definition A.1. Let N. decision problem {0, 1} is in the complexity class TCi if there exists family {Cn}nN of Boolean circuits such that: Each circuit Cn decides whether for all {0, 1}n. Each circuit Cn has size polynomial in n, i.e., Cn = O(nk) for some constant k. Each circuit has depth O(logi n). The circuit gates are of unbounded fan-in AND, OR, NOT, and MAJORITY gates. majority gate outputs 1 if more than half of its inputs are 1. The family {Cn} is L-uniform, where stands for logspace. This means that there exists , i), outputs the ith bit of the description deterministic Turing machine that, on input (11 1 (cid:124) (cid:123)(cid:122) (cid:125) repeats of Cn using working tape of length only O(log n). In addition, decision problem {0, 1} is in the complexity class TC if it is in the union of all classes TCi over N, that is, (cid:91) TC = TCi. (1) iN In the literature, TC is also called NC, or Nicks Class. as follows: Given two languages L, L, that is, Define the L-uniform many-one reduction relation two sets of binary strings, we say L if and only if there exists function such that if and only if (x) L, and the function is computable by deterministic Turing machine with logspace working tape. decision problem is P-complete if and only if there exists Turing machine that decides it in polynomial time, and any problem decidable in polynomial time can be L-uniformly many-one reduced to it."
        },
        {
            "title": "B Computation capabilities of modern machine learning methods",
            "content": "In this section, we point out an observational fact that machine learning methods that allow parallelization of their computation graph, such as MLPs, Transformers, and SSMs, have so far been shown to lack the ability to solve inherently serial problems. On the contrary, the only methods that are capable of solving serial problems are those that cannot be parallelized, such as RNNs, repeating layers, and Chain-of-Thought (CoT). In terms of computational complexity, multi-layer perceptrons (MLPs) have been formalized as family of Boolean circuits with threshold gates called TC0 [85]. Consider family of MLPs with constant O(1) depth, poly(n) neurons per layer, and O(1) numerical precision. problem is in TC0 if and only if it is decidable by one forward pass through an MLP under this setup. It is widely suspected that TC1 is strictly larger than TC0. Therefore, any TC1-hard problem requires computations deepening at rate of at least O(ln n). We may consider the problem as being serial in weaker, logarithmic sense. simple example of TC1 is the word problem of the symmetric group on 5 elements S5: Given g1, g2, . . . , gn S5, find (cid:81) gn [63]. Intuitively, this is because S5 is not solvable group, and therefore there is no better way to multiply its elements than via binary tree, which has O(ln n) layers. Attempting to perform this with just O(1) layers would effectively require one to memorize the entire Sn 5 Sn multiplication table, which scales exponentially as en ln(120), an exponential depth-width trade-off. To solve the word problem of S5 by Transformer in one forward pass, we present it with input tokens g1, g2, . . . , gn, y, where is special token. The Transformers output at is read out as the answer. Similarly, we may solve such problem by presenting the same input tokens to State Space Model (SSM) or Recurrent Neural Network (RNN). Since Transformer with O(1) layers and 22 poly(n) dimensions can only solve problems in TC0 [71] in one forward pass, it cannot solve the word problem of S5 that lies outside TC0. On the contrary, for an RNN, we can write the multiplication table of S2 5 S5 directly into its weights, so it can solve the problem by unrolling for O(n) recurrence steps with O(1) layers and O(1) dimensions. Intuitively, the hidden states of an RNN keep track of the progress of multiplication as it performs the forward passes. However, RNNs recurrence state dependency renders it nonparallelizable. Several families of SSMs were developed as compromise that still have recurrence on hidden states, like an RNN, while making forward passes parallelizable, like Transformer. The prototypical SSM architecture is Mamba [35], though there are many others. Unfortunately, it has been proven that there is not yet free serial-compute lunch, in the sense that the main families of SSMs proposed so far still cannot solve the word problem of S5 under the constraints of O(1) layers, poly(n) dimensions per layer, and one forward pass. Despite its apparent recurrence, the hidden state offered by an SSM is weaker in computational sense than that offered by an RNN [110, 70, 74]. This theoretical fact rhymes with the empirical fact that in practice, MLPs, Transformers, and SSMs are all more parallelizable than RNNs. Only genuinely serial method has been shown to go beyond the TC0 class. In addition to RNNs, this includes repeating layers and Chain-of-Thought (CoT). Repeating layers for O(ln n) times enables standard Transformer to solve tasks in the TC1 class [72]. With poly(n) CoT, requiring multiple forward passes before producing final answer, Transformer can be lifted from TC0 to [31, 59]. As discussed in Section 4.5, the power of CoT has been well-attested in practice by the improved performance of reasoning models in complex math and science tasks. Such uniformity restriction is necessary for technical reasons. Specifically, it is necessary because one may hide large amount of computation into small circuit that requires long time to find. The final circuit produced might run in time O(log n), but if the time required to find such circuit requires time 2O(n), then this would not be parallelin the sense used throughout this paper. Diffusion is in non-uniform TC0 In this section, we consider the non-uniform TC0 class, in contrast to the usual uniform classes. Note that uniform TCi non-uniform TCi. We prove that many diffusion models are restricted within that class. We need to assume non-uniformity because, at certain point in the proof of the main theorem, we merely prove that something exists, without showing that it is also efficiently computable. We will highlight this in the proof. C.1 Language modeling At the most abstract level, language is simply set of words made of letters. Formally: An alphabet Σ is finite nonempty set. Each element in the alphabet may be called letter or token. word in an alphabet Σ is finite sequence of elements of Σ. language in an alphabet Σ is set of words in the alphabet Σ. prefix language modeling problem is, given sequence of tokens x1, . . . , xn, to compute the next token xn+1. This is deterministic formalization of next-token prediction, the dominant paradigm in language modeling since the GPT-2 of 2019. This is also decision problem formulation: input of size n, x1, . . . , xn, and decision of either yes or no, xn+1. While usually, diffusion model is used for generating from continuous state space such as Rd, it can be used to model discrete distributions as well. This is necessary for language modeling. We consider the case closest to continuous state space modelingquantization: One divides the continuous state space Rd into regions and assigns token to each region. This then allows sampling discrete distribution from diffusion model with continuous state space. Formally, if Σ = {a1, a2, . . . , aM } is the alphabet, then we divide Rd into regions V1, . . . , VM , such that each region Vi maps to token ai. 23 In non-uniform circuit complexity, for each task instance n, we have TC0 score-network fθ,n such that: Each fθ,n takes as input + 2 elements x1, . . . , xn, x, t, and produces an output fθ,n(x, tx1, . . . , xn). The family fθ,n has O(1) depth, poly(n) width, and O(1) precision. Comment. The theorem holds for any family of score-networks for which single forward pass is in TC0. This includes, for example, Transformers and state-space models [71, 74]. Finally, since diffusion model may solve problem only with high enough probability, instead of solving it deterministically, we make the following definition: prefix language modeling problem is solved with constant probability bound if there exists some ϵ > 0, such that for each input token sequence x1, . . . , xn, let xcorrect be the correct response, then p(xcorrectx1, . . . , xn) > p(xx1, . . . , xn) + ϵ, = xcorrect. (2) C.2 Main theorem Theorem C.1. Suppose there exists TC0 family of score-networks fθ,0, fθ,1, . . . , such that for each and each x1, . . . , xn, the function fθ,n(x, tx1, . . . , xn) exactly computes the score function of some distribution ρx0x1,...,xn with bounded first moment: Ex0ρx0x1,...,xn If this family solves prefix language modeling problem at the limit of infinite time Score-Matching with Langevin Dynamics (SMLD) [107] with constant probability bound, then the problem is in the TC0 class. [x0] 1. Proof. The idea of the proof is simple. We first construct universal O(1)-bound on how many steps are sufficient for sampling the SMLD within constant probability bound, then we derandomize it according to [40], while still remaining within the TC0 class. Since the score-network exactly computes the score function for ρ0, the score-matching error is exactly zero. By [57], there exists constant > 0 that does not depend on any parameter in the statement of the theorem (a \"universal constant\"), such that there exists noise schedule for DDPM that takes steps of the following kind. Using the schedule and the score-matching function fθ, we sample from distribution ρDDP M,T . It satisfies the inequality (ρDDP M,T , ρˆx,0) d(log )3 Now, since is universal constant, we can take to be ccd2, to obtain an upper bound (ρDDP M,T , ρˆx,0) (log(ccd2))3 cd (3) (4) The key is that does not increase with n. Since the growth of log3 is dominated by linear growth, for any ϵ > 0, there exists large enough universal constant c, such that (ρDDP M,T , ρˆx,0) ϵ (5) for all = 1, 2, 3, . . . . Let ϵ be the constant probability bound, then setting ϵ = ϵ/2, we find that ρDDP M,T already solves the problem with constant probability bound. Now we can derandomize this family, obtaining TC0 family of Boolean circuits that solves the problem deterministically. The details of the derandomization method appear in [40, Prp. 4.2]. The big picture is as follows: we remove the probability by hard-coding magic constant7 per member of the family, such that sampling for polynomially many times, and taking the majority output, would 7This is the part where we assume non-uniformity. In complexity theory, this is an advice string. 24 always give the correct output. By Hoeffdings inequality, such magic constants exist for large enough polynomial. Comment. The requirement for exact score-matching is necessary for the following two reasons: First, the full form of the inequality from [57] is (ρDDP M,T , ρˆx,0) d(log )3 + cϵscore (cid:112)log , (6) where the term ϵscore denotes the score-matching error between the true score function of ρˆx,0 and the approximation fθ. As this extra term increases with , the proof above does not apply. Second, if we have no requirement on score-matching, then there is essentially no constraint on the computational power of SMLD, since any Turing machine can be constructed in SMLD with some (extremely non-score-matching) network.8 Practically relevant score-networks are intermediate between two extreme cases. We believe that if fθ is good enough, but not perfect, score-matching network, then generalized version of the above theorem still applies. However, finding the right way to quantify the goodness, as well as proving such generalization, is left as future work."
        },
        {
            "title": "D Inherently Serial Problems in RL",
            "content": "Throughout this section, by parallel algorithm, we mean specifically an L-uniform TC Boolean circuit familyas usual throughout this paper. In this section, we begin with problem from computational complexity theory that is proven to be impossible to parallelize (assuming, as always, that TC = P), then convert it into deterministic decision problem. We then prove theorem, showing that any parallel decision rule for this problem has arbitrarily bad worst-case performance. As special cases, this includes maximizing parallel value functions, maximizing parallel Q-functions, parallel policies, and parallel learning rules that produce parallel policies. D.1 Definitions Boolean circuit is alternating when it consists solely of AND and OR gates such that every AND gate connects to only OR gates and every OR gate connects to only AND gates. Alternating circuits are monotonic in the following sense. Consider an alternating circuit that takes inputs, and let x, {0, 1}n be two possible inputs to it. If (i.e., is obtained by flipping some zeros of to ones), then C(x) C(x). More generally, for any gate output Ci of C, we have Ci(x) Ci(x); intuitively, this can be visualized as hot wires carrying True-signals monotonically upwards through the circuit. The depth of gate is the length of the longest directed path from any circuit input to that gate. chain of the form OR AND OR therefore has gate depths 1, 2, 3, . . . in order. For an alternating circuit on input x, the depth-of-1 (DO1) of this circuit configuration is d1(C, x) := max(cid:8)depth(g) : outputs 1 on x(cid:9). The problem of computing the DO1 of any circuit configuration is the DO1 problem. Assuming that TC = P, as usual in the paper, then the DO1 problem cannot be solved by parallel algorithm. In fact, much more can be said. Not only would it be non-parallelizable, any approximation of it is also non-parallelizable. Fix constants 0 < ϵ < b. Given (C, x), the problem of computing number d(C, x) satisfying d(C, x) [ϵ d1(C, x), d1(C, x)]. Because solution for [ϵ, b] yields one for [ϵ/b, 1], it suffices to consider the special case = 1, which we call the ϵ-approximate DO1 problem. 8An explicit construction is published on Yuxis personal website. Yuxi would have published it on arXiv, but arXiv has put the paper on hold for months with no sign of actually publishing it. 25 D.2 Non-parallelizability Results We quote the following result from Kirousis and Spirakis [54]. Theorem D.1. For every ϵ (0, 1), the ϵ-approximate DO1 problem is P-complete. Consequently, if TC = P, no parallel algorithm can solve the ϵ-approximate DO1. To import the theorem from computational complexity theory into RL theory, we need to construct specific decision environment in which an agent must perform actions to maximize rewards. The idea of the following construction is that an approximately optimal agent can be exploited as problem-solving resource for other ends. Specifically, we will construct some environments in which the first action of the agent is forced choice between two circuits. In the language of psychologists, we construct two-alternative forced choice experiments. The agents choice can then be interpreted as an agents judgment as to which circuit has deeper depth-of-1. 1, 1), (C 2, 1), (C 3, 1), . . . , (C 1, 1), (C, x) vs (C Given circuit configuration (C, x), we construct, in parallel, many other circuit configurations (C C, 1), such that d1(C 3, 1) = 3, . . . , d1(C C, 1) = C. Then, we perform in parallel all forced choices for these pairs: (C, x) vs (C C, 1). At some point, the agents forced binary choice should switch from preferring (C k, 1) to preferring (C, x). This can be taken as judgment that d1(C, x) k. If the agent is approximately optimal, then d1(C, x) is correct, in the sense that we can guarantee [ϵd1(C, x), bd1(C, x)] for some constants 0 < ϵ < that do not depend on either or x. 2, 1), ..., (C, x) vs (C 1, 1) = 1, d1(C 2, 1) = 2, d1(C This is the essential idea of the construction and the subsequent proof. The rest are tedious details designed to close loopholes. We define the DO1 environment as follows. Given circuit configuration (C, x), define corresponding deterministic decision problem as follows. 1. At = 0, the agent observes two circuit configurations: the original (C, x) and length-k alternating chain of the form OR AND OR , whose input is 1. 2. The agent selects one gate from one of the circuits; the unchosen one is then removed. This is the two-alternative forced choice. 3. Subsequently, the agent may select at most one additional gate per time-step, for := max{k, C} steps, designed so that the agent has enough time to choose every desired gate. 4. The episode ends at = H. If every chosen gate outputs 1, then the reward at this step is rH = the maximal depth among all the chosen gates. Otherwise, rH = 0. The reward at all other steps is zero. This means that each DO1 environment can have the following states: No gates are selected. Some gates of are selected. Some gates of the alternating chain are selected. Theorem D.2 (optimal decision in the DO1 environment is inherently serial). Assume TC = P. (a) No parallel algorithm can compute an approximate optimal value function such that 0 < ϵ < with (s) [ϵV (s), bV (s)] for every state in every DO1 environment. (b) For any parallel algorithm producing value function and any ϵ (0, 1), there exists DO1 environment where the greedy policy πV (s) := arg maxa (s) achieves terminal reward rH < ϵr, where is optimal. Here, denotes the next state if, at state s, the agent performs action a. (c) Statement (b) extends to any parallel algorithm that, given state in DO1 environment, outputs an action. 26 Proof. (a) We argue by contradiction. Assume there exists parallel algorithm that, for some constant 0 < ϵ < 1, outputs value function satisfying (s) [ϵ (s), (s)] for every state in every DO1 environment. Since any factor > 1 can be removed by division, we set = 1 without loss of generality. Special case. We need to check first the special case where d1(C, x) = 0. This is done as follows. First, perform topological sort of C, which is parallelizable (in TC2, in fact [21]). This then allows us to find all the gates that are in the first layer. Next, for each gate in the first layer, we test in parallel whether that gate outputs 1. Let their outputs be y1, . . . , ym. If OR(y1, . . . , ym) = 0, then d1(C, x) = 0, because alternating circuits are monotonic. Otherwise, d1(C, x) 1. Having thus handled the special case, we assume that d1(C, x) 1 for the rest of the proof. Two-alternative forced choice experiments in parallel. Given an input (C, x) with := gates and depth-of-1 equal to d1 = d1(C, x), choose such that 2m1 < 2m. For each index ℓ {0, . . . , m} construct, in parallel, DO1 environment Eℓ whose second circuit is an alternating chain of length 2ℓ, with the only input being 1. Within each Eℓ evaluate (again in parallel) V(cid:0)s(ℓ) V(cid:0)s(ℓ) (cid:1), where s(ℓ) (cid:1), where s(ℓ) chain is the state after initially selecting gate C; chain is the state after selecting the input gate of the chain. The whole procedure requires (n + 1)(m + 1) = (cid:0)C + 1(cid:1)(cid:6)log2 C(cid:7) parallel evaluations. Identifying the switchover index. For ℓ = the chain depth 2m is at least as long as what depth-of-1 that can create, hence, if the value function is any good, it should satisfy (cid:1) for all g. As ℓ decreases the chain shortens; eventually picking V(cid:0)s(m) should be better than picking the chain. This intuition allows us to define the following decision procedure. (cid:1) V(cid:0)s(m) chain Let be the smallest ℓ such that V(cid:0)s(ℓ) chain (cid:1) max V(cid:0)s(ℓ) (cid:1). Output := 2mk as the estimate for d1. There are two special cases to handle. If the agent rejects the chain even for ℓ = 0, then output C. If the agent always picks the chain, then output = 1. Quality of the estimate. Suppose the switchover occurs, such that the agent picks the chain for Ek, but switches to picking the circuit for Ek1. Consider in detail what happens for Ek. Choose attaining the maximum in the definition of and set := s(k) , := s(k) chain by construction, (s) (s), and by the assumed guarantee on , (s) [ϵd1, d1], (s) [ϵd, d]. Hence d1 ϵd. Similarly, the argument in the case for Ek1 shows ϵd1 2d, so altogether d1 (cid:2)ϵd, 2 ϵ d(cid:3). similar argument applies for the two special cases where no switchover occurs. Therefore the procedure is an approximation algorithm for DO1 that operates in parallel, contradicting Theorem D.1. 27 (b) special case of (c). (c) Assume for contradiction that there exists parallel decision algorithm that is within an ϵ of optimality. That is, when the decision algorithm is applied in any DO1 environment, it always achieves reward at least ϵr. Then, for any particular DO1 environment, its first action must be an approximate solution as to whether (C, x) or the chain possesses larger depth-of-1. Now, the same construction of (a) yields parallel algorithm solving ϵ-approximate DO1, again contradicting Theorem D.1. Comment. By prepending beneath the circuit length-C alternating chain of gates, and making some minor adjustments to the proof, we can show that any parallel decision algorithm achieves linear regret in the worst case, meaning that rH = Θ(H). Although our analysis uses the DO1 problem in particular, there are some other P-complete problems, such as linear programming, that are P-complete to approximate. See for instance [91, 99, 25, 34] for some examples. This leads us to conjecture that inherent seriality may be fairly common phenomenon in RL, not particular to the DO1 setup. The barrier in Theorem D.2 can be bypassed in several ways: In the unlikely case that TC = is proven, it would have great consequences for complexity theory in general, analogous to the case where = NP is proven. This rejects the fundamental assumption in the theorem. By employing serial learning algorithm that runs in polynomial (not polylog) time, one may discover the right policy, which can then be compressed down into policy that runs faster than serial. This bypasses the L-uniform part of the obstacle. By allowing the learned policy or value function to be non-parallel, the TC circuit family part of the obstacle is bypassed. This is true for certain RL algorithms, especially modelbased methods that simulate the environment dynamics before making decision. For some RL applications, one may be able to tolerate arbitrarily poor worst-case performance, as long as they occur rarely. Part (b) of the theorem may explain the observation in Kevin et al. [53]. In that work, they trained both policy networks and value function networks by actor-critic methods, and noted that using deeper networks on both policy and value function improved performance. Part (b) of the theorem suggests that, when the network for approximating the value function contains less serial compute than the environment demands, then the corresponding exact -maximizing policy would suffer arbitrarily bad worst-case performance. Though the theorem does not exactly apply when the policy is inexact, it suggests that the same phenomenon happens for an actual policy network trained to merely approximate the -maximizing policy."
        }
    ],
    "affiliations": [
        "Independent Researcher",
        "UC Berkeley"
    ]
}