{
    "paper_title": "lmgame-Bench: How Good are LLMs at Playing Games?",
    "authors": [
        "Lanxiang Hu",
        "Mingjia Huo",
        "Yuxuan Zhang",
        "Haoyang Yu",
        "Eric P. Xing",
        "Ion Stoica",
        "Tajana Rosing",
        "Haojian Jin",
        "Hao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Playing video games requires perception, memory, and planning, exactly the faculties modern large language model (LLM) agents are expected to master. We study the major challenges in using popular video games to evaluate modern LLMs and find that directly dropping LLMs into games cannot make an effective evaluation, for three reasons -- brittle vision perception, prompt sensitivity, and potential data contamination. We introduce lmgame-Bench to turn games into reliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and narrative games delivered through a unified Gym-style API and paired with lightweight perception and memory scaffolds, and is designed to stabilize prompt variance and remove contamination. Across 13 leading models, we show lmgame-Bench is challenging while still separating models well. Correlation analysis shows that every game probes a unique blend of capabilities often tested in isolation elsewhere. More interestingly, performing reinforcement learning on a single game from lmgame-Bench transfers both to unseen games and to external planning tasks. Our evaluation code is available at https://github.com/lmgame-org/GamingAgent/lmgame-bench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 6 4 1 5 1 . 5 0 5 2 : r LMGAME-BENCH: How Good are LLMs at Playing Games? Lanxiang Hu1 Mingjia Huo1 Yuxuan Zhang 1 Haoyang Yu 1 Eric P. Xing2 Ion Stoica3 Tajana Rosing1 Haojian Jin1 Hao Zhang1 1UC San Diego 2 MBZUAI 3UC Berkeley"
        },
        {
            "title": "Abstract",
            "content": "Playing video games requires perception, memory, and planning exactly the faculties modern large language model (LLM) agents are expected to master. We study the major challenges in using popular video games to evaluate modern LLMs and find that directly dropping LLMs into games cannot make an effective evaluation, for three reasons: brittle vision perception, prompt sensitivity, and potential data contamination. We introduce lmgame-Bench to turn games into reliable evaluations. lmgame-Bench features suite of platformer, puzzle, and narrative games delivered through unified Gym-style API and paired with lightweight perception and memory scaffolds, and is designed to stabilize prompt variance and remove contamination. Across 13 leading models, we show lmgame-Bench is challenging while still separating models well. Correlation analysis shows that every game probes unique blend of capabilities often tested in isolation elsewhere. More interestingly, performing reinforcement learning on single game from lmgame-Bench transfers both to unseen games and to external planning tasks. Our evaluation code is available at https://github.com/lmgame-org/GamingAgent/lmgame-bench."
        },
        {
            "title": "Introduction",
            "content": "Games have long been standard testbed for reinforcement learning (RL) [1, 2]. Recent advancements show LLMs can be trained as agents using RL to navigate interactive environments [3, 4], including games [3, 5]. Gaming environments therefore naturally emerge as promising benchmarks [610], because they stress the same skills we expect from useful LLM agents seeing, reasoning, and planning over many steps and because there are thousands of existing games that can be turned into benchmarks [11, 12]. Despite the advantages, existing work lacks quantitative understandings of how to interpret gaming performance and the effectiveness of using games to evaluate todays LLMs. At first glance, evaluating LLM agents on games appears straightforward, by simply sending game screenshots to vision-language models (VLMs) to generate the next actions. However, directly placing model in gaming environments can result in low performance, often close to that of random action-taking baselines. This is because even the most advanced reasoning models fall short in vision perception and long-horizon decision making [1315]. This leaves an open question: can we turn games into more effective benchmarks for evaluating LLMs? We introduce lmgame-Bench: benchmark that builds on well-established video games, including platformer, puzzle solving, narrative-driven detective games. lmgame-Bench introduces scaffolds in principled way to overcome common challenges, including poor vision perception, potential data contamination, and prompt sensitivity. First, lmgame-Bench enriches evaluation settings by developing gaming harness, including perception and memory modules to amortize vision perception limitations and facilitate long-horizon planning as shown in Fig. 1. Second, lmgame-Bench adapts game settings to mitigate data contamination once it is detected. Finally, lmgame-Bench also employs standardized prompt optimization technique to reduce prompt sensitivity. Equal contributions. Significant contributions. Figure 1: lmgame-Bench uses modular harnessessuch as perception, memory, and reasoning modulesto systematically extend models game-playing capabilities, allowing the model to engage with simulated game environment through iterative interaction loops. We put leading models in lmgame-Bench. Evaluation results from 13 models across 6 games demonstrate that lmgame-Bench presents challenging benchmark far from being saturated due to the gap between model performance and human-level proficiency. The benchmark effectively differentiates models: o3 [16] and o1 [17] achieve top-2 best performance across all games, followed by other models with reasoning capabilities such as Gemini-2.5-pro-preview [18] and Claude-3.7 [7]. Among non-reasoning models, GPT-4.1 leads the pack. Given the lack of quantitative framework for interpreting the performance of the game, we develop set of techniques to understand LLM performance in lmgame-Bench. To study the correlation between lmgame-Bench and other widely used benchmarks, we perform low rank matrix factorization in the model benchmark performance matrix and use linear regression to predict game rankings. This allows us to uncover latent relationships between games and other domain-specific benchmarks. For instances, we show mathand coding-related benchmarks align more closely with long-horizon games like Sokoban and Tetris, while language-related benchmarks align closely with text-rich detective games like Ace Attorney. More interestingly, RL training on Sokoban and Tetris leads to cross-game improvements and boosts performance on planning and agentic tasks like Blocksworld and WebShop. These results demonstrate that gaming environments are not only effective in evaluating core LLM abilities but also serve as valuable training environments to improve those abilities. In summary, our paper makes the following contributions: We introduce lmgame-Bench, the first benchmark that uses video games to evaluate stateof-the-art LLMs with or without scaffolds. We show that under different settings, lmgame-Bench can effectively discriminate between models by underscoring their key strengths and weaknesses. We provide quantitative analysis of model performance on lmgame-Bench and show games evaluate combinations of existing capabilities commonly assessed in isolation. We show training LLMs with RL on games improves their performance not only on games of various settings, but also on planning and agent-based tasks like Blocksworld and WebShop."
        },
        {
            "title": "2 LMGAME-BENCH",
            "content": "We build the backbone of lmgame-Bench on six well-known games ( 2.1) to evaluate leading models performance without providing them with scaffolds. We choose the games to ensure diverse skill coverageincluding spatial reasoning, pattern recognition, and long-horizon planningfor evaluating different aspects of model capabilities. However, directly evaluating models on games in their original forms poses three key challenges: (1) low discriminability model scores often cluster near random-play baselines, making it difficult to distinguish models ability; (2) contamination risk - game assets and solutions may appear in training data, so high scores can reflect memorization rather 2 than skill; (3) prompt variance inconsistencies in prompt formatting can cause large performance swings, undermining comparability. To address these, we introduce mitigation techniques in 2.2. 2.1 Benchmark Design In designing lmgame-Bench, we intentionally recycle well-known games not only for their familiarity and popularity but also because they encapsulate broad spectrum of reasoning and interaction skills. Our goal is to preserve the original game settings that are carefully designed to challenge human cognition. In this section, we highlight the broad range of perception and generation abilities evaluated in lmgame-Bench game settings. 2.1.1 Games Super Mario Bros. Super Mario Bros is side-scrolling platformer game where the player controls Mario to navigate through obstacles, defeat enemies, and reach the end of each level by navigating through the environment. Success requires precise timing and strategic movement, making it classic benchmark for evaluating (1) visual perception, (2) spatial reasoning in 2D for character control, and (3) goal-directed planning with partial observability [19] in interactive environments. Tetris. Tetris is tile-matching puzzle game where players must strategically rotate and place falling Tetris tiles of 7 different geometric shapes to complete and clear horizontal lines. The game emphasizes (1) visual perception for pattern recognitions, (2) spatial reasoning for correct tile matching and geometric rotations [20], and (3) long-horizon planning with partial observability for decision-making on where and how to drop tile [21]. Sokoban. Sokoban is grid-based puzzle game where the player pushes boxes to designated target locations within confined spaces. It emphasizes (1) visual perception, (2) spatial reasoning to navigate both the character and the box, and (3) long-horizon planning to avoid deadlocks [22]. The games low fault tolerance is especially pronounced. Many actions are irreversible, and single wrong move can fail the puzzle. Candy Crush. Candy Crush is match-three puzzle game where players swap adjacent candies to form aligned sequences and trigger cascading effects to eliminate matched sequences. It requires (1) visual perception to identify different candies, (2) spatial reasoning to anticipate chain reactions at different locations, and (3) long-horizon planning to conserve moves to maximize total points. The gameplay features limited moves, making it crucial to plan moves carefully. 2048. 2048 is sliding-tile puzzle game where players combine numbered tiles on grid to reach the 2048 tile. It evaluates (1) visual perception for tracking tile values and movements, (2) spatial reasoning to manage merging paths, and (3) goal-directed planning to maximize merge potential [23]. Errors compound quickly due to the games limited space and could lead to irreversible failure states. Ace Attorney. Ace Attorney is an interactive courtroom-drama visual novel in which the player, acting as defense attorney Phoenix Wright, must investigate crime scenes, interview witnesses, and present evidence in court to reveal contradictions and secure Not Guilty verdict. The game stresses (1) long-context language understanding, tracking hundreds of dialogue turns, testimonies, and evidentiary facts, (2) causal & deductive reasoning under partial observabilitylinking dispersed clues, inferring hidden motives, and spotting logical gaps, and (3) long-horizon, low-fault-tolerance decision making, to decide when to press, object, or present evidence over multi-stage trials. 2.1.2 Game Settings lmgame-Bench maintains the integrity of the original design choices, which ensure scalability. In this section, we focus on standardizing game settings including inputs and outputs of the gaming environments as part of our benchmark design as shown in Fig. 1 (right). We can formalize gaming process as partially or fully observable Markov Decision Process (MDP) with the following definitions as generalizable formalism applicable to all games. Observation Space Representations. Many existing games are inherently graphical, requiring human players to process multimodal information, including visual, textual, and spatial cues, from the user interface (UI) to interpret game states and make decisions [24, 25]. We denote symbolic and 3 graphical representations of the game states as the set of all possible observations S. we dont make distinction between game state space and observation space, which is not the main focus of this work. Action Space. Let the set of all actions in action space be A. To interface with the game, lmgame-Bench streams game states si to lmgame-Bench considers multi-turn interactions. model , each time it generates action ai in response to the current state to maximize rewards, which are meticulously crafted scores in classical games, with details specified in 2.1.3. Difficulty. Games in lmgame-Bench are designed with varying levels of difficulty, structured along two key dimensions: (1) fault tolerance and (2) state-action space complexity. We define three levels of fault tolerance -low (one wrong move fails), medium (errors accumulate but can be recovered), and high (many mistakes can be tolerated without significantly affect future game states). We employ memory module to curb search-space explosion ( 2.2.1). 2.1.3 Metrics Design In line with Gymnasium [2], we treat reward as function, : R, which returns the payoff obtained when the agent executes action in state and transitions to state s. lmgame-Bench adopts this definition in lmgame-Bench to evaluate the performance of LLM-based agents. Depending on the nature of the game, we categorize evaluation metrics into two types: progression rewards and long-horizon rewards. Progression Rewards. Progression rewards apply to games that are either designed to run indefinitely or follow linear narative structure, where success is measured by completing full progression sequence. These rewards capture how far an agent advances before failure or termination. For example, in Super Mario Bros, progression can be quantified linearly by the agents movement along the x-axis. In games like Tetris and 2048, progression is reflected in the cumulative score as the agent strategically places Tetrominoes or merges tiles to clear the board. Long-Horizon Rewards. Long-horizon rewards are especially useful for games that span multiple levels and where intermediate progress is difficult to quantify. For example, in Sokoban, solving level often requires series of non-obvious steps to maneuver boxes into their target positions. Simply moving an individual box into place may not represent meaningful progress, as it could later need to be repositioned. Thus, linear metrics fail to capture the agents true advancement toward the final objective of completing the level. 2.2 Benchmark Effectiveness Enhancement While using games as evaluation presents challenging environments and breadth, we find that directly evaluating models on games exposes several challenges: low discriminability, contamination risk, and prompt variance. In this section, we address these issues by introducing gaming scaffolds for LLMs, contamination detection, and prompt standardization, enabling lmgame-Bench to function as more robust benchmark that reliably differentiates LLMs. 2.2.1 Gaming Harness Excluding text-only models, 40% of game runs without the harness fail to outperform random-play baseline. To raise the cap and bring higher contrast, lmgame-Bench provides suite of harness modules that can be toggled on or off for any experiment (workflow in Fig. 1). Activating the harness boosts scores far beyond both random play and the unharnessed setting, creating clearer performance gaps between models. With harnessing, 86.7% of game runs beat the random baseline, and paired-sample t-tests confirm that harnessed runs score significantly higher than their unharnessed counterparts on Candy Crush, 2048, Tetris, Ace Attorney, and Sokoban (details in Appx. and F). Perception Modules. Since the video games are inherently multimodal, we build perception modules that convert UI inputs into symbolic representations or textual descriptions of game states to facilitate understanding. For grid-based games (Sokoban, Candy Crush, 2048, Tetris), the module converts the visual layout into text-based table from game backends, listing object coordinates and their properties, e.g. \"Box at (2,3)\", \"Wall at (4,5)\". This allows models to directly understand spatial relationships in replacement of raw image inputs to minimize perception errors. For text-based games (e.g. Ace Attorney), the module extracts dialogues and describes visual elements in text format to 4 provide narrative context and critical visual cues. Likewise, we use perception module to extract visual elements in platformer and action games (e.g. Super Mario) to facilitate decision making. Memory Modules. Some games, like Sokoban and Tetris, exhibit rapidly growing decision space as gameplay advances and interactive elements scale (e.g., boxes, grid size, tetromino types). As result, they come with higher difficulty levels than the other games. To better distinguish models, we integrate an additional memory modules into lmgame-Bench. This setup allows selective activation of two components: (1) transient memory module, which records the past game states and actions, and (2) reflection module, which encodes explicit lessons learned to avoid failure, inducing actions in specific game states, thereby helping to narrow the action space. Reasoning Modules. Reasoning models [26, 16, 18, 7] have emerged as new inference paradigm, where models explore multiple reasoning paths and synthesize more accurate answer at the end. These models have shown strong performance in tasks such as mathematics, code generation, and planning. lmgame-Bench is designed to support such reasoning traces by allowing models to be evaluated with or without long chain-of-thought (long-CoT) reasoning. 2.2.2 Data Contamination Because lmgame-Bench reuses publicly available game assets, many images and scripts may already appear in model pre-training data. To ensure the model isnt merely recalling artifacts, we implement both vision-level and text-level contamination checks. Concretely, we test vision-level data contamination in Super Mario Bros and text-level data contamination in Ace Attorney, whose sprite and dialogue are widely distributed online (Appx. B). The other games, Tetris, 2048, Candy Crush, and Sokoban, feature combinatorial state spaces [27, 21, 28], making overlap with training data negligible. Vision-level. We assess whether models recall the visual structure of Super Mario Bros level 1-1 by prompting them to reorder shuffled RGB frames. Only few models exhibit moderate positive alignment, yet these alignment scores do not significantly track with their performance rankings. This suggests that they rely on local perception rather than memorized sequences. Since our metric evaluates how far models can play Super Mario Bros, we focus on vision-level contamination that may expose future frames, and disregard contamination within the current frame-such as prior knowledge that \"?\" brick may contain mushroom-as it does not affect sequence prediction. Text-level. In Ace Attorney, we test whether the models reproduce public fan transcripts. Using Sentence-BERT similarity, we find strong correlation between output similarity and performance, especially in 6-model subset. However, after applying structured prompt-based mitigationincluding entity masking, paraphrasing, and enforced reasoning [29, 30] the correlation disappears, and model rankings instead align with judged reasoning quality  (Fig. 2)  . Text Comparison: Ground Truth vs. o3 Ground Truth Text: Frank Sahwit, Round 1 Statement: remember the time exactly: It was 1:00 PM. Present: Cindys Autopsy Report. Contradiction Exposed: Death occurred between 4 PM and 5 PM, so the body could not be found at 1 PM. o3 Generated Text: Frank Sahwit, Round 1 - Statement: \"It was exactly 1:00 PM when saw Larry Butz run out of the apartment.\" Present: Cindys Autopsy Report -Contradiction Exposed: Autopsy shows death occurred between 4:00 and 5:00 PM, making 1:00 PM murder impossible. Figure 2: (Left) Example from Ace Attorney showing contradictions in O3-generated text vs. ground truth. (Right) Effect of mitigation on similarity-performance correlation; red and blue lines show correlations with old and new leaderboard ranks, respectively. 5 2.2.3 Prompt Standardization Evidence show prompt engineering is very effective in enhancing LLM performance across various games [8, 31]. However, we find performance variability may exceed the 1 standard deviation range even among empirically optimized prompts as shown in Table 9. In lmgame-Bench, we develop two-stage optimization technique to reduce prompt variance. Empirical Prompt Engineering with Standardized Formats. lmgame-Bench with harness can be regarded as an agentic workflow. We follow empirical practices in agent prompt design from recently released agents [32, 33]. Our empirical prompt follows the format [{J[min(0,iN ):i1]}, Ri1, si] to generate the next action ai for the current turn. {J[min(0,iN ):i1]} is the gaming trajectory of past turns, each composes of tuple of state, and action reward [sj, aj, rj], and Ri1 is reflection made by the memory module. DSPy-based Optimization. The second stage intends to standardize prompt optimization using DSPy [34]. It leverages the chain-of-thought module and Instantiate Stochastic Introspective MiniBatch Ascent (SIMBA) optimizer from DSPy (details are provided in Algorithm 1), using rewards from gaming environments as evaluation metrics. Given set of game rules and an initial prompt, the optimizer iteratively refines the prompt, guided by evaluation metrics to maximize cumulative rewards. This approach results in highly optimized system prompt with best average performance across all target models for each game. Results from Table 9 show across 3 runs, standardized prompt optimization with DSPy can indeed reduce performance variance across different empirically empirically optimized initializations in games such as 2048 by 33.8% to 63.5% (Appx. C). 2.2.4 Other Limitations and Discussion After implementing these improvements, lmgame-Bench significantly enhances model performance, achieves prompt convergence through bootstrapping in DSPy, and effectively detects and mitigates data contamination. However, few limitations still persist, primarily in two areas. (1) Performance variance continues to be high in partially observable games like Super Mario Bros, where game randomness plays significant role. (2) The computational cost remains substantial, as generating actions could result in long reasoning chains that are highly repetitive across multiple turns. We observe that the challenges faced by contemporary LLMs generalize across most existing games beyond those evaluated in lmgame-Bench, highlighting the need for improved model capabilities and more efficient inference to reduce operational costs."
        },
        {
            "title": "3 Experiments",
            "content": "In this section, we present the rankings of 13 state-of-the-art models, both with and without the gaming harness, evaluated on suite of 6 classical video games. We also analyze the effectiveness of each harness module ( 3.1), as well as the modules combined effectiveness. Then we investigate how gaming environments reveal core capabilities commonly evaluated in LLMs through correlation analysis, low-rank factorization, linear modeling ( 3.2), and RL training ( 3.3). We quantify the issue of gaming data contamination and propose mitigation techniques, which are detailed in Appx. B. 3.1 Model Performance When putting LLMs and VLMs in gaming environments, we first study if they can play the games well without gaming harness. Table 1 shows most models perform poorly across classic video games. Specifically, over three fourths of models often scores no points on Sokoban and Ace Attorney without harness support. On Tetris and Candy Crush, their scores are close to random play, which suggests they succeed by chance rather than understanding. As result, its numerically hard to distinguish models given poor model performances and randomness inherent to games. To address this issue, we design different levels of scaffolding, as described in 2.1.2, to better differentiate model capabilities. Results in Table 2 show that the harness leads to consistent and sometimes substantial gains across both games. Harness can also pull scores far away from random play, and reduce variance for more stable benchmark results. We provide detailed analysis for each module below. 6 Perception Modules. In grid-based games like Sokoban, vision scaffolding helps models perform better by providing them with symbolic representation of game states read from the game backend. Models like Gemini and O4-mini show substantial improvements, revealing that structured spatial inputs can unlock planning capabilities not expressed under raw input conditions. In games with more complex graphical interface like Super Mario Bros and Ace Attorney, lmgame-Bench uses o3 in perception module to extract key iterative visual elements and status indicators as textual descriptions. We find in such settings perception module palys less significant role and model performance still primarily relies on models vision understanding capability. Memory Modules. In lmgame-Bench, we employ o3 to generate reflections in the module (Appx. F). This module proves especially valuable in games like 2048, where incorporating memory significantly enhances performance. Models with weaker zero-shot capabilities benefit the most, underscoring the importance of leveraging sequential context in long-horizon games. By reflecting on past interactions, the memory module helps models make more informed decisions and avoid common pitfalls. Combined Support. The strongest results are observed when both modules are enabled as shown in Table 2. Direct comparison of model performance before and after using effect sizes in Appx. show models whose performance that were previously indistinguishable from random baselines now separate well, making comparisons more meaningful. We also employ paired-sample t-Test to demonstrate statistically significant improvement in model performance after applying gaming harness support in lmgame-Bench. Analysis of the coefficient of variation (CV, expressed as percentage) across gaming settings shows that, for most games, CV is consistently lower when harness support is enabled compared to the unharnessed setting. This indicates that harness support tends to yield more stable game performance. In summary, perception and memory modules boost performance across games. Perception is more useful in spatial reasoning tasks like Sokoban, while memory is essential for temporal planning in 2048. Together, they amplify model performance differences and make lmgame-Bench benchmarking results more informative. 3.2 Understanding Gaming Performance Games are designed to challenge human reasoning, perception, and planning abilities. Similarly, they require combination of core LLM capabilities for strong performance. To identify which capabilities are involved, we evaluate the following 8 models: Claude-3.5-Sonnet, Claude-3.7Sonnet-Thinking, Gemini-2.5-Pro-Preview, Llama-4-Maverick, GPT-4o, o1, o3, and o4-mini in 20 established benchmarks that span factual knowledge [3541], physics [42, 43], mathematics [4348], code generation [4951], visual reasoning [5255], language understanding [56, 57], and puzzle solving [6, 58, 59] to analyze their correlations with gaming performance. See Appx. D.1 for the complete benchmark list. Correlation Analysis. We calculate Spearmans rank correlation coefficient to assess alignment between model performance on games from lmgame-Bench and widely-used benchmarks. Results from Fig. 3 (left) reveal positive correlations between several games and few commonly used benchmarks. In particular, Sokoban shows strong correlations with math and coding benchmarks. Tetris and 2048 closely align with pattern recognition tasks like EnigmaEval and NYT-connections. Candy Crush is notably related to coding, hinting at algorithmic reasoning. Ace Attorney strongly correlates with LiveBench-Language, suggesting focus on narrative understanding distinct from other games. High variance games like Super Mario Bros.  (Table 1)  , is excluded from this analysis. More details are provided in Appx. D.2. Latent Ability Decomposition. To uncover relationships between the benchmarks and the capabilities of the model, we apply low-rank matrix factorization to the modelbenchmark performance matrix. This decomposes each LLM as vector in latent ability space and each benchmark, including our games, as sparse, weighted combination of these abilities. Due to the non-uniqueness of low-rank matrix factorization and entanglement of features in vector spaces, the resulting components are not necessarily human-interpretable. However, meaningful patterns can still be observed. As shown in Fig. 3b, Feature 1 aligns with language and multi-task knowledge (e.g., MMLU-Pro, MultiChallenge). Feature 2 captures coding capabilities (e.g., Big7 Table 1: Model performance raw scores, evaluated in both with and without harness settings. For games marked with , evaluation for text-only models is not supported, as vision understanding is essential for decision-making. The reported results represent averages over three runs, except for models or games marked with *, which are based on single run due to the high costs as of May 1, 2025. N/A indicates non-applicable evaluation setting, as the specific model currently does not support image input. Model Harness Sokoban Super Mario Bros Tetris 2048 Candy Crush Ace Attorney* claude-3-5-sonnet-20241022 claude-3-7-sonnet-20250219 (thinking) deepseek-r1 gemini-2.5-flash-preview-04-17 (thinking) gemini-2.5-pro-preview-05-06 (thinking) grok-3-mini-beta (thinking) llama-4-maverick-17b-128e-instruct-fp8 gpt-4.1-2025-04-14 gpt-4o-2024-11-20 o1-2024-12-17 * o1-mini-2024-09-12 o3-2025-04-16 * o4-mini-2025-04-16 Random No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes 0.00.0 0.00.0 0.00.0 2.31. N/A 1.31.2 0.00.0 1.71.5 1.00.0 4.30.6 N/A 5.70.6 0.00.0 0.00.0 0.00.0 0.00. 0.00.0 0.00.0 0.00.0 2.30.6 N/A 1.30.6 2.00.0 8.02.8 1.30.6 5.31.2 0.00. 1540.021.7 1267.7484.1 1430.0162.2 1418.7660.3 N/A N/A 1540.7262.4 1395.0240.1 1025.3443.2 1498.3203.4 N/A N/A 786.0462.6 1468.7555.7 1991.31018.5 2126.31778.4 1028.3656.0 2047.3528.2 1434.00.0 855.00.0 N/A N/A 1955.00.0 3445.00. 1348.3178.1 1448.0161.0 12.32.5 14.71.2 13.00.0 16.32.3 N/A 14.30.6 19.04.6 16.33.2 12.33.1 23.30. N/A 21.37.1 11.71.2 10.31.5 13.01.7 13.70.6 14.72.1 14.03.6 13.00.0 35.00.0 N/A 11.71. 31.00.0 42.00.0 15.03.6 25.38.5 57.816.4 108.25.8 114.27.2 113.33.1 N/A 105.212.2 107.43.4 106.65. 120.53.9 117.35.9 N/A 118.67.1 44.611.8 106.03.8 94.517.0 105.77.0 70.415.2 106.73.5 128.10.0 128.90. N/A 114.03.7 128.20.0 128.00.0 97.629.2 120.64.9 17.018.1 106.053.4 126.369.1 484.053.7 N/A 447.345. 97.736.1 334.765.5 177.364.9 416.36.8 N/A 254.0107.8 32.341.4 128.757.2 101.0120.2 182.028.7 59.054.6 147.353. 90.00.0 159.00.0 N/A 48.033.9 106.00.0 647.00.0 110.749.7 487.3198.0 987.0414.5 10.21. 100.47.8 116.551.5 1.00.0 2.00.0 3.00.0 7.00.0 N/A 0.00.0 1.00.0 4.00. 8.00.0 7.00.0 N/A 0.00.0 0.00.0 0.00.0 0.00.0 2.00.0 0.00.0 0.00.0 3.00.0 16.00. N/A 0.00.0 8.00.0 16.00.0 2.00.0 4.00.0 0.00.0 Table 2: Model performance (averaged across 3 runs) in Sokoban and 2048 under various conditions. ZS indicates zero-shot without any module support or memory prompt. Model Sokoban 2048 ZS +Memory +Vision +Both ZS +Memory +Vision +Both o4-mini-2025-04-16 1.30. 1.30.6 5.32.1 5.31.2 97.629.2 115.19.7 117.06. 120.64.9 gemini-2.5-Pro-03-25(thinking) 1.00.0 1.00.0 6.02.0 4.30. 120.53.9 118.08.5 117.45.8 117.35.9 claude-3-7-Sonnet-2025-0219 (thinking) 0.00. 0.30.6 0.70.6 2.31.5 114.27.2 107.15.1 115.32. 113.33.1 llama-4-maverick-17b-128e-instruct-fp8 0.00.0 0.00.0 0.00.0 0.00. 44.611.8 98.13.8 73.715.6 106.03.8 claude-3-5-sonnet-20241022 0.00. 0.00.0 0.00.0 0.00.0 57.816.4 102.51.6 66.39. 108.25.8 gpt-4o-2024-11-20 0.00.0 0.00.0 0.00.0 0.00. 70.415.2 107.06.3 73.35.4 106.73.5 CodeBench, Aider). Feature 3 corresponds to symbolic and puzzle-solving skills (e.g., EnigmaEval, NYT Connections). Feature 4 reflects physical reasoning (e.g., PHYBench, EMMA). Each game in lmgame-Bench loads on different subsets of these latent abilities. Sokoban emphasizes symbolic and physical reasoning (Features 3 and 4), while Ace Attorney strongly engages longcontext language reasoning (Feature 1). Tetris and 2048 mainly represents mathematical and spatial reasoning (Features 3), and Candy Crush reflects visual pattern recognition with moderate ties to coding (Features 2 and 3). This decomposition aligns closely with the correlation study above, while also suggests that games cover compositional capabilities rather than isolated skills. Linear Modeling. We also employ linear modeling to predict game ranking based on model capabilities across different categories (benchmark choices are provided in Appx. D.1; technical details and more results on polynomial models fitted with different categorical combinations are presented in Appx. D.4). When using the following five benchmark categories: language, physics 8 (a) Spearman Correlation among lmgame-Bench and other benchmarks. (b) Top-weight benchmarks under each feature after low-rank decomposition. Figure 3: Correlation Analysis and Latent Feature Decomposition Among Benchmarks. Table 3: Learned weights for game ranking prediction using linear model, where and RE denote for Pearsons and mean-normalized residual errors respectively. For the chosen set of categories, the linear models can hardly predict SMB and 2048 rankings correctly. Game Language Physics Visual Math Coding Offset RE Sokoban Tetris Ace Attorney Super Mario Bros 2048 Candy Crush 0.408 1.759 3.392 0.275 0.008 0.678 1.011 0.001 0.000 1.905 0.332 3.444 0.810 1.356 0.962 0.000 0.000 0. 2.160 1.979 2.430 0.597 1.880 1.275 2.206 2.222 0.004 0.000 0.000 2.456 0.297 0.825 0.853 2.940 3.130 0.088 0.930 0.825 0.853 0.295 0.248 0.864 0.4758 0.814 0.800 1.377 1.467 0.730 understanding, visual understanding, mathematics, and coding as explanatory variables, Table 3 reveals how each game decomposes into familiar skill domains. In linear modeling, our results show long-horizon games like Sokoban, Tetris and 2048s rankings are driven primarily by math and coding performance. Games requiring spatial reasoning, like Sokoban, Candy Crush and Super Mario Bros, align closely with the physics-understanding and visual understanding benchmark. Text-heavy narrative games like Ace Attorney are dominated by language-related benchmarks. Beyond linear modeling, we find in most cases (an example is provided in Table 11), polynomial models lack interpretability because the feature categories are not strictly orthogonal, leading to the emergence of cross terms that complicate attribution. Similarly, we do not consider higher-order polynomial models. 3.3 Training Generalizability Study In this section, we investigate the generalization effects of game-based training, providing empirical evidence that optimizing on game benchmarks can lead to broader improvements in LLM capabilities. Experiment Settings. Our training experiments involve two games, simplified Sokoban and Tetris. For Sokoban, we trained on gym-sokoban [60] environment on 6 6 board, and for Tetris, we build on TetrisRL [61] with 4 4 board and 2 simple tetrominoes types (1x2 and 2x1 blocks). We finetuned Qwen2.5-7B-Instruct [62] under RAGEN [3], multi-turn RL framework for LLMs, with prompts that include explicit thinking tokens. We evaluate the performance on various downstream tasks, including same-game with different settings, cross-game, planning, math/code, and agentic tasks, using the same prompting formats as training. Note that math and coding are not directly applicable to multi-turn settings, so we simulate multi-turn behavior by allowing self-correction attempts using feedback Incorrect. Please think again. Detailed training settings, hyperparameters, and additional experiments on task and prompt variations are provided in Appx. A. Cross-Game and Planning Generalization. As shown in Table 4, training on Sokoban yields strong gains in cross-game and spatial reasoning performanceimproving results on larger Sokoban boards, Table 4: Model performance on diverse tasks after training on simplified Sokoban and Tetris. Model Sokoban Tetris 6 6 8 1 type 2 types Games Planning Blocksworld 1D 2D Text Math/Coding GSM8K 1 turn 5 turns Agentic BIRD WebShop 1 turn Qwen-7B-Instruct Ours (Sokoban) Ours (Tetris) 11.3 24.2 13.3 5.9 9.0 6.7 9.0 17.6 49.5 4.7 5.1 14.5 64.7 64.1 66.2 17.9 32.7 21. 9.0 29.5 15.2 89.5 89.0 89.0 95.3 94.1 93.4 25.0 17.5 19.8 7.0 19.1 13.4 boosting Blocksworld 1D/2D by at least 10%, and achieving up to 8% zero-shot improvement on Tetris. Similarly, training on Tetris also enhances performance on cross-game and planning tasks. These results suggest that the spatial reasoning and planning heuristics learned during training transfer effectively across board rules and settings considered in our study. Math, Coding, and Agentic-Task Generalization. Despite gains in board games and planning, neither Sokoban nor Tetris training transfers to tasks like GSM8K, and BIRD, indicating that spatial reasoning and search heuristics alone are insufficient for the reasoning skills required in math and coding. On the multi-turn WebShop benchmark, game-trained models achieve at least 6% improvement, demonstrating that grid-gamederived skills can benefit real-world decision-making."
        },
        {
            "title": "4 Related Work",
            "content": "Games as AI Testbeds. Games have long served as foundational benchmarks in AI research, particularly in reinforcement learning. From TD-Gammon [63] to AlphaGo [64], they have offered controlled environments for studying planning and sequential decision-making. OpenAI Gym [1] further standardized this paradigm by providing unified interface for interacting with diverse game environments. More recently, games have been adopted to evaluate LLM agents on specific domains, such as grid-based games [65], open-ended strategy games [66], or murder-mystery games [67]. Others evaluate natural language reasoning through text-based or conversational games [9, 6870], lacking visual understanding. Multimodal gaming benchmark like BALROG [8] assess planning in rich game environments, but focus primarily on qualitative observations on game-related LLM abilities. Instead, we link games to LLM abilities through correlation analysis and multi-turn training. LLM Agentic Benchmarks. Current agentic benchmarks tend to focus on domain-specific taskssuch as code editing [71], web browsing [72, 73], API control flows [74], GUI control [32] or system operations [75]. Recent efforts also explored broader evaluation across multiple aspects [76, 77]. While these benchmarks provide valuable insights into specialized domains, they often require significant engineering effort to construct, limiting their scalability. In contrast, games offer more scalable and skill-diverse environment for evaluating general agentic behavior. Fine-tuning LLMs via Games. Recent work [26] has shown that post-training LLMs with verifiable rewards in domains like math and code can produce strong reasoning models. Games similarly offer structured, verifiable rewards in sequential decision makings, making them natural testbed for training decision-capable agents. Recent efforts such as RAGEN [3] and RL4VLM [78] fine-tune LLMs in game-based environments, but they do not systematically examine generalization to broader planning or agentic benchmarks. In contrast, we trained on well-known games and evaluated whether this can transfer to both in-domain and out-of-domain tasks."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce lmgame-Bench, the first agentic benchmark for evaluating LLMs on games with and without gaming harness support. lmgame-Bench leverages gaming harness composed of agentic modules to better distinguish state-of-the-art models. Our benchmark identifies and addresses data contamination through series of mitigation, and it reduces prompt variance by integrating twostage prompt optimization using DSPy. Additionally, we present lmgame-Bench can be regarded as compositions of existing core LLMs capabilities often evaluated in isolation, supported by comprehensive correlational study. Lastly, we show performance improvement on lmgame-Bench through RL training can transfer to both in-domain tasks (e.g. variations of the same game, similar game types) and some out-of-distribution tasks (e.g. BlocksWorld and WebShop)."
        },
        {
            "title": "References",
            "content": "[1] Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., Zaremba, W.: Openai gym. arXiv preprint arXiv:1606.01540 (2016) [2] Towers, M., Kwiatkowski, A., Terry, J., Balis, J.U., De Cola, G., Deleu, T., Goulão, M., Kallinteris, A., Krimmel, M., KG, A., et al.: Gymnasium: standard interface for reinforcement learning environments. arXiv preprint arXiv:2407.17032 (2024) [3] Wang, Z., Wang, K., Wang, Q., Zhang, P., Li, L., Yang, Z., Yu, K., Nguyen, M.N., Liu, L., Gottlieb, E., et al.: Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073 (2025) [4] Zhou, Y., Jiang, S., Tian, Y., Weston, J., Levine, S., Sukhbaatar, S., Li, X.: Sweet-rl: Training multi-turn llm agents on collaborative reasoning tasks. arXiv preprint arXiv:2503.15478 (2025) [5] Zhai, Y., Bai, H., Lin, Z., Pan, J., Tong, S., Zhou, Y., Suhr, A., Xie, S., LeCun, Y., Ma, Y., Levine, S.: Fine-tuning large vision-language models as decision-making agents via reinforcement learning. In Globerson, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J., Zhang, C., eds.: Advances in Neural Information Processing Systems. Volume 37., Curran Associates, Inc. (2024) 110935 [6] Wang, C.J., Lee, D., Menghini, C., Mols, J., Doughty, J., Khoja, A., Lynch, J., Hendryx, S., Yue, S., Hendrycks, D.: Enigmaeval: benchmark of long multimodal reasoning challenges. arXiv preprint arXiv:2502.08859 (2025) [7] Anthropic: Claude 3.7 sonnet: Frontier reasoning made practical (February 2025) Accessed: 2025-05-02. [8] Paglieri, D., Cupiał, B., Coward, S., Piterbarg, U., Wolczyk, M., Khan, A., Pignatelli, E., Kucinski, Ł., Pinto, L., Fergus, R., et al.: Balrog: Benchmarking agentic llm and vlm reasoning on games. arXiv preprint arXiv:2411.13543 (2024) [9] Costarelli, A., Allen, M., Hauksson, R., Sodunke, G., Hariharan, S., Cheng, C., Li, W., Clymer, J., Yadav, A.: Gamebench: Evaluating strategic reasoning abilities of llm agents. arXiv preprint arXiv:2406.06613 (2024) [10] Wu, Y., Tang, X., Mitchell, T.M., Li, Y.: Smartplay: benchmark for llms as intelligent agents. arXiv preprint arXiv:2310.01557 (2023) [11] Perez-Liebana, D., Liu, J., Khalifa, A., Gaina, R.D., Togelius, J., Lucas, S.M.: General video game ai: multi-track framework for evaluating agents, games and content generation algorithms. IEEE Transactions on Games 11(3) (2019) 195202 [12] Laird, J.E., van Lent, M.: Human-level ais killer application: Interactive computer games. AI Magazine 22(2) (2001) 1525 [13] Yang, J., Yang, S., Gupta, A.W., Han, R., Fei-Fei, L., Xie, S.: Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv preprint arXiv:2412.14171 (2024) [14] Waytowich, N.R., White, D., Sunbeam, M., Goecks, V.G.: Atari-gpt: Investigating the capabilities of multimodal large language models as low-level policies for atari games. arXiv preprint arXiv:2408.15950 (2024) [15] Mosquera, M., Pinzon, J.S., Rios, M., Fonseca, Y., Giraldo, L.F., Quijano, N., Manrique, R.: Can llm-augmented autonomous agents cooperate?, an evaluation of their cooperative capabilities through melting pot. arXiv preprint arXiv:2403.11381 (2024) [16] OpenAI: Openai o3 and o4-mini system card (April 2025) Accessed: 2025-05-10. [17] OpenAI: Openai o1 system card (December 2024) arXiv preprint arXiv:2412.16720. [18] DeepMind, G.: Gemini 2.5: Our most intelligent ai model (March 2025) Accessed: 2025-05-10. 11 [19] Rintanen, J.: Complexity of planning with partial observability. In ICAPS. Volume 4. (2004) 345354 [20] Lau-Zhu, A., Holmes, E.A., Butterfield, S., Holmes, J.: Selective association between tetris game play and visuospatial working memory: preliminary investigation. Applied cognitive psychology 31(4) (2017) 438445 [21] Demaine, E.D., Hohenberger, S., Liben-Nowell, D.: Tetris is hard, even to approximate. In International Computing and Combinatorics Conference (COCOON). Volume 2697 of Lecture Notes in Computer Science., Springer (2003) 351363 [22] Culberson, J.: Sokoban is pspace-complete. (1997) [23] Zaky, A.: Minimax and expectimax algorithm to solve 2048. (2014) [24] Group, A.R.: Ufo: unified framework for gui interaction in windows applications. arXiv (2024) [25] Group, A.R.: Infiguiagent: multimodal agent for gui interaction and reasoning. arXiv (2025) [26] Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al.: Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025) [27] Dor, D., Zwick, U.: SOKOBAN and other motion planning problems. Computational Geometry 13(4) (1999) 215228 [28] Gualà, S., Leucci, S., Natale, E.: Bejeweled, candy crush and other match-three games are (np-)hard. arXiv preprint arXiv:1403.5484 (2014) https://arxiv.org/abs/1403.5484. [29] Dong, Y., Jiang, X., Liu, H., Jin, Z., Gu, B., Yang, M., Li, G.: Generalization or memorization: Data contamination and trustworthy evaluation for large language models. arXiv preprint arXiv:2402.15938 (2024) [30] Carlini, N., Liu, C., Erlingsson, Ú., Kos, J., Song, D.: The secret sharer: Evaluating and testing unintended memorization in neural networks. arXiv preprint arXiv:1802.08232 (2018) [31] Wang, X., Zhuang, B., Wu, Q.: Are large vision language models good game players? In The Thirteenth International Conference on Learning Representations [32] Agashe, S., Han, J., Gan, S., Yang, J., Li, A., Wang, X.E.: Agent s: An open agentic framework that uses computers like human (2024) [33] Tan, W., Zhang, W., Xu, X., Xia, H., Ding, Z., Li, B., Zhou, B., Yue, J., Jiang, J., Li, Y., An, R., Qin, M., Zong, C., Zheng, L., Wu, Y., Chai, X., Bi, Y., Xie, T., Gu, P., Li, X., Zhang, C., Tian, L., Wang, C., Wang, X., Karlsson, B.F., An, B., Yan, S., Lu, Z.: Cradle: Empowering foundation agents towards general computer control. arXiv preprint arXiv:2403.03186 (2024) [34] Khattab, O., Singhvi, A., Maheshwari, P., Zhang, Z., Santhanam, K., Haq, S., Sharma, A., Joshi, T.T., Moazam, H., Miller, H., et al.: Dspy: Compiling declarative language model calls into state-of-the-art pipelines. In The Twelfth International Conference on Learning Representations. (2024) [35] VALS AI: Mmlu-pro benchmark leaderboard. https://www.vals.ai/benchmarks/mmlu_ pro-05-09-2025 (2025) Accessed: 2025-05-13. [36] Hendrycks, D., Burns, C., Kadavath, S., Arora, P., Basart, S., Tang, D.S., et al.: Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 (2021) [37] Phan, L., Gatti, A., Han, Z., Li, N., Hu, J., Zhang, H., Zhang, C.B.C., Shaaban, M., Ling, J., Shi, S., Choi, M., Agrawal, A., Chopra, A., Khoja, A., Kim, R., Ren, R., Hausenloy, J., Zhang, O., Mazeika, M., Yue, S., Wang, A., Hendrycks, D.: Humanitys last exam: Benchmarking ai on grade school science olympiad exams. arXiv preprint arXiv:2501.14249 (2024) 12 [38] Scale AI: Humanitys last exam leaderboard. https://scale.com/leaderboard/ humanitys_last_exam Accessed: 2025-05-14. [39] Scale AI: Humanitys last exam leaderboard (text only). https://scale.com/leaderboard/ humanitys_last_exam_text_only Accessed: 2025-05-14. [40] Rein, D., Hou, B.L., Stickland, A.C., Petty, J., Pang, R.Y., Dirani, J., Michael, J., Bowman, S.R.: Gpqa: Graded physics question answering benchmark for large language models. arXiv preprint arXiv:2311.12022 (2023) [41] VALS AI: Gpqa benchmark leaderboard. https://www.vals.ai/benchmarks/ gpqa-05-09-2025 Accessed: 2025-05-14. [42] Qiu, S., Guo, S., Zhuo, Y., Wang, Y., Li, Z., Zhang, Y., Wang, Y., Li, Z., Zhang, Y., Wang, Y., Li, Z., Zhang, Y.: Phybench: Holistic evaluation of physical perception and reasoning in large language models. arXiv preprint arXiv:2504.16074 (2025) [43] Hao, Y., Gu, J., Wang, H.W., Li, L., Yang, Z., Wang, L., Cheng, Y.: Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444 (2025) [44] Vals AI: Math 500 benchmark leaderboard. https://www.vals.ai/benchmarks/ math500-05-09-2025 Accessed: 2025-05-14. [45] Patel, B., Chakraborty, S., Suttle, W.A., Wang, M., Bedi, A.S., Manocha, D.: Aime: Ai system optimization via multiple llm evaluators. arXiv preprint arXiv:2410.03131 (2024) [46] Vals AI: Aime benchmark leaderboard. https://www.vals.ai/benchmarks/ aime-2025-05-09 Accessed: 2025-05-14. [47] White, C., Dooley, S., Roberts, M., Pal, A., Feuer, B., Jain, S., Shwartz-Ziv, R., Jain, N., Saifullah, K., Naidu, S., Hegde, C., LeCun, Y., Goldstein, T., Neiswanger, W., Goldblum, M.: Livebench: challenging, contamination-free llm benchmark. arXiv preprint arXiv:2406.19314 (2024) [48] LiveBench Team: Livebench leaderboard. https://livebench.ai/#/?Coding=a& Mathematics=a&Data+Analysis=a&Language=a&IF=a Accessed: 2025-05-14. [49] Zhuo, T.Y., Vu, M.C., Chim, J., Hu, H., Yu, W., Widyasari, R., Yusuf, I.N.B., Zhan, H., He, J., Paul, I., Brunner, S., Gong, C., Hoang, T., Zebaze, A.R., Hong, X., Li, W.D., Kaddour, J., Xu, M., Zhang, Z., Yadav, P., Jain, N., Gu, A., Cheng, Z., Liu, J., Liu, Q., Wang, Z., Lo, D., Hui, B., Muennighoff, N., Fried, D., Du, X., de Vries, H., Von Werra, L.: Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877 (2024) [50] Aider Team: Aider llm leaderboards. https://aider.chat/docs/leaderboards/ Accessed: 2025-05-14. [51] BigCodeBench Team: Bigcodebench leaderboard. https://bigcode-bench.github.io/ Accessed: 2025-05-14. [52] Scale AI: Vista: Visual language understanding benchmark leaderboard. https://scale. com/leaderboard/visual_language_understanding Accessed: 2025-05-14. [53] LMSYS Org: Chatbot arena leaderboard. https://lmarena.ai/leaderboard Accessed: 2025-05-14. [54] Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., et al.: Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502 (2023) [55] Vals AI: Mmmu benchmark leaderboard. mmmu-05-09-2025 Accessed: 2025-05-14. https://www.vals.ai/benchmarks/ [56] Sirdeshmukh, V., Deshpande, K., Mols, J., Jin, L., Cardona, E.Y., Lee, D., Kritz, J., Primack, W., Yue, S., Xing, C.: Multichallenge: realistic multi-turn conversation evaluation benchmark challenging to frontier llms. arXiv preprint arXiv:2501.17399 (2025) [57] Scale AI:"
        },
        {
            "title": "Multichallenge",
            "content": "leaderboard. https://scale.com/leaderboard/ multichallenge Accessed: 2025-05-14. [58] Scale AI: Enigmaeval benchmark leaderboard. https://scale.com/leaderboard/ enigma_eval Accessed: 2025-05-14. [59] Mazur, L.: Nyt connections benchmark: Evaluating llms with extended word association puzzles. https://github.com/lechmazur/nyt-connections Accessed: 2025-05-14. [60] Schrader, M.P.B.: gym-sokoban. https://github.com/mpSchrader/gym-sokoban (2018) [61] TetrisRL: Tetrisrl: Reinforcement learning for tetris. https://github.com/jaybutera/ tetrisRL (2023) GitHub repository. [62] Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al.: Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115 (2024) [63] Tesauro, G., et al.: Temporal difference learning and td-gammon. Communications of the ACM 38(3) (1995) 5868 [64] Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al.: Mastering the game of go without human knowledge. nature 550(7676) (2017) 354359 [65] Nasir, M.U., James, S., Togelius, J.: Gametraversalbenchmark: Evaluating planning abilities of large language models through traversing 2d game maps. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. (2024) [66] Hopkins, J., Bakler, M., Khan, A.: Factorio learning environment. arXiv preprint arXiv:2503.09617 (2025) [67] Xie, J., Zhang, R., Chen, Z., Wan, X., Li, G.: Whodunitbench: Evaluating large multimodal agents via murder mystery games. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. (2024) [68] Hudi, F., Winata, G.I., Zhang, R., Aji, A.F.: Textgames: Learning to self-play text-based puzzle games via language model reasoning. arXiv preprint arXiv:2502.18431 (2025) [69] Qiao, D., Wu, C., Liang, Y., Li, J., Duan, N.: Gameeval: Evaluating llms on conversational games. arXiv preprint arXiv:2308.10032 (2023) [70] Hu, L., Li, Q., Xie, A., Jiang, N., Stoica, I., Jin, H., Zhang, H.: Gamearena: Evaluating llm reasoning through live computer games. arXiv preprint arXiv:2412.06394 (2024) [71] Jimenez, C.E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., Narasimhan, K.: Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770 (2023) [72] Zhou, S., Xu, F.F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Ou, T., Bisk, Y., Fried, D., et al.: Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854 (2023) [73] He, H., Yao, W., Ma, K., Yu, W., Dai, Y., Zhang, H., Lan, Z., Yu, D.: Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919 (2024) [74] Trivedi, H., Khot, T., Hartmann, M., Manku, R., Dong, V., Li, E., Gupta, S., Sabharwal, A., Balasubramanian, N.: Appworld: controllable world of apps and people for benchmarking interactive coding agents. arXiv preprint arXiv:2407.18901 (2024) [75] Xie, T., Zhang, D., Chen, J., Li, X., Zhao, S., Cao, R., Hua, T.J., Cheng, Z., Shin, D., Lei, F., et al.: Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems 37 (2024) 5204052094 14 [76] Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Ding, H., Men, K., Yang, K., et al.: Agentbench: Evaluating llms as agents. In ICLR. (2024) [77] Mialon, G., Fourrier, C., Wolf, T., LeCun, Y., Scialom, T.: Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations. (2023) [78] Zhai, S., Bai, H., Lin, Z., Pan, J., Tong, P., Zhou, Y., Suhr, A., Xie, S., LeCun, Y., Ma, Y., et al.: Fine-tuning large vision-language models as decision-making agents via reinforcement learning. Advances in neural information processing systems 37 (2024) 110935110971 [79] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017) [80] Settles, B.: Active learning literature survey. Science 10(3) (1995) 237304 [81] Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., et al.: Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476 (2025) [82] Valmeekam, K., Marquez, M., Olmo, A., Sreedharan, S., Kambhampati, S.: Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. Advances in Neural Information Processing Systems 36 (2023) 38975 [83] Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., Schulman, J.: Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 (2021) [84] Li, J., Hui, B., Qu, G., Yang, J., Li, B., Li, B., Wang, B., Qin, B., Geng, R., Huo, N., et al.: Can llm already serve as database interface? big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems 36 (2024) [85] Yao, S., Chen, H., Yang, J., Narasimhan, K.: Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems 35 (2022) 2074420757 [86] Kendall, M.G.: new measure of rank correlation. Biometrika 30(1/2) (1938) 8193 [87] Webber, W., Moffat, A., Zobel, J.: similarity measure for indefinite rankings. In Proceedings of the 19th international conference on World wide web, ACM (2010) 577 [88] Glass, G.V.: Primary, secondary, and meta-analysis of research. Educational Researcher 5(10) (1976) 38 [89] Gosset, W.S.: The probable error of mean. Biometrika 6(1) (1908)"
        },
        {
            "title": "A Training Details",
            "content": "A.1 Training Framework Sokoban and Tetris training is based on the StarPO-S (State-Thinking-Actions-Reward Policy Optimization - Stable) framework introduced in the RAGEN system [3], which extends reinforcement learning to multi-turn agent settings by optimizing full interaction trajectories. LLM responses are structured in two formats: with <think> tokens, where the LLM generates intermediate reasoning traces followed by actions enclosed in <answer> tokens; and without <think> tokens, where the LLM directly outputs actions within <answer> tokens. The only exception is GSM8K, where both thinking and answer tokens are omitted to maintain consistency with standard evaluation prompts on math datasets. Each task is implemented as an environment that provides rule-based cumulative rewards over entire rollouts. Optimization is performed using Proximal Policy Optimization (PPO) [79], applied at the token level with clipped probability ratios. Training proceeds by generating multiple trajectories from batch of prompts. The LLM (policy model) is updated using estimated advantages computed by separately trained critic model. Example Response Formats With thinking tokens: <think>...</think><answer>action action</answer> Without thinking tokens: <answer>action action</answer> Hyperparameter Settings. The maximum model response length per turn is 400 tokens during training, and 1024 tokens during evaluation. During training, the temperature is set to 1.0 to encourage exploration with 16 trajectories generated per sample; at inference, we use greedy sampling to ensure reproducibility. We train Qwen-2.5-7B-Instruct for 200 steps, with batch size of 32 samples. We then report the performance on separate test set on the checkpoint saved at step 100 or 200, whichever leads to better performance on the validation set on the trained task. The test set size is 256 samples for all tasks, except for Blocksworld, where the complete set of 156 possible configurations is evaluated exhaustively. To improve training stability in long-horizon tasks, we follow most of the default training settings from StarPO-S. We filter out low-variance rollouts and retain only the top 25% most informative trajectories [80], selected based on the standard deviation of rewards across trajectories generated from the same prompt. We remove the KL penalty term from PPO, and apply asymmetric clipping (with lower and upper bounds set to 0.2 and 0.28 respectively) to encourage learning from high-reward rollouts [81]. Experiments were conducted using 8 NVIDIA H100 GPUs (80 GB each) with model parallelism. Batch size and response length were adjusted to fit within hardware memory constraints, as reported above. Each training instance takes approximately 23 hours for 100 steps. A.2 Tasks We consider the following tasks in this analysis. We trained Qwen2.5-7B-Instruct [62] on 6 6 Sokoban and 2-block-type Tetris separately, and evaluated on all the below tasks. LLM is allowed to output up to 5 actions per response. The number of maximum response turns varies across tasks, as specified below. Detailed prompts are listed in A.5 SimpleSokoban [60, 3]: We consider simplified Sokoban with 66 or 88 grids, containing only single box. The LLM is allowed up to 5 response turns to complete the task and in total 10 actions to complete the task. SimpleTetris [61]: This task involves simplified version of Tetris on 4 4 board with only one or two types of blocks. In the 1-type setting, all blocks are 1 1. In the 2-type setting, we use 1 2 and 2 1 blocks. We disable rotation operations for both settings. The LLM is allowed up to 10 response turns and up to 40 actions in total. Blocksworld [82]: Blocksworld is classical planning task that stacks blocks on table, where block can only be moved if no other block is on top of it. We focus on the 3-block setting and evaluate three different formats to represent states: natural language, 1D list, and 2D table representations. We allow up to 10 response turns and up to 20 actions in total. Natural Language: Block 1 is on top of block 3, block 3 is on top of block 2, and block 2 is on the table. 16 Table 5: Impact of thinking-token prompting on model generalization across games, planning, and agentic Tasks. Model Thinking Token Qwen-7B-Instruct Ours (Sokoban) Ours (Sokoban) Qwen-7B-Instruct Ours (Sokoban) Ours (Sokoban) Train - - Test Games Sokoban Tetris 6 6 8 8 1 type 2 types Planning Blocksworld 1D 2D Text Agentic WebShop 11.3 24.2 10.9 21.1 24.6 19. 5.9 9.0 8.6 7.8 12.9 5.1 9.0 17.6 26.9 1.6 34.4 32.8 4.7 5.1 7.0 11.3 19.5 12. 64.7 64.1 68.6 34.0 44.9 44.9 17.9 32.7 21.2 17.3 23.1 30.1 9.0 29.5 15.4 12.8 13.5 23. 7.0 19.1 10.5 37.9 43.8 41.8 1D List: [3, 0, 2] indicating block 1 is on top of block 3, block 2 is on the table, and block 3 is on top of block 2. 2D Table: See the prompt example for Blocksworld in Appendix A.5. GSM8K [83]: This dataset consists of grade school math word problems. We use chain-ofthought prompting (e.g., Lets think step by step) to elicit reasoning from the LLM. Unlike other tasks that require multiple turns, LLMs will give the answer in single response. To adapt it to our multi-turn setting, if the LLM provides correct answer, the session ends. If the answer is incorrect, we append user messageIncorrect. Please think again.to trigger new response. We report both single-turn and 5-turn success rates. BIRD [84]: This task evaluates text-to-SQL translation. We report only the single-turn success rate for this task. WebShop [85]: WebShop simulates an online shopping environment where the model can search for items and complete purchases. The LLM is allowed up to 10 response turns and up to 30 actions in total to complete the order. A.3 Thinking Tokens Table 4 shows that training on Sokoban and Tetris improves cross-game, planning, and agentic task performance when models are explicitly prompted to \"think\" using thinking tokens. To test whether this improvement persists under different prompting formats, Table 5 compares model generalization with and without thinking tokens at training and inference. The impact varies across tasks: for same-game and WebShop, performance is highest when the prompting format is consistent between training and inference. For Tetris and planning, the effects are mixed. Notably, in planning tasks, models trained with thinking tokens mostly outperform others regardless of test-time prompting, highlighting the importance of training models to think to perform well in planning. A.4 Effectiveness of Game-Only, Math-Only, and Mixed Training As shown in Section 3.3, training solely on games such as Sokoban or Tetris improves planning and spatial reasoning tasks, but yields limited benefits for math and coding. To further explore cross-domain generalization, we evaluate two additional training settings: (1) training solely on GSM8K, and (2) training on an equal mixture of Sokoban and GSM8K. The performance on Sokoban, Tetris and Blocksworld are evaluated using thinking tokens, while GSM8K is prompted using chain-of-thought (CoT) style. The resulting generalization performance is compared in Figure 4. Success rates are evaluated for every 10 steps during Sokoban training, and smoothed trend line is shown using running average with centered window. Training solely on Sokoban leads to the strongest gains in Sokoban itself, as well as significant improvements in other spatial-reasoning environments such as Blocksworld-2D and Tetris (1-type). However, it provides no improvement on GSM8K, which focuses on math reasoning. In contrast, models trained only on GSM8K achieve the best performance on GSM8K but show minimal or no generalization to Sokoban, Blocksworld, or Tetris. The mixture-trained agent exhibits performance in between the two extremes. Notably, it starts out matching the GSM8K-only model on GSM8K, but plateaus earlier and underperforms slightly by the end of training. This suggests that while mixture training promotes moderate generalization across domains, it may compromise peak performance on domain-specific tasks. Interestingly, the mixture-trained model achieves comparable gains to 17 Figure 4: Success rates during training across different evaluation tasks, with models trained on Sokoban, GSM8K, or an half-half mixture of both. GSM8K-only training within the first 100 steps on GSM8K, indicating mixture training has potential for reducing the number of math-specific samples required during the early training phase. A.5 Prompts. We use the following list of prompts for training and evaluation. Specifically, each prompt explicitly informs the LLM of the expected reasoning format, the maximum number of actions allowed per response, and the response length limit, and reward. Notably, the length limit stated in the prompt is intentionally shorter than the actual enforcement threshold to discourage overly long responses. SimpleSokoban Prompt: You are solving the Sokoban puzzle. You are the player and you need to push all boxes to targets. When you are right next to box, you can push it by moving in the same direction. You cannot push box through wall, and you cannot pull box. The answer should be sequence of actions, like <answer>Right Right Up</answer>. The meaning of each symbol in the state is: #: wall, _: empty, O: target, Your available actions are: Up, Down, Left, Right You can make up to 10 actions, separated by the action separator : box on target, X: box, P: player, S: player on target Turn 1: State: ###### ###### #O#### #XP### #__### ###### You have 10 actions left. Always output: <think> [Your thoughts] </think> <answer> [your answer] </answer> with no extra text. Strictly follow this format. Max response length: 100 words (tokens). Prompt for next turn: Reward: -0.1 Turn 2: State: {Current State} You have 9 actions left. Always output: <think> [Your thoughts] </think> <answer> [your answer] </answer> with no extra text. Strictly follow this format. Max response length: 100 words (tokens). 18 Blocksworld Prompt: You are solving Blocksworld problems. Each state is represented by graphical stack of blocks. Each state is represented by graphical stack of blocks. There are 3 blocks: block 1, block 2, and block 3. You can only move block if there is no other block on top of it. Each action should be formatted as:\"move to Y\", where is the block number youre moving, and is either 0, meaning move the block to the table; or another block number, meaning move it on top of that block. state is like this: _ 2 _ _ 13 This means: Block 2 is on top of block 1, and Block 1, 3 is on the table. Another example: _ 1 _ 3 _ 2 This means: Block 1 is on top of block 3, Block 3 is on top of block 2, and Block 2 is on the table. You are given the current state and the goal state, and you need to find sequence of actions to move the blocks to the goal state. The answer should be sequence of actions, like <answer>(move 2 to 0) (move 1 to 2)</answer> Turn 1: State: Graphical representation (Current): {Current State} Graphical representation (Goal): {Goal State} You have 20 actions left. Always output: <think> [Your thoughts] </think> <answer> [your answer] </answer> with no extra text. Strictly follow this format. Max response length: 100 words (tokens). Prompt for next turn: Reward: - Turn 2: State: Graphical representation (Current): {Current State} Graphical representation (Goal): {Goal State} You have 19 actions left. Always output: <think> [Your thoughts] </think> <answer> [your answer] </answer> with no extra text. Strictly follow this format. Max response length: 100 words (tokens). 19 SimpleTetris Prompt: You are solving simplified version of Tetris puzzle. The game board is 4*4 grid, and blocks fall one at time from the top. Your goal is to fill the last row completely with blocks to get points, with the state such as ____ ____ ____ XXXX You can only move the current block left, right, or down. The answer should be sequence of actions, like <answer>Left Right Down</answer> Hint: If we see empty space (_) in the last row, try to left/right move the current block to let it drop down to the empty space. The meaning of each symbol in the state is: #: filled spaces, _: empty, X: current block Your available actions are: Left, Right, Down You can make up to 40 actions, separated by the action separator Turn 1: State: XX__ ____ ____ ____ You have 20 actions left. Always output: <think> [Your thoughts] </think> <answer> [your answer] </answer> with no extra text. Strictly follow this format. Max response length: 100 words (tokens). Prompt for next turn: Reward: -0. Turn 2: State: ____ XX__ ____ ____ You have 19 actions left. Always output: <think> [Your thoughts] </think> <answer> [your answer] </answer> with no extra text. Strictly follow this format. Max response length: 100 words (tokens). GSM8K Prompt: You are solving Math problems. Lets think step by step. Always put the answer in integer at the end of your response. Question: Frankie watches TV after he finishes his homework every night. On Monday and Tuesday, he watched 1-hour episode of his favorite show each night. On Wednesday, he watched few episodes of 30-minute show. On Thursday, he finished homework early and watched 1-hour episode and 30-minute show. On Friday, he got to stay up late for the weekend, so he watched two 1-hour episodes. If he watched 7 hours of TV in all, how many 30-minute episodes did he watch on Wednesday? Prompt for next turn: Reward: -0.1 Question: Incorrect. Please think again. 20 BIRD Prompt: Translate the question into SQL given database information. Replay with one fenced block: sql {SQL query here} Turn 1: State: DB schema: {Table Schema 1} {Table Schema 2} {Table Schema 3} ... Question: How many customers share the most common last name? You have 3 actions left. Always output: <think> [Your thoughts] </think> <answer> [your answer] </answer> with no extra text. Strictly follow this format. Max response length: 1024 words (tokens). Prompt for next turn: Reward: -0.1 Turn 2: State: DB schema: {Table Schema 1} {Table Schema 2} {Table Schema 3} ... DB query output from last turn: {Query output/error} You have 2 actions left. Always output: <think> [Your thoughts] </think> <answer> [your answer] </answer> with no extra text. Strictly follow this format. Max response length: 100 words (tokens). 21 Webshop Prompt: Youre helpful assistant. You are browsing an online shop. Based on the instruction, find the product that matches the production description. You need to iteratively take action, read the website and decide what action to take next until finding the product. Available actions depends on the page: in the search page you can search keywords, in the search result page you can click an item url or click[next >] to navigate to next page, in the product page you can click[description] or click[features] to see the details, click[blue] or click[x-large] to choose size and colors, click[buy now] when you decided to buy the product, click[back to search] to return to search page. Example process: need gingko light and 20x20 pillow cover that is hand painted. First search[gingko light 20x20 pillow cover hand painted], answer format: <answer>search[blanket with fleece throw]</answer>. Valid answer is search[<keywords>] or click[<clickable>]. Turn 1: State: Instruction: {target task with constraints} Available actions: search[<content>] You have 10 actions left. Always output: <think> [Your thoughts] </think> <answer> [your answer] </answer> with no extra text. Strictly follow this format. Max response length: 100 words (tokens). Prompt for next turn: Reward: 0.0 Turn 2: State: Instruction: {target task with constraints} {web page description} Available actions: {all available actions} You have 9 actions left. Always output: <think> [Your thoughts] </think> <answer> [your answer] </answer> with no extra text. Strictly follow this format. Max response length: 100 words (tokens)."
        },
        {
            "title": "B Data contamination Study",
            "content": "We evaluate two types of potential data contamination in lmgame-Bench : vision-level and text-level. Our goal is to determine whether pretrained LLMs rely on memorized assets instead of real-time reasoning. B.1 Vision-Level Contamination: Super Mario Bros Setup. We extracted the first ten RGB frames from SMB Level 1-1 and randomly shuffled their order. Each model was then prompted to reconstruct the original temporal sequence 15 times. Reconstruction quality was measured by pairwise frame-order accuracy, Kendalls τ rank coefficient [86], and Rank-Biased Overlap (RBO) [87]. Finally, we computed the Pearsons and Spearmans correaltion coefficients between these alignment metrics and the overall performance rank of each model. Models tested: Claude-3.5-Sonnet, Claude-3.7-Sonnet-Thinking, Gemini-2.5-pro-Preview, o4-mini, o3, LLaMA-4-Maverick. Results. The pairwise accuracy remains relatively low overall, with the highest accuracy reaching only around 30%, as shown in Table 6. Notably, both the beginning and final positions exhibit relatively high accuracy in terms of ordering, whereas the middle positions perform significantly worse, as illustrated in Fig. 5. Beyond pairwise accuracy, we compute Kendalls rank correlation coefficient and Rank-Biased Overlap (RBO) to evaluate how well each models predicted frame order aligns with the ground-truth temporal sequence. As shown in Fig. 6, all models exhibit positive correlation, although only Gemini-2.5-pro-preview, o3, and o4-mini achieve moderate agreement with ground truth, while the remaining models show weak alignment. To further quantify alignment strength, we normalize each metric by computing the percentage of the perfect score, using the formula (value random)/(perfect random), which reveals similar ranking patterns for both Kendalls τ and RBO. To test whether alignment quality is predictive of general model performance, we compute both Pearson and Spearman correlations between the alignment metrics and the performance ranks of the models. Kendalls τ shows moderate negative Pearson correlation with performance rank (r = 0.7089, = 0.1148, testing the null hypothesis of no linear association) and moderate negative Spearman correlation (ρ = 0.5429, = 0.2657, testing the null hypothesis of no monotonic association). RBO shows weaker Pearson correlation (r = 0.3847, = 0.4515, testing the null hypothesis of no linear association) and moderate negative Spearman correlation (ρ = 0.6571, = 0.1562, testing the null hypothesis of no monotonic association). Kendalls τ and RBO remain highly correlated (Pearson = 0.8772, = 0.0217, testing the null hypothesis of no linear association). Despite this internal consistency, the lack of statistically significant correlation with model performance rank suggests that visual sequence alignmentused here as proxy for vision-based data contaminationdoes not appear to be major factor in determining current model performance rankings. Table 6: Evaluation metrics (Average Pairwise Accuracy, Kendalls τ , and RBO) on the frame ordering task for Super Mario Bros level 1-1. Higher values indicate better reconstruction of the correct temporal sequence from shuffled RGB inputs. Model Accuracy Kendalls τ RBO o3 gemini-2.5-pro-preview (thinking) o4-mini claude-3-7-sonnet claude-3-5-sonnet llama-4-maverick Perfect Random 0.307 0.300 0.233 0.147 0.180 0.100 1.000 0. 0.449 0.458 0.324 0.044 0.099 0.019 1.000 -0.037 0.498 0.489 0.463 0.422 0.418 0.324 0.651 0.299 Figure 5: Position-wise reconstruction accuracy for the shuffled Super Mario Bros level 1-1 frames. Lighter cells denote higher accuracy; only the first and last positions remain relatively high accuracy 23 Figure 6: Kendalls τ and RBO scores for each model on the frame ordering task. Higher values indicate stronger alignment between predicted and ground-truth frame sequences. Only Gemini-2.5pro-preview, o3, and o4-mini achieve moderate agreement. Figure 7: Pearson and spearman correlation between model performance ranks and their alignment scores (Kendalls τ and RBO). Although negative trend is observed, the correlation is not statistically significant. B.2 Text-Level Contamination: Ace Attorney Setup. We test whether models reproduce scripted lines from the first two publicly available cases of Ace Attorney. Each case is split into an evidence list and cross-examination script. We then prompt models to generate these sections and compute cosine similarity to the ground truth using Sentence-BERT embeddings. Models tested: Claude-3.5-Sonnet, Claude-3.7-Sonnet-Thinking, Gemini-2.5-pro-Preview, o4-mini, o3, LLaMA-4-Maverick. 24 Table 7: Text-level similarity metrics and performance on Ace Attorney. Similarity is measured via Sentence-BERT cosine scores. Cross-case comparisons verify metric reliability. Model Sister Turnabout Total Sim. Rank Game Score Gen[F] vs GT[S] Gen[S] vs GT[F] Gen[F] vs Gen[S] 1st Turnabout o3-2025-04-16 gemini-2.5-pro-preview-05-06 (thinking) claude-3-5-sonnet-20241022 o1-2024-12-17 grok-3-mini-beta gpt-4.1-2025-04-14 claude-3-7-sonnet-20250219(thinking) gemini-2.5-flash-preview-04-17 (thinking) gemini-2.0-flash-thinking-exp-1219 LLaMA-4 Maverick O4 Mini 0.863 0.867 0.845 0.809 0.782 0.755 0.708 0.731 0.659 0.491 0.586 0.904 0.867 0.816 0.842 0.833 0.812 0.830 0.728 0.780 0.734 0.625 1.767 1.734 1.660 1.651 1.614 1.567 1.538 1.460 1.438 1.224 1.212 2 3 6 1 5 7 4 8 9 13 23 20 6 26 7 6 8 4 4 0 1 0.607 0.561 0.609 0.634 0.646 0.697 0.684 0.617 0.719 0.594 0.574 0.593 0.544 0.538 0.589 0.550 0.591 0.535 0.538 0.562 0.535 0.531 0.657 0.532 0.672 0.675 0.707 0.787 0.588 0.729 0.810 0.772 0.764 Figure 8: Global Embedding Correlation (Generated + Ground Truth) between model-generated texts across \"First\" and \"Sister\" turnabouts. Results. Table 7 presents similarity scores for 11 models in both cases, alongside their in-game performance and cross-case comparisons. In particular: Models with higher similarity to the script tend to perform better in the game (e.g., o3, Gemini-2.5-Pro-Preview), suggesting possible memorization effects. Cross-case similarities (e.g., Gen[First] vs GT[Sister]) are consistently lower, demonstrating that the metric is sensitive to true alignment rather than generic language similarity. Self-similarity between generated case outputs (Gen[First] vs Gen[Sister]) is relatively high for some models, suggesting stylistic or template reuse. The Sentence-BERT cosine similarity between the ground-truth scripts for the two cases (1st Turnabout vs Sister Turnabout) is 0.599, which serves as baseline for evaluating cross-case similarities. To quantify the relationship between textual similarity and performance, we compute linear correlations between total similarity scores and both game score and leaderboard rank. As shown in Figure 9, the results hold consistently across both the full model set and the 6-model subset. In all cases, similarity is significantly correlated with better performance, confirming that models may benefit from memorized content. Mitigation. To suppress memorized recall and enforce reasoning-based responses, we apply structured prompt interventions. These include: (1) explicitly instructing the model to forget prior knowledge of the game; (2) requiring detailed causal reasoning (cause, evidence, effect) for each action; (3) asking the model to self-evaluate whether its behavior was memoryor reasoning-driven; and (4) modifying the input by replacing all character and item names with neutral tokens (e.g., \"Lawyer 1\", \"Evidence A\"), and paraphrasing both background context and key contradictions. Results. 25 (a) Full model set. Left: similarity vs. game score (r = 0.773, = 0.005). Right: similarity vs. leaderboard rank (r = 0.911, = 9.840 105). (b) 6-model subset. Left: similarity vs. game score (r = 0.870, = 0.024). Right: similarity vs. leaderboard rank (r = 0.943, = 0.005). Figure 9: Linear correlations between script similarity and model performance across the full model set (left) and the 6-model subset used for cross-modality comparison (right). Shaded areas indicate 95% confidence intervals. In both settings, higher similarity is significantly associated with higher game scores and higher leaderboard position (i.e., lower rank number) Before mitigation (full model set), total similarity scores strongly correlate with leaderboard rank (r = 0.773, = 0.005), indicating that models with higher overlap to the original script tend to perform better. After applying our prompt-based mitigation (name masking, paraphrasing, and reasoning enforcement), this correlation becomes statistically insignificant (r = 0.700, = 0.120), suggesting reduced reliance on memorization. Similarly, the correlation between similarity and game score drops to = 0.617 (p = 0.192), further supporting the effectiveness of our intervention (see Figure 10a). Independent reasoning-based evaluations using o3 as an LLM judge remain predictive of post-mitigation performance, with strong negative correlation to rank (r = 0.850, = 0.031). (see Figure 10b). (a) Similarity vs. new rank (left) and new score (right). No statistically significant correlations remain after mitigation, suggesting similarity no longer explains model success. (b) Evaluator score vs. new rank (left) and similarity (right). Evaluated reasoning correlates strongly with new rank (r = 0.854, = 0.031) but not with similarity, suggesting post-mitigation rankings reflect reasoning. Figure 10: Post-mitigation analysis. Left: model similarity no longer predicts rank or score. Right: LLM-as-Judge (o3) evaluations suggest model ranking is now aligned with reasoning quality, not memorized content. Conclusion. Initial high performance in the Ace Attorney task is partially attributable to memorization of publicly available scripts. Prior to mitigation, script similarity was significantly predictive of performance. After structured prompt-based mitigationincluding name masking, paraphrasing, and enforced reasoningthis correlation disappears. Post-intervention rankings instead align with reasoning quality, as verified by an independent evaluator (o3). B.3 Extra Text-Level Contamination: Super Mario Bros Setup. To assess text-level contamination for Super Mario Bros., we extract detailed level layout description of World 1-1 from MarioWiki. This description is segmented into sequence of temporally ordered segments, denoted by [T 1] through [T 18], representing key points in gameplay. We prompt models to generate layout descriptions aligned with these time points and compare them to the ground truth. Models tested: o4-mini, o3, Claude-3.7-Sonnet, Claude-3.5-Sonnet, LLaMA-4-Maverick. Result. We use Sentence-BERT embeddings to calculate pairwise cosine similarity between generated texts and the 1-1 ground truth. Notably, the similarity between the generated descriptions and the true 26 1-1 layout is consistently lower than the similarity between two distinct ground truth descriptions (World 1-1 vs. 1-2). This suggests that models do not rely on memorized textual layout from training corpora and instead generalize loosely or hallucinate. See Table 8 for quantitative comparisons. representative side-by-side sample comparison between O3 and ground truth is shown in Figure 11. Table 8: Text-level similarity metrics for Super Mario Bros. 1-1 layout descriptions. Similarity is measured using Sentence-BERT cosine scores. The last column shows the ground truth similarity between level 1-1 and 1-2 as baseline. Model Gen[1-1] vs GT[1-1] Gen[1-2] vs GT[1-2] Gen[1-1] vs Gen[1-2] Gen[1-1] vs GT[1-2] Gen[1-2] vs GT[1-1] GT[1-1] vs GT[1-2] o3 (2025-04-16) o4-Mini (2025-04-16) Claude-3.7 Sonnet (2025-02-19) Claude-3.5 Sonnet (2024-10-22) LLaMA-4 Maverick FP8 0.753076 0.708403 0.679185 0.686953 0.643627 0.813358 0.725378 0.627961 0.604426 0.590322 0.882990 0.852628 0.821135 0.795933 0.846092 0.749037 0.707597 0.560105 0.587022 0.542652 0.800998 0.799113 0.750460 0.701139 0. 0.844817 0.844817 0.844817 0.844817 0.844817 Representative side-by-side sample comparison (T3T8) Ground Truth [T3] They then reach the first ? Block of the game. [T4] They encounter the six-block triangle formation of ? Blocks and regular bricks, with Magic Mushroom inside the left block. [T5] set of three Pipes can be found, with Little Goombas located in between them. [T6] The final pipe among the set leads to bonus area with 19 coins and can be entered to bypass much of the level, exiting at the fifth pipe near the end. [T7] If the pipe is not taken, hidden 1-Up Mushroom block appears just after the pipes. [T8] pit follows, then another ? Block containing either Magic Mushroom or Fire Flower, depending on the players form. o3 Generated [T3] Striking the Question Block yields Super Mushroom that slides right along the ground past the hill. [T4] Mario next encounters pair of spaced green pipes emerging from the ground, each occasionally spawning snapping Piranha Plant. [T5] Between those pipes, two more Little Goombas approach in tandem over flat terrain. [T6] Three floating Question Blocks form row above bricks, with the middle hiding helpful Fire Flower. [T7] lone Koopa Troopa patrols beneath these blocks. [T8] Shortly past this, tall staircase of ground bricks leads up then down, concealing hidden 1-Up Block atop the first step. Figure 11: representative side-by-side sample comparison between o3 and ground truth. B.4 Summary Our contamination analysis reveals: Vision-level contamination remains negligible: models reliably reconstruct only the first and last frames, and their alignment coefficients (Kendalls τ , RBO) show at best moderate correlation that is not statistically linked to overall performance rank, indicating no reliance on memorized visual sequences. Text-level contamination is initially significant in Ace Attorney, where models performance strongly correlates with script similarity. After prompt-based mitigation, this correlation disappears, and performance aligns instead with independently judged reasoning quality. LLM-as-Judge (o3) evaluations confirm that post-mitigation success stems from causal reasoning rather than rote recall. This reinforces the importance of controlled prompting for disentangling memorization from genuine inference."
        },
        {
            "title": "C Prompt Optimization",
            "content": "In this section, we present case study illustrating our two-stage prompt optimization approach. We design two empirically optimized baseline prompts developed by equally adequate computer science graduate students based on Figure 1. Subsequently, we employ DSPy to bootstrap an optimized prompt for each baseline, selecting from diverse set of five optimizer models and retaining only the best-performing prompt. We show performance variance across prompts optimized through bootstrapping is lower than that of baseline prompts. 27 C.1 Empirically Optimized Baseline Prompts Game 2048 Empirically Optimized Prompt Template 1 system_prompt: You are an intelligent AI player playing the 2048 game. Your goal is to make strategic moves to combine tiles and reach the highest possible tile value. IMPORTANT: You MUST format your response using EXACTLY these lines: thought: [Your reasoning about the game state] move: [move] Where [move] must be one of: \"up\", \"down\", \"left\", or \"right\". Do not include # or any other prefix. Start directly with \"thought:\" followed by your analysis. user_prompt: 2048 Game Quick Guide: Primary Goal: Combine like tiles to create tiles with higher values. Ultimate Goal: Create tile with the value 2048 or higher. Game Mechanics: - The game is played on 4x4 grid. - Each move (up, down, left, right) shifts all tiles in that direction. - Tiles with the same value that collide during move combine into single tile with twice the value. - After each move, new tile (2 or 4) appears in random empty cell. - The game ends when there are no valid moves left. Action Space: You must select one of these 4 moves: - up: Shift all tiles upward - down: Shift all tiles downward - left: Shift all tiles to the left - right: Shift all tiles to the right Key Strategies: 1. Build stable structure - Keep your highest value tiles in corner. 2. Maintain clear path - Always have direction where you can combine tiles. 3. Chain reactions - Set up sequences of merges that can happen in single move. 4. Look ahead - Think about the consequences of your moves 2-3 steps ahead. 5. Building patterns - Common patterns include: (1) Snake/Zig-zag pattern: Arrange tiles in decreasing order in zigzag; (2) Corner anchoring: Keep the highest tile in corner and build around it. Avoid: - Getting high-value tiles stuck in the middle of the board - Creating scattered small values that block potential merges - Making moves that could lead to grid lock Previous Game History: {Previous Game History} Please analyze the 2048 board and determine the best move. {Symbolic Board Features} Key considerations: - Look for opportunities to merge similar tiles - Maintain your highest tiles in corner - Keep space for new tiles to appear - Avoid trapping high-value tiles in the middle IMPORTANT - FORMAT YOUR RESPONSE EXACTLY LIKE THIS: thought: [your analysis here] move: [move] Where [move] must be one of: \"up\", \"down\", \"left\", or \"right\". Do NOT use # or any other prefix. Start directly with \"thought:\" followed by your analysis. 28 Game 2048 Empirically Optimized Prompt Template 2 system_prompt: You are an AI agent specialized in 2048 gameplay, your purpose is to analyze board states and suggest optimal moves that maximize your scores. ## Your Available Actions For each turn, you must select one command: - up: Shifts the entire grid upward - down: Shifts the entire grid downward - left: Shifts the entire grid leftward - right: Shifts the entire grid rightward When you choose direction (up, down, left, or right), all tiles shift accordingly. Matching tiles that collide during this shift combine into single tile representing their sum. After every move, new tile with value of either 2 or 4 appears in random empty cell. The game concludes when no legal moves remain. user_prompt: ## 2048 Gameplay Strategies ### Principles The most successful 2048 strategies typically involve: 1. Establish your highest-value tile in one corner and build descending value structure around it. 2. Maintain consistent movement patterns that preserve your high-value corner configuration while allowing for regular merges. 3. Anticipate how each potential move affects not just the immediate board state but your options 2-3 moves ahead. 4. Create opportunities for chain reactions where multiple merges can occur in single directional move. 5. Implement proven arrangements such as: - Decreasing value snakes that zigzag across the board. - Corner-anchored structures with decreasing values along the edges. ### Pitfalls to Avoid Certain decisions consistently lead to board deterioration: - Allowing high-value tiles to become isolated in central positions. - Creating scattered low-value tiles that impede potential combinations. - Making moves that reduce overall board fluidity and movement options. ## Current Game Context {Previous Game History} ## Board Analysis {Symbolic Board Features} ## Response Protocol **YOUR ANALYSIS MUST STRICTLY ADHERE TO THIS FORMAT:** thought: [Provide your detailed reasoning about the current board state, potential moves, and strategic implications] move: [move] Your move selection must be one of these exact terms: \"up\", \"down\", \"left\", or \"right\". Begin your response directly with \"thought:\" followed by your strategic analysis. Do not include any prefixes, headers, or additional formatting. 29 Algorithm 1: Standardizing Gaming Prompt Optimization with SIMBA from DSPY Input: Training environments Etrain, development environments Edev, target LM set Mt for performance evaluation, optimizer LM set Mo = {o3, gemini-2.5-pro, claude-3.7-think, deepseek-R1, grok3-mini}, maximum optimisation steps Output: Best prompt module (highest mean dev score over all Mt) ChainOfThought(state action) sbest , P; foreach Mo Mo do dspy.configure(lm=Mo); // joint optimisation across all target LMs SIMBA({Mt}, k); (cid:98)P O.compile(P, Etrain); // evaluate average dev score over every Mt savg 0; foreach Mt {Mt} do dspy.configure(lm=Mt); savg += Evaluate( (cid:98)P, Edev); savg savg/{Mt}; if savg > sbest then sbest savg; (cid:98)P; return 30 C.2 DSPy Optimized Prompts and Comparison Game 2048 DSPy Optimized Prompt Template system_prompt: You are an AI agent specifically designed to play the game 2048. Your primary objective is to make strategic moves that effectively merge tiles to achieve the highest possible tile value. user_prompt: ## Game Overview The game 2048 involves combining identical number tiles on grid to create tiles with progressively higher values. ## Game Mechanics - The game is played on **44 grid** - Each move (up, down, left, right) shifts all tiles in the chosen direction - When two identical tiles collide during move, they merge into single tile with twice the value - After each move, new tile (either 2 or 4) appears randomly in an empty cell - The game concludes when no legal moves remain available ## Action Space - **up**: Shifts all tiles toward the top of the grid - **down**: Shifts all tiles toward the bottom of the grid - **left**: Shifts all tiles toward the left side of the grid - **right**: Shifts all tiles toward the right side of the grid ## Strategic Principles 1. **Corner Anchoring**: Position your highest-value tile in corner and build around it 2. **Structural Stability**: Arrange surrounding tiles in descending order to create stable formation 3. **Maintaining Merge Paths**: Always keep at least one direction available for safe combinations 4. **Creating Chain Reactions**: Set up moves that trigger multiple merges in single action 5. **Forward Planning**: Think 2-3 moves ahead to avoid grid-lock and maintain empty spaces ## Pitfalls to Avoid - Allowing high-value tiles to drift into central positions - Scattering small-value tiles that obstruct potential merges - Making moves that leave the board with no follow-up merge opportunities ## Context Variables ### Previous Game History {Previous Game History} ### Board Features {Previous Game History} ## Response Format Your response must follow this exact two-line format: thought: [your brief analysis of the current board state] move: [updownleftright] **Important**: Include nothing else beyond these two lines. No additional text, prefixes, symbols, or explanations. Game 2048 DSPy Optimized Prompt Template 2 system_prompt: You are an **AI agent** playing **2048**. Your objective is to select moves that merge tiles efficiently and achieve the highest possible tile value. user_prompt: # 2048 Gaming Guide ## Primary Goal Combine like tiles to reach **2048** or higher. ## Game Mechanics - Played on **44 grid**; each move (up, down, left, right) shifts every tile. - Identical tiles that collide merge into one tile with **double the value**. - After each move, new tile (**2** or **4**) appears randomly in an empty cell. - The game ends when **no legal moves** remain. ## Action Space - **up**: Shift all tiles upward. - **down**: Shift all tiles downward. - **left**: Shift all tiles to the left. - **right**: Shift all tiles to the right. ## Key Strategies 1. **Corner anchor** - Park your highest tile in one corner and build around it. 2. **Stable structure** - Arrange surrounding tiles in descending order to protect the corner. 3. **Clear merging path** - Keep at least one direction available for safe combinations. 4. **Chain reactions** - Set up moves that trigger multiple merges in one swipe. 5. **Look ahead** - Plan 23 moves in advance to avoid grid-lock. Preserve empty spaces for new tiles. ## Avoid - Letting high-value tiles drift into the center. - Scattering small tiles that block merges. - Moves that leave the board with no follow-up merges. ## Previous Game History {Previous Game History} ## Board Features {Symbolic Board Features} **Response format (use exactly two lines):** thought: your brief analysis of the current board move: up down left right Include nothing elseno prefixes, symbols, or extra text. 32 C.3 Performance Comparison In lmgame-Bench, we follow the SIMBA optimizer implementation in DSPy with performance metrics defined in Section 2.1.3 to optimize the prompt with five optimizer models: Claude-3-7sonnet, Gemini-2.5-Pro-Preview, o3, Deepseek-r1, Grok-3-Mini-Beta, to search the best performing prompt yielding highest average reward cross all target models in 20 optimization steps (k = 20). Table 9: Model performance across various prompt types in Game 2048 with harness, where and stand for performance difference between empirically deisgn prompt pairs and DSPy optimzied prompt pairs. P1 and P2 denotes to different prompt templates. Model Empirical P1 Empirical P2 () DSPy P1 DSPy P2 () gemini-2.5-flash-preview-04-17 claude-3-5-sonnet-20241022 o4-mini-2025-04-16 1697.3548 2624.0466 4432.01096 1478.7440 2235.3862 3680.0 218.6 388.7 752.0 1746.0518 2786.0290 3851.3864.4 1601.3174 2928.0318 4320.0700 144.7 142.0 468.7 Among three target models: Gemini-2.5-Flash-Preview, Claude-3-5-Sonnet, o4-mini, experiments results from lmgame-Bench show evidence that our prompt optimization pipeline can reduce performance discrepancy between two candidate prompts by 33.8% to 63.5% on the three models across 3 runs. Details are reported in Table 9."
        },
        {
            "title": "D Additional Correlation Study",
            "content": "D.1 Benchmark List for Correlation Study We use 20 publicly available benchmarks spanning seven capability categories, including factual knowledge, physics, mathematics, coding, visual reasoning, language understanding, and puzzle solving. These benchmarks are chosen to provide comprehensive view of general-purpose model abilities and to support the correlation and decomposition analyses. Table 10 summarizes the per-model rankings across all benchmarks, grouped by category. The rankings are used to compute Spearman correlations and to uncover latent capability axes through low-rank decomposition. D.2 Correlation and Latent Feature Analysis with Super Mario Bros. To better understand the impact of high-variance games, we conducted supplementary analysis that includes Super Mario Bros. (SMB) in both the Spearman correlation matrix and latent ability decomposition. Spearman Correlation with SMB. Figure 12a shows the extended Spearman correlation matrix including SMB. Although most benchmarks retain positive mutual correlations, SMB displays notably weaker with language-heavy and code-generation benchmarks, where correlations drop to near zero or slightly negative values. This suggests that SMB performance is less stable between models and less aligned with other benchmarked capabilities, likely due to high variance or vision-specific difficulties. These observations support our decision to exclude SMB from the main correlation and decomposition analyses in Section 3.2. Latent Ability Decomposition with SMB. We also repeated the low-rank factorization analysis with SMB included. Figure 12b shows the contribution of each benchmark (including SMB) to the discovered latent features. SMB contributes moderately across most features, particularly in Feature 1 (long-context language reasoning) and Feature 3 (puzzle solving & coding capabilities). This aligns with SMBs demand for multimodal reasoningvisual perception, spatial planning, and action timing. However, its weights are less concentrated, likely due to variance in model rankings across runs. D.3 Visualizing Benchmark Relationships. To better understand how lmgame-Bench compares with established benchmarks, we visualize benchmark similarity using t-SNE. We embed each benchmark and game as high-dimensional 33 Table 10: Model rankings (1 = best) across 20 benchmarks, grouped by capability category. Abbreviated model names: C3.5 = claude-3.5-Sonnet-20241022, C3.7 = claude-3.7-Sonnet-20250219thinking, G4O = gpt-4o-2024-11-20, O1 = o1-2024-12-17, O3 = o3-2025-04-16, Gem = gemini-2.5pro-preview-05-06(thinking), L4 = llama-4-maverick-17b-128e-instruct-fp8, O4m = o4-mini. C3.5 C3.7 G4O O1 O3 Gem L4 O4m Category Benchmark Factual Physics Math Code Vision MMLU-Pro GPQA HLE HLE (Text) EMMA-Physics PHYBench Math 500 AIME 2025 EMMA-Math LiveBench-Math BigCodeBench Aider Coding LiveBench-Code VISTA MMMU Chatbot Arena (Vision) Language Puzzle MultiChallenge LiveBench-Lang EnigmaEval NYT Connections 7 7 7 7 7 7 8 8 6 7 5 6 6 7 7 5 6 6 8 4 3 4 4 4 5 4 5 4 1 4 4 4 5 5 2 4 4 5 2 2 3 3 1 1 1 1 1 3 2 5 1 1 1 3 3 5 4 6 6 6 8 8 6 6 8 6 6 8 8 7 6 6 7 7 8 8 8 8 8 6 6 7 7 6 8 4 7 6 8 8 8 8 7 6 3 5 5 5 5 4 5 4 5 4 2 5 7 5 4 4 2 3 2 1 1 1 1 3 2 2 2 3 2 7 1 3 2 2 1 1 1 1 5 4 2 2 2 3 3 3 2 8 3 1 2 3 3 6 5 2 3 (a) Spearman correlation matrix including Super Mario Bros. (SMB). (b) Top-weight benchmarks for each latent feature when SMB is included. Figure 12: Benchmark relationships overview. vector based on either model performance scores (ranging from 0100) or model rankings, and project them into 2D using t-SNE. Benchmarks with NaN values (e.g., missing model scores) were excluded to ensure reliable embeddings. We show two versions: - The first version (Figure 13b) excludes Super Mario Bros. (SMB), reflecting the setup used in the main paper. - The second version (Figure 13a) includes SMB to explore its positioning relative to other benchmarks. (a) t-SNE analysis. Without Super Mario Bros. (b) t-SNE analysis. With Super Mario Bros. Figure 13: Benchmark relationships overview. Table 11: Additional ablation on polynomial (linear & quadratic) modeling with different combinations of core capablities. Game Knowledge Linear Model (n = 5) Puzzle Visual Math Coding Offset Sokoban Tetris Ace Attorney Super Mario Bros 2048 Candy Crush 1.299 0.000 1.579 0.653 0.000 2.188 2.426 6.559 4.850 0.000 4.958 3.326 0.009 0.964 0.003 0.000 0.000 0.000 1.731 0.005 0.680 1.304 0.000 0.916 1.482 0.455 0.000 0.737 0.000 1.583 0.106 0.009 0.245 2.970 1.588 0. 0.9754 0.9370 0.8519 0.2215 0.7338 0.9086 Game Knowledge Visual Math Coding Offset Linear Model (n = 4) Sokoban Tetris Ace Attorney Super Mario Bros 2048 Candy Crush 2.581 3.432 4.493 0.942 0.004 3.646 0.011 0.405 0.014 0.715 0.000 0.000 1.954 1.063 1.582 1.102 2.249 1. 2.029 2.116 0.000 0.974 0.000 2.393 0.308 0.558 0.820 2.289 3.111 0.198 0.9471 0.8128 0.7481 0.2535 0.3610 0.8388 Game Klg Math Coding Klg2 Math2 Coding2 Klg Math Klg Coding Math Coding Offset RE Quadratic Model (n = 3) Sokoban Tetris Ace Attorney Candy Crush 1.284 1.266 1.546 1.456 0.439 0.423 0.629 0.428 0.758 0.745 0.0111 1. 0.499 0.481 1.184 0.343 0.934 0.921 0.725 1.627 0.963 0.946 0.025 1.021 0.673 0.657 0.910 0.951 0.616 0.623 0.428 0.000 0.586 0.600 0.330 0. 1.497 1.489 1.626 1.098 0.801 0.797 1.037 0.7503 In the score-based plots, Ace Attorney, Sokoban, and Tetris cluster closely with reasoning-heavy benchmarks like EnigmaEval and HLE, reflecting shared demands in long-horizon planning and symbolic reasoning. With SMB included, it appears adjacent to 2048 and ChatbotArena-Vision, consistent with its reliance on visual perception and spatial coordination. In the rank-based projections, Candy Crush and 2048 are near Math 500 and AIME 2025, while Sokoban remains isolated, likely due to its unforgiving action space. SMBs placement shifts closer to vision benchmarks, but does not cluster tightly, likely due to its high inter-model variance.. Putting together, the two tSNE graphs support the conclusion that different lmgame-Bench games probe distinct capabilities and align with well-established benchmark clusters in meaningful ways. D.4 Fitting Polynomial Models Technical Details. Let be the number of models to be ranked, be the total number of existing benchmark categories, Ri,j Z1 be an ordinal rank of model on benchmark from category j, Gi,g Z1 be the rank for model on game g. Correspondingly, we define Rj and Gg be the rank vector for category and game across all models. Since we can collect ranking data from all models across combinations of benchmarks in different categories, for each benchmark combination, we can then define polynomial feature map for polynomial expansion as shown in Eq. 1. 35 ϕ : Rn Rp(d), where p(d) = (cid:88) k=0 (cid:19) (cid:18)n + 1 , denote Φ = ϕ(L1) ... ϕ(Lm) Rmp (1) As result, let Rp be the parameters to be learned, we can predict the gaming ranking of model given its ranking on set of benchmarks from all categories of interest. ˆGi,g = wϕ (Li) , ˆGg = Φw. (cid:13)Φw g(cid:13) (cid:13) 2 With the non-negative least-square-fit objective, min 2 s.t. wk 0 for all linear terms k, (cid:13) where the closed-form solution can be expressed as = (cid:0)ΦΦ(cid:1)1 Φg. Non-negativity ensures every feature contributes additively, otherwise being worse in categoryj makes gaming performance better is counter-intuitive. each term in quantifies how much and in what direction the polynomial term of the categories drives the game ranking. We show linear and quadratic models trained on benchmarks presented in Appx. D.1. Comparisons of polynomial fitting using linear models and quadratic models of different categorical combinations are presented in Table 11. Harness Effectiveness: Quatitative Analysis In this section, we employ statistical methods to explore the effectiveness of applying all harness combined in bringing improvements to model performance. Given the tiny sample due to the cost of running latest models, results should be considered preliminary. Given that gameplay inherently involves random noise, we aim for our harnessed model performance to be both noise-resistant and consistent, enabling clearer assessment of the models true ability. Accordingly, we make two key claims: (1) harnessed evaluations better isolate model ability from game randomness; and (2) harnessed performance is more consistent and robust to random variation. E.1 Separation from Random Baseline: Glasss δ Effect Sizes To quantify how far harnessed and unharnessed model evaluations depart from random play, we simulated 30 random runs per game to estimate the baseline mean Xrand and standard deviation srand. Glasss δ for each modelgamecondition is then [88]: Xmodel Xrand srand δ = (2) Because Sokoban and Ace Attorney exhibit zero variance under random play, we exclude them, focusing on the four remaining games. Importantly, harnessed runs yield positive δ in 38 out of 40 modelgame pairs, compared to only 26 out of 40 for unharnessed runsdemonstrating that harnessed evaluations are far more consistently pulled away from the random baseline. Across those 40 pairs, harnessed runs outperform unharnessed in 29 cases (72.5%), with overall averages δharness = 3.334, δno = 0.750, = δharness δno = 2.585 (3) This demonstrates that the harness pulls model scores substantially farther from randomness than unharnessed evaluations. E.2 Direct Comparison of Harnessed vs. Unharnessed: Paired-Sample t-Test Beyond Glasss δ, we directly compare harnessed and unharnessed mean scores via paired-sample t-tests [89] across our ten models for each game. All six games exhibit positive mean improvements under harnessing; for five of themCandy Crush (+217.50 points, t(9) = 4.22, = 0.0022), Sokoban (+1.97 points, t(9) = 3.02, = 0.0144), 2048 (+17.81 points, t(9) = 2.36, = 0.0424), Ace Attorney (+3.20 points, t(9) = 2.36, = 0.0427), and Tetris (+5.60 points, t(9) = 2.27, = 0.0490)the increase is statistically significant at < 0.05. Super Mario Bros. shows smaller, non-significant gain (+289.10 points, t(9) = 1.45, = 0.1806). Figure 14 displays the full distribution of per-model score differences (Harness No Harness) for each game, with boxes indicating the interquartile range and whiskers covering 1.5 IQR. Candy 36 Table 12: Glasss δ per Model, Condition, and Game (rounded to 3 decimals) Model Cond. 2048 Candy Crush SMB Tetris claude-3-5-sonnet-20241022 claude-3-7-sonnet-20250219 (thinking) gemini-2.5-flash-preview-04-17 (thinking) gemini-2.5-pro-preview-05-06 (thinking) gpt-4.1-2025-04gpt-4o-2024-11-20 llama-4-maverick-17b-128e-instruct-fp8 o1-2024-12-17 o3-2025-04-16 o4-mini-2025-04-16 With Without With Without With Without With Without With Without With Without With Without With Without With Without With Without 0.992 5.446 1.648 1.752 0.787 0.883 2.148 2.558 0.675 0.762 0.793 3.833 0.707 7.124 3.631 3.530 3.516 3.541 2.577 0.368 0.204 1.933 7.140 0.191 4.238 0.366 5.825 1.182 1.273 0.301 0.599 1.117 0.236 1.635 0.826 0.515 10.306 0.204 7.204 0.113 1.763 2.593 2.223 2.258 2.151 2.595 2.466 1.024 4.382 3.970 4.141 1.033 2.376 0.293 0.504 2.270 8.404 3.859 2.313 2.009 2.524 1.215 3.459 1.589 3.459 4.955 7.386 1.215 1.963 1.589 2.150 2.524 0.093 0.841 13.930 1.589 17.856 11.686 8.508 2.711 Crush and Sokoban show the largest median gains, while Super Mario Bros. exhibits the greatest spread, underscoring its high inherent stochasticity. Table 13: Paired-Sample t-Test Results for Harnessed vs. Unharnessed Mean Scores Game Candy Crush Sokoban 2048 Ace Attorney Tetris Super Mario Bros. * < 0.05, ** < 0.01 Mean +217.50 +1.97 +17.81 +3.20 +5.60 +289.10 % (df = 9) +224.8% +537.5% +22.4% +123.1% +27.1% +19.3% 4.22 3.02 2.36 2.36 2.27 1.45 0.0022 ** 0.0144 * 0.0424 * 0.0427 * 0.0490 * 0.1806 E.3 Consistency of Performance: Coefficient of Variation Across Conditions Because our per-model samples are small (n 3), raw variance comparisons can be misleading. We therefore compute the coefficient of variation (expressed as percentage) CV = 100% (4) for each modelgamecondition (Random, With Harness, Without Harness) to measure relative dispersion around the mean. Sokoban and Ace Attorney are excluded (zero random variance), and models with only single run (o1, o3) are omitted. Table 14 lists the rounded CV values (in %) for the remaining ten models across four games. Across the four games, harnessed runs yield lower CV than random play in 8/8 cases for 2048 (100.0%), 6/8 for Candy Crush (75.0%), 6/8 for Super Mario Bros. (75.0%), and 5/8 for Tetris (62.5%). Comparing harnessed to unharnessed, CV is lower under harness in 6/8 for 2048 (75.0%), 8/8 for Candy Crush (100.0%), 4/8 for Super Mario Bros. (50.0%), and 4/8 for Tetris (50.0%). Overall, out of 32 valid modelgame pairs, 25 (78.1%) have smaller CV under harness versus random, and 22 (68.8%) have smaller CV under harness versus unharnessed. These results indicate that the 37 Figure 14: Distribution of paired score improvements (Harness No Harness) across ten models for each game. harness not only elevates mean performance but also lowers the coefficient of variationi.e. reduces relative score dispersionwhich yields more stable, reliable assessments of model ability. Table 14: Coefficient of Variation (CV %) by Model, Condition, and Game Model Random claude-3-5-sonnet-20241022 claude-3-7-sonnet-20250219 (thinking) Condition Random With Harness Without Harness With Harness Without Harness gemini-2.5-flash-preview-04-17 (thinking) With Harness gemini-2.5-pro-preview-05-06 (thinking) gpt-4.1-2025-04-14 gpt-4o-2024-11-20 llama-4-maverick-17b-128e-instruct-fp o4-mini-2025-04-16 Without Harness With Harness Without Harness With Harness Without Harness With Harness Without Harness With Harness Without Harness With Harness Without Harness 2048 Candy Crush Super Mario Bros Tetris 7.798 5.351 28. 2.766 6.285 4.962 3.172 4.993 3.245 6.605 17.945 3.266 21.545 3.556 26. 4.030 29.880 44.183 50.372 106.371 11.090 54.694 19.581 36.978 1.635 36. 15.763 119.047 36.249 92.509 44.459 128.055 40.634 44.866 41.995 17. 38.184 1.409 46.546 11.341 17.210 17.034 13.573 43.228 83.636 51.144 25.797 63. 37.835 58.857 11.116 13.210 7.873 20.405 14.139 0.000 19.681 24.119 2.474 24. 4.225 13.323 25.754 14.193 14.783 9.897 33.572 24.037 Harness Effectiveness: Qualitative Analysis In this section, we present case studies that demonstrate the reasoning traces of the latest models during their 2048 gameplay. F.1 Memory Module Memory Module Reflection Comparison: GPT-4o vs. o3 In our 2048 case study, GPT-4os reflections tend to focus on immediate effect of each move (\"up increased empty spaces but produced no merges\"), whereas o3s reflections unpack exactly which 38 tiles merged, how merge ladders are forming, and even quantify changes in empty-cell counts(\"two 2s became 4 and two 16s became 32s, opening an extra cell\"). Importantly, an interactive game environment isnt one-off Q&A but multi-turn dialogue with constantly shifting game board - each action affects next decision. o3 better links its fine-grained observations to concrete next steps, integrating short-term move outcomes into its long-term corner-dominance strategy. The success of local decisions is crucial to achieving long-term goals in multi-turn interactive game environment. Highlight Legend: Red text: the models immediate game-state reflection Yellow text: o3s immediate game-state reflection Green text: the models future move suggestion 39 GPT-4o Game 2048 Memory Module Reflections Sample Reflection: The move \"up\" increased empty spaces from 2 to 4, improving board flexibility while maintaining the highest tile (256) in position. However, no significant merges occurred, suggesting the move was minimally effective for progress. The key pattern is the consistent maintenance of the highest tile in corner, aligning with the strategy. Future moves should focus on creating merge opportunities, particularly around the 256 tile, while avoiding disruptions to its placement. Consider prioritizing \"down\" or \"left\" next to consolidate lower tiles toward the main corner, optimizing for larger merges. Sample 2 Reflection: The \"left\" action was suboptimal as it failed to create merges or improve board position, leaving only two empty spaces while slightly disrupting alignment. The highest tile (256) remains safe in corner, but the addition of 2 at (0,3) weakens the merging path potential. Merges were not prioritized, and the action did not capitalize on available opportunities to consolidate tiles. Future moves should focus on maximizing merges and preserving space, particularly by aligning higher tiles on one edge to maintain control and avoid corner disruption. Aim to optimize empty space usage for better tile positioning. Sample 3 Reflection: The \"down\" move consolidated smaller tiles, creating merges (e.g., 2s to 4s), but did not impact the highest tile (256) or significantly improve positioning. Empty spaces shifted, but the same two remain, limiting opportunities. The action maintained balance but lacked strategic progression toward higher merges. The key issue is the scattered mid-tier tiles, which could restrict future merges. Focus on better clustering tiles toward one corner and maintain merge paths for the 256 tile. Prioritizing \"up\" or \"left\" next may help consolidate the board and preserve the highest tiles strategic position. o3 Game 2048 Memory Module Reflections Sample 1 Reflection: The right swipe merged the two 8-tiles in row 2 (3 4), created 16 and opened an extra gap; the new 2 spawned harmlessly at (1,0). 256 stayed anchored in the bottom-right, and the right column is now 256-128-64-16, strong merge ladder. Empty count stayed at four, so the board is still breathable.Good move: it scored, preserved the corner strategy and improved board order.Watch for: isolated 2s creeping into the left half; they can freeze rows if ignored. Next: keep sliding Right/Down to compress toward the bottom-right; avoid Up unless desperate to prevent dislodging the 256. Sample 2 Reflection: Up combined the two 2-tiles in column 2 into an 8 and merged the pair of 2-tiles in column 1 into 4, netting +2 empty squares; after the new spawn you still gained one extra space (4 5). 256 stayed anchored in the bottom-right, so the corner strategy is intact. Good move: it advanced tile values, improved board looseness and preserved ordering. Watch the freshly spawned 2 at (3,2); if the bottom row clogs youll lose the 256 corner. Prefer more up/left sequences to clear column 2 and funnel new merges toward the right column. Sample Reflection: Down kept the 256 anchored and slid every column smoothly, but it yielded zero merges and cost one empty slot (5 4). Column 0 merely shuffled 8-2 downward and spawned new 2 on top, so no pairing potential was created. Repeated space-losing moves like this can clog the board; always look for turns that both respect the corner strategy and create or set up merges (e.g., left could have joined the two 2-tiles on the bottom row next). Prioritize moves that grow space or prepare obvious combines before defaulting to safe corner-holding slides. 41 Memory Module Prompt Template Game 2048 Memory Module system_prompt: You are an analytical assistant for 2048 AI agent. Your task is to generate brief, insightful reflection on the game state changes and the effectiveness of recent actions.Focus on strategic insights and patterns that would help the agent make better decisions. Keep your reflections short, precise, and actionable. user_prompt: Please analyze the following 2048 game states and actions to generate brief reflection: Previous Game States and Actions: {Previous Game History} Focus your reflection on: 1. How the game state changed after the last action 2. Whether the action was effective for the situation 3. Patterns or issues to be aware of 4. Any strategic insights for future actions Keep your reflection under 100 words and focus only on the most important insights."
        },
        {
            "title": "G Details of Evaluation Metrics",
            "content": "Because games are well-designed, we utilize their built-in metrics to quantify models proficiency. For each game, we choose the single score that most faithfully reflects models capability, then transform and normalize it onto continuous, linear scale. This curation ensures that our evaluation can sensitively capture performance differences and supports consistent statistical analysis. Sokoban: Total number of boxes pushed onto targets, summed over all levels, until the first deadlock. Super Mario Bros.: Cumulative horizontal distance traveled by Mario (in game units) across all levels, until all three lives are lost or the final level is completed. Tetris: Total pieces dropped plus total lines cleared, measured up to the point of game over. 2048: Sum of all merged tile values (e.g. merging two 2s yields +2), recorded until the board stagnates (no merges or moves that change the board for ten consecutive turns). We then report Score2048 = 10 log2 (cid:0)total merged sum(cid:1). Candy Crush: Total number of candies eliminated over fixed 50move session. Ace Attorney: Total count of correct actions (evidence submissions, dialogue choices, etc.) across all case levels, measured until five incorrect decisions (lives) have been used."
        }
    ],
    "affiliations": [
        "MBZUAI",
        "UC Berkeley",
        "UC San Diego"
    ]
}