{
    "paper_title": "Training-free Diffusion Acceleration with Bottleneck Sampling",
    "authors": [
        "Ye Tian",
        "Xin Xia",
        "Yuxi Ren",
        "Shanchuan Lin",
        "Xing Wang",
        "Xuefeng Xiao",
        "Yunhai Tong",
        "Ling Yang",
        "Bin Cui"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have demonstrated remarkable capabilities in visual content generation but remain challenging to deploy due to their high computational cost during inference. This computational burden primarily arises from the quadratic complexity of self-attention with respect to image or video resolution. While existing acceleration methods often compromise output quality or necessitate costly retraining, we observe that most diffusion models are pre-trained at lower resolutions, presenting an opportunity to exploit these low-resolution priors for more efficient inference without degrading performance. In this work, we introduce Bottleneck Sampling, a training-free framework that leverages low-resolution priors to reduce computational overhead while preserving output fidelity. Bottleneck Sampling follows a high-low-high denoising workflow: it performs high-resolution denoising in the initial and final stages while operating at lower resolutions in intermediate steps. To mitigate aliasing and blurring artifacts, we further refine the resolution transition points and adaptively shift the denoising timesteps at each stage. We evaluate Bottleneck Sampling on both image and video generation tasks, where extensive experiments demonstrate that it accelerates inference by up to 3$\\times$ for image generation and 2.5$\\times$ for video generation, all while maintaining output quality comparable to the standard full-resolution sampling process across multiple evaluation metrics. Code is available at: https://github.com/tyfeld/Bottleneck-Sampling"
        },
        {
            "title": "Start",
            "content": "Training-free Diffusion Acceleration with Bottleneck Sampling Ye Tian1 Xin Xia2* Yuxi Ren2 Shanchuan Lin 2 Xing Wang2 Xuefeng Xiao2 Yunhai Tong1 Ling Yang1 Bin Cui1 Project: Bottleneck-Sampling-Page Code: Bottleneck-Sampling-Code 1Peking University 2Bytedance 5 2 0 2 4 ] . [ 1 0 4 9 8 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Diffusion models have demonstrated remarkable capabilities in visual content generation but remain challenging to deploy due to their high computational cost during inference. This computational burden primarily arises from the quadratic complexity of self-attention with respect to image or video resolution. While existing acceleration methods often compromise output quality or necessitate costly retraining, we observe that most diffusion models are pretrained at lower resolutions, presenting an opportunity to exploit these low-resolution priors for more efficient inference without degrading performance. In this work, we introduce Bottleneck Sampling, training-free framework that leverages low-resolution priors to reduce computational overhead while preserving output fidelity. Bottleneck Sampling follows high-low-high denoising workflow: it performs high-resolution denoising in the initial and final stages while operating at lower resolutions in intermediate steps. To mitigate aliasing and blurring artifacts, we further refine the resolution transition points and adaptively shift the denoising timesteps at each stage. We evaluate Bottleneck Sampling on both image and video generation tasks, where extensive experiments demonstrate that it accelerates inference by up to 3 for image generation and 2.5 for video generation, all while maintaining output quality comparable to the standard full-resolution sampling process across multiple evaluation metrics. 1. Introduction Denoising Diffusion models have emerged as cornerstone of modern generative modeling, achieving state-of-the-art performance in tasks such as text-to-image synthesis and video generation [9, 10, 28, 29, 31, 32, 40, 44]. Among the recent developments, Diffusion Transformers (DiTs) [24] *Equal Contribution. Project Leader. Corresponding Authors. Figure 1. Comparison of sampling strategies in our framework. (i) Standard Sampling. (ii) Our Bottleneck Sampling: high-lowhigh workflow that captures semantics early, improves efficiency in the middle, and restores details at the end. Images generated by FLUX.1-dev using the prompt: Design stylish dancers back logo with the letters and Y. have gained significant attention due to their superior scalability, establishing them as the dominant approach in both image [2, 6, 17] and video generation [16, 35, 42]. Despite their impressive performance, the practical deployment of DiTs is hindered by their considerable computational cost, particularly during inference. The major bottleneck in the efficiency of dit is the attentions quadratic complexity of O(L2) with respect to the input token length that are determind by the image/video resolution. As the resolution of the generated images or videos increases, this quadratic scaling leads to prohibitive computational costs. For instance, generating 2K image [3] that is tokenized into 16k tokens results in several seconds of attention computation even on high-end Nvidia A100 GPU. This problem becomes particularly severe when generating high-resolution content, where increases significantly, causing inference to be both time-consuming and resource-intensive. Existing efforts to address this issue focus on optimizing 1 Figure 2. Main Results of our Bottleneck Sampling on both text-to-image generation and text-to-video generation. Bottleneck Sampling maintains comparable performance with 2.5 - 3 acceleration ratio in training-free manner. attention computation within the high-resolution regime. Techniques such as attention optimization [8, 43] and feature reuse [22, 23] aim to reduce computational redundancy by discarding less important tokens or reusing intermediate activations. While these methods achieve some speedup, they inevitably introduce trade-offs in generation quality, as aggressive sparsification or reuse can degrade fine-grained details. Moreover, such approaches often require extensive hyperparameter tuning to balance efficiency and performance, making them less practical for widespread adoption. Crucially, these approaches overlook fundamental property of corrent SOTA diffusion models: their pretraining at lower resolutions [26, 42] (e.g., 256256), where sequence lengths are orders of magnitude smaller, and attention computations remain tractable. This observation raises pivotal question: Can we leverage low-resolution pretrained priors to accelerate high-resolution inference without sacrificing fidelity? We hypothesize that strategically incorporating low-resolution computations during inference can substantially reduce computational overhead while preserving high-resolution output fidelity. Previous studies have explored similar concept in cascaded diffusion models [11, 14], where low-resolution image undergoes denoising before being upsampled for super-resolution generation. However, when directly applied to inference, these methods often result in fine detail loss or text degradation due to the absence of high-resolution priors. To address these limitations, we introduce Bottleneck Sampling, novel, training-free framework that redefines diffusion inference through hierarchical resolution scheduling. Our key insight is that early denoising stages primarily establish global structurea process efficiently handled at lower resolutionswhile later stages refine high-frequency details at full resolution. To achieve this, we propose bottleneck-inspired sampling strategy that first compresses information, processes it efficiently at reduced resolutions, and then progressively restores full resolution to recover fine details. Specifically, Bottleneck Sampling begins with high-resolution noise initialization, transitions to lower resolutions for efficient global denoising, and gradually reinstates resolution for detail refinement. To mitigate aliasing and blurring artifacts, we further optimize resolution changing points and adjust denoising timesteps at each stage. We apply Bottleneck Sampling to both text-to-image and text-to-video models, demonstrating its versatility across different generative tasks. Extensive evaluations across diverse metrics show that Bottleneck Sampling achieves up to 3 speedup on FLUX.1-dev[17] for image generation and 2 on hunyuanvideo[16] for video generation, all while maintaining comparable output quality. Notably, our method requires no architectural modifications or retraining, making it plug-and-play acceleration strategy for existing diffusion frameworks. By effectively bridging the efficiency-quality trade-off in high-resolution generation, Bottleneck Sampling enhances the practical deployment of diffusion models in resource-constrained environments. 2. Related Work Diffusion Models Diffusion models have established themselves as dominant paradigm in generative modeling, surpassing the performance of traditional generative adversarial networks. Early implementations, such as those by Ho et al. [9] and Rombach et al. [29], utilized U-Net architectures for iterative denoising. To address scalability limitations, recent advancements have transitioned to transformer-based architectures. Notably, the Diffusion Transformer (DiT) [2, 24] replaces U-Net with transformer backbone, achieving superior scalability in high-resolution image and video generation tasks. State-of2 the-art text-to-image generation models, such as SD3 [6] and Flux [17], as well as video generation models like CogVideo [42] and HunyuanVideo [16], share critical design principle: initial pretraining at low resolutions (e.g., 256 256) followed by resolution-specific fine-tuning. While effective, these approaches inherit the quadratic computational complexity of self-attention mechanisms, which becomes prohibitive at high resolutions. In this work, we for the first time explore novel paradigm that leverages low-resolution pretrained priors to enable efficient highresolution generation. dancy. Agent Attention [8] condenses information using proxy tokens, lowering the cost of full self-attention while maintaining critical dependencies. Similarly, DiTFastAttn [43] employs modified window attention mechanism to focus computation on essential queries and keys. While these approaches achieve notable speedups, they inherently introduce trade-offs. Feature reuse disrupts alignment between training and inference, potentially affecting generation quality, whereas attention optimization, despite reducing computational overhead, may lead to information loss due to selective attention reduction. Image Pyramid The concept of image pyramids has long been fundamental tool in computer vision, facilitating multi-scale analysis, efficient visual data processing, and various visual understanding tasks [1, 19, 37]. More recently, image pyramids have been explored in generative models, particularly in cascaded diffusion frameworks, where generation is first performed at low resolution and subsequently refined via super-resolution [11, 25, 30, 34]. More recent studies have investigated the use of pyramid sampling to enhance generation efficiency. Methods such as Efficient Diffusion Transformers [4] and Pyramid Flow Matching [14] have demonstrated the potential of coarseto-fine generation pipelines in both image and video synthesis by restructuring network architectures and training paradigms. While these methods achieve promising results in terms of both efficiency and quality, they require extensive computational resources for retraining, making their adoption computationally expensive. In contrast, our method leverages low-resolution pretrained priors to propose novel Bottleneck Sampling framework. Unlike existing approaches, Bottleneck Sampling requires no additional training, achieving comparable performance with minimal computational overhead. Training-Free Diffusion Acceleration To improve the efficiency of diffusion models, numerous training-free acceleration techniques have been proposed, primarily focusing on feature reuse and attention optimization. Feature reuse methods leverage caching to eliminate redundant computations without requiring retraining. For instance, DeepCache [23] accelerates Stable Diffusion [29] by reusing intermediate U-Net features, while Faster Diffusion [18] caches encoder features across timesteps to skip redundant computations. Learning-to-Cache [22] further introduces an adaptive caching policy, and ToCa [47] extends this idea by caching token-wise attention features in DiT for state-of-the-art acceleration. In video generation, Pyramid Attention Broadcast [45] enables multidimensional attention reuse, significantly improving multiframe synthesis efficiency. Beyond caching, attention optimization methods directly reduce computational redun3. Method 3.1. Preliminaries Modern state-of-the-art generative models, such as Flux [17] and HunyuanVideo [16], have widely adopted Flow Matching [6, 20, 21, 36, 41] as preferred alternative to traditional Denoising Diffusion Probabilistic Models (DDPM) due to its superior training efficiency and faster convergence. Flow Matching reformulates generative modeling as learning continuous transformation between probability distributions [36, 41], mapping complex data distributions to simple prior (e.g., Gaussian) through learned velocity field, and enabling sample generation via inverse integration. During training, given data sample X1 and Gaussian noise X0 (0, I), we define linear interpolation trajectory: Xt = (1 t)X0 + tX1, [0, 1], (1) where Xt represents the interpolated state at timestep t. The ground-truth velocity field is given by: Vt = dXt dt = X1 X0, (2) capturing the instantaneous transformation from noise to data. The model uθ(Xt, y, t), conditioned on optional inputs (e.g., text prompts), is optimized to minimize the velocity alignment loss: = Et,X0,X1,y (cid:2)uθ(Xt, y, t) Vt2(cid:3) , (3) where is sampled uniformly. This objective ensures the model learns direct transport path, improving efficiency and sample quality. At inference, starting from random noise X0 (0, I), we reconstruct X1 via ODE-based integration over discretized timesteps {ti}n i=0: Figure 3. Overall pipeline of our Bottleneck Sampling. The process consists of three stages: (i) High-Resolution Denoising to preserve semantic information, (ii) Low-Resolution Denoising to improve efficiency, and (iii) High-Resolution Denoising to restore fine details. Images generated by FLUX.1-dev using the prompt: 2D cartoon,Diagonal composition, Medium close-up, whole body of classical doll being held by hand, the doll of young boy with white hair dressed in purple, He has pale skin and white eyes.. X1 = X0 + n1 (cid:88) i=0 uθ(Xti, y, ti) (ti+1 ti), (4) where t0 = 0 and tn = 1. The learned velocity field uθ iteratively refines Xt, enabling high-quality synthesis with minimal sampling steps. 3.2. Bottleneck Sampling In this section, we detail the proposed Bottleneck Sampling pipeline. Taking image generation as an example, consider generating an image with height and width w, during the generation phase, we start from an initial noise sample x0 Rbchw, where is the batch size and is the number of channels. Our objective is to generate predicted output x1 by progressively refining this initial noise. To achieve efficient inference while preserving quality, we design low-high-low sampling pipeline. Specifically, during the intermediate denoising steps, we perform inference at progressively lower resolutions to reduce computational overhead. In the later stages, we gradually restore the resolution, refining the details at full scale. Specifically, let there be stages, characterized by hierarchical resolution schedule satisfying h1 > h2 > > hK1 < hK, with corresponding widths w1 > w2 > > wK1 < wK. We predefine the number of inference steps in each stage as N1, N2, . . . , NK, where the total inference steps sum to = (cid:80)K i=1 Ti. At each stage i, we construct discretized timesteps {ti,j}K = 0, 1, . . . , Ni., the inference process follows: i=0, Xti,j = Xti,j1 +uθ(Xti,j1, y, ti,j1)(ti,j ti,j1) (5) where {0, 1, . . . , Ni}. Resolution Change Point After establishing the overall workflow, the next critical aspect is the resolution change. Conventional methods [14] adjust the mean and variance of noise at boundaries to ensure smooth transitions and reduce error propagation. However, we find more effective approach: rather than directly connecting resolution stages, we reintroduce noise to intermediate latents, enabling fresh high-to-low noise denoising process at the new resolution. This strategy offers two advantages. First, it aligns inference with training distribution, avoiding mismatches caused by direct latent connections. Second, it leverages the models multi-resolution priors. Since diffusion models learn to denoise latents based on given conditions, adding noise at transitions enables natural refinement at the new resolution, ensuring more coherent and efficient generation process. Specifically, at each stage transition, we first apply standard upsampling or downsampling operation to adjust the resolution. The transition is defined as: (cid:40) Xti,0 = Up(Xti1,Ni1 Down(Xti1,Ni , hi, wi), , hi, wi), if hi > hi1 if hi < hi1 (6) where Xti1,Ni1 represents the final latent at stage 1, which is then upsampled or downsampled to match the resolution of stage i. Following resolution adjustment, we reintroduce noise through Flow Matchings noise addition mechanism and subsequently perform denoising for ti steps. The noise injection strength at each stage is parameterized by {wi}n i=0, representing distinct noise levels across different stages. The target timestep for noise injection and the noise addition are mathematically defined as: 4 τi = ti,Ni(1wi) Xti,0 = (1 τi)Xti,0 + τiη, η (0, I) (7) (8) This formulation can be interpreted as applying noise to the latent representations from the preceding stage with controlled intensity of wi. As result, the subsequent stage requires only Niwi steps rather than the complete Ni steps. Our experimental results demonstrate that this architectural design steadily enhances the stability and efficiency of both image and video generation processes. Tailored Scheduler Re-Shifting Following the handling of stage changing, crucial subsequent step involves adapting the denoising scheduler for the next stage to accommodate potential variations in the signal-to-noise ratio (SNR). In contrast to DDPM [9], which employs linear scheduling strategy throughout the inference steps, flow matching typically adopts shifting formulation [6, 16, 17], as illustrated in Fig. 4. This approach emphasizes denoising in high-noise regions, aligning with prior research demonstrating that concentrating the denoising process in low-SNR regions yields superior outcomes. Within our framework, reswhere ti,n is the original timestep and ti,m is the modified timestep actually used in Eq. (5) with scale factor si. This re-shifting strategy ensures that denoising aligns with the shifting SNR across resolutions, leading to improved generative performance in both image and video generation. 3.3. Overall Algorithm of Bottleneck Sampling Algorithm 1 Bottleneck Sampling Require: Noise sample x0 Rbchw, sampling stages K, resolution list (h1, w1), (h2, w2), . . . , (hK, wK), number of inference steps {Ni}K i=0, noise injection strengths {wi}K i=0, shifting factors {si}K i=0 Ensure: Generated output X1 = XK,NK 1: InitializeXt0,0 x0 2: for = 1, . . . , do 3: 4: 5: if > 1 then using Eq. (6) // Stage Change Points: Upsample or Downsample Xti1,Ni to Xti,0 6: 7: 8: 9: // Tailoerd Scheduler Shifting: Update flow matching scheduler with shift factor si using Eq. (9) end if Crafting timesteps {ti,j}Ni j=0 using tailored scheduler after re-shifting. Adjust inference steps: Ni (1 wi) Ni for = 1, . . . , Ni do Compute velocity field uθ(Xti,j , y, ti,j) Perform denoising step using Eq. (5) 10: 11: 12: 13: 14: 15: end for 16: return X1 XK,NK end for Figure 4. Timestep Shifting Visualization at different shifting factors settings. Higher shifting scales lead to denoising in highernoise regions olution adjustments not only modify spatial characteristics but also influence the SNR properties of each latent region, effectively diminishing the retained signal from the preceding stage. Given the reintroduction of noise during stage transitions, each latent representation inherently reverts to low-SNR regime. To mitigate this, we incorporate an additional scheduler shift at stage transitions, facilitating more stable denoising. Let the shift factors be defined as {s}n At each stage, the scheduling process is then adjusted as: i=0 ti,m = si ti,n 1 + (si 1)ti,n . (9) 5 4. Experiments 4.1. Settings Baselines We evaluate Bottleneck Sampling on two widely used diffusion transformers: FLUX.1-dev[17] for image generation and HunyuanVideo[16] for video generation. Both models, based on MM-DiT[6] with flow matching scheduler, enable unified implementation of our approach. We primarily compare against ToCa[47], stateof-the-art training-free acceleration method that employs token-wise caching for DiTs. Additionally, we include varients of baseline that directly reduce inference steps. Metrics For both text-to-image and text-to-video tasks, we use CLIP Score[27] to assess the alignment between generated content and textual prompts. For image generation, we further employ ImageReward[39] for human preference evaluation, along with compositional metrics GenEval[7] and T2I-Compbench[12]. For video generation, we adopt VBench[13] to evaluate overall quality and T2V-Compbench[33] for compositional capabilities. To account for the subjectivity in evaluation, we also conduct an extensive user study detailed in Appendix C. Stage Configurations We explore different stage configurations for image and video generation and report the optimal settings in our evaluations. For image generation with FLUX.1-dev, we adopt three-stage pipeline with resolutions 1024 512 1024. For video generation, we use the same sampling strategy while adjusting the resolution to fit the constraints of HunyuanVideo, adopting 1240p 738p 1240p configuration. Lanczos resampling [5] is used for both upsampling and downsampling. Detailed hyperparameters and ablation studies are provided in Appendix A.3 and Appendix D. 4.2. Results on Image Generation We present the performance of Bottleneck Sampling in Fig. 5 and Sec. 4.2. We compare it against the FLUX.1-dev baseline, which employs the FlowMatch scheduler with 50 inference steps, as well as accelerated FLUX variants that utilizes 33% and 50% of the original steps. Additionally, we include ToCa with 50 steps as representative training-free acceleration method for further comparison. Qualitative Results To evaluate robustness, we test each approach on challenging prompts that demand high fidelity and creativity, including text rendering, creative composition, and artistic generation. As shown in Fig. 5, Bottleneck Sampling consistently outperforms other methods in text rendering, accurately generating all letters without cherry-picking. In contrast, the baseline model occasionally misaligns characters, while cache-based methods like ToCa fail to produce coherent typography. We attribute this to partial attention map reuse, which may reduce effective computation needed for precise letter formation. For creative and artistic prompts, Bottleneck Sampling maintains stronger semantic consistency. For instance, when generating descriptions like giraffe constructed entirely from the Eiffel Tower or an elephant facing away from the viewer, our method preserves coherence across resolution stages, reinforcing both structural integrity and textual fidelity. Moreover, it achieves threefold acceleration without quality degradation, maintaining critical details such as typography and semantic accuracy. These results demonstrate Bottleneck Samplings effectiveness in enhancing efficiency while preserving high-quality generation. Quantitative Results As shown in Sec. 4.2, we compare the latency, FLOPs, and acceleration ratios of different models in the DiT computation process, along with their corresponding performance. To ensure fair comparison, we exclude the computational cost of the text encoder and VAE, as these components remain identical across all models. Across various evaluation metrics, Bottleneck Sampling consistently achieves superior performance at baseline 2 acceleration, even outperforming ToCas 1.5 acceleration. Remarkably, our method maintains performance comparable to the baseline even at 3 acceleration. In more challenging benchmarks like T2I-CompBench, Bottleneck Sampling demonstrates clear advantage, reinforcing findings from our controlled experiments. 4.3. Results on Video Generation We present the results of our video generation experiments in Fig. 6 and Sec. 4.2. The baseline model is HunyuanVideo [16] with 50 inference steps. Due to the lack of opensource implementations based on the HunyuanVideo architecture, we compare our method against an equivalent acceleration variant of the original model with reduced number of inference steps. ToCas reported results are based on Open-Sora [46], an outdated and weaker baseline that lacks modern generative architectures such as MM-DiT and flow matching. To ensure fair comparison, we exclude ToCa from our evaluation. In contrast, As shown in Fig. 6, Bottleneck Sampling effectively preserves motion details, background consistency, semantic coherence, and composition fidelity, maintaining quality level comparable to the baseline. the reduced-step baseline introduces noticeable artifacts and inconsistencies, often struggling with semantic misalignment (e.g., abrupt motion stoppages, such as cat suddenly halting) and visual defects (e.g., object disappearance or incorrect motion trajectories, like duck vanishing midframe). Additional qualitative results are provided in Appendix F. Furthermore, as reported in Sec. 4.2, Bottleneck Sampling achieves performance on par with the baseline across multiple evaluation metrics, including VBench, T2VCompBench, and CLIP Score, while attaining up to 2.5 acceleration. These results further highlight the efficiency of our approach in accelerating video generation without compromising output quality. 4.4. Ablation Study In this section, we investigate the key design choices in Bottleneck Sampling through ablation studies, including the overall bottleneck design, noise reintroducing at stage transitions, and scheduler re-shifting. We report quantitative results for image generation in Sec. 4.4 and provide qualitative evaluations in Fig. 7. Additional ablation studies, including upsampling methods and stage configurations, are provided in Appendix D. 6 Figure 5. Qualitative comparison of our Bottleneck Sampling with FLUX.1-dev. Our method achieves up tp 3 speedup while maintaining or improving visual fidelity. Incorrect text rendering and anatomical inconsistencies are highlighted with different colors. Full prompts are provided in Appendix E. Method Latency(s) FLOPs(T) Speed CLIP Score Image Reward [39] Gen Eval [39] Average on T2ICompbench FLUX.1-dev [17] 33% steps 50% steps ToCa [47] Bottleneck Sampling ( 2) Bottleneck Sampling ( 3) 33.85 11.28 16.93 19.88 17.37 14.46 3719.50 1239.83 1859.75 2458.06 1870.10 1234. 1.00 3 2 1.51 2 3 0.460 0.432 0.453 0.447 0.460 0. 1.258 1.048 1.239 1.169 1.257 1.254 0.6807 0.6423 0.6698 0.6630 0.6762 0. 0.7032 0.6140 0.6808 0.6738 0.6820 0.6946 Table 1. Quantitative Results on Text-to-Image Generation Effect of Noise Reintroducing We examine the impact of noise reintroducing at stage change points. In standard cascaded diffusion setup, noise remains continuous without intermediate reintroduction. However, results in Fig. 7 indicate that this leads to persistent ghosting artifacts and overlapping pixels in high-resolution inference, severely degrading quality. By contrast, our noise reintroduction strategy alleviates these issues, enabling cleaner and more coherent outputs. Effect of Scheduler Re-Shifting To assess the impact of scheduler re-shifting, we conduct an ablation study where no re-shifting is applied and fixed scheduling is applied. As shown in Fig. 7, this results in detail loss and increased ghosting, consistent with our prior observations. These artifacts arise from signal-to-noise ratio variations across stages, where static schedule leads to suboptimal denoising. In contrast, our re-shifting mechanism mitigates these issues, ensuring more stable and high-quality generation. 7 Method Latency(s) FLOPs(T) Speed CLIP Score Vbench T2V-Compbench HunyuanVideo [16] 40% steps 50% steps Bottleneck Sampling ( 2) Bottleneck Sampling ( 2.5) Bottleneck Sampling ( 3) 1896 758.4 948.0 1321.4 834.7 743.2 605459.49 1.00 242183.80 302729.74 311319.54 232756.28 203192.35 2.5 2 2 2.5 3 0.455 0.427 0. 0.446 0.443 0.421 83.24 82.93 83.14 83.18 83.19 81.98 0.5832 0.5530 0. 0.5739 0.5737 0.5626 Table 2. Quantitative Results on Text-to-Video Generation Figure 6. Qualitative comparison of our Bottleneck Sampling with HunyuanVideo [16].Our method achieves up to 2.5 speedup while maintaining visual fidelity. Incorrect object motion and object disappearing are highlighted with red circles. Effect of Bottleneck Design Another common approach to leveraging low-resolution priors is the cascaded diffusion framework [11, 14], where denoising is first performed at lower resolution before upsampling and refinement. We conduct comparative experiments, ensuring that Bottleneck Sampling and cascaded-like sampling operate under the same acceleration ratio, as shown in Fig. 7. The results demonstrate that our design, which introduces an initial high-resolution stage, consistently improves semantic coherence, particularly in text rendering and fine-detail preservation and achieves better generation quality. Method CLIP Score ImageReward Baseline Ours w/o Scheduler Re-Shifting Ours w/o Noise Reintroducing Ours w/o Bottleneck Design Ours 3 0.460 0.276 0.379 0.448 0.460 1.258 0.781 0.923 1.134 1.257 Table 3. Ablation Study Results on Image Generation. 5. Conclusion Motivated by the observation that diffusion models are pre-trained on diverse image and video resolutions, we introduce Bottleneck Sampling, training-free method that accelerates inference by strategically leveraging lowresolution priors. Following high-low-high workflow, Bottleneck Sampling reduces computation by primarily performing denoising at lower resolutions in intermediate steps while refining details at high resolutions at the start and end. Our method achieves 3 speedup for image generation and Figure 7. Visualization of Ablation Studies. 8 2.5 for video generation, surpassing previous training-free acceleration approaches while maintaining output generation quality. These findings provide insights into future training-free acceleration in diffusion models."
        },
        {
            "title": "References",
            "content": "[1] Edward Adelson, Charles Anderson, James Bergen, Peter Burt, and Joan Ogden. Pyramid methods in image processing. RCA engineer, 29(6):3341, 1984. 3 [2] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 1, 2 [3] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of In diffusion transformer for 4k text-to-image generation. European Conference on Computer Vision, pages 7491. Springer, 2024. 1 [4] Xinwang Chen, Ning Liu, Yichen Zhu, Feifei Feng, and Jian Tang. Edt: An efficient diffusion transformer framework inspired by human-like sketching. Advances in Neural Information Processing Systems, 37:134075134106, 2025. 3 [5] Claude Duchon. Lanczos filtering in one and two dimensions. Journal of Applied Meteorology (1962-1982), pages 10161022, 1979. 6, 4 [6] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 1, 3, 5 [7] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. 6, [8] Dongchen Han, Tianzhu Ye, Yizeng Han, Zhuofan Xia, Siyuan Pan, Pengfei Wan, Shiji Song, and Gao Huang. Agent attention: On the integration of softmax and linear attention. In European Conference on Computer Vision, pages 124 140. Springer, 2024. 2, 3 [9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1, 2, 5 [10] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 1 [11] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):133, 2022. 2, 3, 8 [12] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and T2i-compbench: comprehensive benchXihui Liu. mark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. 6, [13] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 6, 1 [14] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video arXiv preprint arXiv:2410.05954, generative modeling. 2024. 2, 3, 4, 8 [15] Robert Keys. Cubic convolution interpolation for digital image processing. IEEE transactions on acoustics, speech, and signal processing, 29(6):11531160, 1981. 4 [16] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models, 2025. 1, 2, 3, 5, 6, 8 [17] Black Forest Lab. Flux. https://blackforestlabs. ai/,, 2024. 1, 2, 3, 5, 7 [18] Senmao Li, Taihang Hu, Fahad Shahbaz Khan, Linxuan Li, Shiqi Yang, Yaxing Wang, Ming-Ming Cheng, and Jian Yang. Faster diffusion: Rethinking the role of unet encoder in diffusion models. CoRR, 2023. [19] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Feature pyraBharath Hariharan, and Serge Belongie. In Proceedings of the mid networks for object detection. IEEE conference on computer vision and pattern recognition, pages 21172125, 2017. 3 [20] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [21] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3 [22] Xinyin Ma, Gongfan Fang, Michael Bi Mi, and Xinchao Wang. Learning-to-cache: Accelerating diffusion transformer via layer caching. arXiv preprint arXiv:2406.01733, 2024. 2, 3 [23] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: In Proceedings of Accelerating diffusion models for free. the IEEE/CVF conference on computer vision and pattern recognition, pages 1576215772, 2024. 2, 3 [24] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF inter- [37] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. IEEE transactions on pattern analysis and machine intelligence, 43(10):3349 3364, 2020. 3 [38] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 1 [39] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. Advances in Neural Information Processing Systems, 36, 2024. 5, 7, 1 [40] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on Machine Learning, 2024. 1 [41] Ling Yang, Zixiang Zhang, Zhilong Zhang, Xingchao Liu, Minkai Xu, Wentao Zhang, Chenlin Meng, Stefano Ermon, and Bin Cui. Consistency flow matching: Defining straight flows with velocity consistency. arXiv preprint arXiv:2407.02398, 2024. 3 [42] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 2, [43] Zhihang Yuan, Hanling Zhang, Lu Pu, Xuefei Ning, Linfeng Zhang, Tianchen Zhao, Shengen Yan, Guohao Dai, and Yu Wang. Ditfastattn: Attention compression for diffusion transformer models. Advances in Neural Information Processing Systems, 37:11961219, 2025. 2, 3 [44] Xinchen Zhang, Ling Yang, Guohao Li, Yaqi Cai, Jiake Xie, Yong Tang, Yujiu Yang, Mengdi Wang, and Bin Cui. Itercomp: Iterative composition-aware feedback learning from model gallery for text-to-image generation. arXiv preprint arXiv:2410.07171, 2024. 1 [45] Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You. Real-time video generation with pyramid attention broadcast. arXiv preprint arXiv:2408.12588, 2024. 3 [46] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. 6 [47] Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, and Linfeng Zhang. Accelerating diffusion transformers with tokenarXiv preprint arXiv:2410.05317, wise feature caching. 2024. 3, 5, 7 national conference on computer vision, pages 41954205, 2023. 1, [25] Pablo Pernias, Dominic Rampas, Mats Richter, Christopher Pal, and Marc Aubreville. Wurstchen: An efficient architecture for large-scale text-to-image diffusion models. arXiv preprint arXiv:2306.00637, 2023. 3 [26] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2 [27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 5, 1 [28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 1 [29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2, 3 [30] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [31] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 1 [32] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 1 [33] Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2v-compbench: comprehensive benchmark for compositional text-to-video generation. arXiv preprint arXiv:2407.14505, 2024. 6, 1 [34] Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350, 2023. 3 [35] Ye Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan, Di ZHANG, et al. Videotetris: Towards compositional text-tovideo generation. Advances in Neural Information Processing Systems, 37:2948929513, 2024. 1 [36] Fu-Yun Wang, Ling Yang, Zhaoyang Huang, Mengdi Wang, and Hongsheng Li. Rectified diffusion: Straightness is not your need in rectified flow. arXiv preprint arXiv:2410.07303, 2024. 10 Training-free Diffusion Acceleration with Bottleneck Sampling"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Implementaion Details A.1. Baseline Models Configuration semantic dimension. A.3. Hyperparameter In this section, we describe the configurations of different baseline models used in our study. We adopt the original model implementations whenever possible. FLUX.1-dev [17]: 12-billion-parameter rectified flow transformer designed for text-to-image generation. Built upon the MMDiT architecture[6], FLUX.1-dev scales to 12B parameters and consistently achieves state-of-the-art performance in image generation. However, its computational cost remains substantial, requiring up to 30 seconds on A100 80G to generate one 1024p image. HunyuanVideo [16]: 13-billion-parameter opensource video generation model. HunyuanVideo is recognized for its smooth motion synthesis, precise semantic alignment, and high-quality aesthetics. However, its computational demands are considerable, requiring 50 minutes on an A100 80G or 30 minutes on an H100 80G to generate 1280p video with 129 frames, posing significant challenges for practical deployment. A.2. Evaluation Metrics For text-to-image generation evaluation, we curated set of 400 prompts, manually selected from HPSv2 [38] and highquality web-sourced prompts, as detailed in Appendix E. These prompts were chosen to emphasize complex, finegrained, and detail-rich scenarios, often involving multiple objects. The generated images were evaluated using several widely adopted benchmarks, including CLIP [27], which assesses text-image alignment, ImageReward [39], reward model trained to reflect human preferences, GenEval [7], and T2I-CompBench [12], which measure compositional abilities. For GenEval and T2I-CompBench, we report the average scores across all metrics. For text-to-video generation evaluation, we mannualy selected subset of prompts from VBench [13], T2VCompBench [33], and Hunyuan Evaluation Prompts [16], focusing on longer and more descriptive text inputs to better assess complex generative capabilities. Each method generated five videos per prompt using different random seeds to ensure more comprehensive comparison. The generated videos were evaluated using per-frame averaged CLIP Score [27], the Compositional Video Benchmark [33], and the widely used VBench [13], which categorizes performance into two primary scores: quality and semantic accuracy. To reflect the greater importance of visual fidelity in generative evaluation, weighted total score was computed, assigning the quality dimension four times the weight of the In this section, we further detailed our selected hyperparameters for bottleneck sampling on both text-to-image generation and text-to-video generation in our presented results. Baseline height width number of inference steps shifting Bottleneck Sampling stage height list {hi}K width list {wi}K i=0 i=0 Noise Reintroducing strength list {wi}K number of inference steps {Ni}K i=0 i=0 Scheduler Re-Shifting shifting list {si}K i=0 1024 1024 50 3.0 3 [1024, 512, 1024] [1024, 512, 1024] [1, 0.8, 0.6] [6, 20, 8] [9, 6, 9] Table 4. Hyperparameters of Bottleneck Sampling for Text-toImage Generation on FLUX.1-dev [17]. Baseline height width number of inference steps shifting 720 1280 50 7.0 Bottleneck Sampling Stage Configuration stage height list {hi}K width list {wi}K i=0 frames list {ti}K i=0 i=0 Noise Reintroducing strength list {wi}K number of inference steps {Ni}K i=0 i=0 Scheduler Re-Shifting shifting list {si}K i=0 3 [720, 544, 720] [1280, 960, 1280] [129, 129, 129] [1, 0.8, 0.7] [4, 24, 16] [7, 9, 9] Table 5. Hyperparameters of Bottleneck Sampling for Text-toVideo Generation on HunyuanVideo [16]. 1 B. Computational Complexity Analysis C. User Study 16 In this section, we conduct detailed complexity analysis to quantify the computational savings achieved by Bottleneck Sampling. Taking an image generation model as an example, given target resolution , the DiT model first compresses the input using VAE and then applies patchification operation before computing attention. In the case of FLUX, the VAE compression ratio is 8, and the patch size is 2, leading to an attention sequence length of = 16 . Since MMDiT processes both text and image tokens together in unified self-attention mechanism, and the text sequence length is typically an order of magnitude smaller than the image sequence length, we approximate the 16 total sequence length as: 16 During computation, tokens are first projected through linear layer to obtain queries, keys, and values. The queries and keys are then multiplied via dot product, passed through softmax function, and multiplied with the values. Finally, the output undergoes another linear transformation. The total computational cost of the Self-Attention layer can be expressed as: FLOPs = 6 D2 + 4 S2 + 2 S2 + 2 D2 + 16 D2. (10) where denotes the sequence length, denotes the hidden dim, and denotes the number of heads. Simplifying and neglect the number of heads which is relatively small, we obtain: FLOPs 24 D2 + 4 S2 (11) This expression provides clear measure of the computational cost associated with self-attention in diffusion transformers. Taking into account real values, where = 3072, we compute: FLOPs512 20.43 TFlops, FLOPs1024 78.86 TFlops. Thus, at each step of computation (NFE), reducing the resolution by half results in speedup of: Given the inherent randomness and diversity in image and video generation, relying solely on quantitative metrics may be insufficient for comprehensive evaluation of performance. To further validate our results, we conducted user study to assess the perceptual quality of generated outputs. We randomly sampled 100 image prompts and 100 video prompts and recruited five trained human annotators for evaluation. For each prompt, guidance scale was randomly selected from three broad ranges during inference. We then generated one pair of outputs: one from the baseline model (FLUX/1-dev for images and HunyuanVideo for videos) and the other from Bottleneck Sampling. The annotators were presented with these outputs in an anonymized manner and asked to select one of three options: Model 1, Model 2, or Same. This process ensured an unbiased assessment of visual quality and text alignment. The results of the user study for both text-to-image and text-to-video tasks are summarized in Tab. 6."
        },
        {
            "title": "Same",
            "content": "FLUX.1-dev [17] HunyuanVideo [16] 16% 14% 17% 67% 13% 73% Table 6. User study results for text-to-image and text-to-video tasks, measured by the selection rate for each option. The study results indicate that in most cases, the outputs from our Bottleneck Sampling model are perceptually comparable to those of the baseline models, with the majority of annotators selecting the Same option. This suggests that our approach preserves high-quality generation while significantly improving efficiency, making it viable alternative for computationally constrained settings. D. Extended Ablation Studies In this section, we conduct additional ablation studies to evaluate the robustness of our approach. Sampling Stages 5 Stage 3 Stage CLIP Score 0.451 0.457 ImageReward 1.253 1.254 Upsampling Method CLIP Score ImageReward FLOPs1024 FLOPs512 3.8. (12) Bilinear Interpolation Bicubic Interpolation Nearest-Neighbor Interpolation Lanczos Interpolation 0.453 0.454 0.461 0. 1.249 1.251 1.249 1.254 In summary, leveraging lower-resolution inference during intermediate steps, where fine-grained details are less critical, significantly reduces computational FLOPs while maintaining comparable performance. Table 7. Comparison of different upsampling methods based on CLIP Score and ImageReward. 2 D.1. Effect of Stage Numbers ting: [1024, 512, 256, 512, 1024]. The performance remains comparable to the three-stage configuration, with the latter potentially offering more favorable trade-off. Although five-stage Bottleneck Sampling setup may theoretically provide higher upper bound in performance, it introduces significantly more hyperparameters. Due to the lack of direct self-evaluation mechanism in diffusion models, determining these hyperparameters can be challenging. As such, we adopt the three-stage Bottleneck Sampling configuration for this study and leave further exploration of multi-stage designs to future work. D.2. Effect of Upsampling Methods Figure 8. Effect of Stage Numbers in our Bottleneck Sampling. Comparison between 3 stages and 5 stages. Our primary experiments report results based on threestage configuration. To further investigate the impact of stage design, we conduct additional ablations by increasing the number of stages. In Fig. 8 and Tab. 7, we present results on FLUX.1-dev using five-stage setFigure 9. Effect of Upsampling Method in our Bottleneck Sampling. We evaluate the impact of different upsampling methods on performance. For simplicity and ease of implementa3 tion, our default choice is bilinear interpolation. However, to explore alternative upsampling strategies, we conduct experiments with the following commonly used methods: 1. Bilinear Interpolation: Computes the output pixel as weighted average of the four nearest input pixels, offering balance between speed and smoothness. 2. Bicubic Interpolation [15]: Uses cubic convolution to estimate pixel values based on the 16 nearest neighbors, typically producing smoother results than bilinear interpolation. 3. Nearest-Neighbor Interpolation: Assigns each output pixel the value of its closest input pixel, preserving sharp edges but often introducing blocky artifacts. 4. Lanczos Interpolation [5]: high-quality resampling method using sinc functions to achieve sharper results, at the cost of higher computational overhead. We report the impact of different upsampling methods on model performance in Tab. 7 and Fig. 9. The results indicate that upsampling methods have minimal effect on overall generation quality, with most methods performing comparably across different images. For text-based image generation, nearest-neighbor interpolation achieves the highest text accuracy but may introduce inconsistencies in visual style. To further assess human perceptual preferences, we evaluate ImageReward scores and ultimately adopt Lanczos interpolation in our final setup. Notably, all upsampling methods produce satisfactory results within our pipeline, highlighting the robustness of the proposed approach. E. Prompt list We provide our prompt list for the generated images presented in our qualitative results and part of our evaluation prompt sets as follows: 1. Brunette pilot girl in snowstorm, full body, moody lighting, intricate details, depth of field, outdoors, Fujifilm XT3, RAW, 8K UHD, film grain, Unreal Engine 5, ray tracing. 2. An ancient spiritual gnomes stone pathway rock garden.sculptured . Style of alex grey,giger.unreal engine.totem sculptures.swirling patterned stone pathway courtyard.wood.stone.driftwood.statues. artistic sculpture.lanterns.air bnb tiny house 3. droplet from small brook enters the ocean, creating rippling water. few bamboo leaves, white and green, are seen with soft focus effect, glinting under the suns twinkling rays. 4. blue coloured pizza. 5. new Human Mecha combined, set against massive post-apocalyptic background, realisticlighting, ruined ruins, high level of rendering, virtual reality. 6. racing car with silver transparent texture, showcasing design sensibility against white background, industry design. 7. beautiful woman facing to the camera, smiling confidently, colorful long hair, diamond necklace, deep red lip, medium shot, highly detailed, realistic, masterpiece. 8. Realism, Unreal Engine, cinematic feel, exaggerated lighting, cyberpunk, future world, advanced technology, neon city at night, car chase scene, busy road with many cars coming and going, police car is chasing yellow taxi, with strong sense of speed, exaggerated lens effects, and movie screenshots. future world, 9. Japanese anime, celluloid style, animation screenshots, cyberpunk, technologically advanced, Close-up, capable woman with dark blue short hair, wearing high-tech outfit, is driving, with nervous expression. 10. American cartoons, 3D modeling, commercial cartoon movies, high-definition rendering, cyberpunk, future world, advanced technology, black interior environment, close-up, backlight, black and blue tones, cold atmosphere, machinery factory. There is silver robot head suspended by cables. 11. Realism, Unreal Engine, cinematic feel, exaggerated lighting, horror movie, over-the-shoulder shot, the center of the picture shows an ancient seaside lighthouse, shrouded in thick fog, exuding gloomy atmosphere, the close shot on the left side of the picture is woman in her twenties with medium-length black hair sitting on boat looking at the lighthouse, she is wearing black shirt, brown suit jacket and long jeans. 12. Japanese anime, celluloid style, animation screenshots,Medium shot, with charming American seaside town in the background, boy with short black hair, in his twenties, wearing black shirt and jeans, and girl with medium-long blond hair, in her twenties, wearing white long-sleeved dress, standing and feeding the pigeons together. 13. 2D cartoon,Diagonal composition, Medium close-up, whole body of classical doll being held by hand, the doll of young boy with white hair dressed in purple, He has pale skin and white eyes. 14. An enchanting dark fantasy scene captures semitransparent woman very perfect mind formed woman body,ponytail hair and bow on the side of the head with her silhouette illuminated by radiant blue hue, accentuating her graceful form. Surrounded by mesmerizing array of glow-in-the-dark butterflies in vibrant neon colors like electric green, pulsating pink, and luminous orange, she seems to be the center of surreal spectacle of radiant beauty the silhouette has reflection as if it were made of glass and shines as if it had varnish. The butterflies, varying in size, fill the silhouette completely, creating breathtaking visual experience. The clean darkness of the background serves as perfect contrast, evoking sense of enchantment, wonder, and mystique. This conceptual artwork masterfully combines elements of wildlife photography, cinematic aesthetics, and fashion illustration to create dreamlike realm where magic and mystery intertwine. wildlife photography, fashion, conceptual art, ukiyo-e, 3d render, cinematic, photo, dark fantasy, illustration, vibrant, portrait photography 15. vibrant and colorful artistic representation of an eye. The eye is the central focus, with its iris exhibiting spectrum of colors ranging from reds to blues. Surrounding the eye are abstract patterns and shapes in myriad of colors, including oranges, yellows, greens, and blues. Some of these patterns resemble fluid or paint splatters, while others have more structured, almost psychedelic appearance. The eyes eyelashes are prominently depicted, and they seem to be made of thick, black strands. The overall feel of the image is dynamic, energetic, and evocative of intense emotion or creativity. 16. An awe-inspiring 3D render of glass bottle magically transformed into miniature tropical paradise. The pristine white sand glistens beneath the warm sunlight, accompanied by sparkling shells and an intricately detailed palm tree swaying gently. charming thatched-roof hut and shimmering blue bottle add to the idyllic beach setting, while the sandy message Maya exudes joy and tranquility. The cinematic and illustrative style of the rendering immerses the viewer in sun-drenched, captivating escape. This 32k, 4D, full HDR, and hyperrealistic masterpiece is testament to the future of digital art and imaging, redefining visual storytelling through its breathtaking depth and detail., poster, typography, illustration, photo, 3d render, cinematic 17. whimsical and creative digital art of giraffe constructed entirely from the brawn-orange Eiffel Tower. Each metal tower segment bends to form the long neck and body of the giraffe, with its iconic pointed tip serving as the head. The background reveals bustling cityscape, where the Arc de Triomphe and other Parisian landmarks emerge in the distance. The overall atmosphere of the digital illustration is playful, offering unique and unexpected perspective on world-famous monument. conceptual art 18. surreal depiction of an elephant, facing away the viewer, sitting on thin, fragile looking bare tree branch, set against serene desert landscape under clear sky. Big elephant sitting on tree branch in the desert with moon in the background, surrealistic digital artwork, surrealism 8k, 4 surrealism, Below the big elephant is vast desert with sand mounds and sparse vegetation. Clear blue sky above, African landscape, overcast, candid photo, cinematic, countryside, curious animals, English, sunset, beautiful landscape, vibrant sky, (((greenery))), (((wide shot))), wildlife, hyper realistic photo, 8k, high resolution, (((lambs))), clear faces, lamb fur, realistic, excellent light, shadows. Super details, illustration, 3d render, painting 19. captivating conceptual art piece featuring ladybug as the centerpiece. The ladybug, initially depicted in monochrome, starts to breathe fire that gradually transforms into kaleidoscope of vibrant colors where other ladybug-suited big roach gracefully floats among breathtaking display of vibrant intergalactic elements. The background is minimal and largely blank, allowing the viewers focus to be entirely on the ladybug and its mesmerizing transformation. The artwork conveys sense of rebirth and the power of change through its striking contrast of color palettes., conceptual art 20. stunning and vibrant artwork of London cityscape, showcasing the iconic Big Ben clock tower as the focal point. The tower stands tall and majestic, with glowing orange and yellow sunset casting warm glow over the scene. To the left, classic red telephone booth adds touch of traditional British charm, its reflection mirrored in the wet, glistening streets. The streets have dreamy, watercolor-like quality, with muted grays, vibrant reds, and splashes of blue in the reflections of the buildings and sky. The overall composition is reminiscent of movie scene, capturing the essence of London in captivating and artistic way., photo, cinematic, poster, vibrant, painting, illustration, portrait photography 21. Imagine rustic wooden sign for bed and breakfast, featuring some elegant welcoming words Welcome to Our Home carved into the wood, in flowing script that complements the natural grain of the wood. 22. chic keychain with small, elegant logo engraved on one side and the words Home Sweet Home written underneath, in polished metal finish 23. Design stylish dancers back logo with the letters and 24. humorous and vibrant digital art piece featuring group of chubby, orange cats serving as protesters. They are holding up witty and creative signs, such as COMMENTS NOW, ON STRIKE and GIMME COMMENTS. The cats have expressions of determination and urgency, standing in front of graffiti-covered wall. The overall atmosphere is lively and humorous, with touch of rebellion 25. logo designs, cheerful cartoon ice cream cones topped with cherry and bright smile, and signs in quirky colourful lettering that read You make me melt. F. More Qualitative Results We provide detailed comparison with baseline models in Fig. 10 and Fig. 11. 5 Figure 10. Comparison with FLUX.1-dev Figure 11. Comparison with HunyuanVideo"
        }
    ],
    "affiliations": [
        "Bytedance",
        "Peking University"
    ]
}