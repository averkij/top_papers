{
    "paper_title": "PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data",
    "authors": [
        "Shijie Huang",
        "Yiren Song",
        "Yuxuan Zhang",
        "Hailong Guo",
        "Xueyin Wang",
        "Mike Zheng Shou",
        "Jiaming Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce PhotoDoodle, a novel image editing framework designed to facilitate photo doodling by enabling artists to overlay decorative elements onto photographs. Photo doodling is challenging because the inserted elements must appear seamlessly integrated with the background, requiring realistic blending, perspective alignment, and contextual coherence. Additionally, the background must be preserved without distortion, and the artist's unique style must be captured efficiently from limited training data. These requirements are not addressed by previous methods that primarily focus on global style transfer or regional inpainting. The proposed method, PhotoDoodle, employs a two-stage training strategy. Initially, we train a general-purpose image editing model, OmniEditor, using large-scale data. Subsequently, we fine-tune this model with EditLoRA using a small, artist-curated dataset of before-and-after image pairs to capture distinct editing styles and techniques. To enhance consistency in the generated results, we introduce a positional encoding reuse mechanism. Additionally, we release a PhotoDoodle dataset featuring six high-quality styles. Extensive experiments demonstrate the advanced performance and robustness of our method in customized image editing, opening new possibilities for artistic creation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 7 9 3 4 1 . 2 0 5 2 : r PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data Shijie Huang1,5* Yiren Song1* Yuxuan Zhang2 Hailong Guo3 Xueyin Wang4 Mike Zheng Shou1 Jiaming Liu5 1National University of Singapore 2Shanghai Jiao Tong University 3Beijing University of Posts and Telecommunications 4Byte Dance 5Tiamat Figure 1. PhotoDoodle can mimic the styles and techniques of human artists in creating photo doodles, adding decorative elements to photos while maintaining perfect consistency between the preand post-edit states."
        },
        {
            "title": "Abstract",
            "content": "We introduce PhotoDoodle, novel image editing framework designed to facilitate photo doodling by enabling artists to overlay decorative elements onto photographs. Photo doodling is challenging because the inserted elements must appear seamlessly integrated with the background, requiring realistic blending, perspective alignment, and contextual coherence. Additionally, the background must be preserved without distortion, and the artists unique style must be captured efficiently from limited training data. These requirements are not addressed by previous methods that primarily focus on global style transfer or *Equal contribution. Corresponding author. Project leader. regional inpainting. The proposed method, PhotoDoodle, employs two-stage training strategy. Initially, we train general-purpose image editing model, OmniEditor, using large-scale data. Subsequently, we fine-tune this model with EditLoRA using small, artist-curated dataset of before-and-after image pairs to capture distinct editing styles and techniques. To enhance consistency in the generated results, we introduce positional encoding reuse mechanism. Additionally, we release PhotoDoodle dataset featuring six high-quality styles. Extensive experiments demonstrate the advanced performance and robustness of our method in customized image editing, opening new possibilities for artistic creation. Code is released at https://github.com/showlab/PhotoDoodle. 1. Introduction The rise of diffusion models has started new chapter for image creation and control. Using the pretrain-finetune approach, the community has achieved remarkable progress in customized image generation [9, 11, 24], with applications spanning identity preservation [17], artistic stylization [1, 27, 29, 44], and subject coherence [9, 11, 13, 15, 24, 25, 47]. However, while customized image generation thrives, critical gap persists in customized image editing. This problem contrasts sharply with the growing need for precise image editing tools, as current methods mainly focus on creating new content rather than making smart edits based on context. This less explored area of custom image editing presents both technical challenges and chances to create new important applications. Photo Doodling represents long-recognized yet historically challenge in customized image editing. This task involves artists enhancing background photographs through the strategic integration of decorative elements and stylized modifications to achieve personalized or artistic effects. Typical techniques include localized stylization, decorative linework, novel object insertion, and ornamental augmentation of existing subjects. Artists and designers exhibit distinctive personal styles and strategic approaches during the photo doodling creation process, which holds significant value. However, the absence of automated tools forces manual workflows requiring hour-level time investments per image, severely constraining both productivity and the feasibility of generating large-scale paired training data. Automating PhotoDoodle workflows is intriguing, yet it faces three interconnected challenges. First, achieving harmonious integration demands that generated decorations align not only spatially (e.g., matching perspective) but also semantically with the background, while accommodating non-photorealistic styles such as cartoon or flat illustration. Second, maintaining strict background consistency requires rigorously preventing unintended alterations to the original content, including color shifts or texture distortions. Third, to achieve efficient style acquisition, it is necessary to extract an artists unique editing patterns from few photodoodle pairs, which is technically challenging. These compounded challenges cause existing methods to be incompetent in addressing the problem comprehensively. The limitations of prevailing image editing paradigms further exacerbate these difficulties. Global editing methods (e.g., Prompt-to-Prompt[10], InstructP2P[4]), while effective for consistent style transfer, inadvertently distort background content during localized modifications. Inpaintingbased approaches, though capable of preserving non-mask regions through localized denoising, impose impractical demands for pixel-perfect user-defined masks, fundamentally conflicting with PhotoDoodles need for flexible creative workflows. To bridge this gap, we introduce an instruction-guided framework that replaces mask dependency with semantic-level control, enabling precise and style-conscious decorative generation while ensuring background consistency. To address these challenges and bridge the gaps in existing solutions, we propose PhotoDoodle, framework designed to learn artistic image editing from few-shot examples. Building upon pre-trained Diffusion Transformer (DiT), our method conditions on background images to generate photo doodles through two-stage training pipeline. First, we adapt well-trained text-to-image model into versatile image editor by fine-tuning it on large-scale editing datasets, thereby establishing the foundational editing model OmniEditor. Then, we train an EditLoRA module using artist-curated before-after pairwise data to learn nuanced editing styles, while introducing Positional Encoding (PE) Cloning strategy to enforce strict background consistency through implicit feature alignment. Our design is guided by two main insights. First, maintaining clean latent conditioningensuring that input image tokens are noise-freeis essential for keeping the output coherent with the background by minimizing divergence in the latent space. Second, implicit alignment through position encoding cloning enables the seamless integration of background context without adding extra trainable parameters, thus preserving the full generative capacity of the base T2I model. These insights allow our framework to strike balance between artistic flexibility and strict consistency. In summary, our contributions are threefold: We propose customizable image editing framework that learns and transfers artistic styles from few photo-doodle pairs. By extending conditional mechanisms and implementing PE cloning strategy for implicit alignment, we achieve high-quality and highly consistent image editing built upon Diffusion Transformers (DiTs), while utilizing EditLoRA to efficiently learn customized editing operations. We propose the first dedicated photo-doodle dataset containing 300+ high-quality pairs across 6 artistic styles, establishing benchmark for reproducible research. 2. Related work 2.1. Diffusion Model and Conditional Generation Recent advances in diffusion models have significantly advanced the state of text-conditioned image synthesis, achieving remarkable equilibrium between generation diversity and visual fidelity. Pioneering works such as GLIDE [19], hierarchical text-to-image models [22], and photorealistic synthesis frameworks [26] have systematically addressed key challenges in cross-modal generation tasks. The emergence of Stable Diffusion [23], which implements Latent Diffusion Model with text-conditioned UNet architecture, has established new benchmarks in text-to-image generation and inspired subsequent innovations including SDXL [21], GLiGEN [16], and Ranni [8]. To enhance domain-specific adaptation, parameter-efficient fine-tuning techniques like Low-Rank Adaptation (LoRA) [11] and DreamBooth [24] have demonstrated effective customization of pre-trained models. Concurrently, research efforts have focused on precise control over pictorial concepts through multi-concept customization [? ], image prompt integration [38], and identity-preserving generation [35]. The introduction of ControlNet [42] further extended controllable generation capabilities to spatial constraints and depth information, with subsequent optimizations improving inference efficiency [45, 46]. Some work [6, 31, 32, 34] also focuses on the security issues of customized generation. Building upon these foundations, our work investigates the novel application of pre-trained diffusion transformers for generating artistic photo-doodles. Unlike previous approaches focusing on photorealism or explicit control modalities, we explore the models potential in capturing freehand artistic expression while maintaining structural coherence. 2.2. Text-guilded Image Editing [10], Imagic [14], EditWorld [37] Recent advances in text-guided image editing have established this field as critical research frontier in visual content manipulation, with current methodologies generally classified into three paradigms: global descriptionlocal description-guided, and instruction-guided guided, Global description-guided approaches (e.g., editing. Prompt2Prompt ), achieve fine-grained manipulation through cross-modal alignment between textual descriptions and image regions, yet demand meticulous text specification for target attributes. Local description-guided methods such as Blended Diffusion [2] and Imagen Editor [36] enable pixel-level control via explicit mask annotations and regional text prompts, though their practical application is constrained by the requirement for precise spatial specifications, particularly in complex editing scenarios like object removal. The emerging instruction-guided paradigm, exemplified by InstructPix2Pix [4] and HIVE [43], represents paradigm shift through its natural language interface that accepts editing commands (e.g., change the scene to spring). This approach eliminates the dependency on detailed textual descriptions or precise mask annotations, significantly enhancing user accessibility. 2.3. Image and video Doodles Image and video doodling involve the creative process of adding hand-drawn elements or animations to static images or video content, blending aspects of graphic design, illustration, and animation to produce playful and In academic research, advanced engaging visual styles. techniques[3, 39, 40] have been developed to automate parts of this process. These methods enable users to generate intricate doodle animations from simple textual descriptions, sketches, or keyframes. By preserving artistic intent and reducing the time and expertise required, these models make multimedia content creation more accessible and efficient. Video doodling extends these capabilities to dynamic video sequences, allowing static doodle elements to be seamlessly integrated and animated in response to video motion and context. This innovation not only enhances interactivity but also introduces greater complexity and realism, making it powerful tool for creative applications in entertainment, education, and storytelling. 3. Method In this section, we begin by exploring the preliminaries on diffusion transformer as detailed in Section 3.1. Next, Section 3.2 outlines our three-stage system design. We then detail two key innovationsOmniEditor Pre-training (Section 3.3) and EditLoRA for style adaptation (Section 3.4). Finally, Section 3.5 describes how we built the PhotoDoodle dataset. 3.1. Preliminary The Diffusion Transformer (DiT)[20] powers modern image generators like Stable Diffusion 3[7] and PixArt[5]. At its core, DiT uses special transformer network that denoise the noisy image tokens step-by-step. DiT operates on two categories of tokens: noisy image tokens RN and text condition tokens cT RM d, where denotes the embedding dimension, and and represent the numbers of image and text tokens, respectively. These tokens retain their shapes consistently as they propagate through multiple transformer layers. In FLUX.1, each DiT block incorporates layer normalization, followed by Multi-Modal Attention (MMA) [7], which employs Rotary Position Embedding (RoPE) [33] to encode spatial information. For image tokens z, RoPE applies rotation matrices based on the tokens position (i, j) in the 2D grid: zi,j zi,j R(i, j), (1) where R(i, j) represents the rotation matrix corresponding to position (i, j). Similarly, text tokens cT are transformed with their positions designated as (0, 0). The multi-modal attention mechanism projects these position-encoded tokens into query Q, key K, and value representations, enabling attention computation across all tokens: MMA([z; cT ]) = softmax (cid:19) (cid:18) QK V, (2) Figure 2. The overall architecture and training prodigim of photodoodle. The ominiEditor and EditLora all follow the lora training prodigm. We use high rank lora for pre-training the OmniEditor on large-scale dataset for general-purpose editing and text-following capabilities, and low rank lora for fine-tuning EditLoRA on small set of paired stylized images to capture individual artists specific styles and strategies for efficient customization. We encode the original image into condition token and concatenate it with noised latent token, controlling the generation outcome through MMAttention. where = [z; cT ] denotes the concatenation of image and text tokens. This approach ensures bidirectional attention between the tokens. 3.2. Overall Architecture The overall architecture of PhotoDoodle is illustrated in Fig. 2. It comprises the following stages: Pre-training OmniEditor. The OmniEditor is pre-trained on large-scale image editing dataset to acquire generalpurpose image editing capabilities and strong text-following abilities. This stage ensures the models capability in diverse editing tasks. Fine-tuning EditLoRA. After pre-training, EditLoRA is fine-tuned on small set of pairwise stylized images (2050 pairs) to learn the specific editing styles and strategies of individual artists. This stage enables efficient customization for personalized editing needs. Inference.: During inference, the input source image Isrc is encoded as condition tokens cI via VAE. We then randomly sample Gaussian noise as image tokens z, cloning the Position Encoding from condition tokens, and concatenate the tokens along the sequence dimension. Subsequently, we apply the flow matching method to predict the target velocity, iterating multiple steps to obtain the predicted image latent representation. Finally, the predicted image tokens are converted by the VAE decoder to achieve the final predicted image. 3.3. OmniEditor Pre-training We denote the pre-edited image as the source image Isrc and the post-edited image as the target image Itar. Previous works, such as SDEdit, model image editing as an addingdenoising problem, they often altering unintended areas. Others like InstructP2P[4] redesign core model parts, significantly degrading the capacity of the pretrained t2i models. Unlike them, our approach, PhotoDoodle, conceptualizes image editing as conditional generation problem and minimizes the modification of pretrained text-to-image DiT. Our model leverages the advanced capabilities of the DiT-based pretrained model, and extends it to function as an image editing tool. Both Isrc and Itar are encoded into their respective latent representations, cI and z, via VAE. After applying position encoding cloning, the latent tokens are then concatenated along the sequence dimension to perform joint attention. The Multi-modal attention mechanisms are used to provide conditional information for the denoising of the doodle image. MMA([z; ci; cT ]) = softmax (cid:19) (cid:18) QK V, (3) where = [z; ci; cT ] denotes the concatenation of noised latent tokens, image condition tokens, and text tokens. Here, cI corresponds to Isrc. This formulation enables bidirectional attention, letting the conditional branch and denoise branch interact on demand. Position Encoding Cloning. Existing approaches to conditional image editing often struggle with pixel-level misalignment between input Isrc and edited output (Itar) that undermine visual coherence. To address this fundamental challenge, we propose novel Position Encoding(PE) Cloning strategy, motivated by the need for implicit spatial correspondence. The PE Cloning is simple yet powerful stragegy, it simply apply the position encoding calculated on Isrc on both Isrc and Itar. The identical positional encoding serves as strong hint so that the DiT is capable of learning correct corresponding easily. By enforcing identical positional encodings between the latent representations cI and z, our method establishes pixel-perfect coordinate mapping that persists throughout the diffusion process. This geometric consistency ensures that every edit respects the original images spatial structure, eliminating ghosting artifacts and misalignments that plague conventional approaches. Noise-free Conditioning Paradigm. critical innovation lies in our noise-free conditioning paradigm. We preserve cI as reference during the generation of Itar. This design choice achieves two objectives through its operational duality. First, by maintaining cI in noise-free state, we ensure the retention of high-frequency textures and fine structural details from the original image, thereby preventing degradation during iterative denoising. This preservation mechanism acts as safeguard against the blurring artifacts commonly observed in conventional approaches. Second, the MM attention machanism is flexible enough to choose either to copy from the source or generate new content via instruction, making the model learns to manipulate only designated target regions. Through the combined action of position encodings cloning and MMA mechanism, our framework achieves unprecedented precision in localized editing while maintaining global consistency, balance previously unattainable in conditional image generation tasks. Conditional flow matching loss. The conditional flow matching loss function is following SD3 [7], which is defined as follows: LCF = Et,pt(zϵ),p(ϵ) vΘ(z, t, cI , cT ) ut(zϵ)2(cid:105) (cid:104) (4) Where vΘ(z, t, cI , cT ) represents the velocity field parameterized by the neural networks weights, is timestep, cI and cT are image condition tokens extracted from source image Isrc and text tokens. ut(zϵ) is the conditional vector field generated by the model to map the probabilistic path between the noise and true data distributions, and denotes the expectation, involving integration or summation over time t, conditional z, and noise ϵ. 3.4. EditLoRA LoRA [11] enhances model adaptation by freezing the pretrained model weights W0 and inserting trainable rank decomposition matrices and into each layer of the model. These matrices, Rrk and Rdr, where min(d, k), are used to fit the residual weights adaptively. The forward computation integrates these modifications as follows: = + = W0x + BAx (5) where Rd is the output and Rk denotes the input. Rdr, Rrk with min(d, k). Normally, matrix is initialized with zero. To learn an individual artists editing style and effectively transfer it from small number of before-and-after image pairs, we introduce EditLoRA. Inspired by recent low-rank adaptation (LoRA) techniques [28, 30], EditLoRA fine-tunes only small set of trainable parameters, significantly reducing the risk of overfitting while preserving most of the pretrained models expressive power. In our work, the general-purpose OmniEditor is trained on large-scale paired dataset with higher-rank lora. The EditLoRAs are lower-rank loras that specifically focus on mimicking the style and strategies of single artists in creating photo doodles. EditLoRAs training set consists of before-and-after pairwise data and corresponding text instruction, which differ from the conventional text-image pairs required for learning image generation models. The EditLora steers the behaviour of the OmniEditor to the specified artists style. When new image Isrc is provided, along with the textual instructions, the model generates Itar that reflects both the previously learned editing capabilities and the distinctive stylistic effects from the artist. 3.5. PhotoDoodle In collaboration with professional artists and designers, we created the first PhotoDoodle dataset. We introduce the dataset containing six high-quality styles and over 300 photo doodle samples. The six styles include cartoon monster, hand-drawn outline, 3D effects, flowing color blocks, flat illustrator, and cloud sketch. Each sample in our dataset consists of pre-edit photo (e.g., real-world scene or portrait) and post-edit photo doodle showing unique modifications by the artist, such as localized stylization, decorative lines, newly added objects, or modifications to existing elements. For each example, we store the raw input image Isrc and the doodled version Itar, along with textual instructions. 4. Experiment 4.1. Experiment Setting Setup. During the OmniEditor pre-training stage, we take the parameters of Flux.1 dev model as the initialization of the DiT architecture, and trained it with the SeedEdit dataset. Images were resized to 768x512. We trained LoRA rank of 256, batch size of 128, and learning rate of 1 104, on 8 H100 GPUs for 330,000 steps. After merge the lora into the base DiT model, we acquired the OmniEditor model for further usage. In the EditLoRA training phase, we fine-tuned the merged model on paired photo doodle dataset (50 pairs) using single GPU for 10,000 steps, with LoRA rank of 128, batch size of 2, and learning rate of 1 104. Baseline Methods. The baselines compared in this paper include InstructP2P[4], MagicBrush[41], and SDEdit[18] Figure 3. The generated results of PhotoDoodle. PhotoDoodle can mimic the manner and style of artists creating photo doodles, enabling instruction-driven high-quality image editing. based on Flux. For fair comparison, tests were conducted in both general image editing and customized editing scenarios. For the general image editing tests, OmniEditor was evaluated against the aforementioned baselines. In the customized editing scenario, we trained Flux LoRA using doodle results created by professional artists and used it alongside SDEdit as baseline. For InstructP2P and MagicBrush, the attention layers were also fine-tuned with the same doodle dataset. Finally, all the trained LoRA models were compared with the proposed EditLoRA model. Benchmarks. As with previous methods, we tested the performance of the proposed OmniEditor on the HQ-Edit benchmark[12]. For the customized generation tasks, this paper introduced new benchmark collected from the Pexels website, consisting of 50 high-quality photographs of portraits, animals, and architecture. 4.2. Generation Results Fig. 3 displays the image editing results of PhotoDoodle, which excels in text following due to training on large Figure 4. Compared to baseline methods, PhotoDoodle demonstrates superior instruction following, image consistency, and editing effectiveness. dataset of before-after pairs. The generated doodles blend well with the original images. When trained on limited dataset of artist-paired data using EditLoRA, PhotoDoodle consistently produces artistic doodles while preserving image consistency and avoiding unwanted changes. Notably, our method maintains stable performance and high success rate, making it suitable for production use and reducing the need for selective sampling. Table 1. Comparison Results in General Image Editing Tasks. The best results are denoted as Bold. Methods CLIP Score GP Score CLIPimg Instruct-Pix2Pix Magic Brush SDEdit(FLUX) Ours 0.237 0.234 0.230 0.261 38.201 36.555 34.329 51.159 0.806 0.811 0.704 0.871 4.2.1. Qualitative Evaluation In this section, we present the results of the qualitative analysis. As illustrated in Fig. 4, OmniEdito demonstrates superior consistency and minimizes unintended alterations in general image editing tasks compared to state-of-the-art (SOTA) methods. This performance is attributed to the use of high-quality datasets and thoughtfully designed model architecture. For custom image editing tasks, our method significantly surpasses baseline methods, as evidenced by the high quality of generated outputs and the strong alignment of the editing style with the original artistic intent, while avoiding undesired modifications. Table 2. Comparison Results in Customized Image Editing Tasks. The best results are denoted as Bold. Methods CLIP Score GP Score CLIPimg Instruct-Pix2Pix Magic Brush SDEdit(FLUX) Ours 0.249 0.247 0.209 0.279 36.359 38.478 21.793 63. 0.832 0.885 0.624 0.854 4.2.2. Quantitative Evaluation In this section, we present the quantitative analysis results. Following InstructP2P[4], we compute the CLIP Score and CLIPimg metrics for both tasks. Furthermore, as proposed in HQ-Edit [12], we employ GPT4-o to evaluate the alignment between text instructions and editing outputs. As shown in Table 1, our method outperforms all baselines across all metrics in general image editing tasks, achieving the highest CLIP Score, GP Score, and CLIPimage Score. In custom image editing tasks, while some models fail to produce meaningful edits, which leads to high CLIPimage scores, our method still holds clear advantage over the baselines. This is evident in the substantial improvements in GP Score and CLIP Score, both of which evaluate the consistency and quality of the generated content in relation to the artists original work. 4.3. Ablation Study To demonstrate the effectiveness of the key strategies and modules proposed in this paper, we conducted detailed ablation experiments. We evaluated OmniEditor Pre-training, Position Encoding Cloning, and EditLoRA. As shown in study are collected in Fig. 6, where we reported the percentage scores of each criterion, highlighting our methods effectiveness in aligning closely with artistic intentions and maintaining high consistency in edits without introducing unwanted changes. Figure 5. Ablation study results. Without OmniEditor pretraining, training EditLoRA directly reduces the harmony between sketches and photos and weakens text-following capabilities; removing Position Encoding Cloning decreases output consistency and introduces unwanted background changes; using only pretrained OmniEditor without EditLoRA significantly reduces the degree of stylization. Fig. 5: Without OmniEditor Pre-training, directly training EditLoRA leads to reduced harmony between the generated sketches and photos, along with weaker text-following capabilities in the output. Removing Position Encoding Cloning results in decreased consistency in the generated outputs, with unwanted changes occurring in the background. When EditLoRA is not used, and only the pretrained OmniEditor is employed for generation, the degree of stylization in the results is significantly reduced. 4.4. User Study To further demonstrate the superiority of our proposed method, we conducted user study with 30 participants via online questionnaires. We evaluatedd user preferences in both general and customized image editing scenarios. Participants were presented with PhotoDoodles outputs alongside baseline methods, and asked to evaluate which results they preferred based on three criteria: 1) Overall preference, 2) Instruction following, and 3) Consistency between the edited images and the original images. During the study, participants viewed the original unedited images, the edit instructions, and reference images edited by models. They were then asked to decide whether PhotoDoodle (Option A) or baseline method (Option B) performed better, or if they were about equally effective. The results of this user Figure 6. User study results. The scores demonstrate the percentage of users who prefer ours over others under three evaluation metrics. PhotoDoodle outweighs all other baselines in user study. 5. Limitation and Future Work One limitation of PhotoDoodle is its dependence on the collection of dozens of paired datasets (pre-edit and post-edit images) and the need for thousands of training steps using LoRA. This data collection process can be challenging, as paired images are not always readily accessible. In the future, we will attempt to learn doodling strategies from single image pairs using an Encoder structure. 6. Conclusion In this paper, we present PhotoDoodle, diffusion-based framework for artistic image editing that learns unique artistic styles from minimal paired examples. By combining large-scale pretraining of the OmniEditor with efficient EditLoRA fine-tuning, PhotoDoodle enables precise decorative generation while maintaining background integrity through positional encoding cloning. Key innovations, including noise-free conditioning paradigm and parameter-efficient style adaptation requiring only 50 training pairs, significantly reduce computational barriers. We also contribute new dataset with six artistic styles and 300+ curated samples, establishing benchmark for reproducible research. Extensive experiments demonstrate superior performance in style replication and background harmony, outperforming existing methods in both generic and customized editing scenarios."
        },
        {
            "title": "References",
            "content": "[1] Namhyuk Ahn, Junsoo Lee, Chunggi Lee, Kunhee Kim, Daesik Kim, Seung-Hun Nam, and Kibeom Hong. 2024. Dreamstyler: Paint by style inversion with text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 674681. 2 [2] Omri Avrahami, Dani Lischinski, and Ohad Fried. 2022. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1820818218. 3 [3] Alla Belova. 2021. Google doodles as multimodal storytelling. Cognition, communication, discourse 23 (2021), 1329. 3 [4] Tim Brooks, Aleksander Holynski, and Alexei Instructpix2pix: Learning to follow Efros. 2023. image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1839218402. 2, 3, 4, 5, 7 [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. 2023. PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis. arXiv:2310.00426 [cs.CV] 3 [6] Hai Ci, Pei Yang, Yiren Song, and Mike Zheng Shou. 2024. Ringid: Rethinking tree-ring watermarking for enhanced multi-key identification. In European Conference on Computer Vision. Springer, 338354. 3 [7] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. 2024. Scaling rectified flow transformers for highresolution image synthesis. In Forty-first International Conference on Machine Learning. 3, [8] Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, and Jingren Zhou. 2023. Ranni: Taming Text-toImage Diffusion for Accurate Instruction Following. arXiv preprint arXiv:2311.17002 (2023). 3 [9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. 2022. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618 (2022). 2 [10] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. 2022. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626 (2022). 2, 3 and Weizhu Chen. 2021. tation of large language models. arXiv:2106.09685 (2021). 2, 3, 5 Lora: Low-rank adaparXiv preprint [12] Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. 2024. Hq-edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990 (2024). 6, [13] Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, and Ziwei Liu. 2024. Videobooth: Diffusion-based video generation with image prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 66896700. 2 [14] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. 2023. Imagic: Text-based real image editthe ing with diffusion models. In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition. 60076017. 3 [15] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. 2023. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 19311941. 2 [16] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. 2023. Gligen: Open-set grounded textto-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2251122521. 3 [17] Yang Liu, Cheng Yu, Lei Shang, Ziheng Wu, Xingjun Wang, Yuze Zhao, Lin Zhu, Chen Cheng, Weitao Chen, Chao Xu, et al. 2023. Facechain: playground arXiv for identity-preserving portrait generation. preprint arXiv:2308.14256 (2023). 2 [18] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. 2021. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073 (2021). [19] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2021. Glide: Towards photorealistic image generation and editing arXiv preprint with text-guided diffusion models. arXiv:2112.10741 (2021). 2 [20] William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 41954205. 3 [11] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, [21] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. 2023. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952 (2023). 3 [22] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical textconditional image generation with clip latents. arXiv 2022. arXiv preprint arXiv:2204.06125 (2022). 2 [23] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. 2022. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 10684 10695. [24] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2023. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2250022510. 2, 3 [25] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. 2024. Hyperdreambooth: Hypernetworks for fast personalization of textto-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 65276536. 2 [26] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 2022. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems 35 (2022), 3647936494. 2 [27] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, et al. 2023. Styledrop: Text-to-image generation in any style. arXiv preprint arXiv:2306.00983 (2023). 2 [28] Yiren Song, Danze Chen, and Mike Zheng Shou. 2025. LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer. arXiv preprint arXiv:2502.01105 (2025). 5 [29] Yiren Song, Shijie Huang, Chen Yao, Xiaojun Ye, Hai Ci, Jiaming Liu, Yuxuan Zhang, and Mike Zheng Shou. 2024. ProcessPainter: Learn PaintarXiv preprint ing Process from Sequence Data. arXiv:2406.06062 (2024). 2 [30] Yiren Song, Cheng Liu, and Mike Zheng Shou. 2025. MakeAnything: Harnessing Diffusion Transformers for Multi-Domain Procedural Sequence Generation. arXiv preprint arXiv:2502.01572 (2025). Anti-Reference: Universal and Immediate Defense Against Reference-Based Generation. arXiv preprint arXiv:2412.05980 (2024). 3 [32] Yiren Song, Pei Yang, Hai Ci, and Mike Zheng Shou. IDProtector: An Adversarial Noise Encoder 2024. to Protect Against ID-Preserving Image Generation. arXiv preprint arXiv:2412.11638 (2024). 3 [33] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing 568 (2024), 127063. 3 [34] Thanh Van Le, Hao Phung, Thuan Hoang Nguyen, Quan Dao, Ngoc Tran, and Anh Tran. 2023. Antidreambooth: Protecting users from personalized textto-image synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2116 2127. 3 [35] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Instantid: Zero-shot identityarXiv preprint Anthony Chen. 2024. preserving generation in seconds. arXiv:2401.07519 (2024). [36] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David Fleet, Radu Soricut, et al. 2023. Imagen editor and editbench: Advancing and evaluating text-guided image inpainting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1835918369. 3 [37] Ling Yang, Bohan Zeng, Jiaming Liu, Hong Li, Minghao Xu, Wentao Zhang, and Shuicheng Yan. 2024. EditWorld: Simulating World Dynamics for Instruction-Following Image Editing. arXiv preprint arXiv:2405.14785 (2024). 3 [38] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt arXiv 2023. adapter for text-to-image diffusion models. preprint arXiv:2308.06721 (2023). 3 [39] Emilie Yu. 2023. Designing tools for 3D content authoring based on 3D sketching. Ph. D. Dissertation. Universite Cˆote dAzur. 3 [40] Emilie Yu, Kevin Blackburn-Matzen, Cuong Nguyen, Oliver Wang, Rubaiat Habib Kazi, and Adrien Bousseau. 2023. Videodoodles: Hand-drawn animations on videos with scene-aware canvases. ACM Transactions on Graphics (TOG) 42, 4 (2023), 112. [41] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. 2024. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems 36 (2024). 5 [31] Yiren Song, Shengtao Lou, Xiaokang Liu, Hai Ci, Pei Yang, Jiaming Liu, and Mike Zheng Shou. 2024. [42] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 38363847. 3 [43] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, ChiaChih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, et al. 2024. Hive: Harnessing human feedback for instructional visual editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9026 9036. 3 [44] Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong, and Changsheng Xu. 2023. Inversion-based style transfer with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 10146 10156. [45] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, and Zhongliang Jing. 2024. SSREncoder: Encoding Selective Subject Representation for Subject-Driven Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 80698078. 3 [46] Yuxuan Zhang, Yiren Song, Jinpeng Yu, Han Pan, and Zhongliang Jing. 2024. Fast Personalized Text to Image Synthesis with Attention Injection. In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). https://doi.org/10.1109/ 61956199. ICASSP48485.2024.10447042 3 [47] Chenyang Zhu, Kai Li, Yue Ma, Chunming He, and Li Xiu. 2024. MultiBooth: Towards Generating All Your Concepts in an Image from Text. arXiv preprint arXiv:2404.14239 (2024)."
        }
    ],
    "affiliations": [
        "Beijing University of Posts and Telecommunications",
        "Byte Dance",
        "National University of Singapore",
        "Shanghai Jiao Tong University",
        "Tiamat"
    ]
}