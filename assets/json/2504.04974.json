{
    "paper_title": "Towards Visual Text Grounding of Multimodal Large Language Model",
    "authors": [
        "Ming Li",
        "Ruiyi Zhang",
        "Jian Chen",
        "Jiuxiang Gu",
        "Yufan Zhou",
        "Franck Dernoncourt",
        "Wanrong Zhu",
        "Tianyi Zhou",
        "Tong Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a non-neglectable limitation remains in their struggle with visual text grounding, especially in text-rich images of documents. Document images, such as scanned forms and infographics, highlight critical challenges due to their complex layouts and textual content. However, current benchmarks do not fully address these challenges, as they mostly focus on visual grounding on natural images, rather than text-rich document images. Thus, to bridge this gap, we introduce TRIG, a novel task with a newly designed instruction dataset for benchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs in document question-answering. Specifically, we propose an OCR-LLM-human interaction pipeline to create 800 manually annotated question-answer pairs as a benchmark and a large-scale training set of 90$ synthetic data based on four diverse datasets. A comprehensive evaluation of various MLLMs on our proposed benchmark exposes substantial limitations in their grounding capability on text-rich images. In addition, we propose two simple and effective TRIG methods based on general instruction tuning and plug-and-play efficient embedding, respectively. By finetuning MLLMs on our synthetic dataset, they promisingly improve spatial reasoning and grounding capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 4 7 9 4 0 . 4 0 5 2 : r a"
        },
        {
            "title": "Towards Visual Text Grounding of Multimodal Large\nLanguage Model",
            "content": "Ming Li2, Ruiyi Zhang1, Jian Chen3, Jiuxiang Gu1, Yufan Zhou1, Franck Dernoncourt1 Wanrong Zhu1, Tianyi Zhou2, Tong Sun1 1Adobe Research 2University of Maryland 3University at Buffalo minglii@umd.edu, ruizhang@adobe.com"
        },
        {
            "title": "Abstract",
            "content": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), non-neglectable limitation remains in their struggle with visual text grounding, especially in text-rich images of documents. Document images, such as scanned forms and infographics, highlight critical challenges due to their complex layouts and textual content. However, current benchmarks do not fully address these challenges, as they mostly focus on visual grounding on natural images, rather than text-rich document images. Thus, to bridge this gap, we introduce TRIG, novel task with newly designed instruction dataset for benchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs in document question-answering. Specifically, we propose an OCR-LLM-human interaction pipeline to create 800 manually annotated question-answer pairs as benchmark and large-scale training set of 90k synthetic data based on four diverse datasets. comprehensive evaluation of various MLLMs on our proposed benchmark exposes substantial limitations in their grounding capability on text-rich images. In addition, we propose two simple and effective TRIG methods based on general instruction tuning and plug-and-play efficient embedding, respectively. By finetuning MLLMs on our synthetic dataset, they promisingly improve spatial reasoning and grounding capabilities."
        },
        {
            "title": "Introduction",
            "content": "Despite the remarkable advancements in LLMs [64, 65, 4, 53] and MLLMs [85, 16, 41], the trustworthiness of their generated outputs remains critical concern [40, 60, 20]. While these models can produce fluent and coherent responses, they often lack grounding capability, which can lead to potential hallucinations [21, 58, 70, 35]. Grounding capability is defined as the models ability to accurately localize relevant regions in the visual content based on the provided semantic description [51, 45, 83, 22, 82]. This capability is essential for ensuring that the responses are accurate and verifiable. Existing grounding efforts in MLLMs mainly focus on natural images, where the task involves associating textual descriptions with corresponding visual elements, such as objects or scenes[57, 7, 87, 89, 56, 82]. However, there is significant gap in the literature when it comes to visual text grounding in text-rich document images. Document images, such as scanned forms, charts, and complex posters, present unique challenges that differ markedly from those found in natural images [50, 48, 49, 86]. They often feature mix of textual and graphical elements and require precise localization and understanding of content, especially textual content. An example is shown in Figure 1, for the given text-rich document image, we expect LLMs to not only generate the answer alone but also to provide the corresponding grounded bounding boxes that can support its This work was done when Ming Li interned at Adobe Research. Preprint. Under review. answer, which requires deeper spatial understanding and reasoning capabilities, and sometimes better instruction-following abilities to correctly provide the grounding information in the desired formats. Despite its importance, there is no established task specifically designed to evaluate the visual text grounding capabilities of MLLMs on TextRich Document Question-Answering tasks and no well-formulated corresponding instruction datasets and baseline methods for the purpose. The absence of such well-formulated task limits the ability to systematically assess and improve the performance of different models in this domain. Thus to bridge this gap, we introduce TRIG, novel Text-Rich Image Grounding task with instruction set for document QA grounding, along with its corresponding benchmark notated as TRIG-Bench. TRIG-Bench consists of 800 question-answer pairs manually collected from DocVQA [50], ChartQA [48], InfographicsVQA [49], and TRINS datasets [86], along with human-inspected ground-truth bounding boxes that support the answer to the corresponding question. The TRIG instruction dataset contains approximately 90k training instances generated and verified by the GPT4o model from the above-mentioned data sources. They provide standardized framework for evaluating the ability of MLLMs to accurately ground their responses to document-related visual questions. Figure 1: An example from InfographicsVQA. Question: What is the resource and expertise level needed for Pinterest? Answer: high. The LLM is expected to generate the answer together with the corresponding grounded bounding boxes that can support its answer, which requires deeper spatial understanding and reasoning and sometimes instruction-following abilities. For text-rich document images, we mainly focus on visual texts as the main grounding target. Considering the supreme performance of modern OCR models [8, 59, 13, 32, 31, 73, 61] and the promising reasoning ability of current LLMs like GPT4o, an OCR-LLM-human interaction pipeline is proposed for the benchmark construction. Specifically, for every given VQA pair, we first utilize PaddleOCR to detect and recognize all the texts. Given all these OCR results, LLMs are asked to judge which bounding boxes support the answer to the corresponding question, followed by another LLM evaluating the correctness of the chosen grounded bounding boxes. The resulting data has already been of good quality, as it is generated and verified by the powerful GPT4o model. Next, more human participants will manually inspect the generated results and add the correct ones to build our benchmark data. In addition, two types of methods are further proposed, including the general instruction-tuningbased method and novel embedding-based method, as potential baselines for TRIG. Specifically, for the instruction-tuning-based method, following the implementation of modern MLLM instruction tuning [38], we train an MLLM with supreme document grounding capability. Moreover, we further propose an efficient plug-and-play embedding-based method, whose inference efficiency is highly promising due to the unnecessity of the iterative token generation process, which largely speeds up the grounding process though with lower performance scores. Our main contributions: We introduce novel task text-rich document image grounding (TRIG), its corresponding benchmark TRIG-Bench and an instruction tuning dataset. The dataset and benchmark are the first of their kind and provide standardized framework for evaluating MLLMs in this domain, filling critical gap in existing research. We propose two types of methods to tackle the challenging visual text grounding task, including the straightforward instruction-tuning-based method and novel efficient embeddingbased method, which not only provides the baseline performances, but also provides more insight into document grounding. We perform comprehensive evaluation of range of existing MLLMs on our new benchmark. Our analysis provides deeper understanding of the limitations and constraints of current MLLMs: Although most of the current MLLMs perform well on well-defined 2 Figure 2: Text-rich document image examples from different source datasets. tasks, they lack the capability to follow customized and complex tasks that require deep understanding and reasoning capabilities."
        },
        {
            "title": "2 Related Works",
            "content": "MLLM for Document Understanding In the area of document understanding, driven by the remarkable capabilities of multimodal large language models in visual-language tasks [63, 12], generative models [88, 78] are gradually replacing traditional supervised techniques [75, 72, 74, 84]. Among them, LLaVAR [88] utilizes GPT-4 [53] with OCR models to generate diverse instructionfollowing data for text-rich images for further training on LLaVA [38]. mPLUG-DocOwl [78], as an evolved version of mPLUG-Owl [80], is designed for the scenario with dense textual content, which achieves outstanding performance in various downstream tasks without OCR. UniDoc [15] mainly addresses potential mismatches between pre-training and fine-tuning data distributions by utilizing PowerPoint presentations to create OCR-related instruction-following data. Additionally, significant research efforts such as UReader [79] and KOSMOS-2.5 [46] also make diverse contributions to the field. MLLM for Visual Grounding Grounding [18] is essential for effective human communication with machines. Currently, grounding datasets have been utilized for vision-language pre-training and improvement for both object-level recognition [26] and language acquisition [47]. Recent studies propose to integrate text and grounding regions into token sequences [76, 44, 66, 69, 68] within language modeling frameworks. Building on such approaches, researchers have developed series of grounded MLLMs, including GPT4ROI [87], Kosmos-2 [55], Shikra [7], PVIT [5], BuboGPT [90], Qwen-VL [1], and Ferret [81]. Despite their impressive performance, these models mainly focus on the grounding of natural image objects, and the grounding of text-rich document images is still under-explored. For the two most related works that also mentioned text grounding on document images, P2G [6] includes the OCR model into the QA pipeline as an information amplifier; TG-Doc [71] utilizes grounding capability to improve models QA capabilities. Both of these works treat the text grounding process as an intermediate step and do not conduct any evaluation of the grounding capability. On the contrary, we entirely focus on the grounding capability and first propose the benchmark for it. Another related work is TextMonkey [42], which describes task that is similar to ours, but there are no examples, illustrations, and systematic evaluations of this ability included. This situation further showcases the value of our benchmark."
        },
        {
            "title": "3 TRIG: Text-Rich Image Grounding",
            "content": "Text-Rich Image Grounding (TRIG) is specifically designed for MLLM-based grounding on text-rich image QA tasks, which is still an underexplored topic. We aim to build the corresponding instruction dataset, benchmark, and evaluation protocols for this task. Specifically, given text-rich document image and question, MLLMs are instructed to generate the bounding boxes of the area that support 3 answers to the given question. As shown in Figure 1, given the image and the question What is the resource and expertise level needed for Pinterest?, the models should generate the bounding boxes directly supporting the final answer. As shown in Figure 2, to ensure the diversity of TRIG, four existing text-rich image datasets are chosen as our data sources, covering wide range of document image types 1 , including DocVQA [50], ChartQA [48], InfographicVQA [49], and TRINS [86]. Examples and detailed illustrations can be found in the supplementary material. The following subsections illustrate the training&benchmark data construction pipeline and evaluation protocols of TRIG. 3.1 Data Construction Our OCR-LLM-human interactive pipeline for training data and benchmark data generation is shown in Figure 3. Step 1 Preprocessing: PaddleOCR2 is utilized to obtain the initial OCR information for its simplicity and promising performance. Step 2 Generation: After obtaining the OCR information, critical issue is how to transmit the OCR information to the LLMs. The common method in the literature is wrapping all the obtained OCR information to the prompt [88, 71], however, we observe severe misalignment between the text information in the prompt and the visual information in the image, leading to unpromising results. Thus, to further align the visual and text information provided in different genres, we innovatively assign every OCR bounding box an identical index, draw every bounding box with its index on the original images, and simultaneously provide all the OCR information with the index in the prompt. By utilizing this strategy, the LLM is able to better align each visual element with its detailed information. Along with the original question and ground truth answer, the LLM is prompted to select the bounding boxes that can support the given answer to the corresponding question. Figure 3: Main Constriction Pipeline. The pipeline contains 4 steps: Preprocessing, Generation, Correction, and Human Evaluation. The benchmark data will go through all of these 4 steps, and the training data will go through the previous 3 steps. Step 3 Correction: Then another Reflection & Rectification module [54, 19, 29, 28] is introduced to examine the correctness of selected bounding boxes. In addition to all the previous information used in Step 2, the previously generated bounding box indices are also provided to the LLM, prompting it to judge if the selected grounded bounding boxes can adequately lead to the given answer. If the judging result is correct, this sample will be kept if it is from the training set, or sent to the next step if it is used for building benchmark. Otherwise, it will be sent back to the previous step until reaching the maximum round and being discarded. The resulting data after the previous 3 steps has already been of good quality as it is generated and verified by the powerful LLM. Step 4 Human Evaluation: The sample sent to step 4 has been evaluated and rectified by LLM, however, its quality might still have discrepancies with the requirements of being benchmark. Thus, to ensure the correctness of our benchmark data, further human inspection is conducted. Specifically, two human participants are invited for the human evaluation. Only those samples that are agreed to be correct for both of the participants can be kept in our benchmark, otherwise, they will be discarded. Finally, for each dataset, 200 QA pairs are manually selected as the ground truth of this benchmark. 1The detailed description of data sources can be found in Appendix A, and more benchmark data examples can be found in Appendix F. 2https://github.com/PaddlePaddle/PaddleOCR 4 Examples from our benchmark and data statistics can be found in the Appendix A, including (1) the average question, answer, and OCR text word counts; (2) the number of bonding boxes provided by OCR models, selected as the ground truth and their ratio; (3) the area of bonding boxes provided by OCR models, selected as the ground truth and their ratio. From these statistics, the characteristics of each dataset can be illustrated, which showcases the variety of our data components and further provides clear understanding of evaluation results. 3.2 Evaluation Settings To simulate various real-world scenarios and test models capability for grounding at different levels, two different evaluation settings are provided, compatible with this benchmark3. OCR-free Grounding (Setting 1) In this setting, only the document image and corresponding question are provided, together with the specific instruction that describes the task requirement and defines the desired format. It represents the hardest level of MLLMs grounding capabilities as it requires MLLMs to answer the question and simultaneously generate the corresponding grounded bounding boxes from scratch. In addition to the measurement of MLLMs grounding capabilities, it also measures their instruction-following abilities. As complex customized task, the instructions for generating grounded bounding boxes have never been seen by MLLMs during training, and it requires deep reasoning capability to correctly follow. Pixel-level IoU (Intersection over Union) is utilized as the evaluation metric. It calculates the overlap between pixels in the predicted bounding boxes and ground truth bounding boxes, normalized by the total number of unique pixels. It is commonly used in image segmentation tasks to assess the accuracy of pixel-wise predictions. OCR-based Grounding (Setting 2) Although the previous setting best aligns with our goal, it is still difficult for most existing models, from proprietary to open-source models. Thus, the setting of OCR-based Grounding is proposed, where an additional OCR model is utilized to facilitate the generation of grounded bounding boxes. Specifically, all the bounding box coordinates and corresponding text content obtained from the OCR model will be wrapped into the prompt for MLLMs as additional information. Under this circumstance, the bounding box generation task is converted into an easier bounding box selection task, in which MLLMs do not need to generate coordinates from scratch but only need to select the proper ones given in the prompt. It is worth noting that this evaluation setting still measures MLLMs visual text grounding capability as they have to align the given OCR information to the image information. In this setting, the IoU score at the instance level, precision, recall, and F1 score are evaluation metrics. Instance-level IoU measures the overlap between the selected grounded bounding boxes and the ground truth bounding boxes, normalized by the total number of unique elements. It evaluates how well the selected bounding boxes match the ground truth ones. Precision measures the proportion of correctly selected elements out of all elements selected by the model. It reflects the accuracy of the models positive predictions. Recall measures the proportion of correctly selected elements out of all actual ground truth elements. It indicates the models ability to capture all relevant elements in the ground truth. F1 Score is the harmonic mean of precision and recall, providing balanced measure that accounts for both false positives and false negatives. Extra Evaluation Setting 3 Although the above two settings represent the most common scenario of grounded bounding box generation for document-level VQA tasks, however, they both have their own flaws: Setting 1 is too difficult for existing MLLMs if not specifically trained on in-domain data, even GPT4o can only have performance under 10%, thus not suitable for the evaluation of most existing MLLMs. Setting 2, on the contrary, provides potential shortcut that models might directly select the bounding boxes according to the text content. Thus, Setting 3 is proposed as the combination of previous settings, where the OCR-generated bounding boxes will be provided without the specific text content. Under this circumstance, the testing model still needs to have spatial understanding to select the correct grounded bounding boxes with no need to generate coordinates from scratch. The evaluation metrics used for this setting are the same as in setting 2. This evaluation setting is mainly used for better understanding of MLLMs performances rather than benchmark setting. 3The detailed evaluation settings and metrics can be found in Appendix C, and the detailed prompts can be found in Appendix 5 Figure 4: The illustrative pipeline of our Embedding-Based Method. The example is from ChartQA. Question: Is the percentage value of STEM segment 52? Answer: Yes. The visualization of ground-truth bounding boxes is presented on the left in green, and the visualization of the generated grounding area (patches) by our embedding-based method is presented on the right in red. The input image will be processed into 32 32 patches before sending it into the MLLM and obtaining the image patch embeddings. After obtaining both the image patch embeddings and text token embeddings, similarity vector with the length of the number of image patches is generated. The higher scores represent the alignment between image and text, whose position will be selected as the grounding patches. For simplicity, the embedding merge and 2-level selection mechanisms are not presented in this figure."
        },
        {
            "title": "4 Proposed Methods",
            "content": "Along with the benchmark and corresponding training data, we propose two baseline methods for this task: (1) the instruction-tuning-based [38, 88, 33] method, (we use the term instruction-based for simplicity), that treats this grounding task as an instruction-following task by iterative next token prediction, and (2) the efficient embedding-based method that directly finds the image patches that have greatest similarities with the input text embeddings. The instruction-based method formulates the generation of bounding boxes into token-prediction framework, in which the inputs include curated prompt and an image, and the output will be sequence of tokens as the response y. For simplicity, we denote each data sample as triplet (Image, x, y) and let pθ() denote the LLMs to be trained, with parameters θ. In this type of method, the bounding box coordinates are also represented by sets of tokens. For simplicity, we denote the text string that represents each bounding box as bi for the ith bounding box. In the instruction tuning setting, pθ is typically fine-tuned by minimizing the next-token-prediction loss on each data (Image, x, y). Different from the previous method, the core of the embedding-based method is the corresponding embeddings of patches of images and text tokens. We denote the embeddings of image patches as Image RlImageD, where lImage is the number of image patches, and is the dimension of embeddings, and we denote the embeddings of text tokens as Text RlTextD, where lText is the number of text tokens. Image represents the ith embedding of the image patches or tokens, respectively. and Text 4.1 Instruction-Based Method For the OCR-free Grounding setting, i.e., evaluation setting 1, the input prompt contains only the original question and the instruction guiding LLMs to generate formatted grounded bounding boxes, which can be simply noted as x. The output of this setting includes the answer to the question concatenated with the texts of corresponding supportive bounding boxes, which can be denoted as y. Thus, the loss function in this setting can be denoted as: Lθ = (cid:88) j= log pθ (cid:0)y jImage, x, <j (cid:1) , = [y; b1; ..; bG] (1) 6 where is the number of golden grounded bounding boxes, and [; ] represents the concatenation of strings. For the OCR-based Grounding setting, i.e. the evaluation setting 2, all the bounding boxes generated by the OCR module will be provided in the input prompt, thus the new input in this setting will be the concatenation of the original question, the instruction guiding LLMs to select the supportive bounding boxes, and all the bounding boxes. The output of this setting is identical to the previous setting, which includes the answer to the question concatenated with the texts of corresponding supportive bounding boxes, which can be denoted as y. Thus, the loss function in this setting can be denoted as: Lθ = (cid:88) j= log pθ (cid:0)y jImage, x, <j (cid:1) , = [x; b1; ..; bN ], = [y; 1; ..; G] (2) where is the number of total bounding boxes generated by the OCR system, is the number of golden grounded bounding boxes, and 1 represents the first selected golden bounding box, which might be different from b1. In this type of method, regular expression will be first utilized to extract the bounding box coordinates before calculating the evaluating scores. 4.2 Embedding-Based Method In addition to the above instruction-based method, motivated by ColPali [14], an embedding-based method is also provided. For instruction-based methods, the instruction-following capability of the LLMs severely affects the final performance, as shown in our experimental results, most of the existing LLMs are not capable of following complex user-customized instructions like ours, leading to unsatisfactory performances. However, the embedding-based method disentangles the grounding capability from the instruction-following capability and makes this process much more efficient than continuous next-token prediction. Thus, it can serve as plug-and-play module adaptive to MLLMs without grounding capability. An illustrative figure for our embedding-based method pipeline is shown in Figure 4. Different from the instruction-based method requiring MLLMs to generate tokens iteratively, which is extremely time-consuming for large number of forward processes, the embedding-based method only needs two times of forward processes, which largely increases the efficiency of the grounding process. Training In the embedding-based method, we fine-tune an MLLM as an encoder and estimate the grounding area using image-text embedding similarities. The text part is the concatenation of the question and the ground-truth answer. To maximize the image-text similarity within the grounding box, we apply binary mask to the patch embeddings, where Mi = 1 for patches overlapping with the grounding box and Mi = 0 otherwise. The training objective is based on modified contextualized )T . late interaction (col) score [23], defined on the interaction matrix X, where Xi,j = Text The masked col-score is then defined as: s(X, M) = (cid:80)n i=1 max{j:Mj =1} Xi,j. Following [14], we use the hardest negative pairs, formed by pairing each image with the text that yields the highest col score from other pairs in the batch. We note the interaction matrices for positive and negative image-text pairs as X+, X, respectively. Finally, we train the MLLM using the InfoCE loss [52], (H Image LInfoCE = log (cid:32) e(s(X+,M)/τ ) e(s(X+,M)/τ ) + e(s(X,1)/τ ) (cid:33) , (3) where 1 is vector of ones. Since there is only one negative pair per sample, the InfoCE loss can be simplified to the Softplus form [17]: = Softplus(s(X, M)) s(X+, 1))). (4) Inference During the inference phase, the main goal is to locate the image patches that have the highest similarities with the text embeddings, aligning with the training objective, the text is the concatenation of the question and the ground-truth answer. Unlike tokens that are naturally discrete, the image patches are generated by cutting the continuous image, thus, the information of one object might be unintentionally contained by several adjacent patches, further causing the low similarities of each patch. To alleviate this phenomenon, novel adjacent patch embedding merging technique is 7 first implemented, in which each image patch embedding Image and its surrounding embeddings: will be updated as the average of it Image = 1 (i) (cid:88)"
        },
        {
            "title": "H Image\nj",
            "content": "jN (i) (5) where Image represents the updated embedding for the ith image patch, (i) represents the set of indices for the ith image patch and its surrounding patches. This adjacent patch embedding merging technique not only completes the semantic information of each patch embedding, but also serves as smoothing that alleviates the influence of single patches with abnormally high similarities. After obtaining the updated patch embeddings, similarity vector RlImage can be calculated by first calculating the similarities between each image patch and text token and then averaging over the text embedding space: Si = 1 lText lText(cid:88) Image , Text , {1, . . . , lImage} (6) k=1 , Text represents the dot product between the image and text embeddings. where Si represents the ith component of the similarity vector, lText is the number of text tokens, and Image The above similarity vector represents the similarity of each image patch to the whole text embeddings. We assume the image patches with higher similarities contain the information mentioned in the text, and thus can be used as the grounding area. Then, the image patches can be found by selecting the top-k highest values. Moreover, to further improve the robustness and alleviate the severe influence of the selection of k, further 2-level selection framework is utilized. In the framework, we first select the image patches with top-5 similarities, and then progressively include later top-k patches if it is in the surroundings of the top-5 patches. After obtaining the image patches, for the OCR-free Grounding setting, the performance is directly calculated by utilizing the pixels in the selected image patches, and for the OCR-based Grounding setting, the bounding boxes that overlap with image patches are selected as the final bounding boxes for calculating the performance."
        },
        {
            "title": "5 Experimental Results",
            "content": "For the instruction-based method, we utilize LLaVA-v1.5-Vicuna-13B [36] as our base model to be fine-tuned. For the embeddingbased method, we utilize PaliGemma-3B [3] as our base model to be fine-tuned. Detailed training configurations, data statistics, and examples can be found in the supplementary material. All experiments are performed on Nvidia A100 GPUs. 5.1 Main Results Testsets Metrics (%) LLaVA-v1.6-Vicuna-13B LLaVA-v1.6-Vicuna-7B Phi3-V DeepSeek-VL-7B-chat Idefics2-8B Qwen-VL CogVLM2-Llama3-19B InternLM-XComposer2-VL-7B Monkey-Chat InternLM-XComposer2-4KHD-7B MiniCPM-Llama3-V 2.5 Chart IoU 0.00 0.00 0.00 0.07 0.21 0.43 0.19 0.15 0.77 1.04 0.44 Doc IoU 0.00 0.00 0.00 0.00 0.01 0.06 0.01 0.20 0.19 0.10 1.40 GPT-4o Ours (Embedding) Ours (Instruction) 3.90 10.51 27.91 1.79 15.02 23.96 Info Trins Avg Avg 0.00 0.00 0.00 0.02 0.06 0.22 0.25 0.26 0.39 0.55 1. IoU 0.00 0.00 0.00 0.00 0.00 0.23 0.66 0.57 0.45 0.14 4.96 13.73 13.88 59.44 5.26 11.82 29.98 IoU 0.00 0.00 0.00 0.02 0.00 0.18 0.16 0.13 0.15 0.90 0. 1.60 7.85 8.61 The overall evaluation results across various MLLMs are presented in Table 1 (Evaluation Setting 1), Table 2 (Evaluation Setting 2). The MLLMs we evaluated are listed as follows: LLaVA-v1.6-Vicuna-13B, LLaVA-v1.6-Vicuna-7B [36, 38, 37], Phi3V[62], DeepSeek-VL-7B-chat [43], Idefics28B [24, 25], Qwen-VL [2], CogVLM2Llama3-19B [67], InternLM-XComposer2VL-7B [10], InternLM-XComposer2-4KHD7B [11], Monkey-Chat [34], MiniCPM-Llama3-V 2.5 [77], and GPT series. Table 1: OCR-free Grounding. Chart, Doc, Info, and Trins represent evaluation results on ChatQA, DocVQA, InfographicsVQA, and TRINS datasets, respectively. IoU represents the pixel-level IoU scores. Avg represents the average IoU score on the 4 datasets, and the ordering of each model is decided by this average score. 8 Testsets Metrics (%) LLaVA-v1.6-Vicuna-13B LLaVA-v1.6-Vicuna-7B Idefics2-8b DeepSeek-VL-7B-chat InternLM-XComposer2-4KHD-7B Monkey-Chat Phi3-V CogVLM2-Llama3-19B Qwen-VL InternLM-XComposer2-VL-7B MiniCPM-Llama3-V 2.5 Ours (Embedding) Ours (Instruction) GPT-4o GPT-3.5-turbo (Without Image) GPT-4 (Without Image) GPT-4o (Without Image) IoU 0.00 0.00 0.00 0.84 1.00 3.58 2.54 1.65 4.36 8.10 7.40 39.97 70.38 83. 8.81 51.58 59.50 ChartQA F1 0.00 0.00 0.00 1.35 2.00 5.38 3.14 2.30 4.33 15.31 12. 57.26 81.58 88.80 14.17 57.29 67.13 0.00 0.00 0.00 2.25 1.00 9.91 5.46 3.68 28.00 9.03 8.40 52.89 73.78 89.24 9.48 53.75 62.84 0.00 0.00 0.00 1.23 1.25 5.05 3.25 2.02 7.27 10.49 9. 49.98 75.78 87.47 10.64 54.34 63.29 DocVQA InfographicsVQA TRINS IoU 0.00 0.00 0.00 0.19 0.25 0.94 0.97 1.56 1.07 7.36 15.78 37.82 73.52 82.14 32.05 52.18 77.83 F 0.00 0.00 0.00 0.19 0.50 1.19 1.22 1.87 1.06 11.28 19.22 40.09 81.67 87.14 40.92 62.79 83.41 0.00 0.00 0.00 1.50 0.25 1.75 2.00 3.58 8.25 8.08 18.42 72.96 75.13 89.50 32.50 53.61 80. 0.00 0.00 0.00 0.34 0.33 1.15 1.21 1.89 1.83 8.73 17.48 48.42 77.02 86.16 35.00 56.28 80.80 IoU 0.00 0.00 0.00 0.00 0.00 0.34 0.40 0.42 0.70 2.32 5.71 25.58 39.23 68. 12.66 47.31 63.34 0.00 0.00 0.00 0.00 0.00 0.30 0.35 0.84 0.71 6.39 9.87 28.68 47.29 79.57 20.45 57.51 72.64 0.00 0.00 0.00 0.00 0.00 2.00 2.08 1.75 4.50 2.85 7.34 49.14 42.86 78.81 14.39 54.34 69.03 F1 0.00 0.00 0.00 0.00 0.00 0.51 0.59 0.71 1.20 3.39 7.14 32.93 43.90 75. 15.75 53.51 68.88 IoU 0.00 0.00 1.17 1.60 3.67 0.00 3.81 6.76 1.51 15.82 54.06 70.01 85.48 89.08 15.81 69.83 71.05 R F1 0.00 0.00 1.17 1.60 6.80 0.00 4.31 7.93 1.83 18.12 62.06 86.38 92.17 96.06 18.75 76.16 80.01 0.00 0.00 3.50 2.00 3.79 0.00 5.00 8.33 3.32 17.32 57. 75.94 79.94 91.53 15.80 71.05 72.54 0.00 0.00 1.69 1.67 4.65 0.00 4.28 7.53 2.08 16.98 57.95 77.32 83.62 92.16 16.62 72.39 74.38 Avg Avg 0.00 0.00 0.47 0.92 1.59 2.01 2.54 3.30 4.50 10.10 23.15 52.84 69.77 85.34 19.58 58.59 71.69 Table 2: OCR-based Grounding. IoU, P, R, F1 represent bounding-box-level IoU score, precision, recall and F1 score. Avg represents the average score on all datasets and evaluation metrics, and the ordering is decided by this score. Settings Testsets Idefics2-8b LlaVa-7b-16 Deepseek-VL-7b-chat InternLM-XP2-4k-7b Phi3V LlaVa-13b-16 CogVLM2-Llama3-19b Qwen-VL Monkey-Chat InternLM-XP2-VL-7b MiniCPM-Llama3 GPT4o GPT-3.5-turbo (w/o Image) GPT-4 (w/o Image) GPT-4o (w/o Image) Evaluation Setting 1 Evaluation Setting 2 Evaluation Setting Chart Doc Info Trins Avg Chart Doc Info Trins Avg Chart Doc Info Trins Avg 9.50 0.00 23.50 6.50 7.50 0.00 24.00 71.50 47.00 23.00 53.00 98.00 - - - 8.50 0.00 2.50 3.00 4.50 0.00 5.00 11.00 38.00 21.00 58.00 94. - - - 3.00 0.00 15.50 13.00 2.50 0.00 8.00 41.00 54.50 22.50 73.00 94.00 - - - 0.00 0.00 0.00 3.00 0.00 0.00 6.00 4.50 12.00 10.00 50.50 98.00 - - - 5.25 0.00 10.38 6.38 3.63 0.00 10.75 32.00 37.88 19.13 58.63 96. - - - 0.00 0.00 4.00 3.50 15.00 0.00 7.50 34.50 45.00 26.00 29.50 100.00 20.00 69.50 84.00 6.50 0.00 3.00 0.50 7.00 0.00 10.50 16.00 12.00 27.50 88.00 99.00 48.00 67.50 93.50 18.50 0.00 0.00 3.50 7.50 0.00 12.00 19.00 10.50 53.50 88.00 100. 38.50 63.00 92.00 3.50 0.00 3.00 10.50 6.00 0.00 9.50 5.50 0.00 20.50 70.50 100.00 21.00 78.50 82.50 7.13 0.00 2.50 4.50 8.88 0.00 9.88 18.75 16.88 31.88 69.00 99.75 31.88 69.63 88.00 0.50 15.50 3.50 5.50 17.50 57.00 8.50 39.00 52.00 25.50 36.00 100. 20.50 2.00 92.00 1.50 19.50 3.00 1.00 2.50 12.50 10.00 14.00 29.50 28.00 81.00 100.00 34.00 2.50 98.00 5.50 9.00 2.00 9.00 8.00 13.00 19.50 17.50 17.50 43.00 83.50 99.00 23.00 12.50 98.50 0.00 14.00 0.00 8.50 13.00 48.50 12.00 2.50 1.00 24.50 80.00 100. 9.50 0.00 93.00 1.88 14.50 2.13 6.00 10.25 32.75 12.50 18.25 25.00 30.25 70.13 99.75 21.75 4.25 95.38 Avg Avg 4.75 4.83 5.00 5.63 7.58 10.92 11.04 23.00 26.58 27.08 55.54 98. 26.81 36.94 91.69 Table 3: The Instruction-Following Rate. This value represents VLMs capability to follow instructions for this task, i.e., generate at least one bounding box regardless of the correctness of the generation. Avg represents the average score on all datasets and settings, and the ordering is decided by this score. As shown in Table 1, all the existing models, including open-source models and the powerful GPT4o, do not perform well for the OCR-free Grounding setting, in which they are required to generate supportive bounding boxes without any additional bounding box information. Under this circumstance, both our proposed methods outperform GPT4o by large margin: our embedding-based method reaches the average IoU of 10.0, and our instruction-based method reaches the average IoU of 29.98, compared with 1.51 for GPT4o. Several conclusions can be made from the evaluation setting 1 results: (1) The capability of existing models to generate grounded bounding boxes from scratch is limited, and thus, the training specifically for this setting is required. (2) Our proposed method is effective, and the quality of our corresponding training dataset is promising as our performance outperforms the powerful GPT4o by large margin. (3) Instruction tuning on reasonably good LLMs can yield much better performance than utilizing the embedding-based method, while the embeddingbased method is much more efficient during inference: Specifically, the instruction method takes an average of 2.37 seconds per document, whereas the Embedding Method takes 0.74 seconds per document, calculated based on inference on 1 Nvidia A100 GPU. As shown in Table 2, for the OCR-based Grounding setting, most of the models can reach much better performance as the OCR information has been provided. However, the performances of open-source models still have large gap with GPT4o. Both of our methods achieve much better performances compared with the existing open-source models while failing to beat GPT4o, which is due to its inherently powerful instruction-following capability for customized instructions. 5.2 Further Findings Finding 1. All existing MLLMs are not good at generating grounded bounding boxes from scratch. As shown in Table 1, even the most powerful GPT4o can only gain an average IoU score of 5.28%, consisting of 13.73% on TRINS and approximately under 4.0% on other datasets. The relatively 9 higher performance on TRINS is due to its special characteristic that the images in TRINS contain the least number of OCR objects while occupying the most area, making it easier to generate intersected bounding boxes. However, when it comes to common information-intense documents, the performances drop dramatically as the ground truth bounding boxes become much smaller. Compared with GPT4o, the IoU values from other open-source models are mostly under 1.0%, which is negligible. GPT4o is able to follow the instructions and generate bounding boxes, though its relatively weak spatial understanding makes the generation not precise enough. However, most of the other open-source models are not able to either understand our instructions or generate reasonable bounding boxes, especially for those MLLMs that get near-zero IoU values. Even if we provide further instructions requiring them to generate bounding boxes that can support their answer, they are not able to understand the instructions nor to follow them. These results reveal critical issue that current MLLMs have relatively weak spatial understanding and are not capable of generating grounded boxes that support their answer from scratch, which makes it potential future direction. Finding 2. Most existing open-source MLLMs are not able to follow customized complex instructions. Table 2 represents the evaluation setting where the OCR information, including bounding boxes and texts, is wrapped into the input to MLLMs, thus making the whole process much simpler bounding box selection process. The performance of GPT4o reaches an astonishingly high value of 85.34% on average compared with the 5.26% on evaluation setting 1, indicating that generating grounded bounding boxes from scratch is hard for GPT4o. In this setting, even the text-only models can achieve reasonably high performance due to the detailed information provided by OCR models while the performances of most of the existing open-source MLLMs still kept low, making it impossible for practical usage. By careful inspection, we observe that these low performances on existing open-source MLLMs are mainly caused by their inability to follow the given instructions: most existing open-source MLLMs will directly generate corresponding answers to the question and ignore the instruction of selecting supporting grounded bounding boxes, potentially due to the overfitting on the format of the training data. To further quantitatively analyze this issue, we introduce another value, the instruction-following rate, defined by the proportion of testing samples for which the MLLMs are able to generate at least one bounding box required in the additional instruction. This value does not measure the correctness of MLLM-generated bounding boxes but only their existence. It directly represents MLLMs instruction-follow ability for this task, i.e., generate at least one bounding box regardless of the correctness. The results of the instruction-following rates are shown in Table 3. GPT4o reaches an average instruction-following rate of 98%. Even if for the OCR-free Grounding setting, in which it can only reach an average IoU score of 5.26%, it still achieves the instruction-following rate of 96%. The comparison between the instruction-following rate and IoU score reveals that (i) GPT4o has strong capability in following customized complex instructions; (ii) The low IoU is caused by GPT4os inability of spatial understanding. On the contrary, the instruction-following rates on existing open-source MLLMs are mostly less than 30.0% or even 10.0%, which means that under most circumstances, they do not understand our instruction and do not generate any bounding boxes, representing not-even-wrong situation. Since the average instruction-following rates are much higher than the grounding metrics, the gaps between them indicate the MLLMs inability to ground. Finding 3. The instruction-following rates are not positively correlated to the difficulty of each evaluation setting. In qualitative analysis, evaluation setting 2 is easier than evaluation setting 3 as it provides more OCR information, making it possible to directly answer the question without real spatial understanding, which is also verified by GPT4os performance gap. However, things are different when it comes to open-source MLLMs. The open-source MLLMs average performance on evaluation setting 2, shown in Table 2, is abnormally lower than setting 3, shown in Table 6, which is contradictory to their natural difficulties. By further inspection, we find this phenomenon is caused by the relatively lower average instruction-following rate on evaluation setting 2. This reveals that the provision of more information, i.e., the OCR texts, does harm to MLLMs ability to correctly follow instructions, thus leading to lower performances. This finding reveals not only the inability of existing open-source MLLMs to follow complex instructions but also their poor robustness: even the useful information might harm the MLLMs instruction-following ability. Our settings reflect both MLLMs instruction-following and grounding ability. Most of the existing open-source models are not able to follow the relatively complex instructions [30] in this task, and even if they correctly follow the instructions, most of them are not able to find the correct grounded bounding boxes that support their answers. Comparing these results with their outstanding performances on standard QA tasks, we reveal this critical issue that most existing open-source MLLMs are potentially overfitted to the standardized tasks, and they still lack instruction-following abilities. 5.3 Ablation Study This section mainly focuses on the ablation experiments toward our embeddingbased methods on the OCR-free Grounding setting since it directly measures the correctness of selected image patches for grounding4. Testsets Metrics (%) No Similarity Merging 3 3 Similarity Merging 5 5 Similarity Merging 7 7 Similarity Merging Chart IoU 8.93 10.17 10.01 9.47 Doc IoU 9.27 11.87 11.32 10.87 Info Trins IoU 6.72 7.66 7.32 6.89 IoU 14.14 15.65 15.14 14. Avg Avg 9.77 11.34 10.95 10.50 7.11 8.16 7.56 6.98 6.50 6.00 9.02 10.51 10.63 10.41 10.18 9.89 9.95 10.37 9.92 9.50 8.95 8. 12.48 14.10 13.77 12.75 12.05 11.19 6.56 9.42 11.25 12.42 13.22 13.91 Top-5 Patches Top-10 Patches Top-15 Patches Top-20 Patches Top-25 Patches Top-30 Patches Effect of Embedding Merging. We evaluate the effectiveness of utilizing our proposed embedding merging techniques. As shown in Table 4, without embedding merging, the average performance is the lowest among the settings in which the embedding merging is utilized. In the table, 3 3 Similarity Merging represents merging the similarities within the 3 3 surrounding window of each patch. Among all the experimental settings, the utilization of 3 3 embedding merging reaches the best performance. The results are reasonable since embedding merging with smaller window size can alleviate the randomness while larger window might smooth the values too much, causing the similarities to be too closed to distinguish. Table 4: Ablation studies of our embedding-based method on OCR-free grounding setting. The results illustrate the effectiveness of our proposed embedding merging and 2-level selection mechanism. Top-5 + Top-10 Patches Top-5 + Top-15 Patches Top-5 + Top-20 Patches Top-5 + Top-25 Patches Top-5 + Top-30 Patches 9.34 11.13 12.24 13.13 13.88 14.90 15.38 15.26 15.24 15.02 10.88 10.96 10.86 10.70 10.51 10.91 11.47 11.64 11.78 11.82 8.53 8.40 8.20 8.06 7.85 Effect of 2-Level Selection. As shown in Table 4, the settings notated as Top-k Patches represent directly selecting the top-k patches as the grounded area, while settings notated as Top-k1 + Top-k2 Patches represent first selecting the top-k1 patches and then adding the Top-k2 patches gradually if they appear in the surrounding of top-k1 patches. From the results, several findings can be observed: (1) When utilizing only the top-k patches without the 2-level selection, the performance first grows and then starts to decline gradually. This phenomenon indicates that when more patches are selected, the growth of the union is faster than the intersection. (2) When the 2-level selection is utilized, the performance is consistently higher than without using it, and no clear decline trend is observed. This phenomenon indicates that the use of this selection metric can largely alleviate the selection of unimportant patches."
        },
        {
            "title": "6 Further Discussions",
            "content": "6.1 Practical Use Case This text-rich document grounding capability helps users understand the source of the information provided in the answers, building users trust in the system. When users ask questions to AI assistants about documents, they need to verify the answer. Without grounding, they may look through the whole document, which damages the user experience. However, if grounded boxes are given, users can verify the answer with only glimpse, largely accelerating users reliance on AI 4More results and illustrative figures can be found in Appendix 11 assistants for their daily work. Besides, text grounding has been deployed in real-world products, while visual text grounding remains challenging. Thus, this benchmark is practically useful. 6.2 Differences Between Common Text Localization Task Our task focuses on text grounding in document images, which is different from text localization and orthogonal to all existing tasks: Text localization requires the model to provide the location of specific visual text or extract from the page. For example, the text localization used in KOSMOS-2.5 [46] contains Document-Level Text Recognition and Image-to-Markdown Generation, which are totally different from ours. On the contrary, our visual text grounding aims to provide the answer with its sources from the image. The key difference lies in that our prediction of text position is coupled with the understanding of image content."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we introduce TRIG, novel task designed to evaluate and enhance the visual text grounding capabilities of MLLMs in text-rich document images. We first build TRIG-Bench, comprehensive benchmark and large-scale instruction tuning dataset. Our evaluation reveals significant limitations in the visual text grounding capabilities of existing MLLMs, particularly in their ability to handle complex document layouts and textual content. Hence, we propose two simple and effective instruction-tuning-based and embedding-based methods that demonstrate substantial improvements over existing models. We believe this task is still an under-explored task that has not been well solved and might require more attention from the community."
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. 2023. [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [3] Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bošnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. Paligemma: versatile 3b vlm for transfer, 2024. [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc., 2020. [5] Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li, Maosong Sun, and Yang Liu. Positionenhanced visual instruction tuning for multimodal large language models. arXiv preprint arXiv:2308.13437, 2023. [6] Jiaxing Chen, Yuxuan Liu, Dehu Li, Xiang An, Weimo Deng, Ziyong Feng, Yongle Zhao, and Yin Xie. Plug-and-play grounding of reasoning in multimodal large language models, 2024. [7] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. 12 [8] Xiaoxue Chen, Lianwen Jin, Yuanzhi Zhu, Canjie Luo, and Tianwei Wang. Text recognition in the wild: survey. ACM Comput. Surv., 54(2), March 2021. [9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. [10] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024. [11] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Zhe Chen, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Kai Chen, Conghui He, Xingcheng Zhang, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2-4khd: pioneering large visionlanguage model handling resolutions from 336 pixels to 4k hd. arXiv preprint arXiv:2404.06512, 2024. [12] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-E: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. [13] Yongkun Du, Zhineng Chen, Caiyan Jia, Xiaoting Yin, Tianlun Zheng, Chenxia Li, Yuning Du, and Yu-Gang Jiang. Svtr: Scene text recognition with single visual model. 2022. [14] Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, and Pierre Colombo. Colpali: Efficient document retrieval with vision language models, 2024. [15] Hao Feng, Zijian Wang, Jingqun Tang, Jinghui Lu, Wengang Zhou, Houqiang Li, and Can Huang. UniDoc: universal large multimodal model for simultaneous text detection, recognition, spotting and understanding. arXiv preprint arXiv:2308.11592, 2023. [16] Akash Ghosh, Arkadeep Acharya, Sriparna Saha, Vinija Jain, and Aman Chadha. Exploring the frontier of vision-language models: survey of current methodologies and future directions, 2024. [17] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 315323. JMLR Workshop and Conference Proceedings, 2011. [18] Stevan Harnad. The symbol grounding problem. Physica D: Nonlinear Phenomena, 42(13):335346, 1990. [19] Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 10511068, Singapore, December 2023. Association for Computational Linguistics. [20] Yue Huang, Qihui Zhang, Philip S. Y, and Lichao Sun. Trustgpt: benchmark for trustworthy and responsible large language models, 2023. [21] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):138, 2023. [22] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17801790, 2021. 13 [23] Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 3948, 2020. [24] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. Obelics: An open web-scale filtered dataset of interleaved image-text documents, 2023. [25] Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models?, 2024. [26] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1096510975, 2022. [27] Ming Li, Han Chen, Chenguang Wang, Dang Nguyen, Dianqi Li, and Tianyi Zhou. Ruler: Improving llm controllability by rule-based data recycling, 2024. [28] Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Jiuxiang Gu, and Tianyi Zhou. Selective reflection-tuning: Student-selected data recycling for LLM instruction-tuning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics ACL 2024, pages 1618916211, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. [29] Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, and Tianyi Zhou. Reflection-tuning: Recycling In NeurIPS 2023 Workshop on Instruction Tuning and data for better instruction-tuning. Instruction Following, 2023. [30] Ming Li, Pei Chen, Chenguang Wang, Hongyu Zhao, Yijun Liang, Yupeng Hou, Fuxiao Liu, and Tianyi Zhou. Mosaic it: Enhancing instruction tuning with data mosaics, 2024. [31] Ming Li, Bin Fu, Han Chen, Junjun He, and Yu Qiao. Dual relation network for scene text recognition. IEEE Transactions on Multimedia, 25:40944107, 2023. [32] Ming Li, Bin Fu, Zhengfu Zhang, and Yu Qiao. Character-aware sampling and rectification for scene text recognition. IEEE Transactions on Multimedia, 25:649661, 2023. [33] Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. From quantity to quality: Boosting LLM performance with selfguided data selection for instruction tuning. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 75957628, Mexico City, Mexico, June 2024. Association for Computational Linguistics. [34] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024. [35] Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng. survey on hallucination in large vision-language models. arXiv preprint arXiv:2402.00253, 2024. [36] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. [37] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. [38] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 14 [39] Michael Xieyang Liu, Frederick Liu, Alexander J. Fiannaca, Terry Koo, Lucas Dixon, Michael Terry, and Carrie J. Cai. we need structured output: Towards user-centered constraints on large language model output. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, CHI 24. ACM, May 2024. [40] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: survey and guideline for evaluating large language models alignment, 2024. [41] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, and Lichao Sun. Sora: review on background, technology, limitations, and opportunities of large vision models, 2024. [42] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai. Textmonkey: An ocr-free large multimodal model for understanding document, 2024. [43] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world vision-language understanding, 2024. [44] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: unified model for vision, language, and multi-modal tasks. In The Eleventh International Conference on Learning Representations, 2022. [45] Ruotian Luo and Gregory Shakhnarovich. Comprehension-guided referring expressions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 71027111, 2017. [46] Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, et al. KOSMOS-2.5: multimodal literate model. arXiv preprint arXiv:2309.11419, 2023. [47] Ziqiao Ma, Jiayi Pan, and Joyce Chai. World-to-words: Grounded open vocabulary acquisition through fast mapping in vision-language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 524544, 2023. [48] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. [49] Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and C.V. Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 16971706, January 2022. [50] Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: dataset for vqa on In 2021 IEEE Winter Conference on Applications of Computer Vision document images. (WACV), pages 21992208, 2021. [51] Varun Nagaraja, Vlad Morariu, and Larry Davis. Modeling context between objects In Computer VisionECCV 2016: 14th European for referring expression understanding. Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 792807. Springer, 2016. [52] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. [53] OpenAI. Gpt-4 technical report, 2023. [54] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies, 2023. 15 [55] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. [56] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Qixiang Ye, and Furu Wei. Grounding multimodal large language models to the world. In The Twelfth International Conference on Learning Representations, 2024. [57] Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 26412649, 2015. [58] Vipula Rawte, Amit Sheth, and Amitava Das. survey of hallucination in large foundation models. arXiv preprint arXiv:2309.05922, 2023. [59] Nishant Subramani, Alexandre Matton, Malcolm Greaves, and Adrian Lam. survey of deep learning approaches for ocr and document understanding, 2021. [60] Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bertie Vidgen, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, Joaquin Vanschoren, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S. Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, William Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, and Yue Zhao. Trustllm: Trustworthiness in large language models, 2024. [61] Yu Sun, Dongzhan Zhou, Chen Lin, Conghui He, Wanli Ouyang, and Han-Sen Zhong. Locr: Location-guided transformer for optical character recognition, 2024. [62] MS Phi-3 Team. Phi-3 technical report: highly capable language model locally on your phone, 2024. [63] Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, and Steven CH Hoi. Plugand-Play VQA: Zero-shot VQA by conjoining large pretrained models with zero training. arXiv preprint arXiv:2210.08773, 2022. [64] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. [65] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. [66] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through simple sequence-to-sequence learning framework. In International Conference on Machine Learning, pages 2331823340. PMLR, 2022. [67] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models, 2023. [68] Xiyao Wang, Jiuhai Chen, Zhaoyang Wang, Yuhang Zhou, Yiyang Zhou, Huaxiu Yao, Tianyi Zhou, Tom Goldstein, Parminder Bhatia, Furong Huang, et al. Enhancing visual-language modality alignment in large vision language models via self-improvement. arXiv preprint arXiv:2405.15973, 2024. [69] Xiyao Wang, Zhengyuan Yang, Linjie Li, Hongjin Lu, Yuancheng Xu, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Scaling inference-time search with vision value model for improved visual comprehension. arXiv preprint arXiv:2412.03704, 2024. [70] Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, et al. Mementos: comprehensive benchmark for multimodal large language model reasoning over image sequences. arXiv preprint arXiv:2401.10529, 2024. [71] Yonghui Wang, Wengang Zhou, Hao Feng, Keyi Zhou, and Houqiang Li. Towards improving document understanding: An exploration on text-grounding via mllms, 2023. [72] Zilong Wang, Yiheng Xu, Lei Cui, Jingbo Shang, and Furu Wei. LayoutReader: Pre-training of text and layout for reading order detection. arXiv preprint arXiv:2108.11591, 2021. [73] Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, Chunrui Han, and Xiangyu Zhang. General ocr theory: Towards ocr-2.0 via unified end-to-end model, 2024. [74] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, et al. LayoutLMv2: Multi-modal pre-training for visually-rich document understanding. arXiv preprint arXiv:2012.14740, 2020. [75] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. LayoutLM: Pre-training of text and layout for document image understanding. In Proceedings of ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 11921200, 2020. [76] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language modeling. 2022. [77] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint 2408.01800, 2024. [78] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, et al. mPLUG-DocOwl: Modularized multimodal large language model for document understanding. arXiv preprint arXiv:2307.02499, 2023. [79] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, et al. UReader: Universal OCR-free visually-situated language understanding with multimodal large language model. arXiv preprint arXiv:2310.05126, 2023. [80] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mPLUG-Owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. [81] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023. [82] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. In The Twelfth International Conference on Learning Representations, 2024. [83] Licheng Yu, Hao Tan, Mohit Bansal, and Tamara Berg. joint speaker-listener-reinforcer model for referring expressions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 72827290, 2017. [84] Yuechen Yu, Yulin Li, Chengquan Zhang, Xiaoqiang Zhang, Zengyuan Guo, Xiameng Qin, Kun Yao, Junyu Han, Errui Ding, and Jingdong Wang. StrucTexTv2: Masked visual-textual prediction for document image pre-training. arXiv preprint arXiv:2303.00289, 2023. [85] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [86] Ruiyi Zhang, Yanzhe Zhang, Jian Chen, Yufan Zhou, Jiuxiang Gu, Changyou Chen, and Tong Sun. Trins: Towards multimodal language models that can read. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2258422594, June 2024. [87] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601, 2023. [88] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. LLaVAR: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint arXiv:2306.17107, 2023. [89] Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, and Joyce Chai. Groundhog: Grounding large language models to holistic segmentation, 2024. [90] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang. Bubogpt: Enabling visual grounding in multi-modal llms. arXiv preprint arXiv:2307.08581, 2023."
        },
        {
            "title": "A Data Information",
            "content": "A.1 Data Sources To ensure the diversity of our benchmark data, four existing document datasets are chosen as our data sources, covering wide range of document image types, including DocVQA [50], ChartQA [48], InfographicVQA [49], and TRINS [86]. DocVQA [50] is designed to test the capabilities of VQA systems in the domain of document understanding. The dataset comprises diverse collection of document images, including invoices, forms, receipts, reports, letters, historical documents, etc. The questions are crafted to test various aspects of document understanding, such as information retrieval, layout interpretation, and relational reasoning. Our training data source is the whole DocVQA training subset. Due to the unavailability of the test subset, our benchmark data is selected from the validation subset. ChartQA [48] is designed to assess the proficiency of VQA systems in interpreting data visualizations. It features wide array of chart types, including bar charts, pie charts, line graphs, scatter plots, histograms, and box plots. It is particularly focused on the task of understanding and reasoning about quantitative information presented in visual formats. The whole ChartQA training subset is used as our training data source and our benchmark data is selected from the test subset. InfographicVQA [49] is dataset that explores the realm of visually rich and diverse infographics, such as posters, flyers, advertisements, educational infographics, and infographic-style presentations. The diverse nature of the infographics in this dataset, coupled with the complexity of the questions, makes it crucial resource for developing VQA systems capable of understanding and interacting with complex visual information. The whole InfographicVQA training subset is used as our training data source. Due to the unavailability of the test subset, our benchmark data is selected from the validation subset. TRINS [86] is novel dataset designed to enhance the ability of multimodal language models to read and understand text-rich images. This dataset addresses the limitation of existing visuallytuned models, which often struggle with textual comprehension within images. It includes text-rich images, such as posters and book covers, providing significant challenge due to the high density and complexity of text present in each image. The dataset supports various tasks, including visual captioning, visual question answering, and text-rich image generation, we mainly utilize its VQA subset. Our benchmark data is randomly sampled from the whole dataset, and the remaining data is used as our training data source. A.2 Data Statistics Total Question # Total Image # Avg Question Len Avg Answer Len Avg OCR Text Len Avg OCR Box # Avg GT Box # Avg GT Box Ratio Avg OCR Area (%) Avg GT Area (%) Avg GT Area Ratio Chart 200 171 11.04 1.39 2.7 Doc 200 190 9.66 2.94 4.73 Info 200 199 12.42 1.93 3.02 Trins Total 200 199 11.69 19.48 3. 800 759 11.20 6.44 3.36 26.15 2.67 102.26 2.78 11.83% 4.88% 3.72% 37.74% 14.54% 47.66 2.41 53.30 1. 8.94 2.45 18.29 12.56 0.80 0.53 8.59% 5.10% 2.89% 42.69% 14.82% 19.75 0.79 16.61 2.05 15.82 6.09 Table 5: Benchmark Data Statistics. Total Question # represents the unique question number from each dataset. Total Image # represents the Unique image number. Other statistics are averaged on each dataset. The benchmark data statistics are presented in Table 5. Avg Question Len, Avg Answer Len, and Avg OCR Text Len calculate the average question, answer, and OCR text word counts. Avg OCR Box #, Avg GT Box, and Avg GT Box Ratio calculate the number of bonding boxes provided by OCR models, selected as the ground truth and their ratio. Avg OCR Area (%), Avg GT Area (%) and Avg GT Area Ratio calculate the area of bonding boxes provided by OCR 19 Figure 5: Illustrations on Evaluation Settings. In evaluation setting 1, no OCR model is used, representing the hardest scenario. While in settings 2 & 3, an additional OCR model is utilized to facilitate LLM on grounding information generation. The Instruction in the prompt describes the requirement of generating grounded bounding boxes and defines the desired format. models, selected as the ground truth and their ratio. From these statistics, the characteristics of each dataset can be illustrated, which showcases the variety of our data components and further provides clear understanding of evaluation results."
        },
        {
            "title": "B Training Configurations",
            "content": "Our training data contains 22, 363 samples from CharQA, 29, 094 samples from DocVQA, 19, 840 samples from InfographisVQA, and 24, 885 samples from TRINS. All experiments are performed on Nvidia A100 GPUs. We utilize 8 GPUs for training and 1 for inference. For the instruction-based method, we utilize LLaVA-v1.5-Vicuna-13B [36] as our base model to be finetuned, based on the codebase from Vicuna [9] and LLaVA [38]. We train our model with batch size of 128 and max token length of 8192. The maximum learning rate is set to 2 105 with warmup rate of 0.03 for 3 epochs. For the embedding-based method, we utilize PaliGemma-3B [3] as our base model to be finetuned, based on the codebase from ColPali [14]. We train our model for 4 epochs with batch size of 32 and learning rate of 5e 5, using the AdamW optimizer. LoRA adapters with rank of 32 are added to all MLP layers in the LLM."
        },
        {
            "title": "C Detailed Evaluation Settings and Metrics",
            "content": "To simulate various real-world scenarios and test models capability for grounding at different levels, three different evaluation settings are provided compatible with this benchmark. C.1 Evaluation Setting 1 (OCR-free Grounding) In this setting, as shown in Figure 5 (a), only the document image and corresponding question are provided for the MLLMs, together with the specific instruction that describes the task requirement and defines the desired format. It represents the hardest level of MLLMs grounding capabilities as it requires MLLMs to answer the question and simultaneously generate the corresponding grounded bounding boxes from scratch. In addition to the measurement of MLLMs grounding capabilities, it also measures their instruction-following abilities. As complex customized task, the instructions for generating grounded bounding boxes have never been seen by MLLMs during training, and it requires deep reasoning capability to correctly follow. Pixel-level IoU (Intersection over Union) is utilized as the evaluation metric. It calculates the overlap between pixels in the predicted bounding boxes and ground truth bounding boxes, normalized by the total number of unique pixels. It is commonly used in image segmentation tasks to assess the accuracy of pixel-wise predictions: IoUpixel = 1 N (cid:88) i=1 Predi GroundTruthi Predi GroundTruthi (7) 20 where represents the total number of testing samples, Predi represents the pixels within the predicted grounded bounding boxes of ith testing sample and GroundTruthi represents the pixels within the groundtruth bounding boxes. C.2 Evaluation Setting 2 (OCR-based Grounding) Although the previous setting aligns with our aim the best, it is too hard for all the existing models, from proprietary models to open-source models. Thus, further evaluation settings are proposed. In this setting, as shown in Figure 5 (b), an additional OCR model is utilized to facilitate the generation of grounded bounding boxes. Specifically, all the bounding box coordinates and corresponding text content obtained from the OCR model will be wrapped into the prompt for MLLMs as additional information. Under this circumstance, the bounding box generation task is converted into an easier bounding box selection/retrieval task, in which MLLMs do not need to generate coordinates from scratch but only need to select the proper ones given in the prompt. It is worth noting that this evaluation setting still measures MLLMs grounding capability as they have to align the given OCR information to the image information. In this setting, instance-level IoU score, precision, recall, and F1 score are utilized as the evaluation metrics. Instance-level IoU measures the overlap between the retrieved grounded bounding boxes and the ground truth bounding boxes, normalized by the total number of unique elements. It evaluates how well the retrieved bounding boxes match the ground truth ones: IoUinst = 1 (cid:88) i=1 Predi GroundTruthi Predi GroundTruthi (8) where represents the number of testing samples, Predi represents the bounding boxes selected from the prompt of ith testing sample and GroundTruthi represents the groundtruth bounding boxes. Precision measures the proportion of correctly retrieved elements out of all elements retrieved by the model. It reflects the accuracy of the models positive predictions: Precision = 1 (cid:88) i=1 Predi GroundTruthi Predi (9) Recall measures the proportion of correctly retrieved elements out of all actual ground truth elements. It indicates the models ability to capture all relevant elements in the ground truth. Recall = 1 (cid:88) i=1 Predi GroundTruthi GroundTruthi (10) F1 Score is the harmonic mean of precision and recall, providing balanced measure that accounts for both false positives and false negatives. i=1 where Precisioni and Recalli represent the precision and recall of ith testing sample. F1 = (cid:88) 1 2 Precisioni Recalli Precisioni + Recalli (11) C.3 Extra Evaluation Setting 3 The evaluation metrics used for this setting are the same as in setting 2. The performances of different models on Evaluation Setting 3 is shown in Table 6."
        },
        {
            "title": "D Detailed Ablation Study",
            "content": "In this section, the detailed ablation study on the Effect of 2-Level Selection is provided: Table 7 presents the performance comparisons between the naive top-K selection and our 2-level selection method, and Figure 6 presents the performance changes with or without our 2-level selection 21 Metrics Testsets Idefics2-8b DeepSeek-VL-7B-chat InternLM-XComposer2-4KHD-7B CogVLM2-Llama3-19B Phi3-V LLaVA-v1.6-Vicuna-7B InternLM-XComposer2-VL-7B Qwen-VL Monkey-chat MiniCPM-Llama3-V 2.5 LLaVA-v1.6-Vicuna-13B IoU 0.00 0.38 0.23 0.36 1.12 0.78 2.03 4.25 6.15 2.40 6.21 0.00 0.38 0.26 0.37 1.36 0.93 3.51 4.21 6.46 3.61 6.49 GPT-4o Ours (Embedding) GPT-3.5-turbo (Without Image) GPT-4 (Without Image) GPT-4o (Without Image) 30.23 39. 2.58 0.43 16.70 37.86 57.26 3.03 0.75 22.54 ChartQA DocVQA InfographicsVQA TRINS F1 0.00 3.00 0.83 3.17 5.92 5.56 5.29 35.82 40.06 3.92 35.55 36.58 52.89 3.30 0.45 19. 0.00 0.67 0.36 0.64 1.81 1.35 3.00 7.35 10.30 3.04 10.01 35.45 49.98 3.02 0.56 19.80 IoU 0.00 0.08 0.00 0.31 0.69 0.77 2.35 1.14 1.51 4.86 1.72 0.00 0.08 0.00 0.31 0.69 0.75 3.00 1.13 1.50 6.02 1.72 21.35 37.82 2.45 0.08 11.41 24.38 40.09 2.53 0.11 11.64 0.00 1.00 0.00 3.50 1.50 9.33 2.50 11.75 18.58 5.83 6.00 26.33 72.96 3.83 0.67 13.63 F1 0.00 0.16 0.00 0.55 0.81 1.35 2.67 2.02 2.72 5.21 2.20 IoU 0.00 0.03 0.22 0.42 0.32 0.50 0.48 0.80 1.25 0.48 0.76 0.00 0.03 0.26 0.46 0.45 0.52 1.14 0.82 1.26 0.81 0.74 23.88 48.42 2.68 0.15 13.02 6.20 25. 10.69 28.68 0.00 0.01 3.41 0.00 0.01 4.58 0.00 0.13 2.42 2.50 2.29 3.20 2.83 7.60 11.92 1.63 5.11 8.62 49. 0.00 0.52 5.90 F1 0.00 0.05 0.38 0.73 0.56 0.88 0.77 1.46 2.25 0.70 1.26 8.33 32.93 0.00 0.01 4.09 IoU 0.00 0.00 2.13 3.54 6.05 3.55 8.70 0.89 0.70 23.82 15.58 59.81 70.01 2.44 0.00 27.56 F 0.00 0.00 5.13 4.60 6.38 3.56 12.93 0.89 0.70 41.41 16.54 79.57 86.38 3.38 0.00 46.45 0.00 0.00 2.43 7.27 11.50 11.83 10.10 2.50 1.00 27.08 39.08 63.69 75.94 2.56 0.00 28. 0.00 0.00 2.97 4.67 7.17 5.08 10.26 1.20 0.79 29.46 20.85 67.24 77.32 2.73 0.00 32.60 Avg Avg 0.00 0.37 1.10 2.09 3.04 3.12 4.47 5.24 6.70 10.02 10. 33.76 52.84 2.16 0.23 17.66 Table 6: Evaluation Setting 3. IoU, P, R, F1 represent bounding-box-level IoU score, precision, recall and F1 score. Avg represents the average score on all datasets and evaluation metrics and the ordering is decided by this score. Testsets Metrics (%) Top-5 Patches Top-10 Patches Top-15 Patches Top-20 Patches Top-25 Patches Top-30 Patches Top-35 Patches Top-40 Patches Top-45 Patches Top-50 Patches Top-3 + Top-5 Patches Top-3 + Top-10 Patches Top-3 + Top-15 Patches Top-3 + Top-20 Patches Top-3 + Top-25 Patches Top-3 + Top-30 Patches Top-3 + Top-35 Patches Top-3 + Top-40 Patches Top-3 + Top-45 Patches Top-3 + Top-50 Patches Top-5 + Top-5 Patches Top-5 + Top-10 Patches Top-5 + Top-15 Patches Top-5 + Top-20 Patches Top-5 + Top-25 Patches Top-5 + Top-30 Patches Top-5 + Top-35 Patches Top-5 + Top-40 Patches Top-5 + Top-45 Patches Top-5 + Top-50 Patches Chart IoU 9.95 10.37 9.92 9.50 8.95 8.45 7.90 7.53 7.14 6.82 9.97 10.89 11.07 11.01 10.89 10.70 10.39 10.27 10.13 9. 9.95 10.88 10.96 10.86 10.70 10.51 10.16 10.00 9.84 9.68 Doc IoU 12.48 14.10 13.77 12.75 12.05 11.19 10.37 9.77 9.17 8.62 12.58 15.03 15.64 15.60 15.78 15.60 15.35 15.22 15.03 14.82 12.48 14.90 15.38 15.26 15.24 15.02 14.73 14.57 14.38 14. Info Trins IoU 7.11 8.16 7.56 6.98 6.50 6.00 5.59 5.35 5.09 4.82 7.26 8.76 8.86 8.72 8.68 8.47 8.38 8.32 8.32 8.16 7.11 8.53 8.40 8.20 8.06 7.85 7.67 7.65 7.62 7.44 IoU 6.56 9.42 11.25 12.42 13.22 13.91 14.51 15.06 15.60 15.94 6.18 8.54 10.06 11.01 11.80 12.47 13.00 13.36 13.84 14.16 6.56 9.34 11.13 12.24 13.13 13.88 14.42 14.91 15.45 15.82 Avg Avg 9.02 10.51 10.63 10.41 10.18 9.89 9.60 9.43 9.25 9. 9.00 10.80 11.41 11.58 11.79 11.81 11.78 11.79 11.81 11.78 9.02 10.91 11.47 11.64 11.78 11.82 11.75 11.79 11.82 11.78 Table 7: Detailed ablation studies of our embedding-based method on OCR-free grounding setting. The results illustrate the effectiveness of the 2-level selection mechanism. mechanism. As shown in the figure, when no 2-level selection mechanism is utilized, the performance declines consistently after = 15, as more random patches are included, making the grounding area unstable. On the contrary, when our 2-level selection mechanism is utilized, only the patches in the surroundings of the top-K1 patches are included, which directly removes the potentially noisy patches. To conclude, the utilization of the 2-level selection mechanism avoids the performance decline when more patches are selected, making the performances more stable and getting better performances. 22 Figure 6: The visualization of detailed ablation studies of our embedding-based method on OCRfree grounding setting for the 2-level selection mechanism. The utilization of the 2-level selection mechanism avoids the performance decline when more patches are selected, making the performances more stable and getting better performances. Prompt for Grounded Bounding Box Generation System Prompt You are helpful and precise assistant in finding the grounding bounding boxes given the question-answer pair and the poster image. User Prompt Question: {question} Answer: {answer} Above is the question and answer for given poster. Sentence-level bounding boxes with indexes are provided in the poster, and detailed indexes with corresponding texts are also provided below: {idx-text pairs} Can you provide me with the indexes of bounding boxes that can accurately and sufficiently lead to the answer? Make sure you check both the poster image and the text provided above. Please provide the index in the first line, use comma to separate different indexes if more than one, and do not output anything else except for indexes or commas. Please then provide the reason in the following lines on why you choose those bounding boxes. Figure 7: The prompt we used to request GPT4o to generate the grounded bounding boxes that support the answer to the question. Generation & Evaluation Prompts E.1 Grounding Generation Prompts The prompt we use for generating grounded bounding boxes is shown in Figure 7 and the prompt we use for evaluating the correctness of the previously generated bounding boxes. 23 Prompt for Grounded Bounding Box Rectification System Prompt You are helpful and precise assistant in analyzing the grounding bounding boxes given the question-answer pair and the poster image. User Prompt Question: {question} Answer: {answer} Above is the question and answer for given poster. Sentence-level bounding boxes with indexes are provided in the poster, and detailed indexes with corresponding texts are also provided below: {idx-text pairs} Do you think the bounding boxes with index {idx-list} can accurately and sufficiently lead to the answer to the given question? Please output YES or NO in the first line, then provide the reason in the following lines. Figure 8: The prompt we used to request GPT4o to evaluate the grounded bounding boxes generated in the previous step. E.2 Evaluation Prompts In order to alleviate the influences of prompts when testing on our benchmark, 3 different prompts with diverse formatting requirements [39, 30, 27] are proposed for each evaluation setting, resulting in total of 9 different evaluation prompts. For each evaluation setting, we present the best results across the 3 different prompts as shown in Figure 9. The main difference between different prompts in the same evaluation setting is the required format for the generation or selection of the grounded bounding boxes. Specifically, prompts with #1 utilize the CSS format, which is widely used for pretraining; prompts with #2 utilize the most naive format containing the necessary information in list; prompts with #3 are similar to #2 but utilize the relative coordinates."
        },
        {
            "title": "F Benchmark Data Examples",
            "content": "The benchmark data examples are visualized in Figure 10 for ChatQA, Figure 11 for DocVQA, Figure 12 for InfographicsVQA, and Figure 13 for TRINS. The questions and answers are given in text form, the supported grounded bounding boxes are directly visualized in the images. The samples from different sources show the diversity of our benchmark data. 24 Figure 9: The evaluation prompts for different evaluation settings. 25 Figure 10: Benchmark data examples from ChatQA. The grounded bounding boxes have already been visualized in the original image for better illustration. Figure 11: Benchmark data examples from DocVQA. The grounded bounding boxes have already been visualized in the original image for better illustration. 27 Figure 12: Benchmark data examples from InfographicsVQA. The grounded bounding boxes have already been visualized in the original image for better illustration. 28 Figure 13: Benchmark data examples from TRINS. The grounded bounding boxes have already been visualized in the original image for better illustration."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "University at Buffalo",
        "University of Maryland"
    ]
}