{
    "paper_title": "Spanning the Visual Analogy Space with a Weight Basis of LoRAs",
    "authors": [
        "Hila Manor",
        "Rinon Gal",
        "Haggai Maron",
        "Tomer Michaeli",
        "Gal Chechik"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual analogy learning enables image manipulation through demonstration rather than textual description, allowing users to specify complex transformations difficult to articulate in words. Given a triplet $\\{\\mathbf{a}$, $\\mathbf{a}'$, $\\mathbf{b}\\}$, the goal is to generate $\\mathbf{b}'$ such that $\\mathbf{a} : \\mathbf{a}' :: \\mathbf{b} : \\mathbf{b}'$. Recent methods adapt text-to-image models to this task using a single Low-Rank Adaptation (LoRA) module, but they face a fundamental limitation: attempting to capture the diverse space of visual transformations within a fixed adaptation module constrains generalization capabilities. Inspired by recent work showing that LoRAs in constrained domains span meaningful, interpolatable semantic spaces, we propose LoRWeB, a novel approach that specializes the model for each analogy task at inference time through dynamic composition of learned transformation primitives, informally, choosing a point in a \"space of LoRAs\". We introduce two key components: (1) a learnable basis of LoRA modules, to span the space of different visual transformations, and (2) a lightweight encoder that dynamically selects and weighs these basis LoRAs based on the input analogy pair. Comprehensive evaluations demonstrate our approach achieves state-of-the-art performance and significantly improves generalization to unseen visual transformations. Our findings suggest that LoRA basis decompositions are a promising direction for flexible visual manipulation. Code and data are in https://research.nvidia.com/labs/par/lorweb"
        },
        {
            "title": "Start",
            "content": "6 2 0 2 7 1 ] . [ 1 7 2 7 5 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Spanning the Visual Analogy Space with a Weight Basis of LoRAs",
            "content": "Hila Manor1,2 Rinon Gal2 Haggai Maron2,1 Tomer Michaeli1 Gal Chechik2,3 1Technion 2NVIDIA 3Bar-Ilan University {hila.manor@campus,tomer.m@ee}.technion.ac.il, rinong@gmail.com, {hmaron,gchechik}@nvidia.com Figure 1. LoRWeB. We present novel method for analogy-based editing, based on learnable mixing of low-rank adapters. Given prompt and an image triplet {a, a, b} that visually describe desired transformation, LoRWeB dynamically constructs single LoRA from learnable basis of LoRA modules, and produces an editing result that applies the same analogy for the new image."
        },
        {
            "title": "Abstract",
            "content": "{ a, a, Visual analogy learning enables image manipulation through demonstration rather than textual description, allowing users to specify complex transformations difficult to articulate in words. Given triplet , the goal is to } generate such that : :: : b. Recent methods adapt text-to-image models to this task using single Low-Rank Adaptation (LoRA) module, but they face fundamental limitation: attempting to capture the diverse space of visual transformations within fixed adaptation module constrains generalization capabilities. Inspired by recent work showing that LoRAs in constrained domains span meaningful, interpolatable semantic spaces, we propose LoRWeB, novel approach that specializes the model for each analogy task at inference time through dynamic composition of learned transformation primitives, informally, choosing point in space of LoRAs. We introduce two key components: (1) learnable basis of LoRA modules, to span the space of different visual transformations, and (2) lightweight encoder that dynamically selects and weighs these basis LoRAs based on the input analogy pair. Comprehensive evaluations 1 demonstrate our approach achieves state-of-the-art performance and significantly improves generalization to unseen visual transformations. Our findings suggest that LoRA basis decompositions are promising direction for flexible visual manipulation. Code and data are in the projects website. 1. Introduction Text-based image editing models [5, 6, 49, 60, 67] have recently emerged as powerful tools for controllable image generation and manipulation, enabling users to modify images through textual descriptions. However, many visual transformations are inherently difficult to articulate precisely through text alone. For example, consider describing the transformation that converts photo into the style of specific painting, or conveying an exact target pose through text. Such limitations motivates alternative paradigms that can capture and apply complex visual transformations. Visual analogy learning [23] offers compelling solution to this challenge by enabling models to understand transformations through examples rather than explicit descriptions. , the In this paradigm, given triplet of images } a, a, { goal is to generate an image such that the visual relationship : :: : holds. That is, the transformation applied between and should be analogously applied to to produce b. This approach allows users to specify complex visual changes through demonstration, making it possible to capture nuanced transformations that would be difficult or impossible to describe textually. Early learning-based approaches trained stand-alone analogy models directly from analogy data [4, 32, 44, 57, 58, 61], but this lead to limited task diversity and image quality, or required extensive compute. More recent work aims to leverage the rich prior of powerful text-to-image backbones by adapting them to the visual analogy task, using single Low-Rank Adaptation (LoRA) module [17, 34, 50]. While effective, these methods face fundamental limitation: they attempt to capture the diverse space of possible transformations within single adapter. This constraint may limit the models ability to generalize across the rich variety of relationships that exist in images. We hypothesize that specializing the model to each specific analogy task at inference time may improve performance and generalization. While this objective could theoretically be achieved via hypernetworks that generate taskspecific LoRAs [50], these are notoriously difficult to train and often suffer from instability [39]. Instead, we draw inspiration from recent work demonstrating that LoRAs from fine-tuned models (e.g., for personalization tasks) can span meaningful semantic basis, and that interpolating between these LoRAs can effectively cover new points in this semantic space [12]. Building on this insight, we explore similar principle for visual analogy learning and propose LoRWeB, two-component system: (1) learnable basis of LoRA modules and (2) lightweight encoder that dynamically combines LoRAs from this basis at inference time based on the input analogy pair. These components are jointly trained, enabling the model to compose appropriate transformations for novel analogies unseen during training. Existing methods typically encode analogy images using vision-language models such as CLIP [42] or SigLIP [63] and provide these encodings as context to the generative model. This can provide the higher-level semantic understanding needed for understanding the analogy task. However, this might lead to loss of detail in fine-grained visual detail preservation. Recent advances have shown that diffusion models can extract remarkably accurate visual details through extended attention mechanisms [5, 8]. Thus, we leverage this capability by providing the full analogy triplet directly to the diffusion model via an extended-attention mechanism, while reserving CLIP-based encodings specifically for LoRA selection. This approach allows LoRWeB to balance fine-detail consistency with the higher-level semantics required to understand the analogy task. We evaluate LoRWeB against established baselines and demonstrate it achieves state-of-the-art results. Our contributions include: (1) novel architecture that decomposes visual analogy learning into basis of LoRAs with dynamic composition, and (2) comprehensive evaluation showing improved generalization to unseen transformations compared to existing single-LoRA approaches. 2. Related work Visual analogies. Visual analogies, also known as Image Analogies [23],Visual Prompting [4] or Visual Relations [17], is the task of learning transformation from pair of before-and-after exemplars and applying it analogously to new images. Early non-neural methods learned explicit perpair filters for simpler tasks like style transfer [23]. Networkbased methods later used image embedding spaces to present analogies through simple vector arithmetic [44]. While these methods showed promise on datasets of simple, isolated objects, they struggled with the complexity of real-world images. Newer methods instead treat analogy learning as in-context learning, where the model is directly conditioned on the exemplar pair and reference image, and is trained to successfully synthesize the matching target [4, 57, 58, 61]. More recently, some works adapt pre-trained text-to-image foundation models to the new task using LoRA module [9, 17, 24]. These methods, while showing impressive results, still struggle to generalize to unseen tasks. Our approach aims to tackle this limitation by avoiding the bottleneck of single LoRA, opting instead to train basis of adapters which can be mixed to achieve greater flexibility and better generalization. Diffusion-based image editing. The unprecedented semantic control offered by large scale text-to-image diffusion models [5, 43, 45] has inspired extensive work leveraging them as priors for image editing. Early works add noise to an image and remove it conditioned on novel prompt [37], but often significantly change image structure. Subsequent work improved content preservation by manipulating internal features [21, 40, 56] or the models denoising trajectory [11, 20, 26, 28]. Recent works go beyond text and incorporate different control modalities for enhanced precision, such as ControlNet [8, 65], or attentionsharing [1, 16, 22, 53]. Others explore text-free editing to enable modifications that cannot be textually described [19, 35], though without direct control. Transformer-based diffusion models further popularized attention-sharing for maintaining subject consistency in personalization [15, 46] and editing [7, 51, 54]. Among these, Flux.1-Kontext [5] was specifically trained for text-based editing, incorporating input images via extended attention mechanisms. Our work extends this models capabilities to visual analogies. LoRA and weight bases. LoRA [24] is parameterefficient fine-tuning method that modifies model using low-rank matrices learned on top of the existing weights. Its 2 success lead to range of downstream approaches trying to improve on the original formula. Of these, line of work explores the combination of multiple LoRa modules, either to combine them post-tuning [48, 64], or as means of turning an existing model into mixture of experts [13, 36, 59]. In visual content generation, recent work [12] showed that independently trained LoRA weights can span semantic basis, and interpolations between them can be meaningful. Similar observations were made in language processing, where LoRAs were combined for tasks like text simplification across different scientific domains [10]. We propose to further expand on this idea by learning joint basis of LoRAs, along with the router to mix and match between them. Thus, we can learn base that is more amenable to interpolations, and enable better downstream generalization. 3. Method 3.1. Preliminaries Low-rank adaption. LoRA [24] offers parameterefficient alternative to conventional fine-tuning of large models by learning low-rank matrices that adapt the pre-trained weights. Specifically, starting from frozen pre-trained Rmn, the update to the weights is weight matrix W0 represented as the product of two learned low-rank matrices Rrn, and the = BA, where min(m, n). This formulation drasrank is typically tically reduces the number of trainable parameters, while typically maintaining model performance. The final weights of the model are then updated to = W0 + α BA, where α is scaling constant. Rmr and Flow models. Flow-based generative models [2, 30, 31] learn series of transformations to map samples from one probability distribution x1 p, to samples from another x0 q. In the generative context, is typically taken as the standard normal distribution, while is the data distribution in latent space [45]. These models learn time-dependent velocity field vθ(zt, t) that models the direction from noisy sample towards the data manifold. The noisy sample zt is linearly interpolated latent between the two data distributions, zt = (1 t)x0 + tx1. Then, rectified flow-matching training loss for conditional models follows: = Etp(t),x0,x1,y,c (cid:104) vθ(zt, t, y, c) (x1 x0) (cid:105) 2 2 . (1) Here, the velocity field is optionally conditioned on context image y, and text-prompt c. 3.2. LoRWeB We aim to perform visual analogy completion [23], where the model infers proposed edit from given image pair and applies it to new image. Formally, two reference images, a, : RD, are related by an unknown transformation RD such that = RD RD, the goal is to generate (a). Given new image RD such that (b). { Naive solutions and limitations. Using pre-trained conditional generative model, such as FLUX.1-Kontext [5], existing solutions fine-tune the model using single LoRA [47]. a, a, For example, given , one can construct composite } 2 image = [a, a; b, b], as shown in the bottom-left 2 part of Fig. 2, which serves as the conditioning input. The goal of the model is to output x0 = [a, a; b, b], such that the bottom-right quadrant was transformed from to b, by training over Eq. (1). While these approaches perform well when the transformation is constrained to the training sets analogy types, they struggle to generalize to new, diverse transformations. We propose this arises in part because the single adapter struggles to capture the wide range of analogical relationships, from different style transfers to objects insertion or layout modifications. more advanced solution could be to span the diverse set of possible analogies using multiple adapters. Recently, Dravid et al. [12] demonstrated that LoRAs trained for model personalization can span semantic basis. Inspired by this, we propose to learn such basis for task LoRAs. naıve adaptation of Dravid et al. [12] to analogy tasks would require us to first optimize single adapter for each of analogy types seen during training, such that each LoRA module excels at different subset of visual edits. Once the specialized adapters are trained, they can be linearly combined to obtain an equivalent single novel adapter (cid:88) = eiAi, = (cid:88) eiBi, (2) where the coefficients ei are optimized for each analogy task through the use of Eq. (1) and . The model using the combined LoRA is then used to transform to b. a, } { However, this approach requires training large number of models, and test-time tuning phase for every new analogy. Indeed, Dravid et al. [12] required 65, 000 LoRAs to capture the constrained space of faces, and collecting significant number of different analogy pairs is more difficult. Our appraoch. Instead, we propose LoRWeB. Rather than training individual LoRAs and combining them only at inference time, we propose to simultaneously train basis of LoRA adapters, jointly with an encoder that predicts linearcombination coefficients for each input analogy pair. Specifically, we maintain set of rank-r LoRAs, and associate each Ai, Bi pair where with learnable key Rd, as depicted in the right part of Fig. 2. Next, vector ki we define an encoder network based on frozen, pre-trained , e.g. CLIP [42]. The encoder takes as input the ViT [62], a, a, , passes them through conditioning image triplet, } the ViT, concatenates the results and projects them through that outputs the results small learnable projection module 1, . . . , { } { 3 Figure 2. LoRWeB Overview. We first encode and a, that describe visual transformation (e.g. adding hat to the man), and b, which should be edited analogously (e.g. adding hat to the woman) with CLIP [42], and small learned projection module. The similarity between the encoded vector and set of learned keys determines the linear coefficients for combining the learned LoRAs into single, mixed LoRA. This mixed LoRA is injected into conditional flow model (e.g. Flux.1-Kontext [5]). Next, we build 2 2 composite image from {a, a, b}. The conditional flow model gets this composite image as its input, along with guiding edit prompt, and produces composite image with the edited results in the bottom-right quadrant. as query vector Rd: q(a, a, b) = (cid:16)(cid:2) Then, based on the conditioning query, we compute coefficients with (b)(cid:3)(cid:17) (a), (a), (3) E . ei(a, a, b) = (cid:20) softmax (cid:18) q(a, a, b)KT (cid:19)(cid:21) , (4) RdN contains the key vectors where i=1 in its columns. The final LoRA combination follows Eq. (2). and is marked as Mixed LoRA in Fig. 2. ki } { We use the same pre-trained encoder across different network layers, but train individual LoRWeB modules, including LoRAs, keys and projections for each targeted weight matrix W0 in the network. This enables capturing different semantic elements for each weight and layer in the model. 4. Experiments Settings. We evaluate our approach using Flux.1Kontext [5] as the pre-trained conditional flow model and CLIP [42] as the image encoder backbone. For our LoRAs Basis, we match the capacity of prior work [17], using = 32 adapters, each of rank = 4, with = 128 as the learned key dimension. We project the CLIP-encoders output to Rd using single fully-connected layer. To save on compute, during training we set the resolution to maximum 512 images, resizing on the long-edge of images. of 512 Additional implementation details are in App. A. We compare LoRWeB to four baselines: standard Flux.1-Kontext LoRA of similar parameter capacity (equivalent to LoRWeB with = 1, = 128), as well as three prior visual analogy methods based on Flux.1-Dev (RelationAdapter [17], VisualCloze [29] and EditTransfer [9]). Dataset. We train our model using the public Relation252k [17] set, which contains 16K analogy image pairs across 208 tasks. Since the train-set split of Relation252k is not fully publicly available, and only 10 unseen analogy tasks were released, we extend it with custom validation set to evaluate visual analogies. Specifically, we focus on analogies that were not found in the training set, which we create in the following manner: First, we collect over 100 Unsplash1 photos covering diverse concepts from three categories: animals, persons, and general objects. Next, we create analogy pairs with focus on two categories: transformations which are in-domain for the base text-to-image model, and transformations that are not. For in-domain transformations, we first use an LLM to summarize the training prompts for each task in the training-set of Relation252k, yielding 208 representative prompts. Next, we ask the LLM to generate novel prompts that differ from the training sets prompts and manually verify that they match the given concept categories. We filter prompts where Flux.1-Kontext fails to produce meaningful edit, and randomly select 15 prompts per concept category from the remainder. We generate three images per prompt, obtaining total of 135 analogy pairs. For out-of-domain analogies, we collect 18 community LoRAs for Flux.1-Kontext from HuggingFace, which were trained to enable edits the base model failed with. We use these pre-trained LoRAs, and repeat the previous random sampling strategy to get 135 analogy pairs. Finally, we randomly select as the input images two images from the matching concept category, with similar aspect ratio to and a, and crop them to the exact size. Our resulting set contains 540 analogy triplets across 90 tasks and 3 concept 1https://unsplash.com/ 4 Figure 3. LoRWeB visual analogy results. Using LoRA Basis allows LoRWeB to generalize to wide variety of new analogy tasks, from adding objects to transferring specific styles or makeup or copying pose changes. Please zoom in for more details. categories. Including the unseen set of Relation252K, this gives 100 tasks across 840 analogy triplets. On all experiments, we first aggregate the results per analogy task, and then aggregate over all tasks. More details appear in App. A. 4.1. Qualitative evaluations Figures 1 and 3 show analogy-based editing using LoRWeB. Notably, the model generalizes to new tasks covering style transfer, background replacements, object insertion, object displacement and more. In Fig. 4 we show qualitative comparisons of LoRWeB against the baselines. Notably, existing approaches either struggle with maintaining the content of the original image, or fail on some of the tasks. LoRWeB shows greater adaptability and succeeds in wider range of tasks. Additional results are in App. B.2. 4.2. Quantitative evaluations Automated evaluation metrics. For quantitative evaluations, we follow prior work [9, 18, 50] and evaluate performance across standard metrics such as LPIPS [66] between the source and generated image, and CLIP directional similarity between both analogy pairs. In addition, we build on recent image editing work [25], which demonstrates that VLMs often better correlate with human preference than CLIP-based methods, and implement VLM-based assessment protocol. Specifically, we conduct two VLM-based experiments: In the first, we provide Gemma-3 [52] with a, a, b, , and ask the VLM to evaluate the quality of { } results on two criteria: consistency with the source image, and accuracy of the applied transformation relative to the reference transformation. We name these metrics Preservation (VLM) and Edit Accuracy (VLM), respectively. As second quality metric, we take 2-alternative-forced-choice , the result a, a, design (2AFC). We show Gemma-3 } of our model, and the result generated by baseline, and ask it to select the image that best applies the analogy. We report this metric as Pairwise VLM. The prompts given to the VLM and further details appear in App. A. The results are shown in Fig. 5 and Fig. 6. When considering preservation and editing accuracy tradeoffs  (Fig. 5)  , our model pushes the Pareto front, achieving high edit accuracy while better maintaining the inputs structure and appearance. { 5 Figure 4. Comparisons with baseline methods on unseen tasks. Our approach generalizes across more diverse tasks, and better maintains the visual details of both the subject and the analogy. User study. Beyond automated metrics, we also conduct two-alternative forced choice user study. We show each user reference pair (a, a), an input image b, and two results (one from our model and one of random baseline), in randomized order, filtering out cases where no method succeeded in editing. Users are asked to select their preferred editing result. In total, we collected responses from 33 users covering 45 image pairs. The results  (Fig. 6)  align with the automated metrics, showing that users favor our approach over all baselines. All in all, our experiments demonstrate that our approach can meaningfully improve on the existing state of the art, and better generalize to unseen tasks. 4.3. Ablations Capacity effect. We compare LoRWeB across modified capacities in both basis sizes and ranks r. Specifically, ), with we compare our original variation ( { . and , { } } We use the same evaluation setup as in Sec. 4.2. Results are reported in Tab. 1. Reducing the basis size while maintaining = 32,r = 16 = 16,r = 4 = 8,r = 16 =32, r=4 { { } } the capacity (r = 16, = 8) leads to slight drop in performance, as does simply reducing capacity (r = 4, = 16). This highlights the importance of large basis for generalization. Similarly, naıve increase in rank can hamper editability, which we hypothesize to be consequence of the data, leading to increased overfitting. We provide additional capacity results for LoRWeB and added capacity for single LoRA in App. B.1. Similarity normalizing function. The normalization function choice in Eq. (4) can also affect the learned basis. For example, the used softmax is bound to [0, 1], hence it cannot result in negative coefficients for any LoRA. An alternative approach is to use Tanh, which is instead bound to [ 1, 1]. In practice, we find it to drastically underperform. We propose that this may be due to Tanh allowing the model to compose mixed LoRAs with much greater norms, possibly taking the model too far out of domain. However, we leave further investigation of activations to future work. Figure 5. Quantitative comparisons. (left) Accuracy of the applied edit and preservation of in using Gemma-3 [52]. Top right is better. (right) CLIP directional similarity and LPIPS between and b. Bottom-right is better. Our method pushes the Pareto front of edit accuracy-preservation, achieving higher edit accuracy while strongly preserving the input image. Figure 6. Pairwise image comparisons. We compare LoRWeB to four baselines on overall edit quality preference via both user study and using VLM. LoRWeB produces edits that are favored by both. Error bars are the 68% Wilson score interval. Layout of encoder input. In our approach, we elected to separately encode each of the conditioning analogy images using CLIP, and concatenate their representations. Our intuition is that CLIP requires resizing the image to 224 224, which can severely constrain the level of detail in each quad2 grid that we provide Flux as context. rant of the 2 Moreover, concatenated features could allow the model to better understand which encoding represents each conditioning image (i.e. a, and b), allowing it to better reason over the analogy. We verify this experimentally by comparing to version that provides CLIP with just the context image (the 2 2 grid). As seen in Tab. 1, this diminishes results, mainly decreasing the editing-accuracy metrics. } { a, that our model is indeed affected by the choice of analogy pair. Specifically, we examine how the same input image, b, reacts to different reference pairs under the same editing prompt. As can be seen in Fig. 7, the reference pair dictates the details of the analogy task, and particularly the visual details that are not captured by the prompts. For example, it can copy the text of the analogy image, adapt its specific style, or match the design and colors of the given crown. In comparison, we observe that some of the baselines are insensitive to the analogy pair, instead relying almost entirely on the prompt. As this experiment demonstrates, our approach has learned to perform analogy-based editing, and to greater degree than the existing baselines. Alternative image encoders. Although our approach uses CLIP [42] as an encoder backbone, we validate our robustness to alternative, common choices, and specifically SigLIP2 [55]. The results in Tab. 1 indicate that changing the encoder does not significantly alter our performances. We leave further tuning of encoders to future work. Importance of prompts and reference images. We follow existing baselines and use prompts to augment the models understanding. Since our goal is analogy based editing, and not simply text-based modification, we verify 5. Discussion We introduced LoRWeB, modular framework for visual analogy completion that learns basis of LoRA adapters and dynamically composes them using shared encoder conditioned on the input analogy. Our approach addresses the limitations of single-adapter fine-tuning or multi-adapters optimization at inference time by enabling flexible, layerspecific adaptations to diverse and unseen transformations. Through structured composition, we showed how LoRWeB 7 Table 1. Results for the ablation study of LoRWeB described in Sec. 4.3, for different hyperparameter and architecture choices. Model LoRWeB (full, = 4, = 32) + = 16 + = 16, = 8 + = 16 + Tanh activation + 2 + SigLip2 + SigLip2 & 2 Enc. Input 2 Enc. Input Pres. (VLM) Acc. (VLM) LPIPS 7.87 8.13 7.82 7.74 7.94 7.90 7.83 7.85 5.94 4.92 5.49 5.95 4.49 5.75 5.82 5.71 0.31 0.20 0.29 0.31 0.18 0.28 0.31 0.29 CLIP Dir. 0.21 0.11 0.19 0.23 0.09 0.20 0.21 0. Pairwise VLM (%) LoRA = 128 ET VC 57.9 51.8 59.9 60.4 48.2 61.9 59.0 59.5 70.4 63.9 73.1 70.5 58.3 73.3 71.7 73. 68.1 62.4 67.0 68.5 51.8 68.2 71.5 66.8 RA 58.5 49.6 56.7 56.6 42.1 53.9 55.5 58.3 Figure 7. Effect of different reference analogy pairs. LoRWeB directly leverages the analogy pair to understand the details of the proposed task, applying an edit that is beyond just text-based editing based on the given prompt. For example, when the prompt is Give this creature crown of crystals, the analogy context passes information on the amount and color of the crystals. outperforms and generalizes better than competing naive LoRA-based methods across various visual analogy tasks. However, this generalization is not without limitations. For example, LoRWeB may still struggle with tasks that are significantly different from the training corpus. While our focus in this work is on visual analogy completion, similar LoRA-basis approach could be broadly applicable, possibly replacing LoRAs in other tasks where generalization is needed. We hope to explore this direction in future work."
        },
        {
            "title": "References",
            "content": "[1] Yuval Alaluf, Daniel Garibi, Or Patashnik, Hadar AverbuchElor, and Daniel Cohen-Or. Cross-image attention for zeroshot appearance transfer. In ACM SIGGRAPH 2024 conference papers, pages 112, 2024. 2 [2] Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In The Eleventh International Conference on Learning Representations, 2023. 3 [3] Anthropic. Introducing claude 4. anthropic.com/news/claude-4, 2025. 12 https : / / www . [4] Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting via image inpainting. Advances in Neural Information Processing Systems, 35: 2500525017, 2022. [5] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. FLUX. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. 1, 2, 3, 4 [6] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. 1 [7] Shengqu Cai, Eric Ryan Chan, Yunzhi Zhang, Leonidas Guibas, Jiajun Wu, and Gordon Wetzstein. Diffusion selfdistillation for zero-shot customized image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1843418443, 2025. 2 [8] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2256022570, 2023. 2 [9] Lan Chen, Qi Mao, Yuchao Gu, and Mike Zheng Shou. Edit transfer: Learning image editing via vision in-context relations. arXiv preprint arXiv:2503.13327, 2025. 2, 4, 5 [10] Ming Cheng, Jiaying Gong, and Hoda Eldardiry. Sci-lora: Mixture of scientific LoRAs for cross-domain lay paraphrasing. arXiv preprint arXiv:2505.18867, 2025. 3 [11] Gilad Deutch, Rinon Gal, Daniel Garibi, Or Patashnik, and Daniel Cohen-Or. Turboedit: Text-based image editing usIn SIGGRAPH Asia 2024 ing few-step diffusion models. Conference Papers, pages 112, 2024. 2 [12] Amil Dravid, Yossi Gandelsman, Kuan-Chieh Wang, Rameen Abdal, Gordon Wetzstein, Alexei Efros, and Kfir Aberman. Interpreting the weight space of customized diffusion models. Advances in Neural Information Processing Systems, 37: 137334137371, 2024. 2, 3 [13] Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Yu Han, and Hao Wang. Mixture-of-LoRAs: An efficient multitask tuning method for large language models. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1137111380, 2024. 9 [14] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. In Advances in Neural Information Processing Systems, pages 5074250768. Curran Associates, Inc., 2023. 14 [15] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In The Eleventh International Conference on Learning Representations, 2023. 2 [16] Rinon Gal, Or Lichter, Elad Richardson, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. LCMlookahead for encoder-based text-to-image personalization. In European Conference on Computer Vision, pages 322340. Springer, 2024. 2 [17] Yan Gong, Yiren Song, Yicheng Li, Chenglin Li, and Yin Zhang. Relationadapter: Learning and transferring visual relation with diffusion transformers. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. 2, 4, 12, 14 [18] Zheng Gu, Shiyuan Yang, Jing Liao, Jing Huo, and Yang Gao. Analogist: Out-of-the-box visual in-context learning with image diffusion model. ACM Transactions on Graphics (TOG), 43(4):115, 2024. 5 [19] Rene Haas, Inbar Huberman-Spiegelglas, Rotem Mulayoff, Stella Graßhof, Sami Brandt, and Tomer Michaeli. Discovering interpretable directions in the semantic latent space of diffusion models. In 2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG), pages 19. IEEE, 2024. [20] Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23282337, 2023. 2 [21] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations, 2023. 2 [22] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared atIn Proceedings of the IEEE/CVF Conference on tention. Computer Vision and Pattern Recognition, pages 47754785, 2024. 2 [23] Aaron Hertzmann, Charles E. Jacobs, Nuria Oliver, Brian Curless, and David H. Salesin. Image analogies. In Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, page 327340, New York, NY, USA, 2001. Association for Computing Machinery. 1, 2, 3 [24] Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 2, 3 [25] Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Liangliang Cao, and Shifeng Chen. Diffusion model-based image editing: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. 5, 14 [26] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly DDPM noise space: Inversion and manipulations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12469 12478, 2024. 2 [27] Reina Ishikawa, Ryo Fujii, Hideo Saito, and Ryo Hachiuma. Human preference-aligned concept customization benchmark via decomposed evaluation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7002 7011, 2025. [28] Vladimir Kulikov, Matan Kleiner, Inbar HubermanSpiegelglas, and Tomer Michaeli. Flowedit: Inversion-free text-based editing using pre-trained flow models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1972119730, 2025. 2 [29] Zhong-Yu Li, Ruoyi Du, Juncheng Yan, Le Zhuo, Zhen Li, Peng Gao, Zhanyu Ma, and Ming-Ming Cheng. Visualcloze: universal image generation framework via visual in-context In Proceedings of the IEEE/CVF International learning. Conference on Computer Vision (ICCV), pages 1896918979, 2025. 4 [30] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. 3 [31] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. 3 [32] Yihao Liu, Xiangyu Chen, Xianzheng Ma, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong. Unifying image processing as visual prompting question answering. In International Conference on Machine Learning, pages 3087330891. PMLR, 2024. [33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 12 [34] Haoguang Lu, Jiacheng Chen, Zhenguo Yang, Aurele Tohokantche Gnanha, Fu Lee Wang, Li Qing, and Xudong Mao. Pairedit: Learning semantic variations for exemplar-based image editing. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. 2 [35] Hila Manor and Tomer Michaeli. Zero-shot unsupervised and text-based audio editing using DDPM inversion. In Proceedings of the 41st International Conference on Machine Learning, pages 3460334629. PMLR, 2024. 2 [36] Fangyuan Mao, Aiming Hao, Jintao Chen, Dongxia Liu, Xiaokun Feng, Jiashu Zhu, Meiqi Wu, Chubin Chen, Jiahong Wu, and Xiangxiang Chu. Omni-Effects: Unified and spatially-controllable visual effects generation. arXiv preprint arXiv:2508.07981, 2025. 3 [37] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. 2 [38] OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 12 10 [39] Jose Javier Gonzalez Ortiz, John Guttag, and Adrian Dalca. Magnitude invariant parametrizations improve hypernetwork learning. In The Twelfth International Conference on Learning Representations, 2024. 2 [40] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 conference proceedings, pages 111, 2023. 2 [41] Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: human-aligned benchmark for personalized image generation. In The Thirteenth International Conference on Learning Representations, 2025. [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, pages 87488763. PMLR, 2021. 2, 3, 4, 7 [43] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 2 [44] Scott Reed, Yi Zhang, Yuting Zhang, and Honglak Lee. Deep visual analogy-making. Advances in neural information processing systems, 28, 2015. 2 [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 10674 10685. IEEE, 2022. 2, 3 [46] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. 2 [47] Simo Ryu. Cloneofsimo/lora: Low-rank adaptation for fast text-to-image diffusion fine-tuning, 2023. 3 [48] Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani. ZipLoRA: Any subject in any style by effectively merging LoRAs. In European Conference on Computer Vision, pages 422438. Springer, 2024. 3 [49] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88718879, 2024. 1 [50] Xue Song, Jiequan Cui, Hanwang Zhang, Jiaxin Shi, Jingjing Chen, Chi Zhang, and Yu-Gang Jiang. LoRA of change: Learning to generate LoRA for the editing instruction arXiv preprint from single before-after image pair. arXiv:2411.19156, 2024. 2, 5 [63] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 2 [64] Jia-Chen Zhang and Yu-Jie Xiong. Subject or style: Adaptive and training-free mixture of LoRAs. arXiv preprint arXiv:2508.02165, 2025. [65] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. 2 [66] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 5 [67] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. Enabling instructional image editing with in-context generation in large scale diffusion transformer. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. 1 [51] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025. 2 [52] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. 5, 7, 12 [53] Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, and Yuval Atzmon. Training-free consistent text-to-image generation. ACM Transactions on Graphics (TOG), 43(4):118, 2024. [54] Yoad Tewel, Rinon Gal, Dvir Samuel, Yuval Atzmon, Lior Wolf, and Gal Chechik. Add-it: Training-free object insertion in images with pretrained diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. 2 [55] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. SigLIP 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. 7 [56] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-toIn Proceedings of the IEEE/CVF conimage translation. ference on computer vision and pattern recognition, pages 19211930, 2023. 2 [57] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Images speak in images: generalist Tiejun Huang. In Proceedings of painter for in-context visual learning. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68306839, 2023. 2 [58] Zhendong Wang, Yifan Jiang, Yadong Lu, Pengcheng He, Weizhu Chen, Zhangyang Wang, Mingyuan Zhou, et al. Incontext learning unlocked for diffusion models. Advances in Neural Information Processing Systems, 36:85428562, 2023. 2 [59] Xun Wu, Shaohan Huang, and Furu Wei. Mixture of loRA experts. In The Twelfth International Conference on Learning Representations, 2024. [60] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generaIn Proceedings of the Computer Vision and Pattern tion. Recognition Conference, pages 1329413304, 2025. 1 [61] Yifan Yang, Houwen Peng, Yifei Shen, Yuqing Yang, Han Hu, Lili Qiu, Hideki Koike, et al. Imagebrush: Learning visual in-context instructions for exemplar-based image manipulation. Advances in Neural Information Processing Systems, 36:4872348743, 2023. 2 [62] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1210412113, 2022. 3 11 A. Experimental Details A.1. Implementation details In all our experiments, we train for 10K steps on 1 H100 GPU, setting 8-bit AdamW [33] as the optimizer with learning rate of 103, β1 = 0.9, β2 = 0.99, weight decay value of 0.05, and bfloat16 mixed-precision training. We enable gradient checkpointing, and use batch size of 6 for all experiments, except for when = 16, = 32 where the batch size is set to 4. As for the encoders, the CLIP checkpoint we use is openai/clip-vit-large-patch14. For the SigLIP2 version in the ablations, we test google/siglip2-base-patch16-224. Both output vector in R768. A.2. Custom inference dataset All images gathered from Unsplash for the inference dataset extension are free to use under the Unsplash license2. To simulate in-domain prompts, we use GPT-4o [38] and Claude Sonnet 4 [3] to summarize the training prompts of Relation252k [17] as described in Sec. 4, and generate novel prompts. The 15 randomly selected prompts per concept category (animals, objects, and persons) appear in Tab. S1. The 18 pre-trained LoRA adapters are sourced from HuggingFace3, and cover range of transformation types such as style transfer, object modification, and artistic reinterpretation. Specifically, we use the following community LoRAs, with their provided trigger prompt: day-dream/MechAnything-Kontext-Dev-Lora drbaph/Fluffy-kontext-LoRA fal/3D-Game-Assets-Kontext-Dev-LoRA fal/Cubist-Art-Kontext-Dev-LoRA fal/Gouache-Art-Kontext-Dev-LoRA fal/Minimalist-Art-Kontext-Dev-LoRA fal/Mosaic-Art-Kontext-Dev-LoRA fal/Pencil-Drawing-Kontext-Dev-LoRA fal/Plushie-Kontext-Dev-LoRA fal/Pop-Art-Kontext-Dev-LoRA fal/Watercolor-Art-Kontext-Dev-LoRA gokaygokay/Bronze-Sculpture-Kontext-Dev-LoRA gokaygokay/Low-Poly-Kontext-Dev-LoRA gokaygokay/Marble-Sculpture-Kontext-Dev-LoRA gokaygokay/Oil-Paint-Kontext-Dev-LoRA Kontext-Style/Clay Toy lora Kontext-Style/Ghibli lora Kontext-Style/Paper Cutting lora . To match between a, and images of different sizes, we only choose images with an original aspect ratio distanced 0.15 from the aspect ratio of and a, and crop to as aspect ratio. The images are resized to the same size with maximum long edge of 512 before entering Flux.1-Kontext. A.3. VLM Based evaluation Part of our automated evaluation metrics include the use of Gemma-3 [52] as VLM to evaluate our results. We use two VLM-based experiments. In the first, we ask the VLM to evaluate our results on two criteria: consistency with the source image and accuracy of the applied transformation relative to the reference transformation described by . For this, we } , and the following prompt: provide Gemma-3 with a, a, b, a, { You are given 4 images: (original image), (edited version of A), { } (another original image), and (an output of an editing method). A, and are reference images that are given to some editing method in order to generate B. The method tries to infer the transformation that underwent to produce A, and then tries (maybe unsuccessfully) to apply the exact same transformation to - in order to generate B. Your task is to evaluate the resulting B: Was the same transformation applied well? Specifically, assess under two metrics, editing accuracy, and consistency 2https://unsplash.com/license 3https://https://huggingface.co/ 12 Table S1. List of prompts generated for the inference sets Category Prompt Add swirling galaxy background Render the object entirely as if its made from hand-knitted or hand-crocheted yarn Add bioluminescent glowing elements Turn this into candy or confectionery version Add flowing fabric or silk textures Turn this into steampunk mechanical design Add intricate filigree patterns Turn this into vintage advertisement poster Give this object coat of rust Turn this photo into cross-section diagram Add collar with bell Add mountainous background Give this animal clockwork mechanical parts Add flowing mane Add camouflage patterns Give this animal ethereal ghost-like transparency Add flowing river background Add metallic golden fur highlights Give this animal translucent fairy wings Add halo of fire Give this animal fantastical set of armor Give this creature crown of crystals Add halo of flowers around this animals head Give this animal bioluminescent markings Animals Animals Animals Animals Animals Animals Animals Animals Animals Animals Animals Animals Animals Animals Animals Make this creature look sleepy Objects Objects Objects Objects Objects Objects Objects Objects Objects Objects Objects Make this look ancient and archaeological Objects Objects Make this look like its growing moss Objects Objects Make this look like its made of clouds Persons Persons Persons Make this person look heroic Persons Persons Persons Make this person look like clown Add swirling vortex background Persons Persons Add natural outdoor lighting to this portrait Persons Make this person look like royalty Persons Persons Persons Persons Persons Make this person look ethereal Persons Add body paint or decorative patterns Add temporary tattoos Turn this person into holographic projection Add elaborate eye makeup Add cape or cloak Add elaborate hairstyling with ornaments Add serene, forested background Add golden hour lighting to this portrait Turn this photo into an architectural rendering Turn this person into steampunk portrait Turn this photo into surrealist floating sculpture Evaluate with the original image B, 1-10 integers only: how closely applies the transformation seen from to A. Are there missing elements, are there redundant elements? Quantify the precision of the editing. 1) editing accuracy: 2) consistency: Asses how well the edited image maintains the context of the original image B. Does it preserve the identity, objects, and layout in that did not require change, based on the infered transformation from to A? Consider in your evaluations other visual factors such as the localization of the edits, existence of redundant elements, style/strength/magnitude/colors of changes. First, describe in detail what the transformation from to A. Then describe what elements of it are present or missing in B, detailing precisely whats wrong regarding each metric. Then, return strict JSON with this scheme: , \"explanation\":\"the reasoning \"metrics\": \"accuracy\":<1-10>,\"consistency\":<1-10> { you described above\" } { . } The JSON is parsed automatically, and we report the numeric values as Preservation (VLM) and Edit Accuracy (VLM). In the second quality metric, we take 2-alternative-forced-choice design (2AFC). We show Gemma-3 five images: , the result of our model, and the result generated by one baseline, and ask it to select the image that better } a, a, { applies the analogy via the following prompt: You are given 5 images: (original image), (edited version of A), A, In which of the two methods was the (another original image), and 2 images (outputs of 2 editing methods). and are reference images that are given to some editing method in order to generate B. The methods try to infer the transformation that underwent to produce A, and then tries (maybe unsuccessfully) to apply the exact same transformation to - in order to generate B. Your task is to evaluate the resulting Bs: same transformation applied well? Specifically, assess under two metrics, editing accuracy, and consistency with the original image B, 1-10 integers only: 1) editing accuracy: Evaluate how closely applies the transformation seen from to A. Are there missing elements, are there redundant elements? precision of the editing. 2) consistency: Asses how well the edited image maintains the context of the original image B. Does it preserve the identity, objects, and layout in that did not require change, based on the inferred transformation from to A? Consider in your evaluations other visual factors such as the localization of the edits, existence of redundant elements, style/strength/magnitude/colors of changes. First, describe in detail what the transformation from to A. Then describe what elements of it are present or missing in B1 and B2, detailing precisely whats wrong regarding each metric. Then, return strict JSON with this scheme: reasoning you described above\" \"better\":<1 or 2>,\"explanation\":\"the Quantify the { We report the winrates parsed from the JSON outputs as pairwise VLM. } Alignment with humans. While VLMs have been used in the past as metric aligned with human preference [25, 27, 41], even in the context of visual analogies [17], we further validate their use in our task. Specifically, we test the alignment between the scores of the VLM and the preferences of humans from our user study described in Sec. 4. Following Fu et al. [14], We calculate the percentage of times the votes of each user agreed with the votes of the VLM and average over all users. We find this average user-VLM agreement to be 66.7%. As baseline, we also compute the average agreement between different users. Namely, we compute the percentage of times the votes of each pair of users agreed and average over all user pairs. We find that this average user-user agreement is 74.2%. This means that our VLM based approach achieves 89.9% evaluation consistency with the evaluation of humans. We also note that the mean standard deviation of user votes is 0.3423, which is similar to the empirical standard deviation of the VLMs predictions from the users mean, which is given by 0.4649. 14 B. Additional results B.1. Additional quantitative results We conduct two additional experiments with LoRWeB of larger capacity (r = 4, = 64), as well as single LoRA with higher capacity, of = 256. The results, along with detailed table of the numerical values in Fig. 5, appear in Tab. S2. As evident, naıve parameter addition does not strictly correlate with better performance, and can cause the methods to overfit. Table S2. additional results for the ablation study of LoRWeB described in Sec. 4.3, for different hyperparameter and architecture choices. Model Pres. (VLM) Acc. (VLM) LPIPS LoRWeB (full, = 4, = 32) LoRWeB on (r = 4, = 64) LoRA = 128 LoRA = 256 VisualCloze RelationAdapter Edit-Transfer 7.87 7.80 7.99 7.88 5.24 7.01 7.38 5.94 5.48 5.70 5.48 4.93 5.93 4.79 0.31 0.27 0.27 0.26 0.53 0.43 0.31 CLIP Dir. 0.21 0.19 0.20 0.18 0.21 0.22 0. Pairwise VLM (%) LoRA = 128 ET VC RA 57.9 56.5 N/A N/A N/A N/A N/A 58.5 68.1 70.4 67.7 52.6 66.3 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A B.2. Additional qualitative results We provide additional qualitative results of our method in Fig. S1, as well as more comparisons of our method to the 4 baselines from Sec. 4 in Fig. S2. 15 Figure S1. LoRWeB visual analoy results. The use of LoRA Basis allows LoRBA to generalize to wide varity of new analogy tasks, from changing given images to certain styles such as clay toys or bronze sculptures, changing the backgrounds, or changing the cloths of the person. Please zoom in for more details. Figure S2. Comparisons with baseline methods on unseen tasks. Our approach generalizes more across diverse tasks, and better maintains the visual details of both the subject and the analogy."
        }
    ],
    "affiliations": [
        "Bar-Ilan University",
        "NVIDIA",
        "Technion"
    ]
}