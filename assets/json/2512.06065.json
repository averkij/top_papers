{
    "paper_title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing",
    "authors": [
        "Runjia Li",
        "Moayed Haji-Ali",
        "Ashkan Mirzaei",
        "Chaoyang Wang",
        "Arpit Sahni",
        "Ivan Skorokhodov",
        "Aliaksandr Siarohin",
        "Tomas Jakab",
        "Junlin Han",
        "Sergey Tulyakov",
        "Philip Torr",
        "Willi Menapace"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit"
        },
        {
            "title": "Start",
            "content": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing Arpit Sahni1 Runjia Li1,3, Moayed Haji Ali1,2 Ashkan Mirzaei1 Chaoyang Wang1 Ivan Skorokhodov1 Aliaksandr Siarohin1 Tomas Jakab3 Sergey Tulyakov1 Philip Torr3 Willi Menapace1 Junlin Han3 5 2 0 2 ] . [ 1 5 6 0 6 0 . 2 1 5 2 : r 1Snap Research 2Rice University 3University of Oxford snap-research.github.io/EgoEdit Figure 1. We propose framework for real-time egocentric video editing. Our system is composed of: EgoEditData, manually curated dataset of 100k video editing pairs focusing on the egocentric case and featuring object substitution and removal under challenging hand occlusions, interactions, and large egomotion; EgoEdit, the first real-time autoregressive model for egocentric video editing running in real time on single H100 with 855ms first-frame latency and enabling live augmented reality (AR) interactions; EgoEditBench, comprehensive benchmark for evaluation of egocentric video editing systems."
        },
        {
            "title": "Abstract",
            "content": "We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges including rapid egomotion and frequent handobject interactions that create significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present complete ecosystem for egocentric video editing. First, we construct EgoEditData, carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting inWork done while interning at Snap Inc. struction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarkswhere existing methods strugglewhile maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. 1. Introduction Altering the perceived world is central to augmented reality (AR). This technology empowers real-time experiences that immerse the user into new worlds, transform the surrounding environment, create virtual characters and objects, and let users interact with these elements. However, traditional AR experiences rely on graphics pipelines and significant expert effort to handcraft each application, tying the potenFigure 2. In-the-wild video edits produced in real time by EgoEdits streaming variant EgoEdit-RT on single H100 GPU. The model demonstrates strong generalization to out-of-distribution scenarios, producing compelling real-time results suitable for immersive AR experiences. Additional results are presented in Appx. and the website. ability of current editors [57] and the usefulness of existing datasets [66, 76, 79]. Moreover, AR requires not only edit fidelity but also real-time, low-latency responses suitable for interaction. Many high-quality diffusion editing pipelines remain too slow for this setting. We address this gap by targeting the egocentric editing regime end-to-end through data, real-time generative model, and an evaluation framework as shown in Figure 1. Data. High-quality paired editing data showing before/after editing examples are main drivers of editing quality [26, 39, 54]. We therefore focus on EgoEditData, manually curated video editing dataset tailored to egocentric scenarios. Our data generation pipeline emphasizes the tasks most relevant to AR, focusing on the challenging removal and substitution of objects under active hand-object interaction, while explicitly preserving hand structure. We automatically identify actively interacted objects at which to target edits to ensure edit relevance, and conduct multi-stage human review to enforce consistently high visual quality. To ensure high instruction alignment, each pair is accompanied by precise, descriptive edit instructions. The resulting EgoEditData dataset comprises 49.7k video samples and 99.7k instructionedit pairs and will be released to support research in this domain. Real-Time Model. Building on this data, we train an instruction-following egocentric editor, EgoEdit, from large video generator. To meet the latency demands of interactive use, we apply Self-Forcing distillation [23] to obtain generator that runs in real-time with low latency on single H100 GPU, enabling responsive, user-in-the-loop editing as shown in Figure 2. Evaluation. To standardize assessment in this setting, we introduce EgoEditBench, an automated benchmark for egocentric video editing. The benchmark targets instruction faithfulness, preservation of hands and manipulated Figure 3. Comparison of EgoEdit and EgoEdit-RT against baselines according to VLM score on EgoEditBench and EditVerseBench [26]. EgoEdit and its real-time variant EgoEdit-RT achieve superior results on egocentric editing tasks and perform competitively with the strongest baselines on general editing tasks. EditVerse is excluded from EgoEditBench as source code is unavailable. Streaming models are indicated in dashed lines. tial achievable by this technology to the amount of available expert labor. The rapid progress of text-conditioned image [6, 30, 31] and video [28, 59, 70] generation however raises natural question: can instruction-guided editing [3, 26, 57, 60, 64] serve as direct engine for AR, enabling users to add, remove, or modify scene elements with simple language while they interact with the world? Editing is especially appealing because it addresses central tasks at the core of AR, such as object insertion and removal, environmental restyling, camera and lighting adjustments, while preserving the core content of scene. However, most recent editors [26, 57] and their training corpora [79] are targeted at exocentric content: third-person views with moderate motion, and low amounts of interaction. However, in AR, the camera is first-person and constantly moving, hands frequently occlude and manipulate objects, and object interactions are complex. These characteristics produce distribution shift that limits the reli2 objects, and temporal consistency under typical egocentric scenarios, providing basis for reproducible comparisons. EgoEdit delivers temporally stable, instruction-faithful edits, and latency suitable for interactive use. In comparisons against recent video-editing baselines [11, 17, 57] (see Figure 3), we observe consistent qualitative and quantitative improvements in the egocentric case, while performing closely to concurrent state-of-the-art editing methods [26] in the exocentric case. Ablations highlight the enabling role of EgoEditData in achieving such performance. In summary, we present complete ecosystem for egocentric video editing comprised of: EgoEditData: the first high-quality, manually curated dataset for egocentric video editing with 49.7k videos and 99.7k instructionedit pairs intended for public release. EgoEdit: real-time egocentric video editing model that enables interactive AR scenarios on single H100 GPU at 38.1fps with first-frame latency of 855ms. EgoEditBench: benchmark that standardizes evaluation for egocentric editing with focus on complex hand interactions and large egomotion typical of AR use cases. 2. Related Work Image & Video Editing. Text-conditioned diffusion initially enabled editing without additional training. Inversionbased methods reconstruct the source along the denoising trajectory and then steer it with the edit prompt [16, 25, 35, 38, 43, 44, 48, 61]. Attention-control methods modify or reweight cross/self-attention to preserve content while changing appearance [5, 22, 48, 58]. While broadly applicable, these approaches remain brittle for large structural edits and long-range consistency. InstructPix2Pix [3] overcame these limitations by collecting paired before/after image edit data, and training conditional editor that directly edits the source image. Because paired video edit data are hard to obtain, early trained video editing approaches sidestepped supervision by coupling image editors with video modules [40, 55]. InsV2V [11], however, brought the InstructPix2Pix recipe to video via synthetic video edit pairs. Recent image editors improve fidelity by scaling both data and models, making use of large curated edit corpora [39, 65] paired with either transformer backbones driven by MLLM embeddings [4, 39, 60, 64] or unified architectures that perform understanding and generation in one model for in-context editing [13, 34, 65]. Video editors follow similar paradigms, with in-context editors unifying conditioning and instructions along the sequence: UNIC [72] composes tasks via composite token sequences with task-aware RoPE and condition bias; EditVerse [26] improves unified image/video editing and generation through careful data curation. In contrast to image editors, sequence concatenation increases inference cost significantly for video editors [26]. Lucy Edit [57] reduces the cost of long sequences by channel-wise conditioning while retaining strong source-video control. Recent works aim at training-free real-time video editing [17, 27, 33]. While showing promise, they currently suffer quality gap with trained methods [26, 57, 72]. Across both images and videos, edit quality tracks the scale and quality of paired data and model capacity. This motivates the construction of our curated egocentric editing dataset, where edit examples focus on egocentric motion and complex hand/object interactions and are not available in existing video editing datasets. Editing Datasets. Editing quality closely follows the scale and curation of paired before/after data. For images, InstructPix2Pix [3] established the data-first recipe with 313k CLIP-filtered pairs generated via Prompt-toPrompt. Since then, most large corpora follow common pattern: task-specialized synthetic pipelines that generate at scale [18, 39, 54, 62, 63, 75], paired with multistage filtering using CLIP [18, 75], MLLMs [63, 75], object detectors [75], heuristics, and human-in-the-loop passes [18, 39, 62, 63, 75], resulting in million-scale filtered datasets. Quality-focused variants tighten these stages with heavier MLLM and human curation [39] or use existing high-quality editing methods as data generators [62]. Complementary, carefully curated sets show that domain alignment and quality can result in improvements despite smaller sizes [77, 78]. For videos, the same principles now dominate. InsV2V [11] pioneered synthetic video edit pairs, demonstrating that scaled paired training transfers to the temporal setting. Follow-ups are based on propagation of edited frames [66, 76] or the creation of specialized pipelines mirroring the image editing domain [79]. EditVerse [26] extensively filtered existing datasets [63, 75, 79] resulting in curated set of 232K video-edit samples. Streaming Video Generation. State-of-the-art video generators [28, 59, 70] yield high-quality but suffer from low throughput, long first-frame latency, and limited clip length. The denoising process is long, and the full video must be generated before the first frame can be shown to the user. Recent methods create autoregressive generators capable of generating long videos by predicting chunk of frames at time. Diffusion forcing [7] and its variants [1, 8, 32, 67] divide the video into chunks and assigning distinct diffusion noise levels so the model can denoise autoregressively chunk-by-chunk. Causal distillation converts slow bidirectional teachers into few-step causal students. CausVid [74] distills 50-step bidirectional model into 4-step causal student using DMD [73], with chunk-wise generation and KV caching. Self-Forcing [23] addresses the exposure bias by rolling out the student at train time, allowing the model to self-correct its errors. APT2 [36] employs adversarial post-training to achieve 1-NFE per frame for interactive 3 generation. Our approach follows the Self-Forcing strategy to achieve interactive egocentric video editing. 3. EgoEditData Compared to conventional video editing, egocentric editing brings distinct challenges due to complex hand-object interactions, frequent occlusions, and large egomotion. Existing editing datasets rarely cover egocentric scenes or such intricate interactions, creating data barriers [77] for learningbased AR experiences. To address data scarcity and provide foundation for egocentric editing, we present EgoEditData, manually curated and high-quality egocentric video editing dataset, focusing on rich hand-object interactions. 3.1. Data Curation Pipeline Because data quality [39, 54, 78] and domain alignment [77] are main drivers of editing performance, our pipeline prioritizes quality over quantity through strict filtering, and emphasizes creation of editing pairs that depict the most challenging egocentric scenarios with rich handobject interaction. Starting from real egocentric videos [19, 20], we ensure that: (1) videos contain an object that is actively manipulated by the egocentric subject, (2) the edit target is the manipulated object, (3) the synthetic videos preserve realistic hand motion while reflecting the intended content change, (4) the instruction prompts are descriptive and accurate. The stages are detailed below, with additional implementation specifics in Appx. A.1. Video selection. We consider videos from the Ego4D [19] and EgoExo4D [20] datasets. From Ego4D we select sequences coming from high-quality camera models only (see Appx. A.1). For EgoExo4D, we select egocentric cameras, and perform rectification of the videos. We also conduct filtering to reduce jittering and motion blur. 1.8% of videos are retained after this stage. Hand mask segmentation. We first detect hands in each frame using hand detection method [47]. Videos without visible hands are removed. For the remaining videos, detected hand regions provide visual prompts to SAM 2 [52], which yields fine-grained and temporally consistent hand masks across the sequence. We conduct human filtering to ensure hand masks are annotated correctly, with 49.6% of samples remained after filtering. Object names extraction. We then identify the object that the hands manipulate using vision language model. Qwen2.5-VL-32B [2] is prompted to name the interacted objects for each video. Videos where no meaningful handobject interaction is found are discarded. Object mask segmentation. Given the identified object name, Grounded SAM [53] predicts an approximate object mask in each frame. Videos with low mask confidence in every frame are removed. To confirm real interaction, we compute the edge distance between the hand and object masks and the distances between the object mask and hand skeleton keypoints [47], and filter out false positives. The coarse grounded masks then seed SAM 2 [52] to obtain finegrained and temporally consistent object masks. We conduct manual filtering to ensure object masks are extracted correctly, retaining 43.6% of the sequences. Curation at this stage ensures the expensive successive object editing stage is only run on correctly processed videos. Object Editing. This stage creates target videos where the original object is replaced with different one or removed entirely. For each segment, we prompt GPT-5 Mini [45] to propose diverse target objects for substitution, including both ordinary and imaginary items. Qwen-Image [64] then synthesizes reference image for each proposal. Next, GPT-5 Mini [45] produces scene-level description of the video assuming interaction with the target object. We feed the reference image, the scene-level prompt, and the object mask to Wan 2.1 VACE 14B [24] to generate the edited video. Object removal is treated as special case with no target object. Although the conditioning is rich and the computation is heavy (0.112 fps on 8 H100 GPUs), Wan 2.1 VACE 14B [24] yields only small fraction of results that meet our standard for dataset quality. Human annotators therefore remove imperfect results to ensure consistent quality, leaving 37.8% of the generated edits. Editing pairs construction. We form video editing pairs that include source video, target video, and natural language instruction. For each real video and its edited variants, we permute pairs among all versions, including the original clip, and prompt GPT-5 Mini [45] to generate precise and faithful description of the edit [62]. 3.2. Statistics After stages of curation and filtering, only 0.4% of the original videos from Ego4D and EgoExo4D are kept. The resulting pairs compose our EgoEditData dataset. It is composed of 10.9k original and 38.8k synthetic videos (70 hours long), for an average of 3.6 synthetic videos per original video, yielding total of 99.7k editing pairs, each comprising source video, target video, and editing instruction, with 93,422 of them derived from Ego4D [19] and 6,237 pairs from EgoExo4D [20]. EgoEditData is diverse. Additional details are presented in Appx. A. 4. EgoEdit Consider source video Xsrc and textual instruction specifying desired edit. The goal is to produce target video Xtgt reflecting the requested change. We focus on egocentric video editing. Unlike the traditional exocentric setting, which can be processed offline, egocentric editing requires real-time results to support interactive experiences. 4 Figure 4. Architecture of EgoEdit. EgoEdit extends video generation DiT model for video editing by performing channel-wise concatenation of the source and noisy target video inputs, avoiding the computational overheads of sequence-wise concatenation. To address these challenges, we introduce EgoEdit, video editing method tailored to the real-time egocentric setting. We first convert pretrained video generator into video editor by adding source video conditioning and finetuning on an editing corpus that includes EgoEditData. Then, we distill the editor into an autoregressive realtime generator using bidirectional DMD [73] and autoregressive Self Forcing [23]. Section 4.1 introduces the Flow Matching framework, Section 4.2 describes the model, and Section 4.3 details the distillation procedure. 4.1. Preliminaries: Flow Matching We train our generators with Rectified Flow flow matching [37, 41], which learns deterministic path from noise distribution pn to the data distribution pd. Let X1 pd and X0 pn = (0, I). We define linear path Xt = (1t) X0 +t X1 for [0, 1], whose ground truth velocity is constant along the path, vt = dXt dt = X1 X0. neural network G() predicts the velocity from noised input and time value and is trained by minimizing: LRF = Etpt, X1pd, X0pn (cid:13) G(Xt, t)(X1 X0) (cid:13) (cid:13) 2 2, (1) (cid:13) where pt is training distribution over chosen as logitnormal [15]. At inference time, an Euler solver integrates the learned ODE from X0 to X1 to produce sample. 4.2. Architecture We base EgoEdit on pretrained text-to-video generator trained on the latent space of Wan 2.1 autoencoder [59] with transformer backbone [46] (see Section B.1). As shown in Figure 4, the model receives Xt at time t, projects it to sequence of tokens through linear patchifier, processes the tokens with transformer blocks, and maps them to the predicted velocity ˆv through final linear head. Text conditions are provided through cross attention layers placed after each self attention block. The computation is expressed as ˆv = G(Xt c). We adapt the model to editing by replacing Xt with the and by conditioning on the source Xsrc; c). We consider noisy target video Xtgt video Xsrc, written as ˆv = G(Xtgt Figure 5. Inference of EgoEdit. EgoEdit performs inference in streaming fashion. camera continuously acquires video sequences which are edited by the model in chunk-by-chunk manner so that the edited video can be served to the user in watch-asyou-generate fashion. Each blue arrow represents model forward pass on single video chunk for the case of 3 steps model. two main strategies to inject Xsrc. Sequencewise concatenation [26, 72] patchifies the source and concatenates its patches with those of the target along the sequence dimension. This approach is common [26, 64], but the longer token sequence increases the cost of self attention quadratically, which conflicts with real-time low latency operation. To avoid this growth, EgoEdit uses channel-wise concatenation [57], where Xsrc and Xtgt are concatenated along channels before patchification, which keeps the cost close to the base model. 4.3. Distillation Procedure The edited model described above is accurate but slow at inference. 40 denoising steps with classifier-free guidance are required to produce video, which corresponds to 80 model invocations (NFEs). In addition, the model generates the full clip at once, which delays the first visible frame until the process finishes. Interactive use calls for autoregressive generation with low latency, as illustrated in Figure 5. We therefore distill the editor into real-time autoregressive model in two phases (see Appx. B.3). Bidirectional DMD distillation. We follow DMD [73] to compress the 40-step model with classifier-free guidance into 4-step model with distilled guidance. This reduces the NFEs from 80 to 4, facilitating subsequent distillation. Self Forcing. Self Forcing [23] runs the causal model autoregressively on video streams and applies DMD loss with score models based on the bidirectional teacher. The model learns to correct its own errors, which reduces exposure bias and enables low latency autoregressive inference. To minimize exposure bias while achieving low latency, we generate chunk at time, where each chunk is composed 5 Figure 6. Qualitative comparison of EgoEdit and its real time streaming variant EgoEdit-RT against baselines on EgoEditBench. EgoEdit and EgoEdit-RT consistently perform better than their baselines. Note that Senorita-2M uses the first frame from EgoEdit for frame propagation. Additional results are presented in Appx. and the website. of three latent frames. Note that the employed Wan [59] autoencoder natively supports autoregressive operation. Table 2 shows latency and throughput for the different model variants. Our Self Forcing distilled model can produce results in real-time at 38.1fps with latency of 855ms for displaying the first frame on single H100 GPU. 4.4. EgoEditBench Existing video editing benchmarks [10, 26, 56] are primarily built on third-person natural videos, making them unsuitable for evaluating egocentric video editing. To properly assess this setting, we follow [26] to construct EgoEditBench, benchmark designed to evaluate editing performance across 15 egocentric tasks [26], including: Add Object, Add Effect, Remove Object, Change Object, Change Background, Change Camera Pose, Stylization, Reasoning, Depth-to-Video, Sketch-to-Video, Pose-to-Video, Video-toPose, Video-to-Sketch, Video-to-Depth, and Combined Task of all previous tasks. To build this benchmark, we sample 100 unique source videos from split of the Ego4D dataset [19] that was not used in constructing EgoEditData, ensuring maximum diversity. This is achieved by first extracting source-object names following the EgoEditData pipeline, and computing BERT [14] embeddings from the concatenation of each source-object name and its corresponding scene description. We then perform K-means clustering with 10 centroids, and select 10 samples per cluster, resulting in 100 diverse source videos. Conditioned on the source video, its caption, and the source object, we prompt GPT-5 [45] to generate task-specific instruction prompts for each of the 15 tasks. For the X-to-Video tasks, we synthesize conditioning signals as follows: Canny edge maps using OpenCV for the Sketch2Video task, 2D poses using DWpose [69] for Pose2Video, and depth maps using Depth Anything [68] for Depth2Video. For Add Object and Remove Object, we sample 50 unique source videos per task and construct the corresponding instruction prompts following EgoEditData. For the Change Object task, we generate four instruction prompts for each unique source video with two focusing on object replacement and two combining object replacement with an added effect. In total, EgoEditBench comprises 1700 source videos paired with instruction prompts, covering 15 diverse egocentric editing tasks. Evaluation is performed according to the EditVerseBench [26] protocol and metrics, with results averaged per task to ensure equal weighting across all tasks. Additional implementation details for EgoEditBench can be found in Appx. C. 5. Experiments 5.1. Experimental Setup Training details. We finetune our pretrained video generation model on EgoEditData and corpus of additional 1.31M video and 3.5M image editing pairs to obtain the base video editing model. We finetune the model with total batch size of 96 for 30k iterations using an AdamW [42] optimizer with lr 1e-5, weight decay of 0.1 and exponential moving average. Bidirectional DMD distillation is performed for 4.5k steps using an AdamW [42] optimizer with lr of 1e-6 for the model and 4e-7 for the critic, weight decay of 0.1 and exponential moving average. Successively,"
        },
        {
            "title": "Family",
            "content": "TokenFlow [49] STDF [71] Senorita-2M AnyV2V [29] [79] Attention manipulation First-frame propagation InsV2V [11] Lucy Edit [57] EditVerse [26] EgoEdit (ours) StreamDiffusion [27] StreamDiffusionV2 [17] EgoEdit-RT (ours) Instruction-guided Streaming models"
        },
        {
            "title": "EgoEditBench",
            "content": "EditVerseBench [26] VLM PS TA TC VLM PS TA TC 4.99 4.59 7.52 6.72 5.24 5.44 7.76 4.32 2.55 7.71 18.91 18. 18.85 18.65 18.81 18.87 19.21 18.92 18.63 19.13 15.89 15.64 16.25 15.35 14.92 15.03 16.89 14.15 12.75 16.34 95.04 93. 95.86 92.37 94.01 94.41 96.70 86.83 94.31 96.41 5.87 6.64 6.99 6.46 5.71 6.27 8.26 8.00 4.33 2.78 8.18 19.90 19. 19.32 19.47 19.08 19.23 19.69 19.61 18.76 18.45 19.59 23.68 24.33 23.07 23.32 22.49 22.55 25.29 24.40 19.01 17.32 17.61 98.21 96. 98.33 95.91 96.39 98.62 98.68 98.54 93.41 98.22 98.55 Table 1. Quantitative comparison of the baseline models and our method on EgoEditBench and EditVerseBench benchmarks: VLM is VLM evaluation score, PS is Pick Score, TA is Text Alignment, TC is Temporal Consistency. Reference-based editing tasks from EditVerseBenchpropagation, inpainting, reference insertion, and edit with maskwere excluded. indicates closed-source models evaluated using their publicly released samples; indicates models utilizing the first frame generated by EgoEdit. EgoEdit-RT stands for the real-time streaming version of EgoEdit. Self Forcing training is conducted for 3.5k steps using an AdamW [42] optimizer with lr of 1e-6 for the model and 4e7 for the critic, weight decay of 0.1 and exponential moving average, producing the final checkpoint. Additional training and dataset details are presented in the Appx. B.2. Evaluation metrics and protocol. EgoEditBench serves as the main framework for evaluating egocentric video editing quality. Evaluation is supplemented by Edto provide references on the nonitVerseBench [26] egocentric case. As our model does not support reference image conditioning, we remove the EditVerseBench tasks of Propagation, Inpainting, Reference Insertion, and Edit with Mask, which require such conditioning. 5.2. Comparison to Baselines Baselines selection. We select range of baselines against which to compare that are based on attention manipulation [49, 71], propagation of the first frame [29, 79], or direct instruction-guided video to video translation [11, 26, 57]. Frame propagation methods [29, 79] receive as input the first frame edited by EgoEdit for fair comparison. Additionally, we select StreamDiffusion [17, 27] as representatives for real-time editing methods. For fair comparison, all baseline models are evaluated using their recommended inference settings, including the number of sampling steps, guidance scale, resolution, and frame rate (following EditVerseBench [26]). For inversion-based methods that require target video caption and cannot process natural instruction prompts, we use GPT-5 [45] to generate the target caption from the source prompt and instruction prompt. Complete inference details for each baseline are provided in Appx. D.1. Results. As shown in Table 1 and Figure 6, EgoEdit produces state-of-the-art results on egocentric videos while still performing strongly on general editing, as demonstrated by EditVerseBench performance. In particular, on the challenging egocentric setting, our method drops only 0.24 points in VLM evaluation when switching from general editing tasks to egocentric ones, while Lucy Edit [57] and InsV2V drop respectively 0.83 and 0.47 points. Only Senorita-2M [79] and AnyV2V [29] retain their performance due to receiving the propagated first frame from EgoEdit. When compared to existing real-time streaming editors, our streaming variant EgoEdit-RT achieves markedly stronger performance across both benchmarks. Relative to the bidirectional full model, EgoEdit-RT delivers comparable results on all quantitative metrics. These findings highlight the strong real-time editing capabilities of EgoEdit-RT and validate the effectiveness of our distillation procedure. Additional qualitative results are provided in the supplementary material and the website. 5.3. Ablations We conduct ablations to investigate performance change during distillation and the effectiveness of EgoEditData. Distillation. Table 2 compares the original non-distilled EgoEdit model to the one obtained after 4-step DMD distillation and the final real-time autoregressive generator obtained after Self Forcing training. We take first chunk latency as the main metric for assessing real-time method suitability. It shows the delay from the moment the user hits the record button on their camera to the moment the corresponding first edited frame is visualized on the screen. Due to the inability of standard methods to perform 7 No Distill. DMD [73] Self Forcing [23] 5.4. In-the-Wild Evaluation Is streaming? NFEs VLM-Eval First chunk size 81 frames 81 frames Next chunk size 4 7.31 80 7.76 N/A N/A 4 7.71 9 frames 12 frames First Chunk Latency [ms] Recording AE Model Total 5062 1520 6850 13432 5062 1520 343 6925 Throughput [fps] Model Model + AE 11.9 9.68 237 43.5 562 217 75.7 855 134 38.1 Table 2. EgoEditBench VLM score, latency and throughput analysis of different distilled EgoEdit models on 1H100 under resolution of 512384px. We consider latency involved in recording the source video, running EgoEdit, and running the autoencoder (AE) for source video encoding and generated video decoding. chunk-by-chunk generation, only the Self Forcing variant achieves sub-second latency compatible with interactive usage. When evaluated on EgoEditBench, the Self Forcing variant reaches comparable VLM scores to the bidirectional teacher model, while enabling interactive generation. Contribution of EgoEditData. To better quantify how EgoEditData improves models ability to adapt to egocentric editing, we vary the number of videos of EgoEditData included during training. Starting from the same textto-video checkpoint, we finetune different models for 10k iterations on our training editing data corpora, removing certain percentage of original videos and corresponding edited versions from EgoEditData in proportion from 0% to 100%. As shown in Table 3, performance on EgoEditBench steadily improves as more samples from EgoEditData are incorporated, highlighting the role of EgoEditData in enabling robust egocentric video editing. We also discuss how EgoEditData influences general editing performance on EditVerse in Appx. D. % of EgoEditData 0% 25% 75% 100% VLM Evaluation 4.87 7.12 7. 7.85 Table 3. Performance of EgoEdit when trained using progressively smaller subsets of EgoEditData. trend is visible: the model performs better with more egocentric editing data included during training. Note that these results differ from Table 1 because all models are evaluated at the 10k-iteration checkpoint. Additional details are provided in Appx. D. We conduct in-the-wild evaluation of the real-time version of EgoEdit to test robustness in the real-world usage and emerging capabilities. Results are presented in Figure 2. We observe that the model is able to perform complex editing tasks such as correctly preserving hands during interactions, modeling environment interactions such as the sidewalk becoming wet when sprayed with an imaginary water gun, modeling lighting effects induced by inserted objects, replacing fast-moving thrown objects, inserting animals that realistically interact with the environment by jumping over or navigating around objects, and correctly rotating inserted objects according to the substituted object orientation. Most excitingly, some instances of inserted objects react to user interactions, such as dogs being walked on leash. We observe, however, that the amount of structural modifications that inserted objects can operate on the surrounding environment is limited: swords will not cut through furniture, and animals will not move real objects. 6. Discussion Limitations. While EgoEdit-RT performs comparably to EgoEdit on automated benchmarks and presents strong in the wild generation capabilities, we notice qualitative gap which manifests as: (i) lower proficiency in out-ofdistribution editing instructions, (ii) less robust performance when editing objects becoming temporarily occluded, (iii) lower temporal consistency. EgoEdit possesses first-frame latency of 855ms, which is sufficient but suboptimal for interactive usage. As shown in Table 2, such latency is dominated by the recording time of the first chunk of 3 latent frames, corresponding to 9 RGB frames. Further reduction in latency can be achieved by lowering the chunk size during Self Forcing training. Finally, EgoEdit operates at resolution of 512384px and frame rate of 16 fps, slightly lower than the common 480p resolution. Conclusions. We introduce comprehensive framework for developing learning-based AR applications through egocentric video editing. this effort, we construct and publicly release EgoEditData, the first highquality and manually curated dataset of egocentric video edits, containing 99.7k editing pairs across 49.7k unique videos. Building on this dataset, we propose EgoEdit, the first real-time video editing model tailored to egocentric applications. Our model demonstrates strong generalization to in-the-wild videos, achieves state-of-the-art performance on egocentric editing tasks, and competitive results on general video editing benchmarks. Finally, we introduce EgoEditBench, standardized benchmark designed to evaluate egocentric video editing performance under realistic handobject interactions. Together, these contributions form foundation and ecosystem for real-time, instructionguided AR generation, paving the way for future research in interactive generative systems for augmented reality."
        },
        {
            "title": "To support",
            "content": ""
        },
        {
            "title": "References",
            "content": "[1] Sand. ai, Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, W. Q. Zhang, Weifeng Luo, Xiaoyang Kang, Yuchen Sun, Yue Cao, Yunpeng Huang, Yutong Lin, Yuxin Fang, Zewei Tao, Zheng Zhang, Zhongshu Wang, Zixun Liu, Dai Shi, Guoli Su, Hanwen Sun, Hong Pan, Jie Wang, Jiexin Sheng, Min Cui, Min Hu, Ming Yan, Shucheng Yin, Siran Zhang, Tingting Liu, Xianping Yin, Xiaoyu Yang, Xin Song, Xuan Hu, Yankai Zhang, and Yuqiao Li. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. 3 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and JunarXiv preprint yang Lin. Qwen2.5-vl technical report. arXiv:2502.13923, 2025. 4, 13 [3] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. 2, 3 [4] Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, Yimeng Wang, Kai Yu, Wenxuan Chen, Ziwei Feng, Zijian Gong, Jianzhuang Pan, Yi Peng, Rui Tian, Siyu Wang, Bo Zhao, Ting Yao, and Tao Mei. Hidream-i1: high-efficient image generative foundation arXiv preprint model with sparse diffusion transformer. arXiv:2505.22705, 2025. 3 [5] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. arXiv preprint arXiv:2304.08465, 2023. 3 [6] Siyu Cao, Hangting Chen, Peng Chen, Yiji Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Gong, Tianpeng Gu, Xiusen Gu, Tiankai Hang, Duojun Huang, Jie Jiang, Zhengkai Jiang, Weijie Kong, Changlin Li, Donghao Li, Junzhe Li, Xin Li, Yang Li, Zhenxi Li, Zhimin Li, Jiaxin Lin, Linus, Lucaz Liu, Shu Liu, Songtao Liu, Yu Liu, Yuhong Liu, Yanxin Long, Fanbin Lu, Qinglin Lu, Yuyang Peng, Yuanbo Peng, Xiangwei Shen, Yixuan Shi, Jiale Tao, Yangyu Tao, Qi Tian, Pengfei Wan, Chunyu Wang, Kai Wang, Lei Wang, Linqing Wang, Lucas Wang, Qixun Wang, Weiyan Wang, Hao Wen, Bing Wu, Jianbing Wu, Yue Wu, Senhao Xie, Fang Yang, Miles Yang, Xiaofeng Yang, Xuan Yang, Zhantao Yang, Jingmiao Yu, Zheng Yuan, Chao Zhang, Jian-Wei Zhang, Peizhen Zhang, Shi-Xue Zhang, Tao Zhang, Weigang Zhang, Yepeng Zhang, Yingfang Zhang, Zihao Zhang, Zijian Zhang, Penghao Zhao, Zhiyuan Zhao, Xuefei Zhe, Jianchen Zhu, and Zhao Zhong. Hunyuanimage 3.0 technical report. arXiv preprint arXiv:2509.23951, 2025. 2 [7] Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffu9 sion. In Neural Information Processing Systems (NeurIPS), 2024. 3 [8] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, Weiming Xiong, Wei Wang, Nuo Pang, Kang Kang, Zhiheng Xu, Yuzhe Jin, Yupeng Liang, Yubing Song, Peng Zhao, Boyuan Xu, Di Qiu, Debang Li, Zhengcong Fei, Yang Li, and Yahui Zhou. Skyreelsv2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025. 3 [9] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024. 14 [10] Yinan Chen, Jiangning Zhang, Teng Hu, Yuxiang Zeng, Zhucun Xue, Qingdong He, Chengjie Wang, Yong Liu, Xiaobin Hu, and Shuicheng Yan. Ivebench: Modern benchmark suite for instruction-guided video editing assessment. arXiv preprint arXiv:2510.11647, 2025. 6 [11] Jiaxin Cheng, Tianjun Xiao, and Tong He. Consistent videoIn International to-video transfer using synthetic dataset. Conference on Learning Representations (ICLR), 2024. 3, 7 [12] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. 2022. [13] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 3 [14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transIn North American formers for language understanding. Chapter of the Association for Computational Linguistics, 2019. 6 [15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. In Proceedings of the 41st International Conference on Machine Learning, pages 1260612633. PMLR, 2024. 5, 14 [16] Kunyu Feng, Yue Ma, Bingyuan Wang, Chenyang Qi, Haozhe Chen, Qifeng Chen, and Zeyu Wang. Dit4edit: diffusion transformer for image editing. In AAAI Conference on Artificial Intelligence, 2025. 3 [17] Tianrui Feng, Zhi Li, Shuo Yang, Haocheng Xi, Muyang Li, Xiuyu Li, Lvmin Zhang, Keting Yang, Kelly Peng, Song Han, Maneesh Agrawala, Kurt Keutzer, Akio Kodaira, and Chenfeng Xu. Streamdiffusionv2: streaming system for dynamic and interactive video generation. arXiv preprint arXiv:2511.07399, 2025. 3, 7, 17 [18] Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024. 3 [19] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolaˇr, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around the world in 3,000 hours of egocentric video. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 4, 6, 13, [20] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, Eugene Byrne, Zach Chavis, Joya Chen, Feng Cheng, FuJen Chu, Sean Crane, Avijit Dasgupta, Jing Dong, Maria Escobar, Cristhian Forigua, Abrham Gebreselasie, Sanjay Haresh, Jing Huang, Md Mohaiminul Islam, Suyog Jain, Rawal Khirodkar, Devansh Kukreja, Kevin Liang, JiaWei Liu, Sagnik Majumder, Yongsen Mao, Miguel Martin, Effrosyni Mavroudi, Tushar Nagarajan, Francesco Ragusa, Santhosh Kumar Ramakrishnan, Luigi Seminara, Arjun Somayazulu, Yale Song, Shan Su, Zihui Xue, Edward Zhang, Jinxu Zhang, Angela Castillo, Changan Chen, Xinzhu Fu, Ryosuke Furuta, Cristina Gonzalez, Prince Gupta, Jiabo Hu, Yifei Huang, Yiming Huang, Weslie Khoo, Anush Kumar, Robert Kuo, Sach Lakhavani, Miao Liu, Mi Luo, Zhengyi Luo, Brighid Meredith, Austin Miller, Oluwatumininu Oguntola, Xiaqing Pan, Penny Peng, Shraman Pramanick, Merey Ramazanova, Fiona Ryan, Wei Shan, Kiran Somasundaram, Chenan Song, Audrey Southerland, Masatoshi Tateno, Huiyu Wang, Yuchen Wang, Takuma Yagi, Mingfei Yan, Xitong Yang, Zecheng Yu, Shengxin Cindy Zha, Chen Zhao, Ziwei Zhao, Zhifan Zhu, Jeff Zhuo, Pablo Arbelaez, Gedas Bertasius, Dima Damen, Jakob Engel, Giovanni Maria Farinella, Antonino Furnari, Bernard Ghanem, Judy Hoffman, C.V. Jawahar, Richard Newcombe, Hyun Soo Park, James M. Rehg, Yoichi Sato, Manolis Savva, Jianbo Shi, Mike Zheng Shou, and Michael Wray. Ego-exo4d: Understanding skilled human activity from firstand thirdperson perspectives. In Computer Vision and Pattern Recognition (CVPR), 2024. 4, 13, 14 [21] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv, 2023. 14 [22] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 3 [23] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the traintest gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. 2, 3, 5, 8, 15 [24] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 4, 13, 14 [25] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Pnp inversion: Boosting diffusion-based editing with 3 lines of code. In International Conference on Learning Representations (ICLR), 2024. [26] Xuan Ju, Tianyu Wang, Yuqian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, Daniil Pakhomov, Zhe Lin, Soo Ye Kim, and Qiang Xu. Editverse: Unifying image and video editing and generation with in-context learning. arXiv preprint arXiv:2509.20360, 2025. 2, 3, 5, 6, 7, 15, 16, 17, 19 [27] Akio Kodaira, Chenfeng Xu, Toshiki Hazama, Takanori Yoshimoto, Kohei Ohno, Shogo Mitsuhori, Soichi Sugano, Hanying Cho, Zhijian Liu, Masayoshi Tomizuka, and Kurt Keutzer. Streamdiffusion: pipeline-level solution for realtime interactive generation. In International Conference on Computer Vision (ICCV), 2025. 3, 7, 17 [28] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2025. 2, 3, 14 [29] Max Ku, Cong Wei, Weiming Ren, Harry Yang, and Wenhu Chen. Anyv2v: tuning-free framework for any video-tovideo editing tasks. arXiv preprint arXiv:2403.14468, 2024. 7 [30] Black Forest Labs. Flux, 2024. 2 [31] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv, 2025. 2 10 [32] Runjia Li, Philip Torr, Andrea Vedaldi, and Tomas Jakab. Vmem: Consistent interactive video scene generation with surfel-indexed view memory. arXiv preprint arXiv:2506.18903, 2025. [33] Feng Liang, Akio Kodaira, Chenfeng Xu, Masayoshi Tomizuka, Kurt Keutzer, and Diana Marculescu. Looking backward: Streaming video-to-video translation with feature banks. arXiv preprint arXiv:2405.15757, 2024. 3 [34] Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model arXiv preprint for interleaved multi-modal generation. arXiv:2505.05472, 2025. 3 [35] Haonan Lin, Mengmeng Wang, Jiahao Wang, Wenbin An, Yan Chen, Yong Liu, Feng Tian, Guang Dai, Jingdong Wang, and Qianying Wang. Schedule your edit: simple yet effective diffusion noise schedule for image editing. arXiv preprint arXiv:2410.18756, 2024. 3 [36] Shanchuan Lin, Ceyuan Yang, Hao He, Jianwen Jiang, Yuxi Ren, Xin Xia, Yang Zhao, Xuefeng Xiao, and Lu Jiang. Autoregressive adversarial post-training for real-time interactive video generation. arXiv preprint arXiv:2506.09350, 2025. 3 [37] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow Matching for Generative Modeling. In The Eleventh International Conference on Learning Representations (ICLR), 2023. 5, 14 [38] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. In Computer Vision and Pattern Recognition (CVPR), 2024. [39] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, Guopeng Li, Yuang Peng, Quan Sun, Jingwei Wu, Yan Cai, Zheng Ge, Ranchen Ming, Lei Xia, Xianfang Zeng, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Gang Yu, and Daxin Jiang. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. 2, 3, 4 [40] Shaoteng Liu, Tianyu Wang, Jui-Hsien Wang, Qing Liu, Zhifei Zhang, Joon-Young Lee, Yijun Li, Bei Yu, Zhe Lin, Soo Ye Kim, and Jiaya Jia. Generative video propagation. In Computer Vision and Pattern Recognition (CVPR), 2025. 3 [41] Xingchao Liu, Chengyue Gong, and qiang liu. Flow Straight and Fast: Learning to Generate and Transfer Data with RecIn The Eleventh International Conference on tified Flow. Learning Representations (ICLR), 2023. 5, 14 [42] Loshchilov. Decoupled weight decay regularization. arXiv, 2017. 6, 7, 14, 15 [43] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 3 [44] Shen Nie, Hanzhong Allan Guo, Cheng Lu, Yuhao Zhou, Chenyu Zheng, and Chongxuan Li. The blessing of randomness: Sde beats ode in general diffusion-based image editing. In ICLR, 2024. [45] OpenAI. Gpt-5, 2025. 4, 6, 7, 13, 15, 16, 18 [46] William Peebles and Saining Xie. Scalable Diffusion Models with Transformers. In International Conference on Computer Vision (ICCV), 2023. 5, 14 [47] Rolandos Alexandros Potamias, Jinglei Zhang, Jiankang Deng, and Stefanos Zafeiriou. Wilor: End-to-end 3d hand localization and reconstruction in-the-wild. arXiv preprint arXiv:2409.12259, 2024. 4, 13 [48] Chenyang QI, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In International Conference on Computer Vision (ICCV), 2023. 3 [49] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel K. Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multiIn Computer Vision modal understanding and generation. and Pattern Recognition Conference (CVPR), 2025. 7 [50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. 2021. 14 [51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research (JMLR), 2022. 14 [52] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. SAM 2: Segment anything in images and videos. In International Conference on Learning Representations (ICLR), 2025. 4, [53] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. 4, 13 [54] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. arXiv preprint arXiv:2311.10089, 2023. 2, 3, 4 [55] Uriel Singer, Amit Zohar, Yuval Kirstain, Shelly Sheynin, Adam Polyak, Devi Parikh, and Yaniv Taigman. Video editing via factorized diffusion distillation. In European Conference on Computer Vision (ECCV 2024), 2025. 3 [56] Shangkun Sun, Xiaoyu Liang, Songlin Fan, Wenxu Gao, and Wei Gao. Ve-bench: Subjective-aligned benchmark suite for text-driven video editing quality assessment. arXiv preprint arXiv:2408.11481, 2024. 6 [57] Decart Team. Lucy edit: Open-weight text-guided video editing. arXiv, 2025. 2, 3, 5, 7, 11 [58] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 3 [59] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and Advanced Large-Scale Video Generative Models, 2025. 2, 3, 5, 6, 14 [60] Peng Wang, Yichun Shi, Xiaochen Lian, Zhonghua Zhai, Xin Xia, Xuefeng Xiao, Weilin Huang, and Jianchao Yang. Seededit 3.0: Fast and high-quality generative image editing. arXiv preprint arXiv:2506.05083, 2025. 2, 3 [61] Wen Wang, Yan Jiang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, and Chunhua Shen. Zero-shot video editing using off-the-shelf image diffusion models. arXiv preprint arXiv:2303.17599, 2024. 3 [62] Yuhan Wang, Siwei Yang, Bingchen Zhao, Letian Zhang, Qing Liu, Yuyin Zhou, and Cihang Xie. Gpt-image-edit1.5m: million-scale, gpt-generated image dataset. arXiv preprint arXiv:2507.21033, 2025. 3, 4, [63] Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. In International Conference on Learning Representations (ICLR), 2025. 3, 14 [64] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-Image Technical Report. arXiv, 2025. 2, 3, 4, 5, 14 [65] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 3 [66] Yuhui Wu, Liyi Chen, Ruibin Li, Shihao Wang, Chenxi Xie, and Lei Zhang. Insvie-1m: Effective instruction-based video editing with elaborate dataset construction. In International Conference on Computer Vision (ICCV), 2025. 2, 3 [67] Zeqi Xiao, Yushi Lan, Yifan Zhou, Wenqi Ouyang, Shuai Yang, Yanhong Zeng, and Xingang Pan. Worldmem: Longterm consistent world simulation with memory, 2025. 3 [68] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1037110381, 2024. 6, 16 [69] Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with two-stages distillation. arXiv preprint arXiv:2307.15880, 2023. 6, [70] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer. arXiv, 2024. 2, 3, 14 [71] Danah Yatim, Rafail Fridman, Omer Bar-Tal, Yoni Kasten, and Tali Dekel. Space-time diffusion features for zero-shot text-driven motion transfer. In Computer Vision and Pattern Recognition (CVPR), 2024. 7 [72] Zixuan Ye, Xuanhua He, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qifeng Chen, and Wenhan Luo. Unic: Unified in-context video editing. arXiv preprint arXiv:2506.04216, 2025. 3, 5 [73] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William T. Freeman. Improved distribution matching distillation for fast image synthesis. arXiv preprint arXiv:2405.14867, 2024. 3, 5, 8 [74] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. 2025. 3 [75] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Computer Vision and Pattern Recognition Conference (CVPR). 3 [76] Shoubin Yu, Difan Liu, Ziqiao Ma, Yicong Hong, Yang Zhou, Hao Tan, Joyce Chai, and Mohit Bansal. Veggie: Instructional editing and reasoning video concepts with arXiv preprint arXiv:2503.14350, grounded generation. 2025. 2, [77] Bohan Zeng, Ling Yang, Jiaming Liu, Minghao Xu, Yuanxing Zhang, Pengfei Wan, Wentao Zhang, and Shuicheng Yan. Editworld: Simulating world dynamics for instructionfollowing image editing. In ACM International Conference on Multimedia, 2025. 3, 4 [78] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. In International Conference on Neural Information Processing Systems (NeurIPS), 2023. 3, 4 [79] Bojia Zi, Penghui Ruan, Marco Chen, Xianbiao Qi, Shaozhe Hao, Shihao Zhao, Youze Huang, Bin Liang, Rong Xiao, and Kam-Fai Wong. Senorita-2m: high-quality instructionbased dataset for general video editing by video specialists. arXiv preprint arXiv:2502.06734, 2025. 2, 3,"
        },
        {
            "title": "Appendix",
            "content": "For more video results, please check the website."
        },
        {
            "title": "Appendix Contents",
            "content": "A. Additional Dataset Details . A.1. Pipeline Details . . . A.2. Dataset statistics . . . . . . . . . B. Additional Method Details . B.1. Text-to-Video Model . B.2. Training Details . . B.3. Distillation Procedure Details . . . . . . . . . . . . . . . . . . . . C. Additional Benchmark Details . C.1. Evaluation Tasks . . C.2. EgoEditBench-Human Alignment . . . . . . D. Evaluation Details D.1. Baseline Details D.2. Metrics Details . . . . . . . . . . . . . . . . . . . E. Additional Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.1. Additional In-the-Wild Results . . E.2. Additional Comparison to Baselines . . . . . E.3. Additional Distillation Ablation Results . . . . . . . E.4. Additional Dataset Ablation Results . . . F. Failed Experiments A. Additional Dataset Details A.1. Pipeline Details 13 13 13 14 14 14 14 15 15 16 16 16 17 17 17 17 17 18 Video selection. We select videos from Ego4D [19] and EgoExo4D [20] according to the following criteria. The video must be captured with one of the following camera models: GoPro Hero 4, GoPro Hero Black 7, GoPro Hero Black 8, GoPro Hero Black 9, GoPro Hero Silver 7, or GoPro Max, and it must be monocular rather than binocular. Furthermore, to ensure that the videos are sharp and visually informative, we apply additional filtering based on jitter scores and aesthetic scores. Hand mask segmentation. We employ WiLoR [47] for hand detection, applying confidence threshold of 0.75 on frame-by-frame basis. From the detected frames, we select the three with the highest confidence scores and use their hand masks to generate point prompts for SAM2 [52], enabling the creation of dense and temporally consistent hand masks across frames. Object names extraction. We employ Qwen2.5-VL32B [2] to extract object names. For inputting the video, we sub-sample the video with 2 fps into frames and ask the model what is held by the hand in the video. 13 Object mask segmentation. We use the extracted object names to prompt Grounded-SAM [53] to generate object masks for each frame. mask confidence threshold of 0.4 and text threshold of 0.35 are applied. Each object mask is then filtered based on its distance to the hand skeleton and the edge of the hand mask. If multiple objects are detected in single frame, we select the mask closest to the hand mask. To refine the prompts, we exclude regions corresponding to the hand masks and use the remaining object mask areas from the top three frames with the highest confidence scores to generate point prompts for SAM2, enabling the creation of dense and consistent object masks. Object Editing. We use the object masks to generate rectangular masks for each frame, expanding them with an additional margin. Gaussian dilation with kernel size of 5050 px is then applied to these rectangular masks. After excluding the regions corresponding to the hand masks, the resulting final masks are used by VACE-14B [24] to produce the edited videos. The resolution of the generated videos is 19201104 px. A.2. Dataset statistics EgoEditData contains approximately 99,700 video editing pairs, each consisting of source video, target video, and corresponding instruction prompt. Our dataset is built upon two egocentric video datasets with 6.4% of the videos originating from EgoExo4D [20], and the remaining 93.6% from Ego4D [19]. EgoEditData focuses on object manipulation in egocentric videos. The majority of samples correspond to the Change Object task, which prompts the model to replace an existing object in the video with an ordinary object, and contains 54,164 pairs. The second largest group consists of Change Object with Special Effects tasks, comprising 39,465 samples, where the source object is replaced with an imaginary object with special effects such as fire or frost. In addition, the dataset includes 3,651 samples for the Add Object task and 2,379 for the Remove Object tasks. Figure 7a presents the word cloud of source objects descriptions, while Figure 7b shows the word cloud of target objects descriptions. Overall, thanks to the sampling of target object names through GPT-5 Mini [45] used during the Object Editing phase of dataset construction, the dataset contains 13,632 distinct target objects compared to 3,199 unique source objects, enhancing diversity. We also report in Figure 7c the distribution of the instruction prompt lengths. With mean of 378 characters, most prompts contain detailed instructions specifying target object, scene details and style, while certain tasks not requiring such context such as Remove/Add Object have more concise prompts like Remove/Add an {object}.. Finally, Figure 8 shows the ten most frequent scenarios in EgoEditData as categorized by the original egocentric datasets, highlighting balanced distribution across different types of scenes and scenarios. (a) Source Word Cloud (b) Target Word Cloud (c) Prompt Length Distribution Figure 7. Overview of dataset statistics. (a) and (b) illustrate the word clouds for the source and target object descriptions respectively. While source object descriptions focus on everyday objects, target descriptions often contain descriptions of materials and special effects. (c) shows the distribution of prompt lengths. Short prompts in (c) correspond to instructions not requiring context such as Remove/Add an {object}. B.2. Training Details Training configuration. We finetune the pretrained text to video model for 30k iterations with total batch size of 96 videos using 48 H100 GPUs. We use AdamW [42] optimizer with lr 1e-5, learning rate scheduler with linear warmup of 1000 steps, weight decay of 0.1, beta values of 0.99 and exponential moving average of 0.9999. Editing data corpora. We collect corpus of editing data to supplement EgoEditData for the non-egocentric case. We consider the following publicly available editing datasets: GPT-Image-Edit-1.5M [62], ShareGPT-4oImage, Complex-Edit, HQEdit, OmniEdit [63], and UltraEdit. In addition, we employ internal data generation pipelines for the creation of additional editing training data. The pipelines employ Qwen-Image-Edit [64] to generate 2M image editing examples, and combination of models including Wan VACE [24], Wan-Animate and MiniMaxRemover to generate an additional corpus of 210k video editing data containing edit categories including object addition and removal, object substitution, human edits. Generated editing pairs are assessed through combination of automated and manual filtering. In addition, the pipeline synthesizes 1.1M video editing data pair by considering natural videos and pairing them with depth estimation, pose estimation, edge estimation, and optical flow estimation models. The training data is composed of multiple datasets. During training, we apply importance sampling with weights of 28% for EgoEditData, 52% for other video editing datasets, and 20% for image editing datasets. Within each category, individual datasets are weighted proportionally to their sizes. B.3. Distillation Procedure Details The distillation procedure is responsible for enabling realtime and low-latency execution of EgoEdit starting from the finetuned video editing method. The original model performs inference of 5s, 512 384px, 16 fps videos using 40 steps with classifier-free guidance, for total of 80 Figure 8. Distribution of most frequent scenarios in EgoEditData according to the original datasets [19, 20] categorization. B. Additional Method Details B.1. Text-to-Video Model State-of-the-art methods [28, 59, 70] in text-to-video generation commonly adopt the latent diffusion [21] paradigm, DiT-like [46] transformer backbones, and the Flow Matching [37, 41] framework. Our pretrained text-to-video model follows the same paradigm. We model videos in the latent space of Wan 2.1 autoencoder [59] which performs an 8 8 4 compression along the height, width, and time dimensions respectively. Input and output linear projections perform 2 2 spatial patchification to further increase compression. The main backbone consists of 10.7B transformer backbone composed of 32 identical transformer blocks with 4096 hidden dimensions and 32 heads. Each block employs self attention, followed by cross attention for text conditioning, and final MLP, and uses modulation [9] for timestep conditioning. QK normalization [15] and Flash Attention [12] are used for every attention operation to improve stability and speed. Text tokens are extracted using combination of pretrained T5 [51] and CLIP [50] text encoders. The model performs inference in 40 steps using an Euler solver and is trained at resolution of 512 px. 14 model evaluations (NFEs). As every model evaluation takes 86ms on single H100 GPU, the resulting throughput is of only 11.9fps, which additionally decreases to 9.68fps when considering autoencoder source video encoding and target video decoding times, far from the 16fps required for realtime performance (see Table 2). The first phase of distillation thus performs step and guidance distillation using DMD to yield model producing 5s 512 384px 16fps video in 4 NFEs, for 20 increase in throughput. While the model now possesses throughput of 43.5fps after application of the autoencoder, the full video needs to be sampled before the first frame can be displayed, and the autoencoder needs to be run on the full source and target videos, creating first frame latency of 6.93s, inhibiting interactive usage. We perform DMD distillation for 4500 iterations with total batch size of 64 using 32 H100 GPUs. We use the AdamW [42] optimizer with lr 1e-6 for the generator and 4e-7 for the fake score model, 5 steps per generator update, weight decay of 0.1, beta values of 0.99 and exponential moving average of 0.99. To enable interactive usage, Self Forcing [23] performs distillation of the bidirectional DMD model into an autoregressive model capable of producing videos in chunk-bychunk fashion. In this way, the first frame can be displayed to the user right after the first chunk is sampled and decoded by the autoencoder, reducing first frame latency to 855ms. We use configuration of Self Forcing where latent frames are denoised in chunks spanning three consecutive latent frames [23]. We conduct Self Forcing distillation starting directly from the DMD distilled checkpoint for 4500 iterations with total batch size of 64 using 64 H100 GPUs. We use an AdamW [42] optimizer with lr 1e-6 for the generator and 4e-7 for the fake score model, 10 steps per generator update, weight decay of 0.1, beta values of 0.99 and exponential moving average of 0.99. We find that the model can quickly adapt to autoregressive modeling when initialized from the DMD checkpoint, so we skip the ODE initialization phase [23]. We use 7 chunks for generation and each chunk contains 3 latent frames with 21 latent frames in total. Following [23], to imitate the generation for longer videos, during training, we mask the first chunk of the latent frames when generating the last chunk and we use window size of 5 chunks as condition during inference. C. Additional Benchmark Details C.1. Evaluation Tasks Starting from 100 unique egocentric source videos, we construct benchmark spanning 15 editing tasks. We ensure diversity by clustering source objects and scenario names using sentence embeddings and then uniformly sample source videos across clusters. Conditioned on each source video, its caption, and the source object, we use GPT-5 Mini [45] to produce task-specific instruction prompts. Below we include details on constructing the source video and instruction prompt for each editing task. Remove Object: We aim to evaluate the models ability to remove certain objects from the video while keeping the other parts of the video consistent with the source video. We select 50 source videos from the EgoEditData and their instruction prompts. We include them in the benchmark and remove them from the training set. Add Object: We aim to evaluate the model ability to insert specified object into the scene while keeping the rest of the video consistent with the source. We select 50 source videos from the EgoEditData together with their instruction prompts. We use source videos from the Add Object task, where target object is absent and synthetically removed following the EgoEditData pipeline. We include these in the benchmark and exclude them from the training set. Change Object: We evaluate the models ability to alter specified object, either by modifying an attribute or replacing it with new specified object while keeping the rest of the video consistent with the source. For each sampled source video, we create four instruction prompts: two that perform pure replacement (object: b) and two that pair the replacement with an added effect on the new object (e.g., fire or glow). In total, we obtain 400 videos for evaluating the Change Object task. Instruction prompts are designed following EgoEditData pipeline. Change Background: We evaluate the models ability to replace or edit the background while preserving foreground identity and motion. To construct instructions, we provide GPT-5 Mini [45] with few source frames and the video caption and request an editing instruction prompt with semantically compatible target background. Change Camera Pose: We assess recomposition via specified camera trajectory (pan/tilt/dolly/zoom) without altering scene events. Instruction prompts are generated by GPT-5 [45] Mini by prompting it with the video caption which contains description of the camera pose in the source video. Add Effect: We evaluate adding post-processing effects while preserving scene content. These effects usually operate as global filters (e.g., motion blur, VHS, glow, film grain) that are independent of the particular video. Accordingly, we construct instructions by prompting GPT-5 Miniprimed with few in-context examples from EditVerseBench [26] and to ask it to propose diverse pool of effect editing instructions, from which we randomly sample one per clip. Stylization: We mirror the procedure of Add Effect by providing GPT-5 Mini with EditVerseBench [26] examples and ask it to propose diverse set of style editing in15 structions. We sample one randomly per source video. Reasoning: We evaluate edits that rely on spatial and temporal reasoning. Given the source video and its caption, we prompt GPT-5 Mini [45] to propose an editing instruction tied to an explicit anchor (event or timestamp) or disambiguating relations (e.g., leftof/behind/before/after). The instruction deliberately avoids explicitly naming unique target and instead focuses on giving an instruction where the correct object must be inferred from context. Depth-to-Video: We convert the source video into depth map using Depth Anything [68], and construct the instruction prompt as (Turn the depth map into video with the following description: caption.) where the caption is the source video caption describing the appearance of the scene. Sketch-to-Video: We convert the source video into Canny edge maps using OpenCV, and construct the instruction prompt as (Turn the canny edge map into video with the following description: caption.), where caption is the source video caption describing the scenes appearance and layout. Pose-to-Video: We convert the source video into 2D human poses using DWpose [69], and construct the instruction prompt as (Turn the DWpose pose map into video with the following description: caption.), where caption is the source video caption specifying the subjects identity, attire, and overall scene appearance. Video-to-Depth: We prompt the model to convert video into temporally consistent depth map. For this task, we use fixed instruction prompt (Turn the video into depth map.). Video-to-Sketch: We use fixed instruction prompt of (Turn the video into Canny edge map.). Video-to-Pose: We use fixed instruction prompt (Turn the video into DWpose pose map.). Combined (Multi-Task): We compose multiple editing prompts from the same source video (e.g., Pose-toVideo + Change Background + Stylization) by sampling subset of instruction prompts and prompting GPT-5 Mini [45] to compose single instruction prompt that combines all of the tasks together. C.2. EgoEditBench-Human Alignment To evaluate the reliability of the VLM score employed in EgoEditBench, we conduct study on EgoEditBench comparing VLM and human preference alignment. Given 30 randomly sampled benchmark element per category, and baseline method, we conduct VLM evaluation of EgoEdit and the baseline following EgoEditBench protocol and, for each sample, assign VLM preference to the method with highest VLM score. Simultaneously we ask human evaluator to express preference for each sample for our method or the baseline. Results are shown in Table 4. VLM and user preferences are in high agreement, with 86.2% and 84.9% of cases on average, respectively when evaluated against LucyEdit and InsV2V. We thus rely on VLM score as the main benchmark metric. D. Evaluation Details D.1. Baseline Details To ensure fair comparison, we use each baselines default inference hyperparameters including the number of frames, frames-per-second (FPS), spatial resolution, and inference settings such as guidance scale and sampling steps. Below, we include the specific hyperparameters we used for evaluating each baseline: TokenFlow. We rely on Stable Diffusion 1.5 as the backbone. We use 16 frames at STDF. We use 24 frames at 10 fps and 40 inference steps and 10 optimization steps. We use guidance scale of 10 and inference at the resolution of 576 320. SENORITA (Senorita-2M). We use 33 frames at 8 fps and 768 448 resolution, with guidance scale of 4 and 30 inference steps. We use CogVideoX as the backbone. To obtain the edited first frame, we use the first frame generated by our model instead of relying on pretrained ControlNets, following [26]. We crop and resize the first frame generated by our method to the default resolution of SENORITA. AnyV2V. We use 16 frames at 8 fps and 512 512, guidance of 9, with 100 inversion steps plus 50 edit steps. Similarly, we use the first frame generated by our model instead of relying on pretrained ControlNets, following [26]. We crop and resize the first frame generated by our method to the default resolution of AnyV2V. InsV2V. We use 32 frames at 15 fps and 384 384, with the default guidance for the video branch (1.2) and text branch (7.5), over 20 inference steps. Lucy-Edit. We use 81 frames at 15 fps with 832 480 resolution, guidance scale of 5, and 50 inference steps. EditVerse. Since EditVerse is closed source model, we only compare on EditVerseBench, where we rely on its published samples. We omit the results on EgoEditBench since we do not have access to the model to generate the required results. StreamDiffusion. We use the image-to-image editing pipeline for the video editing task. We process 81 frames and 16 fps at the resolution 832 480. StreamDiffusionV2. We use the streaming setup at 832 480 with an 81 frames sequence and 16 fps, using very light 24 denoising steps. Task EgoEdit LucyEdit InsV2V vs LucyEdit vs InsV2V vs LucyEdit vs InsV2V LucyEdit InsV2V VLM Mean Score Preference (VLM) Preference (User Study) Agreement (%) Add Object Change Camera Pose Change Object Change Background Combined (Multi-Task) Depth-to-Video Add Effect Video-to-Pose Pose-to-Video Reasoning Remove Object Sketch-to-Video Stylization Video-to-Depth Video-to-Sketch Overall 7.83 7.11 7.61 7.28 7.96 8.53 6.41 7.90 8.58 6.69 6.72 8.81 7.88 8.80 9.00 7.76 4.12 6.93 6.49 5.10 6.36 7.44 5.29 4.63 7.18 5.30 5.04 6.31 4.68 4.00 3.78 5.44 5.47 7.30 4.09 4.46 4.66 5.72 5.96 4.39 4.77 4.85 5.71 5.54 6.70 4.80 4.34 5. 29 (100%) 20 (67%) 23 (77%) 29 (97%) 30 (100%) 30 (100%) 24 (80%) 30 (100%) 28 (93%) 24 (80%) 25 (83%) 30 (100%) 29 (97%) 30 (100%) 30 (100%) 29 (97%) 14 (47%) 26 (87%) 27 (90%) 29 (97%) 30 (100%) 19 (63%) 30 (100%) 30 (100%) 23 (77%) 23 (77%) 30 (100%) 25 (83%) 30 (100%) 30 (100%) 29 (100%) 27 (90%) 29 (97%) 28 (93%) 29 (97%) 30 (100%) 24 (80%) 28 (93%) 28 (93%) 25 (83%) 26 (87%) 27 (90%) 29 (97%) 28 (93%) 24 (80%) 29 (97%) 28 (93%) 30 (100%) 29 (97%) 30 (100%) 30 (100%) 27 (90%) 28 (93%) 29 (97%) 29 (97%) 29 (97%) 30 (100%) 29 (97%) 28 (93%) 26 (87%) 100.0 70.0 80.0 90.0 96.7 100.0 80.0 93.3 86.7 63.3 76.7 90.0 93.3 93.3 80.0 411 (91%) 395 (88%) 411 (91%) 431 (96%) 86.2 93.3 40.0 86.7 86.7 96.7 100.0 73.3 93.3 96.7 73.3 73.3 100.0 80.0 93.3 86.7 84. Table 4. Study on EgoEditBench comparing VLM and human preference alignment. We conduct VLM evaluation following EgoEditBench protocol and for each sample, assign VLM preference to the method with highest VLM score. Simultaneously we ask human evaluator to express preference for sample from one of two compared methods. We find VLM and user preference to be in agreement in 86.2% and 84.9% of cases on average, respectively when evaluated against LucyEdit and InsV2V. D.2. Metrics Details E.2. Additional Comparison to Baselines We closely follow [26] when computing metrics for our benchmark. We evaluate edit quality with VLM-based judge, overall video quality with PickScore, text alignment at the frame and video levels, and temporal consistency with CLIP and DINO. Because frameand video-level text alignment are highly correlated in our setting, we report only the video-level score for brevity. We also find the CLIPand DINO-based consistency metrics to be strongly correlated, so we rely on the CLIP consistency metric for temporal consistency. Among all metrics, we found the VLM score to align well with human judgment (see Table 4), so we use it as our primary quality signal. For EgoEditBench, we use exactly the same evaluation prompts as [26] when computing the VLM editing-quality score. E. Additional Results E.1. Additional In-the-Wild Results We present in Figure 10 and Figure 9 additional in-the-wild qualitative examples produced by EgoEdit-RT in real-time on single H100 GPU for the egocentric and exocentric cases respectively. EgoEdit-RT can perform complex edits such as transforming markers placed on the floor into pillars of the Golden Gate Bridge, model fluid effects, change location into haunted mansion, add animals that interact with the environment and the user, or perform stylization. We present additional qualitative results in Figure 11 comparing EgoEdit to baseline video editing methods. EgoEdit and its real-time variant EgoEdit-RT are capable of performing range of editing tasks including object attribute changes, object replacement, object insertion, style transfer, background changes, and conversion to depth map. We observe concurrent offline methods [57] and real-time video editing methods [17, 27] to often fail in the egocentric case, with failure modes often manifesting as inability to produce any change over the source video, or modifying the input beyond what is requested in the editing instruction. In addition, Table 5 reports per-category VLM scores on EgoEditBench for our method and baselines. E.3. Additional Distillation Ablation Results Figure 12 shows additional qualitative results comparing variants of EgoEdit at different stages of distillation: the starting 40-step (80 NFEs) video editing checkpoint obtained after finetuning of the pretrained video generation model (EgoEdit), the 4-step (4 NFEs) variant obtained as result of the DMD distillation phase (EgoEdit-DMD), and the real-time streaming variant obtained at the end of Self Forcing training (EgoEdit-RT). We observe similar qualitative performance among the distilled variants as shown in the figure. We observe EgoEdit-RT to occasionally present temporal shifts at the boundaries between different chunks of predicted frames. In addition, when qualitatively evalu17 Method TokenFlow STDF Senorita-2M AnyV2V InsV2V Lucy Edit StreamDiffusion StreamDiffusionV2 EgoEdit EgoEdit-DMD EgoEdit-RT dd 5.56 4.59 7.85 7.72 5.67 4.31 3.95 1.63 7.89 7.91 7.79 m. ov. hange hg. o bined epth2 Vid 5.67 4.40 7.34 7.55 7.26 6.92 3.06 1.76 6.79 6.76 6. 5.37 4.83 7.35 7.22 3.99 6.25 3.44 2.42 7.84 8.04 8.09 5.02 4.82 6.64 6.58 4.42 4.67 5.29 2.21 7.45 5.36 7.57 5.39 5.27 7.10 6.42 4.85 6.15 5.60 3.62 7.74 7.29 7. 4.05 4.71 8.48 7.14 5.60 7.31 5.53 4.66 8.57 8.01 8.52 Vid2Pose. Pose2 Vid easoning 5.62 5.33 6.68 6.66 4.39 4.49 5.43 3. 7.87 8.07 8.48 0.52 2.58 7.85 2.62 4.69 7.13 2.66 2.45 8.57 5.60 8.31 5.24 4.13 7.61 7.52 4.41 5.68 3.26 1.10 6.79 7.16 7.54 Effect 5.80 4.67 7.24 7.32 6.30 5.16 3.62 2.42 6.32 5.92 6.40 ove 5.49 4.55 6.93 7.21 5.52 4.81 2.91 1.02 6.82 7.46 4.01 Sketch2 Vid Stylization Vid2 epth Vid2Sketch 3.73 4.00 8.49 5.29 5.55 6.52 5.20 3.33 8.70 7.76 7.92 6.01 5.33 6.75 6.58 6.73 4.65 6.32 3. 7.46 6.45 7.43 5.88 5.17 8.41 7.35 4.79 3.88 4.45 3.53 8.70 8.95 8.89 5.54 4.45 8.05 7.56 4.40 3.72 4.02 1.37 8.93 8.96 8.98 verall 4.99 4.59 7.52 6.72 5.22 5.46 4.31 2.55 7.76 7.42 7.83 Table 5. Breakdown of per-category EgoEditBench VLM scores for EgoEdit and baselines."
        },
        {
            "title": "Method",
            "content": "VLM PS TA TC F. Failed Experiments Usage of unfiltered data. We initially experiment with video editing dataset corpora consisting of lightly filtered video editing pairs. In this setting, we notice weak video editing performance, with the model often reproducing artifacts encountered in low-quality video editing data pairs such as failure of an object in being replaced with different object, or failure in adding an object. This motivates us to integrate extensive automated and manual curation into EgoEditData at the expense of dataset size, which resulted in increased editing performance. Usage of detailed editing instructions. We initially experiment with simple editing instructions for EgoEditData that are obtained by filling templates with known source and target object names such as Replace source object with target object, where source and target objects are represented by strings obtained during the Object names extraction and Object Editing stages of the data curation pipeline. We observe low editing performance when training on such prompts. Such captions suffer from several issues: (i) their variety is limited, (ii) object names extracted in the Object names extraction stage are often short, (iii) objects in generated target videos produced in the Object Editing stage might not faithfully correspond to the requested edited object description. After training on editing instructions using GPT-5 Mini [45], we observe increased ability to follow instruction prompts. EgoEdit EgoEdit-DMD EgoEdit-RT 7.80 7.42 7.83 19.09 18.95 19.04 16.91 16.52 16.49 96.74 96.87 96.49 Table 6. Quantitative comparison of EgoEdit variants on EgoEditBench: VLM is VLM evaluation score, PS is Pick Score, TA is Text Alignment, TC is Temporal Consistency. EgoEditRT is the real-time version, and EgoEdit-DMD is the bidirectional DMD variant. ated on in-the-wild cases, distilled variants (EgoEdit-DMD, EgoEdit-RT) exhibited lower capacity of handling complex out-of-distribution editing instructions. In addition, Table 6 reports full evaluation scores on EgoEditBench for all distilled variants of EgoEdit. E.4. Additional Dataset Ablation Results In Figure 13, we show qualitative examples produced by different variants of EgoEdit trained on our training data mixture same as the main experiment  (Table 1)  with reduced portions of EgoEditData. As the portion of EgoEditData increases, we notice increase quality of edits and improved alignment to the source video. We additionally evaluate whether increasing the amount of data in EgoEditData can result in improved performance in non-egocentric benchmarks. Table 7 compares variants of EgoEdit trained with data mixture same as the main experiment including versions of EgoEditData of reduced size, on the EditVerseBench benchmark, which mostly consists of exocentric videos. We find that the introduction of EgoEditData consistently improves the overall benchmark score, with particular regards to the Reasoning, Remove, Camera Movement, and Change categories. We speculate these improvements originate from strict quality filtering and descriptive captions, whose benefits extend beyond the egocentric case. 18 m. ov. 6.27 6.43 7.37 7."
        },
        {
            "title": "A dd",
            "content": "7.80 7.23 7.73 7."
        },
        {
            "title": "C hange",
            "content": "6.38 6.82 6.88 7.46 hg. 8.33 8.23 7.80 8."
        },
        {
            "title": "C o m bined",
            "content": "5.54 7.42 5.29 7.17 epth2 Vid 7.63 6.00 8.07 8.13 Pose2 Vid 8.17 8.63 8.63 8."
        },
        {
            "title": "R easoning",
            "content": "6.43 6.73 7.53 8."
        },
        {
            "title": "R e m ove",
            "content": "4.30 5.00 5.27 7.00 Sketch2 Vid Stylization 7.40 6.63 7.53 7.17 7.87 8.20 8.60 8."
        },
        {
            "title": "O verall",
            "content": "6.89 7.00 7.30 7.79 % of EgoEditData 0% 25% 75% 100% Table 7. EditVerseBench [26] results for our model trained on data mixture where the number of samples retained in EgoEditData is varied. As the amount of retained samples in EgoEditData increases, non-egocentric editing performance as measured by EditVerseBench [26] increases, showing usefulness of EgoEditData beyond the egocentric case. Figure 9. Exocentric video edits produced in real time by EgoEdits streaming variant EgoEdit-RT on single H100 GPU. Figure 10. In-the-wild video edits produced in real time by EgoEdits streaming variant EgoEdit-RT on single H100 GPU. 20 Figure 11. Qualitative comparison of EgoEdit and its real-time streaming variant EgoEdit-RT against baselines on EgoEditBench. EgoEdit and EgoEdit-RT consistently perform better than their baselines. Note that Senorita-2M uses the first frame from EgoEdit for frame propagation. 21 Figure 12. Qualitative comparison of EgoEdit at different stages of distillation. EgoEdit indicates the original 80 NFEs model, EgoEditDMD represents the model after the 4-step DMD distillation, EgoEdit-RT represents the final real-time streaming model obtained after Self Forcing distillation. Figure 13. Qualitative comparison of different variants of EgoEdit trained on data mixture with reduced portions of EgoEditData. Percentages indicate the proportion of unique source video samples in EgoEditData retained for training."
        }
    ],
    "affiliations": [
        "Rice University",
        "Snap Research",
        "University of Oxford"
    ]
}