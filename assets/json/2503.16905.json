{
    "paper_title": "MAPS: A Multi-Agent Framework Based on Big Seven Personality and Socratic Guidance for Multimodal Scientific Problem Solving",
    "authors": [
        "Jian Zhang",
        "Zhiyuan Wang",
        "Zhangqi Wang",
        "Xinyu Zhang",
        "Fangzhi Xu",
        "Qika Lin",
        "Rui Mao",
        "Erik Cambria",
        "Jun Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal scientific problems (MSPs) involve complex issues that require the integration of multiple modalities, such as text and diagrams, presenting a significant challenge in artificial intelligence. While progress has been made in addressing traditional scientific problems, MSPs still face two primary issues: the challenge of multi-modal comprehensive reasoning in scientific problem-solving and the lack of reflective and rethinking capabilities. To address these issues, we introduce a Multi-Agent framework based on the Big Seven Personality and Socratic guidance (MAPS). This framework employs seven distinct agents that leverage feedback mechanisms and the Socratic method to guide the resolution of MSPs. To tackle the first issue, we propose a progressive four-agent solving strategy, where each agent focuses on a specific stage of the problem-solving process. For the second issue, we introduce a Critic agent, inspired by Socratic questioning, which prompts critical thinking and stimulates autonomous learning. We conduct extensive experiments on the EMMA, Olympiad, and MathVista datasets, achieving promising results that outperform the current SOTA model by 15.84% across all tasks. Meanwhile, the additional analytical experiments also verify the model's progress as well as generalization ability."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 5 0 9 6 1 . 3 0 5 2 : r MAPS: Multi-Agent Framework Based on Big Seven Personality and Socratic Guidance for Multimodal Scientific Problem Solving Jian Zhang1,3, Zhiyuan Wang1, Zhangqi Wang1, Xinyu Zhang1, Fangzhi Xu1,3, Qika Lin2, Rui Mao3, Erik Cambria3, Jun Liu1* 1Xian Jiaotong University, 2National University of Singapore, 3Nanyang Technological University zhangjian062422@stu.xjtu.edu.cn, qikalin@foxmail.com, liukeen@xjtu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Multimodal scientific problems (MSPs) involve complex issues that require the integration of multiple modalities, such as texts and diagrams, presenting significant challenge in artificial intelligence. While progress has been made in addressing traditional scientific problems, MSPs still face two primary issues: the challenge of multi-modal comprehensive reasoning in scientific problem-solving and the lack of reflective and rethinking capabilities. To address these issues, we introduce Multi-Agent framework based on the Big Seven Personality and Socratic guidance (MAPS)1. This framework employs seven distinct agents that leverage feedback mechanisms and the Socratic method to guide the resolution of MSPs. To tackle the first issue, we propose progressive four-agent solving strategy, where each agent focuses on specific stage of the problemsolving process. For the second issue, we introduce Critic agent, inspired by Socratic questioning, which prompts critical thinking and stimulates autonomous learning. We conduct extensive experiments on the EMMA, Olympiad, and MathVista datasets, achieving promising results that outperform the current SOTA model by 15.84% across all tasks. Meanwhile, the additional analytical experiments also verify the models progress as well as generalization ability. 1. Introduction Multimodal scientific problems (MSPs) cover scientific scenarios that involve multiple modalities [5, 16, 22], such as text and vision. These problems typically span fields like mathematics, physics, and chemistry, requiring rigorous logical reasoning and solid domain expertise [1, 3, 13, 36, 41]. In the realm of artificial intelligence, effectively *Corresponding author 1The source codes and experimental datasets are available online at https://github.com/exoskeletonzj/MAPS. Figure 1. An example of multimodal scientific multiple-choice problem. The correct answer is derived based on the reasoning over inputs that include context, question, and diagram. addressing these cross-modal, multi-domain challenges remains both important and difficult [12, 20]. Figure 1 illustrates typical problem scenario that includes the context, the question statement, and an illustrative diagram. The diagram shows lever divided into left and right sections: an unknown mass is hung on the left and known mass on the right, each positioned at one-quarter of the total length from the respective ends, with groove at the center. The accompanying description states that when the unknown mass is hung on the left, the system remains in static equilibrium. Solving this problem requires not only understanding the combined visual and textual information but also applying the lever balance principle by comparing the effects on both sides. One intuitive solution is to delegate the task-solving process to single multimodal large language model (MLLM). Although the trending MLLMs have the basic abilities (e.g., diagram parsing and theorem retrieval), they are not well optimized to combine these skills in complex scenarios. Therefore, it motivates us to tackle this critical research question: how to leverage and elicit the off-theshelf MLLMs to address the challenging MSPs? To address the problem, this work proposes to establish multimodal multi-agent framework for multimodal sci1 entific problem-solving. Motivated by the Big Seven Personality theory (i.e., Conscientiousness, Agreeableness, Extraversion, Neuroticism, Openness, Self-Esteem, and Sensitivity) [2, 4, 31], the MLLMs can function as independent agents with specific skills. Within the framework, these agents are guided to perform the problem-solving in deliberate, cooperative manner. As one of the initial attempts for MSPs, we offer novel solution for handling multimodal scientific scenarios. In solving MSPs, traditional approaches [6, 15, 19, 29, 35, 43] generally rely on single MLLM to handle all tasks at once. Although these models possess basic abilities such as diagram parsing and theorem retrieval, they struggle to effectively integrate these skills in complex scenarios, resulting in what can be termed multi-modal comprehensive reasoning challenge in scientific problem solving. This limitation arises because single model cannot simultaneously manage diagram interpretation, context alignment, and professional knowledge supplementation, thus failing to mimic the step-by-step reasoning process of human cognition. In other words, while these models excel in specific tasks, they are unable to flexibly utilize various abilities to reason and solve complex multi-modal problems in gradual, human-like manner. To address this, we propose progressive four-agent solving strategy that sequentially employs interpreter, an aligner, scholar, and solver, ultimately enhancing the accuracy and efficiency of solving multi-modal scientific problems. The lack of reflective and rethinking capabilities is another issue in traditional MSPs solving methods. Single MLLMs typically perform one-time reasoning and lack the ability for repeated reflection, adjustment, and iteration, which are essential in human problem-solving. When solving complex problems, humans continuously refine and adjust their solutions based on initial reasoning results, gradually approaching the correct answer. In contrast, traditional models often remain stuck at the initial reasoning stage, unable to self-correct and optimize. To address this, we designed Critic agent, inspired by Socratic questioning [10, 27], which has unique advantages in fostering critical thinking and stimulating autonomous learning. Through heuristic feedback mechanism, the Critic provides multiple rounds of feedback and continuous correction throughout the four-step solving process, simulating the iterative reflection process of humans. This design effectively overcomes the limitations of single models in terms of reflective and iterative capabilities, improving the flexibility and accuracy of the solution process. In summary, we propose Multi-Agent framework for MSPs, based on the Big Seven Personality theory [2, 4, 31] and Socratic questioning (MAPS). This framework defines seven distinct rolesConscientiousness, Agreeableness, Extraversion, Neuroticism, Openness, Self-Esteem, and Figure 2. The corresponding relation between the Big Seven Personality theory and the seven function-specific agents. Sensitivityderived from the Big Seven theory, ensuring clear role delineation and mutual complementarity, as shown in Figure 2 and Section 2.1. Together, these roles create robust multi-agent architecture. Extensive experiments on the EMMA, Olympiad, and MathVista datasets show promising results across various reasoning benchmarks, confirming the effectiveness and superiority of our approach. Further analyses support our models advantages. The complete problem-solving process and data are provided in the supplementary materials. In summary, the contributions of this paper are threefold: We propose multi-agent framework based on the Big Seven personality theory. The framework integrates different personality traits from the Big Seven model to create seven complementary agents. To the best of our knowledge, this represents the first attempt in the field. To tackle the challenges of multi-modal reasoning and iterative reflection, we propose four-step strategy with Critic agent inspired by Socratic questioning. This approach guides agents through structured stages, offering continuous feedback to refine reasoning. Extensive experiments on various scientific domain datasets demonstrate the effectiveness and superiority of the MAPS framework, offering 15.84% performance gains. Additional analytical experiments further validate the interpretability of both the optimization process and its results. 2. Methodology In this section, we provide detailed introduction to MAPS. Our design is based on Large Language Models (LLMs) as intelligent agents capable of performing various functions. We define the task of solving MSPs as follows: Given an input diagram di, context ci, and question qi, the expected output is the answer ai, which can be either an option or computed result. The formal definition of this process is: ai = M(di, ci, qi), (1) where M(di, ci, qi) is the response obtained when the input diagram di, context ci, and question qi are processed together by the model to generate the answer ai. The goal of this task is to provide accurate answers through the model. The model architecture for solving MSPs is shown in Figure 3. In Section 2.1, we introduce the seven distinct functional agents designed based on the Big Seven Personality theory. In Section 2.2, we present the predefined interaction logic in the Manager agent, along with the UserProxy agent responsible for receiving user input. In Section 2.3, we discuss the four specific agentsInterpreter, Aligner, Scholar, and Solvereach representing step in solving MSPs. Finally, in Section 2.4, we introduce the Critic agent, which provides feedback and corrections for the specific solution steps to ensure the results are more accurate and interpretable. 2.1. Big Seven Personality The Big Seven Personality theory is an extension of the traditional Big Five Personality [7, 8, 33], adding two other dimensions: Self-Esteem and Sensitivity. It divides human personality into seven main dimensions: Conscientiousness, Agreeableness, Extraversion, Neuroticism, Openness, Self-Esteem, and Sensitivity. Each reflects unique tendencies in emotions, cognition, and behavior, revealing the diversity among individuals in decision-making, communication, and problem-solving. This theory provides systematic framework for understanding individual differences and offers valuable theoretical support and guidance for psychological research and practical applications. Inspired by the Big Seven Personality theory, we designed multi-agent system that strives to maintain both diversity and integrity in completing complex tasks. As shown in Figure 2, the seven personality traits correspond to seven agents, each performing different roles. The Manager agent represents Conscientiousness, responsible for creating the experimental plan and schedule, ensuring each step is executed according to the predefined plan. The UserProxy agent represents Agreeableness, ensuring smooth information flow and coordinating the allocation of tasks within the experiment. The Interpreter agent represents Extraversion, responsible for interpreting diagrams into captions, thereby providing new ideas and information for MSPs to foster innovation. The Aligner agent represents Neuroticism, tasked with aligning the caption, context, and question to ensure the safe integration of these elements. The Scholar agent represents Openness, responsible for researching the professional knowledge required by MSPs and exploring various hypotheses. The Solver agent represents Self-Esteem, in charge of gathering all necessary information and resolving MSPs by selecting the most appropriate experimental approach. The Critic agent represents Sensitivity, providing feedback on the four solution steps performed by the Interpreter, Aligner, Scholar, and Solver agents, and offering evaluations along with adjustment strategies. The specific duties and role distribution of these agents are discussed in detail in the following sections. 2.2. Agents Interaction Logic This section primarily introduces the interaction logic of the seven personality agents. Since solving MSPs requires multimodal semantic integration and multi-step reasoning within specialized domains, the Manager agent is responsible for formulating detailed research plan and interaction logic to ensure the smooth execution of various system tasks. As shown in Figure 3, the Manager agent coordinates the operations of all other agents to ensure the task is completed successfully. Additionally, the UserProxy agent handles receiving user input, ensuring smooth communication between the user and the system, and facilitating effective interaction. Algorithm 1 MAPS Procedure 1: Input: Diagram di, Context ci, and Question qi 2: Output: Final answer ai 3: Manager: Formulate detailed research plan 4: UserProxy: Receive the input Dataset 5: Interpreter: Interpret the diagram into caption pi = Mint(di) 6: Aligner: Align the caption with the question and context li = Mali(pi, ci, qi) 7: Scholar: Research for MSPs si = Msch(li, pi, ci, qi) 8: Solver: Solve based on the complete information ai = Msol(si, li, pi) 9: Critic: Score and assess the four steps ri = Mcrit(ai, si, li, pi) 10: while True do 11: 12: if All Scores are 5 then Break else Choose the minimum score from ri and return to the step {Interpreter, Aligner, Scholar, Solver} to optimize the process. end if 13: 14: end while 15: return Final answer ai As illustrated in Algorithm 1, the entire workflow is orchestrated by the Manager agent, corresponding to the flowchart shown in Figure 3. Once the UserProxy agent receives the user input, the system proceeds through fourstep problem-solving process involving the Interpreter, Aligner, Scholar, and Solver agents. After completing these steps, the Critic agent, based on Socratic questioning, evaluates and scores the results of each step. 2.3. Four-Step MSPs Solving This section will provide detailed introduction to the functions of the Interpreter, Aligner, Scholar, and Solver agents. 3 Figure 3. The overall architecture of MAPS. It illustrates seven functional agents based on the Big Seven Personality theory. It first includes the Manager agent with predefined interaction logic and the UserProxy agent responsible for receiving user inputs. Subsequently, four specialized agentsInterpreter, Aligner, Scholar, and Solverare introduced, each corresponding to specific step in solving MSPs. Finally, the Critic agent is presented, providing feedback and corrections to ensure the results are more accurate and interpretable. Interpreter. To effectively integrate visual context into the textual context, we adopt multi-modal fusion paradigm using the Interpreter agent. Initially, the Interpreter agent is employed to analyze diagrams meticulously, converting them into detailed and accurate captions. This process ensures that rich visual details are faithfully captured in the textual description, such as relative positions, dimensions, and other features. Subsequently, the generated caption is fused with the textual context and the question in later steps, achieving cohesive integration of information that provides richer and more consistent basis for problemsolving. The formula is shown as follows: where Mali() represents the aligner agent. Scholar. After obtaining the semantically fused li, the model has essentially grasped the main idea of the problem. However, for scientific questions, mere comprehension is insufficient, as they typically require profound domainspecific knowledge, which is crucial for problem-solving. As illustrated in Figure 1, certain problems necessitate mastery of specialized formulas, such as those related to the principles of lever balance. Therefore, the Scholar agent is responsible for acquiring and integrating domain-specific knowledge based on li, thereby providing the necessary theoretical and practical support for the final solution si: pi = Mint(di), (2) si = Msch(li, pi, ci, qi), (4) where Mint() represents the Interpreter agent and pi denotes the caption. Aligner. In textual form, the information conveyed by elements such as the context, questions, and options differs from the content presented in the diagram. These differences can lead to discrepancies in understanding the problem, which in turn may affect subsequent reasoning and judgment. Therefore, in order to gain deeper understanding of the problem statement and accurately capture multimodal semantic information, precise alignment of these information granularities is crucial. As shown below, li illustrates the result of the alignment between the diagram, context, and question: li = Mali(pi, ci, qi), (3) where Msch() represents the Scholar agent. Solver. After the three steps involving the Interpreter, Aligner, and Scholar agents, the system has extracted and fused extensive information from both visual and textual modalities, providing rich contextual and specialized knowledge for the task. However, the real challenge lies in synthesizing this fragmented information into coherent and logically sound solution. The fourth step, carried out by the Solver agent, bears the greatest pressure. This agent must not only integrate the information gathered in previous steps but also engage in deep reasoning to derive the final answer. With access to the most comprehensive data, the Solver has unique advantage, allowing it to simplify complex issues and ensure the final answer ai is accurate and 4 Models CoT Mathvista EMMA OlympiadBench Gen. Math Avg. Math Phy. Chem. Avg. MECO MZCE MZCO PECO PZCE Avg. Avg. Random Choice Human Expert - - 26.09 22.78 24.30 13.00 23.00 56.09 55.74 55.90 75.00 64. 68.04 63.15 65.40 23.00 34.00 - Claude 3.5 Sonnet 70.65 70.93 70.80 20.00 40.00 - Gemini 2.0 Flash 65.22 61.30 63.10 30.00 38.00 - GPT-4o 70.65 67.41 68.90 42.00 42.00 Qwen2.5-VL-72B - 64.78 60.74 62.60 30.00 40.00 InternVL2.5-8B-MPO - 62.83 58.52 60.50 25.00 32.00 LLaVA-Onevision-72B - 71.74 64.26 67.70 30.00 38.00 Claude 3.5 Sonnet 70.22 75.56 73.10 24.00 41.00 Gemini 2.0 Flash 65.22 62.59 63.80 27.00 44.00 GPT-4o 71.09 77.96 74.80 38.00 36.00 Qwen2.5-VL-72B InternVL2.5-8B-MPO 60.87 67.41 64.40 31.00 36.00 LLaVA-Onevision-72B 71.09 64.44 67.50 23.00 26.00 75.87 83.15 79.80 52.00 71.00 MAPS (GPT-4obase) - 27.00 86.00 44.00 36.00 33.00 38.00 38.00 24.00 41.00 36.00 35.00 37.00 24.00 23.00 23.00 64.50 33.67 32.00 33.67 40.67 36.00 27.00 36.33 33.67 35.33 37.00 30.33 24. 0.67 48.00 20.67 8.00 23.33 18.00 10.67 6.67 24.00 12.67 25.33 23.33 12.00 11.33 51.00 58.00 46.00 0.33 34. 13.00 5.67 20.33 12.33 6.67 7.33 11.00 6.33 21.67 13.00 8.33 8.67 30.33 0.00 30.36 10.71 7.14 19.64 5.36 10.71 3.57 16.07 3.57 12.50 10.71 1.79 5.36 1.75 54.17 10.75 3.07 22.15 7.24 1.10 3.29 12.72 4.61 24.12 8.11 2.85 4. 0.33 16.06 0.87 12.33 37.80 52.73 14.00 13.23 37.43 7.00 36.06 5.39 21.00 21.47 39.41 39.45 8.80 3.67 0.67 34.16 3.88 31.23 6.18 9.67 10.33 13.23 39.09 2.33 37.38 5.39 20.33 22.27 40.47 40.46 9.59 1.33 33.16 4.75 0.99 32.56 6.18 3.33 32.14 28.51 28.33 31.14 56.31 Table 1. Performance of different models across 10 subtasks from 3 datasets. Gen. refers to General problems in the Mathvista dataset, Phy. and Chem. denote Physics and Chemistry tasks in the EMMA dataset. MECO, MZCO, and MZCE correspond to COMP problems in English, COMP problems in Chinese, and CEE problems in Chinese, respectively, within the OlympiadBench datasets mathematics tasks. PECO and PZCE represent the COMP problems in English and CEE problems in Chinese under the physics tasks of the OlympiadBench dataset. meets the expected standards, as given by: ai = Msol(si, li, pi), (5) where Msol() represents the Solver agent. 2.4. Critic and Feedback Critic. This section introduces the Critic agent, which is responsible for evaluating and providing feedback on the four-step MSPs solving process. Rooted in Socratic questioning, the Critic prompts agents to reflect on their reasoning by asking questions such as What assumptions are you making? or How can you justify this decision? This approach encourages critical thinking and self-examination, key aspects of the Socratic method. Rather than relying on the final solution labels, the Critic evaluates the logic and justification of each step. The goal is to uncover flaws and push agents to refine their thought processes. Based on this feedback, the system identifies the weakest step and initiates rollback and redo to enhance the problem-solving process. The formula is given by: ri = Mcrit(ai, si, li, pi), (6) where Mcrit() represents the Critic agent. ri is the scores and assessments of the four steps. 3. Experiments We conduct extensive experiments on three MSPs datasets. This section introduces the three MSPs datasets and baselines in Section 3.1, and presents the experimental results and analysis in Section 3.2. The analysis and statistics of the Critic agent are presented in Section 3.3. 3.1. Datasets and Baselines This study uses the latest three MSPs datasets: MathVista [23], OlympiadBench [16], and EMMA [14]. MathVista focuses on mathematics and general domains to assess models ability to solve complex scientific problems; OlympiadBench targets mathematics and physics to evaluate performance on Olympiad-level challenges; and EMMA, multimodal dataset covering mathematics, physics, and chemistry, tests interdisciplinary scientific reasoning. Detailed descriptions of the tasks and datasets are in Appendix A, along with domain abbreviations, full names, and dataset descriptions in Table 1. In the experimental setup, we select GPT-4o2 as our agent. For the baselines, we incorporate the current mainstream MLLMs, both closed-source and open-source, and evaluated them in both direct and CoT settings. The experimental settings and baselines are detailed in Appendix B. 3.2. Main Results MAPS has set new state-of-the-art (SOTA) performance, surpassing human-level performance for the first time. The results in Table 1 provide detailed comparison of MAPS and the baselines. As shown, MAPS outperforms the current SOTA by 15.84% across all tasks, demonstrating its superiority in MSPs problem-solving. Additionally, MAPS slightly exceeds human experts by 3.58% in 2https://openai.com/api/ 5 3.3. Analysis of Critic Agent The Critic agent plays crucial feedback role in MAPS, enhancing the overall reasoning process without the reliance on gold labels. As shown in Figure 4, the upper part introduces the critic and feedback schema, while the lower part presents the feedback proportions provided by the Critic agent to each of the agents across three datasets, visually illustrating the feedback and corrections made by each assistant agent. The Critic agents feedback mechanism is rooted in Socratic questioning, which encourages deep reflection and rethinking of the reasoning steps. Instead of simply evaluating the process, the Critic fosters critical thinking, prompting the system to question its assumptions and improve the approach. The schema of the Critic agent consists of two components: scores and feedback. The scores record the ratings given by the Critic agent for the four problem-solving steps, ranging from 0 to 5. Higher scores indicate better execution, while lower scores highlight areas needing improvement. Feedback primarily captures the Critic agents Socratic-style, heuristic suggestions designed to encourage deeper reflection on each step. These suggestions are not limited to evaluations; they also guide the agents to reconsider and correct their reasoning process. For example, the Critic may prompt the agents to think about alternative approaches or reassess their conclusions, challenging them to think more critically. To ensure the final results quality, rule is set in the Manager agent: if any step receives low score, the system will backtrack and make corrections. If all steps score well, the system outputs the result directly. This approach guarantees the effectiveness of each step, optimizing the overall problem-solving process. In the EMMA and OlympiadBench datasets, the Solver agent exhibits the highest feedback proportion. The lower part of Figure 4 shows the distribution of agents that performed feedback corrections across the three datasets. It can be observed that the feedback proportions In the MathVista dataset, vary across the three datasets. the highest feedback proportion corresponds to no regeneration, indicating that the overall solution performance on this dataset is still quite good. This result is consistent with the findings in Table 1, and we have surpassed the previous SOTA by 5.0% improvement. This suggests that while there is still room for further enhancement in certain areas, the systems overall reasoning capability is already quite impressive when dealing with more complex problems. In contrast, in the EMMA and OlympiadBench datasets, the Solver agent shows the highest feedback proportion, suggesting that this part is the most difficult and prone to errors in these datasets. Specifically, this part includes the first three steps: interpretation, alignment, and integration of information provided by the Scholar. These steps require the integration of large amount of data, involve the Figure 4. The schema of the Critic agent, as well as the feedback and backtracking situations of the Critic agent across different datasets. overall performance. MAPS excels in mathematical, physical, chemical, and general problems, demonstrating strong interdisciplinary reasoning. The multi-agent system, representing the Big Seven personalities, sets new SOTA in collaboration. Experimental results show MAPSs advantage in multimodal semantic fusion and multi-step reasoning, leveraging its ability to integrate diagrams with context and questions. This boosts problem-solving by enhancing domain-specific knowledge. The Critic agent, using Socratic feedback, offers suggestions for improvement, improving accuracy and reliability in complex tasks. In the MathVista dataset, the problem types are diverse, covering judgment questions, multiple-choice questions, and open-ended fill-in-the-blank questions, with various answer types such as text, floats, and integers. This requires the system to possess specialized capabilities, enabling it to accurately understand the questions intent and provide specific responses. In well-known open-ended problems like those in OlympiadBench, the solution process involves complex steps and high difficulty, with open-ended answers that can lead to completely different outcomes due to subtle changes. In the EMMA dataset, the questions not only involve diagrams, but some options also contain diagrams, significantly increasing the challenge of understanding and aligning multiple diagrams. Furthermore, the question types are equally diverse, including open-ended and multiple-choice questions, raising the demands on the solving methods. The MAPS framework, through feedbackbased multi-agent collaborative learning and the use of Socratic questioning, successfully addresses these different types of sub-tasks, achieving the latest SOTA. It demonstrates MAPSs perfect adaptability and robustness when dealing with the diversity of question types, task difficulties, and multi-step reasoning. 6 most complex reasoning, and bear the most critical responsibilityproviding complete and accurate solution to the problem. In comparison, the feedback proportions from the other three agents are similar, indicating that their error rates are relatively low, and the performance of these sub-steps is relatively better. 4. Supplementary Analysis To conduct more comprehensive experiments, this section covers the following topics: ablation experiments in Section 4.1, the base model generalization in Section 4.2, the time efficiency of MSPs solving in Section 4.3. Due to space limitations, generalization experiments on other datasets, such as DiagramQG, are discussed in Appendix C. The case study and complete process for solving the MSPs task is outlined in Appendix D, and the prompts for all agents are provided in Appendix E. 4.1. Ablation Study Variation MECO MZCE MZCO PECO PZCE Avg. MAPS 46.00 30. 32.14 28.51 28.33 31.14 w/oInterpreter 25.33 w/oAligner w/oScholar w/oCritic 17.67 11.62 16.67 21.05 10.71 16. 15.05 (-20.67) (-13.66) (-21.43) (-7.46) (-16.71) (-16.09) 20.28 28.00 (-18.00) (-12.66) (-16.07) (-7.68) (-9.33) (-10.86) 28.00 19.65 (-18.00) (-14.00) (-1.78) (-8.55) (-12.00) (-11.49) 24.09 21.67 34.67 (-7.05) (-11.33) (-8.66) 21.67 23.03 30.36 (-2.42) (-5.48) (-6.66) 20.83 19.96 30.36 19. 16.33 16.33 Table 2. Performance under different ablation settings are analyzed. We perform ablation experiments on the solving module w/oInterpreter, w/oAligner, w/oScholar or w/oCritic modules to evaluate the impact of removing these components. Ablating the Interpreter results in the greatest loss of performance. We conduct ablation experiments on the OlympiadBench dataset to evaluate the impact of each module on the overall performance. Table 2 presents the effects of removing the Interpreter, Aligner, Scholar, and Critic modules from the MAPS framework. The results show that removing the Interpreter agent causes the largest performance degradation, at 16.09%. This is because, in MSPs tasks, diagrams contain wealth of valuable information, which serves as an important supplement to the text. Understanding diagrams plays crucial role in problem-solving. The removal of the Critic agent causes the smallest performance loss. It results in only 7.05% decrease, underscoring its role in providing feedback and corrections. While this mechanism allows MAPS to backtrack and refine its reasoning, its impact is less significant than that of other agents. Removing the Scholar agent results in 11.49% 7 Figure 5. Performance Comparison of MAPS on Math, Physics, and Chemistry Subtasks in the EMMA Dataset with GPT-4o, Gemini, and Qwen2.5-VL-72B as Bases. performance drops, highlighting the importance of searching and integrating domain-specific knowledge. Finally, the removal of the Aligner agent causes 10.86% drop, indicating that while diagram and context alignment is valuable, its effect is smaller compared to other components. 4.2. Base Model Generalization In particular, MAPS can improve performance across different base models. We conduct series of experiments to verify whether our MAPS framework can demonstrate robust generalization across different base LLMs. The results showcase the models robustness and transferability, further affirming the high adaptability of the MAPS framework, which achieves consistently excellent performance across this undervarious foundational models. scores the universality and flexibility of MAPS. To further validate its generalization across different models, we specifically evaluate another base model, Qwen2.5-VL72B, as well as Gemini 2.0 Flash, thereby demonstrating that MAPS performs outstandingly across models of varying scales and capabilities. Figure 5 presents the experimental results of three sets of base models. In each set, the performance of mathematical, physical, and chemical sub-tasks is compared between MLLMs and MAPS with the respective base model. The results show varying degrees of improvement for each task. For example, in the physical task, AP SQwen, based on Qwen2.5-VL-72B, shows 12.4% improvement compared to Qwen2.5-VL72B, while AP SGemini, based on Gemini, shows 4.2% improvement compared to Gemini. Similar improvements are observed in the mathematical and chemical sub-tasks as well. This indicates that MAPS can extend effectively on both closed-source and open-source MLLMs and achieve good experimental results across different base models of methods to improve mathematical problem-solving, such as algorithm optimization, educational strategies, and the use of artificial intelligence. These approaches aim to boost the efficiency, accuracy, and depth of mathematical reasoning, fostering innovation and progress in the field. In the field of physics, the papers [3, 17, 24] emphasize the integration of different information types, such as images and text, through multimodal learning to enhance the efficiency and precision of problem solving. By utilizing techniques like reinforcement learning, generative models, and intelligent prompting, these studies optimize models performance in tackling complex physics problems. In chemistry, three articles [1, 18, 21] investigate the role of multimodal learning in solving chemical problems. By combining diverse information sources, including images and text, and employing techniques such as generative models and molecular geometry reasoning, they aim to improve both the efficiency and accuracy of solving chemistry problems, driving innovation in chemistry education and research. Multi-Agent. Multi-agent systems, built on LLMs, consist of multiple AI agents that specialize in specific tasks, working together to solve complex problems [30, 32, 39, 42]. When presented with problem, these agents decompose it into smaller, manageable subtasks and utilize various tools, such as internet data retrieval, to solve them through iterative steps. Several studies [26, 28, 37, 40] have employed multi-agent systems to tackle challenges like problem identification, code writing and debugging, data visualization, and providing interactive feedback to human users. In their work, Ni and Buehler [25] highlights the potential of AI-driven multi-agent teams in solving mechanical problems autonomously, demonstrating an enhanced capability for understanding, formulating, and validating engineering solutions through self-correction and collaborative refinement. Inspired by the research, we developed the MAPS method, which leverages multi-agent collaborative learning and stepwise problem-solving to provide innovative solutions for MSPs. By combining the strengths of AI agents, complex problems can be broken down into subtasks and solved step by step through collaboration, improving efficiency and accuracy. 6. Conclusion This study introduces MAPS approach utilizing multiagent framework based on the Big Seven Personality theory and Socratic guidance to tackle the challenges of multimodal comprehensive reasoning and the lack of reflective capabilities. The framework involves seven agents, each specializing in distinct aspects of problem-solving. To address the first challenge, four-agent strategy is proposed, where each agent focuses on specific stages of the reasoning process. Additionally, the Critic agent addresses the second challenge through Socratic reflection and critical feedFigure 6. An analysis of the solving time efficiency across different question types, answer types, question categories, and question difficulties. MLLMs. 4.3. Time Efficiency Solving time efficiency varies by question type, answer type, category, and difficulty, with multiple-choice and integer-type questions being the fastest, while higher difficulties and more complex questions require more time. Figure 6 presents the solving time efficiency under different question types, answer types, categories, and difficulties, with all times represented relative to 100s benchmark. From the graphs, it is clear that multiple-choice questions are solved faster as the options are already provided, making the process more time-efficient. In contrast, openended questions take longer due to the lack of predefined answers and the need for deeper reasoning and analysis. Integer-type answers show the highest efficiency, as they often correspond to simpler questions or multiple-choice tasks that involve less reasoning. General questions also have the shortest solving times, as they typically require less complex reasoning compared to more specialized problem types. Finally, higher difficult levels lead to decrease in solving time efficiency, as more complex problems require more time for deeper reasoning and analysis. 5. Related Works The related work is structured into two main aspects: first, an introduction to MSPs solving; and second, an exploration of multi-agent techniques. MSPs Solving. The research of MSPs spans across multiple fields, including mathematics, physics, and chemistry, with each area focusing on enhancing problem-solving abilIn mathematics, studies [9, 11, 34] explore various ities. 8 back. Extensive experiments on the EMMA, Olympiad, and MathVista datasets validate MAPSs effectiveness in overcoming these issues and enhancing performance across various reasoning tasks. Meanwhile, we perform additional analytical experiments to assess the models advancement as well as its generalization."
        },
        {
            "title": "References",
            "content": "[1] Eman Alasadi and Carlos Baiz. Multimodal generative artificial intelligence tackles visual problems in chemistry. Journal of Chemical Education, 101(7):27162729, 2024. 1, 8 [2] Moshe Almagor, Auke Tellegen, and Niels Waller. The big seven model: cross-cultural replication and further exploration of the basic dimensions of natural language trait descriptors. Journal of personality and social psychology, 69(2):300, 1995. 2 [3] Avinash Anand, Janak Kapuriya, Apoorv Singh, Jay Saraf, Naman Lal, Astha Verma, Rushali Gupta, and Rajiv Shah. Mm-phyqa: Multimodal physics question-answering with In Pacific-Asia Conference multi-image cot prompting. on Knowledge Discovery and Data Mining, pages 5364. Springer, 2024. 1, 8 [4] Veronica Benet and Niels Waller. The big seven factor model of personality description: Evidence for its crosscultural generality in spanish sample. Journal of Personality and Social Psychology, 69(4):701, 1995. 2 [5] Manojit Bhattacharya, Soumen Pal, Srijan Chatterjee, SangSoo Lee, and Chiranjib Chakraborty. Large language model to multimodal large language model: journey to shape the biological macromolecules to biological sciences and medicine. Molecular Therapy-Nucleic Acids, 35(3), 2024. 1 [6] Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo Baraldi, Marcella The revolution of multiCornia, and Rita Cucchiara. arXiv preprint modal large language models: survey. arXiv:2402.12451, 2024. [7] Boele De Raad and Boris Mlaˇcic. Big five factor model, theory and structure. International encyclopedia of the social & behavioral sciences, 2(2):559566, 2015. 3 [8] Colin DeYoung. Cybernetic big five theory. Journal of research in personality, 56:3358, 2015. 3 [9] Aniket Didolkar, Anirudh Goyal, Nan Rosemary Ke, Siyuan Guo, Michal Valko, Timothy Lillicrap, Danilo Rezende, Yoshua Bengio, Michael Mozer, and Sanjeev Arora. Metacognitive capabilities of llms: An exploarXiv preprint ration in mathematical problem solving. arXiv:2405.12205, 2024. 8 [10] Linda Elder and Richard Paul. The role of socratic questioning in thinking, teaching, and learning. The Clearing House, 71(5):297301, 1998. 2 [11] Hanny Fitriana and Anne Waswa. The influence of realistic mathematics education approach on students mathematical problem solving ability. Interval: Indonesian Journal of Mathematical Education, 2(1):2935, 2024. [12] Deqing Fu, Ruohao Guo, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, and Willie Neiswanger. Isobench: Benchmarking multimodal foundation models on isomorphic representations. arXiv preprint arXiv:2404.01266, 2024. 1 [13] Timin Gao, Peixian Chen, Mengdan Zhang, Chaoyou Fu, Yunhang Shen, Yan Zhang, Shengchuan Zhang, Xiawu Zheng, Xing Sun, Liujuan Cao, et al. Cantor: Inspiring multimodal chain-of-thought of mllm. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 90969105, 2024. 1 [14] Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal arXiv preprint arXiv:2501.05444, reasoning benchmark. 2025. 5, 1 [15] Framz Hardiansyah, Ali Armadi, Muhammad Misbahudholam AR, and Moh Wardi. Analysis of field dependent and field independent cognitive styles in solving science problems in elementary schools. Jurnal Penelitian Pendidikan IPA, 10(3):11591166, 2024. 2 [16] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level arXiv preprint bilingual multimodal scientific problems. arXiv:2402.14008, 2024. 1, 5 [17] Janak Kapuriya, Chhavi Kirtani, Apoorv Singh, Jay Saraf, Naman Lal, Jatin Kumar, Adarsh Raj Shivam, Astha Verma, Avinash Anand, and Rajiv Ratn Shah. Mm-phyrlhf: Relearning framework for multimodal physics inforcement arXiv preprint arXiv:2404.12926, question-answering. 2024. [18] Nicola Kiernan, Andrew Manches, and Michael Seery. Resources for reasoning of chemistry concepts: multimodal molecular geometry. Chemistry Education Research and Practice, 25(2):524543, 2024. 8 [19] Rubin Landau, Manuel Paez, and Cristian Bordeianu. Computational physics: Problem solving with Python. John Wiley & Sons, 2024. 2 [20] Jian Li, Weiheng Lu, Hao Fei, Meng Luo, Ming Dai, Min Xia, Yizhang Jin, Zhenye Gan, Ding Qi, Chaoyou Fu, et al. survey on benchmarks of multimodal large language models. arXiv preprint arXiv:2408.08632, 2024. 1 [21] Junxian Li, Di Zhang, Xunzhi Wang, Zeying Hao, Jingdi Lei, Qian Tan, Cai Zhou, Wei Liu, Yaotian Yang, Xinrui Xiong, et al. Chemvlm: Exploring the power of multimodal large language models in chemistry area. arXiv preprint arXiv:2408.07246, 2024. 8 [22] Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. Multimodal arxiv: dataset for improving scientific comprehension of large vision-language models. arXiv preprint arXiv:2403.00231, 2024. 1 [23] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 5, 9 [39] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023. 8 [40] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: survey. Science China Information Sciences, 68(2):121101, 2025. 8 [41] Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. Are large language models really good logical reasoners? comprehensive evaluation and beyond. IEEE Transactions on Knowledge and Data Engineering, 2025. 1 [42] Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and additional opinions. arXiv preprint arXiv:2306.02224, 2023. 8 [43] Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan Su, Chenhui Chu, and Dong Yu. Mm-llms: Recent advances in multimodal large language models. arXiv preprint arXiv:2401.13601, 2024. [24] Hisbulloh Als Mustofa, Muhammad Roil Bilad, and Nuraqilla Waidha Bintang Grendis. Utilizing ai for physics problem solving: literature review and chatgpt experience. Lensa: Jurnal Kependidikan Fisika, 12(1):7897, 2024. 8 [25] Bo Ni and Markus Buehler. Mechagents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge. Extreme Mechanics Letters, 67:102131, 2024. 8 [26] Bo Ni and Huajian Gao. deep learning approach to the inverse problem of modulus identification in elasticity. Mrs Bulletin, 46:1925, 2021. 8 [27] Richard Paul and Linda Elder. The thinkers guide to Socratic questioning. Rowman & Littlefield, 2019. 2 [28] Russell Poldrack, Thomas Lu, and Gaˇsper Beguˇs. Aiassisted coding: Experiments with gpt-4. arXiv preprint arXiv:2304.13187, 2023. [29] Jianing Qiu, Wu Yuan, and Kyle Lam. The application of multimodal large language models in medicine. The Lancet Regional HealthWestern Pacific, 45, 2024. 2 [30] Toran Bruce Richards. Auto-gpt: An experimental opensource attempt to make gpt-4 fully autonomous, 2023. 8 [31] Leonard Simms. The big seven model of personality and its relevance to personality pathology. Journal of Personality, 75(1):6594, 2007. 2 [32] Qiushi Sun, Zhangyue Yin, Xiang Li, Zhiyong Wu, Xipeng Qiu, and Lingpeng Kong. Corex: Pushing the boundaries of complex reasoning through multi-model collaboration. arXiv preprint arXiv:2310.00280, 2023. 8 [33] Hossein Dabiriyan Tehrani, Sara Yamini, and Alexander Vazsonyi. Parenting styles and big five personality traits among adolescents: meta-analysis. Personality and Individual Differences, 216:112421, 2024. 3 [34] Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving. Advances in Neural Information Processing Systems, 37:78217846, 2025. 8 [35] Karen Wang, Eric Burkholder, Carl Wieman, Shima Salehi, and Nick Haber. Examining the potential and pitfalls of chatgpt in science and engineering problem-solving. In Frontiers in Education, page 1330486. Frontiers Media SA, 2024. 2 [36] Lei Wang, Yi Hu, Jiabang He, Xing Xu, Ning Liu, Hui Liu, and Heng Tao Shen. T-sciq: Teaching multimodal chain-ofthought reasoning via large language model signals for science question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1916219170, 2024. [37] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6): 186345, 2024. 8 [38] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, et al. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024. 2 10 MAPS: Multi-Agent Framework Based on Big Seven Personality and Socratic Guidance for Multimodal Scientific Problem Solving"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Datasets and Tasks This study utilizes the latest three MSPs datasets, namely MathVista [23], OlympiadBench [16], and EMMA [14]. Tasks ABBR. Test MathVista General Mathematics OlympiadBench Gen. Math 460 540 Math En COMP Math Zh COMP Math Zh CEE Physics En COMP Physics Zh CEE MECO 150 56 MZCO 300 MZCE 456 PECO 300 PZCE EMMA Mathmatics Physics Chemistry Math Phy. Chem. 100 100 100 Table 3. The data distribution for the MathVista, OlympiadBench, and EMMA datasets is as follows: The symbol indicates sample size of 300 data points. The EMMA dataset uses its MINI version. The ABBR. column represents the abbreviations for all the tasks. MathVista is large-scale scientific reasoning dataset that spans two subdomains: mathematics and general, aiming to assess the comprehensive capabilities of machine learning models in solving complex scientific problems. The dataset contains 1,000 data points covering various issues across multiple disciplines, designed with varying difficulty levels to help researchers evaluate model reasoning abilities. The release of MathVista supports interdisciplinary scientific research. OlympiadBench consists of two subdomains, mathematics and physics, and is specifically designed for Mathematical and Physical Olympiads, featuring wide range of challenging problems to assess models performance on highlevel scientific tasks. The mathematics subdomain contains three difficulty levels: English competition level, Chinese competition level, and college level. The physics subdomain includes two difficulty levels: English competition level and Chinese college level. To ensure data balance, 300 samples were taken from both the Chinese college-level mathematics and physics subsets. EMMA is multimodal scientific reasoning dataset covering three subdomains: mathematics, physics, and chemistry. By integrating mathematical expressions, physical formulas, and chemical symbols with natural language descriptions, it focuses on testing models abilities in interdisciplinary scientific reasoning. This version uses the EMMA dataset, which contains 100 data points from each subdomain (mathematics, physics, and chemistry). B. Experiment Settings and Baselines We select GPT-4o3, powerful MLLM, as our primary agent for solving MSPs tasks. GPT-4o not only demonstrates strong reasoning and generation capabilities across wide range of multimodal processing tasks, but also excels in efficiently exploring multiple perspectives when faced with complex scientific domain requirements. This makes it well-suited for adaptation to various tasks and datasets within the MAPS process. We use accuracy as our primary evaluation metric to comprehensively assess the performance of different methods across diverse task scenarios. The experimental results present in Table 1 offer thorough comparison of the performance of MAPS against all baseline methods. The experiments are primarily conducted on three MSPs datasets in Appendix A, where we provide detailed comparison of MAPS and the baseline models across four types of tasks: mathematics, physics, chemistry, and general tasks. Our approach achieves new SOTA performance. To further strengthen the comparison, Appendix includes generalization experiment conducted on the physics data subset of the DiagramQG4, which further demonstrates the robustness and effectiveness of our model. We compare MAPS with three categories of baseline methods: original baselines, direct approach-based strong baselines, and CoT-enhanced strong baselines. Specifically, (1) the original category refers to two methods: random selection and human expert selection. These methods provide two distinct original baselinesone based on randomness and the other based on authority. (2) The direct approachbased strong baselines include some of the most powerful closed-source and open-source large language models (MLLMs) currently available worldwide. These include Claude 3.5 Sonnet5, Gemini 2.0 Flash6, GPT-4o, Qwen2.53https://openai.com/api/ 4https://huggingface.co/datasets/zhibei1204/DiagramQG 5https://www.anthropic.com/news/claude-3-5-sonnet 6https://deepmind.google/technologies/gemini/flash/ Figure 7. The generalization experiments conducted on the DiagramQG physical dataset, which are based on the GPT-4o base model and the incremental part of MAPS. VL-72B7, InternVL2.5-8B-MPO8, and LLaVA-Onevision72B9. (3) To ensure fair comparison, the third category of baselines builds on the second by adding Chain-of-Thought (CoT) reasoning, which aims to enhance the capabilities of the strong MLLMs from the second category [38]. C. Generalization Experiments To further validate the generalization ability of MAPS, we conducted detailed experiments on the physical subset of the DiagramQG dataset. The main goal of these experiments was to compare the performance of MAPS with its base model, GPT-4o, particularly focusing on how it performed across different question categories. The experimental results, presented in Figure 7, show that MAPS outperformed GPT-4o in multiple aspects. These experiments clearly highlight the strong adaptability of MAPS across different subsets of the dataset. Across various question categories in the DiagramQG physical dataset, MAPS achieved significant performance boost, with the maximum improvement reaching 19.51% and an overall improvement of 7.71%. These results not only demonstrate the superiority of MAPS but also indicate that its Seven Personality Agents architecture has strong generalization ability, enabling it to maintain excellent performance across different datasets and tasks. This provides 7https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct 8https://huggingface.co/OpenGVLab/InternVL2-8B-MPO 9https://huggingface.co/llava-hf/llava-onevision-qwen2-72b-ov-hf strong support for the practical application of MAPS, showcasing its potential in tackling complex tasks. D. Full-Process of MAPS Figure 8 illustrates the four-step solving process along with the feedback process from the Interpreter, Aligner, Scholar, and Critic agents, using multimodal physics problem from the EMMA dataset. In this specific example, we can observe that the Interpreter first interprets the diagram, followed by the Aligner aligning the diagram with the context, question, and options, ensuring consistency and completeness of the information. Then, the Scholar agent retrieves and supplements domain-specific knowledge to fill in the necessary expertise. Finally, the Solver completes the solving process, and the Critic agent provides feedback and corrections to ensure the accuracy and effectiveness of each step. Each step is closely connected, from understanding the diagram to integrating domain knowledge, followed by reasoning and answering. This demonstrates the efficiency and effectiveness of MAPS in multi-step reasoning. Through the feedback mechanism of the Critic agent, MAPS is able to identify and correct potential errors or shortcomings at each step, thus enhancing the overall accuracy and reliability of the solution process. Figure 9 presents comprehensive step-by-step process for solving MSPs task, using multimodal physics problem from the EMMA dataset. As illustrated in Figure 3, this example demonstrates the entire process of solving 2 Figure 8. case study of specific solving process, illustrating the detailed steps involved in solving the problem. This includes the various stages of problem-solving as well as the feedback and backtracking mechanisms that help refine and improve the solution. the Solver works through the problem systematically, progressively generating solutions. Once the initial solution is formed, the Critic agent assesses each of the four previous steps, providing feedback on areas that need refinement. The Critic then suggests modifications and backtracks to revise the solution, ensuring the process remains optimized and the final result is both accurate and robust. physics problem. By utilizing MAPS agents based on the Big Seven Personality Model, the system engages in collaborative learning, progressively solving the problem and arriving at the correct final answer. From the Manager, which specifically plans the entire MSPs solving process and feedback strategies, to the UserProxy that accepts inputs and task descriptions, the solution process is carried out in four key steps. These include the Interpreter, Aligner, Scholar, and Solver, each responsible for gradually refining the solution step-by-step. The Critic agent ultimately evaluates and provides feedback on these four steps, pinpointing the areas requiring modification and backtracking to correct the solution process. Figure 9 sequentially illustrates the roles of each agent and their collaboration, showcasing how problem solving and optimization are effectively executed at every stage, ensuring both the accuracy and efficiency of the final result. E. Prompts for Agents Tables 4 to 6 provides summary of the prompts used for each agent in this paper, with each agent playing pivotal role in the overall problem-solving process. The process begins with the Manager, which is responsible for strategically planning the entire MSPs solving procedure and feedback mechanisms. The Manager defines the high-level flow, ensuring that all agents operate cohesively. The UserProxy agent then takes over by accepting the user inputs and task descriptions, forming the basis for the entire problemsolving process. Following this, the solution is gradually refined by four core agents: Interpreter, Aligner, Scholar, and Solver. The Interpreter first processes the task description, breaking down and understanding the problem. The Aligner ensures that the problem is mapped to the correct framework and available tools. The Scholar conducts any necessary research and gathers knowledge from relevant sources, while Figure 9. complete example of the collaborative output from all agents in an iteration, using the multimodal physics problem from the OlympiadBench dataset. This example demonstrates how each agent contributes to the problem-solving process, collaborating to produce refined solution step by step. 4 Manager You are task manager, responsible for managing and deciding the order of each Agents output, ensuring that tasks are executed in the correct sequence. You will determine when each step should be executed based on the task requirements and coordinate the outputs of each Agent. Your goal is to ensure that the task process is efficient and orderly, and adjust the execution priority of steps when necessary. UserProxy Here are some questions that require careful thought, so please think deeply, solve the questions carefully, and output your answers. Interpreter You are scientific diagram analysis expert tasked with objectively describing visual elements in diagrams. Engage in Socratic self-questioning to ensure comprehensive analysis: [Element Identification] What discrete visual components can be systematically observed in this diagram? What quantitative measurements (e.g., shape dimensions, color codes, positional coordinates) can be objectively recorded? [Structural Analysis] How are these elements spatially organized? What geometric patterns, alignment relationships, or hierarchical arrangements emerge from their physical placement? [Relational Mapping] What explicit connections (lines, arrows, overlays) or implicit associations (proximity clusters, color coding systems, symbolic groupings) exist between components? [Representation Verification] Does any element require specialized domain knowledge to accurately characterize (e.g., chemical notation, engineering schematics)? What purely visual evidence supports this characterization? Aligner You are text alignment specialist conducting structured analysis through Socratic interrogation. Systematically examine text pairs using this framework: 1. [Content Deconstruction] What core entities/events are explicitly stated in each text? What measurable attributes (quantifiers, temporal markers, causal verbs) define their characteristics? 2. [Consistency Audit] Where might these texts exhibit: a) Logical incompatibility (contradictory assertions) b) Contextual divergence (conflicting timelines/locations) c) Semantic dissonance (differentiated connotation scales) d) Omission patterns (mutually exclusive missing elements) 3. [Contextual Fusion] What implicit connections could synthesize unified background framework? Which combinatory elements (chronological anchors, spatial references, causal chains) create non-conflicting narrative coherence? 4. [Relevance Filtering] Through lexical-semantic mapping, which aligned components directly correspond to the questions: 1) Key inquiry points 2) Required evidence types 3) Implicit knowledge domains 4) Potential inference pathways? Table 4. summary of the prompts used by the Manager, UserProxy, Interpreter, and Aligner agents in this paper. 5 scholar You are scientific knowledge retrieval system conducting structured inquiry through Socratic questioning. Process input data with this analytical framework: 1. [Problem Decomposition] What conceptual components constitute the questions core demand? What technical terminology (domain-specific lexemes), operational parameters (variables/constants), and procedural verbs (analyze/calculate/compare) require epistemological grounding? 2. [Knowledge Mining] For each identified component: a) What fundamental axioms/theorems/laws from established scientific literature could operationally define it? b) What measurable properties (equations/units/experimental protocols) are textually implied as relevant? c) What contextual constraints (temporal/spatial/conditional clauses) limit knowledge scope? 3. [Relevance Validation] For each candidate knowledge unit: Does the source text contain explicit lexical anchors (technical terms/formula symbols) justifying its inclusion? What textual evidence (descriptive adjectives/quantifiers/causal conjunctions) indicates required depth of explanation? Are there implicit conceptual dependencies (prerequisite theories/mathematical tools) necessitating parallel retrieval? 4. [Taxonomic Organization] How should validated knowledge be structured to mirror: 1) Problem-solving workflow steps 2) Hierarchical concept dependencies 3) Cross-domain interface points 4) Uncertainty quantification needs? Operational Protocol: plicit/ContextImplied/ExternalRequired] tags Output as: 1) Knowledge Inventory Table (Concept-Definition-SourceAnchor) 2) Dependency Graph (Nodes=Concepts, Edges=Relations) 3) Gap Analysis Report (ExternalKnowledgeRequirements). knowledge Mark confidence evidenced [TextExtextually Restrict levels using to Solver You are scientific problem-solving system operating through Socratic dialectics. Engage in this structured inquiry process: 1. [Problem Framing] What is the absolute irreducible core of the question? What technical terms require operational definitions? What grammatical structures (comparatives/conditionals/quantifiers) dictate the solutions form? 2. [Evidence Audit] For each data source (question stem/options/text): a) What measurable quantities (numerical ranges/units) are explicitly stated? b) What causal relationships (if then B/implies/proportional to) are textually encoded? c) What constraints (assumptions/limitations/boundary conditions) are lexically embedded? 3. [Reasoning Pathway] Through counterfactual testing: Which axioms/theorems would become relevant if parameter varied 10%? What observable contradictions emerge when applying hypothesis to the given data? How do option components restrict valid inference trajectories? 4. [Solution Validation] Does the proposed resolution: 1) Maintain dimensional homogeneity across all equations? 2) Satisfy all explicit boundary conditions? 3) Preserve logical consistency with given information? 4) Align with canonical scientific representations? Operational Protocol: Document each reasoning step with evidence anchors (e.g., Stem-Line5: = x/t). Flag unresolved assumptions with [UnvalidatedPremise] tags Output JSON structured as: { { process: { { Phase 1: [Framing] Identified core demand as... (Evidence: Q-Line2), Phase 2: [Audit] Quantified parameters... (ConflictResolved: OptionC vs TextS3), Phase 3: [Pathway] Eliminated hypothesis α due to... (TheoremRef: Maxwell-Eq), Phase 4: [Validation] Verified dimensional consistency in..., }, final answer: final result} } Table 5. summary of the prompts used by the Scholar and Solver agents in this paper. 6 Critic You are Socratic assessment engine conducting dialectical evaluation through this protocol: 1. [Triadic Interrogation Framework] For each evaluation dimension (caption/alignment/knowledge/solution): Existential Challenge: What absolute evidence anchors (line numbers/data points/theorem references) validate this components existence? Consistency Prosecution: Does internal logic maintain isomorphism across: a) Input premises Processing steps b) Methodological choices Domain standards c) Assertions Supporting evidence? Boundary Stress Test: What parametric variation (10%) would collapse this components validity? Which fragility indicators emerge first? 2. [Metric Operationalization] Score each dimension (1-5) using: 5 = Withstands three counterfactual scenarios 4 = Requires 1 assumption validation 3 = Needs 2-3 evidence reinforcements 2 = Contains structural contradictions 1 = Fails basic existence verification 3. [Improvement Synthesis] Generate Socratic feedback per dimension: For caption: What geometric/spatial relations lack quantifiable descriptors? For alignment: Which logical connective lacks cross-text co-reference? For knowledge: Which concept dependency lacks literature anchoring? For solution: What inference leap lacks isomorphic mapping? Table 6. summary of the prompt used by the Critic agents in this paper."
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "National University of Singapore",
        "Xian Jiaotong University"
    ]
}