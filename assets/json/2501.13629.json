{
    "paper_title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient Language Models",
    "authors": [
        "Zhenghao Lin",
        "Zihao Tang",
        "Xiao Liu",
        "Yeyun Gong",
        "Yi Cheng",
        "Qi Chen",
        "Hang Li",
        "Ying Xin",
        "Ziyue Yang",
        "Kailai Yang",
        "Yu Yan",
        "Xiao Liang",
        "Shuai Lu",
        "Yiming Huang",
        "Zheheng Luo",
        "Lei Qu",
        "Xuan Feng",
        "Yaoxiang Wang",
        "Yuqing Xia",
        "Feiyang Chen",
        "Yuting Jiang",
        "Yasen Hu",
        "Hao Ni",
        "Binyang Li",
        "Guoshuai Zhao",
        "Jui-Hao Chiang",
        "Zhongxin Guo",
        "Chen Lin",
        "Kun Kuang",
        "Wenjie Li",
        "Yelong Shen",
        "Jian Jiao",
        "Peng Cheng",
        "Mao Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Sigma, an efficient large language model specialized for the system domain, empowered by a novel architecture including DiffQKV attention, and pre-trained on our meticulously collected system domain data. DiffQKV attention significantly enhances the inference efficiency of Sigma by optimizing the Query (Q), Key (K), and Value (V) components in the attention mechanism differentially, based on their varying impacts on the model performance and efficiency indicators. Specifically, we (1) conduct extensive experiments that demonstrate the model's varying sensitivity to the compression of K and V components, leading to the development of differentially compressed KV, and (2) propose augmented Q to expand the Q head dimension, which enhances the model's representation capacity with minimal impacts on the inference speed. Rigorous theoretical and empirical analyses reveal that DiffQKV attention significantly enhances efficiency, achieving up to a 33.36% improvement in inference speed over the conventional grouped-query attention (GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various sources, including 19.5B system domain data that we carefully collect and 1T tokens of synthesized and rewritten data. In general domains, Sigma achieves comparable performance to other state-of-arts models. In the system domain, we introduce the first comprehensive benchmark AIMicius, where Sigma demonstrates remarkable performance across all tasks, significantly outperforming GPT-4 with an absolute improvement up to 52.5%."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 9 2 6 3 1 . 1 0 5 2 : r SIGMA: DIFFERENTIAL RESCALING OF QUERY, KEY AND VALUE FOR EFFICIENT LANGUAGE MODELS First Authors: Zhenghao Lin, Zihao Tang, Xiao Liu, Yeyun Gong Core Authors: Yi Cheng, Qi Chen, Hang Li, Ying Xin, Ziyue Yang Contributors: Kailai Yang, Yu Yan, Xiao Liang, Shuai Lu, Yiming Huang, Zheheng Luo, Lei Qu, Xuan Feng, Yaoxiang Wang, Yuqing Xia, Feiyang Chen, Yuting Jiang, Yasen Hu, Hao Ni, Binyang Li, Guoshuai Zhao, Jui-Hao Chiang, Zhongxin Guo, Chen Lin, Kun Kuang, Wenjie Li, Yelong Shen, Jian Jiao Leaders: Peng Cheng, Mao Yang"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce SIGMA, an efficient large language model specialized for the system domain, empowered by novel architecture including DiffQKV attention, and pre-trained on our meticulously collected system domain data. DiffQKV attention significantly enhances the inference efficiency of SIGMA by optimizing the Query (Q), Key (K), and Value (V) components in the attention mechanism differentially, based on their varying impacts on the model performance and efficiency indicators. Specifically, we (1) conduct extensive experiments that demonstrate the models varying sensitivity to the compression of and components, leading to the development of differentially compressed KV, and (2) propose augmented to expand the head dimension, which enhances the models representation capacity with minimal impacts on the inference speed. Rigorous theoretical and empirical analyses reveal that DiffQKV attention significantly enhances efficiency, achieving up to 33.36% improvement in inference speed over the conventional grouped-query attention (GQA) in long-context scenarios. We pre-train SIGMA on 6T tokens from various sources, including 19.5B system domain data that we carefully collect and 1T tokens of synthesized and rewritten data. In general domains, SIGMA achieves comparable performance to other state-of-arts models. In the system domain, we introduce the first comprehensive benchmark AIMICIUS, where SIGMA demonstrates remarkable performance across all tasks, significantly outperforming GPT-4 with an absolute improvement up to 52.5%."
        },
        {
            "title": "INTRODUCTION",
            "content": "In recent years, significant progress has been made in the development of large language models (LLMs), which have demonstrated remarkable performance across wide range of domains (Bubeck et al., 2023; Jiang et al., 2023a; GLM et al., 2024; Dubey et al., 2024; Yang et al., 2024). Meanwhile, novel research direction, known as the system domain, has emerged with promising potential to further accelerate AI development through automated optimization of AI infrastructure (Xiong et al., 2024; Shi et al., 2024; Hu et al., 2024b). This domain focuses on leveraging AI models to autonomously validate, evaluate, diagnose, and optimize the key components of AI infrastructure (e.g., hardware, configurations, cloud services, databases, and workloads). By advancing the robustness, efficiency, and scalability of these foundational systems, the system domain offers powerful pathway to enhance the capabilities and performance of AI. Despite its promise, however, this important area of research has yet to receive commensurate attention. 1 To bridge this gap, this paper introduces SIGMA, an efficient large language model specialized for the system domain, empowered by novel architecture including DiffQKV attention and our carefully collected system domain data. The DiffQKV attention adopted by SIGMA substantially improves its inference efficiency by mitigating the bottleneck associated with KV cache (Pope et al., 2023).1 Though there have already been significant efforts to address the KV cache issue (Ainslie et al., 2023; Kwon et al., 2023; Luohe et al., 2024; Zhang et al., 2024b), prior studies tend to treat the compression of and vectors uniformly and rarely take the optimization of into consideration. In contrast, DiffQKV attention differentially optimizes the Query (Q), Key (K), and Value (V) components in the attention mechanism with tailored strategies, based on their varying impacts on the model performance and efficiency indicators. Specifically, it involves two critical techniques: differentially compressed KV and augmented Q. Differentially compressed KV is grounded on our experimental findings that the overall model performance is more sensitive to compression in vectors than K, both in terms of the dimension and the number of heads. We leverage this disparity by implementing more aggressive compression algorithm for K, whereas employs lighter form of compression. Despite the relatively low compression on V, we can further optimize this part by loading vectors selectively during inference. Due to the high sparsity exhibited by the attention scores (Xiao et al., 2023; Zhang et al., 2024b), it can still preserve the model performance with only small number of vectors to approximate the results, while largely reducing the memory usage. Augmented involves adopting higher dimension for the head compared to the KV heads, as we discover that introducing extra parameters to the head components can effectively boost the model performance. With minimal impacts on the inference speed, augmented can to some degree counteract the performance decline that inevitably results from KV compression. Rigorous theoretical and empirical analyses reveal that SIGMAs DiffQKV significantly enhances efficiency, achieving up to 33.36% improvement in inference speed over the conventional grouped-query attention (GQA) in long-context scenarios. To equip SIGMA with the capability of addressing system domain tasks, we carefully identify 15 primary source categories from over 120 system-related websites, collecting total of 19.5 billion data for system domain pre-training and fine-tuning. Moreover, we construct the AIMICIUS benchmark to facilitate the evaluation of system domain task performance. It includes four major tasks - CMDGen, Infrawise, Optiflow, and NL2KQL - based on Azure services, which assess the critical capabilities in the system domain, such as command generation, benchmark retrieval, topology optimization, and infrastructure issue analysis. Building on the above commitments, we pre-train SIGMA on 6T tokens from various sources, with around 1T tokens from synthesized and rewritten data and 19.5B system domain data, all of which have undergone extensive quality screening. In general domains, SIGMA achieves comparable performance to other state-of-arts models. Besides, SIGMA demonstrates remarkable performance across all tasks in AIMICIUS, significantly outperforming GPT-4 with an absolute improvement up to 52.5%."
        },
        {
            "title": "2 DIFFQKV ATTENTION",
            "content": "In the standard Multi-Head Attention (MHA) introduced by Vaswani et al. (2017), queries, keys, and values consistently employ an equal number of heads, with each head maintaining the same dimension. Additionally, existing autoregressive decoding approaches typically require loading the entire KV cache before computing attention scores at each generation step. Our research explores more generalized form of attention mechanism, termed attention with differential rescaling of QKV (DiffQKV attention), which involves two crucial variations: (1) the Q, K, and components can have varying numbers of heads and differing dimensions per head; (2) during inference, the and caches are retrieved respectively with distinct strategies. Its inference process is formally defined as follows. 1KV cache is common technique adopted by the decoder-only Transformer architecture (Vaswani et al., 2017), which stores the Key (K) and Value (V) vectors in the attention operation for future reuse at each decoding step. It can consume substantial amount of memory (Pope et al., 2023) and place substantial demands on memory bandwidth (Shazeer, 2019; Ribar et al., 2024)."
        },
        {
            "title": "2.1 FORMULATION OF DIFFQKV ATTENTION",
            "content": "k, nh The arguments for DiffQKV attention include: ht Rdm: the attention input for the t-th token within specific attention layer. nh : number of the Q, K, and heads, respectively. Typically, the number of heads is , where , nh set as multiple of heads and heads. This can be expressed as nh gk and gv are positive integers. , dh dh WQ, WK, WV , WO: weight tensors for query, key, value, and output projections, with shapes of : dimension of the Q, K, and heads, respectively. = gv nh = gk nh k, dh [nh , dm, dh ], [nh k, dm, dh k], [nh , dm, dh ], and [nh dh , dm], respectively. &Kcache, &Vcache: memory addresses for the and caches, respectively. We begin by transforming the input ht to Q, K, heads through: (1) (2) (3) (4) (5) qt = ht WQ, kt = ht WK, vt = ht WV , ], [nh , dh Kt-1 = LoadCacheK(&Kcache), Kt = [Kt-1; kt], k, dh where the dimensions of qt, kt, and vt are [nh k], and [nh load the previous vectors from the cache and concatenate it with kt. k, dh , dh ] respectively. Then, we where the dimension of Kt is [t, nh k]. To prepare for the attention score computation, Kt and qt are sliced into series of heads. In addition, since the number of heads and heads can be inconsistent, we employ GroupSharing operation on Kt, which allows multiple heads to be jointly associated with the same head: [K(t,1); K(t,2); ..; K(t,nh [q(t,1); q(t,2); ..; q(t,nh )] = GroupSharing(Kt, nh )] = qt, , nh k), (6) (7) where q(t,i) Rdh attention scores are then computed as: represents the i-th head (i = 1, 2, .., nh ). K(t,i) is in the shape of [t, dh k]. The α(t,i) = Softmaxi[ Attend(q (t,i), K(t,i)) dk ], (8) where α(t,i) Rt. Alternatively, FlexHeadFA can be used to perform attention computations without the inefficient group sharing operation. For more details, we kindly refer readers to 3.2. In standard MHA, the Attend function computes the inner products of the heads and heads. Nonetheless, in our paradigm, which accommodates varying dimensions for the and heads, an alternative implementation of the Attend function is necessitated. In our experiments, we implement this part by transforming the dimension of to the same as through another feed-forward layer. Next, we load cache and prepare the vectors for computing the outputs. Vt-1 = LoadCacheV(&Vcache, α(t,i)), Vt = [Vt-1; vt], )] = GroupSharing(Vt, nh , nh ), [V(t,1); V(t,2); ..; V(t,nh (9) (10) (11) , vh] and that of V(t,i) is [t, dh where the dimension of Vt is [t, nh ]. Notably, the LoadCacheV function, used for retrieving previous vectors, can differ from the strategy for loading cache (i.e., LoadCacheK). Finally, the outputs are calculated as: o(t,i) = (cid:88) j=1 α(j,i)V(j,i), xt = Concat(o(t,1), o(t,2), .., o(t,nh ))WO, (12) (13) where o(t,i) Rdh and xt Rdm . Figure 1: Overview of our proposed method for differential rescaling of QKV, compared alongside Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped Query Attention (GQA). Specifically, our method involves: (1) differentially compressed KV: applying more aggressive compression on the number of heads and their dimensions than on the components, which more significantly reduces the size of cache. We can also optionally adopt selective cache fetching for compression; (2) augmented Q: adopting higher dimension for the head compared to the KV heads. Comparison with Existing Attention Mechanisms. The three widely used attention mechanisms, MHA, Multi-Query Attention (MQA), and Grouped-Query Attention (GQA), can be viewed as special forms of the above algorithm. As shown in Figure 1, these three methods all employ the same dimensions for Q, K, and heads (dh ). In terms of the head number, MHA sets the same number of Q, K, and heads (nh ), MQA only employs single head and head (nh = nh = 1), and GQA features an intermediate number of and heads, yet still aligns their values (nh = nh ). Additionally, most existing approaches directly retrieve the entire and caches before computing the attention scores, without tailored designed for LoadCacheK and LoadCacheV, respectively. = dh = nh = nh = dh Research Questions. Our goal is to find the optimal configurations for DiffQKV attention that facilitate strong model performance while ensuring inference efficiency by reducing memory consumption and burden on memory bandwidth. More specifically, the crucial configurations for DiffQKV attention include: (1) the number of QKV heads (nh ), (2) the dimensions of QKV heads (dh ), and (3) the strategies for loading and caches (LoadCacheK, LoadCacheV). The memory cost of KV cache per token is O(nh + nh ). The time required to retrieve KV cache is determined by the practical data transfer conducted in LoadCacheK and LoadCacheV. k, nh , nh k, dh , dh kdh dh In the following section (2.2), we present our experimental findings on how to set the above configurations, offering insights into achieving an optimal balance between model performance and inference efficiency. These insights lay the foundation for determining the architecture of SIGMA. 2.2 OBSERVATIONS In this section, we present our experimental findings on how various configurations of the Q, K, and components in the DiffQKV attention would influence the model performance. To achieve this, we explore different model architectures and train them from scratch to assess their performance, entailing substantial number of pre-training experiments. In the following experiments, we adopt 100 billion tokens from FineWeb-Edu (Penedo et al., 2024) as the pre-training data. All model architectures are scaled to approximately 1 billion parameters, which consists of 22-layer transformer blocks with hidden dimension of 2048. To evaluate the models performance on different experimental settings, we employed the following benchmarks: HellaSwag (Hella.) (Zellers et al., 2019), OpenBookQA (ObQA) (Mihaylov et al., 2018), WinoGrande (Wino.) (Sakaguchi et al., 2021), ARC Challenge (ARC.) (Clark et al., 2018), PIQA (Bisk et al., 2020), SciQ (Welbl et al., 2017), BoolQ (Bool.) (Clark et al., 2019), LogiQA (Logi.) (Liu et al., 2020), and LAMBADA (Paperno et al., 2016)(LAMB.). 4 Table 1: Comparisons of model performance when reducing the same number of heads versus heads. The number of heads is 32 for all models (nh = 32). The results show that compressing the number of heads has relatively smaller impact on the overall model performance. Model Overall Commonsense & Comprehension Continued LM Hella. ObQA Wino. ARC. PIQA SciQ Bool. Logi. LAMB. k=nh MHA (nh =32) -50% Heads (nh -50% Heads (nh =16) k=16) k=nh GQA (nh =16) -75% Heads (nh -75% Heads (nh =4) k=4) k=nh GQA (nh =4) -75% Heads (nh -75% Heads (nh =1) k=1) 52.40 51.74 (0.66) 52.83 (0.43) 52.14 51.76 (0.38) 51.97 (0.17) 51.66 51.03 (0.63) 51.67 (0.01) 55.6 55.5 55.1 55.1 54.0 54.6 54.0 53.5 53.9 37.6 39.6 38. 39.6 38.2 37.8 38.0 38.4 36.2 57.6 55.0 56.4 56.3 55.6 57.1 56.0 57.0 58.6 36.0 35.9 35. 35.4 34.8 35.1 37.5 35.1 36.9 73.9 71.6 71.9 71.9 72.7 72.3 72.3 72.1 71.1 85.5 85.9 85. 85.0 85.0 84.1 82.0 82.6 83.5 59.6 56.9 63.6 61.4 60.3 62.5 61.3 56.9 60.7 28.9 28.3 29. 27.8 29.9 28.3 28.6 28.4 28.7 36.8 37.0 39.7 36.8 35.3 36.0 35.4 35.1 35.5 Table 2: The ablation studies of halving the head dimension. The results indicate that this adjustment, while largely improving the inference efficiency by reducing the size of KV cache, does not significantly compromise performance. The number of heads is 32 for all models (nh = 32). Model Overall Commonsense & Comprehension Continued LM Hella. ObQA Wino. ARC. PIQA SciQ Bool. Logi. LAMB. =32) k=nh MHA (nh w/ Half Dim. GQA (nh k=nh w/ Half Dim. GQA (nh k=nh w/ Half Dim. =4) =16) 52.40 52.56 (0.16) 52.14 52.06 (0.08) 51.66 51.92 (0.26) 55.6 55. 55.1 54.3 54.0 53.4 37.6 39.4 39.6 39.8 38.0 39.2 57.6 56. 56.3 56.9 56.0 56.6 36.0 36.9 35.4 36.7 37.5 35.6 73.9 72. 71.9 72.0 72.3 72.5 85.5 84.1 85.0 83.9 82.0 84.0 59.6 63. 61.4 59.5 61.3 62.3 28.9 27.8 27.8 29.2 28.6 28.9 36.8 36. 36.8 36.2 35.4 34.8 ) than in the number of heads (nh Observation 1 The model performance is more sensitive to the decrease in the number of heads (nh k). Specifically, we reduce the number of heads of and in the DiffQKV attention, respectively, and assess their impacts on model performance by comparing with the baseline that has the same number of KV heads. Based on the results shown in Table 1, in most cases, reducing the number of heads has relatively minor impact on model performance compared to reducing the number of heads. This differential impact is reasonable, considering the distinct roles played by the and components within the attention mechanisms. The heads primarily serve to compute attention matrices, which prove to have remarkable sparsity exceeding 95% during inference (Zhang et al., 2024b). This high sparsity nature implies that slight decrease in the parameters used for its calculation can still yield precise approximation. In contrast, the vectors, which directly influence the final attention output, demands more nuanced approach. = dh = dh Observation 2 Reducing the dimension of heads to half of the dimension of heads (dh /2) can preserve the model performance, compared to the scenario where the dimensions of and heads are equal (dh ). In Observation 1, we demonstrate the minimal effects resulting from decrease in the number of heads. Here, we explore an alternative approach to compressing components by reducing the dimension of heads. More specifically, the dimension of heads refers to dh in 2 and its reduction indicates smaller WK and kt). To compute the attention scores when the dimensions of and are different (Eq. (8)), we transform the dimension of to the same as through trainable feed-forward layer. In essence, the reduction of head dimension reduces KV cache at the cost of additional computation from feed-forward layer. This trade-off can be very cost-effective as the primary bottleneck for LLM inference speed lies in memory consumption rather than computation (Shazeer, 2019; Ribar et al., 2024). To investigate the impact of reducing head dimension, we conduct comparison of various attention settings, such as MHA and GQA, along with their counterparts having only half of the head dimension. The results presented in Table 2 demonstrate that the decrease in performance due to reducing the head dimension is negligible. 5 Table 3: The ablation studies of the model performance when only selectively loading the vectors corresponding to the highest attention scores for approximate calculation. This operation significantly enhances inference efficiency by reducing memory usage. The number of heads is 32 for all models in the table (nh = 32). Model Overall Commonsense & Comprehension Continued LM Hella. ObQA Wino. ARC. PIQA SciQ Bool. Logi. LAMB. =32) MHA (nh k=nh + Sel.V-top100 GQA (nh k=nh + Sel.V-top100 GQA (nh k=nh + Sel.V-top =16) =4) 52.40 52.10 (0.30) 52.14 52.08 (0.06) 51.66 51.67 (0.01) 55.6 55. 55.1 55.2 54.0 54.0 37.6 37.6 39.6 39.6 38.0 38.2 57.6 57. 56.3 56.3 56.0 55.9 36.0 36.0 35.4 35.4 37.5 37.5 73.9 74. 71.9 71.8 72.3 72.2 85.5 84.8 85.0 84.4 82.0 82.0 59.6 59. 61.4 61.6 61.3 61.2 28.9 27.0 27.8 27.8 28.6 28.6 36.8 36. 36.8 36.7 35.4 35.4 Table 4: Comparisons between the baseline model architectures and those incorporating augmented Q. dh refers to the intermediate head dimension. The number of heads is 32 for all models in the table (nh = 32). For the baseline without AugQ, the intermediate dimension of head is dh = 2048. Model Overall MHA + AugQ (dh =5632) 52.40 53.03 (0.63) k=nh GQA (nh + AugQ (dh + AugQ (dh + AugQ (dh =16) =3072) =4096) =5632) GQA (nh + AugQ (dh k=nh =4) =5632) 52.14 53.38 (1.24) 52.93 (0.79) 53.07 (0.93) 51.66 53.13 (1.47) Commonsense & Comprehension Continued LM Hella. ObQA Wino. ARC. PIQA SciQ Bool. Logi. LAMB. 55.6 57.4 55.1 56.6 56.7 57.3 54.0 56.5 37.6 38. 39.6 40.2 40.8 39.8 38.0 40.8 57.6 57.9 56.3 56.1 56.9 57.3 56.0 58.2 36.0 39. 35.4 40.0 37.1 36.4 37.5 37.6 73.9 72.9 71.9 73.2 73.6 74.2 72.3 73.6 85.5 85. 85.0 87.3 83.5 83.6 82.0 84.7 59.6 60.1 61.4 61.0 61.7 61.4 61.3 61.2 28.9 27. 27.8 28.3 28.0 28.7 28.6 27.5 36.8 38.3 36.8 37.7 38.1 39.0 35.4 37.9 In certain configurations, the model with smaller head dimension even outperforms the baseline setting prior to compression. Observation 3 Using only small proportion of vectors, specifically those corresponding to the highest attention scores, in the attention output calculation can still preserve the model performance. As in previous work, topk can improve the memory efficiency of the model (Gupta et al., 2021), inspiring us to adopt this strategy in this KV imbalance scenario. As shown in Table 3, even when we only use 100 vectors corresponding to the top100 highest attention scores (denoted as Sel.Vtop100), the model can still maintain comparable effect to the original setting. These results indicate that we only need to retrieve small proportion of vectors from the cache during inference to achieve competitive performance. It significantly reduces data transfer, and thereby improves inference speed. Observation 4 Augmenting components can boost model performance. Augmenting the selfattention layer with extra parameters is straightforward way to enhance its representational capacity and improve model performance. Since the vectors in self-attention do not need to be cached during inference, their impact on memory usage and data transfer duration is minimal. Therefore, investing extra parameters in the components can be more cost-effective compared to the KV components. In the following, we investigate whether augmenting components can boost model performance through experiments. In our experiments, we varied the number of parameters in to assess their impact on model performance. As demonstrated in Table 4, adding extra parameters to can enhance the models performance to different extents, especially when the compression levels of and are higher (nh k=4). Based on the overall model performance, scaling factor of 1.5 (i.e. dh = 3072) seems to be optimal. =4,nh 6 Table 5: Comparisons of the model performance when incorporating the augmented component (AugQ) with different sizes and enlarging the FFN module (AugF). The baseline method is GQA, with the FFN dimension being 5632 and nh =16. dF denotes the enlarged dimension for the FFN module, while dh represents the intermediate head dimension (δ=3072). k=nh Model GQA + AugF (dF=δ) + AugQ (dh =δ) =δ) + AugF (dF=2δ) + AugF (dF=δ) & AugQ (dh + AugF (dF=3δ) + AugF (dF=2δ) & AugQ (dh + AugF (dF=5δ) + AugF (dF=3δ) & AugQ (dh =δ) =δ) Overall 52.14 53.26 (1.12) 53.38 (1.24) 53.16 (1.02) 54.55 (2.41) 54.50 (2.36) 54.67 (2.53) 55.08 (2.94) 55.09 (2.95) Commonsense & Comprehension Continued LM Hella. ObQA Wino. ARC. PIQA SciQ Bool. Logi. LAMB. 55.1 57.6 56. 59.3 58.8 60.5 60.5 62.4 61.6 39.6 39.6 40.2 40.0 41.8 42.4 41. 41.4 39.8 56.3 57.3 56.1 57.0 57.5 59.8 57.4 57.3 61.0 35.4 38.5 40. 38.3 39.7 39.8 39.9 41.3 40.5 71.9 73.2 73.2 73.9 74.4 74.7 74. 75.3 75.1 85.0 87.3 87.3 85.2 86.9 87.3 87.3 88.3 88.7 61.4 59.0 61. 59.8 62.4 59.9 59.6 60.6 59.6 27.8 27.6 28.3 24.9 28.1 27.0 27. 26.6 28.3 36.8 39.2 37.7 40.1 41.5 39.2 43.8 42.5 41.2 Table 6: Combinations of three strategies for optimizing the self-attention architecture: augmented Q, compressing the number of heads, and compressing head dimension. and represent whether the corresponding strategy is used or not respectively. If the strategy is not used, the standard model setting is adopted (i.e. nh =16, and is not augmented). k=nh Model Config(nh =16) k=nh -75% Heads Half Dim /2) (dh (nh k=4) k=dh (dh AugQ =3072) Commonsense & Comprehension Continued LM Overall Hella. ObQA Wino. ARC. PIQA SciQ Bool. Logi. LAMB. 52.14 52.97 52. 51.74 52.61 55.1 56.0 55.9 54. 55.8 39.6 38.4 39.4 37.4 40. 56.3 57.7 59.1 57.5 54.9 35. 37.0 37.5 36.3 38.5 71.9 72. 73.1 72.9 74.0 85.0 83.8 84. 85.5 84.6 61.4 62.5 60.6 60. 61.1 27.8 30.1 27.0 26.3 26. 36.8 38.8 37.4 35.1 37.6 Observation 5 With an equal number of additional parameters, the performance gain from augmenting is superior to that from expanding the FFN module. Furthermore, the performance gains from augmenting and expanding the FFN module are independent of each other. In other words, with fixed augmented Q, further enlarging the FFN module continues to enhance model performance. By adding the same number of parameters to the FFN and modules, respectively, we examine and compare their impacts on the model performance. According to the results shown in Table 5, the performance enhancement yielded by augmented is consistently more obvious than adding the same number of parameters in the FFN module. By comparing the last two lines of results in Table 5, we can observe that even when the number of additional parameters on FFN (2δ) is twice that of (dh = δ), the model performance is still slightly inferior to the latter. These results indicate that adding parameters on the self-attention layer can more effectively improve model performance than on the FFN module. 2.3 SUMMARY: OPTIMAL CONFIGURATION FOR DIFFQKV ATTENTION Based on the above observations, we can summarize the optimal configurations for DiffQKV attention that can well balance model performance and inference efficiency as follows: 1) differentially compressed KV: unlike compressing components, compressing incurs less performance degradation, whether in terms of the number of heads (nh k); Furthermore, for with less compression, we can reduce its memory cost by employing the selective cache fetching. 2) augmented Q: introducing extra parameters to the components can enhance the representational capacity of the self-attention layer, while having relatively small impact on inference efficiency, since the vectors do not need to be cached during inference. In Table 6, by integrating three aforementioned strategies (i.e., -75% Heads (nh /2), and AugQ k) or the dimension of (dh k=4), Half Dim(dh k= dh =3072), we observe that the models performance surpasses that of the original model and also (dh exhibits advantages in terms of inference speed and KV cache utilization. In summary, by employing an imbalanced KV head, we have developed more efficient model architecture. Furthermore, the introduction of the augmented module allows us to compensate for the loss incurred by compression while achieving superior performance compared to standard models. The implications of this approach extend beyond the SIGMA model, particularly in light of recent trends toward increasing model capacity. To reduce the KV cache cost during inference in large models, there has been gradual trend toward reducing the proportion of attention mechanism parameters within the entire Transformer block. Given the critical role of the attention mechanism in language models, its compression cannot be done without limits. By reinforcing the head that does not require caching, our method introduces new possibilities for scaling up the attention mechanism, thereby enriching its representational space. Table 5 highlights the parameter efficiency gained from scaling up the parameters of the attention mechanism. However, incorporating the augmented inevitably introduces additional inference costs. We will further elucidate the practical efficiency of SIGMA in 3. 2.4 SIGMA MODEL ARCHITECTURE Building on the insights of the DiffQKV attention discussed in 2.3, we construct SIGMA, pretrained language model characterized by strong performance and inference efficiency. Specifically, we adopt two model scales with 1.5 billion parameters and 10 billion parameters, respectively (i.e. SIGMA-1.5B and SIGMA-10B). For the sake of balancing the model performance and the cost of the KV cache, during the training of SIGMA-1.5B and SIGMA-10B, no dimension compression is applied to the heads. Only the number of heads was decreased. Concretely, the head is set to 4, and the number of heads, which is half of the number of heads, is set to 16. For SIGMA-1.5B, we set dh = 6144, corresponding to 1.5 times the dimension of the hidden state, so as to extend the representational space of Q. We utilize the same vocabulary as Llama3 (Dubey et al., 2024), with vocabulary size of 128k. For more detailed configurations of the SIGMA architecture, please refer to Table 7. = 3072, and for SIGMA-10B, we set dh Table 7: Key configurations and hyperparameters of SIGMA 1.5B and 10B. Parameter Scale Layers Hidden Dimension FFN Dimension Aug Dimension Attention Heads Key Heads Value Heads Peak Learning Rate Activation Function Vocabulary Size Positional Embeddings 1.5B 26 2,048 6,144 3,072 32 4 16 4.0e-4 SwiGLU 128,256 10B 32 4,096 14,336 6,144 32 4 16 1.5e-4 SwiGLU 128,256 ROPE (θ =50,000) ROPE (θ =500,000)"
        },
        {
            "title": "3 EFFICIENCY ANALYSIS",
            "content": "In this section, we conduct detailed efficiency analysis of SIGMA-1.5B, using flash attention 2 (Dao et al., 2022; Dao, 2024) as the default attention implementation. We provide both theoretical and empirical evaluations to explore the efficiency gains achieved by SIGMA. Through detailed examination of its design and performance across multiple metrics, we aim to highlight the architectural innovations and optimization techniques that underpin its superior efficiency in addressing complex and diverse tasks."
        },
        {
            "title": "3.1 THEORETICAL ANALYSIS",
            "content": "As outlined in 2.3, the efficiency gains of SIGMA can be largely attributed to the reduction in the number of key heads. In SIGMA 1.5B, the number of key heads is reduced from 16 to 4, while maintaining the same number of value heads. This reduction directly impacts two critical components of the attention layer: KV Cache and Attention Computation. Below, we delve into the implications and benefits of this architectural adjustment. KV Cache. Generally, key cache and value cache take dimensions (bsz, seq len, nh, dh), where the first two elements denote the batch size and the sequence length individually. The third element corresponds to the number of attention heads, and the last element represents the dimension of each head. Since the KV cache is responsible solely for storing and loading the key and value tensors of the attention layers, we model the store and load operations in the KV cache as being proportional to the total number of elements in the key and value caches. Therefore, the cost of these operations can be expressed as linear function of the cache size, with constant multiplier α representing the proportional cost per element in the cache and constant offset β accounting for additional fixed overheads or losses unrelated to the cache size. The total cost of KV cache operations can thus be formulated as Eq. (14): = α [bsz seq len (nh dh + nh dh )] + β. (14) the key cache has the dimension of According to the configuration of SIGMA 1.5B, (bsz, seq len, 4, 64), whereas the value cache takes dimensions (bsz, seq len, 16, 64). Compared to the vanilla grouped query attention (GQA) design, since the number of key heads is reduced, the total cost of KV cache operations will decrease accordingly. As the size of cache increases, the reduction rate of the total cost of KV cache operations converges toward theoretical value as shown in Eq. (15). = lim seq len+ LGQA LSigma LGQA = 32 20 32 = 37.5%. (15) Attention Computation. Here, the cost reduction also originates from the reduction in key heads. The theoretical reduction rate therefore remains 37.5%. However, while KV cache operations are primarily I/O-intensive, attention computation is significantly more computation-intensive. Due to the increased complexity of this module, the observed reduction in practice is likely to deviate more significantly from the theoretical rate compared to the KV cache. Above, we discussed the theoretical efficiency improvements of SIGMA compared to the vanilla GQA design. Additionally, it is important to highlight that the reduction in the size of the key cache also decreases the overall memory consumption for the KV cache. As result, SIGMA allows for larger batch sizes, which can naturally enhance the efficiency of inference. 3.2 IMPLEMENTATION Although the reduction in the number of key heads theoretically leads to significant efficiency improvements, we find it challenging to implement in practice due to the high integration of KV cache and attention operations within flash attention and existing LLM deployment frameworks (e.g., vLLM (Kwon et al., 2023) and TensorRT2). In this section, we propose several temporary solutions for deploying SIGMA and highlight the need for broader support for DiffQKV. Load and Storage of KV Cache. In SIGMA, since key and value have different number of heads, their cache sizes differ. Generally, it poses no issues for most implementations that store KV cache separately. However, for frameworks like vLLM, it becomes challenging, as these kinds of frameworks combine key cache and value cache into whole matrix. To accommodate these frameworks, one workaround is to duplicate the key matrix to match the size of the value matrix (named KV 2https://github.com/NVIDIA/TensorRT-LLM/ Group Sharing). However, this approach is highly discouraged, as it negates the efficiency improvements entirely while merely maintaining the same performance as the original design. We release the official implementation of KV Group Sharing in Appendix E. Flexible Attention Computation. The official Flash Attention only supports attention computations when the number of key heads equals the number of value heads, and the number of query heads is an integer multiple of the number of key and value heads. Therefore, we could not directly use Flash Attention in SIGMA. To address this issue, we introduce flexible version of FlashAttention, named FlexHeadFA3, which leverages address probing to support attention computation with flexible number of heads. Specifically, FlexHeadFA decouples the address calculations for key heads and value heads. For each query head index idxq, it calculates the corresponding key head index idxk and value head index idxv using equations (16) and (17). This approach enables FlexHeadFA to retrieve the appropriate elements during element-wise multiplication using the calculated indices, thereby eliminating the constraint of requiring equal head numbers. idxk = idxq idxv = idxq nh nh nh nh , . (16) (17) More precisely, FlexHeadFAs implementation follows that of Flash Attention 2, consisting of flash_fwd_splitkv_kernel and flash_fwd_splitkv_combine_kernel. These are two major GPU kernel operations. The former, referred to as the split kernel, manages the primary attention computation workload by dividing the key and value matrices into smaller chunks and performing multiplications to generate output chunks. Meanwhile, the latter, referred to as the combine kernel, assembles these output chunks into complete output matrix. 3.3 EMPIRICAL ANALYSIS In this section, we conduct comprehensive experiments to validate and substantiate our theoretical estimations of efficiency improvements over the vanilla GQA design. 3.3.1 EXPERIMENT SETUP Model Settings. To demonstrate the efficiency gains achieved by reducing the number of key heads, we compare SIGMA 1.5B with hypothetical baseline model, referred to as the standard model (STD). To be specific, while SIGMA features unbalanced KV heads (nh = 16) and includes an additional augmented (dh = 3072), STD employs standard Group Query Attention (GQA) mechanism, configured with balanced and heads (nh = 4 and nh = 16 and nh = 16). Metric Settings. Two methods are employed to accurately measure the efficiency improvements: Cuda Event Elapsed Time and Kernel Execution Time. We also record the absolute and relative improvement of these metrics. Cuda Event Elapsed Time places checkpoints (torch.cuda.Event) within the code (modeling sigma.py) and measures the elapsed time between them to estimate the total duration of specific operations. Its primary advantage lies in its high flexibility, allowing developers to record any time interval by modifying the code. However, it would be unstable when measuring operations with small absolute time scales. We offer an official implementation in Appendix for reference. Kernel Execution Time provides more precise time estimate by profiling GPU applications directly and recording the execution times of GPU kernels using nsys. While this method ensures high accuracy, it incurs significant overhead as the inference scale increases. Additionally, capturing certain kernel operations requires substantial expertise. 3https://github.com/xiayuqing0622/flex_head_fa 10 (a) KET of the split kernel. (b) KET of the combine kernel. (c) Total KET. Figure 2: KET comparison of FlexHeadFA between Standard model(STD) and SIGMA. Table 8: KET Results (ns) with the prefix length increase from 2k to 32k, keeping the output length as 10. Split represents the split kernel and Combine represents the combine kernel. Prefix STD SIGMA 1.5B Relative Improvement Length Split Combine Total Cost Split Combine Total Cost Split Combine Total Cost 2k 4k 16k 32k 2.53E+6 4.68E+6 1.52E+7 2.75E+7 1.88E+6 1.91E+6 1.94E+6 1.99E+6 4.41E+6 6.59E+6 1.72E+7 2.95E+7 2.50E+6 3.49E+6 1.12E+7 2.00E+7 1.85E+6 1.91E+6 1.94E+6 2.01E+6 4.34E+6 5.40E+6 1.31E+7 2.21E+ 1.17% 25.33% 26.30% 27.21% 1.68% 0.08% 0.25% -0.93% 1.39% 18.02% 23.35% 25.31% Task Settings. Since the above two metrics have separate application scopes, we assign different tasks to them, conducting all experiments on NVIDIA H100 80G HBM3 GPU. Kernel Execution Time (KET Test): In this experiment, we measure the cost of two kernels: flash fwd splitkv kernel and flash fwd splitkv combine kernel, treating their summation as the overall cost of applying FlexHeadFA. As this metric is designed for small-scale recording, both models (SIGMA 1.5B and STD) are tasked with generating 10 tokens while varying the prefix length in grid pattern: [2k, 4k, 16k, 32k]. Due to CUDA memory limitations caused by nsys, we were unable to record KET for 64k prefix. We compare the cumulative value of these metrics. Cuda Event Elapsed Time (CEET Test): In this experiment, we record the cost of three components: the cost of storing and loading KV Cache, applying Attention Computation and applying Augmented Q. Although STD does not principally include the Augmented operation, we incorporate it additionally and compare its performance to ensure that the efficiency comparison is not influenced by variations in machine status. Both models are evaluated on larger-scale inference tasks, with the output length increasing in grid pattern of [2k, 4k, 8k, 16k, 32k, 64k], and the prefix length varying independently as [0, 2k, 4k, 16k, 32k, 64k]. We compare the cumulative value of these metrics. 3.3.2 EMPIRICAL RESULTS KET Results. We present Kernel Execution Time results in Figure 2 and Table 8, which align closely with our previous analyses. The cost flash fwd splitkv kernel constitutes the majority of the computational time and increases sharply as the prefix length grows. In contrast, the cost of flash fwd splitkv combine kernel rises at much slower rate, gradually accounting for smaller proportion of the overall computation time. Regarding relative improvement, the efficiency gains become increasingly pronounced as the prefix length increases. We anticipate that the absolute improvement value will gradually approach 37.5% as the prefix length continues to grow. Due to GPU memory constraints, we were unable to further validate this assertion using this metric. Examining the individual improvement of the separate kernels, flash fwd splitkv kernel, which directly operates on the key and value matrices, benefits naturally from the reduction in key heads. This reduction decreases the memory load for the key matrix, and as result, the improvement ratio for this kernel is expected to converge to 37.5% with increasing prefix length. In contrast, the combination of output chunks is less associated with the reduction of key heads. Hence, flash fwd splitkv combine kernel, responsible for combining output chunks, is less in11 (a) Output Length = 2k. (b) Output Length = 4k. (c) Output Length = 8k. (d) Output Length = 16k. (e) Output Length = 32k. (f) Output Length = 64k. (g) SIGMAs absolute CEET improvment vs STD. (h) SIGMAs relative CEET improvment vs STD. Figure 3: CEET comparison of augmented between Standard model(STD) and SIGMA. From (a) to (f), the output length increases progressively from 2k to 64k tokens. fluenced by the reduction in key heads. Consequently, its improvement ratio remains consistently near zero, indicating that this operation is unaffected by changes in the key matrix. However, as we report cumulative values, the absolute value naturally increases alongside the prefix length. Here, we report the Cuda Event Elapsed Time for three modules: KV Cache, Attention Computation, and Augmented Q, with the results presented in Figures 36. CEET Results - Augmented Q. The augmented module is not inherently included within the standard model(STD). In this experiment, we attach the augmented module to STD to demonstrate that the augmented modules cost remains consistent regardless of variations in other parts of the model or different context lengths. This setup ensures the reliability of efficiency evaluations for other modules. The CEET results shown in Figure 3 can strongly testify to it. The relative improvement ratio in Figure 3h for Augmented offers clearer support to our hypothesis, as it consistently hovers near zero across nearly all settings. To provide more rigorous validation, we perform t-test to evaluate whether the relative improvement ratio of Augmented has an expectation of zero. The computed T-value is 0.214, and the corresponding P-value is 0.832, indicating no evidence to reject our hypothesis. 12 (a) Output Length = 2k. (b) Output Length = 4k. (c) Output Length = 8k. (d) Output Length = 16k. (e) Output Length = 32k. (f) Output Length = 64k. (g) SIGMAs absolute CEET improvment vs STD. (h) SIGMAs relative CEET improvment vs STD. Figure 4: CEET comparison of KV cache between Standard model(STD) and SIGMA. From (a) to (f), the output length increases progressively from 2k to 64k tokens. CEET Results - KV Cache. CEET results of KV cache can be found in Figure 4. CEET here primarily reflects the cost of loading and storing the KV cache. These operations are largely proportional to the size of the key and value matrices with minimal extraneous influencing factors. Therefore, the results for KV cache align most closely with our theoretical analysis. In Figures 4a4f, SIGMA demonstrates significantly lower cost for KV cache operations. As the output length increases from Figure 4a to Figure 4e, the CEET gap between SIGMA and STD in KV cache becomes increasingly pronounced. As illustrated in Figure 4h, when the prefix and output lengths reach 64k, SIGMA achieves speedup of 36.57% compared to STD, which is very close to the theoretically calculated improvement rate of 37.5%. CEET Results - Attention Computation. CEET results of the attention computation are demonstrated in Figure 5. We observe steady increase in the relative improvement of Attention Computation. The ratio deviates more significantly from the theoretical value compared to the one of KV Cache. This observation aligns with our analysis in 3.1, as Attention Computation involves greater number of operations that are not directly influenced by the number of key heads, e.g. flash fwd splitkv combine kernel. These operations will partially dilute the efficiency improvement we achieve. Besides, the relative improvement ratio here is also lower than the observed KET improvement shown in Figure 2. The underlying reason is the same: since CEET 13 (a) Output Length = 2k. (b) Output Length = 4k. (c) Output Length = 8k. (d) Output Length = 16k. (e) Output Length = 32k. (f) Output Length = 64k. (g) SIGMAs absolute CEET improvment vs STD. (h) SIGMAs relative CEET improvment vs STD. Figure 5: CEET comparison of attention computation between Standard model(STD) and SIGMA. From (a) to (f), the output length increases progressively from 2k to 64k tokens. measures the end-to-end time of attention computation, it accounts for additional overhead, such as context switching and other CPU operations, making the improvement less pronounced. It is also important to note that when the prefix length increases and the output length is kept fixed, the relative improvement still increases. As shown in Figure 5h, SIGMA achieves up to 15.4% reduction in the CEET of attention computation and becomes increasingly efficient as the prefix or output lengths increase. This is attributed not only to the reduction in the number of key heads but also to the great capability of flash attention kernels to handle long sequences efficiently. CEET Results - Total Cost. Apart from the aforementioned modules (i.e. KV Cache, Attention Computation, and Augmented Q), STD and SIGMA share largely similar design with no notable differences. Since these three modules are the primary contributors to the computational cost of the attention mechanism, we define their combined cost as the total cost. Given that STD does not incorporate the augmented module, its corresponding cost is excluded from the summation, whereas this modules cost is accounted for in SIGMA. As illustrated in Figure 6, our findings indicate that the advantages of SIGMA become increasingly pronounced with the increase in output length and prefix length. Specifically, from Figure 6a to Figure 6f, while SIGMA exhibits higher total inference cost than STD for shorter prefix lengths, the point at which the costs of both models (a) Output Length = 2k. (b) Output Length = 4k. (c) Output Length = 8k. (d) Output Length = 16k. (e) Output Length = 32k. (f) Output Length = 64k. (g) SIGMAs absolute CEET improvment vs STD. (h) SIGMAs relative CEET improvment vs STD. Figure 6: Comparison of total CEET cost between Standard model(STD) and SIGMA. From (a) to (f), the output length increases progressively from 2k to 64k tokens. The gray dashed line indicates where the inference costs of both models are equal. As the output length increases, this intersection point moves progressively earlier. When generating 64k tokens, SIGMA achieves up to 33% reduction in inference cost compared to STD. intersect shifts progressively earlier as the output length increases. For instance, when output 2k tokens, SIGMA only outperforms the STD in terms of cost efficiency with prefix lengths exceeding 16k tokens. However, when the requirement is to output 64k tokens, SIGMA demonstrates superior cost efficiency even without any prefix. Notably, in scenarios where the prefix length is 64k tokens, SIGMA consistently incurs lower CEET compared to the STD across all output lengths, as the results shown in Figure 6f. In particular, when output 64k tokens, SIGMA achieves up to 33.36% reduction in computational time relative to the STD. Figure 6g and Figure 6f illustrate the overall efficiency improvements of SIGMA compared to STD. At 0 prefix length and 2k output length, SIGMA exhibits 40.11% slower in CEET than STD, resulting in the difference of 3.37 103 ms. However, starting from 16k prefix and 16k output length, SIGMA begins to demonstrate significant efficiency advantages. Specifically, when the prefix and output length reaches 64k, SIGMA achieves 33.36% efficiency improvement over STD, with the difference of 3.71 105 ms. This substantial time gap represents different order of magnitude compared to the CEET at shorter context lengths. Table 9: Composition of the system domain pre-training data for SIGMA. Data Type Sources Size # Tokens General System CCF Ranking list arXiv 14.0 33.0 Design Capability Technical blogs & Developer forums 14.5 Debug Capability Stack Overflow 38.9 3.3 5.4 3.2 7.6 B"
        },
        {
            "title": "3.4 DISCUSSION AND SUMMARY ON SIGMA EFFICIENCY",
            "content": "In this section, 1) we design efficiency experiments, 2) attempt to optimize the performance of models with imbalanced KV heads, and 3) evaluate the inference cost of SIGMA using the KET test and the CEET test. Our findings indicate that while SIGMA is less efficient than the Standard model (STD) in short-context scenarios, it demonstrates significant advantages in long-context scenarios, which are common and critical in practical applications, while recent studies suggest that extended test times can facilitate deeper model reasoning and exploration (Snell et al., 2024). From specific cost perspective, although STD exhibits superior efficiency in short-context settings, the cost difference is on the order of 103 ms, which is negligible compared to the 106 ms scale observed in long-context scenarios, as shown in Figure 6. Therefore, considering the pronounced benefits of SIGMA in handling long-context tasks, its architecture holds greater promise for the future development of test-time scale-up in large language models. This outlook underscores the potential of SIGMAs architecture to enhance the scalability and efficiency of language models, particularly as they evolve to support more complex and extended inference processes."
        },
        {
            "title": "4 SYSTEM DOMAIN PRE-TRAINING AND AIMICIUS BENCHMARK",
            "content": "Since the SIGMA architecture described in 2.4 is inherently different from the existing language models, we need to pre-train it from scratch. In our work, we pre-train it to be specialized model tailored for system-related tasks, called SIGMA-SYSTEM, with our meticulously collected domain data. Specifically, our goal is to create LLMs that can automatically diagnose AI infrastructure issues and profile AI workloads (e.g., being able to generate command lines to monitor GPU utilization). These features would allow the LLMs to oversee the training processes of neural models, even including their own, and support automated optimization. To facilitate the development of such specialized LLMs, we carefully collect system domain data used from pre-training and fine-tuning (4.1), and construct AIMICIUS, novel benchmark used for system domain evaluation (4.2). 4.1 SYSTEM DOMAIN DATA COLLECTION For system domain data, we identify 15 primary source categories from over 120 system-related websites. In particular, to enrich general system knowledge, we gather data from diverse range of sources, including academic papers from arXiv and renowned conferences or journals such as IEEE and ACM. StackOverflow is the main source for enhancing debugging skills, while technical blogs and developer forums are crucial for refining system design capability. Additionally, we collect data from related websites to include knowledge on Azure VM, hardware abstraction, Linux commands, GitHub issues, and Stack Exchange. These sources make up the bulk of our pre-training data, supplemented by several other minor resources. As these data sources exhibit varied formats, we process each source individually to extract publicly available system data. To efficiently cleanse the substantial data, we utilize LLM labeling and various AI tools automatically, including category classification, quality control, and data format conversion. For instance, we employ GPT to categorize thousands of data items from StackOverflow, subsequently training smaller, faster model to classify the remaining data, thereby saving on costs. Ultimately, we gather approximately 19.5 billion tokens for pre-training, yielding promising results on AIMICIUS. Table 9 presents more detailed statistics of our collected data."
        },
        {
            "title": "4.2 AIMICIUS BENCHMARK",
            "content": "Currently, AIMICIUS comprises the following tasks: CMDGen, Infrawise, Optiflow, and NL2KQL, and we leave more tasks for future work. Most data is collected from the Azure online service. The details of these tasks are as follows."
        },
        {
            "title": "4.2.1 CMDGEN",
            "content": "CMDGen is task focused on GPU-related command-line generation across two major platforms: NVIDIA and AMD. In this task, LLMs are tasked with generating optimal commands to address specific GPU-related challenges described in the prompts. The commands in CMDGen can be categorized into seven distinct subgroups: NCCL, Nvidia-smi, NVCC, RCCL, Rocm-smi, Superbench, and general others category. The data in CMDGen are sourced from variety of origins, ensuring diversity and realism. Some examples are curated from official documentation, paired with human-written or LLM-generated queries for context; others are directly extracted from Azure service logs or websites like StackOverflow to capture real-world usage patterns. The examples of CMDGen data are included in Appendix A. We consider the following evaluation metrics on the CMDGen task: CMD Score computes the cosine similarity between the embeddings of the generated command and the ground-truth command. The embeddings are encoded with all-MiniLM-L6-v2.4 Output Score evaluates the cosine similarity between the execution results of the generated command and that of the ground-truth command. The embeddings are also obtained with allMiniLM-L6-v2. Calibration Score serves as an approximate measure of accuracy. For given test sample, the calibration score is assigned as one if either the CMD Score or the Output Score exceeds predefined threshold. Exact Match checks if the output command exactly matches the ground-truth command. Success Ratio measures if the execution result of the generated command is similar enough to that of the ground-truth command. Accuracy serves as comprehensive measure of the models overall performance. For given test sample, generated command is deemed accurate if it either exactly matches the groundtruth command (Exact Match = 1) or produces highly similar execution results to the ground-truth (Success Ratio = 1). We consider it as the primary metric on the CMDGen task. It comprises total of 2,838 instruction-tuning examples and 395 test cases, with 200 for the NVIDIA platform and 195 for the AMD platform. It takes around 35 minutes to conduct evaluation on the 200 test cases for NVIDIA when using an H100 GPU. 4.2.2 INFRAWISE In Infrawise, LLMs are tasked with retrieving the benchmark results of particular model in terms of its infrastructure-wise performance (e.g. retrieving the inference speed of GPT-3 on single A100). The process of performing this task generally involves two critical steps: DCW Generation and Benchmark Result Retrieval. DCW generation aims to produce JSON object, referred to as DCW, based on the user instructions. Specifically, DCW stands for: Design, kind of virtual machine (probably with specific feature) or GPU type. It defines the testbed to be evaluated and should be inferred based on the users intentions. The design has two sub-keys: baseline and target, implying the baseline design and target design user wants to retrieve. Commonly used values include NDv4, NDmv4, NDv4 MI200, NDv5, NCv3, A100, H100, MI200 and MI300x. Workload: the benchmarks, models, algorithms, or performance measurement tools that the user intends to run on the design testbed. Criterion: the evaluation principles or performance metrics the user intends to apply. It specifies how the effectiveness, quality, or performance between the two designs should be com4https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 17 pared. Common criteria include evaluating both testbeds peak performance, assessing costeffectiveness, or measuring results within specified time duration. Benchmark result retrieval entails two substeps for enhanced performance. In the first phase, set of benchmark results (typically 10 samples) is retrieved based on initial filtering criteria. In the second phase, LLMs are instructed to select all the appropriate benchmark results from this set, guided by the generated DCW. If none of the retrieved results align with the DCW specifications, the final answer would be marked as None. We adopt the following metrics to conduct evaluation on Infrawise: Target, Baseline, Criterion, Workload: These four metrics evaluate individual components of the generated results. It equals one if the generated result (e.g., Target Design, Baseline Design, Criterion, Workload) exactly matches the ground truth. DCW: This composite metric evaluates the correctness of the entire DCW. It equals one if all four components: Target, Baseline, Criterion, and Workload, are correct (i.e., each equals one individually). Benchmark Result Recall: This metric measures the completeness of the retrieval phase. It equals one if all suitable benchmark results are retrieved without omission. Benchmark Result Accuracy: The most critical metric. For given test sample, its benchmark result accuracy is assigned as one only if all suitable benchmark results are retrieved and no irrelevant results are included. The dataset of Infrawise comprises 422 instruction-tuning samples and 911 test cases. Its evaluation takes around 4 hours on an H100 GPU. The example of Infrawise data can be found in the Appendix C. 4.2.3 OPTIFLOW In Optiflow, LLMs are tasked with optimizing network topology and data flow within specified multi-GPU SKU and data size to minimize all-gather latency. It is further divided into two subtasks: (1) Plan Generation and (2) Plan Improvement. In Plan Generation, the LLM generates Python codes that represent the optimized network topology and data flow for the given multiGPU configuration. In Plan Improvement, users provide the latency of their current design, and the LLM is tasked with refining the existing plan to deliver improved code with reduced latency. This task challenges the models ability to reason about complex hardware configurations and effectively deliver actionable, efficient solutions. The evaluation of Optiflow focuses on assessing the quality of the generated Python code using four key metrics, described as follows: Code Detected: This metric verifies whether the output includes Python code block. If code block is present, the metric is set to one. Code Executable: This metric evaluates the validity of the Python code. If the generated code can be executed without errors, the metric equals one. Plan Valid: This metric assesses whether the generated Python code provides valid plan based on the specified SKU. The metric equals one if the plan meets the given requirements. Plan Improved: Each test case involves both Plan Generation and Plan Improvement tasks. The evaluator first measures the latency of the initial output provided by the LLM and then requests an improved plan. This metric equals one if the latency of the second output is reduced compared to the first. Optiflow comprises total of 8,000 instruction-tuning examples and 1,258 test cases. Among the instruction-tuning examples, 5,047 are derived from the Plan Generation subtask, while 2,953 focus on the Plan Improvement subtask. The test cases are structured in an end-to-end manner: LLM is first tasked with generating plan based on the input, followed by refining the plan with the current latency provided. The evaluation process typically requires approximately 3 days to complete when utilizing an H100 GPU. The example of Optiflow data can be found in the Appendix D."
        },
        {
            "title": "4.2.4 NL2KQL",
            "content": "In NL2KQL, LLMs are tasked with converting the user instruction in the form of natural language into Kusto Query Language (KQL)5, which is specifically designed for querying and analyzing large datasets in Azure Data Explorer and other Microsoft services, such as Log Analytics, Application Insights, and Sentinel. typical KQL query consists of four key components: CLUSTER, DATABASE, TABLE, and COLUMN, taking the structure of: cluster(CLUSTER).database(DATABASE).TABLE where COLUMN The evaluation of NL2KQL focuses on the exact match of each KQL component and the overall syntax accuracy of the generated queries. Specifically, the evaluation metrics include: Syntax Accuracy, which measures whether the generated KQL query adheres to the correct syntax; Similarity, which assesses the Jaccard similarity between the generated KQL query and the ground-truth query; Cluster Score, Database Score, Table Score, Column Score, used to assess the correctness of individual components (CLUSTER, DATABASE, TABLE, and COLUMN) by checking for the recall rate with the corresponding elements in the ground-truth query. The NL2KQL dataset includes total of 5,166 instruction-tuning examples and 43 test cases. It takes 30 minutes to complete when utilizing an H100 GPU. The example of NL2KQL data can be found in Appendix B."
        },
        {
            "title": "5 PERFORMANCE EVALUATIONS",
            "content": "5.1 PRE-TRAINING SETTINGS The pre-training data includes general domain data and some domain-specific property data, amounting to total of 6 trillion tokens. For general domain data, we combine DCLM (Li et al., 2024) and FineWeb-EDU (Penedo et al., 2024) and then remove duplicates to obtain General Dataset I. The number of tokens is approximately 4 trillion. After further quality filtering, we take the filtered data as General Dataset II, with the number of tokens 1 trillion tokens. Finally, we select the data with higher scores and stricter rules as General Dataset III, which contains approximately 200 billion tokens that participate in the annealing phase of SIGMAs pre-training. For data in the math domain, we use proof-pile-2 (Azerbayev et al., 2023) and combine it with 280 billion math-related data filtered from General Dataset as the pre-training data for the math domain. Regarding data in the code domain, we refer to the filtering method of StarcoderV2 (Li et al., 2023) and select the dataset related to the code domain of 500 billion tokens. Moreover, we have approximately 1 trillion tokens of synthesized and rewritten pre-training data that have undergone quality screening and contain content from multiple domains, which participate in the later phase of SIGMA pre-training. We conducted pre-training for SIGMA-1.5B utilizing 512A100-40G GPUs and for SIGMA-10B using 256H100-80G GPUs. Initially, we combined datasets from general, mathematical, and coding domains with General Dataset I, adhering to distribution ratio of General:Math:Code = 8:1:1. The training was carried out with maximum learning rate of 1.5e-4, starting from batch size of 4 million tokens, which was incrementally scaled up to 16 million tokens, culminating in total training volume of 3.5 trillion tokens. In the subsequent phase, we adjusted the data mix to favor mathematical and coding content, employing the quality-filtered General Dataset II. The mixing ratio for this phase was set to General: Math: Code = 4:3:3, with total training volume of 1.0 trillion tokens. For the third phase, we introduced blend of synthesized and rewritten pre-training data alongside the General Dataset II at ratio of 6: 4. This phase encompassed total training volume of 1 trillion tokens, during which the learning rate was reduced to 20% of its peak value, i.e., 3e-5. 5https://learn.microsoft.com/en-us/kusto/query/ 19 Finally, in the annealing phase, we utilized General Dataset III, which was selected for its highest quality, along with meticulously chosen synthesized and rewritten pre-training data and the system domain data. The learning rate was gradually decreased to zero, concluding with total training volume of 1 trillion tokens."
        },
        {
            "title": "5.2 SYSTEM DOMAIN PERFORMANCE",
            "content": "Table 10: Performance of different models on CMDGen NVIDIA subtask in AIMICIUS. The postfix of -S indicates that the model has been SFTed using preliminary version of our SFT dataset, while -P denotes that the model has been pre-trained on our system-domain pre-training dataset. Model GPT-3.5 GPT-4 Mistral-7B-S Mistral-7B-P-S Llama3-8B-S Llama3-8B-P-S Codegemma-7B-P-S Starcoder2-7B-P-S DeepSeekCoder1.5-7B-P-S Gemma2-9B-P-S SIGMA-SYSTEM-10B CMD Score Output Score Calibration Score Exact Match Success Ratio Accuracy 70.0 84. 80.6 83.4 86.4 87.5 84.2 86.5 86.3 90.3 87.5 36.0 61.0 58.7 65.3 69.1 72.2 61.8 66.5 68.4 72.2 80.9 21.0 62. 62.0 66.3 64.4 69.3 65.9 64.9 63.9 78.1 78.0 6.0 13.0 24.9 23.9 42.0 46.3 23.9 31.2 41.0 34.2 57.0 11.0 21. 19.0 21.5 32.7 37.1 21.0 23.4 30.7 26.8 74.0 13.0 25.0 30.7 32.2 50.7 57.1 32.7 38.1 49.3 43.9 74.5 We pre-train SIGMA-SYSTEM-10B on our system domain pre-training data and fine-tuned with fullparameter updates on our SFT dataset, tailored individually for each task. We first evaluate the performance of 8 widely-used LLM releases, including GPT-3.5 (OpenAI., 2022), GPT-4 (Bubeck et al., 2023), Mistral-7B (Jiang et al., 2023a), Llama3-8B (Dubey et al., 2024), Codegemma-7B (Team et al., 2024), Starcoder2-7B (Li et al., 2023), DeepSeekCoder1.5-7B (Dai et al., 2024), Gemma2-9B (Team et al., 2024), on the CMDGen NVIDIA task in the AIMICIUS benchmark (detailed in 4.2.1). For those open-source ones, we further train them on our system domain pre-training data (denoted with the -P postfix) and fine-tuning data (denoted with the -S postfix). The evaluation results are presented in Table 10. While the GPT series (GPT-3.5 and GPT-4) excel in general domain tasks, they still fall short when tasked with the system domain. Similarly, other baseline LLMs, even after continually trained with our system domain data, still fail to reliably generate accurate and executable commands on the CMDGen task. It is important to note that CMDGen NVIDIA is relatively simple task in the AIMICIUS benchmark, yet the majority of LLMs struggle to handle it effectively. This underperformance underscores the significant gap in LLMs capabilities when applied to system-specific tasks and highlights the urgent need for targeted research in the system domain. In contrast, our SIGMA-SYSTEM series demonstrates significantly superior performance compared to these baselines. Based on the results in Table 10, we select the most competitive closed-source and open-source baselines, i.e., GPT-4, Gemma2-9B-Instruct (Gemma2), Deepseek-Coder-7b-Instruct-v1.5 (Deepseek), Qwen2.5-Coder-7B-Instruct (Qwen2.5), and Llama3-8B-Instruct (Llama3), to compare them with SIGMA-SYSTEM on the complete AIMICIUS benchmark. The results are presented in Table 11.6 As shown in Table 11, SIGMA-SYSTEM showcases superior performance across all tasks in the AIMICIUS benchmark, substantially outperforming the baseline models. Specifically, compared 6Note that in Table 11, the evaluation of Gemma2 could not be completed on the Infrawise task because of its slow inference speed in long context scenarios. Similar problems have been reported in several Github issues, such as https://github.com/huggingface/transformers/issues/31953. The Exact Match scores of Gemma2, Deepseek, Qwen2.5, and Llama3 on the two CMDGen tasks, as well as their Plan Improved scores on Optiflow, are extremely low, approaching 0. We mark these metrics as - in the table for now. Table 11: Evaluation results on the AIMICIUS benchmark. The baselines include GPT-4, Gemma2-9B-Instruct, Deepseek-Coder-7b-Instruct-v1.5, Qwen2.5-Coder-7B-Instruct, and Llama38B-Instruct. All metrics are normalized to scale of 0 to 100, with higher values indicating better performance. Bolded metrics represent the most critical evaluation criteria for each task. SIGMASYSTEM 10B is fine-tuned (SFT) using our proprietary dataset. Task Metric () GPT-4 Gemma2 Deepseek Qwen2.5 Llama3 Sigma-System CMDGen NVIDIA CMDGen AMD Infrawise Optiflow NL2KQL CMD Score Output Score Calibration Score Exact Match Success Ratio Accuracy CMD Score Output Score Calibration Score Exact Match Success Ratio Accuracy Target Baseline Criterion Workload DCW Benchmark Result Recall Benchmark Result Accuracy Code Detected Code Executable Plan Valid Plan Improved Syntax Accuracy Similarity Cluster Score Database Score Table Score Column Score 84.0 61.0 62.0 13.0 21.0 25.0 73.0 49.0 43.0 14.0 13.0 17.0 40.7 34.1 55.1 52.1 22.5 19.8 18.7 95.8 50.3 16.8 0. 100.0 31.8 - 2.3 4.7 17.3 64.9 19.0 14.5 - 5.5 5.5 59.3 21.3 15.3 - 14.0 14.0 - - - - - - - 100.0 30.5 30.5 - 90.7 33.4 - 4.7 4.7 22. 59.4 12.9 0.5 - 0.5 0.5 55.0 20.3 3.1 - 3.1 3.1 43.5 43.6 36.3 39.9 16.6 11.6 11.6 82.2 18.6 18.6 - 81.4 33.9 - 2.3 4.7 19.3 81.1 38.3 52.0 - 15.5 15. 77.2 49.1 51.3 - 35.8 35.8 44.9 42.7 44.2 47.8 20.8 20.4 12.6 100.0 28.3 28.3 - 100 36.7 - 4.7 4.7 22.2 33.6 12.0 5.0 - 3.5 3.5 31.4 14.1 2.6 - 1.6 1. 28.1 34.5 11.1 14.2 4.6 6.5 6.5 98.2 51.2 16.4 1.1 83.7 33.6 - 2.3 4.7 20.8 87.5 80.9 78.0 57.0 74.0 74.5 88.9 78.0 79.3 53.9 69.4 69.4 95.2 92.9 75.1 48.4 40.3 28.3 28. 100.0 85.9 86.7 66.7 100.0 34.9 43.0 40.7 17.4 29.0 to the second-best model, its absolute improvement is as significant as 49.5%, 33.6%, 9.6%, 65.6%, and 36% on the major metrics of the CMDGen NVIDIA, CMDGen AMD, Infrawise, Optiflow, and NL2KQL tasks, respectively. Its high scores in various metrics indicate its remarkable ability to generate high-quality commands, monitor infrastructure effectively, and perform well in other system-related tasks. Though the baseline models excel in general domains and the code domain, they exhibit significant performance degradation in AIMICIUS, particularly in CMDGen and Optiflow tasks. This underscores the critical importance of the system domain in advancing the capabilities of LLMs and addressing their limitations in specialized tasks. In the CMDGen NVIDIA task, SIGMA-SYSTEM leads with the highest scores across all metrics. It attains the CMD Score of 87.5%, Output Score of 80.9%, Calibration Score of 78.0%, Exact Match of 57.0%, Success Ratio of 74.0%, and Accuracy of 74.5%. GPT-4 performs well in terms of the CMD Score (84.0%), Output Score (61%) and Calibration Score (62.0%). However, its Exact Match, Success Ratio, and Accuracy are quite low, not exceeding 25%. Other models lag significantly in all metrics. As introduced above, CMDGen is arguably the most likely task to overlap with the pre-training data of these baseline models. However, the performance of the baseline models on CMDGen remains subpar. Similarly, for the CMDGen AMD task, SIGMA-SYSTEM again excels with the top CMD Score (88.9%), Output Score (78.0%), Calibration Score (79.3%), Exact Match (53.9%), and Success Ratio (69.4%). Qwen2.5 performs relatively well on this task. It outperforms GPT-4 on most metrics, reaching an Accuracy of 35.8%, but still falls substantially short of SIGMA-SYSTEM. 21 Regarding the Optiflow task, SIGMA-SYSTEM gets perfect 100.0% in Code Detected, and has high scores in Code Executable (85.9%), Plan Valid (86.7%), and Plan Improved (66.7%). In contrast, all baseline models encounter difficulties in this task. Notably, these baseline models even struggle to generate valid Python script, as evidenced by their Code Executable scores being below 52%. GPT-4 presents competitive performance in Code Detected, scoring 95.8%. However, it has much lower scores in Code Executable (50.3%), Plan Valid (16.8%), and Plan Improved (0.5%). While other models have varying levels of performance, SIGMA-SYSTEM surpasses them in all aspects. Although our model surpasses the baselines in nearly all metrics, it is crucial to recognize that its absolute performance in specific tasks, such as Infrawise and NL2KQL, is still subpar. In the Infrawise task, SIGMA-SYSTEM performs exceptionally in terms of the Target (95.2%), Baseline (92.9%), Criterion (75.1%) scores. Nonetheless, its other metrics do not exceed 50%, with the Benchmark Result Accuracy standing at 28.3%. The performance of other models on this task is even more disappointing. For instance, GPT-4 only achieves 18.7% in Benchmark Result Accuracy. Likewise, in the NL2KQL task, SIGMA-SYSTEM achieves perfect Syntax Accuracy of 100.0%, but its Cluster Score and Database Scoretwo other vitally important metrics for this taskare comparatively low, registering at 43.0% and 40.7% respectively. This indicates that there is room for further improvement, particularly in scenarios requiring more nuanced understanding and precise decision-making. As part of our future work, we plan to enrich the training dataset and refine the data generation policy to better capture the complexity and variability of these tasks, aiming to achieve enhanced performance across all evaluation metrics. 5.3 GENERAL DOMAIN PERFORMANCE We also pre-train general domain model using the SIGMA-1.5B architecture to investigate its performance in more settings. Evaluation Setup. Following Zhang et al. (2024a) and Chen et al. (2024), we conduct comprehensive evaluation of SIGMA in terms of commonsense reasoning tasks, reading comprehension tasks, text understanding tasks, language proficiency, general knowledge, coding, and math problemsolving. With reference to the evaluation procedure of lm-evaluation-harness (Gao et al., 2024) and EvalPlus (Liu et al., 2023), we present the results of HellaSwag (Hella.) (Zellers et al., 2019), OpenBookQA (ObQA) (Mihaylov et al., 2018), WinoGrande (Wino.) (Sakaguchi et al., 2021), ARC easy/Challenge (ARC.e/c) (Clark et al., 2018), PIQA (Bisk et al., 2020), SciQ (Welbl et al., 2017), BoolQ (Bool.) (Clark et al., 2019), and LogiQA (Logi.) (Liu et al., 2020). The above benchmarks evaluate the models commonsense reasoning and text-understanding ability. We also evaluate on three general problem-solving benchmarks MMLU (5-shot) (Hendrycks et al., 2020), MMLU-Pro (5-shot) (Wang et al., 2024), and BBH (3-shot) (Suzgun et al., 2022), two code generation benchmarks HumanEval (0-shot) (Chen et al., 2021) and MBPP (Austin et al., 2021), and two math problem datasets MATH (5-shot) (Hendrycks et al., 2021) and GSM8K (5-shot) (Cobbe et al., 2021). Baselines. We compare SIGMA against several state-of-the-art models renowned for their performance, with parameter scales ranging up to and including 2 billion, including TinyLlama1.1B (Zhang et al., 2024a), Pythia-(1.0B, 1.4B) (Biderman et al., 2023), OPT-1.3B (Zhang et al., 2022), Bloom-1.1B (Muennighoff et al., 2022), Cerebras-GPT-1.1B (Dey et al., 2023), Phi11.3B (Gunasekar et al., 2023), StableLM-2-1.6B (Bellagente et al., 2024), SmolLM-1.7B (Allal et al., 2024), DCLM-1.4B (Li et al., 2024), OLMo-1.2B (Groeneveld et al., 2024), and Gemma2B (Team et al., 2024). Commonsense Reasoning Tasks. The evaluation results on commonsense reasoning benchmarks are shown in Table 12. All results are obtained via zero-shot prompting. According to the results, SIGMA-1.5B achieves an outstanding average performance of 61.6 on 9 benchmarks, significantly outperforming strong baseline models such as OLMo-1.2B and TinyLlama-1.1B on various reasoning and comprehension tasks. It also achieves comparable performance to state-of-the-art large language models such as Gemma-2B, and DCLM-1.4B. Specifically, SIGMA achieves top-2 performance at WinoGrande, PIQA, ARC.e/c and SciQ, showing its broad and accurate grasp of both intuitive and logical commonsense during its pre-training on large-scale text sequences, which provides reliable knowledge backups for reasonable text completions and further task-specific tuning. Although it excels at common sense reasoning, SIGMA shows limited performance on BoolQ and 22 Table 12: Comparisons with baseline models on commonsense reasoning and text understanding tasks. Differences with original reports in the baseline models are due to our unified re-evaluations for fair comparisons. Model Params Avg. Commonsense & Comprehension Continued Hella. ObQA Wino. ARC.e ARC.c PIQA SciQ Bool. Logi. Pythia TinyLlama Bloom OLMo OPT CerebrasGPT Phi1 Pythia DCLM StableLM2 SmolLM Gemma SIGMA (Ours) 1.0B 1.1B 1.1B 1.2B 1.3B 1.3B 1.3B 1.4B 1.4B 1.6B 1.7B 2.0B 1.5B 49.3 54.0 46.5 54.5 51.2 46.5 39.6 51.7 62.8 61.4 61.0 62.2 61.6 47.1 61.5 43.0 63.0 53.7 38.4 30.4 52.0 71.6 69.0 65.7 71.4 67.3 31.4 36.8 29.4 36.2 33.2 29.0 25.0 33.2 42.6 38.8 42.0 40. 40.4 53.4 59.5 55.0 59.9 59.8 52.1 49.9 57.3 66.2 63.6 60.8 64.6 67.5 49.0 55.6 45.5 57.3 51.0 45.8 34.6 53.9 71.6 68.2 73.5 72.3 72.3 27.1 32.7 25.6 30.9 29.5 25.3 23.5 28.3 43.5 38.9 46.3 41. 43.9 69.3 73.6 67.2 75.1 72.4 66.8 56.0 70.9 77.7 76.6 76.1 78.2 77.8 76.1 84.2 74.5 78.7 76.7 73.0 64.5 79.3 90.7 95.3 89.6 91.5 94.0 60.8 56.0 59.1 61.8 57.7 59.4 45.2 63.3 71.4 74.7 66.0 69. 63.0 29.8 25.8 18.9 27.8 26.9 29.2 27.3 27.5 29.8 27.2 28.7 30.4 28.4 Table 13: Comparisons with baseline models on general, coding, and math problem-solving tasks. Differences with original reports in the baseline models are due to our unified re-evaluations for fair comparisons. Model Params Avg. General Coding Math MMLU MMLU-Pro BBH HumanEval MBPP MATH GSM8K Pythia TinyLlama Bloom OLMo OPT CerebrasGPT Phi1 Pythia DCLM StableLM2 SmolLM Gemma SIGMA (Ours) 1.0B 1.1B 1.1B 1.2B 1.3B 1.3B 1.3B 1.4B 1.4B 1.6B 1.7B 2.0B 1.5B 10.76 11.13 5.20 10.53 8.95 9.67 19.81 10.73 17.92 17.88 17.60 26.58 27. 26.1 26.7 26.5 26.2 24.7 26.6 24.9 25.7 47.7 38.1 29.9 41.2 47.0 11.1 11.3 2.1 10.8 10.8 11.2 11.0 10.9 16.3 8.7 11.7 14.7 17.6 24.6 25.4 6.5 25.8 22.8 24.5 22.9 24.8 29.9 26.8 28.9 36.0 32. 5.5 - - 5.5 - 1.8 48.2 4.9 9.1 7.3 1.2 25.0 21.3 4.0 10.8 - 0.3 - 0.8 27.2 4.9 13.2 17.7 41.0 41.5 30.7 2.1 1.9 0.1 2.3 1.7 1.0 2.0 2.1 2.5 5.2 4.2 10.9 12. 1.9 1.8 1.2 2.8 2.7 1.8 2.5 1.7 6.8 21.3 6.4 16.8 27.8 LogiQA, reflecting moderate level of reading comprehension on option-form questions. We observe significant decrease in these benchmarks during the annealing stage when math problemsolving ability improves, showing potential conflict between natural and formal language understanding for small-scale models. Problem-Solving Tasks. The evaluation results on various general, coding and math problemsolving benchmarks are shown in Table 13. According to the results, SIGMA-1.5B achieves an average of 27.1 on all tasks, which is comparable to strong baseline models such as Phi1 and Gemma2B. Specifically, our model reaches top-2 performances on all three general benchmarks: MMLU, MMLU-Pro, and BBH, showing its capability to apply world knowledge to various problem-solving scenarios. In the code domain, SIGMA achieves 21.3% and 30.7% pass rates on the HumanEval and MBPP benchmarks, outperforming Phi1-1.3B on MBPP, model customized for code generation. However, in HumanEval, SIGMA exhibits inferior performance relative to Gemma-2B and Phi11.3B, which may be attributed to the relatively lower proportion of code in our pre-training corpus. In the math domain, SIGMA reaches 12.7% and 27.8% precision on the MATH and GSM8K benchmarks, surpassing competitive baseline models such as StableLM-1.6B and Gemma-2B by large margin. Notably, SIGMA-1.5B maintains similar commonsense reasoning and general problemsolving capabilities to DCLM-1.4B, but greatly advances on all coding and math benchmarks. Despite some overlap in the pre-training corpus adopted by SIGMA and that by DCLM-1.4B, the results 23 presented above further demonstrate the importance of mixing mathematical and coding data into the general corpus throughout the entire pre-training process, which contributes to more balanced improvement of the models general, reasoning, and programming capabilities."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Over the past few years, Large Language Models (LLMs) have exerted considerable impact across various domains (Bubeck et al., 2023; Jiang et al., 2023a; GLM et al., 2024; Dubey et al., 2024; Yang et al., 2024). Nevertheless, the formidable computational demands and memory requirements of LLM inference present considerable challenges for deployment in many resource-constrained scenarios. Thus, while the exploration into scaling up larger model scale to achieve even more advanced level of intelligence is still ongoing (Kaplan et al., 2020; Aghajanyan et al., 2023; Brown et al., 2024), the development of smaller, more efficient language models has also garnered increasing research interest due to their significantly reduced inference costs and lower deployment requirements (Hu et al., 2024a; Zhang et al., 2024a; Abdin et al., 2024; Lu et al., 2024; Liu et al., 2024a; Team et al., 2024; Dai et al., 2024; Mehta et al., 2024). Meanwhile, variety of methods have been explored to improve the inference efficiency of LLMs (Kwon et al., 2023; Ainslie et al., 2023; Xiao et al., 2023; Mu et al., 2024; Zhou et al., 2024). Significant efforts have been dedicated to addressing inference bottleneck associated with KV cache (Ainslie et al., 2023; Kwon et al., 2023; Luohe et al., 2024; Zhang et al., 2024b). KV cache is common technique adopted by the decoder-only Transformer architecture (Vaswani et al., 2017), which stores the Key (K) and Value (V) vectors in the attention operation for future reuse at each decoding step to avoid recomputation. It can consume substantial amount of memory that linearly increases with the sequence length (Pope et al., 2023). This consumption practically limits the context length that the model can handle. In addition, repeatedly loading KV cache also places substantial demands on memory bandwidth, which is recognized as the major bottleneck for LLM inference speed (Shazeer, 2019; Ribar et al., 2024). Notably, most prior studies tend to treat the compression of and vectors uniformly, both in terms of the optimization methodology and the compression ratio. For instance, Grouped-Query Attention (GQA) (Ainslie et al., 2023) reduces the number of and heads in the attention layer to the same extent by organizing Query (Q) heads into groups, with each group sharing single K/V head. The same applies to methods based on KV cache eviction, which carefully select fixed number of tokens and cache both their and vectors (Beltagy et al., 2020; Xiao et al., 2023; Han et al., 2024; Liu et al., 2024b; Zhang et al., 2024b; Ge et al., 2024; Ribar et al., 2024). However, the different roles and vectors play in the attention mechanism might allow for differential treatment in their efficiency optimization, an area that remains underexplored. Moreover, existing research rarely takes the optimization of into consideration when devising the optimization approach, which might also provide opportunities for further improvement in model performance and efficiency. In this study, we investigate more efficient model architectures for LLMs from the perspective of differentially adjusting the QKV components in the attention mechanism, and introduce SIGMA, pre-trained language model featured by its efficient inference capabilities. Apart from KV cache optimization, the studies on improving inference efficiency can be roughly divided into three categories: pre-training stage optimization, post-training stage optimization, and system-level optimization. Pre-training stage optimization approaches typically involve refinement on the model architecture (Shazeer, 2019; Ainslie et al., 2023; Sun et al., 2024; Brandon et al., 2024; Dai et al., 2024). Among them, Multi-Query Attention (MQA) (Shazeer, 2019) and GroupedQuery Attention (GQA) (Ainslie et al., 2023) are the two most widely-used methods. MQA is variant of the standard Multi-Head Attention (MHA) mechanism, where all query heads share single key head and value head. GQA further generalizes MQA by using an intermediate number of shared key and value heads. Post-training stage optimization is usually accomplished by selectively evicting part of the KV cache via token elimination (Beltagy et al., 2020; Xiao et al., 2023; Han et al., 2024; Liu et al., 2024b; Zhang et al., 2024b; Ge et al., 2024; Ribar et al., 2024). These approaches identify and eliminate non-essential tokens by employing criteria based on the attention scores at each decoding step. Another line of research (Mu et al., 2024; Jiang et al., 2023b) focuses on compressing prompts into smaller number of gisting tokens, which can be cached and reused to enhance efficiency. The last category of approaches improves efficiency by designing systems specifically tailored to the characteristics of LLM inference (Kwon et al., 2023; Jin et al., 2024; Ye et al., 2024; He & Zhai, 2024). In particular, Kwon et al. (2023) proposed paged attention and 24 introduced the vLLM framework, effectively mitigating the memory fragmentation issues associated with LLM inference."
        },
        {
            "title": "7 CONCLUSION AND FUTURE WORK",
            "content": "We introduce SIGMA, an efficient large language model specialized for the system domain. The architecture of SIGMA features novel attention module that called DiffQKV, which is equipped with augmented for performance improvement and differential KV compression for enhanced inference efficiency. Through theoretical and empirical analyses, we demonstrate the competitive efficiency of DiffQKV, with up to 33.36% speed improvement compared with Grouped-Query Attention(GQA) when processing long documents. Moreover, after pre-training on 6 trillion tokens, SIGMA demonstrates performance on par with the latest state-of-the-art models across general domains. On the AIMICIUS benchmark, the first comprehensive system domain benchmark that we propose, SIGMA substantially surpass all baseline models across all tasks, achieving an absolute improvement up to 52.5%. Despite its advancements, SIGMA still presents significant opportunities for improvement. For instance, further optimization on the architecture of SIGMA has not been fully explored. Key areas for investigation include the trade-off between Augmented and Feed-Forward Network (FFN) parameters, varied key-value (KV) heads compression across layers, and appropriate hyper-parameters for scale up. Additionally, the number of tasks currently included in AIMICIUS is still limited. It is essential to evaluate the models ability on wider spectrum of system domain challenges. Besides, we discover that SIGMAs performance is largely constrained by the quality of the synthetic data used for pre-training. We believe that model self-evolution and lifelong learning could be possible avenues to overcome this limitation. In future work, we plan to address these issues, unlock the vast potential of SIGMA within the system domain."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Hassan Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Singh Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sebastien Bubeck, Martin Cai, Caio Cesar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allison Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Young Jin Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xianmin Song, Olatunji Ruwase, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Cheng-Yuan Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yunan Zhang, and Xiren Zhou. Phi3 technical report: highly capable language model locally on your phone. arXiv preprint, arXiv:2404.14219, 2024. Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling laws for generative mixed-modal language models. In International Conference on Machine Learning, pp. 265279. PMLR, 2023. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 48954901, 2023. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Leandro von Werra, and Thomas Wolf. Smollm - blazingly fast and remarkably powerful, 2024. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023. Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. Stable lm 2 1.6 technical report. arXiv preprint arXiv:2402.17834, 2024. Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 23972430. PMLR, 2023. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. William Brandon, Mayank Mishra, Aniruddha Nrusimha, Rameswar Panda, and Jonathan Ragan Kelly. Reducing transformer key-value cache size with cross-layer attention. arXiv preprint arXiv:2405.12981, 2024. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with GPT-4. arXiv preprint arXiv:2303.12712, 2023. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Yilong Chen, Linhao Zhang, Junyuan Shang, Zhenyu Zhang, Tingwen Liu, Shuohuan Wang, and Yu Sun. Dha: Learning decoupled-head attention from transformer checkpoints via adaptive heads fusion. arXiv preprint arXiv:2406.06567, 2024. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Wu, et al. DeepSeekMoE: Towards ultimate expert specialization in mixtureof-experts language models. arXiv preprint arXiv:2401.06066, 2024. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. 26 Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Nolan Dey, Gurpreet Gosal, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, Joel Hestness, et al. Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster. arXiv preprint arXiv:2304.03208, 2023. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/ 12608602. Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive KV cache compression for LLMs. In The Twelfth International Conference on Learning Representations, 2024. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, et al. ChatGLM: family of large language models from GLM130B to GLM-4 all tools. arXiv preprint arXiv:2406.12793, 2024. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838, 2024. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. arXiv preprint arXiv:2106.06899, 2021. Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. LMinfinite: Zero-shot extreme length generalization for large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 39914008. Association for Computational Linguistics, 2024. Jiaao He and Jidong Zhai. FastDecode: High-throughput GPU-efficient LLM serving using heterogeneous pipelines. arXiv preprint arXiv:2403.11421, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. MiniCPM: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024a. Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, Yuhuai Li, Shengze Xu, Shawn Wang, Xinchen Xu, Shuofei Qiao, Kun Kuang, Tieyong Zeng, Liang Wang, Jiwei Li, Yuchen Eleanor Jiang, Wangchunshu Zhou, Guoyin Wang, Keting Yin, Zhou Zhao, Hongxia Yang, Fan Wu, Shengyu Zhang, and Fei Wu. Os 27 agents: survey on mllm-based agents for general computing devices use. Preprints, December 2024b. doi: 10.20944/preprints202412.2294.v1. URL https://doi.org/10.20944/ preprints202412.2294.v1. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023a. Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LongLLMLingua: Accelerating and enhancing llms in long context scenarios via prompt compression. arXiv e-prints, pp. arXiv2310, 2023b. Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu. Llm maybe longlm: Self-extend llm context window without tuning. In Forty-first International Conference on Machine Learning, 2024. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611626, 2023. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models. arXiv preprint arXiv:2406.11794, 2024. Raymond Li, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, LI Jia, Jenny Chim, Qian Liu, et al. StarCoder: may the source be with you! Transactions on Machine Learning Research, 2023. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. arXiv preprint arXiv:2007.08124, 2020. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:// openreview.net/forum?id=1qvx610Cu7. Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, et al. MobileLLM: Optimizing sub-billion parameter language models for on-device use cases. arXiv preprint arXiv:2402.14905, 2024a. Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for LLM KV cache compression at test time. Advances in Neural Information Processing Systems, 36, 2024b. Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas Lane, and Mengwei Xu. Small language models: Survey, measurements, and insights. arXiv preprint arXiv:2409.15790, 2024. Shi Luohe, Hongyi Zhang, Yao Yao, Zuchao Li, et al. Keep the cost down: review on methods to optimize LLMs KV-cache consumption. In First Conference on Language Modeling, 2024. Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, et al. OpenELM: An efficient language model family with open-source training and inference framework. arXiv preprint arXiv:2404.14619, 2024. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In EMNLP, 2018. Jesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with gist tokens. Advances in Neural Information Processing Systems, 36, 2024. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022. OpenAI. Introducing chatgpt, 2022. URL https://openai.com/blog/chatgpt. Denis Paperno, German Kruszewski, Angeliki Lazaridou, QuanNgoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The lambada dataset: Word prediction requiring broad discourse context. Cornell University - arXiv,Cornell University - arXiv, Jun 2016. Guilherme Penedo, Hynek Kydlıˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. arXiv preprint arXiv:2406.17557, 2024. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems, 5:606624, 2023. Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas Orr. SparQ attention: Bandwidth-efficient LLM inference. In Forty-first International Conference on Machine Learning, 2024. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. Honghao Shi, Longkai Cheng, Wenli Wu, Yuhang Wang, Xuan Liu, Shaokai Nie, Weixv Wang, Xuebin Min, Chunlei Men, and Yonghua Lin. Enhancing cluster resilience: LLM-agent based autonomous intelligent cluster diagnosis system and evaluation framework. arXiv e-prints, pp. arXiv2411, 2024. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei. You only cache once: Decoder-decoder architectures for language models. arXiv preprint arXiv:2405.05254, 2024. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, , and Jason Wei. Challenging bigbench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Team, Gemma, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. 29 Johannes Welbl, Nelson Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. arXiv preprint arXiv:1707.06209, 2017. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. Yifan Xiong, Yuting Jiang, Ziyue Yang, Lei Qu, Guoshuai Zhao, Shuguang Liu, Dong Zhong, Boris Pinzur, Jie Zhang, Yang Wang, et al. {SuperBench}: Improving cloud {AI} infrastructure reliability with proactive validation. In USENIX Annual Technical Conference, pp. 835850, 2024. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, arXiv preprint Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv:2407.10671, 2024. Lu Ye, Ze Tao, Yong Huang, and Yang Li. ChunkAttention: Efficient self-attention with prefix-aware KV cache and two-phase partition. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1160811620. Association for Computational Linguistics, 2024. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. TinyLlama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024a. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, et al. H2O: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2024b. Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, et al. survey on efficient inference for large language models. arXiv preprint arXiv:2404.14294, 2024."
        },
        {
            "title": "A CMDGEN EXAMPLES",
            "content": "B NL2KQL EXAMPLES"
        },
        {
            "title": "C INFRAWISE EXAMPLES",
            "content": ""
        },
        {
            "title": "D OPTIFLOW EXAMPLES",
            "content": ""
        },
        {
            "title": "E OFFICIAL IMPLEMENTATION OF KV GROUP SHARING",
            "content": "Here, we release the official implementation of KV Group Sharing to ensure compatibility with mainstream LLM frameworks. 1 def kv_group_sharing(self, key_states, value_states): 2 3 \"\"\" This is for balance the number of KV heads to facilitate subsequent 4 5 6 7 8 9 10 11 12 13 14 16 flash attention calculations \"\"\" batch, _, slen, head_dim = key_states.shape if self.num_key_heads > self.num_value_heads: n_rep = int(self.num_key_heads / self.num_value_heads) value_states = value_states[:, :, None, :, :].expand(batch, self. num_value_heads, n_rep, slen, head_dim) return key_states, value_states.reshape(batch, self. num_value_heads * n_rep, slen, head_dim) else: n_rep = int(self.num_value_heads / self.num_key_heads) if n_rep == 1: return key_states, value_states key_states = key_states[:, :, None, :, :].expand(batch, self. num_key_heads, n_rep, slen, head_dim) return key_states.reshape(batch, self.num_key_heads * n_rep, slen , head_dim), value_states You can use this code in the forward function like this: 1 def forward( 2 3 4 5 6 7 8 9 10 ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch. self, hidden_states: torch.Tensor, attention_mask: Optional[torch.LongTensor] = None, position_ids: Optional[torch.LongTensor] = None, past_key_value: Optional[Cache] = None, output_attentions: bool = False, use_cache: bool = False, cache_position: Optional[torch.LongTensor] = None, 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 Tensor]]]: if isinstance(past_key_value, StaticCache): raise ValueError( \"static cache implementation is not compatible with attn_implementation==flash_attention_2 \" \"make sure to use sdpa in the mean time, and open an issue at https://github.com/huggingface/transformers\" ) output_attentions = False bsz, q_len, _ = hidden_states.size() query_states = self.q_proj(hidden_states) key_states = self.k_proj(hidden_states) query_states = self.q_down_proj(self.act_fn(self.q_gate_proj( query_states)) * self.q_up_proj(query_states)) value_states = self.v_proj(hidden_states) # Flash attention requires the input to have the shape # batch_size seq_length head_dim hidden_dim # therefore we just need to keep the original shape query_states = query_states.view(bsz, q_len, self.num_heads, self. head_dim).transpose(1, 2) 34 32 33 34 35 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 key_states = key_states.view(bsz, q_len, self.num_key_heads, self. head_dim).transpose(1, 2) value_states = value_states.view(bsz, q_len, self.num_value_heads, self.head_dim).transpose(1, 2) cos, sin = self.rotary_emb(value_states, position_ids) query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin) if past_key_value is not None: # sin and cos are specific to RoPE models; cache_position needed for the static cache cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position} key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs) key_states, value_states = self.kv_group_sharing(key_states, value_states) query_states = query_states.transpose(1, 2) key_states = key_states.transpose(1, 2) value_states = value_states.transpose(1, 2) (Code Omitted.) flash attention attn_output = self._flash_attention_forward( query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate ) attn_output = attn_output.reshape(bsz, q_len, -1).contiguous() attn_output = self.o_proj(attn_output) if not output_attentions: attn_weights = None return attn_output, attn_weights, past_key_value"
        },
        {
            "title": "F OFFICIAL IMPLEMENTATION OF EFFICIENCY RECORDING",
            "content": "Here, we offers an official implementation to use Cuda Event Elapsed Time to record the cost of attention computation. 1 global total_attention_cost 2 start = torch.cuda.Event(enable_timing=True) 3 end = torch.cuda.Event(enable_timing=True) 4 start.record() 5 attn_output = flash_attn_func( 6 query_states, key_states, value_states, dropout, softmax_scale= softmax_scale, causal=causal 7 ) 8 end.record() 9 end.synchronize() 10 latency = start.elapsed_time(end) 11 total_attention_cost += latency To use Kernel Execution Time to record the cost, there is no need to edit the modeling python file, we can directly use command line tool to implement this: nsys profile --output xxx --stats=true -t cuda python xxx.py."
        }
    ],
    "affiliations": []
}