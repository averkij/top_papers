{
    "paper_title": "Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation",
    "authors": [
        "Mufei Li",
        "Dongqi Fu",
        "Limei Wang",
        "Si Zhang",
        "Hanqing Zeng",
        "Kaan Sancak",
        "Ruizhong Qiu",
        "Haoyu Wang",
        "Xiaoxin He",
        "Xavier Bresson",
        "Yinglong Xia",
        "Chonglin Sun",
        "Pan Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern long-context large language models (LLMs) perform well on synthetic \"needle-in-a-haystack\" (NIAH) benchmarks, but such tests overlook how noisy contexts arise from biased retrieval and agentic workflows. We argue that haystack engineering is necessary to construct noisy long contexts that faithfully capture key real-world factors -- distraction from heterogeneous biased retrievers and cascading errors in agentic workflows -- to test models' long-context robustness. We instantiate it through HaystackCraft, a new NIAH benchmark built on the full English Wikipedia hyperlink network with multi-hop questions. HaystackCraft evaluates how heterogeneous retrieval strategies (e.g., sparse, dense, hybrid, and graph-based) affect distractor composition, haystack ordering, and downstream LLM performance. HaystackCraft further extends NIAH to dynamic, LLM-dependent settings that simulate agentic operations, where models refine queries, reflect on their past reasonings, and decide when to stop. Experiments with 15 long-context models show that (1) while stronger dense retrievers can introduce more challenging distractors, graph-based reranking simultaneously improves retrieval effectiveness and mitigates more harmful distractors; (2) in agentic tests, even advanced models like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated distractors or struggle to perform early stops. These results highlight persistent challenges in agentic long-context reasoning and establish HaystackCraft as a valuable testbed for future progress."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 2 4 1 4 7 0 . 0 1 5 2 : r Preprint HAYSTACK ENGINEERING: CONTEXT ENGINEERING FOR HETEROGENEOUS AND AGENTIC LONGCONTEXT EVALUATION Mufei Li, Dongqi Fu, Limei Wang, Si Zhang, Hanqing Zeng, Kaan Sancak, Ruizhong Qiu, Haoyu Wang, Xiaoxin He, Xavier Bresson, Yinglong Xia, Chonglin Sun, Pan Li Georgia Institute of Technology, {mufei.li, haoyu.wang, panli}@gatech.edu Meta AI, {dongqifu, limeiwang, sizhang, zengh, kaansancak, yxia, clsun}@meta.com University of Illinois UrbanaChampaign, {rq5}@illinois.edu National University of Singapore, {xiaoxin, xaviercs}@comp.nus.edu.sg"
        },
        {
            "title": "ABSTRACT",
            "content": "Modern long-context large language models (LLMs) perform well on synthetic needle-in-a-haystack (NIAH) benchmarks, but such tests overlook how noisy contexts arise from biased retrieval and agentic workflows. We argue that haystack engineering is necessary to construct noisy long contexts that faithfully capture key real-world factorsdistraction from heterogeneous biased retrievers and cascading errors in agentic workflowsto test models long-context robustness. We instantiate it through HaystackCraft, new NIAH benchmark built on the full English Wikipedia hyperlink network with multi-hop questions. HaystackCraft evaluates how heterogeneous retrieval strategies (e.g., sparse, dense, hybrid, and graph-based) affect distractor composition, haystack ordering, and downstream LLM performance. HaystackCraft further extends NIAH to dynamic, LLM-dependent settings that simulate agentic operations, where models refine queries, reflect on their past reasonings, and decide when to stop. Experiments with 15 long-context models show that (1) while stronger dense retrievers can introduce more challenging distractors, graph-based reranking simultaneously improves retrieval effectiveness and mitigates more harmful distractors; (2) in agentic tests, even advanced models like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated distractors or struggle to perform early stops. These results highlight persistent challenges in agentic long-context reasoning and establish HaystackCraft as valuable testbed for future progress. Our implementation is available at https://github.com/Graph-COM/HaystackCraft."
        },
        {
            "title": "INTRODUCTION",
            "content": "Effective context engineering (Mei et al., 2025)optimizing information for LLMs contextsand robust long-context reasoning are essential for large language models (LLMs) to enable sophisticated agents and handle complex, information-intensive tasks. Recent algorithmic and engineering innovations have significantly expanded LLMs context windows and enhanced their long-context reasoning capabilities (Su et al., 2024; Peng et al., 2024; Dao et al., 2022; Dao, 2024; Liu et al., 2024a; Xiao et al., 2024; Yuan et al., 2025; Kwon et al., 2023). Consequently, modern LLMs can process extended contexts and often achieve near-perfect recall on synthetic needle-in-a-haystack (NIAH) benchmarks (Yen et al., 2025). These benchmarks test whether model can retrieve and reason over relevant information needle buried in large haystack context that also contains many distractors. Yet such synthetic setups neglect fundamental question: how are noisy long contexts constructed in real-world applications? Work done during Mufeis internship at Meta 1 Preprint Figure 1: Overview of the core challenges that HaystackCraft addresses. (a) Retrieval-Dependent Haystacks. The composition and ordering of the noisy long context (haystack) are shaped by the retrieval strategy (e.g., sparse, dense, hybrid, and graph-based). (b) Agentic Error Propagation. In dynamic agentic workflows, early errorssuch as misidentifying John Durys death placecan propagate through query refinements. This leads to cascading failures where the agent deviates from the original querys intent and inflates distractor rankings. To engineer long contexts in practice, retrieval-augmented generation (RAG) (Lewis et al., 2020) is one of the most widely adopted strategies, where external retrievers rank candidate context documents with respect to queries. However, retrieval systems are imperfect and inherently biased, introducing retriever-dependent ranked distractors. To be specific, sparse retrievers such as BM25 (Robertson et al., 1994; Robertson & Zaragoza, 2009) often populate haystacks with lexically similar but semantically irrelevant documents, while dense retrievers (Karpukhin et al., 2020) surface semantically close but potentially factually incorrect near misses. Because no single retriever is universally optimal (Thakur et al., 2021), it is crucial to study how heterogeneous retrieval strategies shape the context and consequently affect NIAH performance. Beyond heterogeneous retrieval biases, interdependencies between needles and distractors introduce another layer of complexity. Many queries are multi-hop, which requires connecting multiple scattered, logically related needle pieces (Yang et al., 2018). In networked corpora such as social or hyperlink networks, these pieces are explicitly connected to each other and to potential distractors, yielding the challenge of identifying smaller needle subgraph within larger haystack graph. Graph-based retrieval methods are central to modern information retrieval and search engines (Page et al., 1999). Collectively, these factors create realistic and nuanced interplay that has been largely overlooked in prior NIAH studies (Fig. 1 (a)). Furthermore, advances in reasoning LLMs (OpenAI, 2024; DeepSeek-AI et al., 2025) enable increasingly LLM-driven agentic context engineering (Yao et al., 2023) for challenging tasks like DeepResearch (Google, 2025). Rather than passively digest provided contexts and directly jump to the conclusion, LLMs can perform multi-round research (Trivedi et al., 2023; Jiang et al., 2023), actively refining the query to improve the retrieval quality (Wang et al., 2023; Ma et al., 2023) and reflecting on their own past reasoning (Shinn et al., 2023; Asai et al., 2024). In such dynamic systems with adaptive retrieval and iterative reasoning, LLMs become distractor source themselves. Earlystage errorssuch as noisy retrievals or hallucinated factscan propagate and compound, leading to cascading failures over iterations or gradual deviation from the original query intent (Fig. 1 (b)). As result, static, single-round, and LLM-independent NIAH evaluations are insufficient, motivating dynamic, multi-round, LLM-dependent test environments. In order to mitigate the simulation-to-reality gap for LLMs long-context utility, we argue that haystack engineering is necessary. Just as context engineering seeks to provide optimal LLM 2 Preprint contexts, haystack engineering addresses the challenge of constructing realistic noisy long contexts. While context engineering emphasizes best-case conditions, haystack engineering emphasizes faithful haystack constructions shaped by heterogeneous retrieval strategies and cascading agentic errors. We instantiate this concept through HaystackCraft, new NIAH benchmark built on the full English Wikipedia hyperlink network and multi-hop questions. HaystackCraft systematically examines how retriever choice shapes distractor composition, haystack ordering, and the resulting LLM performance. It evaluates widely adopted retrieval strategies, including sparse, dense, hybrid, and graph-based methods. To contextualize the evaluation in agentic context engineering, we introduce novel LLM-dependent, dynamic NIAH tests. Featuring crucial agentic operations like query refinement and summarization, HaystackCraft challenges models in two dynamic long-context settings: (1) an enforced multi-round scenario to measure robustness against cascading errors and (2) variable-round scenario to examine if models can proactively escape cascading errors by early stop. We perform extensive studies covering 15 long-context LLMs, featuring general-purpose, reasoning, open-source, and commercial models. First, we find that retrieval strategy strongly impacts haystack difficulty. While dense retrievers introduce harder distractors than sparse ones, graphbased reranking with Personalized PageRank (PPR) simultaneously improves retrieval effectiveness and mitigates more harmful distractors, improving NIAH performance by up to 44%. Second, our dynamic NIAH tests reveal that current models are surprisingly brittle in agentic workflows. Even advanced models like Gemini 2.5 Pro and GPT-5 suffer from cascading self-distraction when multiple reasoning rounds are enforced. Crucially, models tend to be more robust to single-round noisy long contexts (width) than to noisy reasoning iterations (depth). Even when models are allowed for an early stop, most models fail to terminate the process appropriately. Overall, our evaluations suggest that long-context challenges in realistic, agentic context engineering are far from solved and that HaystackCraft provides valuable testbed for measuring progress on these issues."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Long-Context Benchmarks. The original NIAH test inserts single needle sentence into increasingly large haystacks (Kamradt, 2023). LV-Eval (Yuan et al., 2024), RULER (Hsieh et al., 2024), and BABILong (Kuratov et al., 2024) extend the test in terms of question types and corpus sources. However, all these attempts construct query-independent distractors, rather than retriever-dependent contexts, as in practical applications like RAG. HELMET (Yen et al., 2025) takes step toward realism by using dense retriever for distractor construction, but it does not capture retriever heterogeneity, network-structured corpora, or the influence of retriever-ranked haystack ordering. Beyond NIAH, other benchmarks assess long-context reasoning in downstream tasks and domain-specific applications (Shaham et al., 2022; Dong et al., 2023; Shaham et al., 2023; An et al., 2024; Bai et al., 2024b;c; Wang et al., 2024a; Zhang et al., 2024; Wang et al., 2025). However, these benchmarks lack the flexibility that NIAH provides in context size and distractor composition. Furthermore, unlike HaystackCraft, all existing long-context benchmarks employ static, LLM-independent contexts. This approach is insufficient for evaluating LLMs in dynamic, multi-round agentic systems. Multi-Round Benchmarks. MT-bench pioneered the evaluation of multi-turn conversations (Zheng et al., 2023), with follow-ups refining conversation patterns and question taxonomies (Kwan et al., 2024; Duan et al., 2024; Bai et al., 2024a; Laban et al., 2025), extending conversation lengths (Deshpande et al., 2025), and considering tool usage (Wang et al., 2024b). These benchmarks purely focus on multi-round conversations rather than agentic context engineering. LLM agent benchmarks like AgentBench (Liu et al., 2023) introduce challenging tasks that necessitate multi-round agentic context engineering, but they do not study how the long-context width at each step contributes to cascading errors across the reasoning depth. Consequently, none of these works explicitly study the joint wide and deep long-context challenges that HaystackCraft tackle."
        },
        {
            "title": "3 HAYSTACKCRAFT",
            "content": "Context engineering aims to select, structure, and optimize an LLMs input context to maximize its reasoning effectiveness (Lewis et al., 2020; Yao et al., 2023; Mei et al., 2025). Its practice shapes LLMs long-context challenges. We introduce the complementary concept of haystack engineering: the principled construction of realistic noisy long contexts that faithfully model the complexities 3 Preprint and failure modes of real-world context engineering pipelines. While context engineering seeks to improve performance, haystack engineering aims to create challenging test conditions to measure model robustness, shaped by factors like heterogeneous retrieval strategies and cascading errors in agentic workflows. We present HaystackCraft, benchmark that instantiates this principle. In this section, we first formalize the NIAH problem arising from RAG, which highlights the central role of the retrieval strategy and motivates studying representative, heterogeneous retrievers. We then introduce, to our knowledge, the first dynamic, LLM-dependent NIAH challenge, designed to characterize long-context challenges in agentic context engineering. Finally, we describe how HaystackCraft is grounded in the full English Wikipedia hyperlink network with multi-hop questions, ensuring realistic and challenging evaluation setting. 3.1 NIAH TESTING FOR RAG: RETRIEVAL, NEEDLE, AND HAYSTACK RAG is popular context engineering strategy due to its simplicity and broad applicability. In standard RAG pipeline, retrieval strategy first fetches the top-N documents deemed most relevant to query. These documents, along with the query, form the input context for an LLM. To achieve high recall, the hyperparameter is often set to value much larger than the number of groundtruth supporting documents for queries. This practice inevitably sacrifices precision and introduces challenging near-miss distractors that have high retrieval scores (Xu et al., 2024). The problem is exacerbated when query requires logically combining information from multiple supporting documents, as in multi-hop question answering (QA). Our empirical studies in Sec. 4.1 demonstrate that larger is required to achieve comparable retrieval recall for multi-hop questions. Correspondingly, the requirement for large inherently is prone to create the NIAH problem, assuming that perfect retrieval could be achieved with sufficiently large , or equivalently, sufficiently large context size. Building on this observation, we formalize the NIAH problem from RAG perspective: Let be the document corpus. For any given query q, set of ground-truth documents Nq is required to answer it; we refer to this set as the needle. retrieval strategy scores and ranks all documents in based on their predicted relevance to q. To construct the haystack HR (S) for target context size of tokens, we first include all needle documents from Nq. We then fill the remaining token budget by adding the top-ranked distractors from DNq. If including the final distractor would exceed the budget, we truncate that to fit. Finally, HR (S)) according to an ordering policy π(q, R, HR (S)) (e.g., by retrieval ranking), before being passed to the LLM. See Appendix for the detailed prompts we use. (S) is linearized into sequence of documents (d1, , dHR 3.2 ASSESSING HETEROGENEOUS RETRIEVAL STRATEGIES Retriever Strategy (R) and Haystack Composition. The above formulation highlights the central role of retrieval strategy (R) in haystack engineering. Different retrieval strategies introduce distinct biases into the distractor composition, which consequently shape the reasoning challenge for LLMs according to the strategys specific failure modes. Since no single method is universally optimal in terms of both effectiveness and efficiency, it is crucial to consider heterogeneous retrievers. To this end, HaystackCraft incorporates broad spectrum of retrievers, including: 1. Sparse (BM25) (Robertson et al., 1994; Robertson & Zaragoza, 2009): classical sparse retriever that measures lexical similarity. 2. Dense (Qwen3-Embedding-0.6B) (Zhang et al., 2025): dense retriever that captures semantic similarity. We choose this model for its competitive retrieval performance on MMTEB (Enevoldsen et al., 2025), small size, and applicability to long documents. 3. Hybrid (BM25 + Qwen3-Embedding-0.6B): combination of the two using reciprocal rank fusion (Cormack et al., 2009; Microsoft, 2025), which is robust to differences in score magnitudes across retrievers. As sparse and dense retrievers are complementary, hybrid of them often yields better performance in practice (Lee et al., 2023). Graph-Based Reranking for Multi-Hop QA. Complex queries, such as those in multi-hop QA (Trivedi et al., 2022), require synthesizing information from multiple interconnected documents 4 Preprint (the needle set Nq). Standard retrievers score documents independently and thus overlook the relational structure among documents (e.g., hyperlinks or citations), limiting their ability to surface supporting chains. This frames the task as finding needle subgraph within larger haystack graph. Graph structure provides powerful retrieval signals. For instance, PageRank (Page et al., 1999), foundational algorithm for modern search engines, leverages this by considering document structurally important if it is heavily referenced by other important documents. Building on this idea, we employ Personalized PageRank (PPR) (Haveliwala, 2002) reranking to study the impact of graph-based retrieval on distractor composition and downstream LLM performance. Specifically, after retrieving an initial candidate set with base retriever, we perform PPR reranking seeded on the top-ranked documents to integrate structural information. Haystack Ordering (π). LLMs exhibit strong positional biases due to autoregressive generation and positional encodings, and the order of documents can significantly impact their long-context performance (Liu et al., 2024b; Xiao et al., 2024; Yang et al., 2025c). While prior NIAH benchmarks often randomize document order to account for this issue, practical RAG systems present documents in ranked order determined by the retriever. To bridge this simulation-to-reality gap, we evaluate both retriever-ranked ordering and random permutations. This dual approach allows us to assess LLM performance in realistic RAG setting while also diagnosing the effects of positional bias. 3.3 DYNAMIC NIAH TESTING FOR AGENTIC CONTEXT ENGINEERING Standard RAG can be ineffective when dealing with imperfect queries or complex tasks. User queries might be ambiguous or contain grammatical errors, which harm effective retrieval. Furthermore, standard RAG struggles with multi-hop queries, which are composed of logically interdependent subqueries. In this case, retrieving enough evidence requires answering earlier subqueries first. For instance, to answer What continent is the country encompassing Luahoko located in?, system must first find that Luahoko is in Tonga before second retrieval for the continent of Tonga . Agentic context engineering (Yao et al., 2023) can mitigate these limitations by transforming LLMs from passive retrieval consumers into proactive researchers. In such systems, LLMs can dynamically initiate further retrievals as needed (Trivedi et al., 2023; Jiang et al., replacing the query above with 2023), What continent is Tonga located in? ) (Wang et al., 2023; Ma et al., 2023), and reflect on their past analyses (Shinn et al., 2023; Asai et al., 2024) until they can confidently draw conclusion. refine queries to optimize retrieval quality (e.g., However, agentic context engineering introduces new challenge: LLMs themselves become potential source of distraction. Recent studies show that even advanced reasoning models struggle to recognize their own reasoning errors, often reinforcing initial mistakes rather than correcting them (Huang et al., 2024; He et al., 2025). Early-stage errors, such as noisy retrievals or flawed reasoning, can propagate and compound through LLMs generation, leading to cascading failures or gradual deviation from the original querys intent. While related issues have been previously observed (Laban et al., 2025), the interplay between wider context windows and deeper agentic iterations introduces critical failure mode not captured by existing static NIAH benchmarks and multi-round benchmarks. This gap highlights need for benchmarks that can test these integrated wide and deep long-context challenges. Existing static, single-round NIAH tests are insufficient, motivating our development of dynamic, multi-round, and LLM-dependent test environment. We perform an extension of our previous NIAH formulation in Section 3.1 for comparable results, simplicity, and controllability, while capturing key characteristics: multi-round retrieval, query refinement, and self-reflection. The process is iterative. We start with the original query q(0) = and an empty LLM reasoning history C(0) = (). At each round t, we use the latest query q(t) to q(t)(S). The LLM receives q(t), the history C(t), and the ordered haystack. construct the haystack HR In intermediate rounds, the LLM outputs refined query q(t+1) and its latest analysis A(t+1), and we update the history C(t+1) = (A(1), , A(t+1)). In the final round, the LLM outputs its answer. We evaluate models in two dynamic settings, with detailed prompts provided in Appendix C: Enforced Multi-Round. We enforce models to perform fixed number of reasoning rounds to measure their robustness against cascading errors. 5 Preprint Variable-Round. We allow models to decide when to stop, testing their ability to balance iterative refinement against the risk of cascading errors. 3.4 CORPUS AND QA SAMPLES To instantiate the static and dynamic NIAH tests introduced above, we need networked corpus and QA dataset that support heterogeneous retrieval strategies and multi-hop reasoning. In this subsection, we detail our choice of corpus and QA samples that satisfy the need. Networked Corpus. We ground our benchmark in the entire English Wikipedia hyperlink network. This choice is deliberate: Wikipedia is dominant information source for retriever development and QA dataset curation and serves as widely recognized proxy for general knowledge (Chen et al., It provides centralized testbed for studying realistic haystack engineering with diverse 2017). retrievers. Furthermore, its large scale and natural network structure, formed by in-text references (hyperlinks), make it an ideal corpus for studying graph-based retrieval. We process the 202504-04 Wikipedia dump using WikiExtractor (Attardi, 2015), resulting in network that comprises 6, 954, 909 articles interconnected by 97, 442, 472 unique hyperlinks. Long Retrieval Unit. We choose to use full articles as the retrieval unit, rather than smaller, broken chunks. This approach mirrors modern search engines, which return entire documents, and avoids fragmenting documents logical flow as with common chunking practices in RAG. By preserving article integrity, we present more realistic and demanding long-context reasoning challenge. QA Datasets. We use two established datasets: Natural Questions (NQ) (Kwiatkowski et al., 2019) for single-hop questions and MuSiQue (Trivedi et al., 2022) for multi-hop questions. The multihop questions require reasoning over up to four supporting documents, presenting challenge that motivates agentic context engineering. Both datasets are built on Wikipedia, providing unified source for needles and distractors. We choose MuSiQue over alternatives (Yang et al., 2018; Ho et al., 2020) as it is specifically designed to be less susceptible to reasoning shortcuts. Since both datasets were curated on earlier Wikipedia versions, we manually filter all samples to ensure validity under our updated corpus, addressing issues like outdated knowledge and ambiguity. This yields final set of 500 high-quality samples. Further details are available in Appendix D. Data Contamination Mitigation. critical concern in LLM evaluation is data contamination, where exposure to benchmark data during pretraining inflates performance (Sainz et al., 2023). While the models we evaluate have almost certainly been trained on versions of Wikipedia and even the QA datasets, our benchmarks design inherently mitigates this risk. The core task demands locating the needle within long context of plausible, retriever-selected distractorsrather than simple fact recall. This challenge is amplified for our multi-hop questions, which require synthesizing information across multiple documents, process robust to memorization. Furthermore, our use of recent Wikipedia dump post-dates the training cutoffs of most current LLMs, minimizing data overlap. Our empirical results in Section 4.1 confirm this mitigation: all models show substantial performance degradation as context size increases, demonstrating that they are actively reasoning over the provided text, not merely recalling memorized answers."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 NIAH WITH HETEROGENEOUS RETRIEVAL STRATEGIES To investigate the impact of retrieval strategies, we evaluate 12 open-source and commercial longcontext LLMs across input context sizes of {8K, 16K, 32K, 64K, 128K}. Our selection spans both reasoning modelsthree Qwen3 variants (Yang et al., 2025b), Gemini 2.5 Flash-Lite, and o4miniand leading general-purpose models, including GPT-4.1 mini and the open-source Llama3.1 (Dubey et al., 2024), Qwen2.5-1M (Yang et al., 2025a), and Gemma 3 (Kamath et al., 2025) families. Following the practice of MuSiQue, we use the F1 score as the QA metric. Our analysis in Appendix confirms that multi-hop questions are less susceptible to data contamination, making them preferable for our study. To ensure fair comparison, we standardize token counts using the Qwen2.5-1M tokenizer. For more LLM and retriever setup details, see Appendix E. 6 Preprint Figure 2: (1) Retrieval performance improves as # retrieved documents (N ) increases. (2) Multi-hop questions pose larger retrieval challenges. (3) Reranking with PPR consistently boosts performance, especially for multi-hop questions. See Appendix for the raw numbers. Retrieval Effectiveness. As preliminary step, we first evaluate the effectiveness of the retrieval strategies to ensure they construct meaningful distractors. We measure both Recall @N , which quantifies the coverage of ground-truth supporting documents, and NDCG @N (Jarvelin & Kekalainen, 2000; 2002), which additionally accounts for the ranking. We study the scaling behavior of retrieval by increasing , the number of retrieved documents, which directly corresponds to increasing the context size in our NIAH setting. Fig. 2 shows that retrieval performance for all methods improves as increases, justifying constructing longer contexts with more distractors. Among the base retrievers, the dense retriever (Qwen3-0.6B) consistently outperforms the sparse retriever (BM25) in both metrics, and combining them with hybrid retriever further improves the performance. The retrieval effectiveness decreases as the question hop count (# supporting documents) increases, validating our claim in Sec. 3.1 that multi-hop questions necessitate larger , leading to more distractors. Graph-based reranking with PPR boosts all base retrievers in both coverage and ranking, especially for multi-hop questions. Impact of Retriever Strategy on NIAH Performance. To study the overall impact of the retrieval strategy (R) on haystack composition (HR (S)) and ordering, we first employ retrieval ranking for haystack ordering (π). Fig. 3 presents the evaluation results. All LLMs, including advanced commercial and reasoning models, suffer significant performance degradation as context size increases to 128K tokens, regardless of the retrieval strategy. For 11 out of 12 cases, the dense retriever (Qwen3-0.6B) introduces more challenging distractors than the sparse retriever (BM25) at larger context sizes. However, combining them with hybrid retriever does not necessarily introduce more challenging distractors. Impact of Graph-Based Retrieval. Using PPR for graph-based reranking leads to significant NIAH performance improvement. By comparing the solid lines with the dashed lines in Fig. 3, we observe that for nearly every model and base retriever, the performance curve paired with PPR is noticeably higher, especially at context sizes of 64K and 128K. This demonstrates that exploiting the relational structure among documents is powerful method for mitigating distraction. For instance, an improvement of 44% was observed for Llama-3.1-70B-Instruct with the hybrid retriever, highlighting how prioritizing structurally central documents can mitigate more harmful structurally isolated lexical and semantic distractors. Retrieval Effectiveness vs NIAH Performance. Previous study by Jin et al. (2025) suggests that better retrievers introduce harder distractors for shorter-context reasoning and single-hop QA. Our study discloses deeper insight, demonstrating that the interplay between the retriever mechanism and task nature plays crucial role. While hybrid retriever substantially improves retrieval recall and ranking, it fails to introduce more challenging distractors. In contrast, graph-based reranking simultaneously improves retrieval effectiveness and mitigates harmful distractors. Our study highlights the critical role of retrieval strategy design in long-context engineering. 7 Preprint Figure 3: Impact of retrieval strategy on NIAH performance as context size increases. 0 stands for the case without distractors. All models experience performance drop as context size increases. Graph-based reranking (dashed lines) consistently improves performance for larger context sizes. See Appendix for the raw numbers. Figure 4: F1 score difference between retriever-ranked and random haystack orderings. The ordering impact is highly model-dependent. The Gemma-3 and Qwen2.5-1M families derive significant and growing benefit from retriever-ranked ordering as context size expands. See Appendix for the raw NIAH performance numbers with random haystack orderings. Impact of Haystack Ordering. To isolate the effect of haystack ordering (π), we compare the performance of retriever-ranked ordering against the average of three random permutations. The results in Fig. 4 reveal complex and highly model-dependent patterns. While some models, such as the Gemma-3 and Qwen2.5-1M families, derive significant and growing benefit from retrieverranked ordering as context size expands, others exhibit more volatile, retriever-dependent, or even negative response. This finding carries crucial implication: to faithfully assess models practical long-context utility in RAG systems, evaluations must mirror the canonical, retriever-ranked input. Furthermore, contrasting this setup with random permutations allows us to better understand the positional biases of individual models. 8 Preprint (a) Haystack construction with BM25 + PPR. (b) Haystack construction with Qwen3-0.6B. Figure 5: Dynamic NIAH performance. 0 stands for the case without distractors. (1) Enforced multi-round reasoning leads to performance drop. (2) Models are generally more robust to wider contexts than deeper reasoning. (3) Models fail to perform early stop properly (variable-round). For raw experiment numbers, see Appendix K. 4.2 DYNAMIC NIAH To assess the wide and deep challenges in agentic context engineering (Sec. 3.3), we perform dynamic NIAH evaluation with multi-round reasoning. We randomly choose 100 QA samples and evaluate eight LLMs, including state-of-the-art models Gemini 2.5 Pro and GPT-5. We exclusively use retriever-ranked haystack ordering, as this realistic setup ensures that LLMs query refinement is directly reflected in the context. Flawed refinements degrade the document ranking, posing dynamic challenge that faithfully evaluates models robustness to cascading errors. We consider two representative retrieval strategies: BM25 + PPR, graph-based strategy effective at mitigating more harmful distractors, and Qwen3-0.6B, dense retriever that introduces more challenging distractors. Enforced Multi-Round: More Rounds Amplify Errors. We first evaluate model robustness by enforcing constant number (2 or 3) of reasoning rounds. Fig. 5 shows that all models, including the most advanced Gemini 2.5 Pro and GPT-5, are vulnerable to cascading errors. Across retrieval strategies and context sizes, performance generally worsens with more rounds. Rather than mitigating distraction, additional iterations often amplify early mistakes or inject new noise. Interestingly, the degradation is not always monotonic with context size in multi-round settings, and more rounds can be more damaging than longer noisy contexts in single pass. Crucially, static NIAH performance is not reliable predictor of multi-round robustness: for instance, GPT-5 mini performs comparably to GPT-5 in the static setting but collapses under enforced multi-round reasoning, revealing weaker agentic robustness. Variable-Round: Self-Correction Is Difficult. We further investigate if models can balance iterative refinement against cascading errors when allowed to stop early before exhausting three rounds. 9 Preprint None of the models reliably improve upon their single-round performance. GPT-5 achieves the best relative performance but still fails to convert multi-round reasoning into sustained improvements. Representative Failure Patterns. Appendix presents representative failure cases. 1) Earlystage errors can propagate and compound through query refinement and summarization, leading to cascading failures that are hard to correct. 2) LLMs can deviate from the original query intent, changing its nature or form. 3) Long-context challenges can still prevent LLMs from reasoning and retrieving relevant information. Implications. These findings reveal consistent pattern: current LLMs are more robust to noisy long contexts than noisy reasoning iterations. In practice, practitioners should prioritize the use of larger context window size (width) over more reasoning iterations (depth). These results underscore the unsolved long-context challenges in agentic context engineering and establish HaystackCraft as valuable testbed for measuring and advancing agentic robustness."
        },
        {
            "title": "5 CONCLUSION",
            "content": "Our benchmark, HaystackCraft, demonstrates that retrieval strategies critically shape distractor composition and ordering. Furthermore, our novel dynamic tests reveal that even state-of-the-art models like Gemini 2.5 Pro and GPT-5 remain vulnerable to cascading self-distractions and fail to selfcorrect. These findings highlight that robust agentic long-context reasoning is far from solved and establish HaystackCraft as valuable testbed for measuring future progress. ACKNOWLEDGMENTS M. Li, H. Wang, and P. Li are partially supported by the NSF under awards IIS-2239565, IIS2428777, and CCF-2402816; the JP-Morgan Chase Faculty Award; the OpenAI Researcher Access Program Credit; the Google Gemini Academic Program; and the IDEaS Cyberinfrastructure Awards. XB is supported by NUS Grant ID R-252-000-B97-133 and MOE AcRF T1 Grant ID 251RES2423. The authors would also like to thank Xi Wang for valuable feedback on early drafts."
        },
        {
            "title": "REFERENCES",
            "content": "Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. L-Eval: Instituting Standardized Evaluation for Long Context Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1438814411, 2024. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. In International Conference on Learning Representations, 2024. Giusepppe Attardi. WikiExtractor. https://github.com/attardi/wikiextractor, 2015. Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and Wanli Ouyang. MT-Bench-101: Fine-Grained Benchmark In Proceedings of the 62nd for Evaluating Large Language Models in Multi-Turn Dialogues. Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 74217454, 2024a. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: Bilingual, Multitask Benchmark for Long Context Understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 31193137, 2024b. Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks. arXiv preprint arXiv:2412.15204, 2024c. 10 Preprint Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer OpenDomain Questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 18701879, 2017. Gordon V. Cormack, Charles Clarke, and Stefan Buettcher. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 758759, 2009. Tri Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. In International Conference on Learning Representations, 2024. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. In Advances in Neural Information Processing Systems, 2022. DeepSeek-AI et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint arXiv:2501.12948, 2025. Kaustubh Deshpande, Ved Sirdeshmukh, Johannes Baptist Mols, Lifeng Jin, Ed-Yeremai Hernandez-Cardona, Dean Lee, Jeremy Kritz, Willow E. Primack, Summer Yue, and Chen Xing. MultiChallenge: Realistic Multi-Turn Conversation Evaluation Benchmark Challenging to Frontier LLMs. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 1863218702, 2025. Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. BAMBOO: Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models. arXiv preprint arXiv:2309.13345, 2023. Haodong Duan, Jueqi Wei, Chonghua Wang, Hongwei Liu, Yixiao Fang, Songyang Zhang, Dahua Lin, and Kai Chen. BotChat: Evaluating LLMs Capabilities of Having Multi-Turn Dialogues. In Findings of the Association for Computational Linguistics: NAACL 2024, pp. 31843200, 2024. Abhimanyu Dubey et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Kenneth Enevoldsen et al. MMTEB: Massive Multilingual Text Embedding Benchmark. In International Conference on Learning Representations, 2025. Google. Gemini Deep Research. https://gemini.google/overview/ deep-research/, 2025. Accessed: 2025-09-14. Taher H. Haveliwala. Topic-sensitive PageRank. In Proceedings of the 11th International Conference on World Wide Web, pp. 517526, 2002. Yancheng He, Shilong Li, Jiaheng Liu, Weixun Wang, Xingyuan Bu, Ge Zhang, Z.y. Peng, Zhaoxiang Zhang, Zhicheng Zheng, Wenbo Su, and Bo Zheng. Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning? In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1846818489, 2025. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing Multihop QA Dataset for Comprehensive Evaluation of Reasoning Steps. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 66096625, 2020. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. RULER: Whats the Real Context Size of Your Long-Context Language Models? In Conference on Language Modeling, 2024. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large Language Models Cannot Self-Correct Reasoning Yet. In International Conference on Learning Representations, 2024. Kalervo Jarvelin and Jaana Kekalainen. IR evaluation methods for retrieving highly relevant documents. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 4148, 2000. 11 Preprint Kalervo Jarvelin and Jaana Kekalainen. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst., 20(4):422446, 2002. Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, In Proceedings Jamie Callan, and Graham Neubig. Active Retrieval Augmented Generation. of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 79697992, 2023. Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan Arik. Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG. In International Conference on Learning Representations, 2025. Aishwarya Kamath et al. Gemma 3 Technical Report. arXiv preprint arXiv:2503.19786, 2025. Gregory Kamradt. Needle In Haystack - Pressure Testing LLMs. https://github.com/ gkamradt/LLMTest_NeedleInAHaystack, 2023. Accessed: Apr. 15, 2025. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi In Chen, and Wen-tau Yih. Dense Passage Retrieval for Open-Domain Question Answering. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 67696781, 2020. Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Igorevich Sorokin, Artyom Sorokin, and Mikhail Burtsev. BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack. In Advances in Neural Information Processing Systems Datasets and Benchmarks Track, 2024. Wai-Chung Kwan, Xingshan Zeng, Yuxin Jiang, Yufei Wang, Liangyou Li, Lifeng Shang, Xin Jiang, Qun Liu, and Kam-Fai Wong. MT-Eval: Multi-Turn Capabilities Evaluation Benchmark for Large Language Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 2015320177, 2024. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: Benchmark for Question Answering Research. Transactions of the Association of Computational Linguistics, 2019. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, and Jennifer Neville. LLMs Get Lost In Multi-Turn Conversation. arXiv preprint arXiv:2505.06120, 2025. Dohyeon Lee, Seung-won Hwang, Kyungjae Lee, Seungtaek Choi, and Sunghyun Park. On Complementarity Objectives for Hybrid Retrieval. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1335713368, 2023. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, Sebastian Riedel, and Douwe Kiela. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In Advances in Neural Information Processing Systems, pp. 94599474, 2020. Hao Liu, Matei Zaharia, and Pieter Abbeel. RingAttention with Blockwise Transformers for NearInfinite Context. In International Conference on Learning Representations, 2024a. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the Middle: How Language Models Use Long Contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024b. 12 Preprint Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents. arXiv preprint arXiv: 2308.03688, 2023. Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. Query Rewriting in RetrievalAugmented Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 53035315, 2023. Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun Cai, Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen Zhang, Chenlin Zhou, Jiayi Mao, Tianze Xia, Jiafeng Guo, and Shenghua Liu. Survey of Context Engineering for Large Language Models. arXiv preprint 2507.13334, 2025. Microsoft. Relevance Scoring in Hybrid Search Using Reciprocal Rank Fusion (RRF). https: //learn.microsoft.com/en-us/azure/search/hybrid-search-ranking, 2025. Accessed: 2025-06-27. OpenAI. Learning to reason with LLMs. https://openai.com/index/ learning-to-reason-with-llms/, 2024. Accessed: Sep. 11, 2025. Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The PageRank Citation Ranking : Bringing Order to the Web. In The Web Conference, 1999. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient Context Window Extension of Large Language Models. In International Conference on Learning Representations, 2024. Stephen Robertson and Hugo Zaragoza. The Probabilistic Relevance Framework: BM25 and Beyond. Found. Trends Inf. Retr., 3(4):333389, 2009. Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. Okapi at TREC-3. In TREC, volume 500-225 of NIST Special Publication, pp. 109126, 1994. Oscar Sainz, Jon Campos, Iker Garcıa-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1077610787, 2023. Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison Over Long Language Sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1200712021, 2022. Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. ZeroSCROLLS: ZeroShot Benchmark for Long Text Understanding. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 79777989, 2023. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems, 2023. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. RoFormer: Enhanced transformer with Rotary Position Embedding. Neurocomputing, 568:127063, 2024. Nandan Thakur, Nils Reimers, Andreas Ruckle, Abhishek Srivastava, and Iryna Gurevych. BEIR: Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models. In Advances in Neural Information Processing Systems Datasets and Benchmarks Track, 2021. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop Questions via Single-hop Question Composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. 13 Preprint Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1001410037, 2023. Cunxiang Wang, Ruoxi Ning, Boqi Pan, Tonghui Wu, Qipeng Guo, Cheng Deng, Guangsheng Bao, Xiangkun Hu, Zheng Zhang, Qian Wang, and Yue Zhang. NovelQA: Benchmarking Question In International Conference on Learning Answering on Documents Exceeding 200K Tokens. Representations, 2025. Liang Wang, Nan Yang, and Furu Wei. Query2doc: Query Expansion with Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 94149423, 2023. Minzheng Wang, Longze Chen, Fu Cheng, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, Yunshui Li, Min Yang, Fei Huang, and Yongbin Li. Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 56275646, 2024a. Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback. In International Conference on Learning Representations, 2024b. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient Streaming Language Models with Attention Sinks. In International Conference on Learning Representations, 2024. Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets Long Context Large Language Models. In International Conference on Learning Representations, 2024. An Yang et al. Qwen2.5-1m technical report. arXiv preprint arXiv:2501.15383, 2025a. An Yang et al. Qwen3 Technical Report. arXiv preprint arXiv:2505.09388, 2025b. Xinyu Yang, Tianqi Chen, and Beidi Chen. APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding. In International Conference on Learning Representations, 2025c. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23692380, 2018. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing Reasoning and Acting in Language Models. In International Conference on Learning Representations, 2023. Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. HELMET: How to Evaluate Long-context Models Effectively and Thoroughly. In International Conference on Learning Representations, 2025. Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Yuxing Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2307823097, 2025. Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang. LV-Eval: Balanced Long-Context Benchmark with 5 Length Levels Up to 256K. arXiv preprint arXiv:2402.05136, 2024. 14 Preprint Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. Bench: Extending Long Context Evaluation Beyond 100K Tokens. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1526215277, 2024. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. In Advances in Neural Information Processing Systems Datasets and Benchmarks Track, 2023."
        },
        {
            "title": "A LLM USAGE DISCLOSURE",
            "content": "We use Gemini 2.5 Pro, GPT-5, and Grok for writing enhancements, primarily to improve grammar and overall text flow. We also use their DeepResearch capabilities to retrieve related works for more contextualized discussions. All LLM outputs were reviewed and verified by the authors to ensure accuracy and avoid factual errors or hallucinations."
        },
        {
            "title": "B NIAH PROMPT",
            "content": "Input Prompt for NIAH Evaluation Read the following articles and answer the question below. {ordered haystack} What is the correct answer to this question: {question} Format your response as follows: The correct answer is (insert answer here)."
        },
        {
            "title": "C DYNAMIC NIAH PROMPTS",
            "content": "C.1 ENFORCED MULTI-ROUND Input Prompt for Intermediate Rounds in Enforced Multi-Round NIAH Evaluation Read your previous analyses and the following articles. Analyze the question below. Previous Analyses: {analyses} Articles: {ordered haystack} Question: {question} Based on your previous analyses and the potentially new articles provided, summarize your findings related to the question and refine the question. Format your response as follows: Summary: (Summarize what you found in the articles that relates to the question, including any partial answers, relevant context, or gaps in information.) Refined Question: (Copy the original question or replace it with more specific question based on your findings.) 15 Preprint Input Prompt for Final Round in Enforced Multi-Round NIAH Evaluation Read your previous analyses and the following articles, and answer the question below. Previous Analyses: {analyses} Articles: {ordered haystack} What is the correct answer to this question: {question} Format your response as follows: The correct answer is (insert answer here). C.2 VARIABLE-ROUND Input Prompt for Variable-Round NIAH Evaluation Read your previous analyses and the following articles. Analyze the question below. Previous Analyses: {analyses} Articles: {ordered haystack} Question: {question} Based on your previous analyses and the potentially new articles provided, decide if you are confident in answering the question or if you need additional information. If you have complete information to fully answer the question, format your response as follows: The correct answer is (insert answer here). If you need more information, format your response as follows: Summary: (Summarize what you found in the articles that relates to the question, including any partial answers, relevant context, or gaps in information.) Refined Question: (Copy the original question or replace it with more specific question based on your findings.)"
        },
        {
            "title": "D MORE DATASET DETAILS",
            "content": "In preparing the Wikipedia hyperlink network, we filter out empty and redirect pages. Table 1 provides dataset breakdown over hop count. Table 1: Question breakdown over hop count. # hops % 1 2 3 20 58 15.6 6."
        },
        {
            "title": "E ADDITIONAL SETUP DETAILS",
            "content": "E.1 LLM SETUP For each LLM, we utilize the recommended inference hyperparameters as specified on its Hugging Face model card. These settings include sampling parameters like temperature, Top-P, Top-K, and Min-P, along with the thinking budget for thinking LLMs. All models considered in this work possess native long-context support for at least 128K tokens, with the exception of the Qwen3 models. To ensure the Qwen3 models could process 128K-token input and generate 32K-token output, we extend their context window to 164K tokens using YaRN (Peng et al., 2024). 16 Preprint E.2 PPR SETUP We perform hyperparameter search for PPR per retriever using 10% of the QA samples. For retrieval criteria, we adopt Normalized Discounted Cumulative Gain (NDCG) @ 10K (Jarvelin & Kekalainen, 2000; 2002) for ranking ground truth supporting documents among the corpus. Table 2 presents the best hyperparameters for each retriever based on three random seeds. Table 2: Retriever-specific PPR hyperparameters. Retriever # Seed Documents Damping Factor BM25 Qwen3-0.6B Hybrid 10 5 5 0.5 0.5 0."
        },
        {
            "title": "F EVALUATION FOR DATA CONTAMINATION",
            "content": "To quantify data contamination, we evaluate LLM performance under two conditions: 1) without context, to test reliance on parametric knowledge, and 2) with ground-truth supporting documents. We measure F1 scores across an increasing number of the question hop count to assess how performance varies with reasoning complexity. Fig. 6 presents the evaluation results. Contamination is evident. All models achieve non-zero F1 scores without context. This indicates degree of data contamination. Context is crucial. Despite contamination, providing ground-truth documents substantially improves the performance of all models. Complexity remains challenge. F1 scores generally decrease as the question hop count increases, even when context is provided. This also suggests that evaluation with multi-hop questions suffers less from data contamination. Figure 6: LLM performance with vs without context across question hop. 17 Preprint"
        },
        {
            "title": "G CASE STUDIES FOR FAILURE PATTERNS IN DYNAMIC NIAH",
            "content": "Gemma-3-12B-IT propagates its reasoning errors via query refinement and summarization. [Original query]: What body of water is by the place John Dury died? [First-round response]: Summary: The provided text discusses John Dury and his life, focusing on his religious and intellectual work. It states that he died in Edinburgh. Edinburgh is located on the Firth of Forth. Therefore, John Dury died near the Firth of Forth. born in Edinburgh. // Wrong. He was Refined Question: What body of water is located near Edinburgh, where John Dury died? will inflate the ranking of distractor documents. // The mistake got consolidated and [Second-round response]: Summary: The provided articles offer several pieces of information related to John Dury and the geography of Edinburgh. Firstly, the article on Edinburgh states its located near the Firth of Forth. Secondly, the article on John Dury mentions that he died in Edinburgh, and Edinburgh is located on the Firth of Forth. Therefore, John Dury died near the Firth of Forth. The articles do not offer any other specific bodies of water near Edinburgh. Refined Question: What body of water is Edinburgh situated on, in which John Dury died? consolidated early mistakes. // The LLM cannot correct [Final answer prediction]: The correct answer is the Firth of Forth. // The final answer is then naturally wrong. GPT-5 deviated from the original query intent. [Original query]: What year saw the formation of the Brazilian republic? [Refined query after the first round]: In what year (and on what date) was the Brazilian Republic proclaimed? what date // GPT-5 added on [Refined query after the second round]: In what year, and on what date, was the Brazilian Republic proclaimed? (Answer: 15 November 1889) // GPT-5 reinforced the earlier query deviation. [Final answer prediction]: The correct answer is 15 November 1889. // The correct answer should be just 1889. GPT-5 mini changed open questions to yes/no questions. [Original query]: What country is the member of the band that performed Visions of Eden citizen of? [Refined query after the first round]: Which country is David DeFeis (the Virgin Steele member who performed on Visions of Eden) citizen of? [Refined query after the second round]: Is David DeFeis citizen of the United States? If so, can you cite source that explicitly confirms his country of citizenship (e.g., place of birth, official biography or other primary documentation)? // The question got changed to an yes/no question. [Final answer prediction]: Yes. // GPT-5 mini even failed to answer its own second question, despite the deviation. GPT-5 mini changed open questions to multiple-choice questions. [Original query]: What year did unification of the country Mintu is located happen? [Refined query after the first round]: Do you mean the year Sarawak became part of the federation of Malaysia (i.e., when Malaya, North Borneo/Sabah, Sarawak and Singapore united) which was 1963 or do you mean the earlier independence of the Federation of Malaya in 1957? [Refined query after the second round]: Do you mean (A) the year Sarawak (and thus Mintu) joined the Federation of Malaysia Malaysia Day, 16 September 1963 or (B) the earlier independence of the Federation of Malaya Hari Merdeka, 31 August 1957? // The question got changed to multiple-choice question. [Final answer prediction]: A. 18 Preprint Gemini 2.5 Pro fails due to long-context challenges. [Original query]: What is the nickname of the state where John Kerrys VP pick was senator? [First-round response]: Summary: The articles identify John Kerrys vice presidential running mate in the 2004 presidential election as John Edwards. At the time of his selection, Edwards was U.S. Senator from North Carolina. The article titled North Carolina provides two slogans used on the states license plates: First in Flight and First in Freedom. While the article also refers to Tar heel voters, it does not explicitly state that The Tar Heel State is the official nickname. Refined Question: What is the nickname of the state where John Kerrys VP pick was senator? identifying relevant text about Old North State. It also does not update the query properly by incorporating partial evidence. // Gemini 2.5 Pro simply fails in [Second-round response]: Summary: The articles confirm that John Kerrys vice presidential running mate in the 2004 presidential election was John Edwards. Multiple articles state that Edwards was U.S. Senator from North Carolina at the time of his selection. The article on North Carolina mentions two slogans used on the states license plates, First in Flight and First in Freedom, but none of the provided texts explicitly identify the states official nickname. Refined Question: What is the nickname of the state where John Kerrys VP pick was senator? [Final answer prediction]: Based on the provided articles, the correct answer is that the texts do not explicitly state an official nickname for North Carolina."
        },
        {
            "title": "H RAW EXPERIMENT RESULTS FOR RETRIEVAL EVALUATION",
            "content": "See Table 3 and Table 4."
        },
        {
            "title": "I RAW EXPERIMENT RESULTS FOR STATIC NIAH WITH",
            "content": "RETRIEVAL-RANKED HAYSTACK ORDERING BM25: Table 5 Qwen3-0.6B: Table 6 Hybrid: Table 7 BM25 + PPR: Table 8 Qwen3-0.6B + PPR: Table 9 Hybrid + PPR: Table"
        },
        {
            "title": "RANDOM HAYSTACK ORDERINGS",
            "content": "BM25: Table 11 Qwen3-0.6B: Table 12 Hybrid: Table 13 BM25 + PPR: Table 14 Qwen3-0.6B + PPR: Table 15 Hybrid + PPR: Table"
        },
        {
            "title": "K RAW EXPERIMENT RESULTS FOR DYNAMIC NIAH",
            "content": "See Table 17 and Table 18."
        },
        {
            "title": "L IMPLEMENTATION DETAILS",
            "content": "We employ vLLM for LLM inference (Kwon et al., 2023). 19 Preprint Table 3: Recall@N of retrieval strategies for coverage evaluation (102, ), with breakdown over question hop. We present the results in way that allows comparing the impact of using PPR or not, and we highlight the better results. Base Retriever Hop PPR @10 @20 @40 @80 @160 BM25 BM BM25 BM25 BM25 Overall 1-hop 2-hop 3-hop 4-hop Qwen3-0.6B Overall Qwen3-0.6B 1-hop Qwen3-0.6B 2-hop Qwen3-0.6B 3-hop Qwen3-0.6B 4-hop Hybrid Overall Hybrid Hybrid Hybrid Hybrid 1-hop 2-hop 3-hop 4-hop 37.93 38 42.83 55.02 47.43 59.97 53.13 63.63 58.73 66. 72 77 41.03 57.41 23.93 35.04 14.06 13.28 49.87 59.05 86 46.38 61.03 29.91 34.62 17.19 13.28 55.88 63.05 95 87 52.07 66. 32.91 38.46 24.22 21.09 78 78 45.17 62.76 29.06 44.02 17.19 17. 53.48 65.7 89 92 50.52 68.97 32.91 39.74 19.53 17.19 60.28 69. 96 90 56.55 71.9 41.88 50 27.34 25 86 78 49.14 66. 38.03 53.42 23.44 21.88 57.3 71.12 91 93 53.97 76.38 39.74 44. 25 21.09 64 73.52 96 92 60.17 76.38 50 56.84 32.81 30. 88 80 54.14 69.31 49.57 55.98 31.25 25.78 61.43 74.28 94 58.28 79.48 44.44 49.57 29.69 25.78 67.2 76.55 97 93 62.93 79. 56.41 61.54 39.06 38.28 66 66 36.21 36.21 20.09 20.51 9.38 9. 45 52.35 80 86 42.07 51.72 24.36 28.21 12.5 11.72 51.07 58. 89 83 47.41 61.21 28.63 31.62 20.31 18.75 20 Preprint Table 4: NDCG@N of retrieval strategies for ranking evaluation (102, ), with breakdown over question hop. We present the results in way that allows comparing the impact of using PPR or not, and we highlight the better results. Base Retriever Hop PPR @10 @20 @ @80 @160 BM25 BM25 BM25 BM25 BM Overall 1-hop 2-hop 3-hop 4-hop Qwen3-0.6B Overall Qwen3-0.6B 1-hop Qwen3-0.6B 2-hop Qwen3-0.6B 3-hop Qwen3-0.6B 4-hop Hybrid Overall Hybrid Hybrid Hybrid Hybrid 1-hop 2-hop 3-hop 4-hop 32.83 38.35 45.3 50. 34.22 40.52 21.58 26.13 8.69 9.71 42.7 47.04 63.3 67.79 43.2 48. 25.51 28.57 15.63 14.67 46.41 50.3 68.87 67.74 46.21 52.58 28.89 32. 20.71 19.76 33.96 39.66 46.5 50.99 35.25 41.87 23.03 28.7 9.71 10. 43.6 48.72 63.89 68.79 44.24 50.46 26.35 30.03 16.35 15.86 47.57 51. 69.06 68.36 47.34 54.07 31.46 35.36 21.66 21 35.16 40.49 47.9 50. 36.07 42.55 25.17 30.95 11.4 12.24 44.43 49.89 64.22 68.96 44.96 52. 28.01 31.07 17.83 16.87 48.39 52.86 69.06 68.69 48.1 55.03 33.37 23.1 22.47 36.22 41.03 48.2 51.27 36.97 43.14 27.56 31.49 13.18 13. 45.2 50.49 64.65 69.11 45.75 52.58 28.97 32.2 18.92 17.93 49 53. 69.21 68.84 48.59 55.53 34.68 37.95 24.51 24.24 31.31 32.86 43.71 47. 32.74 33.64 20.14 20.76 6.76 8.06 41.18 44.91 61.72 67.53 41.86 45. 23.58 26.25 13.8 14.09 44.91 48.81 67.33 66.75 44.75 51.09 27.37 29. 19.06 18.82 21 Preprint Table 5: Static NIAH performance in F1 score (102, ) using BM25 for haystack construction, where retriever-ranked haystack ordering is used. 0 stands for the case without distractors. Context Size (# Tokens) 8K 16K 32K 64K 59.8 Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct 67.7 Qwen2.5-7B-Instruct-1M 55.56 Qwen2.5-14B-Instruct-1M 61.76 Gemma-3-12B-IT 67.49 67.71 Gemma-3-27B-IT GPT-4.1 mini 70.19 71.66 Qwen3-8B 71.9 Qwen3-14B Qwen3-32B 71.32 71.6 Gemini 2.5 Flash-Lite o4-mini 75.95 58.41 66.28 52.12 59.46 64.15 66.24 69.05 71.62 71.08 73.62 70.96 76. 53.81 62.56 48.42 55.28 59.54 62.25 67.36 69.84 69.5 69.97 69.94 74.24 46.16 57.72 44.39 51.3 56.47 58.05 64.28 67.94 67.7 68.59 69.36 74.02 42.06 48.2 42.38 46.65 52.4 54.34 60.73 61.72 64.74 67.15 68.93 70.36 128K 37.24 30.71 39.5 45.87 44.45 51.68 60.55 60.1 62.42 64.87 66.14 67.58 Table 6: Static NIAH performance in F1 score (102, ) using Qwen3-0.6B for haystack construction, where retriever-ranked haystack ordering is used. 0 stands for the case without distractors. Context Size (# Tokens) 0 8K 16K 32K 64K 59.8 Llama-3.1-8B-Instruct 67.7 Llama-3.1-70B-Instruct Qwen2.5-7B-Instruct-1M 55.56 Qwen2.5-14B-Instruct-1M 61.76 67.49 Gemma-3-12B-IT 67.71 Gemma-3-27B-IT 70.19 GPT-4.1 mini 71.66 Qwen3-8B 71.9 Qwen3-14B 71.32 Qwen3-32B 71.6 Gemini 2.5 Flash-Lite 75.95 o4-mini 57.22 64.36 52.59 56.4 60.75 63.86 67.6 70.41 69.29 69.49 70.81 75.28 52.92 61.6 47.04 51.66 57.24 60.92 66.49 68.15 69.38 69.14 70.69 73.42 44.74 54.42 42.9 49.61 52.01 55.5 62.73 65.14 65.96 68.65 70.27 73.73 40.42 45.73 39.23 45.8 48.21 53.05 61.33 62.2 64.89 64.01 64.72 69.62 128K 31.51 25.85 36.14 42.39 42.43 48.98 60.05 59.22 61.39 62.41 63.49 66.98 Table 7: Static NIAH performance in F1 score (102, ) using hybrid retriever for haystack construction, where retriever-ranked haystack ordering is used. 0 stands for the case without distractors. Context Size (# Tokens) 0 8K 16K 32K 64K 59.8 Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct 67.7 Qwen2.5-7B-Instruct-1M 55.56 Qwen2.5-14B-Instruct-1M 61.76 Gemma-3-12B-IT 67.49 67.71 Gemma-3-27B-IT 70.19 GPT-4.1 mini 71.66 Qwen3-8B 71.9 Qwen3-14B 71.32 Qwen3-32B 71.6 Gemini 2.5 Flash-Lite 75.95 o4-mini 53.41 62.44 48.42 52.48 56.63 60.7 66.02 68.74 68.82 69.98 69.08 73.54 43.02 52.04 42.16 50.7 53.68 54.78 64.42 66.52 67.29 67.55 67.69 73.12 36.99 42.33 37.5 46.24 48.62 50.73 60.72 61.54 64.42 64.66 64.83 68. 56.71 66.09 52.54 57.17 62.26 65.79 68.56 71.36 70.46 71.19 70.78 76.43 22 128K 30.22 25.11 35.16 42.91 44.1 48.4 58.27 57.29 61.28 62.07 62.78 67.9 Preprint Table 8: Static NIAH performance in F1 score (102, ) using BM25 + PPR for haystack construction, where retriever-ranked haystack ordering is used. 0 stands for the case without distractors. Context Size (# Tokens) 0 8K 16K 32K 64K 59.8 Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct 67.7 Qwen2.5-7B-Instruct-1M 55.56 Qwen2.5-14B-Instruct-1M 61.76 Gemma-3-12B-IT 67.49 67.71 Gemma-3-27B-IT GPT-4.1 mini 70.19 71.66 Qwen3-8B Qwen3-14B 71.9 71.32 Qwen3-32B Gemini 2.5 Flash-Lite 71.6 75.95 o4-mini 60.52 66.49 51.69 58.17 64.11 65.98 69.07 71.32 72.62 71.58 71.95 75.42 53.63 64.14 49.6 56.17 60.85 63.08 66.67 71.23 71.33 71.37 71.72 74.82 49.81 60.66 46.16 54.61 58.82 58.13 64.46 69.38 70.19 69.13 69.52 74.67 45.87 51.26 46.91 51.68 54.43 56.23 63.36 64.44 66.31 67.93 69.18 72.88 128K 42.8 37.08 45.95 49.78 47.89 53.71 62.2 62.37 66.17 65.7 64.86 69.88 Table 9: Static NIAH performance in F1 score (102, ) using Qwen3-0.6B + PPR for haystack construction, where retriever-ranked haystack ordering is used. 0 stands for the case without distractors. Context Size (# Tokens) 0 8K 16K 32K 64K 59.8 Llama-3.1-8B-Instruct 67.7 Llama-3.1-70B-Instruct Qwen2.5-7B-Instruct-1M 55.56 Qwen2.5-14B-Instruct-1M 61.76 67.49 Gemma-3-12B-IT 67.71 Gemma-3-27B-IT 70.19 GPT-4.1 mini 71.66 Qwen3-8B 71.9 Qwen3-14B 71.32 Qwen3-32B 71.6 Gemini 2.5 Flash-Lite 75.95 o4-mini 55.85 65.75 52.51 57.95 64.25 65.08 67.64 71.35 69.97 71.11 71.48 73.73 54.26 63.01 50.93 52.83 59.27 61.46 65.43 70.08 69.46 70.8 70.43 74.18 45.25 55.82 46.47 54.41 56.33 59.64 64.74 66.23 68.39 70.28 70.33 72. 41.37 48.65 42.88 50.96 53.72 57.39 61.92 62.68 65.33 68.26 66.05 70.24 128K 36.69 33.42 42.96 48.82 47.2 50.93 61.45 60.94 62.42 64.55 66.53 69.03 Table 10: Static NIAH performance in F1 score (102, ) using hybrid + PPR retriever for haystack construction, where retriever-ranked haystack ordering is used. 0 stands for the case without distractors. Context Size (# Tokens) 8K 16K 32K 64K 59.8 Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct 67.7 Qwen2.5-7B-Instruct-1M 55.56 Qwen2.5-14B-Instruct-1M 61.76 67.49 Gemma-3-12B-IT 67.71 Gemma-3-27B-IT 70.19 GPT-4.1 mini 71.66 Qwen3-8B 71.9 Qwen3-14B 71.32 Qwen3-32B 71.6 Gemini 2.5 Flash-Lite 75.95 o4-mini 50.45 61.56 49.07 52.93 58.43 60.97 66.63 69.22 69 70.93 68.79 73. 46.64 56.6 46.02 51.29 57.35 59.56 65.23 68.45 67.97 69.29 69.51 73.61 45.41 48.64 44.31 51.14 54 57.55 64.73 63.59 67 65.8 68.07 70.23 55.04 65.85 52.06 57.92 62.95 65.45 69.48 71.37 70.32 71.11 71.86 75.1 23 128K 38.11 36.22 45.65 49.88 48.8 52.51 62.09 63.98 63.85 64.14 66.07 70. Preprint Table 11: Static NIAH performance in F1 score (102, ) using BM25 for haystack construction, where we average the results over three random Haystack orderings. 0 stands for the case without distractors. Context Size (# Tokens) 0 8K 16K 32K 64K 128K 59.8 Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct 67.7 Qwen2.5-7B-Instruct-1M 55.56 Qwen2.5-14B-Instruct-1M 61.76 67.49 Gemma-3-12B-IT Gemma-3-27B-IT 67.71 70.19 GPT-4.1 mini Qwen3-8B 71.66 71.9 Qwen3-14B 71.32 Qwen3-32B 71.6 Gemini 2.5 Flash-Lite 75.95 o4-mini 56.18 0.85 66.51 0.15 50.7 1.3 58.32 0.73 62.66 0.97 63.17 0.65 69.04 0.27 71.3 0.81 71.39 0.43 71.73 0.46 72.16 0.89 75.6 0.41 52.25 1.21 63.79 1.21 45.73 0.66 53.95 1.4 57.18 0.78 60.53 0.26 67.41 0.26 71.02 0.82 69.8 1.18 71.22 0.9 72.07 0.73 74.73 0. 44.71 0.86 58.41 0.57 39.22 0.17 48.86 0.75 52.39 1.69 53.19 1.44 64.57 0.86 67.13 0.39 68.39 0.69 68.94 1.1 69.52 1.35 73.39 0.2 40.43 1.78 49.14 1 36.85 0.38 44.16 0.7 45.88 1.31 47.35 0.94 60.28 0.4 58.53 1.33 66.22 0.41 67.71 0.61 67.66 0.19 70.48 1.09 34.3 0.76 30.58 0.72 33.66 0.47 37.7 0.42 33.39 0.42 38.93 1.21 56.84 0.36 53.48 1.55 61.1 0.62 62.11 1.81 65.61 0.34 67.24 0.57 Table 12: Static NIAH performance in F1 score (102, ) using Qwen3-0.6B for haystack construction, where we average the results over three random Haystack orderings. 0 stands for the case without distractors. Context Size (# Tokens) 8K 16K 32K 64K 128K 59.8 Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct 67.7 Qwen2.5-7B-Instruct-1M 55.56 Qwen2.5-14B-Instruct-1M 61.76 67.49 Gemma-3-12B-IT 67.71 Gemma-3-27B-IT 70.19 GPT-4.1 mini 71.66 Qwen3-8B 71.9 Qwen3-14B 71.32 Qwen3-32B 71.6 Gemini 2.5 Flash-Lite 75.95 o4-mini 54.87 0.75 65.69 0.77 50.79 1.29 57.1 0.39 61.2 0.2 62.87 0.39 68.2 0.53 71.69 0.81 69.83 0.98 70.66 1.22 71.39 0.49 74.54 0.17 50.71 0.89 62.52 1.01 46.75 0.85 52.83 1.81 55.51 1.0 57.77 0.26 66.09 1.67 69.9 0.52 69.04 1.14 70.29 0.76 70.61 0.68 75.33 0.43 43.41 0.6 56.81 1.1 41.05 1.00 48.7 0.22 51.24 0.46 51.71 0.37 63.57 0.45 64.12 0.82 68.03 0.13 68.68 0.56 69.67 0.87 72.93 0.44 39.08 1.00 47.87 0.2 34.85 0.50 40.78 0.88 42.66 0.79 42.95 1.05 59.97 0.55 54.24 0.41 64.21 0.52 64.65 0.39 67.39 0.35 69.72 0.59 33.33 0.5 26.49 0.73 29.55 0.46 35.36 0.28 32.37 1.66 34.67 0.38 56.74 1.38 48.89 1.62 56.77 1.45 57.11 1.22 62.58 0.64 63.91 1.39 Table 13: Static NIAH performance in F1 score (102, ) using hybrid retriever for haystack construction, where we average the results over three random Haystack orderings. 0 stands for the case without distractors. Context Size (# Tokens) 0 8K 16K 32K 64K 128K 59.8 Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct 67.7 Qwen2.5-7B-Instruct-1M 55.56 Qwen2.5-14B-Instruct-1M 61.76 67.49 Gemma-3-12B-IT 67.71 Gemma-3-27B-IT 70.19 GPT-4.1 mini 71.66 Qwen3-8B 71.9 Qwen3-14B 71.32 Qwen3-32B 71.6 Gemini 2.5 Flash-Lite 75.95 o4-mini 54.95 1.16 66.29 0.19 52.46 1.21 58.28 0.29 62.41 0.43 64.26 0.43 68.46 0.53 72.03 0.68 70.21 0.15 71.61 0.69 71.56 0.81 74.67 0.32 51.33 0.85 62.54 0.25 46.7 1.14 52.69 0.46 56.83 0.77 57.56 0.35 66.4 0.81 69.57 0.42 69.97 0.4 70.66 0.84 70.04 0.6 74.73 1.23 43.77 1.42 57.51 0.05 40.32 0.78 47.98 0.8 51.66 0.25 52.08 0.22 62.48 0.39 65.96 0.97 66.74 0.37 68.19 1.14 69.24 0.78 72.88 0.38 37.42 0.92 47.76 1.77 35.26 1.37 42.9 0.71 44.11 0.63 44.95 1.07 59.72 0.38 57.68 0.65 65.4 0.71 63.68 0.59 66.75 0.65 70.22 0. 33.34 0.58 28.82 1.01 32.52 0.61 36.16 1.04 32.51 1.72 36.85 1.16 56.77 0.81 51.85 0.86 59.58 1.13 57.99 0.65 64.06 0.51 65.64 0.2 24 Preprint Table 14: Static NIAH performance in F1 score (102, ) using BM25 + PPR for haystack construction, where we average the results over three random Haystack orderings. 0 stands for the case without distractors. Context Size (# Tokens) 8K 16K 32K 64K 128K 59.8 Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct 67.7 Qwen2.5-7B-Instruct-1M 55.56 Qwen2.5-14B-Instruct-1M 61.76 67.49 Gemma-3-12B-IT Gemma-3-27B-IT 67.71 70.19 GPT-4.1 mini Qwen3-8B 71.66 71.9 Qwen3-14B 71.32 Qwen3-32B 71.6 Gemini 2.5 Flash-Lite 75.95 o4-mini 56.76 0.34 66.83 0.41 52.14 0.51 58.73 0.73 62.8 0.42 64.31 0.83 69.12 0.28 71.71 0.35 70.93 0.11 71.48 0.25 72.66 1.48 75.86 0.71 51.34 0.64 63.39 0.96 45.37 1.58 53.72 1.03 58.08 0.49 60.06 0.64 67.36 0.96 70.6 0.78 70.24 0.81 70.86 0.26 72.69 0.66 74.77 0.8 45.13 1.19 59.46 0.95 42.73 1.63 49.81 1.26 52.37 0.85 53.15 1.13 64.48 0.71 66.74 0.81 68.22 0.51 69.2 0.96 69.43 0.96 73.73 0.22 41.23 1.16 50.4 1.22 37.8 1.88 44.38 1.29 45.95 1.31 49.49 0.64 60.93 0.7 60.06 0.4 66.72 1.13 66.73 0.96 66.61 0.52 71.73 0.36 37.44 2.2 33.53 1.22 37.74 0.19 41.94 0.79 38.5 0.21 42.65 0.94 59.26 1.29 57.87 0.37 62.8 0.65 63.91 0.68 65.64 0.26 68.96 0.47 Table 15: Static NIAH performance in F1 score (102, ) using Qwen3-0.6B + PPR for haystack construction, where we average the results over three random Haystack orderings. 0 stands for the case without distractors. Context Size (# Tokens) 0 8K 16K 32K 64K 128K 59.8 Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct 67.7 Qwen2.5-7B-Instruct-1M 55.56 Qwen2.5-14B-Instruct-1M 61.76 67.49 Gemma-3-12B-IT 67.71 Gemma-3-27B-IT 70.19 GPT-4.1 mini 71.66 Qwen3-8B 71.9 Qwen3-14B 71.32 Qwen3-32B 71.6 Gemini 2.5 Flash-Lite 75.95 o4-mini 55.22 1.19 66.49 0.78 51.54 0.68 58.45 0.29 62.78 0.38 63.72 0.52 68.08 1.18 71.53 0.59 70.46 1.0 70.63 0.57 72.13 1.34 74.49 0.38 51.4 1.12 62.85 0.07 47.34 1.24 53.54 0.49 56.4 0.75 59.01 1.53 66.18 0.3 69.82 0.89 70.19 0.77 70.19 0.22 70.21 0.25 74.35 0.19 46.64 0.94 57.63 0.3 44.77 0.48 49.4 1.65 53.29 0.87 54.35 0.84 63.23 0.56 67.09 0.52 67.61 0.1 68.77 1.35 69.26 0.77 73.86 0.51 41.6 1.04 50.4 0.95 41.14 1.06 44.76 1.43 46.8 0.77 49.01 1.02 61.09 0.61 58.84 1.45 66.08 0.66 65.64 0.47 67.64 0.64 70.81 0. 36.71 0.6 33.35 0.65 36.05 1.55 39.46 1.15 38.47 0.21 40.79 0.93 57.49 0.31 57.84 1.53 61.4 0.64 61.54 1.12 65.26 1.6 67.71 0.41 Table 16: Static NIAH performance in F1 score (102, ) using hybrid + PPR retriever for haystack construction, where we average the results over three random Haystack orderings. 0 stands for the case without distractors. Context Size (# Tokens) 0 8K 16K 32K 64K 128K 59.8 Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct 67.7 Qwen2.5-7B-Instruct-1M 55.56 Qwen2.5-14B-Instruct-1M 61.76 67.49 Gemma-3-12B-IT 67.71 Gemma-3-27B-IT 70.19 GPT-4.1 mini 71.66 Qwen3-8B 71.9 Qwen3-14B 71.32 Qwen3-32B 71.6 Gemini 2.5 Flash-Lite 75.95 o4-mini 55.19 1.37 66.97 0.78 52.21 1.02 58.03 0.55 62.2 0.98 64.02 1.13 68.7 0.76 71.5 0.71 70.14 0.32 71.96 0.67 71.57 0.87 75.59 0.19 51.69 0.71 62.81 1.17 47.8 0.19 53.63 0.79 57.27 1.04 58.84 0.49 66.51 0.81 69.7 0.32 68.83 0.74 71.03 0.57 70.39 0.6 74.14 0. 45.57 1.02 58.75 0.6 44.18 0.38 50.8 0.69 53.32 0.47 55.6 0.68 64.43 0.61 66.95 0.51 66.87 2.12 70.13 0.43 69.07 0.35 73.68 0.23 41.97 0.54 51.36 0.61 40.59 0.49 46.9 1.25 48.2 0.88 50.27 0.14 61.79 1.28 60.46 0.28 66.81 0.78 65.37 0.55 66.42 0.4 71.03 0.61 38.99 1.78 33.16 1.12 39.17 1.27 42.15 0.7 37.51 4.94 41.95 1.08 60.75 0.31 58.62 0.52 62.13 0.47 63.4 0.62 65.27 0.52 68.13 0.4 25 Preprint Table 17: Dynamic NIAH performance in F1 score (102, ) using BM25 + PPR for haystack construction, where retriever-ranked haystack ordering is used. 0 stands for the case without distractors. 128K 39.97 27.88 32.77 29.66 48.12 36.8 21.93 34.97 45.51 36.3 29.28 40.87 68.72 54.58 46.22 59.83 65.38 61.78 57.03 64. 69.65 60.88 54.75 62.03 72.51 51.79 40.74 65.46 75.15 68.58 63.31 71.96 Context Size (# Tokens) # Rounds 8K 16K 32K 64K Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct-1M Gemma-3-12B-IT Qwen3-8B Gemini 2.5 Flash-Lite Gemini 2.5 Pro GPT-5 mini GPT1 2 3 max 3 1 2 3 max 3 1 2 3 max 3 1 2 3 max 3 1 2 3 max 3 1 2 3 max 1 2 3 max 3 1 2 3 max 3 60.46 49.52 33.79 43.16 55.16 46.31 25.35 38.99 63.48 47.96 38.94 60.1 74.3 58.54 47.34 69. 73.29 72.75 71.25 70.97 72.78 65.99 70.38 71.08 78.04 58.96 41 70.4 77.36 72.33 70.13 74.89 52.06 51.41 37.04 34.85 52.63 37.53 28.14 35. 61.94 53.82 40.31 62.19 74.28 58.24 52.3 63.86 72.67 64.79 63.92 69.73 74.97 64.39 66.36 67.85 79.1 58.24 32.89 71.42 77.2 68.73 67.2 75. 51.93 39.18 28.63 31.62 45.96 28.57 22.88 36.51 57.27 40.63 32.48 52.01 71.93 53.1 42.98 64.25 72.42 61.44 63.2 65.83 70.45 60.68 58.63 63. 75.78 55.54 38.66 68.2 75.03 70.17 66.45 73.56 42.42 37.71 23.79 30.45 46.3 34.78 20.98 34.5 54.64 41.77 35.18 51.11 67.54 54.79 46.79 59. 69.4 63.12 60.49 65.97 69.34 60.98 55.98 66.35 75.27 52.15 36.39 71.81 72.98 66.29 64.07 74.03 56.86 50.88 26.51 34.52 60.59 37.64 24 43. 66.54 48.91 39.07 64.88 71.98 62.89 49.87 69.06 74.62 69.3 71.46 71.55 75.86 72.66 69.06 68.36 79.87 56.48 40.81 73.47 78.28 72.72 71.7 76. 26 Preprint Table 18: Dynamic NIAH performance in F1 score (102, ) using Qwen3-0.6B for haystack construction, where retriever-ranked haystack ordering is used. 0 stands for the case without distractors. Context Size (# Tokens) # Rounds 8K 16K 32K 64K Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct-1M Gemma-3-12B-IT Qwen3-8B Gemini 2.5 Flash-Lite Gemini 2.5 Pro GPT-5 mini GPT1 2 3 max 3 1 2 3 max 3 1 2 3 max 3 1 2 3 max 3 1 2 3 max 3 1 2 3 max 1 2 3 max 3 1 2 3 max 3 56.86 56.68 36.68 38.5 60.59 32.44 26.1 44.36 66.54 55.13 40.07 61.37 71.98 58.95 49.26 69. 74.62 68.04 69.62 69.35 75.86 69.07 67.26 71.37 79.87 60.24 39.63 73.27 78.28 71.92 71.21 76.22 56.76 45.59 30.48 31.12 55.45 35.22 23.45 36. 63.77 51.63 43.32 63.83 69.61 59.54 46.95 65.31 72.25 66.85 64.97 66.77 73.34 65.81 63.84 68.28 77.18 57.4 41.02 71.7 74.8 69.38 68.35 76. 53.41 51.41 38.74 37.8 50.54 37.24 22.02 33.9 58.67 49.39 37.54 59.1 65.56 62.48 50.32 63.26 73.99 65.56 64.6 67.34 72.20 65.95 62.91 65. 76.97 57.2 38.99 70.77 76.31 70.28 63.01 71.92 49.45 39.18 27.87 31.7 44.36 37.41 24.84 31.46 52.66 47.28 32.23 58.86 69.44 63.91 49.84 59. 71.92 64.87 58.91 68.09 73.9 62.05 61.02 63.12 73.68 53.82 35.26 68.98 76.98 72.96 68.52 71.15 42.81 37.71 29.65 33.55 39.97 38.49 20.33 38. 55.18 44.81 32.57 57.05 62.41 57.94 40.14 55.2 70.23 62.67 63.56 63.23 69.61 61.16 53.09 59.73 72.14 51.23 37.73 67.59 73.47 68.26 63.35 72. 128K 30.15 27.88 26.08 25.24 35.51 35.63 27.18 38.12 47.59 35.55 23.7 50.16 55.57 49.83 41.64 54.32 63.28 62.76 63.23 64. 68.52 58.08 51.43 60.93 70.14 53.33 37.92 65.59 71.7 66.3 65.48 72."
        }
    ],
    "affiliations": [
        "Georgia Institute of Technology",
        "Meta AI",
        "National University of Singapore",
        "University of Illinois Urbana-Champaign"
    ]
}