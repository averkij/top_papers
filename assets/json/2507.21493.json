{
    "paper_title": "BANG: Dividing 3D Assets via Generative Exploded Dynamics",
    "authors": [
        "Longwen Zhang",
        "Qixuan Zhang",
        "Haoran Jiang",
        "Yinuo Bai",
        "Wei Yang",
        "Lan Xu",
        "Jingyi Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D creation has always been a unique human strength, driven by our ability to deconstruct and reassemble objects using our eyes, mind and hand. However, current 3D design tools struggle to replicate this natural process, requiring considerable artistic expertise and manual labor. This paper introduces BANG, a novel generative approach that bridges 3D generation and reasoning, allowing for intuitive and flexible part-level decomposition of 3D objects. At the heart of BANG is \"Generative Exploded Dynamics\", which creates a smooth sequence of exploded states for an input geometry, progressively separating parts while preserving their geometric and semantic coherence. BANG utilizes a pre-trained large-scale latent diffusion model, fine-tuned for exploded dynamics with a lightweight exploded view adapter, allowing precise control over the decomposition process. It also incorporates a temporal attention module to ensure smooth transitions and consistency across time. BANG enhances control with spatial prompts, such as bounding boxes and surface regions, enabling users to specify which parts to decompose and how. This interaction can be extended with multimodal models like GPT-4, enabling 2D-to-3D manipulations for more intuitive and creative workflows. The capabilities of BANG extend to generating detailed part-level geometry, associating parts with functional descriptions, and facilitating component-aware 3D creation and manufacturing workflows. Additionally, BANG offers applications in 3D printing, where separable parts are generated for easy printing and reassembly. In essence, BANG enables seamless transformation from imaginative concepts to detailed 3D assets, offering a new perspective on creation that resonates with human intuition."
        },
        {
            "title": "Start",
            "content": "BANG: Dividing 3D Assets via Generative Exploded Dynamics LONGWEN ZHANG, ShanghaiTech University, China and Deemos Technology Co., Ltd., China QIXUAN ZHANG, ShanghaiTech University, China and Deemos Technology Co., Ltd., China HAORAN JIANG, ShanghaiTech University, China and Deemos Technology Co., Ltd., China YINUO BAI, ShanghaiTech University, China and Deemos Technology Co., Ltd., China WEI YANG, Huazhong University of Science and Technology, China LAN XU, ShanghaiTech University, China JINGYI YU, ShanghaiTech University, China 5 2 0 2 9 2 ] . [ 1 3 9 4 1 2 . 7 0 5 2 : r Fig. 1. An exploded view, generated and enhanced by our framework BANG, of futuristic mechanical humanoid where the fusion of organic form and mechanical precision is laid bare. Each component of the humanoid is generated by recursively exploding its parent component using Generative Exploded Dynamics (Sec. 3) and enhanced through Per-part Geometric Details Enhancement (Sec. 5.1). This process is conducted iteratively to create the final exploded view, which is rendered using Blender [Blender Foundation oing]. 3D creation has always been unique human strength, driven by our ability to deconstruct and reassemble objects using our eyes, mind and hand. However, current 3D design tools struggle to replicate this natural process, Corresponding author. Authors addresses: Longwen Zhang, ShanghaiTech University, Shanghai, China and Deemos Technology Co., Ltd., Shanghai, China, zhanglw2@shanghaitech.edu.cn; Qixuan Zhang, ShanghaiTech University, Shanghai, China and Deemos Technology Co., Ltd., Shanghai, China, zhangqx1@shanghaitech.edu.cn; Haoran Jiang, ShanghaiTech University, Shanghai, China and Deemos Technology Co., Ltd., Shanghai, China, jianghr2024@shanghaitech.edu.cn; Yinuo Bai, ShanghaiTech University, Shanghai, China and Deemos Technology Co., Ltd., Shanghai, China, baiyn2022@ shanghaitech.edu.cn; Wei Yang, Huazhong University of Science and Technology, Wuhan, China, weiyangcs@hust.edu.cn; Lan Xu, ShanghaiTech University, Shanghai, China, xulan1@shanghaitech.edu.cn; Jingyi Yu, ShanghaiTech University, Shanghai, China, yujingyi@shanghaitech.edu.cn. requiring considerable artistic expertise and manual labor. This paper introduces BANG, novel generative approach that bridges 3D generation and reasoning, allowing for intuitive and flexible part-level decomposition of 3D objects. At the heart of BANG is \"Generative Exploded Dynamics\", which creates smooth sequence of exploded states for an input geometry, progressively separating parts while preserving their geometric and semantic coherence. BANG utilizes pre-trained large-scale latent diffusion model, fine-tuned for exploded dynamics with lightweight exploded view adapter, allowing precise control over the decomposition process. It also incorporates temporal attention module to ensure smooth transitions and consistency across time. BANG enhances control with spatial prompts, such as bounding boxes and surface regions, enabling users to specify which parts to decompose and how. This interaction can be extended with multimodal models like GPT-4, enabling 2D-to-3D manipulations for more intuitive and creative 2 Longwen Zhang, Qixuan Zhang, Haoran Jiang, Yinuo Bai, Wei Yang, Lan Xu, and Jingyi Yu workflows. The capabilities of BANG extend to generating detailed part-level geometry, associating parts with functional descriptions, and facilitating component-aware 3D creation and manufacturing workflows. Additionally, BANG offers applications in 3D printing, where separable parts are generated for easy printing and reassembly. In essence, BANG enables seamless transformation from imaginative concepts to detailed 3D assets, offering new perspective on creation that resonates with human intuition. CCS Concepts: Computing methodologies Artificial intelligence. Additional Key Words and Phrases: Generative Exploded Dynamics, PartLevel 3D Generation, 3D Asset Generation"
        },
        {
            "title": "INTRODUCTION",
            "content": "Three-dimensional (3D) creation begins with our innate ability to understand the world around us in terms of parts and how they fit together. As children, we learned this through playstacking stones to build majestic castles or dismantling toys like model cars and wind-up robots to explore their inner structures. Through deconstruction and recreation, we grasped the complexity of objects and experienced the joy of creation. This component-based 3D creation extends far beyond childhood and has profoundly influenced fields like computer graphics, industrial design, films, and games. However, current 3D creation tools often fail to mimic this natural ability to break down and reassemble objects. The process of decomposing and adjusting parts requires substantial artistic expertise and tedious manual effort. An ideal tool should integrate both understanding and generation at the component level, effortlessly transforming our innate creativity into tangible, interactive 3D objects. Recent progress in Generative AI and large models has highlighted the immense potential of bridging the generation and understanding capabilities. In the 2D image modality, tools like DALL-E 3 [OpenAI 2023], leveraging advancements in large language models like the GPT-4 family [Achiam et al. 2023], showcase the potential of combining generative and reasoning capabilities to transform text into compelling visuals. In the text modality, the huge breakthrough of the next-token prediction in large-language models (LLMs) has exemplified the principle that successful way to understand is through generation. However, unlike the image/text modalities, the 3D domain, especially for object-level content, has taken distinct developmental path. This divergence lies in the subtle disconnection between 3D generation and reasoning. Over the past two years, 3D generation has made remarkable progress, evolving from early distillation techniques [Poole et al. 2023], to multi-view methods [Long et al. 2024; Shi et al. 2024] and more recently to 3D native ones [Xiang et al. 2024; Zhang et al. 2023a]. Yet, current mainstream approaches predominantly focus on generating entire objects in one piece, lacking the component-based capability for flexible manipulation and detailed design. On the other hand, 3D understanding has advanced in component-level analysis. For instance, some methods [Yang et al. 2024; Zhou et al. 2025] provide instance-level part segmentation, while others [Qi et al. 2025a; Xu et al. 2025] integrate 3D features with LLMs to enable dialoguedriven reasoning. However, they often focus on the visible outer surface, neglecting the occluded internal structure, and hence struggle to establish spatial and semantic interconnections within the 3D object. In nutshell, more natural approach is needed to bridge 3D generation and reasoningone that mirrors how we intuitively understand and create by dividing and assembling objects as children, aligning with the idea that understanding is achieved through generation. Inspired by the Big Bang Theory, where singularity bursts into stars, planets, and life, we introduce BANGa generative approach that dynamically divides complex 3D assets into interpretable parts through smooth, consistent exploding process. Much like how the universe transitioned from unified state to dispersed one, BANG allows 3D objects to be divided and reassembled in way that preserves both structure and coherence. Just as children naturally learn by taking apart and reassembling their toys, BANG deconstructs in generation and reconstructs in understanding. BANG allows for high-quality 3D decomposition, generation, and enhancement while seamlessly integrating part-level analysis, bridging our 3D concept imagination into digital creation. The core of BANG lies in novel design called Generative Exploded Dynamics, which transforms an input geometry into continuous sequence of exploded states through smooth radial explosion process. Each intermediate state is represented as single mesh, where constituent parts progressively separate while preserving semantic and geometric consistency. It culminates in fully divided state, akin to the exploded view commonly used for asset visualization. Unlike static surface segmentation, generating exploded dynamics progressively separates parts over time, enabling the model to uncover latent volumetric structures and internal boundaries. This dynamic separation process naturally captures geometric and semantic dependencies that are otherwise difficult to infer. To achieve this, we adopt diffusion-based generative model with the pretrain-then-adaptation paradigm. We first pre-train large-scale latent diffusion model on static 3D geometry with neural field representation based on 3DShape2VecSet [Zhang et al. 2023a], leveraging high-quality geometry priors. Then, we fine-tune the base model for exploded dynamics, using carefully designed dataset with rich part-level assembly structures. Specifically, we propose light-weight exploded view adapter to condition the base model on input geometry and timestamps, enabling precise and smooth decomposition. We also adopt temporal attention module to enhance smooth transitions and maintain semantic and geometric consistency across timestamps. Beyond generating divided parts, we further utilize part-aware trajectory tracking compatible with the neural field representation. It associates the components back to the original mesh for accurate reassembly and preserves part semantics and spatial coherence. Achieving control over object decomposition is crucial for innovative and efficient 3D creative workflows. To enhance controllability, we further explore two kinds of cross-attention-based spatial prompts for BANG: bounding boxes and surface regions. Bounding boxes can specify volumetric regions even for geometries without internal structures, while surface regions enable precisely isolating and manipulating detailed areas on the objects surface. Additionally, BANG inherently preserves geometric and spatial semantics. Thus, we decode and align its 3D features with 2D feature extractors and collaborate with multimodal models (e.g., DINOv2 [Oquab et al. 2024] and Florence-2 [Xiao et al. 2024]). This enables intuitive 2D-to-3D interactions where one can specify object regions directly on 2D rendered views or sketches for controllable generation. The strength of BANG lies in its ability to transform complex 3D assets into detailed, interpretable parts. BANG allows users to generate, decompose, and reassemble objects from simple text or image inputs, enhancing geometric details at the part level. Integrated with large multi-modal models, BANG enables interactive dialogues for part-level 3D analysis and creation, while also supporting 3D printing and assembly with an engaging, hands-on creation experience. Through BANG, the process of creating and understanding 3D objects becomes as intuitive and joyful as assembling puzzle, piece by piece. As Feynman once said, What cannot create, do not understand. BANG brings this idea to life, turning imagination into reality."
        },
        {
            "title": "2 RELATED WORK\n2.1",
            "content": "3D Structural Understanding Understanding the intricate structure of 3D objects and providing functional and semantic analysis of their constituent parts facilitates advanced operations of 3D assets. Here we primarily review the methods for part-level semantic segmentation and those integrating large models for dialogue-driven reasoning. Part Segmentation. Current approaches for 3D part segmentation largely focus on exploring network architectures for point cloud or mesh of outer surface [Guo et al. 2015; Li et al. 2018; Ma et al. 2022; Qi et al. 2017a,b; Qian et al. 2022; Wu et al. 2024a, 2022; Xu et al. 2017; Zhao et al. 2021]. They heavily rely on labeled datasets such as PartNet [Mo et al. 2019b], which, while valuable, are limited in size and scope, often encompassing specific categories like furniture. To enhance generalization ability, recent zero-shot and open-vocabulary approaches [Abdelreheem et al. 2023; Cen et al. 2023; Jatavallabhula et al. 2023; Liu et al. 2024a, 2023b; Takmaz et al. 2023; Tang et al. 2024c; Thai et al. 2025; Umam et al. 2024; Yang et al. 2024, 2023; Zhang et al. 2022a; Zhong et al. 2024; Zhou et al. 2023; Zhu et al. 2023] leverage pretrained large-scale vision models, i.e., CLIP [Radford et al. 2021], DINO [Caron et al. 2021; Oquab et al. 2024], GLIP [Li et al. 2022b; Zhang et al. 2022b], and SAM [Kirillov et al. 2023; Ravi et al. 2024]. They render 3D objects into 2D images to apply these vision models, hence inherently limiting segmentation to visible surfaces and ignoring the internal components. Multi-modality Analysis. Recent methods focus on multi-modality analysis of 3D objects. They have led to the development of scalable 3D encoders that align 3D features with those from text and image encoders. These encoders facilitate range of tasks, i.e., 3D feature extraction [Liu et al. 2024d; Xue et al. 2023, 2024; Zhang et al. 2023b; Zhou et al. 2024] and descriptive question and answer (Q&A) systems combined with LLMs [Fei et al. 2024; Hong et al. 2023; Ma et al. 2024; Qi et al. 2025a, 2024, 2025b; Tang et al. 2024a; Xu et al. 2025; Yin et al. 2023a]. These models, trained on extensive datasets such as Objaverse [Deitke et al. 2023], offer comprehensive understanding of 3D objects by capturing both their geometric features and semantic attributes. However, they focus on surface geometry, overlooking the essential aspect of internal volumetric structural understanding. Differently, our BANG approach effectively displaces parts and models interior components through generative exploded dynamics, BANG: Dividing 3D Assets via Generative Exploded Dynamics 3 surpassing surface-level methods for purely 3D understanding. Our isolated parts improve generative mesh quality and semantic consistency for precise manipulation. It can serve as plausible precursor for 3D segmenting anything from outer to inner and is compatible with LLMs such as GPT-4 family to facilitate component-level descriptive and query capabilities. 2.2 3D Generation Here, we systematically review recent progress in 3D object generation, including those generating entire objects as whole through 2D lifting or using 3D native representation, as well as those focusing on part-aware generation. 2D Lifting. Pioneering works like DreamFusion [Poole et al. 2023] introduce Score Distillation Sampling (SDS) and optimize underlying geometric representations using 2D diffusion priors, while Zero-1to-3 [Liu et al. 2023a] generates multi-view images from single image input. Building upon them, significant volume of subsequent research has explored this 2D-to-3D lifting paradigm [Chen et al. 2024e; Gu et al. 2023; Huang et al. 2023; Lin et al. 2023; Liu et al. 2024e; Melas-Kyriazi et al. 2023; Qian et al. 2024; Raj et al. 2023; Tang et al. 2023; Wang et al. 2023a, 2024; Watson et al. 2023; Xiang et al. 2023; Xu et al. 2023; Yi et al. 2024]. key direction is to improve multi-view consistency, achieving more coherent and accurate 3D reconstruction [Chan et al. 2023; Chen et al. 2024f; Gao et al. 2024; Li et al. 2024c; Liu et al. 2024c,b; Long et al. 2024; Qiu et al. 2024; Shi et al. 2023, 2024; Tang et al. 2025]. Besides, researchers have also explored the creation of composite objects and entire scenes [Chen et al. 2025; Cohen-Bar et al. 2023; Epstein et al. 2024; Han et al. 2024; Li et al. 2024a; Po and Wetzstein 2024; Vilesov et al. 2023; Wang et al. 2023b; Yan et al. 2024a,b]. These methods usually leverage the understanding of object arrangements embedded in the image-based generative models and employ differentiable rendering to optimize the placements of individual objects within scene. 3D Native Generation. Another direction involves training the generative models directly using extensive 3D data of diverse shapes and styles. Exemplified by 3DShape2VecSet [Zhang et al. 2023a], CLAY [Zhang et al. 2024b], and TRELLIS [Xiang et al. 2024], these 3D native methods produces impressive geometry and appearance [Deng et al. 2024; Jun and Nichol 2023; Li et al. 2024b; Nichol et al. 2022; Ren et al. 2024; Wu et al. 2024b; Zheng et al. 2023]. Further explorations, exemplified by PolyGen [Nash et al. 2020], MeshGPT [Siddiqui et al. 2024], and Meshtron [Hao et al. 2024], adopt an autoregressive generation approach for mesh faces [Chen et al. 2024a,b,d; Tang et al. 2024b; Weng et al. 2025, 2024]. Besides, related area of research focuses on 3D CAD generation, which rely on structured CAD representations with explicit awareness of components and primitives [Alam and Ahmed 2024; Badagabettu et al. 2024; Dupont et al. 2025; Khan et al. 2024; Li et al. 2022a; Uy et al. 2022; Xu et al. 2024; You et al. 2024]. Part-aware Generation. While the above generative models excel at producing unified meshes, their lack of explicit part separation limits component-level editing and interaction. Part-level generation addresses this limitation which requires not only the creation of individual parts but also their coherent assembly into complete 4 Longwen Zhang, Qixuan Zhang, Haoran Jiang, Yinuo Bai, Wei Yang, Lan Xu, and Jingyi Yu Fig. 2. The overview illustrates the proposed framework for Generative Exploded Dynamics. The pipeline consists of four stages: Exploded Data Synthesis generates the training data (Section. 3.2). Exploded Dynamics Generator produces the exploded dynamics based on the input geometry (Section. 3.1). Trajectory Optimization refines the trajectories of the exploded parts, ensuring smooth reassembly of the components (Section. 3.3). Finally, Controllable Generation allows users to interactively control and refine the explosion by conditions (Section. 4). objects. Early studies [Gao et al. 2019; Hertz et al. 2022; Mo et al. 2019a; Petrov et al. 2023; Wu et al. 2020, 2019] focused on encoding and decoding part geometries and positions, laying the groundwork for part-aware generation. Subsequently, approaches [Koo et al. 2023; Nakayama et al. 2023] have demonstrated the potential for fine-grained part generation using diffusion models and part-specific latent representations, yet within relatively smaller and specialized datasets such as ShapeNet [Chang et al. 2015] and PartNet [Mo et al. 2019b]. The recent PartGen [Chen et al. 2024c] handles occlusion through two-stage process: first, producing artist-inspired part segmentation through multi-view synthesis, followed by generating the detailed 3D shapes for each part. In stark contrast, our BANG approach natively decomposes objects into meaningful parts and ensures their coherent reassembly through innovative exploded dynamics. Unlike prior methods that rely on multi-view segmentations or two-stage processes, it inherently encodes structural understanding within unified large-scale generative paradigm, offering flexibility and precision for both creation and downstream applications. 2.3 4D Generation and Exploded View Our BANG produces special dynamic sequence of exploded 3D geometries where constituent parts of the original mesh progressively separate. Hence, it partially shares common insights with those approaches about generating dynamic 4D objects and traditional exploded views. Specifically, recent efforts generate dynamic objects or scenes using NeRF or Gaussian representations [Bahmani et al. 2024; Jiang et al. 2024; Liang et al. 2024; Pan et al. 2024; Rahamim et al. 2024; Ren et al. 2023; Singer et al. 2023; Yin et al. 2023b; Zeng et al. 2025; Zhao et al. 2023]. Some of them have adapted 3D generative models to handle 4D temporal sequences [Cao et al. 2024; Erkoç et al. 2023; Zhang et al. 2024a] using similar strategies in BANG, i.e., temporal attention. On the other hand, traditional exploded views separate the components of 3D object to expose its internal structure, providing an intuitive way to perceive complex 3D architectures. Existing work on exploded view generation has predominantly concentrated on 2D representations [Bruckner and Groller 2006; Karpenko et al. 2010; Li et al. 2008, 2004; Shao et al. 2021]. Exploded views in 3D have been largely overlooked despite their intuitive appeal. Differently, our approach introduces native method for generative exploded dynamics that integrates 3D part-level decomposition within large-scale generative framework. This not only offers novel approach for part-aware 3D generation but also open up new possibilities for 3D creation workflows."
        },
        {
            "title": "3 GENERATIVE EXPLODED DYNAMICS",
            "content": "Differently, our BANG approach dynamically divides complex 3D assets into interpretable part-level structures, to deconstruct in generation and reconstruct in understanding. As illustrated in Fig. 2, the core of BANG is novel design called Generative Exploded Dynamics. Within conditioning-generative paradigm, it simulates smooth and radial explosion process, transitioning complete and assembled geometry into its constituent parts. Crucially, each intermediate exploded state preserves part-level geometric and semantic consistency, ensuring sequence of meaningful decomposition. As result, our framework encapsulates sophisticated structural insights to facilitate both the fidelity and controllability of 3D geometry generation and analysis. For clarity of exposition, we first detail the architecture design and training strategy, explaining how geometry is encoded, interpreted, and ultimately decomposed into continuous exploded dynamics (Sec. 3.1). We then describe our data preprocessing to facilitate robust training (Sec. 3.2). Finally, we introduce our post-generation trajectory tracking procedure (Sec. 3.3), which is applied to geometry sequences generated by our model, ensuring stable part-wise transitions, semantic consistency, and accurate reassembly. BANG: Dividing 3D Assets via Generative Exploded Dynamics 5 base model consists of geometry variational autoencoder (VAE) and latent diffusion model (LDM). To encode 3D geometry, we first sample point cloud 𝑿 from the surface of the input mesh M. 𝑿 is then transformed into latent representation 𝒁 R𝐿𝐶 by transformer-based VAE encoder: 𝒁 = (𝑿 ) = CrossAttn(PosEmb( 𝑿 ), PosEmb(𝑿 )), (1) where 𝑿 denotes down-sampled version of 𝑿 , 𝐿 is the number of points in 𝑿 and 𝐶 is the channel dimension. Next, we apply diffusion transformer (DiT) model 𝜖 (𝒁 + 𝜖𝜏 , 𝜏) to learn to denoise the noisy latent 𝒁 + 𝜖𝜏 . Finally, the VAE decoder processes these latent codes and list of query points 𝒑 in space, outputting SDF values: (𝒁, 𝒑) = CrossAttn(PosEmb(𝒑), SelfAttn24 (𝒁 )). (2) We adopt the pretraining scheme [Zhang et al. 2023a, 2024b] to train both the VAE and the LDM on the Objaverse dataset [Deitke et al. 2023]. Additionally, we enhance the pretrained model by incorporating text, image, and point cloud conditioning schemes (implementation details are provided in Sec. 6.1). Our pretrained base model establishes robust geometry prior and can generate diverse 3D geometries from diverse inputs like text prompts and images. Exploded View Adapter. We aim to adapt the above pretrained model to generate sequence of geometries, M𝑡 , 𝑡 {𝑡1, . . . , 𝑡𝑇 }, from an arbitrary 3D geometry M. These M𝑡 provide unique perspective of the original by smoothly and radically exploding its constituent parts, akin to the exploded view commonly used for asset visualization. Specifically, we freeze the pretrained base model, then inject conditional signals derived from and time {𝑡 } into it. This design minimizes data requirements by restricting training to lightweight adapter while retaining the strong geometric priors encoded in the base model. The adapter begins by encoding into unordered feature representations. Following the structure of the VAE encoder E, we uniformly sample point cloud 𝑺 R𝑁 3 from the surface of the input mesh M. This sampled point cloud is then embedded and processed through cross-attention encoding module, mirroring the encoding pipeline of E, as follows: 𝑮 = CrossAttn(PosEmb( 𝑺), PosEmb(𝑺)), (3) where 𝑺 denotes down-sampled version of 𝑺 via farthest-point sampling (FPS). In our implementation, 𝑁 is set to 20480 with down-sampling factor of 10. The resulting geometry feature 𝑮 is then passed through lightweight transformer equipped with adaptive Layer Normalization (adaLN) to incorporate the time condition 𝑡 and the expected parts count. This process produces the conditioning feature 𝑮explode, which is fed into the diffusion backbone. Finally, 𝑮explode is integrated into the main DiT backbone through parallel cross-attention layers. Specifically, the input of each crossattention layer in the DiT backbone is cross-attended with 𝑮explode, and the resulting features are added back to the output of the corresponding DiT cross-attention layer. This mechanism ensures that the conditioning information from the exploded view adapter seamlessly guides the generative process. The adapter module is trained to align with the target exploded dynamics as follows: 𝜖 (𝒁𝑡 + 𝜖𝜏 , 𝜏, 𝑮explode) 𝒁𝑡 (4) Fig. 3. The architecture of the base generative model and adaptation modules in our Generative Exploded Dynamics framework. The gray blocks represent the pretrained base model, which is transformer-based latent diffusion model, and remains frozen after pretraining. The red blocks include the exploded view adapter and temporal attention module, which are learnable during the exploded dynamics training phase. During inference, input geometry, along with target time sequence {𝑡 }, is fed into the exploded view adapter. Temporal attention ensures that the entire diffusion model outputs continuous, smoothed geometry sequence in one pass."
        },
        {
            "title": "3.1 Exploded Dynamics Generation Model",
            "content": "We adopt diffusion-based generative model to produce series of meshes from an assembled state to smoothly exploded configuration. Formally, given an input geometry and time series 𝑡 {𝑡1, . . . , 𝑡𝑇 } as conditions, it generates the corresponding watertight mesh sequence {M𝑡 }. In {M𝑡 }, all the constituent parts in the original mesh are naturally and continuously transited from fully assembled state (𝑡 = 0) to completely divided state (𝑡 = 1). As shown in Fig. 3, we adopt pretrain-then-adaptation scheme. We first pretrain large-scale 3D generative model for high-quality and static geometry modeling similar to previous methods [Zhang et al. 2023a, 2024b]. Next, we fine-tune the large model into our exploded setting using part-specific exploded-view dataset. Specifically, to achieve precise and smooth part-level decomposition, we propose an Exploded View Adapter that conditions the model on input geometry and various timestamps. Additionally, we adopt Temporal Attention Module to ensure smooth and coherent part transition across the exploded process. These designs collectively enhance the ability to generate part-aware dynamics with high fidelity. 3D Generative Model pretraining. Similar to prior works leveraging 3DShape2VecSet representation [Zhang et al. 2023a, 2024b], our 6 Longwen Zhang, Qixuan Zhang, Haoran Jiang, Yinuo Bai, Wei Yang, Lan Xu, and Jingyi Yu Fig. 4. Example data from our dataset illustrating synthetic exploded dynamics. The images show cannon (top), spaceship (middle), and valve (bottom) transitioning from 𝑡 = 0 (left) to 𝑡 = 1 (right), highlighting the decomposition of each object over time. where 𝒁𝑡 denotes the exploded view latent code at time 𝑡, encoded from M𝑡 as 𝒁𝑡 = (M𝑡 ), and 𝜖𝜏 represents Gaussian noise at noise step 𝜏. As illustrated in Fig. 3, this modular design ensures that the adapter can be trained independently to interpret and 𝑡 without altering the pretrained diffusion parameters. This approach simplifies the overall pipeline while preserving the broad shape priors learned from large-scale data. Once trained, the adapter directs the model to generate exploded states from the input geometry at target time, establishing foundation for the subsequent multi-frame or time-sequence generation. Temporal Attention for Smooth Exploded Sequence Generation. To ensure smooth transitions between exploded states, we adopt temporal attention mechanism across the exploded process. This mechanism facilitates continuity by modeling dependencies between consecutive frames, inspired by recent video diffusion models. With the fine-tuned exploded view adapter, we extend our approach to generate smooth exploded dynamic sequence of 𝑇 frames. To share contextual information across frames, we integrate temporal attention mechanism within each DiT block. During the training of the temporal attention module, batch of full-length exploded dynamics sequences {𝒁𝑡 }, 𝑡 {𝑡1, . . . , 𝑡𝑇 } is fed into the DiT model. To enable the temporal attention module to distinguish time progression, we introduce frame-wise time embedding, TimeEmb(𝑡), where tokens corresponding to the same frame index share the same embedding. This embedding is defined as: TempAttn = SelfAttn(𝒁𝑡1 TimeEmb(𝑡1), . . . , 𝒁𝑡𝑇 TimeEmb(𝑡𝑇 )), where denotes that the time embedding is only added to the query and key representations of the attention layer, similar to the Rotary Positional Embedding (RoPE) widely applied in large language models: 𝒒 𝒒 TimeEmb(𝑡), 𝒌 𝒌 TimeEmb(𝑡). (5) To train the temporal attention module, we merge the token and frame dimensions into single dimension prior to feeding the data into the temporal attention module, forming contiguous set of 𝑇 𝐿 tokens for each instance in the batch. This transformation allows the multi-head self-attention operation to be applied across Fig. 5. Histograms illustrating the distribution of key geometric characteristics within the data used to train exploded dynamics. The plots show the distribution of: (top left) the number of parts comprising each 3D asset; (top right) the asset bounding box expansion ratio after explosion optimization, which is calculated as the maximum dimension of the bounding box after explosion divided by the maximum dimension of the bounding box at the assembled state; (bottom left) the ratio between the minimum and maximum volumes of the parts, in logarithmic scale; and (bottom right) the initial overlapping volume between parts in the assembled state, in logarithmic scale. all 𝑇 𝐿 tokens, enabling the model to establish both intra-frame consistency and inter-frame transitions by allowing tokens to attend across frames. After the temporal attention operation, the tokens are reshaped back to separate the frame dimension, and the frame dimension is then merged into the batch dimension for frame-wise generation. This design ensures that the temporal attention module captures global temporal context while maintaining the ability to generate each frame independently during subsequent stages. This design requires only the addition of lightweight layer for temporal coordination, which can be trained independently. It ensures seamless integration of temporal coherence into the generation process while preserving the flexibility and robustness of the underlying 3D generative model."
        },
        {
            "title": "3.2 Dataset Construction",
            "content": "Obtaining high-quality part-level mesh data is critical for training the generative exploded dynamics model. However, this task presents significant challenges, as most publicly available 3D assets were not designed or curated with explicit sub-component structures. Even within large repositories like Objaverse [Deitke et al. 2023], many assets are single-piece meshes, contain incomplete or poorly defined part geometries, or fail to meet quality or technical standards necessary for reliable training. To address these issues and ensure consistency in our training pipeline, we implement rigorous filtering process for 3D assets in Objaverse, prioritizing quality over quantity to curate robust and reliable dataset. Data Filtering. For assets in Objaverse, we begin by identifying assets with component count between 2 and 30. We exclude meshes with extreme vertex counts (e.g., < 1e3 or > 1e6) and those containing skins intended for animation. To further ensure data quality, similar to previous work [Luo et al. 2024], we conduct thorough quality check using GPT-4 [Achiam et al. 2023] to identify and remove problematic meshes. Specifically, we render each 3D asset from multiple viewpoints and prompt GPT-4 to assess its suitability for training, filtering out scans, incomplete or unrecognizable objects, and complex scenes. This filtering process produces stable subset of meshes that balance geometric detail with computational feasibility. For the accepted meshes, GPT-4 is also used to annotate key geometric and semantic attributes, including symmetry, polygon density, and visual complexity. Explosion Vector Optimization. For each remaining mesh, we calculate the axis-aligned bounding boxes of its components and optimize translation vector for each component to simulate radial explosion outward. This optimization process aims to minimize collisions between bounding boxes while constraining excessive translations, ensuring the objects layout stays cohesive. Its terminated when the overlap between bounding boxes falls below predefined small threshold. This results in visually coherent radial explosion process. We then interpolate the translation vectors from 𝑡 = 0 (assembled) to 𝑡 = 1 (fully exploded), and sample intermediate time steps to form smooth sequence of exploded states. To ensure consistency and simplify downstream processing, these sequential meshes are re-centered or uniformly scaled so that their overall bounding box remains within standardized size. If the parts remain too close or the final exploded view becomes excessively large and unrealistic, we discard the corresponding data. Finally, We record the exploded sequence, the transformations, and all relevant metadata in our dataset to ensure reproducibility and adherence to consistent standards for training and evaluation. The mesh examples of our synthetic exploded dynamics are shown in Fig. 4. Exploded Dynamics Dataset. After rigorous filtering, we curate an exploded dynamics dataset containing approximately 20k highquality assets, with the corresponding statistics illustrated in Fig. 5. Although this final data set is relatively small compared to the original pool of millions of meshes, it offers precise and rich data that ensure high-quality training. Besides, as discussed in Sec 3.1, our method leverages large-scale pretrained generative model, reducing the need for dedicated dataset of exploded shapes for fine-tuning. This strategy combines the broad geometric knowledge from large-scale pretraining with the precise and unique part-level annotations in our final dataset. It enables robust part-aware generation while preserving the benefits of large-scale pretraining. In practice, this dataset is crucial for guiding the generative model in accurately decomposing parts and transitioning smoothly between assembled and exploded states. By prioritizing consistency and correctness, we reduce the noise that could hinder the convergence of our exploded dynamics generation. BANG: Dividing 3D Assets via Generative Exploded Dynamics 7 Fig. 6. Visualization of the SDF-based part trajectory tracking process. The figure illustrates this process for two example generated objects, with the cross-section SDF at 𝑡 = 0 shown as the background. Each row, from left to right, represents: (1) the assembled and exploded geometry; (2) the parts at 𝑡 = 1, indicating the fully exploded state; (3) an intermediate state at 𝑡 = 0.5, showing the parts moving along their trajectories; and (4) the final assembled result at 𝑡 = 0. Red arrows denote the optimized trajectories, and part centers are highlighted with red circles. Parts far from the cross-section plane are omitted for clarity."
        },
        {
            "title": "3.3 Part Trajectory Tracking",
            "content": "Given generated exploded dynamics sequence, each state is represented as single mesh composed of multiple disconnected components. To enable accurate part-level understanding, we must establish consistent correspondences between parts in the fully exploded state and their counterparts in the original geometry. This tracking process is applied after generation and allows us to follow individual components throughout the explosion sequence, preserving semantic meaning and geometric consistency across frames. This not only enhances structural understanding but also unlocks versatile editing capabilities and seamless integration with downstream applications. SDF-based Trajectory Optimization. Our approach adopts the SDF representation for geometry generation, and hence accommodates companion SDF-based part tracking scheme. This volumetric perspective ensures that each parts position can be optimized to align with its designated region in the final shape, accommodating intentional intersections where necessary. Specifically, given generated exploded dynamics {M𝑡 =0, . . . , M𝑡 =1}, we identify all individual parts {𝑷𝑖 } from the fully exploded state M𝑡 =1 through connected component analysis. To formalize how each part 𝑷𝑖 moves from its position 𝒑1 𝑖 in M𝑡 =1 (exploded state) back to 𝒑0 𝑖 in M𝑡 =0 (assembled state), we format linear parametrization of translation 𝒑𝑡 𝑖 + 𝒗𝑖 (1 𝑡), where 𝒗𝑖 is the translation vector of 𝑷𝑖 as 𝑡 goes 𝑖 = 𝒑0 from 1 to 0. Notice this parametrization is valid as our training data assumes linear translation of structural parts. Here, our target is to optimize per-part translation vector 𝒗𝑖 . Notably, the SDF values serve as natural metric for evaluating the fitness of each part, since well-aligned parts exhibit SDF values near zero at the boundaries. Thus, we randomly sample surface point cloud 𝑷𝑖 from 𝑷𝑖 and minimize the absolute SDF value on the motion path of the point 8 Longwen Zhang, Qixuan Zhang, Haoran Jiang, Yinuo Bai, Wei Yang, Lan Xu, and Jingyi Yu cloud across frames. The optimization of 𝒗𝑖 is formulated as: {𝒗𝑖 } arg min 𝑡 𝑖 (cid:12)QuerySDF(M𝑡 , 𝑷𝑖 + 𝒗𝑖 (1 𝑡))(cid:12) (cid:12) (cid:12), (6) where QuerySDF(M𝑡 , ) represents querying SDF values from the corresponding 3D points of the watertight mesh M𝑡 . Fig. 6 illustrates examples of optimized trajectory with target SDF. Stop Overlapped Point Gradients. The SDF-guided optimization works effectively when there is no overlap between parts. However, when two parts 𝑷𝑖 and 𝑷𝑗 overlap, the surface points within their intersection region (identified by negative SDF values) become invalid for optimization. Only the frontier points on the actual boundaries provide meaningful gradient signals. Hence, we mask out any surface points located inside another part (i.e., those with negative SDF values) during the loss computation, focusing the optimization on the boundaries. This results in better tracking accuracy, which will be evaluated in Sec. 6.4. The optimized translation vectors of each part ensure plausible reassembly or disassembly path, so we can generate intuitive exploded and reassembled trajectories that maintain the structural coherence of the original asset."
        },
        {
            "title": "4 CONTROLLABLE GENERATION",
            "content": "In creative workflows, artists often require high degree of control of how an object decomposes. To meet this need, we introduce two complementary schemes to enable our generative explode dynamics for controllable generation via spatial prompts, including bounding boxes and surface regions. Moreover, our generative decomposition inherently preserves geometric semantics and spatial relationships in the exploded dynamics dataset. We hence integrate our framework with 2D feature extractors (e.g., DINOv2 [Oquab et al. 2024]) and multi-modal models (e.g., GPT-4 family [Achiam et al. 2023]) for further intuitive and seamless creation."
        },
        {
            "title": "4.1 Spatial Control",
            "content": "We provide two kinds of natural conditioning prompts for spatial control in our generative exploded dynamics, i.e., 3D bounding boxes and surface regions on the mesh. Bounding boxes allow users to specify volumetric region, even if the original geometry lacks an internal structure (for instance, if table with drawer is only modeled externally, which will be discussed in Sec. 6.2). Surface regions, on the other hand, provide more precision for cases in which an artist wants to isolate finely detailed area on the objects surface. To support the spatial condition, we use the positions of points as prompts, i.e. the diagonal corners of bounding box and sampled points for surface regions, and extend the exploded view adapter by incorporating dedicated transformer branch to handle the spatial prompts. More specifically, we create new transformer blocks to process the spatial prompts. We apply positional embedding PosEmb() to the bounding box corners as tokens, and use point encoder (as in Eqn. 3) to encode the surface point-cloud into tokens. To differentiate between multiple spatial prompts, we add unique positional embeddings. The encoded tokens are then integrated with the geometry features 𝑮 through interleaved cross-attention Fig. 7. The architecture of the extended exploded view adapter for processing spatial conditions. The conditions, provided as either 3D bounding box or selected surface region on the input geometry, is first encoded into tokens. These tokens are then fed into the Prompt Transformer Blocks, which exchange information with the Transformer Blocks of the exploded view adapter. This enables the system to dynamically adjust the exploded view generation process based on the user-defined constraints. mechanisms, allowing the model to effectively interpret and utilize the spatial guidance provided by the user, as shown in Fig. 7. During training, both bounding boxes and surface regions are randomly selected from the training data, with varying numbers of prompts per instance to enhance the models flexibility. Additionally, an auxiliary binary token is included to indicate whether the bounding boxes correspond to all parts to be generated, or the unselected regions should still be exploded automatically. This training strategy ensures that the model can handle an arbitrary number of spatial prompts during inference, providing users with the ability to control the decomposition process according to their specific requirements. As illustrated in Fig. 8, users can seamlessly guide the generation by specifying spatial regions, resulting in controlled and intuitive exploded views tailored to their creative intentions. With the spatial conditioning scheme, our exploded generative framework empowers users with the ability to decide how and where an explosion conducts. For example, artist can maintain creative oversight by selecting only the wheels of wooden horse for separation, or merging all body parts into single chunk. This approach drastically expands the potential use cases and fosters the precise control that designers, hobbyists, and other practitioners often need in real-world scenarios."
        },
        {
            "title": "4.2 Cross-modal Creative Framework",
            "content": "While bounding-box and surface-region prompts address fundamental controllability requirement, practical workflows often demand BANG: Dividing 3D Assets via Generative Exploded Dynamics 9 Fig. 8. Effect of spatial prompt control for exploded view generation. The figure illustrates two distinct examples: sofa (left) and wooden horse toy (right). The first row displays the generated exploded views without any spatial prompt input, serving as the baseline. The middle and bottom rows show the effects of varying spatial prompt settings, including different surface regions and varying bounding boxes. These prompts enable users to selectively control which parts to exploded, demonstrating the controllability and flexibility of our method. more accessible interaction approach. Common users are more comfortable with indicating regions in 2D domain than manipulating 3D space. Fortunately, our generative model naturally preserves geometric semantics and spatial relationships distilled from largesale of 3D data, which can be effectively aligned with 2D feature extractors. To this end, we can match the specified region rendered in 2D image to its corresponding location on the 3D mesh, allowing user to select part of the object from its rendered view or even sketch image. Geometry and 2D Feature Alignment. To align 3D geometry with 2D image feature, we re-purpose VAE decoder to produce geometric features aligned with DINOv2 [Oquab et al. 2024] rather than produce SDF values. Formally, for geometry latent code 𝒁 and point cloud 𝒑 on the corresponding mesh surface, we compute: Dfeature (𝒁, 𝒑) = CrossAttn(PosEmb(𝒑), SelfAttn24 (𝒁 )), (7) which result in feature vectors at surface points 𝒑. For training Dfeature, we render the corresponding 3D asset into color image 𝑰RGB and depth map 𝑰depth, extract the DINOv2 features of 𝑰RGB, and un-project 𝑰depth back to 3D points 𝒑depth. We compute the features of 𝒑depth using Dfeature, match them with the DINOv2 features: Lalign = (cid:13) (cid:13)Dfeature (𝒁, 𝒑depth) DINOv2(𝑰RGB)(cid:13) (cid:13)2 . (8) Fig. 9. 2D-3D semantic correspondence is established by aligning features from 2D image (top left) and its 3D geometry (top right) using geometric feature extractor and DINOv2. The bottom row demonstrates how selected regions of interest (ROIs) in the image corresponds to regions on the 3D geometry. Once trained, we can use the rendered images or concept images to specify semantic regions of interest (e.g., via segmentation tool like SAM2 [Ravi et al. 2024]), and identify the corresponding 3D regions through feature similarity of sampled surface points with the DINOv2 features in the 2D region. This involves selecting 3D features within certain distance threshold of any 2D ROI features. As illustrated in Fig. 9, this bridging mechanism enables guidance through intuitive 2D annotations, and hence lowers the usage barrier of controllable exploded dynamics generation. Building upon the geometry feature alignment strategy, our system can be easily integrated with broader generative pipelines, i.e., large multi-modal generative model that synthesizes 3D objects from text or images. For example, creator can first produce virtual asset of chair using an image promptleveraging highlevel attributes such as style, color, or shape, and then feed this newly generated 3D mesh into our generative exploded dynamics. At that point, additional bounding boxes, surface regions, or 2D region-of-interest selections can specify precisely which parts of the chair explode or how the explosion proceeds. This creates an end-to-end workflow where novel 3D objects are designed and interactively decomposed, seamlessly bridging initial concept generation with precise part-level manipulation. Collectively, these strategies highlight the value of controllability in exploded dynamics generation. By enabling user interaction through bounding-box prompts, surface regions, or 2D region selections, our approach seamlessly integrates advanced part-level decomposition into existing creative workflows, enabling intuitive and efficient exploration of design arts, manufacturing, or educational tasks."
        },
        {
            "title": "5 APPLICATIONS",
            "content": "The unique capacity of BANG to transform complex 3D assets into detailed, interpretable parts benefits various fields, from industrial design to virtual reality and digital art. By enabling granular partlevel control, smooth temporal transitions, and implicit structural 10 Longwen Zhang, Qixuan Zhang, Haoran Jiang, Yinuo Bai, Wei Yang, Lan Xu, and Jingyi Yu Fig. 10. Enhanced geometry quality through per-part enhancement. We start with an input image of robotic dinosaur from TRELLIS [Xiang et al. 2024] (bottom left). We generate 3D geometry (center left) and explode it into parts (top left). Each part is then regenerated based on its coarse geometry, enhancing its detail (top right). Finally, the enhanced parts are reassembled, resulting in more detailed and accurate 3D geometry (center right) that closely matches the input image. awareness, BANG with generative exploded dynamics empowers users to efficiently manipulate and interpret complex 3D assets. In the following sections, we illustrate three key applications of BANG, showcasing its impact on component-driven 3D creation, understanding, and manufacturing workflows. It can significantly streamline creations, and enhance collaborative design and immersive experiences, highlighting its huge potential to drive long-term innovation in 3D creation and interaction."
        },
        {
            "title": "5.1 Per-part Geometric Detail Enhancement",
            "content": "Our framework facilitates full cycle of part disassembly, per-part refinement, and subsequent reassembly for geometric detail enhancement. The reliance on signed distance function (SDF) representation within normalized space of [1, 1]3 introduces challenges in simultaneously modeling the entire structure and capturing intricate surface details. By isolating each component in its exploded state, our method re-scales individual parts to the normalized space and reconditions them based on coarse geometry and corresponding image regions, thus enabling high-fidelity local refinements. During this refinement stage, defects can be corrected while local geometries are enhanced, and each part is then reassembled through Fig. 11. Interactive exploded views via chatbot integration: Our framework combines object understanding and generative explosion through interactive dialogue with Chatbot. We showcase two interaction paradigms: Exploded then Understanding (left), where an automatic explosion generates functional descriptions, and Understanding then Explosion (right), where user queries guide the decomposition of specific parts. the trajectory optimization process in Sec 3.3. As result, the regenerated components align seamlessly with the original global structure, producing multi-part object that preserves fine-grained detail across all regions. We illustrate this process in Fig. 10. Given an input geometry generated by our base model, our approach first explodes it into individual parts. Each part is then scaled into the normalized space and regenerated based on its coarse geometry and corresponding image regions, producing highly detailed geometry. Finally, the enhanced parts are re-assembled into their original position. This approach achieves higher level of detail compared to those singlemesh pipelines that generate the entire geometry as whole, and further facilitates artist-friendly topologies and enables part-specific animations. By focusing on part-level regeneration, our method enhances both the visual quality and functional versatility of 3D assets, surpassing previous methods that are limited by resolution and single-mesh representations. BANG: Dividing 3D Assets via Generative Exploded Dynamics 11 parts to isolate or modify, enabling more precise and targeted manipulations. This interaction is facilitated by MLLMs generating text-based instructions, which are then used in combination with models like Florence-2 [Xiao et al. 2024] for 2D region-of-interest (ROI) selection, SAM2 [Ravi et al. 2024] for segmentation, and DINOv2 [Oquab et al. 2024] with our geometric feature extractor (Sec. 4.2) to map these selections accurately to the 3D geometry. Spatial prompts are then applied for controllable generation based on these selections. This multi-modal integration enriches the semantic annotations of 3D objects, providing users with intuitive, flexible control over part-level manipulations. By linking textual and visual understanding to 3D geometry, our framework opens new possibilities for creative and industrial workflows, enhancing design, analysis, and modeling processes. This fusion of generative and semantic reasoning not only streamlines the development cycle but also fosters more dynamic and collaborative environments, pushing the boundaries of interactive 3D modeling and intelligent system integration."
        },
        {
            "title": "5.3 Expedite Combinable Structure 3D Printing",
            "content": "3D physical prototyping of combinable structures is widely used in industrial design, customizable product development, robotics, etc. It necessitates the segmentation of designs into print-friendly components while ensuring that the final assembly maintains consistency and functionality. BANG inherently supports this requirement by generating part-level meshes with clear separations and precise alignments. These exploded parts can be individually 3D printed, allowing for optimized orientations and tailored material choices for each component. This workflow effectively reduces the need for support materials, mitigates overhang-related printing challenges, and provides flexibility in selecting distinct materials or colors for different parts. Furthermore, our framework enables the integration of movable joints between components, facilitating dynamic assemblies that can articulate or adjust post-printing. By incorporating such joints, the printed object not only adheres to the intended static design but also gains mechanical versatility, allowing for interactive or functional customization. This enhancement leverages the inherent part structure to achieve both mechanical functionality and aesthetic diversity, thereby increasing the overall utility and appeal of the prototype. As illustrated in Fig. 12, using our approach, one can generate printable parts of complex toy robot from single image input, to enjoy hands-on assembly and creation."
        },
        {
            "title": "6 EXPERIMENTS\n6.1 Experimental Setup and Implementation Details",
            "content": "We train our BANG model in progressive manner, with the first step of pretraining our base generative model following the previous practice [Zhang et al. 2023a, 2024b]. Specifically, we train the model on an Objaverse subset [Deitke et al. 2023], containing 500, 000 diverse 3D geometries which are processed into water-tight format. The base model employs geometry variational autoencoder (VAE) to encode dense point clouds into latent representations of size 2048 64. The VAE encoder consists of 1 cross-attention layer, and the decoder has 24 self-attention layers with 1 cross-attention layer. Both the encoder and decoder use feature dimension of 512. Fig. 12. Expedite physical prototyping of combinable structures. Each part of robot, generated from cute robot design (featured in TRELLIS [Xiang et al. 2024]), is 3D printed using the X1 Carbon [Bambu Lab 2022] and then assembled (right column). The interlocking structures between parts is programmatically generated, allowing parts to be seamlessly connected and assembled after printing. This demonstrates our approachs ability to preserve structural integrity while enabling easy post-printing assembly."
        },
        {
            "title": "5.2 Multi-modal Integration for Structural Understanding",
            "content": "and Control The integration of BANG with multimodal large language models (MLLMs) greatly enhances the part-level understanding of 3D objects, bridging the gap between generative 3D creation and semantic comprehension. By leveraging MLLMs such as GPT-4 family [Achiam et al. 2023], BANG can automatically assign descriptive labels, functional attributes, and contextual information to individual sub-components of 3D meshes, offering deeper understanding of the objects structure and purpose. As illustrated in Fig. 11, our framework supports two key interaction paradigms to exemplify how MLLMs can be used in conjunction with 3D geometry. Explosion then Understanding. In this paradigm, the 3D object is first decomposed into its constituent parts through the generative exploding process. After the explosion, the system provides detailed textual descriptions and contextual insights for each part, facilitating iterative design and evaluation. The exploded view with clear part decomposition is rendered into images, which are then analyzed by the MLLM to generate these descriptions. To ensure clear part identification, each part is assigned distinct visual marker during rendering, such as color coding or numbered overlays. These annotated images are then provided to GPT-4, enabling it to reference specific parts unambiguously and generate corresponding descriptions, functions, or semantic roles for each. Understanding then Explosion. In this paradigm, users interact with the system through natural language commands, guiding the decomposition process based on the objects functional descriptions or relationships between parts. For example, users can specify which 12 Longwen Zhang, Qixuan Zhang, Haoran Jiang, Yinuo Bai, Wei Yang, Lan Xu, and Jingyi Yu Fig. 13. fictional journey from Earths surface to the far reaches of space, celebrating humanitys boundless ingenuity and spirit of discovery. Each object is generated from concept image and illustrated in four assembly states, using parts generated from our Generative Exploded Dynamics. The latent diffusion model uses 24-layer transformer with hidden size of 2560 and 20 attention heads per layer, following pre-norm configuration with sequential self-attention, cross-attention, and feed-forward blocks. Each feed-forward block contains an expansion ratio of 4, with GELU activation. The attention layers incorporate qk-normalization, and no gating mechanisms are used. To enhance convergence, we adopt multi-resolution training schedule, where the latent code resolution is gradually increased from 512 to 2048 during training. Text, image, and point cloud conditioning are handled by CLIP, DINOv2, and point encoder, with cross-attention for feature modulation. The training uses AdamW with learning rate of 1𝑒 5 and batch size of 512, conducted over 1600 epochs on 128 GPUs, yielding robust model capable of generating diverse 3D geometries. We then train specialized exploded view adapter to adapt the base model to generate explode views given mesh. The exploded view adapter consists of 4 transformer layers with hidden size of 512 to condition the model on both the input geometries, explosion time index, and the expected number of parts. These conditions are embedded via sinusoidal position encodings and added to the latent input. The adapter modulates the base model using cross-attention after the initial embedding layers. We collect 20k high-quality exploded-view data as described in Sec. 3.2, and randomly sample explosion time 𝑡 Unif(0, 1) during training. We freeze weights of the pretrained base model, and only train the adapter, using AdamW with learning rate of 1𝑒 5 and batch size of 128, over 3000 epochs on 128 GPUs. Finally, we freeze the weights of the base model and the exploded view adapter, and train the temporal attention module for smooth exploded dynamics generation with the same settings as the exploded view adapter. The temporal attention layers are applied after each cross-attention layer in the base model with the same dimension, ensuring consistent exploded dynamics generation. We send multiple frames in sequence, with frames count randomly sampled in [2, 5] and each explosion time uniformly sampled 𝑡 Unif(0, 1), at one time and train the temporal attention module using the same settings as the exploded view adapter. For extraction of geometric features, we distill from DINOv2-Tiny with 384-dimensional feature, maintaining the same network structure and training settings with the VAE. The entire model is implemented in PyTorch and trained on NVIDIA A800 GPUs, utilizing FP16 mixed precision training for computational efficiency. During inference, exploded dynamics sequences are sampled with 5 frames, setting {𝑡 } = {0, 0.25, 0.5, 0.75, 1}, using 50 diffusion steps and DDPM scheduler. Classifier-free guidance is applied with guidance scale of 7 to enhance generation quality. Gradient clipping (L2 magnitude 1) and learning rate warmup are applied to stabilize training. Due to GPU memory limitations, sequences are limited to 5 frames during training, which provides balance between quality and memory efficiency and will be discussed in Sec. 6.4. This approach ensures the generation of high-quality BANG: Dividing 3D Assets via Generative Exploded Dynamics Fig. 14. steampunk workshop, where blueprints transform into tangible reality, powered by our BANG framework. Each asset begins as concept image generated by FLUX [Black Forest Labs 2023], is then transformed into an integral 3D mesh via our base generative model, and subsequently exploded into parts and are meticulously enhanced part-wise for maximum visual fidelity. The generated exploded structures are displayed against the backdrop, showcasing the enhanced details achieved through our exploded-enhance pipeline. exploded dynamics, with clear part decomposition and temporal consistency, while maintaining computational efficiency."
        },
        {
            "title": "6.2 Visualization of Generated Exploded Dynamics",
            "content": "We showcase the power of our method in decomposing complex object into distinct parts in Fig. 13. This exploded view demonstrates how, starting from concept image, our framework generates 3D assets and then breaks them down into individual components. Each part is distinct, making it ideal for applications that require part-level generation and manipulation. Controlling the structure of parts and their positioning in the exploded view is one of the key features of our approach. As shown in Fig. 8, spatial promptssuch as bounding boxes and surface regionsallow users to selectively decompose the object. This enables more targeted control, whether isolating specific parts or choosing how many parts should be exposed. Fig. 17 further illustrates how our system can generate the interior components of an object, such as drawer, by interpreting user-supplied prompts. Once exploded, individual parts can be regenerated for higher fidelity. Fig. 10 shows how we begin with an initial coarse geometry, decompose it, and then regenerate each part for more detailed and accurate surfaces. This approach is exemplified in Fig. 14, where steampunk workshop scene showcases how regenerated parts elevate the designs visual quality. Finally, in Fig. 1, the recursively exploded and regenerated parts come together to form humanoid mech, demonstrating the practical application of our method in achieving high-quality geometric designs. This showcases multilevel creative pipeline: we begin from concept image, generate base 3D asset using our pretrained model, apply exploded dynamics with controllable prompts, and recursively enhance and re-explode each part to reveal structural richness, highlighting the iterative generative capabilities of our framework. Our system also enables interactive exploration, which can deepen understanding through semantic dialogue. Fig. 9 illustrates how users can interact with exploded views, gaining better understanding of the individual components. Additionally, Fig. 11 demonstrates the integration of chatbot, allowing users to request specific information about parts or modify the exploded structure interactively. This interaction bridges the gap between 3D generation and semantic understanding. Another practical application of our method is in 3D printing. Fig. 12 illustrates how exploded parts are generated and printed individually, with optimized orientations and material choices. This process ensures that combinable parts can be assembled easily postprinting while maintaining structural integrity and visual coherence. 14 Longwen Zhang, Qixuan Zhang, Haoran Jiang, Yinuo Bai, Wei Yang, Lan Xu, and Jingyi Yu Input geometry SAMesh segmentation SAMesh exploded view SAMPart3D segmentation SAMPart3D exploded view Ours assembled parts Ours exploded view Fig. 15. Qualitative comparison with part segmentation methods. We compare our approach with part segmentation techniques, including SAMesh [Tang et al. 2024c] and SAMPart3D [Yang et al. 2024], by displaying both segmentation results and their visualization in exploded views (note these methods are not designed to generate individual parts, we manually separate the segmented components for illustration). For our method, we show the parts both in their assembled state and exploded view, with each part assigned distinct color. The test cases include various configurations, i.e.: (1) two intact meshes from PartObjaverse-Tiny [Yang et al. 2024] (top two rows), featuring clean topology with an artistic style; (2) two re-meshed meshes from PartObjaverse (middle two rows), featuring uniform triangular faces; and (3) watertight meshes generated from concept images using our base generative model (bottom two rows), representing unseen data. Our method generates part geometries with meaningful decomposition, while the segmentation methods merely separate face regions, failing to preserve the volumetric integrity of individual parts."
        },
        {
            "title": "6.3 Structural Segmentation Comparison",
            "content": "Our BANG is designed for part-aware 3D generation, and there is no exact baseline on this task currently available for comparison. To evaluate its effectiveness, we instead compare our method with leading surface segmentation techniques, as part decomposition is key aspect of our approach. We compare BANG with two prominent 3D part segmentation methods: SAMesh [Tang et al. 2024c] and SAMPart3D [Yang et al. 2024]. Fig. 15 and Fig. 16 showcases these comparisons across different types of input geometries, including meshes from PartObjaverseTiny [Yang et al. 2024] (with artist-crafted topology, not included in our training data), remeshed datasets with uniform triangular faces, and watertight assets generated by our base model. SAMPart3D is applied to textured assets, while SAMesh and our framework take pure geometry as input. For visualization, the baseline methods display both their segmentation results and manually separated exploded views, whereas our method directly generates exploded views automatically. For both methods, hyperparameters were tuned to yield segmentation with moderate part granularity. While SAMesh and SAMPart3D produce reasonable segmentations for simple objects, they struggle with more complex geometries, such as mechanical parts or castle towers. These methods often exhibit inconsistent results due to the limitations of 2D segmentation from multi-view rendered images, and their performance degrades further on non-artist-created triangular meshes. Furthermore, these segmentation methods produce surface-based resultsisolating only face regions without any volume or interior structures, limiting their applicability for tasks requiring volumetric part representation. In contrast, BANG consistently produces high-quality part decompositions across all test cases, maintaining robust part-level generation and volumetric understanding throughout. User Study. To further assess the effectiveness of our method, we conducted user study where 50 participants were shown result BANG: Dividing 3D Assets via Generative Exploded Dynamics"
        },
        {
            "title": "Input geometry",
            "content": "Exploded view 1 Exploded view 2 Fig. 17. For an input geometry containing only the surface geometry of table, our approach can generate an exploded view by disassembling the surface mesh into its constituent parts (center). Alternatively, given bounding box prompts, it can infer and generate the corresponding interior structure of the drawer (right). Fig. 16. Comparison on complex generated 3D assets. While segmentation methods exhibit fragmented patches, inconsistent part groupings, and jagged segmentation boundaries, especially under structural complexity, our method produces clean, volumetric part decompositions with consistent semantics and clear structural logic. produced by BANG, SAMesh, and SAMPart3D on 10 generated assets, and asked to evaluate which segmentation method best aligned with intuitive part decomposition and offered superior visual appeal. Notably, our method achieved this with significantly lower computational cost, averaging 45 seconds per asset, compared to 386 seconds for SAMesh and 940 seconds for SAMPart3D. The results demonstrated clear preference for our method, with 65.5% of users favoring BANGs generated exploded views. 26.2% of users selected SAMesh, which benefits from multi-view segmentation and classical mesh face processing techniques like smoothing, splitting and graph-cut, offering smooth transitions between parts. 8.3% of users preferred SAMPart3D, which is based on per-asset MLP learning that is resource-intensive and produces segmented outputs with more noise at the part boundaries. These results highlight that, while SAMesh and SAMPart3D provide reasonable part segmentation for simple geometries, BANG excels in producing more consistent, intuitive, and aesthetically pleasing part decompositions across wider range of 3D assets."
        },
        {
            "title": "6.4 Evaluations",
            "content": "To quantitatively assess the quality of the generated exploded dynamics sequences and facilitate systematic comparisons, we establish comprehensive evaluation framework with carefully designed metrics. We select 50 objects from the PartObjaverse-Tiny [Yang et al. 2024] dataset, which were not included in our training data, Fig. 18. Visualization of the gradient in overlapping regions for 2D toy case. By masking out the gradients from sampling points in the overlapping regions (indicated by the small red arrows within the rectangle), the yellow circle follows the correct optimization direction (orange arrows), aligning with the target geometry. as the evaluation set. Each asset in PartObjaverse-Tiny contains high quality human-annotated parts. For each object, we generate exploded view sequences conditioned on ground truth bounding boxes and evaluate the performance using part trajectory tracking. Specifically, we assess three key metrics after transforming the parts back to their original 𝑡 = 0 positions: generation time cost, weighted IoU (wIoU), and SDF objective. We define wIoU as the weighted intersection-over-union between predicted and ground truth bounding boxes for each part. The formula is given by: wIoU = 𝑉𝑖 IoU(𝑩𝑖, 𝑩gt 𝑖 ) (cid:205)𝑗 𝑉𝑗 𝑖 (9) where 𝑉𝑖 represents the convex hull volume of the 𝑖-th part, 𝑩𝑖 and 𝑩gt 𝑖 are the predicted and ground truth bounding boxes, respectively. This metric quantifies the accuracy of part localization after explosion. The SDF objective is introduced to evaluate the geometric alignment between the fitted and actual surfaces of the parts: SDFobj = 1 QuerySDF(M, P) (10) where is the set of sampled points on the fitted surface, and QuerySDF(M, ) calculates the signed distance to the ground truth surface. This objective quantifies how closely the generated parts align with the true surface geometry. Evaluation of Temporal Attention. We conduct an ablation study to evaluate the effectiveness of the temporal attention mechanism in improving the quality of generated sequences. As shown in Table 1, 16 Longwen Zhang, Qixuan Zhang, Haoran Jiang, Yinuo Bai, Wei Yang, Lan Xu, and Jingyi Yu Table 1. An ablation study examining the impact of temporal attention and the stopping of overlapped point gradients. The evaluation is based on metrics for part trajectory tracking, which assess both the temporal consistency of the generated exploded dynamics and the accuracy of part trajectory tracking. Enabling temporal attention improves the temporal consistency of the generated dynamics, while stopping gradients for overlapping points enhances the accuracy of part trajectory tracking."
        },
        {
            "title": "Variants",
            "content": "w/o temporal attention w/o stopping gradients ours full Weighted IoU 0.6874 0.7665 0.8163 SDF Objective 0.0124 0.0092 0.0085 incorporating temporal attention leads to significant improvement in both metrics: 18.8% increase in weighted IoU and 31.5% reduction in the SDF objective. This demonstrates that temporal attention enhances temporal consistency and explosive linearity by enabling tokens to share information across different frames, ensuring smoother and more accurate part movements. Evaluation of Stopping Overlapped Point Gradients. Next, we investigate the impact of our method for stopping overlapped point gradients. In Fig. 18, we visualize 2D example where overlapping parts can lead to incorrect gradient directions during optimization. In this example, the cyan contours represent the target geometry boundaries, with the SDF values shown in the background. The optimization of the yellow circles translation is considered, where the gradients contributing to the translation are depicted by the small red arrows. When using uniform surface point sampling across the entire object, gradients from overlapping regions contribute equally, resulting in incorrect optimization directions (orange arrows) for the yellow circle. Our method resolves this issue by masking out the gradients from sampling points within the overlapped regions, leading to correct optimization directions (orange arrows) that align with the target geometry. This adjustment ensures that the optimization process is not adversely affected by overlapping regions, which is crucial in real-world 3D modeling where parts often overlap. As shown in Table 1, incorporating this technique significantly improves the fitting metric, demonstrating that our method addresses the challenges posed by overlapping components and enhances the overall accuracy of the part fitting process. Evaluation of Number of Frames Generated. To evaluate the impact of sequence length on the quality of generated exploded dynamics, we analyze part trajectory tracking across varying numbers of input frames, using both ground truth (synthetic) and generated sequences. As shown in Fig. 19, for ground truth sequences, both quality metricsweighted IoU and SDF objectiveshow rapid convergence with just 3 frames, indicating that the explosion dynamics can be effectively captured with minimal temporal sampling. This curve suggests that increasing the number of frames improves the tracking accuracy. For generated sequences, the metrics continue to improve until 5 frames. During training, our network was only trained with up to 5 frames given the constraints of training and GPU memory limitations, so performance naturally starts to drop after that point. However, the results still show some generalizability Fig. 19. Quantitative analysis of the impact of frame number on part trajectory tracking. Top: For ground truth sequences, both weighted IoU (red) and SDF objective (blue) stabilize after 3 frames, indicating that more frames improve tracking accuracy. Bottom: For generated exploded dynamics, performance metrics continue to improve up to 5 frames, while computational time cost (purple bars) increases with more frames. Although our model was trained with maximum of 5 frames due to GPU memory limitations, this result suggests that 5 frames offer reasonable trade-off between tracking accuracy and computational efficiency, balancing performance with processing time. Fig. 20. Evaluation of the effect of part number embedding. The figure demonstrates the ability to control the number of parts through part number embedding. While achieving precise control can be challenging, the approach enables coarse control, where increasing the number of exploded parts in the embedding leads to the generation of more parts. beyond the 5-frame training limit, demonstrating that more frames improve the accuracy of part trajectory tracking. As illustrated in the figure, the computational cost increases at rate slightly faster than linear as the number of frames increases, making it impractical to use excessively long sequences due to the high inference time and memory requirements. Therefore, while more frames theoretically improve accuracy, balance between training, inference time, and quality led us to select 5 frames as the optimal configuration. Evaluation of Part Number Control. Our model allows control over the number of generated parts by adjusting the parts count during the generation process, as described in Sec. 3.1. While achieving precise control over the exact number of parts can be challengingparticularly for diffusion model due to the continuous nature of the processthe model demonstrates the ability to adjust the number of exploded parts at coarse level. Fig. 20 shows the results of controlling the number of exploded parts for the same input geometry. The model effectively adjusts the segmentation granularity, generating fewer parts when specified and more parts when higher count is requested. This control is achieved without compromising the semantic consistency of the object. For example, the model merges functionally related components when fewer parts are specified, while more detailed structural decomposition is produced when more parts are generated. These results confirm that our method strikes balance between controlling segmentation granularity and maintaining semantic coherence."
        },
        {
            "title": "7 DISCUSSIONS AND CONCLUSIONS",
            "content": "In this work, we introduce BANG, generative framework that dynamically decomposes complex 3D assets into interpretable partlevel structures via smooth and consistent exploded view process. Built on large-scale 3D generative model, BANG integrates two core components: the Exploded View Adapter, which conditions the model on input geometry and timestamps, and the Temporal Attention Module, which ensures smooth transitions across the exploded process. This framework captures sophisticated structural insights, enabling high-quality 3D decomposition, generation, and enhancement. BANG seamlessly integrates part-level multimodal analysis into creative workflows, making it versatile tool for enhancing digital creation, especially where intuitive, component-based design is crucial. By mimicking the natural process of deconstruction and reassembly, BANG not only advances current 3D technologies but also aligns with human cognitive processes of understanding and creativity. Future work focused on improving physical realism, incorporating material properties, and expanding applicability could significantly enhance its potential, empowering creators across industries to bring complex designs to life. Limitations and Future Work. Despite its strengths, BANG faces several limitations. While trained on 20k exploded dynamic data, it struggles with highly complex objects, particularly those with poorly defined structural components. Expanding the dataset to include wider range of intricate structures, particularly real-world mechanisms, is essential for improving BANGs ability to handle more diverse 3D assets. Another challenge is the preservation of precise geometric details during the exploded dynamics generation process. Although BANG isolates and regenerates parts at high level of detail, subtle discrepancies between the exploded views and the original geometry persist, and some local details are lost in the process. As illustrated in Fig. 21, the generated exploded views exhibit noticeable deviation from the original geometry, particularly in highly detailed regions. This is due to the lack of explicit per-part geometric supervision during training, and the limited latent token length, which constrains the models ability to represent detailed geometry at part level. Future research could incorporate advanced geometric constraints and scale up the model training to minimize these discrepancies, ensuring that exploded geometry aligns more BANG: Dividing 3D Assets via Generative Exploded Dynamics 17 Fig. 21. Failure cases on highly detail geometry. Top: input meshes with complex structures. Bottom: generated exploded views. While BANG captures the overall structure, local detail is lost, and the exploded geometry drifts from the original. This is due to the lack of per-part supervision and limited token length in the current latent representation. closely with the original geometry while benefiting from part-level regeneration. Currently, BANG follows an artistic pipeline tailored for visual representation, which may not fully meet the needs of applications that require realistic mechanical assembly or physical constraints, such as in manufacturing or robotics. While effective for digital design and visualization, bridging the gap between artistic modeling and engineering realism is necessary for industrial applications. Future versions could incorporate physical simulation techniques to account for material properties, structural interactions, and real-world assembly processes. Finally, BANG currently focuses exclusively on geometry, neglecting material properties (e.g., flexibility, weight distribution, or compatibility) as well as appearance attributes (e.g., color or texture). Material and appearance considerations both play crucial roles in real-world assembly and disassembly tasks, affecting not only how parts physically interact and fit together but also how they are visually perceived. Integrating material properties alongside appearance attributes into BANG could improve its ability to handle realistic disassembly tasks, particularly in fields like product teardown, repair, manufacturing, and design, where these factors strongly influence the process."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was supported by National Key R&D Program of China (2022YFF0902301), NSFC programs (61976138, 61977047), STCSM (2015F0203-000-06), and SHMEC (2019-01-07-00-01-E00003). We also acknowledge support from Shanghai Frontiers Science Center of Human-centered Artificial Intelligence (ShangHAI), MoE Key Lab of Intelligent Perception and Human-Machine Collaboration (ShanghaiTech University), Core Facility Platform of Computer Science and Communication of ShanghaiTech University, and HPC Platform of ShanghaiTech University. 18 Longwen Zhang, Qixuan Zhang, Haoran Jiang, Yinuo Bai, Wei Yang, Lan Xu, and Jingyi Yu"
        },
        {
            "title": "REFERENCES",
            "content": "Ahmed Abdelreheem, Ivan Skorokhodov, Maks Ovsjanikov, and Peter Wonka. 2023. Satr: Zero-shot semantic segmentation of 3d shapes. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1516615179. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). Md Ferdous Alam and Faez Ahmed. 2024. Gencad: Image-conditioned computer-aided design generation with transformer-based contrastive representation and diffusion priors. arXiv preprint arXiv:2409.16294 (2024). Akshay Badagabettu, Sai Sravan Yarlagadda, and Amir Barati Farimani. 2024. arXiv Query2CAD: Generating CAD models using natural language queries. preprint arXiv:2406.00144 (2024). Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David Lindell. 2024. 4d-fy: Text-to-4d generation using hybrid score distillation sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 79968006. Bambu Lab. 2022. X1 Carbon. https://bambulab.com/en/x1. Black Forest Labs. 2023. FLUX. https://github.com/black-forest-labs/flux. Blender Foundation. Ongoing. Blender - Free and Open Source 3D Creation Suite. https://www.blender.org/. Accessed: 2025-01-20. Stefan Bruckner and Eduard Groller. 2006. Exploded views for volume data. IEEE transactions on visualization and computer graphics 12, 5 (2006), 10771084. Wei Cao, Chang Luo, Biao Zhang, Matthias Nießner, and Jiapeng Tang. 2024. Motion2vecsets: 4d latent vector set diffusion for non-rigid shape reconstruction and tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2049620506. Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. 2021. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision. 96509660. Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Chen Yang, Wei Shen, Lingxi Xie, Dongsheng Jiang, Xiaopeng Zhang, and Qi Tian. 2023. Segment Anything in 3D with NeRFs. In NeurIPS. Eric Chan, Koki Nagano, Matthew Chan, Alexander Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. 2023. Generative novel view synthesis with 3d-aware diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 42174229. Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. 2015. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012 (2015). Minghao Chen, Roman Shapovalov, Iro Laina, Tom Monnier, Jianyuan Wang, David Novotny, and Andrea Vedaldi. 2024c. PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models. arXiv preprint arXiv:2412.18608 (2024). Sijin Chen, Xin Chen, Anqi Pang, Xianfang Zeng, Wei Cheng, Yijun Fu, Fukun Yin, Zhibin Wang, Jingyi Yu, Gang Yu, BIN FU, and Tao Chen. 2024a. MeshXL: Neural Coordinate Field for Generative 3D Foundation Models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview.net/ forum?id=Gcks157FI3 Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Xin Chen, Zhongang Cai, Lei Yang, Gang Yu, et al. 2024b. MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers. arXiv preprint arXiv:2406.10163 (2024). Yongwei Chen, Tengfei Wang, Tong Wu, Xingang Pan, Kui Jia, and Ziwei Liu. 2025. Comboverse: Compositional 3d assets creation using spatially-aware diffusion guidance. In European Conference on Computer Vision. Springer, 128146. Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, and Guosheng Lin. 2024d. MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh Tokenization. CoRR abs/2408.02555 (2024). https://doi.org/10.48550/ arXiv.2408.02555 Zilong Chen, Feng Wang, Yikai Wang, and Huaping Liu. 2024e. Text-to-3d using gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2140121412. Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, and Huaping Liu. 2024f. V3D: Video Diffusion Models are Effective 3D Generators. CoRR abs/2403.06738 (2024). https://doi.org/10.48550/arXiv.2403.06738 Dana Cohen-Bar, Elad Richardson, Gal Metzer, Raja Giryes, and Daniel Cohen-Or. 2023. Set-the-scene: Global-local training for generating controllable nerf scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2920 2929. Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. 2023. Objaverse: universe of annotated 3d objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1314213153. Ken Deng, Yuanchen Guo, Jingxiang Sun, Zixin Zou, Yangguang Li, Xin Cai, Yanpei Cao, Yebin Liu, and Ding Liang. 2024. DetailGen3D: Generative 3D Geometry Enhancement via Data-Dependent Flow. arXiv preprint arXiv:2411.16820 (2024). Elona Dupont, Kseniya Cherenkova, Dimitrios Mallis, Gleb Gusev, Anis Kacem, and Djamila Aouada. 2025. Transcad: hierarchical transformer for cad sequence inference from point clouds. In European Conference on Computer Vision. Springer, 1936. Dave Epstein, Ben Poole, Ben Mildenhall, Alexei A. Efros, and Aleksander Holynski. 2024. Disentangled 3D scene generation with layout learning. In Proceedings of the 41st International Conference on Machine Learning (Vienna, Austria) (ICML24). JMLR.org, Article 500, 13 pages. Ziya Erkoç, Fangchang Ma, Qi Shan, Matthias Nießner, and Angela Dai. 2023. Hyperdiffusion: Generating implicit neural fields with weight-space diffusion. In Proceedings of the IEEE/CVF international conference on computer vision. 1430014310. Junjie Fei, Mahmoud Ahmed, Jian Ding, Eslam Mohamed Bakr, and Mohamed Elhoseiny. 2024. Kestrel: Point Grounding Multimodal LLM for Part-Aware 3D Vision-Language Understanding. arXiv preprint arXiv:2405.18937 (2024). Lin Gao, Jie Yang, Tong Wu, Yu-Jie Yuan, Hongbo Fu, Yu-Kun Lai, and Hao Zhang. 2019. SDM-NET: Deep generative network for structured deformable mesh. ACM Transactions on Graphics (TOG) 38, 6 (2019), 115. Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin Brualla, Pratul P. Srinivasan, Jonathan T. Barron, and Ben Poole. 2024. CAT3D: Create Anything in 3D with Multi-View Diffusion Models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview. net/forum?id=TFZlFRl9Ks Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua Susskind, Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi. 2023. Nerfdiff: Single-image view synthesis with nerfguided distillation from 3d-aware diffusion. In International Conference on Machine Learning. PMLR, 1180811826. Kan Guo, Dongqing Zou, and Xiaowu Chen. 2015. 3D mesh labeling via deep convolutional neural networks. ACM Transactions on Graphics (TOG) 35, 1 (2015), 112. Haonan Han, Rui Yang, Huan Liao, Jiankai Xing, Zunnan Xu, Xiaoming Yu, Junwei Zha, Xiu Li, and Wanhua Li. 2024. REPARO: Compositional 3D Assets Generation with Differentiable 3D Layout Alignment. arXiv preprint arXiv:2405.18525 (2024). Zekun Hao, David Romero, Tsung-Yi Lin, and Ming-Yu Liu. 2024. Meshtron: HighFidelity, Artist-Like 3D Mesh Generation at Scale. arXiv preprint arXiv:2412.09548 (2024). Amir Hertz, Or Perel, Raja Giryes, Olga Sorkine-Hornung, and Daniel Cohen-Or. 2022. Spaghetti: Editing implicit shapes through part aware generation. ACM Transactions on Graphics (TOG) 41, 4 (2022), 120. Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 2023. 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems 36 (2023), 2048220494. Yukun Huang, Jianan Wang, Yukai Shi, Boshi Tang, Xianbiao Qi, and Lei Zhang. 2023. Dreamtime: An improved optimization strategy for diffusion-guided 3d generation. In The Twelfth International Conference on Learning Representations. Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Alaa Maalouf, Shuang Li, Ganesh Subramanian Iyer, Nikhil Varma Keetha, Ayush Tewari, Joshua B. Tenenbaum, Celso de Melo, Madhava Krishna, Liam Paull, Florian Shkurti, and Antonio Torralba. 2023. ConceptFusion: Open-set Multimodal 3D Mapping. In ICRA2023 Workshop on Pretraining for Robotics (PT4R). https: //openreview.net/forum?id=zEyavwx3qf Yanqin Jiang, Li Zhang, Jin Gao, Weiming Hu, and Yao Yao. 2024. Consistent4D: Consistent 360 Dynamic Object Generation from Monocular Video. In The Twelfth International Conference on Learning Representations. https://openreview.net/ forum?id=sPUrdFGepF Heewoo Jun and Alex Nichol. 2023. Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463 (2023). Olga Karpenko, Wilmot Li, Niloy Mitra, and Maneesh Agrawala. 2010. Exploded view diagrams of mathematical surfaces. IEEE Transactions on Visualization and Computer Graphics 16, 6 (2010), 13111318. Mohammad Sadil Khan, Sankalp Sinha, Sheikh Talha Uddin, Didier Stricker, Sk Aziz Ali, and Muhammad Zeshan Afzal. 2024. Text2CAD: Generating Sequential CAD Designs from Beginner-to-Expert Level Text Prompts. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview.net/ forum?id=5k9XeHIK3L Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. 2023. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 40154026. Juil Koo, Seungwoo Yoo, Minh Hieu Nguyen, and Minhyuk Sung. 2023. Salad: Part-level latent diffusion for 3d shape generation and manipulation. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1444114451. Changjian Li, Hao Pan, Adrien Bousseau, and Niloy Mitra. 2022a. Free2cad: Parsing freehand drawings into cad commands. ACM Transactions on Graphics (TOG) 41, 4 (2022), 116. Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. 2024c. Instant3D: Fast Textto-3D with Sparse-view Generation and Large Reconstruction Model. In ICLR. https://openreview.net/forum?id=2lDQLiH1W4 Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. 2022b. Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1096510975. Wilmot Li, Maneesh Agrawala, Brian Curless, and David Salesin. 2008. Automated generation of interactive 3D exploded view diagrams. ACM Transactions on Graphics (TOG) 27, 3 (2008), 17. Wilmot Li, Maneesh Agrawala, and David Salesin. 2004. Interactive image-based exploded view diagrams. In Proceedings of Graphics Interface 2004. 203212. Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. 2024b. CraftsMan: High-fidelity Mesh Generation with 3D Native Generation and Interactive Geometry Refiner. arXiv preprint arXiv:2405.14979 (2024). Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. 2018. PointCNN: convolution on X-transformed points. In Proceedings of the 32nd International Conference on Neural Information Processing Systems (Montréal, Canada) (NIPS18). Curran Associates Inc., Red Hook, NY, USA, 828838. Yuhan Li, Yishun Dou, Yue Shi, Yu Lei, Xuanhong Chen, Yi Zhang, Peng Zhou, and Bingbing Ni. 2024a. Focaldreamer: Text-driven 3d editing via focal-fusion assembly. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 3279 3287. Hanwen Liang, Yuyang Yin, Dejia Xu, Hanxue Liang, Zhangyang Wang, Konstantinos Plataniotis, Yao Zhao, and Yunchao Wei. 2024. Diffusion4D: Fast Spatialtemporal Consistent 4D Generation via Video Diffusion Models. arXiv preprint arXiv:2405.16645 (2024). Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. 2023. Magic3d: Highresolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 300309. Anran Liu, Cheng Lin, Yuan Liu, Xiaoxiao Long, Zhiyang Dou, Hao-Xiang Guo, Ping Luo, and Wenping Wang. 2024a. Part123: part-aware 3d reconstruction from single-view image. In ACM SIGGRAPH 2024 Conference Papers. 112. Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. 2024c. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1007210083. Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Xuanlin Li, Shizhong Han, Hong Cai, Fatih Porikli, and Hao Su. 2024d. Openshape: Scaling up 3d shape representation towards open-world understanding. Advances in neural information processing systems 36 (2024). Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. 2024e. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. Advances in Neural Information Processing Systems 36 (2024). Minghua Liu, Yinhao Zhu, Hong Cai, Shizhong Han, Zhan Ling, Fatih Porikli, and Hao Su. 2023b. Partslip: Low-shot part segmentation for 3d point clouds via pretrained image-language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2173621746. Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. 2023a. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision. 92989309. Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. 2024b. Syncdreamer: Generating multiview-consistent images from single-view image. In ICLR. Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. 2024. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 99709980. Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. 2024. Scalable 3d captioning with pretrained models. Advances in Neural Information Processing Systems 36 (2024). Xianzheng Ma, Yash Bhalgat, Brandon Smart, Shuai Chen, Xinghui Li, Jian Ding, Jindong Gu, Dave Zhenyu Chen, Songyou Peng, Jia-Wang Bian, et al. 2024. When LLMs step into the 3D World: Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models. arXiv preprint arXiv:2405.10255 (2024). Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, and Yun Fu. 2022. Rethinking Network Design and Local Geometry in Point Cloud: Simple Residual MLP Framework. In International Conference on Learning Representations. https://openreview.net/ forum?id=3Pbra-_u76D Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. 2023. Realfusion: 360 reconstruction of any object from single image. In 2023 IEEE. In CVF BANG: Dividing 3D Assets via Generative Exploded Dynamics 19 Conference on Computer Vision and Pattern Recognition (CVPR), Vol. 1. 4. Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka, Niloy Mitra, and Leonidas Guibas. 2019a. StructureNet: Hierarchical Graph Networks for 3D Shape Generation. ACM Transactions on Graphics (TOG), Siggraph Asia 2019 38, 6 (2019), Article 242. Kaichun Mo, Shilin Zhu, Angel Chang, Li Yi, Subarna Tripathi, Leonidas Guibas, and Hao Su. 2019b. Partnet: large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 909918. George Kiyohiro Nakayama, Mikaela Angelina Uy, Jiahui Huang, Shi-Min Hu, Ke Li, and Leonidas Guibas. 2023. Difffacto: Controllable part-based 3d point cloud generation with cross diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1425714267. Charlie Nash, Yaroslav Ganin, SM Ali Eslami, and Peter Battaglia. 2020. Polygen: An autoregressive generative model of 3d meshes. In International conference on machine learning. PMLR, 72207229. Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. 2022. Point-e: system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751 (2022). OpenAI. 2023. DALL-E 3. https://openai.com/index/dall-e-3/. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, ShangWen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. 2024. DINOv2: Learning Robust Visual Features without Supervision. Transactions on Machine Learning Research (2024). https://openreview.net/forum?id=a68SUt6zFt Featured Certification. Zijie Pan, Zeyu Yang, Xiatian Zhu, and Li Zhang. 2024. Fast dynamic 3d object generation from single-view video. arXiv preprint arXiv:2401.08742 (2024). Dmitry Petrov, Matheus Gadelha, Radomír Měch, and Evangelos Kalogerakis. 2023. Anise: Assembly-based neural implicit surface reconstruction. IEEE Transactions on Visualization and Computer Graphics (2023). Ryan Po and Gordon Wetzstein. 2024. Compositional 3d scene generation using locally conditioned diffusion. In 2024 International Conference on 3D Vision (3DV). IEEE, 651663. Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. 2023. DreamFusion: Textto-3D using 2D Diffusion. In The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=FjNys5c7VyY Charles Qi, Hao Su, Kaichun Mo, and Leonidas Guibas. 2017a. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition. 652660. Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas Guibas. 2017b. Pointnet++: Deep hierarchical feature learning on point sets in metric space. Advances in neural information processing systems 30 (2017). Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, and Kaisheng Ma. 2025a. Shapellm: Universal 3d object understanding for embodied interaction. In European Conference on Computer Vision. Springer, 214238. Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, and Hengshuang Zhao. 2024. Gpt4point: unified framework for point-language understanding and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2641726427. Zhangyang Qi, Zhixiong Zhang, Ye Fang, Jiaqi Wang, and Hengshuang Zhao. 2025b. GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models. arXiv preprint arXiv:2501.01428 (2025). Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, and Bernard Ghanem. 2022. Pointnext: Revisiting pointnet++ with improved training and scaling strategies. Advances in neural information processing systems 35 (2022), 2319223204. Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, and Bernard Ghanem. 2024. Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors. In The Twelfth International Conference on Learning Representations (ICLR). https://openreview.net/forum?id=0jHkUDyEO9 Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi Zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, and Xiaoguang Han. 2024. Richdreamer: generalizable normal-depth diffusion model for detail richness in text-to-3d. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9914 9925. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 87488763. Ohad Rahamim, Ori Malca, Dvir Samuel, and Gal Chechik. 2024. Bringing Objects to Life: 4D generation from 3D objects. arXiv preprint arXiv:2412.20422 (2024). 20 Longwen Zhang, Qixuan Zhang, Haoran Jiang, Yinuo Bai, Wei Yang, Lan Xu, and Jingyi Yu Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. 2023. Dreambooth3d: Subject-driven text-to-3d generation. In Proceedings of the IEEE/CVF international conference on computer vision. 23492359. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. 2024. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714 (2024). Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. 2023. DreamGaussian4D: Generative 4D Gaussian Splatting. CoRR abs/2312.17142 (2023). https://doi.org/10.48550/arXiv.2312.17142 Xuanchi Ren, Jiahui Huang, Xiaohui Zeng, Ken Museth, Sanja Fidler, and Francis Williams. 2024. Xcube: Large-scale 3d generative modeling using sparse voxel hierarchies. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 42094219. Shuai Shao, Yufei Xing, Ligang Qu, and Xin Li. 2021. An Automatic Generation Method of Exploded View Based on Projection. Manufacturing Technology Journal 21, 5 (2021), 691699. https://doi.org/10.21062/mft.2021.067 Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. 2023. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110 (2023). Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. 2024. MVDream: Multi-view Diffusion for 3D Generation. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id= FUgrjq2pbB Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, and Matthias Nießner. 2024. Meshgpt: Generating triangle meshes with decoder-only transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1961519625. Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, and Yaniv Taigman. 2023. Text-to-4D dynamic scene generation. In Proceedings of the 40th International Conference on Machine Learning (Honolulu, Hawaii, USA) (ICML23). JMLR.org, Article 1323, 15 pages. Ayça Takmaz, Elisabetta Fedele, Robert W. Sumner, Marc Pollefeys, Federico Tombari, and Francis Engelmann. 2023. OpenMask3D: Open-Vocabulary 3D Instance Segmentation. In Advances in Neural Information Processing Systems (NeurIPS). George Tang, William Zhao, Logan Ford, David Benhaim, and Paul Zhang. 2024c. Segment any mesh: Zero-shot mesh part segmentation via lifting segment anything 2 to 3d. arXiv preprint arXiv:2408.13679 (2024). Jiaxiang Tang, Zhaoshuo Li, Zekun Hao, Xian Liu, Gang Zeng, Ming-Yu Liu, and Qinsheng Zhang. 2024b. Edgerunner: Auto-regressive auto-encoder for artistic mesh generation. arXiv preprint arXiv:2409.18114 (2024). Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. 2023. Make-it-3d: High-fidelity 3d creation from single image with diffusion prior. In Proceedings of the IEEE/CVF international conference on computer vision. 2281922829. Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang, Fuyang Zhang, Yuchen Fan, Vikas Chandra, Yasutaka Furukawa, and Rakesh Ranjan. 2025. Mvdiffusion++: dense high-resolution multi-view diffusion model for single or sparse-view 3d object reconstruction. In European Conference on Computer Vision. Springer, 175191. Yuan Tang, Xu Han, Xianzhi Li, Qiao Yu, Yixue Hao, Long Hu, and Min Chen. 2024a. Minigpt-3d: Efficiently aligning 3d point clouds with large language models using 2d priors. In Proceedings of the 32nd ACM International Conference on Multimedia. 66176626. Anh Thai, Weiyao Wang, Hao Tang, Stefan Stojanov, James Rehg, and Matt Feiszli. 2025. 3x2: 3D Object Part Segmentation by 2D Semantic Correspondences. In European Conference on Computer Vision. Springer, 149166. Ardian Umam, Cheng-Kun Yang, Min-Hung Chen, Jen-Hui Chuang, and Yen-Yu Lin. 2024. PartDistill: 3D Shape Part Segmentation by Vision-Language Model Distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 34703479. Mikaela Angelina Uy, Yen-Yu Chang, Minhyuk Sung, Purvi Goel, Joseph Lambourne, Tolga Birdal, and Leonidas Guibas. 2022. Point2cyl: Reverse engineering 3d objects from point clouds to extrusion cylinders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1185011860. Alexander Vilesov, Pradyumna Chari, and Achuta Kadambi. 2023. Cg3d: Compositional generation for text-to-3d via gaussian splatting. arXiv preprint arXiv:2311.17907 (2023). Haochen Wang, Xiaodan Du, Jiahao Li, Raymond Yeh, and Greg Shakhnarovich. 2023a. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1261912629. Zhaoning Wang, Ming Li, and Chen Chen. 2023b. Luciddreaming: Controllable objectcentric 3d generation. arXiv preprint arXiv:2312.00588 (2023). Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. 2024. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems 36 (2024). Daniel Watson, William Chan, Ricardo Martin Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. 2023. Novel View Synthesis with Diffusion Models. In The Eleventh International Conference on Learning Representations. https: //openreview.net/forum?id=HtoA0oT30jC Haohan Weng, Yikai Wang, Tong Zhang, C. L. Philip Chen, and Jun Zhu. 2025. PivotMesh: Generic 3D Mesh Generation via Pivot Vertices Guidance. In The Thirteenth International Conference on Learning Representations. https://openreview.net/ forum?id=WAC8LmlKYf Haohan Weng, Zibo Zhao, Biwen Lei, Xianghui Yang, Jian Liu, Zeqiang Lai, Zhuo Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, et al. 2024. Scaling Mesh Generation via Compressive Tokenization. arXiv preprint arXiv:2411.07025 (2024). Rundi Wu, Yixin Zhuang, Kai Xu, Hao Zhang, and Baoquan Chen. 2020. Pq-net: generative part seq2seq network for 3d shapes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 829838. Shuang Wu, Youtian Lin, Yifei Zeng, Feihu Zhang, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. 2024b. Direct3D: Scalable Image-to-3D Generation via 3D Latent Diffusion Transformer. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview.net/forum?id=vCOgjBIZuL Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. 2024a. Point Transformer V3: Simpler Faster Stronger. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 48404851. Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Hengshuang Zhao. 2022. Point transformer v2: Grouped vector attention and partition-based pooling. Advances in Neural Information Processing Systems 35 (2022), 3333033342. Zhijie Wu, Xiang Wang, Di Lin, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. 2019. Sagnet: Structure-aware generative network for 3d-shape modeling. ACM Transactions on Graphics (TOG) 38, 4 (2019), 114. Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. 2024. Structured 3D Latents for Scalable and Versatile 3D Generation. arXiv preprint arXiv:2412.01506 (2024). Jianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin Tong. 2023. 3d-aware image generation using 2d diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 23832393. Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. 2024. Florence-2: Advancing unified representation for variety of vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 48184829. Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, and Zhangyang Wang. 2023. Neurallift-360: Lifting an in-the-wild 2d photo to 3d object with 360deg views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 44794489. Haotian Xu, Ming Dong, and Zichun Zhong. 2017. Directionally convolutional networks for 3D shape segmentation. In Proceedings of the IEEE International Conference on Computer Vision. 26982707. Jingwei Xu, Chenyu Wang, Zibo Zhao, Wen Liu, Yi Ma, and Shenghua Gao. 2024. CADMLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM. arXiv preprint arXiv:2411.04954 (2024). Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. 2025. Pointllm: Empowering large language models to understand point clouds. In European Conference on Computer Vision. Springer, 131147. Le Xue, Mingfei Gao, Chen Xing, Roberto Martín-Martín, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savarese. 2023. Ulip: Learning unified representation of language, images, and point clouds for 3d understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1179 1189. Le Xue, Ning Yu, Shu Zhang, Artemis Panagopoulou, Junnan Li, Roberto MartínMartín, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, et al. 2024. Ulip-2: Towards scalable multimodal pre-training for 3d understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 27091 27101. Han Yan, Mingrui Zhang, Yang Li, Chao Ma, and Pan Ji. 2024a. PhyCAGE: Physically Plausible Compositional 3D Asset Generation from Single Image. arXiv preprint arXiv:2411.18548 (2024). Zizheng Yan, Jiapeng Zhou, Fanpeng Meng, Yushuang Wu, Lingteng Qiu, Zisheng Ye, Shuguang Cui, Guanying Chen, and Xiaoguang Han. 2024b. DreamDissector: Learning Disentangled Text-to-3D Generation from 2D Diffusion Priors. In Computer Vision ECCV 2024: 18th European Conference, Milan, Italy, September 29October 4, 2024, Proceedings, Part XII (Milan, Italy). Springer-Verlag, Berlin, Heidelberg, 124141. https://doi.org/10.1007/978-3-031-73254-6_8 Yunhan Yang, Yukun Huang, Yuan-Chen Guo, Liangjun Lu, Xiaoyang Wu, Edmund Lam, Yan-Pei Cao, and Xihui Liu. 2024. Sampart3d: Segment any part in 3d objects. arXiv preprint arXiv:2411.07184 (2024). Yunhan Yang, Xiaoyang Wu, Tong He, Hengshuang Zhao, and Xihui Liu. 2023. Sam3d: Segment anything in 3d scenes. arXiv preprint arXiv:2306.03908 (2023). Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. 2024. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 67966807. Fukun Yin, Xin Chen, Chi Zhang, Biao Jiang, Zibo Zhao, Jiayuan Fan, Gang Yu, Taihao Li, and Tao Chen. 2023a. Shapegpt: 3d shape generation with unified multi-modal language model. arXiv preprint arXiv:2311.17618 (2023). Yuyang Yin, Dejia Xu, Zhangyang Wang, Yao Zhao, and Yunchao Wei. 2023b. 4dgen: Grounded 4d content generation with spatial-temporal consistency. arXiv preprint arXiv:2312.17225 (2023). Yang You, Mikaela Angelina Uy, Jiaqi Han, Rahul Thomas, Haotong Zhang, Suya You, and Leonidas Guibas. 2024. Img2cad: Reverse engineering 3d cad models from images through vlm-assisted conditional factorization. arXiv preprint arXiv:2408.01437 (2024). Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, and Yao Yao. 2025. Stag4d: Spatial-temporal anchored generative 4d gaussians. In European Conference on Computer Vision. Springer, 163179. Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 2023a. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions on Graphics (TOG) 42, 4 (2023), 116. Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. 2022b. Glipv2: Unifying localization and vision-language understanding. Advances in Neural Information Processing Systems 35 (2022), 3606736080. Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. 2024b. CLAY: Controllable Large-scale Generative Model for Creating High-quality 3D Assets. ACM Transactions on Graphics (TOG) 43, 4 (2024), 120. Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng Li. 2022a. Pointclip: Point cloud understanding by clip. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 85528562. BANG: Dividing 3D Assets via Generative Exploded Dynamics 21 Renrui Zhang, Liuhui Wang, Yu Qiao, Peng Gao, and Hongsheng Li. 2023b. Learning 3d representations from 2d pre-trained models via image-to-point masked autoencoders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2176921780. Xinyi Zhang, Naiqi Li, and Angela Dai. 2024a. DNF: Unconditional 4D Generation with Dictionary-based Neural Fields. arXiv preprint arXiv:2412.05161 (2024). Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. 2021. Point transformer. In Proceedings of the IEEE/CVF international conference on computer vision. 1625916268. Yuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhenguo Li, and Gim Hee Lee. 2023. Animate124: Animating one image to 4d dynamic scene. arXiv preprint arXiv:2311.14603 (2023). Xin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong, Yang Liu, and Heung-Yeung Shum. 2023. Locally Attentional SDF Diffusion for Controllable 3D Shape Generation. ACM Trans. Graph. 42, 4, Article 91 (July 2023), 13 pages. https://doi.org/10.1145/ 3592103 Ziming Zhong, Yanyu Xu, Jing Li, Jiale Xu, Zhengxin Li, Chaohui Yu, and Shenghua Gao. 2024. Meshsegmenter: Zero-shot mesh semantic segmentation via texture synthesis. In European Conference on Computer Vision. Springer, 182199. Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun Huang, and Xinlong Wang. 2024. Uni3d: Exploring unified 3d representation at scale. In International Conference on Learning Representations (ICLR). Yuchen Zhou, Jiayuan Gu, Tung Yen Chiang, Fanbo Xiang, and Hao Su. 2025. PointSAM: Promptable 3D Segmentation Model for Point Clouds. In The Thirteenth International Conference on Learning Representations. https://openreview.net/ forum?id=yXCTDhZDh6 Yuchen Zhou, Jiayuan Gu, Xuanlin Li, Minghua Liu, Yunhao Fang, and Hao Su. 2023. PartSLIP++: Enhancing Low-Shot 3D Part Segmentation via Multi-View Instance Segmentation and Maximum Likelihood Estimation. arXiv preprint arXiv:2312.03015 (2023). Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao Zeng, Zipeng Qin, Shanghang Zhang, and Peng Gao. 2023. Pointclip v2: Prompting clip and gpt for powerful 3d open-world learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 26392650."
        }
    ],
    "affiliations": [
        "Deemos Technology Co., Ltd., China",
        "Huazhong University of Science and Technology, China",
        "ShanghaiTech University, China"
    ]
}