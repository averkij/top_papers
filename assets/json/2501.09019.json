{
    "paper_title": "Ouroboros-Diffusion: Exploring Consistent Content Generation in Tuning-free Long Video Diffusion",
    "authors": [
        "Jingyuan Chen",
        "Fuchen Long",
        "Jie An",
        "Zhaofan Qiu",
        "Ting Yao",
        "Jiebo Luo",
        "Tao Mei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The first-in-first-out (FIFO) video diffusion, built on a pre-trained text-to-video model, has recently emerged as an effective approach for tuning-free long video generation. This technique maintains a queue of video frames with progressively increasing noise, continuously producing clean frames at the queue's head while Gaussian noise is enqueued at the tail. However, FIFO-Diffusion often struggles to keep long-range temporal consistency in the generated videos due to the lack of correspondence modeling across frames. In this paper, we propose Ouroboros-Diffusion, a novel video denoising framework designed to enhance structural and content (subject) consistency, enabling the generation of consistent videos of arbitrary length. Specifically, we introduce a new latent sampling technique at the queue tail to improve structural consistency, ensuring perceptually smooth transitions among frames. To enhance subject consistency, we devise a Subject-Aware Cross-Frame Attention (SACFA) mechanism, which aligns subjects across frames within short segments to achieve better visual coherence. Furthermore, we introduce self-recurrent guidance. This technique leverages information from all previous cleaner frames at the front of the queue to guide the denoising of noisier frames at the end, fostering rich and contextual global information interaction. Extensive experiments of long video generation on the VBench benchmark demonstrate the superiority of our Ouroboros-Diffusion, particularly in terms of subject consistency, motion smoothness, and temporal consistency."
        },
        {
            "title": "Start",
            "content": "Ouroboros-Diffusion: Exploring Consistent Content Generation in Tuning-free Long Video Diffusion* Jingyuan Chen1, Fuchen Long2, Jie An1, Zhaofan Qiu2, Ting Yao2, Jiebo Luo1, Tao Mei2 1 University of Rochester, Rochester, NY USA 2 HiDream.ai Inc. jchen157@u.rochester.edu, longfuchen@hidream.ai, jan6@cs.rochester.edu {qiuzhaofan, tiyao}@hidream.ai, jluo@cs.rochester.edu, tmei@hidream.ai 5 2 0 2 5 1 ] . [ 1 9 1 0 9 0 . 1 0 5 2 : r Abstract The first-in-first-out (FIFO) video diffusion, built on pretrained text-to-video model, has recently emerged as an effective approach for tuning-free long video generation. This technique maintains queue of video frames with progressively increasing noise, continuously producing clean frames at the queues head while Gaussian noise is enqueued at the tail. However, FIFO-Diffusion often struggles to keep longrange temporal consistency in the generated videos due to the lack of correspondence modeling across frames. In this paper, we propose Ouroboros-Diffusion, novel video denoising framework designed to enhance structural and content (subject) consistency, enabling the generation of consistent videos of arbitrary length. Specifically, we introduce new latent sampling technique at the queue tail to improve structural consistency, ensuring perceptually smooth transitions among frames. To enhance subject consistency, we devise Subject-Aware Cross-Frame Attention (SACFA) mechanism, which aligns subjects across frames within short segments to achieve better visual coherence. Furthermore, we introduce self-recurrent guidance. This technique leverages information from all previous cleaner frames at the front of the queue to guide the denoising of noisier frames at the end, fostering rich and contextual global information interaction. Extensive experiments of long video generation on the VBench benchmark demonstrate the superiority of our Ouroboros-Diffusion, particularly in terms of subject consistency, motion smoothness, and temporal consistency."
        },
        {
            "title": "Introduction",
            "content": "With the rise of artificial visual content generation technologies, significant breakthroughs have been made in video diffusion (Blattmann et al. 2023; Peng et al. 2024; Ma et al. 2024b; Long et al. 2024). However, most current video diffusion models are trained on short clips (e.g., 16 frames), significant challenge when scaling to long video generation. Instead of relying on extensive training data to extend video length, our work focuses on leveraging pre-trained video diffusion model to generate long videos without extensive training data or fine-tuning. Additionally, we place strong emphasis on content consistency to enhance both the visual and motion quality of long videos in diffusion. *This work was performed at HiDream.ai. Copyright 2025, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: Illustration of FIFO-Diffusion (Kim et al. 2024) (top) and our Ouroboros-Diffusion (bottom) for tuning-free long video generation. The recent first-in-first-out diffusion (Kim et al. 2024) has successfully employed the pre-trained video diffusion model for infinite frame generation. Compared to general video diffusion that all frames share the same noise level, the diagonal denoising approach proposed by Kim et al. (2024) maintains queue of frames with progressively increasing noise levels, enabling frame-by-frame generation at each step. The upper part of Figure 1 conceptualizes the diagonal denoising process. In this process, fully denoised frame at the queue head is popped out, while random noisy latent is pushed to the tail. The latent enqueue-dequeue cycle allows for the incremental generation of video frames. However, the independently enqueued Gaussian noise can lead to content discrepancies between the frame latents near the tail of the queue. Moreover, FIFO-Diffusion fails to utilize visual information contained in the earlier denoised frames, which exacerbates temporal inconsistency in continuous video diffusion. For instance, the appearance of the cat changes significantly in the output video of Figure 1. Our work addresses this issue with FIFO-Diffusion from two key perspectives: structural and subject consistency. To this end, we introduce novel denoising framework termed OuroborosDiffusion, designed for tuning-free long video generation. Inspired by the ancient symbol of Ouroborosa serpent or dragon eating its own tail, symbolizing wholeness and selfrenewalour framework embodies these concepts by seamlessly integrating information across time. The design of Ouroboros-Diffusion is guided by three core principles targeting distinct information flows to improve structural and subject consistency: present infers future, present influences present, and past informs present, as depicted in Figure 1. To enhance structural consistency, we address the critical step of enqueueing new tail noise, which can lead to structural incoherence. From video continuation perspective, this step involves sampling future frame that should maintain visual structure continuity with previous frames. To achieve this, we propose inferring the future frame from the present frame in the denoising queue by exploiting the low-frequency component connection between them. Specifically, instead of initializing the tail latent with Gaussian noise, OuroborosDiffusion extracts the low-frequency component from the second-to-last frame latent using Fast Fourier Transform (FFT) and combines it with the high-frequency part of random noise to create the enqueued latent. The low-frequency component preserves layout information for overall video consistency, while the high-frequency component introduces necessary video dynamics. For subject consistency, we consider the semantic dependencies of long videos from two angles: within the current denoising queue and between previously generated clear frames and the current queue. To enhance subject temporal coherence within the present queue, we extend self-attention across frames through SubjectAware Cross-Frame Attention (SACFA) module. This module leverages segmented subject regions from the crossattention map to extract subject tokens in each frame, which serve as auxiliary contexts for subject alignment. These tokens are then stored in subject feature bank. To model longer-range subject dependencies, we introduce longterm memory of past subjects to guide the appearance of the present subject. Specifically, Ouroboros-Diffusion utilizes the long-term memory derived from the frame at the head of the queue to guide the denoising of noisier frames near the tail, optimizing the latent through subject-aware gradient during video denoising. The main contribution of this work is the proposal of Ouroboros-Diffusion to address content consistency in turning-free long video generation. Our solution elegantly explored how diagonal denoising could benefit from lowfrequency content preservation, and how to preserve subject consistency through cross-frame attention and gradientbased latent optimization in diffusion. Extensive experiments on VBench verify the effectiveness of our proposal in terms of both visual and motion quality."
        },
        {
            "title": "2 Related Work\nText-to-Video Diffusion Models. The great success of\ntext-to-video (T2V) diffusion models (Ho et al. 2022a; Vo-\nleti, Jolicoeur-Martineau, and Pal 2022; Villegas et al. 2023;\nWang et al. 2023b; Yin et al. 2023a; Chen et al. 2023; Zhang\net al. 2024; Guo et al. 2024) for video generation based on\ntext prompts has been witnessed in recent years. VDM (Ho\net al. 2022b) is one of the early works that combines spa-\ntial and temporal attention to construct space-time factorized\nUNet for video synthesis. Later in Make-A-Video (Singer\net al. 2023), the prior knowledge of text-to-image diffusion",
            "content": "models is explored in video diffusion and the 2D-UNet is extended with the temporal modules (e.g., temporal convolution and self-attention (Long et al. 2022a)) for motion modeling (Long et al. 2019, 2022b, 2023). The advances (An et al. 2023; Blattmann et al. 2023; Wu et al. 2023a; Chen et al. 2024b) further execute the video synthesis on latent space and push the boundaries of high-resolution video generation. Inspired by the impressive performances of Diffusion Transformers (DiTs) (Peebles and Xie 2023; Ma et al. 2024a), the spatial-temporal transformer architecture starts to emerge in video diffusion (Hong et al. 2023; Xu et al. 2024). Here, we choose VideoCrafter2 (Chen et al. 2024a) as backbone text-to-video model for long video generation. Long Video Diffusion. Despite the achievements of textto-video diffusion, long video generation is still grand challenge. Existing works have explored two strategies, i.e., tuning-based and tuning-free long video diffusion. Typically, the tuning-based methods (Yin et al. 2023b; Henschel et al. 2024; Tian et al. 2024; Jin et al. 2024) usually exploits an auto-regressive manner which leverages the information of past generated frames to guide the synthesis of current frames. Nevertheless, tuning the auto-regressive video diffusion model usually involves huge computational cost. To overcome this limitation, tuning-free approaches (Wang et al. 2023a; Qiu et al. 2024; Kim et al. 2024; Oh et al. 2024; Tan et al. 2024) try to adopt pre-trained basic video diffusion model with the power of short-clip synthesis to generate multiple frames with temporal coherence. Qiu et al. (2024) introduces sliding-window temporal attention mechanism to simultaneously denoising all frames for keeping temporal consistency. Recent advance FIFO-Diffusion (Kim et al. 2024) proposes to store frames with different noise levels in queue, and continuously output one clean frame from the head and enqueue Gaussian noise at the tail in each denoising step. However, the model still faces the temporal flickering issue due to insufficient long-range modeling and discrepancy arising from enqueueing Gaussian tail noise. Guidance in Denoising. In the procedure of image/video denoising, various guidance (e.g., visual tokens in attention (Tewel et al. 2024; Jain et al. 2024) and gradient guidance (Meng et al. 2022; Epstein et al. 2023; An et al. 2024; Chen, Laina, and Vedaldi 2024)) has been investigated to control the content generation. For instance, ConsiStory (Tewel et al. 2024) takes the visual tokens from the reference image to facilitate the content alignment across images in batch, while FreeDoM (Yu et al. 2023) optimizes the denoising procedure with the gradient of the target energy function. In short, our work exploits basic T2V model for long video generation without tuning. The proposed OuroborosDiffusion contributes by not only studying how the lowfrequency component in latent influences the structural consistency, but also how the guidance in video denoising can be better leveraged to achieve frame-level subject consistency."
        },
        {
            "title": "3 Preliminaries: Video Denoising Approach\nParallel Denoising. Latent Video Diffusion Models\n(LVDMs) perform diffusion processes in the latent space",
            "content": "Figure 2: An overview of our Ouroboros-Diffusion. The whole framework (a) contains three key components: coherent tail latent sampling in queue manager , (b) Subject-Aware Cross-frame Attention (SACFA), and (c) self-recurrent guidance. The coherent tail latent sampling in queue manager derives the enqueued frame latents at the queue tail to improve structural consistency. The Subject-Aware Cross-frame Attention (SACFA) aligns subjects across frames within short segments for better visual coherence. The self-recurrent guidance leverages information from all historical cleaner frames to guide the denoising of noisier frames, fostering rich and contextual global information interaction. for efficient video generation. Given video latent zt Rf chw at timestep [1, . . . , ], where zt consists of frame latents: zt = (cid:8)zi (cid:9)f i=1, represents the total number of denoising steps used in the sampling process. Conventionally, LVDMs adopt parallel denoising approach, where zt is iteratively denoised to obtain the clean video latent z0. In parallel denoising, the noise level remains consistent across all frames throughout the denoising process. The parallel denoising is formulated as: zt1 = Ψ (zt, t, ϵθ(zt, t, c)) , (1) where Ψ() denotes the sampler such as DDIM and ϵθ denotes spatial-temporal UNet that predicts the added noise at each denoising step, conditioned on the text embedding c. Diagonal Denoising. Unlike the conventional parallel denoising, FIFO-Diffusion (Kim et al. 2024) introduces diagonal denoising technique that sequentially produces clear frame latents. This is achieved by employing fixed-length queue of frame latents with progressively increasing noise levels (i.e., 1 to ), as illustrated in Figure 1. In the time step τ of diagonal denoising, τ 1 frames have already been (cid:9)T generated. We denote Qτ = (cid:8)zτ +t t=1 as all frame latents in the queue. Here, zτ +t is the (τ + t)-th frame latent with noise level t. The denoising step is reformulated as: t t1 t1 t=1 , c)) , (cid:9)T t=1 = Ψ(Qτ , {t}T (cid:8)zτ +t t=1 , ϵθ(Qτ , {t}T (2) (cid:9)T where (cid:8)zτ +t t=1 denotes the one-step denoised Qτ . After the denoising step, the first frame latent zτ 0 in the queue becomes clear latent and is dequeued from the head. newly sampled Gaussian noise is then enqueued at the tail, making the queue Qτ to transition to Qτ +1. Iteratively performing this enqueue-dequeue process allows for video generation in frame-by-frame manner. When the queue size exceeds the frame capacity of the base video diffusion model, the frame latents are denoised window-by-window with the basic temporal length ."
        },
        {
            "title": "4 Methodology",
            "content": "Figure 2 provides an overview of the Ouroboros-Diffusion framework. To address the limitations of diagonal denoising, we model three distinct types of information flow to achieve structural and subject consistency. Coherent tail latent sampling ensures smooth transitions by using the second-tolast latent as the guidance, enabling present to infer future. SACFA enhances subject consistency by extending spatial self-attention with subject contexts from neighboring frames, allowing mutual influence between present frames. Self-recurrent guidance further elevates long-range subject coherence by leveraging past subject memory derived from the head of the queue to inform the denoising of the tail, demonstrating how the past informs the present. In this section, we first discuss the limitations of the FIFO-Diffusion as the motivation of the proposed method. Then we introduce details of our Ouroboros-Diffusion."
        },
        {
            "title": "4.1 Limitation of Diagonal Denoising",
            "content": "The FIFO-Diffusion (Kim et al. 2024) enables the generation of videos with infinite frames. However, the subject consistency of the videos produced by diagonal denoising is often compromised due to the following two limitations: Lack of Global Consistency Modeling. In the design of diagonal denoising, the global visual consistency is not explicitly considered during the diffusion process. Once frames are fully denoised, they are dequeued and no longer contribute to the generation of subsequent frames. This leads to the underutilization of embedded subject information(e.g., semantics, motion). Consequently, later frames cannot reference the information from previously generated frames, which causes the visual appearance of backgrounds and objects to gradually shift during continuous frame denoising, resulting in inconsistencies over time. Qτ +1. In this way, the future is faithfully inferred from the present information within the queue."
        },
        {
            "title": "4.3 Subject-Aware Cross-Frame Attention\nTo improve subject consistency during denoising, we\npropose Subject-Aware Cross-Frame Attention (SACFA),\nwhich extends the vanilla spatial attention layer to incorpo-\nrate subject tokens from multiple frames, enhancing visual\nalignment across frames with enriched subject context.",
            "content": "Subject Mask Construction. Central to SACFA is subject token masking mechanism, which relies on segmenting the subject within each frame. To obtain the segmentation, key subject words are first extracted from prompt using GPT-4o (OpenAI 2023). These words are then tokenized and encoded into the embedding Csubj using the CLIP (Radford et al. 2021) text encoder. The subject tokens are fed into the linear projection layer of each cross-attention layer to form the text subject key Ksubj. Subject-related attention maps are obtained by computing the attention between the query and Ksubj for each cross-attention layer. These maps are averaged across the token dimension and converted into binary subject mask using Otsus method. Finally, subject masks of different resolutions are interpolated to uniform resolution and averaged, resulting in the final subject mask. Attention Processing of SACFA. To enhance temporal subject consistency across neighboring frames, we extend the spatial self-attention layer into subject-aware crossframe approach. The essence is to enable each frame to incorporate subject visual content from other frames when modeling its own appearance. To achieve this, we start by applying subject-specific masks to the keys and values of all frames to extract relevant visual content. These masked keys and values are then concatenated across frames to form subject-aware cross-frame keys and values, denoted as and , with the shape hw d. This process creates collection of subject references that capture the subject information across multiple frames. Finally, and are concatenated with the regular key Ki and value Vi of the target frame i. The attention processing in SACFA for the i-th frame is then computed as: = Softmax (cid:32) Qi [Ki, K] (cid:33) [Vi, ] . (4) SACFA strengthens local subject correspondence by allowing subject information from neighboring frames to influence current frames, ensuring consistent subject representation. The subject-related key is then stored in subject feature bank, forming subject memory that serves as the foundation for subsequent self-recurrent guidance."
        },
        {
            "title": "4.4 Self-Recurrent Guidance\nWe introduce self-recurrent guidance to leverage the past\nsubject features to guide the present denoising steps. Central\nto this approach is the Subject Feature Bank, which stores\nthe long-term memory of video subjects. The Subject Fea-\nture Bank is initialized using the averaged subject-masked\nkeys K′ from the first f cleaner frame latents of the video",
            "content": "Figure 3: The detailed illustration of coherent tail latent sampling in the queue manager. Latent Discrepancy Near the Tail. Another limitation arises in the enqueue-dequeue denoising process, specifically in the construction of the tail latent. The tail latent is typically sampled from standard Gaussian distribution, which contains pure noise without any visual information. Meanwhile, neighboring latents have already undergone partial denoising in previous steps, introducing some degree of visual content. This creates discrepancy between the visual information in the tail latent and its neighboring latents, leading to inconsistencies. As result, the model may struggle to reconcile these differences, often resulting in frame flickering during video generation. These limitations in diagonal denoising motivate us to devise targeted strategies for improvement."
        },
        {
            "title": "4.2 Coherent Tail Latent Sampling\nAfter completing a DDIM sampling step for queue Qτ , a\nclean latent is removed from the queue head, leaving a va-\ncant spot at the tail with a noise level of T . Instead of sam-\npling Gaussian noise as in Kim et al. (2024), we propose\ncoherent tail latent sampling to retain similar structural in-\nformation by using the second-to-last latent zτ +T\nT −1 as a struc-\ntural guidance. To ensure the structure similarity between\nthe last two latents, a straightforward approach is to directly\napply noise to the second-to-last frame latent and use it as\nthe new tail latent. However, we discover that this approach\nresults in generated videos with limited dynamics due to\nthe excessive similarities in visual content. Recent advances\n(Wu et al. 2023b; Everaert et al. 2024) indicate that the low-\nfrequency component in the latent space primarily corre-\nsponds to the layout and overall structure in the pixel space.\nAs shown in Figure 3, we apply a 2D low-pass filter to ex-\ntract the low-frequency component of the re-noised latent\nˆzτ +T\n, preserving the layout as the base latent, and introduce\nT\ndynamics by adding the high-frequency component of a ran-\ndomly sampled Gaussian noise η. The coherent tail latent\nsampling is formulated as:",
            "content": "zτ +1+T = low(ˆzτ +T ) + high(η) , (3) low() and where high() denotes the low-pass and highpass filter functions with threshold r, respectively. The coherent tail latent allows for consistent yet dynamic visual continuation, ensuring smooth transition from Qτ to Figure 4: Visual examples of single-scene long video generation by different approaches. The text prompt is cat wearing sunglasses and working as lifeguard at pool. Approach Subject Consistency Background Consistency Motion Smoothness Temporal Flickering Aesthetic Quality StreamingT2V (Henschel et al. 2024) StreamingT2V-VideoTetris (Tian et al. 2024) FIFO-Diffusion (Kim et al. 2024) FreeNoise (Qiu et al. 2024) Ouroboros-Diffusion 90.70 89.06 94.04 94.50 96. 95.46 94.80 96.08 96.45 96.90 97.34 96.79 95.88 95.42 97.73 95.93 95.30 93.38 93.62 96. 54.98 52.89 59.06 59.32 59.89 Table 1: Single-scene video generation performances on VBench. For each video, 128 frames are synthesized for evaluation. sequence, denoted as ltm. These initial frames are particularly valuable as they contain clearer and more critical visual information, making them an essential basis for the construction of long-term memory. After each denoising step of the queue, the bank is updated using an exponential moving average as follows: ltm λ ltm + 1 λ (cid:88) t=1 Kτ +t , (5) where λ denotes the strength of memorization and only the first frames in the queue contribute to the update. Then, we exploit the subject tokens from the feature bank as reference to minimize the gradient of the subject discrepancy at the tail. This gradient serves as guidance to optimize the latent denoising (Yu et al. 2023) as follows: t1 zτ +t zτ +t t1 γt zτ +t (cid:88) t=1 ltm Kτ +t 2 2 , (6) where γt denotes time-dependent strength of the guidance. By integrating this term, the self-recurrent guidance aligns the generated latents more closely with the subject features of the previously generated frames, thereby enhancing longrange subject consistency in the synthesized video."
        },
        {
            "title": "5.1 Experimental Settings\nBenchmark. We empirically verify the merit of our\nOuroboros-Diffusion for both single-scene and multi-scene",
            "content": "long video generation on the VBench (Huang et al. 2024) benchmark. We sample 93 common prompts from VBench as the testing set for single-scene video generation. All methods are required to generate 128 video frames for each prompt. To explore multi-prompt scenarios, we extended the single prompts into multiple prompts using GPT-4o (OpenAI 2023), resulting in 78 groups of multi-scene prompts. Each group contains 2 to 3 prompts with consistent subject phrasing. For each multi-prompt group, we generate 256 video frames for performance comparison. Implementation Details. We implement our OuroborosDiffusion on the text-to-video model VideoCrafter2 (Chen et al. 2024a). The total number of time steps in the DDIM sampler is set to 64, matching the queue length. The threshold for the low-pass filter in coherent tail latent sampling is set to 0.25. SACFA is applied only in the down-blocks and mid-block (with down-sampling factors of 2 and 4) of the spatial-temporal UNet empirically. The last 16 frames in the queue are involved in SACFA calculation. The self-recurrent guidance derived from the first 16 frames at the queue head applies to the last 16 frames at the tail. The parameter λ for updating the subject feature bank is set to 0.98. Evaluation Metrics. We choose five evaluation metrics from VBench (Huang et al. 2024) for performance comparison: Subject Consistency, Background Consistency, Motion Smoothness, Temporal Flickering, and Aesthetic Quality. Subject Consistency assesses the uniformity and coherence of the primary subject across frames using DINO (Zhang Figure 5: Visual examples of multi-scene long video generation by different approaches. The multi-scene prompts are: 1). an astronaut is riding horse in space; 2). an astronaut is riding dragon in space; 3). an astronaut is riding motorcycle in space. Approach Subject Consistency Background Consistency Motion Smoothness Temporal Flickering Aesthetic Quality FIFO-Diffusion (Kim et al. 2024) FreeNoise (Qiu et al. 2024) Ouroboros-Diffusion 93.96 95.07 95.73 96.17 96.52 96.82 96.36 96.57 97. 93.59 95.06 95.82 60.12 61.26 61.17 Table 2: Multi-scene video generation performances on VBench. For each video, 256 frames are synthesized for evaluation. et al. 2023) features. Background Consistency is measured by the CLIP (Radford et al. 2021) feature similarity. Temporal Flickering evaluates the frame-wise consistency and Motion Smoothness assesses the fluidity and jittering of motion. Finally, Aesthetic Quality indicates the quality of overall visual appearance including composition and color harmony."
        },
        {
            "title": "5.2 Comparisons with State-of-the-Art Methods\nWe compare our proposal with four state-of-the-art long\nvideo diffusion models, i.e., StreamingT2V (Henschel et al.\n2024), StreamingT2V-VideoTetris (Tian et al. 2024), FIFO-\nDiffusion (Kim et al. 2024) and FreeNoise (Qiu et al. 2024)\non long video generation.",
            "content": "Single-Scene Video Generation. Table 1 summarizes the performance comparison of single-scene long video generation on VBench. Overall, Ouroboros-Diffusion consistently outperforms other baselines across various metrics. Notably, Ouroboros-Diffusion achieves Temporal Flickering score of 96.12%, surpassing the tuning-free approaches FIFODiffusion and FreeNoise by 2.74% and 2.50%, respectively. The highest frame consistency, as indicated by the Temporal Flickering metric, demonstrates the effectiveness of our coherent tail latent sampling, which enforces similarity in image layout between adjacent frames to enhance structural consistency. Additionally, the best performances of Subject Consistency (96.06%) and Background Consistency (96.90%) further show that Ouroboros-Diffusion benefits from subject-level guidance, resulting in natural coherence throughout long video generation. It is important to note that our approach does not compromise video motion strength (e.g., causing static video generation) to improve temporal consistency. To validate this, we calculate the dynamic degree of the pre-trained base model VideoCrafter2 (Chen et al. 2024a), which achieves score of 42.01. OuroborosDiffusion attains higher dynamic degree of 44.12, confirming that our method maintains motion variability while further enhancing content consistency in video synthesis. Figure 4 showcases single-scene long video generation results across different approaches. Compared to other baselines, Ouroboros-Diffusion consistently produces videos with more seamless transitions and superior visual consistency. For example, StreamingT2V and FreeNoise often generate unreasonable or inconsistent content (e.g., the change of red collar in FreeNoise). Although the FIFODiffusion denoising strategy maintains some content coherence (e.g., the appearance of the cat), the enqueued independent Gaussian noise still results in background variations (e.g., changes in the building behind the pool). In contrast, the video generated by our Ouroboros-Diffusion preserves both subject and background consistency effectively, demonstrating the advantage of using information guidance within the queue to enhance visual alignment in diffusion. Multi-Scene Video Generation. Next, we compare our Ouroboros-Diffusion on the task of multi-scene video generation. Table 2 details the performance across different baselines on VBench. Ouroboros-Diffusion outperforms all baselines in Subject/Background Consistency, Motion Smoothness, and Temporal Flickering. Specifically, our approach demonstrates substantial performance boosts (0.66%1.77%) in Subject Consistency. Note that FreeNoise exploits noise scheduler for multi-scene video generation, but it emphasizes on prompt adjustment in different denoising steps for motion injection. Our OuroborosDiffusion differs fundamentally since ours not only integrates subject visual tokens into cross-frame attention for local alignment but also leverages these moving-averaged tokens to recurrently optimize video latents, ensuring global coherence. Our Aesthetic Quality is slightly lower (by Model Coherent Tail Latent Sampling SACFA Self-Recurrent Guidance Subject Consistency Background Consistency Motion Smoothness Temporal Flickering Aesthetic Quality C - - - - - - 94.04 95.56 95.71 96.06 96.08 96.66 96.73 96. 95.88 97.61 97.70 97.73 93.38 95.86 96.00 96.12 59.06 59.61 59.67 59.89 Table 3: Performance contribution of each component (i.e., Coherent Tail Latent Sampling, SACFA and Self-Recurrent Guidance) in Ouroboros-Diffusion on single-scene video generation. For each video, 128 frames are synthesized for evaluation. Motion Smoothness Temporal Flickering Model Subject Consistency Motion Smoothness Model Gaussian Noise Head Frame Second-to-Last Frame (Ours) 95.88 97.51 97. 93.38 95.87 96.12 Table 4: Evaluation on the coherent tail latent sampling. 0.09%) than that of FreeNoise. We speculate that this may be due to discrepancy between the parallel and diagonal denoising approaches (i.e., consistent noise versus inconsistent noise). This issue could be addressed through model training, and it shows direction for our future work. Figure 5 further illustrates the multi-scene long video generation results of three different approaches. As shown, OuroborosDiffusion successfully generates smoother transitions (e.g., scene changes with the astronaut maintaining the same motion direction) and more consistent visual content (e.g., single, identical astronaut rather than two)."
        },
        {
            "title": "5.3 Ablation Study on Ouroboros-Diffusion\nIn this section, we conduct ablation studies to evaluate the\nimpact of each design component in Ouroboros-Diffusion\nfor long video generation. All experiments follow previous\nsingle-scene video generation settings for comparison.",
            "content": "Overall Framework. We first investigate how each component of the overall framework impacts the quality of video generation. Table 3 summarizes the performance results for single-scene long video generation. When integrating coherent tail latent sampling into the base model (A), significant performance boost (1.52%) is attained by model in Subject Consistency. This highlights weakness of the base model (i.e., FIFO-Diffusion), where structural information may be overlooked due to the enqueueing of independent Gaussian noise at the queue tail. By enhancing subject token alignment through subject-aware cross-frame attention, model outperforms model across all metrics. Finally, model D, (i.e., our Ouroboros-Diffusion), achieves the best performance by recurrently propagating the subject information of frames from the queue head to the tail frames for latent optimization during video denoising. Coherent Tail Latent Sampling. Next, we present the performance of different variants explored in the design of coherent tail latent sampling. Table 4 details the results of two additional runs:1). enqueueing independent Gaussian noise at the tail, and 2) replacing the second-to-last frame with the head frame for latent sampling. w/o Guidance Guidance with λ=1 Guidance with λ=0 Moving-Average (Ours) 94.04 95.87 96.00 96. 95.88 97.71 97.71 97.73 Table 5: Evaluation on the self-recurrent guidance. As expected, independent Gaussian noise yields the lowest Motion Smoothness. Using the head frames lowfrequency component for latent sampling improves it from 95.88% to 97.51%. However, the appearance gap between queue head and tail latents limits structural guidance. By adjusting enqueued latents with tail information, OuroborosDiffusion further enhances motion quality. Self-Recurrent Guidance. We have also analyzed the updating mechanism of the subject feature bank when devising the self-recurrent guidance. As shown in Table 5, whether using historic frame guidance (λ=1) or the current frame guidance (λ=0) at the queue head, there are notable performance improvements in Subject Consistency and Motion Smoothness. These results highlight the advantage of guiding the denoising process using subject information through gradient-based adjustments. To achieve better subject consistency in the synthesized video, we implement movingaverage strategy for feature bank updating, which combines information from both historic and current subject tokens. The λ is empirically set as 0.98 in our framework."
        },
        {
            "title": "6 Conclusions\nThis paper addresses consistent content generation in\ntuning-free long video diffusion. We introduce Ouroboros-\nDiffusion, a framework based on the first-in-first-out sam-\npling strategy, which maintains a queue for frame-wise de-\nnoising. Our approach examines temporal consistency from\ntwo perspectives: structural and subject levels. To materi-\nalize this idea, we inject structural information into newly\nenqueued Gaussian noise by leveraging the low-frequency\ncomponent of latents near the queue tail, thereby enhanc-\ning the consistency of the overall structural\nlayout. At\nthe subject level, we emphasize short-range consistency\nthrough visual token alignment in cross-frame attention,\nwhile long-range consistency is achieved via latent guid-\nance using subject-aware gradient adjustment. Experiments\non the VBench benchmark validate the effectiveness of\nOuroboros-Diffusion, demonstrating improvements in both\nvisual quality and motion smoothness.",
            "content": "References An, J.; Yang, Z.; Li, L.; Wang, J.; Lin, K.; Liu, Z.; Wang, L.; and Luo, J. 2024. OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation. In ACM MM. An, J.; Zhang, S.; Yang, H.; Gupta, S.; Huang, J.-B.; Luo, J.; and Yin, X. 2023. Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation. arXiv preprint arXiv:2304.08477. Blattmann, A.; Rombach, R.; Ling, H.; Dockhorn, T.; Kim, S. W.; Fidler, S.; and Kreis, K. 2023. Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. In CVPR. Chen, H.; Xia, M.; He, Y.; Zhang, Y.; Cun, X.; Yang, S.; Xing, J.; Liu, Y.; Chen, Q.; Wang, X.; et al. 2023. VideoCrafter1: Open Diffusion Models for High-Quality Video Generation. arXiv preprint arXiv:2310.19512. Chen, H.; Zhang, Y.; Cun, X.; Xia, M.; Wang, X.; Weng, C.; and Shan, Y. 2024a. VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models. arXiv preprint arXiv:2401.09047. Chen, M.; Laina, I.; and Vedaldi, A. 2024. Training-Free Layout Control with Cross-Attention Guidance. In WACV. Chen, Z.; Long, F.; Qiu, Z.; Yao, T.; Zhou, W.; Luo, J.; and Mei, T. 2024b. Learning Spatial Adaptation and Temporal Coherence in Diffusion Models for Video Super-Resolution. In CVPR. Epstein, D.; Jabri, A.; Poole, B.; Efros, A. A.; and Holynski, A. 2023. Diffusion Self-Guidance for Controllable Image Generation. In NeurIPS. Everaert, M. N.; Fitsios, A.; Bocchio, M.; Arpa, S.; Susstrunk, S.; and Achanta, R. 2024. Exploiting the SignalLeak Bias in Diffusion Models. In WACV. Guo, Y.; Yang, C.; Rao, A.; Liang, Z.; Wang, Y.; Qiao, Y.; Agrawala, M.; Lin, D.; and Dai, B. 2024. AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning. In ICLR. Henschel, R.; Khachatryan, L.; Hayrapetyan, D.; Poghosyan, H.; Tadevosyan, V.; Wang, Z.; Navasardyan, S.; and Shi, H. 2024. StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text. arXiv preprint arXiv:2403.14773. Ho, J.; Chan, W.; Saharia, C.; Whang, J.; Gao, R.; Gritsenko, A.; Kingma, D. P.; Poole, B.; Norouzi, M.; Fleet, D. J.; et al. Imagen Video: High Definition Video Generation 2022a. with Diffusion Models. arXiv preprint arXiv:2210.02303. Ho, J.; Salimans, T.; Gritsenko, A.; Chan, W.; Norouzi, M.; and Fleet, D. J. 2022b. Video Diffusion Models. In NeurIPS. Hong, W.; Ding, M.; Zheng, W.; Liu, X.; and Tang, J. 2023. CogVideo: Large-Scale Pretraining for Text-to-Video Generation via Transformers. In ICLR. Huang, Z.; He, Y.; Yu, J.; Zhang, F.; Si, C.; Jiang, Y.; Zhang, Y.; Wu, T.; Jin, Q.; Chanpaisit, N.; et al. 2024. VBench: Comprehensive Benchmark Suite for Video Generative Models. In CVPR. Jain, Y.; Nasery, A.; Vineet, V.; and Behl, H. 2024. PEEKABOO: Interactive Video Generation via Masked-Diffusion. In CVPR. Jin, Y.; Sun, Z.; Xu, K.; Chen, L.; Jiang, H.; Huang, Q.; Song, C.; Liu, Y.; Zhang, D.; Song, Y.; Gai, K.; and Mu, Y. 2024. Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization. arXiv preprint arXiv:2402.03161. Kim, J.; Kang, J.; Choi, J.; and Han, B. 2024. FIFODiffusion: Generating Infinite Videos from Text without Training. arXiv preprint arXiv:2405.11473. Long, F.; Qiu, Z.; Pan, Y.; Yao, T.; Luo, J.; and Mei, T. 2022a. Stand-Alone Inter-Frame Attention in Video Models. In CVPR. Long, F.; Qiu, Z.; Pan, Y.; Yao, T.; Ngo, C.-W.; and Mei, T. 2022b. Dynamic Temporal Filtering in Video Models. In ECCV. Long, F.; Qiu, Z.; Yao, T.; and Mei, T. 2024. VideoStudio: Generating Consistent-Content and Multi-Scene Videos. In ECCV. Long, F.; Yao, T.; Qiu, Z.; Tian, X.; Luo, J.; and Mei, T. 2019. Gaussian Temporal Awareness Networks for Action Localization. In CVPR. Long, F.; Yao, T.; Qiu, Z.; Tian, X.; Luo, J.; and Mei, T. 2023. Bi-calibration Networks for Weakly-Supervised Video Representation Learning. IJCV. Ma, N.; Goldstein, M.; Albergo, M. S.; Boffi, N. M.; Vanden-Eijnden, E.; and Xie, S. 2024a. SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers. arXiv preprint arXiv:2401.08740. Ma, Y.; He, Y.; Cun, X.; Wang, X.; Chen, S.; Li, X.; and Chen, Q. 2024b. Follow Your Pose: Pose-Guided Text-toVideo Generation Using Pose-Free Videos. In AAAI. Meng, C.; He, Y.; Song, Y.; Song, J.; Wu, J.; Zhu, J.-Y.; and Ermon, S. 2022. SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations. In ICLR. Oh, G.; Jeong, J.; Kim, S.; Byeon, W.; Kim, J.; Kim, S.; Kwon, H.; and Kim, S. 2024. MEVG: Multi-event Video Generation with Text-to-Video Models. In ECCV. OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774. Peebles, W.; and Xie, S. 2023. Scalable Diffusion Models with Transformers. In ICCV. Peng, B.; Chen, X.; Wang, Y.; Lu, C.; and Qiao, Y. 2024. ConditionVideo: Training-Free Condition-Guided Video Generation. In AAAI. Qiu, H.; Xia, M.; Zhang, Y.; He, Y.; Wang, X.; Shan, Y.; and Liu, Z. 2024. FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling. In ICLR. Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning Transferable Visual Models from Natural Language Supervision. In ICML. Singer, U.; Polyak, A.; Hayes, T.; Yin, X.; An, J.; Zhang, S.; Hu, Q.; Yang, H.; Ashual, O.; Gafni, O.; et al. 2023. Make-A-Video: Text-to-Video Generation without TextVideo Data. In ICLR. Tan, Z.; Yang, X.; Liu, S.; and Wang, X. 2024. VideoInfinity: Distributed Long Video Generation. arXiv preprint arXiv:2406.16260. Tewel, Y.; Kaduri, O.; Gal, R.; Kasten, Y.; Wolf, L.; Chechik, G.; and Atzmon, Y. 2024. Training-free Consistent Text-toImage Generation. ACM Transactions on Graphics (TOG). Tian, Y.; Yang, L.; Yang, H.; Gao, Y.; Deng, Y.; Chen, J.; Wang, X.; Yu, Z.; Tao, X.; Wan, P.; et al. 2024. VideoTetris: Towards Compositional Text-to-Video Generation. arXiv preprint arXiv:2406.04277. Villegas, R.; Babaeizadeh, M.; Kindermans, P.-J.; Moraldo, H.; Zhang, H.; Saffar, M. T.; Castro, S.; Kunze, J.; and Erhan, D. 2023. Phenaki: Variable Length Video Generation from Open Domain Textual Description. In ICLR. Voleti, V.; Jolicoeur-Martineau, A.; and Pal, C. 2022. MCVD-Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation. In NeurIPS. Wang, F.-Y.; Chen, W.; Song, G.; Ye, H.-J.; Liu, Y.; and Li, H. 2023a. Gen-L-Video: Multi-Text to Long Video arXiv preprint Generation via Temporal Co-Denoising. arXiv:2305.18264. Wang, X.; Yuan, H.; Zhang, S.; Chen, D.; Wang, J.; Zhang, Y.; Shen, Y.; Zhao, D.; and Zhou, J. 2023b. VideoComposer: Compositional Video Synthesis with Motion Controllability. In NeurIPS. Wu, J. Z.; Ge, Y.; Wang, X.; Lei, W.; Gu, Y.; Shi, Y.; Hsu, W.; Shan, Y.; Qie, X.; and Shou, M. Z. 2023a. Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-toVideo Generation. In ICCV. Wu, T.; Si, C.; Jiang, Y.; Huang, Z.; and Liu, Z. 2023b. FreeInit : Bridging Initialization Gap in Video Diffusion Models. arXiv preprint arXiv:2312.07537. Xu, J.; Zou, X.; Huang, K.; Chen, Y.; Liu, B.; Cheng, M.; Shi, X.; and Huang, EasyAnimate: High-Performance Long Video Generation Method arXiv preprint based on Transformer Architecture. arXiv:2405.18991. Yin, S.; Wu, C.; Liang, J.; Shi, J.; Li, H.; Ming, G.; and Duan, N. 2023a. DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory. arXiv preprint arXiv:2308.08089. Yin, S.; Wu, C.; Yang, H.; Wang, J.; Wang, X.; Ni, M.; Yang, Z.; Li, L.; Liu, S.; Yang, F.; et al. 2023b. NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation. In ACL. Yu, J.; Wang, Y.; Zhao, C.; Ghanem, B.; and Zhang, J. 2023. Freedom: Training-Free Energy-Guided Conditional Diffusion Model. In ICCV. Zhang, H.; Li, F.; Liu, S.; Zhang, L.; Su, H.; Zhu, J.; Ni, L. M.; and Shum, H.-Y. 2023. DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection. In ICLR. Zhang, Z.; Long, F.; Pan, Y.; Qiu, Z.; Yao, T.; Cao, Y.; and Mei, T. 2024. TRIP: Temporal Residual Learning with Image Noise Prior for Image-to-Video Diffusion Models. In CVPR. J. 2024."
        }
    ],
    "affiliations": [
        "HiDream.ai Inc.",
        "University of Rochester, Rochester, NY USA"
    ]
}