{
    "paper_title": "Uniform Discrete Diffusion with Metric Path for Video Generation",
    "authors": [
        "Haoge Deng",
        "Ting Pan",
        "Fan Zhang",
        "Yang Liu",
        "Zhuoyan Luo",
        "Yufeng Cui",
        "Wenxuan Wang",
        "Chunhua Shen",
        "Shiguang Shan",
        "Zhaoxiang Zhang",
        "Xinlong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Continuous-space video generation has advanced rapidly, while discrete approaches lag behind due to error accumulation and long-context inconsistency. In this work, we revisit discrete generative modeling and present Uniform discRete diffuSion with metric pAth (URSA), a simple yet powerful framework that bridges the gap with continuous approaches for the scalable video generation. At its core, URSA formulates the video generation task as an iterative global refinement of discrete spatiotemporal tokens. It integrates two key designs: a Linearized Metric Path and a Resolution-dependent Timestep Shifting mechanism. These designs enable URSA to scale efficiently to high-resolution image synthesis and long-duration video generation, while requiring significantly fewer inference steps. Additionally, we introduce an asynchronous temporal fine-tuning strategy that unifies versatile tasks within a single model, including interpolation and image-to-video generation. Extensive experiments on challenging video and image generation benchmarks demonstrate that URSA consistently outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods. Code and models are available at https://github.com/baaivision/URSA"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 7 1 7 4 2 . 0 1 5 2 : r a"
        },
        {
            "title": "UNIFORM DISCRETE DIFFUSION WITH METRIC PATH\nFOR VIDEO GENERATION",
            "content": "Haoge Deng1,3,5, Ting Pan2,3,5, Fan Zhang5, Yang Liu4,5, Zhuoyan Luo5, Yufeng Cui5 Wenxuan Wang5, Chunhua Shen4, Shiguang Shan2,3, Zhaoxiang Zhang1,3, Xinlong Wang5 1National Laboratory of Pattern Recognition, CASIA 2Key Laboratory of Intelligent Information Processing, ICT, CAS 3University of Chinese Academy of Sciences 4Zhejiang University 5Beijing Academy of Artificial Intelligence Figure 1: Visualization of URSA across diverse video generation tasks: text-to-video generation, video interpolation, and long video generation. These examples underscore the versatility of URSA."
        },
        {
            "title": "ABSTRACT",
            "content": "Continuous-space video generation has advanced rapidly, while discrete approaches lag behind due to error accumulation and long-context inconsistency. In this work, we revisit discrete generative modeling and present Uniform discRete diffuSion with metric pAth (URSA), simple yet powerful framework that bridges the gap with continuous approaches for the scalable video generation. At its core, URSA formulates the video generation task as an iterative global refinement of discrete spatiotemporal tokens. It integrates two key designs: Linearized Metric Path and Resolution-dependent Timestep Shifting mechanism. These designs enable URSA to scale efficiently to high-resolution image synthesis and long-duration video generation, while requiring significantly fewer inference steps. Additionally, we introduce an asynchronous temporal fine-tuning strategy that unifies versatile tasks within single model, including interpolation and image-to-video generation. Extensive experiments on challenging video and image generation benchmarks demonstrate that URSA consistently outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods. Code and models are available at https://github.com/baaivision/URSA Equal Contribution. This work was done when H.Deng, T.Pan, and Y.Liu were interns at BAAI. Corresponding Author: wangxinlong@baai.ac.cn, zhaoxiang.zhang@ia.ac.cn 1 Figure 2: Illustration of different image/video generation paradigms. Discrete-space approaches such as AR and MDM adopt non-refinable local generation, where produced tokens are fixed once generated. In contrast, URSA introduces iterative global refinement, conceptually aligning discrete methods with continuous-space approaches, and substantially narrowing their performance gap."
        },
        {
            "title": "INTRODUCTION",
            "content": "Continuous-space visual generation has achieved remarkable progress in both image and video synthesis (Batifol et al., 2025; Baldridge et al., 2024; Betker et al., 2023; Brooks et al., 2024; Wang et al., 2025a; Gao et al., 2025b; Yang et al., 2025b; Kong et al., 2024). Driven by advances in diffusion model algorithms (Ho et al., 2020; Song et al., 2021), these continuous-space methods have demonstrated strong capabilities in producing high-fidelity and visually coherent content, establishing themselves as the dominant paradigm for generative modeling. In parallel, discrete-space text generation has become the de facto paradigm for large language models (Radford et al., 2018; 2019; Brown et al., 2020). Inspired by the success of LLMs, recent works have extended similar ideas to visual generation through discrete tokenization, using either next-token prediction (Sun et al., 2024a; Wang et al., 2024b; Kondratyuk et al., 2024) or masked token prediction (Chang et al., 2023; Xie et al., 2025c). However, discrete approaches still lag behind their continuous counterparts, facing challenges such as error accumulation and maintaining long-context consistency, especially in video generation. For instance, even though masked diffusion models employ bidirectional transformers, we still observe low visual quality and unnatural object motions. In this work, we first revisit discrete generative modeling and introduce URSA, powerful visual generation framework built upon Uniform discRete diffuSion with metric pAth. Our approach is simple: we generate videos and images by iterative refinement over discrete spatiotemporal tokens. As illustrated in Fig. 2, unlike classic autoregressive (AR) models and masked diffusion models (MDM) that adopt non-refinable local generation, where produced tokens are fixed once generated, URSA emphasizes iterative refinement over global discrete tokens, conceptually aligning discrete methods with continuous counterparts, and substantially narrowing their performance gap. URSA starts from categorical noise, x0 Unif([K])D, where each of the D-dimensional discrete token is independently sampled from uniform distribution over the vocabulary [K] = {1, 2, . . . , K}, and iteratively performs global refinement along metric-guided probability path to obtain x1 on the data manifold, i.e., the target image or video. This iterative process enables URSA to capture the hierarchical structure of video data, from global layouts to detailed dynamics, while leveraging temporal redundancy to preserve spatiotemporal coherence. We propose novel metric probability path tailored for long visual sequences by incorporating two key components: linearized metric path and resolution-dependent timestep shifting mechanism. Collectively, these designs enable precise control over data perturbations, property essential for effectively learning hierarchical data manifolds. This construction allows URSA to scale efficiently to long-sequence tasks, such as high-resolution image synthesis and long video generation, while requiring substantially fewer inference steps. Furthermore, we introduce an asynchronous timestep scheduling strategy, where timesteps are independently sampled for each frame. This asynchronous design empowers URSA to generate minute-level long videos and support wide range of tasks within unified model, including image-to-video generation, video interpolation, and extrapolation. 2 URSA achieves text-to-video score of 82.4 on VBench (Huang et al., 2024a), outperforming discrete and continuous baselines. In image-to-video generation tasks, URSA reaches VBench score of 86.2, on par with the state-of-the-art open-source models. For text-to-image generation, URSA achieves DPG-Bench (Hu et al., 2024) score of 86.0, exceeding previous discrete approaches. Moreover, URSA exhibits strong zero-shot generalization in variable length contexts, underscoring its versatility. Our contributions can be summarized as follows: 1) We propose URSA, simple yet powerful framework that bridges the gap to continuous diffusion methods and enables scalable video generation. 2) We highlight two key designs, Linearized Metric Path and Resolution-dependent Timestep Shifting for long-sequence training. We further propose an asynchronous timestep scheduling strategy that enables multi-task video generation. 3) URSA substantially pushes the envelope of discrete generation, attaining state-of-the-art performance on VBench, DPG-Bench, and GenEval (Ghosh et al., 2024)."
        },
        {
            "title": "2.1 CONTINUOUS-SPACE VISUAL GENERATION",
            "content": "Continuous methods for visual generation have achieved significant progress in recent years. Early endeavors such as variational autoencoders (VAEs) (Kingma & Welling, 2014) and flow-based models (Dinh et al., 2014; 2017) exploit continuous latent spaces to model complex images, while GANs (Goodfellow et al., 2014) generate high-resolution images with strong perceptual quality via adversarial training (Brock et al., 2019; Karras et al., 2020). Diffusion models (Ho et al., 2020; Song et al., 2021), which learn to recover data by progressively denoising Gaussian noise in continuous space, demonstrated remarkable performance in both image and video generation (Gao et al., 2025a; Batifol et al., 2025; Baldridge et al., 2024; Betker et al., 2023; Wu et al., 2025a; Brooks et al., 2024; Kong et al., 2024; Gao et al., 2025b; Wang et al., 2025a; Kuaishou, 2024; Ma et al., 2025). MAR (Li et al., 2024) employs an autoregressive framework with diffusion head to produce continuousvalued outputs, and NOVA (Deng et al., 2025b) further extends this idea to video generation, applying autoregressive modeling to spatiotemporal sequences. URSA shares the same spirit as continuous diffusion models, performing global iterative refinement, but operates over discrete tokens. 2.2 DISCRETE-SPACE VISUAL GENERATION Discrete visual generation can be broadly categorized into autoregressive and masked diffusion models, both operating on discrete visual tokens such as pixels (Kalchbrenner et al., 2017; Reed et al., 2017) or latent codes (Oord et al., 2017; Esser et al., 2021). Autoregressive models generate discrete visual tokens sequentially, with each prediction conditioned on previously generated context. This approach has been applied to both image (Sun et al., 2024a; Ramesh et al., 2021; Ding et al., 2021; Yu et al., 2022; Zhu et al., 2025) and video synthesis (Wang et al., 2024b; Yan et al., 2021; Kondratyuk et al., 2024; Wang et al., 2024c). Although simple in concept, this design often has slow inference and significant error accumulation. In contrast to autoregressive methods, masked diffusion models (Gat et al., 2024; Chang et al., 2022; 2023; Yu et al., 2023) introduce the prediction of masked tokens, enabling parallel generation and improved modeling of global context. Despite these advantages, it remains challenging to apply these methods to long sequences, e.g. high-fidelity long-form video. FUDOKI (Wang et al., 2025b) investigates the integration of discrete flow matching (Gat et al., 2024) within native multimodal models. In this work, we adopt uniform discrete diffusion approach, which performs iterative global refinement from categorical noise. By addressing key challenges, URSA enables both efficient inference and high-quality long-sequence generation."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "We first review the concepts of uniform discrete diffusion / discrete flow matching in Sec. 3.1, which provide the theoretical foundation for our framework. In Sec. 3.2.1-3.2.2, we introduce URSA, simple yet powerful framework that bridges the gap between discrete and continuous approaches, enabling effective and scalable video generation."
        },
        {
            "title": "3.1 PRELIMINARY: DISCRETE FLOW MATCHING",
            "content": "Discrete Flow Matching (DFM) (Gat et al., 2024; Shaul et al., 2025) introduces family of generative models designed to map data from an initial distribution p0(x), to final distribution p1(x), within discrete state space. The model utilizes time-dependent probability path, pt(x), which interpolates between these two distributions over the interval [0, 1]. The key idea behind DFM is to define velocity field, ut, which drives the evolution of this probability path, enabling the model to simulate Markov process and generate new data samples. Probability paths. We consider the probability path pt(x), where [0, 1] indexes time-dependent probability distribution between source distribution p0(x) and target distribution p1(x) over t. Given data distribution q(x) over = (x1, . . . , xD) [K]D, the probability path is defined as pt(x) (cid:88) x1S pt(xx1)q(x1), where pt(xx1) (cid:89) i=1 pt(xixi 1), (1) pt(xi xi xi given the initial state xi 1. 1) denotes conditional forward probability path, characterizing the evolution of the state Probability velocities. To generate the predefined probability path pt(x), we consider ContinuousTime Markov Chain (CTMC), modeled as stochastic process. The dynamics of this CTMC are governed by probability velocity ut, also known as the transition rate. The transition rate models how the current state xt evolves toward the target state x1 over time. Within this framework, each token is updated independently according to the following transition rule: xi t+h δxi () + ui t( xi t, xi 1), (2) t( xi t, xi where ui 1) represents velocity field, conditional rate function that governs the flow of probability from the current state xi 1 over time. Equation (2) can be interpreted as small perturbation of the point mass δxi , scaled by the step size h, effectively modeling discrete state transitions as continuous-time stochastic process. This velocity field is central to DFM, as it characterizes the dynamics of the probability path and is the primary quantity learned during training. to the target state xi 3.2 UNIFORM DISCRETE DIFFUSION WITH METRIC PATH We present URSA, novel framework built on uniform discrete diffusion with metric path for image and video generation. In this section, we first introduce three key innovations: (1) Linearized MetricPath for structured and tractable trajectory design, (2) Resolution-dependent Timestep Shifting mechanism to improve training stability and representation learning for long video sequences, and (3) Frame-wise Independent Perturbation Scheduling strategy for unified long-video generation and multitask learning. After introducing these core components, we further provide the training procedure and sampling algorithm. 3.2.1 METRIC PROBABILITY PATH FOR LONG SEQUENCE DATA For data with varying sequence lengths, the degree of perturbation should be adapted during training. This requires probability path to effectively handle sequences of different lengths, such as highresolution images or videos. In this section, we introduce two key techniques, linearized metric path and resolution-dependent timestep shifting, to address this challenge, ensuring that the perturbation process is appropriately adjusted based on the sequence length. Linearized metric path. Inspired by (Shaul et al., 2025), We introduce the linearized metric path, novel probability path derived from token embedding distances. Formally, we define the distance function : R0, which measures the discrepancy between the codebook embeddings of generated token and the target tokens x1. The distance satisfies the property d(x, x1) = 0 = x1, ensuring well-defined metric structure. Based on this, the probability path is defined as pt(xx1) = softmax (βtd(x, x1)) = softmax (cid:0)βtd(xi, xi 1)(cid:1) , (3) (cid:89) i=0 Figure 3: Global refinement via token distance in embedding space. Starting from categorical noise x0 (left), our framework refines data based on token distance to get target data x1 (right), enabling hierarchical structure generation from global semantics to fine details. where βt : [0, 1] R0 is monotonic scheduler function with boundary conditions β0 = 0, β1 = . The core of linearized path lies in the functional form of βt, which is parameterized as 1 )α, [0, 1), βt = ( (4) where > 0 and α > 0 are hyperparameters that control the relationship between the sampling distance d(xt, x1) and time t. Specifically, the forward process samples xt pt1( x1), with boundary conditions yielding uniform distribution over codebook embeddings at = 0 and deterministic sample at x1 when = 1, illustrated in Figure 3. When is between 0 and 1, our objective is to find an appropriate set of values for and α that preserve the linear relationship between and d(xt, x1). This linearity provides finer control of perturbations over the probability path, as described next. Additional experiments and discussions on the impact of linearized metric path on model convergence speed and overall performance are provided in Sec. 4.3. Resolution-dependent timestep shifting. Intuitively, since higher resolutions contain more pixels, more perturbation is needed to alter the signal. To address this, we introduce time shift parameter λ, which adjusts the timestep based on the resolution. For any given t, we define the shifted timestep t = + λ(1 t) . (5) Because our proposed linearized metric path enforces linear relationship between and d(xt, x1), we modulate this path using λ to accommodate varying data resolutions. For higher resolutions, we set λ > 1 to create convex relationship between and d(xt, x1) that introduces stronger perturbations. For lower resolutions, we set λ < 1, yielding concave relationship with more gradual perturbations. 3.2.2 ASYNCHRONOUS TIMESTEP SCHEDULING Due to their complex spatiotemporal dynamics and broad applicability across downstream tasks, prior video generation methods render task-specific modeling both inefficient and resource-intensive. Motivated by diffusion forcing (Chen et al., 2024a), we propose an asynchronous timestep scheduling strategy tailored for multi-task training and sampling. Rather than applying the same noise level across all frames in video sequence (i.e., synchronous timestep scheduling), we assign uniform noise levels independently to each frame. Formally, given clean video sequence = {f (1), (2), . . . , (n)} with frames, we assign each frame continuous time ti U(0, 1), forming the timestep schedule = {t1, t2, . . . , tn}. The corresponding noisy sequence is denoted as = {f (1) t2 , . . . , (n) tn }, where the metric-induced probability path in Eq. 3 is applied frame-wise according to its assigned ti. This strategy enables fine-grained temporal modeling over the timestep schedule via decoupling noise levels across frames. As result, the training progress adaptively balances local frame reconstruction with global temporal coherence, facilitating versatile generation objectives, such as text-to-video, image-to-video, video extrapolation, and startend frame control within unified model architecture. Additional visualizations for these advanced generation tasks are provided in Appendix & C. t1 , (2)"
        },
        {
            "title": "3.2.3 TRAINING AND SAMPLING",
            "content": "1 , (2) Training. We first encode video clips into discrete token sequences using pre-trained tokenizer, resulting in clean video sequence x1 = {f (1) 1 , . . . , (n) 1 }, where denotes the number of video frames and (i) 1 denotes the i-th frame tokens. At each training step, we uniformly sample timesteps ti [0, 1] for each frame (i) in the sequence and obtain perturbed sequence xt pt( x1) via 1 the proposed metric probability path. The backbone, implemented with the LLM architecture, takes as input the concatenation of text tokens and noisy tokens xt, and produces logits over the token vocabulary to predict the original sequence x1. The training objective is formulated as the expected cross-entropy between the ground-truth visual tokens and the models predicted distribution: = EtU [0,1], x1,xt (cid:2) log p1t (x1 xt, e)(cid:3) . (6) Sampling. We follow Gat et al. (2024); Shaul et al. (2025) and employ the Euler solver for efficient and high-quality generation. Specifically, we first uniformly sample x0 from the full vision vocabulary and feed it into the model to obtain the prediction ˆx1. Following Shaul et al. (2025), we compute the velocity field ut( xt, ˆx1). We then iteratively refine xt using ut, where each iteration updates the sample xt along the estimated direction. Once the refinement steps have been completed, the sampling process returns clean image or video. Additional details are provided in Appendix A."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "4.1 EXPERIMENT SETUP Datasets. We leverage curated selection of high-quality datasets to effectively train URSA models. For text-to-image training, we collect 16M image-text pairs sourced from Unsplash (Unsplash, 2020), DataComp (Gadre et al., 2024), COYO (Byeon et al., 2022), and JourneyDB (Sun et al., 2023). These pairs are filtered by image resolution and aesthetic score, and further supplemented with 14M AI-generated image samples using the FLUX.1 model (Batifol et al., 2025). For text-to-video training, we select 12M video-text pairs from the highest scoring subset of Koala-36M (Wang et al., 2025c) and complement them with 12M internal video-text pairs. The internal videos are captioned using the Emu2-17B model (Sun et al., 2024b) in conjunction with the captioning engine (Diao et al., 2024). We uniformly sample short and long captions during training, with maximum length of 320 tokens. Architectures. We initialize our visual generation model with weights from pre-trained LLM. Specifically, we adopt the Qwen3 LLM architecture (Yang et al., 2025a), which natively incorporates QK-Norm (Dehghani et al., 2023) layer to stabilize the multimodal training. To better capture the spatiotemporal structure inherent in videos, we introduce an enhanced M-RoPE (Wang et al., 2024a) that allocates interleaved frequency components across temporal, height, and width dimensions, following the approach of Mogao (Liao et al., 2025). Crucially, unlike Liao et al. (2025), our 3DRoPE assigns identical positions for texts, ensuring equivalence with the 1D-RoPE (Su et al., 2024). We use the Cosmos (Agarwal et al., 2025) tokenizer to extract image and video tokens, achieving 4 temporal and 88 spatial compression through 64K FSQ (Mentzer et al., 2024) codebook. Furthermore, we train an IBQ (Shi et al., 2025) tokenizer for high-resolution image generation, facilitating efficient 1616 spatial compression via 256-dimensional codebook with 131K entries. Diffusion schedulers. We adopt the Kinetic Optimal Scheduler (Shaul et al., 2025), equipped with metric-induced probability path specifically designed for the embedding space of vision tokenizers. Following Shaul et al. (2025), we perform grid search over the path hyperparameters α and c, visually inspecting the reconstructed samples for each (α, c) that fully exploit the time interval [0, 1]. Eventually, we select (α, c) to (1.0, 5) for the Cosmos tokenizer and (0.5, 6) for our IBQ tokenizer. For standard uniform diffusion, we use the mixture probability path proposed by Gat et al. (2024). In contrast, for masked diffusion, we adopt the MaskGIT (Chang et al., 2022) scheduler, which has been empirically shown to achieve state-of-the-art performance in both image and video generation models (Kondratyuk et al., 2024; Bai et al., 2025). Following established practice in continuous diffusion models, we default to 25 inference steps for image generation and 50 for video generation. Training details. URSA is trained on 128 A100 (40GB) GPUs. In all experiments, we use the AdamW optimizer (Loshchilov & Hutter, 2019) with β1 = 0.9, β2 = 0.95, weight decay of 0.05, and an initial learning rate of 1e-4. The learning rate employs cosine decay (Loshchilov & Hutter, 2017). We first pre-train text-to-image models and leverage their weights to initialize text-to-video models. Subsequently, following Chen et al. (2025a), we adapt full-sequence video diffusion models to diffusion forcing architectures by applying frame-wise noise schedules for autoregressive generation. Evaluation. We evaluate text-to-image alignment using benchmarks DPG-Bench (Hu et al., 2024) and GenEval (Ghosh et al., 2024). Each image is generated from original or rewritten text prompts, with resolution determined by model type: 10241024 for image generation models to support high fidelity, and 512320 for video generation models to effectively measure cross-modal generalization. We access text-to-video generation using VBench (Huang et al., 2024a) and image-to-video generation with VBench++ (Huang et al., 2024b), its comprehensive successor tailored for real-world scenarios. The videos, sized 49512320, are generated from rewritten prompts for text-to-video evaluation, and from original text prompts with official cropped first-frame images for image-to-video evaluation. We apply classifier-free guidance (Ho & Salimans, 2022) with scale value of 7.0 in all evaluations."
        },
        {
            "title": "4.2 MAIN RESULTS",
            "content": "URSA rivals Sora-like text-to-video generation models despite using discrete video tokenizer. Current discrete video tokenizers offer limited spatiotemporal compression and reconstruction quality, posing significant challenges to bidirectional diffusion transformers. However, URSA excels in generating video clips from text, achieving strong performance on the VBench, as shown in Table 1. Compared to Sora-like diffusion models: Vchitect (Fan et al., 2025), Pyramid Flow (Jin et al., 2025), LuminaVideo (Liu et al., 2025a), OpenSora (Peng et al., 2025) and OpenSoraPlan (Lin et al., 2024), URSA matches or exceeds their performance, particularly in the semantic field. These results further underscore the need for tokenizer that satisfies the imaging quality of state-of-the-art continuous models (Kong et al., 2024; Teng et al., 2025; Ma et al., 2025; Yang et al., 2025b; Wang et al., 2025a). Table 1: Text-to-video evaluation on VBench. For clarity and to better highlight distinctions between models, we report only the most relevant metrics across the quality and semantic dimensions. Model #params #videos Total Score Quality Score Semantic Score Dynamic Degree Aesthetic Quality Imaging Quality Object Class Multiple Objects Spatial Relationship Color Scene Continuous models NOVA Vchitect-2.0 Pyramid Flow LuminaVideo OpenSoraPlan v1.5 OpenSora 2.0 MAGI-1 Step-Video CogVideoX1.5 HunyuanVideo Wan2.1 Discrete models Lumos-1 Emu3 URSA 0.6B 2B 2B 2B 8B 11B 24B 30B 5B 13B 14B 3.6B 8B 1.7B 20M 134M 10M 12M 40M 85M - - - - - 10M - 24M 80.1 81.6 81.7 83.0 83.0 83.6 81.8 81.8 82.0 83.2 83.7 78.3 81.0 82.4 80.4 82.5 84.7 83.9 84.2 84.4 84.7 84.5 82.7 85.1 85.6 79.5 84.1 83. 79.1 77.8 69.6 79.3 78.2 80.3 70.4 71.3 79.2 75.8 76.1 73.5 68.4 78.5 20.1 58.3 64.6 67.1 64.4 56.4 72.5 53.1 56.2 70.8 65.5 - 79.3 81. 59.4 61.5 63.3 62.3 66.9 65.3 59.3 61.2 62.1 60.4 66.1 - 59.6 63.1 59.4 65.6 65.0 64.6 68.5 65.7 65.3 70.6 65.3 67.6 69.4 58.0 62.6 62. 92.0 87.8 86.7 91.0 91.9 94.6 84.1 80.6 83.4 86.1 86.3 90.1 86.2 93.4 77.5 69.4 50.7 68.3 70.7 78.0 50.6 50.6 65.3 68.6 69.6 - 44.6 70. 77.5 54.6 59.5 67.3 80.1 76.8 73.0 71.5 79.4 68.7 75.4 - 68.7 62.1 87.7 86.9 82.9 90.2 81.8 86.3 87.5 88.3 88.4 91.6 88.6 82.0 88.3 90. 54.1 57.5 43.2 56.1 52.1 53.4 28.9 24.4 53.3 53.9 45.8 - 37.1 52.3 URSA emerges frame-conditioned video generation by accurately modeling the future motion. Prior methods typically adapt text-to-image (Ren et al., 2024; Chen et al., 2024b; Xing et al., 2024) or text-to-video models with clean first frame for image-to-video generation. In contrast, URSA seamlessly integrates asynchronous frame conditions, enabling zero-shot generalization for this task. As depicted in Table 2, URSA excels in camera control and subject movement versus specialized frame-conditioned models (Agarwal et al., 2025; Yu et al., 2025; Wang et al., 2025a; Liu et al., 2025b). Our results demonstrate that diffusion forcing effectively generalizes to image-to-video generation, pushing the boundaries of autoregressive discrete video generation models without causal attention. URSA performs on par with the state-of-the-art models in generating high-resolution images. We compare URSA against continuous models in Table 3, encompassing specialist architectures: SDXL (Podell et al., 2024), SD3 (Esser et al., 2024), FLUX (Batifol et al., 2025), SANA (Xie et al., 2025a) and NOVA (Deng et al., 2025b), as well as unified architectures: Mogao (Liao et al., 2025), Bagel (Deng et al., 2025a), OmniGen2 (Wu et al., 2025b) and Show-o2 (Xie et al., 2025d). Through joint modeling of discrete text and visual tokens, URSA demonstrates strong text-image alignment. For example, on the DPG-Bench, URSA reaches leading overall score with dense text prompts. This strong performance is consistently sustained on the GenEval when using the rewritten prompts. Table 2: Image-to-video evaluation on VBench++. To evaluate temporal consistency, we focus on image-to-video (I2V) metrics of visual similarity between each generated frame and reference image. Model #params #videos Total Score Quality Score I2V Score Dynamic Degree Aesthetic Quality Imaging Quality Camera Motion I2V Subject Consistency I2V Background Consistency Continuous models ConsistI2V SEINE DynamiCrafter 2B 3B 2B Cosmos VideoMAR CogVideoX HunyuanVideo Wan2.1 Pusa Step-Video MAGI-1 Discrete models Lumos-1 URSA 13B 1.4B 5B 13B 14B 14B 30B 24B 3.6B 1.7B 10M 25M 10M 100M 0.5M - - - - - - 10M 24M 84.1 85.5 86.9 84.2 84.8 86.7 86.8 86.9 87.3 88.4 89. 84.7 86.2 76.2 78.4 80.5 75.8 75.6 78.6 78.5 80.8 79.8 81.2 82.4 76. 79.8 91.9 92.7 93.5 92.6 94.0 94.8 95.1 92.9 94.8 95.5 96.1 93.3 92. 18.6 27.1 69.7 18.7 11.0 33.2 22.2 51.4 52.6 48.8 68.2 - 65.3 59.0 64.6 60. 55.8 55.8 61.9 62.6 64.8 63.2 62.3 64.7 - 57.4 66.9 71.4 68.6 59.9 62. 70.0 70.1 70.4 68.3 70.4 69.7 69.2 64.2 33.9 21.0 31.2 25.4 21.6 67.7 49.9 34.8 29.5 49.2 50. - 37.6 95.8 97.2 97.2 96.0 97.9 97.2 98.5 97.0 97.6 97.9 98.4 97. 96.1 96.0 97.0 97.4 97.4 98.4 96.7 97.4 96.4 99.2 98.5 99.0 97.4 96. At high resolutions, URSA surpasses the autoregressive (Wang et al., 2024b; Han et al., 2025; Chen et al., 2025b) and masked diffusion (Bai et al., 2025; Yuan et al., 2025) approaches in efficiency, effectively reducing inference steps through iterative refinement while preserving fine-grained detail. Table 3: Text-to-image evaluation on DPG-Bench and GenEval. We prefer the DPG-Bench metrics to mitigate potential prompt template leakage concerns (Xie et al., 2025b) associated with GenEval. refers to the methods using rewritten GenEval prompts for clearer position and attribute guidance. Model Continuous models SDXL SD3 FLUX.1-dev NOVA SANA-1.5 OmniGen2 Mogao Bagel Show-o2 Discrete models Show-o Emu3 FUDOKI Janus-Pro Meissonic Lumos-1 Infinity URSA (512320) URSA (10241024) URSA (10241024) ModelSpec DPG-Bench GenEval #params #images Overall Entity Attribute Relation Overall Single Two Counting Colors Position ColorAttr 2.6B 2B 12B 1.4B 4.8B 7B 7B 14B 7B 1.3B 8B 1.5B 7B 1B 3.6B 2B 1.7B 1.7B 1.7B - - - 600M 50M - - - 66M 2B - 13M 72M 210M 60M - 30M 30M 30M 74.7 84.1 84.9 83.0 84.7 83.6 84.3 85.1 86.1 67.3 81.6 83.6 84.2 - - 83.5 82.5 86.0 - 82.4 91.0 - 88.7 - 88.8 90.0 90.4 91.8 75.4 87.2 89.7 88.9 - - - 88.3 91.5 - 80.9 88.8 - 86.4 - 90.2 88.3 91.3 90. 78.0 86.3 88.1 89.4 - - - 86.4 89.6 - 86.8 80.7 - 91.9 - 89.4 93.2 90.8 91.8 84.5 90.6 93.7 89. - - 90.8 92.9 94.7 - 0.55 0.62 0.68 0.71 0.81 0.80 0.89 0.82 0.76 0.68 0.66 0.77 0.80 0.54 0.66 0. 0.64 0.68 0.80 0.98 0.98 0.99 0.99 0.99 1.00 1.00 0.99 1.00 0.98 0.99 0.96 0.99 0.99 0.95 0.99 0.99 0.99 1. 0.44 0.74 0.85 0.91 0.93 0.95 0.97 0.94 0.87 0.80 0.81 0.85 0.89 0.66 0.80 0.85 0.83 0.92 0.92 0.39 0.63 0.74 0.62 0. 0.64 0.83 0.81 0.58 0.66 0.42 0.56 0.59 0.42 0.46 0.64 0.47 0.63 0.64 0.85 0.67 0.79 0.85 0.84 0.88 0.93 0.88 0. 0.84 0.80 0.88 0.90 0.86 0.81 0.84 0.83 0.86 0.89 0.15 0.34 0.21 0.33 0.59 0.55 0.84 0.64 0.52 0.31 0.49 0.68 0. 0.10 0.48 0.49 0.30 0.25 0.67 0.23 0.36 0.48 0.56 0.65 0.76 0.80 0.63 0.62 0.50 0.45 0.67 0.66 0.22 0.48 0. 0.41 0.40 0.69 4.3 ABLATION STUDY Effectiveness of iterative refinement for visual generation. Discrete diffusion models inherently incur elevated sampling errors, as exhibited in prior studies (Tang et al., 2022; Feng et al., 2025). To systematically investigate this issue in image and video generation, we train three variants of the discrete diffusion model, assessing performance across insufficient and excessive sampling regimes. Figure 4 compares key performance metrics of text-to-image models on GenEval and text-to-video models on VBench, with all models evaluated after being trained for an identical number of iterations. In the image generation task, which is characterized by low structural redundancy, all three models can generate feasible images within the conventional 25 inference steps. Without iterative refinement, reducing the number of steps substantially decreases the GenEval score in masked diffusion sampling. As we progress into video generation, task rich in contextual redundancy, it becomes essential to correct sampling errors at each step, ensuring temporal coherence and visual fidelity across frames. 8 (a) Text-to-image performance on GenEval. (b) Text-to-video performance on VBench. Figure 4: Sampling performance across inference steps. Using the Cosmos tokenizer, we evaluate the image samples at 256256 (1K tokens) and the video samples at 25384240 (10K tokens). Effectiveness of path linearity for uniform diffusion. As shown in Figure 5, the left plot shows the average Euclidean distance between the embedding of noisy images and the clean image using 10K images sampled from the training set. We compute the Pearson correlation coefficients between the Euclidean distance and the timestep, which are -0.995, -0.921, -0.997, and -0.949. We find that the choice of the probability path is significantly influenced by the values of and α, which has substantial impact on the model performance. To determine the optimal values for and α, we draw inspiration from the continuous diffusion model SD3 (Esser et al., 2024), where the relationship between and d(xt, x1) demonstrates strong linear correlation. This insight guides our approach to calibrating and α to effectively reach the limits of model performance for different vision tokenizers. Figure 5: Sampling performance of different paths. We evaluate the image samples at 256256. Effectiveness of model size for uniform diffusion. To study the scaling properties of URSA models, we train three variants that are initialized from Qwen3 models with 0.6B, 1.7B, and 4B parameters. Figure 6 compares the performance of different model sizes on DPG-Bench, GenEval, and VBench, with all models trained for the same epoch count as in Sec. 4.2. We find that increasing model size considerably enhances semantic performance across both text-to-image and text-to-video evaluations but does not significantly improve generation quality. This suggests that while larger models better capture high-level semantics and align more accurately with text prompts, the fidelity of the generated outputs may ultimately be constrained by the representation capacity of the discrete vision tokenizer. Effectiveness of timestep conditioning for uniform diffusion. Recent work explores time-agnostic (i.e., noise-unconditional) methods for both continuous diffusion (Sun et al., 2025; Tang et al., 2025) and masked diffusion (Zheng et al., 2025; Ou et al., 2025), effectively narrowing the architectural gap between diffusion transformers (DiTs) and LLMs. In this context, we analyze whether timestep conditioning remains indispensable for uniform diffusion. The results are illustrated in Figure 7. Figure 6: Sampling performance of different model sizes. All models are trained for the same epoch count as in the main experiments and evaluated on 256256 images and 25384240 videos. Figure 7: Model metrics across training iterations. We sample 256256 images for evaluation. Specifically, we train three model variants with distinct conditioning strategies and evaluate GenEval across training iterations. After one epoch (30K iterations), embedding or prompting with the timestep provides no measurable benefit. Notably, timestep embedding could degrade performance as their variance increases, potentially disrupting token embedding and compromising training stability. Figure 8: Timestep shifting across SNR schedules. We sample 25384240 videos for evaluation. 10 Effectiveness of timestep shifting for video generation. As outlined in Section 4.1, our probability path is designed to maximize the time interval. In line with continuous models (Kong et al., 2024; Wang et al., 2025a; Liu et al., 2025a), the optimal SNR schedule should be tailored with video size. To study the impact of the SNR schedule on video generation, we train four text-to-video models with divergent timestep shifting and evaluate their performance using the respective value on VBench. Figure 8 presents our shifting schedules, accompanied by their evaluation metrics and visualizations. Surprisingly, the shifting strategy proposed by Esser et al. (2024) demonstrates strong effectiveness for uniform diffusion, empowering URSA to match the performance of its continuous counterparts."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we revisited discrete generative modeling for video synthesis and introduced URSA, uniform diffusion framework with metric path that bridges discrete and continuous paradigms. URSA employs two key innovations, linearized metric path and resolution-dependent timestep shifting strategy, to provide fine-grained control over perturbations for long sequences. On top of this, our asynchronous temporal scheduling strategy enables multi-task video generation in one model. Extensive experiments show that URSA not only consistently outperforms existing discrete methods but also achieves highly competitive results compared to state-of-the-art continuous diffusion models. We contend that this work represents significant step toward unifying discrete and continuous paradigms and provides promising direction for scalable, versatile, and efficient video generation. Acknowledgements. This work was supported in part by the National Natural Science Foundation of China (No. 62320106010, No. U21B2042). We thank Jin Wang and Jiahao Wang (HKU) for their help with algorithm implementation and early feedback. We are grateful to Zhengxiong Luo for insightful comments on the algorithm design during the initial phase. We also thank Yuanzhi Zhu ( Ecole Polytechnique), Shilin Lu (NTU), Honghao Chen (CASIA) and Jinming Wu for their helpful suggestions and editorial improvements on the manuscript. We appreciate the stimulating discussions with Chengyuan Wang (BAAI-Vision) and acknowledge the support from other colleagues at BAAI."
        },
        {
            "title": "REFERENCES",
            "content": "Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Xiangtai Li, Zhen Dong, Lei Zhu, and Shuicheng Yan. Meissonic: Revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis. In ICLR, 2025. Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Kelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, Zach Eaton-Rosen, et al. Imagen 3. arXiv preprint arXiv:2408.07009, 2024. Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions, 2023. Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. In ICLR, 2019. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators, 2024. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020. Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset, 2022. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In CVPR, 2022. Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. In ICML, 2023. Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. In NeurIPS, 2024a. Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, et al. Skyreels-v2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025a. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025b. Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and prediction. In ICLR, 2024b. Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In ICML, 2023. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025a. 12 Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. In ICLR, 2025b. Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, and Xinlong Wang. Unveiling encoder-free vision-language models. In NeurIPS, 2024. Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. In NeurIPS, 2021. Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014. Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In ICLR, 2017. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. Weichen Fan, Chenyang Si, Junhao Song, Zhenyu Yang, Yinan He, Long Zhuo, Ziqi Huang, Ziyue Dong, Jingwen He, Dongwei Pan, et al. Vchitect-2.0: Parallel transformer for scaling up video diffusion models. arXiv preprint arXiv:2501.08453, 2025. Guhao Feng, Yihan Geng, Jian Guan, Wei Wu, Liwei Wang, and Di He. Theoretical benefit and limitation of diffusion language model. In NeurIPS, 2025. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. In NeurIPS, 2024. Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025a. Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025b. Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky TQ Chen, Gabriel Synnaeve, Yossi Adi, and Yaron Lipman. Discrete flow matching. In NeurIPS, 2024. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. In NeurIPS, 2024. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014. Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. In CVPR, 2025. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. 13 Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, 2024a. Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, et al. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503, 2024b. Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong MU, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. In ICLR, 2025. Nal Kalchbrenner, Aaron Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu. Video pixel networks. In ICML, 2017. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In CVPR, 2020. Diederik Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. In ICML, 2024. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Kuaishou. Kling ai, 2024. URL https://klingai.com. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In NeurIPS, 2024. Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model for interleaved multi-modal generation. arXiv preprint arXiv:2505.05472, 2025. Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. Dongyang Liu, Shicheng Li, Yutong Liu, Zhen Li, Kai Wang, Xinyue Li, Qi Qin, Yufei Liu, Yi Xin, Zhongyu Li, et al. Lumina-video: Efficient and flexible video generation with multi-scale next-dit. arXiv preprint arXiv:2502.06782, 2025a. Yaofang Liu, Yumeng Ren, Aitor Artola, Yuxuan Hu, Xiaodong Cun, Xiaotong Zhao, Alan Zhao, Raymond Chan, Suiyun Zhang, Rui Liu, et al. Pusa v1.0: Surpassing wan-i2v with $500 training cost by vectorized timestep adaptation. arXiv preprint arXiv:2507.16116, 2025b. Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In ICLR, 2017. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: VQ-VAE made simple. In ICLR, 2024. Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NeurIPS, 2017. 14 Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. In ICLR, 2025. Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, et al. Opensora 2.0: Training commercial-level video generation model in $200k. arXiv preprint arXiv:2503.09642, 2025. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al."
        },
        {
            "title": "Improving language",
            "content": "understanding by generative pre-training. OpenAI Blog, 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI Blog, 2019. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021. Scott Reed, Aaron Oord, Nal Kalchbrenner, Sergio Gomez Colmenarejo, Ziyu Wang, Yutian Chen, Dan Belov, and Nando Freitas. Parallel multiscale autoregressive density estimation. In ICML, 2017. Weiming Ren, Huan Yang, Ge Zhang, Cong Wei, Xinrun Du, Wenhao Huang, and Wenhu Chen. Consisti2v: Enhancing visual consistency for image-to-video generation. TMLR, 2024. Neta Shaul, Itai Gat, Marton Havasi, Daniel Severo, Anuroop Sriram, Peter Holderrieth, Brian Karrer, Yaron Lipman, and Ricky T. Q. Chen. Flow matching with general discrete paths: kinetic-optimal perspective. In ICLR, 2025. Fengyuan Shi, Zhuoyan Luo, Yixiao Ge, Yujiu Yang, Ying Shan, and Limin Wang. Scalable image tokenization with index backpropagation quantization. In ICCV, 2025. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, Limin Wang, and Hongsheng Li. JourneyDB: benchmark for generative image understanding. In NeurIPS, 2023. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024a. Qiao Sun, Zhicheng Jiang, Hanhong Zhao, and Kaiming He. Is noise conditioning necessary for denoising generative models? In ICML, 2025. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In CVPR, 2024b. Bingda Tang, Boyang Zheng, Sayak Paul, and Saining Xie. Exploring the deep fusion of large language models and diffusion transformers for text-to-image synthesis. In CVPR, 2025. Zhicong Tang, Shuyang Gu, Jianmin Bao, Dong Chen, and Fang Wen. Improved vector quantized diffusion models. arXiv preprint arXiv:2205.16007, 2022. Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. 15 Unsplash. Unsplash dataset, 2020. Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025a. Jin Wang, Yao Lai, Aoxue Li, Shifeng Zhang, Jiacheng Sun, Ning Kang, Chengyue Wu, Zhenguo Li, and Ping Luo. Fudoki: Discrete flow-based unified understanding and generation via kineticoptimal velocities. In NeurIPS, 2025b. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, et al. Koala-36m: large-scale video dataset improving consistency between fine-grained conditions and video content. In CVPR, 2025c. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024b. Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, and Xihui Liu. Loong: Generating minute-level long videos with autoregressive language models. arXiv preprint arXiv:2410.02757, 2024c. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025b. Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng YU, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, Bingchen Liu, Daquan Zhou, and Song Han. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. In ICML, 2025a. Ji Xie, Trevor Darrell, Luke Zettlemoyer, and XuDong Wang. Reconstruction alignment improves unified multimodal models. arXiv preprint arXiv:2509.07295, 2025b. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. In ICLR, 2025c. Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models. In NeurIPS, 2025d. Jinbo Xing, Menghan Xia, Yong Zhang, Hao Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In ECCV, 2024. Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan.Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. In ICLR, 2025b. 16 Hu Yu, Biao Gong, Hangjie Yuan, DanDan Zheng, Weilong Chai, Jingdong Chen, Kecheng Zheng, and Feng Zhao. Videomar: Autoregressive video generatio with continuous tokens. In NeurIPS, 2025. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. TMLR, 2022. Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In CVPR, 2023. Hangjie Yuan, Weihua Chen, Jun Cen, Hu Yu, Jingyun Liang, Shuning Chang, Zhihui Lin, Tao Feng, Pengwei Liu, Jiazheng Xing, et al. Lumos-1: On autoregressive video generation from unified model perspective. arXiv preprint arXiv:2507.08801, 2025. Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling. In ICLR, 2025. Yuanzhi Zhu, Xi Wang, Stephane Lathuili`ere, and Vicky Kalogeiton. Di[m]o: Distilling masked diffusion models into one-step generator. In ICCV, 2025."
        },
        {
            "title": "APPENDIX",
            "content": "We have published code and pre-trained models to improve interpretability and ensure reproducibility. In this appendix, implementation details, experiments, and qualitative results are organized as follows: Training and Sampling Details (Sec. A) Video extrapolation experiments (Sec. B) Start-End frame control experiments (Sec. C)"
        },
        {
            "title": "A TRAINING AND SAMPLING DETAILS",
            "content": "Algorithm 1 URSA Training Require: Predictor pθ, Shift λ 1: repeat 2: 3: 4: 5: x1 pdata U(0, 1) t/(t + λ(1 t)) xt pt1(x1) (cid:80)D θ θ ηθL 6: 7: 8: until converged 9: return Trained predictor pθ i=1 log pθ(xi time shift 1 xt) Algorithm 2 URSA Sampling Require: Predictor pθ, Steps , Shift λ, Visual vocabulary 1: Sample x0 Uniform(V) 2: for = 1 to do (k 1)/T 3: t/(t + λ(1 t)) 4: ˆx1 pθ(xt) 5: ut ut(x, zˆx1) 6: xt xt + ut 7: 8: end for 9: return x1 Generated discrete sample time shift Training. URSA is predictor, parametric model pθ(xt), that takes xt as input and refine all tokens simultaneously.We first encode images and videos into discrete latent tokens using pretrained tokenizer. For visual tokens, we adopt DFM training objective based on the probability path. At each iteration, we randomly sample timestep [0, 1] and use the metric path to obtain the noised tokens xt. Text prompts are tokenized using the Qwen3 tokenizer and embedded into the same semantic space. We concatenate text embeddings and noised visual tokens into unified sequence. The training objective is defined as the expected cross-entropy between the ground-truth visual token sequence and the models predicted distribution. We list the detailed training process in Algorithm 1. Sampling. This velocity field ensures that transitions occur only from state to state when is closer to x1 than z, i.e., d(x, x1) < d(z, x1). Using the distance metric and the time-dependent factor βt, the velocity guides the flow of particles in manner that is both kinetic-optimal and aligned with the underlying geometry of the state space. We list the complete sampling process in Algorithm 2."
        },
        {
            "title": "B VIDEO EXTRAPOLATION EXPERIMENTS",
            "content": "As URSA is trained by applying independent noise levels to each frame, it naturally lends itself to video extrapolation via sliding window. Specifically, new frames are generated sequentially, conditioned on the most recent 13 frames, thereby extending future predictions beyond the initial 49-frame context window. To effectively mitigate sampling errors in autoregressive video generation, we introduce small amount of noise into historical frames by resampling them at timestep = 0.9. Figure 9 presents the qualitative results for video of 481 frames, where the initial text-to-video segment is extended through 12 extrapolation steps, producing videos up to 10 the original length. START-END FRAME CONTROL EXPERIMENTS We evaluate URSA on the start-end frame control task, specialized form of video generation to prevent future predictions from drifting. Concretely, we extract sequence of frames from the video at 4-second intervals and place them sequentially at the beginning and the end of the context window. This setup enables the generation of video featuring coherent motion of both objects and cameras, preserving spatial relationships throughout the scene. We present the qualitative results in Figure 10. 18 Figure 9: Zero-shot video extrapolation. We extend the 4-second text-to-video result to 40 seconds. Figure 10: Zero-shot start-end frame control. The start-end frames are rendered with transparency."
        }
    ],
    "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Key Laboratory of Intelligent Information Processing, ICT, CAS",
        "National Laboratory of Pattern Recognition, CASIA",
        "University of Chinese Academy of Sciences",
        "Zhejiang University"
    ]
}