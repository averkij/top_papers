{
    "paper_title": "Faster Video Diffusion with Trainable Sparse Attention",
    "authors": [
        "Peiyuan Zhang",
        "Haofeng Huang",
        "Yongqi Chen",
        "Will Lin",
        "Zhengzhong Liu",
        "Ion Stoica",
        "Eric P. Xing",
        "Hao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D attention, even though most of the attention mass concentrates on a small subset of positions. We turn this observation into VSA, a trainable, hardware-efficient sparse attention that replaces full attention at \\emph{both} training and inference. In VSA, a lightweight coarse stage pools tokens into tiles and identifies high-weight \\emph{critical tokens}; a fine stage computes token-level attention only inside those tiles subjecting to block computing layout to ensure hard efficiency. This leads to a single differentiable kernel that trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of FlashAttention3 MFU. We perform a large sweep of ablation studies and scaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA reaches a Pareto point that cuts training FLOPS by 2.53$\\times$ with no drop in diffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention time by 6$\\times$ and lowers end-to-end generation time from 31s to 18s with comparable quality. These results establish trainable sparse attention as a practical alternative to full attention and a key enabler for further scaling of video diffusion models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 9 8 3 3 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Faster Video Diffusion with Trainable Sparse\nAttention",
            "content": "Peiyuan Zhang1 Haofeng Huang1 Yongqi Chen1 Will Lin1 Zhengzhong Liu2 Ion Stoica3 Eric P. Xing2 Hao Zhang1 1UC San Diego 2MBZUAI 3UC Berkeley"
        },
        {
            "title": "Abstract",
            "content": "Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D attention, even though most of the attention mass concentrates on small subset of positions. We turn this observation into VSA, trainable, hardware-efficient sparse attention that replaces full attention at both training and inference. In VSA, lightweight coarse stage pools tokens into tiles and identifies high-weight critical tokens; fine stage computes token-level attention only inside those tiles subjecting to block computing layout to ensure hard efficiency. This leads to single differentiable kernel that trains end-to-end, requires no post-hoc profiling, and sustains 85% of FlashAttention3 MFU. We perform large sweep of ablation studies and scalinglaw experiments by pretraining DiTs from 60M to 1.4B parameters. VSA reaches Pareto point that cuts training FLOPS by 2.53 with no drop in diffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention time by 6 and lowers end-to-end generation time from 31s to 18s with comparable quality. These results establish trainable sparse attention as practical alternative to full attention and key enabler for further scaling of video diffusion models."
        },
        {
            "title": "Introduction",
            "content": "Attention computation is the primary bottleneck when scaling video Diffusion Transformers (DiT) [34, 28]. Even seemingly short 5-second 720p clip unfolds into more than 100K tokens [29, 20] once flattened as sequence. Consequently, state-of-the-art video DiTs [20, 35, 43, 26] expend the majority of compute on attention when training on full-resolution, long-sequence data; the trained DiTs remain painfully slow at inference. Fortunately, recent studies [37, 47, 6, 46] reveal an inherent sparsity in DiTs trained using full attention: only tiny subset of entries in the attention matrix Softmax(QK / d), which we refer to as critical tokens, significantly influence outputs, while the vast majority approach zero. This inherent sparsity calls for developing native, trainable sparse attention mechanism purpose-built for video DiTs. Most prior work approaches sparsity as post-hoc speedup for pretrained DiTs rather than as firstclass training primitive. Sliding Tile Attention (STA) [47] and Sparge Attention [46], for example, begin with model trained under full attention and then substitute each head with fixed or profilederived sparse mask only at inference time [37, 38]. Because the sparsity pattern is decided after training, these methods leave the bulk of training cost untouched and introduce train-test mismatch: the DiT learns parameters in dense context yet is evaluated in sparse one. That mismatch caps best-case quality at the dense models ceiling and, in practice, often erodes quality once sparsity is pushed beyond gentle budget. Consequently, state-of-the-art DiTs still default to quadratic 3D attention despite its prohibitive cost [43, 35, 20, 2, 11]. Designing trainable sparse attention for video DiTs faces fundamental chicken-and-egg dilemma: identifying critical token positions traditionally requires computing the full attention matrix, which *Equal contribution. Work performed during an internship at UC San Diego. then erases any computational gains and defeats the purpose of sparse attention. Conversely, resorting to cheap heuristics and not precisely identifying critical tokens may miss high-weight regions and yield suboptimal results. More importantly, any practical attention implementation must honor the block-sparse layouts expected by modern GPU kernels such as Flash Attention (FA) [5] otherwise theoretical savings do not translate to wall-clock speedup. The central research question is therefore: how can we predict critical tokens accurately, subject to hardware-aligned block structure, without paying the quadratic cost we aim to avoid? This paper presents VSA (Video Sparse Attention), trainable, hardware-aligned sparse attention mechanism for video DiTs, drawing inspiration from recent developments in large language models [44, 25, 30]. At the high level, VSA is hierarchical granular attention, illustrated in Figure 1. The coarse stage first aggregates cube containing (4, 4, 4) tokens into single representation to compute cube-to-cube dense attention. Since attention operates on pooled (short) sequence, it is lightweight, yet simultaneously predicts which cube contains critical tokens while modeling global context. fine stage then applies token-level attention only inside the top-K selected cube. The final output of attention combines both stages through differentiable gate. Because VSA is end-to-end trainable, it identifies critical tokens not by heuristics, but by learning from data. To ensure hardware efficiency, VSA is meticulously designed to map spatial-temporal cube to kernel-level tile 1 [47]. This ensures that tokens within cube are loaded on the same GPU SM and adhere to the block sparse compute layout (2.2). One critical parameter in VSA is the tile size. Small tiles let the coarse stage localize critical tokens close to token resolution, while the fine stage attends only to those pinpointed cubes. This sharpens sparsity and improves model quality at the cost of fragmenting work into tiny blocks, which hurts the GPU throughput. In contrast, large tiles boost arithmetic intensity, but the fine stage must then process whole cubes even if only few tokens inside the cubes matter, thus blurring sparsity (Figure 1 (b)). Another key parameter is whether to inject dedicated local or spatial-temporal patterns into the fine stage. Our systematic ablation studies reveal that an effective VSA configuration combines global coarse stage with freely-selectable fine stage. We found that tile size of 64 and 87.5% attention sparsity achieves performance comparable to full attention while maintaining efficient kernel execution. Furthermore, purposely injecting locality heuristics proved unnecessary. large sweep of scaling experiments, in which we pretrain video DiTs from scratch (60M to 1.4B parameters with up to 4 1021 FLOPS) with 16K sequence length, uncovers Pareto frontier where VSA achieves near 8 reduction in attention FLOPS and 2.53 reduction in total training FLOPS. To support VSAs hierarchical sparse attention, we prototype GPU kernel where the coarse stage kernel fuses softmax, Top-K selection, and block indexing into single pass, and the fine stage leverages block-sparse attention kernel based on FA [5]. This leads to our VSA implementation that retains 85% of FA3s MFU [31]. We further retrofit VSA into SoTA open source DiT, Wan2.1 1.3B [35], originally trained with full attention. This integration speedups attention time by 6x and reduces end-to-end inference latency from 31s to 18s (1.7x) on H100. As result, VSA enables the first video DiT where attention accounts for only 20% of runtime during both training and inference without quality degradation. To our best knowledge, VSA is the first trainable sparse attention approach that, based on extensive experiments totaling around 90k H200 hours, shows better scaling than full attention on DiTs. Finally, we hope that our explicit ablation of the key parameters (e.g., tile size, critial token prediction, locality prior, sparsity, etc.) will enable more targeted exploration of sparse attention in scaling video DiTs."
        },
        {
            "title": "2 Methods",
            "content": "This section introduces VSA, our sparse attention designed to reduce both training and inference costs of video DiTs. We begin by detailing the design space, key components, and core motivations behind sparse attention in 2.1. Although the final method is presented in 2.2, we emphasize that design exploration and ablation studies (deferred to 3) are central to understanding the effectiveness of VSA. Finally, 2.3 describes how to adapt VSA to pretrained Video DiTs originally trained with full attention. 1We use the word block and tile interchangeably in this paper. They refer to submatrix that GPU threadblock loads into SRAM when performing matrix multiplication. 2 Figure 1: Overview of VSA. (a) VSA introduce hierarchical attention with sparsity and different granularity (coarse & fine). (b) Larger tile sizes (left) blur attention pattern while small tiles let the coarse stage localize critical tokens close to token resolution. The red dots indicate critical tokens. (c) An illustration of (2, 2, 2) cube partition (in practice we use (4, 4, 4) in VSA). 2.1 Sparse Attention Design Space Modern video DiTs use 3D full attention to capture dependencies across the entire video volume. Given video latent of shape (T, H, ), it is flattened into 1D sequence of length = HW by mapping each token location (t, w, h) in the 3D latent to its position in the 1D sequence following = tHW + hW + w. Then full attention is applied across the entire 1D sequence, allowing each token to interact with all the others. Let Q, K, RLd denote the query, key, and value matrices for single attention head; let {, 0}LL denote the attention mask specifying the allowed connections between each pair of tokens. The attention output is then calculated as: = QK dk , = Softmax(S + M), = AV Block Size vs. Hardware Efficiency. In full attention, all entries in are zero. Sparse attention introduces entries in and theoretically reduces the total FLOPS by allowing the computation of the corresponding elements in both QK and AV to be skipped. However, modern accelerators are optimized for dense computation, making unstructured sparsity ineffective for real speedup. Blocksparse attention [5] addresses this by structuring the sparsity to align with the hardware capabilities. In this approach, the attention mask is divided into tiles of size (Bq, Bk) 2, with all entries of each tile sharing the same value. This enables each tile in GPU SM to be processed as dense block or skipped entirely, maximizing hardware efficiency. The tile size is key design parameter: smaller tiles allow flexible, fine-grained sparsity but are less efficient on hardware, while larger tiles improve throughput but restrict the model to coarser attention patterns, potentially reducing modeling expressiveness (see Figure 1 (b)). Thus, selecting involves tradeoff between expressiveness and efficiency. In practice, we find that modest reductions in speed can be acceptable if they yield significant improvements in generation quality. Prediction Cost vs. Coverage Quality in Critical Token Selection. Recent studies have shown that the attention score matrix is naturally sparse [37, 47, 6, 46], with most values close to zero. This suggests that, by constructing mask that preserves only the high-value regions of the so-called critical tokens we can closely approximate full attention while significantly reducing computation. central design choice is how much computation to spend identifying these critical tokens. Computing full attention scores provides the most accurate selection, but largely negates 2Technically (Bq, Bk) can be non-square with different values for Bq and Bk. To keep the notation simple, we assume that = Bq = Bk. 3 computational savings, as only the AV operation benefits from sparsity. In contrast, fixed patterns (e.g., windowed or spatiotemporal attention) incur no prediction cost but often miss informative tokens. Inspired by NSA [44] and MoBA [25], we propose lightweight, trainable, coarse-granular attention module to estimate the locations of critical tokens without fully computing A. The main challenge is to balance prediction accuracy with computational efficiency for practical use in DiT architectures. Maintaining Global and Local Context in Sparse Attention. key challenge with sparse attention is its restricted receptive field, which can limit the models ability to capture global context. One approach to address this is to augment sparse attention with lightweight global module that captures coarse global signals. Conversely, incorporating local contextmotivated by the locality priors commonly used in vision models such as CNNscan also poteantially enhance feature learning. We empirically ablate both strategies to assess their impact on video generation quality in 3.1. 2.2 VSA: Video Sparse Attention VSA employs cube-based partitioning strategy followed by two-stage attention mechanism to efficiently process video latents. The method first divides the input video latent into spatially and temporally contiguous cubes, then processes these cubes through coarse stage that identifies important regions and fine stage that performs detailed token-level attention within these regions. This design enables efficient computation while maintaining the ability to capture both global and local dependencies in the video data. Given video latent with shape (T, H, ), VSA divides it into multiple cubes, each with shape (Ct, Ch, Cw) (Figure 1 (c)). VSA co-designs the sparse attention algorithm and its kernel implementation by mapping each cube in the video latent into single tile on GPU SM, where the tile size = Ct Ch Cw. We assume the video latent shape (T, H, ) is an integer multiple of the (cid:17) , tile size and define (Nt, Nh, Nw) = . When flattening the 3D video latent into 1D Cw sequence, each token at position (t, h, w) is assigned 1D index using the following mapping: (cid:16) Ct , Ch = (cid:23) (cid:18)(cid:22) Ct NhNw + (cid:23) (cid:22) Ch Nw + (cid:23)(cid:19) (cid:22) Cw + (t mod Ct)ChCw + (h mod Ch)Cw + (w mod Cw). Building on this cube-based partitioning, VSA implements its two-stage attention mechanism to efficiently predict critical token locations without computing the full attention matrix A, as shown in Figure 1 (a). In the coarse stage, we apply mean pooling over each (Ct, Ch, Cw) cube to obtain cube-level representations, producing Qc, Kc, Vc d. This stage then computes attention scores Ac and outputs Oc. The attention mask is derived by selecting the Top-K entries per row in Ac and setting others to , followed by broadcasting to full-resolution mask of size L. This mask naturally conforms to block-sparse structure because the coarse stage operates on cube-level representations. When broadcasting the mask from to L, each selected entry in Ac expands into block in M3. This block-sparse pattern is crucial for hardware efficiency as it allows the next stage to process attention in contiguous blocks that align with GPU memory access patterns and enable efficient parallel computation. Next, in the fine stage, this mask guides fine-grained attention computation for Q, K, RLd, yielding output Of . Finally, outputs from both stages are combined to obtain the final output O: = Oc Gc + Of Gf where Gc and Gf are gating vectors obtained from linear projections of the input hidden states. Since the coarse stage introduces negligible computational cost (less than 1% of total FLOPS), the overall sparsity can be approximated by KB . However, due to the row-wise Top-K selection, FlashAttention cannot be directly applied to the coarse stage, resulting in increased latency. 2.4 discusses how we mitigate this overhead. Appendix gives pseudo code implementation of VSA . In 3.1 and 3.2, we show that using smaller leads to more expressive sparse attention pattern and improved performance, but comes at the cost of slower attention kernel execution. Setting = 64 with (Ct, Ch, Cw) = (4, 4, 4) provides favorable trade-off between expressiveness and 3In practice we do not broadcast to full-resolution mask but only input the selected block index to fine stage. 4 efficiency. We further demonstrate that combining coarse stage for global context with fine stage for token-level sparse attention is both necessary and sufficientdedicated modules for local context modeling offer minimal additional benefit. Setting = 32 consistently yields strong performance across wide range of sequence lengths. VSA adopts these hyperparameters as default. 2.3 Sparse Adaptation VSA is designed to train video DiTs from scratch, reducing both training and inference FLOPS. It can also be adapted to pretrained video DiTs originally trained with full attention. However, directly replacing full attention with VSA leads to unstable training. We hypothesize two main reasons: (1) the gating projection weights are not present in the full attention checkpoint and are randomly initialized, and (2) VSA differs significantly from full attention, introducing coarse stage and sparsity in the fine stage. To address this, we develop an annealing strategy that smoothly transitions the model from full attention to VSA. We initialize the weights of the coarse gate Gc to zero, and remove the fine gate Gf (equivalent to Gf = 1). We also initialize the sparsity level by setting = , effectively making VSA equivalent to full attention at the start of training. As training progresses, we gradually reduce to the target sparsity level. Meanwhile, Gc is updated through training, enabling the model to learn how to balance the contributions of the coarseand fine stages. 2.4 Kernel Implementation VSA requires implementing both forward and backward kernels. We write block-sparse attention kernel with ThunderKittens [32] for fine stage. Despite using relatively small tile size of 64, our kernel achieves over 85% of FAs MFU (3.4). The coarse stage, illustrated in Figure 1, requires row-wise Top-K selection over the cube-level attention matrix. This step necessitates materializing the attention matrix, which precludes direct use of FA-style fused kernels. One possible workaround is to modify the FA kernel to incorporate in-kernel bitonic sorting for Top-K, thereby avoiding materialization. However, such fusion demands intrusive kernel rewriting and careful tuning. We instead ask: is this complexity necessary? For VSA, the coarse stage operates on (4, 4, 4) cubes, reducing sequence length by 64, e.g., 100K-token sequence reduces to 1.5K. At this scale, the memory overhead from materialization is negligible. FLOPS-wise, the coarse stage contributes less than 0.2% of total attention compute, and we show in 3.4 that its runtime accounts for only 14%, even when the fine stage is 87.5% sparse. This makes further kernel fusion unnecessary. Nonetheless, we still make some efforts to speed up the coarse stage. Our block-sparse kernel consumes block indices, not binary masks. Therefore, converting the Top-K mask into index form incurs additional overhead. To mitigate this, we fuse softmax, Top-K selection, and mask-to-index conversion into single kernel. This fused kernel reduces coarse stage runtime modestly(3.4C)."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Ablation Studies We conduct extensive pretraining experiments to ablate various design choices. Our experiments are based on the Wan2.1 model architecture, state-of-the-art open-source video DiT. Unless otherwise specified, we train models with 120M parameters from scratch for 4.5 1020 FLOPS using video latents of shape (16, 32, 32) from the Vchitect-T2V-Dataverse dataset [10]. We determined 4.51020 to be compute-optimal for our setup by following established scaling laws [15, 19]: when comparing models of different sizes under fixed compute budgets, we observe that 120M parameter model with full attention trained at 4.5 1020 FLOPS achieves better performance than smaller models (60M) with the same compute budget, while increasing compute beyond this point yields diminishing returns. For VSA and its variants, we set the number of selected KV tiles to 32, achieving an attention sparsity of approximately 87.5%. Detailed experimental setup and the rationale behind our experiment design can be found in Appendix B. Key findings from these ablations are presented below. Data-Dependent Trainable Sparsity Wins Over Fixed-Patterns. We first investigate why full attention predominates despite sparse alternatives. Table 1a shows existing sparse methods (Exp 1-4) outperform full attention (Exp 5) with compute-optimal training budget (4.5 1020 FLOPS), but this advantage reverses with extended training (4 1021 FLOPS). VSA (Exp 6) outperforms both 5 1 2 3 4 5 6 256 128 64 16 Exp ID Case Loss Opt Loss Over Exp ID Case Loss Compress KV Spatial Temporal Spatial Full Strided Window Full VSA 0.15281 0.13574 0.13555 0.13271 0.13877 0.13162 0.14282 0.13034 0.12811 0.12716 0.12703 0.12687 7 8 9 10 11 12 13 0.13330 (no Oc) 0.13296 0.13220 & 0.13162 & & & 0.13194 & & & 0.13124 0.13192 & (F + L) (a) VSA v.s. other attention. (b) Different attention design. (Ct, Ch, Cw) Exp ID Pooling Tile TFLOPS Loss (4, 8, 8) (4, 8, 4) (4, 4, 4) (2, 4, 2) 14 15 16 17 18 256x256 128x128 64x64 64x64 16x64 256x256 128x128 256x256 64x64 16x64 478 444 478 408 181 0.13375 0.13244 0.13328 0.13162 0.13155 (c) to (Ct, Ch, Cw). (d) Tile size ablation. Exp ID Pooling Loss 19 20 21 Conv Max Avg 0.27787 0.13929 0. (e) Critical token prediction study. Table 1: Ablation results for key design parameters of VSA. previous fixed-pattern methods and full attention. To understand why, in Table 1b(b) we examine two key factors: pattern type and stage contributions. We compare data-dependent patterns against fixed local patterns (\"L\") that use (3, 3, 3) window. Simultaneously, we investigate the impact of the coarse stage by including or excluding its output Oc (denoted as \"C\") from the final attention output, versus using only the fine stage output (denoted as \"F\"). Data-dependent patterns consistently outperform fixed patterns, both without the gated residual (Exp 7 vs. 8) and with it (Exp 9 vs. 10), demonstrating the inherent advantage of adaptive sparsity and the gated residual. Global Information is Necessary; Locality Priors Offer Limited Gains. We tested three approaches for incorporating local context: (1) adding separate local stage for (3, 3, 3) window attention (Exp 11), (2) explicitly excluding (\"E\") cubes selected by the local stage from the fine stage (Exp 12), and (3) forcing the fine stage to include local cubes, without separate local stage (Exp 13). All three variations performed similarly to the simpler & architecture, indicating that explicit local modeling provides minimal benefit. VSA therefore adopts the simpler & architecture (Exp 10), which effectively captures both global and local information. Finegrained Attention Map Leads to Better Performance But Slower Kernel. As analyzed in Section 2.1, the tile size in VSA is critical hyperparameter that balances computational efficiency against model performance. It directly affects two key aspects: (1) how accurately critical-tokens can be identified through the coarse stages cube size, and (2) the granularity of attention in the fine stage. Hardware constraints partially dictate this parameterNVIDIA Hopper GPUs optimize for matrix multiplications with dimensions divisible by 16, and smaller tiles generally reduce arithmetic intensity. Our benchmarks in Table 1d show that decreasing tile size from 256 256 to 64 16 significantly reduces Model FLOPS Utilization (MFU). Training with various tile sizes (Table 1c) reveals smaller tiles consistently reduce model loss through finer attention granularity. This improvement stems from the finer granularity allowing the coarse stage to more accurately predict critical-token cubes and the fine stage to focus attention on smaller, more relevant regions. Experiment 16 specifically tests mismatched granularity between stages, confirming both coarse and fine stage granularity significantly impact performance. Here, the coarse stage used smaller pooling cubes (Ct, Ch, Cw) = (4, 4, 4) (effectively = 64) while the fine stage operated on larger tiles corresponding to (Ct, Ch, Cw) = (4, 8, 8) (effectively = 256). To reconcile the finer granularity of the coarse stages predicted attention map with the coarser blocksparse attention in the fine stage, we applied an additional (1, 2, 2) average pooling before selecting top-K entries. Balancing these findings, we selected 64 64 tiles ((Ct, Ch, Cw) = (4, 4, 4)) as our default configuration. While 64 16 tiles offer slightly better performance, they run 2.26 slower (Exp 18 vs. 17), making this tradeoff unfavorable. Mean Pooling Is Sufficient. We also examined different pooling methods for the coarse stage. Table 1e shows average pooling outperforms both max pooling and convolutional approaches, with the latter causing training instability. 6 Figure 2: VSA scaling experiments. (a): Video DiT trained with VSA achieves similar loss curve compared to one trained with full attention. (b): VSA consistently produces better Pareto frontier when scaling model size up to 1.4B. (c) & (d): The optimal Top-K value (dictating sparsity) depends on both sequence length and training compute. larger is needed for larger training budget. 3.2 Scaling Studies To validate VSA, we pretrained 410M video DiT with with latent shape (16, 32, 32) (16,384 tokens), larger than our 120M ablation models. Figure 2(a) shows VSA achieves nearly identical loss to full attention despite 87.5% sparsity (K = 32 out of 256 cubes), while reducing attention FLOPS by 8 and end-to-end training FLOPS by 2.53. Further scaling experiments from 60M to 1.4B parameters (Figure 2(b)) confirm that VSA consistently produces better Pareto frontier than full attention. The parallel fitted curves indicate that VSA maintains its 2.53 FLOPS reduction across scales. Each model was trained with compute budgets up to 4 1021 FLOPS on 128 H200 GPUs with sequence length of 16K. To our knowledge, VSA is the first trainable sparse attention for video DiTs demonstrating superior performance compared to full attention under rigorous scaling evaluation. While we leave comprehensive scaling studies at longer sequence lengths to future work, our fine-tuned Wan 2.1 already shows 1.7 inference speedup at 23K sequence length (3.3). An important design question for VSA is determining the optimal sparsity level via the Top-K parameter. In Figure 2(c), we pretrained 120M models with varying sequence lengths under fixed 4.5 1020 FLOPS budget. Surprisingly, = 32 performs consistently well across sequence lengths of 8192, 16384, and 24675, but underperforms compared to = 16 at 61440 sequence length. This contradicts the conventional intuition that longer sequences require higher values. Further investigation with increased compute budget at 61440 sequence length (Figure 2(c)) reveals that = 32 eventually outperforms = 16 at 1 1021 FLOPS, with similar patterns at other lengths. These findings suggest that optimal depends on both sequence length and training budget. We hypothesize that the ideal Top-K increases with available compute, converging to full attention with infinite resources. However, precisely predicting optimal given budget, model size, and sequence length remains an open question. One promising direction is to explicitly model sparsity as an additional axis in the scaling law framework [19, 15], alongside model size and total FLOPS. Incorporating inference costs further complicates this analysis, as higher values may improve training loss but increase inference overhead. We leave comprehensive treatment of these tradeoffs to future work. 7 Model Qual. Sem. Total Ori-Wan Full f.t. VSA f.t. 83.33% 67.56% 80.18% 84.50% 69.89% 81.57% 83.89% 69.16% 80.95% Table 2: Wan-1.3B finetuning results on VBench. 3.3 Sparse Adaptation Figure 3: VSA vs. SVG human evaluation on 200 randomly sampled prompts from MovieGen-Bench [29]. SVG has 82.5% sparsity with (fp0.03, fl0.025, s0.1). To evaluate VSAs effectiveness, we further finetune Wan-1.3B with VSA on synthetic data generated by Wan-14B with video latent 16 28 52 . We set to 32, corresponding to 91.2% attention sparsity. As shown in Table 2, VSA achieves even higher VBench [16] score compared to the original Wan-1.3B. We hypothesize training with synthetic data from larger model may contributes to this boost. To ensure fair comparison, we also finetune Wan-1.3B using the same synthetic data. The results show that all models perform closely on VBench, indicating that VSA can retain generation quality despite significant attention sparsity. We additionally compared VSA to SVG [37], training-free attention sparsification method, under extreme sparsity. Figure 3 shows that VSA is preferred even though it has higher sparsity, demonstrating the effectiveness of training with sparse attention. The DiT inference time of Wan-1.3B drops from 31s (full attention with torch compile) to 18s with VSA. 3.4 Kernel Performance As Figure 4b shows, VSAs fine block sparse kernel approaches theoretical limit with nearly 7 speedup over FlashAttention-3 at long sequence lengths (85% MFU over FA3). Even after accounting for the coarse stage computations, VSA still maintains over 6 speedup. In contrast, FlexAttention [8] with an identical block-sparse mask (6464 block size) achieves only 2 speedup. Applying VSAs speedup to Wan-1.3B and Hunyuan brings 2-3inference speedup, as shown in Figure 4a. (a) (b) Figure 4: Kernel benchmarks. (a): Runtime breakdown of single transformer block for Wan1.3B and Hunyuan. VSA reduces the attention latency by 6. (b): Speed of VSA with fixed 87.5% sparsity under various sequence length with head dim 64. VSA approach the theorectical 8 speedup over FA3. 3. Inspecting VSA To gain deeper insight into VSAs mechanism, we inspect the block-sparse attention maps generated by the coarse stage of our finetuned 1.3B model. As illustrated in Figure 5(a-f), the predicted attention patterns are highly dynamic, confirming our hypothesis that effective sparse attention must be data-dependent rather than relying on predefined structures. Even within single layer, different attention heads often exhibit markedly distinct behaviors. Many observed patterns echo established heuristics, such as local attention focused on tokens near the query (akin to sliding tile attention), or spatial-temporal attention concentrating on tokens within the same frame (d), the same temporal-width plane (e), or the temporal-height plane. Conversely, some patterns deviate from simple heuristics, displaying highly global characteristics (b) or combination of local and global focus (c). 8 Figure 5: Visualization of the attention pattern of VSA. (a)-(f): VSA dynamically select different cubes to attend, where the blue cube indicates query and red cubes indicated selected key and values.(e): VSA critical-token prediction accuracy. We quantify the accuracy of critical token prediction calculated as the sum of attention scores within the top-32 cubes selected by the coarse stage. As baseline, random selection of 32 cubes from the 386 total (for (16, 28, 52) latent) captures only 8% of the attention score, as shown by the red plane in Figure 5(e). In stark contrast, VSA maintains high accuracy rate, consistently achieving at least 60% in most layers and timesteps, and reaching as high as 90% in some instances. This underscores VSAs strong capability in identifying critical tokens. Critically, even if the fine stage misses small portion of the attention weight, the direct output from the coarse stage can potentially compensate for this. Further examination of Figure 5 (e) reveals systematic variations in prediction accuracy. Accuracy tends to increase monotonically with the timestep. Across transformer layers, however, the accuracy rate exhibits zig-zag pattern. These accuracy dynamics across layers and timesteps suggest avenues for future optimizations with adaptive Top-K value."
        },
        {
            "title": "4 Qualitative Examples",
            "content": "We qualitatively illustrate the finetuning process (2.33) of Wan-1.3B in Figure 6. All frames are sampled from validation videos at selected training steps with = 32. At the start the training, the model exhibits noticeable artifacts when switching from full attention to VSA, reflecting the change in attention structure. As training progresses, the model gradually adapts to the sparse attention mechanism and recovers the ability to generate coherent videos."
        },
        {
            "title": "5 Related Work",
            "content": "Sparse Attention in LLMs. There had been proliferation of fixed-pattern sparse attention mechanisms in large language models [3, 45, 1, 7, 13]. In practice, however, most LLM training (over 90% of total FLOPs) happens on short sequences (32K tokens) under the train-short, adapt-long paradigm [40, 23, 12, 42], so sparse attention saw little uptake beyond sliding window attention variants like in Mistral [17]. With LLMs targeting contexts beyond 1M tokens, sparse attention has seen renewed interest. Recent work primarily targets inference-time speedups [39, 48, 18, 41], while the latest methods explore trainable, dynamic sparsity patterns (MoBA [25], NSA [44]) to enable efficient end-to-end training on extreme-length sequences. We draw inspirations from them, however, VSA differs from MoBA by directly contributing the coarse-grained attention output to the final representation and using smaller blocks compatible with efficient block-sparse kernels. Compared to NSA, the nature of video and bidirectional attention avoids grouped query constraints of attention pattern. We discuss similarities and difference in depth in D. Sparse Attention in Video DiTs. Recent work has explored applying sparse attention post-hoc to DiTs pretrained with full attention [47, 6, 46, 37, 14] at inference. However, we argue that the case for trainable sparse attention in video DiTs is both distinct from and more urgent than in LLMs. First, video DiT demands far longer sequences, e.g., 100K-token context yields only 5s video, making scaling inherently more costly than. Second, unlike LLMs, where long-context adapaption is small fraction of total training, state-of-the-art video DiTs [20, 35, 29, 33] dedicate most of their compute 9 Figure 6: Qualitative examples. In (a), we sample the same middle frame at each step of the video. In (b), we uniformly sample four frames across the video. budget to full-resolution, long-sequence training. As result, these models remain bottlenecked by quadratic attention at both training and inference. This calls for trainable sparse attention mechanisms, like VSA, as core design of video DiTs, not post-hoc fix. DSV [33] also explore adding sparsity to attention during DiT training, but their multi-stage and profiler-based design may complicate the training pipeline, which we further discuss in D."
        },
        {
            "title": "6 Limitation and Conclusion",
            "content": "We present VSA, trainable and hardware-efficient sparse attention tailored for scaling DiTs. Unlike prior work that applies sparsity post-hoc, VSA jointly learns to predict and apply attention sparsity at training and remains compatible with block-sparse compute layout. VSA currently operates with fixed cube size of (4, 4, 4), which requires video latent dimensions to be divisible by 4. While this may restrict the set of compatible resolutions, it can be addressed in practice by generating slightly larger latent and cropping to the target shape. Another open question is how to determine the optimal sparsity level. While our scaling experiments (3.2) provide preliminary insights, complete understanding may require extending scaling laws to explicitly account for sparsity, alongside model size and training compute. Across diverse model sizes (60M to 1.4B) and budgets (up to 4 1021 FLOPS), we show that VSA matches the performance of full attention at 2.53 lower training cost, and achieves 85% MFU of FA3. When integrated with Wan2.1-1.3B, it reduces end-to-end latency by 1.7. We hope this work establishes trainable sparse attention as practical and scalable alternative to full attention in further scaling video DiTs."
        },
        {
            "title": "References",
            "content": "[1] Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [2] Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, et al. Goku: Flow based video generative foundation models. arXiv preprint arXiv:2502.04896, 2025. [3] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [4] Hyung Won Chung, Noah Constant, Xavier Garcia, Adam Roberts, Yi Tay, Sharan Narang, and Orhan Firat. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. arXiv preprint arXiv:2304.09151, 2023. [5] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022. [6] Hangliang Ding, Dacheng Li, Runlong Su, Peiyuan Zhang, Zhijie Deng, Ion Stoica, and Hao Zhang. Efficient-vdit: Efficient video diffusion transformers with attention tile. arXiv preprint arXiv:2502.06155, 2025. [7] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486, 2023. [8] Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. Flex attention: programming model for generating optimized attention kernels. arXiv preprint arXiv:2412.05496, 2024. [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [10] Weichen Fan, Chenyang Si, Junhao Song, Zhenyu Yang, Yinan He, Long Zhuo, Ziqi Huang, Ziyue Dong, Jingwen He, Dongwei Pan, et al. Vchitect-2.0: Parallel transformer for scaling up video diffusion models. arXiv preprint arXiv:2501.08453, 2025. [11] Genmo. Mochi 1. https://github.com/genmoai/models, 2024. [12] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [13] Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. Longt5: Efficient text-to-text transformer for long sequences. arXiv preprint arXiv:2112.07916, 2021. [14] Ali Hassani, Fengzhe Zhou, Aditya Kane, Jiannan Huang, Chieh-Yun Chen, Min Shi, Steven Walton, Markus Hoehnerbach, Vijay Thakkar, Michael Isaev, et al. Generalized neighborhood attention: Multi-dimensional sparse attention at the speed of light. arXiv preprint arXiv:2504.16922, 2025. [15] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, pages 3001630030, 2022. [16] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 11 [17] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, et al. Mistral 7b, 2023. [18] Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. Advances in Neural Information Processing Systems, 37:5248152515, 2024. [19] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [20] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [21] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. [22] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations. [23] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. [24] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [25] Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, et al. Moba: Mixture of block attention for long-context llms. arXiv preprint arXiv:2502.13189, 2025. [26] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. [27] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. CoRR, 2024. [28] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [29] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, et al. Movie gen: cast of media foundation models, 2025. [30] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:5368, 2021. [31] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. Advances in Neural Information Processing Systems, 37:6865868685, 2024. [32] Benjamin Spector, Simran Arora, Aaryan Singhal, Daniel Fu, and Christopher Ré. Thunderkittens: Simple, fast, and adorable ai kernels. arXiv preprint arXiv:2410.20399, 2024. [33] Xin Tan, Yuetao Chen, Yimin Jiang, Xing Chen, Kun Yan, Nan Duan, Yibo Zhu, Daxin Jiang, and Hong Xu. Dsv: Exploiting dynamic sparsity to accelerate large-scale video dit training, 2025. 12 [34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [35] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [36] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. International Journal of Computer Vision, 133(5):30593078, 2025. [37] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, Jianfei Chen, Ion Stoica, Kurt Keutzer, and Song Han. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity, 2025. [38] Yifei Xia, Suhan Ling, Fangcheng Fu, Yujie Wang, Huixia Li, Xuefeng Xiao, and Bin Cui. Training-free and adaptive sparse attention for efficient long video generation, 2025. [39] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. [40] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 46434663, 2024. [41] Ruyi Xu, Guangxuan Xiao, Haofeng Huang, Junxian Guo, and Song Han. Xattention: Block sparse attention with antidiagonal scoring. arXiv preprint arXiv:2503.16428, 2025. [42] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [43] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. CoRR, 2024. [44] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. arXiv preprint arXiv:2502.11089, 2025. [45] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:1728317297, 2020. [46] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference, 2025. [47] Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhenghong Liu, and Hao Zhang. Fast video generation with sliding tile attention, 2025. [48] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023. [49] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. [50] Lei Zhu, Xinjiang Wang, Zhanghan Ke, Wayne Zhang, and Rynson WH Lau. Biformer: Vision transformer with bi-level routing attention. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1032310333, 2023."
        },
        {
            "title": "A Pseudocode of VSA",
            "content": "We provide pseudocode in pytorch-like API for easier understanding of VSA. def tile ( ) : return rearrange (x , \" ( n_t ts_t n_h ts_h n_w ts_w ) -> ( n_t n_h n_w ts_t ts_h ts_w ) \" , n_t =4 , n_h =8 , n_w =8 , ts_t =4 , ts_h =4 , ts_w =4) def untile ( ) : return rearrange (x , \" ( n_t n_h n_w ts_t ts_h ts_w ) -> ( n_t ts_t n_h ts_h n_w ts_w ) \" , n_t =4 , n_h =8 , n_w =8 , ts_t =4 , ts_h =4 , ts_w =4) , , , gate = tile ( ) , tile ( ) , tile ( ) , tile ( ) coarse_attn_gate , fine_attn_gate = gate . chunk (2 , dim =1) # Coarse stage , , , = . shape block = 64 topk = 32 q_c = . view (B , , // block , block , ) . mean ( dim =3) k_c = . view (B , , // block , block , ) . mean ( dim =3) v_c = . view (B , , // block , block , ) . mean ( dim =3) score = torch . matmul ( q_c , k_c . transpose ( -2 , -1) ) / ( ** 0.5) score = torch . nn . functional . softmax ( score , dim = -1) output_coarse = torch . matmul ( score , v_c ) output_coarse = output_coarse . view (B , , // block , 1 , ) . repeat (1 , 1 , 1 , block , 1) . view (B , , , ) # Keep only top - blocks topk_vals , topk_idx = score . topk ( topk , dim = -1) score = torch . zeros_like ( score ) score . scatter_ ( -1 , topk_idx , topk_vals ) score = score . view (B , , // block , // block , 1 , 1) . repeat (1 , 1 , 1 , 1 , block , block ) score = score . permute (0 ,1 ,2 ,4 ,5 ,3) . reshape (B , , , ) # Fine stage QK = torch . matmul (q , . transpose ( -2 , -1) ) / ( ** 0.5) QK = QK . masked_fill ( attn_mask == 0 , float ( \" - inf \" ) ) QK = torch . nn . functional . softmax ( QK , dim = -1) output_fine = torch . matmul ( QK , ) # Combine stages with residual connection hidden_states = output_coarse * coarse_attn_gate + output_fine * fine_attn_gate hidden_states = untile ( hidden_states ) . transpose (1 , 2) . flatten (2 , 3) Listing 1: Pseudocode of ViLAS in pytorch-like API with no kernel optimization, assuming cube size (4,4,4) and video size (16,32,32). Note that the tile and untile operation can be moved to the beginning and end of the transformer to avoid calling them for each attention."
        },
        {
            "title": "B Experimental Details",
            "content": "We document the detailed experiments setups for results presented in Section 3. 14 Table 3: Model configuration and training hyperparameters used for the ablation studies. Model Config Head Dim FFN Dim Cross Attn Norm Freq Dim In Channels Out Channels Num Heads Num Layers Patch Size QK Norm Epsilon Text Dim Value Hyperparameter Value 64 3072 True 256 16 16 12 12 1 2 2 RMS Norm (across heads) 1 106 4096 Learning Rate LR Scheduler Warmup Steps Batch Size Video Latent Shape Sequence Length Attention FLOPs Ratio Weight Decay AdamW Betas Objective Timestep Sampler Total Traning FLOPS 6 104 Constant 100 1024 16 32 32 16,384 1 102 (0.9, 0.95) Flow Matching [24, 22] LogitNormal(0.0, 1.0) [9] 4.5 1020 (a) 120M model used for ablation studies. (b) Training hyperparameters B.1 Model Architecture We follow the architecture of Wan2.1 [35] for all experiments. Ablation studies are conducted on 120M-parameter model initialized using the GPT-NeoX scheme, which leads to faster convergence than the default PyTorch initialization. The model adopts the pretrained UMT5-XXL [4] as the text encoder and the pretrained VAE from Wan2.1 for video tokenization. The architecture includes two types of attention: (1) self-attention among video tokens and (2) cross-attention for injecting textual information. Sparse attention is applied only to the self-attention layers. Detailed model configurations are provided in Table 3a. B.2 Ablation Experiments Setup We train on long sequences of shape 61 512 512, motivated by two factors. First, attention dominates the computational cost at this scale, making it the primary bottleneck. Second, sparse attention must be evaluated under long-context settings to demonstrate its effectiveness; short sequences do not present sufficient challenge. To establish strong baseline, we perform grid search over batch sizes {512, 1024, 2048} and learning rates {5 105, 1 104, 2 104, 6 104}. The best hyperparameters is used for all ablation variants. Training is conducted under fixed compute budget of 4.5 1020 FLOPs, which we find to be sufficient for training 120M-parameter model 120M model with full attention outperforms 60M model trained with 4 1020 FLOPs, indicating FLOPS budget around 4.5 1020 is compute-optimal for 120M model. Each ablation job takes around 10 hours on 64 Nvidia H200 GPU. Full training hyperparameters are provided in Table 3b. B.3 Baseline Attention Variants Spatial-Temporal widely adopted approach in early video generation works, including OpenSora [49], OpenSora-Plan [21], LaVie [36], and Latte [27]. We alternate between spatial and temporal attention across layers. Spatial-Full In spatial-temporal attention, the temporal stage can become overly sparse. For example, with latent shape of (16, 32, 32), the temporal attention accounts for less than 1% of the FLOPs of full 3D attention. To mitigate this, we design variant with four spatial layers and one full-attention layer every five layers. Compress KV This variant pools only the key and value tokens using 2 2 2 average pooling, reducing attention FLOPs by 8. The query tokens remain at full resolution. This setup mimics the coarse-grained stage of VSA with smaller pooling size and no pooling on query tokens. 15 Strided Window Inspired by Swin Transformer, we propose strided window attention that increases token interaction on top of spatial-temporal attention. Let Ws and Wt denote the spatial and temporal window sizes. For spatial attention, query attends to all tokens in the same frame and to those in the same temporal window (Wt = 2). For temporal attention, token attends to the same spatial location and to others in the same spatial window (Ws = 8). Conv Pooling Instead of mean pooling for block-level token aggregation, we use 3D convolution with kernel size and stride of (4, 4, 4) (same as the block size). The output channel dimension matches the head dimension. B.4 FLOPS Calculation Elementwise operations such as LayerNorm and RoPE contribute negligibly to the total computational cost in transformers. Following the approximation in [15], we omit these operations and estimate the model FLOPs as 6N D, where is the number of model parameters and is the number of input tokens. However, for video DiTs trained on long sequences, attention computation becomes dominant cost. We therefore incorporate the attention FLOPs following the formulation from FlashAttention [5]: FLOPs = 6N + 4 3.5 where is the number of tokens, is the sequence length, is the number of attention heads, is the head dimension, and is the number of transformer layers. For sparse attention, we adjust the attention portion according to their sparse pattern. B.5 Finetuning Setup To bridge the gap between full and sparse attention, we adopt sparsity decay schedule that gradually reduces the number of cubes used in attention computation. The model is first trained with full attention for the initial 50 steps, to accommodate the changed resolution and aspect ratio. Thereafter, we decrease the number of attended cubes by 4 (i.e., reduce Top-K by 4) every 40 steps, until reaching the target sparsity level (In our setting Top-K = 32). Unlike directly training the model with extremely sparse attention, our progressive decay schedule enables smooth transition and mitigates training instability. In the finetuning experiments for Wan-1.3B, we trained on 80,000 synthetically generated videos from Wan-14B, each with resolution of 448 832 and 61 frames. To accelerate training and reduce memory usage, we preprocessed both the VAE latents and text encodings. Training was conducted on 32 H200 GPUs using DDP as the parallelism strategy. We set the per-GPU batch size to 1, applied gradient accumulation of 4, and used learning rate of 1e5. The training ran for 3,000 steps."
        },
        {
            "title": "C Coarse Stage Runtime",
            "content": "For shorter sequence lengths, the coarse stage overhead is more pronounced. Our profiling experiments using nsys  (Table 4)  reveals that Top-K selection dominates this runtime. Although we fused the kernels for attention scaling, softmax, and Top-K operations to reduce memory traffic and improve data locality, it only provided modest improvements. Since the coarse stage overhead becomes negligible at longer sequence lengths our primary target we did not pursue further optimizations in this work. However, coarse stage acceleration remains an important direction for future research. 16 Table 4: Coarse stage runtime (µs). Breakdown w/o fusion w/ fusion QKT scale softmax topk PV 0.046 0.060 0.095 0.869 0. 0.046 0.912 0."
        },
        {
            "title": "D Discussion and Extended Related Work",
            "content": "VSA builds upon insights from prior work on trainable sparse attention mechanisms in both language modeling [44, 25] and computer vision [50, 33]. This section situates VSA within this landscape, highlighting key similarities and differentiating design choices. MoBA [25]: VSA shares conceptual similarities with MoBA, particularly in: (1) employing coarsegrained stage that utilizes mean pooling, akin to MoBAs gating mechanism, and (2) using attention scores from this pooled representation to guide block selection for sparse attention. However, key divergence lies in the utilization of the coarse-grained stage output. While MoBA employs pooled attention solely for block selection, VSA incorporates the output of its coarse-grained stage directly into the final attention output, potentially enriching global context representation. More critically, MoBAs implementation, which relies on token gathering and variable-length FlashAttention, constrains it to larger tile sizes (e.g., 512). This can limit the granularity of its sparsity patterns and reduce speedup efficacy, especially for sequences like those at 128K. In contrast, VSA is implemented with block-sparse attention leveraging smaller, fixed-size blocks (e.g., 64x64 as per our findings), aiming for better balance between performance (Table 1d) and practical world-clock speedup (Section 3.1). NSA [44]: The two-stage (coarse/compress and fine/select) architecture of VSA bears resemblance to NSAs design. However, fundamental differences arise from their target domains. NSA is tailored for causal language model decoding, where typically only single query token is processed at time. This necessitates specific strategies like group query attention to enable efficient kernel implementation(NSA can only pool Key-Value pairs). Video DiTs, operating bidirectionally on entire sequences, do not face the same single-query constraint, allowing VSA to apply pooling more broadly and employ distinct sparse patterns for different attention heads without resorting to grouped queries. Furthermore, NSA includes an additional sliding window stage for local information, component VSA found unnecessary for video generation in our setup (Table 1b). BiFormer [50]: Similar to BiFormer, VSA utilizes coarse-grained, tile-to-tile attention mechanism. However, in BiFormer, this coarse attention serves only to derive the dynamic sparse pattern for the subsequent token-to-token attention and does not directly contribute to the final output. Our ablations (Table 1b) indicate that for VSA , the output of the coarse-grained stage is paramount for achieving optimal performance. Additionally, BiFormers original implementation lacked direct FlashAttention compatibility, impacting its throughput compared to VSA design, which is optimized for hardwarealigned block-sparse operations. DSV [33]: DSV represents pioneering work in exploring trainable sparse attention specifically for video DiT training. Both VSA and DSV aim to reduce the cost of identifying load-bearing regions. DSV achieves this by introducing dedicated low-rank attention predictors with reduced head dimension, which are trained in separate, multi-stage process and are not fully end-to-end integrated with the main DiT training. VSA , on the other hand, reduces this cost by performing attention on spatially pooled representations (e.g., from 4x4x4 cube of tokens) within its coarse-grained stage. Crucially, VSA is designed to be end-to-end trainable, requiring minimal modifications to existing DiT training frameworks, unlike the more complex training system designed for DSVs predictors."
        },
        {
            "title": "E Broader Impact",
            "content": "VSA aims to make high-quality video generation more accessible by significantly reducing the training and inference cost of video diffusion models. Our work may help democratize video creation tools for broader range of users and use cases, including education, animation, and independent media production. However, as with many generative technologies, VSA also presents potential risks. In particular, the ability to generate realistic videos at scale may increase the risk of malicious applications, such as generating deepfakes or misleading media content. We emphasize the importance of developing robust detection tools, usage guidelines, and ethical standards in parallel with technical advances to ensure responsible deployment of such models."
        }
    ],
    "affiliations": [
        "MBZUAI",
        "UC Berkeley",
        "UC San Diego"
    ]
}