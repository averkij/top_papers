{
    "paper_title": "MaterialFusion: Enhancing Inverse Rendering with Material Diffusion Priors",
    "authors": [
        "Yehonathan Litman",
        "Or Patashnik",
        "Kangle Deng",
        "Aviral Agrawal",
        "Rushikesh Zawar",
        "Fernando De la Torre",
        "Shubham Tulsiani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent works in inverse rendering have shown promise in using multi-view images of an object to recover shape, albedo, and materials. However, the recovered components often fail to render accurately under new lighting conditions due to the intrinsic challenge of disentangling albedo and material properties from input images. To address this challenge, we introduce MaterialFusion, an enhanced conventional 3D inverse rendering pipeline that incorporates a 2D prior on texture and material properties. We present StableMaterial, a 2D diffusion model prior that refines multi-lit data to estimate the most likely albedo and material from given input appearances. This model is trained on albedo, material, and relit image data derived from a curated dataset of approximately ~12K artist-designed synthetic Blender objects called BlenderVault. we incorporate this diffusion prior with an inverse rendering framework where we use score distillation sampling (SDS) to guide the optimization of the albedo and materials, improving relighting performance in comparison with previous work. We validate MaterialFusion's relighting performance on 4 datasets of synthetic and real objects under diverse illumination conditions, showing our diffusion-aided approach significantly improves the appearance of reconstructed objects under novel lighting conditions. We intend to publicly release our BlenderVault dataset to support further research in this field."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 2 ] . [ 1 3 7 2 5 1 . 9 0 4 2 : r MaterialFusion: Enhancing Inverse Rendering with Material Diffusion Priors Yehonathan Litman1 Or Patashnik2 Kangle Deng1 Aviral Agrawal1 Rushikesh Zawar1 Fernando De la Torre Shubham Tulsiani1 1Carnegie Mellon University 2Tel Aviv University https://yehonathanlitman.github.io/material_fusion Figure 1. Given an image set of an object under unknown illumination, MaterialFusion recovers the objects geometry, BRDF appearance, and the environmental illumination, via inverse rendering. Our method utilizes 2D material diffusion prior to accurately reconstruct these properties. On the left, we display the input image set of the bills alongside the output of the reconstructed properties, visualized as the materials, albedo, and mesh from top to bottom, respectively. On the right, we show different objects rendered under novel lighting conditions with the reconstructed physical properties."
        },
        {
            "title": "Abstract",
            "content": "Recent works in inverse rendering have shown promise in using multi-view images of an object to recover shape, albedo, and materials. However, the recovered components often fail to render accurately under new lighting conditions due to the intrinsic challenge of disentangling albedo and material properties from input images. To address this challenge, we introduce MaterialFusion, an enhanced conventional 3D inverse rendering pipeline that incorporates 2D prior on texture and material properties. We present StableMaterial, 2D diffusion model prior that refines multilit data to estimate the most likely albedo and material from given input appearances. This model is trained on albedo, material, and relit image data derived from curated dataset of approximately 12K artist-designed synthetic Blender objects called BlenderVault. we incorporate this diffusion prior with an inverse rendering framework where we use score distillation sampling (SDS) to guide the optimization of the albedo and materials, improving relighting performance in comparison with previous work. We validate MaterialFusions relighting performance on 4 datasets of synthetic and real objects under diverse illumination conditions, showing our diffusion-aided approach significantly improves the appearance of reconstructed objects under novel lighting conditions. We intend to publicly release our BlenderVault dataset to support further research in this field. 1. Introduction Recently, there has been an increased interest in methods that try to recover 3D representations from 2D images. Novel view synthesis approaches, particularly Neural Radiance Fields (NeRF) [35] and follow-up works have proven highly effective for accurately representing 3D scenes from posed 2D images. Nevertheless, one of the main drawbacks of these approaches is relighting, since novel view synthesis methods bake in all the lighting information into the 3D representation, rather than disentangling it from the underlying scene data. In this paper, our goal is to infer relightable 3D representations that factorize these properties, allowing for the editing of materials, geometry, and lighting independently. Some approaches do pursue factorized, relightable 3D representations [5, 19, 57]. These methods employ signed distance functions (SDFs), meshes, or volumetric representations to model geometry, while also estimating underlying properties like diffuse albedo and specular parameters and their results can be used for relighting in novel environments. However, as these approaches are supervised on captured image data under fixed illumination, there still exists an ambiguity between the underlying properties that images alone cannot explain. Multiple possible materials and textures could be composed onto the geometry to produce the same final images in the training data, leading to fundamental ambiguities when inferring underlying albedo and material properties using simple pixel-level reconstruction loss. The ill-posed nature of this problem ultimately leads to suboptimal factorization. Our key insight is that 2D priors over plausible materials and albedos, in addition to reconstruction losses, can resolve ambiguities in factorized inverse rendering. We learn large scale conditional diffusion prior over likely materials for RGB images under different illuminations. In addition to reconstruction loss, we distill the fine-tuned diffusion model to provide additional signal about plausible texture and material combinations for the depicted object during 3D optimization. We demonstrate our 3D inverse rendering approach, MaterialFusion, on the NeRF Synthetic, NeRFactor datasets [35, 62], test set of our BlenderVault dataset and the Stanford-ORB dataset [25]. We use these datasets to show significant improvements in novel view synthesis under relighting as well as material estimation compared to prior state-of-the-art work on both synthetic and real data. We trained conditional diffusion model, StableMaterial, with albedos, materials, and relit images rendered from 29K high quality objects, augmented with our own BlenderVault dataset of 12K high quality synthetic Blender objects curated from online sources, and show its superior performance compared to previous approaches that recover albedo and materials from input images. Using our prior, our learned factorized representation generalizes better to novel lighting conditions across diverse lighting, object, and underlying material scenarios, as shown in the relighting results in Fig. 1. 2. Related Works 2.1. Inverse Rendering In recent years, reconstruction methods that learn 3D representation from set of multi-view images have rapidly improved [1, 2, 23, 35, 37] in terms of quality and speed. However, many of these methods do not disentangle the underlying texture and materials, from the illumination. Therefore, rendering the acquired scene under novel lighting conditions remains challenge. To address this, inverse rendering works have begun focusing on reconstructing the 3D appearance along with the underlying materials of scene or object. Given set of images of scene or object under fixed illumination, some works have aimed to recover the texture, materials, and lighting [4, 16, 19, 22, 27, 38, 62, 63]. This task is inherently challenging due to its high dimensionality and ambiguity in explaining the image appearance, as multiple illumination and material parameter combinations can be used to reproduce the final appearance. To tackle this ambiguity, other works simplify the problem setting by assuming or modeling scene lighting [10, 18, 22, 61] or employing domain-specific priors [3, 9, 62] to inject additional information on physical properties. Nevertheless, assumptions about lighting limit the applicability of these methods in real-world scenarios such as online marketplaces, where lighting conditions can be difficult to capture and are constantly changing. Moreover, the priors used in the aforementioned works are either trained on small-scale or procedurally generated data or focus on specific object category. In contrast, our approach does not rely on controlled lighting conditions; instead, it primarily utilizes largescale 2D texture and material prior trained with large synthetic object dataset we curated. The objects in this dataset contain complex Physically Based Rendering (PBR) assets, enhancing our priors predictions. 2.2. 2D Diffusion Priors For 3D Tasks The success of diffusion models in text-to-image synthesis [20, 42, 43] has also brought attention to employing large scale 2D priors for 3D generation [8, 28, 34, 40, 50, 53, 55]. Dreamfusion [40] and SJC [53] first propose Score Distillation Sampling (SDS) to optimize 3D representation using 2D diffusion model gradients. Some follow-up works enriched the 2D model prior with 3D knowledge by finetuning the model to generate novel views of an object [31], to generate images of several views simultaneously [32, 49, 54]. Moreover, it has been shown that such enriched models perform better in generating 3D models from scratch and in single-view reconstruction [2933, 48, 49, 54, 64]. Additionally, ReconFusion [58] also uses 2D diffusion priors to improve sparse-view 3D reconstruction. However, common to all of these works is the lack of material and illumination disentanglement, thereby limiting the relighting performance of the generated or reconstructed objects. To predict physical properties, previous works showed success in finetuning pretrained diffusion model. Specifically, some works predict material parameters given an RGB image [24, 46, 52, 60]. However, these works reconstruct only 2D representation of the underlying physical properties, and do not consider the 3D reconstruction from set of images. In contrast to the aforementioned works, our approach reconstructs the underlying 3D geometry, material properties, and environmental lighting from set of multi-view images via score distillation. Closest to ours, [59] concurrently used 2D diffusion model to guide relightable 3D inference, but used diffusion samples to guide the optimization while ours uses likelihood maximization via SDS. 2.3. 3D Datasets with PBR assets The availability of 3D datasets is considerably smaller than the availability of 2D datasets, even more so in terms of PBR information, imposing challenge in 3D-related tasks. In particular, commonly used 3D datasets [15, 21, 41] lack PBR information. Some datasets [11, 39] offer 3D objects with PBR information but are limited in diversity to only furniture. Objaverse [13] offers diversity yet contains many objects that are partial reconstructions, low in quality, or cartoonish. Artist-designed high-quality 3D objects with PBR data are available in different sources, but are not organized in dataset suitable for research. In this work, we introduce new dataset of Blender objects containing high quality PBR assets curated from online sources. We use this dataset to augment previous datasets, greatly enhancing the diversity of PBR information available for training. 3. Methodology This section introduces MaterialFusion, our approach to reconstructing 3D representation of an object from set of multi-view images. An overview of our approach is shown in Fig. 3. Specifically, given set of posed images of the object captured under an unknown illumination, our goal is to reconstruct the objects geometry and BRDF appearance, as well as recover the environmental illumination. Accurately reconstructing these components allows us to faithfully recreate the object appearance under new lighting conditions. We represent the geometry as mesh, as its explicit nature is more suitable for downstream tasks. For the material, we use simplified Disney principled BRDF model [7] representation. Specifically, the material texture contains three components per texel, albedo R3, roughness R, and metallicness R. Following prior works [19, 38], we represent the albedo texture as an albedo UVmap kd, and roughness and metallicness as part of an occlusion, roughness, metallicness (ORM) UV-map korm, where each texel is (o, r, m) with unused. The environment illumination is represented as high dynamic range (HDR) environment map. Our key idea is to leverage strong 2D prior obtained from an image diffusion model which is trained to estimate the underlying material given RGB image input. To accomplish this, we first adapt an existing image diffusion model (Stable Diffusion 2.1 [42]) to predict the albedo and ORM from an image of an arbitrary object rendered under randomly selected illumination. This allows us to extend an existing 2D diffusion prior such that it has material understanding. The finetuning procedure is shown in Fig. 2. We then leverage the extended 2D prior in an inverse rendering framework to infer disentangled 3D representation of given object and an HDR map of the environment lighting. Specifically, we utilize variant of SDS loss [40] to employ the 2D prior for 3D optimization. We show an overview of the 3D inference procedure in Fig. 3. 3.1. Training Data Learning diffusion prior for albedo and ORM prediction from images we leverage diverse dataset of synthetic object renderings with high-quality PBR textures. Using such data, we generate training images with graphics engine capable of reproducing realistic appearances such as Blender. We examined existing datasets such as Objaverse [13] and ABO [11] for this purpose. Objaverse is large and diverse dataset, but it contains many unrealistic, low-quality, or textureless objects. To address this, we followed similar filtering procedure as [51], and then further filtered for non-cartoon objects with PBR textures. This resulted in filtered subset of 8.5K objects from Objaverse. While the filtered Objaverse dataset provided good coverage, we found that augmenting it with the ABO dataset (which contains 8K objects from only 63 categories) was not sufficient to achieve the desired diversity in our training data. To further improve the diversity, we created our own BlenderVault dataset, which contains an additional 12K high-quality, PBR-textured objects. BlenderVault consists of Blender obejcts designed and validated by artists across arbitrary categories for use in commercial projects. To render the training images, we replaced any glass surfaces in the objects with black surface of roughness 0.25 and metallicness 0. We then rendered 30 images of each object, with randomly selected azimuth [0, 360] and elevation [15, 90] on hemisphere with radius [1.5, 2.0]. The lighting conditions were also varied, Figure 2. StableMaterial receives an RGB image as input and outputs the albedo ˆId and ORM ˆIorm 2D maps. To train StableMaterial, we use BlenderVault objects to render dataset of multi-view images under varying illuminations as well as the corresponding albedo and ORM maps. Given triplet (x, Id, Iorm) of an image and its albedo and ORM maps, we encode them using the pretrained Stable Diffusion encoder and concatenate the image latent with the noisy albedo and ORM latents. The model is then trained with diffusion loss to denoise the albedo and ORM maps. using random selection of StreetLearn [36] environment maps, Laval [17] indoor environment maps, point lights, or directional sun lights. In total, our training dataset consists of 28K synthetic objects with high quality PBR assets, combining the filtered Objaverse, ABO, and our own BlenderVault data. 3.2. StableMaterial 2D Material Denoising Diffusion Prior To have strong 2D material prior, we build on Stable Diffusion 2.1 [42] and fine-tune it from the pretrained model on dataset consisting of triplets (x, Id, Iorm), where is an RGB image of the object, and Id, Iorm are its corresponding rendered albedo and ORM components, respectively. Formally, given an RGB input of an object under unknown illumination, we fine-tune Stable Diffusion to output its underlying albedo Id and ORM Iorm components. Model Architecture. We modify only Stable Diffusions UNet, so that it is conditioned on the input image in two ways. First, we encode it with Stable Diffusions pre-trained frozen VAE and concatenate the resulting clean latent z0 to the noisy latent codes zt in the channel dimension, where is the diffusion timestep. Specifically, the noisy latent code d, zt zt = [zt orm], i.e. the concatenation of the noisy albedo and the noisy ORM map zt latent zt orm in the channel dimension. The input of our UNet is then (cid:0)z0 x, zt, t(cid:1). The text conditioning is also replaced with CLIP image embedding of the input image. These two ways of inputting the image into the model allow it to have both global and local reasoning about the image. To output both albedo and ORM maps, our noisy latent codes zt consist of 8 channels, 4 corresponding to the albedo and the other 4 corresponding to the ORM. In total, the input of our network is composed of 12 channels consisting of the encoded input image, noisy albedo latents and the noisy ORM latents. To obtain the RGB albedo and ORM maps we decode the denoised ˆz through the pretrained Stable Diffusion decoder D. To account for the different input and output channels, the first and last layers are changed and randomly initialized, while the other UNet parameters are kept unchanged. Loss. To fine-tune the model, we utilize v-prediction diffusion loss [44]. At each training iteration, we sample triplet (x, Id, Iorm), and encode each of the images with E. We concatenate E(Id), E(Iorm) in the channel dimension and denote their concatenation by z. We sample diffusion timestep along with an 8-channel random noise ϵ, and add the noise to to obtain zt. The diffusion loss is defined as (cid:2)ϵθ (cid:0)E(x), zt, t(cid:1) vt2 Ldiff = Ex,kd,korm,ϵN (0,I),t (cid:3) , (1) where x, zt are as defined above. As in [44], vt = αtϵ σtz, where αt, σt are the parameters of the scheduler. Similarly to [6], we enable classifier-free guidance by setting the input images, input image prompt, or both to all zeros with 5% probability each and set the guidance scale to 3.0. 3.3. Prior-guided Inverse Rendering Having material knowledge in our trained StableMaterial model, we can distill this knowledge and reconstruct 3D disentangled representation of the object. Specifically, given set of multi-view images under an unknown illumination depicting an object, we aim to reconstruct the underlying geometry represented as mesh and denoted by G, the underlying UV material texture denoted by (kd, korm), and the environment illumination L. To this end, we directly optimize these representations and build on recent advancements in the distillation of 3D information from 2D Figure 3. MaterialFusion reconstructs an objects geometry, PBR materials, and environmental illumination from set of multi-view images under fixed lighting condition. In addition to the reconstruction and regularization losses computed between our rendered images ˆx and reference RGB images x, MaterialFusion employs priors from our pre-trained StableMaterial to enhance PBR material reconstruction. Specifically, it calculates an SDS loss for the rendered albedo and ORM components, ˆId and ˆIorm conditioned on x. diffusion models [40], in conjunction with the off the shelf nvdiffrecmc inverse rendering pipeline [19] as part of MaterialFusion. Following previous works [19, 38] we parameterize the geometry through an SDF denoted by S, and extract mesh in each optimization iteration using DMTet [47]. Given the mesh G, the texture (kd, korm), camera view C, and an HDR environment light map L, we use nvdiffrecmcs differentiable renderer to produce 2D rendering. In each optimization iteration, we sample some images and their associated views x, C, respectively, from the training set, and differentiably render the object using the optimized parameters from the views C. We obtain the rendered image ˆx with nvdiffrecmcs renderer and apply reconstruction loss to optimize (S, kd, korm, L): Lrecon = EC [L2(ˆx, x)] , (2) At the same time, we also render the corresponding components of the albedo and the material, ˆId and ˆIorm, respectively, encode them with the Stable Diffusion model encoder and concatenate their latent representations to get z. Then, we sample Gaussian noise ϵ and add it to according to random diffusion timestep [0.02, 0.98], to obtain zt. We then denoise zt using the diffusion model, and sample clean latent representation of the materials denoted by ˆz via DDIM sampling for 5 steps, conditioned on x. We use the SDS loss in both latent and pixel space: LSDS+ = Et,ϵ,v (cid:2)λlatentz ˆz2 + λrgbD(z) D(ˆz)2(cid:3) , (3) where is the Stable Diffusion decoder. This loss is inspired by HiFA [65], as we empirically found the RGB term important for boosting the quality of materials estimated during training, shown in Tab. 4. Using 5 denoising steps was key for producing crisp and accurate predictions for the albedo and ORM materials. Finally, our total loss consists of the reconstruction loss, regularization loss, and SDS loss: LMaterialFusion = Lrecon + Lreg + γiLSDS+, (4) where γi is an iteration dependent hyperparameter which decays as training progresses. We find that reducing the weight on SDS loss towards the end of the optimization helps preserve finer details that may be lost due to the encoder/decoder operation. We use the same Lreg as nvdiffrecmc [19]. Conceptually, using SDS loss during inverse rendering maximizes the likelihood of the ORM and albedo under our prior for all training images. 4. Experiments We evaluate MaterialFusion and StableMaterial on image sequences of objects made of various materials and textures and show the qualitative and quantitative comparisons. We first evaluate MaterialFusion against prior inverse rendering methods for object relighting on number of synthetic and real diverse objects and highlight the advantages of our approach in terms of appearance relighting. We further compare the trained 2D prior against other previous approaches that predict albedo and material from single image using test data from BlenderVault excluded from training. Figure 4. Qualitative comparison for MaterialFusion vs. other methods. We present the 3D reconstructed albedo, ORM, environment light map, and relit rendered images for three different objects, both synthetic and real. Our method demonstrates better accuracy compared to the baseline methods, as can be seen by the accuracy of the reconstructed materials and the relit image appearance. Our prior also acts as an additional regularizer on other 3D properties such as geometry and illumination. 4.1. Relightable 3D Reconstruction For MaterialFusion, we adopt validation setup similar to nvdiffrecmc by relighting the objects under novel illuminations and then comparing to the groundtruth relit object images. For synthetic objects, we acquire the groundtruth relightings by rendering images of synthetic objects under unseen illuminations, while real objects are captured in novel environments for which the illuminations are computed. albedo used were PSNR, SSIM, and L1, and PSNR and L1 for ORM. LPIPS was excluded for both since perceptual similarity does not matter for ORM, and the VGG network likely has not seen albedo images. Given the scaling ambiguity between the albedo and light intensity during inference, the channels of RGB and albedo images are scaled against groundtruth during validation [19]. Since the ORM and albedo are fundamentally pixel-wise material parameters, we use the L1 metric to measure physical similarity. Datasets. We use 4 objects from NeRFactor [62], 5 objects from the NeRF synthetic dataset [35], 9 test objects from BlenderVault, and 14 objects from the StanfordORB [25] datasets. The first three datasets consist of diverse synthetic objects with camera poses and their groundtruth data allows for us to re-render and compare objects with different illuminations. The NeRFactor and BlenderVault objects are relit by eight low resolution environment maps while NeRF synthetic objects are relit by four high resolution environment maps, and the quality comparison is computed on test set of eight unseen poses per environment map. We also show our relighting performance on real objects from the Stanford-ORB dataset, which has images, corresponding poses, and groundtruth illuminations allowing us to re-render and relight objects. Objects are relit under two novel illuminations and the relighting comparison is done using test set of unseen poses per environment map. Metrics. The final results for the 3D pipeline relighting comparison are the average PSNR, SSIM, and LPIPS across all relighting test views for each dataset. The metrics for the Baselines. We compare MaterialFusion against three current state-of-the-art inverse rendering methods that estimate geometry, albedo, roughness, and metallicness from set of images. These approaches are nvdiffrecmc [19], which is the method our pipeline is built upon, Relightable 3D Gaussian [16], and TensoIR [22]. Results. We present qualitative and quantitative results for both albedo and ORM estimation quality as well as performance during relighting. Fig. 4 shows visual comparison of the albedo and ORM estimated by all methods. Our method is able to recover high frequency details in both the albedo and ORM that other methods are not able to. This in turn leads to better performance under novel relighting, where Tab. 1 shows our method achieving the highest scores across all three datasets. The source of improvement in the relighting performance is best understood via the results in Tab. 2, where the estimated albedo and ORM quality for BlenderVault objects were directly compared to the groundtruth. We were unable to compare for the NeRF and NeRF Synthetic NeRFactor BlenderVault Stanford-ORB nvdiffrecmc TensoIR Relightable3DG MaterialFusion PSNR 25.70 24.32 23.08 26. SSIM LPIPS 0.924 0.923 0.897 0.927 0.090 0.090 0.094 0.085 PSNR 25.91 24.87 23.99 26.31 SSIM LPIPS 0.921 0.916 0.908 0.922 0.092 0.094 0.082 0.091 PSNR 25.11 25.01 22.83 26. SSIM LPIPS 0.910 0.903 0.909 0.921 0.167 0.162 0.148 0.143 PSNR 31.10 28.81 27.40 31.68 SSIM LPIPS 0.968 0.959 0.955 0.967 0.048 0.047 0.048 0.046 Table 1. Comparison of novel view synthesis relighting. In each column, the best , second best , and third best results are marked. Albedo SSIM 0.939 0.927 0.925 0.949 PSNR 28.24 25.82 24.30 29.31 ORM L1 0.021 0.026 0.036 0.015 PSNR 15.93 14.19 20.96 22. L1 0.062 0.063 0.041 0.033 nvdiffrecmc TensoIR Relightable3DG MaterialFusion Table 2. Reconstructed 3D albedo and ORM comparison on BlenderVault objects. NeRFactor datasets as the underlying material shaders used did not conform directly to the albedo and ORM. The comparisons show that our method performs best in both albedo and ORM estimation, as other methods suffer from poor albedo or ORM estimates, leading to poorer relighting. The synthetic example of the clock in Fig. 4 shows how MaterialFusion is able to accurately disambiguate different areas of the material and albedo, leading to much more accurate rendering under novel illumination where details arent lost like in the other methods renderings. This can also be seen in the real can example where our method accurately deduces it is metallic (shown by the strength of the blue channel in the ORM map) and is able to accurately replicate the reflection of the can similarly to the real world. Our method shows better semantic material understanding as it is able to correctly distinguish between different parts of an object that are made of different materials, leading to better decoupling between the reflectance and environment illumination. Our results confirm our priors improvements against baselines by better inferring underlying physical properties on synthetic and real data. 4.2. Validating Material Inference from 2D Input To validate StableMaterials performance, we evaluate its performance on RGB images of synthetic test objects captured under an unknown illumination. We then directly compare the predictions to the groundtruth albedo and materials using extracted data from the synthetic objects. Datasets. We utilize 8 diverse test objects from our BlenderVault datasets whose material data was not seen during training. We then render the groundtruth albedo, ORM, and RGB appearance under an unknown randomly selected fixed illumination for 4 views. This is done per each object. Albedo SSIM 0.874 0.905 0.871 0.902 0.907 PSNR 21.69 22.62 21.71 24.25 24.70 ORM L1 0.025 0.026 0.031 0.018 0.018 PSNR 18.68 18.87 24.86 26.34 L1 0.041 0.045 0.029 0.014 Derender3D IIR IID StableMaterial StableMaterialMV Table 3. Comparison of albedo and ORM 2D predictions produced by our method versus other methods. We use the mean of 10 samples generated by StableMaterial and StableMaterialMV for the evaluation, where StableMaterialMV denotes our prior utilizing multi-view attention during inference. Metrics. To account for the variance in StableMaterials outputs, we follow the procedure described in [24] and compute 10 estimates for the albedo and ORM images and average them together before comparing to the groundtruth. We further account for the scale ambiguity in the resulting albedo for all the baselines by rescaling to the groundtruth albedo. Similarly to the 3D evaluation, the metrics used for the albedo were PSNR, SSIM, and L1, and PSNR and L1 for ORM. The final results are computed as the mean across views for all objects. Baselines. We test StableMaterial against Inverse Indoor Rendering (IIR) [66] and Intrinsic Image Diffusion (IID) [24], which were trained on scene data to directly predict the albedo, roughness, metallicness given single image. We also include [56], which was trained on diverse data and predicts the albedo but not materials. Multi-view Attention at Inference. To make StableMaterial produce material outputs consistent across 2D views, we follow previous works [49, 54] and incorporate multiview attention. Specifically, we input batch of 4 images, and modify the self-attention layers of the model so that each latent pixel in each of the images attends to the latent pixels of all other images. As such, StableMaterial predicts the most likely material given all input image appearances. The self-attention layers of the network process the 4 images as single large image, while the other layers process them independently. Importantly, this multi-view attention mechanism is only employed during inference for 2D images. We show that using multi-view attention improves the quality of inferred materials against other baselines. Figure 5. Qualitative comparison of the albedo and ORM 2D predictions. The Derender3D ORM data is marked as N/A since it does not offer ORM predictions. Given 4 images of an object, StableMaterial recovers complex material data. StableMaterialMV attends to appearance details across views, recovering consistent and high quality materials across challenging views, as seen in the cup example. BlenderVault 4.3. Ablation Studies γi = 1 λRGB = 0 λlatent = 0 Ours PSNR 25.50 21.41 26.12 26.33 SSIM LPIPS 0.916 0.871 0.917 0.921 0.150 0.233 0.147 0.143 Table 4. Effects of ablating elements from LMaterialFusion. Results. As shown in Tab. 3, our trained model shows strong performance across multiple objects in estimating the Albedo and ORM quality. Notably, performance jumps further when multi-attention is used across 4 views, as our model can handle difficult views that offer little appearance information during inference. Fig. 5 shows qualitative comparison of our model against previous approaches. The consistent improvements in both 2D and 3D tasks highlight the effectiveness of our approach in capturing the underlying physical properties of objects. The multi-view variant further demonstrates the benefits of leveraging additional viewpoints to enhance the albedo and ORM prediction quality for difficult 2D views. We found no significant differences when employing multi-view attention during inverse rendering, given that the albedo and ORM representations are already optimized to be multi-view consistent. However, the performance boost for 2D images raises the potential for usage in sparse view scenarios or where 3D reconstruction is not needed or infeasible. In Tab. 4, we ablate three terms of LMaterialFusion and evaluate relighting performance on the BlenderVault test dataset. All other parameters are unchanged when ablating one parameter. Setting λRGB = 0 particularly affects performance; by backpropagating through the SD encoder, the latent SDS term gradient introduces artifacts in the materials, degrading their quality. We also conduct an ablation where γi is set to 1 throughout the inverse rendering optimization. This leads to noticeable drop in performance, as materials estimated for objects with finer details suffer. 5. Conclusion In this paper, we introduced MaterialFusion, 3D inverse rendering approach that utilizes StableMaterial, 2D diffusion model finetuned from Stable Diffusion as prior for enhancing the underlying materials during training. We introduced BlenderVault, dataset of high quality objects and underlying PBR assets used to finetune our prior, enabling it with knowledge to recreate complex materials from images. Utilizing our prior on top of an off the shelf inverse rendering approach lead to significant performance boost when for inferring relightable 3D representations. While our work introduces distillation of material knowledge in 3D scenario, we believe there is great potential in utilizing our prior for applications in 2D or sparse-view settings. 6. Acknowledgments The authors would like to thank Jianjin Xu and the rest of the students from the Human Sensing Laboratory for their helpful feedback. This work was supported in part by the NSF GFRP (Grant No. DGE1745016) and NSF Award IIS2345610."
        },
        {
            "title": "References",
            "content": "[1] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. CVPR, 2022. 2 [2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased gridbased neural radiance fields. ICCV, 2023. 2 [3] Sai Bi, Zexiang Xu, Kalyan Sunkavalli, David Kriegman, and Ravi Ramamoorthi. Deep 3d capture: Geometry and reflectance from sparse multi-view images. In CVPR, 2020. 2 [4] Mark Boss, Varun Jampani, Raphael Braun, Ce Liu, Jonathan T. Barron, and Hendrik P.A. Lensch. Neural-pil: Neural pre-integrated lighting for reflectance decomposition. In NeurIPS, 2021. 2 [5] Aljaˇz Boˇziˇc, Denis Gladkov, Luke Doukakis, and Christoph Lassner. Neural assets: Volumetric object capture and rendering for interactive environments. arxiv, 2022. 2 [6] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. [7] Brent Burley and Walt Disney Animation Studios. In SIGGRAPH. Physically-based shading at disney. ACM Transactions on Graphics (ToG), 2012. 3 [8] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highquality text-to-3d content creation. In ICCV, 2023. 2 [9] Zhaoxi Chen, Gyeongsik Moon, Kaiwen Guo, Chen Cao, Stanislav Pidhorskyi, Tomas Simon, Rohan Joshi, Yuan Dong, Yichen Xu, Bernardo Pires, He Wen, Lucas Evans, Bo Peng, Julia Buffalini, Autumn Trimble, Kevyn McPhail, Melissa Schoeller, Shoou-I Yu, Javier Romero, Michael Zollhofer, Yaser Sheikh, Ziwei Liu, and Shunsuke Saito. Urhand: Universal relightable hands. In CVPR, 2024. 2 [10] Ziang Cheng, Junxuan Li, and Hongdong Li. Wildlight: Inthe-wild inverse rendering with flashlight. CVPR, 2023. 2 [11] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and benchmarks for real-world 3d object understanding. In CVPR, 2022. 3 [12] R. L. Cook and K. E. Torrance. reflectance model for computer graphics. ACM Transactions on Graphics (ToG), 1982. [13] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In CVPR, 2023. 3 [14] Maximilian Denninger, Dominik Winkelbauer, Martin Sundermeyer, Wout Boerdijk, Markus Knauer, Klaus H. Strobl, Matthias Humt, and Rudolph Triebel. Blenderproc2: procedural pipeline for photorealistic rendering. Journal of Open Source Software, 2023. 1 [15] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B. McHugh, and Vincent Vanhoucke. Google scanned objects: highquality dataset of 3d scanned household items. arxiv, 2022. 3 [16] Jian Gao, Chun Gu, Youtian Lin, Hao Zhu, Xun Cao, Li Zhang, and Yao Yao. Relightable 3d gaussian: Real-time point cloud relighting with brdf decomposition and ray tracing. In ECCV, 2024. 2, 6 [17] Marc-Andre Gardner, Kalyan Sunkavalli, Ersin Yumer, Xiaohui Shen, Emiliano Gambaretto, Christian Gagne, and Jean-Francois Lalonde. Learning to predict indoor illumination from single image. ACM Transactions on Graphics (ToG), 2017. 4 [18] Yuan-Chen Guo, Di Kang, Linchao Bao, Yu He, and SongHai Zhang. Nerfren: Neural radiance fields with reflections. In CVPR, 2022. [19] Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg. Shape, Light, and Material Decomposition from Images using Monte Carlo Rendering and Denoising. NeurIPS, 2022. 2, 3, 5, 6, 1 [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. 2 [21] Varun Jampani, Kevis-Kokitsi Maninis, Andreas Engelhardt, Arjun Karpur, Karen Truong, Kyle Sargent, Stefan Popov, Andre Araujo, Ricardo Martin-Brualla, Kaushal Patel, Daniel Vlasic, Vittorio Ferrari, Ameesh Makadia, Ce Liu, Yuanzhen Li, and Howard Zhou. NAVI: Categoryagnostic image collections with high-quality 3d shape and pose annotations. In NeurIPS, 2023. 3 [22] Haian Jin, Isabella Liu, Peijia Xu, Xiaoshuai Zhang, Songfang Han, Sai Bi, Xiaowei Zhou, Zexiang Xu, and Hao Su. Tensoir: Tensorial inverse rendering. In CVPR, 2023. 2, 6 [23] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics (ToG), 2023. [24] Peter Kocsis, Vincent Sitzmann, and Matthias Nießner. Intrinsic image diffusion for indoor single-view material estimation. CVPR, 2024. 3, 7 [25] Zhengfei Kuang, Yunzhi Zhang, Hong-Xing Yu, Samir Agarwala, Elliott Wu, Jiajun Wu, et al. Stanford-orb: realworld 3d object inverse rendering benchmark. NeurIPS, 2023. 2, 6 [26] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila. Modular primitives for high-performance differentiable rendering. ACM Transactions on Graphics (ToG), 2020. 1 [27] Zhihao Liang, Qi Zhang, Ying Feng, Ying Shan, and Kui Jia. Gs-ir: 3d gaussian splatting for inverse rendering. CVPR, 2024. 2 [28] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In CVPR, 2023. [29] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. CVPR, 2024. 3 [30] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. NeurIPS, 2024. [31] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. ICCV, 2023. 2 [32] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. ICLR, 2024. 2 [33] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. CVPR, 2024. 3 [34] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. In CVPR, 2023. [35] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. 2, 6 [36] Piotr Mirowski, Andras Banki-Horvath, Keith Anderson, Denis Teplyashin, Karl Moritz Hermann, Mateusz Malinowski, Matthew Koichi Grimes, Karen Simonyan, Koray Kavukcuoglu, Andrew Zisserman, et al. The streetlearn environment and dataset. NeurIPS, 2018. 4 [37] Thomas Muller, Alex Evans, Christoph Schied, and AlexanInstant neural graphics primitives with mulder Keller. tiresolution hash encoding. ACM Transactions on Graphics (ToG), 2022. 2 [38] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Muller, and Sanja Fidler. Extracting Triangular 3D Models, Materials, and Lighting From Images. In CVPR, 2022. 2, 3, 5 [39] Keunhong Park, Konstantinos Rematas, Ali Farhadi, and Steven M. Seitz. Photoshape: photorealistic materials for large-scale shape collections. ACM Transactions on Graphics (ToG), 2018. 3 [40] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben MildenICLR, hall. Dreamfusion: Text-to-3d using 2d diffusion. 2023. 2, 3, [41] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In ICCV, 2021. 3 [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. CVPR, 2022. 2, 3, 4 [43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-toimage diffusion models with deep language understanding. NeurIPS, 2022. 2 [44] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In ICLR, 2021. 4 [45] Marcel Santana Santos, Tsang Ing Ren, and Nima Khademi Kalantari. Single image hdr reconstruction using cnn with masked features and perceptual loss. ACM Transactions on Graphics (ToG), 2020. 1 [46] Sam Sartor and Pieter Peers. Matfusion: generative diffusion model for svbrdf capture. In SIGGRAPH Asia. ACM, 2023. [47] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: hybrid represenIn NeurIPS, tation for high-resolution 3d shape synthesis. 2021. 5 [48] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arxiv, 2023. 3 [49] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. ICLR, 2024. 2, 3, 7 [50] Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen Liu, Zhenda Xie, and Yebin Liu. Dreamcraft3d: Hierarchical In ICLR, 3d generation with bootstrapped diffusion prior. 2024. 2 [51] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. ECCV, 2024. 3, 1 [52] Giuseppe Vecchio, Rosalie Martin, Arthur Roullier, Adrien Kaiser, Romain Rouffet, Valentin Deschaintre, and Tamy Boubekeur. Controlmat: controlled generative approach to material capture. arxiv, 2023. [53] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In ICCV, 2023. 2 [54] Peng Wang and Yichun Shi. Imagedream: Image-prompt multi-view diffusion for 3d generation. ICLR, 2024. 2, 3, 7 [55] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In NeurIPS, 2023. 2 [56] Felix Wimbauer, Shangzhe Wu, and Christian Rupprecht. De-rendering 3d objects in the wild. CVPR, 2022. 7 [57] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In CVPR, 2024. 2 [58] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul Srinivasan, Dor Verbin, Jonathan Barron, Ben Poole, et al. Reconfusion: 3d reconstruction with diffusion priors. CVPR, 2024. 3 [59] Chen Xi, Peng Sida, Yang Dongchen, Liu Yuan, Pan Bowen, Lv Chengfei, and Zhou. Xiaowei. Intrinsicanything: Learning diffusion priors for inverse rendering under unknown illumination. arxiv, 2024. [60] Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, Ling-Qi Yan, and Miloˇs Haˇsan. Rgbx: Image decomposition and synthesis using materialand lighting-aware diffusion models. In SIGGRAPH. ACM, 2024. 3 [61] Kai Zhang, Fujun Luan, Zhengqi Li, and Noah Snavely. Iron: Inverse rendering by optimizing neural sdfs and materials from photometric images. In CVPR, 2022. 2 [62] Xiuming Zhang, Pratul Srinivasan, Boyang Deng, Paul Debevec, William Freeman, and Jonathan Barron. Nerfactor: Neural factorization of shape and reflectance under an unknown illumination. ACM Transactions on Graphics (ToG), 2021. 2, 6 [63] Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei Jia, and Xiaowei Zhou. Modeling indirect illumination for inverse rendering. In CVPR, 2022. 2 [64] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Distilling view-conditioned diffusion for 3d reconstruction. In CVPR, 2023. 3 [65] Junzhe Zhu and Peiye Zhuang. Hifa: High-fidelity text-to3d generation with advanced diffusion guidance. In ICLR, 2024. [66] Jingsen Zhu, Fujun Luan, Yuchi Huo, Zihao Lin, Zhihua Zhong, Dianbing Xi, Rui Wang, Hujun Bao, Jiaxiang Zheng, and Rui Tang. Learning-based inverse rendering of complex indoor scenes with differentiable monte carlo raytracing. In SIGGRAPH Asia. ACM, 2022. 7 MaterialFusion: Enhancing Inverse Rendering with Material Diffusion Priors"
        },
        {
            "title": "Supplementary Material",
            "content": "lowpoly, poly]. These objects were filtered from the dataset gathered by [51]. MaterialFusion utilizes the nvdiffrast [26] differentiable renderer in order to render out the appearance. nvdiffrast can also render other properties for the system, such as the albedo, ORM, diffuse and specular lighting, and more. In particular, we feed the rasterized estimated albedo and ORM batches to StableMaterial. Lrecon required sRGB tonemapped image inputs, and we took care to make sure that the inputs to StableMaterial were converted to RGB. Lreg contains normal, albedo, and ORM smoothness regularizers, as well as normal perturbation regularizer that encourages normal map perturbations. The last term included is demodulated lighting regularization term that utilizes the rendered specular and diffuse lighting on the object. These regulaizations ae kept as they are as part of MaterialFusion. For more details we refer the reader to the nvdiffrecmc paper [19]. We show relighting comparisons for the rest of the objects used in our evaluation in Figs. 10 12 along with the environment illumination used for rendering the training data images. TensoIR Details. Given that TensoIR doesnt model metallicness, we added it as an additional parameter to the appearance representation decoded from the appearance tensor Ga. The appearance features are interpolated from Ga and are then decoded with radiance network Dc, shading normal network Dn, and material network Dβ to produce the corresponding representations. Of interest is the material decoder which we modify to decode the additional metallicness parameter in addition to the albedo and roughness. All losses and hyperparameters are kept similar. Physically-based rendering is then used to render the resulting image, where metallicness is applied to the diffuse component of the Cook-Torrance reflectance model [12]: fr = (1 m) π + fs (5) where fs is the specular component of the Cook-Torrance model. 7. Additional Visualizations We show visualizations of StableMaterial and MaterialFusions performance on additional examples from the BlenderVault test dataset as well as the NeRFactor and NeRF synthetic datasets. webpage containing videos of the material reconstruction and relighting results has also been attached. 8. Additional Details BlenderVault. To collect BlenderVault, we utilized BlenderProc [14]to download objects from the BlenderKit website. We use Blender in order to render out 30 512512 multi-view images, specifically using Cycles engine with 64 SPP. At each rendering operation, we load in the lighting by randomly selecting between three options with equal probability: StreetLearn environment maps that were reconstructed into HDR maps [45], Laval indoor environment maps, or Blender light source. If Blender light source is picked, then either point light is set up at the camera location with 150W power, or sun light points codirectionally with the camera at the object, with power amount randomly sampled between 10 and 20. To accurately render the albedo and ORM parameters of objects, we tried to approximate them as Disney principled BRDF model as best as possible, corresponding to the Base Color and roughness and metalness parameters of the principled BSDF. Due to the diversity of shaders used to represent materials, dome objects had material or albedo that couldnt be rendered due to either the complexity of the object, size (>500MB), or due to interference from features such as procedural generation. In such cases we skip the objects and continue to rendering the next one. Overall, around 300 objects were skipped but are still included in the final dataset. Training Details. StableMaterial was trained in similar fashion to Zero123, by using batch size of 1536 with images resized to 256256 and learning rate of 104 with an AdamW optimizer for 25k steps. fully connected layer (1028 1024) that converts the concatenation of the CLIP embedding and pose to compatible embedding for Stable Diffusions UNet was trained with learning rate of 103, where the pose representation used was similar Zero123s. Stable Diffusions UNet and the fully connected layers were trained and all other components were frozen. Finetuning took 2 days with 8 H100 GPUs. The filtering keys used for including Objaverse data were [pbr, pbrtexture, substance, substancepainter], while excluding objects that included [style, stylized, cartoon, Figure 6. Full albedo and ORM comparison results for StableMaterial on the cup and armatures examples shown in Fig. 5. Figure 7. Additional albedo and ORM comparisons for randomly selected examples from the BlenderVault test dataset. Figure 8. Additional albedo and ORM comparisons for randomly selected examples from the BlenderVault test dataset. Figure 9. Comparison of MaterialFusion vs. other methods for relighting the clock object from the BlenderVault test dataset. Figure 10. Additional comparisons for MaterialFusion vs. other methods on 3D physical properties reconstruction on more objects from the BlenderVault test dataset. Figure 11. Additional comparisons for MaterialFusion vs. other methods on 3D physical properties reconstruction on the NeRFactor dataset. Figure 12. Additional comparisons for MaterialFusion vs. other methods on 3D physical properties reconstruction on the NeRF dataset."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Tel Aviv University"
    ]
}