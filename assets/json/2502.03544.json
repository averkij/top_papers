{
    "paper_title": "Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2",
    "authors": [
        "Yuri Chervonyi",
        "Trieu H. Trinh",
        "Miroslav OlÅ¡Ã¡k",
        "Xiaomeng Yang",
        "Hoang Nguyen",
        "Marcelo Menegali",
        "Junehyuk Jung",
        "Vikas Verma",
        "Quoc V. Le",
        "Thang Luong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present AlphaGeometry2, a significantly improved version of AlphaGeometry introduced in Trinh et al. (2024), which has now surpassed an average gold medalist in solving Olympiad geometry problems. To achieve this, we first extend the original AlphaGeometry language to tackle harder problems involving movements of objects, and problems containing linear equations of angles, ratios, and distances. This, together with other additions, has markedly improved the coverage rate of the AlphaGeometry language on International Math Olympiads (IMO) 2000-2024 geometry problems from 66% to 88%. The search process of AlphaGeometry2 has also been greatly improved through the use of Gemini architecture for better language modeling, and a novel knowledge-sharing mechanism that combines multiple search trees. Together with further enhancements to the symbolic engine and synthetic data generation, we have significantly boosted the overall solving rate of AlphaGeometry2 to 84% for $\\textit{all}$ geometry problems over the last 25 years, compared to 54% previously. AlphaGeometry2 was also part of the system that achieved silver-medal standard at IMO 2024 https://dpmd.ai/imo-silver. Last but not least, we report progress towards using AlphaGeometry2 as a part of a fully automated system that reliably solves geometry problems directly from natural language input."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 4 4 5 3 0 . 2 0 5 2 : r 2025-2Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 Yuri Chervonyi*,1,, Trieu H. Trinh*,1,, Miroslav OlÅ¡Ã¡k,1,2, Xiaomeng Yang,1, Hoang Nguyen1,3, Marcelo Menegali1, Junehyuk Jung1,4, Vikas Verma1, Quoc V. Le1 and Thang Luong1, 1Google DeepMind, 2University of Cambridge, 3Georgia Institute of Technology, 4Brown University This work was conducted entirely at Google DeepMind by all authors. We present AlphaGeometry2, significantly improved version of AlphaGeometry introduced in Trinh et al. (2024), which has now surpassed an average gold medalist in solving Olympiad geometry problems. To achieve this, we first extend the original AlphaGeometry language to tackle harder problems involving movements of objects, and problems containing linear equations of angles, ratios, and distances. This, together with other additions, has markedly improved the coverage rate of the AlphaGeometry language on International Math Olympiads (IMO) 2000-2024 geometry problems from 66% to 88%. The search process of AlphaGeometry2 has also been greatly improved through the use of Gemini architecture for better language modeling, and novel knowledge-sharing mechanism that combines multiple search trees. Together with further enhancements to the symbolic engine and synthetic data generation, we have significantly boosted the overall solving rate of AlphaGeometry2 to 84% for all geometry problems over the last 25 years, compared to 54% previously. AlphaGeometry2 was also part of the system that achieved silver-medal standard at IMO 2024 https://dpmd.ai/imo-silver. Last but not least, we report progress towards using AlphaGeometry2 as part of fully automated system that reliably solves geometry problems directly from natural language input. Keywords: Mathematics, theorem proving, language models, search."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 More general domain language 3 Automated problem formalization and diagram generation"
        },
        {
            "title": "6 Novel search algorithm",
            "content": "Corresponding author(s): cyuri@google.com, thtrieu@google.com, thangluong@google.com *, : Equal contributions 2025 Google DeepMind. All rights reserved 2 3 5 6 7 8 8 10 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 7 Better language model 12 7.1 Training setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 7.2 Inference setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 8 Results 9 Conclusions and Future work Fine-tuning of math specialized language models on AG data Multi-modal Featured AlphaGeometry2 solutions Additional evaluation on the hardest IMO shortlist problems Towards generating full proofs with language model 1. Introduction 16 20 21 22 26 The International Mathematical Olympiad (IMO) is prestigious mathematics competition for high school students worldwide. IMO problems are known for their difficulty, and solving them requires deep understanding of mathematical concepts and the ability to apply them creatively. Geometry, one of the four IMO categories, is the most uniform across problems, hence the most approachable. It is also well suited for basic reasoning research. There has been two main approaches for automatically solving geometry problems. One is bashing the problems algebraically with Wus method Chou (1985); Wu (2008), Area method Chou et al. (1993, 1994), or GrÃ¶bner bases Kapur (1986a,b), and second approach relies in synthetic techniques such as Deduction database Chou et al. (2000), or Full angle method Chou et al. (1996). We focus on the latter as more human-like approach suitable for transferring the research knowledge to other domains. In our previous work Trinh et al. (2024), we introduced AlphaGeometry (AG1), neuro-symbolic system that demonstrated significant step towards mastering this domain, achieving 54% solve rate on all 2000-2024 IMO geometry problems. AG1 combines language model (LM) with symbolic engine to effectively tackle these challenging problems. Despite its success, AG1 exhibited limitations in several key areas. Its performance was constrained by the scope of its domain-specific language, the efficiency of the symbolic engine, and the capacity of the initial language model. As result, when considering all the recent IMO geometry problems from the year 2000 until now, AG1 can only achieve solving rate of 54%. This paper introduces AlphaGeometry2 (AG2), substantial upgrade that addresses these limitations and significantly enhances performance. AG2 leverages more powerful Gemini-based language model trained on larger and more diverse dataset. We also introduce significantly faster and more robust symbolic engine, incorporating optimizations such as reduced rule set and enhanced handling of double points. Furthermore, we expand the domain language to encompass wider range of geometric concepts, including locus theorems and linear equations. To further improve performance, 2 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry we develop novel search algorithm that explores broader range of auxiliary construction strategies, and employs knowledge-sharing mechanism to expand and accelerate the search process. Finally, we make progress towards building fully automated and reliable system that solves geometry problems in natural language. To do this we utilize Gemini Team Gemini (2024) to translate problems from natural language into the AlphaGeometry language and implement new automated diagram generation algorithm. These enhancements culminate in substantial improvement in performance: AG2 achieves an impressive 84% solve rate on all 2000-2024 IMO geometry problems, demonstrating significant leap forward in AIs ability to tackle challenging mathematical reasoning tasks, and surpassing an average IMO gold medalist. AlphaGeometry2 Key Improvements: Expanded Domain Language: Covering locus-type theorems, linear equations, and nonconstructive problem statements. Stronger and faster Symbolic Engine: Optimized rule set, added handling of double points, and faster implementation in C++. Advanced Novel Search Algorithm: Utilizing multiple search trees with knowledge sharing. Enhanced Language Model: Leveraging the Gemini architecture, trained on larger and more diverse dataset. 2. More general domain language First introduced in Trinh et al. (2024), AG1 uses simple domain specific language that consists of nine basic predicates listed in Table 1. While these predicates are sufficient to cover 66% of all geometry problems for 2000-2024 IMO, AG1 language does not allow talking about linear equations, movements of points/lines/circles or common problems such as Find the angle .... Below we explain how AG2 addresses these and other challenges. Name Meaning ğ´ğµ = ğ¶ ğ· ğ´ğµ ğ¶ ğ· ğ´ğµ ğ¶ ğ· ğ´, ğµ, ğ¶ are collinear ğ´, ğµ, ğ¶, ğ· are concyclic points cong d perp d para d coll cyclic d eqangle d h Directed angle between ğ´ğµ and ğ¶ ğ· is the same as the one between ğ¸ğ¹ and ğº ğ» eqratio d h aconst d rconst d ğ´ğµ/ğ¶ ğ· = ğ¸ğ¹/ğº ğ» Angle between ğ´ğµ and ğ¶ ğ· is equal to ğ‘¥, where ğ‘¥ [0, 180) ğ´ğµ : ğ¶ ğ· = ğ‘¦ where ğ‘¦ is constant Table 1 AG1 predicates. First of all, AG2 adds two predicates to allow questions of type Find x: 1. acompute d means Find the angle between ğ´ğµ and ğ¶ ğ·. 2. rcompute d means Find the ratio ğ´ğµ/ğ¶ ğ·. In some geometry problems, including the one appearing at IMO 2024, there are linear equations 3 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 of geometric quantities (angles, distances) that AG1 cannot capture. To express these notions, AG2 adds the following three predicates: 1. distmeq a1 b1 a2 b2 ... an bn t1 t2 ... tn means ğ‘¡1 log( ğ´1 ğµ1) + ğ‘¡2 log( ğ´2 ğµ2) + ... + ğ‘¡ğ‘› log( ğ´ğ‘› ğµğ‘›) + ğ‘¦ = 0. 2. distseq a1 b1 a2 b2 ... an bn t1 t2 ... tn means ğ‘¡1 ğ´1 ğµ1 + ğ‘¡2 ğ´2 ğµ2 + ... + ğ‘¡ğ‘› ğ´ğ‘› ğµğ‘› = 0. 3. angeq a1 b1 a2 b2 ... an bn t1 t2 ... tn means ğ‘¡1ğ‘‘( ğ´1 ğµ1) +ğ‘¡2ğ‘‘( ğ´2 ğµ2) +...+ğ‘¡ğ‘›ğ‘‘( ğ´ğ‘› ğµğ‘›) + ğ‘¦ = 0 where ğ‘‘( ğ´ğµ) is the angle between the undirected line ğ´ğµ and the horizontal line. Another category that was not supported in AG1, so-called locus problems, talk about movements of objects such as points, lines, and circles. AG2 captures this through new predicate syntax. Table 2 lists 11 locus cases with the corresponding predicate and their syntax. Here we make use of one new token * to serve as the fixed-point placeholder. Case Name Subcase Syntax for question 1 2 3 4 5 6 7 8 9 10 11 circle through fixed points line through fixed points point on fixed line point on fixed circle fixed distance fixed direction fixed angle ? cyclic * : circumcircle center radius ? cong * : line bline of pline of tline of ? coll *: ? cong * * : ? para * : ? perp * : ? coll * * : ? cyclic * * * : ? cong * * : ? para * * : ? eqangle c * * * * : Table 2 The 11 types of locus-type statements, and their corresponding predicate syntax in the AG domain language. Furthermore, in AG2 proofs, we include explicit predicates to represent diagram checks for topological/non-degeneracy conditions: 1. sameclock d means the direction ğ´ ğµ ğ¶ has the same clockwise direction to ğ· ğ¸ ğ¹. 2. noverlap means ğ´ and ğµ are distinct points. 3. lessthan d means ğ´ğµ < ğ¶ ğ·, which is statement used in the SSA triangle congruence theorem. AG2 can also prove points being non-distinct by introducing new predicate, overlap (points ğ´ and ğµ are overlapping points), where any predicate involving ğ´ can also be used for ğµ and vice versa. During the deduction closure, overlapping points can be defined by being center of the same circle; we therefore introduce another predicate cyclic_with_center to capture this scenario. Here, cyclic_with_center a1 a2 ... an means ğ‘1 = ğ‘2 = = ğ‘ğ‘¥ is the center of the circle that goes through ğ‘ğ‘¥+1...ğ‘ğ‘› (in case ğ‘¥ = 0, it is equivalent to cyclic). Notice that, when describing problem, AG1 uses at most 2 predicates to define point, i.e. each point is defined as the intersection between at most two objects (line or circle). This limits AG1 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 to only constructive problems - problems where all points can be straightforwardly constructed by following their definition order and taking the intersection of two well-defined objects. In AG2, we relax this constraint to cover more problems where points can be defined by at least three predicates, making the diagram construction non-trivial. Our approach for automating this process is discussed in the next section. All changes described in this section improve the AG domain language coverage from 66 to 88% on all 2000-2024 IMO geometry problems. The remaining 12% contain 3D geometry, inequalities, non-linear equations, and countably many points (i.e. problems that have ğ‘› points where ğ‘› is an arbitrary positive integer). All problems (covered and not covered) by AG1 and AG2 can be found on Figure 8. Not covered are referred as \"Not attempted\". 3. Automated problem formalization and diagram generation Automated formalization. major weakness of AlphaGeometry, and other similar neuro-symbolic systems, is the need to manually transform input problems from natural language into domain specific language. For example, simple geometry problem in natural language, Given triangle ğ´ğµğ¶ with two equal sides ğ´ğµ = ğ´ğ¶, prove that angles ğµ and ğ¶ are equal, becomes triangle c; = ? eqangle c a in the AlphaGeometry domain language. Automating this process, called formalization, is an active area of research (see for example, Jiang et al. (2023); Poiroux et al. (2024); Szegedy (2020); Wu et al. (2022)). It is significantly more complicated problem compared to translation between human languages. While translation aims to preserve meaning, formalization frequently requires re-formulating the original problem into an alternative form, and sometimes disambiguating the nuances in the original problem statement. Automated formalization (auto-formalization), therefore, demands significant background knowledge and problem-solving skills on its own. Given that recently foundation models started demonstrating such capabilities, we use one such model, Gemini Team Gemini (2024), to automate the problem formalization for AlphaGeometry. We start by manually translating several dozens of geometry problems into the AG language. Then we use these examples to write few-shot prompt asking Gemini to translate given geometry problem from natural language into the AG language. We query Gemini five times with this prompt followed by another Gemini call asking to combine these results into one final answer. With this approach we are able to formalize 30 out of 39 formalizable IMO 2000-2024 geometry problems. For easier geometry problems, it is very consistent and makes almost no mistakes. Automated diagram generation. Another manual part of our pipeline was diagram generation. In AG1, each point is defined by at most two basic predicates recalled in Table 1, the problem is therefore defined constructively and diagrams can be generated automatically. In AG2, we allow one or multiple points being defined simultaneously by an arbitrary number of predicates, allowing us to also cover non-constructive problems. Consider non-constructive problem statement, Let ğ´ğµğ¶ be triangle with incenter ğ¼, such that ğ¼ ğ´ = 2ğ¼ ğµ ..., here point ğ¼ is not only defined as an incenter, i.e. the intersection of two internal bisectors, but also defined by third predicate ğ¼ ğ´ = 2ğ¼ ğµ and there is no general strategy to construct such four points. Since AG2 covers non-constructive problems, diagram construction becomes non-trivial part of the pipeline and generally requires human intervention. Similar to Krueger et al. (2021), we propose the following algorithm to automatically generate diagrams given non-constructive problem specifications: Let ğ‘¥ â„2ğ‘› be vector representing all coordinates of all points. We encode every constraint ğ‘ in the diagram, including the goal, as ğ‘“ğ‘ (ğ‘¥) = 0 with nonlinear function ğ‘“ğ‘. We numerically search for 5 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 suitable ğ‘¥ in two steps. First, we run the ADAM gradient descent optimization on the mean-squared error loss (cid:205)ğ‘ğ¶ ğ‘“ğ‘ (ğ‘¥) where ğ¶ is the set of all the constraints, together with non-degeneracy loss. For every two points ğ´, ğµ, we add the loss of the form 1/( ğ´ğµ2 + ğœ–), and an ğ¿2 normalization on all points to prevent their value from becoming too large. After the loss in the ADAM optimization meets certain threshold, we stop caring about the non-degeneracy, and switch from gradient descent optimization to the Gauss-Newton-Levenberg method to look for numerical solution of combined underand over-determined system of nonlinear equations. This two-stage optimization method builds upon the methodology introduced in Krueger et al. (2021). While the first stage remains unchanged, we incorporate novel second stage. This addition addresses the practical limitations encountered when tuning the gradient descent optimization in the original method, where achieving consistently satisfactory error margin proved challenging. We benchmark this method on 44 IMO problems formalized in AG language (see Figure 8) and are able to find diagrams for 41 of them. We run the two-stage convergence procedure in multiple parallel processes, and in loop which restarts and generates another random initial configuration after failure. This way, 40 / 44 problems got their diagram generated within 1 hour using approximately 40 processes for each problem (many problems got their diagram within seconds, on the first try). For the remaining 4 problems, we run the same procedure longer and with more parallelization. This way, we also obtained diagram for IMO-2011-6 after 400 minutes using 3333 processes. 4. Stronger and faster symbolic engine symbolic engine is core component of AlphaGeometry. We call it DDAR, Deductive Database Arithmetic Reasoning. It is an algorithm to compute the deduction closure, i.e. the set of all deducible facts given core set of initial facts. DDAR builds this deduction closure by following fixed set of deduction rules, iteratively adding new facts into the deduction closure until no more can be added. DDAR drives both the training data generation for our language model and the search for deduction steps during the test-time proof search. In both scenarios, speed is crucial. Faster data generation allows larger and more aggressive data filtering, while faster proof search enables more extensive search, which increases the likelihood of finding solution within given time budget. There are three main DDAR improvements that will be discussed in the next three sections. Capability of handling double points. Faster algorithm. Faster implementation. 4.1. Handling double points While re-implementing DDAR, we tried to keep approximately the same logical strength as the original algorithm, just little stronger because of the implementation differences (for example Thales Theorem was replaced with more general Central Angle Theorem). However, DDAR1 is missing one key feature, which is crucial for tackling hard problems: it is unable to accept two points with different names and the same coordinates. For example, imagine problem where we intersect two lines ğ‘, ğ‘ at point ğ‘‹, and intend to prove that ğ‘‹ lies on certain circle ğœ”. The most plausible approach might be via reformulation instead of proving that the intersection of ğ‘, ğ‘ is on ğœ”, we prove that the intersection of ğ‘, ğœ” lies on ğ‘. This is equivalent, yet can be much easier to prove because we can move angles on the circle. For 6 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 Figure 1 Handling double\" points in AG2. It is hard to prove that the intersection of ğ‘, ğ‘ is on ğœ”. But if language model suggests construction ğ‘‹ ğ‘ ğœ”, then DDAR can prove the goal by proving ğ‘‹ ğ‘, and hence ğ‘‹ = ğ‘‹ . illustration see Figure 1. To do such reformulation of reasoning with double points, we proceed through the following 4 steps: Construct new point ğ‘‹ as the intersection of ğ‘, ğœ” (we dont know yet that ğ‘‹ coincides with ğ‘‹). This is an auxiliary construction that must be predicted by language model. Prove that ğ‘‹ lies on ğ‘. Since both ğ‘‹ and ğ‘‹ lie on both, ğ‘, ğ‘, we conclude ğ‘‹ = ğ‘‹ . Consequently ğ‘‹ lies on ğœ”. 4.2. Faster algorithm The DDAR1 algorithm is processing list of rules, and tries to apply each rule to all combinations of points. This process involves candidate searching step, whose time complexity is polynomial in the number of points, and clause matching step, whose time complexity is exponential in the number of clauses per premise. In theory the worst case for searching similar triangle candidates in AG1 is ğ‘‚(ğ‘ 8), which is one of the most time consuming steps. The exponential clause matching step is another expensive step. To make the search more efficient, we take all essential rules, and hard-code search for their application, which reduces the number of queries for the AR sub-engine to at most cubic. Furthermore, we discard the explicit rules for angles and distances (e.g. about perpendicular or parallel lines) all such deductions happen automatically in the AR engine. The two main time-consuming parts of DDAR are search for similar triangles and search for cyclic quadrilaterals. In AG2, we designed an improved DDAR2 algorithm. For similar triangles, we go through all triples of points, hash their shape, and detect similar pair if the shape is recognized twice. For cyclic quadrilaterals, we go through all pairs (point ğ‘‹, segment ğ´ğµ), and hash the value of ( ğ´, ğµ, ğ´ğ‘‹ ğµ). If such triple repeats, we get cyclic quadrilateral. By the value of segment ğ´ğµ, or ğ´ğ‘‹ ğµ, we mean symbolic normal form calculated by the AR-submodule. This submodule keeps track of known linear equations between angles, distances, and log-distances, understands its algebraic consequences, and can reduce any linear expression to its normal form. 7 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 4.3. Faster implementation While the new algorithm already significantly accelerates DDAR, we make further speed improvements by implementing its core computation (Gaussian Elimination) in C++. The new C++ library, which is exported into Python via pybind11 Jakob et al. (2017), is over 300 times faster than DDAR1. In order to benchmark the speed improvement, we select set of 25 IMO problems that cannot be solved by DDAR (see Figure 8), and run the test 50 times on machine with AMD EPYC 7B13 64 core CPU. While on average DDAR1 finishes its computations in 1179.57 8.055 seconds, DDAR2 is much faster - finishing in 3.44711 0.05476 seconds1. 5. Better synthetic training data Supplementing the symbolic engine with language model was key to AG1 success, bringing the solve rate from 14 (pure deduction proofs) to 25 out of 30 selected IMO problems Trinh et al. (2024). This language model was trained on large amount of algorithmically generated synthetic data. In AG2 we use the same procedure. Similar to AG1, our synthetic data generation method starts by sampling random diagram, and uses the symbolic engine to deduce all possible facts from it. For each of the deduced facts, traceback algorithm is used to extract the corresponding premises, auxiliary points, and deduction steps that prove the fact. Our data generation method deliberately avoids the use of human-crafted problems as initial diagram seeds, and strictly starts from random diagrams. This design choice eliminates the risk of data contamination and allows for the exploration of theorem distributions that may extend beyond established human knowledge. This approach contrasts with methods like TongGeometry Zhang et al. (2024), which rely on human expertise and existing problem diagrams to guide and filter data generation. In AG2, we keep using random diagrams as initial seeds and continue to push for better synthetic training data. Larger, more complex diagrams and better data distribution. First of all, we scale up resources for data generation, and do more careful re-balancing of the data distribution. As demonstrated on Figure 2, compared to AG1, AG2: Explores random diagrams at twice the size, allowing for extracting much more complex problems. Produces theorems at up to 2x more complex, i.e. number of points and premises. Produces up to 10x more complex proofs, i.e. 10x more proof steps. Has more balanced data distribution between question types. Has more balanced data distribution between problems with and without auxiliary points. More types of theorems. Besides generating theorems that prove classic statements such as AB = CD, AG2 data generating algorithm also produces problems of locus type, i.e. asserting statements such as When moves on line/circle Y, then moves on fixed line/circle T. Introduced in Section 2, these statements are not supported in the AG1 data generation algorithm, as there is no notion of movement and movement dependency. In AG2, we record the movement dependency for each point ğ‘‹ during random diagram generation through function ğ‘ƒ(.) with the following definition: 1The average running time may vary depending on the machine status at different times. 8 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 (a) AG2 includes more complicated/longer problems compared to AG1. (b) AG2 has more balanced distribution of examples per question type. Figure 2 Training data distributions for AG2 compared to AG1. (c) AG2 has much more balanced mix between proofs with auxiliary points and proofs without (50:50 in AG2 vs 9:91 in AG1). ğ‘ƒ( ğ´): the set of points that control the movements of ğ´, where ğ´ is point or set of points, defined in constructive problem statement. Two examples of ğ‘ƒ are presented in Table 3 and all cases where locus-type statements are detected are shown in Table 5. If = midpoint c, = midpoint = on_line Then ğ‘ƒ(ğ‘‘) = {ğ‘, ğ‘} ğ‘ƒ(ğ‘) = {ğ‘, ğ‘, ğ‘} Table 3 Two examples of ğ‘ƒ. Top row: since ğ‘‘ is uniquely defined as the midpoint of ğ‘ and ğ‘, and ğ‘ is uniquely defined as the midpoint of ğ‘ and ğ‘, the source of movement for ğ‘‘ is ğ‘ and ğ‘. Second row: Since ğ‘ can be anywhere on line ğ‘ğ‘, ğ‘ itself is also part of its movement source. 9 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 Faster data generation algorithm. We also improved the speed of the data generation algorithm. Recall that in AG1, we first run deduction closure on random diagrams, and traceback\" to obtain the minimal problem and minimal proof that proves each fact in the closure. To obtain the minimal problem in AG1, we have to exhaustively remove different subsets of points from the problem and rerun DDAR to check provability. Such search can find the subset with the smallest cardinality, however as an exponential search, it is infeasible for larger number of points. Therefore, we switched to greedily discarding algorithm shown in Figure 3, which uses just linear number of checks for whether set of points suffices to prove the goal. The greedy algorithm is guaranteed to find minimal set of points with respect to inclusion as long as the check is monotonic (if ğ´ ğµ, then check_provable( ğ´) check_provable(ğµ)). In reality, we also require the pruned set to remain closed under construction dependencies (so that we can still run random construction). If we incorporate this condition into the check_provable predicate, it stops being monotonic. This difficulty can be fixed by processing the points via the algorithm from Figure 3 in reversetopological order (first points that do not depend on any other points, and last the initial points of the construction). def n _ n ( n h _ v e : l e [ [ [ n ] ] , bool ] ) : : [ n ] , pruned = ( n ) p in e _ o i ( n ) : h _ v e ( pruned {p } ) : pruned = pruned {p} return Figure 3 Basic greedy algorithm to find minimal set of points satisfying monotonic predicate check. 6. Novel search algorithm In AG1, we use simple beam search to discover proofs. In AG2, we design novel search algorithm, in which several differently configured beam searches are executed in parallel and are allowed to help each other through knowledge sharing mechanism (see Figure 4). To improve the robustness of our system we use multiple different language models for each search tree configuration. We call this search algorithm Shared Knowledge Ensemble of Search Trees (SKEST). It works as follows. In each search tree, node corresponds to one attempt at auxiliary construction followed by one attempt of the symbolic engine run. If the attempt succeeds, all search trees terminate. If the attempt fails, the node will write down the facts that the symbolic engine managed to prove into shared facts database. These shared facts are filtered such that they are not the auxiliary point specific to the node itself, but only relevant to the original problem. This way, these facts can also be useful to the other nodes in the same search tree, and the nodes in different search trees. Below we list various types of search trees, which we employ to make sure different parts of the search space are explored effectively: \"Classic\" search tree: the same beam tree search used in AG1, where language model is asked to produce one auxiliary point at each node. Tree predicting multiple auxiliary points at each node: language model is allowed to produce as many auxiliary points as it wants at each tree node. Recall that this is possible because our 10 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 Figure 4 Overview of our search algorithm. We employ several different search trees which can share facts they proved via special knowledge sharing mechanism. LM is trained to produce full proofs, starting with auxiliary points and followed by the deduction steps2. Note that even though we want our models to generate all necessary auxiliary points in one query, in practice we observe the need to call the model multiple times given previously produced auxiliary points. Allowing the model to produce multiple auxiliary points accelerates finding solution and effectively increases the tree search depth. Tree predicting different types of aux points uniformly. Recall that LM outputs for auxiliary points look like x00 : cong d (00) coll (01), i.e. construct point such that = and are collinear\". Typically to predict aux points we prompt the language model with the first token x00 and let it generate the rest. Here, instead, we prompt the LM with x00 : cong, x00 : coll, x00 : cyclic, x00 : perp, etc. to force uniform distribution 2See more detailed discussion on producing full proofs with language model alone in Section 11 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 across the first 4 tokens, and then let the LM generate the rest. Deep but narrow tree (e.g. beam size 64 and depth 10). Shallow but wide tree (e.g. beam size 512 and depth 4). System design details. For proof search, we use TPUv4 to serve multiple replicas per model3 and let different search trees within the same model to query the same server under their own search strategy. Besides running these search trees asynchronously, we also run the LM workers asynchronously with DDAR workers. The LM workers write down the content of the nodes they explored to database, and the DDAR workers asynchronously pick up these nodes and attempt them. The DDAR workers coordinate between themselves to make sure they divide work equally. single DDAR worker pool is shared across different problems (if multiple problems are solved at once), such that problems that got solved earlier release its own DDAR compute resources for the rest of the problems that are being solved. 7. Better language model The final AG2 improvement is new language model. In this section we discuss our new training and inference setups. 7.1. Training setup AG1 language model, custom transformer, was trained in the unsupervised fashion in two phases: training on problems with and without auxiliary constructions followed by training on only problems that contain auxiliary constructions. For AG2 we leverage the Gemini training pipeline and simplify training to just one phase: unsupervised learning on all data. Our new language model is sparse mixture-of-expert Transformer-based model that builds on Gemini Team Gemini (2024) and trained on AG2 data described in Section 5. We train multiple models of different size using three training setups: 1. Training from scratch with custom tokenizer in the domain specific language (AG1 setup). 2. Fine-tuning already pre-trained custom math specialized Gemini models in natural language (for more details see Appendix A). 3. Multimodal training from scratch with an additional image input - diagram of the given geometry problem (for more details see Appendix B). Apart from large synthetic training set of around 300 million theorems, we create three evaluation sets: 1. Synthetic problem set with and without auxiliary points, eval\". 2. Synthetic problem set with only auxiliary points, eval_aux\". 3. Special set of geometry problems from IMO 2000-2024 that have been solved by AlphaGeometry previously, imo_eval\". All these sets contain full proofs, and during training we compute perplexity loss on them. Note, however, that these are only proxy metrics for two reasons. First, during inference (just like in AG1) we only use auxiliary points suggested by the language model, while the perplexity is computed on 3The exact number of TPUs depends on the model size. 12 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 the entire proof. Second, there might be multiple ways to solve given problem, but perplexity is computed for one particular solution. Just like in AG1, our main downstream metric is the solve rate on IMO problems, where the language model produces auxiliary points followed by DDAR run via beam search described in section 6. These results will be discussed in Section 8. We train our models with the largest possible batch size allowed by the hardware4 using TPUv4. learning rate schedule is linear warm-up followed by the cosine anneal. Learning rate hyperparameters are determined from scaling laws. On Figure 5 we illustrate learning curves for different size Gemini models in terms of parameter count. As expected, increasing the model size decreases perplexity loss for train, eval and our special IMO evaluation set. Figure 5 Learning curves for AlphaGeometry2 language models of different sizes in terms of parameter count (m\" - million, B\" - billion). Increasing the model size results in decreasing loss for train, eval and IMO evaluation sets. 7.2. Inference setup new problem is solved via the search algorithm described in section 6 with multiple search trees and multiple language models of different sizes. In contrast to AG1, we use top-k sampling with temperature ğ‘¡ = 1.0 and ğ‘˜ = 32. Note that high temperature and multiple samples are essential for solving IMO problems. With the greedy decoding ğ‘¡ = 0.0, ğ‘˜ = 1, and no tree search, our models can solve only two problems out of 26 that require auxiliary constructions. Increasing the temperature to ğ‘¡ = 1.0 and using ğ‘˜ = 32 samples (without search tree) allows our language models to solve 9 out of 26 problems. Lower temperatures ğ‘¡ < 1.0 do not produce diverse enough auxiliary constructions (see Figure 6), while higher temperatures result in the increasing number LM outputs with wrong domain language syntax. The analysis string. In AG1, the interface between LM and DDAR is minimal: DDAR takes auxiliary constructions proposed by LM, and the LM stops proposing auxiliary constructions when DDAR 4We did not observe any training issues compared to smaller batches. 13 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry Figure 6 Ratio of unique samples for various temperatures for top-k sampling. Figure 7 Number of 2000-2024 IMO problems solved by one language model as function of seen tokens during training. succeeds in finding solution. In AG2, we enrich this neuro-symbolic interface by letting the LM know about the deductions made by DDAR before proposing auxiliary constructions. Namely, we feed the following information into the LM: ğ‘†1: Set of facts deducible by DDAR given the original problem premises. ğ‘†2: Set of facts deducible by DDAR given the original problem premises and assuming the goal predicate is also true. ğ‘†3: Set of facts that is correct numerically (by inspecting the diagram). Note that by definition, ğ‘†1 ğ‘†2 ğ‘†3. Once these three sets are computed, we serialize and concatenate them into string called analysis string, using our domain specific language. This string is fed into the LM, together with the original problem statement as follows: <problem_statement> serialized(ğ‘†1) serialized(ğ‘†2 ğ‘†1) serialized(ğ‘†3 ğ‘†2). In contrast, the input to the AG1 LM is simply <problem_statement>. 8. Results Our main downstream metric is the solve rate on IMO geometry problems. There are total of 45 geometry problems in 2000-2024 IMO, which we translate into 50 AlphaGeometry problems (we call this set IMO-AG-50). Some problems are split into two due to specifics of our formalization. Figure 8 demonstrates our main result: AlphaGeometry2 solves 42 out of 50 of all 2000-2024 IMO geometry problems, thus surpassing an average gold medallist for the first time5. More details are presented in Table 4, which compares various AG2 configurations with other systems, such as AG1 Trinh et al. (2024) and TongGeometry Zhang et al. (2024). We also perform an additional evaluation on new set of 30 hardest IMO shortlist problems, which are formalizable in the AG2 language, and which have never appeared at IMO. For these additional results see Appendix D. On Figure 7 we present the IMO solve rate as function of training time (seen tokens during training) for one language model coupled with DDAR via the \"classical\" tree search described in Section 6. Interestingly, AlphaGeometry2 can already solve 27 out of 50 problems after only 250 training steps with batch size 256, or around 200 million tokens6. We also run ablation studies on how inference settings affect the overall performance (see Figure 9). For single search tree we find 5Sinha et al. (2024) previously claimed to achieve gold medalist performance, but it was done on subset of IMO problems. 6Note that even without the language model, AlphaGeometry2 can solve 16 problems with its symbolic engine alone (see Figure 8). 14 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 Figure 8 AlphaGeometry2 results on all 2000-2024 IMO geometry problems. Problems are grouped together based on their status, and ordered chronologically within the groups. System description OpenAI o1 Gemini thinking AG1 DDAR Trinh et al. (2024) AG2 DDAR TongGeometry DD Zhang et al. (2024) Average bronze medalist Wu with AG1 DDAR Sinha et al. (2024) Average silver medalist AG1 Trinh et al. (2024) Average gold medalist Wu + AG1 Sinha et al. (2024) TongGeometry w/o value Zhang et al. (2024) AG2 with AG1 setup TongGeometry full setting Zhang et al. (2024) AG2 full setting IMO-AG-50 solved 0 0 14 16 - 27.1 - 33.9 27 40.9 - - 38 - 42 IMO-AG-30 solved 0 0 14 15 18 19.3 21 22.9 25 25.9 27 28 28 30 30 Table 4 Evaluation on IMO-AG-50 benchmark. IMO-AG-50 contains all IMO 2000-2024 geometry problems, while IMO-AG-30 introduced in Trinh et al. (2024) contains only subset formalizable in terms of the AG1 language. that the optimal configuration is beam size of 128, beam depth of 4 and 32 samples. More samples or larger beam search does not help solve more problems. Our geometry experts and IMO medalists consider many AlphaGeometry solutions to exhibit superhuman creativity. In Appendix we provide several such examples with the detailed analysis. Out of the unsolved IMO problems, we have 2 attempted but not solved and 6 unformalizable Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 Figure 9 Number of 2000-2024 IMO geometry problems solved for different inference settings with one search tree. We start with beam size 512, beam depth 4, 32 samples and vary one of the parameters while keeping others fixed. problems. The unformalizable problems involve inequalities and variable number of points, which are currently not covered by the AlphaGeometry2 language. Two of the remaining unsolved IMO problems (IMO 2018 P6, IMO 2023 P6) involve advanced geometry problem solving techniques such as inversion, projective geometry or radical axis, which are not implemented in our current DDAR. While such problems in theory can be solved without these techniques, such solutions would require longer inference time, longer proofs and more auxiliary constructions to make up for the lack of the aforementioned machinery in our current DDAR, which hinders AlphaGeometrys current problem solving capabilities. 9. Conclusions and Future work This paper introduced AlphaGeometry2, significant upgrade to AlphaGeometry that addresses previous limitations and enhances performance in several key areas. AG2 incorporates more powerful language model trained on larger and more diverse dataset, faster and more general symbolic engine, an expanded domain language, and novel proof search algorithm. These improvements have resulted in substantial leap in performance, with AG2 achieving an 84% solve rate on 2000-2024 IMO geometry problems, significantly improving upon the 54% solve rate achieved by its predecessor. We also presented several studies related to language modeling. First, we showed that our models are quite capable in generating not only auxiliary constructions, but also full proofs, demonstrating potential for modern language models to operate without external tools, such as symbolic engines. Secondly, we discovered that for AlphaGeometry neither tokenizer, nor domain language used to train the model, plays decisive role. We obtained similar results for custom tokenizers with small vocabularies and the generic large Gemini tokenizer. Training in the domain specific language got us similar results compared to training in natural language. Thirdly, we compared training from scratch and fine-tuning language models pre-trained on math datasets. We found that despite training on the same AlphaGeometry dataset, these models learn slightly different skills, and combining them into our novel search algorithm, Shared Knowledge Ensemble of Search Trees, improves the overall solve rate. Despite achieving an impressive 84% solve rate on all 2000-2024 IMO geometry problems, there is 16 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 still room for improvements. First, our domain language does not allow talking about variable number of points, non-linear equations and problems involving inequalities, which must be addressed in order to fully solve geometry. Second, AG2 has not solved all IMO and IMOSL problems. We hypothesize that breaking problems into subproblems and applying Reinforcement learning approaches could close this gap. Finally, in this paper we reported progress on building fully automated geometry problem solving system, which takes input in natural language and outputs solution reliably without any hallucinations. Despite good initial results, we think the auto-formalization can be further improved with more formalization examples and supervised fine-tuning."
        },
        {
            "title": "References",
            "content": "H. Chae, S. Yoon, C. Y. Chun, G. Go, Y. Cho, G. Lee, and E. K. Ryu. Decomposing complex visual In The 4th Workshop on comprehension into atomic visual skills for vision language models. Mathematical Reasoning and AI at NeurIPS24, 2024. URL https://openreview.net/forum? id=nFU4xCyoe0. S.-C. Chou. Proving and discovering geometry theorems using Wus method. The University of Texas at Austin, 1985. S.-C. Chou, X.-S. Gao, and J.-Z. Zhang. Automated production of traditional proofs for constructive geometry theorems. In [1993] Proceedings Eighth Annual IEEE Symposium on Logic in Computer Science, pages 4856. IEEE, 1993. S.-C. Chou, X. Gao, and J.-Z. Zhang. Machine proofs in geometry: Automated production of readable proofs for geometry theorems, volume 6. World Scientific, 1994. S.-C. Chou, X.-S. Gao, and J.-Z. Zhang. Automated generation of readable proofs with geometric invariants: I. multiple and shortest proof generation. Journal of Automated Reasoning, 17(3): 325347, 1996. S.-C. Chou, X.-S. Gao, and J.-Z. Zhang. deductive database approach to automated geometry theorem proving and discovering. Journal of Automated Reasoning, 25(3):219246, 2000. B. Deiseroth, M. Brack, P. Schramowski, K. Kersting, and S. Weinbach. T-free: Tokenizer-free generative llms via sparse representations for memory-efficient embeddings, 2024. URL https: //arxiv.org/abs/2406.19223. W. Jakob, J. Rhinelander, and D. Moldovan. pybind11 seamless operability between c++11 and python, 2017. https://github.com/pybind/pybind11. A. Q. Jiang, W. Li, and M. Jamnik. Multilingual mathematical autoformalization. arXiv preprint arXiv:2311.03755, 2023. D. Kapur. Geometry theorem proving using hilberts nullstellensatz. In Proceedings of the fifth ACM symposium on Symbolic and algebraic computation, pages 202208, 1986a. D. Kapur. Using grÃ¶bner bases to reason about geometry problems. Journal of Symbolic Computation, 2(4):399408, 1986b. R. Krueger, J. M. Han, and D. Selsam. Automatically building diagrams for olympiad geometry problems. In CADE, pages 577588, 2021. A. Poiroux, G. Weiss, V. KunÄak, and A. Bosselut. Improving autoformalization using type checking. arXiv preprint arXiv:2406.07222, 2024. 17 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry A. K. Singh and D. Strouse. Tokenization counts: the impact of tokenization on arithmetic in frontier llms, 2024. URL https://arxiv.org/abs/2402.14903. S. Sinha, A. Prabhu, P. Kumaraguru, S. Bhat, and M. Bethge. Wus method can boost symbolic ai to rival silver medalists and alphageometry to outperform gold medalists at imo geometry, 2024. URL https://arxiv.org/abs/2404.06405. C. Szegedy. promising path towards autoformalization and general artificial intelligence. In Intelligent Computer Mathematics: 13th International Conference, CICM 2020, Bertinoro, Italy, July 2631, 2020, Proceedings 13, pages 320. Springer, 2020. Team Gemini. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. T. H. Trinh, Y. Wu, Q. V. Le, H. He, and T. Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476, 2024. W.-t. Wu. On the decision problem and the mechanization of theorem-proving in elementary geometry. In Selected Works Of Wen-Tsun Wu, pages 117138. World Scientific, 2008. Y. Wu, A. Q. Jiang, W. Li, M. N. Rabe, C. Staats, M. Jamnik, and C. Szegedy. Autoformalization with large language models, 2022. URL https://arxiv.org/abs/2205.12615. C. Zhang, J. Song, S. Li, Y. Liang, Y. Ma, W. Wang, Y. Zhu, and S.-C. Zhu. Proposing and solving olympiad geometry with guided tree search. arXiv preprint arXiv:2412.10673, 2024. 18 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 a n i o h l g y e n d . . . C - t c i u - a e w , m n f n = l T , g m a a i - e b i n , o e \" : s t o i t : o A i 2 9 7 4 1 8 7 1 1 8 5 0 1 6 0 1 7 ğ‘‘ , ğ‘ , ğ‘ i x fi s m ğ‘ ) ğ‘‘ , ğ‘ , ğ‘ ( ğ‘ƒ ) ğ‘ ( ğ‘ƒ i ğ‘ , ğ‘ fi l a s l i ğ‘‘ ğ‘ l ) ğ‘ , ğ‘ ( ğ‘ƒ ) ğ‘‘ , ğ‘ ( ğ‘ƒ o x fi u t ğ‘‘ , ğ‘ , ğ‘ ğ‘‘ , ğ‘ ğ‘ , ğ‘ ğ‘ ğ‘ ğ‘ , ğ‘ , ğ‘ ğ‘ , ğ‘ ğ‘ fi ğ‘ ğ‘ w e t e l c fi o v ğ‘ o x fi ) ğ‘‘ , ğ‘ , ğ‘ ( ğ‘ƒ ) ğ‘ ( ğ‘ƒ ) ğ‘‘ , ğ‘ ( ğ‘ƒ ) ğ‘ , ğ‘ ( ğ‘ƒ m h , o x fi t s q e ğ‘ & ğ‘ ğ‘€ ) ğ‘ ( ğ‘ƒ ) ğ‘ , ğ‘ ( ğ‘ƒ l fi s m ğ‘ ) ğ‘ , ğ‘ ( ğ‘ƒ ) ğ‘ ( ğ‘ƒ a c o s m ğ‘‘ ğ‘ ğ‘ e i c e ) ğ‘ ( ğ‘ƒ ) ğ‘‘ , ğ‘ , ğ‘ ( ğ‘ƒ a c t p fi u t g ğ‘ ğ‘ l i x fi s m ğ‘ ) ğ‘ ( ğ‘ƒ ) ğ‘ , ğ‘ ( ğ‘ƒ ) ğ‘ , ğ‘ ( ğ‘ƒ ) ğ‘ ( ğ‘ƒ o x fi c d fi s m ğ‘‘ ) ğ‘ , ğ‘ , ğ‘ ( ğ‘ƒ ) ğ‘‘ ( ğ‘ƒ l ğ‘ u t g ğ‘‘ ğ‘ d , ğ‘ n l c ) ğ‘ ( ğ‘ƒ ) ğ‘‘ , ğ‘ , ğ‘ ( ğ‘ƒ a c ğ‘“ , ğ‘’ , ğ‘‘ , ğ‘ , ğ‘ ğ‘“ , ğ‘’ , ğ‘‘ r e fi s m ğ‘ o t l e fi h ğ‘ ğ‘ ğ‘ n e ) ğ‘“ ) ğ‘“ , ğ‘’ , ğ‘‘ ( ğ‘ƒ ) ğ‘ , ğ‘ , ğ‘ ( ğ‘ƒ , ğ‘’ , ğ‘‘ , ğ‘ , ğ‘ ( ğ‘ƒ ) ğ‘ ( ğ‘ƒ e b n ğ‘ o ğ‘‘ ğ‘ a ğ‘ o e e ) ğ‘ ( ğ‘ƒ ) ğ‘‘ , ğ‘ , ğ‘ ( ğ‘ƒ a p ğ‘‘ , ğ‘ , ğ‘ ğ‘ , ğ‘ i x fi y a ğ‘‘ ğ‘ l fi e l t o ğ‘ ) ğ‘ , ğ‘ ( ğ‘ƒ ) ğ‘‘ , ğ‘ ( ğ‘ƒ ) ğ‘‘ , ğ‘ , ğ‘ ( ğ‘ƒ ) ğ‘ ( ğ‘ƒ o x fi o t ğ‘ o ğ‘‘ ğ‘ ğ‘ u t l ) ğ‘ ( ğ‘ƒ ) ğ‘‘ , ğ‘ , ğ‘ ( ğ‘ƒ b e Table 5 17 cases where locus-type statements are detected during data generation. These 17 cases produce 11 different types of locus-type statements, as numbered in the last column. 19 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 A. Fine-tuning of math specialized language models on AG data Even though during the initial transition to AG2, we maintained the AG1 training setup (training from scratch using custom tokenizer in the AG domain specific language), it is natural to ask whether fine-tuning language models that already possess problem solving capabilities can improve the performance. Such fine-tuning is not immediately possible due to the difference in the utilized tokenizers and training language. In this section we explore the role of the custom tokenizer and the domain specific language, followed by discussion about fine-tuning of math specialized Gemini model on AG data. Tokenizers. Tokenizer is an essential part of modern language models and, more broadly, any foundation models7. It is generally believed that tokenizer might be the major bottleneck in the models abilities to do math, e.g. see Singh and Strouse (2024). We investigate this hypothesis in the controlled setting of AlphaGeometry. To do so we train models of the same architecture with different tokenizers: custom tokenizers with vocabularies of few thousand tokens and the large language model tokenizers with vocabulary of 300k tokens. Recall that our custom tokenizers are created at word-level, i.e. each token has full meaning, as opposed to subword level tokens. In AG language there are the following types of tokens: 1. Point names: a, b, c, . . . z, a1, . . . , z1. 2. Predicate names: coll, cong, coll, cyclic, eqangle, eqratio, acompute, rcompute, aconst, rconst, distmeq, distseq, angeq, overlap, noverlap, sameclock, lessthan. 3. Number and fractions: 1, 2, 3, . . . , , /. 4. Predicate reference tokens: (000), (001), (002), . . . (999). 5. Reserved tokens: {Analysis}, {Numerical}, {FromGoal}, {Proof}, x00, :, ;, .. Somewhat surprisingly, we find that AlphaGeometry performance on the 2000-2024 IMO geometry problems stays the same with different tokenizers, which suggests that modern LLM tokenizers might be flexible enough to perform mathematical manipulations. Domain specific language. Alongside tokenizers, it is interesting to study the role of the domain specific language in the LMs ability to solve math problems. It is natural to assume that using domain specific languages simplifies mathematical manipulations and prevents obvious mistakes, which might occur from using less strict language. To investigate this, we translate all AlphaGeometry2 data from the AlphaGeometry language into natural language and train new model. Then we compare its performance against the model of the same size trained on the original AlphaGeometry data. Somewhat surprising again, we get the same results on 2000-2024 IMO geometry problems, which opens path for fine-tuning large language models pre-trained in natural language on math data. Below we demonstrate an example of translating AlphaGeometry into natural language. AlphaGeometry language: g : coll (000) coll (001) coll (002) coll (003) cong c (004) cong b (005) Natural language: Construct points g such that are collinear (000), are collinear (001), are collinear (002), are collinear (003), b = c (004), a = b (005) 7Tokenizer-free models is an active area of research, for example, see Deiseroth et al. (2024) 20 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 Fine-tuning of language models pre-trained on math data. Having shown that the custom tokenizer and the domain specific language does not play critical role for AlphaGeometry, we leverage language models pre-trained on various math data. We start with Gemini model with 3.3B parameters trained on public math datasets (see Section 7 in Team Gemini (2024)), and fine-tune it in the unsupervised manner on the AlphaGeometry data. On our IMO-AG-50 evaluation set the fine-tuned model performs on par with smaller models and the 3.3B model trained from scratch8. On the other hand, we find that even though all these models are trained on the same AG data, they do produce slightly different auxiliary points proposals and do help each other via the knowledge sharing mechanism described in Section 6, thus forming an ensemble-like system (see Figure 4). Figure 10 Learning curves for two 3B models: one is trained from scratch and another one pre-trained on math data and then fine-tuned on the AG data. The model pre-trained on math has initially lower loss but both converge to the same point after training for 200B tokens. B. Multi-modal Until now we talked about AG2 as system that couples language model together with symbolic engine. However, since our language model is based on Gemini 1.5, which is multi-modal by design (see Team Gemini (2024)), it is natural to enhance AG model through multi-modal reasoning. For this, we train new family of models that, alongside the problem text, take the corresponding diagram image as the input. For training and during test time, diagrams are built as described in Section 3. Despite promising results during the training, we do not observe any improvements in the solve rate on the downstream IMO problems when using this model alone. However, just like in case of fine-tuning pre-trained models (see Section A), we find that the multimodal model produces slightly different auxiliary point proposals. Combined with other models via the knowledge sharing 8We also train even larger models in the supervised manner and achieve the same results Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 mechanism (see Section 6) this boosts the overall performance. We hypothesize that adding the image on its own might not help that much due to very complicated diagrams, which become very crowded for the IMO problems. The image tokenization process might also play negative role, as it splits the diagram into independent sequential patches which leads to losing some spatial information. Recall also that some details about the diagram are already provided through the text, as mentioned in Section 7.2, and our symbolic engine, DDAR, does have access to topological aspects of the diagram, e.g. through inspecting sameclock predicates. Furthermore, Chae et al. (2024) shows that visionlanguage models have poor atomic visual skills, which suggests that adding visual elements might not aid the geometry problem-solving process. Finally note that the core of geometry problem-solving lies in algebraic reasoning, as previously demonstrated in Trinh et al. (2024), rather than geometric reasoning. Many human IMO contestants can reliably solve geometry problems (including very hard problems such as IMO 2011 P6) using computational methods such as complex numbers, barycentric coordinates and trigonometry bashing, which means that visual information and diagrams are not critical to solving geometry problems. C. Featured AlphaGeometry2 solutions Out of the solved IMO problems (see Figure 8), our geometry experts consider many AlphaGeometry solutions to exhibit superhuman creativity. One such example is the IMO 2024 P4 problem (see Figure 11). IMO 2024 P4: Let triangle ğ´ğµğ¶ with incenter ğ¼ satisfying ğ´ğµ < ğ´ğ¶ < ğµğ¶. Let ğ‘‹ be point on line ğµğ¶, different from ğ¶, such that the line through ğ‘‹ and parallel to ğ´ğ¶ is tangent to the incircle. Similarly, let ğ‘Œ be point on line ğµğ¶, different from ğµ, such that the line through ğ‘Œ and parallel to ğ´ğµ is tangent to the incircle. Line ğ´ğ¼ intersects the circumcircle of triangle ğ´ğµğ¶ again at ğ‘ƒ. Let ğ¾ and ğ¿ be the midpoints of ğ´ğ¶ and ğ´ğµ, respectively. Prove that ğ¾ğ¼ğ¿ + ğ‘Œ ğ‘ƒğ‘‹ = 180. The problem asks about the relationship between ğ¾ğ¼ğ¿ and ğ‘‹ ğ‘ƒğ‘Œ . The former is the angle formed by midpoint and the incenter, which usually does not go well together and cannot be computed by the angles of the main triangle ğ´ğµğ¶. Typically, human contestant would rely on trigonometry, complex numbers or other computational methods to find the solution. For AlphaGeometry, its DDAR system only relies on simple angle chasing and ratio chasing, so this necessitates the need for some auxiliary point constructions. To this end, AlphaGeometry constructs ğ¸ as point on the line BI such that ğ´ğ¸ğµ = 90, which elegantly ties these seemingly unrelated geometric elements together by creating pairs of similar triangles ğ´ğµğ¸ and ğ‘Œ ğµğ¼, ğ´ğ¿ğ¸ and ğ¼ğ‘ƒğ¶. These pairs of similar triangles create new pairs of equal angles and equal side length ratio. That being said, the point ğ¸ gives purposes to the midpoint ğ¿ of ğ´ğµ. To complete the proof, we need to prove ğ´ğ¼ ğ¾ = ğµğ‘Œ ğ‘ƒ and ğ´ğ¼ğ¿ = ğ¶ğ‘ƒğ‘‹. To this end, we need to prove that triangle ğ´ğ¾ğ¼ is similar to triangle ğµğ‘ƒğ‘Œ and triangle ğ´ğ¿ğ¼ is similar to triangle ğ¶ğ‘ƒğ‘‹, which is done by side length ratio chasing, which is obtained from the pairs of similar triangles above. full solution is published at https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/ imo-2024-solutions/P4/index.html. This solution was obtained within 30 seconds at IMO 2024 and was given the full seven points by Joseph Myers, two-time IMO gold medalist and Chair of the IMO 2024 Problem Selection Committee. Along with IMO 2024 P4, AlphaGeometry can solve many challenging problems with only 1 extra auxiliary point, some of which involves rather unconventional constructions. One such problem is IMO 2013 P3. 22 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 Figure 11 IMO 2024 P4 diagram with AlphaGeometry auxiliary construction, point ğ¸. IMO 2013 P3: Let the excircle of triangle ğ´ğµğ¶ opposite the vertex ğ´ be tangent to the side ğµğ¶ at the point ğ´1. Define the points ğµ1 on ğ¶ ğ´ and ğ¶1 on ğ´ğµ analogously, using the excircles opposite ğµ and ğ¶, respectively. Suppose that the circumcenter of triangle ğ´1 ğµ1ğ¶1 lies on the circumcircle of triangle ğ´ğµğ¶. Prove that triangle ğ´ğµğ¶ is right-angled. In this problem, AlphaGeometry simply takes the midpoint ğ· of arc Ë†ğ´ğµğ¶ containing ğµ as the extra point, which is highly unconventional construction as it is non-symmetric. Yet, it allows AlphaGeometry to uncover the fact that ğµ, ğ´1, ğ·, ğ¼ğ‘ are concyclic points, which is key result that is only true if and only if ğ´ğµ ğ´ğ¶. To prove this fact, AlphaGeometry exploits the fact that ğ‘‚1 and ğ· give rise to the similar triangle pairs ğ‘‚1ğ¶1 ğµ1 ğ‘‚1 ğµğ¶ and ğ·ğ´1 ğµ1 ğ·ğµğ´ and then uses these results to facilitate angle chasing, which gives ğ·ğ´1ğ¼ğ‘ = ğ·ğµğ¼ğ‘ and the fact that ğµ, ğ´1, ğ·, ğ¼ğ‘ are concyclic points follows. Another example is IMO 2014 P3, one of the hard geometry problems given at the IMO. 23 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 Figure 12 IMO 2013 P3 diagram with AlphaGeometry auxiliary construction, point ğ·. It allows proving ğµğ´1 ğ·ğ¼ğ‘ is cyclic, which is the key to solve this problem. Figure 13 IMO 2014 P3 diagram with AlphaGeometry auxiliary constructions. IMO 2014 P3: Convex quadrilateral ğ´ğµğ¶ ğ· has ğ´ğµğ¶ = ğ¶ ğ·ğ´ = 90. Point ğ» is the foot of the perpendicular from ğ´ to ğµğ·. Points ğ‘† and ğ‘‡ lie on sides ğ´ğµ and ğ´ğ·, respectively, such that ğ» lies inside triangle ğ‘†ğ¶ğ‘‡ and ğ¶ ğ»ğ‘† ğ¶ğ‘†ğµ = 90, ğ‘‡ ğ»ğ¶ ğ·ğ‘‡ğ¶ = 90. Prove that line ğµğ· is tangent to the circumcircle of triangle ğ‘‡ğ‘†ğ». 24 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 To our surprise, AlphaGeometry manages to prove more generalized result ğ‘‚ğ» ğµğ·, which implies that the circumcircle of ğ»ğ‘†ğ‘‡ touches ğµğ· when combining with the condition ğ» ğµğ· in the original problem. To do this, AlphaGeometry constructs points ğ¸, ğ¹, ğº, ğ¼ as reflections of ğ‘† w.r.t ğ‘‚ğ», ğ» w.r.t ğ´ğ‘‡, ğ» w.r.t ğ´ğ‘†, ğ» w.r.t ğ‘†ğ‘‡ respectively. Since the given conditions ğ¶ ğ»ğ‘† ğ¶ğ‘†ğµ = 90, ğ‘‡ ğ»ğ¶ ğ·ğ‘‡ğ¶ = 90 imply that the circumcenters of ğ¶ ğ»ğ‘†, ğ¶ ğ»ğ‘‡ lie on ğ´ğµ, ğ´ğ· respectively, the constructions of ğ¹ and ğº create cyclic quadrilaterals ğ¶ ğ»ğ‘†ğº, ğ¶ ğ»ğ‘‡ ğ¹, which will facilitate angle chasing. Moreover, the constructions of ğ¸ and ğ¼ create the cyclic quadrilateral ğ»ğºğ¼ğ¸ with center ğ‘†, and the points ğ¶, ğ‘‡ now become the circumcenter of ğ¹ ğ» ğ¼ and ğ¹ğºğ¼ respectively. Combining these facts altogether, AlphaGeometry obtains an extraordinary angle chasing proof, in contrast to the common approaches using ratio chasing (possibly combined with the knowledge of Apollonius circles), trigonometry or inversion by most human contestants. This shows that AlphaGeometry is capable of solving hard problems with only simple deduction engine. Figure 14 IMOSL 2009 G7 diagram with AlphaGeometry auxiliary constructions (colored red), key cyclic properties (colored polygons) and key similar triangle pairs (colored triangle pairs). Our final example is G7 problem from the IMO 2009 Shortlist. IMOSL 2009 G7: Let ğ´ğµğ¶ be triangle with incenter ğ¼ and let ğ‘‹, ğ‘Œ and ğ‘ be the incenters of the triangles ğµğ¼ğ¶, ğ¶ğ¼ ğ´ and ğ´ğ¼ ğµ, respectively. Let the triangle ğ‘‹ğ‘Œ ğ‘ be equilateral. Prove that ğ´ğµğ¶ is equilateral too. To the best of our knowledge, this problem previously had only computational solutions, e.g. by using complex numbers, trigonometric computations or proof by contradiction via an inequality argument. Since AlphaGeometry does not have access to these computational and reasoning tools, as well as advanced Euclidean geometry knowledge, we originally expected that this problem cannot be solved by AlphaGeometry. Nevertheless, AlphaGeometry was able to produce an elegant solution with only angle and ratio chasing by constructing key auxiliary constructions. First, AlphaGeometry shows that ğ‘‹ and ğ‘ are reflections of each other w.r.t. ğµğ¼, and by symmetry it follows that ğ¼ is the circumcenter of ğ‘‹ğ‘Œ ğ‘. From this we can show that ğ´ğµ = ğ´ğ¶, and by symmetry we have ğ´ğµğ¶ is an 25 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 equilateral triangle. However, the main challenge with this problem is to use the condition ğ‘‹ğ‘Œ ğ‘ being an equilateral triangle, i.e. ğ‘‹ğ‘Œ = ğ‘Œ ğ‘ and its cyclic variants. To this end, AlphaGeometry constructs series of circumcenters of key triangles: 1. ğ· as the circumcenter of ğµğ‘‹ğ¶. 2. ğ¸ as the circumcenter of ğ´ğ‘Œ ğ‘. 3. ğ‘‹1 as the circumcenter of ğµğ¼ğ‘‹. 4. ğ‘‹2 as the circumcenter of ğ´ğ¼ğ‘Œ . 5. ğ‘‹3 as the circumcenter of ğ¶ğ¼ğ‘‹. 6. ğ‘‹4 as the circumcenter of ğ´ğµğ‘. 7. ğ‘‹5 as the circumcenter of ğ´ğ¶ğ‘Œ . 8. ğ‘‹6 as the circumcenter of ğ´ğ‘‹ ğ‘ (which we will later show that ğ´, ğ¶, ğ‘‹, ğ‘ are concyclic). 9. ğ‘‹7 as the reflection of ğ¼ w.r.t ğµğ‘ 10. ğ‘‹8 as the circumcenter of ğ´ğ‘‹ğ‘Œ (which we will later show that ğ´, ğµ, ğ‘‹, ğ‘Œ are concyclic). 11. ğ‘‹9, ğ‘‹10 are points such that ğ¼ğ‘ğ‘‹9, ğ¼ğ‘ğ‘‹10 are equilateral triangles. 12. ğ‘‹11 as the reflection of ğ‘ w.r.t ğµğ¼ (which is shown to be equivalent to ğ‘‹ using the point substitution technique described in Section 4.1). At first, these constructions seem very counter-intuitive since most humans would not construct these points. Given the nature of the points ğ‘‹, ğ‘Œ , ğ‘, there are not many geometric properties related to these points and this particular configuration as whole, which makes this problem very hard for humans to come up with synthetic solution. Nevertheless, these circumcenter constructions help facilitate pairs of equal/similar triangles, which allow AlphaGeometry to exploit the fact that ğ‘‹ğ‘Œ ğ‘ is an equilateral triangle and solve the problem. All these examples demonstrate that AlphaGeometry is very efficient in constructing auxiliary points and can offer rather elegant solutions to hard problems without using highly complex Euclidean geometry knowledge and machinery. As such, it leads to creative and efficient solutions that humans normally would not come up with. D. Additional evaluation on the hardest IMO shortlist problems To further investigate the robustness of AG2, we perform additional evaluations on problems from the IMO shortlist that were nominated by experts but have never been selected for the IMO. Since the shortlist problems are sorted by difficulty, we select 29 problems from the end of each years IMO shortlist that did not appear at the IMO from 2002 to 2022. These problems are selected such that they can be formalized in the AG2 language. After formalization we get 30 problems and call it IMOSL-AG-30. As demonstrated on Figure 15, the full AG2 system (see Figure 4) solves 20 out of 30 problems. This shows that even though AG2 is very capable system that can solve wide range of olympiad geometry problems, there is still room for future improvements. E. Towards generating full proofs with language model As discussed in the rest of the paper, our inference setup utilizes language model only to produce auxiliary points followed by symbolic engine run. On the other hand, the model is trained on whole proofs, so it is natural to ask how good the model is at generating full proofs without using the symbolic engine. Given that with greedy decoding, our model + DDAR can only solve 2 IMO problems, it is not surprising that without any further tuning, the model cannot generate complete full proofs. But can it generate partial solutions? To investigate this question we build tools to verify 26 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 Figure 15 AlphaGeometry2 results on the hardest IMO shortlist problems. validity of each deduction proof step. Namely, we isolate the predicates in the premise of the proof step, and add them to new DDAR engine, then run deduction closure with respect to only the deduction rule being used in that step. If the new DDAR manages to prove the conclusion of the step, and the numerical check of the conclusion in the diagram passed, the step is considered verified. Our step verification recognizes the following errors: Wrong grammar: The step has wrong grammar. Theorem name error: The step refers to name of theorem (deduction rule) that does not exist. Step reference error: The step refers to previous step that does not exist. Point not found error: The step refers to point that does not exist, or point with an invalid construction. Numerical error: DDAR fails due to numerical instability. Unverified: The premises of the step does not imply the conclusion of the step under the deduction rule that it uses. Invalid auxiliary point: The auxiliary point is invalid because the step is wrong in grammar, or it is geometrically invalid (e.g. intersection of two parallel lines, etc.) Verified: all checks passed and no error from above found. For evaluation we query our language models with the 2000-2024 IMO problems and 32 samples at temperature 1.0. The models are queried without any end of sentence tokens such that they generate full proofs. Then we compute how many valid proof steps the models produce on average across samples and all problems. It turns out our models do not make many syntax mistakes (see Figure 16), the majority of generated steps are either valid (either fully verified or correct but unverified). One surprising find is that both small and larger models perform similarly. These results support ideas that large language models can be self sufficient without depending on external tools, but until inference speed is improved and hallucinations are completely resolved, the tools will stay essential for math applications. 27 Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry Figure 16 Proof steps validity statistics. Models almost do not make any syntax errors. Small and larger models perform similarly."
        }
    ],
    "affiliations": [
        "Brown University",
        "Georgia Institute of Technology",
        "Google DeepMind",
        "University of Cambridge"
    ]
}