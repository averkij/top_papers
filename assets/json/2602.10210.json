{
    "paper_title": "How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge",
    "authors": [
        "Junhong Lin",
        "Bing Zhang",
        "Song Wang",
        "Ziyan Liu",
        "Dan Gutfreund",
        "Julian Shun",
        "Yada Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) continue to struggle with knowledge-intensive questions that require up-to-date information and multi-hop reasoning. Augmenting LLMs with hybrid external knowledge, such as unstructured text and structured knowledge graphs, offers a promising alternative to costly continual pretraining. As such, reliable evaluation of their retrieval and reasoning capabilities becomes critical. However, many existing benchmarks increasingly overlap with LLM pretraining data, which means answers or supporting knowledge may already be encoded in model parameters, making it difficult to distinguish genuine retrieval and reasoning from parametric recall. We introduce HybridRAG-Bench, a framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge. HybridRAG-Bench automatically couples unstructured text and structured knowledge graph representations derived from recent scientific literature on arXiv, and generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths. The framework supports flexible domain and time-frame selection, enabling contamination-aware and customizable evaluation as models and knowledge evolve. Experiments across three domains (artificial intelligence, governance and policy, and bioinformatics) demonstrate that HybridRAG-Bench rewards genuine retrieval and reasoning rather than parametric recall, offering a principled testbed for evaluating hybrid knowledge-augmented reasoning systems. We release our code and data at github.com/junhongmit/HybridRAG-Bench."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 1 ] . [ 1 0 1 2 0 1 . 2 0 6 2 : r How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge Junhong Lin junhong@mit.edu Massachusetts Institute of Technology Cambridge, MA, United States Bing Zhang Bing.Zhang@ibm.com IBM Research San Jose, CA, United States Song Wang song.wang@ucf.edu University of Central Florida Orlando, FL, United States Ziyan Liu zl488@cornell.edu Jersey City, NJ, United States Dan Gutfreund dgutfre@us.ibm.com IBM Research United States Julian Shun jshun@mit.edu Massachusetts Institute of Technology Cambridge, MA, United States"
        },
        {
            "title": "Abstract",
            "content": "Yada Zhu yzhu@us.ibm.com IBM Research Yorktown Heights, NY, United States"
        },
        {
            "title": "Keywords",
            "content": "Large language models (LLMs) continue to struggle with knowledgeintensive questions that require up-to-date information and multihop reasoning. Augmenting LLMs with hybrid external knowledge, such as unstructured text and structured knowledge graphs, offers promising alternative to costly continual pretraining. As such, reliable evaluation of their retrieval and reasoning capabilities becomes critical. However, many existing benchmarks increasingly overlap with LLM pretraining data, which means answers or supporting knowledge may already be encoded in model parameters, making it difficult to distinguish genuine retrieval and reasoning from parametric recall. We introduce HybridRAG-Bench, framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge. HybridRAG-Bench automatically couples unstructured text and structured knowledge graph representations derived from recent scientific literature on arXiv, and generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths. The framework supports flexible domain and time-frame selection, enabling contamination-aware and customizable evaluation as models and knowledge evolve. Experiments across three domains (artificial intelligence, governance and policy, and bioinformatics) demonstrate that HybridRAG-Bench rewards genuine retrieval and reasoning rather than parametric recall, offering principled testbed for evaluating hybrid knowledgeaugmented reasoning systems. We release our code and data at github.com/junhongmit/HybridRAG-Bench. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference acronym XX, Woodstock, NY 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXX Large Language Model, Knowledge Graph, Retrieval, Reasoning ACM Reference Format: Junhong Lin, Bing Zhang, Song Wang, Ziyan Liu, Dan Gutfreund, Julian Shun, and Yada Zhu. 2018. How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym XX). ACM, New York, NY, USA, 12 pages. https://doi.org/ XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) increasingly rely on external knowledge to solve knowledge-intensive tasks that require multi-hop reasoning with dispersed evidence. Rather than continually retraining models to absorb newly emerging knowledge, recent systems augment LLMs with external text corpora and structured knowledge graphs (KGs) through retrieval-augmented generation (RAG) and structured knowledge integration (KG-RAG). While these approaches have demonstrated strong empirical gains, fundamental question remains unresolved: how much additional reasoning do retrieval-augmented models actually contribute beyond the base LLM? Recent studies have indicated that pretraining contamination, the overlap between benchmark data and model pretraining corpora, can inflate performance and undermine fair comparison across model generations [10, 23, 25, 32, 33, 40]. This issue is severe for hybrid KG-RAG methods, such as Chain-of-Knowledge (CoK) [22], Reasoning-on-Graph (RoG) [28], Think-on-Graph (ToG) [34], Planon-Graph (PoG) [6], which are explicitly designed to incorporate structured retrieval, graph traversal, and multi-hop reasoning. To evaluate these methods meaningfully, benchmarks must require models to retrieve relevant evidence from large external sources and compose multiple facts through reasoning; however, the validity of existing benchmarks is increasingly questionable due to Conference acronym XX, June 0305, 2018, Woodstock, NY Junhong Lin, Bing Zhang, Song Wang, Ziyan Liu, Dan Gutfreund, Julian Shun, and Yada Zhu Table 1: Average accuracy (%) and standard deviation of directly prompting several LLMs with different knowledge cutoff times on the Movie and Sports Q&A datasets (developed in March 2024), calculated over 5 runs. The best results are highlighted in bold. LLM (Cutoff Time) Qwen2-72B (June 2023) Qwen2.5-72B (October 2023) LMArena [8] OpenLLMLeaderboard [12] 1263 43.59% 1303 (+3.1%) 47.98% (+4.4%) Qwen3-32B (mid-2024) 1347 (+6.7%) Movie (All) Sports (All) 24.42 0.38 34.27 0.32 (+40.3%) 16.17 0.45 23.78 0.20 (+47.1%) 43.86 1.00 (+79.6%) 32.71 0.96 (+102%) Movie (Fast-changing Facts) Sports (Fast-changing Facts) 5.38 2.11 15.58 0.45 (+189%) 3.79 0.34 11.56 0.26 (+205%) 22.31 1.05 (+315%) 27.39 0.40 (+622%) both pretraining contamination and the difficulty of continually constructing well-grounded multi-hop questions at scale to shed light on models strengths and weaknesses. Many widely-used multi-hop QA benchmarks, such as HotpotQA (2018) [43], MetaQA (2018) [46], CWQ (2018) [35], TriviaQA (2017) [19], WebQuestions (2013) [2], were constructed before the widespread deployment of LLMs and rely on manual curation over static knowledge sources that widely exist in LLM pretraining corpora, making it difficult to isolate the contribution of retrieval and reasoning modules. The impact of this overlap is apparent. For example, when asked What is the latest film that Denis Villeneuve has been involved in?, both Qwen2-72B and Qwen2.5-72B (trained in 2023) incorrectly answer Dune (2021), while Qwen3-32B (trained in 2024) correctly answers Dune: Part Two (2024), despite identical prompting and comparable (even smaller) model parameter size. More broadly, we consider controlled motivating example by directly prompting them on two CRAG datasets (Movie and Sports) [42] developed in March 2024. As shown in Table 1, the QA accuracy increases sharply (up to +102%) with later pretraining cutoffs, while improvements on general reasoning benchmarks remain modest (+6.7%), which cannot solely explain these QA accuracy improvements. When restricting evaluation to questions tagged with involving fast-changing facts, earlier models achieve near-zero accuracy. These results indicate that benchmark performance can be dominated by pretraining exposure rather than retrieval or reasoning ability, obscuring genuine methodological progress. To address this gap, we introduce HybridRAG-Bench, fully automated benchmark construction framework designed to faithfully evaluate retrieval-intensive and multi-hop reasoning. HybridRAGBench explicitly externalizes all knowledge required for question answering, thereby satisfying four key properties for valid evaluation. (1) To ensure questions are derived from recent and evolving sources, HybridRAG-Bench collects time-framed scientific corpora from user-specified topics on arXiv, reducing overlap with LLM pretraining data and ensuring that correctness depends on retrieval rather than memorization. (2) To guarantee that questions are knowledge-intensive and sufficiently difficult, the framework constructs hybrid knowledge environment consisting of aligned text chunks and extracted knowledge graphs, making the location, structure, and provenance of required knowledge explicit. (3) To support diverse reasoning behaviors, HybridRAG-Bench generates challenging questionanswer pairs grounded in explicit reasoning paths that cover single-hop lookup, conditional reasoning, multihop inference, hard multi-hop chains, counterfactual reasoning, and open-ended synthesis. (4) To maintain scalability, automation, and reusability, HybridRAG-Bench automatically validates QA quality and enables rapid regeneration of benchmarks across new topics, time periods, and knowledge scopes without manual curation. By meeting these criteria, HybridRAG-Bench provides robust and evolving testbed for evaluating retrieval-augmented and multi-hop reasoning systems. HybridRAG-Bench makes the following contributions: benchmarking framework for retrieval-intensive reasoning. HybridRAG-Bench is reusable benchmark construction framework for evaluating multi-hop reasoning over hybrid structured and unstructured knowledge. The framework generates diverse, knowledge-intensive questions grounded in latent knowledge graph paths, yielding challenges that require genuine retrieval and reasoning. The code and data of HybridRAGBench is available at github.com/junhongmit/HybridRAG-Bench. Diagnostic benchmark instances for RAG and KG-RAG methods. Using the proposed framework, HybridRAG-Bench instantiates evaluation benchmarks that expose clear and consistent performance gaps across retrieval and reasoning strategies, distinguishing na√Øve augmentation from effective hybrid reasoning over text and graphs. Controlled evaluation of retrieval versus memorization. HybridRAG-Bench enables systematic control over domains and document time ranges, allowing principled evaluation of whether performance gains stem from genuine retrieval and multi-hop reasoning rather than parametric memorization."
        },
        {
            "title": "2 Related Works\nKGQA Benchmarks. The Knowledge-Graph Question Answering\n(KGQA) benchmarks evaluate systems‚Äô reasoning and accuracy in\nhandling natural language queries over knowledge graphs. Early\nbenchmarks primarily relied on Freebase [3] and its subsets. We-\nbQuestions [2] provided foundational question-answer pairs, which\nwere re-annotated in WebQSP [44] with semantic parses (SPARQL\nqueries) to allow limiting reasoning to semantically valid paths.\nCWQ [35] tested reasoning capabilities on questions of increased\ncompositional complexity. GrailQA [15] and KQA Pro [5] further\nexpanded question diversity. Because Freebase was discontinued,\nmuch of its data has been ingested by modern LLMs during pre-\ntraining. Other datasets like MetaQA [46], LC-QuAD [36], and LC-\nQuAD 2.0 [11] used collaboratively maintained knowledge graphs\nsuch as DBpedia [20] and Wikidata [37]. Recent KG-RAG datasets\nretrieve and expose graph evidence to LLMs and evaluate the gen-\nerated natural-language responses. CRAG [42] extends beyond\nWikipedia to include public data categorized by query dynamism.\nKGQAGen [45] generates multi-hop QA pairs from local subgraphs\nand ensures accuracy via SPARQL execution.",
            "content": "As LLMs increasingly incorporate web-scale and scientific corpora during pretraining, KGQA datasets face growing risk of knowledge overlap with the models internal memory. Several works have tried to mitigate this issue. For example, DynamicKGQA [9] attempts to generate adaptive QA pairs using varied Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge Conference acronym XX, June 0305, 2018, Woodstock, NY Figure 1: Illustration of the HybridRAG-Bench benchmarking framework. reasoning paths within localized sub-knowledge graphs but it does not provide definitive isolation from parametric memory in LLMs. Recent works such as ArxivRoll [25] and OKBench [24] offer dynamic frameworks for generating private evaluation questions from recent documents or news reports; however, it is not designed for KGQA, as it operates on single-document fragments and does not support graph-structured retrieval or multi-fact reasoning. Retrieval-Augmented and Knowledge-Grounded LLMs. To overcome the limitations of parametric knowledge and avoid the cost of continual pretraining, large body of work has explored augmenting LLMs with external knowledge through retrieval-augmented generation (RAG) or from knowledge graphs. Text-based RAG [1, 21] methods retrieve unstructured documents to improve factual accuracy, while KG-based approaches leverage structured graphs for relational and multi-hop reasoning. Specifically for knowledge graph reasoning and retrieval: CoK [22] iteratively retrieves external knowledge from unstructured text, knowledge graphs, and tables into subsequent reasoning steps; RoG [28] relies on questionspecific subgraph and allows self-correcting erroneous reasoning paths; ToG [34] and ToG2.0 [29] optimize retrieval by performing path expansion and pruning based on seed entities and context; PoG [6] and R2-KG [18] improve answer reliability by enhancing LLM-based judging during exploration and answering. To reason over temporal facts (e.g., TimeQuestions [17], MultiTQ [7]), EvoReasoner [26] includes relation and entity reranking based on their temporal alignment with reasoning subgoals. While these frameworks offer diverse advantages in relational reasoning and hallucination reduction, they are primarily evaluated using static benchmarks such as WebQSP [44], CWQ [35], GrailQA [15], HotpotQA [43] and MetaQA [46]. These benchmarks are constructed once from Freebase [3], DBpedia [20], and Wikidata [37]. Consequently, their reported performance can be overestimated due to the overlap between the models internal parametric knowledge and the external knowledge used for evaluation."
        },
        {
            "title": "3 Problem Definition and Preliminaries",
            "content": "We formalize the problem addressed by HybridRAG-Bench as contamination-aware evaluation of knowledge-intensive question answering over hybrid knowledge composed of unstructured text and structured knowledge graphs. Evaluation is performed within single domain at time, while the framework supports instantiation across flexible domains and time ranges."
        },
        {
            "title": "3.1 Domains, Corpora, and Knowledge Graphs\nLet {D (1), . . . , D (ùêæ ) } denote document corpora corresponding to\nùêæ distinct domains. Each domain ùëö is specified by a set of document\nselection criteria (e.g., subject categories and keyword constraints),\nwhich define the scope of domain-relevant documents.",
            "content": "For each domain ùëö, documents are collect within user-specified ], forming the corpus and its induced time window [ùëá (ùëö) ,ùëá (ùëö) start end evolving knowledge graph: (ùëö) = {ùëë (ùëö) 1 , . . . , ùëë (ùëö) ùëÅùëö }, (ùëö) ùë° = (ùëâ (ùëö) ùë° , ùê∏ (ùëö) ùë° , ùúã (ùëö) ùë° ), Conference acronym XX, June 0305, 2018, Woodstock, NY Junhong Lin, Bing Zhang, Song Wang, Ziyan Liu, Dan Gutfreund, Julian Shun, and Yada Zhu ùë° where (ùëö) represents structured knowledge extracted from documents up to time ùë° [ùëá (ùëö) ,ùëá (ùëö) ]. Each knowledge graph is start end constructed independently per domain and may differ in entity types, relation schemas, graph topology, and temporal dynamics. No entities or relations are shared across domains."
        },
        {
            "title": "3.2 Questions and Evaluation Scope\nFor each domain ùëö, we define a set of question‚Äìanswer pairs",
            "content": "Q (ùëö) = {(ùëûùëñ, ùë°ùëñ, ùëéùëñ )}, where ùëûùëñ is question issued at time ùë°ùëñ and ùëéùëñ is the corresponding ground-truth answer. Each question is evaluated exclusively with respect to the domain-specific knowledge graph snapshot (ùëö) and documents available up to time ùë°ùëñ . ùë°ùëñ We assume that all documents, knowledge graph facts, and answers are strictly newer than the pretraining cutoff of the evaluated large language models. Consequently, correct answers cannot be recovered via parametric memorization alone."
        },
        {
            "title": "3.3 Task Definition\nGiven a question ùëû ‚àà Q (ùëö) issued at time ùë°ùëû, the task is to predict\nthe answer ùëé using information available up to ùë°ùëû. A model ùëì may\nreason over: (1) the knowledge graph snapshot G (ùëö)\n, and (2) the\nùë°ùëû\nretrieved documents D (ùëö)\nùë°ùëû ‚äÜ D (ùëö) . Formally, the model produces",
            "content": "ÀÜùëé = ùëì (cid:16) ùëû, (ùëö) ùë°ùëû , (ùëö) ùë°ùëû (cid:17) , which is evaluated against the ground-truth answer ùëé."
        },
        {
            "title": "4 Benchmark Construction",
            "content": "As illustrated in Figure 1, HybridRAG-Bench is fully automated framework for constructing benchmarks that evaluate retrievalintensive, multi-hop reasoning over hybrid knowledge. Concretely, HybridRAG-Bench follows four-stage pipeline. First, given userspecified topic filters and time ranges, the framework collects timeframed arXiv corpora that define the external knowledge source available to retrieval-based methods. Second, it constructs hybrid knowledge environments by extracting aligned unstructured text chunks and structured knowledge graphs from the corpus, enabling both document retrieval and graph-based traversal. Third, the framework generates diverse question-answer pairs grounded in explicit reasoning paths and supporting evidence, covering single-hop, conditional, multi-hop, hard multi-hop, counterfactual, and openended reasoning. Finally, an automated quality control stage filters and validates generated questions to ensure answerability, independence from document-specific phrasing, and non-redundancy. Together, these components enable reproducible construction of retrieval-intensive benchmarks for evaluating RAG and KG-RAG methods under controlled knowledge settings. We next describe each stage in detail."
        },
        {
            "title": "4.1 Time-Framed Corpus Collection",
            "content": "For each domain, we collect corpus of arXiv papers based on userspecified document selection criteria, including subject categories (e.g., cs.AI, cs.LG) and optional keyword constraints (e.g., reinforcement learning). This mechanism allows HybridRAG-Bench to be instantiated flexibly across domains while maintaining topical coherence. Document collection is performed over configurable time window, enabling evaluation under different knowledge cutoff regimes. In our experiments, we select document windows that postdate the public pretraining cutoffs of contemporary LLMs, reducing the likelihood that answers can be recovered via parametric memorization alone. For each paper, the parsed text, section-level segmentation (e.g., abstract, introduction, methods), and metadata, including title, authors, categories, and submission timestamps, are stored. These documents serve as the unstructured knowledge source for retrieval-based methods."
        },
        {
            "title": "4.2 Knowledge Graph Construction",
            "content": "From the collected corpus, we construct knowledge graph using EvoKG [26], document-driven framework for extracting and organizing structured knowledge from unstructured text. The resulting graph captures entities (e.g., methods, datasets, tasks, policies) and relations expressed in the literature, and is constructed independently for each domain. Entity Extraction and Alignment. Given batch of documents, EvoKG applies large language model to extract candidate entities and relations. Extracted entities may exhibit lexical variation (e.g., abbreviated method names), semantic ambiguity, or partial descriptions. To address this, EvoKG performs context-aware entity alignment, matching each extracted entity against existing KG nodes using joint embeddings over entity type, name, and description. If no existing node exceeds similarity threshold, new entity is created; otherwise, the extracted mention is merged with the matched node, preserving provenance information. This alignment step prevents entity fragmentation and ensures consistency across documents and time. Relation Normalization and Evidence Tracking. Extracted relations are normalized to domain-specific schema and linked to supporting textual evidence. Rather than enforcing single canonical fact, the knowledge graph retains multiple candidate relations when supported by the corpus, along with confidence scores derived from frequency, recency, and textual support. This design allows the graph to represent uncertainty and variation commonly observed in scientific writing. The resulting knowledge graph provides structured relational scaffolding that complements the unstructured document corpus, enabling hybrid retrieval and reasoning."
        },
        {
            "title": "Generation",
            "content": "HybridRAG-Bench constructs benchmark questions by grounding them in both structured reasoning paths from the knowledge graph and supporting unstructured textual evidence from the document corpus. This hybrid grounding ensures that questions cannot be answered solely by graph traversal or document retrieval alone, and instead require effective integration of both modalities. Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge Conference acronym XX, June 0305, 2018, Woodstock, NY Table 2: Question statistics by types. Question Type Arxiv-AI Arxiv-CY Arxiv-BIO Single-hop Single-hop w. Condition Multi-hop (Regular) Multi-hop (Difficult) Counterfactual Open-ended 249 (29%) 139 (16%) 165 (19%) 149 (17%) 114 (13%) 47 (5%) 238 (25%) 128 (13%) 149 (15%) 166 (17%) 173 (18%) 112 (12%) 264 (25%) 193 (19%) 195 (19%) 139 (13%) 59 (6%) 190 (18%) All 863 966 1040 Reasoning Path Sampling. We first sample reasoning paths from the knowledge graph of the form ùëü1 ùë£ ùëù = (ùë£0 ùëü2 . . . ùëüùëò ùë£ùëò ), where ùë£ùëò is the answer entity. Each sampled path defines valid relational structure connecting the question subject to its answer. For every entity and relation along the path, we retrieve the associated supporting text spans from the corpus, which provide contextual descriptions, qualifiers, and evidence. Hybrid Question Construction. For all question types, we condition large language model on structured context that includes: (1) sampled reasoning path from the knowledge graph, which specifies the relational constraints, (2) textual evidence associated with the entities and relations along the path, and (3) small set of in-context examples illustrating the desired question format. The reasoning path serves as structural scaffold, while the textual context guides natural language formulation and enables questions that require interpretation or synthesis beyond symbolic lookup. The model is instructed to generate question that is faithful to the provided evidence and to produce ground-truth answer that can be verified from the same context. If the available evidence is insufficient to support well-posed question (e.g., missing attributes or ambiguous relations), the model is explicitly instructed to abstain from generating an example. Using this hybrid grounding mechanism, we generate diverse set of questionanswer pairs. The distribution of question types across domains is summarized in Table 2. Single-hop questions. These questions correspond to single relation lookup (ùë£0, ùëü, ùë£1). The generated question directly queries an attribute or relation of the subject entity, while the answer is the terminal entity or value supported by both the KG edge and its textual evidence. Single-hop questions with conditions. In addition to single KG relation, these questions include explicit constraints extracted from textual descriptions, such as temporal qualifiers (e.g., in 2018), categorical restrictions, or contextual conditions. The condition is derived from attributes or modifiers mentioned in the supporting text, requiring models to align symbolic relations with unstructured qualifiers. Multi-hop questions. These questions are grounded in reasoning paths with ùëò 2 relations. Answering them requires composing multiple relational steps and retrieving evidence across intermediate entities. The question is formulated to obscure the intermediate nodes, requiring implicit multi-hop inference. Difficult multi-hop questions. To increase difficulty, we preferentially sample paths that traverse high-degree entities in the knowledge graph. Such entities introduce large candidate spaces and increase the likelihood of spurious retrieval, placing greater demands on both retrieval precision and reasoning robustness. Counterfactual questions. Counterfactual questions are constructed by minimally perturbing relation or attribute in the original reasoning path (e.g., replacing method, dataset, or condition) and asking how the outcome or conclusion would change. The generated answers are constrained to provide cautious, evidence-based reasoning without introducing unsupported facts. Open-ended questions. Open-ended questions require synthesizing explanations or summaries from multiple pieces of textual evidence aligned with reasoning path. Rather than returning single entity, the answer is short, evidence-grounded natural language response that integrates information from several nodes and relations."
        },
        {
            "title": "4.4 QA Pairs Quality Control",
            "content": "Lastly, we apply multi-stage quality control process to ensure that all generated questionanswer pairs in HybridRAG-Bench are well-posed, answerable, and non-redundant. Answerability and Faithfulness. Using an LLM-as-a-Judge protocol [14], we verify that each question can be answered solely from the provided hybrid context (KG paths and supporting text), and that the ground-truth answer is faithful to that evidence. Questions requiring external knowledge or unsupported inference are removed. Context Independence and Clarity. We filter out questions that contain document-local references (e.g., in this paper, Theorem 12) or are ambiguous or poorly phrased. Retained questions are lightly normalized (e.g., lowercasing and punctuation standardization) without altering their semantic content. Only questionanswer pairs that pass all checks are included in the final benchmark."
        },
        {
            "title": "5 Experiment",
            "content": "We design our experiments to evaluate HybridRAG-Bench as retrieval-intensive benchmark for knowledge-grounded reasoning and to assess whether it meaningfully differentiates between LLMonly, RAG-based, and KG-RAG reasoning approaches. In particular, we seek to answer the following research questions. RQ1: Are HybridRAG-Bench questions genuinely challenging across LLM scales? RQ2: Does HybridRAG-Bench require external retrieval to answer correctly? RQ3: Does structured knowledge (KG) provide complementary value beyond text-only retrieval? RQ4: Does HybridRAG-Bench discriminate between different retrieval and reasoning strategies? Conference acronym XX, June 0305, 2018, Woodstock, NY Junhong Lin, Bing Zhang, Song Wang, Ziyan Liu, Dan Gutfreund, Julian Shun, and Yada Zhu Table 3: Experiment results (accuracy, %) across LLMs of different scales. We highlight the first and second best results. Datasets Arxiv-AI Arxiv-CY Arxiv-BIO Methods Deep-Seek V3.2 685B Qwen 2.5 72B LLaMA 3.3 70B LLaMA 3.1 8B Deep-Seek V3.2 685B Qwen 2.5 72B LLaMA 3.3 70B LLaMA 3.1 8B Deep-Seek V3.2 685B Qwen 2.5 72B LLaMA 3.3 70B LLaMA 3.1 8B IO CoT SC RAG 37.750.64 36.690.31 35.410.63 43.680. 27.070.47 34.110.67 23.080.64 35.320.35 35.320.58 27.650.63 34.020.72 36.270.88 26.950.85 50.440.07 47.300.15 38.320.27 1-hop KG 27.550.31 RAG+1-hop KG 49.420.08 23.080.59 34.790.38 40.690.78 63.590.80 38.700.27 49.010.99 62.390.13 75.690.55 CoK RoG ToG ToG2.0 PoG R2-KG HippoRAG2.0 EvoReasoner 22.690.19 26.190.34 22.570.38 50.640.20 51.490.18 43.680.12 42.691.33 38.490.80 35.622.61 37.910.62 43.550.36 25.680.48 44.500.40 39.910.23 17.541.22 58.610.31 59.370.53 49.250.51 43.130.38 39.860.22 16.551.05 42.970.92 46.510.42 25.931.55 19.472.92 56.110.47 47.860.12 71.301.36 66.780.67 55.740.82 43.620.62 39.500.75 36.891.49 41.820. 29.630.28 47.270.52 27.230.93 40.810.38 39.340.45 66.170.47 37.370.50 42.670.91 58.570.09 69.150.59 27.830.20 35.320.17 24.060.28 39.010.63 40.640.59 28.471.04 37.620.63 41.240.42 29.750.70 40.994.94 42.990.23 35.560.07 21.860.24 23.480.20 23.560.70 47.480.16 45.691.66 42.550.29 40.430.69 39.010.74 36.890.97 40.910.14 44.240.48 26.151.25 37.330.25 34.580.56 15.652.64 59.050.45 60.850.31 51.910.84 35.940.40 34.020.32 15.782.04 43.810.70 46.070.53 27.761.55 12.240.88 56.000.59 48.900.21 66.896.13 66.310.75 55.220.28 39.880.56 39.750.91 37.290.90 48.610.75 40.670.66 58.990.34 32.040.89 36.960.73 44.150.34 64.690.95 42.130.71 50.150.73 62.190.11 75.920.72 22.810.16 34.630.08 25.130.66 36.330.08 37.460.55 29.960.63 35.150.64 37.190.86 29.350.86 52.160.07 49.180.48 43.850. 25.150.19 29.750.20 28.770.28 57.160.07 54.570.07 51.200.61 41.215.22 39.900.34 35.980.67 38.670.99 40.020.16 24.211.18 44.690.29 42.230.34 16.065.88 60.360.39 57.408.39 51.581.83 44.100.28 41.790.34 21.232.30 44.253.21 48.730.59 27.651.26 32.810.16 56.690.20 50.330.09 73.310.55 69.940.36 58.131."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Benchmark Instantiation. We instantiate HybridRAG-Bench on three domain-specific corpora collected from recent arXiv papers: AI (reinforcement learning), CY (governance and policy), and BIO (bioinformatics). For each domain, we construct domain-specific knowledge graph and generate approximately 1,000 questionanswer pairs grounded in explicit graph paths and supporting textual evidence. As shown in Table 2, the questions cover single-hop, conditional single-hop, multi-hop, hard multi-hop, counterfactual, and open-ended reasoning. We manually inspect subset of the generated data to verify question quality and faithfulness. For answer evaluation, we adopt an LLM-as-a-Judge protocol following CRAG [42], using the same judge prompt and evaluation models to assess answer correctness and faithfulness. Baselines. We use four state-of-the-art LLMs with different parameter sizes and knowledge cutoff, including DeepSeek V3.2 (685B, Jul 2024) [27], Qwen 2.5 (72B, Oct 2023) [41], LLaMA 3.3 (70B, December 2023), and LLaMA 3.1 (8B, Dec 2023) [13]. We compare broad range of baselines that differ in how they retrieve and integrate external knowledge: (1) LLM-only prompting, without access to external knowledge, including IO-prompt (IO) [4], Chain-of-Thought (CoT) [39], Self-Consistency (SC) [38]; (2) Text-based RAG, which retrieves relevant passages from the full arXiv corpus using dense retrieval, including Retrieval-Augmented Generation (RAG) [21]; (3) Naive KG augmentation, where local graph neighborhoods (e.g., one-hop neighbors) are directly injected into the prompt, including 1-hop KG [42] (augmenting the LLM with facts from 1-hop KG neighbors of the topic entities) and RAG + 1-hop KG [42]; (4) Hybrid KG-RAG methods, which jointly leverage graph structure and textual evidence through structured retrieval and multi-hop reasoning, including Chain-ofKnowledge (CoK) [22], Reasoning-on-Graph (RoG) [28], Think-onGraph (ToG) [34], Think-on-Graph 2.0 (ToG2.0) [29], Plan-on-Graph (PoG) [6], R2-KG [18], HippoRAG2.0 [16] and EvoReasoner [26]. We report the average and standard deviation over 5 runs for all models."
        },
        {
            "title": "5.2 Experiment Results\nRQ1: Are HybridRAG-Bench Questions Challenging Across\nLLM Scales? As shown in Table 3, across all three domains, LLM-\nonly prompting achieves consistently low accuracy across all three\ndomains, ranging from 23‚Äì40%, even when scaling model size from\nLLaMA-3.1-8B to DeepSeek-V3.2-685B. While larger models yield\nmodest improvements, performance remains far from saturation.\nThis trend indicates that HybridRAG-Bench questions cannot be\nreliably answered through parametric knowledge alone, even by\nfrontier-scale LLMs. The limited gains from model scaling suggest\nthat success on HybridRAG-Bench does not primarily stem from\nmemorization or general language competence, but instead requires\naccess to external knowledge and non-trivial reasoning. Overall,\nthese results confirm that HybridRAG-Bench poses a substantial\nand persistent challenge across LLM scales.",
            "content": "RQ2: Does HybridRAG-Bench Require External Retrieval? Introducing external retrieval leads to large and consistent performance gains. As observed in Table 3, for example, text-based RAG improves accuracy by 729 absolute points over LLM-only prompting across most modeldomain combinations, with the exception of DeepSeek-V3.2 on Arxiv-CY. These gains demonstrate that external knowledge is critical for answering HybridRAG-Bench questions. Without retrieval, models fail to obtain the domain-specific and up-to-date information required by the benchmark. Notably, na√Øve KG-based augmentation can degrade performance relative to LLMonly prompting. Directly injecting one-hop graph neighborhoods without selective retrieval or structured reasoning introduces substantial noise, distracting the model from relevant evidence. This highlights that HybridRAG-Bench does not merely reward access to more information, but specifically requires effective retrieval and evidence selection. Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge Conference acronym XX, June 0305, 2018, Woodstock, NY Table 4: Qwen2.5-72B Model performance (accuracy, %) grouped by question types. We highlight the first and second best results. Due to space limitations, we have omitted the standard deviations, but provide them in the Appendix A. Datasets Arxiv-AI Arxiv-CY Arxiv-BIO - gle Methods o - gle s t o w. IO CoT SC RAG 21.20 25.22 25.46 53.55 20.29 29.35 26.76 67.63 - ult 16.36 25.94 24.00 41.82 al a t C n - O H - gle p - gle s t o w. ) - ult ult ffi ( 17.72 23.76 22.68 29.31 68.07 45.96 17.96 87.02 50.64 27.48 84.04 50.64 26.30 48.54 85.11 51.68 1-hop KG 33.98 14.90 RAG + 1-hop KG 58.77 32.21 37.19 31.81 37.43 30.87 53.09 39.60 43.89 59.52 51.65 35.70 43.13 30.34 5.23 HippoRAG2.0 17.67 EvoReasoner 71.75 75.54 65.86 53.47 17.58 42.22 33.21 27.64 38.79 44.73 30.55 36.36 20.48 23.88 65.47 41.01 32.66 35.11 54.82 36.12 37.99 29. CoK RoG ToG ToG2.0 PoG R2KG 8.07 37.45 43.03 37.72 82.98 65.97 74.74 66.81 34.45 71.23 33.62 47.98 45.09 60.85 57.14 90.35 83.40 63.61 53.68 60.85 54.62 73.68 45.53 50.34 23.68 30.21 13.45 88.30 90.78 69.75 al a t C n - O - gle p - gle S o d w. ) - ult ult ffi ( 15.36 20.96 21.45 23.49 68.64 34.15 19.70 91.91 40.71 32.20 87.05 40.54 29.55 23.41 72.32 51. 19.38 35.34 33.47 68.91 - ult 15.10 24.83 23.09 35.23 - ult 7.35 18.06 19.29 34.69 ) H - ult ult ffi ( 16.81 22.61 21.01 22.10 12.62 10.36 38.03 28.51 28.19 23.61 25.77 18.19 29.53 25.78 29.53 29.16 28.19 19.40 30.87 26.14 4.58 8.59 54.50 49.04 1.73 34.64 35.61 17.54 15.03 74.70 61.36 37.68 69.83 55.18 42.58 29.71 71.45 41.79 38.79 29.13 13.99 55.00 45.83 45.29 96.07 69.46 60.08 51.88 20.92 50.89 46.21 42.17 60.92 48.39 50.53 32.32 11.88 14.34 14.29 30.30 83.12 82.86 75.15 77.82 56.94 56.09 17.35 35.20 25.51 22.86 32.78 38.98 29.80 35.31 22. 20.52 69.17 43.63 38.03 40.28 58.24 38.13 39.38 41.45 al a t C n - O 56.27 40.53 83.39 57.26 83.05 56.42 40.68 79.21 0.00 36.74 35.59 82.63 71.53 62.00 77.97 50.21 45.34 58.68 93.56 80.84 60.00 58.42 64.07 53.37 56.27 46.11 91.53 89.89 16.02 27.34 27.19 68. 24.22 68.75 35.00 32.81 40.31 55.78 38.91 42.50 19.53 63.28 RQ3: Does Structured Knowledge Complement Text-Based Retrieval? Hybrid KG-RAG KG-RAG that jointly leverage structured graph information and unstructured text consistently outperform text-only RAG baselines across all domains  (Table 3)  . While text-based RAG is effective for retrieving descriptive or explanatory information, it struggles with questions that require relational reasoning, entity disambiguation, or multi-hop composition. Structured knowledge graphs provide explicit relational scaffolding that complements unstructured retrieval, enabling more reliable traversal and reasoning. The consistent advantage of hybrid methods over text-only RAG demonstrates that structured knowledge offers additive value beyond unstructured retrieval alone. These results confirm that HybridRAG-Bench is sensitive to how models integrate text and graph evidence, making it suitable testbed for evaluating structured reasoning capabilities in KG-RAG systems. RQ4: Does HybridRAG-Bench Discriminate Between Retrieval and Reasoning Strategies? For clarity of analysis, we present per-question-type results  (Table 4)  using Qwen2.5-72B as representative strong open-source model; results for other LLMs exhibit consistent trends and are provided in the Appendix A. HybridRAG-Bench reveals substantial and consistent performance gaps among KG-RAG baselines, far exceeding the gains obtained by scaling LLM size alone  (Table 3)  . These gaps persist across domains and are especially pronounced when performance is analyzed by question type  (Table 4)  , indicating that the benchmark meaningfully differentiates retrieval and reasoning strategies. Several clear and interpretable trends emerge in the breakdown by question type: Multi-hop and hard multi-hop questions. KG-RAG methods that explicitly model graph structure (e.g., ToG, ToG2.0, EvoReasoner) consistently outperform text-only RAG. This reflects the importance of structured traversal and relational composition when reasoning requires chaining multiple facts. Single-hop and conditional questions. While these questions require limited multi-step composition, methods that effectively integrate both unstructured text and structured knowledge (e.g., ToG2.0, EvoReasoner, RAG+1-hop KG) consistently outperform text-only RAG and KG-only baselines. This suggests that even seemingly simple questions benefit from hybrid evidence integration, particularly when entity grounding or conditional constraints must be resolved. Open-ended questions. Methods with strong unstructured text retrieval perform better, highlighting the role of descriptive evidence and synthesis beyond discrete relational paths. Counterfactual questions. Performance on counterfactual queries correlates strongly with models reasoning capability rather than raw retrieval. Methods with explicit reasoning mechanisms (e.g., CoT, Self-Consistency, ToG2.0, EvoReasoner) achieve substantially higher accuracy, while na√Øve KG augmentation (1-hop KG) performs near zero. Inspection of model outputs shows that one-hop KG often induces overly cautious responses (e.g., dont know), as local graph neighborhoods provide insufficient support for hypothetical reasoning. This highlights that counterfactual questions primarily test reasoning and uncertainty handling, rather than factual retrieval alone. These results indicate that HybridRAG-Bench not only measures overall accuracy, but also diagnoses how different retrieval and reasoning mechanisms succeed or fail across distinct reasoning regimes. By disentangling performance across reasoning regimes, HybridRAG-Bench provides fine-grained diagnostic signals for evaluating KG-RAG systems. Conference acronym XX, June 0305, 2018, Woodstock, NY Junhong Lin, Bing Zhang, Song Wang, Ziyan Liu, Dan Gutfreund, Julian Shun, and Yada Zhu Table 5: KG construction quality comparison. Methods OpenIE GraphRAG KGGen EvoKG (Ours) Facts Captured (Average on the 106 corpus) 29.36% 47.08% 66.46% 71.36%"
        },
        {
            "title": "5.3 KG Construction Effectiveness",
            "content": "Since HybridRAG-Bench relies on automatically constructed knowledge graphs, we evaluate whether our KG construction pipeline reliably recovers factual information from source documents. This analysis isolates the quality of KG extraction itself and does not involve downstream QA or reasoning performance. We measure fact recovery rate, defined as the fraction of verifiable facts from the source corpus that are successfully recovered in the constructed KG: Recovery Rate = Number of facts recovered from corpus Number of verifiable facts in corpus . We conduct this evaluation on the MINE benchmark [31], which contains 106 document corpora, each annotated with 15 manually verified facts. fact is considered recovered if it can be matched to at least one triplet in the constructed KG. Table 5 compares our pipeline with OpenIE, GraphRAG, and the state-of-the-art KGGen method. Our approach achieves the highest recovery rate, recovering approximately 71% of verifiable facts, outperforming KGGen by about 5 absolute points. These results indicate that the KG construction component of HybridRAG-Bench is effective at extracting factual structure from text and is competitive with prior state-of-the-art methods. Importantly, this suggests that the difficulty of HybridRAG-Bench does not stem from weak or incomplete knowledge graph construction, but rather from the retrieval and reasoning demands imposed by the benchmark."
        },
        {
            "title": "5.4 KG Construction Cost and Scalability",
            "content": "Since HybridRAG-Bench relies on automated KG construction, we briefly assess the efficiency and scalability of the underlying KG evolution pipeline. Empirically, both extraction latency and token usage grow approximately linearly with document length, consistent with the expected complexity dominated by LLM token processing and bounded merge operations. No superlinear cost growth is observed. In practice, KG construction can be parallelized across documents, substantially reducing end-to-end wall-clock time. Detailed latency and token scaling results are provided in Appendix A.1. The observed near-linear scaling is consistent with the theoretical complexity of the KG construction pipeline, which is dominated by HNSW-based entity and relation alignment with total complexity ùëÇ (ùëõ log ùëÅ + ùëö log ùëÄ). Token Cost Analysis. Figure 2 presents token usage as function of document length during knowledge graph construction. As shown in the figure, token consumption increases approximately linearly with the size of the input corpus. This behavior arises from two primary factors: (1) input tokens scale proportionally with the raw text length, and (2) output tokens scale smoothly with the semantic density of each document. Because EvoKG performs Figure 2: Token usage during KG construction plotted against corpus length. Token cost grows approximately linearly with input size due to proportional input tokens and fixed number of extraction calls per document. The smooth scaling pattern confirms that EvoKGs update cost remains predictable and stable across document sizes. fixed and deterministic number of LLM extraction calls per document, the overall token footprint exhibits consistent slope rather than abrupt jumps or nonlinear spikes. This linear scaling pattern confirms that the computational cost of constructing the KG remains predictable and bounded. In practice, this ensures that EvoKG can support long-term or continuously updated corpora without unexpected surges in token usage, making the system practical for real-world deployments where cost control and scalability are critical."
        },
        {
            "title": "6 Conclusion",
            "content": "We presented HybridRAG-Bench, benchmark construction framework for evaluating retrieval-intensive reasoning over hybrid knowledge. HybridRAG-Bench is designed to address long-standing challenges in evaluating LLM-based reasoning systems, particularly the difficulty of separating retrieval and reasoning from memorization when benchmarks overlap with model pretraining data. By constructing hybrid knowledge environments from recent scientific literature and generating questions grounded in explicit reasoning paths and supporting evidence, HybridRAG-Bench enables diagnostic evaluation of LLM-only, RAG-based, and hybrid KG-RAG methods. Our results show that the benchmark poses substantial challenges to LLM-only baselines and meaningfully distinguishes methods based on their ability to retrieve and integrate graph and textual information. While the framework relies on LLM-based components for knowledge extraction and question generation, and is currently instantiated using scientific literature, we view these choices as practical design decisions that enable scalable, contamination-aware evaluation rather than fundamental limitations. HybridRAG-Bench is intended as reusable research infrastructure, supporting flexible domain and temporal instantiation for future studies of retrieval, reasoning, and generalization over evolving knowledge. Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge Conference acronym XX, June 0305, 2018, Woodstock, NY References [1] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-rag: Learning to retrieve, generate, and critique through self-reflection. (2024). [2] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing. 15331544. [3] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data. 12471250. [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language Models Are Few-Shot Learners. Advances in Neural Information Processing Systems (NeurIPS) 33 (2020), 18771901. [5] Shulin Cao, Jiaxin Shi, Liangming Pan, Lunyiu Nie, Yutong Xiang, Lei Hou, Juanzi Li, Bin He, and Hanwang Zhang. 2022. KQA pro: dataset with explicit compositional programs for complex question answering over knowledge base. In Proceedings of the 60th annual meeting of the Association for Computational Linguistics (volume 1: long papers). 61016119. [6] Liyi Chen, Panrong Tong, Zhongming Jin, Ying Sun, Jieping Ye, and Hui Xiong. 2024. Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language Model on Knowledge Graphs. In Advances in neural information processing systems (NeurIPS). [7] Ziyang Chen, Jinzhi Liao, and Xiang Zhao. 2023. Multi-granularity temporal question answering over knowledge graphs. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1137811392. [8] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph Gonzalez, et al. 2024. Chatbot arena: An open platform for evaluating llms by human preference. In International Conference on Machine Learning (ICML). [9] Preetam Prabhu Srikar Dammu, Himanshu Naidu, and Chirag Shah. 2025. Dynamic-kgqa: scalable framework for generating adaptive question answering datasets. In Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval. 34983508. [10] Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Bin Gu, Mengfei Yang, and Ge Li. 2024. Generalization or memorization: Data contamination and trustworthy evaluation for large language models. arXiv preprint arXiv:2402.15938 (2024). [11] Mohnish Dubey, Debayan Banerjee, Abdelrahman Abdelkawi, and Jens Lehmann. 2019. Lc-quad 2.0: large dataset for complex question answering over wikidata and dbpedia. In International semantic web conference. Springer, 6978. [12] Cl√©mentine Fourrier, Nathan Habib, Alina Lozovskaya, Konrad Szafer, and Thomas Wolf. 2024. Open LLM Leaderboard v2. https://huggingface.co/spaces/ open-llm-leaderboard/open_llm_leaderboard. [13] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). [14] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. [n. d.]. survey on llm-as-a-judge. The Innovation ([n. d.]). [15] Yu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy Liang, Xifeng Yan, and Yu Su. 2021. Beyond iid: three levels of generalization for question answering on knowledge bases. In Proceedings of the web conference 2021. 34773488. [16] Bernal Jim√©nez Guti√©rrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, and Yu Su. 2025. From RAG to Memory: Non-Parametric Continual Learning for Large Language Models. In International Conference on Machine Learning (ICML). [17] Zhen Jia, Soumajit Pramanik, Rishiraj Saha Roy, and Gerhard Weikum. 2021. Complex temporal question answering on knowledge graphs. In Proceedings of the 30th ACM international conference on information & knowledge management. 792802. [18] Sumin Jo, Junseong Choi, Jiho Kim, and Edward Choi. 2025. R2-KG: GeneralPurpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs. arXiv preprint arXiv:2502.12767 (2025). [19] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551 (2017). [20] Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, S√∂ren Auer, et al. 2015. Dbpediaa large-scale, multilingual knowledge base extracted from wikipedia. Semantic web 6, 2 (2015), 167195. [21] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems (NeurIPS) 33 (2020), 94599474. [22] Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing. 2024. Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources. In International Conference on Learning Representations (ICLR). [23] Yucheng Li, Yunhao Guo, Frank Guerin, and Chenghua Lin. 2024. An opensource data contamination report for large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024. 528541. [24] Yanhong Li, Tianyang Xu, Kenan Tang, Karen Livescu, David McAllester, and Jiawei Zhou. 2025. OKBench: Democratizing LLM Evaluation with Fully Automated, On-Demand, Open Knowledge Benchmarking. arXiv preprint arXiv:2511.08598 (2025). [25] Zi Liang, Liantong Yu, Shiyu Zhang, Qingqing Ye, and Haibo Hu. 2025. How Much Do Large Language Model Cheat on Evaluation? Benchmarking Overestimation under the One-Time-Pad-Based Framework. arXiv preprint arXiv:2507.19219 (2025). [26] Junhong Lin, Song Wang, Xiaojie Guo, Julian Shun, and Yada Zhu. 2025. Temporal reasoning with large language models augmented by evolving knowledge graphs. arXiv preprint arXiv:2509.15464 (2025). [27] Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, et al. 2025. Deepseekv3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556 (2025). [28] Linhao Luo, Yuan-Fang Li, Reza Haf, and Shirui Pan. 2024. Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning. In International Conference on Learning Representations (ICLR). [29] Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Cehao Yang, Jiaxin Mao, and Jian Guo. 2025. Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning with Knowledge-guided Retrieval Augmented Generation. In International Conference on Learning Representations (ICLR). [30] Yu Malkov and Dmitry Yashunin. 2018. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and machine intelligence 42, 4 (2018), 824836. [31] Belinda Mo, Kyssen Yu, Joshua Kazdan, Proud Mpala, Lisa Yu, Chris Cundy, Charilaos Kanatsoulis, and Sanmi Koyejo. 2025. KGGen: Extracting Knowledge Graphs from Plain Text with Language Models. Advances in neural information processing systems (NeurIPS) (2025). [32] Medha Palavalli, Amanda Bertsch, and Matthew Gormley. 2024. taxonomy for data contamination in large language models. arXiv preprint arXiv:2407.08716 (2024). [33] Aaditya Singh, Muhammed Yusuf Kocyigit, Andrew Poulton, David Esiobu, Maria Lomeli, Gergely Szilvasy, and Dieuwke Hupkes. 2024. Evaluation data contamination in LLMs: how do we measure it and (when) does it matter? arXiv preprint arXiv:2411.03923 (2024). [34] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel Ni, Heung-Yeung Shum, and Jian Guo. 2024. Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph. In International Conference on Learning Representations (ICLR). [35] Alon Talmor and Jonathan Berant. 2018. The Web as Knowledge-base for Answering Complex Questions. In Proceedings of NAACL-HLT. 641651. [36] Priyansh Trivedi, Gaurav Maheshwari, Mohnish Dubey, and Jens Lehmann. 2017. Lc-quad: corpus for complex question answering over knowledge graphs. In International semantic web conference. Springer, 210218. [37] Denny Vrandeƒçiƒá and Markus Kr√∂tzsch. 2014. Wikidata: free collaborative knowledgebase. Commun. ACM 57, 10 (2014), 7885. [38] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency Improves Chain of Thought Reasoning in Language Models. In International Conference on Learning Representations (ICLR). [39] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems (NeurIPS) 35 (2022), 2482424837. [40] Cheng Xu, Shuhao Guan, Derek Greene, Kechadi, et al. 2024. Benchmark data contamination of large language models: survey. arXiv preprint arXiv:2406.04244 (2024). [41] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2024. Qwen2.5 Technical Report. arXiv preprint arXiv:2412.15115 (2024). [42] Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Gui, Ziran Jiang, Ziyu Jiang, et al. 2024. Crag-comprehensive rag benchmark. Advances in Neural Information Processing Systems (NeurIPS) 37 (2024), 1047010490. Conference acronym XX, June 0305, 2018, Woodstock, NY Junhong Lin, Bing Zhang, Song Wang, Ziyan Liu, Dan Gutfreund, Julian Shun, and Yada Zhu [43] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. 2018. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 conference on empirical methods in natural language processing. 23692380. [44] Wen-tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei Chang, and Jina Suh. 2016. The value of semantic parse labeling for knowledge base question answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 201206. [45] Liangliang Zhang, Zhuorui Jiang, Hongliang Chi, Haoyang Chen, Mohammed Elkoumy, Fali Wang, Qiong Wu, Zhengyi Zhou, Shirui Pan, Suhang Wang, et al. 2025. Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking. Advances in Neural Information Processing Systems (NeurIPS) (2025). [46] Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander Smola, and Le Song. 2018. Variational reasoning for question answering with knowledge graph. In Proceedings of the AAAI conference on artificial intelligence, Vol. 32. Additional Experiment Results A.1 KG Construction Efficiency and Cost"
        },
        {
            "title": "Analysis",
            "content": "Theoretical Complexity Analysis. Let ùëõ and ùëö denote the number of extracted entities and relations from corpus, and ùëÅ and ùëÄ denote the number of nodes and edges in the existing KG. KG extraction requires single LLM call with cost ùëÇ (ùëõ + ùëö). Entity and relation alignment are performed using HNSW-based similarity search [30], with sublinear complexity ùëÇ (ùëõ log ùëÅ + ùëö log ùëÄ). Subsequent merging and insertion operations involve lightweight attribute aggregation and incur linear cost ùëÇ (ùëõ + ùëö). Overall, the total complexity of KG evolution is ùëÇ (ùëõ log ùëÅ + ùëö log ùëÄ). In addition, the pipeline uses constant number of LLM calls per corpus (five in our implementation), making it practical for continuous and large-scale KG updates. Empirical Complexity Analysis. To assess the efficiency of KG construction, we measure the per-document extraction time across all corpora. Figure 3 plots the KG-update latency as function of document length (in characters). The results exhibit clear positive trend: longer documents incur proportional increases in processing time. This is expected, as extraction latency is dominated by the LLM forward pass, whose computational cost grows with input token count. We observe no evidence of superlinear blow-up. Instead, the empirical trend aligns with the theoretical ùëÇ (ùëõ log ùëõ + ùëö log ùëö) complexity of EvoKGs alignment and merge operations. Variance in the vertical direction is attributable to LLM-provider latency: extraction is executed via the OpenRouter API, whose backend routing introduces heterogeneity in response times that is unrelated to algorithmic complexity. Figure 3: Per-document KG construction latency as function of corpus length (in characters). Longer documents incur proportionally higher extraction time, and the trend follows the expected near-linear scaling dominated by LLM token processing. To clarify practical deployment: although Figure 3 reports perdocument latency, EvoKG supports data-parallel extraction. When Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge Conference acronym XX, June 0305, 2018, Woodstock, NY Table 6: DeepSeek V3.2-685B Model performance (accuracy, %) grouped by question types. We highlight the first and second best results. Datasets Arxiv-AI Arxiv-CY Arxiv-BIO H - gle p - gle s t o w. - ult Methods ) H - ult ult ffi ( al a t C n - O - gle p - gle S o d w. - ult ) - ult ult ffi ( al a t C d - O - gle p - gle s t o w. - ult ) H - ult ult ffi ( al a t C n - O IO CoT SC RAG 30.681.10 38.851.29 26.062.30 23.360.99 78.252.57 60.434.58 33.700.67 38.751.27 30.341.43 25.180.70 85.782.27 50.181.73 33.561.03 37.721.00 25.611.09 26.380.98 78.312.25 63.471.18 30.441.23 34.821.68 30.421.04 26.041.43 69.123.53 52.342.89 29.660.98 37.033.07 33.020.99 25.541.85 70.291.66 45.002.56 35.611.61 42.900.83 25.311.39 25.511.80 73.902.75 56.951.95 29.401.20 35.832.00 29.702.60 24.301.55 66.673.88 45.532.89 28.991.13 35.942.61 31.141.83 21.691.90 65.201.52 41.252.49 35.911.60 40.931.70 19.807.08 24.782.88 64.072.49 51.473.43 48.330.19 68.110.90 42.220.57 27.070.63 16.373.94 70.924.37 47.690.21 69.530.00 41.280.34 25.300.00 11.851.45 70.092.23 50.570.19 69.430.52 32.911.28 19.930.36 15.250.00 72.111. 1-hop KG 37.270.69 35.830.54 23.150.59 18.521.57 4.390.55 51.912.55 51.760.67 37.340.58 21.610.99 16.020.30 1.160.37 48.571.34 46.821.03 43.111.92 30.201.38 27.391.33 2.711.73 61.890.79 RAG + 1-hop KG 59.840.80 70.860.36 45.760.30 35.231.01 9.210.44 86.171.06 62.821.47 68.750.00 43.620.00 32.830.30 7.800.29 74.550.45 64.200.19 74.090.00 43.110.77 38.040.36 20.341.69 80.000.53 24.020.30 27.771.17 21.822.17 15.031.78 24.392.85 31.063.18 28.991.93 32.663.14 24.301.82 17.473.47 30.401.13 30.712.44 33.481.74 39.691.66 19.801.92 19.423.19 29.153.29 44.951.78 36.551.34 35.971.02 28.731.06 28.991.55 42.462.05 42.982.48 44.450.62 42.030.58 32.621.24 19.280.93 59.191.35 46.070.91 40.911.12 36.681.11 28.370.83 26.230.54 30.174.47 50.530.88 48.671.00 47.051.33 34.302.19 41.610.95 22.632.11 42.981.59 56.551.02 49.841.04 29.800.91 35.541.37 14.100.94 48.042.96 47.541.43 46.370.45 35.590.56 38.591.73 30.081.41 54.871.01 62.011.93 60.291.90 47.521.25 53.560.99 96.321.02 90.643.46 69.241.04 64.690.91 44.700.91 41.931.77 97.110.97 78.041.45 61.440.94 69.431.70 44.590.89 51.882.18 95.591.36 84.840.91 48.840.83 45.610.98 33.331.27 40.401.15 13.681.31 38.723.66 59.330.81 46.091.10 30.201.47 31.200.70 5.900.99 48.041.31 47.500.78 44.040.46 31.731.59 40.580.79 12.203.46 53.891.58 47.151.15 43.881.88 45.822.56 45.101.96 68.421.24 50.643.66 53.610.43 46.092.96 36.642.42 27.472.63 39.881.42 50.362.56 54.021.72 49.222.54 40.711.18 37.831.06 68.812.54 58.631.55 HippoRAG2.0 56.390.20 73.380.00 53.940.38 38.260.00 94.740.00 89.360.00 60.500.00 74.220.00 44.300.00 32.890.30 66.010.23 82.140.00 55.230.15 65.600.25 45.610.25 47.830.00 94.920.00 85.790.00 EvoReasoner 73.800.33 83.270.78 71.062.20 61.741.57 88.161.32 93.622.61 81.371.05 79.430.97 62.190.84 63.051.02 47.592.88 86.310.84 76.521.29 81.970.83 60.200.85 59.572.65 90.510.83 92.531.47 CoK RoG ToG ToG2.0 PoG R2KG Table 7: Qwen2.5-72B Model performance (accuracy, %) grouped by question types. We highlight the first and second best results. Datasets Arxiv-AI Arxiv-CY Arxiv-BIO - gle p - gle s t o w. - ult Methods ) - ult ult ffi ( al a t C n - O - gle S H - gle s t o w. - ult ) - ult ult ffi ( al a t C n - O - gle p - gle s t o w. - ult ) - ult ult ffi ( al a t C n - O IO CoT SC RAG 21.200.47 20.290.29 16.360.77 17.720.33 68.071.19 45.961.70 17.960.18 16.020.39 15.100.34 15.360.67 68.640.25 34.150.74 19.700.24 19.380.25 7.350.25 16.810.29 56.271.27 40.530.00 25.220.30 29.350.54 25.940.59 23.760.68 87.021.02 50.642.48 27.480.73 27.340.86 24.830.42 20.960.70 91.911.32 40.710.91 32.200.41 35.340.51 18.060.69 22.610.54 83.391.98 57.260.70 25.461.21 26.761.67 24.000.82 22.681.30 84.040.66 50.642.08 26.300.73 27.191.74 23.091.51 21.451.97 87.053.22 40.540.91 29.550.63 33.471.69 19.291.09 21.011.30 83.053.03 56.420.70 53.550.19 67.630.59 41.820.49 29.310.32 48.540.41 85.110.00 51.680.00 68.360.39 35.231.01 23.490.00 23.410.29 72.320.00 51.700.19 68.910.00 34.690.00 22.100.36 40.680.00 79.210.26 1-hop KG 33.980.20 23.880.29 17.580.00 14.900.27 8.070.35 37.451.70 43.030.21 24.220.00 12.621.96 10.360.59 1.730.37 34.640.67 35.610.24 20.520.25 17.350.56 17.540.54 0.000.00 36.740.21 RAG + 1-hop KG 58.770.68 65.470.00 42.221.03 32.210.55 37.720.72 82.980.00 65.970.00 68.750.00 38.030.32 28.510.28 15.030.00 74.700.42 61.360.38 69.170.26 35.200.51 37.680.00 35.590.00 82.630.00 37.191.60 41.012.13 33.211.50 31.812.90 74.742.68 66.813.95 34.452.04 35.003.22 28.192.68 23.612.10 69.831.73 55.182.61 42.581.69 43.632.44 25.512.16 29.711.30 71.534.72 62.001.71 37.430.53 32.661.08 27.640.62 30.870.42 71.232.44 33.620.85 47.980.81 32.810.99 25.770.81 18.190.96 71.451.07 41.791.04 38.793.53 38.030.41 22.861.27 29.130.54 77.973.39 50.210.86 53.090.39 35.110.70 38.791.38 39.600.74 45.091.31 60.851.70 57.140.88 40.311.61 29.531.20 25.781.50 13.990.85 55.000.91 45.830.71 40.280.56 32.780.75 45.290.36 45.342.20 58.680.26 59.520.30 54.820.70 44.730.45 43.891.32 90.351.24 83.401.59 63.610.57 55.781.27 29.531.20 29.161.64 96.070.43 69.461.04 60.080.66 58.240.70 38.980.83 51.880.58 93.561.27 80.840.42 51.650.20 36.120.54 30.551.56 35.700.78 53.680.35 60.851.70 54.620.46 38.910.58 28.190.95 19.400.59 20.920.85 50.890.98 46.210.63 38.130.78 29.800.41 42.170.54 60.001.36 58.420.74 43.130.87 37.991.06 36.361.63 30.340.89 73.682.60 45.531.70 50.341.08 42.501.61 30.870.42 26.140.30 60.921.25 48.391.43 50.530.51 39.380.73 35.310.59 32.324.31 64.073.28 53.371.08 HippoRAG2.0 17.671.81 29.783.74 20.483.88 5.230.27 23.681.75 30.217.66 13.450.00 19.530.00 8.591.61 4.581.69 14.341.39 14.290.00 30.300.24 41.450.00 22.450.00 11.880.35 56.270.68 46.110.26 EvoReasoner 71.750.76 75.540.59 65.863.02 53.472.70 88.304.20 90.784.37 69.753.40 63.2831.65 54.501.23 49.041.46 83.121.23 82.862.07 75.150.78 77.821.41 56.941.95 56.092.03 91.531.07 89.890.61 CoK RoG ToG ToG2.0 PoG R2KG run in parallel, the effective end-to-end KG construction time is substantially lower than the sum of individual extraction times. Together, Figure 3 and 2 empirically validate the scalability of EvoKG and demonstrate that both latency and token cost grow smoothly and predictably with corpus size. A.2 Additional Per-Question-Type Results. To provide more complete view of benchmark behavior across models, we report detailed per-question-type performance for all evaluated LLMs, including DeepSeek-V3.2, Qwen2.5-72B, LLaMA3.3-70B, and LLaMA-3.1-8B. For each model and baseline, we include accuracy grouped by question type along with standard deviation across 5 runs. Conference acronym XX, June 0305, 2018, Woodstock, NY Junhong Lin, Bing Zhang, Song Wang, Ziyan Liu, Dan Gutfreund, Julian Shun, and Yada Zhu Table 8: LLaMa 3.3-70B Model performance (accuracy, %) grouped by question types. We highlight the first and second best results. Datasets Arxiv-AI Arxiv-CY Arxiv-BIO - gle p - gle s t o w. - ult Methods ) - ult ult ffi ( al a t C n - O H - gle p - gle s t o w. - ult ) - ult ult ffi ( al a t C n - O - gle p - gle s t o w. H - ult ) - ult ult ffi ( al a t C n - O IO CoT SC RAG 31.930.20 31.830.31 22.580.50 20.300.87 73.031.14 46.812.13 27.390.49 31.090.91 29.930.68 15.900.30 71.561.18 36.960.71 29.360.33 32.900.26 20.280.66 25.180.60 77.540.73 51.971.14 32.730.88 33.990.60 25.911.16 20.300.87 75.661.14 39.891.76 29.580.43 33.441.04 30.601.62 27.111.01 80.121.07 44.820.36 34.090.86 36.680.83 25.201.10 28.410.71 74.241.98 50.741.55 35.141.19 33.630.93 27.121.38 18.960.87 76.754.04 39.363.84 30.420.57 33.590.70 33.560.95 26.750.90 79.311.23 45.892.00 33.410.88 38.451.20 24.490.97 26.380.74 73.561.73 50.841.51 51.610.45 66.730.31 39.240.50 18.960.87 47.370.62 67.021.84 50.280.52 66.670.37 37.360.32 27.310.28 19.270.72 67.560.42 47.730.76 65.800.00 35.710.51 25.000.36 44.070.00 67.370.00 1-hop KG 32.330.35 31.470.60 27.270.43 21.140.34 9.650.00 32.450.92 41.760.43 29.380.62 23.360.27 14.340.45 3.240.28 22.860.71 37.350.39 31.300.41 26.630.20 20.430.54 8.470.00 34.210.58 RAG + 1-hop KG 59.640.60 66.190.00 46.970.30 32.210.00 39.910.44 71.281.06 59.803.07 63.286.63 40.942.85 27.710.98 14.071.66 68.153.37 61.550.19 67.880.00 39.290.00 30.430.00 43.220.85 68.160.26 34.941.22 36.261.33 29.451.46 21.140.34 74.561.66 48.514.34 31.181.17 32.030.86 33.152.06 23.011.39 72.141.29 43.931.54 34.550.44 43.731.02 26.221.78 24.201.87 74.584.01 58.212.17 41.160.45 39.030.60 34.240.68 36.910.47 76.970.96 40.431.50 49.411.21 39.060.00 36.780.50 24.701.26 66.590.99 43.570.67 42.950.45 39.690.70 29.800.25 25.800.98 65.081.73 49.370.61 50.800.35 42.271.28 36.970.74 42.280.47 10.531.39 51.061.50 53.280.62 41.250.31 32.891.85 33.131.37 2.430.43 41.251.04 50.380.76 44.560.73 35.920.83 34.201.16 11.191.36 50.531.25 59.440.40 55.220.93 46.670.96 43.461.20 92.542.01 85.111.50 64.370.90 62.030.62 37.851.73 34.220.45 93.290.69 71.961.66 35.915.48 42.383.48 41.021.10 48.991.08 91.531.86 81.470.52 49.700.66 41.550.31 39.390.74 41.780.56 11.180.73 47.871.84 53.280.56 40.310.38 32.481.24 31.810.59 2.540.28 39.820.91 49.550.15 44.460.51 34.180.32 32.751.41 12.880.83 51.680.39 48.900.77 46.040.51 38.940.50 40.441.45 63.382.73 39.891.76 48.741.16 51.561.91 42.822.18 36.020.80 48.550.73 49.461.84 49.770.66 52.641.45 38.781.12 32.030.54 70.512.30 58.951.60 HippoRAG2.0 52.010.67 66.010.93 48.330.26 30.870.00 87.720.00 82.451.76 47.140.62 71.093.16 40.400.78 27.830.96 87.280.63 71.790.91 50.980.19 64.770.00 41.120.25 33.770.58 85.421.36 80.210.26 EvoReasoner 71.290.20 76.980.72 63.641.21 53.361.01 56.140.88 85.110.00 75.291.60 77.812.07 57.051.53 51.081.76 56.761.43 83.751.54 75.571.43 76.940.45 53.950.91 53.440.94 53.811.85 89.341.31 CoK RoG ToG ToG2.0 PoG R2KG Table 9: LLaMa 3.1-8B Model performance (accuracy, %) grouped by question types. We highlight the first and second best results. Datasets Arxiv-AI Arxiv-CY Arxiv-BIO - gle p - gle s t o w. H - ult Methods ) - ult ult ffi ( al a t C d - O - gle p - gle s t o w. - ult ) H - ult ult ffi ( al a t C n - O - gle p - gle S o d w. - ult ) - ult ult ffi ( al a t C d - O IO CoT SC RAG 20.480.92 19.570.54 12.481.12 14.901.49 56.671.53 26.382.17 16.890.49 16.250.58 16.240.27 12.050.85 54.221.81 29.821.07 19.850.62 26.110.96 11.730.72 17.390.46 57.291.66 40.951.50 25.540.32 27.341.93 20.850.73 17.320.99 54.561.95 31.063.18 25.711.17 21.251.51 24.031.30 20.361.34 50.062.98 27.140.44 26.360.98 31.193.76 17.860.91 21.161.80 51.532.03 44.212.47 26.592.15 25.760.29 19.761.94 16.241.23 53.864.10 26.381.70 25.381.24 24.061.88 26.311.77 20.241.50 54.572.27 25.891.49 25.681.98 31.710.39 18.571.35 22.173.06 57.972.92 39.472.35 47.590.20 60.071.80 34.850.30 23.150.34 3.510.00 68.090.00 47.690.21 63.280.00 31.881.01 22.290.00 2.310.00 54.020.45 47.350.00 64.770.00 30.100.51 25.000.36 1.690.00 58.680.79 1-hop KG 32.290.32 25.320.54 20.120.89 18.521.73 0.880.00 37.021.70 45.970.68 26.561.10 16.911.15 11.080.61 0.000.00 36.251.66 35.610.34 28.080.21 20.710.83 17.101.69 0.000.00 45.680.84 RAG + 1-hop KG53.820.40 68.350.72 40.910.30 29.530.67 3.510.00 70.210.00 59.870.21 69.530.78 33.560.00 28.920.60 1.730.00 70.090.45 57.770.57 68.650.26 32.910.77 34.060.72 5.081.69 70.000.53 29.645.72 33.672.96 25.093.03 23.222.19 76.843.95 49.364.34 30.252.33 31.562.60 25.501.12 15.901.73 76.181.57 42.683.72 34.092.57 35.752.60 21.022.24 23.912.25 67.124.87 53.371.40 28.591.06 22.450.54 16.121.41 22.421.83 42.281.70 23.401.35 32.771.68 24.221.48 17.182.19 10.241.48 41.731.69 25.712.96 28.641.65 24.461.52 13.781.02 8.121.06 39.322.92 35.581.65 19.441.21 23.742.03 13.091.31 20.133.03 1.751.24 34.892.89 24.375.50 19.145.11 12.083.36 9.191.56 0.870.65 30.131.93 16.896.30 15.854.33 11.534.00 12.904.45 0.680.83 26.849.28 52.451.00 45.180.84 30.061.06 41.072.14 77.021.87 70.213.01 56.620.91 53.911.91 29.362.34 31.171.16 79.911.93 57.142.75 50.534.72 52.231.20 37.040.52 35.071.63 74.582.14 72.211.87 17.671.68 17.701.08 15.270.80 20.671.37 2.460.35 32.773.18 24.543.07 15.314.27 11.142.27 10.120.89 1.390.28 34.461.66 20.232.25 17.933.51 18.671.76 22.171.63 0.680.83 34.322.73 26.751.52 26.911.08 20.732.11 23.222.31 34.043.82 25.965.93 36.972.14 26.562.84 22.281.77 16.751.54 30.874.50 28.752.49 31.291.47 25.801.84 18.371.96 19.283.96 30.516.95 39.261.13 HippoRAG2.0 43.370.00 55.400.00 40.850.30 24.430.33 81.050.43 67.661.59 43.360.31 56.720.38 36.510.33 23.490.38 73.290.43 68.210.44 43.410.19 64.870.39 32.140.00 32.900.74 76.270.00 68.530.21 EvoReasoner 53.730.59 62.160.98 49.582.28 35.842.90 78.422.39 77.024.13 55.970.67 60.941.78 40.271.12 35.061.39 70.752.52 72.862.86 56.290.98 66.421.29 42.551.63 34.351.63 74.244.21 80.630.91 CoK RoG ToG ToG2.0 PoG R2KG"
        }
    ],
    "affiliations": [
        "Cornell University",
        "IBM Research",
        "Massachusetts Institute of Technology",
        "University of Central Florida"
    ]
}