{
    "paper_title": "Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models",
    "authors": [
        "Tiezheng Zhang",
        "Yitong Li",
        "Yu-cheng Chou",
        "Jieneng Chen",
        "Alan Yuille",
        "Chen Wei",
        "Junfei Xiao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Building state-of-the-art Vision-Language Models (VLMs) with strong captioning capabilities typically necessitates training on billions of high-quality image-text pairs, requiring millions of GPU hours. This paper introduces the Vision-Language-Vision (VLV) auto-encoder framework, which strategically leverages key pretrained components: a vision encoder, the decoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large Language Model (LLM). Specifically, we establish an information bottleneck by regularizing the language representation space, achieved through freezing the pretrained T2I diffusion decoder. Our VLV pipeline effectively distills knowledge from the text-conditioned diffusion model using continuous embeddings, demonstrating comprehensive semantic understanding via high-quality reconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the intermediate language representations into detailed descriptions, we construct a state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o and Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and significantly reduces data requirements; by primarily utilizing single-modal images for training and maximizing the utility of existing pretrained models (image encoder, T2I diffusion model, and LLM), it circumvents the need for massive paired image-text datasets, keeping the total training expenditure under $1,000 USD."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 2 4 0 1 7 0 . 7 0 5 2 : r Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models Tiezheng Zhang1, Yitong Li2, Yu-Cheng Chou1, Jieneng Chen1, Alan Yuille1, Chen Wei3, Junfei Xiao1, 1Johns Hopkins University 2Tsinghua University 3Rice University Project Lead https://lambert-x.github.io/Vision-Language-Vision/"
        },
        {
            "title": "Abstract",
            "content": "Building state-of-the-art Vision-Language Models (VLMs) with strong captioning capabilities typically necessitates training on billions of high-quality image-text pairs, requiring millions of GPU hours. This paper introduces the Vision-LanguageVision (VLV) auto-encoder framework, which strategically leverages key pretrained components: vision encoder, the decoder of Text-to-Image (T2I) diffusion model, and subsequently, Large Language Model (LLM). Specifically, we establish an information bottleneck by regularizing the language representation space, achieved through freezing the pretrained T2I diffusion decoder. Our VLV pipeline effectively distills knowledge from the text-conditioned diffusion model using continuous embeddings, demonstrating comprehensive semantic understanding via high-quality reconstructions. Furthermore, by fine-tuning pretrained LLM to decode the intermediate language representations into detailed descriptions, we construct state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o and Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and significantly reduces data requirements; by primarily utilizing single-modal images for training and maximizing the utility of existing pretrained models (image encoder, T2I diffusion model, and LLM), it circumvents the need for massive paired image-text datasets, keeping the total training expenditure under $1,000 USD. Figure 1: VLV matches GPT-4os descriptive fidelity at three orders of magnitude lower cost. Left: VLV captures all salient objects, matching GPT-4o in coverage without hallucinations, yet better preserving their spatial layout. Right: On the FIDcostthroughput plane, VLV reaches comparable FID, trains for orders-of-magnitude less, and delivers vastly higher captions-per-dollar at inferenceproving that detail-rich descriptions need not demand massive budgets. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Multimodal representation learning aims to capture meaningful semantic relationships between vision and language. Broadly, existing approaches can be categorized into three major paradigms based on the interaction between textual and visual modalities: (1) vision-language models (VLMs), where images serve as inputs and text as outputs [50, 37, 5, 84, 68, 55, 52]; (2) contrastive learning frameworks [51, 83, 10], where image and text embeddings are aligned into shared latent space through contrastive objective; and (3) text-to-image generative models, where textual descriptions condition the generation of visual content [55, 52]. Traditionally, the first two paradigms, vision-language modeling and contrastive learning, have been predominantly utilized for learning robust multimodal embeddings. In contrast, text-to-image generative models, such as diffusion-based architectures [27], are generally considered generative tools rather than effective mechanisms for multimodal embedding learning. Although intuitively these generative models must implicitly encode detailed semantic relationships to produce coherent images, their potential for multimodal tasks like image captioning has not been fully realized. Recent research suggests that text-to-image generative models indeed capture rich, nuanced semantic structures [66, 69], highlighting potential opportunities in applying the analysis-by-synthesis approach [82]. Rooted in cognitive science, this idea has long argued that perception works by imagining the hidden causes of signal and selecting the one that best explains it. Motivated by this insight, our work demonstrates how pretrained text-to-image diffusion models can effectively transfer their inherently rich multimodal representations to downstream vision-language tasks such as captioning and VQA, where text-to-image diffusion models imagine the image, whose corresponding multimodal representation serves the best explanation. Specifically, we introduce novel architecture termed the Vision-Language-Vision (VLV) autoencoder. In this framework, an open-source pretrained diffusion model, specifically Stable Diffusion 2.1 [55], is used as powerful frozen diffusion decoder. We distill knowledge from this decoder into bottleneck representation through regularization process on the language embedding space produced by an encoder [72]. Next, these continuous intermediate representations are decoded through pretrained LLM decoder [77] after alignment, generating detailed captions. Our approach achieves captioning performance competitive with leading proprietary models, including GPT-4o [1] and Gemini 2.0 Flash [22], while utilizing significantly smaller, open-source models. Our methodology also exhibits strong scalability: We obtain substantial performance improvements when scaling the training dataset from 6M to 40M images. Notably, by primarily leveraging singlemodal images, the data collection approach is much less of burden compared to extensive paired image-text datasets. Adding up maximizing the utility of existing pretrained models, training costs remain below $1,000 USD (less than 1,000 GPU hours), significantly enhancing accessibility and promoting broader innovation within the vision-language research community. Additionally, we explore emergent properties of the proposed VLV autoencoder: a) semantic richness, where learned embeddings encode detailed semantic aspects, including object 3D pose and orientation, resulting in robust spatial consistency; and b) compositional generalization, achieved by concatenating caption embeddings from distinct images, allowing the model to disentangle foreground objects from backgrounds effectively and compose novel, coherent, and visually plausible images. In summary, the primary contributions of this work are as follows: We introduce Vision-Language-Vision (VLV) Auto-Encoder, novel framework for scalable and efficient knowledge distillation from pretrained text-to-image diffusion models. This approach learns language-semantic representations only using image-based training. The construction of lightweight yet effective LLM-based caption decoder, achieved by strategically integrating pretrained models, resulting in negligible training overhead. Comprehensive experimental results validate that the proposed captioner exhibits highly competitive captioning performance relative to SoTA VLMs, such as GPT-4o, and surpasses other open-source models of comparable parameter counts. An investigation into the emergent properties of the VLV framework, specifically highlighting the preservation of spatial semantics and advanced multi-image compositionality. These findings underscore the efficacy and potential of the learned representations."
        },
        {
            "title": "2 Related Work",
            "content": "Visual Autoencoder (VAE) has long served as foundational method for unsupervised representation learning [19, 24, 26, 32, 61]. Variants such as VQ-VAE [60, 54] and discrete VAE [53] extend this idea by learning discrete and structured representations. Although widely used for image tokenization in multimodal learning [21, 53, 55, 80, 81, 41], their quantized latent spaces often entangle semantics and require co-trained decoders to be effective. Recent works, like De-Diffusion [69], replace the latent bottleneck with text sequences decoded by pretrained diffusion models, aiming for interpretability. ViLex [66] further pushes this by directly training visual embeddings through generative supervision from frozen diffusion backbones, bypassing token-level representations entirely. Despite these closed-sourced advances, our VLV model is the first to efficiently build vision-language-vision auto-encoder with all open-source modules with minimal training cost. Vision-Language Captioners. Recent advances in vision-language models (VLMs) have significantly advanced image captioning by leveraging large-scale image-text pretraining and powerful multimodal architectures. Some previous works [79, 37, 36, 62, 73] aligned visual encoders with language decoders, while Flamingo [5], Kosmos [47, 29], and ShareGPT4V [11] highlighted few-shot and interleaved vision-text capabilities. Recent models like GPT-4o [2], Gemini [22], Qwen-VL [8, 76], and LLaVA [42] combined instruction tuning with powerful language backbones for fluent captioning. Large-scale systems such as PaLI-X [12], mPLUG-2 [74], InternVL [16], and CogVLM [63] scaled model and data size to achieve top performance on COCO [14], Flickr [48], NoCaps [3], and TextCaps [57], while IDEFICS [33], OpenFlamingo [6], Fuyu-8B [9], and Baichuan-omni [39] offered strong open-source alternatives. Emerging models like Emu3 [65], NVLM [17], Pixtral [4], and Molmo [18] further demonstrated the effectiveness of diverse multimodal modeling strategies. Despite these advances, most models depend on massive image-text pairs and costly training. In contrast, our VLV framework distills knowledge from pretrained diffusion model using single-modal image data, enabling high-quality captioning without requiring web-scale, high-quality labels. Representation Learning with Diffusion Models. growing body of work has explored leveraging diffusion models for representation learning across diverse modalities and tasks [49, 67, 30, 59]. De-Diffusion [69] and ViLex [66] used frozen T2I models for language-aligned embedding learning. Other works, like DreamTeacher [35], distilled diffusion model features into discriminative backbones, while DiffMAE [70] recast denoising as masked autoencoding. Several studies also demonstrated that diffusion models can serve directly as zero-shot classifiers [34] or that their intermediate activations encode linearly separable features [71]. In the vision-language domain, SPAE [81] and RLEG [85] bridged image-language understanding using semantic autoencoding and synthetic contrastive supervision, respectively. ODISE [75] and DIVA [64] used diffusion priors to boost open-vocabulary segmentation and CLIPs perception, while RepFusion [78] explicitly mined time-step features for classification. Finally, simplification studies like Deconstructing DDMs [15] revealed that even stripped-down DAEs retain strong representational power. Unlike prior methods that require co-training of text and vision modules, handcrafted bottlenecks, or synthetic supervision, our method directly transfers generative knowledge into latent space that supports both high-fidelity reconstruction and competitive caption generation with minimal compute."
        },
        {
            "title": "3 Method",
            "content": "In this section, we introduce our proposed pipeline, which employs vision-language-vision (VLV) autoencoding to distill high-fidelity semantic information from images and subsequently decodes these semantics into descriptive captions using multi-modal language model. We begin by outlining the pipeline architecture in 3.1. Next, in 3.2, we describe how we leverage pretrained diffusion model to encode images into compact, continuous semantic embeddings, eliminating the need for explicit image-text pairs during training. Finally, in 3.3, we detail how these embeddings are decoded into natural-language captions via alignment with pretrained large language model (LLM)."
        },
        {
            "title": "3.1 Pipeline Overview",
            "content": "VLV aims to extract high-fidelity semantic information from images through pretrained T2I diffusion model. Previous similar work [69] utilizes discrete text token of CLIP as latent representation directly and Gumbel-Softmax [31, 45] for optimization, resulting in training inefficiency and lack of finegrained semantic details. In contrast, we train our model using continuous embedding space for 3 Figure 2: Method Overview. Our method has two stages: 1) vision-language-vision autoencoding for learning language semantics, 2) representation decoding into discrete language tokens through multi-modal LLM alignment. Our model has three major modules (i) VLV Encoder: visual backbone augmented with lightweight multi-modal adapter maps an input image into continuous caption embedding with compact semantic information; (ii) Diffusion Decoder: frozen text-to-image diffusion model reconstructs the image; (iii) Caption Decoder: pretrained large language model with an MLP projector decodes language-centric representations into comprehensive captions. better training convergence, stability, and efficiency and decode the embeddings to discrete language tokens like multi-modal LLMs to generate text tokens given encoded visual embeddings of images. Our VLV encoder extracts continuous caption embeddings directly from images. Training is fully self-supervised: frozen text-to-image diffusion model serves as the decoder, reconstructing each image from its caption embeddings. Because the text-to-image diffusion model is fixed, the encoder must embed all information necessary for faithful reconstruction, effectively distilling the diffusion models rich visual knowledge into lightweight vision backbone, while eliminating the need for paired imagetext data. Next, we fine-tune VLV encoder together with an LLM-based decoder that maps them to natural-language captions. Since the caption embeddings obtained by the VLV encoder are compact and encode only implicit semantics, we utilize pretrained LLM to decode them into descriptive image captions. The autoregressive architecture of the LLM and its rich linguistic knowledge enable it to generate natural, coherent sentences with flexible length. This alignment uses paired imagetext data specified in 4.1."
        },
        {
            "title": "3.2 Knowledge Distillation from Diffusion Models",
            "content": "Following self-supervised learning framework, this stage adopts symmetric auto-encoder architecture that encodes to and decodes from latent tokens as information bottleneck. Given an image RHW 3, visual backbone produces visual tokens RNvDv . linear projection followed by LayerNorm [7] maps them to RNvD. These tokens are concatenated with Nt dummy prompt embeddings tprompt to form = [ v; tprompt ] R(Nv+Nt)D, which multimodal Transformer encoder converts to contextual states hE = Enc(X). Since there is no caption for supervision in this stage, we inject Nq learnable query tokens RNqD on the Transformer decoder side; cross-attention with hE yields ˆh = Dec(q, hE) RNqD. lightweight MLP ϕ projects these states to the channel dimension of the frozen CLIP text encoder in the diffusion model, producing the caption embedding = ϕ(ˆh) RNqdCLIP . The text-to-image diffusion model remains frozen; it receives as conditioning and is optimised only indirectly. Specifically, with latent z0 = E(x) and 1 αt ϵ, the frozen U-Net predicts the noise ϵθ(zt, t, z); the αt z0 + its noisy counterpart zt = encoder parameters are updated by the standard denoising loss Ldenoise = Ex,ϵ,t (cid:13)ϵ ϵθ(zt, t, z)(cid:13) (cid:13) 2 2. (cid:13) (1) 4 The auto-encoder architecture forces visual encoder to distill all information required for faithful reconstruction into the compact caption embedding z. Instead of using image-text paired data, visual encoder learns the inverse I2T mapping process through pretrained T2I diffusion decoder, which contains rich cross-modal knowledge. Rather than discrete text token and Gumbel-softmax, we use implicit and continuous embedding as latent for remaining detailed semantic information in compact way without losing fidelity. The faithful encoding performed in Stage-1 forms the foundation for high-quality understanding and captioning in Stage-2, ultimately enabling accurate reconstruction."
        },
        {
            "title": "3.3 Caption Decoding from Language-centric Representations",
            "content": "The aim of this stage is to decode intermediate representations into readable, high-quality captions. Previous structure design has fixed-length word tokens, contradicting with the inherent difference of complexities among all kinds of images, e.g., picture of an apple and picture of big city should have semantic complexities of different levels. The setting limits the effectiveness and flexibility of image encoding, result in losing the potential of faithful reconstruction. Thus we introduce our LLM-based VLV Caption Decoder, which can decode unlimited and length-flexible natural language descriptions of images from compact semantic embeddings. As shown in Section 3, we train our VLV encoder and LLM decoder with our image-text pair (x, y). We first obtain the caption embeddings RNqdCLIP via VLV encoder (E). Since is in the CLIP textembedding space, we pass it through the frozen CLIP text encoder , obtaining contextual representations = (z) RNqdT . lightweight trainable MLP ψ : RdT RdLM then projects these vectors to the hidden size of causal language model G: = ψ(c) RNqdLM. During training with paired imagetext pair {(x, y1:T )}, the projected vectors are prepended to the ordinary token embeddings of the caption, forming the input stream [ e; Embed(y1), . . . , Embed(yT ) ]. With positions corresponding to masked out, we compute the autoregressive loss only on real words: LLM = (cid:88) t=1 log pθ (cid:0)yt e, y<t (cid:1), (2) where θ = {E, ψ, G} are the only trainable parameters; the CLIP text encoder remain untouched. At inference, we compute = E(x) = (z) = ψ(c) and feed the projected vectors (without any text tokens) into the language model G, which autoregressively samples the caption. Thus this stage bridges the previous visual semantics to natural language with only lightweight projection head, while fine-tuning and and keeping frozen. This design lets compact latent embedding be flexibly decoded into human-readable captions of arbitrary length, while preserving fine-grained image semantics. And the progressive training-and-inference strategy achieves superior performance, as demonstrated empirically in Table 4."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Data Collection. From LAION-2B-en-aesthetic, subset of LAION-5B [56], we curate 40M image subset. For training stability we keep only images whose shorter side is greater than 512, aspect ratio in the range of 0.5 to 2, and watermark probability less than 0.5. The resulting images are used to train the VLV auto-encoder under image-only supervision, without any accompanying text. Next, we query Gemini-2.0 Flash [58] to generate captions for 6M images in our dataset, producing aligned image-text pairs that fine-tune the lightweight language decoder. An overview for crafting our image-text pairs dataset used in alignment training is shown in appendix. Despite using only 0.4% (40M/10B) of the WebLI dataset [13] used by De-Diffusion [69], our method still learns strong language-oriented semantics through the vision-language-vision auto-encoding pipeline. 5 Figure 3: Reconstruction with language semantics. For each original input image (top), we feed its caption embedding directly to the frozen diffusion decoder and obtain reconstruction (middle) that preserves high-level semantics and fine-grained appearance cues. The same embedding is then decoded by the LLM; prompting Midjourney with that caption yields an image of high fidelity. Guidance Scale Original [14] Recap-7B [38] LLaVA-v1.5-7B [42] Qwen2.5-VL-7B [8] Florence-2 Large [72] VLV (Ours) Gemini 2.0 Flash [58] GPT-4o [2] 1.0 16.62 12.26 14.54 12.61 10.61 11.47 12.82 12.16 2.0 9.90 7.70 9.99 6.98 7.51 6.64 5.87 6. 3.0 12.69 10.16 12.93 9.19 9.95 8.56 7.57 7.96 4.0 14.49 11.82 14.91 10.59 11.35 9.90 8.77 9.25 Table 1: Benchmark Captions Through Textto-Image Reconstructions. We evaluate the captions through FID scores (), with image recontruction. We use Stable Diffusion 3.5 Medium to reconstruct images with captions. Best results with open-source models are bolded; : best for all. Users Gemini 2.0 Average Qwen2.5-7B [8] 5.00 GPT4o [2] VLV (Ours) 5.20 5.17 5.07 5.25 5. 5.03 5.23 5.18 Table 2: Benchmark Captions Through Users and VLM Rating. We asked human users and state-of-the-art vision-language model (VLM), i.e., Gemini 2.0 Flash, to rate captions generated by different models, employing scoring rubric ranging from 1 to 6. The evaluation criteria encompassed semantic accuracy, linguistic fluency, and relevance to the corresponding images. Training Details. When training our VLV auto-encoder, we initialize the image encoder part with Florence-2 [72] pretrained weights. The additional Nq = 77 learnable queries are randomly initialized. We use AdamW [44] optimizer with (β1, β2) = (0.9, 0.99) and decoupled weight decay of 0.01. Training runs for 200K steps with batch size 512 on 8 RTXTM 6000 Ada GPUs ( 4 days). The learning rate starts at 5e-5 and follows cosine schedule [43]. We use Qwen-2.5 [77] pretrained models for initializing the LLM decoder. We train the captioning decoder with 100K steps, having the batch size of 64. The learning rate decays linearly starting at 1e-5. We use FP32 in autoencoder training to make models converge with stability, while the LLM decoder training uses BF16."
        },
        {
            "title": "4.2.1 Text-Conditioned Reconstruction with Captions",
            "content": "We assess caption quality by feeding each decoded caption to Stable Diffusion 3.5 Medium [20] and computing the Fréchet Inception Distance (FID) [25] between the synthesized and original images on 30K samples from the MS-COCO 2014 validation split [14]. Captions are generated with four state-of-the-art VLMs: Florence-2 [72], Qwen2.5-VL [8], Gemini 2.0 Flash [58], and GPT-4o [2]. Image synthesis employs the rectified flow-matching sampler using 40 inference steps and classifier-free guidance [28] scale from 1.0 to 4.0. As Table 1 shows, our captions achieve an FID essentially indistinguishable from GPT-4os (difference < 0.5) and markedly lower (better) than those of Florence-2 and Qwen2.5-VL, indicating that our captions convey visual semantics on par with the strongest public baseline; only the closed-source Gemini 2.0 Flash attains marginally better score. Figure 3 shows qualitative results on generated images by both caption embeddings and corresponding decoded captions, illustrating the faithfulness of our caption embeddings. 4.2.2 Captioner Arena: Rating with VLMs and Humans We benchmark caption fidelity by comparing state-of-the-art visionlanguage models (VLMs) with human raters under the identical three-criterion rubriccoverage, no hallucination, and spatiallayout consistencyand the 7-point rating scale (06) introduced in Appendix. random sample 6 Benchmark Shot VQAv OK-VQA 0 4 32 0 4 32 Accuracy (%) Florence-2 Large Qwen-2.5-VL-7B Gemini 2.0 Flash VLV (ours) 58.74 60.05 63.28 46.80 55.36 59.72 61.74 62.37 63.77 45.83 55.11 61. 62.52 63.36 64.05 46.34 56.48 62.31 58.55 61.72 63.60 45.31 54.10 60.25 Table 3: Few-shot VQA Evaluation(Text-only). We evaluate the VQA accuracy (%) on VQAv2 and OK-VQA under zero-shot or few-shot settings. DeepSeek-V3 answers only from the caption text. By 32-shot, VLV matches the best open-source model (Qwen-2.5) and sits within 1 percentage of the overall leader (Gemini 2.0 Flash), despite being far cheaper to train and run. Nq 16 32 77 FID 5.72 5.60 5.30 Trainable modules MLP only MLP + LLM decoder MLP+ LLM decoder + VLV encoder 1.0 14.27 12.22 11.47 2.0 9.71 7.55 6.64 3.0 11.87 9.58 8.56 4.0 13.22 10.84 9.90 Table 4: Ablation Studies. Left: Effect of the number of learnable query tokens (Nq). Right: Effect of unfreezing modules in Stage-2; both reported by FID (). # Images(M) 6 18 1.0 11.38 10.14 9.71 2.0 9.01 7.57 7.22 3.0 10.33 8.41 7.70 4.0 10.81 8.78 7."
        },
        {
            "title": "1.0\nQwen-2.5 0.5B 14.70\nQwen-2.5 1.5B 12.25\n11.47\nQwen-2.5 3B",
            "content": "2.0 9.37 7.30 6.64 3.0 11.26 9.16 8.56 4.0 12.45 10.26 9.90 Table 5: Scalability in Data and Decoder Scale. FID () computed at guidance scales 14 for (left) training-data size and (right) caption-decoder size. VLV demonstrates strong scalability. of 200 images from the MS-COCO 2014 validation split [14] is paired with captions produced by Qwen-2.5 VL, GPT-4o, and VLV. Each imagecaption pair is then evaluated by one VLM judge (Gemini 2.0 Flash) and three independent human raters. For every pair the judge returns single score {0, . . . , 6}; the same rubric is applied by the human raters. Table 2 shows that VLV matches GPT-4o within < 0.05 points on the 06 scale, surpasses Qwen-2.5-VL-7B by 0.15 on average, and is preferred by one of the three human raters. These results confirm that our caption embeddings yield human-level captions while remaining competitive with the strongest commercial VLMs."
        },
        {
            "title": "4.2.3 Text-Only Question-Answering with Captions",
            "content": "Because our caption embeddings capture both global semantics and fine-grained appearance cues, we assess their effectiveness on open-ended visionlanguage tasks using VQAv2 [23] and OKVQA [46] validation sets. Following Wei et al. [69], each caption is inserted as image context in large-language-model (LLM) prompt, which the LLM then completes to answer the visual question. An answer is deemed correct only if it exactly matches the ground truth. We evaluate our captions with DeepSeek-V3 [40] in both zero-shot and few-shot settings, without any additional fine-tuning. Table 3 shows the zero-, 4-, and 32-shot accuracies using captions generated by different VLMs. In strict zero-shot, VLV trails the best baseline by roughly three percentage points, yet it gains the most from extra in-context examples (about five points on VQAv2 and fifteen on OK-VQA),so that by thirty-two shots it lies within single point of the state of the art. Although VLV is not the top scorer in every setting, it reaches comparable while training at lower cost, underscoring its scalability."
        },
        {
            "title": "4.3 Ablation Studies",
            "content": "We conduct two complementary ablation studies in this section. (1) Trainable-parameter analysis. We probe the impact of trainable parameters by (i) varying the dimensionality of the learnable queries when training VLV auto-encoder and (ii) selectively unfreezing individual modules of the VLV encoder while training the LLM decoder. (2) Scalability analysis. We test how performance scales by (i) scaling the training corpus from 6M to 18M and 40M images, and (ii) increasing the size of the autoregressive captioning decoder from 0.5 to 1.5 and 3 parameters. Progressive Training Leads Better Performance. Herein, we train VLV with different trainable parameters settings to explore the trade-off between performance and training cost. Stable Diffusion 7 Figure 4: Representation Learning Beyond Text: Spatial Preservation. The figure compares the original images (left) with those reconstructed by our embeddings. The accurate 6D poses of individual objects and the relative spatial configurations among multiple objects demonstrate the methods strong capability in capturing spatial structure. # Images(M) 18 34 40 Angle () 0.1564 0.1287 0.1227 0.1016 Center () 0.1625 0.1498 0.1402 0.0988 Scale () 0.1775 0.1773 0.1835 0.1222 Table 6: Quantitative Comparison of Spatial Awareness. With more supervision images, VLV demonstrates improved spatial awareness. We evaluate this by measuring the L1 distance deviation between the bounding boxes of original and generated images with identical labels, as detected by Gemini 2.0 Flash [22]. Figure 5: Continual Spatial Representation Learning VLV enables continual 3D spatial representation learning. 2.1s CLIP text encoder accepts at most 77 tokens, and our default uses this full budget (Nq=77). We halve the number of learnable queries to Nq=16, 32 and gauge the impact by reconstructing MS-COCO 2017 test images from the resulting caption embeddings and reporting FID. In our second stage training, we progressively unfreeze the modules, starting with MLP first followed by the LLM decoder and finally the VLV encoder to see how many extra parameters are worth optimizing. Table 4 shows how reconstruction FID and caption quality improve smoothly with more trainable weights, clarifying the trade-off between performance and training cost. Scalability of VLV. During training of the VLV auto-encoder we save intermediate checkpoints after the model has processed 6M and 18M images. To assess scalability, each checkpoint is used to extract caption embeddings for the 30 images in the MS-COCO 2014 validation split described in 4.2.1. These embeddings are passed to the frozen diffusion decoder to reconstruct the images, and the resulting FID scores are reported in Table 5. We further probe model capacity by replacing the Qwen-2.5 3B caption decoder with its 1.5 and 0.5 variant while keeping all other components fixed (same table). In both cases FID degrades smoothly as data or decoder size is reduced, confirming that VLV benefits predictably from more training images and larger language decoder."
        },
        {
            "title": "4.4.1 Representation Learning beyond Text: 3D Visual Awareness",
            "content": "Besides rich details, we also find our embeddings have scalable spatial awareness. During training, as the diffusion decoder is exposed to larger pool of images, the model steadily refines its spatial priors. To quantify this effect, we use Gemini 2.0 Flash to recover 3D bounding boxes for the primary objects in original images and compare them with boxes reconstructed from caption embeddings. Table 6 show consistent reduction in pose estimation errors, and the examples in Figure 4 illustrate that VLV not only captures the poses of individual objects more accurately but also better preserves their spatial relationships. These results demonstrate that VLV effectively translates larger training image sets into sharper spatial understanding, as visualized in Figure 5. 8 Figure 6: Emerging compositionality with multi-image semantics. Given two input imagesa Siberian cat at the left edge of the frame and either (above) Van Gogh-style painting or (bottom) Mount Fuji landscapewe truncate and concatenate their caption embeddings and feed the composite vector to Stable Diffusion 2.1. The generated outputs faithfully preserve the cats spatial layout while transferring the desired artistic style or background, without any extra fine-tuning or text prompts."
        },
        {
            "title": "4.4.2 Compositionality with Multi-image Semantics",
            "content": "VLV semantic representation space exhibits strong compositional properties across multiple images, as illustrated in Figure 6. In the leftmost example, we begin with two images: (i) photograph of Siberian cat positioned on the left side of the frame, and (ii) Van Goghstyle painting. By truncating the trailing tokens of each caption embedding and concatenating the resulting vectors, we create joint embedding that is fed to Stable Diffusion 2.1. The synthesized output preserves the spatial layout of the cat while adopting the Van Gogh style, indicating that our embeddings encode both content (e.g., object identity and position) and style (e.g., artistic rendering). Notably, this compositional behavior emerges without any additional fine-tuning or reliance on text prompts. Further style transfer examples, including cartoon and Disney-style Shiba Inus, as well as try-on scenarios like Shiba Inu or man wearing sunglasses and man trying on hoodie or simple compositional of two objects like Shiba Inu sitting in front of Fuji Mount and sunglasses on hat."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we presented the Vision-Language-Vision (VLV) auto-encoder, novel framework for scalable and efficient knowledge distillation from open-source pretrained text-conditioned diffusion models. By leveraging strategically designed two-stage training process, VLV distills semanticrich representations from frozen diffusion decoders into compact, continuous embeddings, and subsequently translates these embeddings into detailed natural language captions using an open-source pretrained Large Language Model. Our experiments demonstrate that VLV achieves state-of-the-art captioning performance comparable to leading models such as GPT-4o and Gemini 2.0 Flash, while dramatically reducing training costs and data requirements. Notably, our method primarily utilizes single-modal images, significantly enhancing accessibility by maintaining training expenditures under $1,000 USD. Additionally, we explored the emergent properties of our framework, highlighting its strong spatial consistency and advanced compositional generalization capabilities. We believe the efficiency, effectiveness, and interpretability of VLV pave promising pathways for future research in scalable and cost-effective multimodal learning. Limitations & Future Work. As our training data is filtered with aesthetic score, VLV performs poorly on OCR (Optical Character Recognition) tasks due to lack of data with texts or watermarks; augmenting with document and street-view images or adding lightweight OCR branch should somehow improve the performance on OCR scenarios. Another thing is that we are using the Stable Diffusion 2.1 as the generation decoder in our pipeline which is outdated also limits the transferable knowledge, limiting our upper bound. so re-distilling from recent state-of-the-art diffusion models such as SD 3.5 or FLUX is an incoming work. Moreover, extending VLV to video modality is also worthy to explore since videos offer more dynamics and could emerge stronger spatial representations as well as physics-based learning for understanding comprehensive world semantics."
        },
        {
            "title": "References",
            "content": "[1] OpenAI GPT 4o Team. Gpt-4o system card, 2024. [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [3] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In ICCV, pages 89488957, 2019. [4] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. [5] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. NeurIPS, 35:2371623736, 2022. [6] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. [7] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [8] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [9] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sagnak Tasırlar. Introducing our multimodal models, 2023. [10] Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, et al. Perception encoder: The best visual embeddings are not at the output of the network. arXiv preprint arXiv:2504.13181, 2025. [11] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In ECCV, pages 370387. Springer, 2024. [12] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On scaling up multilingual vision and language model. arXiv preprint arXiv:2305.18565, 2023. [13] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: jointly-scaled multilingual languageimage model. arXiv preprint arXiv:2209.06794, 2022. [14] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. [15] Xinlei Chen, Zhuang Liu, Saining Xie, and Kaiming He. Deconstructing denoising diffusion models for self-supervised learning. arXiv preprint arXiv:2401.14404, 2024. [16] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, pages 2418524198, 2024. [17] Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms. arXiv preprint arXiv:2409.11402, 2024. [18] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. 10 [19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. [20] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis, 2024. URL https://arxiv. org/abs/2403.03206, 2, 2024. [21] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, pages 1287312883, 2021. [22] Google. Introducing gemini 2.0: our new ai model for the agentic era, 2024. Accessed: Dec 2024. [23] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913, 2017. [24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, pages 1600016009, 2022. [25] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 30, 2017. [26] Geoffrey Hinton and Richard Zemel. Autoencoders, minimum description length and helmholtz free energy. NeurIPS, 6, 1993. [27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:6840 6851, 2020. [28] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [29] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al. Language is not all you need: Aligning perception with language models. NeurIPS, 36:7209672109, 2023. [30] Drew Hudson, Daniel Zoran, Mateusz Malinowski, Andrew Lampinen, Andrew Jaegle, James McClelland, Loic Matthey, Felix Hill, and Alexander Lerchner. Soda: Bottleneck diffusion models for representation learning. In CVPR, pages 2311523127, 2024. [31] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. [32] Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. [33] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. NeurIPS, 36:7168371702, 2023. [34] Alexander Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is secretly zero-shot classifier. In ICCV, pages 22062217, 2023. [35] Daiqing Li, Huan Ling, Amlan Kar, David Acuna, Seung Wook Kim, Karsten Kreis, Antonio Torralba, and Sanja Fidler. Dreamteacher: Pretraining image backbones with deep generative models. In ICCV, pages 1669816708, 2023. [36] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, pages 1973019742. PMLR, 2023. [37] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, pages 1288812900. PMLR, 2022. [38] Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, Yuyin Zhou, and Cihang Xie. What if we recaption billions of web images with llama-3? arXiv preprint arXiv:2406.08478, 2024. 11 [39] Yadong Li, Haoze Sun, Mingan Lin, Tianpeng Li, Guosheng Dong, Tao Zhang, Bowen Ding, Wei Song, Zhenglin Cheng, Yuqi Huo, Song Chen, Xu Li, Da Pan, Shusen Zhang, Xin Wu, Zheng Liang, Jun Liu, Tao Zhang, Keer Lu, Yaqi Zhao, Yanjun Shen, Fan Yang, Kaicheng Yu, Tao Lin, Jianhua Xu, Zenan Zhou, and Weipeng Chen. Baichuan-omni technical report. arXiv preprint arXiv:2410.08565, 2024. [40] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [41] Hao Liu, Wilson Yan, and Pieter Abbeel. Language quantized autoencoders: Towards unsupervised text-image alignment. NeurIPS, 36:43824395, 2023. [42] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36:3489234916, 2023. [43] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. [44] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [45] Chris Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. [46] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In CVPR, pages 31953204, 2019. [47] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. [48] Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 26412649, 2015. [49] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion autoencoders: Toward meaningful and decodable representation. In CVPR, pages 1061910629, 2022. [50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763. PmLR, 2021. [51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. [52] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [53] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, pages 88218831. Pmlr, 2021. [54] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. NeurIPS, 32, 2019. [55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 1068410695, 2022. [56] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 35:2527825294, 2022. [57] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: dataset for image captioning with reading comprehension. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 742758. Springer, 2020. [58] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 12 [59] Changyao Tian, Chenxin Tao, Jifeng Dai, Hao Li, Ziheng Li, Lewei Lu, Xiaogang Wang, Hongsheng Li, Gao Huang, and Xizhou Zhu. Addp: Learning general representations for image recognition and generation with alternating denoising diffusion process. arXiv preprint arXiv:2306.05423, 2023. [60] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. NeurIPS, 30, 2017. [61] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and Léon Bottou. Stacked denoising autoencoders: Learning useful representations in deep network with local denoising criterion. JMLR, 11(12), 2010. [62] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022. [63] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, et al. Cogvlm: Visual expert for pretrained language models. NeurIPS, 37:121475 121499, 2024. [64] Wenxuan Wang, Quan Sun, Fan Zhang, Yepeng Tang, Jing Liu, and Xinlong Wang. Diffusion feedback helps clip see better. arXiv preprint arXiv:2407.20171, 2024. [65] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [66] XuDong Wang, Xingyi Zhou, Alireza Fathi, Trevor Darrell, and Cordelia Schmid. Visual lexicon: Rich image features in language space. arXiv preprint arXiv:2412.06774, 2024. [67] Yingheng Wang, Yair Schiff, Aaron Gokaslan, Weishen Pan, Fei Wang, Christopher De Sa, and Volodymyr Kuleshov. Infodiffusion: Representation learning using information maximizing diffusion models. In ICML, pages 3633636354. PMLR, 2023. [68] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904, 2021. [69] Chen Wei, Chenxi Liu, Siyuan Qiao, Zhishuai Zhang, Alan Yuille, and Jiahui Yu. De-diffusion makes text strong cross-modal interface. In CVPR, pages 1349213503, 2024. [70] Chen Wei, Karttikeya Mangalam, Po-Yao Huang, Yanghao Li, Haoqi Fan, Hu Xu, Huiyu Wang, Cihang Xie, Alan Yuille, and Christoph Feichtenhofer. Diffusion models as masked autoencoders. In ICCV, pages 1628416294, 2023. [71] Weilai Xiang, Hongyu Yang, Di Huang, and Yunhong Wang. Denoising diffusion autoencoders are unified self-supervised learners. In ICCV, pages 1580215812, 2023. [72] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing unified representation for variety of vision tasks (2023). URL https://arxiv. org/abs/2311.06242, 2023. [73] Junfei Xiao, Zheng Xu, Alan Yuille, Shen Yan, and Boyu Wang. Palm2-vadapter: progressively aligned language model makes strong vision-language adapter. arXiv preprint arXiv:2402.10896, 2024. [74] Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei Wang, et al. mplug-2: modularized multi-modal foundation model across text, image and video. In ICML, pages 3872838748. PMLR, 2023. [75] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Openvocabulary panoptic segmentation with text-to-image diffusion models. In CVPR, pages 29552966, 2023. [76] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [77] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. 13 [78] Xingyi Yang and Xinchao Wang. Diffusion model as representation learner. In ICCV, pages 1893818949, 2023. [79] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022. [80] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. [81] Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, et al. Spae: Semantic pyramid autoencoder for multimodal generation with frozen llms. NeurIPS, 36:5269252704, 2023. [82] Alan Yuille and Daniel Kersten. Vision as bayesian inference: analysis by synthesis? Trends in cognitive sciences, 10(7):301308, 2006. [83] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, pages 1197511986, 2023. [84] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and In CVPR, pages Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. 55795588, 2021. [85] Liming Zhao, Kecheng Zheng, Yun Zheng, Deli Zhao, and Jingren Zhou. Rleg: Vision-language representation learning with diffusion-based embedding generation. In ICML, pages 4224742258. PMLR, 2023."
        },
        {
            "title": "A Data Processing",
            "content": "B VQA Analysis: Are Ground Truth\" labels really ground truth? Vision-Language-Vision Autoencoding Does Help Caption Evaluation with SoTA Multi-modal LLM (Gemini) Qualitative Results: Reconstruction from Captions Dataset & Model License F.1 Training Datasets . F.2 Testing Datasets . . . . F.3 Pre-trained Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 17 17 19 21 21 21"
        },
        {
            "title": "A Data Processing",
            "content": "This section details our data collection and filtering procedure. We annotate subset of the corpus with Gemini 2.0 Flash[58]. Figure 7 shows the whole pipeline how we obtain our data for Stage-1 and Stage-2. Figure 8 provide the token length distribution of our captions used for training Stage-2. Figure 7: Data Filtering Principles. We filter and collect 40M images from LAION-2B-en-aesthetic. We apply filtering based on the image resolution and aspect ratio to ensure the image quality and then prompt Gemini 2.0 Flash with image-conditioned templates to generate rich, descriptive captions. Figure 8: VLV Captions Length Statistics. Histogram of token counts for all captions (our 6M image-text paired data, used for stage-2 captioning). Most captions fall in the 170280 token band, with mean µ = 226.82 (red dashed) and median = 226 (green dashed). 16 Figure 9: OK-VQA Example. Both our caption and Gemini caption do not mention the states information. But our caption not only capture the oranges but also the number of oranges. Our answers contain the right ones highlighting in LimeGreen. VQA Analysis: Are Ground Truth\" labels really ground truth? Following Wei et al. [69], we evaluate on OK-VQA with DeepSeek-V3 [40] under the strict exactmatch metric. Our raw score is 45.31% (2,295 / 5,064), trailing the Gemini 2.0 Flash caption baseline of 46.34% by 1.03% (52 questions). Among the 526 cases where Gemini is marked correct and our model wrong, we compute answeranswer cosine similarity in CLIP space and relabel pairs with similarity 0.8, recovering 94 additional correct answers. The adjusted accuracy is therefore 47.17% This shows that the apparent deficit stems mainly from lexical mismatches rather than missing visual content. We show an example (one of the 94 cases) in Figure 9. Vision-Language-Vision Autoencoding Does Help We do an ablation study of the stage-1 Vision-Language-Vision autoencoding. To be specific, we only train our Stage-2 with pretrained our VLV Encoder, and assess the generated captions with T2I tasks. Table 7 reports the resulting FID scores () on MS-COCO 2014. Skipping Stage-1 (first three rows) yields very poor fidelity, even larger decoders cannot compensate, whereas with Stage-1 training (grey row) drops FID to 12.2, confirming its critical role. Decoder Qwen-2.5 0.5B Qwen-2.5 1.5B Qwen-2.5 3B Qwen-2.5 3B + Stage-1 1.0 57.29 49.15 45.63 12.22 2.0 64.02 56.66 51.03 7. 3.0 66.92 59.15 53.28 9.58 4.0 68.18 60.14 54.40 10.84 Table 7: Effect of Stage-1 training on FID (). The gray row demonstrates that our Vision-LanguageVision auto-encoding pipeline makes the encoder distill the knowledge from the text-conditioned diffusion model effectively and efficiently. This leads the effectively 17 Figure 10: Captioner Arena Example. All captions show the correct objects without hallucinations. Both our caption and GPT-4o caption show the spatial relationship while Qwen-2.5 VL does not. Caption Evaluation with SoTA Multi-modal LLM (Gemini) We assess caption quality by querying Gemini 2.0 Flash with tailored rubric. Figure 10 displays an evaluation case with, together with Gemini 2.0 Flashs rationale, confirming that our captions are on par with those from GPT-4o. Your role is to serve as an impartial and objective evaluator of an image caption generated by Large Multimodal Model (LMM). Based on the single image input, assess the caption on three main criteria: 1. Coverage of image elements how well the caption mentions the salient objects, their attributes, actions, and contextual details. 2. Absence of hallucinations the caption must not invent objects, attributes, counts, spatial relations, or other details not present or implied by the image. 3. Object spatial layout consistency whether spatial relationships (left/right, above/below, front/behind, center, background/foreground) are described accurately. Any incorrect or invented spatial relation is hallucination. Omitting an obvious spatial relation reduces coverage. Stating relation that is ambiguous or uncertain in the image is also hallucination. Evaluation protocol Start with brief explanation of your evaluation process. Then assign one rating using the scale below. Output only the rating numberno extra text, symbols, or commentary."
        },
        {
            "title": "6 Comprehensive coverage, correct spatial layout, no hallucinations\n5 Very informative, correct spatial layout, no hallucinations, minor omissions\n4 Moderate coverage, correct spatial layout, no hallucinations, several omissions\n3 Limited coverage, minimal spatial detail, no hallucinations\n2\n1 Limited coverage and at least one hallucination (object or spatial)\n0 Not informative and/or multiple hallucinations",
            "content": "Informative but contains at least one hallucination (object or spatial) 18 Qualitative Results: Reconstruction from Captions We show some qualitative results of our captions of MS-COCO 2014 validation split in Figure 11, Figure 12, Figure 13, Figure 14. In each figure, we show the original images and the reconstructed images generated by Text-to-Image generation models with our VLV captions. We show the generation results using Midjourney, FLUX.1-dev and Imagen 3. The reconstructed images preserve comprehensive semantics, demonstrating our VLV can do high-quality, comprehensive captioning. Figure 11: VLV can capture spatial layout. The caption shows bears layout (in the center of the frame) in this image as well as the bears posture (head turned towards the right side), showing VLVs ability of capturing spatial layout. Figure 12: VLV can capture text (OCR). VLV has reasonable OCR ability, even though the training set is heavily filtered (we filter the data by watermark probability less than 0.5). There is still potential to improve OCR performance with further training on more OCR-oriented data. 19 Figure 13: VLV can capture complex objects. Caption enumerates almost every object and correctly describe their spatial relationships, highlighting VLVs comprehensive scene understanding. Figure 14: VLV can capture human posture. Captions show details of human as well as his posture, demonstrating VLVs fine-grained posture awareness. 20 Dataset & Model License F.1 Training Datasets LAION-5B License: Creative Common CC-BY 4.0 https://laion.ai/blog/laion-5b/ F.2 Testing Datasets MS-COCO License: Creative Common CC-BY 4.0 https://cocodataset.org/#termsofuse VQAv License: CC-BY 4.0 https://visualqa.org/terms.html Dataset website: https://visualqa.org/index.htmll OK-VQA License:N/A. Dataset website: https://okvqa.allenai.org/ F.3 Pre-trained Models stable-diffusion-3.5-medium (used for image generation). https://huggingface.co/stabilityai/stable-diffusion-3.5-medium/blob/main/ LICENSE.md Qwen-2.5 (used in stage-2 for LLM decoder). https://huggingface.co/Qwen/Qwen2.5-72B-Instruct/blob/main/LICENSE Qwen-2.5-VL (used in image captioning). https://github.com/QwenLM/Qwen2.5-VL/blob/main/LICENSE Florence-2-Large (used in image captioning). https://huggingface.co/microsoft/Florence-2-large/blob/main/LICENSE LLaVA-v1.5 (used in image captioning). https://huggingface.co/liuhaotian/llava-v1.5-7b"
        }
    ],
    "affiliations": [
        "Johns Hopkins University",
        "Rice University",
        "Tsinghua University"
    ]
}