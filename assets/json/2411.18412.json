{
    "paper_title": "Adaptive Blind All-in-One Image Restoration",
    "authors": [
        "David Serrano-Lozano",
        "Luis Herranz",
        "Shaolin Su",
        "Javier Vazquez-Corral"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Blind all-in-one image restoration models aim to recover a high-quality image from an input degraded with unknown distortions. However, these models require all the possible degradation types to be defined during the training stage while showing limited generalization to unseen degradations, which limits their practical application in complex cases. In this paper, we propose a simple but effective adaptive blind all-in-one restoration (ABAIR) model, which can address multiple degradations, generalizes well to unseen degradations, and efficiently incorporate new degradations by training a small fraction of parameters. First, we train our baseline model on a large dataset of natural images with multiple synthetic degradations, augmented with a segmentation head to estimate per-pixel degradation types, resulting in a powerful backbone able to generalize to a wide range of degradations. Second, we adapt our baseline model to varying image restoration tasks using independent low-rank adapters. Third, we learn to adaptively combine adapters to versatile images via a flexible and lightweight degradation estimator. Our model is both powerful in handling specific distortions and flexible in adapting to complex tasks, it not only outperforms the state-of-the-art by a large margin on five- and three-task IR setups, but also shows improved generalization to unseen degradations and also composite distortions."
        },
        {
            "title": "Start",
            "content": "Adaptive Blind All-in-One Image Restoration David Serrano-Lozano1,2 1Computer Vision Center Luis Herranz3 Shaolin Su1 2Universitat Aut`onoma de Barcelona Javier Vazquez-Corral1,2 3Universidad Autonoma de Madrid {dserrano, shaolin, jvazquez}@cvc.uab.cat luis.herranz@uam.es 4 2 0 2 7 2 ] . [ 1 2 1 4 8 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Blind all-in-one image restoration models aim to recover high-quality image from an input degraded with unknown distortions. However, these models require all the possible degradation types to be defined during the training stage while showing limited generalization to unseen degradations, which limits their practical application in complex cases. In this paper, we propose simple but effective adaptive blind all-in-one restoration (ABAIR) model, which can address multiple degradations, generalizes well to unseen degradations, and efficiently incorporate new degradations by training small fraction of parameters. First, we train our baseline model on large dataset of natural images with multiple synthetic degradations, augmented with segmentation head to estimate per-pixel degradation types, resulting in powerful backbone able to generalize to wide range of degradations. Second, we adapt our baseline model to varying image restoration tasks using independent low-rank adapters. Third, we learn to adaptively combine adapters to versatile images via flexible and lightweight degradation estimator. Our model is both powerful in handling specific distortions and flexible in adapting to complex tasks, it not only outperforms the state-of-the-art by large margin on fiveand three-task IR setups, but also shows improved generalization to unseen degradations and also composite distortions. https://aba-ir.github.io/ 1. Introduction Image restoration (IR) is fundamental task in computer vision, essential for enhancing visual quality and optimizing the performance of downstream tasks [37, 45]. IR aims to reconstruct high-fidelity images by systematically removing various degradations present in low-quality inputs. These degradations often emerge through complex interplay of external environmental conditions and camera limitations during the image acquisition process such as adverse weather conditions [28, 48, 49], noise [13], blur [41] and low-light environments [4, 68]. Figure 1. Our model significantly outperforms state-of-the-art allin-one image restoration (IR) methods, Restormer [63], PromptIR [40], and DiffUIR [66], across five known IR tasks, three unseen tasks, and three mixed degradation scenarios. The plot is normalized along each axis, with the lowest value positioned on the second circle and the highest value on the outermost circle. The inherently ill-posed nature of IR presents significant challenge for conventional approaches limiting their effectiveness [12, 15, 18, 21, 22, 34, 38, 47]. Recent advances in deep learning techniques have led to remarkable progress in IR [11, 27, 40, 41, 63], achieving substantial improvements in reconstruction accuracy. However, these frameworks demonstrate superior performance only in modeling dedicated degradations, as they are typically trained on datasets with individual and specific IR tasks. To address the limitations of using separate models for each degradation type, recent image restoration (IR) models have adopted an all-in-one approach, i.e., they are designed to handle multiple degradation types within single model, alleviating the dependency on dedicated models for 1 various IR tasks. While these models have demonstrated success in integrating diverse IR tasks into unified framework, several challenges persist when applied to real-world IR problems. First, these models assume that the type of distortions are known in advance, allowing them to specifically target and remove them [63]. In practice, however, IR must operate in blind setting, with no prior knowledge of the degradation present in given image, making blind IR significantly more challenging. Second, real images frequently contain composite degradations e.g., moving vehicle in low-light scene, or compressed image captured in hazy weather. Although existing models are designed to handle diverse degradation types, they typically process only single type of distortion at time, limiting their effectiveness in practical cases. Third, current allin-one methods require access to all degradation types during training, which restricts their generalizability to unseen degradations. Adapting these models to new, unobserved degradation types while retaining the all-in-one functionality typically necessitates retraining the entire model with the expanded set of degradations process that is both computationally expensive and time-consuming. Contribution: This paper presents an Adaptive Blind All-in-One Image Restoration (ABAIR) method, designed to bridge the gap between IR techniques and their application in practical complex scenarios. ABAIR effectively addresses multiple and composite degradations with flexible structure that can be easily updated to include new degradations. Our approach is simple yet effective scheme that combines three main components. First, we propose large-scale pre-training with synthetic degradations to obtain robust weight initialization. We propose modified CutMix [61] to generate images under multiple distortions with segmentation head for degradation recognition. Second, we learn disentangled restoration modules by training independent adapters specifically LoRA [19] to bridge the gap between synthetic and real-world data. Third, to derive blind all-in-one IR method, we learn lightweight image-degradation estimator to select the most suitable combination of adapters based on the input image. With the pre-training on synthetic degradations and the blending operation, our model achieves the ability to handle composite distortions within single image. Moreover, the adapter-based design enables flexible model structure that can be easily updated to address new distortions. We can simply train new adapter module for the added degradation and retrain the lightweight estimator without losing prior knowledge on other IR tasks. Our approach offers blind all-in-one model for versatile IR, achieving superior performance on all-in-one restoration benchmarks including threeand fivedegradation settings, generalizing to three unseen degradations and handling mixed degradations, as shown in Fig. 1. 2. Related Work Single Degradation Image Restoration Most prior work on IR has typically considered removing single type of degradation in an image to recover its clean counterpart. Typical single degradation IR tasks include denoising [13], deblurring [41], deraining [7], dehazing [59], low-light enhancement [4, 68], etc. Though achieving promising progress on individual tasks, these methods are only capable of dealing with specific type of distortion, thus limiting their generalizability to wider range of IR scenarios. all-in-one"
        },
        {
            "title": "IR approaches",
            "content": "Image Restoration Recently, and All-in-One multihave degradation gained significant attention. Multi-degradation methods [5, 39, 60, 62, 63] proposed unified model architectures that are effective on multiple IR tasks. However, these methods are trained in way that one set of parameters can handle only one specific type of degradation. Therefore, one still needs to allocate different weights (though with the same architecture) for different degradations. Furthermore, to restore versatile image, the type of degradation has to be known so that the corresponding parameters can be loaded, this non-blind scheme further hinders the efficiency and effectiveness for real-world applications. Meanwhile, blind all-in-one IR approaches utilize specialized modules to blindly distinguish degradation types. For instance, AirNet [27] uses contrastive-based encoder to extract latent degradation representation from the input images. X-Restormer [8] adds spatial self-attention module to the transformer block to enhance spatial mapping capabilities. IDR [64] learns degradation-specific priors and incorporates them into restoration, while DiffUIR [66] equips diffusion model with condition guidance. Despite the ability to blindly process images, these methods are still incapable of dealing with composite distortions or unseen distortions, due to their focus on dedicated degradations and standard IR benchmarks. Prompt learning techniques, which capture task-specific context, have shown promise in guiding adaptation for vision tasks [20]. Leveraging this concept, recent methods encode degradation-specific information and guide the restoration model to enhance low-fidelity images with more than one possible distortion. PromptIR [40], for instance, integrates dedicated prompt block to capture degradationspecific features from input images, while DA-CLIP [30], MPerceiver [2], ProRes [31] and Painter [50] leverage large pre-trained models as prompt generators. The latter approaches, however, are often constrained by high memory demands of large models. Additionally, existing all-in-one approaches require all degradation types to be predefined during training, limiting the addition of new distortion to already trained models. In contrast, aiming at versatile image restoration, our Figure 2. General schema of our proposed method. Our method is divided into three phases. In Phase we pre-train our baseline model with synthetic degradations of high-fidelity images. Each image contains different degradations in different regions, and segmentation head learns to predict them, while restoration loss aims at restoring the image. In this way, the model is able to distinguish and generalize well to multiple degradations. In Phase II, we learn degradation-specific adaptors using standard image restoration datasets. In Phase III, we learn lightweight degradation estimator to adaptively blend the adapters based on the degradation profile of the input image. This 3-phase methodology makes our method flexible to deal with images containing multiple distortions and easy to update for new ones as it only requires training an adapter for the new distortion and retraining the degradation estimator. model is developed to blindly process images with various kinds of distortion, either in single or composite forms. With the mixed design of lightweight degradation adapters, our model is also flexible and extensible to unseen distortions, thus shows great generalizability towards the challenging blind all-in-one IR problem. Parameter Efficient Fine-Tuning Fine-tuning models by updating all parameters is computationally inefficient, especially for large-scale models. Parameter Efficient FineTuning (PEFT) addresses this by reducing the number of trainable parameters and memory usage while achieving comparable performance to full fine-tuning [54]. LowRank Adaptation (LoRA) [19] introduced reparametrization strategy for fine-tuned weights, where the weights of specific layer = + are represented as linear combination of the pre-trained weights and the updated term obtained through low-rank decomposition, = BA. Here, Rdk, Rrk with rank min(d, k). By optimizing only the low-rank matrices and B, LoRA reduces the number of parameters required for adapting that layer (vs tuning all parameters), minimizing memory and computational demands. Building upon LoRA, alternative decomposition methods have been proposed, such as Vector-based Random Matrix Adaptation [24] and Conv-LoRA[67]. PEFT techniques have proven valuable not only for domain adaptation in large models but also for applications like task arithmetic and continual learning, mitigating the problem of catastrophic forgettingwhere models lose previously acquired knowledge upon learning new tasks[10]. In IR, Park et al. [39] applied low-rank decomposition within single-task IR frameworks to enhance model performance. In contrast, we propose simple but novel approach to merge different adaptors LoRAs to obtain flexible blind all-in-one model capable of learning new tasks without forgetting previously learned IR tasks. This is achieved through disentangled scheme that requires minimal retraining for each new task. 3. Method Figure 2 presents an overview of our proposed approach for enhancing low-quality images by systematically addressing the key limitations in current IR methods. Our approach consists of three phases, roughly targeting the following challenges: (i) robust generalization to varying types of degradations (Phase I), (ii) effective adaptation to specific degradations (Phase II), (iii) flexible all-in-one mechanism able to handle unknown and mixed degradations (Phase III). 3 (a) Rain (b) Haze (c) Noise (d) Blur (e) Low-light Figure 3. Examples of our synthetic degradation generation for five traditional distortions. Note that we freeze the parameters of baseline after Phase and adapters after and II, thus preserving maximal knowledge learned from previous tasks and avoid catastrophic forgetting. This design also allows us to adapt to new distortions with minimal additional training as only new adapter and the light-weighted estimator will be trained. Phase I: Pre-training with synthetic degradations. Recent advances demonstrate that large-scale pre-trained models can significantly improve performance across various tasks [1]. Moreover, pre-training with synthetic data has also shown effectiveness when the domain gap is properly addressed [55]. We hypothesize that applying large-scale pre-training to IR can yield notable performance boost compared to training solely on traditional IR datasets. Thus, as depicted in Fig. 2, Phase involves training baseline IR model with large-scale data with synthetic degradations to provide robust weight initialization for handling complex degradations. To obtain large-scale data for pre-training, we define degradation pipeline that generates low-quality counterpart for each high-quality input image by introducing different synthetic distortions. We focus on five common distortions: noise, blur, rain, haze, and low-light conditions. Each synthetic distortion closely approximates its real-world counterpart. For instance, low-light conditions are simulated by compressing the image histogram, reducing the dynamic range of pixel intensities. In contrast, haze is introduced by adding an achromatic layer based on the depth map estimated from DepthAnythingv2 [55]. Figure 3 illustrates examples of each synthetic degradation. We use the Google Landmarks dataset (GLD) [53], largescale collection of five million images of diverse landmarks worldwide. To ensure that the inputs to our pipeline are high quality, we filter out the images with resolution smaller than 400 pixels on any side and exclude images with NIMA score [46] below the 4.90 threshold, retaining only high-quality natural images. To construct our baseline model, we build on recent advancements in all-in-one IR methods. Particularly, we use the Restormer architecture [63] with spatial attention mechanisms [8] and incorporate modified version of PromptIR [40] prompt blocks. This combination enables the baseline model to effectively capture the inherent information of various degradations, establishing strong foundation for generalized IR tasks. While training on single distortions enables the model to address each type individually, real-world IR often involves mixed degradations. However, directly applying multiple distortions to an image often leads to severe quality degradation, making it difficult for the model to learn to reconstruct the original image. To tackle this, we implement two strategies: (i) degradation CutMix [61] technique, and (ii) segmentation head with cross-entropy loss. Our first strategy inspired by Yang et al. [55] for depth estimation applies two different degradations divided into different regions. This setup helps the model distinguish and manage multiple degradation types within the same image. As our second strategy, we incorporate segmentation head into the baseline model, which outputs per-pixel map of the distortions. By comparing this map with the ground truth, we guide the model to recognize and differentiate multiple types of degradation within single image. We report additional details about our baseline and the generation of the synthetic distortions in the Supplementary Material. Phase II: Single-task adaptation. By generating distortions synthetically, we achieve robust weight initialization for our baseline model. However, domain gap persists between our pre-training data (due to both the different dataset and the synthetic distions), standard IR datasets, and realworld conditions. For instance, accurately simulating haze is particularly challenging due to its dependence on light scattering by particles, which varies with depth and atmospheric conditions. To bridge this gap, we propose adapting the baseline model with adapters learned for each specific task in its standard dataset, representing Phase II of our approach, as outlined in Fig. 2. In our approach, each linear and convolutional layer is augmented with set of LoRA, one per type of degradation, parametrized by {An} and {Bn} where is the index of the degradation. Following LoRA, these low-rank matrices adjust the initial frozen weights through linear combinations, as described earlier. For simplicity, we omit the subindex indicating the specific layer of the model, but each layer has separate set of adapters. Due to the proposed pre-training approach and the subsequent specialization via adaptation, our approach achieves superior performance across diverse tasks and generalizes effectively. 4 Table 1. 5-degradations setup. Quantitative results on five IR datasets comparing the state-of-the-art all-in-one methods and our approach. Ours (Oracle) is an upper bound for our approach: it computes the best reachable value in case our estimator always chooses the correct degradation. PSNR/SSIM AirNet [27] Uformer [51] IDR [64] X-Restormer [8] DA-CLIP [30] DiffUIR [66] Restormer [63] PromptIR [40] Ours OH Ours SW Deraining Dehazing Denoising Deblurring Low-Light Rain100L SOTS (Out) BSD68 σ=25 GoPro LoLv1 Average Param. 32.98 35.48 35.63 35.42 35.49 35.52 35.56 35.40 37.73 37.79 .951 .967 .965 .968 .970 .969 .970 . .978 .979 21.04 27.20 25.24 27.58 28.10 28.17 27.94 28.26 33.46 33.48 .884 .958 .943 .959 .962 .964 .962 .965 .983 .984 30.91 30.59 31.60 30.92 30.42 30.92 30.74 30. 31.38 31.38 .882 .869 .887 .880 .859 .879 .875 .872 .898 .898 24.35 26.41 27.87 27.54 26.50 26.99 26.84 26.55 29.00 29.00 .781 .809 .846 .835 .807 .821 .818 . .878 .878 18.18 21.40 21.34 20.88 21.94 20.92 21.74 21.80 24.20 24.19 .735 .808 .826 .817 .817 .789 .815 .815 .865 .865 25.49 28.21 28.34 28.47 28.49 28.50 28.56 28. 31.15 31.17 .847 .882 .893 .891 .880 .880 .888 .885 .920 .921 Ours (Oracle) 39.09 0. 33.54 .984 31.40 0.901 29.10 . 24.45 .866 31.39 .922 9M 52M 15M 26M 174M 36M 26M 36M 59M 59M 59M Phase III: Multi-task integration. While LoRA adapters serve as plug-and-play solution for pre-trained models, there is still the need to select the most suitable adapter based on the input image, when the type of degradation is unknown. To address this limitation and derive blind all-in-one IR method, we propose to use lightweight degradation estimator (nx; θ) to estimate the probability of each degradation given the input image x, parametrized by θ. This estimator is trained on the combination of all (seen) datasets, thus learning to identify the type of degradation present in the image. As shown in Fig. 2, the probability of the estimator is used as weights to linearly combining the task-specific adapters with the baseline parameters. Specifically, given an input image x, the weight update for specific layer with baseline weights and adapter weights {An} and {Bn} is computed as: (x) = + (cid:88) n=1 (nx; θ) BnAn, (1) where is the number of seen degradations. We propose two variants of the estimator: (i) onehot and (ii) soft-weights. The one-hot variant simply selects the adapter corresponding to the degradation with the maximum probability (equivalent to 1 with one-hot probabilities), while the soft-weight variant uses computes the weighted average as in Eq. (1). Our approach allows the model to linearly combine the degradation-specific knowledge to address both specific and multiple distortions. Its modular and parameter efficient architecture also allows to add additional degradations with small training effort: train additional adapters for the new task, and update the estimator. 4. Experiments We evaluate our method on two setups for all-in-one IR: fiveand three-degradation setups. Additionally, we test on datasets excluded from training, novel IR degradation types, and mixed degradation scenarios. The accuracy of the methods is assessed using two well-established metrics: PSNR and SSIM. In all cases we report the mean value for all the test images, and we highlight best and second-best values for each metric. Our approach is compared against recent all-in-one IR methods. Notably, some current methods are either evaluated under different setups or lack available code and models, which complicates direct comparisons. Therefore, we train three state-of-the-art methods, Restormer [63], PromptIR [40], and X-Restormer [8], on the five-degradation IR setup to provide fair comparison. We used the authors provided code. For our approach, we evaluate the two variations based on the variants of the estimator, namely one-hot (OH) and soft weights (SW). Implementation details: Our training is conducted in three phases using Adam [23] with weight decay [29], an initial learning rate of 2104, cosine learning rate scheduler, and warmup start of one epoch. In phase I, we apply weighted combination of cross-entropy loss LCE for per-pixel degradation maps and reconstruction loss LR, where LR consists of L1 loss and SSIM loss. Both the cross-entropy and SSIM components are weighted with 0.5 In phase II, we use only the reconstruction loss, where, as before, SSIM loss is weighted with 0.5. In phase III, we apply cross-entropy loss between the regressors output and the input images degradation type. 5 Table 2. 3-degradations setup. Quantitative results on three IR datasets comparing the state-of-the-art all-in-one methods and our approach. PSNR/SSIM Deraining Dehazing Denoising Average Rain100L SOTS (Out) BSD68 σ=15 BSD68 σ=25 BSD68 σ= DL [14] MPRNet [62] AirNet [27] Restormer [63] PromptIR [40] Ours OH Ours SW 32.62 33.57 34.90 35.56 36.37 38.58 38.52 .931 .954 .967 .969 .972 .981 . 26.92 25.28 27.94 29.92 30.58 33.71 33.62 .931 .954 .962 .970 .974 .985 .984 33.05 33.54 33.92 33.86 33.98 33.95 33. .914 .927 .933 .933 .933 .934 .933 30.41 30.89 31.26 31.20 31.31 31.29 31.24 .861 .880 .888 .888 .888 .889 . 26.90 27.56 28.00 27.90 28.06 28.04 28.01 .740 .779 .797 .794 .799 .798 .796 29.98 30.17 31.20 31.69 32.06 33.11 33. .875 .899 .909 .911 .913 .918 .916 Table 3. Quantitative results on additional test datasets with the learned degradations. PSNR/SSIM Deraining Deblurring Low-Light Rain100H HIDE Lolv2-Real IDR [64] 11.32 .397 16.83 .621 17.61 .697 X-Restormer [8] 14.08 .437 25.40 .801 25.42 .876 14.78 .487 23.98 .739 26.12 .861 DiffUIR [66] 14.50 .464 24.42 .781 27.12 .877 Restormer [63] 14.28 .444 24.49 .762 27.70 .870 PromptIR [40]"
        },
        {
            "title": "Ours OH\nOurs SW",
            "content": "21.69 .692 27.04 .850 28.09 .907 19.37 .594 27.05 .850 28.09 .906 Table 4. Quantitative results for unseen IR tasks. Note that the models have not been trained for these degradations. Ours* shows results for the lightweight re-training scenario. New adapters are trained for the new tasks and the estimator is retrained with 8 tasks (5-IR case + 3 new ones; only 8M training parameters). PSNR/SSIM 4-to-8 bits JPEG Q20 Desnowing Live Live1 City-Snow IDR [64] 24.02 .738 26.51 .913 18.00 .649 X-Restormer [8] 24.73 .745 26.86 .922 18.51 .681 24.68 .743 26.88 .921 18.39 .671 DiffUIR [66] 24.64 .743 26.90 .929 18.14 .655 Restormer [63] 24.70 .740 26.60 .920 18.49 .673 PromptIR [40]"
        },
        {
            "title": "Ours OH\nOurs SW",
            "content": "Ours OH Ours SW 25.25 .742 29.20 .931 18.71 .684 25.32 .743 29.35 .926 18.67 .683 29.14 .826 30.82 .943 24.19 .797 29.03 .810 30.71 .939 24.02 .779 5-Degradation blind IR: For the blind five-task setup, we follow the protocol by Zhang et al. [64]. Specifically, we use Rain200L[56] for deraining, RESIDE [25] for dehazing, BSD400 [33] and WED [32] for denoising with σ = 25, GoPro [35] for deblurring, and LOL [52] for low-light enhancement. For evaluation, we employ Rain100L [56], SOTS-Outdoor [25], BSD68 [33], GoPro [35], and LOL [52]. Results are reported in Tab. 1. Our approach outperforms all state-of-the-art methods across all tasks, with the exception of PSNR for denoising. Notably, our method achieves substantial improvement over the state of the art, yielding 2.91 dB average PSNR gain In particular, our method surpasses across the five tasks. IDR [64] by 2.16 dB on image deraining and provides 2.40 dB improvement over PromptIR [40] on low-light image enhancement. 3-Degradation Blind IR: Following Li et al. [27], we further evaluate our approach on 3-task blind IR setup, comparing our approach with specialized all-in-one methods for deraining, dehazing, and denoising. Compared to the fivetask setup, we omit deblurring and low-light enhancement and instead introduce two additional noise levels: σ = 15 and σ = 50. Two LoRA adapters for these noise levels are trained, and the estimator is retrained for the new setting. Results for this 3-task setup are presented in Tab. 2. Our approach outperforms all the other state-of-the-art methods on average and stays consistent in all the degradation types. Additional test sets: To assess the generalization capabilities of our approach, we evaluate the model on three datasets not used during training. Specifically, we use the Rain100H [56] dataset for heavy deraining, HIDE [43] for human-centric deblurring, and LoLv2-Real [58] for low-light image enhancement. The results of the three best methods on the 5-degradation setup and our approach are presented in Tab. 3. Our method outperforms all other approaches across all datasets and metrics. Notably, we achieve significant improvement of 7.19 dB over Restormer [63] on Rain100H [56]. This performance boost can be attributed to the pre-training on synthetic data, which allows our model to learn from broader set of natural images and degradation scenarios compared to the other methods small training set, like Rain200L [56]. Additionally, we achieve over 1 dB PSNR improvement on both HIDE and LoLv2-Real, further demonstrating the effectiveness and robustness of our approach. 6 ] 5 3 [ o ] 2 5 [ 1 L ] 6 5 [ 0 0 1 R Input Restormer [63] PromptIR [40] Ours-OH Ground truth Figure 4. Qualitative results for single degradation removal, including deblurring on the GoPro [35] dataset, denoising on the LoLv1 [52] dataset, and deraining on the Rain100H [56] dataset. ] 2 4 [ J ] 2 4 [ b 8 - -"
        },
        {
            "title": "Input",
            "content": "PromptIR [40] Ours-SW Ours-SW retrained"
        },
        {
            "title": "Ground truth",
            "content": "Figure 5. Qualitative results for unseen IR tasks, including JPEG artifact removal and 4-to-8 bit reconstruction. PromptIR [40] and Ours are not trained for this task, while Ours retrained has specified LoRA in an 8-degradation setup. Unseen IR tasks: Although our approach was initially trained on five types of degradations, we further evaluate its generalization to three additional IR tasks not encountered during training. Specifically, we assess our method on JPEG artifact removal and bit-depth reconstruction using the Live1 dataset [42], and on desnowing using CityScapesSnow-Medium [65]. Results for the top four previous stateof-the-art methods from the five-degradation setup, alongside our approach, are presented in Tab. 4. Remarkably, our method outperforms all other approaches across all tasks and metrics. To show the adaptability of our approach, we additionally train separate adapter for each new task, along with new estimator that blends all eight task-specific adapters, thereby expanding the models versatility. The last two rows of Tab. 4 report results for the newly integrated tasks shown with . Notably, with the addition of these task-specific adapters, our model achieves notable boost in performance, while maintaining an average PSNR of 30.08 dB across the original five IR tasks, outperforming the other state-of-the-art methods trained on the initial setup. Mixed degradations: The limited availability of datasets with mixed degradations presents challenge for training IR models under these complex conditions. To evaluate our approach, we evaluate it on three mixed-degradation scenarios: blur combined with noise σ=25 using the GoPro dataset[35], blur with JPEG artifacts using the REDS dataset [36], and haze combined with snow using the SRRS dataset [6]. Notably, some degradations, such as JPEG artifacts and snow, are unseen during training, further increasing the difficulty of these tests. As shown in Tab. 5, our model consistently outperforms other methods across all scenarios and metrics. Handling blur with noise is particularly challenging due to their contrasting nature: blur affects low frequencies, while noise affects high frequencies. Even though, our model achieved around 2dB improvement while others remained at the level of 22dB. Qualitative results: In Fig. 4, we present examples featuring three types of single degradations from the five-task setup on which the model was trained. Our approach effec7 o & B n & H"
        },
        {
            "title": "Input",
            "content": "Restormer [63] PromptIR [40] Ours-SW"
        },
        {
            "title": "Ground truth",
            "content": "Figure 6. Qualitative results for two examples of mixed degradations. The first row depicts an image with blur and noise from the GoPRO dataset [35], while the second row shows an image with haze and snow from the SRRS dataset [6]. Columns display the input image, results from Restormer [63], PromptIR [40], our method, and the ground truth, respectively. Table 5. Quantitative results on datasets with mixed degradations. PSNR/SSIM Blur&Noise Blur&JPEG Haze&Snow"
        },
        {
            "title": "SRRS",
            "content": "IDR [64] 21.98 .683 23.02 .681 20.51 .789 X-Restormer [8] 22.67 .669 23.98 .710 20.76 .805 22.71 .670 24.00 .711 20.86 .802 DiffUIR [66] 22.35 .662 23.24 .698 20.76 .800 Restormer [63] PromptIR [40] 22.89 .671 23.92 .705 20.94 .803 X-Restormer [8] 22.67 .669 23.98 .710 20.76 ."
        },
        {
            "title": "Ours OH\nOurs SW",
            "content": "24.30 .743 24.81 .717 21.48 .834 25.14 .750 24.97 .719 22.09 .839 Table 6. Ablation studies on types of pre-training for Phase I, and the rank of LoRA [19] for Phase II and III. Pre-training PSNR SSIM IR datasets 28.50 .892 GLD+synth. 30.63 .913 + CutMix 31.09 .920 + Aux. segm. 31.17 ."
        },
        {
            "title": "Rank PSNR SSIM Params",
            "content": "4 8 16 3.6M 31.17 .921 31.14 .920 7.2M 30.97 .916 14.3M tively enhances the license plate in the first row, reduces noise in low-light conditions in the second row, and removes heavy rain streaks in the third. In Fig. 5, we show unseen IR tasks including JPEG artifact removal and 4to-8-bit reconstruction using the Live1 dataset [42]. Our method successfully removes JPEG artifacts from the parrots plumage and artifacts caused by bit-depth reduction from the cloud and blue sky. By training new adapter for these tasks (ours-SW retrained), we achieve superior results with minimal additional training time. Finally, in Fig. 6, we demonstrate our methods performance under mixed degradations. The first row shows that our method achieves the best reconstruction of the text in the image, while the second row illustrates our models effectiveness in removing both haze and snowflakes. 8 Ablation studies: In Tab. 6, we perform an ablation study to assess the contribution to the final performance of the proposed pre-training pipeline and the LoRAs rank. For pre-training in Phase I, we use the same setup as the fivetask configuration, i.e. training the LoRAs and the estimator using task-specific IR datasets and the obtained pretrained weights. The first row IR datasets reflects pretraining on standard IR benchmarks, yielding performance comparable to state-of-the-art methods see Tab. 1. In contrast, simply pre-training on GLD with synthetic degradations improves performance by 2.15 dB PSNR, demonstrating the importance of Phase to provide the model with strong generalization capabilities. Further incorporating both the CutMix-like [61] strategy and the auxiliar segmentation task provides an additional performance boost. We also compare LoRA [19] with different ranks for Phase II and III. The model achieves the best performance and efficiency when the rank is set to 4, which we adopted as our default setting. For more ablation results, please refer to our Supplementary Material. 5. Conclusion In this work, we introduced an adaptive blind all-in-one IR model aiming towards practical IR. We design both dedicated adapters for strongly handling specific distortions but also flexible architecture for dealing with practical IR challenges.We first developed pre-training pipeline featuring multiple synthetic degradations over large dataset that boosts the generalization of the model. Second, we derived compact per-task adapters that robustly adapt to specific degradations. Third, we developed lightweight degradation estimator that identifies varying degradations to blend the respective adapters, which is also capable of efficiently incorporate new degradations by training small fraction of parameters. Our model largely outperforms the state-of-theart on fiveand three-task IR setups, and shows improved generalization to unseen datasets and IR tasks."
        },
        {
            "title": "Acknowledgements",
            "content": "funded DSL, LH, and JVC were supported by Grant PID2021128178OB-I00 by MCIN/AEI/10.13039/ 501100011033 and by ERDF way of making Europe, by the Departament de Recerca Universitats from Generalitat de Catalunya with reference 2021SGR01499, and by the Generalitat de Catalunya CERCA Program. DSL also acknowledges the FPI grant from Spanish Ministry of Science and Innovation (PRE2022-101525). LH was also supported by the Ramon Cajal grant RYC2019-027020-I. SS was supported by the HORIZON MSCA Postdoctoral Fellowships funded by the European Union (project number 101152858) 6. Supplementary Material We provide additional material to supplement our main submission. Specifically, we cover: A. Details on our baseline architecture. B. Description of the generation process for synthetic degradations, including rain, haze, noise, blur, and lowlight conditions. Additional dataset examples. C. Additional ablation experiments on LoRAs rank and other decomposition methods. D. Details on our estimator architecture and additional analysis of the estimators performance. E. Additional qualitative results on known datasets, unseen tasks, and images with mixed degradations. A. Baseline Architecture Details We present an Adaptive Blind All-in-One Image Restoration (ABAIR) method, designed to bridge the gap between IR techniques and their application in practical complex scenarios. Our approach follows three-phase scheme. The first phase involves pre-training an IR baseline using natural images with synthetic degradations. In this section, we describe this baseline architecture. Figure 7 shows the details of our baseline model. Our baseline model adopts the Restormer [63] architecture, transformer-based UNet-like framework. Given degraded image, the model first applies convolutional layer to extract low-level features of size C, where and are the spatial dimensions, and C=48 in all our experiments. These features are then processed through four-level encoder-decoder structure composed of transformer blocks, with pixel unshuffling and shuffling [44] used for downsampling and upsampling, respectively. Finally, convolutional layer generates the residual image, which is added to the degraded input image to produce the restored output. Each transformer block comprises channel selfattention module followed by spatial self-attention module. For channel self-attention, we adopt the implementation from Zamir et al. [63], while for spatial attention, we use the overlapping cross-attention mechanism proposed by Chen et al. [9]. This combination effectively addresses the limitations of Restormers U-shaped architecture, particularly its difficulty in reconstructing high-frequency details [8]. Additionally, inspired by Potlapalli et al. [40], we integrate Prompt Block between the transformer blocks in the upsampling path. This block facilitates the architecture in identifying degradation-specific features in the input image by combining extracted features with set of model parameters. The design of our Prompt Block is depicted in Fig. 8. B. Synthetic Degradations Generation The first phase of our approach involves pre-training the baseline model using natural images with synthetically generated degradations, including rain, haze, noise, blur, and low-light conditions. Unlike standard IR datasets, our pipeline introduces degradations dynamically to clean input images, offering greater flexibility by enabling the model to be trained on the same image with diverse degradation types and varying severity levels. Moreover, each degradation type is parametrized by set of values that constrain the severity of the distortion. During training, these parameters are randomly selected for each forward pass. Specifically, we utilize 450K images from the Google Landmarks dataset [53] those with NIMA score higher than 4.90 and with short-edge resolution larger than 400, providing diverse set of real scenes. Figure 9 show examples for the different degradations under different parameter settings. In what follows, we detail the generation process for each type of synthetic degradation. Rain: Capturing paired degraded and clean images with rain is inherently challenging, as environmental conditions often vary when capturing the same scene under rainy and clear weather. Consequently, deraining datasets typically simulate rain by creating set of predefined rain stroke masks, which are then added to input images. However, these datasets often include limited number of masks [56], leading to potential overfitting on the specific patterns present in the dataset. In our case, aiming to derive more general set of masks, we consider five adjustable parameters: density, length, angle, drop size, and blending weight. The process for generating rainy image Irain begins by creating an empty mask of the same size as the input image Iinput, which will hold the rain-stroke patterns. The density parameter determines the number of raindrops, computed as fraction of the total image pixels. Random coordinates for the starting positions of the drops are generated, ensuring they stay within bounds to accommodate the specified drop size s. These coordinates are then used to populate with raindrops. Next, motion blur kernel is constructed to simulate the appearance of rain streaks, Figure 7. Overview of our baseline model. The input degraded image is processed through four-level U-shaped network with transformer blocks. In the upsampling path, prompt blocks are integrated to assist the model in capturing degradation-specific information. mask weight [0.75, 1]. Haze: As with rainy images, capturing paired degraded and clean images under hazy conditions is nearly impossible due to the variability of environmental factors. Inspired by prior works [3, 16, 26], we model haze degradation using the Kochsmieder model [17], which describes how the visibility of distant objects diminishes, vanishing into the horizon as function of their distance from the observer. We can formulate it as: Ihaze = Iinput + (1 ) A, (3) where Iinput is the input clean image, is the transmission map derived from the estimated depth map, is the airlight, and Ihaze is the resulting image with synthetic haze. We estimate the depth map of the input image using DepthAnythingv2 [55]. This depth map is normalized within the predefined minimum and maximum haze values to generate the transmission map . The depth map is then replicated for each color channel to compute Eq. (3). To ensure variability, all parameters are randomly sampled for each forward pass. The parameter ranges are as follows: minimum haze [0.2, 0.4], and maximum haze [0.7, 0.9], airlight is unique for the three color channels with values ranging [140, 200]. Blur: Blur in images can arise from various sources, such as motion blur, out-of-focus blur, and lens blur, among others. In this work, we focus on simulating the motion blur due to its relevance in practical applications. The blur effect is introduced by convolving the input image Iinput with parametrized kernel K, designed to model motion blur along specific direction. The kernel is defined by its size Figure 8. Overview of our Prompt Block. The input features, derived from the output of transformer block, are modulated through weighted element-wise multiplication with set of model parameters. These modulated features are then further refined using an additional transformer block, producing the enhanced output features of the Prompt Block. based on the specified rain length and angle θ. The mask is convolved with K, creating streaks that mimic natural rain patterns. Finally, the rain streaks are normalized and expanded into three RGB channels to match the input image dimensions. These streaks are blended with the original image Iinput using weight w, resulting in the final rain-augmented image Irain. All the input parameters are randomly selected for each forward pass. Mathematically omitting normalizations for clarity, Irain = wIinput + (1 w)(K ). (2) The value ranges are rain density d:[0.005, 0.02], rain length l: [25, 35], rain angle θ: [70, 110], raindrop size s: [1, 3], and 10 d=0.005, l=25, θ=70, s=1, and w=0.75 min=0.2, max=0.7 A=140 σ=15 d=9 and Θ=0 c=0.25 and σ=0.5 d=0.01, l=30, θ=90, s=2, and w=1 min=0.3, max=0.8 A=160 σ=25 d=25 and Θ=45 c=0.5 and σ= d=0.01, l=30, θ=90, s=3, and w=1 min=0.3, max=0.8 A=180 σ=25 d=25 and Θ=90 c=0.5 and σ=1.5 d=0.02, l=35, θ=110, s=1, and w= min=0.4, max=0.9 A=200 σ=50 d=35 and Θ=135 c=0.25 and σ=1."
        },
        {
            "title": "Blur",
            "content": "Low-Light Figure 9. Additional examples showing our synthetic degradation generation for rain, haze, noise, blur, and low-light conditions. On top of each image, we report the specific parameters used to produce the corresponding degraded output. in pixels (d) and its angle (Θ), to simulate the directional streaks characteristics of motion blur. Mathematically, Iblur = KΘ,d Iinput. (4) The kernel size (d) is selected from odd values within the range [9, 35], while the angle (Θ) specifies the orientation of the blur in degrees, ranging from [0, 360]. details, amplifying noise, and leading to grainy or speckled artifacts. Furthermore, the reduced dynamic range limits the ability to capture the range of intensities, resulting in color inconsistencies, and loss of detail in shadows and highlights. To simulate these conditions, we compress the input image histogram by factor and add noise with meager σ value. This process can be expressed mathematically as: Noise: We use the standard Additive White Gaussian Noise (AWGN) approach for noise. In short, we add to the original image second image that follows Gaussian distribution with mean 0 and variance σ. Mathematically, Ilol = Iinput + (0, σ). (6) The compression factor ranges in the interval [0.25, 0.5] and the σ takes values in the interval [0.5, 1.5]. Inoise = Iinput + (0, σ). (5) C. Additional Phase II Ablation Studies Low-Light: When an image is captured under low-light conditions, cameras amplify the sensors signal to capture The second phase of our approach involves training set of adapters LoRA [19] in the main manuscript for each type of degradation. In this section, we extend our analysis 11 Table 7. Ablation study on different low-rank adapters and their rank. Results are the mean for all images. LoRA outperforms both VeRA and Conv-LORA. Lower ranks perform better. PSNR/SSIM Deraining Dehazing Denoising Deblurring Low-Light Method Rank Rain100L SOTS (Out) BSD68 σ=25 GoPro LoLv1 Average Adapter Param. LoRA [19] VeRA [24] Conv-LoRA [67] 4 8 16 4 8 16 4 8 37.79 37.75 37.61 37.02 37.09 37.04 37.00 36. .979 .978 .972 .971 .971 .970 .969 .968 33.48 33.4 33.21 32.67 32.69 32.62 32.55 32. .984 .982 .977 .972 .972 .970 .971 .968 31.38 31.39 31.31 31.32 31.32 31.33 31.32 31. .898 .898 .896 .896 .896 .896 .896 .895 29.00 29.02 28.77 28.61 28.64 28.62 28.54 28. .878 .878 .875 .872 .873 .872 .870 .868 24.19 24.18 23.96 23.78 23.79 23.84 23.70 23. .865 .865 .862 .580 .580 .581 .576 .575 31.17 31.15 30.97 30.68 30.71 30.69 30.62 30. .921 .920 .916 .858 .858 .858 .856 .855 3.6M 7.2M 14.3M 460K 468K 476K 3.9M 7.5M we perform last ablation comparing our approach to just adding or averaging the five degradation task-specific adapters. Tab. 8 shows the results of this ablation. When all adapters are added, the values become excessively large for the baseline model to effectively handle them. On the other hand, averaging the adapters results in suboptimal restored images, as the model fails to specialize in any particular degradation type. E. Additional Qualitative Results Figure 11 presents additional qualitative results for known degradations, including examples from both seen and unseen datasets. Figure 12 highlights results for unknown degradations, with the corresponding mean absolute error (MAE) map displayed below each image to emphasize the differences. Finally, Figure 13 shows qualitative results for mixed degradation scenarios. to include other low-rank decompositions and their rank. Specifically, we evaluate VeRA [24] and Conv-LoRA [67]. The results for the five-degradation setup across the three adapter types are presented in Tab. 7, which also includes the number of trainable parameters for each adapter in the last column. For fair comparison, we utilize the same baseline model weights and estimator, while training only the task-specific adapters. Among the methods, LoRA [19] achieves the best overall performance. However, VeRA [24] provides competitive results with significantly fewer parameters, as it estimates only two vectors per layer instead of two low-rank matrices. On the other hand, Conv-LoRA [67] performs worse despite having more parameters, owing to its Mixture-of-Experts approach with convolution layers in the decomposed space. Regarding the decomposition rank, we find that ranks of 4 and 8 consistently outperform rank of 16 in terms of both accuracy and parameter efficiency. D. Additional Phase III Analysis In the third phase of our method, we train an estimator to select or blend the most suitable set of adapters based on the input image. The estimator architecture comprises four blocks of Conv2D layers, each followed by batch normalization, ReLU activation, and max pooling, culminating with global average pooling layer and linear projection. The total parameter count for the estimator is 538K. We present confusion matrix in Fig. 10, showing the One-Hot predictions for known degradations both seen and unseen datasets. The estimator predicts known degradations with probabilities exceeding 90% in most cases. However, for one of the unseen datasets, the Rain100H dataset [56], the prediction probability falls below 90%, likely due to the severity of the rain streaks and their resemblance to haze-related degradations. Notably, even when the estimator selects an incorrect adapter, no significant artifacts or undesired effects are introduced, owing to the robustness of the large-scale pre-training. Finally, to highlight the importance of our estimator, 12 Table 8. Ablation study on different methods for blending the five degradations task-specific LoRA [19] adapters. PSNR/SSIM Deraining Dehazing Denoising Deblurring Low-Light Average Method Rain100L SOTS (Out) BSD68 σ=25 GoPro LoLv1 Sum Average Ours OH Ours SW 19.50 30.54 37.73 37.79 .755 .939 .978 .979 18.80 20.87 33.46 33. .745 .855 .983 .984 18.10 28.98 31.38 31.38 .730 .785 .898 . 18.25 21.34 29.00 29.00 .728 .792 .878 .878 17.70 15.49 24.20 24. .715 .673 .865 .865 18.67 23.84 31.15 31.17 .736 .809 .920 . Figure 10. Confusion matrix for our estimator in test images. First five columns: known degradations with the datasets considered at training. Last three columns: unknown datasets for rain, blur, and low light. Specifically, from left-to-right columns, the datasets are: Rain100L [56], BSD68 [33], SOTS (outdoors) [26], GoPro [35], LoLv1 [52], Rain100H [56], HIDE [43], and LoLv2-Real [57]."
        },
        {
            "title": "References",
            "content": "[1] Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi. Exploring the limits of large scale pretraining. In ICLR, 2022. 4 [2] Yuang Ai, Huaibo Huang, Xiaoqiang Zhou, Jiexiang Wang, and Ran He. Multimodal prompt perceiver: Empower adaptiveness generalizability and fidelity for all-in-one image restoration. In CVPR, 2024. 2 [3] Cosmin Ancuti, Codruta O. Ancuti, and Christophe De Vleeschouwer. D-hazy: dataset to evaluate quantitatively dehazing algorithms. In ICIP, pages 22262230, 2016. 10 [4] Yuanhao Cai, Hao Bian, Jing Lin, Haoqian Wang, Radu Timofte, and Yulun Zhang. Retinexformer: One-stage retinexbased transformer for low-light image enhancement. In ICCV, 2023. 1, 2 [5] Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun. Simple baselines for image restoration. In ECCV, 2022. 2 [6] Wei-Ting Chen, Hao-Yu Fang, Jian-Jiun Ding, Cheng-Che Tsai, and Sy-Yen Kuo. Jstasr: Joint size and transparencyaware snow removal algorithm based on modified partial In ECCV, pages convolution and veiling effect removal. 754770. Springer, 2020. 7, 8, 15 [7] Xiang Chen, Hao Li, Mingqiang Li, and Jinshan Pan. Learning sparse transformer network for effective image deraining. In CVPR, pages 58965905, 2023. 2 [8] Xiangyu Chen, Zheyuan Li, Yuandong Pu, Yihao Liu, Jiantao Zhou, Yu Qiao, and Chao Dong. comparative study of image restoration networks for general backbone network design. arXiv preprint arXiv:2310.11881, 2023. 2, 4, 5, 6, 8, 9 [9] Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong. Activating more pixels in image superresolution transformer. In CVPR, pages 2236722377, 2023. 9 [10] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleˇs Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. continual learning survey: Defying forIEEE TPAMI, 44(7):3366 getting in classification tasks. 3385, 2022. 3 [11] Mauricio Delbracio and Peyman Milanfar. Inversion by di13 ] 6 5 [ R ] 6 2 [ H ] 5 3 [ i ] 5 3 [ B ] 2 5 [ i - ] 6 5 [ 0 0 1 R ] 8 5 [ R - 2 L"
        },
        {
            "title": "Input",
            "content": "Restormer [63] PromptIR [40] Ours-OH"
        },
        {
            "title": "Ground truth",
            "content": "Figure 11. Qualitative results for known degradation removal, both seen datasets Rows 1 to 5 and unseen datasets Rows 6 to 8. 14 ] 2 4 [ J ] 5 6 [ S Input/Ground truth Restormer [63] PromptIR [40] Ours-SW Ours-SW retrained Figure 12. Qualitative results on JPEG artifact removal on Live1 dataset [42] and desnowing on CityScapes-Snow-Medium [65]. We show the mean absolute error map below each image. ] 6 3 [ J & B ] 6 [ S & H"
        },
        {
            "title": "Input",
            "content": "Restormer [63] PromptIR [40] Ours-OH"
        },
        {
            "title": "Ground truth",
            "content": "Figure 13. Qualitative results on mixed degradation scenarios. Specifically, blur and JPEG on the REDS [36] dataset and haze and snow in the SRRS [6] dataset. rect iteration: An alternative to denoising diffusion for image restoration. TMLR, 2023. 1 [12] Weisheng Dong, Lei Zhang, Guangming Shi, and Xiaolin Wu. Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization. IEEE TIP, 20(7):18381857, 2011. 1 [13] Weisheng Dong, Peiyao Wang, Wotao Yin, Guangming Shi, Fangfang Wu, and Xiaotong Lu. Denoising prior driven deep neural network for image restoration. IEEE TPAMI, 41(10): 23052318, 2018. 1, 2 [14] Qingnan Fan, Dongdong Chen, Lu Yuan, Gang Hua, Nenghai Yu, and Baoquan Chen. general decoupled learning framework for parameterized image operators. IEEE TPAMI, 43(1):3347, 2019. 6 [15] Sina Farsiu, Dirk Robinson, Michael Elad, and Peyman Milanfar. Fast and robust multiframe super resolution. IEEE TIP, 13(10):13271344, 2004. [16] Adrian Galdran, Javier Vazquez-Corral, David Pardo, and Marcelo Bertalmıo. Enhanced variational image dehazing. SIAM Journal on Imaging Sciences, 8(3):15191546, 2015. 10 [17] Koschmieder Harald. Theorie der horizontalen sichtweite: Kontrast und sichtweite. Keim and Nemnich, Munich, 12, 1924. 10 [18] Kaiming He, Jian Sun, and Xiaoou Tang. Single image haze IEEE TPAMI, 33(12): removal using dark channel prior. 23412353, 2010. 1 [19] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2021. 2, 3, 8, 11, 12, 13 [20] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In ECCV, 2022. 2 [21] Kwang In Kim and Younghee Kwon. Single-image superresolution using sparse regression and natural image prior. IEEE TPAMI, 32(6):11271133, 2010. 1 [22] Stefan Kindermann, Stanley Osher, and Peter Jones. Deblurring and denoising of images by nonlocal functionals. Multiscale Modeling & Simulation, 4(4):10911115, 2005. 1 [23] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. In ICLR, 2015. [24] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki Asano. VeRA: Vector-based random matrix adaptation. In ICLR, 2024. 3, 12 [25] Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun Zeng, and Zhangyang Wang. Benchmarking singleIEEE TIP, 28(1):492505, image dehazing and beyond. 2018. 6 [26] Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun Zeng, and Zhangyang Wang. Benchmarking singleIEEE TIP, 28(1):492505, image dehazing and beyond. 2019. 10, 13, 14 [27] Boyun Li, Xiao Liu, Peng Hu, Zhongqin Wu, Jiancheng Lv, and Xi Peng. All-in-one image restoration for unknown corruption. In CVPR, 2022. 1, 2, 5, 6 [28] Ruoteng Li, Robby T. Tan, and Loong-Fah Cheong. All in one bad weather removal using architectural search. In CVPR, 2020. 1 [29] Loshchilov. Decoupled weight decay regularization. In ICLR, 2019. 5 [30] Ziwei Luo, Fredrik Gustafsson, Zheng Zhao, Jens Sjolund, and Thomas Schon. Controlling vision-language arXiv preprint models for universal image restoration. arXiv:2310.01018, 3(8), 2023. 2, 5 [31] Jiaqi Ma, Tianheng Cheng, Guoli Wang, Qian Zhang, Xinggang Wang, and Lefei Zhang. Prores: Exploring degradation-aware visual prompt for universal image restoration. arXiv preprint arXiv:2306.13653, 2023. 2 [32] Kede Ma, Zhengfang Duanmu, Qingbo Wu, Zhou Wang, Hongwei Yong, Hongliang Li, and Lei Zhang. Waterloo exploration database: New challenges for image quality assessment models. IEEE TIP, 26(2):10041016, 2016. 6 [33] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In ICCV, 2001. 6, 13 [34] Tomer Michaeli and Michal Irani. Nonparametric blind super-resolution. In ICCV, 2013. 1 [35] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural network for dynamic scene deblurring. In CVPR, 2017. 6, 7, 8, 13, 14 [36] Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik Moon, Sanghyun Son, Radu Timofte, and Kyoung Mu Lee. Ntire 2019 challenge on video deblurring and superresolution: Dataset and study. In CVPR Workshops, pages 00, 2019. 7, 15 [37] Xuejing Niu, Bo Yan, Weimin Tan, and Junyi Wang. Effective image restoration for semantic segmentation. Neurocomputing, 374:100108, 2020. 1 [38] Joao Oliveira, Jose Bioucas-Dias, and Mario AT Figueiredo. Adaptive total variation image deblurring: majorizationminimization approach. Signal processing, 89 (9):16831693, 2009. [39] Dongwon Park, Hayeon Kim, and Se Young Chun. Contribution-based low-rank adaptation with pre-training model for real image restoration. In ECCV, 2024. 2, 3 [40] Vaishnav Potlapalli, Syed Waqas Zamir, Salman Khan, and Fahad Shahbaz Khan. Promptir: Prompting for all-inone image restoration. In NeurIPS, 2024. 1, 2, 4, 5, 6, 7, 8, 9, 14, 15 [41] Mengwei Ren, Mauricio Delbracio, Hossein Talebi, Guido Gerig, and Peyman Milanfar. Multiscale structure guided diffusion for image deblurring. In ICCV, 2023. 1, 2 [42] Hamid Sheikh, Muhammad Sabir, and Alan Bovik. statistical evaluation of recent full reference image quality assessment algorithms. IEEE TIP, 15(11):34403451, 2006. 7, 8, 15 [43] Ziyi Shen, Wenguan Wang, Xiankai Lu, Jianbing Shen, Haibin Ling, Tingfa Xu, and Ling Shao. Human-aware motion deblurring. In ICCV, pages 55725581, 2019. 6, 13 [44] Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In CVPR, pages 18741883, 2016. 9 [45] Shangquan Sun, Wenqi Ren, Tao Wang, and Xiaochun Cao. Rethinking image restoration for object detection. In NeurIPS, 2022. 1 [46] Hossein Talebi and Peyman Milanfar. Nima: Neural image assessment. IEEE TIP, 27(8):39984011, 2018. 4 [47] Radu Timofte, Vincent De Smet, and Luc Van Gool. Anchored neighborhood regression for fast example-based super-resolution. In ICCV, 2013. 1 [48] Jeya Maria Jose Valanarasu, Rajeev Yasarla, and Vishal Patel. Transweather: Transformer-based restoration of imIn CVPR, ages degraded by adverse weather conditions. 2022. 1 [49] Cong Wang, Jinshan Pan, Wanyu Lin, Jiangxin Dong, Wei Wang, and Xiao-Ming Wu. Selfpromer: Self-prompt dehazing transformers with depth-consistency. In AAAI, 2024. 1 [50] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: generalist painter for in-context visual learning. In CVPR, 2023. 2 [51] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: general u-shaped transformer for image restoration. In CVPR, pages 1768317693, 2022. [52] Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. In BMVC, 2018. 6, 7, 13, 14 16 [68] Dewei Zhou, Zongxin Yang, and Yi Yang. Pyramid diffusion models for low-light image enhancement. In IJCAI, 2024. 1, 2 [53] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim. Google landmarks dataset v2-a large-scale benchmark for In CVPR, pages instance-level recognition and retrieval. 25752584, 2020. 4, 9 [54] Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang. Parameter-efficient fine-tuning methods for pretrained language models: critical review and assessment. arXiv preprint arXiv:2312.12148, 2023. 3 [55] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv preprint arXiv:2406.09414, 2024. 4, 10 [56] Wenhan Yang, Robby Tan, Jiashi Feng, Jiaying Liu, Zongming Guo, and Shuicheng Yan. Deep joint rain detection and removal from single image. In CVPR, pages 13571366, 2017. 6, 7, 9, 12, 13, [57] Wenhan Yang, Shiqi Wang, Yuming Fang, Yue Wang, and Jiaying Liu. From fidelity to perceptual quality: semisupervised approach for low-light image enhancement. In CVPR, 2020. 13 [58] Wenhan Yang, Wenjing Wang, Haofeng Huang, Shiqi Wang, and Jiaying Liu. Sparse gradient regularized deep retinex network for robust low-light image enhancement. IEEE TIP, 30:20722086, 2021. 6, 14 [59] Yang Yang, Chaoyue Wang, Risheng Liu, Lin Zhang, Xiaojie Guo, and Dacheng Tao. Self-augmented unpaired image In CVPR, dehazing via density and depth decomposition. 2022. 2 [60] Zongsheng Yue, Jianyi Wang, and Chen Change Loy. Efficient diffusion model for image restoration by residual shifting. In NeurIPS, 2024. 2 [61] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019. 2, 4, 8 [62] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. Multi-stage progressive image restoration. In CVPR, 2021. 2, [63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In CVPR, 2022. 1, 2, 4, 5, 6, 7, 8, 9, 14, 15 [64] Jinghao Zhang, Jie Huang, Mingde Yao, Zizheng Yang, Hu Yu, Man Zhou, and Feng Zhao. Ingredient-oriented multidegradation learning for image restoration. In CVPR, 2023. 2, 5, 6, 8 [65] Kaihao Zhang, Rongqing Li, Yanjiang Yu, Wenhan Luo, and Changsheng Li. Deep dense multi-scale network for snow IEEE TIP, removal using semantic and geometric priors. 2021. 7, 15 [66] Dian Zheng, Xiao-Ming Wu, Shuzhou Yang, Jian Zhang, Jian-Fang Hu, and Wei-Shi Zheng. Selective hourglass mapping for universal image restoration based on diffusion model. In CVPR, 2024. 1, 2, 5, 6, 8 [67] Zihan Zhong, Zhiqiang Tang, Tong He, Haoyang Fang, and Chun Yuan. Convolution meets loRA: Parameter efficient finetuning for segment anything model. In ICLR, 2024. 3,"
        }
    ],
    "affiliations": [
        "Computer Vision Center",
        "Universidad Autonoma de Madrid",
        "Universitat Aut`onoma de Barcelona"
    ]
}