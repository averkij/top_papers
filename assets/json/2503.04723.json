{
    "paper_title": "Shifting Long-Context LLMs Research from Input to Output",
    "authors": [
        "Yuhao Wu",
        "Yushi Bai",
        "Zhiqing Hu",
        "Shangqing Tu",
        "Ming Shan Hee",
        "Juanzi Li",
        "Roy Ka-Wei Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in long-context Large Language Models (LLMs) have primarily concentrated on processing extended input contexts, resulting in significant strides in long-context comprehension. However, the equally critical aspect of generating long-form outputs has received comparatively less attention. This paper advocates for a paradigm shift in NLP research toward addressing the challenges of long-output generation. Tasks such as novel writing, long-term planning, and complex reasoning require models to understand extensive contexts and produce coherent, contextually rich, and logically consistent extended text. These demands highlight a critical gap in current LLM capabilities. We underscore the importance of this under-explored domain and call for focused efforts to develop foundational LLMs tailored for generating high-quality, long-form outputs, which hold immense potential for real-world applications."
        },
        {
            "title": "Start",
            "content": "Shifting Long-Context LLMs Research from Input to Output Yuhao Wu 1 Yushi Bai 2 Zhiqing Hu 1 Shangqing Tu 2 Ming Shan Hee 1 Juanzi Li 2 Roy Ka-Wei Lee 1 5 2 0 2 7 ] . [ 2 3 2 7 4 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in long-context Large Language Models (LLMs) have primarily concentrated on processing extended input contexts, resulting in significant strides in long-context comprehension. However, the equally critical aspect of generating long-form outputs has received comparatively less attention. This paper advocates for paradigm shift in NLP research towards addressing the challenges of long-output generation. Tasks such as novel writing, long-term planning, and complex reasoning require models to understand extensive contexts and produce coherent, contextually rich, and logically consistent extended text. These demands highlight critical gap in current LLM capabilities. We underscore the importance of this under-explored domain and call for focused efforts to develop foundational LLMs tailored for generating high-quality, longform outputs, which hold immense potential for real-world applications. 1. Introduction Advancements in Long-Context LLMs (Inputs). Research on long-context Large Language Models (LLMs) has progressed rapidly in recent years, particularly in expanding context window lengths. These have grown from an initial 8K tokens to as much as 128K or even 1M tokens (OpenAI, 2024a; Anthropic, 2024; Reid et al., 2024b; GLM et al., 2024; Dubey et al., 2024). This dramatic expansion has enabled significant improvements in performance across long-context benchmarks (Kamradt, 2023; Bai et al., 2024b; Hsieh et al., 2024), unlocking new possibilities for real-world applications. Such advancements facilitate enhanced long-document and multi-document retrieval and more nuanced text comprehension tasks. For example, applications like summarizing lengthy reports, answering questions based on entire books, and analyzing multi-chapter 1Singapore University 2Tsinghua University. Lee <roy lee@sutd.edu.sg>. of Technology and Design Correspondence to: Roy Ka-Wei Figure 1. Difference between long-input and long-output LLMs. documents are now increasingly viable (Bai et al., 2024b; An et al., 2024a; Hsieh et al., 2024; Vodrahalli et al., 2024; Reid et al., 2024b). Consequently, the ability to process extended text has evolved from specialized feature into fundamental capability of state-of-the-art LLMs. The Case for Prioritizing Long Output. While the focus on long-context LLMs has primarily centered on processing extended input contexts, comparatively less attention has been given to long-output generation. This is surprising, given the growing number of applications requiring extended, coherent, and contextually rich text. Recent studies reveal significant performance limitations in existing models when tasked with generating long-form content beyond thousands of words (Wu et al., 2024; Bai et al., 2024d; Ye et al., 2025; Tu et al., 2025). This paper advocates for shift in research priorities for foundational LLMs, urging researchers to focus on this relatively unexplored area of long text generation. Several real-world applications, such as novel writing, long-term planning, and complex reasoning, require generating long texts exceeding 4,000 tokens (approximately 2,600 words) for successful task completion. Despite their importance, these applications have been significantly overlooked. These applications demand models capable of processing extensive contexts while producing high-quality, logically consistent outputs. We define these 1 models, optimized for long-output tasks, as long-output LLMs (see Figure 1). foundational long-output LLMs could be an immensely rewarding and exciting opportunity for many researchers and workflow. Why Long-Output LLMs have been Overlooked? The limited progress in long-output generation can be attributed to three primary challenges. ① Data limitations pose significant obstacle. Existing datasets for instruction-following tasks are predominantly composed of short input-output pairs, with only limited number of high-quality datasets featuring long output sequences (Bai et al., 2024a; Xiong et al., 2024; Chen et al., 2023). This scarcity of suitable data constrains both research and the practical application of long-output models. ② Task execution complexities add further difficulty. Generating long-form content, particularly for creative and structured tasks such as novel writing or article composition, requires models to maintain coherence and logical consistency across extended contexts. This level of complexity is significantly greater than what is required for shorter tasks (Wu et al., 2024; Yang et al., 2024; Tan et al., 2024). ③ Computational cost constraints present substantial hurdle. The computational demand for generating long texts increases linearly in certain architectures (Gu & Dao, 2023; Dao et al., 2022). Furthermore, proprietary models often impose token limits (e.g., 4,096 or 8,192 tokens) (OpenAI, n.d.; Anthropic, 2024; Reid et al., 2024a), restricting their capacity to generate extended outputs. These combined challenges highlight the need for more targeted research and innovation to advance long-output LLM capabilities. Why Care about the Long Output Domain? Addressing the challenges of long-output LLMs is crucial for meeting real-world needs across various domains. ① Fields, such as healthcare, law, education, and media depend on long-form content for tasks such as generating research papers, drafting legal documents, and preparing detailed reports (Zhao et al., 2024b; Chiang et al., 2024). Long-output LLMs can bridge the gap in these areas by automating the production of coherent, high-quality content, thereby streamlining workflows. ② Enhancing creativity and productivity is another key benefit. Long-output LLMs facilitate the co-authoring of extensive works such as novels and academic papers, reducing the time and effort required for content creation. This allows professionals to allocate more attention to higherlevel tasks like analysis and ideation (Atmakuru et al., 2024; Chiang et al., 2024). ③ Advancing complex reasoning is critical contribution of these models. By exploring larger output spaces and enhancing capabilities in summarization and inference, long-output LLMs enable deeper analysis and support intricate reasoning processes. Together, these advancements underscore the transformative potential of long-output LLMs in addressing real-world challenges. Paper Organization. This paper begins by defining the concept of long-output LLMs and highlighting their underrepresentation in current research (Section 2). It then reviews the current state of research on long generation (Section 3) and explores practical applications (Section 4). The paper then discusses challenges and opportunities for advancing long-output LLMs (Section 5), followed by alternative views on long-output LLMs and long-output generation (Section 6). Finally, the paper concludes by advocating for strategic research shift towards long-output generation (Section 7). 2. Long-Output LLMs In this section, we first explore the prevalence of tasks involving these models in real-world applications, highlighting the significant neglect of related research, as revealed by statistical data. Next, we define long-output LLMs and outline the requirements for model to qualify as such 2.1. High Demand, Low Research Focus Despite the growing need for models capable of generating long-form content in real-world applications, significant gaps remain in targeted research. Tasks such as scientific writing, technical documentation, and AI-driven dialogues require models capable of producing coherent, high-quality outputs over extended spans. However, research has largely concentrated on input processing (An et al., 2024a; Hsieh et al., 2024; Vodrahalli et al., 2024; Reid et al., 2024b), often neglecting the complexities of long-output generation (Wu et al., 2024; Bai et al., 2024d). To substantiate this claim, we provide statistical evidence that underscores the limitations of prevailing research trends. High Demand. To quantify the increasing demand for long-output generation in natural language processing (NLP), we analyzed 100 user requests from real-world scenarios, calculating the input-output length ratios using the Llama-3.3-70B model (Dubey et al., 2024) and employed Few-shot learning predictions (Brown et al., 2020) to estimate the output length required for real-user queries1. Specifically, we examined four output lengths ranges[2K, 4K), [4K, 8K), [8K, 16K), and [16K, +) wordsand compared them with the distribution of input lengths. The results reveal that demand for long-output generation exceeds equivalent-length inputs by more than 2-3 times in all cases except those with outputs greater than 1In Appendix A, describes the specific implementation of our In nutshell, designing the first generation of truly large, statistics. 2 Figure 2. Proportion of real-user demand: The aforementioned 2K (words) range refers to the interval [2K, 4K), and similarly for the other ranges. Solid color fill for input demand, slash fill for output. Figure 3. ML and NLP Conf Long-context Research Trends Statistics (sorted by conference date). Solid color fill for Input-paper, slash fill for Output-paper. 16K , with the ratio reaching nearly 15 times at [4K, 8K) level(Figure 2). These findings underscore the necessity of long-output models for generating extensive, coherent content in practical applications2. LongBench-V2 (Bai et al., 2024c) demonstrate that these capabilities go beyond mere context processing, requiring deep understanding of long contexts to answer questions accurately. Low Research Focus: While significant progress has been made in research on large input processing, particularly for long-context models, the area of long-output generation remains underexplored. This imbalance is evident in our analysis of papers from leading ML (ICML, ICLR, NeurIPS) and NLP (ACL, EMNLP, NAACL) conferences in 20243. Out of 104 papers addressing long-context tasks, only two specifically focused on long-output generationa stark 102:2 ratio (Figure 3). This imbalance focus is particularly concerning given the real-world demand for longoutput models, which often surpasses the demand for longinput models. 2.2. Defining Long-Output LLMs We propose that long-output LLMs must satisfy two key requirements to effectively address the challenges of longoutput generation: Input: Context Handling Capabilities. models ability to handle extensive context is critical for producing coherent and contextually relevant outputs over long spans. As the length of generated text increases, the model must reference and integrate previous output segments to ensure logical flow and consistency. Transitioning from long-context models to long-output LLMs requires enhanced capabilities in managing long-range dependencies and understanding complex, long-range contextual relationships. Benchmarks like 2The user query statistics analysis of WildChat (Zhao et al., 2024b) is provided in Appendix B. 3Appendix includes all paper titles. 3 Output: Length and Quality of Generated Text. While long-context models focus on processing extensive input (e.g., LLaMa 3.1 (Dubey et al., 2024) with 128K tokens or Gemini (Reid et al., 2024b) with 1M tokens), long-output LLMs prioritize the generation of long, coherent, and meaningful text. This involves producing outputs that span thousands or even millions of tokens while maintaining logical consistency, creativity, and relevance. Unlike traditional long-context models, which emphasize context size, longoutput LLMs excel at both the length and quality of the generated output, marking significant step forward in natural language generation. This shift requires models to maintain coherence and quality across significantly longer and more complex content, setting long-output LLMs apart as foundational advancement. In this paper, we establish performance baseline, starting with 4K tokens (approximately 2.6K words)4 as the effective length for long-content generation tasks. Takeaway: We define long-output LLMs as foundational LLMs specifically designed to excel at long-output tasks. While this definition allows for some flexibility, it broadly refers to large-scale language models capable of generating extended and coherent text. 4The selection of 4K as the starting point for long-output LLMs is based on two key reasons. First, it aligns with the starting point established in long-context benchmarks such as Ruler (Hsieh et al., 2024). Second, It aligns with the length of real-world requirements, as shown in Figure 2. Dataset Input Length Output Length 3.2. Benchmarks and Evaluation LongAlpaca-12k LongAlign-10k Suri LongWriter-6k 5,945 12,134 347 262 218 169 4,371 5,333 Table 1. Comparison of Average Input and Output Lengths (words) for Long-Context SFT Datasets. 3. Current State of Long-Output LLMs This section provides an overview of the current landscape of Long-Output LLMs, organized into three key areas: Data, Benchmarks, and Models. These dimensions collectively represent the core elements driving progress in the fieldData provides the foundation, Benchmarks set the evaluation standards, and Models showcase the cutting-edge advancements in long-output generation. 3.1. Data During the long-context continual-pretraining phase, the datasets used for Long-input LLMs and Long-output LLMs overlap. However, significant divergence occurs during supervised fine-tuning. Early research primarily focused on datasets featuring extended input sequences while limiting output sequences to shorter lengths (Xiong et al., 2024; Xu et al., 2024). For instance, datasets like LongAlpaca12k (Chen et al., 2023) and LongAlign-10k (Bai et al., 2024a) were designed for tasks such as summarization and question answering, where outputs length remained relatively constrained. More recently, datasets have been developed to support the generation of longer, more detailed outputs. Suri (Pham et al., 2024), for example, employs backtranslation to transform long-content data into comprehensive instructions as input. Similarly, LongWriter-6k (Bai et al., 2024d) uses an agent-based methodology to generate plan for user queries and then produces responses in segments, ensuring coherence in long-form outputs. Another work, SelfLengthen (Quan et al., 2024), uses iterative expansion to progressively extend responses through repeated elaboration, resulting in more detailed and lengthy outputs. This research represents significant evolution in the field, as more datasets are specifically constructed to facilitate long-output generationan essential capability for tasks that demand extensive reasoning or the production of extended text. Table 1 summarizes several key datasets average input and output lengths, underscoring the growing focus on generating longer outputs to fine-tune long-context LLMs. Long-output benchmarks for long-context LLMs are designed to assess both the length and quality of outputs exceeding 4K tokens (approximately 2.6K words). These benchmarks differ significantly from traditional benchmarks that primarily focus on processing long input contexts (16K tokens) (Kamradt, 2023; Hsieh et al., 2024; Bai et al., 2024c) or generating moderate-length outputs of around 1K tokens (Xu et al., 2020; Stelmakh et al., 2022; Xu et al., 2022; Tan et al., 2024). The unique challenge of long-output benchmarks lies in evaluating coherence, depth, and overall quality, where manual assessment becomes infeasible due to the extensive length of the generated text. To address this, three primary evaluation approaches have been developed. The first approach is rule-based evaluation, which focuses on verifying output length by counting tokens or words (Bai et al., 2024d; Quan et al., 2024; Liu et al., 2024a). While this method ensures compliance with predefined length requirements, it provides little to no insight into the quality, coherence, or depth of the generated content. Consequently, it is limited in its ability to offer holistic evaluation of long-output models. The second approach is LLM-based evaluation, which leverages the capabilities of LLMs to evaluate outputs in two distinct ways. One method involves using an LLM to assess the entire output holistically (Bai et al., 2024d; Quan et al., 2024; Ye et al., 2025), while the other relies on predefined checklist to determine whether the output meets specific criteria (Pham et al., 2024; Que et al., 2024). Although LLM-based evaluations provide more comprehensive insights compared to rule-based methods, they are computationally expensive and heavily reliant on the models ability to understand and evaluate long, complex texts. The third approach is segment-based evaluation, exemplified by frameworks like LongGenBench (Wu et al., 2024). This method divides the output into smaller, more manageable segments, allowing for detailed and interpretable assessments of each portion. However, this approach is best suited to tasks that involve structured outputs and is less applicable to unstructured or narrative-based long-output tasks. 3.3. Models While many recent models claim strong long-context capabilities, they often focus on handling long inputs rather than generating extended outputs. Benchmarks like LongGenBench (Wu et al., 2024) and LongWrite-Ruler (Bai et al., 2024d) reveal that current models struggle to maintain quality and coherence in outputs exceeding 4,000 tokens5. This limitation persists despite advancements in model architec5The details of the results from existing models are presented in Appendix E. 4 tures and training methods. Three models demonstrate potential in generating extended outputs: Bai et al. (2024d), Pham et al. (2024), and Quan et al. (2024). These models share common methodologies, including the use of specialized datasets (as outlined in Section 3.1) and fine-tuning techniques to optimize long-output performance. Additionally, approaches like Direct Preference Optimization (DPO) (Rafailov et al., 2024) are used to refine output length control. However, despite these innovations, current models still face significant challenges in generating coherent, high-quality outputs at longer lengths, as evidenced by suboptimal performance on benchmarks like LongGenBench (Wu et al., 2024) and LongBenchWrite (Bai et al., 2024d). Takeaway: We greatly appreciate the early work on longoutput LLMs. Their insightful discoveries have identified this direction as one with enormous potential and opportunities, making it promising area for further research. 4. Real-World Application 4.1. Creative Writing Task The advancement of long-output LLMs significantly broadens the scope of creative writing applications beyond traditional short-form tasks. This expansion facilitates the addressing of more complex and demanding writing scenarios, thereby demonstrating the versatile potential of long-output LLMs in real-world contexts. First, long-output LLMs excel in generating complex and standardized documents, such as academic papers and legal documents. Unlike traditional models, which are often restricted to producing brief emails or small sections of report, long-output LLMs are capable of composing comprehensive, coherent documents in their entirety. This automation not only enhances efficiency by handling repetitive writing tasks but also frees up professionals to focus on higher-level analytical and decision-making activities, ultimately improving productivity and output quality. Second, long-output LLMs are particularly adept at facilitating creative writing endeavors, including genres such as childrens literature and science fiction. Writers can utilize long-output LLMs to generate complete narratives, refine existing drafts, or maintain consistency and coherence across an entire work. This capability mitigates the common issue of disjointedness when generating content in smaller segments, allowing for seamless creative process. By enabling large-scale, coherent content creation, long-output LLMs support authors in realizing ambitious creative projects that would otherwise require significant time and effort. In addition, long-output LLMs contribute to complex planning and decision-making tasks, such as project design or 5 itinerary creation. By generating detailed and holistic plans that consider multiple factors, these models ensure comprehensive and integrated solutions. This capability is particularly valuable for scenarios where the output exceeds typical token limits (e.g., 4K tokens), providing more thorough and contextually informed outcomes. In summary, long-output LLMs extend the functionality of existing language models by enabling the generation of extensive, coherent, and high-quality content across creative and strategic domains. This transformative potential positions long-output LLMs as essential tools for automating and enhancing complex writing and planning tasks, driving innovation and efficiency across professional fields. 4.2. Long Chain-of-Thought Task One of the most impactful applications of long-output generation is its ability to support long chain-of-thought (CoT) tasks, which require extended sequences of reasoning to solve complex problems. These tasks serve as key benchmark for evaluating and advancing the capabilities of LLMs. The long CoT approach, as exemplified by the OpenAI o1 model (OpenAI, 2024d), has demonstrated remarkable success across range of domains. In mathematics, for instance, this method enables LLMs to tackle challenging problems, such as those encountered in Math Olympiads, while also excelling in tasks like complex code generation. These achievements underscore the transformative potential of long CoT techniques in domains that demand rigorous, systematic reasoning. critical enabler of long CoT success is the advancement of long-context scaling (MoonshotAI-KiMi, 2025). Complex reasoning tasks often result in extended outputs, requiring models to effectively manage both lengthy input and output sequences while maintaining coherence, relevance, and accuracy throughout. This necessitates innovations in scaling techniques to accommodate extended sequences without compromising performance. By refining these techniques, researchers ensure that LLMs can continue to perform effectively as the complexity and length of their outputs increase. The progress in long CoT training highlights the importance of prioritizing long-output generation in LLM research. By tailoring models to meet the demands of long CoT applications, researchers unlock new possibilities in areas such as advanced problem-solving, strategic planning, and decisionmaking. As the field of long-output generation continues to evolve, its integration with long CoT tasks will play pivotal role in shaping the future capabilities of LLMs, enabling them to address increasingly complex challenges across disciplines. 5. Challenges and Opportunities This section explores the challenges in advancing longoutput large language models (long-output LLMs ) across three key areas: Data, Benchmarks, and Models. Additionally, it highlights opportunities to address these challenges and drive progress in the field. 5.1. Data As discussed in Section 3.1, current supervised fine-tuning (SFT) datasets for long-output tasks, such as LongWriter6k (Bai et al., 2024d) and Suri-30K (Pham et al., 2024), face significant limitations. These challenges can be categorized into two main areas: user demand alignment and reliance on synthetic data. User Demand Alignment: Real-world user demands are often not reflected in the inputs provided by existing datasets. Our analysis of user Long-Output demands from WildChat (Zhao et al., 2024b)6 compared these inputs with those in LongWriter and Suri, using all-mpnet-base-V2 (Reimers & Gurevych, 2020) for embedding and T-SNE for visualization (Van der Maaten & Hinton, 2008). As shown in Figure 4, LongWriter only partially aligns with user demands, while Suri shows minimal overlap due to its synthetic instruction generation. This misalignment suggests that models trained on these datasets may struggle to generalize effectively to real-world scenarios, highlighting critical gap in meeting user needs. Synthetic Data: Given the scarcity of natural long-text data, synthetic datasets such as Longskywork (Zhao et al., 2024a) and FILM (An et al., 2024b) have been widely used. However, as demonstrated by Wu et al. (2024), synthetic data often introduces artificial dependencies that fail to capture the nuanced contextual relationships of real-world text. While these datasets may support performance on inputcentric benchmarks, they fall short in enabling coherent and meaningful long-form text generation. Opportunities: The limitations of current datasets create significant opportunities for innovation. First, real-world data collection, through collaborations with domain experts and industries, can yield high-quality, natural long-form datasets that better align training data with user demands. Second, agent-based approaches can simulate real-world scenarios, generating diverse, intent-rich data and larger instruction-tuning datasets7. Third, hybrid approaches that combine synthetic and real-world data can balance scala6Appendix provides detailed description of the specific implementation for obtaining long-output user requests. 7Current SFT datasets are significantly smaller than traditional ones. bility with contextual richness. Fourth, data augmentation techniques, such as iterative refinement and backtranslation, can enhance both dataset diversity and model robustness. Addressing these data-related challenges is critical for developing more robust and capable long-output LLMs. 5.2. Benchmarks Long-output benchmarks aim to evaluate both the length and quality of outputs exceeding 4K tokens ( 2.6K words). However, they face several challenges that hinder their effectiveness, particularly in model evaluation and applicability. Limited Scope of Benchmarks. Similar to the gap between training data and real-world user demands, significant mismatch exists at the level of benchmarks, with the disparity often being even more pronounced. As illustrated in Figure 5 through UMAP results8, while benchmarks like LongWriter (Bai et al., 2024d) exhibit reasonable alignment with actual user needs, others, such as LongGenBench (Wu et al., 2024) and HelloBench (Que et al., 2024), demonstrate considerable divergence. This misalignment stems from the narrow scope of these benchmarks, which tend to focus on limited subset of long-output tasks, leaving vast range of real-world applications unaddressed. Consequently, the results derived from such benchmarks lack generalizability, impeding the development of models capable of tackling the diverse and complex requirements of long-output scenarios. Evaluating Output Quality. Assessing the quality of long-form text is challenging, as existing methods have significant limitations. Rule-based evaluations effectively measure specific aspects, such as mathematical reasoning (Liu et al., 2024a) or instruction-following (Wu et al., 2024), but fail to capture broader qualities like coherence, logical consistency, and narrative flow, providing only partial picture of long-text generation quality. LLM-based evaluation methods offer broader capabilities but suffer from lack of interpretability. They rarely explain why text is rated poorly, making it difficult to identify areas for improvement. For instance, an experiment (Appendix D) introduced logical error into Snow White, where the shattered magic mirror continued to speak inconsistently9. Most models failed to detect this flaw in the 3K-words version but succeeded with shorter text (300 words), highlighting current limitations in evaluating extended outputs. Additionally, LLM-based evaluations depend on the models ability to understand long 8The two benchmarks have the same abbreviation, longGenBench. In this study, we use Wu et al. (2024), as the concatenation of GSM8K and MMLU in Liu et al. (2024a) as long inputs differ significantly from the actual long-output demand. 9At critical point in the story, sentence is introduced that states the magic mirror has shattered and can no longer speak. As result, any subsequent dialogue from the mirror becomes logically inconsistent. 6 Figure 4. UMAP visualization results for different SFT datasets. WildChat is derived from the long output demands of real users, filtered and referenced in Section 2.1. Figure 5. UMAP visualization results for different benchmark. We use the instructions from the benchmark to evaluate whether the benchmark assesses wide range of long-output demand. texts, which remains an area of weakness (Bai et al., 2024c). High API costs further hinder their accessibility, limiting their adoption. These challenges emphasize the need for more interpretable, scalable, and cost-effective evaluation frameworks. Opportunities. The challenge of effectively evaluating long-output LLMs remains an open issue. We propose that rule-based benchmarks can be enhanced by expanding their evaluation criteria to encompass qualities such as coherence, logical consistency, and creativity. While these qualities may lack readily available ground-truth labels, they can still be evaluated through the development of well-designed rules. For instance, creativity can be assessed using metrics like novelty and originality, in line with established methodologies (Zhao et al., 2024c). To assess coherence, we propose potentially methods that dynamically construct graph representing the narrative flow, allowing inconsistencies to be detected by identifying conflicts within the graph10. The current limitations of LLM-based methods primarily stem from their lack of interpretability and inherent difficulties in understanding long texts. We propose that, in developing LLMs for long-output generation, it would be beneficial to concurrently develop specialized reward LLMs tailored to specific tasks. This co-development strategy could enhance the accuracy, interpretability, and cost-effectiveness of evaluating long-output generation. 5.3. Train & Inference The training and inference processes for long-output LLMs share similarities with those of long-context models. How10For example, if character is stated to have died, subsequent parts of the story should not depict them as living with their spouse. ever, our extensive experimentation reveals two significant, largely unresolved challenges. Model size: notable limitation of current long-output models is their reliance on smaller-scale architectures (10B parameters) (Bai et al., 2024d; Pham et al., 2024). While these models demonstrate the ability to handle long texts, scaling them to larger architectures capable of supporting more complex and higher-quality long-output generation remains substantial challenge. Despite the advancements in state-of-the-art models, their performance highlights the pressing need for more sophisticated strategies to overcome scalability constraints. Larger model sizes, combined with efficient optimization techniques, are essential to fully realize the potential of generating extended, coherent, and high-quality text sequences. Inference Time Overhead Long-output inference incurs significantly higher time overheads than long-input inference, even for sequences of the same length. Long-output inference is often several times slower than long-input inference, as shown in Fig 6. This observation aligns with the pricing models of existing APIs (Reid et al., 2024a; OpenAI, n.d.; GLM et al., 2024), where generating output tokens is typically more expensive than processing input tokens. The primary cause of this discrepancy lies in the iterative nature of output generation, where each token depends on the preceding ones. This sequential dependency limits parallelization and increases latency. Moreover, the lack of dedicated optimizations for long-output scenarios may reduce the effectiveness of existing KV-cache compression techniques, especially those focused on input processing. These factors collectively introduce significant bottleneck in the efficient generation of long-output sequences, imped7 Long-Context Input Optimization Over Long-Output Generation: counterargument to the proposed focus on long-output generation is the continued prioritization of long-context input optimization. Critics may argue that the challenges associated with processing and understanding extensive input contexts are still unresolved, making them prerequisite to achieving high-quality long outputs. Without robust mechanisms for efficiently handling long and diverse input, the coherence and logical consistency of extended outputs might remain unattainable. This perspective suggests that resources and research should remain directed toward input comprehension and scaling input capacity as the foundation for any downstream long-output tasks. Computational Trade-offs: The substantial computational costs associated with long-output generationboth during training and inferencemight render this direction impractical for widespread use. Opponents may argue that research should focus on developing cost-effective techniques for moderate-length tasks, given that long-output LLMs might not be economically viable for most organizations and applications. These constraints could limit their adoption, especially in resource-constrained environments. Evaluation Challenges as Bottleneck: Critics may emphasize that the lack of reliable evaluation metrics for long-output generation represents fundamental bottleneck. Without robust evaluation frameworks, progress in this domain may remain speculative, with limited ability to measure real advancements or their relevance to user needs. This perspective supports prioritizing research in benchmark development before delving into long-output generation. 7. Conclustion In this paper, we have defined and explored the potential of Long-Output LLMs. Despite their significant real-world applications, such models have yet to receive the attention they deserve in both academic research and practical implementations. As artificial intelligence and natural language processing continue to evolve, the ability to generate long-form content is becoming increasingly crucial, particularly for automated content creation, intelligent assistants, and complex information processing. We have identified the defining features, challenges, and emerging trends surrounding long-output LLMs , calling for greater focus on advancing this domain. Future research should prioritize improving the quality, efficiency, and controllability of these models, while also developing new evaluation metrics and exploring diverse application scenarios. The development of long-output LLMs has the potential to drive substantial change across industries, ushering in new era of intelligent transformation. Figure 6. We set the total context length to 12,000 and gradually increased the proportion of output tokens. ing the practical deployment of such models in real-time applications. Opportunities Several approaches can address the challenges in training and inference for long-output LLMs. To mitigate inference time overhead, innovations in KVcache management, parallelization techniques, and hybrid inference methods (e.g., combining autoregressive and nonautoregressive decoding) can significantly improve efficiency. Exploring architectures like Mamba (Gu & Dao, 2024), LongMamba (Anonymous, 2025), and KAN (Liu et al., 2024b) offers opportunities to optimize computational performance. Scaling beyond 10B parameters requires advancements in infrastructure, including distributed RL training frameworks (Hu et al., 2024) and low-memory optimization. Research into evaluating these solutions impacts on latency, coherence, and scalability metrics is essential for fully unlocking the potential of long-output models. 6. Alternative Views While this paper advocates for prioritizing long-output generation in LLMs, there are alternative perspectives that challenge this position and propose different research priorities. Long-Output Generation is Not Always Necessary: An alternative view is that long-output generation might not be essential. Instead, we could use long-context LLMs for chained inference, where each output is used as input for the next step. This approach allows us to generate long texts step by step, avoiding the need for single model to handle long outputs directly. By focusing on improving the coherence and efficiency of this chaining process, we can still achieve high-quality long outputs without the complexity of training models specifically for long-generation tasks."
        },
        {
            "title": "References",
            "content": "An, C., Gong, S., Zhong, M., Zhao, X., Li, M., Zhang, J., Kong, L., and Qiu, X. L-eval: Instituting standardized evaluation for long context language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1438814411, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.776. URL https: //aclanthology.org/2024.acl-long.776. An, S., Ma, Z., Lin, Z., Zheng, N., and Lou, J.-G. Make your llm fully utilize the context, 2024b. URL https: //arxiv.org/abs/2404.16811. Anonymous. Longmamba: Enhancing mambas longcontext capabilities via training-free receptive field enlargement. In The Thirteenth International Conference on Learning Representations, 2025. URL https:// openreview.net/forum?id=fMbLszVO1H. Anthropic. Anthropic: Introducing claude 3.5 sonnet, 2024. URL https://www.anthropic.com/ news/claude-3-5-sonnet. Atmakuru, A., Nainani, J., Bheemreddy, R. S. R., Lakkaraju, A., Yao, Z., Zamani, H., and Chang, H.-S. Cs4: Measuring the creativity of large language models automatically by controlling the number of story-writing constraints. arXiv preprint arXiv:2410.04197, 2024. Bai, Y., Lv, X., Zhang, J., He, Y., Qi, J., Hou, L., Tang, J., Dong, Y., and Li, J. LongAlign: recipe for long context alignment of large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 13761395, Miami, Florida, USA, November 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp. 74. URL https://aclanthology.org/2024. findings-emnlp.74. Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J., and Li, J. LongBench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 31193137, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long. 172. URL https://aclanthology.org/2024. acl-long.172. Bai, Y., Tu, S., Zhang, J., Peng, H., Wang, X., Lv, X., Cao, S., Xu, J., Hou, L., Dong, Y., et al. Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. arXiv preprint arXiv:2412.15204, 2024c. 9 Bai, Y., Zhang, J., Lv, X., Zheng, L., Zhu, S., Hou, L., Dong, Y., Tang, J., and Li, J. Longwriter: Unleashing 10,000+ word generation from long context llms. arXiv preprint arXiv:2408.07055, 2024d. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023. Chiang, W.-L., Zheng, L., Sheng, Y., Angelopoulos, A. N., Li, T., Li, D., Zhang, H., Zhu, B., Jordan, M., Gonzalez, J. E., et al. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132, 2024. Dao, T., Fu, D., Ermon, S., Rudra, A., and Re, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. GLM, T., Zeng, A., Xu, B., Wang, B., Zhang, C., Yin, D., Rojas, D., Feng, G., Zhao, H., Lai, H., Yu, H., Wang, H., Sun, J., Zhang, J., Cheng, J., Gui, J., Tang, J., Zhang, J., Li, J., Zhao, L., Wu, L., Zhong, L., Liu, M., Huang, M., Zhang, P., Zheng, Q., Lu, R., Duan, S., Zhang, S., Cao, S., Yang, S., Tam, W. L., Zhao, W., Liu, X., Xia, X., Zhang, X., Gu, X., Lv, X., Liu, X., Liu, X., Yang, X., Song, X., Zhang, X., An, Y., Xu, Y., Niu, Y., Yang, Y., Li, Y., Bai, Y., Dong, Y., Qi, Z., Wang, Z., Yang, Z., Du, Z., Hou, Z., and Wang, Z. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793, 2024. Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces, 2024. URL https: //arxiv.org/abs/2312.00752. Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia, F., Zhang, Y., and Ginsburg, B. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. Hu, J., Wu, X., Zhu, Z., Xianyu, Wang, W., Zhang, D., and Cao, Y. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. Kamradt, G. Needle in haystack - pressure testhttps://github.com/gkamradt/ URL ing llms. LLMTest_NeedleInAHaystack, 2023. https://github.com/gkamradt/LLMTest_ NeedleInAHaystack. Liu, X., Dong, P., Hu, X., and Chu, X. Longgenbench: Long-context generation benchmark. arXiv preprint arXiv:2410.04199, 2024a. Liu, Z., Wang, Y., Vaidya, S., Ruehle, F., Halverson, J., Soljaˇcic, M., Hou, T. Y., and Tegmark, M. Kan: Kolmogorov-arnold networks, 2024b. URL https: //arxiv.org/abs/2404.19756. MoonshotAI-KiMi. Kimi k1.5: learning with llms, Scaling reinforcement URL https://github.com/MoonshotAI/Kimi-k1. 5/blob/main/Kimi_k1.5.pdf. Accessed: 2025-01-21. 2025. OpenAI. Openai: Hello gpt-4o, 2024a. URL https: //openai.com/index/hello-gpt-4o/. OpenAI. Gpt-4o mini: advancing cost-efficient intelURL https://openai.com/ ligence, 2024b. index/gpt-4o-mini-protectpenalty- @Madvancing-cost-efficient-intelligence/. OpenAI. URL learning-to-reason-with-llms/. Learning to reason with llms, 2024c. https://openai.com/index/ OpenAI. Openai o1 system card, 2024d. URL https: //arxiv.org/abs/2412.16720. OpenAI. Overview, n.d. URL https://platform. openai.com/docs/overview. Accessed: 202412-21. Pham, C. M., Sun, S., and Iyyer, M. Suri: Multi-constraint instruction following for long-form text generation. arXiv preprint arXiv:2406.19371, 2024. Quan, S., Tang, T., Yu, B., Yang, A., Liu, D., Gao, B., Tu, J., Zhang, Y., Zhou, J., and Lin, J. Language models can self-lengthen to generate long texts. arXiv preprint arXiv:2410.23933, 2024. Que, H., Duan, F., He, L., Mou, Y., Zhou, W., Liu, J., Rong, W., Wang, N., Yang, J., Zhang, G., Peng, J., Zhang, Z., Zhang, S., and Chen, K. Hellobench: Evaluating long text generation capabilities of large language models, 10 2024. URL https://openreview.net/forum? id=QM2WoPu1It. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lillicrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat, O., Schrittwieser, J., et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024a. Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lillicrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat, O., Schrittwieser, J., et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024b. Reimers, N. and Gurevych, I. Making monolingual sentence embeddings multilingual using knowledge distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2020. URL https: //arxiv.org/abs/2004.09813. Stelmakh, I., Luan, Y., Dhingra, B., and Chang, M. ASQA: factoid questions meet long-form answers. In Goldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 82738288. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.EMNLP-MAIN. 566. URL https://doi.org/10.18653/v1/ 2022.emnlp-main.566. Tan, H., Guo, Z., Shi, Z., Xu, L., Liu, Z., Feng, Y., Li, X., Wang, Y., Shang, L., Liu, Q., and Song, L. ProxyQA: An alternative framework for evaluating long-form text generation with large language models. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 68066827, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long. 368. URL https://aclanthology.org/2024. acl-long.368. Tu, S., Wang, Y., Zhang-Li, D., Bai, Y., Yu, J., Wu, Y., Hou, L., Liu, H., Liu, Z., Xu, B., et al. Longwriter-v: Enabling ultra-long and high-fidelity generation in vision-language models. arXiv preprint arXiv:2502.14834, 2025. Van der Maaten, L. and Hinton, G. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. Fang, H., and Zhou, Y. Longskywork: training recipe for efficiently extending context length in large language models, 2024a. URL https://arxiv.org/abs/ 2406.00605. Zhao, W., Ren, X., Hessel, J., Cardie, C., Choi, Y., and Deng, Y. Wildchat: 1m chatgpt interaction logs in the wild. arXiv preprint arXiv:2405.01470, 2024b. Zhao, Y., Zhang, R., Li, W., Huang, D., Guo, J., Peng, S., Hao, Y., Wen, Y., Hu, X., Du, Z., et al. Assessing and understanding creativity in large language models. arXiv preprint arXiv:2401.12491, 2024c. Vodrahalli, K., Ontanon, S., Tripuraneni, N., Xu, K., Jain, S., Shivanna, R., Hui, J., Dikkala, N., Kazemi, M., Fatemi, B., et al. Michelangelo: Long context evaluations beyond haystacks via latent structure queries. arXiv preprint arXiv:2409.12640, 2024. Wu, Y., Hee, M. S., Hu, Z., and Lee, R. K.-W. Longgenbench: Benchmarking long-form generation in long context llms. arXiv preprint arXiv:2409.02076, 2024. Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., et al. Effective long-context scaling of foundation models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 46434663, 2024. Xu, J., Szlam, A., and Weston, J. Beyond goldfish memory: Long-term open-domain conversation. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 51805197. Association for Computational Linguistics, 2022. doi: 10. 18653/V1/2022.ACL-LONG.356. URL https://doi. org/10.18653/v1/2022.acl-long.356. Xu, P., Patwary, M., Shoeybi, M., Puri, R., Fung, P., Anandkumar, A., and Catanzaro, B. MEGATRON-CNTRL: controllable story generation with external knowledge In Webber, B., using large-scale language models. Cohn, T., He, Y., and Liu, Y. (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 28312845. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020. EMNLP-MAIN.226. URL https://doi.org/10. 18653/v1/2020.emnlp-main.226. Xu, P., Ping, W., Wu, X., Xu, C., Liu, Z., Shoeybi, M., and Catanzaro, B. Chatqa 2: Bridging the gap to proprietary llms in long context and rag capabilities. arXiv preprint arXiv:2407.14482, 2024. Yang, R., Zhang, C., Zhang, Z., Huang, X., Yang, S., Collier, N., Yu, D., and Yang, D. Logu: Long-form generation with uncertainty expressions. arXiv preprint arXiv:2410.14309, 2024. Ye, X., Yin, F., He, Y., Zhang, J., Yen, H., Gao, T., Durrett, G., and Chen, D. Longproc: Benchmarking long-context language models on long procedural generation, 2025. URL https://arxiv.org/abs/2501.05414. Zhao, L., Wei, T., Zeng, L., Cheng, C., Yang, L., Cheng, P., Wang, L., Li, C., Wu, X., Zhu, B., Gan, Y., Hu, R., Yan, S., A. Proportion of real-user demand A.1. Input Length Statistics and Classification This subsection introduces method for classifying the length of input text based on language and content size. The process involves two main steps: language detection and length calculation. 1. Language Detection: Use regular expressions to identify the language of the text based on character counts. The text is classified as Chinese, Japanese, Korean, or English, depending on the dominant script. 2. Length Calculation: The length of the text is calculated differently for different languages: For Chinese, Japanese, and Korean, character count is used. For English, word count is used. 3. Length Classification: Text is categorized into length buckets based on word count: 2K-4K: 2,000 to 4,000 words 4K-8K: 4,000 to 8,000 words 8K-16K: 8,000 to 16,000 words 16K+: More than 16,000 words A.2. Predicting Users Length Requirement with LLaMA 3.3-70B We describe method for predicting the length of users input requirement using the LLaMA 3.3-70B model for few-shot learning. The process involves two main steps: predicting whether the input exceeds 2,000 words, and predicting the exact length requirement based on the first prediction. 1. Step 1: Predicting Length Exceedance (Prompt 1): The first prediction is made by checking whether the input exceeds 2,000 words. carefully crafted prompt (Prompt 1) is provided to the model to predict if the contents expected word count will surpass the 2K threshold. The model utilizes few-shot learning with example inputs to classify the task into either above 2K or below 2K based on the nature of the input. 2. Step 2: Predicting Exact Length Requirement (Prompt 2): Once the model predicts whether the task exceeds 2,000 words, second prediction is made to determine the exact length category. Based on the result from Step 1, Prompt 2 is designed to predict whether the content is in the 2K-4K, 4K-8K, 8K-16K, or 16K+ category. The model provides the final prediction by analyzing the contextual hints and the input length characteristics. Prompt-1 Guidelines: To determine whether the expected output will exceed 2000 words, consider the following factors: 1. Depth and Complexity: Does the task require detailed explanations, in-depth analysis, or comprehensive coverage of complex topics? 2. Scope and Breadth: Does the task cover multiple subtopics, perspectives, or extensive subject matter? 3. Structure and Sections: Does the output need to include multiple sections such as introductions, literature reviews, methodologies, results, discussions, and conclusions? 4. Research and References: Does the task require extensive research, citations, and referencing of multiple sources? Response Format: Answer with either #*# Yes or #*# No. Provide concise justification based on the guidelines above. Example 1: Query: Is Sanskrit the oldest language? Answer: This question requires concise factual answer, not an extensive output. #*# No *** END Example 2: Query: Create detailed business plan for new cat litter product. Answer: Creating detailed business plan involves multiple sections such as market research, product development, financial projections, marketing strategy, and competitive analysis, all of which require in-depth exploration and explanation. #*# Yes *** END ..... ..... Assess the following statement and decide whether the expected response is likely to require more than 2000 words. Answer with either #*# Yes or #*# No, and include brief justification, like above example. Query: User Query Answer: 13 Prompt-2 Guidelines: To estimate the expected length of the output, consider the following factors: 1. Depth and Complexity: Does the task require detailed explanations, in-depth analysis, or complex reasoning? 2. Scope and Breadth: Does the task cover multiple subtopics, perspectives, or an extensive subject matter? 3. Structure and Sections: Does the output require multiple sections (e.g., introduction, literature review, methodologies, results, discussions, conclusions)? 4. Research and References: Does the task require significant research, citations, or references to multiple sources? 5. Detail Level: Is the task expected to be highly detailed, or can it be summarized concisely? Response Format: - Choose the most likely word count category: Less than 2000 words, 2000 words, 4000 words, 8000 words, or 16000 words. Using (### Category: Chosen category) as the response format. - Provide brief justification based on the guidelines above. Example 1: Less than 2000 words Query: Is Sanskrit the oldest language? Answer: This is factual question requiring brief answer with no complex analysis or subtopics. Likely to be less than 2000 words. ### Category: Less than 2000 words Explanation: Similar to short blog post or brief news article, this task needs minimal detail and is concise. *** END Example 2: 2000 words (2000 to 4000 words) Query: Describe the key differences between classical and quantum computing. Answer: This question requires moderate detail, comparing classical and quantum computing without exhaustive technical exploration. Likely to be around 2000 words. ### Category: 2000 words Explanation: Similar to moderate-length essay or detailed blog post, this task covers key points with enough depth but remains manageable. *** END ..... ..... Example 5: More than 16000 words Query: Write full-length book on the history of the Industrial Revolution, covering all major events, technological innovations, and global impacts. Answer: This would require an in-depth exploration of the entire history of the Industrial Revolution, with detailed analysis across multiple chapters. Likely to be more than 16000 words. ### Category: 16000 words Explanation: Similar to book-length content, such as thesis or encyclopedia entry, requiring substantial detail and coverage over multiple sections or chapters. *** END Assess the following statement and decide what the expected output length is. Answer with the appropriate word count category and provide brief justification. Query: User Query Answer: B. WildChat Long-Output Query Statistics and analysis We analyzed input-output length ratios using the WildChat: 1M ChatGPT Interaction Logs dataset (Zhao et al., 2024b) and the Llama-3.3-70B model (Dubey et al., 2024). Using few-shot learning (Brown et al., 2020), we categorized output lengths into four ranges[2K, 4K), [4K, 8K), [8K, 16K), and [16K, +) wordsand compared them to input lengths. The results show that demand for long-output generation is more than five times higher than for equivalent-length inputs, peaking at nearly 20 times at 4K-8K levels (Figure 7). However, due to the lack of file upload support in WildChat, the statistics for long-input queries are likely underestimated. 14 Figure 7. Proportion of real-user demand: The aforementioned 2K range refers to the interval [2K, 4K), and similarly for the other ranges. Solid color fill for input demand, slash fill for output demand in the Wildchat dataset. C. Long-context paper list C.1. ICML 2024 1. Linguistic Calibration of Long-Form Generations 2. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences 3. Human-Inspired Reading Agent with Gist Memory of Very Long Contexts 4. Memory Consolidation Enables Long-Context Video Understanding 5. QUEST: Query-Aware Sparsity for Efficient Long-Context LLM Inference 6. LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning 7. LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens 8. Training-Free Long-Context Scaling of Large Language Models 9. LoCoCo: Dropping In Convolutions for Long Context Compression 10. Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT 11. Data Engineering for Scaling Language Models to 128k Context C.2. ICLR 2024 1. LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models 2. Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis 3. BooookScore: Systematic Exploration of Book-Length Summarization in the Era of LLMs 4. In-Context Pretraining: Language Modeling Beyond Document Boundaries 5. Functional Interpolation for Relative Positions Improves Long Context Transformers 6. RingAttention with Blockwise Transformers for Near-Infinite Context 7. Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs 15 8. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores 9. CLEX: Continuous Length Extrapolation for Large Language Models 10. Retrieval Meets Long Context Large Language Models 11. IceFormer: Accelerated Inference with Long-Sequence Transformers on CPUs 12. PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training 13. Efficient Streaming Language Models with Attention Sinks 14. YaRN: Efficient Context Window Extension of Large Language Models 15. Parallelizing Non-linear Sequential Models over the Sequence Length 16. In-context Autoencoder for Context Compression in Large Language Model 17. HyperAttention: Long-context Attention in Near-Linear Time C.3. NIPS 1. Streaming Long Video Understanding with Large Language Models 2. BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack 3. Perceiving Longer Sequences with Bi-Directional Cross-Attention Transformers 4. Video Token Merging for Long Video Understanding 5. Chain of Agents: Large Language Models Collaborating on Long-Context Tasks 6. LoTLIP: Improving Language-Image Pre-training for Long Text Understanding 7. Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack 8. MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention 9. MMLONGBENCH-DOC: Benchmarking Long-context Document Understanding with Visualizations 10. An Efficient Recipe for Long Context Extension via Middle-Focused Positional Encoding 11. Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding 12. Mini-Sequence Transformers: Optimizing Intermediate Memory for Long Sequences Training 13. Mixture of In-Context Experts Enhance LLMs Long Context Awareness 14. MMBench-Video: Long-Form Multi-Shot Benchmark for Holistic Video Understanding 15. StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses 16. InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory 17. LongVideoBench: Benchmark for Long-context Interleaved Video-Language Understanding 18. Rethinking Transformer for Long Contextual Histopathology Whole Slide Image Analysis 16 C.4. ACL 2024 1. L-Eval: Instituting Standardized Evaluation for Long Context Language Models 2. Analyzing Temporal Complex Events with Large Language Models? Benchmark Towards Temporal, Long Context"
        },
        {
            "title": "Understanding",
            "content": "3. LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression 4. Making Long-Context Language Models Better Multi-Hop Reasoners 5. LongBench: Bilingual, Multitask Benchmark for Long Context Understanding 6. Landmark Embedding: Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language"
        },
        {
            "title": "Models",
            "content": "7. CoCA: Fusing Position Embedding with Collinear Constrained Attention in Transformers for Long Context Window"
        },
        {
            "title": "Extending",
            "content": "8. NextLevelBERT: Masked Language Modeling with Higher-Level Representations for Long Documents 9. RelayAttention for Efficient Large Language Model Serving with Long System Prompts 10. Marathon: Race Through the Realm of Long Context with Large Language Models 11. Bench: Extending Long Context Evaluation Beyond 100K Tokens 12. FinTextQA: Dataset for Long-form Financial Question Answering 13. Long Context is Not Long at All: Prospector of Long-Dependency Data for Large Language Models 14. M4LE: Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models 15. Chunk, Align, Select: Simple Long-sequence Processing Method for Transformers 16. LooGLE: Can Long-Context Language Models Understand Long Contexts? 17. Never Lost in the Middle: Mastering Long-Context Question Answering with Position-Agnostic Decompositional Training 18. DocFinQA: Long-Context Financial Reasoning Dataset 19. SumSurvey: An Abstractive Dataset of Scientific Survey Papers for Long Document Summarization 20. Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization C.5. EMNLP 2024 1. LongEmbed: Extending Embedding Models for Long Context Retrieval 2. Forgetting Curve: Reliable Method for Evaluating Memorization Capability for Long-Context Models 3. LUQ: Long-text Uncertainty Quantification for LLMs 4. Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA 5. CItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling 6. Attribute or Abstain: Large Language Models as Long Document Assistants 7. Summary of Haystack: Challenge to Long-Context LLMs and RAG Systems 8. AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies 17 9. Where am I? Large Language Models Wandering between Semantics and Structures in Long Contexts 10. FinDVer: Explainable Claim Verification over Long and Hybrid-content Financial Documents 11. LONGAGENT: Achieving Question Answering for 128k-Token-Long Documents through Multi-Agent Collaboration 12. Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP 13. SEGMENT+: Long Text Processing with Short-Context Language Models 14. One Thousand and One Pairs: Novel Challenge for Long-Context Language Models 15. LLoCO: Learning Long Contexts Offline 16. Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition 17. Memorize Step by Step: Efficient Long-Context Prefilling with Incremental Memory and Decremental Chunk 18. LongRAG: Dual-perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering 19. Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts 20. LongWanjuan: Towards Systematic Measurement for Long Text Quality 21. LongHeads: Multi-Head Attention is Secretly Long Context Processor 22. Insights into LLM Long-Context Failures: When Transformers Know but Dont Tell 23. LongGenBench: Long-context Generation Benchmark 24. Cant Remember Details in Long Documents? You Need Some R&R 25. GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models 26. More Bang for Your Context: Virtual Documents for Question Answering over Long Documents 27. LongAlign: Recipe for Long Context Alignment of Large Language Models 28. Long Sequence Modeling with Attention Tensorization: From Memory-Efficient Design to Long-context OpenQA 29. LSM1K: Large Scale Memory-based Dataset for Long Text Modeling C.6. NAACL 2024 1. RST-LoRA: Discourse-Aware Low-Rank Adaptation for Long Document Abstractive Summarization 2. Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks 3. Effective Long-Context Scaling of Foundation Models 4. WESOME: GPU Memory-constrained Long Document Summarization using Memory Mechanism and Global Salient Content C.7. Long-output 1. Suri: Multi-constraint Instruction Following in Long-form Text Generation 2. LongGenBench: Long-context Generation Benchmark 18 D. Broken Mirror Test We conducted simple test, and almost all models (OpenAI, 2024a; Dubey et al., 2024; OpenAI, 2024b; Reid et al., 2024b; GLM et al., 2024) failed to correctly answer the Prompt-long (only O1 (OpenAI, 2024c) was able to answer correctly, but not stable). We identified logical errors in handling long text cases, while the models were almost always able to correctly respond to the Prompt-short. Prompt-long (3089 words) This is fairy tale. Please carefully consider whether there are any logical issues in the following story. If there are, point out just one significant logical flaw? Once upon time in midwinter, when the snowflakes were falling like feathers from heaven, queen sat sewing at her window, which had frame of black ebony wood. As she sewed she looked up at the snow and pricked her finger with her needle. ..... ..... Mirror, mirror, on the wall, Who in this land is fairest of all? To this the mirror answered: You, my queen, are fairest of all. ..... ..... With that, she slammed the mirror with all her strength, shattering it into pieces, her fury burning in her heart. The mirror was instantly destroyed, unable to speak again. ..... ..... Back at home she asked her mirror: Mirror, mirror, on the wall, Who in this land is fairest of all? It finally answered: You, my queen, are fairest of all. ..... ..... Prompt-short (390 words) This is fairy tale. Please carefully consider whether there are any logical issues in the following story. If there are, point out just one significant logical flaw? Once upon time in midwinter, when the snowflakes were falling like feathers from heaven, queen sat sewing at her window, which had frame of black ebony wood. As she sewed she looked up at the snow and pricked her finger with her needle. ..... Mirror, mirror, on the wall, Who in this land is fairest of all? To this the mirror answered: You, my queen, are fairest of all. When the queen heard the mirror say this, she shook and trembled with anger, shouting, Snow-White must die. With that, she slammed the mirror with all her strength, shattering it into pieces, her fury burning in her heart. The mirror was instantly destroyed, unable to speak again. Tell me, mirror, she said, her voice softer now, will always be the fairest in the land? True beauty comes not from the surface, but from the soul, the mirror answered. What you seek is fleeting. Find peace, and you will see beauty that lasts beyond your own. The Queens eyes hardened again. do not seek peace. seek power. And so, with cold smile, she turned away from the mirror, her heart set on path that would lead her further from the light. E. Current model in Long-output Benchmark The Fig 8 is from Longwrite (Bai et al., 2024d), and it reveals that most existing LLMs are unable to meet the output requirements for long instructions. 19 Figure 8. LongWriter-Ruler test demonstrates maximum output length limitation of approximately 2k words for all models tested."
        }
    ],
    "affiliations": [
        "Singapore University",
        "Singapore University of Technology and Design",
        "Tsinghua University"
    ]
}