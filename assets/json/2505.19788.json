{
    "paper_title": "Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured Multi-Turn Decomposition",
    "authors": [
        "Zihao Zeng",
        "Xuyao Huang",
        "Boxiu Li",
        "Hao Zhang",
        "Zhijie Deng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) are criticized for the excessively lengthy Chain-of-Thought (CoT) to derive the final answer, suffering from high first-token and overall latency. Typically, the CoT of LRMs mixes multiple thinking units; each unit attempts to produce a candidate answer to the original query. Hence, a natural idea to improve efficiency is to reduce the unit number. Yet, the fact that the thinking units in vanilla CoT cannot be explicitly managed renders doing so challenging. This paper introduces Multi-Turn Decomposition (MinD) to decode conventional CoT into a sequence of explicit, structured, and turn-wise interactions to bridge the gap. In MinD, the model provides a multi-turn response to the query, where each turn embraces a thinking unit and yields a corresponding answer. The subsequent turns can reflect, verify, revise, or explore alternative approaches to both the thinking and answer parts of earlier ones. This not only makes the answer delivered more swiftly, but also enables explicit controls over the iterative reasoning process (i.e., users may halt or continue at any turn). We follow a supervised fine-tuning (SFT) then reinforcement learning (RL) paradigm to realize MinD. We first rephrase the outputs of an LRM into multi-turn formats by prompting another LLM, and then tune the LRM with such data. Observing that the tuned model tends to consume even more tokens than the original one (probably due to that the multi-turn formats introduce additional answer tokens), we advocate leveraging RL algorithms like GRPO to prioritize correct outputs with fewer turns. Trained on the MATH dataset using R1-Distill models, MinD can achieve up to ~70% reduction in both output token usage and time to first token (TTFT), while maintaining competitive performance on reasoning benchmarks such as MATH-500, AIME24, AMC23, and GPQA-Diamond."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 8 8 7 9 1 . 5 0 5 2 : r Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured Multi-Turn Decomposition Zihao Zeng12, Xuyao Huang1, Boxiu Li1, Hao Zhang3, and Zhijie Deng1 1Shanghai Jiao Tong University 2RealAI 3University of California, San Diego {zengzihao, huangxuyao, lbxhaixing154}@sjtu.edu.cn haozhang@ucsd.edu, zhijied@sjtu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) have gained increasing attention over the past few months. Despite being effective, LRMs are criticized for the excessively lengthy Chain-of-Thought (CoT) to derive the final answer, suffering from high first-token and overall latency. Typically, the CoT of LRMs mixes multiple thinking units, some of which are split by markers like aha, wait, or alternatively; each unit attempts to produce candidate answer to the original query. Hence, natural idea to improve efficiency is to reduce the unit number. Yet, the fact that the thinking units in vanilla CoT cannot be explicitly managed renders doing so challenging. This paper introduces Multi-Turn Decomposition (MinD) to decode conventional CoT into sequence of explicit, structured, and turn-wise interactions to bridge the gap. In MinD, the model provides multi-turn response to the query, where each turn embraces thinking unit and yields corresponding answer. The subsequent turns can reflect, verify, revise, or explore alternative approaches to both the thinking and answer parts of earlier ones. This not only makes the answer delivered more swiftly, but also enables explicit controls over the iterative reasoning process (i.e., users may halt or continue at any turn). We follow supervised fine-tuning (SFT) then reinforcement learning (RL) paradigm to realize MinD. We first rephrase the outputs of an LRM into multi-turn formats by prompting another LLM, and then tune the LRM with such data. Observing that the tuned model tends to consume even more tokens than the original one (probably due to that the multi-turn formats introduce additional answer tokens), we advocate leveraging RL algorithms like GRPO to prioritize correct outputs with fewer turns. Trained on the MATH dataset using R1-Distill models, MinD can achieve up to 70% reduction in both output token usage and time to first token (TTFT), while maintaining competitive performance on reasoning benchmarks such as MATH-500, AIME24, AMC23, and GPQA-Diamond."
        },
        {
            "title": "Introduction",
            "content": "Large Reasoning Models (LRMs) have recently attracted significant attention due to their advancing reasoning capabilities, including OpenAI-o1 [10], DeepSeek-R1 [7], and Kimi-1.5 [29]. These models have achieved remarkable performance on complex tasks, e.g., mathematical competitions, thanks to their ability to engage in think-then-answer paradigm, where intermediate reasoning chains are generated to induce the final answer. The resultant Chain-of-Thought (CoT) activates contextually accurate responses through iterative exploration and verification of potential solutions. Equal contribution. Corresponding author. Preprint. Under review. Figure 1: An illustration of responses from DeepSeek-R1-Distill-Qwen-7B and the transformed MinD-7B model on the same math problem. The original LRM follows think-then-answer format, where the reasoning process consists of multiple thinking units (the start of each new unit is marked with an orange highlight). In contrast, MinD-7B adopts multi-turn reasoning paradigm, where each turn contains thinking unit followed by an answer. Also note that MinD-7B tends to use fewer thinking units due to the GRPO training (see Section 3.3). Despite these advantages, LRMs often suffer from inefficiency issues as the CoT can become excessively lengthy, exhibiting substantially increased computational costs and latency compared to non-reasoning Large Language Models (LLMs). To mitigate these, several strategies have been proposed in recent works. For example, some approaches encourage models to generate answers more directly through strategically designed prompts [11], truncate the chain of thought to avoid unnecessary token generation [6, 30], or leverage speculative reasoning via model collaboration [21, 24]. Other approaches focus on reducing token redundancy by refining model reasoning paths through supervised fine-tuning (SFT) [34], or by enhancing decision efficiency with improvements to Group Relative Policy Optimization (GRPO) algorithms [35, 16]. The CoT reasoning process in LRMs is typically composed of multiple thinking unitsdiscrete cognitive steps like initial attempts, follow-up validations, reflections, and strategic shifts. Each unit can contribute to generating candidate answer, while current LRMs tend to employ redundant units to ensure the final answer is close to perfect (see an empirical analysis of such redundancy in Figure 2 (right)). While reducing the number of thinking units could improve reasoning efficiency, the inability to explicitly manage these units in standard CoT makes this challenging. This highlights the need for more fine-grained approaches to improve reasoning efficiency. Building on this insight, we introduce Multi-Turn Decomposition (MinD) to decode the thinkthen-answer CoT reasoning into sequence of multi-turn interactions to enable the explicit control of the number of thinking units, where each turn contains single thinking unit and an answer generated based on both the current and all preceding units. Refer to Figure 1 for an illustration of the paradigm shift. To implement MinD, we adopt pipeline combining SFT and GRPO. We first convert conventional CoT traces into structured, multi-turn formats using GPT-4o [20] and then fine-tune the target model on such data. To further enhance efficiency, we apply GRPO to encourage the model to generate accurate responses within fewer reasoning turns, thereby reducing latency and computational costs. To evaluate the effectiveness of MinD, we conduct extensive experiments across range of reasoning benchmarks. On DeepSeek-R1-Distill-Qwen-1.5B, MinD reduces token usage by up to 70% and accelerates time to first token (TTFT) by 4.2 on MATH-500, while maintaining over 95% accuracy. Furthermore, MinD demonstrates strong out-of-distribution generalization on this model, with token reductions of 69% on AIME24 and 53% on GPQA-Diamond. These results highlight the efficiency and broad applicability of MinD in diverse reasoning scenarios."
        },
        {
            "title": "2 Related Work",
            "content": "Efficient Reasoning Paradigms The evolution of reasoning frameworks for LLMs has progressed significantly since the introduction of CoT prompting [31]. CoT has proven effective in enhancing LLMs reasoning abilities by explicitly guiding models through intermediate reasoning steps [7], but this approach often leads to excessively lengthy outputs, resulting in high token consumption and increased latency [2]. These inefficiencies have motivated researchers to explore more compact and 2 efficient reasoning paradigms. One prominent line of work aims to reduce intermediate token usage without sacrificing reasoning quality. For example, methods like token skipping [32] and lengthharmonizing pruning [17] have demonstrated significant reductions in token counts while maintaining strong task performance [6]. These approaches directly target the redundancy challenge by refining the granularity of reasoning traces, thereby reducing overall token overhead. Another approach seeks to decouple the reasoning process from explicit token generation by leveraging continuous latent spaces. For instance, Token-Assorted Mixing [28] and Hidden Thinking frameworks [25] aim to perform internal computations without generating extensive token sequences, achieving 3-5 faster processing speeds compared to conventional CoT [8]. This direction effectively compresses intermediate steps into compact latent representations, significantly improving efficiency. Additionally, several studies have explored integrating reasoning and non-reasoning models to enhance efficiency. For example, the C3OT system [13] employs multi-stage verification pipeline to reduce token redundancy, while speculative reasoning approaches [21] dynamically adjust the reasoning depth based on task complexity, further reducing token usage. Hybrid architectures like Hawkeye [24] also leverage speculative decoding [36] to balance accuracy and computational efficiency. Reinforcement Learning for Reasoning Optimization Reinforcement learning (RL) has become an essential tool for optimizing LLM reasoning, providing precise control over decision-making processes. Group Relative Policy Optimization (GRPO) [23] is one of the most influential methods in this domain, aligning reward signals with step-wise reasoning validity rather than simply final answer correctness. This strategy allows models to prioritize accurate intermediate steps, enhancing both response precision and computational efficiency. Building on this foundation, frameworks like DAPO [35] and R1-Zero [16] incorporate dynamic reward shaping and entropy-controlled exploration to further refine model outputs. These methods extend GRPO by introducing adaptive mechanisms that reduce token redundancy while maintaining high accuracy, making them particularly effective for complex reasoning tasks. Recent advancements have also focused on integrating searchbased techniques to enhance reasoning efficiency. For instance, Search-R1 [12] combines Monte Carlo Tree Search with policy gradients to optimize reasoning path selection, reducing unnecessary token usage. Similarly, length-aware control frameworks like L1-Controller [1] balance correctness and token efficiency through dual reward signals, achieving substantial latency reductions. Other approaches, such as R1-Searcher [27], incorporate dynamic halting mechanisms to automatically terminate unproductive reasoning chains, significantly improving efficiency in open-domain tasks. ThinkPrune [9] adopts length clipping to the reward function, pruning outputs to reduce redundancy. Training-Based Efficiency Enhancements Training strategies have also played critical role in improving reasoning efficiency. Supervised fine-tuning (SFT) methods like Thinking-Optimal Scaling [34] align models with optimal solution trajectories, reducing token redundancy without compromising accuracy. This approach effectively reshapes the internal reasoning paths of models, ensuring more concise outputs. Hybrid training regimes have also gained traction, combining imitation learning and reinforcement learning to refine reasoning efficiency. For example, the SpecReason framework [21] employs two-stage process, beginning with teacher-student distillation for foundational policy approximation, followed by adversarial reward shaping for fine-grained optimization. This blend of supervised and reinforcement learning techniques has proven effective in reducing token counts while maintaining response quality."
        },
        {
            "title": "3 Method",
            "content": "In this section, we first introduce the standard Chain-of-Thought (CoT) reasoning of Large reasoning models (LRMs) and briefly review Group Relative Policy Optimization (GRPO) [4]. We then present an empirical study showing how redundant reasoning steps commonly arise in LRMs. Finally, we outline MinD, which reformulates the standard CoT into multi-turn structure, and discuss how to leverage GRPO to encourage concise and effective multi-turn reasoning. 3.1 Preliminary CoT for LRMs LRMs commonly adopt think-then-answer paradigm for complex problem solving. Given query q, an LRM typically produces an output of the form: = <think> </think> , (1) Figure 2: Left: An example of standard CoT from DeepSeek-R1, naturally containing multiple discrete thinking units (the start of each new unit is marked with an orange highlight). Right: Empirical analysis of unit-level redundancy, which is calculated based on Equation (5), in R1-distilled models on the MATH-500 dataset, showing an average redundancy rate of 69.8% for the 1.5B model and 35.8% for the 7B model. where denotes the internal thinking process, delimited by <think> and </think>, and is the final answer. The thinking process can be viewed as an exploration of the solution space and is naturally decomposed into multiple thinking unitsself-contained logical steps that can induce candidate answer to q, with an example from DeepSeek-R1 [7] depicted in Figure 2 (left). Formally, letting ui denote thinking unit, there is = (u1, u2, . . . , un). These units may arise from (1) an initial attempt to solve the problem, (2) depth-wise exploration such as validation, backtracking, or correction along single line of reasoning, or (3) breadth-wise search involving alternative methods or perspectives. Each unit can thus be interpreted as path in the reasoning space, potentially building on previous steps, and may terminate with provisional answer to the query. However, current LRMs tend to employ numerous thinking units before gaining the final answer to solve the problem as perfectly as possible, causing significant inefficiency issues. GRPO Let πθ denote the current policy and πθold the reference policy from the previous iteration. Given query q, GRPO samples completions o1, . . . , oG and optimizes the objective: q, {oi}G i=1 1 (cid:88) i= 1 oi oi (cid:88) j=1 min (ρi,jAi, clip(ρi,j, 1 ϵ, 1 + ϵ)Ai) , (2) where ρi,j = πθ(oi,j q,oi,<j ) oi and oi is the sequence length. Ai is the group-standardized advantage: πθold (oi,j q,oi,<j ) is the ratio between the new and old policies for token in sequence Ai = R(oi) mean({R(o1), . . . , R(oG)}) std({R(o1), . . . , R(oG)}) , (3) where denotes the reward function, and mean({r1, . . . , rG}) and std({r1, . . . , rG}) represent the mean and standard deviation of group rewards, respectively. For clarity, we omit the KL regularization term, as it is not the focus of our analysis. 3.2 Unit-Level Redundancy in LRMs Before devoting to reducing the number of thinking units of LRMs, we first systematically investigate the unit-level redundancy, which is intuitively high considering the repeated depth-wise validations or breadth-wise explorations of alternative solution paths, even after repeatedly arriving at essentially the same valid answer, in long CoTs. Concretley, we conducted detailed analysis using DeepSeek-R1-Distill-Qwen-1.5B/7B [4]. We extracted their CoT traces from the MATH [14] and GSM8K [3] training sets (restricted to correctly answered examples), and segmented each trace into discrete thinking units using GPT-4o [20] (see Appendix for details). Figure 3: Transforming think-then-answer LRMs into multi-turn reasoning paradigm, consisting of four steps: (1) Rejection sampling to filter out responses with correct final answers; (2) Unit segmentation using GPT-4o to divide CoTs into discrete reasoning units; (3) Intermediate answer completion to extract answers (ak) for each prefix sub-trace (tk); and (4) SFT to align LRMs with the multi-turn format. For each segmented trace = (u1, u2, . . . , un), we constructed prefix sub-traces tk = (u1, . . . , uk) for 1 n. We then prompted the model to generate an intermediate answer ak by appending special stop token </think> after tk given the current partial reasoning: ok = <think> tk </think> ak , = 1, , . (4) To quantify unit-level redundancy, we define the minimal sufficient prefix tn as the shortest prefix that leads to correct final answer. The unit-level redundancy rate is then defined as: URR = 1an is correct , (5) where is the total number of thinking units and is the minimal number required for correctness. higher URR indicates greater proportion of unnecessary reasoning steps. Our empirical results, summarized in Figure 2 (right), show that the average unit-level redundancy rates are 69.8% for the 1.5B model and 35.8% for the 7B model. This reveals that significant portion of the reasoning process in current LRMs is redundant for solving the problem, underscoring the potential for substantial efficiency gains by explicitly mitigating unit-level redundancy. 3.3 Multi-Turn Decomposition (MinD) Our basic notion is that the model should not be that cautious. Given that done is better than perfect, we aim to let the model yield candidate answer as soon as possible. Besides, we would also like to penalize the unit-level redundancy. MinD realizes these through two key innovations. Multi-Turn CoT Reformulation MinD first employs supervised fine-tuning (SFT) to shift the reasoning paradigm from think-then-answer (i.e., Equation (1)) to structured multi-turn format: <think> u1 </think> a1 <think> u2 </think> a2 <think> un </think> an , (6) where the thinking units (u1, u2, . . . , un) in the original CoT are distributed into sequence of reasoning turns. Each turn also includes an intermediate answer ak. To construct the training data for multi-turn SFT, we first segment the original thinking process into (u1, u2, . . . , un), and then generate an intermediate answer ak after each uk, as described in Section 3.2. The overall pipeline is illustrated in Figure 3. After training, the learned multi-turn LRM enables flexible management of the thinking units (e.g., choose to continue or abort from the reasoning by manipulating the token </think>), but we empirically observe that when applying no control, the model tends to generate even more output tokens than the original one (see Table 4). This is because SFT primarily reshapes the reasoning format without directly addressing unit-level redundancy, and ak incurs further token usage. To bridge the gap, we suggest leveraging GRPO to prioritize efficient reasoning traces. 5 Reducing Reasoning Turns via GRPO We define reward function comprises three components for GRPO: = Rformat + Raccuracy + Runit . (7) In detail, they are: (1) Format Consistency Reward Rformat, which ensures that the generated output adheres to the multi-turn structure described in Equation (6). (2) Answer Accuracy Reward Raccuracy, which rewards the model for producing correct final answer, as determined by matching an to the ground truth. (3) Unit Compactness Reward Runit, which penalizes cases where single reasoning unit contains multiple exploratory trajectories and thus encourages clear separation between reasoning turns. See Section 4.3 for further analysis of this component. The specific weights for each reward component are detailed in Table 1. Note that we do not introduce an explicit reward term regarding the number of turns, because GRPO inherently introduces an implicit bias toward generating shorter CoTs that yield correct answers. As shown in Equation (2), for fixed advantage Ai, the per-token normalization 1/oi results in larger per-token updates for shorter outputs [15, 35, 16], thereby encouraging the model to produce more concise and efficient completions. This effect is particularly pronounced in LRMs, which typically possess strong reasoning capabilities and can generate multiple correct yet diverse completions per group during training. Thus, the GRPO framework naturally incentivizes the model to favor responses with fewer reasoning turns. This behavior is empirically validated in Figure 5, where we observe substantial reduction in the number of reasoning turns following GRPO training."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we evaluate the efficiency of MinD across several benchmarks. Section 4.1 describes the experimental setup. Section 4.2 presents the main results, focusing on token reduction, accuracy, and latency. Ablation studies and additional discussion are provided in Section 4.3. 4.1 Setup Table 1: Reward function value settings. Table 2: Training data sizes. Compliance Non-Compliance Rformat Raccuracy Runit +2 - 0 -0.3 +1 -1 1.5B 3610 7500 7B 3532 SFT GRPO Training Details The training process for MinD consists of two key phases, as described in Section 3.3. The first SFT phase is conducted using the LLaMA-Factory repository [37]. We perform full-parameter fine-tuning for 2 epochs with learning rate of 5e-5. The second GRPO phase leverages the veRL repository [26]. During this phase, we train for 1 epoch with an actor learning rate of 1e-6. For each training step, 10 roll-out completions are generated for each sample, with all other hyperparameters set to the default values provided by veRL. The reward function described in Section 3.3 is adopted with the weight configurations listed in Table 1. Models & Datasets We conduct our experiments using DeepSeek-R1-Distill-Qwen-1.5B/7B [4]. For SFT, the training data consists of questions from the GSM8K [3] and MATH [14] training sets. Model-generated responses are filtered via rejection sampling to retain only correct answers, then pre-processed as shown in Figure 3. For GRPO, we use the MATH training set exclusively, with sample sizes detailed in Table 2. We evaluate on both in-distribution (MATH-500 [14]) and out-of-distribution benchmarks, including AIME24 [18], AMC23 [19], and GPQA-Diamond [22], to assess generalization. Baselines To assess the efficiency of our method, we compare against the following baselines: Original LRM: The base models used in this work, DeepSeek-R1-Distill-Qwen-1.5B and 7B. ThinkPrune [9]: Adds length clipping to the GRPO reward and is trained on the AIME-AMC subset, progressively pruning outputs at the token level to reduce response length. DEER [33]: training-free approach that detects action transition points (e.g., Wait, Alternatively, Hmm) to 6 Table 3: Performance comparison of various baselines and our proposed method, MinD, across four reasoning benchmarks: MATH-500, AIME24, AMC23, and GPQA-Diamond. The table reports both accuracy (Acc.; higher is better) and average output token usage (Tokens; lower is better) for each model. Results are shown for both 1.5B and 7B parameter configurations, covering the original LRM (DeepSeek-R1-Distill-Qwen-1.5B and 7B), ThinkPrune [9], Dynasor [6], DEER [33], and our method, MinD. Note that for MinD, GRPO is performed only on the MATH training set, making MATH-500 in-domain and the others out-of-domain. As shown in the table, MinD consistently achieves competitive or superior accuracy while significantly reducing token usage, demonstrating its effectiveness for efficient and generalizable reasoning. MATH-500 AIME24 AMC23 GPQA-Diamond Acc. Tokens Acc. Tokens Acc. Tokens Acc. Tokens Original LRM 85.4 ThinkPrune [9] DEER [33] MinD 83.2 -2.6% 73.2 -14.3% 82.8 -3.0% Original LRM 93. Dynasor [6] DEER [33] MinD 88.5 -4.8% 87.4 -6.0% 91.6 -1.5% 5389 1938 -64% 1118 -79% 1719 -68% 3928 2591 -34% 975 -75% 2859 -27% 1.5B 26.7 15177 67.5 27.1 +1.5% 20.0 -25.1% 30.0 +12.4% 5631 -63% 3302 -78% 4856 -68% 73.2 +8.4% 47.5 -29.6% 77.5 +14.8% 7B 50.0 14107 90.0 47.7 -4.6% 33.3 -33.4% 46.7 -6.6% 8760 -38% 3235 -77% 7258 -49% 87.1 -3.2% 82.5 -8.3% 95.0 +5.6% 9956 3039 -70% 2384 -76% 2384 -76% 6076 4913 -19% 1622 -73% 3777 -38% 32.3 - 5.6 -82.7% 31.3 -3.1% 50.5 - 27.3 -45.9% 53.0 +5.0% - 4128 -58% 4690 -52% 8390 - 2265 -73% 6845 -18% trigger answer generation, and halts decoding when the mean token probability surpasses confidence threshold. Dynasor [6]: Periodically inserts probes (e.g., every 32, 64, or 128 tokens) to extract intermediate answers and assess their consistency, enabling early termination of generation. Evaluation Metrics We evaluate MinD using three primary metrics: accuracy, average output token usage, and time-to-first-token (TTFT). TTFT measures the time it takes for the model to generate the first answer token of the response, from when the prompt was senta key determinant of user experience. The evaluations are conducted using the Open-R1 evaluation scripts [5], with maximum sequence length of 32,768 tokens, temperature setting of 0.6, and top-p value of 0.95, running on four NVIDIA A100 GPUs. 4.2 Main Results Reducing Output Tokens for Efficient Reasoning After training the 1.5B and 7B multi-turn reasoning models as described in Section 4.1, we evaluated their token efficiency across range of reasoning benchmarks. The results, summarized in Table 3, show that MinD consistently reduces output token usage while maintaining strong performance. On in-domain MATH-500, MinD lowers the average token usage to 1719 for the 1.5B modela 68% reduction from the Original LRM (5389 tokens)while achieving 82.8% accuracy. Although ThinkPrune attains similar accuracy (83.2%), it requires more tokens (1938). DEER achieves the lowest token usage (1118), but with substantial accuracy drop to 73.2%. For the 7B model, MinD reduces average token usage by 27% compared to the Original LRM (2859 vs. 3928), with high accuracy of 91.6%, outperforming both Dynasor and DEER in the balance of accuracy and efficiency. MinDs efficiency generalizes well to out-of-domain benchmarks. For example, on AMC23 (1.5B), MinD reaches 77.5% accuracy with 2384 tokens, substantially outperforming ThinkPrune and DEER in both accuracy and token reduction. Similar trends are observed on AIME24 and GPQA-Diamond. These results demonstrate that MinD effectively eliminates unnecessary reasoning steps, producing concise, efficient outputs without compromising performance. Reducing TTFT and Total Latency The TTFT and total response latency for the original R1distilled LRMs and our MinD models are summarized in Figure 4. As shown, MinD significantly reduces both TTFT and total latency across both model sizes. For the 1.5B configuration, the original 1.5B model requires 35.4s TTFT, which drops to 21.8s after SFT and further to 8.4s with MinD, resulting in 4.2 speedup. The total latency is similarly reduced from 35.8s (original) to 25.8s 7 Figure 4: TTFT (time to first token) and total latency of two DeepSeek-R1-distilled models on MATH-500. MinD achieves up to 4.2 (1.5B) and 2.1 (7B) speedups over the original LRMs in TTFT, and 3.2 (1.5B) and 1.6 (7B) in total latency. Figure 5: The distribution of reasoning turns for MinD at different training stages (1.5B model) on the MATH-500 dataset. Each bar represents model checkpoint, including the SFT model and successive GRPO training steps. As GRPO training progresses, the number of reasoning turns per output decreases and becomes increasingly concentrated at 1 or 2 turns (highlighted in red and orange), demonstrating the effectiveness of GRPO in mitigating reasoning redundancy. Table 4: Comparison of different training strategies on DeepSeek-R1-Distill-Qwen-1.5B. Original LRM refers to the pretrained baseline. SFT-Only applies only the supervised fine-tuning step from MinD. Non-Multi-Turn applies GRPO without explicit multi-turn segmentation. MinD denotes our full method with both multi-turn segmentation and GRPO. Acc. indicates accuracy (higher is better), and Tokens indicates average output length (lower is better). Original LRM SFT-Only Non-Multi-Turn MinD Acc. Tokens Acc. Tokens Acc. Tokens Acc. Tokens MATH-500 AIME24 AMC23 GPQA-Diamond 85.4 26.7 67.5 32.3 5389 15177 9956 9842 82.8 26.7 77.5 28.3 5655 20675 8409 82.0 20.0 65.0 28.8 1866 7654 3415 3397 82.8 30.0 77.5 37.4 1719 4856 2384 4345 (SFT) and 11.3s (MinD), 2.1 improvement. For the 7B model, TTFT decreases from 27.8s (original) to 21.6s (SFT) and 13.2s (MinD), achieving 2.1 speedup. The total latency is reduced from 30.5s to 25.3s and 18.9s, for 1.6 speedup. These results show that MinD shortens both the time to first answer token and the overall response latency, making the models more responsive. 4.3 Discussion & Ablation GRPO is Crucial for Efficient Reasoning As discussed in Section 3.3, SFT alone does not guarantee efficient reasoning. To demonstrate this, we compare the performance of models after SFT and after the full MinD pipeline, as shown in Table 4. The results reveal that SFT-only training often increases average output token usage relative to the original LRM. In contrast, applying GRPO further leads to substantial reductions in token usage while preserving accuracy, underscoring the essential role of GRPO in enabling concise and effective reasoning. Role of Runit in Maintaining Multi-Turn Reasoning As discussed in Section 3.3 and detailed in Table 1, our GRPO framework introduces Unit Compactness Reward, Runit, to enforce that each reasoning turn contains only single, coherent exploratory trajectory. This mechanism is essential for preventing the model from degenerating into the original monolithic think-then-answer stylea common outcome under GRPOs token-level averaging (Section 3.3), which tends to favor shorter correct outputs. Without specific penalty for multi-trajectory turns, the model may skip intermediate answers, collapsing the multi-turn reasoning structure into single-block CoT. To counteract this, Runit penalizes reasoning turns that contain multiple exploratory trajectories, detected by linguistic cues such as phrases like double-check. This strategy encourages each turn to contain only one 8 Figure 6: Left: Comparison of GRPO training with and without Runit on MATH-500 for different 1.5B model checkpoints, showing Average Output Tokens for each. Removing Runit leads to instability and collapse in output length. Right: An illustrative case comparing the outputs of GRPO-100-step and GRPO-400-step checkpoints trained without Runit. While the earlier checkpoint (GRPO-100) maintains clear multi-turn reasoning, the later checkpoint (GRPO-400) exhibits several thinking units within single turn (the start of each new unit is marked with an orange highlight), demonstrating that omitting Runit results in blurred step boundaries and loss of controllable, structured reasoning. exploratory trajectoryespecially in the critical first turnwithout requiring external supervision, and thus maintains the multi-turn paradigm throughout training. The impact of Runit is demonstrated in Figure 6, which shows how its absence leads to collapse in output structure and length. MinD Effectively Alleviates Redundancy To demonstrate the effectiveness of GRPO in reducing redundancy, we plotted the distribution of reasoning turns for SFT and GRPO models on the MATH500 dataset, as shown in Figure 5. The figure clearly illustrates that GRPO significantly reduces the number of reasoning turns, indicating more compact and efficient reasoning process compared to the purely SFT-trained models. Additionally, from the data in Table 3, GRPO reduces the average output tokens on MATH-500 by 68.1% for the 1.5B model and 27.2% for the 7B model, compared to their respective original LRMs. This aligns well, though not directly, with the redundancy rates of 69.8% and 35.8% for these models, as reported in Figure 2 (Right). While these figures cannot be directly equated, they collectively indicate that MinD, through GRPO, substantially alleviates redundancy, resulting in more concise and efficient outputs. The Importance of Multi-Turn Structure To evaluate the impact of the multi-turn design, we performed SFT using responses from the original distilled-1.5B model, without applying any multiturn segmentation (i.e., using the same question set as in step (1) of Figure 3), followed by GRPO with only the format and outcome rewards. As shown in Table 4, the Non-Multi-Turn model achieves comparable results to MinD on in-distribution MATH-500, but exhibits notable drop in accuracy and only marginal reductions in token usage on out-of-distribution benchmarks. We hypothesize that, under the conventional CoT format, models lack the flexibility to adjust the number of thinking units, making it difficult to learn reasoning process that is both controllable and generalizable. Additional discussion can be found in Appendix A."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduced Multi-Turn Decomposition (MinD), an efficient method for improving the reasoning efficiency of large language models. By structuring the reasoning process into multiturn steps, MinD significantly reduces token usage and response latency while maintaining strong performance across various reasoning tasks. Our results demonstrate that structured reasoning provides practical solution to challenges such as slow response times and high computational costs in large language models."
        },
        {
            "title": "6 Limitation",
            "content": "Our work is limited by experiments on only 1.5B and 7B models and primary focus on mathematical reasoning. Future directions include scaling to larger models, expanding to other reasoning domains, and developing adaptive multi-turn strategies that adjust the number of turns based on problem difficulty or user preference."
        },
        {
            "title": "References",
            "content": "[1] Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning, 2025. [2] Cheng-Han Chiang and Hung yi Lee. Over-reasoning and redundant calculation of large language models, 2024. [3] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [4] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [5] Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. [6] Yichao Fu, Junda Chen, Yonghao Zhuang, Zheyu Fu, Ion Stoica, and Hao Zhang. Reasoning without self-doubt: More efficient chain-of-thought through certainty probing. In ICLR 2025 Workshop on Foundation Models in the Wild, 2025. [7] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [8] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language model to reason in continuous latent space, 2025. [9] Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning, 2025. [10] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [11] Renlong Jie, Xiaojun Meng, Lifeng Shang, Xin Jiang, and Qun Liu. Prompt-based length controlled generation with multiple control types, 2024. [12] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning, 2025. [13] Yu Kang, Xianghui Sun, Liangyu Chen, and Wei Zou. C3ot: Generating shorter chain-ofIn Proceedings of the AAAI Conference on thought without compromising effectiveness. Artificial Intelligence, volume 39, pages 2431224320, 2025. [14] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. [15] Zhihang Lin, Mingbao Lin, Yuan Xie, and Rongrong Ji. Cppo: Accelerating the training of group relative policy optimization-based reasoning models. arXiv preprint arXiv:2503.22342, 2025. [16] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective, 2025. 10 [17] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning, 2025. [18] Mathematical Association of America. American invitational mathematics examination - aime 2024, 2024. [19] Australian Academy of Science. Australian mathematics competition - amc 2023, 2023. [20] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. [21] Rui Pan, Yinwei Dai, Zhihao Zhang, Gabriele Oliaro, Zhihao Jia, and Ravi Netravali. Specreason: Fast and accurate inference-time compute via speculative reasoning. arXiv preprint arXiv:2504.07891, 2025. 11 [22] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. [23] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. [24] Jianshu She, Zhuohao Li, Zhemin Huang, Qi Li, Peiran Xu, Haonan Li, and Qirong Ho. Hawkeye:efficient reasoning with model collaboration, 2025. [25] Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, and Jiuxiang Gu. Efficient reasoning with hidden thinking, 2025. [26] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [27] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025. [28] DiJia Su, Hanlin Zhu, Yingchen Xu, Jiantao Jiao, Yuandong Tian, and Qinqing Zheng. Token assorted: Mixing latent and text tokens for improved language model reasoning, 2025. [29] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [30] Qwen Team. Qwen3, April 2025. [31] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 2482424837, 2022. [32] Heming Xia, Weilin Wang, Han Yu, Xin Wang, Xiangning Lin, and Ming Zhou. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067, 2024. [33] Chenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Zheng Lin, Li Cao, and Weiping Wang. Dynamic early exit in reasoning models, 2025. [34] Wenkai Yang, Shuming Ma, Yankai Lin, and Furu Wei. Towards thinking-optimal scaling of test-time compute for llm reasoning, 2025. [35] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. [36] Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. Draft & verify: Lossless large language model acceleration via self-speculative decoding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1126311282, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [37] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics."
        },
        {
            "title": "A Word Frequency Analysis of Thinking Units",
            "content": "In this section, we collect and compare the number of distinct words representing thinking units in DeepSeek-R1-Distill-1.5B, including the Original LRM, Non-Multi-Turn (GRPO applied without explicit multi-turn segmentation) , and MinD. Although these words do not precisely correspond to the number of actual thinking units, they serve as meaningful proxy and offer indicative insights into their distribution(see Table 5 for details). Table 5: The frequency of words representing thinking units in outputs generated by Original LRM, Non-Multi-Turn and MinD across MATH-500, AIME24 and AMC23."
        },
        {
            "title": "Alternatively",
            "content": "double-check check verify MATH-500 Original LRM Non-Multi-Turn MinD 13993 1822 2206 333 237 Original LRM Non-Multi-Turn MinD 13993 1822 1651 Original LRM Non-Multi-Turn MinD 2302 246 215 AIME 2206 333 237 AMC23 385 38 30 368 41 10 368 41 10 35 3 1272 347 434 1272 347 434 205 42 50 124 193 249 124 193 249 45"
        },
        {
            "title": "B Prompting for MinD",
            "content": "In this section, we present the complete prompt formats used in the MinD process (see Figure 3 for details). Q&A Template {Question} Please reason step by step, and put your final answer within boxed{}. Decomposing into Thinking Units One Reasoning Round is part of the full model You will be provided with math problem and solution generated by reasoning model. The models response may contain multiple Reasoning Rounds. generation and is defined as complete reasoning process or verification process that explicitly contains the final answer. Your task is to carefully analyze the response and segment it into individual Reasoning Rounds. Specifically, insert [split] between every two consecutive Reasoning Rounds. - Problem: {question} Solution: {prediction} - Please give the solution with [split] tags without any redundant words."
        }
    ],
    "affiliations": [
        "RealAI",
        "Shanghai Jiao Tong University",
        "University of California, San Diego"
    ]
}