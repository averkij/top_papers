{
    "paper_title": "Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian",
    "authors": [
        "Michael Hoffmann",
        "Jophin John",
        "Stefan Schweter",
        "Gokul Ramakrishnan",
        "Hoi-Fong Mak",
        "Alice Zhang",
        "Dmitry Gaynullin",
        "Nicolay J. Hammer"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Llama-GENBA-10B, a trilingual foundation model addressing English-centric bias in large language models. Built on Llama 3.1-8B and scaled to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens (82B English, 82B German, and 80M Bavarian), balancing resources while preventing English dominance. Targeted at the German NLP community, the model also promotes Bavarian as a low-resource language. Development tackled four challenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2) creating a unified tokenizer for English, German, and Bavarian, (3) optimizing architecture and language-ratio hyperparameters for cross-lingual transfer, and (4) establishing the first standardized trilingual evaluation suite by translating German benchmarks into Bavarian. Evaluations show that Llama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned variant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing itself as the best model in its class for this language, while also outperforming EuroLLM in English and matching its results in German. Training on the Cerebras CS-2 demonstrated efficient large-scale multilingual pretraining with documented energy use, offering a blueprint for inclusive foundation models that integrate low-resource languages."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 8 6 6 5 0 . 9 0 5 2 : r Llama-GENBA-10B: Trilingual Large Language Model for German, English and Bavarian Michael Hoffmann* Leibniz Supercomputing Centre (LRZ) Garching, Germany Michael.Hoffmann@lrz.de Jophin John* Leibniz Supercomputing Centre (LRZ) Garching, Germany Jophin.John@lrz.de Stefan Schweter Independent Researcher Holzkirchen, Germany Stefan@Schweter.bayern Alice Zhang Cerebras Systems Sunnyvale, USA Alice.Zhang@cerebras.net Gokul Ramakrishnan Cerebras Systems Sunnyvale, USA gokul.ramakrishnan@ cerebras.net Dmitry Gaynullin Cerebras Systems Sunnyvale, USA dmitry.gaynullin@ cerebras.net Hoi-Fong Mak Leibniz Supercomputing Centre (LRZ) Garching, Germany Hoi-fong.Mak@lrz.de Nicolay J. Hammer Leibniz Supercomputing Centre (LRZ) Garching, Germany Nicolay.Hammer@lrz.de"
        },
        {
            "title": "Abstract",
            "content": ""
        },
        {
            "title": "Introduction",
            "content": "We present Llama-GENBA-10B, trilingual foundation model addressing Englishcentric bias in large language models. Built on Llama 3.1-8B and scaled to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens (82B English, 82B German, and 80M Bavarian), balancing resources while preventing English dominance. Targeted at the German NLP community, the model also promotes Bavarian as low-resource language. Development tackled four challenges: (1) curating multilingual corpus despite Bavarian scarcity, (2) creating unified tokenizer for English, German, and Bavarian, (3) optimizing architecture and language-ratio hyperparameters for cross-lingual transfer, and (4) establishing the first standardized trilingual evaluation suite by translating German benchmarks into Bavarian. Evaluations show that Llama-GENBA-10B achieves strong cross-lingual performance, with the finetuned variant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing itself as the best model in its class for this language, while also outperforming EuroLLM in English and matching its results in German. Training on the Cerebras CS2 demonstrated efficient large-scale multilingual pretraining with documented energy use, offering blueprint for inclusive foundation models that integrate low-resource languages. *Equal contribution. Large Language Models (LLMs) have evolved from English-dominant architectures to embrace multilingual capabilities, spanning proprietary systems like GPT-4 (Achiam et al., 2023), Gemini 2 (Team et al., 2023), and Qwen-3 (Yang et al., 2025), alongside open-source alternatives such as Llama 2 (Touvron et al., 2023) and Llama 3 (Dubey et al., 2024). Recent advances have expanded language coverage to over 100 languages through models like xLlama-100 and xBLOOM-100 (Lai et al., 2024), yet substantial challenges remain in achieving balanced representation across diverse linguistic communities. Despite these developments, English continues to dominate pretraining datasets and evaluation frameworks. Leading open-source models like Llama-3 (Dubey et al., 2024), while supporting multilinguality across dozens of languages, remain predominantly trained on English-centric corpora, treating multilingual performance as secondary rather than foundational. Although truly multilingual architectures trained on more balanced datasuch as Bloom (Workshop et al., 2022) and PolyLM (Wei et al., 2023)are emerging, they remain the exception. Consequently, performance disparities persist, particularly affecting underrepresented languages (Wang et al., 2020) and highlighting the critical need for models that prioritize multilingual parity from inception. This study introduces GENBA-10B, trilingual model for German, English, and Bavarian, with less than half of the training data in English. The model is intended to support the German and Bavarian NLP community, and enable applications ranging from tourism to media accessibility while contributing to linguistic preservation1. Built upon Llama3.1-8B and expanded to 10B parameters through block expansion techniques (Gosal et al., 2024; Wu et al., 2024), GENBA10B was trained on 164B tokens comprising 82B English, 82B German, and 80M Bavarian tokens. Development addressed four critical challenges: curating training data despite the scarcity of Bavarian resources; developing custom trilingual tokenizer; selecting optimal architecture and language distribution ratios; and establishing appropriate evaluation benchmarks, including Bavarianspecific assessment tools. We evaluate GENBA-10B with trilingual benchmark suite translating German tasks (HellaSwag, ARC, TruthfulQA, MMLU) into Bavarian, enabling direct cross-lingual comparison across Bavarian, German, and English. In its base form, GENBA-10B ranks second among European models, following Apertus-8B. It performs particularly well in English and Bavarian, and achieves comparable results to EuroLLM in German. After trilingual fine-tuning, the model attains state-of-the-art performance in Bavarian, even surpassing Apertus-8B and Gemma-2-9Bit, while also outperforming EuroLLM in English and matching its results in German. Overall, the fine-tuned variant establishes GENBA-10B as the strongest sub-10B parameter model for Bavarian language tasks. Training on single Cerebras CS-2 AI Accelerator (Lie, 2024) shows that large-scale multilingual pretraining can be conducted efficiently by small research teams2. We tracked energy consumption and training time to evaluate hardware efficiency and resource use, providing practical insights into the computational costs of foundation model development and guidance for resourceconstrained groups."
        },
        {
            "title": "This study contributes to multilingual Large",
            "content": "Language Model development in three ways: First, we present GENBA-10B, trilingual 1Initial distribution is restricted to approved academic and non-profit organizations under non-commercial license; we are working to resolve the legal aspects for broader release. 2GENBA-10B was developed by team of five core contributors. model designed to mitigate data scarcity, tokenizer limitations, cross-lingual transfer challenges, and English dominance by training on corpus with equal-scale English and German components (82B tokens each) and complementary Bavarian component (80M tokens). Second, we establish the first trilingual benchmark suite and show that GENBA-10B achieves strong cross-lingual performance, with the finetuned variant setting new state of the art for Bavarian among sub-10B models. Third, we demonstrate efficient large-scale pretraining on single Cerebras CS-2 system while recording energy use, providing practical guidance for resource-constrained teams. The remainder of this study is organized as follows: Section 2 reviews related work on multilingual and dialect-specific LLMs; Section 3 outlines our methodology, including data collection, tokenizer design, architecture, and training infrastructure; Section 4 reports experiments on language ratios, architecture scaling, hyperparameters, finetuning, and energy use; Section 5 presents evaluation results for both base and fine-tuned models together with pretraining energy measurements; and Section 6 concludes with future directions."
        },
        {
            "title": "2 Related Work",
            "content": "This section reviews prior research on large language models (LLMs) in two areas: multilingual LLM development as well as German-and dialecttailored approaches."
        },
        {
            "title": "2.1 Multilingual LLMs",
            "content": "Transformer-based large language models (LLMs) have recently moved from English-only training to genuinely multilingual settings. Prominent examples include Llama-3.1 (Dubey et al., 2024), Mistral 7B (Jiang et al., 2023), and Qwen3-8B (Yang et al., 2025), which can process non-English input but rarely disclose the distribution of languages in their pretraining corpora. Although efforts such as Aya (Üstün et al., 2024) and Llama-3.1 (Dubey et al., 2024) explicitly aim to broaden linguistic coverage, their training data remain dominated by English, constraining performance in underrepresented languages (Choudhury et al., 2025). Moreover, these models are subject to the welldocumented curse of multilinguality (Pfeiffer et al., 2022). To mitigate these imbalances, several equityfocused models have emerged. EuroLLM-9B (Martins et al., 2025) was trained from scratch on over 4 trillion tokens across three phases, covering all 24 official EU languages plus 11 additional ones. Despite this broad coverage, English still dominates, comprising 50 per cent of phase 1 and 32.5 per cent of phases 2 and 3, while no single non-English language exceeds 6 per cent of any phase. Nonetheless, EuroLLM-9B achieves competitive results on multilingual benchmarks. Similarly, Teuken-7B (Ali et al., 2024a) trains from scratch on corpus that is roughly 60 per cent non-English and employs custom multilingual tokenizer to support all 24 EU languages, demonstrating competitive performance on European variants of ARC, HellaSwag, MMLU, and TruthfulQA. Finally, the Swiss AI initiative recently released Apertus 3, open multilingual models trained from scratch on 15 trillion tokens spanning 1,000+ languages, including previously underrepresented Swiss German and Romansh. Despite only 40 per cent of the training data being non-English, the models remain competitive. Full multilingual pretraining yields strong results but at the cost of immense compute and data requirements. An alternative is continual pretraining of established models on targeted corpora. Building on the block-expansion method of Wu et al. (2024) and Gosal et al. (2024), one can insert identity-initialized Transformer blocks into frozen base model and train only these new blocks on specialized data. In the Llama Pro variant, derived from Llama 2-7B with eight extra blocks (totaling 8.3 billion parameters), this approach used 80 billion tokens of code and mathematics to improve domain-specific capabilities without catastrophic forgetting (De Lange et al., 2021). We extend block-expansionbased continual pretraining to the multilingual setting with our GENBA-10B model. Starting from Llama-3.18B checkpoint, we expand the architecture to 10B parameters and continue training on 164B tokens of English, German, and Bavarian text. Unlike prior work focusing on code or mathematics, we target language-specific adaptation. GENBA-10B attains competitive results on multilingual benchmarks, illustrating that block expansion offers resource-efficient approach to trilingual adaptation and step toward more equitable multilingual LLMs. 3https://github.com/swiss-ai/apertus-tech-report 2.2 German and Dialect-Specific LLMs Research on German-centric language models has progressed from early encoder-only systems like GBERT and GELECTRA, trained on German text and achieving SoTA on classification and NER tasks (Chan et al., 2020), to more advanced models. In late 2023, LeoLM emerged as Llama-2-based German foundation model pretrained on high-quality German corpus, with versions at 7B and 13B parameters4. Separately, the LLäMmlein series (120M and 1B decoder trained fully from scratch with transmodels), parency, matched or surpassed comparable models on benchmarks like SuperGLEBer (Pfister et al., 2024). More recently, BübleLM-2B (2B parameters), adapted from Gemma-2-2B with Germanspecific tokenizer, delivered substantial performance gains in tasks like commonsense reasoning and knowledge-based QA, outperforming both Gemma-2-2B and LLäMmlein-1B5. Modeling regional dialects like Bavarian introduces notable challenges: dialectal corpora are scarce, and models frequently default to standardized forms rather than retaining dialect nuances, sometimes resulting in translationese (Riley et al., 2019) outputs. For instance, recent datasets like MaiBaam (Blaschke et al., 2024) and Betthupferl (Blaschke et al., 2025) underscore both the scarcity of resources and the tendency of models to normalize dialectal inputs. In parallel, work on Galician by Gamallo et al. (2024) demonstrates that 1.3B parameter GPT-style model trained on 2.1B word Galician corpus achieved promising improvements in fluency and adequacy. Building on these insights, we introduce Llama-GENBA-10B, available in both base and instruction-tuned variants. The model is 10Bparameter architecture initialized from the publicly available Llama-3.1-8B checkpoint6. We perform continual pretraining on 164B-token multilingual corpus comprising English, German, and Bavarian. Our methodological contribution lies in the timing and manner of dialectal data integration: Bavarian material, upsampled by factor, is introduced only after 90 per cent of training has elapsed. This strategy provides practical way to add dialectal resources to large-scale pretraining while supporting trilingual development. 4https://laion.ai/blog/leo-lm 5https://huggingface.co/flair/bueble-lm-2b 6https://huggingface.co/meta-llama/Llama-3.1-8B"
        },
        {
            "title": "3 The Approach",
            "content": "This section describes the methodology used to develop the trilingual GENBA-10B model. The process comprised four stages: (i) compiling multilingual corpus, (ii) designing tokenizer tailored to GermanEnglishBavarian, (iii) conducting architecture search experiments, and (iv) establishing scalable training infrastructure. Each stage is described in the following subsections. 3.1 Data Collection and Corpus Composition Data Collection Methodology: The study began with assembling training data in three languages. While English and German resources were readily available, Bavarian data were scarce. Ultimately, two reliable sources were identified: the Boarische Wikipedia corpus from the Wortschatz Uni Leipzig project and four datasets from OPUS. To extract further Bavarian content systematically, the team initially developed language identification script using METAs FastText classifier (Joulin et al., 2016), trained on balanced set of sentences from the Boarisch Wikipedia and standard German texts using high-performance computing infrastructure. However, this approach proved inadequate for reliably detecting Bavarian in unstructured or unlabeled corpora. The team subsequently adopted the GlotLID-m classifier (Kargaran et al., 2023), which demonstrated higher accuracy and enabled the extraction of approximately 262,373 lines of Bavarian sentences from the Fine-Web dataset. Several key methodological decisions were made to guide the collection process. To maintain manageable scope, the team chose not to differentiate between dialectal variants within Bavarian. Although promising social media sources were identified, such as Facebook groups like Boarisch redn is in (16,000 members, August 2024) and Niederbairisch für Anfänger und Runaways (504 members), these were excluded to avoid copyright concerns and expedite development. Similarly, the Bayrisches Wörterbuch was omitted due to insufficient token volume. Corpus Composition: The training corpus comprises three partitions: English, German, and Bavarian. English Corpus: The English partition is derived from the following sources: Knowledge Pile (Fei et al., 2024): is 188B large-scale dataset curated from web crawls (Common Crawl) using retrieval methods, primarily designed to enhance language models with high-quality knowledge-intensive text spanning diverse topics such as mathematics, biology, physics, and humanities, Cosmopedia (Ben Allal et al., 2024): dataset of synthetic textbooks, blogposts, stories, posts, and WikiHow articles generated by Mixtral-8x7B-Instruct-v0.1. UltraTextbooks-2.0: collection of highquality synthetic and human-written textbooks spanning various subjects and programming languages. Proof-pile 2 (Azerbayev et al., 2023): 55 billion token dataset of mathematical and scientific documents. From these four datasets, we constructed an English partition of 82.85B tokens. German Corpus: In recent years, several largescale German corpora have emerged, including G4Corpus, OSCAR, or Occiglot as well as smaller collections like Tagesschau and the Newspaper Articles collection from the Wortschatz Leipzig University Project. After evaluating these resources, we selected occiglot-fineweb-v1.0 as the sole source for our German corpus. The dataset contains roughly 430 million cleaned documents spanning 10 languages and combines curated collections with pre-filtered web data. Using custom German tokenizer, processing required about seven hours on CS-2 user node and produced 110.96 billion German tokens. Bavarian Corpus: The Bavarian corpus consists of 20M tokens, assembled from the datasets listed in Table 1."
        },
        {
            "title": "Size",
            "content": "Uni Leipzig Wikipedia Wikimatrix Opus XLEnt Opus Wikimedia Opus Opus Tatoeba Filt. FineWeb Wiki/Non-Wiki 262k sentences 100k 579k tokens 19k tokens 55k tokens 0.6k tokens Table 1: Overview of Bavarian language corpus. Corpus Statistics for GENBA-10B: The three language-specific sources were combined into the final training corpus, with the German and English portions each truncated at 82 billion tokens and the Bavarian portion upsampled to 80 million tokens. The overall composition is summarized in Table 2 diminishing returns against memory and compute costs, we selected the 20 per cent expansion for our final tokenizer. Language Dataset Tokens 3.3 Model Architecture Selection EnglishDataset 82B English German GermanDataset 82B Bavarian BavarianDataset 80M Table 2: Corpus composition for GENBA-10B. 3.2 Tokenizer Optimization Existing research shows that tokenizer choice has major impact on multilingual LLM performance, yet it often goes under-examined (Ali et al., 2024b). In our EnglishGermanBavarian model, ensuring that German umlauts (ä, ö, ü) and the eszett (ß) are handled correctly was essential to prevent token fragmentation and preserve semantic integrity. To address this challenge, the authors systematically expanded the Llama-3-8B tokenizer vocabulary to better accommodate German and Bavarian linguistic elements through three-phase process: 1. Initial Validation: We ran tokenization script on small, high-frequency sample of Bavarian tokens drawn from our corpus. After confirming that fragmentation was minimal, we applied the same script to the full Bavarian corpus. 2. Vocabulary Extension via Byte-Pair Encoding (BPE):) We merged common subword units into the original 128,256-token Llama-3 vocabulary, yielding three expanded variants: 10% expansion: +12,800 tokens (141,053 total), Fertility Score: 1.9026 architecture, decoder-only GENBA-10B employs standard transformerbased, following Vaswani et al. (2017). We build upon the Llama 3 (Dubey et al., 2024) architecture, which shares structural foundations with Llama 2 (Touvron et al., 2023) while offering extended context (8K versus 4K tokens) and being trained on significantly larger and more multilingual data. To determine the optimal architectural foundation, we conducted comparative experiments between the Llama 3 and Llama 3.1 variants across established benchmarks. The results are presented in Figure 1. It shows that Llama 3.1 consistently outperforms Llama 3 across multiple evaluation tasks. Figure 1: Comparative Performance Across Evaluation Tasks Based on these empirical results, we selected Llama 3.1 as the foundational model for subsequent training and development. 20% expansion: +25,600 tokens, Fertility Score: 1."
        },
        {
            "title": "3.4 Training Infrastructure",
            "content": "30% expansion: +38,400 tokens, Fertility Score: 1.8214 3. Performance Evaluation: We evaluated these configurations using fertility scores, where lower scores indicate fewer tokens required per word, desirable characteristic that reduces sequence lengths and improves training and inference efficiency. The evaluation showed sharp fertility score decline from 10% to 20% expansion (1.9026 to 1.8372), but minimal improvement from 20% to 30% (1.8372 to 1.8214). Balancing Model training was conducted on the Cerebras Wafer-Scale Engine 2 (CS-2), single-chip AI accelerator with 850 000 AI optimized compute cores, 40 GB of on-chip SRAM, and memory bandwidth of 20 PB/s (Lie, 2024). The architecture also provides 220 Pb/s of fabric interconnect bandwidth, enabling high-throughput parallelism over large datasets and mitigating the scalability challenges of distributed GPU training such as inter-node communication and synchronization costs. This setup enabled efficient training of the trilingual model at scale."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Finding the Right Mix of Languages In developing the JAIS language models, Sengupta et al. (2023) found that mixing Arabic and English at 1:2 ratio outperformed training on Arabic alone (p. 9). Inspired by this result, we conducted controlled experiments varying the allocation of English and German data prior to pretraining. Each experiment was constrained to 16B token budget, with the GermanEnglish split adjusted across two scenarios. To ensure comparability, the relative proportions of the English subcorpora (Cosmopedia, Ultra Textbooks, Proof Pile 2, and Pile Knowledge) were fixed at 1:1:1:1. Experiment (1:1) allocated 8B tokens to German and 8B tokens to English. Experiment (9:1) allocated 14.4B tokens to German and 1.6B tokens to English. Experiment Acc-EN Acc-DE (1:1) (9:1) 0.4040 0.4009 0.5256 0.5169 Table 3: Results for different language mixes. Table 3 and Figure 2 present accuracy results showing that balanced 1:1 English-German ratio yields optimal performance for our trilingual model. We applied this language mix for the initial 90 per cent of pretraining before incorporating also Bavarian datasets. Figure 2: Performance For Different Language Splits"
        },
        {
            "title": "Methodology",
            "content": "We expanded the LLaMA-3.1-8B base architecture by 20 percent through the addition of eight In the Transformer blocks before pretraining. newly inserted blocks, the FFNs final linear layer and the attention output projection layer were initialized with zero weights, while the modelwide embedding and output layers were extended with semantically derived vectors for the new tokens. Zero-initialization, combined with backbone freezing, enables the new layers to learn meaningful transformations without altering the pretrained backbone. Pretraining was conducted with the aim of balancing multilingual knowledge acquisition across languages of differing resource availability. For the first 90 percent of training iterations, only English and German data were used, enabling the model to establish strong representations in these high-resource languages. In the final 10 per cent, Bavarian data were introduced, upsampled and combined with the remaining English and German data to create trilingual training regime. This staged integration was motivated by two considerations: (i) the expanded architecture provides capacity to represent broader multilingual feature space, and (ii) delaying the introduction of Bavarian reduces potential interference between highand low-resource languages. Progressive incorporation of Bavarian thus allows the model to allocate capacity to this low-resource variety without immediate competition from the larger English and German datasets."
        },
        {
            "title": "4.3 Pretraining Hyperparameters",
            "content": "GENBA-10B pretraining employed the hyperparameters detailed in Table 4. We implemented learning rate schedule comprising an initial warmup phase where the rate linearly increased from zero to 0.00015 over approximately 1 per cent of training steps (417 steps), followed by cosine annealing decay to 0.000015 over the remaining 41,290 steps. This schedule mitigates early training instability through gradual warmup while enabling refined convergence through progressive learning rate reduction in later stages."
        },
        {
            "title": "4.4 Supervised Fine-Tuning",
            "content": "We conducted supervised fine-tuning of the Llama-GENBA-10B-base model to enhance its multilingual instruction-following capabilities. In contrast to the pretraining stage, during which the Llama backbone was frozen, all model parameters remained trainable in this phase. Fine-tuning proceeded for three epochs, with checkpoints saved and evaluated at the end of each. The checkpoint Hyperparameter Value Parameters Attention heads Layers Training tokens Training steps Final learning rate Batch size 10B 32 40 164B 41,707 1.5 105 4M tokens Table 4: Pretraining hyperparameters from epoch 2 demonstrated the strongest performance and was therefore selected as the final finetuned version of GENBA-10B (Llama-GENBA10B-instruct). The fine-tuning process was supported by the GENBA-10B-Post-Training dataset (see Table 5), which we constructed through systematic extension of established instruction-following corpora. As the foundation, we employed the Databricksdolly-15k dataset, providing high-quality English and German instructionresponse pairs. To broaden multilingual coverage, these pairs were automatically translated into Bavarian using the Gemini-flash model (Comanici et al., 2025), yielding 15,000 additional pairs for each target language. The same translation methodology was subsequently applied to the Alpaca and OpenHermes datasets, ensuring methodological consistency and comparable quality across all corpora. Hence in total, the GENBA-10B-Post-Training dataset comprises 867k instructionresponse pairs with balanced representation of English, German, and Bavarian. detailed composition and statistical analysis are presented in Table 5."
        },
        {
            "title": "Per language Total",
            "content": "Databricks-Dolly15k Alpaca OpenHermes"
        },
        {
            "title": "Total",
            "content": "15k 61k 213k 45k 183k 639k 289k 867k Table 5: Datasets Used for Supervised Fine-Tuning"
        },
        {
            "title": "4.5 Energy Consumption Analysis",
            "content": "energy during We measured GENBA-10B pretraining by monitoring Power (PSU) measurements across the Supply Unit consumption Wafer Scale Engine throughout the training Our measurements tracked power process. consumption patterns to understand the energy characteristics of wafer-scale computing during large language model training."
        },
        {
            "title": "5 Evaluation and Results",
            "content": "We adopt three-part evaluation strategy: (i) evaluation of the Llama-GENBA-10B-base model against comparably sized systems, (ii) evaluation of the supervised fine-tuned Llama-GENBA-10Binstruct model, and (iii) analysis of GENBA-10Bs (Llama-GENBA-10B-base) energy consumption during pretraining. 5.1 Evaluation of the Base Model Benchmarks: Evaluation was conducted using the EleutherAI evaluation harness, including both the original English version (Gao et al., 2024) and its German (Dac Lai et al., 2023) adaptation. For Bavarian we translated the German versions of these benchmarks to enable evaluation in To measure reasoning capabilities, we employ ARC-Challenge (Clark et al., 2018), which consists of 7,787 grade-school multiple-choice science questions. For commonsense reasoning, we use HellaSwag (Zellers et al., 2019), benchmark requiring models to select the most plausible continuation of partial event description; the English set contains 59,950 instances. We also include TruthfulQA (Lin et al., 2021), an 817-question benchmark covering 38 categories such as health, law, finance, and politics, which evaluates whether models generate factually accurate answers. To assess broad, domain-diverse knowledge and reasoning, we use MMLU (Hendrycks et al., 2020), comprising 15,908 multiple-choice questions from 57 subjects spanning STEM, humanities, social sciences, and professional domains. Finally, we use WinoGrande (Sakaguchi et al., 2021), large-scale pronoun resolution benchmark with roughly 44,000 fill-in-the-blank problems inspired by the Winograd Schema Challenge but expanded in size and difficulty. Results: Figure 3 presents comparative evaluation of Llama-GENBA-10B-base against other LLMs of similar scale. Overall, Llama-GENBA10B-base trails models such as gemma-2-9b, Qwen2.5-7B, Apertus-8B-2509, Llama-3.1-8B, Figure 3: Comparison of Llama-GENBA-10B-base against peer base models, showing competitive performance with strong results in English, moderate results in German, and solid results in Bavarian, where it ranks fourth among baseline models. Pre-trained Non-European Arc Hellaswag MMLU TruthfulQA Winogrande Llama-3.1-8B OLMo-2-1124-7B Qwen2.5-7B SauerkrautLM-Gemma-2b gemma-2-9b gemma-3-4b-pt granite-3.0-8b-base European Apertus-8B-2509 EuroLLM-9B LLaMmlein_7B Mistral-7B-v0.3 Teuken-7B-base-v0.6 bloom-7b1 bueble-lm-2b occiglot-7b-eu5 salamandra-7b Llama-GENBA-10B-base 0.3840 0.3256 0.3769 0.3034 0.4750 0.3888 0. 0.4235 0.3771 0.2593 0.3585 0.3459 0.2282 0.2482 0.3545 0.3754 0.3776 0.4679 0.4213 0.4497 0.4016 0.5032 0.4526 0.4737 0.4949 0.4718 0.3951 0.4483 0.4528 0.3449 0.3401 0.4658 0.4605 0.4792 0.5322 0.4362 0.6229 0.3484 0.5992 0.4893 0.4702 0.5107 0.4615 0.2359 0.4428 0.3910 0.2500 0.2536 0.3861 0.3494 0.4631 0.3528 0.3331 0.4043 0.3262 0.3425 0.3213 0. 0.3831 0.3737 0.3104 0.3382 0.3140 0.3088 0.3514 0.3317 0.3351 0.3729 0.7380 0.7459 0.7269 0.6811 0.7411 0.6914 0.7443 0.6890 0.6946 0.5722 0.7348 0.6914 0.6440 0.5454 0.6930 0.6875 0.7364 Table 6: Comparison of Llama-GENBA-10B-base model versus other LLMs on core benchmarks (ARC, HellaSwag, MMLU, TruthfulQA, Winogrande). Llama-GENBA-10B-base shows balanced performance across tasks, with particularly strong Winogrande results nearly matching top-performing systems. Figure 4: Performance comparison of Llama-GENBA-10B-instruct against peer instruction-tuned models, showing GENBA-10B-instruct as the best-performing model in Bavarian. and granite-3.0-8B-base. However, it outperforms several other models, particularly those developed in Europe, including EuroLLM-9B, Teuken-7Bbase-v0.6, SauerkrautLM-Gemma-2B, bueble-lm2b, LLaMmlein-7B, and bloom-7B1. The reported aggregated accuracies are derived from individual task-specific results (HellaSwag, MMLU, TruthfulQA, and Winogrande), with further details provided in Table 6 . It On English benchmarks, Llama-GENBA-10Bbase performs comparatively strongly, placing close to the top-performing models. lags only slightly behind Qwen2.5-7B, gemma-2-9B, Granite-3.0-8B, Apertus-8B-2509, Llama3.1-8B and Olmo-2-1124-7B but achieves higher accuracy than Mistral-7B-v0.3, EuroLLM-9B, and other European-developed baselines. This suggests that Llama-GENBA-10B-base has solid foundation in English, though it is not yet at the level of state-of-the-art 7B9B models. In German, Llama-GENBA-10B-base performs competitively, with accuracy levels close to those of mid-tier models. It trails gemma-29B, Apertus-8B, Qwen2.5-7B, and EuroLLM9B, but surpasses several European-built models such as Teuken-7B-base-v0.6, SauerkrautLLMGemma-2B, and bueble-lm-2b. On Bavarian, Llama-GENBA-10B-base perranking among the top forms relatively well, models. It achieves accuracy close to leading systems such as gemma-2-9b, Apertus-8B-2509, and Llama-3.1-8B, while surpassing Qwen2.57B and EuroLLM-9B. This result suggests that Llama-GENBA-10B-base generalizes effectively to this dialect, even outperforming models such as EuroLLM-9B or Teuken-7B-base-v0.6. As key take away, Llama-GENBA-10B-base shows balanced performance across languages, with strong competitiveness in English, moderate ability in German, and surprisingly strong generalization in Bavarian. Its relative success on Bavarian suggests robustness in handling low-resource settings, which may stem from its training data composition or design choices. benchmarking. As shown in Figure 4, finetuning substantially improves the models performance across benchmarks. Compared to its base version, the instruction-tuned variant (Llama-GENBA-10B-Instruct) achieves stronger aggregate accuracy, placing close to leading mid-sized models such as gemma-2-9B-it and Qwen2.5-7B-Instruct. In English and German tasks, Llama-GENBA-10B-Instruct secures mid-tier position while it trails top performers like gemma-2-9B-it, Apertus-8B-Instruct-2509, and Qwen2.5-7B-Instruct, it consistently outperforms several instruction-tuned baselines, including salamandra-7B-instruct and Teuken-7Binstruct-research-v0.4. The most striking gain is observed on Bavarian tasks. Whereas the base Llama-GENBA-10Bbase ranked among the top five models, LlamaGENBA-10B-instruct advances to the top posiintion, outperforming all evaluated systems, cluding gemma-2-9B-it and Apertus-8B-Instruct2509. This shift underscores the effectiveness of instruction tuning for low-resource and dialectal setting, where general-purpose pretraining alone is insufficient. Together, these results demonstrate that LlamaGENBA-10B-instruct benefits substantially from instruction tuning, with the fine-tuned variant surpassing Apertus-8B-Instruct-2509 and gemma-29B-it in Bavarian and establishing itself as the best model in its class for this language, while also outperforming EuroLLM-9B-instruct in English and matching its results in German."
        },
        {
            "title": "5.3 Energy Usage of Pre-Training",
            "content": "The pretraining of the Llama-GENBA-10B-base model consumed approximately 35.23 megawatthours (MWh) of electricity over period of 66 days, using the Cerebras CS-2 system (See Table 7). This amount of energy is roughly equivalent to the annual electricity usage of ten average European households, based on 2022 Eurostat data (Eurostat, 2024)7."
        },
        {
            "title": "5.2 Evaluation of the Fine-Tuned Model",
            "content": "After evaluating Llama-GENBA-10B-bases performance across pretraining, we compared the models final fine-tuned version (as described in Section 4.4) with other models using the same benchmark datasets as for the pretraining We present Llama-GENBA-10B, trilingual foundation model comprising both base version (Llama-GENBA-10B-base) and an instruction- (LLama-GENBA-10B-instruct). tuned variant 7Based on 2022 Eurostat data, average EU household electricity use was about 3.6 MWh per year. Metric CS-"
        },
        {
            "title": "References",
            "content": "Average power draw Training duration Total energy consumption 35.23 MWh 22.3 kW 66 days Table 7: Estimated energy usage during pretraining. Built on Llama3.1-8B and scaled to 10B parameters, GENBA-10B is pretrained on 164B tokens distributed evenly between English and German, complemented by 80M tokens of Bavarian. This design mitigates English dominance, strengthens resources for the German NLP community, and promotes Bavarian as low-resource language. Model development addressed four central challenges: (i) constructing balanced multilingual corpus, (ii) designing unified tokenizer, (iii) optimizing architectural and data ratios for cross-lingual transfer, and (iv) creating the first trilingual evaluation suite by translating German benchmarks into Bavarian. Our evaluation demonstrates that the LlamaGENBA-10B-base model achieves competitive performance, with strong results in English, moderate outcomes in German, and state-of-the-art performance in Bavarian. The instruction-tuned variant further improves upon this, surpassing both Apertus-8B-instruct-2509 and gemma-2-9Bit in Bavarian, thereby establishing new benchIn addition, it outpermark for this language. forms EuroLLM-9B in English and reaches parity in German. Furthermore, our experiments on the Cerebras CS-2 confirm that large-scale multilingual pretraining is feasible for small research teams, with the added benefit of real-time energy monitoring. Taken together, Llama-GENBA-10B provides blueprint for building linguistically inclusive and resource-efficient foundation models. Future work could focus on instruction-tuning safety through harm risk categorization and detection mechanisms. We may integrate this model into chatbot for manual evaluation and conduct systematic human assessment of outputs across all three languages. More broadly, this multilingual LLM development approach on the Cerebras CS2 System could be applied to other dialects and endangered languages, potentially enabling new downstream applications. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, arXiv et al. 2023. Gpt-4 technical report. preprint arXiv:2303.08774. Mehdi Ali, Michael Fromm, Klaudia Thellmann, Jan Ebert, Alexander Arno Weber, Richard Rutmann, Charvi Jain, Max Lübbering, Daniel Steinigen, Johannes Leveling, et al. 2024a. Teuken-7b-base & teuken-7binstruct: Towards european llms. arXiv preprint arXiv:2410.03730. Mehdi Ali, Michael Fromm, Klaudia Thellmann, Richard Rutmann, Max Lübbering, Johannes Leveling, Katrin Klug, Jan Ebert, Niclas Doll, Tokenizer Jasper Buschhoff, et al. 2024b. choice for llm training: Negligible or crucial? In Findings of the Association for Computational Linguistics: NAACL 2024, pages 3907 3924. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2023. Llemma: An open language model for mathematics. Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. 2024. Cosmopedia. Verena Blaschke, Barbara Kovaˇcic, Siyao Peng, Hinrich Schütze, and Barbara Plank. 2024. Maibaam: multi-dialectal bavarian univerarXiv preprint sal dependency treebank. arXiv:2403.10293. Verena Blaschke, Miriam Winkler, Constantin Förster, Gabriele Wenger-Glemser, and Barbara Plank. 2025. multi-dialectal dataset for german dialect asr and dialect-to-standard speech translation. arXiv preprint arXiv:2506.02894. Branden Chan, Stefan Schweter, and Timo Möller. 2020. Germans next language model. arXiv preprint arXiv:2010.10906. Monojit Choudhury, Shivam Chauhan, Rocktim Jyoti Das, Dhruv Sahnan, Xudong Han, Haonan Li, Aaryamonvikram Singh, Alok Anil Jadhav, Utkarsh Agarwal, Mukund Choudhary, Llama-3-nanda-10b-chat: An et al. 2025. open generative large language model for hindi. arXiv preprint arXiv:2504.06011. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Think you and Oyvind Tafjord. 2018. try arc, have solved question answering? arXiv preprint the ai2 reasoning challenge. arXiv:1803.05457. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261. Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan Rossi, and Thien Huu Nguyen. 2023. Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback. arXiv e-prints, pages arXiv2307. Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. 2021. continual learning survey: Defying forgetIEEE transactions ting in classification tasks. on pattern analysis and machine intelligence, 44(7):33663385. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Eurostat. 2024. Electricity and heat statistics. Scientific Reports. Zhaoye Fei, Yunfan Shao, Linyang Li, Zhiyuan Zeng, Conghui He, Hang Yan, Dahua Lin, and Xipeng Qiu. 2024. Query of cc: unearthing large scale domain-specific knowlarXiv preprint edge from public corpora. arXiv:2401.14624. Pablo Gamallo, Pablo Rodríguez, Daniel Santos, Susana Sotelo, Nuno Miquelina, Silvia Paniagua, Daniela Schmidt, Iria de Dios-Flores, Paulo Quaresma, Daniel Bardanca, et al. 2024. In galician-portuguese generative model. EPIA Conference on Artificial Intelligence, pages 292304. Springer. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2024. The language model evaluation harness. Gurpreet Gosal, Yishi Xu, Gokul Ramakrishnan, Rituraj Joshi, Avraham Sheinin, Biswajit Mishra, Natalia Vassilieva, Joel Hestness, Neha Sengupta, Sunil Kumar Sahu, et al. 2024. Bilingual adaptation of monolingual foundation models. arXiv preprint arXiv:2407.12869. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759. Amir Hossein Kargaran, Ayyoob Imani, François Yvon, and Hinrich Schütze. 2023. Glotlid: Language identification for low-resource languages. arXiv preprint arXiv:2310.16248. Wen Lai, Mohsen Mesgar, and Alexander Fraser. 2024. Llms beyond english: Scaling the multilingual capability of llms with cross-lingual feedback. arXiv preprint arXiv:2406.01771. Sean Lie. 2024. Inside the cerebras wafer-scale cluster. IEEE Micro, 44(3):4957. 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958. Pedro Henrique Martins, João Alves, Patrick Fernandes, Nuno Guerreiro, Ricardo Rei, Amin Farajian, Mateusz Klimaszewski, Duarte Alves, José Pombal, Manuel Faysse, et al. 2025. Eurollm-9b: Technical report. arXiv preprint arXiv:2506.04079. Jonas Pfeiffer, Naman Goyal, Xi Victoria Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe. 2022. Lifting the curse of multilinguality by pre-training modular transformers. arXiv preprint arXiv:2205.06266. Jan Pfister, Julia Wunderle, and Andreas Hotho. 2024. Ll\" ammlein: Compact and competitive german-only language models from scratch. arXiv preprint arXiv:2411.11171. Parker Riley, Isaac Caswell, Markus Freitag, and David Grangier. 2019. Translationese as lanarXiv preprint guage in\" multilingual\" nmt. arXiv:1911.03823. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99106. Ahmet Üstün, Viraat Aryabumi, Zheng Yong, Wei-Yin Ko, Daniel Dsouza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, HuiLee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. 2024. Aya model: An instruction finetuned open-access multilingual language model. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1589415939, Bangkok, Thailand. Association for Computational Linguistics. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Zirui Wang, Zachary Lipton, and YuOn negative interferlia Tsvetkov. 2020. ence in multilingual models: Findings and arXiv preprint meta-learning treatment. arXiv:2010.03017. Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, et al. 2023. Polylm: An open source polyglot large language model. arXiv preprint arXiv:2307.06018. Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, William Marshall, Gurpreet Gosal, Cynthia Liu, Zhiming Chen, et al. 2023. Jais and jais-chat: Arabic-centric foundation and instruction-tuned open generative large language models. arXiv preprint arXiv:2308.16149. BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. 2022. Bloom: 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ying Shan, and Llama pro: Progressive Ping Luo. 2024. arXiv preprint llama with block expansion. arXiv:2401.02415. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830."
        }
    ],
    "affiliations": [
        "Cerebras Systems Sunnyvale, USA",
        "Independent Researcher Holzkirchen, Germany",
        "Leibniz Supercomputing Centre (LRZ) Garching, Germany"
    ]
}