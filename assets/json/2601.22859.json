{
    "paper_title": "MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering",
    "authors": [
        "Chuanzhe Guo",
        "Jingjing Wu",
        "Sijun He",
        "Yang Chen",
        "Zhaoqi Kuang",
        "Shilong Fan",
        "Bingjin Chen",
        "Siqi Bao",
        "Jing Liu",
        "Hua Wu",
        "Qingfu Zhu",
        "Wanxiang Che",
        "Haifeng Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The evolution of Large Language Model (LLM) agents for software engineering (SWE) is constrained by the scarcity of verifiable datasets, a bottleneck stemming from the complexity of constructing executable environments across diverse languages. To address this, we introduce MEnvAgent, a Multi-language framework for automated Environment construction that facilitates scalable generation of verifiable task instances. MEnvAgent employs a multi-agent Planning-Execution-Verification architecture to autonomously resolve construction failures and integrates a novel Environment Reuse Mechanism that reduces computational overhead by incrementally patching historical environments. Evaluations on MEnvBench, a new benchmark comprising 1,000 tasks across 10 languages, demonstrate that MEnvAgent outperforms baselines, improving Fail-to-Pass (F2P) rates by 8.6% while reducing time costs by 43%. Additionally, we demonstrate the utility of MEnvAgent by constructing MEnvData-SWE, the largest open-source polyglot dataset of realistic verifiable Docker environments to date, alongside solution trajectories that enable consistent performance gains on SWE tasks across a wide range of models. Our code, benchmark, and dataset are available at https://github.com/ernie-research/MEnvAgent."
        },
        {
            "title": "Start",
            "content": "MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering Chuanzhe Guo * 1 2 Jingjing Wu * 2 Sijun He 2 Yang Chen 2 Zhaoqi Kuang 2 Shilong Fan 2 Bingjin Chen 2 Siqi Bao 2 Jing Liu 2 Hua Wu 2 Qingfu Zhu 1 Wanxiang Che 1 Haifeng Wang"
        },
        {
            "title": "Abstract",
            "content": "The evolution of Large Language Model (LLM) agents for software engineering (SWE) is constrained by the scarcity of verifiable datasets, bottleneck stemming from the complexity of constructing executable environments across diverse languages. To address this, we introduce MEnvAgent, Multi-language framework for automated Environment construction that facilitates scalable generation of verifiable task instances. MEnvAgent employs multi-agent Planning-Execution-Verification architecture to autonomously resolve construction failures and integrates novel Environment Reuse Mechanism that reduces computational overhead by incrementally patching historical environments. Evaluations on MEnvBench, new benchmark comprising 1,000 tasks across 10 languages, demonstrate that MEnvAgent outperforms baselines, improving Fail-to-Pass (F2P) rates by 8.6% while reducing time costs by 43%. Additionally, we demonstrate the utility of MEnvAgent by constructing MEnvData-SWE, the largest open-source polyglot dataset of realistic verifiable Docker environments to date, alongside solution trajectories that enable consistent performance gains on SWE tasks across wide range of models. Our code, benchmark, and dataset are available at GitHub. 6 2 0 2 2 ] . [ 2 9 5 8 2 2 . 1 0 6 2 : r 1. Introduction The rapid evolution of Large Language Models (LLMs) has significantly advanced the exploration of repository-level code modification tasks within software engineering. Realworld issue resolution benchmarks, such as SWE-bench 1Research Center for Social Computing and Interactive Robotics, Harbin Institute of Technology, Harbin, China 2Baidu Inc., Shenzhen, China. Correspondence to: Chuanzhe Guo <czguo@ir.hit.edu.cn>, Jingjing Wu <wujingjing05@wbaidu.com>, Siqi Bao <baosiqi@wbaidu.com>. Preprint. February 3, 2026. 1 Figure 1. Comparison between manual environment construction and MEnvAgent (Ours). MEnvAgent leverages multi-agent collaboration to achieve automated environment construction, characterized by an efficient environment reuse mechanism. and its variants (Jimenez et al., 2024; Yang et al., 2025c; Zan et al., 2025), have emerged as the standard for evaluating the coding capabilities of LLMs. In these settings, autonomous agents like OpenHands (Wang et al., 2025b) and SWE-Agent (Yang et al., 2024) are tasked with exploring repositories, localizing issues, generating patches (Pull Requests), and executing tests to validate solutions. This execution-based verification is pivotal, not only for evaluation but also for emerging training paradigms like Reinforcement Learning with Verifiable Rewards (RLVR) (Wen et al., 2025). However, the efficacy of such methods is constrained by the scalability of executable environment construction. Consequently, existing efforts face dilemma: approaches based on static code metrics (Xie et al., 2025; Wei et al., 2025) scale efficiently but provide only approximate verification signals, while manual construction (Pan et al., 2025) ensures quality but remains labor-intensive and largely restricted to Python. This leaves critical gap for scalable, verifiable support across diverse programming languages. MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering To bridge this gap, we introduce MEnvAgent, an automated framework engineered for scalable, polyglot environment construction (see Figure 1). Our approach aims to address two fundamental challenges in this field: (1) Complexity. Managing diverse dependencies across non-standard repositories requires deep expertise. Frequent construction failures (e.g., version conflicts, compilation errors) and inconsistent testing protocols (e.g., pytest or mvn test) often lead to low success rates. (2) Time Consumption. The build process is inherently slow due to installation and compilation steps. Furthermore, environments are fragile; single error often necessitates costly clean-slate restart, creating prohibitive overhead for large-scale data expansion. To tackle the complexity, we design multi-agent architecture featuring an iterative Planning-Execution-Verification closed loop. Within this loop, specialized agents fulfill distinct responsibilities to iteratively diagnose and autonomously resolve construction failures to ensure high success rates. To address the time consumption, we propose novel Environment Reuse Mechanism. Instead of building every instance from scratch, this mechanism retrieves similar historical environments and adapts them to the target repository snapshot by synthesizing and executing incremental environment patches. This approach avoids the heavy cost of full rebuilds, thereby boosting efficiency. Current environment construction benchmarks (Milliken et al., 2025; Eliseeva et al., 2025; Guo et al., 2025) are limited by narrow language coverage, non-executable evaluation, or insufficient quality assurance. To address these limitations and rigorously evaluate our approach, we construct MEnvBench, comprehensive benchmark comprising 1,000 tasks across 10 languages, with strict executionbased evaluation and quality assurance. Extensive evaluations demonstrate that MEnvAgent outperforms state-of-theart baselines across all languages, improving Fail-to-Pass (F2P) rates by 8.6% while reducing time costs by 43%. Furthermore, we leverage MEnvAgent to scale up verifiable data construction, yielding MEnvData-SWE, realistic verifiable SWE training dataset. By fine-tuning open-source models on solution trajectories synthesized from this dataset, we achieve substantial performance gains on downstream SWE tasks, effectively validating the utility of MEnvAgent. The main contributions of this paper are as follows: We introduce MEnvAgent, multi-agent environment construction framework covering 10 programming languages, based on Planning-Execution-Verification architecture. Notably, it incorporates novel environment reuse mechanism that significantly reduces computational overhead. We present MEnvBench, the first comprehensive benchmark for evaluating multi-language executable environment construction. This benchmark covers 10 mainstream languages across 200 open-source repositories, comprising total of 1,000 tasks. We release MEnvData-SWE, the largest open-source polyglot dataset of realistic verifiable Docker environments to date (see Table 12). Constructed via MEnvAgent, this dataset enables consistent performance gains for LLMs on downstream SWE tasks. 2. Problem Formulation In this section, we define the task of environment construction for verifiable SWE datasets. verifiable task instance consists of two core components: Task Context and Executable Environment. The task context is gathered from GitHub, comprising the repository snapshot and the issue with the related pull-request (PR). From the PR, we extract two distinct code changes: the fix patch representing logic modifications to resolve the issue, and the test patch containing new test cases to verify the fix. Let Rf ix denote the repository state after applying the fix patch to R. Given task context, the objective of environment construction is to determine configuration triplet (B, P, ). Here, denotes the base image, represents the build process consisting of sequence of installation commands, and specifies the test configuration, which involves applying the test patch and executing the test command. The constructed environment is formally defined as = δ(B, P), where δ represents the transition function. The fundamental goal is executability. Specifically, the constructed environment must allow repository state Rf ix after applying the fix patch to pass the tests (PASS): ε(Rf ix, S, ) = 0 (1) Here, ε() = 0 denotes that the tests are successfully passed. However, executability alone is insufficient. To ensure its validity as verifiable environment, we enforce the Fail-toPass (F2P) criterion: ε(R, S, ) = 1 ε(Rf ix, S, ) = (2) This differential outcome guarantees that the environment accurately reproduces the specific issue (Fail) and verifies its resolution (Pass). (See Appendix for further details). 3. MEnvAgent Design In this section, we introduce the design of MEnvAgent, as illustrated in Figure 2. We elaborate on two key components: the multi-agent architecture designed to construct executable environments and resolve construction failures, and the Environment Reuse Mechanism developed to accelerate this process by adapting historical environments (see Appendix for specific details). 2 MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering Figure 2. Overview of MEnvAgent. (Top) The Environment Reuse Mechanism retrieves and adapts historical environments via incremental patching to reduce overhead. (Bottom) The Planning-Execution-Verification loop, where agents autonomously draft scripts, interactively repair build errors, and diagnose test failures to guide iterative refinement. 3.1. Multi-Agent Architecture The architecture of MEnvAgent is structured into three iterative stages: Planning, Execution, and Verification. Planning Stage. This stage is orchestrated by three specialized agents to formulate the blueprint for the environment. First, the Repository Analysis Agent explores the file structure and contents of the target repository, producing comprehensive summary of its project type, dependency requirements, and entry points. This summary is passed to the downstream agents. Next, the Environment Setup Agent determines the most suitable base image and generates complete environment installation script (denoted as building process P), which includes all necessary installation commands. Subsequently, the Test Configuration Agent analyzes the repository structure alongside the proposed installation script to synthesize compatible test configuration script (denoted as ), ensuring that the verification logic aligns with the environment setup. errors (e.g., missing packages or version conflicts). If the installation completes successfully, the workflow proceeds to the Verification Stage. However, if the agent fails to resolve installation errors after multiple attempts, the process aborts the current attempt and reverts to the Planning Stage to regenerate new build strategy. Verification Stage. The final stage verifies the correctness of the built environment and the test configuration . The Verification Agent executes the tests defined in within the container. If the tests pass (satisfying ε(Rf ix, S, ) = 0), the task is considered successful. If validation fails, the agent performs error attribution to diagnose whether the failure stems from missing environment dependency or an incorrect test command. This diagnostic feedback is propagated back to the Planning Stage to guide the agents in the next iteration. Finally, we verify the successful environment against the F2P criterion (Eq. 2) to confirm its validity as verifiable SWE environment. Execution Stage. Once the plan is established, the system transitions to execution. The Environment Execution Agent instantiates container based on the selected image and executes the commands in P. Crucially, this agent monitors the terminal output in real-time, capable of dynamically adjusting commands to resolve immediate execution 3.2. Environment Reuse Mechanism To reduce the computational overhead of deriving and executing the complete build process from raw base image B, we introduce an Environment Reuse Mechanism. We reformulate the problem as first identifying similar existing 3 MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering environment state Ssim from pool Spool that minimizes the expected adaptation effort Cadapt: Table 1. Comparison with existing benchmarks. Langs: Language count. Exec-Eval: Execution support. Quality: Quality Assurance. Domain: Domain diversity. Ssim = arg min SSpool Cadapt(S, R) (3) Once Ssim is retrieved, we employ an EnvPatchAgent to generate an incremental command sequence that adapts this environment to the target repository snapshot R. Formally, we seek an environment patch = EnvPatchAgent(R, Ssim) that transitions the retrieved environment to valid state Snew satisfying: Benchmark Langs # Repos # Tasks Exec-Eval Quality Domain INSTALLAMATIC EXECUTIONAGENT EnvBench Repo2Run-bench SweSetupBench-lite MEnvBench (Ours) 1 5 3 2 4 10 40 50 994 420 12 40 50 994 420 671 1000 4.1. Data Collection and Filtering ε(R, Snew, ) = 1 ε(Rf ix, Snew, ) = 0, where Snew = δ(Ssim, P) (4) Our data acquisition pipeline consists of two phases, transforming raw GitHub data into high-quality candidate pool. Our approach executes this mechanism through two stages: Environment Retrieval and Verification-Driven Adaptation. Environment Retrieval. We maintain an Environment Pool Spool containing previously verified environments. To approximate the optimal Ssim in Eq. 3, we employ hierarchical retrieval strategy grounded in software evolution patterns. First, we construct candidate set based on Version Consistency, prioritizing historical environments associated with the exact version of the target repository snapshot. If no exact match is found, we broaden the scope to include all historical environments belonging to the same repository. Subsequently, we leverage Backward Compatibility, premised on the observation that newer environments typically support older dependencies. Consequently, we select an environment state newer than the target repository snapshot yet temporally closest to minimize compatibility risks. Verification-Driven Adaptation. Once Ssim is retrieved, the EnvPatchAgent operates within feedback loop to generate P. The process commences with the Test Configuration Agent synthesizing the test script , which is subsequently executed within Ssim by the Verification Agent. If execution succeeds, the environment is reused directly. However, verification failure triggers the EnvPatchAgent to analyze the diagnostic feedback and synthesize incremental commands P. This iterative process continuously patches the environment to produce an updated state Snew, continuing until the success condition ε(Rf ix, Snew, ) = 0 is met (see Appendix C.2 for concrete case). 4. MEnvBench Construction To address the limitations of existing benchmarks (as compared in Table 1) and rigorously evaluate our framework, we construct MEnvBench following strict pipeline to ensure high quality, execution validity, and broad representativeness (see Appendix for details). 4 Phase 1: Repository Acquisition. We targeted highquality repositories across 10 mainstream programming languages. To minimize construction failures stemming from inherent code defects, we applied strict criteria: repositories must have (1) > 1, 000 stars, (2) > 200 forks, issues, PRs, and (3) primary language ratio of > 60%. This stage yielded candidate pool of 8,000 repositories. Phase 2: Instance Extraction & Quality Assurance. From these repositories, we extracted Issue-PR pairs spanning 20182025. We enforced strict quality controls, including retaining only closed issues explicitly linked to PR containing test patch and employing an LLM-based assessment to filter out low-quality issues (score < 5). This process yielded refined pool of 213,766 instances. 4.2. Benchmark Composition From the filtered candidate pool, we employed sampling strategy to construct MEnvBench, comprising 1,000 tasks (10 languages 20 repositories 5 instances selected from distinct historical versions). This allocation structure strikes strategic balance between inter-project breadth and intraproject depth: it ensures coverage of diverse repository types while capturing sufficient internal variability to verify build robustness. To ensure comprehensive representativeness, we selected repositories based on two key dimensions: Domain Diversity: We leveraged LLMs to classify repositories into specific domains (e.g., AI, System). Our sampling prioritizes wide coverage to ensure robustness across diverse software ecosystems (see Figure 10a). Project Scale: We sampled across five size bands (from <10MB to >500MB) to encompass full spectrum of difficulty levels, as repository size typically correlates with build complexity (see Figure 10b). MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering (a) F2P Rate (%) vs. Time Cost (s) (b) Pass Rate (%) vs. Time Cost (s) Figure 3. Performance trade-off analysis on MEnvBench. The x-axis represents the average time cost (lower is better), and the y-axis represents the pass rate (higher is better). MEnvAgent points cluster in the top-left region, indicating it achieves higher validity and success rates with significantly lower time consumption compared to baselines. 4.3. Evaluation Metrics Table 2. Averaged performance comparison across 10 languages. We employ three metrics to evaluate the performance of our framework: Method MEnvBench F2P (%) PASS (%) TIME (s) Pass Rate (PASS): The percentage of tasks satisfying the Executability condition (Eq. 1). Fail-to-Pass Rate (F2P): The percentage of tasks satisfying the strict Validity criterion (Eq. 2). Time Cost (TIME): The average wall-clock time consumed per task, reflecting the efficiency of the environment construction process. 5. Experiment We design our experiments to answer the primary research question: How does MEnvAgent compare to state-of-the-art baselines in terms of environment construction success rates and computational efficiency across diverse programming languages on MEnvBench? Model Details. To assess the robustness and generalization of our framework, we employ two representative Large Language Models (LLMs) as the reasoning backbone for the agents. For the open-source model, we select Kimi-K2 (kimi-k2-0905-preview) (Team et al., 2025), which demonstrates superior capability in agentic planning and longcontext understanding. For the closed-source model, we utilize Gemini-3-Flash (Google, 2025). We selected this model because it represents the latest state-of-the-art capabilities while maintaining low latency and high cost-efficiency, which are critical prerequisites for scalable environment construction scenarios. Baseline Methods. We compare MEnvAgent against three categories of baselines: (1) Repo2Run (Hu et al., 2025), Python-specialized tool evaluated exclusively on the Python subset due to its extensibility constraints; (2) SWE-Bench-Live (Zhang et al., 2025a), which supports 6 of the languages in MEnvBench, allowing for multiKimi-K2 SWE-Factory MEnvAgent 26.2 35.7 (+9.5) 34.5 45.9 (+11.4) 6356 3339 (-47.5%) Gemini-3-Flash SWE-Factory MEnvAgent 33.3 41.1 (+7.8) 41.5 52.0 (+10.5) 6175 3808 (-38.3%) Average SWE-Factory MEnvAgent 29.8 38.4 (+8.6) 38.0 49.0 (+11.0) 6266 3574 (-43.0%) language sub-evaluation; and (3) SWE-Factory (Guo et al., 2025), state-of-the-art agent framework. We evaluated this baseline across all 10 languages in MEnvBench. For detailed hyperparameter settings, please refer to Appendix E. Results on MEnvBench. We evaluate the overall effectiveness and efficiency of our framework on MEnvBench. The aggregated metrics are presented in Table 2, and the efficiency-quality trade-off is visualized in Figure 3. In the scatter plots, the x-axis represents the average time cost, while the y-axis denotes the Pass Rate or F2P Rate. As illustrated, MEnvAgent consistently occupies the upperleft quadrant across all backbone models and programming languages. This distribution signifies an optimal performance state simultaneously achieving the highest validity while minimizing computational overhead. In contrast, the baselines exhibit distinct limitations: SWE-Factory is predominantly distributed in the right-hand region, indicating that while it achieves competitive runnability, it suffers from excessive latency due to inefficient trial-and-error loops. Meanwhile, Repo2Run and SWE-Bench-Live cluster in the lower region, where despite maintaining acceptable efficiency, their capability to generate valid environments is significantly compromised. This visual superiority is 5 MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering (a) Reuse Success Rate (b) Time Cost (c) Pass Rate Figure 4. Impact of data scale on performance metrics. We illustrate the trends of (a) Reuse Success Rate, (b) Time Cost, and (c) Pass Rate as the number of instances per repository increases from 1 to 10. The results confirm that larger data scale significantly enhances reuse probability and overall efficiency. quantitatively corroborated by Table 2, which compares our method directly against the strongest baseline, SWE-Factory. Averaged across models, MEnvAgent improves the strict F2P Rate by 8.6% and the Pass Rate by 11.0%, while simultaneously reducing time costs by 43.0%. For complete per-language statistics and granular comparisons, we refer readers to Table 9 in Appendix F. Table 3. Ablation results on component effectiveness (10 instances per repository). RSR denotes the Reuse Success Rate. Method RSR (%) PASS (%) TIME (s) MEnvAgent w/o EnvPatchAgent w/o Reuse 39.0 25.0 0.0 59.0 52.0 40. 2314 2777 4283 6. Analysis 6.1. Ablation Study of Environment Reuse To validate the effectiveness of the Environment Reuse mechanism and the critical role of the EnvPatchAgent, we conduct comprehensive ablation study. Furthermore, we investigate how the data scale (the number of historical instances per repository) influences reuse performance. Experimental Setup. To isolate the contribution of each component, we evaluate our framework against two ablated variants: (1) MEnvAgent (Full), our complete framework incorporating both the Environment Retrieval and the EnvPatchAgent; (2) w/o EnvPatchAgent (Direct), variant that retrieves the most similar environment Ssim but applies it directly without modification, validating the necessity of the patching mechanism; and (3) w/o Reuse (Scratch), the minimal baseline that disables the reuse mechanism entirely and builds every task from the base image. To measure reuse efficacy, we employ the Reuse Success Rate (RSR), defined as the proportion of tasks successfully verified via the reuse pathway without falling back to the scratch build. All ablation experiments are conducted using Kimi-K2 on Python subset from MEnvBench, with the data scale extended to 10 instances per repository. Component Effectiveness. We analyze the contribution of each component in Table 3. MEnvAgent achieves optimal performance across all metrics, validating the synergy between retrieval and patching. In terms of efficiency and reuse stability, removing the EnvPatchAgent (w/o EnvPatchAgent) causes the Reuse Success Rate to drop significantly from 39.0% to 25.0%, leading to 20% increase in time cost due to frequent fallbacks to scratch builds. Furthermore, compared to the baseline without reuse (w/o Reuse), our full framework reduces the average computational time by 46.0%. Crucially, beyond these efficiency gains, MEnvAgent significantly boosts the overall Pass Rate by 18.5% compared to the w/o Reuse baseline. This improvement stems from the reuse mechanism, which avoids the error-prone process of resolving complex dependencies from scratch. Impact of Data Scale. We further investigate how the volume of historical data influences performance by scaling the number of instances per repository from 1 to 10, as shown in Figure 4. In sparse data settings (1 instance), the Reuse Success Rate is negligible, resulting in performance similar to the scratch baseline. However, as the data scale expands to 10 instances, the Reuse Success Rate rises steadily to 39%, driving concurrent improvements in Pass Rate and corresponding reductions in Time Cost. This trend highlights the scalability of our approach, suggesting that in real-world scenarios characterized by large-scale data accumulation, the framework is poised to deliver even greater efficiency gains. 6 MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering Figure 5. F2P performance analysis relative to repository size. 6.2. In-depth Result Analysis Performance vs. Repository Scale. Figure 5 analyzes the correlation between model performance and repository characteristics. We observe significant negative correlation between Fail-to-Pass (F2P) rates and repository size. This trend is attributable to the intricate dependency graphs and substantial build overheads inherent in large-scale projects, which exacerbate the complexity of automated environment configuration. Error Distribution and Behavioral Patterns. Figure 6 categorizes task outcomes for Kimi-K2 and Gemini-3-Flash into four states: Fail-to-Pass (F2P), Pass-to-Pass (P2P), Test Execution Failure, and Environment Setup Failure. Our analysis reveals significant cross-language performance disparity. Modern languages with standardized package ecosystems, such as Go and Python, demonstrate high resolution rates (F2P), indicating that current LLMs effectively handle dependency management. In contrast, discrepancies emerge in complex ecosystems like Java. Gemini-3-Flash exhibits superior robustness in environment setup, consistently maintaining lower setup failure rates compared to Kimi-K2 across most languages. This advantage is most pronounced in Java, where Gemini reduces the setup failure rate by nearly half relative to Kimi, suggesting better generalization in generating intricate build scripts (e.g., Maven/Gradle configurations). Conversely, C-family languages (C/C++) are dominated by compilation errors derived from complex CMake configurations and high resource consumption, which frequently lead to timeouts. These diverse failure patterns underscore the necessity of the Verification Agent within the MEnvAgent framework to enable precise error attribution and iterative refinement beyond initial setup. Figure 6. Error distribution across 10 programming languages. experiment workflow is as follows: First, we leverage MEnvAgent to establish fully automated pipeline to scale up the construction of verifiable software engineering tasks from real-world GitHub repositories. Through this pipeline, we construct MEnvData-SWE, diverse dataset comprising 3,005 task instances from 942 repositories across 10 programming languages, all equipped with executable environments. Next, we deploy an agent framework with an expert model on MEnvData-SWE to collect solution trajectories. Finally, we fine-tune the student model on 3,872 trajectories derived from resolved instances and evaluate them on separate benchmarks (see Appendix for details). Models. For the expert model, we employ Claude-4.5Sonnet (Anthropic, 2025), representing the state-of-the-art in coding capabilities. For student models, we select diverse array of architectures to verify robustness: the dense Qwen-2.5-Coder-Instruct series (7B, 14B, and 32B) (Hui et al., 2024), the MoE-based Qwen-3-Coder-30B-A3BInstruct, and GLM-4.5-Air (Zeng et al., 2025). Training details are provided in Appendix G.6. 6.3. Scaling Verifiable SWE Datasets via MEnvAgent To validate the utility of MEnvAgent for training software engineering agents, we employ rejection sampling finetuning as the primary procedure for improving base LLMs, following the methodology established in prior works (Yang et al., 2025c; Guo et al., 2025; Wang et al., 2025a). Our Agent Scaffolding. We use OpenHands (Wang et al., 2025b), an event-driven framework that provides sandboxed environment where agents interact with the codebase by editing files (str-replace-editor), executing shell commands (via execute-bash), or submitting the task (via finish). We selected OpenHands as it has established strong baselines on benchmarks like SWE-bench. 7 MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering Table 4. Performance on SWE-bench Verified and SWE-bench Multilingual. Performance is measured by Resolved Rate (%). Category Model SWE-bench Verified SWE-bench Multilingual Reference Models GPT-4.1 Claude-4.5-Sonnet Our Fine-tuned Models Qwen2.5-Coder-7B-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Coder-32B-Instruct Qwen3-Coder-30B-A3B-Instruct GLM-4.5-Air Baseline SFT Baseline SFT 54.6 77. 0.0 5.8 7.5 45.2 58.0 - - 21.8 (+21.8) 39.8 (+34.0) 54.6 (+47.1) 53.4 (+8.2) 62.8 (+4.8) 31.5 68.0 0.0 0.0 0.0 34.7 42.3 - - 12.3 (+12.3) 31.2 (+31.2) 38.3 (+38.3) 38.0 (+3.3) 47.7 (+5.4) Evaluation Benchmarks. We evaluate on SWE-bench Verified (Chowdhury et al., 2024) (500 curated Python tasks) and SWE-bench Multilingual (Yang et al., 2025c) (encompassing 9 languages), reporting the Resolved Rate (%). It is worth noting that we rectified git log manipulation issue to prevent potential data leakage1. Consequently, our reported scores may be slightly lower than the official baselines due to this more rigorous evaluation setting. fixed test commands that do not execute verification tests. Similarly, SWE-Bench-Live (Zhang et al., 2025a) extends the task scope to encompass both environment setup and test configuration, utilizing single-agent method via interactive bash sessions. In contrast, SWE-Factory (Guo et al., 2025) further broadens applicability by supporting four programming languages, introducing collaborative multi-agent architecture for automated environment construction. Results. Table 4 demonstrates that fine-tuning on our dataset consistently boosts performance across all models. Within the Qwen2.5-Coder series, we observe positive correlation between model scale and performance gains. Strikingly, Qwen2.5-Coder-32B matches GPT-4.1 (OpenAI, 2025) on SWE-bench Verified and significantly outperforms it on the Multilingual benchmark. Furthermore, we achieve substantial gains even on MoE baselines (Qwen3-Coder and GLM-4.5-Air) that have already been heavily optimized with agentic data. This confirms that scaling verifiable data effectively pushes the performance boundaries of SOTA models, strongly validating the utility of MEnvAgent. 7. Related Work Automated Environment Construction. Early attempts at automated environment setup primarily relied on static heuristics to infer dependencies from source code, offering determinism but struggling with complex configurations and version incompatibilities (Gruber & Fraser, 2023; Zhang et al., 2024; Yang et al., 2025b). With the rapid evolution of Large Language Models (LLMs), series of LLM-based automated approaches have emerged to address these limitations (Bouzenia & Pradel, 2025; Milliken et al., 2025; Vergopoulos et al., 2025; Badertdinov et al., 2025). Among the works most relevant to ours, Repo2Run (Hu et al., 2025) employs dual-agent framework tailored with Python-specific tools, focusing exclusively on environment installation via 1https://github.com/SWE-bench/SWE-bench/issues/465 Environment Construction Benchmarks. Initial efforts evaluated environment construction capability implicitly within comprehensive tasks (Bogin et al., 2024; Siegel et al., 2024; Tang et al., 2025), offering little insight into the isolated challenges LLMs face during the construction phase. To explicitly assess this capability, dedicated benchmarks emerged: INSTALLAMATIC (Milliken et al., 2025) and EXECUTIONAGENT (Bouzenia & Pradel, 2025) established rigorous execution-based standards but remained small-scale. Conversely, EnvBench (Eliseeva et al., 2025) and Repo2Run-bench (Hu et al., 2025) scaled up data but relied on approximate evaluation metrics like static compilation checks or test collection, which often fail to detect runtime incompatibilities essential for robust agent feedback. To further enhance diagnostic depth, EnCondaBench (Kuang et al., 2025) introduces process-level diagnostics, yet it remains restricted to Python and rigid configuration patterns. Notably, SweSetupBench-lite (Guo et al., 2025) aligns evaluation with realistic software evolution by supporting historical repository snapshots and adopting metrics like Fail-to-Pass (F2P) rates and efficiency costs. While these features suit scalable evaluation scenarios and capture dependency drift, the representativeness of the benchmark is hindered by limited scope of just 12 repositories. 8. Conclusion In this paper, we introduced MEnvAgent, polyglot framework that automates the complex task of environment construction through multi-agent Planning-ExecutionVerification architecture. Notably, it incorporates novel MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering environment reuse mechanism that significantly reduces computational overhead. We rigorously evaluated our framework on MEnvBench, new benchmark comprising 1,000 tasks across 10 programming languages, where MEnvAgent demonstrated superior performance, achieving significantly higher F2P rates and lower time costs compared to state-ofthe-art baselines. Furthermore, to validate the utility of our approach, we leveraged MEnvAgent to construct MEnvDataSWE. Experiments demonstrate that models fine-tuned on this dataset achieve substantial performance gains on SWE tasks. Finally, we open-source our code, benchmark, and dataset to facilitate future research in the community. 9. Impact Statement We expect MEnvAgent to offer significant advantages by automating the traditionally labor-intensive task of environment construction, thereby lowering the barrier for researchers to conduct large-scale, polyglot software engineering studies. By scaling up execution-verified data construction across 10 programming languages, our framework empowers the community to develop more robust and versatile coding agents. However, we acknowledge the potential risks associated with automated environment setup and code execution. Malicious actors could potentially exploit the framework to construct environments for developing harmful software or executing unauthorized scripts. Furthermore, LLMs may inevitably generate erroneous commands that could lead to severe consequences. To address this inherent risk, our framework executes all tasks within strictly isolated Docker sandbox environment by default. We strongly recommend that users adhere to this default configuration to ensure system safety and isolation. Finally, all datasets and models utilized in this study are open-source and adhere strictly to their respective licenses. We hope our findings and contributions will catalyze future research in this field and foster the responsible advancement of AI technologies within the software engineering domain."
        },
        {
            "title": "References",
            "content": "Anthropic. Introducing Claude 4.5 Sonnet. Anthropic Blog, https://www.anthropic.com/ news/claude-sonnet-4-5, 2025. Badertdinov, I., Golubev, A., Nekrashevich, M., Shevtsov, A., Karasik, S., Andriushchenko, A., Trofimova, M., Litvintseva, D., and Yangel, B. Swe-rebench: An automated pipeline for task collection and decontaminated evaluation of software engineering agents. arXiv preprint arXiv:2505.20411, 2025. tasks from research repositories. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1262212645, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main. 702. URL https://aclanthology.org/2024. emnlp-main.702/. Bouzenia, I. and Pradel, M. You name it, run it: An llm agent to execute tests of arbitrary projects. Proceedings of the ACM on Software Engineering, 2(ISSTA):10541076, 2025. Chowdhury, N., Aung, J., Shern, C. J., Jaffe, O., Sherburn, D., Starace, G., Mays, E., Dias, R., Aljubeh, M., Glaese, M., Jimenez, C. E., Yang, J., Ho, L., Patwardhan, T., Liu, K., and Madry, A. Introducing SWE-bench verified. OpenAI Blog, https://openai.com/index/ introducing-swe-bench-verified/, August 2024. Eliseeva, A., Kovrigin, A., Kholkin, I., Bogomolov, E., and Zharov, Y. Envbench: benchmark for autoIn ICLR 2025 Third Workmated environment setup. shop on Deep Learning for Code, 2025. URL https: //openreview.net/forum?id=izy1oaAOeX. Google. Gemini 3 flash: Frontier intelligence built for Google Blog, https://blog.google/ speed. products-and-platforms/products/ gemini/gemini-3-flash/, 2025. Gruber, M. and Fraser, G. Flapy: Mining flaky python tests at scale. In 2023 IEEE/ACM 45th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion), pp. 127131. IEEE, 2023. Guo, L., Wang, Y., Li, C., Yang, P., Chen, J., Tao, W., Zou, Y., Tang, D., and Zheng, Z. Swe-factory: Your automated factory for issue resolution training data and evaluation benchmarks. arXiv preprint arXiv:2506.10954, 2025. He, X., Liu, Q., Du, M., Yan, L., Fan, Z., Huang, Y., Yuan, Z., and Ma, Z. SWE-perf: Can language models optimize code performance on real-world repositories?, 2025. URL https://arxiv.org/abs/2507.12415. Hu, R., Peng, C., Xu, J., Gao, C., et al. Repo2run: Automated building executable environment for code repository at scale. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. Bogin, B., Yang, K., Gupta, S., Richardson, K., Bransom, E., Clark, P., Sabharwal, A., and Khot, T. SUPER: Evaluating agents on setting up and executing Hui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Lu, K., et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. 9 MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. R. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=VTF8yNQM66. Kuang, J., Li, Y., Zhang, X., Li, Y., Yin, D., Sun, X., Shen, Y., and Yu, P. S. Process-level trajectory evaluation for environment configuration in software engineering agents. arXiv preprint arXiv:2510.25694, 2025. Li, W., Zhang, X., Guo, Z., Mao, S., Luo, W., Peng, G., Huang, Y., Wang, H., and Li, S. FEA-bench: benchmark for evaluating repository-level code generation for feature implementation. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1716017176, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-2510. doi: 10.18653/v1/2025.acl-long.839. URL https: //aclanthology.org/2025.acl-long.839/. Milliken, L., Kang, S., and Yoo, S. Beyond pip install: Evaluating llm agents for the automated installation of python projects. In 2025 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER), pp. 111. IEEE, 2025. OpenAI. Introducing gpt-4.1 in the api. https:// openai.com/index/gpt-4-1/, 2025. Pan, J., Wang, X., Neubig, G., Jaitly, N., Ji, H., Suhr, A., and Zhang, Y. Training software engineering agents and verifiers with SWE-gym. In Forty-second International Conference on Machine Learning, 2025. URL https: //openreview.net/forum?id=Cq1BNvHx74. Pham, M. V. T., Phan, H. N., Phan, H. N., Chi, C. L., Nguyen, T. N., and Bui, N. D. Q. Swe-synth: Synthesizing verifiable bug-fix data to enable large language models in resolving real-world bugs, 2025. URL https://arxiv.org/abs/2504.14757. Siegel, Z. S., Kapoor, S., Nadgir, N., Stroebl, B., and Narayanan, A. CORE-bench: Fostering the credibility of published research through computational reproducibility agent benchmark. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https: //openreview.net/forum?id=BsMMc4MEGS. Tang, X., Liu, Y., Cai, Z., Shao, Y., Lu, J., Zhang, Y., Deng, Z., Hu, H., An, K., Huang, R., Si, S., Sheng, C., Zhao, H., Chen, L., Liu, T., Fang, Y., Qin, Y., Zhou, W., Zhao, Y., Jiang, Z., Chang, B., Cohan, A., and Gerstein, M. ML-bench: Evaluating large language models for code generation in repository-level machine learning tasks, 2025. URL https://openreview.net/forum? id=sf1u3vTRjm. Team, K., Bai, Y., Bao, Y., Chen, G., Chen, J., Chen, N., Chen, R., Chen, Y., Chen, Y., Chen, Y., et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Vergopoulos, K., Mueller, M. N., and Vechev, M. Automated benchmark generation for repository-level coding tasks. In ICLR 2025 Third Workshop on Deep Learning for Code, 2025. URL https://openreview.net/ forum?id=BQA7dkV3iZ. Wang, J., Zan, D., Xin, S., Liu, S., Wu, Y., and Shen, K. Swe-mirror: Scaling issue-resolving datasets by arXiv preprint mirroring issues across repositories. arXiv:2509.08724, 2025a. Wang, X., Li, B., Song, Y., Xu, F. F., Tang, X., Zhuge, M., Pan, J., Song, Y., Li, B., Singh, J., Tran, H. H., Li, F., Ma, R., Zheng, M., Qian, B., Shao, Y., Muennighoff, N., Zhang, Y., Hui, B., Lin, J., Brennan, R., Peng, H., Ji, H., and Neubig, G. Openhands: An open platform for AI software developers as generalist agents. In The Thirteenth International Conference on Learning Representations, 2025b. URL https: //openreview.net/forum?id=OJd3ayDDoF. Wei, Y., Duchenne, O., Copet, J., Carbonneaux, Q., Zhang, L., Fried, D., Synnaeve, G., Singh, R., and Wang, S. I. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025. Wen, X., Liu, Z., Zheng, S., Ye, S., Wu, Z., Wang, Y., Xu, Z., Liang, X., Li, J., Miao, Z., et al. Reinforcement learning with verifiable rewards implicitly incentivizes correct reasoning in base llms. arXiv preprint arXiv:2506.14245, 2025. Xie, C., Li, B., Gao, C., Du, H., Lam, W., Zou, D., and Chen, K. SWE-fixer: Training open-source LLMs for effective and efficient GitHub issue resolution. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 11231139, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl. 62. URL https://aclanthology.org/2025. findings-acl.62/. Yang, J., Jimenez, C. E., Wettig, A., Lieret, K., Yao, S., Narasimhan, K. R., and Press, O. SWE-agent: Agentcomputer interfaces enable automated software engineering. In The Thirty-eighth Annual Conference on Neural 10 MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering Information Processing Systems, 2024. URL https: //openreview.net/forum?id=mXpq6ut8J3. Yang, J., Jimenez, C. E., Zhang, A. L., Lieret, K., Yang, J., Wu, X., Press, O., Muennighoff, N., Synnaeve, G., Narasimhan, K. R., Yang, D., Wang, S., and Press, O. SWE-bench multimodal: Do AI systems generalIn The Thirteenth ize to visual software domains? International Conference on Learning Representations, 2025a. URL https://openreview.net/forum? id=riTiq3i21b. Yang, J., Lieret, K., Jimenez, C. E., Wettig, A., Khandpur, K., Zhang, Y., Hui, B., Press, O., Schmidt, L., and Yang, D. SWE-smith: Scaling data for software engineering agents. In The Thirty-ninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2025b. URL https://openreview. net/forum?id=63iVrXc8cC. Yang, J., Lieret, K., Jimenez, C. E., Wettig, A., Khandpur, K., Zhang, Y., Hui, B., Press, O., Schmidt, L., and Yang, D. Swe-smith: Scaling data for software engineering agents. arXiv preprint arXiv:2504.21798, 2025c. Zan, D., Huang, Z., Liu, W., Chen, H., Xin, S., Zhang, L., Liu, Q., Li, A., Chen, L., Zhong, X., Liu, S., Xiao, Y., Chen, L., Zhang, Y., Su, J., Liu, T., LONG, R., Ding, M., and liang xiang. Multi-SWE-bench: In The multilingual benchmark for issue resolving. Thirty-ninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2025. URL https://openreview.net/forum? id=MhBZzkz4h9. Zeng, A., Lv, X., Zheng, Q., Hou, Z., Chen, B., Xie, C., Wang, C., Yin, D., Zeng, H., Zhang, J., et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025. Zhang, K., Li, J., Li, G., Shi, X., and Jin, Z. Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1364313658, 2024. Zhang, L., He, S., Zhang, C., Kang, Y., Li, B., Xie, C., Wang, J., Wang, M., Huang, Y., Fu, S., et al. Swe-bench goes live! arXiv preprint arXiv:2505.23419, 2025a. Zhang, L., Yang, J., Yang, M., Yang, J., Chen, M., Zhang, J., Cui, Z., Hui, B., and Lin, J. Synthesizing software engineering data in test-driven manner. In Fortysecond International Conference on Machine Learning, 2025b. URL https://openreview.net/forum? id=P9DQ2IExgS. 11 MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering"
        },
        {
            "title": "Appendix",
            "content": "A. Extended Related Work In this section, we provide more granular discussion on the landscape of verifiable software engineering, focusing specifically on the evolution of execution-based benchmarks and the emerging domain of verifiable training dataset construction. A.1. Verifiable SWE Benchmarks The foundational SWE-bench (Jimenez et al., 2024) established the standard for execution-based evaluation, focusing on issue resolution within Python repositories. Recognizing the need for broader language support, subsequent works such as SWE-bench Multilingual (Yang et al., 2025c) and Multi-SWE-bench (Zan et al., 2025) extended this paradigm to polyglot environments, incorporating languages like Java, JavaScript, and Go. Beyond textual code changes, SWE-bench Multimodal (Yang et al., 2025a) introduced visual debugging tasks, adding new dimension to agent evaluation. Furthermore, the scope of tasks has diversified beyond bug fixing: FEA-Bench (Li et al., 2025) targets feature implementation, while SWE-Perf (He et al., 2025) focuses on code optimization and performance enhancement. Connection to MEnvAgent: The maintenance and expansion of these benchmarks heavily rely on successful environment construction. MEnvAgent can significantly accelerate this process, enabling the continuous update of these benchmarks with fresh, real-world repositories to prevent data contamination and stagnation. A.2. Verifiable SWE Training Datasets To improve agent performance, recent research has pivoted towards constructing verifiable training datasets. SWE-gym (Pan et al., 2025) demonstrates the value of rigorous verification but relies on manual curation, limiting its scale. To overcome scalability bottlenecks, SWE-Smith (Yang et al., 2025c) utilizes limited set of base environments and injects synthetic bugs via code mutation to mass-produce tasks. In different approach, SWE-Flow (Zhang et al., 2025b) introduces synthesis framework grounded in Test-Driven Development (TDD); instead of relying on human-submitted issues, it automatically infers incremental development steps directly from unit tests. Similarly, SWE-Synth (Pham et al., 2025) proposes an agent-based framework that simulates human debugging workflows to synthesize human-like bugs at the repository level. More recently, SWE-mirror (Wang et al., 2025a) addresses the sim-to-real gap by reproducing real-world GitHub issues within containerized environments, ensuring data authenticity. Connection to MEnvAgent: Unlike approaches that rely on synthetic mutations or limited base environments, MEnvAgent enables the scaling of training data directly from diverse, real-world scenarios. Our work is orthogonal to methods like SWE-Smith, SWE-Flow and SWE-Flow; by providing massive pool of successfully built environments, MEnvAgent can serve as the foundational infrastructure to further boost their generation pipelines. B. Detailed Problem Formulation In this appendix, we provide the formal mathematical definitions for the task of executable environment building. Building upon the formulation in Repo2Run (Hu et al., 2025), we extend the notation to support the joint synthesis of the build process and test configuration. B.1. State Transition Dynamics Environment State (S). Let denote the set of all possible environment states. An environment state represents comprehensive snapshot of the computer system, encompassing all variables, files, installed packages, and system caches. Command Sequence (C). Let denote the set of all possible command sequences. command sequence consists of series of individual instructions (e.g., shell commands) that, when executed, modify the system state. State Transition Function (δ). We define the state transition as deterministic function δ : S. It maps starting state Sstart and command sequence to resulting state Send: δ(Sstart, C) = Send (5) This function encapsulates the execution of commands (e.g., via bash interface) that transform the system environment. MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering B.2. Base Image Initialization Empty State (S). The empty state represents bare-metal operating system or hypothetical null state with no user-level configurations. Base Image (B). base image is specific environment state, typically pre-configured for convenience (e.g., python:3.10). Formally, base image is reachable from the empty state via predefined command sequence CB C: = δ(S, CB) (6) In our framework, the selection of is the first step in the construction pipeline. B.3. Building Process and Verification Building Process (P). The building process is synthesized command sequence designed to install dependencies and configure the environment starting from the base image B. The final environment state is obtained by: = δ(B, P) (7) Test Configuration (T ). Unlike prior works that assume fixed test commands, we define as synthesized specification that includes the test entry points and execution arguments tailored to the repository. State Verification (ε). The verification function ε determines whether the constructed environment is valid for given repository under the test configuration . We define ε as Boolean function: (cid:40) ε(R, S, ) = 0 if all tests defined in pass in state 1 otherwise (8) Therefore, the goal of our task is to find the triplet (B, P, ) such that ε(R, δ(B, P), ) = 0. C. MEnvAgent Implementation Details This appendix details the technical implementation of MEnvAgent, including agent specifications, the core algorithm, and concrete case study. C.1. Agent Specifications and Workflow In this section, we provide additional technical specifications for the MEnvAgent framework to ensure reproducibility and clarity of the multi-agent interactions. First, Table 5 presents the granular Input-Output (I/O) specifications for each specialized agent, detailing the specific information consumed and the artifacts generated during the environment construction process. Subsequently, Algorithm 1 outlines the comprehensive operational workflow of MEnvAgent. This includes the logic for the Environment Reuse Mechanism (Phase 1), which leverages historical environments to minimize overhead, and the Iterative Construction Loop (Phase 2), which coordinates the planning, execution, and verification stages to autonomously resolve complex build failures. C.2. Case Study: Environment Reuse Process In this section, we present detailed execution trace to illustrate the practical operation of the Environment Reuse Mechanism. The case study, detailed in Figure 7, focuses on real-world scenario from the home-assistant-core repository. Here, the system attempts to reuse verified historical environment to execute new test case. The trace demonstrates the collaborative process between the Verification Agent and the EnvPatchAgent: the former identifies missing dependency through execution failure, while the latter synthesizes context-aware incremental patch (P) by analyzing the original build logic. This process exemplifies how MEnvAgent achieves rapid environment adaptation without the prohibitive cost of rebuilding base images from scratch. 13 MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering Table 5. Detailed Input and Output Specifications for MEnvAgent Components. Agent Name Input Information Generated Outputs Repository Analysis Agent Environment Setup Agent Test Configuration Agent Target Repository Snapshot(R): File tree structure, key configuration files (e.g., package.json, CMakeLists.txt), and documentation. 1. Project Summary: From Repository Analysis Agent. 2. Feedback: Diagnosis from Verification Agent (in retry loops). 1. Project Summary: To identify test frameworks. 2. Setup Script (P): To align test commands with installed binaries. 3. Feedback: Diagnosis from Verification Agent (in retry loops). Environment Execution Agent 1. Base Image (B): Docker image context. 2. Setup Script (P): Commands to execute. Verification Agent 1. Environment (S): The built container. 2. Test Script (T ): Commands to validate correctness. Project Summary: Extracted metadata including primary language, build system type, and identified dependencies. Build Plan: Selected Base Image (B) and the complete Setup Script (P) containing installation commands. Test Script (T ): Executable commands to trigger the repositorys test suite, including necessary environment variables. Runtime Environment (S): built container instance (if successful). Execution Logs: stdout/stderr streams (if failed). 1. Result: Boolean success status. 2. Diagnosis: Error attribution report (e.g., Missing Dependency) used as Feedback for planning agents. EnvPatchAgent (Reuse Mechanism) 1. Target Repository (R) 2. Similar Env (Ssim): Retrieved historical env. 3. Feedback: From verification failure in Ssim. Incremental Patch (P): sequence of commands to adapt Ssim to satisfy Rs requirements. D. MEnvBench Construction Details To ensure the high quality and reproducibility of MEnvBench, we implemented rigorous data acquisition pipeline. This pipeline, which also serves as the foundation for the MEnvData-SWE dataset (see Appendix G), consists of three stages: Collection, Filtering, and Quality Evaluation. D.1. Data Collection and Filtering Strategy We target high-quality repositories and Issue-Pull Request (PR) pairs from GitHub. To filter out noise and ensure the tasks are solvable, we apply set of strict heuristic rules for both repositories and instances, as detailed in Table 6. Notably, we require the presence of both test patches and fix patches to enable execution-based verification. Table 6. Filtering criteria for Repositories and Instances used in our pipeline. Scope Metric / Criteria Repository Popularity (Stars) Primary Language Ratio Community Activity (Forks, Issues, PRs) Instance Issue Status Problem Description Test Patch & Fix Patch Code Modification Patch Size (Lines of Code) Scope (Number of Files) Threshold 1,000 60% 200 each Closed Non-empty Non-empty Required 1,000 10 MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering else end if end if return Ssim return Snew TESTCONFIGAGENT(R) Verified , Logs VERIFICATIONAGENT(Ssim, ) if Verified is True then // Reuse failed, attempt patching ENVPATCHAGENT(R, Ssim, Logs) Snew EXECUTE(Ssim, P) if VERIFICATIONAGENT(Snew, ) is Success then Algorithm 1 MEnvAgent Environment Construction Workflow Input: Target Repository R, Environment Pool Spool Output: Valid Environment or Failure 1: // Phase 1: Environment Reuse Mechanism 2: Ssim RETRIEVESIMILARENV(R, Spool) 3: if Ssim = Null then 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end if 17: 18: // Phase 2: Iterative Construction 19: Feedback 20: for 1 to MaxRetries do Stage 1: Planning 21: Summary REPOANALYSISAGENT(R) 22: P, ENVSETUPAGENT(Summary, Feedback ) 23: TESTCONFIGAGENT(Summary, P, Feedback ) 24: Stage 2: Execution 25: S, Status, ExecLogs ENVEXECAGENT(B, P) 26: if Status is Failure then 27: Feedback ExecLogs 28: continue 29: 30: 31: 32: 33: 34: 35: 36: 37: end if 38: 39: end for 40: return Failure end if Stage 3: Verification Verified , Diagnosis VERIFICATIONAGENT(S, ) if Verified is True then Spool Spool {S} return Feedback Diagnosis else D.2. Automated Quality Evaluation Heuristic filtering alone cannot guarantee the semantic clarity of issue descriptions. To eliminate vague or irrelevant tasks, we employ an LLM-based evaluator (DeepSeek-V3.2). As shown in the prompt template in Figure 8, the model acts as judge, scoring the completeness of the problem description. We strictly discard any instances with score lower than the threshold of 5. MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering Case Study: Environment Reuse Process (home-assistant-core) Phase 1: Initial Verification (Failure) The agent attempts to execute the new test case in the historical environment without modification. Agent: Verification Agent $ bash eval.sh tests/components/rainbird/conftest.py:10: in <module> from pyrainbird import encryption EXIT CODE=4 ModuleNotFoundError: No module named pyrainbird Phase 2: Error Diagnosis The agent identifies the missing dependency and generates specific guidance. Agent: Verification Agent Diagnosis Output (JSON): { \"error category\": \"guidance\": activate testbed; 2. \"Missing pyrainbird. \"dependency error\", 1. Install: pip install pyrainbird\" } Activate conda: source ... && conda Agent: EnvPatchAgent Phase 3: Patch Generation Based on the guidance and original setup script context (detecting Conda + pip usage), the agent generates minimal incremental patch. Generated Patch (P): #!/usr/bin/env bash set -euxo pipefail source /opt/miniconda3/etc/profile.d/conda.sh conda activate testbed # Context-aware activation pip install pyrainbird # Incremental fix Phase 4: Final Verification (Success) The patch is applied, and the tests are re-executed successfully. Agent: Verification Agent [PATCH] Installing pyrainbird package... Successfully installed. tests/components/rainbird/test switch.py::test has unique id PASSED ================ 11 passed in 0.65s ================ EXIT CODE=0 Figure 7. An execution trace of the Environment Reuse Mechanism. MEnvAgent successfully adapts historical environment by identifying missing dependency and generating context-aware patch (Phase 3) without rebuilding the base image. D.3. Data Collection Statistics Table 7 presents the detailed statistics of our data acquisition pipeline across 10 programming languages. The Filtered columns represent the high-quality candidate pool (repositories with > 1k stars, > 200 forks and PRs; instances with closed issues, PRs, and test patches) from which the final MEnvBench was sampled. The remaining high-quality instances were leveraged to construct MEnvData-SWE. D.4. Repository Diversity Analysis To ensure MEnvBench covers diverse range of software ecosystems, we implemented multi-dimensional selection strategy. key component of this strategy is Domain Diversity. We leveraged Large Language Models (LLMs) to automatically classify the filtered repositories into 10 distinct domains (e.g., Machine Learning & AI, Database Systems, Web Application) based on their metadata, including repository name, description, topics, and primary language. This classification allows us to sample tasks that simulate development scenarios across various industries and technical stacks. Figure 9 illustrates the specific prompt template used for this domain classification task. 16 MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering"
        },
        {
            "title": "Prompt for Issue Quality Evaluation",
            "content": "Role Definition You are an experienced software engineer. You need to evaluate the quality of an Issue to determine if it contains sufficient information for an engineer to unambiguously provide solution. Issue Scoring Standards The full score is 10 points (Excellent quality: clear description, explicit requirements for the solution), and 0 points indicates very poor quality (impossible to understand the problem). Note: If solution cannot be implemented based on the Issue, the score should not exceed 5. Deduction Rules Check for deduction items item by item. If the issue has problems or violates the standards, point it out and deduct points in the subsequent evaluation. 1. Major Deductions (Violating any item results in 5-point deduction): Key Information Missing: (1) Lack of expected results: No description of correct behavior/output; for data processing, missing input examples and expected/error outputs. (2) Lack of reproduction steps: No operation flow or runnable code to reproduce the issue. (3) Missing version info: Unspecified libraries, frameworks, OS, or environment details. (4) Incomplete error logs: Snippets provided without context or stack traces. Non-Issue Type Submission: (1) Misuse of PR description: Using Pull Request description text directly as an Issue. (2) Solved problem: The problem described is already fixed or closed. (3) Non-problem inquiry: Content involves release plans, future feature questions, etc., rather than actual defects. 2. Common Deductions (Deduct points based on severity): Unclear Description: (1) Mixed problems: Single Issue contains multiple unrelated problems or logical contradictions. (2) Undefined terminology: Uses unexplained jargon or abbreviations. (3) Unquantified requirements: Uses vague descriptions (e.g., reasonable defaults, user-friendly, faster) without measurable acceptance criteria. (4) Low-quality test cases: Insufficient, overly broad, or missing test cases (passing the test does not prove the issue is resolved). Excessive Reliance on External Resources: (1) Core info relies on external links: Key descriptions/logs/steps are in links that may fail or be inaccessible. (2) Reliance on private repos: Reproduction depends on non-public codebases. Input Content Issue {issue} Output Format Please provide your professional analysis and strictly output number in the following format: reason for evaluation: xxx issue score: 0-10 Figure 8. The prompt template used for the deduction-based Issue quality evaluation. Table 7. Statistics of the data collection and filtering process. Total: Raw data scraped from GitHub (20182025). Filtered: High-quality candidates remaining after applying strict quality and verifiability constraints. Language Python Java Go JavaScript TypeScript Rust C++ PHP Ruby 9,515 3,743 3,563 7,388 4,839 1,743 2,273 3,061 1,962 1,638 Total 39,725 Repositories Instances Total Filtered Total Filtered 1,722 692 771 1,128 1,295 467 443 672 427 8,000 91,072 33,164 61,216 18,952 58,637 20,106 2,965 11,283 14,129 5,759 60,872 21,379 41,113 13,864 41,753 13,500 2,057 7,227 8,376 3,625 317,283 213,766 Beyond domain categories, we also emphasize Project Scale to ensure the benchmark reflects the complexity of real-world software engineering. As illustrated in Figure 10, MEnvBench achieves balanced distribution across both dimensions: 17 MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering"
        },
        {
            "title": "Prompt for Repository Domain Classification",
            "content": "Task Definition Please classify the target repository into one of the 10 specific domain categories based on its metadata. Classification Categories 1. Application Development: Repositories focused on building end-user applications across web, mobile, and desktop platforms. 2. Database Systems: Repositories implementing or interfacing with data storage and retrieval systems. 3. Data Science & Engineering: Repositories for building data pipelines, processing large-scale data, and performing analytics. 4. Machine Learning & AI: Repositories implementing machine learning algorithms, deep learning models, and AI applications. 5. Infrastructure Development: Repositories for managing and automating cloud infrastructure and deployment workflows. 6. Specialized Programming Domain: Repositories for domain-specific applications requiring specialized frameworks or engines. 7. Security Engineering: Repositories focused on security, cryptography, and blockchain technologies. 8. UI/UX Engineering: Repositories for building user interfaces and design systems. 9. Quality Assurance & Testing: Repositories providing tools for testing, building, and ensuring code quality. 10. Embedded Systems & IoT: Repositories for low-level programming, embedded systems, and Internet of Things applications. Input Format Please classify the following repository information: Repository Name: Description: Primary Language: {topics} Topics: {description} {full name} {language} Output Format Please return only the classification result in JSON format: { \"category\": \"Category Name\", \"reason\": \"Brief explanation (1-2 sentences) based on description/topics\" } Classification Guidelines Primary Purpose: Focus on the repositorys main functionality. Technology Stack: Consider the primary language and tags. Specificity: If multiple categories apply, choose the most specific one. Default Handling: If information is insufficient, infer the most likely category from context. Figure 9. The prompt template used for the LLM-based repository domain classification. Figure 10(a) shows the coverage of 10 distinct application domains, while Figure 10(b) demonstrates that the dataset includes repositories of varying sizes, confirming its representativeness of substantial, complex codebases. D.5. Representative Data Instances from MEnvBench To provide comprehensive view of the MEnvBench data structure, we present representative task instances from both the Python and Java subsets. Listing 1 illustrates Python sample sourced from the home-assistant/core repository, while Listing 2 depicts Java task from keycloak/keycloak. These examples demonstrate the unified schema used across languages, capturing essential metadata, detailed problem statements, and the ground-truth verification patches. Listing 1. representative Python data instance from MEnvBench (Home Assistant). Long text fields (e.g., patches and problem statements) are truncated for brevity. { \"repo\": \"home-assistant/core\", \"pull_number\": 104627, \"instance_id\": \"home-assistant__core-104627\", \"issue_numbers\": [ ], 18 MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering (a) Domain Diversity (b) Project Scale Figure 10. MEnvBench Diversity Statistics. The dataset is analyzed across two key dimensions: (a) the distribution of repositories across 10 distinct application domains, and (b) the distribution of project scale. \"base_commit\": \"e594c19c1ecb9bc947b37d2af75e8d84f6a922e9\", \"version\": \"0.38\", \"language\": \"Python\", \"created_at\": \"2023-11-28T00:01:38Z\", \"commit_urls\": [ \"https://github.com/home-assistant/core/commit/7bee0aea66673e92265106cee79efbb8582cd1f0\" ], \"problem_statement\": \"Significant Change support for remotenAdd [significant change](https://developers.homeassistant.io/docs/significant_change_index) support to the remote integration. All official properties need to be taken into consideration when deciding if change is significant... [Content Truncated]\", \"hints_text\": \"There hasnt been any activity on this issue recently. Due to the high number of incoming GitHub notifications... [Content Truncated]\", \"all_hints_text\": \"There hasnt been any activity on this issue recently... [Content Truncated]\", \"patch\": \"diff --git a/homeassistant/components/remote/significant_change.py b/homeassistant/components/remote/ significant_change.pynnew file mode 100644nindex 0000000000000..8e5a36690411dn--- /dev/nulln+++ b/ homeassistant/components/remote/significant_change.pyn@@ -0,0 +1,27 @@n+\"\"\"Helper to test significant Remote state changes.\"\"\"n+from __future__ import annotationsn+n+from typing import Anyn+... [Full content truncated for brevity]\", \"test_patch\": \"diff --git a/tests/components/remote/test_significant_change.py b/tests/components/remote/ test_significant_change.pynnew file mode 100644nindex 0000000000000..dcbfce213d65en--- /dev/nulln+++ b/ tests/components/remote/test_significant_change.pyn@@ -0,0 +1,62 @@n+\"\"\"Test the Remote significant change platform.\"\"\"n+from homeassistant.components.remote import ATTR_ACTIVITY_LIST, ATTR_CURRENT_ACTIVITYn+... [Full content truncated for brevity]\" } Listing 2. representative Java data instance from MEnvBench (Keycloak). This security-related task requires the agent to modify the secret generation logic to ensure sufficient entropy, involving changes to both utility classes and integration tests. { \"repo\": \"keycloak/keycloak\", \"pull_number\": 39637, \"instance_id\": \"keycloak__keycloak-39637_test\", \"issue_numbers\": [ 38621 ], \"base_commit\": \"61fdfc2352a6e9da2e5dbeeb121cf731f48dfef9\", \"version\": \"2.5\", \"language\": \"Java\", \"created_at\": \"2025-05-12T10:47:31Z\", \"commit_urls\": [ \"https://github.com/keycloak/keycloak/commit/aec69609309216a0535955714a93a3c7423e2f9e\" ], \"problem_statement\": \"Client secret generation provides lower than expected entropyn### Describe the bugnThe way how we generate client secrets in authentication flows... uses character set consisting of 62 alphanumeric characters... For example, 256-bit secret generated using 32 characters from 62-character set results in only 192 bits of entropy... [Content Truncated]\", \"hints_text\": \"Changing to an enhancement and adding to sprint 67 for now...nWe already use SecureRandom for generating random strings, but we can likely increase the character set and/or increase the length of the secret... [Content Truncated]\", \"all_hints_text\": \"Changing to an enhancement... [Content Truncated]\", \"patch\": \"diff --git a/common/src/main/java/org/keycloak/common/util/SecretGenerator.java b/common/src/main/java/ 19 MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering org/keycloak/common/util/SecretGenerator.javanindex ff73e855eeec..42eb86fb66d8 100644n--- a/common/src/main /java/org/keycloak/common/util/SecretGenerator.javan+++ b/common/src/main/java/org/keycloak/common/util/ SecretGenerator.javan@@ -70,4 +71,28 @@ public byte[] randomBytes(int length) {n }n n+ entropy bits than byte array random generated.n+ byteLengthEntropy, int dstAlphabetLeng) {n+ dstAlphabetLeng);n+ * Returns the equivalent length for destination alphabet to have the samen+ return equivalentEntropySize(byteLengthEntropy, 256, }n... [Full content truncated for brevity]\", public static int equivalentEntropySize(int return buf;n /**n+ */n+ * \"test_patch\": \"diff --git a/testsuite/integration-arquillian/tests/base/src/test/java/org/keycloak/testsuite/oauth /ClientAuthSecretSignedJWTTest.java b/testsuite/integration-arquillian/tests/base/src/test/java/org/keycloak/ testsuite/oauth/ClientAuthSecretSignedJWTTest.javanindex 6b91a1a01848..388c38447925 100644n--- a/testsuite/ integration-arquillian/tests/base/src/test/java/org/keycloak/testsuite/oauth/ClientAuthSecretSignedJWTTest. javan+++ b/testsuite/integration-arquillian/tests/base/src/test/java/org/keycloak/testsuite/oauth/ ClientAuthSecretSignedJWTTest.javan@@ -290,16 +290,16 @@ private void processAuthenticateWithAlgorithm( String algorithm, Integer secretLen firstSecret = clientResource.generateNewSecret().getValue(); //clientResource.getSecret().getValue();nassertThat(firstSecret.length(), is( assertThat(firstSecret.length(),is(secretLength));n+ configureDefaultProfileAndPolicy();n String SecretGenerator.equivalentEntropySize(secretLength, SecretGenerator.ALPHANUM.length)));n generate new secret, rotate the secretn getValue();n... [Full content truncated for brevity]\" // String newSecret = clientResource.generateNewSecret(). } E. Detailed hyperparameter settings for MEnvAgent and baselines on MEnvBench. All experiments were conducted on the MEnvBench benchmark. Unless otherwise specified, we used fixed temperature of 0.5 and global timeout of 3 hours per task to ensure fair comparison. The detailed hyperparameter configurations for MEnvAgent and all baseline methods are summarized in Table 8. Table 8. Detailed hyperparameter settings for MEnvAgent and baselines on MEnvBench. Method Repo2Run SWE-Bench-Live SWE-Factory MEnvAgent (Ours) Parameter Max Iterations Time Limit Temperature Concurrency Max Install Iterations Max verify Iterations Time Limit Temperature Concurrency Max Iterations Time Limit Temperature Concurrency Max Iterations Time Limit Temperature Concurrency Value 50 3 hours 0.5 15 20 20 3 hours 0.5 5 3 hours 0.5 15 5 3 hours 0.5 15 F. Detailed Results on MEnvBench Performance. Table 9 provides the comprehensive performance breakdown across all 10 programming languages evaluated in MEnvBench. The table reports Fail-to-Pass Rate (F2P), Pass Rate (PASS), and Time Cost (TIME) for all compared methods using both Kimi-K2 and Gemini-3-Flash backbones. The results indicate that MEnvAgent consistently achieves superior stability and resolution rates compared to baselines, particularly in complex system-level languages. Cost. In addition to performance, we analyze the economic efficiency in Table 10. The data demonstrates that both MEnvAgent and SWE-Factory represent low-cost solutions, maintaining minimal token consumption per task compared to single-agent baselines that incur substantial overhead due to unplanned exploration. Although MEnvAgent incurs marginally higher cost than SWE-Factory, this slight increment is well-justified by the significant gains in both F2P rate and time efficiency. Given that both methods operate within highly affordable low-cost regime, this difference is negligible in practice and does not constitute bottleneck for large-scale data expansion. MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering Table 9. Detailed performance comparison on MEnvBench across 10 languages. F2P, Pass, and Time denote Fail-to-Pass (%), Pass Rate (%), and Average Time Cost (s), respectively. - indicates the method is not applicable to the specific language. Our method is highlighted in bold. Method Python Go Java JavaScript F2P PASS TIME F2P PASS TIME F2P PASS TIME F2P PASS TIME F2P PASS TIME Repo2Run SWE-Bench-Live SWE-Factory MEnvAgent Repo2Run SWE-Bench-Live SWE-Factory MEnvAgent Method 24 13 31 27 21 44 61 26 15 37 60 32 23 51 66 3112 2589 5512 2311 3769 2547 5529 2530 3 50 13 53 61 - 6 57 70 - 15 61 66 Kimi-K2 4382 5742 3171 Gemini-3-Flash 6 13 3689 5193 3173 8 18 37 - 13 16 33 - 10 24 43 6231 6182 4909 5146 6582 11 37 44 13 39 48 - 16 40 51 - 24 41 63 4725 5650 3210 2441 5690 13 13 18 20 - - 35 39 - - 37 46 5777 3364 5482 C++ Rust TypeScript PHP Ruby F2P PASS TIME F2P PASS TIME F2P PASS TIME F2P PASS TIME F2P PASS TIME Repo2Run SWE-Bench-Live SWE-Factory MEnvAgent Repo2Run SWE-Bench-Live SWE-Factory MEnvAgent 7 7 6 7 - - 21 23 - - 24 26 5832 5361 5995 11 19 33 21 37 43 Kimi-K2 7142 7773 4662 Gemini-3-Flash 5 23 5484 7311 4828 10 24 35 - 6 27 43 - 18 28 43 - 15 32 42 - 25 42 7867 8962 2330 5187 8050 3632 30 37 39 40 - - 34 40 - - 45 5759 2394 39 49 5969 2415 55 59 - - 46 58 - - 62 6368 1782 6582 2399 Table 10. Cost analysis on MEnvBench across 10 languages. Input and Output represent token counts in thousands (k). Cost is the estimated average cost per task in USD ($), calculated based on the pricing: Kimi-K2 ($0.6/1M In, $2.5/1M Out) and Gemini-3-Flash ($0.5/1M In, $1.5/1M Out). - indicates the method is not applicable to the specific language. Our method is highlighted in bold. Method Python Go Java JavaScript Input Output Cost Input Output Cost Input Output Cost Input Output Cost Input Output Cost Repo2Run SWE-Bench-Live SWE-Factory MEnvAgent Repo2Run SWE-Bench-Live SWE-Factory MEnvAgent Method 630 1220 90 890 2010 86 211 2 36 12 9 12 228 42 104 C++ 0.38 0.82 0.08 0.11 0.46 1.35 0.11 0. 640 65 116 1300 77 148 - 47 7 7 - 144 42 111 Rust Kimi-K 770 102 130 0.50 0.06 0.09 Gemini-3-Flash 0.87 0.10 0.24 110 192 - 37 10 10 - - 54 TypeScript 0.55 0.09 0.10 710 72 167 1900 65 166 0.14 0.40 - 21 8 - 149 41 224 PHP 0.48 0.06 0.12 1.17 0.09 0.42 114 198 283 - - 14 13 - - 85 212 Ruby 0.10 0.15 0.27 0.48 Input Output Cost Input Output Cost Input Output Cost Input Output Cost Input Output Cost Repo2Run SWE-Bench-Live SWE-Factory MEnvAgent Repo2Run SWE-Bench-Live SWE-Factory MEnvAgent 107 187 224 282 - - 13 15 - - 76 286 580 111 97 0.10 0. 1170 77 136 0.23 0.57 Kimi-K2 580 61 106 0.38 0.09 0.07 Gemini-3-Flash 0.79 0.10 0. 71 151 - 45 8 7 - - 45 134 - 12 11 6 - 134 44 77 0.46 0.06 0. 79 122 0.10 0.28 73 182 - - 9 8 - - 46 180 0.07 0. 62 92 0.11 0.36 110 157 - - 7 7 - - 54 168 0.05 0. 0.14 0.33 21 MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering G. Details of Scaling Verifiable SWE Datasets In this section, we describe how we leveraged the MEnvAgent framework to scale up the production of verifiable environments and training trajectories, resulting in the MEnvData-SWE dataset. G.1. Dataset Construction Pipeline Our pipeline builds upon the high-quality candidate pool established in Appendix D. Utilizing the filtered repositories and instances presented in Table 7, we focus on the subsequent stages: Environment Construction via MEnvAgent and Execution-based Verification. G.2. Environment Construction This phase constitutes the core of our pipeline, where MEnvAgent is deployed to automatically retrieve codebases, resolve dependencies, and generate Docker images. Environment construction is the most computationally intensive and time-consuming component of the pipeline, serving as the primary performance bottleneck. On standard local infrastructure, the Docker daemon severely limits concurrent build operations; our preliminary benchmarks indicate that effective concurrency is capped at approximately 1015 tasks. Given that resolving complex dependencies for legacy repositories can span several hours per instance, achieving large-scale dataset generation on local machines is virtually infeasible. To overcome this scalability barrier, we re-engineered the underlying infrastructure to support Kubernetes (K8s) orchestration. By decoupling the build agents from the host limits, we achieved (1,000+ parallel builds). This architectural shift enabled the rapid construction of thousands of environment-aware images in short timeframe. We commit to open-sourcing this high-throughput build infrastructure and the resulting data to accelerate community research in large-scale software engineering. G.3. Execution-based Verification Once the environments are constructed, we verify their validity to ensure they represent genuine, reproducible bug-fix scenarios. Fail-to-Pass (F2P). We implement rigorous Fail-to-Pass verification protocol using test cases extracted from the original Pull Requests. task is deemed valid SWE task only if it satisfies the following two-stage strict check: 1. Reproduction Phase (Fail): The test script is executed in the environment with only the Test Patch applied (simulating the buggy state). The outcome must be Failure, confirming that the reported issue is reproducible within the environment. 2. Verification Phase (Pass): The test script is executed in the environment with both the Test Patch and the Fix Patch applied (simulating the fixed state). The outcome must be Success, confirming that the provided patch effectively resolves the issue. Only instances that survive this rigorous pipeline are included in the final dataset, guaranteeing that every sample is grounded in reproducible, executable environment. G.4. Details of MEnvData-SWE Dataset In this section, we provide detailed statistical breakdown of the MEnvData-SWE dataset, which was constructed using the MEnvAgent pipeline and utilized for the Supervised Fine-Tuning (SFT) experiments described in Section 6.3. Table 11 presents the distribution of data across different programming languages. The dataset metrics are defined as follows: Repos: The number of unique source repositories sourced from GitHub. Instances: The number of unique task instances (comprising specific issue and corresponding pull request snapshot). 22 MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering Trajectories: The number of successfully verified agent interaction traces (Thought-Action sequences) collected via rejection sampling. These trajectories serve as the high-quality instruction data for model fine-tuning. As shown in the table, the dataset exhibits diverse distribution across ecosystems. While popular languages like Rust and JavaScript contribute significant portion of the data due to their active open-source communities, the dataset also maintains coverage for lower-level languages like and enterprise-heavy languages like Java and PHP, ensuring multilingual generalization for trained models. Table 11. Detailed Breakdown. Language-specific statistics of the MEnvData-SWE dataset. The table details the count of unique repositories, task instances, and verifiable solution trajectories collected for each language. Language # Repos # Instances # Trajectories Go Java JavaScript PHP Python Ruby Rust TypeScript C++ Total 169 14 119 21 192 79 249 76 9 14 502 33 578 159 477 283 769 157 16 31 502 47 694 395 543 749 769 157 16 0 3,005 3,872 G.5. Comparison with Other Verifiable Datasets To contextualize the scale and diversity of our contribution, we compare MEnvData-SWE against prominent verifiable SWE benchmarks and training datasets in Table 12. Existing benchmarks, while pivotal, are often restricted to single-language ecosystems (primarily Python) or lack repository diversity. similarly, recent training datasets typically rely on synthetic mutations or generated tests (e.g., SWE-Smith, SWE-Flow), lacking the fidelity of real-world development scenarios. In contrast, MEnvData-SWE distinguishes itself by simultaneously achieving high polyglot coverage and grounding all data in authentic, user-submitted issues. Table 12. Comparative Overview. Comparison of MEnvData-SWE with existing verifiable SWE benchmarks and training datasets. Langs: Number of supported languages. # Repos: Number of source repositories. # Instances: Number of task instances. # Trajectories: Number of agent solution trajectories. Realistic: Originates from real-world issues () vs. synthetic/generated (). Dataset Langs # Repos # Instances # Trajectories Realistic Verifiable SWE Benchmarks SWE-bench Verified SWE-bench Multilingual Multi-SWE-bench SWE-bench Multimodal FEA-Bench SWE-Perf 1 9 7 1 1 1 Verifiable SWE Training Datasets 1 SWE-Factory 1 SWE-Smith 1 SWE-Flow 1 SWE-Synth 1 SWE-Mirror 10 MEnvData-SWE (Ours) - - - - - - 2,809 5,016 - 3,018 6,431 3, 500 300 1,632 617 1,401 140 - 50,000 16,061 9,459 60,000 3,005 12 42 39 17 83 12 - 128 62 7 40 23 MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering Conclusion. As demonstrated by the comparison, MEnvData-SWE represents the largest open-sourced polyglot dataset of realistic verifiable Docker environments to date, alongside solution trajectories that enable consistent performance gains on SWE tasks across wide range of models. G.6. Training Implementation Details We fine-tune all models using unified supervised fine-tuning (SFT) framework based on Megatron-LM. To equip the models with long-context capabilities, we scale the training sequence length to 128k tokens (131, 072). Optimization and Hyperparameters. We fine-tune the models on dataset of approximately 4k instances for 3 epochs. We employ the AdamW optimizer with global batch size of 8. The learning rate is scheduled with constant strategy, where the peak learning rate is set to 1 105 and decays to minimum of 1 109, with no warmup steps. We set the gradient clipping norm to 1.0 and use weight decay of 0.1. For MoE-based models, we apply an auxiliary loss with coefficient of 1 103 to ensure load balancing among experts. Infrastructure and Efficiency. To efficiently train large-scale models with such long contexts, we utilize comprehensive 3D parallelism strategy deployed on NVIDIA H800 GPUs, configuring Tensor Parallelism (TP), Pipeline Parallelism (PP), and Expert Parallelism (EP) all to size 8, alongside Sequence Parallelism. Furthermore, we leverage Flash Attention 2 to accelerate attention computation and enable full activation checkpointing (recompute) to significantly reduce memory fragmentation during training."
        }
    ],
    "affiliations": [
        "Baidu Inc., Shenzhen, China",
        "Research Center for Social Computing and Interactive Robotics, Harbin Institute of Technology, Harbin, China"
    ]
}