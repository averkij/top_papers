{
    "paper_title": "Homeomorphism Prior for False Positive and Negative Problem in Medical Image Dense Contrastive Representation Learning",
    "authors": [
        "Yuting He",
        "Boyu Wang",
        "Rongjun Ge",
        "Yang Chen",
        "Guanyu Yang",
        "Shuo Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Dense contrastive representation learning (DCRL) has greatly improved the learning efficiency for image-dense prediction tasks, showing its great potential to reduce the large costs of medical image collection and dense annotation. However, the properties of medical images make unreliable correspondence discovery, bringing an open problem of large-scale false positive and negative (FP&N) pairs in DCRL. In this paper, we propose GEoMetric vIsual deNse sImilarity (GEMINI) learning which embeds the homeomorphism prior to DCRL and enables a reliable correspondence discovery for effective dense contrast. We propose a deformable homeomorphism learning (DHL) which models the homeomorphism of medical images and learns to estimate a deformable mapping to predict the pixels' correspondence under topological preservation. It effectively reduces the searching space of pairing and drives an implicit and soft learning of negative pairs via a gradient. We also propose a geometric semantic similarity (GSS) which extracts semantic information in features to measure the alignment degree for the correspondence learning. It will promote the learning efficiency and performance of deformation, constructing positive pairs reliably. We implement two practical variants on two typical representation learning tasks in our experiments. Our promising results on seven datasets which outperform the existing methods show our great superiority. We will release our code on a companion link: https://github.com/YutingHe-list/GEMINI."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 2 8 2 5 0 . 2 0 5 2 : r JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST"
        },
        {
            "title": "Homeomorphism Prior for False Positive and\nNegative Problem in Medical Image Dense\nContrastive Representation Learning",
            "content": "Yuting He, Boyu Wang, Rongjun Ge, Yang Chen, Guanyu Yang, Shuo Li AbstractDense contrastive representation learning (DCRL) has greatly improved the learning efficiency for image dense prediction tasks, showing its great potential to reduce the large costs of medical image collection and dense annotation. However, the properties of medical images make unreliable correspondence discovery, bringing an open problem of large-scale false positive and negative (FP&N) pairs in DCRL. In this paper, we propose GEoMetric vIsual deNse sImilarity (GEMINI) learning which embeds the homeomorphism prior to DCRL and enables reliable correspondence discovery for effective dense contrast. We proposes deformable homeomorphism learning (DHL) which models the homeomorphism of medical images and learns to estimate deformable mapping to predict the pixels correspondence under the condition of topological preservation. It effectively reduces the searching space of pairing and drives an implicit and soft learning of negative pairs via gradient. We also proposes geometric semantic similarity (GSS) which extracts semantic information in features to measure the alignment degree for the correspondence learning. It will promote the learning efficiency and performance of deformation, constructing positive pairs reliably. We implement two practical variants on two typical representation learning tasks in our experiments. Our promising results on seven datasets which outperform the existing methods show our great superiority. We will release our code at companion website. Index TermsMedical image analysis, Dense contrastive representation learning, False positive and negative pairs problem, Homeomorphism prior, Correspondence problem"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "[7] is crucial for medical ENSE contrastive representation learning (DCRL) [1] image dense prediction (MIDP) tasks, e.g., segmentation [8][10]. With the increasing demand for deep learning in medical image applications [11], the extremely high cost of medical image collection and dense annotation are becoming large bottleneck [12]. The DCRL discovers the corresponding pixel1-wise features [13][16] to drive the learning of consistent or distinct representation for them (Fig.1 A)) which will effectively capture the dense posterior distribution of the underlying explanatory factors for the input. Therefore, it will make models Corresponding authors: G. Yang. (e-mail: yang.list@seu.edu.cn) This research was supported by the National Natural Science Foundation of China (Grant No. 82441021), the Natural Science Foundation of Jiangsu Province (Grant No. BK20210291), the National Natural Science Foundation of China (Grant No. 62101249, T2225025), the Jiangsu Shuangchuang Talent Program (Grant No. JSSCBS20220202). Y. He, Y. Chen and G. Yang are with the Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, Nanjing, China, the Centre de Recherche en Information Biomedicale Sino-Francais (CRIBs), and Jiangsu Provincial Joint International Research Laboratory of Medical Information Processing, Nanjing, China. (e-mail: yang.list@seu.edu.cn) R. Ge is with the School of Instrument Science and Engineering, Southeast University, Nanjing, China (e-mail: rongjun ge@seu.edu.cn) B. Wang is with the Department of Computer Science, Western University, London, ON N6A 3K7, Canada. (e-mail: bwang@csd.uwo.ca) S. Li and Y. He are with the Department of Biomedical Engineering and the Department of Computer and Data Science, Case Western Reserve University, Cleveland, OH 44106 USA (e-mail: shuo.li11@case.edu). Manuscript received April 19, 2005; revised August 26, 2015. 1. pixel for 2D and voxel for 3D images, we call pixel uniformly Fig. 1: The DCRL with the large-scale FP&N problem. A) The DCRL pulls and pushes the positive and negative feature pairs for consistent or distinct representation. However, B) medical images properties cause unreliable correspondence discovery, resulting in the open problem of large-scale FP&N features pairs in DCRL and extremely limiting the representation learning ability. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2 utilize the correspondence from the manual transformation of the same images weakening the FP problem, the diversity of positive pairs is limited (only paired from the same images) and they are still limited by large-scale FN problem. 2) Large-scale false negative (FN) pairs caused by semantic continuity and semantic overlap: a. Different from the imagewise contrastive learning [21][24] whose unit is instance, the DCRL utilizes the pixel as the unit which continuously arranges on images and constructs semantics via numerous pixels due to the continuity of image signal [21] (Fig.1 B) b)). This makes the semantics continuously distributed on images, so it is unreliable to absolutely divide the pixelwise features as different semantics on images as negative pairs. b. Some natural DCRL methods [1], [3], [4] utilize the features from different images as negative pairs, like momentum contrast mechanism [1], [3], [25] which utilizes memory bank to save previous features as negative samples (Fig.1 B) c)). However, due to the consistency of human body, medical images have similar global content which shares numerous same anatomies. This makes numerous overlapped semantics between images, so that it will construct numerous negative pairs with the same semantics resulting in FN pairs. During correspondence discovery, these properties will enlarge the risk of the FP&N feature pairs [18], [19] resulting in large-scale FP&N problem. It will train the networks representation to deviate from reality, making the pre-trained network even worse than the randomly initialized network. Therefore, we seek to answer the following question: How to cope with the FP&N problem for reliable pixel-wise correspondence in the medical image DCRL? Motivation: Inspired by topologie [26]  (Fig.2)  , homeomorphism [27] between medical images [28][30], e.g., CT, MR, X-ray, provides prior knowledge for reliable pixelwise correspondence. An often-repeated mathematical joke is that topologists cannot tell the difference between coffee cup and donut [31] (Fig.2 A)), because the coffee and donut are homeomorphic (have the same topology) and they are able to transform to each other via topologypreserved mapping (homeomorphism, bijective and continuous transformation). The consistency of human genes determines that the human bodies have similar anatomies [32], for example, human hearts have four chambers with fixed spatial relationship, and the human brain has fixed functional regions distribution [29]. This makes the medical images scanned from the same body ranges have stable similar anatomies [32] with consistent context topology [33], showing the homeomorphic topology (Fig.2 B)). Therefore, based on the topologie principle and the intrinsic homeomorphic topology of medical images, it will be easy to align the semantic regions via deformation (a homeomorphism mapping function, we name it deformable mapping in this paper). This makes an effective prior knowledge that enables reliable pixel-wise correspondence discovery inter images in DCRL under the condition of medical images inherent topology (Fig.2 C)), reducing the searching space of constructing the correspondence. Therefore, we hypothesize that Embedding this homeomorphism prior knowledge to the medical images DCRL will prompt the pixel-wise correspondence discovery to improve the dense representation. However, it is challenging to directly utilize this homeFig. 2: The homeomorphism prior enables the pixel-wise correspondence discovery under the condition of medical images inherent topology, promoting its reliability. A) In topologie, the homeomorphic objects are able to align their topologies via homeomorphism mapping for point-topoint correspondence with topological preservation. B) Due to the consistency of human body, the medical images are homeomorphic in image space. This provides prior knowledge to construct deformable mapping for the pixels correspondence under the condition of their inherent topology, which will effectively reduce the searching space of pairing. C) This gives potential to enable reliable pixel-wise correspondence discovery in the medical image DCRL. easier to extract useful information [6] when learning MIDP tasks, thus pushing the label and data efficiency to soaring heights in the learning process and coping with the large challenge in the data collection and dense annotation [17]. Although some DCRL works [1][5] on natural images have been reported, medical images properties will cause extremely unreliable correspondence discovery (Fig.1 B)), leading to an open problem of large-scale false positive and negative (FP&N) feature pairs [18], [19] in DCRL: 1) Large-scale false positive (FP) pairs caused by the semantic dependence: (Fig.1 B) a)) Some existing works [1], [3] measure the similarity between the pixel-wise features to discover the positive pairs. However, due to the lowand noisy-contrast acquisitions [20] of medical images, numerous semantic regions in these images are insignificant (e.g., the soft tissues in CT images). These insignificant regions have large dependence with each other making it challenge to distinguish them. Therefore, if directly measuring the similarity for all features, the features will be mispaired easily resulting in large-scale FP pairs. Although some other works [2], [4], [5] JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST omorphism prior knowledge in DCRL due to: 1) Lack of negative pairs: Although the homeomorphism prior enables reliable pixel-wise correspondence for positive pairs, the non-corresponding positions are unable to be directly divided as negative pairs due to the semantic continuity, limiting the contraction of negative pairs. 2) Weak positive pairs: The estimation of the deformation requires measurement of the alignment degree between images. However, due to the limitation in the insignificant and varied appearance of medical images, the widely used visual similarity [33][37], which utilizes the pixel intensity of the images to measure the alignment degree, will be interrupted. It will limit the deformation accuracy and cause numerous false positive pairs on misalignment regions, resulting in weak learning once very poor alignment occurs. In this paper, we advance the geometric visual similarity learning (GVSL, our CVPR 2023 work [33] discussed in Sec.6.2.4) in DCRL, model the homeomorphism behind the GVSL, and propose the GEoMetric vIsual deNse sImilarity (GEMINI) Learning. Its objective is to model the homeomorphism prior to DCRL to cope with the large-scale FP&N problem in DCRL, thus advancing the effectiveness of dense representation learning for medical images with reliable pixel-wise correspondence discovery. It has two aspects: 1) Deformable Homeomorphism Learning (DHL) for soft learning of feature pairs. Based on the homeomorphism prior, it promotes the GVSL, models that two images have homeomorphic topology and learns to estimate deformable mapping to align them for pixel-wise correspondence. It consists of two share-weighted representation networks (backbone) and one deformation network (deformer). The deformer is trained on the represented dense features from the backbones to predict displacement vector flow (DVF, deformable mapping) which deforms one image to align the other image via moving the pixels. Instead of directly dividing negative pairs [1][5], the deformer learns to discover the corresponding feature pairs from numerous pixel-wise features in the receptive field. This gradient-driven approach encourages the backbones to extract distinct features for non-corresponding (negative) pairs and consistent features for corresponding (positive) pairs between the image with overlapped semantic regions. Therefore, it will implicitly learn the feature pairs and also avoid the hard division of pixel-wise features as negative pairs, making soft learning for the continuous image signal. 2) Geometric Semantic Similarity (GSS) for reliable learning of positive pairs. Our GSS fuses semantic similarity into the measurement of alignment degree to promote the learning of correspondence for accurate alignment, thus constructing and learning the positive pairs reliably. Due to the representability of the backbones, the extracted features will represent significant semantic information inner their corresponding receptive fields. The same and different semantic regions will have consistent and distinct features, which will bring more efficient measurement than the original geometric visual similarity (GVS) [33]. Therefore, our GSS calculates the distance of the features between images, contributing to the DCRL in two aspects: a) This measurement will consider the significant semantic information and avoid the interference of appearances limitation, driving an effective learning of correspondence in 3 DHL as novel loss function. Therefore, it will improve the soft learning of feature pairs in DHL. b) It reliably discovers pixel-wise correspondence under the condition of medical images inherent topology, reliability enhancing the effectiveness of consistency learning from these positive features for powerful representation. Finally, our GEMINI trains the backbones to extract consistent and distinct features for the same and different semantic regions, achieving powerful dense representation ability. Based on our GEMINI learning, we implement two practical variant frameworks on two typical representation learning tasks (semi-supervised learning, representation pre-training) [6] in our experiments, proving the powerful ability of this novel and effective learning paradigm. This paper is an extension of the CVPR 2023 conference vision (GVSL), we have detailed discussed the advancement of the preliminary work in Sec.6.2.4. This paper makes four significant contributions: 1) For the first time, we propose the GEMINI learning which is the first framework for the open problem of large-scale FP&N pairs in the medical image DCRL. It embeds the homeomorphism prior and constructs reliable correspondence discovery under the condition of topological preservation, thus building an effective learning of pixel-wise features, and promoting the representation for MIDP tasks. 2) Our proposed Deformable Homeomorphism Learning (DHL) models the homeomorphism prior as the prediction of deformable mapping which trains the network distinct representation for non-corresponding regions via gradient, achieving soft learning of negative pairs. 3) Our proposed Geometric Semantic Similarity (GSS) utilizes the semantic information represented in features to efficiently measure the alignment degree and promote the correspondence learning, reliably learning positive pairs. 4) Based on our GEMINI learning, we implement two practical variant frameworks on two typical representation learning tasks, and our complete experiments on these two tasks on seven datasets demonstrate our powerful representation ability and application potential. Overall, our GEMINI learning has three key advantages: a) Great efficiency: Our GEMINI captures the posterior distribution of the underlying explanatory factors for the observed input from unlabeled images [6], improving label and data efficiency to soaring heights in the MIDP learning process. b) Higher reliability: Compared with other DCRL methods [1][5], our homeomorphism prior of medical images significantly promote the correspondence discovery, bringing reliable learning of positive feature pairs and soft learning of negative feature pairs for powerful dense representation learning. c) Powerful flexibility: As general DCRL paradigm, our GEMINI only needs to make some simple adjustments, e.g., adding the learning loss of MIDP tasks, changing the dimensions of the backbone and deformer networks, etc., to achieve variant frameworks that adapt to different settings and dimensions for efficient learning. In this paper, we provide two variant frameworks on both 2D and 3D experiment settings only via very simple adjustments showing our GEMINIs powerful flexibility. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST"
        },
        {
            "title": "2 RELATED WORK",
            "content": "1) Correspondence Problem: Broadly speaking, the correspondence problem is one of the basic problems in cognitive science [38], [39], machine learning [40], and computer vision [41]. It studies the notion of correspondence between two autonomous agents in human cognition [38], thus further exploring the social learning, imitation, copying, or mimicry in human activation. These human cognition studies for correspondence have been influential in machine learning in the past several decades [40], [42]. Numerous machine learning tasks, such as the protein 3D structure prediction from amino acid sequence [43], the machine translation [44], the position alignment between images [41], etc., are able to be modeled as correspondence problems. In this paper, we limit our scope to visual contrastive representation learning and review the below topics that are relevant to the applications considered in the sequel. 2) Dense Contrastive Representation Learning: DCRL is typical correspondence problem that learns consistent or distinct representation for pixel-wise features of positive or negative pairs via constructing pixel-wise correspondence [13][15]. It will effectively capture the posterior distribution of the underlying explanatory factors and make models easier to extract useful information [6] when learning MIDP tasks. Therefore, the label and data efficiency will be effectively improved to cope with the large challenge of the extremely high cost of medical image collection and pixelwise annotation. It has three kinds to construct positive and negative pairs. a. The pixel similarity-based methods [1], [3], [5], [45], [46] measure the Mahalanobis [47] or Euclidean [48] distance between pixel-wise features for the correspondence. However, the low-contrast medical images limit the discrimination of features making the measurement unreliable and constructing FP&N pairs. b. The location-based methods take the shared part of two cropped patches from one image [2] or the same position between two medical images [49] as the positive pairs and the features from different images or different positions as the negative pairs, avoiding the limitation of mismeasurement. However, due to the consistency of image content, the same semantic pixel regions are widely existing in different images making serious false negative problem, and interfering with the representation learning process. c. The attention-based method [4] utilizes the attention maps to extract positive pairs and their negative pairs are still directly paired from different images. It relies on large dataset to train an attention prediction model, once the dataset is not large enough, the inaccurate attention will bring numerous miscorrespondence. The unreliable negative pairs also bring large limitations. 3) False Positive and Negative Pairs Problem: FP&N Problem [18], [19] is one of the key open problems in contrastive representation learning [22], [24], [25], [50], [51] and the existing works focus on FP&N problem in imagewise. FP&N problem is caused by the mis-correspondence of feature pairs which makes the networks learn distinct representations for the same-semantic pairs and consistent representations for the different-semantic pairs. The network will learn the inaccurate posterior distribution which is contrary to the underlying explanatory factors, extremely 4 limiting the learning of practical tasks. Some methods [5], [24] remove the construction of negative pairs, and only learn the positive pairs constructed by different views of one image to avoid the FP&N problem. However, it will bring the risk of dimensional collapse [52] which makes the network unable to represent the information in images. Some other methods [18], [19], [53] construct FP&N-robust losses or FP&N-discovery mechanisms to reduce the interference of inaccurate feature pairs, and have achieved promising results. However, these methods are sensitive to their additional hyper-parameters, and these hyper-parameters have to be carefully adjusted for their effectiveness. There is no success reported to cope with the FP&N problem in the medical image DCRL whose special properties (continuous image signal, low contrast, varied appearance, consistent image content) bring more challenges in correspondence. 4) Homeomorphism in Medical Images Analysis: Homeomorphism is powerful prior for medical image learning. As introduced in Sec.1, this property comes from the consistency of human anatomy [32], and numerous medical image categories, e.g., CT, MR, X-ray, etc., inherit it. Therefore, using homeomorphism as prior knowledge, some medical image works have been studied. One of the classic applications is the registration [34][37], [41]. According to this prior, it aligns the anatomical structures in medical images to the same spatial coordinate system, so that the images consistent anatomies will be aligned in image space. Based on the registration, some works [29], [54][57] further study the atlas-based segmentation methods. They align the labeled images to unlabeled images, thus indirectly mapping the labels to unlabeled images for segmentation results. These methods have very small label requirements and large robustness due to homeomorphism. With the development of deep learning [58], [59], the homeomorphism prior is further promoting the medical image learning, e.g., few-shot segmentation [8], [60][63]. They have effectively improved the learning efficiency and robustness in these tasks, owing to the large contribution from the homeomorphism prior. However, due to the limitations illustrated in Sec.1, it is still challenging to embed this great prior into the DCRL tasks and there are no preliminary studies reported."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "The proposed GEMINI paradigm  (Fig.3)  constructs soft learning of negative pairs (DHL, Sec.3.2) and reliable learning of positive pairs (GSS, Sec.3.3) based on the homeomorphism prior knowledge for the large-scale FP&N problem in DCRL for powerful representation. 3.1 Preliminary of Homeomorphism and Formulation In topologie [26], the two objects are homeomorphic means they have topological equivalence. There is bijective and continuous function (like Fig.2 A)) between them for topological-preserved transformation (homeomorphism), and each point of one object corresponds to point of the other object. Lets define two objects in topological space, and , where the and are the point sets of the objects (e.g., the semantic regions in images). There is mapping function : that transforms the to the . If the JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5 Simple Discussion: The hypothesis of the homeomorphism prior in medical images is based on an ideal situation, i.e., the images have the exactly same content (one-by-one correspondence of the semantic regions). However, due to the potential difference in the scanning fields and the human body, medical images do not always have the same content. For example, the cardiac CT images in large scanning window will have whole lungs, but if they are in small scanning window, the images will only have small part of the lungs. This makes only the shared parts homeomorphic and the whole images are not. Fortunately, in our method, we construct this homeomorphism mapping function by deformation and use 0 value to fill the hole caused by deformation. This means that we append point set of all zeros on the original image point set so that the points between the medical images that do not meet homeomorphism will correspond to these zero points, thus finally making the whole point set meet the homeomorphism. It is formulated as {xA, ϕ0} = {xB, ϕ0}, where the ϕ0 is the zero set has no gradient in training. Therefore, it enables us only need to focus on the homeomorphic part between medical images. 3.2 Deformable Homeomorphism Learning (DHL) As shown in Fig.3 a), the DHL leans deformable mapping that transforms the space of one medical image to the other image, thus driving the soft learning of feature pairs. It has two shared-weight networks which learn representation (backbone) Nθ and network that learns deformation (deformer) Dξ. Two medical images sampled from the dataset {xA, xB} are putted into the backbone networks to extract their features = Nθ(xA), = Nθ(xB). These features are put into the deformer networks Dξ in order ([f A, B]) and reverse order ([f B, A]) to estimate their deformable mapping functions, i.e., DVF ψAB Rn from xA to xB, and DVF ψBA Rn from xB to xA. It is formulated as ψAB Rn = Dξ(f A, B), ψBA Rn = Dξ(f B, A). (2) According to the homeomorphism prior (Equ.1) which constructs continuous and bijective correspondence in DVF, we train the network to learn these properties: 1) For correspondence, we propose geometric semantic similarity LGSS for efficient measurement of the correspondence degree and drive the deformer to predict reliable DVF together with the original geometric visual similarity LGV in GVSL [33] (Sec.3.3). 2) For continuity, we utilize smooth loss Lsmo to constrain the DVF ψRn to perform topological-preserved (smooth) transformation, i.e., the deformable mapping, so the semantics of the regions inner medical images will be preserved, improving the reliability of correspondence: Lsmo(ψRn) = (cid:88) pRn ψp2, (3) Fig. 3: Our GEMINI embedded the homeomorphism prior in medical images achieves reliable correspondence discovery in DCRL. It has two aspects: a) The DHL (Sec.3.2) learns deformable mapping for soft learning of negative pairs. b) The GSS (Sec.3.3) fuses semantic similarity into the measurement of correspondence degree to construct the positive pairs reliably. The opt is the optimization. More details are described in Sec.D of our Supplementary Materials. is bijection, is continuous, and its inverse function 1 is also continuous, this mapping function is homeomorphism, the two objects are homeomorphic = . Based on the above theory, the bijection and continuity are two key elements in constructing homeomorphic mapping for our reliable correspondence discovery. Formulation: Due to the consistency of human anatomy and the intrinsic topology property of images [64], numerous categories of medical images scanned from the same human body region are approximately homeomorphic in topologie. Lets denote two same category medical images xA and xB sampled from the dataset are homeomorphic xA = xB. The individual variations of human body, such as height, figure, and scanning posture, make the semantic regions position in image space different between medical images. Therefore, this enables us to construct homeomorphism mapping function ψRn that transforms the pixel positions in the image space Rn with dimensions for the alignment of images. It is formulated as: ψRn (xA) = xB s.t. xA, ψRn (I) xB xB, ψ1 Rn (J) xA, (1) where the and are subsets of xA and xB. The xA, ψRn (I) xB means that the mapping ψRn is topological-preserved (continuous), and the xB, ψ1 Rn (J) xA further means that the ψRn is bijective and there is continuous inverse mapping ψ1 Rn . where the is the position of the pixels in image space Rn, the ψp is the gradient of position p. Therefore, it will avoid over-transformation which breaks the topological structures of semantic regions, and keep the homeomorphic JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 property of the mapping. Therefore, the loss LDM to learn deformable mapping (DM) is LDM (θ, ξ, {xA, xB}) = λsmoLsmo(ψAB Rn ) (cid:125) (cid:124) (cid:123)(cid:122) Continuity (4) + λGV SLGV S(xA, xB, ψAB (cid:123)(cid:122) Correspondence Rn ) + LGSS(f A, B, ψAB Rn ) , (cid:125) (cid:124) where the λsmo is the weights for the smooth loss and λGV is the weight of the geometric visual similarity. 3) For bijection, we simultaneously learn forward deformable mapping and inverse deformable mapping, thus training the deformers to learn the symmetry between two medical images and constructing bidirectional optimization objective in our DHL LDHL: LDHL = LDM (θ, ξ, {xA, xB}) + LDM (θ, ξ, {xB, xA})) (cid:125) (cid:123)(cid:122) Bijectivity: xAxB , xBxA (cid:124) (5) Therefore, during the learning of bijective deformable mapping, the deformers will try to learn reliable correspondence between images in the DVFs (ψAB Rn ). The gradient of the loss LDHL will optimize the backbones Nθ for soft learning of feature pairs implicitly, bringing reliable optimization for representation (analyzed in Sec.3.4). Rn , ψBA Simple discussion of the property: Our DHL serves as the framework of GEMINI, enabling homeomorphism mapping between images. By leveraging the key elements of continuity and bijection, it trains the backbone to extract distinct features for non-corresponding pairs and consistent features for corresponding pairs through deformable learning gradients. This soft feature learning mitigates false negatives caused by direct negative pair distinctions. However, the backbones weak initial representation, stemming from the two-player game learning process (Sec.3.4), is addressed by embedding foundational tasks in GEMINI variants to warm up learning (analyzed in Sec.6.1.8). 3.3 Geometric Semantic Similarities (GSS) Our GSS measures the correspondence (alignment) degree via the represented dense features to promote the learning efficiency of deformation to improve the alignment, thus constructing and learning the positive pairs reliably. As shown in Fig.3 a), the original GVS [33] utilizes the DVF ψAB Rn to transform the image xA to align the image xB, and calculate the distance of pixel intensity between the aligned image xAB and the image xB for their similarity. Due to the limitation of the appearance, the distance will be interfered which makes an unreliable measurement. Our GSS measure the similarity between the represented features (Fig.3 b)) further considering the important semantic information. It utilizes the DVF ψAB Rn to align the dense features to the dense features B, and calculate the distance of the features between the aligned features AB and the feature for their similarity. In our GEMINI, the original GVS still takes part in the measurement due to its training-invariability which will not be interrupted by the learning process for basic optimization objective. Different from the similarity measured throughout the whole space [3], our GSS or GVS measures the similarities only between the corresponding Fig. 4: The gradients from the loss of our GSS simultaneously train the explicit contrast of positive pairs and drive the implicit and soft learning in our DHL. positions of the features or images in the image grid. Therefore, this will make the measurement under the condition of semantic regions inherent topology which significantly reduces the searching space of the correspondence discovery, so we call the GSS and GVS geometric. As discussed in Sec.3.1, during the calculation of the geometric similarities, we only focus on the homeomorphic regions. Therefore, we utilize an adaptive mask mechanism to remove the zero point set and measure the alignment degree of the potentially shared regions between two images. Specifically, it generates mask ϵ with the same size as the image xA and the value of 1, and then transforms it to the space of xB via the DVF ψAB Rn for ϵAB. The void region caused by the transformation in the mask is filled with 0 which is the appended zero point set. Therefore, the regions with value of 1 are the shared regions between images, and those with value of 0 are the non-corresponding regions. Only the value 1 regions highlighted in the mask will be calculated for similarity. We utilize the normalized cross-correlation [41] for the GVS and the cosine similarity for our GSS with the mask ϵ into the measurement process: Lϵ GV = (cid:88) p{ϵAB =1} ((cid:80) ((cid:80) pi (xAB ˆxAB pi ))((cid:80) ˆxAB pi (xAB pi )(xB pi ˆxB (xB pi ))2 ˆxB pi )) , Lϵ GSS = (cid:88) p{ϵAB =1} AB f AB , (6) (7) where the ϵAB = ψAB AB = ψAB Rn (f A). Rn (ϵ), the xAB = ψAB Rn (xA), and the Our GSS will both drive the learning of homeomorphism mapping as loss in the DHL and train the reliable learning of positive pairs for their representation due to the homeomorphism-based correspondence. As shown in Fig.4, the gradient from the GSS will be divided into two parts, including the gradient for positive pairs (red path) and the gradient for the soft learning of feature (both positive and negative) pairs (blue path). The rad path directly learns the consistency of features in corresponding positions predicted by the deformer Dξ. Due to the homeomorphism prior, the correspondence discovery is under the condition of topological preservation which will be reliable for the contrastive learning of positive pairs. The blue path utilizes the gradient form the deformer Dξ to optimize the backbone network Nθ JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST indirectly, i.e., the learning in our DHL, bringing implicit and soft learning of feature pairs (analyzed in Sec.3.4). The whole optimization of our GEMINI can be modeled as two parts, including the optimization of parameters θ for representation and the optimization of the parameters ξ for deformable mapping: 7 ξ = arg min ξ LDHL(θ, ξ, S) s.t. θ = OPT (θ, θ LDHL(θ, ξ, S) θ (cid:124) (cid:123)(cid:122) Gradient for soft negative pairs ) (cid:125) (8) , Lpos(θ, S) + λposarg min θ (cid:124) (cid:123)(cid:122) Objective for positive pairs (cid:125) where the Lpos represents the red path in Fig.4 for positive pairs, λpos is the weight of Lpos, and the OPT() is the optimization strategy to minimize the LDHL for negative pairs (yellow path). If it is SGD [65] with one step, it will be θ θ η LDHL(θ,ξ,S) , where the η is learning rate. The two optimization parts correspond to the two forward paths  (Fig.4)  in the inference process, so the whole optimization process is compatible with the existing gradient descent methods. We utilize the Adam [66] for optimization. θ Simple discussion of the property: Our GSS, specific loss in GEMINI, facilitates correspondence learningkey to homeomorphism mappingby leveraging the deformer to align features based on the inherent topology of medical images. This enables reliable positive pair learning, addressing the large-scale false-positive problem. As part of DHL, GSS also supports soft feature learning via gradients. To overcome limitations in discovering positive pairs in non-homeomorphic regions, our adaptive mask mechanism highlights shared regions. 3.4 Intuitions on Behavior: Learning Reliable Positive and Implicit Negative Pairs for Dense Representation The two optimization objectives in Equ.8 for θ train the backbones to learn reliable positive feature pairs and implicit negative feature pairs, bringing an effective dense representation learning to the continuous image signal. For positive pairs, it effectively reduces the searching space of pairing, bringing reliable learning for positive pairs. Our homeomorphism prior makes the correspondence discovery under the condition of the consistent topology of images formulated as ci = Dξ(f A, B)i = ψAB Rn i. In the training of GEMINI, the ψAB Rn have to meet the minimization of Lsmo, LGSS, and LGV which make the ψAB Rn have topology-preservation ability and good alignment degree. Therefore, as shown in Fig.5 a), it makes our positive feature pairing consider the topology, i.e., pairing features on topology manifold, efficiently reducing the searching space and improving the reliability. The objective for positive pairs arg minθ Lpos(θ, S) learns their consistency on this manifold. The feature (q = 7 in Fig.5) from the dense features and the feature from the dense features will be consistent, i.e., = . For negative pairs, it constructs implicit learning via the gradient OPTθ(θ, LDHL(θ,ξ,S) ) from the DHL, avoiding the direct division of negative pairs and learning soft contrast. Specifically, the deformer network Dξ learns to discover the correspondence of the consistent features {f A, B}, thus θ Fig. 5: Intuitions on behavior. a) The two optimization objectives in Equ.8 for θ drive the reliable learning of positive and implicit learning of negative pairs. b) The feature pairs are learned softly via the gradient from the DHL. 0 , 1 , 2 , ..., r2 = {f driving the backbone represent distinct features for the features with inconsistent semantics. Fig.5 b) shows an example of this process. The features rr} is the image xAs features in the receptive field of the Deformer network Dξ, where the is the width of the receptive field (it is 26 in our experiments). Due to the learning of is constrained to positive pairs, the where the is the corresponding position. Therefore, as demonstrated in Fig.5 a), in order to discover the corresponding position, it will train the features in non-corresponding position to distinct to the feature via gradient. r2 ) = ψBA , = B Dξ(f s.t. , , ψBA is (9) Throughout the whole training process, the learning of representation in the backbone network Nθ and the deformable mapping in the deformer network Dξ is two-player game [67]. The Dξ learns to estimate the correspondence of semantic regions from the represented dense features A, and measure their pixel displacement. The Nθ learns to provide features of semantic regions to the Dξ for their correspondence. To achieve more accurate correspondence, the deformer network Dξ has to drive the backbone network Nθ to output more distinct features in turn for different semantic regions via the gradient in backpropagation. Therefore, under this interaction, the Nθ will provide more representative features for the Dξ to improve the correspondence estimation, and the Dξ will have more powerful ability to learn the correspondence of pixel-wise features. This process needs promising representation in backbone, but it is always unavailable at the beginning of training. Therefore, we train the GEMINI with fundamental learning task, e.g., restoration [68], [69] in our implementation. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 TABLE 1: Total seven publicly available datasets are involved in this paper for the experiments of our GEMINIs variants, achieving great reproducibility. Dataset Type ASOCA [71] CAT08 [72] WHS-CT [73] CANDI [74] SCR [75] KiPA22 [9] ChestX-ray8 [76] 3D cardiac CT 3D cardiac CT 3D cardiac CT 3D brain MRI 2D chest X-ray 3D kidney CT 2D chest X-ray Num 60 32 60 103 247 130 112,120 FS-Semi SS-MIP"
        },
        {
            "title": "4 EXPERIMENT 1: FEW-SHOT SEMI-SUPERVISED\nMEDICAL IMAGE SEGMENTATION (FS-SEMI)\nlearning on few-shot\nWe\nimplement our GEMINI\nimage segmentation\nsemi-supervised (FS-Semi) medical\n(GEMINI-Semi) providing a variant on the situation that\nlabels are very few. Three public-available tasks are enrolled\nin our experiments for a very complete evaluation.",
            "content": "4.1 Experiments configurations 4.1.1 Variant design The variant of our GEMINI-Semi learns segmentation head Segκ on the extracted dense features A, B. Therefore, except the optimization for deformable homeomorphism learning LDHL, the GEMINI-Semi also has an additional optimization for segmentation LSeg: arg min ξ,θ,κ (LDHL(θ, ξ, Sul) + LSeg(θ, κ, Sl)), (10) where the Sul and the Sl are the unlabeled dataset and the labeled dataset. In our experiment, we utilize the sum of Dice loss and cross-entropy loss [70] to train segmentation objective LSeg. The other compared DCRL methods (Sec.4.1.3) also use the same setting as this variant which adds the LSeg in the training to learn segmentation. 4.1.2 Datasets We evaluate GEMINI on three public tasks in 2D and 3D dimensions, showcasing its powerful representation ability in semi-supervised tasks [77], [78] with minimal labels (Tab.1). Task 1: FS-Semi cardiac structure segmentation (3D) targets seven cardiac structures on 3D CT images, combining WHS-CT [73] (20 labeled, 40 unlabeled), ASOCA [71] (60 unlabeled), and CAT08 [72] (32 labeled from2). Images are cropped and resampled to 144 144 128, with five-shot evaluation (5, 100, and 47 images as labeled training, unlabeled training, and testing sets). Task 2: FSSemi brain tissue segmentation (3D) involves 27 brain tissues on 3D T1 MR images from the CANDI dataset [74] (103 labeled). Cropped volumes of 160 160 128 undergo five-shot evaluation (5, 78, and 20 images as labeled training, unlabeled training, and testing sets). Task 3: FS-Semi chest structure segmentation (2D) focuses on three chest-related structures in 2D chest X-rays using the SCR dataset [75] (247 labeled) whose images are from the JSRT database [79], split into 5 labeled, 142 unlabeled, and 100 testing images for five-shot evaluation. All tasks use rotation [20, 20] and scaling [0.75, 1.25] for data augmentation. 8 Fig. 6: Our GEMINI-Semi has significant visual superiority on three FS-Semi medical image segmentation tasks. 4.1.3 Comparison setting We compare GEMINI-Semi with 19 widely-used methods and our GVSL [33] (CVPR 2023) to demonstrate its superiority. 1) We train U-Net [80] to establish upper and lower bounds using 5 labeled images (Five) and all labeled training data (Full). 2) Semi-supervised methods without homeomorphism prior (UA-MT [81], MASSL [82], DPA-DBN [83], CPS [84]) highlight the significance of prior knowledge for semi-supervised learning with limited labels. 3) Atlas-based methods with homeomorphism prior (VM [35], LC-VM [57], LT-Net [56]) illustrate the limitation caused by the inefficient correspondence learning. 4) Learning registration to learn segmentation methods with homeomorphism prior (DeepAtlas [63], DataAug [62], DeepRS [60], PC-Reg-RT [41], BRBS [8]) show gains from improved correspondence but are limited by pseudo-labels from unreliable GVS. 5) Dense contrastive representation learning methods without homeomorphism prior (VADeR [2], GLCL [49], DSCPM [1], PixPro [5], DenseCL [3], SetSim [4]) reveal FP&N problem in DCRL. For fairness, all methods use 2D/3D UNet [80] with group normalization [85] as the backbone. 4.1.4 Implementation and evaluation metrics In this task, our GEMINI-Semi is implemented by PyTorch [86] on NVIDIA GeForce RTX 3090 GPUs with 24 GB memory. We take Adam whose learning rate is 1 104 to optimize our framework for fast convergence. For task 1 and task 2, we sample two unlabeled images and one labeled image randomly in each iteration to save the memory for large 3D images, and for task 3, we sample 10 unlabeled images and 5 labeled images randomly in each iteration for 2D images. Following [8], we perform an affine transformation on these images via AntsPy3 to normalize the spatial system. We utilize the DSC [%] to evaluate the area-based overlap index and the average Hausdorf distances (AVD) to evaluate the coincidence of the surface [87]. 4.2 Results and Analysis 4.2.1 Quantitative evaluation shows metric superiority As shown in Tab.2, 19 compared methods demonstrate that the DCRL will greatly improve the representability, and the homeomorphism prior (HP) further improves the reliability of the representation learning. There are three interesting observations in Tab.2: 1) The semi-supervised methods are limited by the extremely few labels. They utilize the pseudo-label generation (UA-MT, CPS) or multi-task 2. http://www.sdspeople.fudan.edu.cn/zhuangxiahai/0/mmwhs/ 3. https://github.com/ANTsX/ANTsPy JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 TABLE 2: The quantitative evaluation demonstrates our powerful representation ability in FS-Semi tasks. Our GEMINISemi achieves the best performance on CT, MR, and X-ray images compared with 19 popular methods and the GVSL. The unable means that the extremely poor results make the AVD unable to be calculated. The - means that the setting is unable to be implemented. The HP means these methods have or do not have homeomorphism prior. T1, T2, T3 are the task 1, task 2, task 3. The red and blue values are the highest and the second-highest values in the columns. 9 Type Method HP T1: 3D cardiac structures T2: 3D brain tissues T3: 2D chest structures AVG DSCstd AVDstd DSCstd AVDstd DSCstd AVDstd DSCstd Full Five Semi Atlas LRLS DCRL U-Net [80] U-Net [80] UA-MT [81] CPS [84] MASSL [82] DPA-DBN [83] VM [35] LC-VM [57] LT-Net [56] DeepAtlas [63] DataAug [62] DeepRS [60] PC-Reg-RT [41] BRBS [8] VADeR [2] DenseCL [3] SetSim [4] DSC-PM [1] PixPro [5] GLCL [49] DCRL GVSL-Semi (CVPR) [33] (Ours) GEMINI-Semi - 84.39.6 66.416.2 87.45.4 77.48.7 68.014.5 81.06.1 81.76.0 77.87.8 87.94.3 82.25.2 87.05.0 88.54.9 91.13.9 85.44.7 87.34.3 87.04.5 87.04.6 89.53.9 84.57.0 90.03.7 91.23. - 2.432.14 4.692.27 1.400.76 9.073.11 5.753.89 2.130.78 2.040.77 2.250.95 1.300.57 2.040.73 1.600.90 1.230.72 0.930.57 1.690.77 1.520.79 1.600.84 1.600.86 1.310.75 1.821.09 1.210.81 0.970.56 88.71.2 69.58.8 75.53.4 37.11.8 80.53.1 68.78.2 83.11.8 83.01.8 82.61.2 79.32.6 83.91.2 73.05.9 73.13.1 87.21.0 81.23.2 83.91.9 81.23.0 82.62.1 86.31.2 83.02.7 82.35.9 87.31.0 0.310.04 1.590.84 1.310.95 unable 0.920.43 3.902.39 0.560.08 0.560.07 0.570.05 0.740.12 0.550.06 0.930.25 1.090.17 0.430.05 0.590.13 0.480.09 0.580.13 0.530.09 0.380.04 0.520.11 0.550.27 0.350.03 96.11.4 83.46.9 83.96.2 63.21.4 81.97.0 67.48.7 59.95.0 60.27.4 60.47.4 64.89.6 22.22.8 86.05.6 59.13.6 71.56.4 79.95.8 77.18.8 79.07.3 85.76.2 83.38.7 85.58.9 86.35.5 87.75.2 2.281.00 10.344.80 9.524.03 19.575.67 10.994.58 24.056.75 15.364.34 14.724.89 14.624.84 12.873.56 unable 8.553.98 20.715.21 10.852.99 8.953.37 12.116.51 11.725.03 7.333.32 8.734.55 8.655.18 7.184.01 7.143.63 - 79.18.4 75.38.6 62.62.9 79.96.3 68.010.5 74.74.3 74.95.1 73.65.5 77.35.5 62.83.1 82.05.5 73.63.9 83.33.8 82.24.6 82.85.0 82.44.9 85.14.3 86.44.6 84.36.2 86.25.0 88.73. learning (MASSL, DPA-DBN) to improve the representation, but the extremely few labels have no enough ability to give them reliable optimization directions to overcome the noise in pseudo labels or multiple tasks. As result, the UAMT, MASSL, and DPA-DBN have worse performance than U-Net on task 1, and the CPS is worse on task 2 and 3. 2) With the HP, the Atlas and LRLS methods achieve robust performance in task 1 and task 2, but are limited in task 3. The HP brings an alignment between labeled and unlabeled images for numerous reliable pseudo labels. Therefore, they have achieved significant improvement on task 1 and task 2 compared with the semi-supervised methods. However, the X-ray images in task 3 have low contrast and their appearances are varied caused by the 2D projection of 3D human body, this makes inefficient GVS brings large misalignment between images, thus interfering with the learning and reducing the performance. 3) The DCRL methods have robust performance in all three tasks compared with the LRLS methods, although the VADeR, DenseCL, SetSim, DSC-PM, PixPro and GLCL have no homeomorphism prior. Because their feature-level learning reduce the direct interference caused by misalignment in LRLSs pseudo labels and the supervision from the few labels bring basic representability which will promote their correspondence discovery. However, the FP&N problem is still problem in the learning and their performance on task 3 is poor without HP like the semi-supervised methods. Compared with the LRLS, other DCRL methods, and our previous GVSL-Semi, our GEMINI-Semi achieves the best performance on three tasks with four observations: 1) Compared with the LRLS methods which have HP, our method has better performance on all tasks. Although the BRBS has similar performance as our GEMINI-Semi on task 1 and task 2, our method achieves 16.2% DSC and 3.71 AVD higher and lower than it on task 3. This is because our GEMINI-Semi utilizes our GSS for alignment measurement and shares the representation between the segmentation and deformation learning, bringing more efficient and robust learning for alignment. It has great ability to construct positive feature pairs even with varied appearances. The gradient from our DHL also trains the soft negative feature pairs to drive the learning of distinct representations for potentially different semantics in shared backbones, bringing regularization for potential mispaired positive pairs. 2) Compared with the other DCRL methods which have no HP, our GEMINI-Semi shows great improvements in all three tasks. It achieves more than 1.7%, 1.0%, and 2.0% DSC improvements on task 1, 2, and 3 compared with the best DCRL models without HP (PixPro in task 1 and 2, DSC-PM in task 3). Because the HP in our GEMINISemi constructs more reliable correspondence discovery process which reduces the production risk of the FP&N pairs, bringing comprehensive improvement for the DCRL. 3) Compared to our CVPR vision (GVSL-Semi), we find even though the GVSL utilizes the visual similarity like the BRBS, it also achieves great performance in task 3, demonstrating the superiority of the DCRL paradigm. The GVSLsemi avoids the interference of pseudo labels like BRBS reducing the noisy information from the extremely misalignment, so that it takes the advantage of DCRL and our homeomorphism prior and achieves good performance in all three tasks. Our GEMINI-Semi promotes the GVSL and utilizes the GSS for more powerful dense representation learning, thus achieving the highest 88.7% average DSC in this experiment. 4) Compared with the fully supervised setting (Full) in task 2 (83 labeled images), our GEMINISemi achieves similar performance only with 5 labeled JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 images demonstrating our great potential in reducing of annotation costs. In the task 3, our framework is lower than the upper bound (96.1%) only with five annotations, but it still achieves significant improvement (4.3%) compared with the model directly trained on five labeled images. 4.2.2 Qualitative evaluation shows visual superiority As shown in Fig.6, we show typical cases on the three tasks in this experiment and our framework has higher accuracy on thin regions and fewer outliers. In the task 1, the segmentation result of our method has better integrity, and the different semantic structures have good adjacency without outliers. However, the other four DCRL methods have discontinuous mis-segmentation which destroys the heart topology. This is because the pairing strategies in the DCRL methods are unable to make the pairs under the condition of topology consistency, so the large-scale mispaired features interrupt the learning and make numerous outliers. The same as the task 3, there are also serious outlier problems in the four typical DCRL methods and the GVSL, and our GEMINI-Semi has fine segmentation. In the task 2, our GEMINI and GVSL show finer segmentation on the thin brain structures which is sensitive and will be interrupted by the noise in the semi-supervised learning process. In some prominent and gully regions of task 2 (enlarged part), the compared four DCRL methods have numerous distortions due to their unreliable correspondence discovery, showing their fragility."
        },
        {
            "title": "5 EXPERIMENT 2: SELF-SUPERVISED MEDICAL\nIMAGE PRE-TRAINING (SS-MIP)\nlearning on self-\nWe further implement our GEMINI\ntask\nsupervised medical\n(GEMINI-MIP), providing powerful tools to transfer poten-\ntial tasks and giving complete experiments. Four public-\navailable datasets are enrolled in our experiments for very\neffective evaluations.",
            "content": "image pre-training (SS-MIP) 5.1 Experiments configurations 5.1.1 Variant design Our GEMINI-MIP task is the pretext task to pre-train the representation of the backbone network Nθ and then the pre-trained network is transferred to the downstream tasks LDS. It also learns self-restoration head Resτ (fundamental task) on the dense features A, due to the initial weak representation in the pretext task for warm-up of our GSS. The self-restoration is based on the prior of edges and shapes in images and trains the network to capture the features from the broken distribution. Therefore, the variant framework for SS-MIP has an additional optimization for restoration LRes: Pretext: arg min (LDHL(θ, ξ, Sul) + LRes(θ, τ, ul), ξ,θ,τ Downstream: arg min κ LDS(θ, κ, Sl), (11) is the unlabeled dataset, and the ul where the Sul is the unlabeled dataset with the appearance transformation (Sul) = ul for self-restoration, the Sl is the labeled dataset in the downstream task, LDS is the loss for the 10 downstream task, and the κ is the parameters in the learning head of the downstream task. In our experiment, we utilize the mean square error as the loss for the self-restoration following [68], LRes(x, (x)) = Resτ (Nθ(T (x)))2, to train self-restoration objective. We utilize the random inpainting, local-shuffling, and non-linear transformation in the to transform the unlabeled images. 5.1.2 Datasets We evaluate the representation learning ability of our GEMINI-MIP on four datasets with three downstream tasks to demonstrate the properties and advantages of our method in different aspects. Pretext datasets: We utilize the ChestX-ray8 [76] which has 112,120 frontal-view chest X-ray images with 1024 1024 resolution and 0 to 255 grayscale values. 44,810 of them are scanned from the anterior to posterior (AP) view and 67,310 of them are scanned from the posterior to anterior (PA) view. 63,340 of them are male and 48,780 of them are female. In our experiment, we resize the images into 512 512 and normalize them to [0, 1]. For better homeomorphic property, we randomly pair these chest X-ray images with the same perspective (PA/AP) and gender (male/female). Downstream datasets: Three publicly available datasets (SCR [75], KiPA22 [9], CANDI [74]) are used to demonstrate the superiorities of our framework. Task 1: SCR dataset [75] segments 3 chest-related structures on 247 X-ray images. We set 47 of them as the validation set, 100 of them as the training set, and 100 of them as the testing set. We utilize 25% of the training set in this experiment (SCR25) to build limited data situation and more data amount evaluations are performed in our analysis (Sec.6). Task 2: KiPA22 dataset [9] segments four renal cancer-related structures on 130 3D CT images. We set 30 of them as the validation set, 70 of them as the training set, and 30 of them as the testing set. Task 3: CANDI dataset [74] segments 28 brain tissues on 103 3D T1 MR images. We set 20 of them as the validation set, 40 of them as the training set, and 43 of them as the testing set. For the 3D datasets used in the 2D task, we train the networks on the 2D slices of the images, predict segmentation results at each 2D slice and evaluate the results for 3D volumes. 5.1.3 Comparison setting We benchmark GEMINI against 17 state-of-the-art or widely-used methods across four categories and compare with our previous GVSL to highlight advancements. 1) 2D supervised network pre-trained on ImageNet [88] evaluates the representation ability of supervised learning. 2) Generative representation learning (GRL) methods (Denoising [89], In-painting [69], Models Genesis [68], Rotation [90]) provide baseline performance for classic approaches. 3) Contrastive representation learning (CRL) methods (SimSiam [50], BYOL [24], SimCLR [22], MoCov2 [23], DeepCluster [51]) reveal limitations of global contrastive representation on MIDP tasks. 4) Dense contrastive representation learning (DCRL) methods (VADeR [2], GLCL [49], DSC-PM [1], PixPro [5], DenseCL [3], SetSim [4]) highlight GEMINIs superior performance through reliable positive and negative pair learning. For downstream tasks, pre-trained 2D CNN feature extractors (backbone Nθ) are used in 2D U-Net JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 TABLE 3: The fine-tuning evaluations demonstrate our great transferring ability on SS-MIP tasks. Our GEMINI-MIP achieves the best performance compared with 18 methods on 3 downstream tasks. T1, T2, and T3 are the task 1, task 2, and task 3. AVG is the average value of the row. The cells with gray backgrounds are the inner-scene (same image category) transferring and the others are the inter-scene (different image category) transferring. 11 Type Pre-training T1: SCR25 Inner-scene T2: KiPA22 Inter-scene T3: CANDI Inter-scene AVG DSCstd AVDstd DSCstd AVDstd DSCstd AVDstd DSCstd - Sup GRL CRL DCRL Scratch (2D U-Net) ImageNet [88] Denosing [89] In-painting [69] Models Genesis [68] Rotation [90] SimSiam [50] BYOL [24] SimCLR [22] MoCov2 [23] DeepCluster [51] VADeR [2] DenseCL [3] SetSim [4] DSC-PM [1] PixPro [5] GLCL [49] DCRL GVSL-MIP (CVPR) [33] (Ours) GEMINI-MIP 81.88.2 92.03.1 83.97.8 85.16.6 86.14.6 80.57.7 87.25.1 89.44.9 89.04.0 84.36.5 84.08.1 85.25.1 85.06.3 85.25.1 90.53.5 91.53.3 87.35.8 89.73.7 92.12. 9.006.37 4.091.64 11.177.81 16.5910.74 6.222.27 20.6212.55 11.878.10 8.484.37 11.286.53 11.065.19 19.7113.15 7.153.05 11.747.02 9.637.64 5.442.87 9.835.34 9.354.68 10.527.23 5.382.65 74.112.3 72.615.5 60.317.7 64.416.4 66.616.3 69.715.3 72.613.3 74.112.6 74.411.3 69.614.4 72.715.1 62.815.6 70.814.8 70.814.4 77.212.2 73.612.9 76.511.9 78.911.2 79.111.1 3.591.97 4.784.86 7.555.18 5.794.13 5.863.14 6.454.33 4.103.25 3.872.93 3.682.65 6.284.70 4.913.50 7.234.73 5.483.95 4.923.26 3.873.34 4.003.33 4.332.94 2.951.55 3.222.24 65.04.4 71.119.8 67.72.1 66.22.3 88.13.1 78.32.6 76.72.1 70.52.1 79.02.6 82.93.6 60.02.2 86.13.4 76.82.9 74.92.5 83.32.4 63.92.0 82.82.6 89.72.6 89.82.6 1.270.21 1.352.07 1.210.08 1.260.08 0.320.10 0.750.09 0.820.06 1.080.07 1.020.40 0.520.12 1.520.07 0.400.12 1.220.68 0.890.08 0.740.64 1.350.06 0.560.09 0.270.08 0.270.08 73.68.3 78.612.8 70.69.2 71.98.4 80.38.0 76.28.5 78.86.8 78.06.5 80.86.0 78.98.2 72.28.5 78.08.0 77.58.0 77.07.3 83.76.0 76.36.1 82.26.8 86.15.8 87.05. by our homeomorphism prior. We can find two interesting observations in Tab.3: 1) The pre-training will bring better performance than random initialization (Scratch) to most of the networks. This is because the learned representation from the pretext task with large-scale data will stimulate the network to learn diverse low-level patterns, although the FN or FP problem will interrupt the representation learning of high-level semantics, the diverse low-level patterns knowledge will promote the transferring. 2) Due to the large interference of FP or FN problem, the CRL and DCRL methods all have weaker performance than the network pretrained by ImageNet in task 1. This is because the FP or FN problem interrupts the representation learning of highlevel semantics and makes their representations deviate from reality. Therefore, even though these self-supervised pre-trained networks have improved the learning of downstream tasks, their upper limit is extremely limited. Compared with the other DCRL methods and the conference vision (GVSL), our GEMINI-MIP achieves the best performance with three observations: 1) Our GEMINI-MIP has great MIP ability with the highest DSC (92.1%, 79.1%, 89.8%) in all tasks. Although the DSC-PM also achieves great performance in these three tasks (90.5%, 77.2%, 83.3% DSC), its average DSC is still lower than our GVSL-MIP and GEMINI-MIP owing to the interference of FP&N. Our homeomorphism prior brings reliable correspondence discovery and significantly weakens the FP&N problem, thus greatly improving the representation. 2) Our GEMINI brings significant improvements in both inner-scene and interscene transferring tasks. It achieves very competitive 92.1% DSC on task 1, and the best score (79.1%, 89.8% DSC) in the other inter-scene transferring tasks. The PixPro has reasonable performance on inner-scene tasks (91.5% DSC), but it only has 73.6% and 63.8% DSC on the inter-scene tasks which is much lower than our framework. Because the reliable positive and negative pairs in our framework Fig. 7: Our GEMINI-MIP also has very significant visual superiority in the three downstream tasks. [80] (introduced in Sec.4), trained with combined Dice and cross-entropy loss (LDS). 5.1.4 Implementation and evaluation metrics All tasks are implemented by PyTorch [86] on NVIDIA GeForce RTX 3090 GPUs with 24 GB memory, optimized by Adam [66] whose learning rate is 104. The pretext task is trained with 2 105 iterations. We utilize the fine-tuning evaluation on the downstream tasks. The downstream tasks are trained with 4 104 iterations and validated every 200 iterations to save the best models on their validation sets. For fair comparison, all methods in our experiment take the same basic training setting. Ten 2D X-ray images are randomly sampled in each iteration for stable pre-training, and five 2D images are randomly sampled in each iteration of downstream transferring. We use the same metrics as our experiment 1 (Sec.4) for the evaluation of the performance. 5.2 Results and Analysis 5.2.1 Quantitative evaluation shows metric superiority As shown in Tab.3, the fine-tuning evaluation demonstrates the great transferring ability of our GEMINI-MIP due to the reliable positive and negative pairs discovery promoted JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12 Fig. 8: The ablation studies on the T1: 3D Cardiac structures and T3: 2D Chest structures demonstrate the great contributions of the components in our framework. The and are the learning for continuity and bijection. enable the network to pre-learn both low-level patterns and high-level semantics, this makes the pre-learned knowledge match the reality showing greater transferring ability. 3) The GVSL-MIP has achieved similar performance as our GEMINI-MIP in inter-scene transferring because of its geometric visual similarity which will learn soft negative pairs. Our GEMINI-MIP further takes the geometric semantic similarity and has achieved 2.4% DSC improvement in innerscene transferring. 5.2.2 Qualitative evaluation shows visual superiority As shown in Fig.7, the visualization of the segmentation results demonstrates our superiority in the SS-MIP tasks. Due to our reliable correspondence discovery, the pre-training process makes the network represent consistent and distinct features for the same and different semantic regions, having very effective initialization. Therefore, our GEMINI-MIP has very fine visualization than the compared DCRL methods and conference vision, GVSL-MIP. In task 3, the VADeR, DenseCL, and SetSim lose some very small brain structures which are sensitive and easy to be interfered with by the misguidance from the FP&N problem. The VADeR also have very poor performance on mixed kidney region in the tasks due to the FN problem which enlarges the networks challenge to distinguish these complex regions."
        },
        {
            "title": "6 DISCUSSION AND ANALYSIS\n6.1 Framework analysis",
            "content": "6.1.1 Ablation study shows improvements of components The ablation studies on the T1: 3D Cardiac structures and T3: 2D Chest structures demonstrate the great contributions of our innovations. It has two observations: 1) The innovations in our framework all contribute to the performance. When we add the GVS and GSS into the correspondence learning, the two tasks all achieve very significant improvement compared with the direct segmentation learning on few labeled images. The smoothness loss Lsmo for the continuity further improves the deformation accuracy and the smoothness (topology-preservation ability), so that the two tasks all achieve further improvement. Finally, when adding the loss for bijection, these tasks all obtain the highest segmentation DSC. This illustrates that learning under the condition of medical images topology will improve the representation of the backbone network Nθ, bringing better performance on target tasks. 2) Compared between T1 and T3, their learning only with GVS have very different Fig. 9: The ablation of the hyper-parameters on the T1: 3D Cardiac structures show the effects from the weight of the smoothness loss λsmo, the positive pairs λpos, and the GVS loss λGV S. The Jϕ 0 (%) is Jacobian matrix [41] which evaluates smoothness of the deformation. results due to the appearance of their images. The learning with GVS in T1 improves the segmentation performance, but it in T3 extremely weakens the performance. Because the cardiac CT images in T1 are enhanced by contrast agents, they have distinct regions and will provide guidance to learn the correspondence. However, the chest X-ray images in T3 are projected from 3D human body and have very low contrast, so they are unable to measure the alignment degree and interfere with the correspondence learning. When adding our GSS, the performances in these two tasks are all improved. Because the measurement of the features will avoid interference from the appearance limitation, thus achieving better optimization guidance for correspondence. 6.1.2 Analysis of hyper-parameters We analyze the three hyper-parameters in our framework, i.e., the weight of the smoothness loss λsmo, the positive pairs λpos and the GVS loss λGV S. With the enlarging of these three hyper-parameters, the segmentation performances of our framework are all improved and then weakened. Because: a) The smoothness loss improves the topology preservation ability and reduces the deformation degree. When the λsmo is small, the deformation accuracy (orange line) and topology preservation degree (blue line) are improved, promoting the reliability of correspondence. However, when the λsmo is large, excessive smoothing reduces the deformation accuracy, weakening the segmentation. b) The learning of positive pairs clusters the features in corresponding positions, making better representation for the same semantic features. However, when the λpos is too large, the gradient from this loss will be much larger than the optimization for the negative pair OPT(), which makes the model tend to represent all features as consistent reducing their discrimination. The positive pairs will be further analyzed in Sec.6.1.4. c) The training-free property of GVS improves the deformation accuracy and stabilizes the correspondence training when the λGV is small. However, the problem of appearance variation in the images is enlarged when the λGV is large, so the unreliable similarity will give an inaccurate optimization target, weakening the deformation accuracy and smoothness, and reducing the final segmentation performance. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13 Fig. 10: The visualization of the deformation in the learning process on the T1: 3D Cardiac structures. The first row is the line chart of the segmentation and deformation performance. The second row is the grids which demonstrates the deformation degree. The third row is the deformed image during the learning process. TABLE 4: The learning for positive pairs Lpos in different setting on the T1: 3D Cardiac structures. The No Lpos means the training without positive pairs. The Half Lpos means when training to half of the total iteration amount, the learning of positive pairs is added. The Full Lpos means the whole training process with positive pairs. Type DSCstd No Lpos 90.33.6 Half Lpos Full Lpos 90.53.5 91.23. 6.1.3 Analysis of deformation in learning process As shown in Fig.10, the deformation in our GEMINI-Semi demonstrates the great correspondence of the semantic regions between images. During the training, our model will quickly learn the correspondence of semantic regions between images in the beginning, so the deformation DSC is 78.1% and the segmentation DSC is 90.3% in the 8 103th iteration (1/5 of all iteration amount). These scores in this iteration have been very close to the final scores (78.4% and 91.2%). Visually, the deformed image in the 8 103th iteration also has very high alignment degree to the target image B. In the later learning iterations, the segmentation DSC is improving slightly and up to 91.2%, although there is no improvement in the deformation DSC. This is because the learn reliable correspondence is still promoting the representation learning via gradient, and the great alignment provides the reliable learning of positive pairs. 6.1.4 Analysis of positive pairs Lpos As shown in Tab.4, the learning of our positive pairs Lpos in different settings demonstrates its reliability. Due to the potential misalignment between images at the beginning of the training, the positive pairs will be constructed between misaligned regions, making some potential false positive pairs. Therefore, this experiment performs three situations to evaluate this potential problem. Without the learning of positive pairs (No Lpos), our GEMINI-Semi has 90.3% segmentation DSC. When adding the learning of positive pairs at half of the total iteration amount (Half Lpos), it has 90.5% DSC which has 0.2% improvement. When directly adding the positive pairs in the whole process (Full Lpos), it brings 0.9% segmentation DSC improvement owing to Fig. 11: The pre-training and fine-tuning data amount analysis on SS-MIP. the constraint for the feature consistency. As conclusion, the potential problem of false positive pairs in our task has less influence on learning, because the alignment accuracy is improved fast as demonstrated in Fig.10, and the potential interference is reduced in most of the training process. Therefore, we adopted the Full Lpos in our framework. 6.1.5 Analysis of pre-training data amount in SS-MIP As shown in Fig.11 a), our GEMINI-MIP has higher representation efficiency with fewer pre-training data. For the DenseCL, when enlarging the pre-training data amount to 75%, it even has weaker performance than the 25%. This is because when the enlarging of pre-training data amount, the contribution of the low-level texture knowledge is reducing and the high-level semantic knowledge is improving. The large-scale FP&N problem makes the DenseCL learn the high-level semantics which deviate from reality, forbidding the improvement from larger datasets. The improvements of the BYOL and Model Genesis all slow down, owing to the limited representation ability in generation-based learning and the learning without negative pairs. Our GEMINI-MIP has achieved the best performance only with 25% data which is higher than the BYOL with 100% data, owing to our reliable learning of positive and negative pairs. 6.1.6 Analysis of fine-tuning data amount in SS-MIP In the inner-scene situation (Fig.11 b)), our GEMINI-MIP also has better-transferring ability with fewer data, demonstrating our great data efficiency and cost-saving ability. In the T1: SCR, we evaluate the transferring performance with the enlarging of the downstream data amount in the SCR dataset (25%, 50%, 75%, 100%). Our GEMINI-MIP achieves the highest performance in all data amount settings. Especially with only 25% data, it achieves higher performance than the DenseCL, Model Genesis, and Scratch models with 100% data, and similar performance as the BYOL with 75% data. Due to the FP&N problem, although the DenseCL has higher performance than the Scratch, its improvement is very slight. Without the FP&N problem, the BYOL shows more competitive performance improvement, but it is still lower than ours owing to the lack of negative pairs in the BYOL learning, limiting the discrimination of features. In the inter-scene situation (Fig.11 c)), our GEMINI-MIP still has better transferring ability than other methods. We further evaluate the transferring performance with the enlarging of the downstream data amount in the KiPA22 dataset (25%, 50%, 75%, 100%) for an inter-scene evaluation. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 Fig. 12: Our GEMINI-MIP has powerful learning efficiency both in the inner-scene and inter-scene transferring tasks. Although our GEMINI-MIP is unable to keep its performance when the data amount reduces which is different from it in the inner-scene situation, it still has the highest performance compared with the other four methods in each data amount. This is because our reliable learning of positive and negative feature pairs makes the networks pre-learn low-level patterns effectively which are shared in different tasks, so its representability of low-level features will transformed to different scenes, improving the transferring performance. The DenseCL, BYOL, and Model Genesis models only have similar or lower performance than the Scratch, because their pre-learned representations deviate from the reality due to their FP&N problems, and this property is still transformed to the other scenes making large limitations. 6.1.7 Analysis of the promotion for learning efficiency As shown in Fig.12, in the transferring of downstream tasks, our GEMINI-MIP has very powerful learning ability both in inner-scene and inter-scene situations. Compared with Scratch, our method has better performance both in accuracy (validation DSC) and learning speed. It only utilizes less than 5 103 iterations and can achieve the performance that exceeds Scratchs best model who has to train more than 20 103 iterations. Compared to the best models, our model also has more than 2.4% DSC higher than the Scratch. This is because our model provides reliable pre-trained initialization when learning the downstream tasks, promoting optimization efficiency. In the inner-scene transferring (T1: SCR25), the DenseCL has worse learning ability than the Scratch, because the large interference of the large scale FP&N problem which makes the initial representation deviate from reality. All four pre-trained models (GEMINI-MIP, DenseCL, BYOL, Model Genesis) have better learning ability than the Scratch in T3: CANDI (numerous small brain regions), because the pre-trained initialization will promote the perception for small structures which are challenging to learn by 2D network. 6.1.8 Analysis of self-restoration in GEMINI-MIP The self-restoration learns basic representation for the features, thus providing an efficient similarity measurement in our GSS and driving more efficient correspondence learning. As shown in Fig.13, we evaluate the GVS loss in the training process to demonstrate the improvement of correspondence degree. When only learning our GEMINI-MIP without the fundamental task, the networks initial weak Fig. 13: The necessity of the fundament in our GEMINI-MIP. When learning without the fundamental learning task, the GVS loss converges slowly due to the initial weak representation limiting the GSS and GVS for correspondence. When adding the fundament (self-restoration), warmup from the basic representation of semantic regions drives correspondence learning efficiently. representation makes inefficient learning of the correspondence, and until the late stage of the training, the GVS began to decline. When adding the fundamental pretext task, driven by the basic representation of semantic regions from the self-restoration, the GVS loss is converging continuously to learn the correspondence of semantic regions. 6.2 Observations and discussions 6.2.1 FS-Semi v.s. SS-MIP With our homeomorphism prior, our GEMINI has powerful representation learning ability with little (FS-Semi) or no (SS-MIP) supervision. In our experiment (Sec.4) of FS-Semi, the unlabeled images and the very few labeled images are learned together, so that the networks in the DCRL methods have supervised optimization target which makes basic distinction for the features of different segmentation regions. Therefore, this will improve the reliability of the correspondence discovery in the DCRL methods, and achieves more than 70% DSC on all tasks in FS-Semi. But the reliability in the existing DCRL methods is still weak without our homeomorphism prior, and they all only have lower performance than our GEMINI-Semi. In our experiment (Sec.5) of SS-MIP, the unlabeled images and the labeled images are trained separately in the pretext tasks and downstream tasks, so the networks in the DCRL methods have no supervision during their learning unlabeled images. Their correspondence discovery is extremely limited owing to the properties of medical images, constructing lot of FP&N pairs, and learning very poor representation. Therefore, in the downstream tasks, some of them even have worse performance than the Scratch. Based on our homeomorphism prior, our GEMINI-MIP has much more reliability to discover the correspondence even without supervision, achieving reliable positive pairs and implicit negative pairs for pre-training and powerful performance for transferring. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 TABLE 5: The comparison between our GEMINI-Semi with only GSS and with only GVS on the T1: 3D Cardiac structures demonstrates the advantages of our geometric semantic similarity. Type Segmentation Deformation DSCstd DSCstd Jϕ 0% Only LSeg No deformation GVS only GSS only 84.39.6 - 89.34.1 90.03.4 - 62.18.7 51.310.3 84.113.0 - - 40.81.2 10.22.2 6.2.2 GSS v.s. GVS The GSS has very effective measurement ability to guide the learning of deformation (correspondence). As shown in Tab.5, we perform the learning of correspondence with the GVS or with the GSS, and the losses for the continuity and bijection are removed to avoid their potential influence. The GVS only has 89.3% DSC on the segmentation which is 5.0% DSC higher than the Only LSeg, demonstrating the improvement from the correspondence learning. However, it only has 51.3% DSC on the deformation which is 10.8% lower than the images without deformation. This is because the appearance variation makes the GVS measurement unreliable, so it will train the network to learn the correspondence for those similar-but-different semantic regions (although it is still better than the direct correspondence discovery like [1][4]), finally the deformation will have very large folds (40.8% if it has no smoothness loss) limiting its performance. The GSS only achieves significant 32.8% DSC improvement and 30.6% Jϕ 0 reduction on the deformation compared with the GVS only owing to its measurement of the features which are robustness for the appearance variation. Therefore, better correspondence learning further promotes segmentation learning, achieving 90.0% DSC which is 0.7% higher. 6.2.3 Non-homeomorphic medical images Our basic hypothesis, homeomorphism prior, limits our GEMINI learning only to be trained between the medical images meeting homeomorphism. However, in the real world, there are large number of medical images that are unsatisfied with this prior, such as the images with lesions, whose features are unable to be paired only via deformation, hindering the learning of GEMINI. Fortunately, the homeomorphic medical images are easy to collect in real world. The physical examination or disease screening will accumulate large number of medical images without lesions every year [91], providing potential to collect big dataset meeting the hypothesis of our framework. This provides data support for our model and hypothesis to have sufficient clinical significance and scope of application. 6.2.4 Advancements of our preliminary work Our preliminary efforts [33] first presented the GVSL in 3D medical image SSP, learning the inter-image similarity for powerful representation and efficiently transferring to downstream tasks. This paper extends it (GVSL [33]) substantially on self-supervised representation learning with the advancements on principle, method, and application. 15 1) We have proposed the GEMINI learning which is novel paradigm for the large-scale FP&N problem in the DCRL with detailed motivation in Sec.1, and proposed new principle concept, the homeomorphism prior, behind the GVSL in Sec.3.1. 2) We have conducted more comprehensive review of the technological and theoretical research related to our task and provided clear overview of the field in Sec.2. 3) We have proposed novel similarity measurement strategy, the GSS, enabling the learned representation in turn to promote the correspondence discovery during the learning process, and promoting learning efficiency for correspondence discovery in Sec.3. 4) We have enlarged the application boundary of GVSL advancing the original model that only runs on 3D medical images to any dimension of medical images that satisfies homeomorphism prior. 5) We have extended our method to more kinds of representation learning tasks, advancing the original model that was only used in pre-training to the variants with both the pre-training and few-shot semi-supervised learning in Sec.4 and Sec.5. 6) We have carried out more experiments for performance analysis and comparison, thus more completely demonstrating the power of our GEMINI learning in Sec.6. 6.2.5 Future works The future works of the proposed GEMINI and the GVSL are in three aspects: 1) As discussed in Sec.6.2.3, one of our important future works is to expand the learning of correspondence to some images without homeomorphic topology, like the images with lesions [9], to cope with the large-scale FP&N problem in more images types. 2) Further explore the homeomorphism mapping between images and non-images, like the medical images and the deformed grids for super-pixel segmentation [92]. 3) Extend the pre-training to the datasets with multiple image categories, and evaluate the potential of the GVSL as foundation models [93], [94] for wider scenes. 4) Design lighter pre-training process to reduce the computing costs (Sec.C.5 of Appendix)."
        },
        {
            "title": "7 CONCLUSION\nIn this paper, we have advanced the homeomorphism\nprior in the open problem of large-scale FP&N pairs in\nthe medical image DCRL, and proposed the GEoMetric\nvIsual deNse sImilarity (GEMINI) Learning for a reliable\ndense correspondence discovery and learning. Based on\nour GEMINI, dense contrastive representation for medical\nimages is learned, effectively reducing the data and anno-\ntation costs in medical image dense prediction tasks. Its\nunique properties of learning implicit negative pairs in our\nDHL and positive pairs in our GSS have bright powerful\nperformance in few-shot semi-supervised medical image\nsegmentation tasks and self-supervised medical image pre-\ntraining tasks. We believe that our GEMINI in DCRL will\npromote the research of efficient learning in medical im-\nage analysis, and coping with the large challenge in data\ncollection and dense annotation. For intuition progress, the",
            "content": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 objects with the homeomorphic property are all able to construct reliable point-to-point correspondence discovery via homeomorphism mapping. This effectively promotes new representation learning paradigm based on topological consistency, and will inspire future researchers for more powerful innovations. APPENDIX SS-MIP ON MORE DATASETS A.1 Self-supervised pre-training on PPMI dataset We further evaluate the SS-MIP task on another pretext dataset for pre-training to demonstrate our representation ability. We extracted 837 3D brain T1 MR images with Parkinsons disease from the PPMI database4 as our pretext dataset. In our experiment, we extract the brain regions via HD-BET [96], crop and resize the images to 160 160 128, and finally normalize them via the zero-score. Due to the consistency of the human brain regions, we randomly pair these brain images to pre-train the frameworks. Following the Experiment 2 (Sec.5) in our manuscript, we take the Task 2: KiPA22 dataset and Task 3: CANDA as the downstream tasks to evaluate the inter-scene and inner-scene transferring abilities. (Because the Task 1: SCR25 dataset is 2D and the pre-trained models are 3D, we exclude this task in this experiment.) We utilize the same implementation and evaluation metrics as the Sec.5 in this experiment. As shown in Tab.6, it achieves similar observations as the SS-MIP experiment in Sec.5. For most of the methods, the pre-training on the PPMI dataset will bring better performance than random initialization (Scratch) both in the T2: KiPA22 and T3: CANDI tasks. Especially in the T3 (innerscene), most of the pre-training methods achieve more than 4.0% DSC improvement compared with the Scratch. Even though the other CRL and DCRL methods have FP&N problems in this experiment, they are still able to learn the representation of some domain features and promote their final performance to the upper limit of the task 3 (near 90%). When transferring the pre-trained models to the T2 (inter-scene), the SimSiam, BYOL, our GVSL-MIP, and our GEMINI-MIP all still have significant improvement (more than 10% DSC). This is because these methods learn the consistency of features and avoid the FP&N problems. The other methods performance improvement is obviously decreased owing to the FP or FN problem which interrupts their representation learning of high-level semantics and makes their representations deviate from reality. On both two tasks, our GEMINI-MIP achieves the highest performance showing our superiority. A.2 Analysis of the gap between the inner-scene and inter-scene transferring As shown in Tab.7, the quantitative evaluation of the gap between the inner-scene and inter-scene transferring show our great transferring ability both inner scene and inter scene. Here, we formulate gap coefficient to quantify this gap: Gi = Si inner Si inner S0 S0 inter inter , (12) 4. PPMI database: https://www.ppmi-info.org/ 16 Fig. 14: The evaluation of the large-scale FP problem. The true positive (TP) pairs constructed by the features similarity (used in DenseCL) only occupy the 5.79% of the foreground region, and our GEMINI is able to bring 60.74% TP pairs. inner S0 where the is the index of the method, is the score of the method (here we take the DSC). The S0 inter is the gap of the Scratch between the two settings which means the difference caused by the initial situation, such as network structure and dimension. The Si inter is the gap of the ith method between the two settings. Therefore, the means the gap of the model in two settings excluding the gap caused by the initial network. If the Gi is larger than 1, it means that the pre-trained model has weaker inter-scene transferring ability than inner-scene transferring. If it is smaller than 1, it means that the model has great inter-scene transferring ability. innerSi Si innerS0 S0 inner Si inter inter Most self-supervised learning methods have large gap between innerand inter-scene transferring, and our GEMINI has great universal representation for different scenes. The BYOL and DeepCluster are limited in the inter-scene transferring (G > 1) because they only take the imagelevel contrast which will represent the high-level semantic features and this representation is very different between scenes. The DenseCL has 0.57 gap coefficient which is better than the BYOL and DenseCL. Because it takes dense contrastive learning which also represent low-level detail features and this representation is shared in different scene. Our GEMINI and the Model Genesis all have good interscene transferring ability with very low gap coefficient (0.01 and 0.03), showing their great universal representation ability and demonstrating their potential as an initialization for more scenes."
        },
        {
            "title": "B DISCUSSION OF THE RESEARCH PROBLEM AND\nMETHOD",
            "content": "B.1 Discussion of FP&N problem As analyzed in the Introduction section, medical images semantic dependence property will make large-scale FP problem, and their semantic continuity and semantic overlap properties will make large-scale FN problem. In this section, we make an experiment to quantitatively count the percentage of FP and FN pairs in the pairing process. For FP pairs, we utilize two cardiac CT images (image and B), and extract their pixel-wise features via random initialized 3D U-Net. Then, we utilize the pixel-wise feature similarity measurement method in the DenseCL [3] to extract the positive pairs. Because the semantics of the background region are unclear, we count the accuracy of the feature pairs in the foreground regions. As shown in Fig.A, only 5.79% of the positive pairs in the foreground region are accurate. Therefore, if we directly pair the features only JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 TABLE 6: The fine-tuning evaluations demonstrate our great transferring ability on SS-MIP tasks which pre-trained on PPMI dataset. Our GEMINI-MIP achieves the best performance compared with 18 methods on two downstream tasks. 17 Type Pre-training T2: KiPA22 Inter-scene T3: CANDI Inner-scene AVG DSCstd AVDstd DSCstd AVDstd DSCstd - Sup GRL CRL DCRL Scratch (3D U-Net) Med3D [95] Denosing [89] In-painting [69] Models Genesis [68] Rotation [90] SimSiam [50] BYOL [24] SimCLR [22] MoCov2 [23] DeepCluster [51] VADeR [2] DenseCL [3] SetSim [4] DSC-PM [1] PixPro [5] GLCL [49] DCRL GVSL-MIP (CVPR) [33] (Ours) GEMINI-MIP 72.416.3 81.712.0 70.015.4 69.717.1 75.813.7 77.414.3 83.811.9 83.611.2 78.913.9 78.015.3 79.713.7 72.113.8 74.015.8 73.515.9 79.014.6 80.014.4 70.716.9 84.310.3 85.010.2 6.115.91 2.612.77 7.605.03 7.575.93 4.644.49 4.826.29 3.697.47 2.785.42 4.495.15 4.425.67 4.285.76 6.565.89 6.428.21 6.346.68 4.906.05 4.606.25 7.337.05 2.855.12 2.555. 84.03.2 72.719.0 83.73.3 88.53.1 88.73.1 89.42.6 87.33.1 89.72.4 89.23.0 89.72.4 89.82.4 87.43.6 87.73.8 88.43.1 88.53.4 89.92.4 87.43.2 89.12.8 90.02.4 0.520.14 1.572.56 1.710.20 0.320.11 0.310.10 0.280.08 0.360.10 0.270.08 0.300.14 0.280.11 0.270.08 0.350.11 0.340.13 0.320.10 0.320.13 0.270.07 0.340.09 0.310.11 0.260.07 78.29.8 77.215.5 76.99.4 79.110.1 82.38.4 83.48.5 85.67.5 86.76.8 84.18.5 83.98.9 84.88.1 79.88.7 80.99.8 81.09.5 83.89.0 85.08.4 79.110.1 86.76.6 87.56.3 TABLE 7: The gap coefficient Gi quantifies the gap between pre-trained on chest X-ray images & fine-tuning on brain T1 MR images (inter-scene) and pre-trained on brain T1 MR images & fine-tuning on brain T1 MR images (innerscene). Index Chest X-ray Brain T1 MR Gap Method 2D U-Net Inter-scene 3D U-Net Inner-scene Scratch BYOL DeepCluster Model Genesis DenseCL Our GEMINI-MIP 65.04.4 70.52.1 60.02.2 88.13.1 76.82.9 89.82. 84.03.2 89.72.4 89.82.4 88.73.1 87.73.8 90.02.4 Gi 1 1.01 1.57 0.03 0.57 0.01 0 1 2 3 4 5 Fig. 15: The FP and FN pairs have serious impact on learning. a) The fitting process with FP and FN pairs on cardiac CT image. b) The models learned segmentation ability on the fitted case and their generalization ability on another testing case. according to their similarity, most of the contrasts (94.21%) for positive pairs are inaccurate in the medical images and will interrupt the whole contrastive learning process. This is because medical images have very weak contrast due to their special imaging way, making the directly extracted features lack discrimination. Therefore, it makes the Semantic dependence one of the inherent properties in medical images constructing large-scale FP pairs. For FN pairs, we further evaluate the percentage of FN pairs caused by the semantic continuity and semantic overlap properties, and the results show large potential limitations in the DCRL. a) For the PN pairs caused by the Semantic continuity, we follow the SimCLR [22] which pairs the negative features for each feature. We pair the features in different positions of image As foreground regions as negative pairs. The result shows that 17.79% of the negative pairs are FN pairs which have the same semantics. Although the existing DCRL methods utilize attention [4] or clustering [1] to avoid directly dividing adjacent pixelwise features as negative pairs, the FN caused by semantic continuity is still an open and challenging problem. b) For the FN pairs caused by the Semantic overlap, we follow the DenseCL [3] which pairs the current features and the memory bank features as the negative pairs. We make the features of image in the foreground as the memory bank features and the features of image in the foreground as the current features. Then, we pair the current and memory bank features as negative pairs and calculate the accuracy. Finally, 17.53% of the negative pairs are FN pairs which have the same semantics. The Semantic overlap property of the medical images makes it inevitable that there will be numerous consistent semantic regions between medical images. Therefore, it will produce 17.53% FN pairs in the training process making the model learn in an unreliable direction. According to the above probability of FP and FN pairs, we simulated the number of these FP and FN pairs in supervised heart segmentation learning task. Specifically, we train U-Net on the cardiac structures segmentation task with cardiac CT image (Image in Fig.14) to evaluate the fitting ability of the model with or without FP&N pairs. a) In the non-FP&N pairs setting, we utilize the contrastive segmentation learning like Wang et al. [97]. b) In the FP&N pairs setting, we randomly generate FP (94%) and FN (17%) pairs in the contrastive segmentation learning. c) We further reduce the probabilities of FP and FN pairs to one-third of JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 original (31% and 6%) to give an ablation of the false pairs degree. We take 1000 iterations, and draw the loss values of the learning process on line chart to visualize the fitting process. We also evaluate the segmentation of the fitted case and another testing case (Image in Fig.14) to evaluate the model learned representation with false pairs. As shown in Fig.15, the FP and FN pairs have serious impact on learning. Without the FP&N pairs, the model is able to be fitted to the target cardiac images, and learn the representation ability of the semantic regions. However, when learning with large-scale FP&N pairs (94%, 17%), the model is unable to be fitted to the targets owing to the interference of the noisy optimization targets. When reducing the FP&N degree to one-third, the model is able to be gradually fitted to the target image and has certain generalization, but its performance is weaker than the no FP&N situation. Therefore, we can draw the following two conclusions in DCRL: a) the large-scale FP&N problem will make the model unable to learn representation; b) alleviating the FP&N degree, the model will be able to learn the representation ability of data with generalization ability. Therefore, our GEMINI embeds the homeomorphism prior to the DCRL for the large-scale FP&N problem, enhancing the learning of true feature pairs. Although it is challenging to remove FP&N pairs without annotation, reducing the FP&N degree via our GEMINI is still able to guides the model to learn generalizable representation. B.2 Discussion of the novelty in GEMINI The proposed GEMINI is novel dense contrastive representation learning paradigm in medical image analysis. Not only in the innovations, i.e., our DHL and GSS, it also achieved great novelty in principle. In principle, our GEMINI has advanced the theoretical foundation of homeomorphism for the dense contrastive representation learning, providing principle inspiration to the community. It modeled the human consistent anatomy in medical images based on the principle of topologie [31], proposed new principal concept, homeomorphism prior, and formulated it in the DCRL task as new paradigm. Therefore, the community will be further inspired by our principle of homeomorphism and make new scientific and technological progress in other tasks and fields. In methodology, our work has proposed novel dense contrastive representation learning framework that enables the contrast of feature pairs under the condition of human inherent topology, thus promoting the DCRL in medical images. It modeled the consistency of human inherent topology (i.e., homeomorphism prior) as learning for deformable mapping to overcome the reliability issue in DCRLs feature correspondence process, giving one potential answer to the long-standing question of how to achieve reliable dense feature correspondence for unlabeled data? Based on the modeling, the proposed DHL and GSS bring soft learning of feature pairs and reliable learning of positive pairs, promoting the contrast of features in DCRL. Finally, our work has achieved new ability to learn reliable semisupervised medical image segmentation and pre-training models. Fig. 16: The ablation study of the receptive field size and the network parameter amounts. a) The segmentation performance on the T1 of FS-Semi setting with the increasing of the receptive field size r. b) The fine-tuning performance with the enlarging of the parameter amount (million, ) in the pre-trained networks. MORE FRAMEWORK ANALYSIS AND EXPERIMENT DISCUSSION C.1 Discussion of the receptive field in the Deformer network The performance is robust for the receptive field r. As shown in Fig.16 a), we enlarge the receptive field via adding the depth and down-sampling stages of the Deformer network and evaluate the models performance on T1 of FS-Semi setting. With the enlarging of the receptive field, the models performance is stalely around 90% DSC. Because the backbone network and Deformer network together constitute whole network to learn the feature representation, and the features from the backbone network have been extracted from large receptive field. Therefore, even the receptive field of the Deformer network is small, the final DVF is still calculated from large receptive field. The layers inner the backbone is still optimized by the gradient with big reception, so that our GEMINI keeps stable performance with the enlarging of r. Owing to the soft learning of feature pairs in our DHL, once added this module, the framework achieves more than 5% DSC improvement. C.2 Discussion of the parameter amount As shown in Fig.16 b), we have evaluated our GEMINI on different settings of model parameters. We pre-trained our GEMINI-MIP on the ChestX-ray8 dataset for the networks with 0.12M , 0.49M , 1.97M , 7.85M , 31.39M , and 125.52M (M is million) parameters, and fine-tuned them on the T1: SCR25 task. With the enlarging of the network, the model performance is improving quickly. This is because the network capacity increases with the enlarging of the networks so that it will be able to learn the representation of more features in the pre-training process. When the parameter amount is 1.97M, the speed of performance improving is reduced, illustrating that the increase of network capacity has approached the upper bound of this task. Therefore, when the network is further enlarged to 125.52M (more than 50 times compared with 1.97M ), the performance is only improved 1.4% DSC. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 patibility across CNN-based (U-Net [3]), transformer-based (SwinUNet [2]), and CNN-transformer-based (TransUNet) networks. For U-Net and TransUNet that utilizes CNN to encode and decode features, our GEMINI has similar significant improvement that achieves 88.7% and 87.9% on AVG DSC. SwinUNet takes patch-embedding and fourtimes down sampling at the beginning, and utilizes the shifted window to learn global features. Therefore, it is challenging to represent fine-grained dense features and makes the whole network easy to overfit to the global features when the amount of training cases is small. As result, SwinUNet has very poor performance on FIVE setting. When adding GEMINI, it learns the inter-image consistency for unlabeled images and effectively reduces the over-fitting, thus also achieving more than 20% DSC improvement. C.5 Discussion of the computing costs As shown in Tab.9, we compare the methods number of floating-point operations (FLOPs) in four architecture types in pre-training stage, i.e., 1Encoder that only runs an encoder in the pre-training, here, we take the Rotation method [90]; 2Encoder that runs two encoders for contrastive representation learning in the pre-training, here we take the BYOL [24]; 1Encoder-decoder that runs an encoderdecoder network, like the U-Net, in the pre-training, here we take the Model Genesis [68]; 2Encoder-decoder that runs two encoder-decoder networks for dense contrastive representation learning in the pre-training, here we take the VADeR [2]. Our GEMINI is also 2Encoder-decoder method. In the pre-training stage, all methods take U-Net (for Encoder-decoder) or the encoder part of the U-Net (for Encoder) as their backbone. In the downstream adaptation stage, all methods pre-trained parameters are used to initialize the U-Net to learn segmentation task (T1: SCR25) and the part without pre-training is initialized randomly. All methods utilize same input sizes with [300300] in pretraining stage and [512512] in downstream stage. Owing to two additional deformer networks to learning the homeomorphism mapping, our GEMINI has the highest FLOPs in the pre-training stage, but it greatly contributes to the pre-training performance. In the downstream stages, owing to all methods take the U-Net with same parameter amount, our GEMINI has same FLOPs as other methods. As results, our GEMINI has very significant performance improvement on the SCR25 task owing our reliable learning for positive and negative feature pairs. The large-scale FP&N problem in VADeR makes it has worse performance than the BYOL even it has larger FLOPs in pre-training. The additional the one-time cost of our GEMINI in the pre-training stage bring obtain better representation, effectively reducing the long-time computing costs in the downstream tasks. Because once the pre-training is completed, stronger representation will accelerate the convergence speed of the model on downstream tasks, thus reducing the long-time computing cost in the training of numerous downstream tasks. As illustrated in the Fig.12 of our paper, compared with the BYOL [24], DenseCL [3], Model Genesis [68], our GEMINI achieved better performance with fewer iterations, illustrating its potential in reducing the computing costs in downstream tasks. Fig. 17: The t-SNE visualization of the learned pixel representations. We provide the coordinates of pixels in zoomed view, indicating their spatial relationship. C.3 Discussion of the feature distribution As shown in Fig.17, we visualize the learned representation by our framework to demonstrate its effectiveness in distinguishing different semantic regions. In the three tasks of the SS-MIP experiment, we randomly select the slices or patches from the test datasets and extract their pixel-wise features via the backbone network initialized from scratch (a) and our GEMINI-MIP (b). Then, these features are zoomed by t-SNE [98] to two dimensions. As demonstrated in the enlarged region, the features from the Scratch model is mixed owing to its initial weak representation. The pixelwise features from our framework are clustered into several meaningful groups. Most of the pixels in each group are spatially close and in different groups are also spatially separated (indicated by their coordinates (c)) in the original image. Because our GEMINI discovers the correspondence of pixel-wise features based on the homeomorphism of human body and learns the representation according to the consistent context topology, the same semantic features which are spatially close will be clustered. C.4 Discussion of the cross-architecture compatibility As shown in the Tab.8, we perform the TransUNet [99] (CNN+transformer), SwinUNet [100] (Transformer), and UNet [80] (CNN) on the FS-Semi tasks with three datasets, and our GEMINI demonstrates great compatibility across these model architectures. There are two observations: a) Our GEMINI has achieved significant improvement on both TransUNet [99], SwinUNet [100], and U-Net [80]. Compared with the lower bound of the architectures that are trained only with five labeled images, our GEMINI has improved them more than 9% DSC on AVG owing to our learning of the homeomorphism mapping between medical images. Especially, for the 3D brain tissues segmentation, GEMINI achieves similar performance compared with the FULL setting (83 labels) only with 5 labels in all architectures, demonstrating our great potential in reducing of annotation costs. b) Our GEMINI has great architecture comJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 TABLE 8: The FS-Semi evaluations on U-Net [80], TransUNet [99], and SwinUNet [100] demonstrate the cross-architecture compatibility of our GEMINI. The - means that the setting is unable to be implemented. 20 Type Method T1: 3D cardiac structures T2: 3D brain tissues T3: 2D chest structures AVG DSCstd AVDstd DSCstd AVDstd DSCstd AVDstd DSCstd Five (Lower) Full (Upper) Semi (Ours) U-Net [80] TransUNet [99] SwinUNet [100] U-Net [80] TransUNet [99] SwinUNet [100] GEMINI+U-Net GEMINI+TransUNet GEMINI+SwinUNet 84.39.6 74.58.3 40.88.0 - - - 91.23.6 90.83.4 88.64.2 2.432.14 4.411.39 11.591.32 - - - 0.970.56 0.940.51 1.280. 69.58.8 67.45.4 67.85.3 88.71.2 85.71.2 82.82.7 87.31.0 84.41.3 79.95.0 1.590.84 2.020.46 4.040.39 0.310.04 0.430.05 0.540.15 0.350.03 0.450.05 0.620.20 83.46.9 76.58.2 63.911.5 96.11.4 95.22.1 95.31.2 87.75.2 88.45.7 86.27.8 10.344.80 16.596.53 14.268.91 2.281.00 2.781.35 2.170.65 7.143.63 8.634.68 6.344.34 79.18.4 72.87.3 57.58.3 - - - 88.73.3 87.93.5 84.95.7 TABLE 9: Owing to the additional deformer networks, our GEMINI has relatively higher computing costs in the pretraining stage, but it has same computing costs in the finetuning for downstream tasks as other methods and achieves much higher performance. Type Method Pre-training Downstream T1: SCR25 1Encoder 2Encoder Rotation [90] BYOL [24] 1Encoder-decoder Model Genesis [68] 2Encoder-decoder 2Encoder-decoder Our GEMINI VADeR [2] FLOPs 5.99G 11.98G 19.74G 39.67G 52.59G FLOPs 20.15G 20.15G 20.15G 20.15G 20.15G DSCstd 80.57.7 89.44.9 86.14.6 85.25.1 92.12.8 C.6 Discussion of the second-best models Compared with the second-best methods (BRBS [8] in Tab.2 and our CVPR version, GVSL [33], in Tab.3), we can find these methods also fused the homeomorphism prior into their framework, and their great performance demonstrates the great potential of this prior knowledge in medical images. In our Experiment 1: FS-Semi (Sec.4), the BRBS is learning registration to learn segmentation method whose registration part is based on the homeomorphism prior. Therefore, its powerful performance in the T1: 3D cardiac structures and T2: 3D brain tissues illustrate the advantages. However, the BRBSs visual similarity make it unable to generalize to the chest X-ray (T3: 2D chest structures) that has relatively low contrast as illustrated in Fig.8. Our GEMINI utilize the semantic similarity based on features and achieves significant improvement on this task, demonstrating our superiority. In our Experiment 2: SS-MIP (Sec.4), our CVPR version, GVSL, benefits from our homeomorphism prior, achieving second-best performance on the T2: KiPA22 and T3: CANDI. However, it also utilizes the visual similarity which is limited on the low-contrast images, i.e., the chest x-ray images in the pre-training dataset. Therefore, its pre-trained representation for chest x-ray is relatively weaker and limits its performance in the innerscene transferring. Our GSS improves the measurement of the correspondence degree, and drive the representation learning for low-contrast targets during pre-training. Therefore, our GEMINI has significantly improved the GVSLs performance on the T1: SCR25 task. C.7 Discussion of the reliability As shown in Tab.10, in the three tasks of our Experiment 1, we calculated the standard deviations (std) and the intertraining Pearson correlation coefficients (Cor) [101]. The results indicate that our GEMINI demonstrates strong reliability across different tested samples and training initializations. a) Reliability across samples: We evaluated the DSC and std of the performance across the tested samples. Our GEMINI-Semi achieved an average of 88.7% DSC with 3.3 std, indicating high performance with robustness across diverse samples, which supports its reliability in real-world applications. b) Reliability across training: We conducted test-retest reliability analysis [102] and reported the Cor for the performance when our GEMINI-Semi was trained twice from different initialization states. The Cors for all three tasks exceeded 0.95 demonstrating very high consistency between the two training sessions. Additionally, all pvalues were below 0.001, indicating significant consistency. Thus, our model shows excellent reliability across different initialization states, which supports its reliability in model implementation."
        },
        {
            "title": "D MORE DETAILS IN EXPERIMENTS",
            "content": "D.1 Details of the training diagram As shown in Fig.18, the training diagram introduces the details of our GEMINIs variants in SSP and Semi experiments. In the forward inference, as described in the Methodology section of our paper, two images xA, xB are put into two shared-weight backbones Nθ separately to extract the features A, B. The features are further put into two shared-weight deformers together to predict two DVFs ψAB, ψBA that are bidirectional. For the variant of GEMINISemi, labeled image xC is put into the shared-weight backbone Nθ, and then put into an additional segmentation head Segκ to predict the segmentation results ˆyC Seg. For the variant of GEMINI-MIP, an appearance transformed image xC (described in Sec.5.1.1) is put into the sharedweight backbones Nθ, and then put into an additional selfrestoration head Resτ to predict the restored image ˆyC Res. In the loss calculation, the smooth loss (Equ.3) is calculated on the DVFs ψAB, ψBA to learn the continuity of the deformable mapping, the GVS loss (LGV S, Equ.6) and GSS loss (LGSS, Equ.7) are calculated on the deformed images xAB = ψAB(xA), xBA = ψBA(xB) and deformed features AB = ψAB(f A), BA = ψBA(f B) (described in Sec.3.3) to learn the correspondence. For the variant of GEMINI-Semi, segmentation loss LSeg is calculated on segmentation result ˆyC Seg. For the variant of GEMINI-MIP, self-restoration loss LRes is calculated on Seg and the groundtruth yC JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 TABLE 10: The evaluation of our GEMINIs reliability on the tasks in Experiment 1. The Cor is the Pearson correlation coefficient [101], and the is the p-value. 21 Evaluations Tasks a) Reliability across samples b) Reliability across training T1: 3D cardiac structures T2: 3D brain tissues T3: 2D chest structures AVG DSC 91.2 Cor 0.989 std 3.6 <0.001 DSC 87. Cor 0.999 std 1.0 <0.001 DSC 87.7 Cor 0.968 std 5. <0.001 DSC 88.7 Cor 0.985 std 3.3 <0.001 pooling layers to reduce the feature maps resolution and in the decoding path, it takes up-sampling layers (bilinear for 2D images and trilinear for 3D images) to restore the features resolution. Skip connections are used to transmit features from the encoding path to the decoding path in each resolution stage. There are five resolution stages in the network and each stage utilizes Conv-GN-LeckyReLU5 modules to extract features. The deformer network also takes lightweight U-Net architecture with very shallow depth to estimate the DVF. It only has three resolution stages and each stage has half of the Conv-GN-LeckyReLU module amount compared with the backbone. Both the segmentation head and self-restoration head take one Conv-GNLeckyReLU module to project the input features and follow convolution layer to predict the targets. The detailed hyper-parameters inner these architectures are marked in Fig.19."
        },
        {
            "title": "REFERENCES",
            "content": "[1] [2] [3] [4] [5] [6] [7] [8] [9] X. Li, Y. Zhou, Y. Zhang, A. Zhang, W. Wang, N. Jiang, H. Wu, and W. Wang, Dense semantic contrast for self-supervised visual representation learning, in Proceedings of the 29th ACM International Conference on Multimedia, 2021, pp. 13681376. P. O. Pinheiro, A. Almahairi, R. Benmalek, F. Golemo, and A. C. Courville, Unsupervised learning of dense visual representations, Advances in Neural Information Processing Systems, vol. 33, pp. 44894500, 2020. X. Wang, R. Zhang, C. Shen, and T. Kong, Densecl: simple framework for self-supervised dense visual pre-training, Visual Informatics, 2022. Z. Wang, Q. Li, G. Zhang, P. Wan, W. Zheng, N. Wang, M. Gong, and T. Liu, Exploring set similarity for dense self-supervised representation learning, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 16 590 16 599. Z. Xie, Y. Lin, Z. Zhang, Y. Cao, S. Lin, and H. Hu, Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 16 68416 693. Y. Bengio, A. Courville, and P. Vincent, Representation learning: review and new perspectives, IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 8, pp. 17981828, 2013. C. You, Y. Zhou, R. Zhao, L. Staib, and J. S. Duncan, Simcvd: Simple contrastive voxel-wise representation distillation for semi-supervised medical image segmentation, IEEE Transactions on Medical Imaging, vol. 41, no. 9, pp. 22282237, 2022. Y. He, R. Ge, X. Qi, Y. Chen, J. Wu, J.-L. Coatrieux, G. Yang, and S. Li, Learning better registration to learn better fewshot medical image segmentation: Authenticity, diversity, and robustness, IEEE Transactions on Neural Networks and Learning Systems, 2022. Y. He, G. Yang, J. Yang, R. Ge, Y. Kong, X. Zhu, S. Zhang, P. Shao, H. Shu, J.-L. Dillenseger et al., Meta grayscale adaptive network for 3d integrated renal structures segmentation, Medical Image Analysis, vol. 71, p. 102055, 2021. 5. Conv is convolution layer and GN is group normalization layer [85]. Fig. 18: The overall training diagram of our GEMINI. a) The inference process of the whole framework. The gray path in the last line is the additional learning part in the variants of our GEMINI in self-supervised pre-training (GEMINI-MIP) and semi-supervised segmentation (GEMINI-Semi). b) The loss calculation to optimize the whole framework. Res and the original image yC the restored image ˆyC Res. The learning of self-restoration in SSP is fundamental task for warm-up of our GSS, due to the initial weak representation in the pretext task. D.2 Details of the architectures and implementation As shown in Fig.19, we utilize the U-Net [80] architecture (3D U-Net for 3D images and 2D U-Net for 2D images) as our backbone architecture for great basic dense representation in our experiment. In the encoding path, it takes max JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 22 Fig. 19: The detailed architecture of our GEMINI. a) The backbone architecture utilizes the 3D U-Net in 3D image tasks and 2D U-Net in 2D image tasks. b) The deformer network architecture utilized lightweight U-Net. c-d) The segmentation head in the variant of GEMINI-Semi and the self-restoration head in the variant of GEMINI-MIP. [10] C. You, R. Zhao, F. Liu, S. Dong, S. Chinchali, U. Topcu, L. Staib, and J. Duncan, Class-aware adversarial transformers for medical image segmentation, Advances in Neural Information Processing Systems, vol. 35, pp. 29 58229 596, 2022. [11] F. Piccialli, V. Di Somma, F. Giampaolo, S. Cuomo, and G. Fortino, survey on deep learning in medicine: Why, how and when? Information Fusion, vol. 66, pp. 111137, 2021. [12] V. Cheplygina, M. de Bruijne, and J. P. Pluim, Not-so-supervised: survey of semi-supervised, multi-instance, and transfer learning in medical image analysis, Medical image analysis, vol. 54, pp. 280296, 2019. [13] T. Milbich, Visual similarity and representation learning, Ph.D. dissertation, 2021. [14] B. Zhang, W. Zheng, J. Zhou, and J. Lu, Attributable visual similarity learning, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 75327541. [15] K. Roth, T. Milbich, and B. Ommer, Pads: Policy-adapted sampling for visual similarity learning, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 65686577. [16] C. You, R. Zhao, L. H. Staib, and J. S. Duncan, Momentum contrastive voxel-wise representation learning for semi-supervised volumetric medical image segmentation, in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2022, pp. 639652. [17] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian, J. A. Van Der Laak, B. Van Ginneken, and C. I. Sanchez, survey on deep learning in medical image analysis, Medical image analysis, vol. 42, pp. 6088, 2017. [18] C.-Y. Chuang, J. Robinson, Y.-C. Lin, A. Torralba, and S. Jegelka, Debiased contrastive learning, Advances in neural information processing systems, vol. 33, pp. 87658775, 2020. [19] C.-Y. Chuang, R. D. Hjelm, X. Wang, V. Vineet, N. Joshi, A. Torralba, S. Jegelka, and Y. Song, Robust contrastive learning against noisy views, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 16 67016 681. S. Zhou, D. Nie, E. Adeli, J. Yin, J. Lian, and D. Shen, Highresolution encoderdecoder networks for low-contrast medical image segmentation, IEEE Transactions on Image Processing, vol. 29, pp. 461475, 2019. [20] [21] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, Momentum contrast for unsupervised visual representation learning, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 97299738. [22] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, simple framework for contrastive learning of visual representations, in International conference on machine learning. PMLR, 2020, pp. 15971607. [23] X. Chen, H. Fan, R. Girshick, and K. He, Improved baselines with momentum contrastive learning, arXiv preprint arXiv:2003.04297, 2020. J.-B. Grill, F. Strub, F. Altche, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Pires, Z. Guo, M. Azar et al., Bootstrap your own latent: new approach to self-supervised learning, in Neural Information Processing Systems, 2020. [24] [25] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, Momentum contrast for unsupervised visual representation learning, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. [26] P. Alexandroff and H. Hopf, Topologie I: Erster Band. Grundbegriffe der Mengentheoretischen Topologie Topologie der Komplexe Topologische Invarianzsatze und Anschliessende Begriffsbildungen Verschlingungen im n-Dimensionalen Euklidischen Raum Stetige Abbildungen von Polyedern. Springer-Verlag, 2013. J. D. Ferrer, new privacy homomorphism and applications, Information Processing Letters, vol. 60, no. 5, pp. 277282, 1996. [27] [28] T. Heimann and H.-P. Meinzer, Statistical shape models for 3d medical image segmentation: review, Medical image analysis, vol. 13, no. 4, pp. 543563, 2009. [29] P.-L. Bazin and D. L. Pham, Homeomorphic brain image segmentation with topological and statistical atlases, Medical image analysis, vol. 12, no. 5, pp. 616625, 2008. [31] [30] M. I. Miller and L. Younes, Group actions, homeomorphisms, and matching: general framework, International Journal of Computer Vision, vol. 41, no. 1, pp. 6184, 2001. J. H. Hubbard and B. H. West, Differential Equations: Dynamical Systems Approach: Dynamical Systems Approach. Part II: Higher Dimensional Systems. Springer Science & Business Media, 1991. [32] F. H. Netter, Atlas of human anatomy, Professional Edition E-Book: including NetterReference. com Access with full downloadable image Bank. Elsevier health sciences, 2014. [33] Y. He, G. Yang, R. Ge, Y. Chen, J.-L. Coatrieux, B. Wang, and S. Li, Geometric visual similarity learning in 3d medical image self-supervised pre-training, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2023. S. Darkner, A. Pai, M. G. Liptrot, and J. Sporring, Collocation for diffeomorphic deformations in medical image registration, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 7, pp. 15701583, 2018. [34] [35] G. Balakrishnan, A. Zhao, M. R. Sabuncu, J. Guttag, and A. V. Dalca, An unsupervised learning model for deformable medical image registration, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 92529260. [36] G. Haskins, U. Kruger, and P. Yan, Deep learning in medical image registration: survey, Machine Vision and Applications, vol. 31, no. 1, pp. 118, 2020. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 23 [37] A. V. Dalca, G. Balakrishnan, J. Guttag, and M. R. Sabuncu, Unsupervised learning of probabilistic diffeomorphic registration for images and surfaces, Medical image analysis, vol. 57, pp. 226 236, 2019. [38] C. L. Nehaniv, K. Dautenhahn et al., The correspondence prob- [57] G. Balakrishnan, A. Zhao, M. R. Sabuncu, J. Guttag, and A. V. Dalca, Voxelmorph: learning framework for deformable medical image registration, IEEE Transactions on Medical Imaging, vol. 38, pp. 17881800, 2019. [58] Y. LeCun, Y. Bengio, and G. Hinton, Deep learning, nature, vol. lem, Imitation in animals and artifacts, vol. 41, 2002. 521, no. 7553, pp. 436444, 2015. [39] M. Brass and C. Heyes, Imitation: is cognitive neuroscience solving the correspondence problem? Trends in cognitive sciences, vol. 9, no. 10, pp. 489495, 2005. [40] B. Sch olkopf, F. Steinke, and V. Blanz, Object correspondence as machine learning problem, in Proceedings of the 22nd international conference on Machine learning, 2005, pp. 776783. [41] Y. He, T. Li, R. Ge, J. Yang, Y. Kong, J. Zhu, H. Shu, G. Yang, and S. Li, Few-shot learning for deformable medical image registration with perception-correspondence decoupling and reverse teaching, IEEE Journal of Biomedical and Health Informatics, 2021. [42] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum, Humanlevel concept learning through probabilistic program induction, Science, vol. 350, no. 6266, pp. 13321338, 2015. J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. ˇZıdek, A. Potapenko et al., Highly accurate protein structure prediction with alphafold, Nature, vol. 596, no. 7873, pp. 583589, 2021. [43] [44] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey et al., Googles neural machine translation system: Bridging the gap between human and machine translation, arXiv preprint arXiv:1609.08144, 2016. [45] H. Chen, J. Li, R. Wang, Y. Huang, F. Meng, D. Meng, Q. Peng, and L. Wang, Unsupervised learning of local discriminative representation for medical images, in Information Processing in Medical Imaging: 27th International Conference, IPMI 2021, Virtual Event, June 28June 30, 2021, Proceedings 27. Springer, 2021, pp. 373385. [46] Z. Gao, C. Jia, Y. Li, X. Zhang, B. Hong, J. Wu, T. Gong, C. Wang, D. Meng, Y. Zheng et al., Unsupervised representation learning for tissue segmentation in histopathological images: From global to local contrast, IEEE Transactions on Medical Imaging, vol. 41, no. 12, pp. 36113623, 2022. [47] R. De Maesschalck, D. Jouan-Rimbaud, and D. L. Massart, The mahalanobis distance, Chemometrics and intelligent laboratory systems, vol. 50, no. 1, pp. 118, 2000. [48] L. Wang, Y. Zhang, and J. Feng, On the euclidean distance of images, IEEE transactions on pattern analysis and machine intelligence, vol. 27, no. 8, pp. 13341339, 2005. [49] K. Chaitanya, E. Erdil, N. Karani, and E. Konukoglu, Contrastive learning of global and local features for medical image segmentation with limited annotations, Advances in neural information processing systems, vol. 33, pp. 12 54612 558, 2020. [50] X. Chen and K. He, Exploring simple siamese representation learning, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021, pp. 15 750 15 758. [51] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, Deep clustering for unsupervised learning of visual features, in European Conference on Computer Vision, 2018. [52] L. Jing, P. Vincent, Y. LeCun, and Y. Tian, Understanding dimensional collapse in contrastive self-supervised learning, in International Conference on Learning Representations, 2021. [53] T. Huynh, S. Kornblith, M. R. Walter, M. Maire, and M. Khademi, Boosting contrastive self-supervised learning with false negative cancellation, in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022, pp. 27852795. [54] H. Wang, J. W. Suh, S. R. Das, J. B. Pluta, C. Craige, and P. A. Yushkevich, Multi-atlas segmentation with joint label fusion, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 3, pp. 611623, 2013. [55] L. M. Koch, M. Rajchl, W. Bai, C. F. Baumgartner, T. Tong, J. Passerat-Palmbach, P. Aljabar, and D. Rueckert, Multi-atlas segmentation using partially annotated data: Methods and annotation strategies, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 7, pp. 16831696, 2018. S. Wang, S. Cao, D. Wei, R. Wang, K. Ma, L. Wang, D. Meng, and Y. Zheng, Lt-net: Label transfer by learning reversible voxelwise correspondence for one-shot medical image segmentation, in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 91629171. [56] [59] D. Shen, G. Wu, and H.-I. Suk, Deep learning in medical image analysis, Annual review of biomedical engineering, vol. 19, p. 221, 2017. [60] Y. He, T. Li, G. Yang, Y. Kong, Y. Chen, H. Shu, J.-L. Coatrieux, J.-L. Dillenseger, and S. Li, Deep complementary joint model for complex scene registration and few-shot segmentation on medical images, in European Conference on Computer Vision. Springer, 2020, pp. 770786. [61] Y. Ding, X. Yu, and Y. Yang, Modeling the probabilistic distribution of unlabeled data for one-shot medical image segmentation, in AAAI, 2021, pp. 12461254. [62] A. Zhao, G. Balakrishnan, F. Durand, J. V. Guttag, and A. V. Dalca, Data augmentation using learned transformations for one-shot medical image segmentation, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2019, pp. 85438553. [63] Z. Xu and M. Niethammer, Deepatlas: Joint semi-supervised learning of image registration and segmentation, in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2019, pp. 420429. [64] T. Y. Kong and A. Rosenfeld, Digital topology: Introduction and survey, Computer Vision, Graphics, and Image Processing, vol. 48, no. 3, pp. 357393, 1989. [65] L. Bottou, Large-scale machine learning with stochastic gradient descent, in Proceedings of COMPSTAT2010. Springer, 2010, pp. 177186. [66] D. P. Kingma and J. Ba, Adam: method for stochastic optimization, arXiv preprint arXiv:1412.6980, 2014. [67] W. Saad, Z. Han, M. Debbah, A. Hjorungnes, and T. Basar, Coalitional game theory for communication networks, Ieee signal processing magazine, vol. 26, no. 5, pp. 7797, 2009. [68] Z. Zhou, V. Sodha, J. Pang, M. B. Gotway, and J. Liang, Models genesis, vol. 67. Elsevier, 2021, p. 101840. [69] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros, Context encoders: Feature learning by inpainting, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 25362544. J. Ma, J. Chen, M. Ng, R. Huang, Y. Li, C. Li, X. Yang, and A. L. Martel, Loss odyssey in medical image segmentation, Medical Image Analysis, vol. 71, p. 102035, 2021. [70] [71] R. Gharleghi, D. Adikari, K. Ellenberger, S.-Y. Ooi, C. Ellis, C.- M. Chen, R. Gao, Y. He, R. Hussain, C.-Y. Lee et al., Automated segmentation of normal and diseased coronary arteries-the asoca challenge, Computerized Medical Imaging and Graphics, p. 102049, 2022. [72] M. Schaap, C. T. Metz, T. van Walsum, A. G. van der Giessen, A. C. Weustink, N. R. Mollet, C. Bauer, H. Bogunovic, C. Castro, X. Deng et al., Standardized evaluation methodology and reference database for evaluating coronary artery centerline extraction algorithms, Medical image analysis, vol. 13, no. 5, pp. 701714, 2009. [73] X. Zhuang, L. Li, C. Payer, D. ˇStern, M. Urschler, M. P. HeinO. Smedby, C. Bian et al., Evaluation rich, J. Oster, C. Wang, of algorithms for multi-modality whole heart segmentation: an open-access grand challenge, Medical image analysis, vol. 58, p. 101537, 2019. [74] D. N. Kennedy, C. Haselgrove, S. M. Hodge, P. S. Rane, N. Makris, and J. A. Frazier, Candishare: resource for pediatric neuroimaging data, Neuroinformatics, vol. 10, no. 3, p. 319, 2012. [75] B. Van Ginneken, M. B. Stegmann, and M. Loog, Segmentation of anatomical structures in chest radiographs using supervised methods: comparative study on public database, Medical image analysis, vol. 10, no. 1, pp. 1940, 2006. [76] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers, Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 20972106. [77] C. You, W. Dai, F. Liu, Y. Min, N. C. Dvornek, X. Li, D. A. Clifton, L. Staib, and J. S. Duncan, Mine your own anatomy: Revisiting medical image segmentation with extremely limited labels, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 24 in proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 91979206. [98] L. Van der Maaten and G. Hinton, Visualizing data using t-sne. [99] Journal of machine learning research, vol. 9, no. 11, 2008. J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille, and Y. Zhou, Transunet: Transformers make strong encoders for medical image segmentation, arXiv preprint arXiv:2102.04306, 2021. [100] H. Cao, Y. Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang, Swin-unet: Unet-like pure transformer for medical image segmentation, in European conference on computer vision. Springer, 2022, pp. 205218. [101] I. Cohen, Y. Huang, J. Chen, J. Benesty, J. Benesty, J. Chen, Y. Huang, and I. Cohen, Pearson correlation coefficient, Noise reduction in speech processing, pp. 14, 2009. [102] L. Guttman, basis for analyzing test-retest reliability, Psychometrika, vol. 10, no. 4, pp. 255282, 1945. [79] [78] C. You, W. Dai, Y. Min, F. Liu, D. Clifton, S. K. Zhou, L. Staib, and J. Duncan, Rethinking semi-supervised medical image segmentation: variance-reduction perspective, Advances in neural information processing systems, vol. 36, 2024. Ikezoe, T. Matsumoto, J. Shiraishi, S. Katsuragawa, T. Kobayashi, K.-i. Komatsu, M. Matsui, H. Fujita, Y. Kodera, and K. Doi, Development of digital image database for chest radiographs with and without lung nodule: receiver operating characteristic analysis of radiologists detection of pulmonary nodules, American Journal of Roentgenology, vol. 174, no. 1, pp. 7174, 2000. J. [80] O. Ronneberger, P. Fischer, and T. Brox, U-net: Convolutional networks for biomedical image segmentation, in International Conference on Medical image computing and computer-assisted intervention. Springer, 2015, pp. 234241. [81] L. Yu, S. Wang, X. Li, C.-W. Fu, and P.-A. Heng, Uncertaintyaware self-ensembling model for semi-supervised 3d left atrium segmentation, in Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 2019, pp. 605613. S. Chen, G. Bortsova, A. G.-U. Juarez, G. van Tulder, and M. de Bruijne, Multi-task attention-based semi-supervised learning for medical image segmentation, in International Conference on Medical Image Computing and Computer-Assisted Intervention, 2019, pp. 457465. [82] [83] Y. He, G. Yang, J. Yang, Y. Chen, Y. Kong, J. Wu, L. Tang, X. Zhu, J.-L. Dillenseger, P. Shao et al., Dense biased networks with deep priori anatomy and hard region adaptation: Semisupervised learning for fine renal artery segmentation, Medical image analysis, vol. 63, p. 101722, 2020. [84] X. Chen, Y. Yuan, G. Zeng, and J. Wang, Semi-supervised semantic segmentation with cross pseudo supervision, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 26132622. [85] Y. Wu and K. He, Group normalization, in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 319. [86] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., Pytorch: An imperative style, high-performance deep learning library, Advances in neural information processing systems, vol. 32, 2019. [87] A. A. Taha and A. Hanbury, Metrics for evaluating 3d medical image segmentation: analysis, selection, and tool, BMC medical imaging, vol. 15, no. 1, pp. 128, 2015. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, Imagenet: large-scale hierarchical image database, in 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009, pp. 248255. [88] [89] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, P.-A. Manzagol, and L. Bottou, Stacked denoising autoencoders: Learning useful representations in deep network with local denoising criterion. Journal of machine learning research, vol. 11, no. 12, 2010. [91] [90] N. Komodakis and S. Gidaris, Unsupervised representation learning by predicting image rotations, in International Conference on Learning Representations (ICLR), 2018. J. Luo, M. Wu, D. Gopukumar, and Y. Zhao, Big data application in biomedical research and health care: literature review, Biomedical informatics insights, vol. 8, pp. BIIS31 559, 2016. [92] F. Yang, Q. Sun, H. Jin, and Z. Zhou, Superpixel segmentation with fully convolutional networks, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 13 96413 973. [93] Y. Liu, R. Ge, Y. He, Z. Wu, C. You, S. Li, and Y. Chen, Imaging foundation model for universal enhancement of non-ideal measurement ct, arXiv preprint arXiv:2410.01591, 2024. [94] Y. He, F. Huang, X. Jiang, Y. Nie, M. Wang, J. Wang, and H. Chen, Foundation model for advancing healthcare: Challenges, opportunities and future directions, IEEE Reviews in Biomedical Engineering, pp. 120, 2024. S. Chen, K. Ma, and Y. Zheng, Med3d: Transfer learning for 3d medical image analysis, arXiv preprint arXiv:1904.00625, 2019. [95] [96] F. Isensee, M. Schell, I. Pflueger, G. Brugnara, D. Bonekamp, U. Neuberger, A. Wick, H.-P. Schlemmer, S. Heiland, W. Wick et al., Automated brain extraction of multisequence mri using artificial neural networks, Human brain mapping, vol. 40, no. 17, pp. 49524964, 2019. [97] K. Wang, J. H. Liew, Y. Zou, D. Zhou, and J. Feng, Panet: Fewshot image semantic segmentation with prototype alignment,"
        }
    ],
    "affiliations": [
        "Centre de Recherche en Information Biomedicale Sino-Francais (CRIBs)",
        "Department of Biomedical Engineering and the Department of Computer and Data Science, Case Western Reserve University, Cleveland, OH 44106 USA",
        "Department of Computer Science, Western University, London, ON N6A 3K7, Canada",
        "Jiangsu Provincial Joint International Research Laboratory of Medical Information Processing, Nanjing, China",
        "Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, Nanjing, China",
        "School of Instrument Science and Engineering, Southeast University, Nanjing, China"
    ]
}