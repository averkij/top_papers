{
    "paper_title": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation",
    "authors": [
        "Hyeonho Jeong",
        "Suhyeon Lee",
        "Jong Chul Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: https://hyeonho99.github.io/reangle-a-video/"
        },
        {
            "title": "Start",
            "content": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation Hyeonho Jeong* Suhyeon Lee* KAIST AI {hyeonho.jeong, suhyeon.lee, jong.ye}@kaist.ac.kr"
        },
        {
            "title": "Jong Chul Ye",
            "content": "5 2 0 2 2 1 ] . [ 1 1 5 1 9 0 . 3 0 5 2 : r Figure 1. From single monocular video of any scene, Reangle-A-Video generates synchronized videos from diverse camera viewpoints or movements without relying on any multi-view generative priorusing only single fine-tuning of video generator. The first row shows the input video, while the rows below present videos generated by Reangle-A-Video. (Left): Static view transport results. (Right): Dynamic camera control results. Full video examples are available on our project page: hyeonho99.github.io/reangle-a-video"
        },
        {
            "title": "Abstract",
            "content": "We introduce Reangle-A-Video, unified framework for generating synchronized multi-view videos from single input video. Unlike mainstream approaches that train multiview video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle- (1) Multi-View Motion A-Video operates in two stages. Learning: An image-to-video diffusion transformer is synchronously fine-tuned in self-supervised manner to distill view-invariant motion from set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time crossview consistency guidance using DUSt3R, generating multiview consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing new solution for multi-view video generation. We will publicly release our code and data. 1. Introduction Diffusion-based video generators [5, 7, 9, 39, 44, 58, 74, 85, 92] are rapidly advancing, enabling the generation of visually rich and dynamic videos from text or visual inputs. Recent progress in video generative models highlights the growing need for user control over object appearance [12, indicates equal contribution Input video Static view transport results Input video Dynamic camera control results Figure 2. Qualitative results on static view transport (left) & dynamic camera control (right). Click with Acrobat Reader to play videos. 41, 48, 77], object motion [10, 26, 38, 40, 56, 86, 94], and camera pose [30, 60, 75, 79, 82, 84, 88, 91]. However, video itself inherently captures only partial perspective of the world, which exists as dynamic 4D environment. To obtain dynamic 4D generative priors, previous works [50, 52, 80, 97] have extended 3D or video generators into 4D generative models using rendered synthetic assets [21, 22] that focus on animated objects or rigged characters. As result, these methods are limited to objectlevel 4D synthesis and fail to generalize to real-world scenes. Recent studies have addressed this limitation by training foundational multi-view video diffusion models [4, 64, 68, 70, 72, 76, 78] on hybrid datasets combining indoor/outdoor static scenes [20, 53, 96], general single-view videos, and realistic scenes rendered by simulation engines [21, 22, 28, 63]. Although promising, these approaches often face challenges in real-world scenarios [70], are limited to specific domains (e.g., human-centric synthesis [64]), and most are not publicly available. More importantly, most existing methods generate multi-view videos from image or text inputs, rather than from user-input videos. To address this, here we present Reangle-A-Video, an alternative solution for synchronized multi-view video generation that does not require specialized multi-view generative priors. Given any input video, we frame the task as video-to-videos translation, capitalizing on publicly available image and video diffusion models. Our unified framework supports both static view transportresimulating video from target viewpointsand dynamic camera control, where the video gradually transitions to the target viewpoints. Both approaches offer six degrees of freedom (see Fig. 1 and 2). In essence, our approach is based on the decomposition of dynamic 4D scene into view-specific appearance (the starting image) and view-invariant motion (image-tovideo generation). To capture view-invariant motion of the scene, we augment the training dataset with warped videos generated via repeated point-based warping of single monocular videothese warped videos provide strong hints about the cameras perspectives. We then fine-tune pre-trained image-to-video model [85] using synchronized few-shot training strategy building on masked diffusion loss [2, 17, 91]. After training, dynamic camera control over video (Fig.1,2-right) is achieved by generating videos using the original first frame and text that specifies the desired camera movement. To achieve this, our method addresses several key technical challenges. First, static view transport of video requires viewpoint-transported starting images, which we generate by inpainting warped images with image diffusion prior [49]. Second, inspired by test-time compute scaling [42, 71, 87], we enforce multiview consistency in inference-time, using stochastic control guidance with an off-the-shelf multi-view stereo reconstruction network [73]. We demonstrate the effectiveness of our approach on variety of real-world scenes and metrics. 2. Related Work Denoising-based Image and Video Generation. The introduction of Diffusion Transformers (DiT) [57] revolutionized image and video generation by replacing the traditional U-Net with transformer-based backbone. Combined with larger-scale, high-quality, curated datasets, this shift enabled more efficient and scalable diffusion generators. In image generation, the PixArt family [1315] extended DiT to text-to-image synthesis, showcasing its versatility. Stable Diffusion 3 [23] and Flux [49] further advanced the field with Multi-modal Diffusion Transformers (MM-DiT), enabling bidirectional text-image interactions. Following this trend, recent video diffusion models also integrate MM-DiT with spatio-temporal VAEs, achieving high-quality, longform video generation. In our work, we adopt Flux [49] for image diffusion and CogVideoX [85] for video diffusion, both built on the MM-DiT architecture. Multi-View Video Generation. Previous efforts on multiview (4D) video generation [50, 52, 80, 97] have mainly focused on synthesizing 4D assetsanimated objects and rigged characters [21, 22]. common approach is to combine motion priors from video diffusion models with 3D priors from multi-view image models and train on curated, synthetic datasets of dynamic 3D objects [50, 80]. While promising, these methods are limited to object-level 4D 2 Figure 3. Multi-view motion learning pipelines for (a) Static view transport and (b) Dynamic camera control. For both tasks, we distill view-robust motion of the underlying scene to pre-trained MM-DiT video model [85], using all visible pixels within the sampled videos. This few-shot, self-supervised training optimizes only the LoRA layers [35, 62], enabling lightweight training. synthesisrendered animations of single objects or animalsand produce multi-view videos with static camera pose per video, failing to generalize to real-world videos and arbitrary viewpoints. More recently, to capture opendomain real-world scenes, several works [4, 45, 68, 70, 72, 78] have built foundational multi-view video generative priors by training on multi-view images/videos of real-world static scenes [53, 96], general single-view video datasets, and photorealistic scenes rendered by various engines [21, 28, 63]. These efforts aim to overcome the scarcity of dynamic, in-the-wild multi-view video data. In contrast, our work reinterprets multi-view video generation as video-to-multi-view-videos translation task. We demonstrate that synchronized few-shot fine-tuning of video diffusion prior with visible pixels across arbitrary views is sufficient for multi-view video generation in realworld settings, without the need for expensive 4D diffusion models trained on large datasets. Camera Controllable Video Generation. Our framework is also related to video generation methods that incorporate controllable camera trajectories [3, 16, 30, 34, 46, 51, 60, 70, 75, 79, 81, 82, 84, 88, 90, 91]. Most approaches fine-tune pre-trained text/image-to-video diffusion models on multi-view datasets of static scenes (e.g., RealEstate10K [96], ScanNet [20], DL3DV-10K [53]) to integrate additional camera control signals. They often rely on ControlNet-like hypernetworks [26, 29, 30, 75, 93], raw camera extrinsics [75], camera ray coordinates [30, 76], Plucker coordinate embeddings [3, 46, 82], or combinations of multiple conditions [60]. Since these methods derive multi-view priors primarily from static scenes, they typically support camera-controlled generation from text or image inputs and are not designed to produce multiple videos that are mutually consistent. In contrast, our work focuses on (i) enabling camera control over an input video and, more importantly, (ii) generating multiple videos that remain consistent with each other. This is more challenging, as our framework must replicate the inherent dynamics of the input video, such as object motion, while ensuring consistency across the generated videos. 3. Reangle-A-Video In this section, we provide comprehensive discussion of our framework. Given an input video describing an arbitrary 4D scene, we aim to generate synchronized multiview videos of the same scene without relying on multiview generative priors. Leveraging pre-trained latent image and video diffusion models (Sec. 3.1), our approach decomposes the dynamic 4D scene into view-specific appearance (starting image) and view-invariant motion (imageto-video), addressing each component separately. We first embed the scenes view-invariant motion into pre-trained video diffusion model using our novel self-supervised training with data augmentation strategy. Initially, to capture diverse perspectives from single monocular video, we repeatedly perform point-based warping to generate set of warped videos (Sec. 3.2). These videos, together with the original video, form the training dataset for fine-tuning pre-trained image-to-video diffusion model with masked diffusion objective (Sec. 3.3). To achieve (b) dynamic camera control, we sample videos using the fine-tuned model with the original first frame as input. In contrast, for (a) 3 static view transport, we generate view-transported starting images by inpainting the warped first frames under an inference-time view consistency guidance using an off-theshelf multi-view stereo reconstruction network (Sec. 3.4). 3.1. Preliminary: Latent Diffusion Models Diffusion models [33, 66, 67] generate clean images or videos from Gaussian noise via an iterative denoising process. This process reverses fixed, time-dependent forward diffusion process, which gradually corrupts the data by adding Gaussian noise. In latent diffusion models [8, 61], this process operates in the lower-dimensional latent space of pre-trained VAE [43], comprised of an encoder E() and decoder D(). Given clean sample, x0 pdata(x), it is first compressed to latent representation z0 = E(x0). Gaussian noise ϵ (0, I) is then added to produce intermediate noisy latents via the forward process zt = αtz0 + σtϵ, where denotes the diffusion timestep, and αt, σt are noise scheduler parameters. The training objective is to learn denoiser network ϵθ that minimizes: (cid:2) ϵ ϵθ(zt, t, c) EϵN (0,I),ztp,t,c (1) (cid:3), 2 with condition provided by text prompt, image, or both. Once trained, the diffusion model generates clean samples by iteratively denoising pure Gaussian noise. 3.2. Stage I: Point-based Warping for Training Data"
        },
        {
            "title": "Augmentation",
            "content": "Given an input video x1:N of frames, our goal is to construct training dataset for the subsequent fine-tuning stage, by lifting pixels into time-aware 3D point clouds and reprojecting them onto the image plane with target perspectives. For each frame xi, {1, ..., }, we first estimate its depth map Di using monocular depth estimator [83]. The corresponding point cloud is then generated from the RGBD image [xi, Di] as follows: = ϕ23([xi, Di], K, Pi src), (2) where is the camera intrinsic matrix, Pi src is the extrinsic matrix for frame xi, and ϕ23 is the function that lifts the RGBD image into point cloud. Note that and Pi src are set by convention as in [18], since they are intractable in open-domain videos. Next, we define target extrinsic matrix trajectories Φ1:M , where each trajectory is given by Φj = {P1 , ..., PN }, {1, ..., }. (3) Each extrinsic matrix Pi comprises rotation matrix R33 and translation vector R31, which together transform the point cloud into the target camera coordinate system. In the static view transport setting, each target trajectory Φj is constant across all frames, i.e., P1 = ... = PN ; for dynamic camera control, each frames camera = P2 4 is determined by incrementally moving and rotat- , with the first target pose pose Pi ing from the previous pose Pi1 set to the pose of the first input frame (P1 = P1 src). Finally, we reproject each point cloud to the image via the plane under the target perspective using and Pi function ϕ32: (ˆxi j, mi j) = ϕ32(P i, K, Pi j), (4) is the rendered warped image and mi where ˆxi is the corresponding visibility mask (1 for visible surfaces, 0 for invisible regions). Repeating this process constructs our training dataset Ω, which consists of pairs of warped videos and corresponding visibility masks: Ω = {(ˆx1:N ) = 1, ..., }. We also add the input video x1:N with uniform visibility masks (all pixels set to 1), resulting in total of +1 video-mask pairs. , m1:N 3.3. Stage II: Multi-View Motion Learning In this stage, we fine-tune pre-trained image-to-video diffusion transformer to learn view-invariant motion from the dataset Ω that captures diverse scene perspectives and corresponding pixels. Here, motion refers to the nuanced dynamics of each objectincluding its type, direction, and speedas well as any inherent camera movement when the original videos camera pose is not fixed. To achieve lightweight fine-tuning while preserving the original models prior, we employ Low-Rank Adaptation (LoRA) [35, 62]. LoRA augments an attention layer by adding residual path comprised of two low-rank matrices, θB Rdr, θA Rrk, to the original weight θ0 Rdk, min(d, k). The modified forward pass is defined as: θ = θ0 + αθ = θ0 + αθBθA, (5) where α controls the strength of the LoRA adjustment. We implement this approach on MM-DiT-based image-tovideo diffusion model [85]which incorporates 3D fullattention in its MM-DiT blocksby attaching LoRA layers to these 3D full-attention layers. We then randomly sample video x1:N and its corresponding visibility mask m1:N from the dataset Ω prepared in Sec. 3.2. Note that if x1:N is the original input video, its mask m1:N is uniformly filled with 1. The 0 = E(x1:N ), and video is compressed into latent space z1:N the mask is downsampled to m1:N down using spatio-temporal downsampling (see Sup. Sec.A.3). The video latents are then noised, patchified, and unfolded into long sequence to form video tokens. These, along with the text tokens, are fed into the video denoiser ϵθ. Since most sampled videos are warped and contain significant black regions, naive LoRA fine-tuning with the standard diffusion loss (Eq. (1)) inevitably degrades the original model prior and makes the model generate warped videos (see Supp. Sec. Figure 4. Multi-view consistent image inpainting using stochastic control guidance. In experiments, we set = 25. B.2). Thus, leveraging the compositionality of diffusion objectives [2, 17, 25, 91, 95], we fine-tune the LoRA layers θ using masked diffusion loss that excludes invisible regions in its loss computation: ,m1:N ϵ,z1:N down ,t,c , t, c) m1:N down (cid:2) (cid:13) (cid:13)ϵ m1:N down ϵθ(z1:N (cid:13) (cid:3), 2 (cid:13) 2 (6) where condition includes both the input text and image, with the latter being the first frame of the sampled video. 1 For text input, (b) dynamic camera control necessitates explicit specification of the camera movement type. As shown in Fig. 3-(b), during the dynamic camera control training, the image-to-video model always starts with the same first framethe original videos first framewhich is uniformly used across warped videos (see Sec. 3.2). As result, the model cannot infer the desired camera movement from the uniform starting frame alone. To resolve this, we explicitly include the camera movement type in the text input during both training and inference. Our LoRA layers, attached to the 3D full-attention modules, enable interactions between text and video tokens, learning to distinguish between different camera movements and map the corresponding text tokens to the appropriate video tokens. The proposed training pipelines are outlined in Fig. 3. This synchronized few-shot training strategy prevents the Instead, video model from overfitting to specific view. it learns general scene motion, enabling video generation even from viewpoints that were unseen during the training (see Sec. 4.3), while ensuring that all output videos are synchronized (e.g., with consistent object motion speeds). Once training is complete, for the (b) dynamic camera control setup, we sample videos directly using the original videos first frame as the input image and text specifying the desired camera movement. However, to achieve (a) static view transport, we require starting input image that captures the scene from the desired viewpointa process addressed in the next stage. 1Following [85], the image input is encoded into latent space, then concatenated along the channel dimension with the noisy video latents. 5 Figure 5. Qualitative inpainting comparisons. We compare naive inpainting to inpainting with stochastic control guidance. 3.4. Stage III: Multi-View Image Inpainting We achieve multi-view consistent starting image generation using warp-and-inpaint approach. In Stage I, given an original image x1 (the first frame of the input video), we render set of warped images ˆx1 1:M from target viewpoints, along with corresponding binary masks m1 1:M that indicate invisible surfaces. These missing regions are then filled via inpainting using image diffusion prior. While state-of-the-art image generators [15, 23, 49] yield plausible inpainting results on single warped images, independently inpainting each warped view fails to ensure crossview consistency (see Fig. 5, 2nd column). Motivated by inference-time scaling practices [36, 42, 71, 87], we address the emerging inconsistencies by introducing stochastic control guidance that enforces multi-view consistency during the denoising process using the DUSt3R multi-view stereo reconstruction model [73]. Specifically, we sequentially inpaint warped images by leveraging previously inpainted views for consistency, guided by stochastic control guidance. At each denoising step, we generate clean estimates from stochastic paths and compute multi-view consistency score for each estimate with respect to the previously inpainted images {x1 }. Following [1], the score is calculated by projecting the images into shared view using DUSt3R [73] and then computing the DINO feature similarity [11] on the projected features. We select the path with the highest score, add new noise to generate new paths, and iterate until the inpainting is complete 2. We present detailed algorithms in Supp. Alg. 1 and 2. 1, ..., x1 2The process is repeated sequentially for all views, with the first two inpainted concurrently. To ensure initial consistency, we generate candidate versions per view at every sampling step, evaluate multi-view consistency for all S2 pairs, and select the optimal pair for the next timestep. Figure 6. Qualitative comparisons. Top half shows (a) Static view transport and bottom half presents (b) Dynamic camera control results. The first row in each half displays the input videos, and for each input video, two generated videos corresponding to target cameras (1 and 2) are shown for each method. Across baseline, same camera parameters were used for each 1,2. Visit our page for full-video results. Table 1. Quantitative comparison results. Reangle-A-Video is evaluated in two modes(a) Static View Transport and (b) Dynamic Camera Controlagainst the baselines. VBench metrics [37] are presented on the left, other metrics [1, 31, 69] appear on the right. (a) Static View Transport Generative Camera Dolly [70] Vanilla CogVideoX [85] Reangle-A-Video (Ours) (b) Dynamic Camera Control NVS-Solver [88] Trajectory Attention [79] Reangle-A-Video (Ours) Subject Background Consistency Consistency 0.8849 0.9448 0.9516 0.9037 0.8984 0. 0.9107 0.9398 0.9327 0.9325 0.9288 0.9364 Temporal Flickering 0.8731 0.9738 0.9764 Motion Smoothness 0.9119 0.9859 0.9907 Dynamic Aesthetic Quality Degree 0.4052 0.7608 0.4997 0.7286 0.5160 0.7657 Imaging Quality 0.5524 0.6475 0. 0.9049 0.9342 0.9386 0.9521 0.9658 0.9794 0.8809 0.8889 0.8884 0.5023 0.4854 0.5238 0.6411 0.5990 0.6271 MEt3R FID FVD 0.1237 0.0539 0.0412 0.1090 0.0965 0.0648 155.15 79.621 53.448 5264.7 3664.2 2690. 95.815 109.20 74.194 3516.5 3624.9 3019.7 4. Experiments 4.1. Implementation Details We experiment with 28 publicly sourced videos [27, 59], covering diverse scenes with varying objects, scene motions, and physical environments. Each video consists of 49 frames at resolution of 480720. On average, we generate videos from 3.5 different camera viewpoints/movements per input, resulting in total of 98 generated videos. For Stage I, Depth Anything V2 model [83] is used to estimate the depth maps of the input videos. For Stage II and III, we use the CogVideoX-5b image-to-video diffusion model [85], and FLUX text-to-image model [49] with pre-trained inpainting ControlNet [19] attached to it. Upon video model fine-tuning, we set the LoRA rank to 128 and optimize the LoRA layers over 400 steps, where the fine-tuned parameters account for only about 2% of the original Video DiT parameters 3. We use AdamW [55] optimizer with learning rate of 1e4 and weight decay of 1e3. The fine-tuning takes about an hour. For the static view transport mode, we set = 12 warped videos; for dynamic camera control, we set = 6 (see Supp. Sec. A.2). For video sampling, we apply 40 sampling steps with CFG [32] scale of 6.0. During multi-view image inpainting, we first resize the input to 1024 1024 and then restore it to its original resolution. We use MEt3R [1] as the reward function for stochastic control with = 25. To introduce stochasticity for the path control, we employ an SDE-based sampler with 50 steps, ensuring the same marginal distribution as the original ODE of the flow trajectory (see Supp. Sec. A.5). All experiments, including inference and finetuning, are performed using 40GB A100 GPUs. 4.2. Comparisons 4.2.1. Baselines Multi-view/camera synchronized video generation from an input video remains largely underexplored. For (a) Static view transport, the closest works include Generative Camera Dolly [70] and the closed-source multi-view video foundation model CAT4D [78]. Although GS-DiT [6] 3To enable video model fine-tuning within 40GB VRAM constraint, we employ gradient checkpointing, which reduces memory usage at the expense of slower gradient back-propagation. aligns with our framework, its code was unavailable during our research. In addition to GCD, we evaluate baseline using naive CogVideoX-I2V inference (Vanilla CogVideoX)[85], which employs the same input frame as our approach. For (b) Dynamic camera control over videos, we compare against two state-of-the-art methods, NVSSolver [88] and Trajectory Attention [79]. While Recapture [91] is closely related, its code is not available, and it employs the proprietary multi-view image foundation model CAT3D [24]. 4.2.2. Qualitative Results Comprehensive qualitative results are shown in Fig. 6. For (a) Static View Transport, Generative Camera Dolly [70] struggles with real-world videos (trained on synthetic data [28]), while Vanilla CogVideoX [85] fails to capture the input videos motion. In contrast, Reangle-A-Video accurately reproduces the input motion from the target viewpoint. For (b) Dynamic Camera Control, NVS-Solver [88] and Trajectory Attention [79] either confuse foreground objects (e.g., girls and cats head in the first video), fail to capture precise motion (e.g., neck movement in the second video), or miss background elements (e.g., bench in the third video). Our method faithfully regenerates the input videos motion, preserves object appearance, and accurately follows the target camera movement. 4.2.3. Quantitative Results Automatic metrics. For automated evaluation, we first use VBench [37], which assesses generated videos across various disentangled dimensions. As shown in Tab. 1-left, our method outperforms baselines in most metrics for both static view transport and dynamic camera control modes. Notably, compared to vanilla CogVideoX I2Vwhich uses the same input imageour approach maintain robust performance even when fine-tuned with warped videos. Additionally, Tab. 1-right presents evaluation results using FID [31], FVD [69], and the recently proposed multi-view consistency metric MEt3R [1]. Human evaluation. We further assess Reangle-A-Video against baselines via user study involving 36 participants who compare our results with randomly selected baselines. For the static view transport evaluation, participants rate: (i) accuracy of the transported viewpoint, (ii) preservation of the input videos motion. For the dynamic camera control 7 Figure 7. User study results. Top: Static view transport results. Bottom: Dynamic camera control results. Table 2. Quantitative evaluation of multi-view consistency in image inpainting with and without stochastic control guidance. Warped image inpainting w/o stochastic control guidance w/ stochastic control guidance MEt3R 0.1431 0.1184 SED 1.1966 1.1844 TSED 0.5241 0.5588 evaluation, participants assess: (i) the accuracy of the target camera movement, (ii) the preservation of the input videos motion in the output. As reported in Fig. 7, our method outperforms the baselines in all aspects. 4.3. Ablation Studies First, we ablate the stochastic control guidance by comparing it with naive inpainting (where each image is inpainted independently). Tab. 2 evaluates multi-view accuracy using the MEt3R [1], SED, and TSED [89] metrics (with Te = 1.25 and Tm = 10 for SED). Fig. 5 shows that our method fills target (invisible) regions consistently across multiple views. Next, we ablate the necessity of our data augmentation strategy (i.e., the use of warped videos) in learning view-invariant motion. Fig. 8 compares finetuning with only the original input video versus using both the original and warped videos. The results indicate that relying solely on the original video fails to accurately capture motionfor example, the rhino moving in front of the tree. For quantitative assessment on input videos motion preservation, we employ user study (see Supp. Sec. B.1.) In Fig. 9-top, we demonstrate unseen view video generation. We exclude specific warped view videos (vertical up/down orbits) from the training dataset and fine-tune the video model without them. Then, using an inpainted first frame as input, we generate video from that omitted view. In Fig. 9bottom, we showcase novel view video generation guided by an edited first frame. Starting with an inpainted image representing the scene from target viewpoint, we apply FlowEdit [47] to modify the image, then generate the novelview video with our fine-tuned model. In both cases, the generated videos faithfully follow the input videos motion, demonstrating that our few-shot training strategy is robust in both view and appearance. Figure 8. Novel view video generation with and without using warped video for training (target viewpoint: dolly zoom in). Figure 9. Top: Unseen view video generation. Bottom: Novel view video generation using an appearance-edited first frame. 5. Conclusion We have presented an approximate yet effective solution for generating synchronized multi-view videos from single monocular real-world videowithout relying on any multiview generative priors. Our approach enables an image-tovideo diffusion model to learn view-invariant scene motion through self-supervised fine-tuning with video augmentation. Then, we sample videos from the fine-tuned model using set of multi-view consistent first-frame images, where the images are generated via warp-and-inpaint process that enforces multi-view consistency during inference-time. Limitations and future work. Input image quality is crucial in image-to-video generation, as the output cannot exceed the first frames quality. While our approach ensures multi-view consistency in the warp-and-inpaint scheme, the warping stage is inherently prone to artifacts caused by camera and depth errors (Supp. Sec. C). We believe that improved depth models could enhance both the warped and in8 painted images. Another limitation of our work is the need for scene-specific model tuning to achieve 4D video synthesis. promising future direction is to extend existing video datasets with warped videos to train 4D foundation models. Acknowledgments. We would like to thank Geon Yeong Park and Mingi Kwon for their valuable discussions at the early stage of the project."
        },
        {
            "title": "References",
            "content": "[1] Mohammad Asim, Christopher Wewer, Thomas Wimmer, Bernt Schiele, and Jan Eric Lenssen. Met3r: Measuring multi-view consistency in generated images. arXiv preprint arXiv:2501.06336, 2025. 5, 7, 8, 2 [2] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel CohenOr, and Dani Lischinski. Break-a-scene: Extracting multiple concepts from single image. In SIGGRAPH Asia 2023 Conference Papers, pages 112, 2023. 2, 5, 3 [3] Sherwin Bahmani, Ivan Skorokhodov, Guocheng Qian, Aliaksandr Siarohin, Willi Menapace, Andrea Tagliasacchi, David Lindell, and Sergey Tulyakov. Ac3d: Analyzing and improving 3d camera control in video diffusion transformers. arXiv preprint arXiv:2411.18673, 2024. 3 [4] Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Xiao Fu, Zuozhu Liu, Haoji Hu, Pengfei Wan, and Di Zhang. Syncammaster: Synchronizing multi-camera video generation from diverse viewpoints. arXiv preprint arXiv:2412.07760, 2024. 2, 3 [5] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: space-time diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 1 [6] Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, FuYun Wang, and Hongsheng Li. Gs-dit: Advancing video generation with pseudo 4d gaussian fields through efficient dense 3d point tracking. arXiv preprint arXiv:2501.02690, 2025. [7] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1 [8] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 4 [9] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 1 iao Li, Mohsen Mousavi, et al. Go-with-the-flow: Motioncontrollable video diffusion models using real-time warped noise. arXiv preprint arXiv:2501.08331, 2025. 2 [11] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 5 [12] Hila Chefer, Shiran Zada, Roni Paiss, Ariel Ephrat, Omer Tov, Michael Rubinstein, Lior Wolf, Tali Dekel, Tomer Michaeli, and Inbar Mosseri. Still-moving: Customized ACM video generation without customized video data. Transactions on Graphics (TOG), 43(6):111, 2024. 1 [13] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [14] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. [15] Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul, Ping Luo, Hang Zhao, and Zhenguo Li. Pixart-δ: Fast and controllable image generation with latent consistency models. arXiv preprint arXiv:2401.05252, 2024. 2, 5 [16] Soon Yau Cheong, Duygu Ceylan, Armin Mustafa, Andrew Gilbert, and Chun-Hao Paul Huang. Boosting camera motion control for video diffusion transformers. arXiv preprint arXiv:2410.10802, 2024. 3 [17] Gene Chou, Kai Zhang, Sai Bi, Hao Tan, Zexiang Xu, Fujun Luan, Bharath Hariharan, and Noah Snavely. Generating 3d-consistent videos from unposed internet photos. arXiv preprint arXiv:2411.13549, 2024. 2, 5, 3 [18] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free genarXiv preprint eration of 3d gaussian splatting scenes. arXiv:2311.13384, 2023. 4 Smart Flux inpainting controlnet, 2025. [19] Alimama Team. https://huggingface.co/alimama-creative/FLUX.1-devControlnet-Inpainting-Beta. 7 Creative and AI Application URL [20] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 58285839, 2017. 2, [21] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. 2, 3 [10] Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingx- [22] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, 9 Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36, 2024. 2 [23] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 2, [24] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. Cat3d: Create anything arXiv preprint in 3d with multi-view diffusion models. arXiv:2405.10314, 2024. 7 [25] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Mdtv2: Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023. 5, 3 [26] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, et al. Motion prompting: Controlling video generation with motion trajectories. arXiv preprint arXiv:2412.02700, 2024. 2, 3 [27] Canva Germany GmbH. pexels, 2024. URL https://www.pexels.com/videos/. 7 [28] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, et al. Kubric: scalable dataset generator. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 37493761, 2022. 2, 3, 7 [29] Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, et al. Diffusion as shader: 3d-aware video diffusion for versatile video generation control. arXiv preprint arXiv:2501.03847, 2025. 3 [30] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 2, [31] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 7 [32] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 7 [33] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 4, 2 [34] Chen Hou, Guoqiang Wei, Yan Zeng, and Zhibo Chen. Training-free camera control for video generation. arXiv preprint arXiv:2406.10126, 2024. 3 [35] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 3, 4 [36] Yujia Huang, Adishree Ghatare, Yuanzhe Liu, Ziniu Hu, Qinsheng Zhang, Chandramouli Sastry, Siddharth Gururani, Sageev Oore, and Yisong Yue. Symbolic music generation with non-differentiable rule guided diffusion. arXiv preprint arXiv:2402.14285, 2024. 5 [37] Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, et al. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503, 2024. [38] Hyeonho Jeong, Jinho Chang, Geon Yeong Park, and Jong Chul Ye. Dreammotion: Space-time self-similar score arXiv preprint distillation for zero-shot video editing. arXiv:2403.12002, 2024. 2 [39] Hyeonho Jeong, Chun-Hao Paul Huang, Jong Chul Ye, Niloy Mitra, and Duygu Ceylan. Track4gen: Teaching video diffusion models to track points improves video generation. arXiv preprint arXiv:2412.06016, 2024. 1 [40] Hyeonho Jeong, Geon Yeong Park, and Jong Chul Ye. Vmc: Video motion customization using temporal attention adapIn Proceedings of tion for text-to-video diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 92129221, 2024. 2 [41] Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, and Ziwei Liu. Videobooth: Diffusion-based video generation with image In Proceedings of the IEEE/CVF Conference prompts. on Computer Vision and Pattern Recognition, pages 6689 6700, 2024. 2 [42] Jaemin Kim, Bryan Kim, and Jong Chul Ye. Free 2 guide: Gradient-free path integral control for enhancing text-tovideo generation with large vision-language models. arXiv preprint arXiv:2411.17041, 2024. 2, 5 [43] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 4 [44] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 1 [45] Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, and Gordon Wetzstein. Collaborative video diffusion: Consistent multi-video generation with camera control. arXiv preprint arXiv:2405.17414, 2024. 3 [46] Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, and Gordon Wetzstein. Collaborative video diffusion: Consistent multi-video generation with camera control. Advances in Neural Information Processing Systems, 37:1624016271, 2025. 3 [47] Vladimir Kulikov, Matan Kleiner, Inbar HubermanSpiegelglas, and Tomer Michaeli. Flowedit: Inversion-free arXiv text-based editing using pre-trained flow models. preprint arXiv:2412.08629, 2024. [48] Gihyun Kwon and Jong Chul Ye. Tweediemix: Improving multi-concept fusion for diffusion-based image/video generation. arXiv preprint arXiv:2410.05591, 2024. 2 10 [49] Black Forest Labs. https://blackforestlabs.ai/. 2, 5, 7 Flux, 2024. URL [50] Bing Li, Cheng Zheng, Wenxuan Zhu, Jinjie Mai, Biao Zhang, Peter Wonka, and Bernard Ghanem. Vivid-zoo: Multi-view video generation with diffusion model. arXiv preprint arXiv:2406.08659, 2024. 2 [51] Teng Li, Guangcong Zheng, Rui Jiang, Tao Wu, Yehao Lu, Yining Lin, Xi Li, et al. Realcam-i2v: Real-world image-tovideo generation with interactive complex camera control. arXiv preprint arXiv:2502.10059, 2025. 3 [52] Hanwen Liang, Yuyang Yin, Dejia Xu, Hanxue Liang, Zhangyang Wang, Konstantinos Plataniotis, Yao Zhao, and Yunchao Wei. Diffusion4d: Fast spatial-temporal consistent 4d generation via video diffusion models. arXiv preprint arXiv:2405.16645, 2024. 2 [53] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2216022169, 2024. 2, 3 [54] Qiang Liu. Let Us Flow Together. 2024. Accessed: 202502-26. 2 [55] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 7 [56] Geon Yeong Park, Hyeonho Jeong, Sang Wan Lee, and Spectral motion alignment for video arXiv preprint Jong Chul Ye. motion transfer using diffusion models. arXiv:2403.15249, 2024. 2 [57] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [58] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models, 2024. 1 11 [59] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 7 [60] Stefan Popov, Amit Raj, Michael Krainin, Yuanzhen Li, William Freeman, and Michael Rubinstein. Camctrl3d: Single-image scene exploration with precise 3d camera control. arXiv preprint arXiv:2501.06006, 2025. 2, 3 [61] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2021. 4 [62] Simo Ryu. Low-rank adaptation for fast text-to-image diffusion fine-tuning. Low-rank adaptation for fast text-to-image diffusion fine-tuning, 3, 2023. 3, [63] Andrew Sanders. An introduction to Unreal engine 4. AK Peters/CRC Press, 2016. 2, 3 [64] Ruizhi Shao, Youxin Pang, Zerong Zheng, Jingxiang Sun, and Yebin Liu. 360-degree human video generation with 4d diffusion transformer. ACM Transactions on Graphics (TOG), 43(6):113, 2024. 2 [65] Saurabh Singh and Ian Fischer. Stochastic sampling from deterministic flow models. arXiv preprint arXiv:2410.02217, 2024. 2 [66] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 4 [67] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [68] Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, and Yikai Wang. Dimensionx: Create any 3d and 4d scenes from single image with controllable video diffusion. arXiv preprint arXiv:2411.04928, 2024. 2, 3 [69] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 7 [70] Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sargent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi Zheng, and Carl Vondrick. Generative camera dolly: ExIn Eutreme monocular dynamic novel view synthesis. ropean Conference on Computer Vision, pages 313331. Springer, 2024. 2, 3, 7 [71] Bram Wallace, Akash Gokul, Stefano Ermon, and Nikhil Naik. End-to-end diffusion latent optimization improves classifier guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 72807290, 2023. 2, 5 [72] Chaoyang Wang, Peiye Zhuang, Tuan Duc Ngo, Willi Menapace, Aliaksandr Siarohin, Michael Vasilkovsky, Ivan Skorokhodov, Sergey Tulyakov, Peter Wonka, and Hsin-Ying Lee. 4real-video: Learning generalizable photo-realistic 4d video diffusion. arXiv preprint arXiv:2412.04462, 2024. 2, 3 [73] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20697 20709, 2024. 2, 5 [74] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. International Journal of Computer Vision, pages 120, 2024. 1 [75] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 2, [76] Daniel Watson, Saurabh Saxena, Lala Li, Andrea Tagliasacchi, and David Fleet. Controlling space and time with diffusion models. arXiv preprint arXiv:2407.07860, 2024. 2, 3 [77] Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, et al. Dreamvideo-2: Zero-shot subjectdriven video customization with precise motion control. arXiv preprint arXiv:2410.13830, 2024. 2 [78] Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan Barron, and Aleksander Holynski. Cat4d: Create anything in 4d with multi-view video diffusion models. arXiv preprint arXiv:2411.18613, 2024. 2, 3, 7 [79] Zeqi Xiao, Wenqi Ouyang, Yifan Zhou, Shuai Yang, Lei Yang, Jianlou Si, and Xingang Pan. Trajectory attention for fine-grained video motion control. arXiv preprint arXiv:2411.19324, 2024. 2, 3, 7 [80] Yiming Xie, Chun-Han Yao, Vikram Voleti, Huaizu Jiang, and Varun Jampani. Sv4d: Dynamic 3d content generation with multi-frame and multi-view consistency. arXiv preprint arXiv:2407.17470, 2024. 2 [81] Dejia Xu, Yifan Jiang, Chen Huang, Liangchen Song, Thorsten Gernoth, Liangliang Cao, Zhangyang Wang, and Hao Tang. Cavia: Camera-controllable multi-view video arXiv preprint diffusion with view-integrated attention. arXiv:2410.10774, 2024. [82] Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat. Camco: Cameracontrollable 3d-consistent image-to-video generation. arXiv preprint arXiv:2406.02509, 2024. 2, 3 [83] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv preprint arXiv:2406.09414, 2024. 4, 7 [84] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with userdirected camera movement and object motion. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 2, 3 [85] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 2, 3, 4, 5, 7 12 [86] Danah Yatim, Rafail Fridman, Omer Bar-Tal, Yoni Kasten, and Tali Dekel. Space-time diffusion features for zero-shot text-driven motion transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 84668476, 2024. 2 [87] Po-Hung Yeh, Kuang-Huei Lee, and Jun-Cheng Chen. Training-free diffusion model alignment with sampling demons. arXiv preprint arXiv:2410.05760, 2024. 2, 5 [88] Meng You, Zhiyu Zhu, Hui Liu, and Junhui Hou. Nvs-solver: Video diffusion model as zero-shot novel view synthesizer. arXiv preprint arXiv:2405.15364, 2024. 2, 3, [89] Jason Yu, Fereshteh Forghani, Konstantinos Derpanis, and Marcus Brubaker. Long-term photometric consistent novel view synthesis with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 70947104, 2023. 8 [90] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 3 [91] David Junhao Zhang, Roni Paiss, Shiran Zada, Nikhil Karnad, David Jacobs, Yael Pritch, Inbar Mosseri, Mike Zheng Shou, Neal Wadhwa, and Nataniel Ruiz. Recapture: Generative video camera controls for user-provided videos using masked video fine-tuning. arXiv preprint arXiv:2411.05003, 2024. 2, 3, 5, 7 [92] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. International Journal of Computer Vision, pages 115, 2024. 1 [93] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 3 [94] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jia-Wei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. Motiondirector: Motion customization of text-to-video diffusion models. In European Conference on Computer Vision, pages 273290. Springer, 2024. 2 [95] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. arXiv preprint arXiv:2306.09305, 2023. 5, [96] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Learning arXiv preprint and Noah Snavely. view synthesis using multiplane images. arXiv:1805.09817, 2018. 2, 3 Stereo magnification: [97] Qi Zuo, Xiaodong Gu, Lingteng Qiu, Yuan Dong, Zhengyi Zhao, Weihao Yuan, Rui Peng, Siyu Zhu, Zilong Dong, Liefeng Bo, et al. Videomv: Consistent multi-view generation based on large video generative model. arXiv preprint arXiv:2403.12010, 2024. 2 Reangle-A-Video: 4D Video Generation as Video-to-Video Translation"
        },
        {
            "title": "Supplementary Material",
            "content": "This supplementary material is structured as follows: In Sec. A, we provide additional experimental details. Sec. presents additional experiment results. Following this, we discuss the limitations and failure cases of Reangle-AVideo in Sec. C. A. Additional Experimental Details A.1. Camera Visualizations We demonstrate six degrees of freedom in both (a) Static view transport and (b) Dynamic camera control. Fig. 10 visualizes the transported viewpoints and camera movements used in our work: orbit left, orbit right, orbit up, orbit down, dolly zoom in, and dolly zoom out. Figure 10. Visualizations of the used camera types. A.2. Warped Video Dataset Composition For (a) static view transport setup, we set as 12, generating warped videos from 12 different viewpoints: two random-angle orbit left, two random-angle orbit right, two random-angle orbit up, two random-angle orbit down, two random-range dolly zoom in, and two random-range dolly zoom out. Including the original input video, the training set consists of 13 videos. For (b) dynamic camera control training, we set as 6, rendering warped videos with six different camera movements: one random-angle orbit left, one random-angle orbit right, one random-angle orbit up, one random-angle orbit down, one random-range dolly zoom in, and one randomrange dolly zoom out. Including the original input video, this also results in training set of 7 videos. A.3. 3D Downsampling Visiblity Masks Figure 11. Temporal downsampling of visibility masks. Except for the first mask frame, pixel-wise (element-wise) logical AND operation is done for every four masks. of RGB pixel-space visibility masks to their corresponding latent regions. In our work, we adopt the 3D VAE architecture of CogVideoX, which features spatio-temporal compression rate given by = 8h 8w (1 + 4n), where (H, W, ) denote the pixel-space resolutions, and (h, w, n) represent the corresponding resolutions in the latent space. Given visibility mask in pixel-space, RHW , we first downsample the spatial dimensions by factor of 8 using nearest-neighbor interpolation. For temporal downsampling, we retain the first frame intact and then compress every subsequent group of four frames via an element-wise logical AND operation (see Fig. 11 for the illustration). This procedure ensures that latent region is marked as visible only if it is visible across all frames within each group. A.4. Pre-trained Model Checkpoints Reangle-A-Video builds upon publicly available pre-trained image and video generative models. Reangle-A-Video builds on publicly available pre-trained image and video generative models. Here, we specify the exact versions used: Text-to-Image generation model: Flux.1-dev 4 Image-to-Image inpainting model: Flux-ControlNetInpainting-Beta 5 Image-to-Video generation model: CogVideoX-I2V-5b 6 Recent video diffusion models rely on 3D VAEs that perform both spatial and temporal compression to alleviate the computational burden of modeling long video sequences. However, such compression complicates the direct mapping 4https://huggingface.co/black-forest-labs/FLUX. 1-dev 5https://huggingface.co/alimamacreative/FLUX. 1-dev-Controlnet-Inpainting-Beta 6https://huggingface.co/THUDM/CogVideoX-5b-I2V 1 Algorithm 2 Multi-View Consistent Image Inpainting (flow-based model) Require: Inpainted images {x1, . . . , xI }, the image to be inpainted ˆxI+1, and the image inpainting flow-based model (ϕ, E, D). E(ˆxI+1) zt1 (0, I) for = 0 to 1 do ti+1 ti z0ti zti ti ˆvϕ(zti, c, ti) for = 1 to do ϵ (0, I) zs zti + ˆvϕ(zti , c, ti)t ti+1 z0ti(1ti)t+(cid:112)2(1 ti)2tϵ + (1 t)ˆvϕ(zs ti+1 , c, ti+1) ti+1 1ti+1 zs zs end for for = 1 to do for = 1 to do rs,j eval 3d consistency(D(zs ), xj) 0ti+1 (rs,1 + rs,2 + + rs,I ) end for rs 1 end for argmaxs rs zti+1 zs ti+1 end for xI+1 D(ztT ) return xI+ uates consistency across all possible sample pairs and selects the best pair for the next step. B. Additional Experimental Results B.1. Using Warped Videos for Fine-tuning Due to the absence of an automatic metric for multi-view motion fidelity, we conduct human evaluation to assess the necessity of using warped videos during fine-tuning (Sec. sec: method-training). Participants were shown two randomly selected videos and asked, Does the generated video accurately preserve the input videos motion? before choosing the superior video. The results are shown in Tab. 3. Table 3. Quantitative ablation on warped videos during finetuning. Multi-view motion fidelity is evaluated via human studies. Multi-view motion fidelity 80.44% 19.56% w/ warped videos w/o warped videos Algorithm 1 Multi-View Consistent Image Inpainting (diffusion model) Require: Inpainted images {x1, . . . , xI }, the image to be inpainted ˆxI+1, and the image inpainting diffusion model (ϕ, E, D). E(ˆxI+1) zT (0, I) for = to 1 do for = 1 to do ϵ (0, I) zs t1 DDPM step(zt, ϕ, c, t, ϵ) 0t1 1 (zs zs end for for = 1 to do t1 αt1 1 αt1ˆϵϕ(zs t1, c, t)) for = 1 to do rs,j eval 3d consistency(D(zs 0t1), xj) (rs,1 + rs,2 + + rs,I ) end for rs 1 end for argmaxs rs zt1 zs t1 end for xI+1 D(z0) return xI+1 A.5. Multi-view Consistent Images Completion We present the detailed algorithm for our proposed multiview consistent image completion. As described in the main text, our approach sequentially performs multi-view inpainting using Algorithm 1 or 2, based on previously completed images. We introduce two versions: diffusion model-based method (Algorithm 1) and flow model-based method (Algorithm 2). To generate multiple sample versions at each denoising step for the stochastic control guidance [42, 65, 87], the diffusion model-based approach employs DDPM [33] sampling, while the flow model-based approach solves an SDE [54] that shares the same marginal distribution as its corresponding ODE. Specifically, for the Wiener process Wt: dZt = vt(Zt)dt Z0t(1 t)dt + (cid:112)2(1 t)2dWt, where Z0t = Zt tvt(Zt). (7) The SDE is solved using the EulerMaruyama method. After obtaining denoised estimates (Z0t for the diffusion model, Z1t for the flow model), they are decoded and compared against previously completed images using the method from [1] to ensure multi-view consistency. The best sample is selected for the next step. Specifically, for the initial view completion, the first two views are inpainted simultaneously by extending the given algorithm, which eval2 Figure 14. Pixel-scale artifacts in the warped frame. In this example, the target camera view is shifted 10 degrees to the (horizontal orbit) left of the input frame. Figure 12. Impact of masked diffusion loss on video quality. Masking the diffusion loss effectively prevents artifacts. B.2. Masking Diffusion Loss Building on the flexible compositionality of diffusion objectives, masked diffusion loss has been applied to diffusionbased customizations [2, 91], video interpolation [17], and In our efficient image diffusion model training [25, 95]. work, we employ masked diffusion loss on pre-trained video diffusion transformer architecture to distill the 4D motion prior of an arbitrary scene. As shown in Fig. 12, this objective effectively prevents artifacts and eliminates warped (black) regions in the generated videos. Figure 13. Geometric misalignment in the warped frame. In this example, the target camera view is shifted 10 degrees to the (horizontal orbit) right of the input frame. C. Limitations and Failure Cases Our method relies on point-based warping using estimated depth maps, making it inherently vulnerable to errors from inaccurate depth estimation and incorrect camera intrinsics. These inaccuracies can distort the warping process, leading to geometric misalignment and depth inconsistencies. For instance, Fig. 13 illustrates an example of geometric misalignment in the warped frame. Moreover, as shown 3 Figure 15. Failure cases. in Fig. 14, warping errors can introduce pixel-level artifacts that may not be fully masked by visibility estimations, allowing these small distortions to persist in the inpainted results. Addressing these limitations would require more accurate depth estimation and better handling of occlusion constraints. In Fig. 15, we present failure cases. For instance, our method produces inaccurate reconstructions in videos with small regions (e.g., blue sign in the background) or fast, complex motion (e.g., man breakdancing). We attribute these issues partly to the fact that video fine-tuning and inference are performed in spatially and temporally compressed latent space."
        }
    ],
    "affiliations": [
        "KAIST AI"
    ]
}