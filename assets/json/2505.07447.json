{
    "paper_title": "Unified Continuous Generative Models",
    "authors": [
        "Peng Sun",
        "Yi Jiang",
        "Tao Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing work often treats these approaches as distinct paradigms, resulting in separate training and sampling methodologies. We introduce a unified framework for training, sampling, and analyzing these models. Our implementation, the Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a 675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID in 20 steps and a few-step model reaching 1.42 FID in just 2 steps. Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at 250 steps) improves performance to 1.06 FID in only 40 steps. Code is available at: https://github.com/LINs-lab/UCGM."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 7 4 4 7 0 . 5 0 5 2 : r a"
        },
        {
            "title": "Unified Continuous Generative Models",
            "content": "Peng Sun1,2 Yi Jiang2 Tao Lin1, 1Westlake University 2Zhejiang University sunpeng@westlake.edu.cn, yi_jiang@zju.edu.cn, lintao@westlake.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in continuous generative models, encompassing multi-step processes such as diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods like consistency models (typically 1-8 steps), have yielded impressive generative performance. Existing work, however, often treats these approaches as distinct learning paradigms, leading to disparate training and sampling methodologies. We propose unified framework designed for the training, sampling, and understanding of these models. Our implementation, the Unified Continuous Generative Models Trainer and Sampler (UCGM-{T, S}), demonstrates state-of-the-art (SOTA) capabilities. For instance, on ImageNet 256 256 using 675M diffusion transformer model, UCGM-T trains multi-step model achieving 1.30 FID in 20 sampling steps, and few-step model achieving 1.42 FID in 2 sampling steps. Furthermore, applying UCGM-S to pre-trained model from prior work (1.26 FID at 250 steps) improves the FID to 1.06 using only 40 steps. Code is made publicly available at: https://github.com/LINs-lab/UCGM."
        },
        {
            "title": "Introduction",
            "content": "(a) NFE = 40, FID = 1.48. (b) NFE = 2, FID = 1.75. Figure 1: Generated samples from two 675M diffusion transformers trained with our UCGM on ImageNet-1K 512512. The figure showcases generated samples illustrating the flexibility of Number of Function Evaluations (NFE) and superior performance achieved by our UCGM. The left subfigure presents results with NFE = 40 (multi-step), while the right subfigure shows results with NFE = 2 (few-step). Note that the samples are sampled without classifier-free guidance or other guidance techniques. Continuous generative models, encompassing diffusion models [14, 35], flow-matching models [25, 29], and consistency models [37, 28], have demonstrated remarkable success in synthesizing highfidelity data across diverse applications, including image and video generation [45, 16]. Despite their capabilities, these models often incur substantial computational costs for training and sampling [18, 20]. Furthermore, the field has evolved with these paradigms largely treated as distinct, leading to paradigm-specific training and sampling algorithms. This fragmentation presents Corresponding author. Preprint. Under review. Table 1: Existing continuous generative paradigms as special cases of our UCGM. Prominent continuous generative models, such as Diffusion, Flow-Matching, and Consistency models, can be formulated as specific parameterizations of our UCGM. The columns detail the required parameterizations for the transport coefficients α(), γ(), ˆα(), ˆγ() and parameters λ, ρ, ν of UCGM. Note that σ(t) is defined as e4(2.68t1.59) in this table."
        },
        {
            "title": "Paradigm",
            "content": "UCGM-based Parameterization"
        },
        {
            "title": "Type",
            "content": "e.g., α(t) = γ(t) = ˆα(t) = ˆγ(t) = λ [0, 1] ρ [0, 1] ν {1, 2} Diffusion EDM[18] σ(t) σ2(t)+ 1 4 1 σ2(t)+ 1 4 0.5 σ2(t)+ 1 4 2σ(t) σ2(t)+ 1 4 Flow Matching Consistency sCM[28] sin(t π OT[25] 1 1 2 ) cos(t π 2 ) cos(t π 2 ) sin(t π 2 ) 0 1 0 0 1 2 1 two primary challenges: (a) lack of unified understanding, hindering the cross-pollination of advancements between different model classes; and (b) limited cross-paradigm generalization, where algorithms designed for one type of model (e.g., diffusion) are often incompatible with others. To address these limitations, we introduce UCGM, novel framework that establishes unified foundation for the training, sampling, and conceptual understanding of continuous generative models. Specifically, the unified trainer UCGM-T is built upon unified training objective, parameterized by consistency ratio λ [0, 1]. This allows single training paradigm to flexibly produce models tailored for different inference regimes: models behave akin to multi-step diffusion or flow-matching approaches when λ is close to 0, and transition towards few-step consistency-like models as λ approaches 1. Moreover, this versatility can extend to compatibility with various noise schedules (e.g., linear, cosine, quadratic) without requiring bespoke algorithm modifications. Complementing the unified trainer UCGM-T, we propose unified sampling algorithm, UCGM-S, designed to work seamlessly with models trained via our objective. Crucially, UCGM-S is also effective for enhancing and accelerating sampling from pre-trained models developed under UCGMT and distinct prior paradigms. The unifying nature of our UCGM is further underscored by its ability to encapsulate prominent existing continuous generative paradigms as specific instantiations of UCGM, as detailed in Tab. 1. Moreover, as illustrated in Fig. 1, models trained with UCGM can achieve excellent sample quality across wide range of NFEs. key innovation within UCGM is the introduction of self-boosting techniques for both training and sampling. The training-time self-boosting mechanism enhances model quality and training efficiency, significantly reducing or eliminating the need for computationally expensive guidance techniques during inference. The sampling-time self-boosting, through methods like estimation extrapolation, markedly improves generation fidelity while minimizing NFEs. In summary, our contributions are: (a) unified trainer (UCGM-T) that seamlessly bridges few-step (e.g., consistency models) and multi-step (e.g., diffusion, flow-matching) generative paradigms, accommodating diverse model architectures, latent auto-encoders, and noise schedules. (b) versatile and unified sampler (UCGM-S) compatible with our trained models and, importantly, adaptable for accelerating and improving pre-trained models from existing distinct paradigms. (c) novel self-boosting mechanism integrated into the training process, which enhances model performance and training efficiency, notably reducing reliance on classifier-free guidance. (d) An analogous self-boosting technique for the sampling process, which markedly improves generation quality while minimizing the number of function evaluations. Extensive experiments validate the effectiveness and efficiency of UCGM. Our approach consistently matches or surpasses SOTA methods across various datasets, architectures, and resolutions, for both few-step and multi-step generation tasks (cf., the experimental results in Sec. 4)."
        },
        {
            "title": "2 Preliminaries",
            "content": "Given training dataset D, let p(x) represent its underlying data distribution, or p(xc) under condition c. Continuous generative models seek to learn an estimator that gradually transforms simple source distribution q(z) into complex target distribution p(x) within continuous space. 2 Typically, q(z) is represented by the standard Gaussian distribution (0, I). For instance, diffusion models generate samples by learning to reverse noising process that gradually perturbs data sample p(x) into noisy version xt = α(t)x + σ(t)z, where (0, I). Over the range [0, ], the perturbation intensifies with increasing t, where higher values indicate more pronounced noise. Below, we introduce three prominent learning paradigms for deep continuous generative models. Diffusion models [14, 38, 18]. In the widely adopted EDM method [18], the noising process is de- (cid:105) ω(t) θ(xt, t) x2 fined by setting α(t) = 1, σ(t) = t. The training objective is given by Ex,z,t 2 where ω(t) is weighting function. The diffusion model is parameterized by θ(xt, t) = cskip(t)xt + cout(t)F θ(cin(t)xt, cnoise(t)) where θ is neural network, and the coefficients cskip, cout, cin, and cnoise are manually designed. During sampling, EDM solves the Probability Flow Ordinary Differential Equation (PF-ODE) [38]: dxt dt = [xt θ(xt, t)]/t, integrated from = to = 0. (cid:104) Flow matching [25]. Flow matching models are similar to diffusion models but differ in the transport process from the source to the target distribution and in the neural network training objective. The forward transport process utilizes differentiable coefficients α(t) and γ(t), such that xt = α(t)z + γ(t)x. Typically, the coefficients satisfy the boundary conditions α(1) = γ(0) = 1 and (cid:105) α(0) = γ(1) = 0. The training objective is given by Ex,z,t . Similar to diffusion models, the reverse transport process (i.e., sampling process) begins at = 1 with x1 (0, I) and solves the PF-ODE: dxt dt = θ(xt, t), integrated from = 1 to = 0. (cid:13)F θ(xt, t) ( dαt dt + dσt dt x)(cid:13) 2 (cid:13) 2 (cid:104) ω(t) (cid:13) Consistency models [37, 28]. consistency model θ(xt, t) is trained to map the noisy input xt directly to the corresponding clean data in one or few steps by following the sampling trajectory of the PF-ODE starting from xt. To be valid, θ must satisfy the boundary condition θ(x, 0) x. Following EDM [18], one approach to enforce this condition is to parameterize the consistency model as θ(xt, t) = cskip(t)xt + cout(t)f θ (cin(t)xt, cnoise(t)) with cskip(0) = 1 and cout(0) = 0. The training objective is defined between two adjacent time steps with finite distance: Ext,t [ω(t)d (f θ(xt, t), θ (xtt, t))] , where θ denotes stopgrad(θ), > 0 is the distance between adjacent time steps, and d(, ) is metric function. Discrete-time consistency models are sensitive to the choice of t, necessitating manually designed annealing schedules [36, 11] for rapid convergence. This limitation is addressed by proposing training objective for continuous consistency models [28], derived by taking the limit as 0. In summary, both diffusion and flow-matching models are multi-step frameworks operating within continuous space, whereas consistency models are designed as few-step approaches."
        },
        {
            "title": "3 Methodology",
            "content": "We first introduce our unified training objective and algorithm, UCGM-T, applicable to both few-step and multi-step models, including consistency, diffusion, and flow-matching frameworks. Additionally, we present UCGM-S, our unified sampling algorithm, which is effective across all these models. 3.1 Unifying Training Objective for Continuous Generative Models We first propose unified training objective for diffusion and flow-matching models, which constitute all multi-step continuous generative models. Moreover, we extend this unified objective to encompass both few-step and multi-step models. Unified training objective for multi-step continuous generative models. We introduce generalized training objective below that effectively trains generative models while encompassing the formulations presented in existing studies: L(θ) := E(z,x)p(z,x),t (cid:20) 1 ω(t) θ(xt, t) zt2 2 (cid:21) , (1) 3 where time [0, 1], ω(t) is the weighting function for the loss, θ is neural network2 with parameters θ, xt = α(t)z + γ(t)x, and zt = ˆα(t)z + ˆγ(t)x. Here, α(t), γ(t), ˆα(t), and ˆγ(t) are unified transport coefficients over time t. Additionally, to efficiently and robustly train multi-step continuous generative models using objective (1), we propose three necessary constraints: (a) α(t) is continuous over the interval [0, 1], with α(0) = 0, α(1) = 1, and dα(t) (b) γ(t) is continuous over the interval [0, 1], with γ(0) = 1, γ(1) = 0, and dγ(t) (c) For all (0, 1), it holds that α(t) ˆγ(t) ˆα(t) γ(t) > 0 to ensure that α(t) ˆγ(t) ˆα(t) γ(t) dt 0. dt 0. is non-zero and can serve as the denominator in (3). Under these constraints, diffusion and flow-matching models are special cases of our unified training objective (1) with additional restrictions: (a) For example, following EDM [18, 20], by setting α(t) = 1 and σ(t) = t, diffusion models based on EDM can be derived from (1) provided that the constraint γ(t)/α(t) = is satisfied3. (b) Similarly, flow-matching models can be derived only when ˆα(t) = dα(t) dt (see Sec. 2 for more technical details about EDM-based and flow-based models). and ˆγ(t) = dγ(t) dt Unified training objective for both multi-step and few-step models. To facilitate the interpretation of our technical framework, we define two prediction functions based on model θ as: x(F t, xt, t) := α(t) ˆα(t) xt α(t) ˆγ(t) ˆα(t) γ(t) & z(F t, xt, t) := ˆγ(t) xt γ(t) α(t) ˆγ(t) ˆα(t) γ(t) , where we define := θ(xt, t). The training objective (1) can thus be transformed into: (cid:20) 1 ˆω(t) x(F θ(xt, t), xt, t) x2 2 L(θ) = E(z,x)p(z,x),t (cid:21) . (2) (3) α(t)ω(t) To align with the gradient of our training objective (1), we define the new weighting function ˆω(t) as ˆω(t) := α(t)ˆγ(t) ˆα(t)γ(t) . To unify few-step models, such as consistency models, with multi-step models, we adopt modified version of (3) by incorporating consistency ratio λ [0, 1]. The loss function L(θ) is then formulated as E(z,x)p(z,x),t (cid:20) 1 ˆω(t) x(F θ(xt, t), xt, t) x(F θ(xλt, λt), xλt, λt)2 2 (cid:21) , (4) where consistency models and conventional multi-steps models are special cases within the context of (4). Specifically, setting λ = 0 yields diffusion & flow-matching models, while setting λ 1 with 0 recovers consistency models. Following previous studies [37], we set ˆω(t) = tan(t) . The final training objective is therefore: 4 (cid:34) E(z,x)p(z,x),t cos(t) (cid:13) (cid:13) θ(xt, t) θ (xt, t) + (cid:13) (cid:13) 4α(t)f sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) (cid:35) (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 , (5) where the detailed derivation from (4) to (5) is provided in App. B.1.1, and we define x := x(F θ (xt, t), , t) x(F θ (xλt, λt), λt, λt) λt . (6) However, optimizing the unified objective in (5) presents challenge: stabilizing the training process as λ approaches 1. In this regime, the training dynamics resemble those of consistency models, known for unstable gradients, especially with FP16 precision [37, 28]. To address this, we propose several stabilizing training techniques stated below. 2For simplicity, unless otherwise specified, we assume that any conditioning information is incorporated into the network input. Thus, θ(xt, t) should be understood as θ(xt, t, c) when is applicable. 3In EDM, with σ(t) = t, the input of neural network θ is cin(t)xt = cin(t) (x + z). Although cin(t) can be manually adjusted, the coefficient before remains times that of x. 4 Algorithm 1 (UCGM-T). Unified and Efficient Trainer for Few-step and Multi-step Continuous Generative Models (including Diffusion, Flow Matching, and Consistency Models) Require: Dataset D, transport coefficients {α(), γ(), ˆα(), ˆγ()}, neural network θ, enhancement ratio ζ, Beta distribution parameters (θ1, θ2), learning rate η, stop gradient operator sg. Ensure: Trained neural network θ for generating samples from p(x). 1: repeat 2: 3: 4: 5: 6: Sample (0, I), D, ϕ(t) := Beta(θ1, θ2) Compute input data, such as xt = α(t) + γ(t) and xλt = α(λt) + γ(λt) Compute model output = θ(xt, t) and set = and = if ζ (0, 1) then = θ (xt, t, ) to get enhanced ξ(x, t, x(sg(F t), xt, t), x(F Let and ξ(z, t, z(sg(F t), xt, t), z(F (cid:0)ζ + 1t>s , xt, t)) := + 2 ζ(cid:1)(cid:1) (b 1t>s d(1 1t>s)), where 1() is the indicator function} (cid:0) 1 , xt, t)){Note that ξ(a, t, b, d) end if if λ [0, 1) then Compute Compute t = x(sg(F t), that for λ = 0, x(F θ (x0, 0), = α(t) + γ(t) and , t) ( 1 0, 0) = x} λt = α(λt) + γ(λt) tλt ) x(F θ (xλt, λt), λt, λt) ( 1 tλt ){Note else if λ = 1 then Comupte Let t+ϵ = α(t + ϵ) + γ(t + ϵ) and xtϵ = α(t ϵ) + γ(t ϵ) tϵ, tϵ)( 1 2ϵ )f x(F θ(xtϵ, tϵ), t = x(F θ (xt+ϵ, t+ϵ), 2ϵ ) t+ϵ, t+ϵ)( 1 α(t)ˆγ(t) ˆα(t)γ(t) clip(f (cid:13) 2 (cid:13)F target 2 and update θ θ ηθ (cid:13) ,1,1) sin(t) 4α(t) (cid:82) 1 0 ϕ(t)Lt(θ)dt end if Compute target = sg(F t) Compute loss Lt(θ) = cos(t) (cid:13) 16: 17: until Convergence 7: 8: 9: 10: 11: 12: 13: 14: 15: Stabilizing gradient as λ 1. We identify that the instability in objective (5) primarily arises from , which subsequently affect the training target target numerical computational errors in the term . ,t) Specifically, our theoretical analysis reveals that as λ 1, . (6) then serves as first-order difference approximation of x(F θ (xt,t),x , which is susceptible to significant computational errors. To mitigate this issue, we propose second-order difference estimation technique by redefining t approaches x(F θ (xt,t),x ,t) t as t = 1 2ϵ (cid:0)f x(F θ (xt+ϵ, + ϵ), t+ϵ, + ϵ) x(F θ (xtϵ, ϵ), tϵ, ϵ)(cid:1) . To further stabilize the training, we implement the following two strategies for t : (a) We adopt distributive reformulation of the second-difference term to prevent direct subtraction between nearly identical quantities, which can induce catastrophic cancellation, especially under limited numerical precision (e.g., FP16). Specifically, we factor out the shared scaling coefficient 2ϵ , namely, t = x(F θ (xt+ϵ, t+ϵ), 1 tϵ, tϵ) 1 2ϵ . In this paper, we consistently set ϵ to 0.005. 2ϵ x(F θ (xtϵ, tϵ), t+ϵ, t+ϵ) 1 (b) We observe that applying numerical truncation [28] to x enhances training stability. Specifically, we clip t to the range [1, 1], which prevents abnormal numerical outliers. Unified distribution transformation of time. Previous studies [46, 7, 37, 28, 18, 20] employ non-linear functions to transform the time variable t, initially sampled from uniform distribution U(0, 1). This transformation shifts the distribution of sampled times, effectively performing importance sampling and thereby accelerating the training convergence rate. For example, the 1 1+exp(µσΦ1(t)) is widely used [46, 7], where Φ1() lognorm function flognorm(t; µ, σ) = denotes the inverse Cumulative Distribution Function (CDF) of the standard normal distribution. In this work, we demonstrate that most commonly used non-linear time transformation functions can be effectively approximated by the regularized incomplete beta function: fBeta(t; a, b) = (cid:82) 0 τ a1(1τ )b1 dτ/(cid:82) 1 0 τ a1(1τ )b1 dτ, where detailed analysis defers to App. B.2.1. Consequently, Algorithm 2 (UCGM-S). Unified and Efficient Sampler for Few-step and Multi-step Continuous Generative Models (including Diffusion, Flow Matching, and Consistency Models) Require: Initial (0, I), transport coefficients {α(), γ(), ˆα(), ˆγ()}, trained model θ, sampling steps , order ν {1, 2}, time schedule , extrapolation ratio κ, stochastic ratio ρ. i=0 over generation process. Ensure: Final generated sample p(x) and history samples {ˆxi}N 1: Let (N + 1)/2 if using second order sampling (ν = 2) {Adjusts total steps to match first-order evaluation count} 2: for = 0 to 1 do 3: 4: 5: 6: 7: Compute model output = θ (x, ti), and then ˆxi = x(F , x, ti) and ˆzi = z(F , x, ti) if 1 then Compute extrapolated estimation ˆz = ˆzi + κ (ˆzi ˆzi1) and ˆx = ˆxi + κ (ˆxi ˆxi1) end if Sample (0, I) {An example choice of ρ for performing SDE-similar sampling is: ρ = clip( titi+12α(ti) , 0, 1)} Compute estimated next time sample = α(ti+1) ( if order ν = 2 and < 1 then ρ z) + γ(ti+1) ˆx 1 ρ ˆz + α(ti+1) Compute prediction = θ(x, ti+1), ˆx = x(F , x, ti+1) and ˆz = z(F , x, ti+1) ˆx+ˆx Compute corrected next time sample = γ(ti+1) 2 α(ti+1) γ(ti+1)α(ti) γ(ti) + γ(ti) (cid:17) (cid:16) 8: 9: 10: 11: end if Reset x 12: 13: 14: end for we simplify the process by directly sampling time from Beta distribution, i.e., Beta(θ1, θ2), where θ1 and θ2 are parameters that control the shape of Beta distribution. Learning enhanced score function. Directly employing objective (5) to train models for estimating the conditional distribution p(xc) results in models incapable of generating realistic samples without Classifier-Free Guidance (CFG) [15]. While enhancing semantic information, CFG approximately doubles the number of function evaluations (NFE), incurring significant computational overhead. recent work [41] proposes modifying the target score function in [38] from xt log(pt(xtc)) pt(xtc) (pt,θ (xtc)/pt,θ (xt))ζ(cid:17) to an enhanced version xt log , where ζ (0, 1) denotes the enhancement ratio. This approach enables the trained models to generate highly realistic samples without relying on CFG. (cid:16) Inspired by this, we propose enhancing the target score function in manner compatible with our unified training objective (5). Specifically, we introduce time-dependent enhancement strategy: (a) For [0, s], enhance and by applying x+ζ (cid:0)f x(F t, xt, t) x(F , xt, t)(cid:1), + ζ (cid:0)f z(F t, xt, t) z(F , xt, t)(cid:1). Here, = θ(xt, t, ) and = θ(xt, t). (b) For (s, 1], enhance and by applying + 1 2 (f x(F t, xt, t) x) and + 1 2 (f z(F t, xt, t) z). We consistently set = 0.75 (cf., App. B.1.4 for more analysis). In summary, our complete algorithm UCGM-T is detailed in Alg. 1. 3.2 Unifying Sampling Process for Continuous Generative Models In this section, we introduce our unified sampling algorithm applicable to both consistency models and diffusion/flow-based models. For classical iterative sampling models, such as trained flow-matching model θ, sampling from the learned distribution p(x) involves solving the PF-ODE [38]. This process typically uses numerical ODE solvers, such as the Euler or Runge-Kutta methods [29], to iteratively transform the initial Gaussian noise into sample from p(x) by solving the ODE (i.e., dx dt = θ(x, t)), where denotes the sample at time t, and θ(x, t) represents the model output at that time. Similarly, sampling processes in models like EDM [18, 20] and consistency models [37] involve comparable gradual 6 denoising procedure. Building on these observations and our unified trainer UCGM-T, we first propose general iterative sampling process with two stages: (a) Decomposition: At time t, the current input xt is decomposed into two components: xt = α(t) ˆzt + γ(t) ˆxt. This decomposition uses the estimation model θ. Specifically, the model output = θ (xt, t) is computed, yielding the estimated clean component ˆxt = x(F t, xt, t) and the estimated noise component ˆzt = z(F t, xt, t). (b) Reconstruction: The next time steps input, t, is generated by combining the estimated components: xt = α(t) ˆzt + γ(t) ˆxt. The process then iterates to stage (a). We then introduce two enhancement techniques below to optimize the sampling process: (a) Extrapolating the estimation. Directly utilizing the estimated ˆxt and ˆzt to reconstruct the subsequent input xt can result in significant estimation errors, as the estimation model θ does not perfectly align with the target function target for solving the PF-ODE. Note that CFG [15] guides conditional model using an unconditional model, namely, θ(x, t) = θ (x, t) θ(x, t)(cid:1), where κ is the guidance ratio. This approach can be θ(x, t) + κ (cid:0)f interpreted as leveraging less accurate estimation to guide more accurate one [19]. Extending this insight, we propose to extrapolate the next time-step estimates ˆxt and ˆzt using the previous estimates ˆxt and ˆzt, formulated as: ˆxt ˆxt +κ(ˆxt ˆxt) and ˆzt ˆzt +κ(ˆzt ˆzt), where κ [0, 1] is the extrapolation ratio. This extrapolation process significantly enhances sampling quality and reduces the number of sampling steps. Notably, this technique is compatible with CFG and does not introduce additional computational overhead (see Sec. 4.2 for details). (b) Incorporating stochasticity. During the aforementioned sampling process, the input xt is deterministic, potentially limiting the diversity of generated samples. To mitigate this, we introduce stochastic term ρ to xt, defined as: xt = α(t) (cid:0) ρ z(cid:1) + γ(t) ˆxt, where (0, I) is random noise vector, and ρ is the stochasticity ratio. This stochastic term acts as random perturbation to xt, thereby enhancing the diversity of generated samples. 1 ρ ˆzt + Empirical results demonstrate that setting ρ = λ consistently yields optimal performance in terms of generation quality. The theoretical analysis of this phenomenon is deferred to future work. Furthermore, empirical investigation of κ indicates that the range [0.2, 0.6] is consistently beneficial (cf., Sec. 4.4). Performance is observed to be relatively insensitive within this specific range. Unifed sampling algorithm UCGM-S. Putting all these factors together, here we introduce unified sampling algorithm applicable to consistency models and diffusion & flow-based models, as presented in Alg. 2. This framework demonstrates that classical samplers, such as the Euler sampler utilized for flow-matching models [29], constitute special case of our UCGM-S (cf. App. B.1.6 for analysis). Extensive experiments demonstrate two key features of this algorithm: (a) Reduced computational resources: It decreases the number of sampling steps required by existing models while maintaining or enhancing performance. (b) High compatibility: It is compatible with existing models, irrespective of their training objectives or noise schedules, without necessitating modifications to model architectures or tuning. For detailed justifications of these features, please refer to Sec. 4."
        },
        {
            "title": "4 Experiment",
            "content": "This section details the experimental setup and evaluation of our proposed methodology, UCGM- {T, S}. Our approach relies on specific parameterizations of the transport coefficients α(), γ(), ˆα(), and ˆγ(), as detailed in Alg. 1 and Alg. 2. Therefore, Tab. 2 provides summary of the parameterizations used in our experiments. We evaluate both the training (UCGM-T) and sampling (UCGM-S) capabilities of our framework. Crucially, we also demonstrate the compatibility of UCGM-S with models trained using prior methods, in which we show how these methods/models can be represented within our unified transport coefficient framework. 4.1 Experimental Setting The experimental settings are detailed below (additional details are provided in App. A.1). 7 Table 2: Comparison of different transport types employed during the sampling and training phases of our UCGM-{T, S}. TrigLinear and Random are introduced herein specifically for ablation studies. TrigLinear is constructed by combining the transport coefficients of Linear and TrigFlow. Random represents randomly designed transport type used to demonstrate the generality of our UCGM. Other transport types are adapted from existing methods and transformed into the transport coefficient representation used by UCGM."
        },
        {
            "title": "Linear ReLinear",
            "content": "α(t) γ(t) ˆα(t) ˆγ(t) 1 1 1 1 1 1 TrigFlow sin(t π 2 ) cos(t π 2 ) cos(t π 2 ) sin(t π 2 ) EDM (σ(t) = e4(2.68t1.59)) TrigLinear sin(t π 2 ) cos(t π 2 ) 1 σ(t)/ 1/ 0.5/ 2σ(t)/ σ2(t)+0.25 σ2(t)+0.25 σ2(t)+0.25 σ2(t)+0.25 Random sin(t π 2 ) 1 1 1 e5t e.g., [29, 48] [46, 44] [28, 3] [37, 20, 18] N/A N/A Datasets. We utilize ImageNet-1K [5] at resolutions of 512 512 and 256 256 as our primary datasets, following prior studies [20, 37] and adhering to ADMs data preprocessing protocols [6]. Additionally, CIFAR-10 [22] at resolution of 32 32 is employed for ablation studies. For both 512 512 and 256 256 images, experiments are conducted using latent space generative modeling in line with previous works. Specifically: (a) For 256 256 images, we employ multiple widely-used auto-encoders, including SD-VAE [32], VA-VAE [46], and E2E-VAE [23]. (b) For 512 512 images, DC-AE (f32c32) [4] with higher compression rate is used to conserve computational resources. When utilizing SD-VAE for 512 512 images, 2 larger patch size is applied to maintain computational parity with the 256 256 setting. Consequently, the computational burden for generating images at both 512 512 and 256 256 resolutions remains comparable across our trained models4. Further details on datasets and autoencoder parameters are provided in App. A.1.1. Neural network architectures. We evaluate UCGM-S sampling using models trained with established methodologies. These models employ various architectures from two prevalent families commonly used in continuous generative models: (a) Diffusion Transformers, including variants such as DiT [31], UViT [1], SiT [29], LighteningDiT [46], and DDT [44]. (b) UNet-based convolutional networks, including improved UNets [18, 38] and EDM2-UNets [20]. For training models specifically for UCGM-T, we consistently utilize DiT as the backbone architecture. We train models of various sizes (B: 130M, L: 458M, XL: 675M parameters) and patch sizes. Notation such as XL/2 denotes the XL model with patch size of 2. Following prior work [46, 44], minor architectural modifications are applied to enhance training stability (details in App. A.1.2). Baselines. We compare our approach against several SOTA continuous and discrete generative models representative of distinct methodologies. We broadly categorize these baselines by their generation process: (a) Multi-step models. These methods typically synthesize data through sequence of steps. We include various diffusion models, encompassing classical formulations like DDPM and score-based models [35, 14], and advanced variants focusing on improved sampling or performance in latent spaces [6, 18, 31, 50, 1]. We also consider flow-matching models [25], which leverage continuous normalizing flows and demonstrate favorable training properties, along with subsequent scaling efforts [29, 48, 46]. Additionally, we include autoregressive models [24, 42, 47], which generate data sequentially, often in discrete domains. (b) Few-step models. These models are designed for efficient, often single-step or few-step, generation. This category includes generative adversarial networks [12], which achieve efficient one-step synthesis through adversarial training, and their large-scale variants [2, 33, 17]. We also 4Previous works often employed the same auto-encoders and patch sizes for both resolutions, resulting in higher computational costs for generating 512 512 images. Table 3: Sample quality comparison for multi-step generation task on class-conditional ImageNet-1K. Notation AB denotes the result obtained by combining methods and B. / indicate decrease/increase, respectively, in the metric compared to the baseline performance of the pre-trained models. METHOD NFE () FID () #Params #Epochs METHOD NFE () FID () #Params #Epochs 512 512 256 ADM-G [6] U-ViT-H/4 [1] DiT-XL/2 [31] SiT-XL/2 [29] MaskDiT [50] EDM2-S [20] EDM2-L [20] EDM2-XXL [20] DiT-XL/1[4] U-ViT-H/1[4] REPA-XL/2 [48] DDT-XL/2 [44] VQGAN[8] MAGVIT-v2 [47] MAR-L [24] VAR-d36-s [42] 2502 502 2502 2502 792 63 63 63 2502 302 2502 2502 256 642 2562 102 7.72 4.05 3.04 2.62 2.50 2.56 2.06 1.91 2.41 2.53 2.08 1.28 18.65 1.91 1.73 2. Diffusion & flow-matching Models 559M 501M 675M 675M 736M 280M 778M 1.5B 675M 501M 675M 675M 227M 307M 479M 2.3B 388 400 600 600 - 1678 1476 734 400 400 200 - ADM-G [6] U-ViT-H/2 [1] DiT-XL/2 [31] SiT-XL/2 [29] MDT [10] REPA-XL/2 [48] REPA-XL/2 [48] Light.DiT [46] Light.DiT [46] DDT-XL/2 [44] DDT-XL/2 [44] REPA-E-XL [23] GANs & masked models - 1080 800 350 VQGAN[40] MAR-L [24] MAR-H [24] VAR-d30-re [42] 2502 502 2502 2502 2502 2502 2502 2502 2502 2502 2502 2502 - 2562 2562 102 4.59 2.29 2.27 2.06 1.79 1.96 1.42 2.11 1.35 1.31 1.26 1.26 2.18 1.78 1.55 1. Ours: UCGM-S sampling with models trained by prior works UCGM-S[20] UCGM-S[20] UCGM-S[20] UCGM-S[44] 4023 5013 4023 200300 2.530.03 2.040.02 1.880.03 1.250.03 280M 778M 1.5B 675M - - - - UCGM-S[44] UCGM-S[46] UCGM-S[23] UCGM-S[23] 100400 100400 80420 20480 1.270.01 1.210.14 1.060.20 2.000.74 Ours: models trained and sampled using UCGM-{T, S} (setting λ = 0) DC-AE [4] DC-AE [4] SD-VAE [32] SD-VAE [32] 40 20 40 1.48 1.68 1.67 1.80 675M 675M 675M 675M 800 800 320 320 SD-VAE [32] VA-VAE [46] E2E-VAE [23] E2E-VAE [23] 60 60 40 20 1.41 1.21 1.21 1. 559M 501M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 3.1B 479M 943M 2.0B 675M 675M 675M 675M 675M 675M 675M 675M 396 400 1400 1400 1300 200 800 64 800 256 400 800 300 800 800 - - - - 400 400 800 800 evaluate consistency models [37], proposed for high-quality generation adaptable to few sampling steps, and subsequent techniques aimed at improving their stability and scalability [36, 28, 51]. Consistent with prior work [38, 14, 25, 2], we adopt standard evaluation protocols. The primary metric for assessing image quality is the Fréchet Inception Distance (FID) [13], calculated on 50,000 generated images (FID-50K). Additional details regarding evaluation metrics and baselines are provided in App. A. Implementation details. Our implementation is developed in PyTorch [30]. Training employs AdamW [27] for multi-step sampling models. For few-step sampling models, RAdam [26] is used to improve training stability. Consistent with standard practice in generative modeling [48, 29], an exponential moving average (EMA) of model weights is maintained throughout training using decay rate of 0.9999. All reported results utilize the EMA model. Comprehensive hyperparameters and additional implementation details are provided in App. A.1.3. 4.2 Comparison with SOTA Methods for Multi-step Generation Our experiments on ImageNet-1K at 512512 and 256256 resolutions systematically validate the three key advantages of UCGM: (1) sampling acceleration via UCGM-S on pre-trained models, (2) ultra-efficient generation with joint UCGM-T+UCGM-S, and (3) broad compatibility. UCGM-S: Plug-and-play sampling acceleration. UCGM-S provides method for accelerating sampling from pre-trained generative models by reducing the required number of Number of Function Evaluations (NFEs) while preserving or improving generation quality, as measured by FID. Applied to 512 512 image generation, the approach demonstrates notable efficiency gains: (a) For the diffusion-based models, such as pre-trained EDM2-XXL model, UCGM-S reduced NFEs from 63 to 40 (a 36.5% reduction), concurrently improving FID from 1.91 to 1.88. 9 Table 4: Sample quality comparison for few-step generation task on class-conditional ImageNet-1K. METHOD NFE () FID () #Params #Epochs METHOD NFE () FID () #Params #Epochs 512 512 256 256 sCT-M [28] sCT-L [28] sCT-XXL [28] sCD-M [28] sCD-L [28] sCD-XXL [28] 1 2 1 2 1 2 1 2 1 2 1 2 BigGAN [2] StyleGAN [33] MAGVIT-v2 [47] VAR-d36-s [42] 1 12 642 102 5.84 5.53 5.15 4.65 4.29 3.76 2.75 2.26 2.55 2.04 2.28 1. 8.43 2.41 1.91 2.63 Consistency training & distillation 498M 498M 778M 778M 1.5B 1.5B 498M 498M 778M 778M 1.5B 1.5B 160M 168M 307M 2.3B 1837 1837 1274 1274 762 762 1997 1997 1434 1434 921 921 iCT [36] Shortcut-XL/2 [9] IMM-XL/2 [51] IMM (ω = 1.5) 2 1 4 128 12 22 42 82 12 22 42 82 GANs & masked models - - 1080 350 BigGAN [2] GigaGAN [17] StyleGAN [33] VAR-d30-re [42] 1 1 12 102 20.3 10.6 7.80 3.80 7.77 5.33 3.66 2.77 8.05 3.99 2.51 1.99 6.95 3.45 2.30 1.73 DC-AE [4] DC-AE [4] DC-AE [4] DC-AE [4] DC-AE [4] DC-AE [4] SD-VAE [32] SD-VAE [32] Ours: models trained and sampled using UCGM-{T, S} (setting λ = 0) 1.55 1.81 3.07 74.0 675M 675M 675M 675M 800 800 800 800 VA-VAE [46] VA-VAE [46] E2E-VAE [23] E2E-VAE [23] 16 8 16 8 2.11 6.09 1.40 2. Ours: models trained and sampled using UCGM-{T, S} (setting λ = 1) 2.42 1.75 2.63 2.11 675M 675M 675M 675M 840 840 360 360 VA-VAE [46] VA-VAE [46] SD-VAE [32] E2E-VAE [23] 2 1 1 1.42 2.19 2.10 2.29 32 16 8 4 1 2 1 2 675M 676M 676M 676M 675M 675M 675M 675M 675M 675M 675M 675M 112M 569M 166M 2.0B 675M 675M 675M 675M 675M 675M 675M 675M - 250 250 250 3840 3840 3840 3840 3840 3840 3840 3840 - - - 350 400 400 800 800 432 432 424 264 (b) When applied to the flow-based models, such as pre-trained DDT-XL/2 model, UCGM-S achieved an FID of 1.25 with 200 NFEs, compared to the original 1.28 FID requiring 500 NFEs. This demonstrates performance improvement achieved alongside enhanced efficiency. This approach generalizes across different generative model frameworks and resolutions. For instance, on 256 256 resolution using the flow-based REPA-E-XL model, UCGM-S attained 1.06 FID at 80 NFEs, which surpasses the baseline performance of 1.26 FID achieved at 500 NFEs. In summary, UCGM-S acts as broadly applicable technique for efficient sampling, demonstrating cases where performance (FID) improves despite reduction in sampling steps. UCGM-T + UCGM-S: Synergistic efficiency. The combination of UCGM-T training and UCGM-S sampling yields highly competitive generative performance with minimal NFEs: (a) 512 512: With DC-AE autoencoder, our framework achieved 1.48 FID at 40 NFEs. This outperforms DiT-XL/1DC-AE (2.41 FID, 500 NFEs) and EDM2-XXL (1.91 FID, 63 NFEs), with comparable or reduced model size. (b) 256 256: With an E2E-VAE autoencoder, we attained 1.21 FID at 40 NFEs. This result exceeds prior SOTA models like MAR-H (1.55 FID, 512 NFEs) and REPA-E-XL (1.26 FID, 500 NFEs). Importantly, models trained with UCGM-T maintain robustness under extremely low-step sampling regimes. At 20 NFEs, the 256 256 performance degrades gracefully to 1.30 FID, result that still exceeds the performance of several baseline models operating at significantly higher NFEs. In summary, the demonstrated robustness and efficiency of UCGM-{T, S} across various scenarios underscore the high potential of our UCGM for multi-step continuous generative modeling. 4.3 Comparison with SOTA Methods for Few-step Generation As evidenced by the results in Tab. 4, our UCGM-{T, S} framework exhibits superior performance across two key settings: λ = 0, characteristic of multi-step regime akin to diffusion and flowmatching models, and λ = 1, indicative of few-step regime resembling consistency models. 10 Intermediate images generated during 60-step sampling from UCGM-S. Columns display Figure 2: intermediate images ˆxt produced at different timesteps during single sampling trajectory, ordered from left to right by decreasing t. Rows correspond to models trained with λ {0.0, 0.5, 1.0}, ordered from top to bottom. (a) Various λ and sampling steps. (b) Different ζ and transport types. (c) Various κ and sampling steps. Figure 3: Ablation studies of UCGM on ImageNet-1K 256256. These studies evaluate key factors of the proposed UCGM. Ablations presented in (a) and (c) utilize XL/1 models with the VA-VAE autoencoder. For the results shown in (b), B/2 models with the SD-VAE autoencoder are used to facilitate more efficient training. Few-step regime (λ = 1). Configured for few-step generation, UCGM-{T, S} achieves SOTA sample quality with minimal NFEs, surpassing existing specialized consistency models and GANs: (a) 512 512: Using DC-AE autoencoder, our model achieves an FID of 1.75 with 2 NFEs and 675M parameters. This outperforms sCD-XXL, leading consistency distillation model, which reports 1.88 FID with 2 NFEs and 1.5B parameters. (b) 256 256: Using VA-VAE autoencoder, our model achieves an FID of 1.42 with 2 NFEs. This is notable improvement over IMM-XL/2, which obtains 1.99 FID with 8 2 = 16 NFEs, demonstrating higher sample quality while requiring 8 fewer sampling steps. In summary, these results demonstrate the capability of UCGM-{T, S} to deliver high-quality generation with minimal sampling cost, which is advantageous for practical applications. Multi-step regime (λ = 0). In the multi-step regime (λ = 0), where models is optimized for multi-step sampling, it nonetheless demonstrates competitive performance even when utilizing moderate number of sampling steps. (a) 512 512: Using DC-AE autoencoder, our model obtains an FID of 1.81 with 16 NFEs and 675M parameters. This result is competitive with or superior to existing methods such as VAR-d30-s, which reports 2.63 FID with 10 2 = 20 NFEs and 2.3B parameters. (b) 256 256: Using an E2E-VAE autoencoder, our model achieves an FID of 1.40 with 16 NFEs. This surpasses IMM-XL/2, which obtains 1.99 FID with 8 2 = 16 NFEs, demonstrating improved quality at the same sampling cost. In summary, our UCGM-{T, S} framework demonstrates versatility and high performance across both few-step (λ = 1) and multi-step (λ = 0) sampling regimes. As shown, it consistently achieves SOTA or competitive sample quality relative to existing methods, often requiring fewer sampling steps or parameters, which are important factors for efficient high-resolution image synthesis. 11 4.4 Ablation Study over the Key Factors of UCGM Unless otherwise specified, experiments in this section are conducted with κ = 0.0 and λ = 0.0. Effect of λ in UCGM-T. Fig. 3a demonstrates that varying λ influences the range of effective sampling steps for trained models. For instance, with λ = 1, optimal performance is attained at 2 sampling steps. In contrast, with λ = 0.5, optimal performance is observed at 16 steps. To investigate this phenomenon, we visualize intermediate samples generated during the sampling process (Alg. 2). Fig. 2 demonstrates: (a) For λ = 1.0, high visual fidelity is achieved early in the sampling process. (b) In contrast, for λ = 0.5, high visual fidelity emerges in the mid to late stages of sampling. Impact of transport type in UCGM. The results in Fig. 3b demonstrates that UCGM-{T, S} is applicable with various transport types, albeit with some performance variation. Investigating these performance differences constitutes future work. The results also illustrate that the enhanced training objective (achieved with ζ = 0.45 compared to ζ = 0.0, per Sec. 3) consistently improves performance across all tested transport types. Setting different κ in UCGM-S. Experimental results, depicted in Fig. 3c, illustrate the impact of κ on the trade-off between sampling steps and generation quality: (a) High κ values (e.g., 1.0 and 0.75) prove beneficial for extreme few-step sampling scenarios (e.g., 4 steps). (b) Moreover, mid-range κ values (0.25 to 0.5) achieve superior performance with fewer steps compared to κ = 0.0."
        },
        {
            "title": "References",
            "content": "[1] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2266922679, 2023. [2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. [3] Junsong Chen, Shuchen Xue, Yuyang Zhao, Jincheng Yu, Sayak Paul, Junyu Chen, Han Cai, Enze Xie, and Song Han. Sana-sprint: One-step diffusion with continuous-time consistency distillation. arXiv preprint arXiv:2503.09641, 2025. [4] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, and Song Han. Deep compression autoencoder for efficient high-resolution diffusion models. arXiv preprint arXiv:2410.10733, 2024. [5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [6] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [7] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [8] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [9] Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter Abbeel. One step diffusion via shortcut models. arXiv preprint arXiv:2410.12557, 2024. [10] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2316423173, 2023. 12 [11] Zhengyang Geng, Ashwini Pokle, William Luo, Justin Lin, and Zico Kolter. Consistency models made easy. arXiv preprint arXiv:2406.14548, 2024. [12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. [13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [15] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [16] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. [17] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1012410134, 2023. [18] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. [19] Tero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. Advances in Neural Information Processing Systems, 37:5299653021, 2024. [20] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2417424184, 2024. [21] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. [22] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 and cifar-100 datasets. URl: https://www. cs. toronto. edu/kriz/cifar. html, 6(1):1, 2009. [23] Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers. arXiv preprint arXiv:2504.10483, 2025. [24] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024. [25] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [26] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265, 2019. [27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [28] Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. arXiv preprint arXiv:2410.11081, 2024. [29] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. [30] Paszke. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019. [31] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [33] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 110, 2022. [34] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. [35] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [36] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. arXiv preprint arXiv:2310.14189, 2023. [37] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. [38] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [39] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [40] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [41] Zhicong Tang, Jianmin Bao, Dong Chen, and Baining Guo. Diffusion models without classifierfree guidance. arXiv preprint arXiv:2502.12154, 2025. [42] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. [43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [44] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. Ddt: Decoupled diffusion transformer. arXiv preprint arXiv:2504.05741, 2025. [45] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. [46] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. arXiv preprint arXiv:2501.01423, 2025. [47] Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 14 [48] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. [49] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. [50] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. arXiv preprint arXiv:2306.09305, 2023. [51] Linqi Zhou, Stefano Ermon, and Jiaming Song. Inductive moment matching. arXiv preprint arXiv:2503.07565, 2025."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Preliminaries 3 Methodology"
        },
        {
            "title": "3.1 Unifying Training Objective for Continuous Generative Models\n. . . . . . . . . .\n3.2 Unifying Sampling Process for Continuous Generative Models . . . . . . . . . . .",
            "content": "4 Experiment"
        },
        {
            "title": "4.1 Experimental Setting .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2 Comparison with SOTA Methods for Multi-step Generation . . . . . . . . . . . . .\n4.3 Comparison with SOTA Methods for Few-step Generation . . . . . . . . . . . . .\n4.4 Ablation Study over the Key Factors of UCGM . . . . . . . . . . . . . . . . . . .",
            "content": ". Detailed Experiment (Work in Progress) A.1 Detailed Experimental Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1.1 Detailed Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1.2 Detailed Neural Architecture . . . . . . . . . . . . . . . . . . . . . . . . . A.1.3 Detailed Implementation Details . . . . . . . . . . . . . . . . . . . . . . . A.2 Detailed Comparison with SOTA Methods for Multi-step Generation . . . . . . . . A.3 Detailed Comparison with SOTA Methods for Few-step Generation . . . . . . . . Theoretical Analysis (Work in Progress) B.1 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1.1 Unifed Training Objective . . . . . . . . . . . . . . . . . . . . . . . . . . B.1.2 Learning Objective as λ 1 . . . . . . . . . . . . B.1.3 Learning Objective as λ 0 . . . . . . . . . . . . . . . B.1.4 Enhanced Target Score . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1.5 Analysis on the Optimal Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1.6 Unified Sampling Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2.1 Beta Transformation . B.2.2 Kumaraswamy Transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Other Techniques . 1 2 3 3 6 7 7 9 10 12 17 17 17 17 17 19 21 21 21 24 25 27 28 29 30 30 31 16 Detailed Experiment (Work in Progress) A.1 Detailed Experimental Setting A.1.1 Detailed Datasets Image datasets. We conduct experiments on two datasets: CIFAR-10 [21], ImageNet-1K [5]: (a) CIFAR-10 is widely used benchmark dataset for image classification and generation tasks. It consists of 60, 000 color images, each with resolution of 32 32 pixels, categorized into 10 distinct classes. The dataset is divided into 50, 000 training images and 10, 000 test images. (b) ImageNet-1K is large-scale dataset containing over 1.2 million high-resolution images across 1, 000 categories. Latent space datasets. However, directly training diffusion transformers in the pixel space is computationally expensive and inefficient. Therefore, following previous studies [48, 29], we train our diffusion transformers in latent space instead. Tab. 5 presents comparative analysis of various Variational Autoencoder (VAE) architectures. SD-VAE is characterized by higher spatial resolution in its latent representation (e.g., H/8 W/8) combined with lower channel capacity (4 channels). Conversely, alternative models such as VA-VAE, E2E-VAE, and DC-AE achieve more significant spatial compression (e.g., H/16 W/16 or H/32 W/32) at the expense of an increased channel depth (typically 32 channels). key consideration is that the computational cost of diffusion transformer subsequently processing these latent representations is primarily dictated by their spatial dimensions, rather than their channel capacity [4]. Specifically, if the latent map is processed by transformer by dividing it into nonoverlapping patches, the cost is proportional to the number of these patches. This quantity is given by (H/Compression Ratio/Patch Size) (W/Compression Ratio/Patch Size). Here, and are the input image dimensions, Compression Ratio refers to the spatial compression factor of the VAE (e.g., 8, 16, 32 as detailed in Tab. 5), and Patch Size denotes the side length of the patches processed by the transformer. Table 5: Comparison of different VAE architectures in terms of latent space dimensions and channel capacity. The table contrasts four variational autoencoder variants (SD-VAE, VA-VAE, E2E-VAE, and DC-AE) by their spatial compression ratios (latent size) and feature channel dimensions. Here, and denote input image height and width (e.g., 256 256 or 512 512), respectively. SD-VAE (ema, mse) [32] VA-VAE [46] E2E-VAE [23] DC-AE (f32c32) [4] Latent Size Channels (H/8) (W/8) 4 (H/16) (W/16) (H/16) (W/16) (H/32) (W/32) 32 32 A.1.2 Detailed Neural Architecture Diffusion Transformers (DiTs) represent paradigm shift in generative modeling by replacing the traditional U-Net backbone with Transformer-based architecture. Proposed by Scalable Diffusion Models with Transformers [31], DiTs exhibit superior scalability and performance in image generation tasks. In this paper, we utilize three key variantsDiT-B (130M parameters), DiT-L (458M parameters), and DiT-XL (675M parameters). To improve training stability, informed by recent studies [46, 44], we incorporate several architectural modifications into the DiT framework: (a) SwiGLU feed-forward networks (FFN) [34]; (b) RMSNorm [49] without learnable affine parameters; (c) Rotary Positional Embeddings (RoPE) [39]; and (d) parameter-free RMSNorm applied to Key (K) and Query (Q) projections in self-attention layers [43]. This modified architecture, termed efficienT diffusion Transformer (TiT), reflects these integrated enhancements. A.1.3 Detailed Implementation Details Hyperparameter Configuration. Detailed hyperparameter configurations are provided in Tab. 6 to ensure reproducibility. The design of time schedules for sampling processes varies in complexity. For few-step models, typically employing 1 or 2 sampling steps, manual schedule design is 17 Table 6: Hyperparameter configurations for UCGM-{T, S} training and sampling on ImageNet-1K. We maintain consistent batch size of 1024 across all experiments. Training durations (epoch counts) are provided in other tables throughout the paper. The table specifies optimizer choices, learning rates, and key parameters for both UCGM-T and UCGM-S variants across different model architectures and datasets. Optimizer UCGM-T UCGM-S Task Resolution VAE/AE Model Type lr (β1,β2) Transport (θ1,θ2) λ ζ E2E-VAE SD-VAE VA-VAE DC-AE SD-VAE E2E-VAE SD-VAE VA-VAE DC-AE SD-VAE XL/1 XL/2 XL/ XL/1 XL/4 XL/1 XL/2 XL/2 XL/1 XL/4 Multi-step model training and sampling AdamW 0.0002 AdamW 0.0002 AdamW 0.0002 AdamW 0.0002 AdamW 0. (0.9,0.95) (0.9,0.95) (0.9,0.95) (0.9,0.95) (0.9,0.95) Linear Linear Linear Linear Linear (1.0,1.0) (2.4,2.4) (1.0,1.0) (1.0,1.0) (2.4,2.4) Few-step model training and sampling RAdam 0.0001 RAdam 0.0001 RAdam 0.0001 (0.9,0.999) (0.9,0.999) (0.9,0.999) RAdam 0.0001 RAdam 0.0001 (0.9,0.999) (0.9,0.999) Linear Linear Linear Linear Linear (0.8,1.0) (0.8,1.0) (0.8,1.0) (0.8,1.0) (0.8,1.0) 0 0 0 0 0 1 1 1 1 0.67 0.44 0.47 0.57 0.60 1.3 2.0 2.0 1.5 1.5 512 256 512 ρ 0 0 0 0 1 1 1 1 1 κ 0.5 0.21 0.5 0.46 0. Auto Auto Auto Auto Auto 0 0 0 0 0 {1,0.5} {1,0.3} {1,0.3} {1,0.6} {1,0.5} ν 1 1 1 1 1 1 1 1 1 1 straightforward. However, the time schedule utilized by our UCGM-S often comprises large number of time points, particularly for large number of sampling steps . Manual design of such dense schedules is challenging and can limit the achievable performance of our UCGM-{T, S}, as prior work [46, 44] has established that carefully designed schedules significantly enhance multi-step models, including flow-matching variants. To address this, we propose transforming each time point using generalized Kumaraswamy transformation: fKuma(t; a, b, c) = (1 (1 ta)b)c. This choice is motivated by the common practice in prior studies of applying non-linear transformations to individual time points to construct effective schedules. For instance, the timeshift function fshift(t; s) = (st)/(1+(s1)t) [46], where > 0 exemplifies such transformation. We find that the Kumaraswamy transformation, by appropriate selection of parameters a, b, c, can effectively approximate fshift and other widely-used functions, including the identity function (t) = [48, 23]. Empirical evaluations suggest that the parameter configuration (a, b, c) = (1.17, 0.8, 1.1) yields robust performance across diverse scenarios, corresponding to the \"Auto\" setting in Tab. 6. Detailed implementation techniques of enhancing target score function. We enhance the target score function for conditional diffusion models by modifying the standard score xt log pt(xtc) [38] to an enhanced version derived from the density pt(xtc) (pt,θ (xtc)/pt,θ (xt))ζ. This corresponds to target score of xt log pt(xtc) + ζ (xt log pt,θ(xtc) xt log pt,θ(xt)). The objective is to guide the learning process towards distributions that yield higher quality conditional samples. Accurate estimation of the model probabilities pt,θ is crucial for the effectiveness of this enhancement. We find that using parameters from an Exponential Moving Average (EMA) of the model during training improves the stability and quality of these estimates, resulting better and in Alg. 1. When training few-step models, direct computation of the enhanced target score gradient typically requires evaluating the model with and without conditioning (for the pt,θ terms), incurring significant computational cost. To address this, we propose an efficient approximation that leverages wellpre-trained multi-step model, denoted by parameters θ. Instead of computing the score gradient explicitly, the updates for the variables and (as used in Algorithm 1) are calculated based on features or outputs derived from single forward pass of the pre-trained model θ. Specifically, we compute = θ (xt, t), representing features extracted by the pre-trained model θ at time given input xt. The enhanced updates and are then computed as follows: (a) For [0, s], the updates are: x+ζ (f x(F t, xt, t) x), z+ζ (f z(F t, xt, t) z). 2 (f x(F t, xt, t) x) and + (b) For (s, 1], the updates are: + 1 2 (f z(F t, xt, t) z). We consistently set the time threshold = 0.75. This approach allows us to incorporate the guidance from the enhanced target signal with the computational cost equivalent to single forward evaluation of the pre-trained model θ per step. 18 A.2 Detailed Comparison with SOTA Methods for Multi-step Generation Table 7: Sample quality comparison for multi-step generation task on class-conditional ImageNet-1K. Notation AB denotes the result obtained by combining methods and B. / indicate decrease/increase, respectively, in the metric compared to the baseline performance of the pre-trained models. METHOD VAE/AE Patch Size Activation Size NFE () FID () IS () #Params #Epochs ADM-G [6] U-ViT-H/4 [1] DiT-XL/2 [31] SiT-XL/2 [29] MaskDiT [50] EDM2-S [20] EDM2-L [20] EDM2-XXL [20] DiT-XL/1[4] U-ViT-H/1[4] REPA-XL/2 [48] DDT-XL/2 [44] VQGAN[8] MAGVIT-v2 [47] MAR-L [24] VAR-d36-s [42] - SD-VAE [32] SD-VAE [32] SD-VAE [32] SD-VAE [32] SD-VAE [32] SD-VAE [32] SD-VAE [32] DC-AE [4] DC-AE [4] SD-VAE [32] SD-VAE [32] - - - - 512 512 Diffusion & flow-matching models - 4 2 2 2 - - - 1 1 2 2 - - - - - 1616 3232 3232 3232 - - - 1616 1616 3232 3232 2502 502 2502 2502 792 63 63 63 2502 302 2502 2502 7.72 4.05 3.04 2.62 2.50 2.56 2.06 1.91 2.41 2.53 2.08 1.28 GANs & masked models - - - - 256 642 2562 102 18.65 1.91 1.73 2.63 172.71 263.79 240.82 252.21 256.27 - - - 263.56 255.07 274.6 305.1 - 324.3 279.9 303.2 Ours: UCGM-S sampling with models trained by prior works EDM2-S [20] EDM2-L [20] EDM2-XXL [20] DDT-XL/2 [44] SD-VAE [32] SD-VAE [32] SD-VAE [32] SD-VAE [32] - - - 2 - - - 3232 4023 5013 4023 200300 2.530.03 2.040.02 1.880.03 1.250. - - - - Ours: models trained and sampled using UCGM-{T, S} (setting λ = 0) Ours-XL/1 Ours-XL/1 Ours-XL/4 Ours-XL/4 DC-AE [4] DC-AE [4] SD-VAE [32] SD-VAE [32] 1 1 4 4 1616 1616 1616 16 256 256 40 20 40 20 Diffusion & flow-matching models ADM-G [6] U-ViT-H/2 [1] DiT-XL/2 [31] SiT-XL/2 [29] MDT [10] REPA-XL/2 [48] REPA-XL/2 [48] Light.DiT [46] Light.DiT [46] DDT-XL/2 [44] DDT-XL/2 [44] REPA-E-XL [23] VQGAN[40] MAR-L [24] MAR-H [24] VAR-d30-re [42] - SD-VAE [32] SD-VAE [32] SD-VAE [32] SD-VAE [32] SD-VAE [32] SD-VAE [32] VA-VAE [46] VA-VAE [46] SD-VAE [32] SD-VAE [32] E2E-VAE[23] - 2 2 2 2 2 2 1 1 2 2 1 - 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 GANs & masked models - - - - - - - - - - - - 2502 502 2502 2502 2502 2502 2502 2502 2502 2502 2502 2502 - 2562 2562 102 1.48 1.68 1.67 1.80 4.59 2.29 2.27 2.06 1.79 1.96 1.42 2.11 1.35 1.31 1.26 1.26 2.18 1.78 1.55 1.73 Ours: UCGM-S sampling with models trained by prior works DDT-XL/2 [44] Light.DiT [46] REPA-E-XL [23] REPA-E-XL [23] SD-VAE [32] VA-VAE [46] E2E-VAE[23] E2E-VAE[23] 2 1 1 1 16 16 16 16 16 16 16 16 100400 100400 80420 20480 1.270.01 1.210.14 1.060.20 2.000. Ours: models trained and sampled using UCGM-{T, S} (setting λ = 0) Ours-XL/2 Ours-XL/1 Ours-XL/1 Ours-XL/1 SD-VAE [32] VA-VAE [46] E2E-VAE [23] E2E-VAE [23] 2 1 1 1 16 16 16 16 16 16 16 16 60 60 40 1.41 1.21 1.21 1.30 - - - - 186.70 263.88 278.24 277.50 283.01 264.0 305.7 - - 308.1 310.6 314.9 - 296.0 303.7 350.2 - - - - - - - - 559M 501M 675M 675M 736M 280M 778M 1.5B 675M 501M 675M 675M 227M 307M 479M 2.3B 280M 778M 1.5B 675M 675M 675M 675M 675M 559M 501M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 3.1B 479M 943M 2.0B 675M 675M 675M 675M 675M 675M 675M 675M 388 400 600 600 - 1678 1476 734 400 400 200 - - 1080 800 350 - - - - 800 800 320 396 400 1400 1400 1300 200 800 64 800 256 400 800 300 800 800 350 - - - - 400 400 800 800 19 A.3 Detailed Comparison with SOTA Methods for Few-step Generation Table 8: Sample quality comparison for few-step generation task on class-conditional ImageNet-1K (512 512). METHOD VAE/AE Patch Size Activation Size NFE () FID () IS #Params #Epochs sCT-M [28] sCT-M [28] sCT-L [28] sCT-L [28] sCT-XXL [28] sCT-XXL [28] sCD-M [28] sCD-M [28] sCD-L [28] sCD-L [28] sCD-XXL [28] sCD-XXL [28] BigGAN [2] StyleGAN [33] MAGVIT-v2 [47] VAR-d36-s [42] Ours-XL/1 Ours-XL/1 Ours-XL/1 Ours-XL/1 Ours-XL/1 Ours-XL/1 Ours-XL/4 Ours-XL/4 512 Consistency training & distillation - - - - - - - - - - - - - - - - - - - - - - - - - - - - 1 2 1 2 1 2 1 2 1 2 1 2 GANs & masked models - - - - 1 12 642 102 5.84 5.53 5.15 4.65 4.29 3.76 2.75 2.26 2.55 2.04 2.28 1.88 8.43 2.41 1.91 2.63 - - - - - - - - - - - - - 267.75 324.3 303. - - - - - - - - - - - - - - - - Ours: models trained and sampled using UCGM-{T, S} (setting λ = 0) DC-AE [4] DC-AE [4] DC-AE [4] DC-AE [4] 1 1 1 1 1616 1616 1616 16 32 16 8 4 1.55 1.81 3.07 74.0 - - - - Ours: models trained and sampled using UCGM-{T, S} (setting λ = 1) DC-AE [4] DC-AE [4] SD-VAE [32] SD-VAE [32] 1 1 4 1616 1616 1616 1616 256 256 1 2 1 2 Consistency training & distillation iCT [36] Shortcut-XL/2 [9] Shortcut-XL/2 [9] Shortcut-XL/2 [9] IMM-XL/2 [51] IMM-XL/2 [51] IMM-XL/2 [51] IMM-XL/2 [51] IMM (ω = 1.5) IMM (ω = 1.5) IMM (ω = 1.5) IMM (ω = 1.5) BigGAN [2] GigaGAN [17] StyleGAN [33] VAR-d30-re [42] - SD-VAE [32] SD-VAE [32] SD-VAE [32] SD-VAE [32] SD-VAE [32] SD-VAE [32] SD-VAE [32] SD-VAE [32] SD-VAE [32] SD-VAE [32] SD-VAE [32] - - - - - 2 2 2 2 2 2 2 2 2 2 2 - - - - - 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 2 1 4 128 12 22 42 82 12 22 42 8 GANs & masked models - - - - 1 1 12 102 Ours-XL/1 Ours-XL/1 Ours-XL/1 Ours-XL/1 Ours-XL/1 Ours-XL/1 Ours-XL/2 Ours-XL/1 Ours: models trained and sampled using UCGM-{T, S} (setting λ = 0) VA-VAE [46] VA-VAE [46] E2E-VAE [23] E2E-VAE [23] 1 1 1 1 1616 1616 1616 1616 16 8 16 8 2.11 6.09 1.40 2.68 - - - - Ours: models trained and sampled using UCGM-{T, S} (setting λ = 1) VA-VAE [46] VA-VAE [46] SD-VAE [32] E2E-VAE [23] 1 1 2 1 2 1 1 1 1.42 2.19 2.10 2.29 - - - - 1616 1616 1616 1616 20 2.42 1.75 2.63 2.11 20.3 10.6 7.80 3.80 7.77 5.33 3.66 2.77 8.05 3.99 2.51 1.99 6.95 3.45 2.30 1.73 - - - - - - - - - - - - - - - - - 225.52 265.12 350.2 498M 498M 778M 778M 1.5B 1.5B 498M 498M 778M 778M 1.5B 1.5B 160M 168M 307M 2.3B 675M 675M 675M 675M 675M 675M 675M 675M 675M 676M 676M 676M 675M 675M 675M 675M 675M 675M 675M 675M 112M 569M 166M 2.0B 675M 675M 675M 675M 675M 675M 675M 675M 1837 1837 1274 1274 762 762 1997 1997 1434 1434 921 921 - - 1080 800 800 800 800 840 840 360 360 - 250 250 250 3840 3840 3840 3840 3840 3840 3840 3840 - - - 350 400 400 800 800 432 432 424 Theoretical Analysis (Work in Progress) B.1 Main Results B.1.1 Unifed Training Objective The unifying loss L(θ) is formulated as: L(θ) = E(z,x)p(z,x),t (cid:20) 1 ˆω(t) x(F θ(xt, t), xt, t) x(F θ(xλt, λt), xλt, λt)2 2 (cid:21) We are interested in the gradient of the term inside the expectation with respect to θ. Let this inner term be J(θ): J(θ) = 1 ˆω(t) x(F θ(xt, t), xt, t) x(F θ (xλt, λt), xλt, λt)2 2 2 = 2 (θv(θ))T v(θ), which we can write using inner product Using the identity θv(θ)2 notation as 2θv(θ), v(θ). Here, θv(θ) denotes the Jacobian of with respect to θ, and the inner product notation J, is understood as when is Jacobian, yielding the gradient vector. Let A(θ) = x(F θ(xt, t), xt, t) and B(θ) = x(F θ (xλt, λt), xλt, λt). The gradient of the squared norm A(θ) B(θ)2 2 is: θA(θ) B(θ)2 2 = 2 (cid:10)θ (cid:2)A(θ) B(θ)(cid:3) , A(θ) B(θ)(cid:11) The term B(θ) depends on θ, which are fixed parameters from previous iteration. Thus, B(θ) is treated as constant with respect to the current parameters θ. This is equivalent to applying stop-gradient operator to B(θ). So, θB(θ) = 0. The gradient expression simplifies to: θA(θ) B(θ)2 2 = 2 (cid:10)θA(θ), A(θ) B(θ)(cid:11) (7) This is 2 θf x(F θ(xt, t), xt, t), x(F θ(xt, t), xt, t) x(F θ (xλt, λt), xλt, λt). We define parameters θ: as finite difference approximation involving only terms dependent on the fixed t := x(F θ (xt, t), xt, t) x(F θ(xλt, λt), xλt, λt) λt , λ (0, 1) and Note that in this definition, λt from the original prompt have been interpreted as xt and xλt respectively, to align with the arguments in the loss function. Critically, t does not depend on θ. To relate (7) to t , an approximation is made: x(F θ(xt, t), xt, t) x(F θ(xt, t), xt, t). This implies that the current models output A(θ) is close to the output using the target parameters θ at time t. Under this approximation, the term A(θ) B(θ) in (7) becomes: x(F θ(xt, t), xt, t)f x(F θ (xλt, λt), xλt, λt) x(F θ(xt, t), xt, t)f x(F θ (xλt, λt), xλt, λt) The right hand side is (t λt)f . So, (7) is approximated as: 2 θf x(F θ(xt, t), xt, t), (t λt)f = 2(t λt) θf x(F θ(xt, t), xt, t), t Since t is independent of θ, we can write this as: 2(t λt)θ x(F θ(xt, t), xt, t), t The factor 2(t λt) is scalar that can be absorbed into the learning rate. Thus, the gradient of J(θ) is considered proportional to 1 ˆω(t) θ x(F θ(xt, t), xt, t), t . Now, we substitute the specific form of x(F t, xt, t): x(F θ(xt, t), xt, t) = α(t) θ(xt, t) ˆα(t) xt α(t) ˆγ(t) ˆα(t) γ(t) 21 Let D(t) = α(t) ˆγ(t) ˆα(t) γ(t), which is scalar independent of θ. The gradient term becomes proportional to: 1 ˆω(t) θ (cid:28) α(t) θ(xt, t) ˆα(t) xt D(t) (cid:29) , t Given ˆω(t) = tan(t) 4 , so 1 ˆω(t) = 4 tan(t) . The expression is: 4 tan(t) θ (cid:28) α(t) θ(xt, t) ˆα(t) xt D(t) (cid:29) , t We can take constants out of the gradient operator and the inner product: 4 tan(t)D(t) θ α(t) θ(xt, t) ˆα(t) xt, t = 4 tan(t)D(t) θ (α(t) θ(xt, t), t ˆα(t) xt, x ) Since ˆα(t) xt and t is constant with respect to θ, so its gradient is zero. This is the justification for \"constants like xt can be dropped\" when they appear as additive terms inside the gradient of an inner product. The expression simplifies to: are independent of θ, the term ˆα(t) xt, 4 tan(t)D(t) θ α(t) θ(xt, t), t = 4α(t) tan(t)D(t) θ θ(xt, t), t Substituting tan(t) = sin(t)/ cos(t) and D(t) = α(t) ˆγ(t) ˆα(t) γ(t): 4α(t) cos(t) sin(t)(α(t) ˆγ(t) ˆα(t) γ(t)) θ θ(xt, t), t Since x is independent of θ, θF θ, (cid:28) θF θ(xt, t), = θF θ, 4α(t) cos(t)f sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) . The expression is: (cid:29) This can be rewritten by moving cos(t) and other θ-independent scalars outside the θ operator if its applied to the entire inner product, or inside the second argument of the inner product if θ applies only to θ. The target expression is: (cid:18) (cid:29)(cid:19) (cid:28) θ cos(t) θ(xt, t), 4α(t)f sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) This is justified because cos(t) and the term of θ. So, 4α(t)f sin(t)(α(t)ˆγ(t) ˆα(t)γ(t)) (lets call it Kt) are independent θ (cos(t)F θ(xt, t), Kt) = cos(t)θF θ(xt, t), Kt = cos(t)θF θ(xt, t), Kt = θF θ(xt, t), cos(t)Kt (cid:28) = θF θ(xt, t), cos(t) 4α(t)f sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) (cid:29) This matches the derived form. We can also view the training objective as: E(z,x)p(z,x),t (cid:20) cos(t)F θ(xt, t) θ (xt, t) + 4α(t)f sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) 2 2 (cid:21) , which would induce the same gradient, owing to the analysis below: The first expression is: (cid:28) θcos(t) θ(xt, t), 4α(t)f sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) (cid:29) Since cos(t) and the term 4α(t)f sin(t)(α(t)ˆγ(t) ˆα(t)γ(t)) do not depend on θ, we can write this as: (cid:28) θcos(t) θ(xt, t), 4α(t)f sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) (cid:29) (cid:28) = cos(t)θ θ(xt, t), = cos(t) (θF θ(xt, t))T (cid:29) 4α(t)f sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) (cid:18) 4α(t)f sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) (cid:19) () The second expression is the training objective: (cid:34) E(z,x)p(z,x),t cos(t) (cid:13) (cid:13) θ(xt, t) θ (xt, t) + (cid:13) (cid:13) 4α(t)f sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) (cid:35) (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 Let us consider the term inside the expectation: cos(t) (cid:13) (cid:13) θ(xt, t) θ (xt, t) + (cid:13) (cid:13) 4α(t)f sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) (cid:13) 2 (cid:13) (cid:13) (cid:13) Expanding the squared L2-norm, + b2 2 = a2 2 + b2 2 + 2a, b: cos(t) (cid:13) (cid:13) θ(xt, t) θ(xt, t) + (cid:13) (cid:13) 4α(t)f sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) (cid:34) = cos(t) θ(xt, t) θ (xt, t)2 2 (cid:13) 2 (cid:13) (cid:13) (cid:13) + (cid:13) (cid:13) (cid:13) (cid:13) 4α(t)f sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) (cid:28) (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 + 2 θ(xt, t) θ(xt, t), 4α(t)f sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) (cid:29) (cid:35) 2 (cid:13) (cid:13) (cid:13) (cid:13) + cos(t) = cos(t) θ(xt, t) θ (xt, t)2 (cid:13) 2 4α(t)f (cid:13) (cid:13) sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) (cid:13) 2 (cid:28) 4α(t)f sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) 4α(t)f sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) θ (xt, t), θ(xt, t), + 2 cos(t) 2 cos(t) (cid:28) (cid:29) (cid:29) Now, we compute the gradient of this entire expression with respect to θ: (cid:34) θ cos(t) θ(xt, t) θ (xt, t)2 + cos(t) (cid:13) (cid:13) (cid:13) (cid:13) + 2 cos(t) 2 cos(t) (cid:13) 2 4α(t)f (cid:13) (cid:13) sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) (cid:13) 2 (cid:28) 4α(t)f sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) 4α(t)f sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) θ (xt, t), θ(xt, t), (cid:28) (cid:29) (cid:29) (cid:35) Applying the gradient operator term by term: (a) The gradient of cos(t) θ(xt, t) θ (xt, t)2 2 is 2 cos(t) (θF θ(xt, t))T (F θ(xt, t) θ (xt, t)), since θ (xt, t) is constant w.r.t. θ. 23 is 0, as this term is constant w.r.t. θ. (b) The gradient of cos(t) (cid:13) (cid:13) (cid:13) (c) The gradient of 2 cos(t) (cid:68) θ(xt, t), 2 cos(t)θ = 2 cos(t) (θF θ(xt, t))T (cid:16) (cid:68) 4α(t)f sin(t)(α(t)ˆγ(t) ˆα(t)γ(t)) (cid:68) (cid:13) 2 (cid:13) (cid:13) 2 4α(t)f sin(t)(α(t)ˆγ(t) ˆα(t)γ(t)) (cid:69) θ(xt, t), 4α(t)f sin(t)(α(t)ˆγ(t) ˆα(t)γ(t)) (cid:69) is 4α(t)f sin(t)(α(t)ˆγ(t) ˆα(t)γ(t)) . 4α(t)f sin(t)(α(t)ˆγ(t) ˆα(t)γ(t)) (cid:17) (d) The gradient of 2 cos(t) θ (xt, t), (cid:69) is 0, as this term is constant w.r.t. θ. Summing these results, the gradient of the term inside the expectation is: 2 cos(t) (θF θ(xt, t))T (F θ(xt, t) θ (xt, t)) 4α(t)f sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) + 2 cos(t) (θF θ(xt, t))T (cid:18) (cid:19) Comparing the second term of this sum with the result () for the first expression: (cid:19) (cid:18) 2 cos(t) (θF θ(xt, t))T 4α(t)f sin(t) (α(t) ˆγ(t) ˆα(t) γ(t)) is exactly 2 (first expression evaluated at ()). Thus, the gradient of the integrand of the training objective contains the first expression multiplied by factor of 2. B.1.2 Learning Objective as λ 1 Since x(F 0, x0, 0) = = = α(0) θ(x0, 0) ˆα(0) x0 α(0) ˆγ(0) ˆα(0) γ(0) 0 θ(x0, 0) ˆα(0) x0 0 ˆγ(0) ˆα(0) 1 0 ˆα(0) x0 0 ˆα(0) = x0 satisfies the boundary condition of consistency model (CM). Also note that our unified training objective is formally similar to that of consistency model, except that we use λt instead of t. When λ 1(i.e. 0), we get so-called continous consistency model[37, 28], and the term . In this case, we could use some numerical methods to estimate this derivative, for instance through the symmetric difference quotient: tends to df x(F θ (xt,t),xt,t) in target dt x(F θ (xt+ϵ, + ϵ), t+ϵ, + ϵ) x(F θ (xtϵ, ϵ), tϵ, ϵ) 2ϵ . Remark. To better understand the unified loss, lets analyze case when coefficients ˆα, ˆγ satisfy the condition of flow model. For simplicity we use the notation θ(xt, t) := x(F θ(xt, t), xt, t), the training objective is then equal to L(θ) = Et,(z,x) (cid:20) 1 ˆω(t) θ(xt, t) θ(xλt, λt)2 2 (cid:21) . where (z, x) p(z, x), xt(z, x) = α(t)z + γ(t)x. Let ϕt(x) be the solution of the flow ODE determined by the velocity field v(xt, t) = E(z,x)xt [α(t)z + γ(t)xxt] and an initial value x. For given xt, let = ϕ1 (xt) and gθ(x, t) = θ(ϕt(x), t). When λ 1, the gradient of the loss tends to lim λ1 θ L(θ) 2(1 λ) = Et = Et (cid:20) ˆω(t) (cid:20) ˆω(t) E(z,x) lim λ1 df θ(xt, t) dt E(z,x) (cid:21) , θgθ(x, t) θ(xt, t) θ(xλt, λt) λt (cid:21) , θf θ(xt, t) 24 The inner expectation can be computed as: E(z,x),xt df θ(xt, t) dt , θgθ(x, t) = E(z,x),xt 1f θ(xt, t) dxt dt + 2f θ(xt, t), θgθ(x, t) = E(z,x),xt 1f θ(xt, t) (α(t)z + γ(t)x) + 2f θ(xt, t), θgθ(x, t) (cid:2)E(z,x)xt 1f θ(xt, t) (α(t)z + γ(t)x) + 2f θ(xt, t), θgθ(x, t) (cid:3) = Ext = Ext 1f θ(xt, t) E(z,x)xt [α(t)z + γ(t)xxt] + 2f θ(xt, t), θgθ(x, t) = Ext 1f θ(xt, t) v(xt, t) + 2f θ(xt, t), θgθ(x, t) = Ext = θEx , θgθ(x, t) dgθ(x, t) dt gθ(x, t) gθ (x, t) + 1 2 dgθ(x, t) dt 2 2 Thus from the perspective of gradient, when λ 1 the training objective is equivalent to Ex,t (cid:20) ˆω(t) gθ(x, t) gθ (x, t) + (cid:21) dgθ(x, t) dt 2 2 which naturally leads to the solution gθ(x, t) = (since gθ(x, 0) x), or equivalently θ (xt, t) = ϕ1 (xt), that is the definition of consistency function. B.1.3 Learning Objective as λ 0 When λ = 0, the training objective is just E(z,x)p(z,x),t (cid:20) 1 ˆω(t) x(F θ(xt, t), xt, t) x2 2 (cid:21) which is the standard x-prediction loss for regular diffusion model. Lets further expand the inner expression: x(F t, xt, t) = α(t) θ(xt, t) ˆα(t) xt α(t) ˆγ(t) ˆα(t) γ(t) = = = α(t) θ(xt, t) ˆα(t)(α(t) + γ(t) x) (α(t) ˆγ(t) ˆα(t) γ(t)) α(t) ˆγ(t) ˆα(t) γ(t) α(t) θ(xt, t) ˆα(t)α(t) α(t)ˆγ(t) α(t) ˆγ(t) ˆα(t) γ(t) α(t) α(t) ˆγ(t) ˆα(t) γ(t) [F θ(xt, t) (ˆα(t) + ˆγ(t) x)] thats why in training algorithm when λ = 0 we can directly set target := ˆα(t) + ˆγ(t) . In this view it could be clearly seen that we are actually training flow matching model when ˆα(t) = dα(t) dt and ˆγ(t) = dγ(t) dt are satisfied. Remark. When λ = 0, we aim to derive the Probability Flow Ordinary Differential Equation (PF-ODE) corresponding to defined forward process. The forward process for state variable xt at time [0, 1] is given by: xt = α(t)z + γ(t)x0, where (0, I) is standard Gaussian random variable, and x0 pdata(x0) is drawn from the data distribution. The scalar functions α(t) and γ(t) satisfy the boundary conditions: α(0) = γ(1) = 0 and α(1) = γ(0) = 1. This process can be associated with Stochastic Differential Equation (SDE) of the form: dxt = (xt, t)dt + g(t)dwt, where wt is standard Wiener process. We need to determine the vector-valued drift term (xt, t) and the scalar diffusion term g(t). 25 First, consider the conditional mean of the process E[xtx0] = γ(t)x0. Its time derivative is: dt E[xtx0] = γ(t)x0, where γ(t) = dγ(t) dt some matrix-valued function H(t), the evolution of the SDEs conditional mean is dt H(t)E[xtx0] = H(t)γ(t)x0. Comparing this with γ(t)x0, we find H(t) = γ(t) where is the identity matrix. Thus, the drift term is: . Assuming the drift of the SDE is linear in xt, i.e., (xt, t) = H(t)xt for E[xtx0] = γ(t) (for γ(t) = 0), (xt, t) = γ(t) γ(t) xt. Next, consider the conditional variance, Var(xtx0) = Σ(t) = α(t)2I. The evolution of this variance for the linear SDE is governed by the Lyapunov equation: dΣ(t) dt = H(t)Σ(t) + Σ(t)H(t)T + g(t)2I. Substituting Σ(t) = α(t)2I and H(t) = γ(t) γ(t) I: dt (α(t)2I) = (cid:19) (cid:18) γ(t) γ(t) (α(t)2I) + (α(t)2I) (cid:18) γ(t) γ(t) (cid:19)T + g(t)2I. Since dt (α(t)2) = 2α(t)α(t) (where α(t) = dα(t) dt γ(t) γ(t) 2α(t)α(t)I = 2 ), the equation becomes: α(t)2I + g(t)2I. Solving for g(t)2, we get: g(t)2 = 2α(t)α(t) 2 γ(t) γ(t) α(t)2. The Probability Flow ODE is given by the general form: 1 2 where pt(xt) is the marginal probability density of xt at time t, and xt denotes the gradient with respect to xt. Substituting the derived expressions for (xt, t) and g(t)2: g(t)2xt log pt(xt), = (xt, t) dxt dt dxt dt = = (cid:18) γ(t) γ(t) (cid:19) xt 1 2 (cid:18) 2α(t)α(t) 2 (cid:20) α(t)α(t) xt γ(t) γ(t) γ(t) γ(t) γ(t) γ(t) (cid:21) α2(t) (cid:19) α(t)2 xt log pt(xt) xt log pt(xt). Thus, the final PF-ODE is: dxt dt = γ(t) γ(t) (cid:20) α(t)α(t) xt (cid:21) α2(t) γ(t) γ(t) xt log pt(xt) . We show in below analysis that the above equation can be also be written as dxt dt = α(t) (F t, xt, t) + γ(t) (F t, xt, t) . where , represent the analytical soulution for and z: (F t, xt, t) = [xxt] , x (F t, xt, t) = [zxt] . Specifically, Tweedies formula states: (F t, xt, t) = xt + α2(t)xt log pt(xt) γ(t) ."
        },
        {
            "title": "Since the identity holds",
            "content": "for we have: α(t) (F t, xt, t) + γ(t) (F t, xt, t) = xt , (F t, xt, t) = α(t)xt log pt(xt) ."
        },
        {
            "title": "Now we can calculate the expression",
            "content": "α(t)f (F t, xt, t) + γ(t)f (F t, xt, t) = α(t)α(t)xt log pt(xt) + γ(t) xt + α2(t)xt log pt(xt) γ(t) = γ(t) γ(t) (cid:20) α(t)α(t) xt (cid:21) α2(t) γ(t) γ(t) xt log pt(xt) , which exactly matches the equation of PF-ODE as above. Actually, this expression is also the velocity field of the flow ODE: v(xt, t) = (cid:20) dxt dt (cid:21) xt = [α(t)z + γ(t)xxt] = α(t) [zxt] + γ(t) [xxt] (F t, xt, t) + γ(t) = α(t) (F t, xt, t) . B.1.4 Enhanced Target Score Recall that CFG proposes to modify the sampling distribution as pθ(xtc) pθ(xtc)pθ(cxt)ζ , Bayesian rule gives so we can futher deduce pθ(cxt) = pθ(xtc)pθ(c) pθ(xt) , pθ(xtc) pθ(xtc)pθ(cxt)ζ = pθ(xtc)( pθ(xtc)( pθ(xtc)pθ(c) pθ(xt) )ζ pθ(xtc) pθ(xt) )ζ . When [0, s] (s = 0.75), inspired by above expression and recent work [41], we choose to use below as the target score function for training (cid:32) xt log pt(xtc) (cid:19)ζ(cid:33) (cid:18) pt,θ(xtc) pt,θ(xt) which equals to xt log pt(xtc) + ζ (xt log pt,θ(xtc) xt log pt,θ(xt)) . For we originally want to learn: (F t, xt, t) = α(t)xt log pt(xt) , now it turns to (F t, xt, t) = α(t)xt log pt(xtc) (cid:32) (cid:19)ζ(cid:33) (cid:18) pt,θ(xtc) pt,θ(xt) = α(t) [xt log pt(xtc) + ζ (xt log pt,θ(xtc) xt log pt,θ(xt))] = α(t)xt log pt(xtc) + ζ (α(t)xt log pt,θ(xtc) + α(t)xt log pt,θ(xt)) = α(t)xt log pt(xtc) + ζ (cid:0)f z(F t, xt, t) z(F , xt, t)(cid:1) , 27 thus in training we set the objective for as: + ζ (cid:0)f z(F t, xt, t) z(F , xt, t)(cid:1) . Similarly, since strategy to modify the training objective for x: = xt+α2(t)xt log pt(xt) γ(t) is also linear in the score function, we can use the same + ζ (cid:0)f x(F t, xt, t) x(F , xt, t)(cid:1) . When (s, 1] (s = 0.75), we further slightly modify the target score function to xt log pt(xtc) + ζ (xt log pt,θ(xtc) xt log pt(xt)) , ζ = 0. which corresponds to the following training objective: + 1 2 (f x(F t, xt, t) x) , + 1 2 (f z(F t, xt, t) z) . B.1.5 Analysis on the Optimal Solution Below we provide some examples to illustrate the property of the optimal solution for the unified loss by considering some simple cases of data distribution. (for simplicity define θ(xt, t) = x(F θ(xt, t), xt, t)) Assume (µ, I) and consider series of together: = tT > tT 1 > . . . > t1 > t0 0. This series could be obtained by tj1 = λ tj, = T, . . . , 0, for instance. With an abuse of notation, denote xtj as xj and α(tj) as αj, γ(tj) as γj. Since t0 0, x0 x, we could conclude the trained model θ (x1, t1) = Exx1 [xx1] , and concequently θ (xj+1, tj+1) = Exj xj+1 [f θ (xj, tj)xj+1] , = 1, . . . , 1 . Using the property of the conditional expectation, we have Exj [f θ (xj, tj)] = Ex [x] , j. Now assume α2(t) + γ2(t) = 1, t. Using the expressions above we have and θ (x1, t1) = (1 γ2 1 )µ + γ1x θ (xj, tj) = µ + (γ1 (cid:89) k=2 βk) (xj γjµ), = 2, . . . , k=2 βk and assume α = where βk = αk1αk + γk1γk and γ1 1. Further denote cj = γ1 sin(t), γ(t) = cos(t). For appropriate choice of the partition scheme (e.g. even or geometric), the coefficient cj can converge as grows. For instance, when evenly partitioning the interval [0, t], we have: (cid:81)j lim c(t) = lim γ1 (cid:89) k=2 βk = lim (cos( ))T = 1 . Thus the trained model can be viewed as an interpolant between the consistency model(λ 1 or ) and the flow model(λ 0 or 1): θ (xt, t) = (1 γ(t)c(t))µ + c(t)xt , CM θ (xt, t) = (1 γ(t))µ + xt , θ (xt, t) = (1 γ2(t))µ + γ(t)xt . can be obtained by first compute the velocity field v(xt, t) = The expression of CM θ [α(t)z + γ(t)xxt] = γ(t)µ then solve the initial value problem of ODE to get x(0). The above optimal solution can be possibly obtained by training. For example if we set the parameterizition as θ(xt, t) = (1 γtct)θ + ctxt, the gradient of the loss can be computed as (let = λ t): 28 θf θ(xt, t) θ(xr, r) 2 = 2(1 γtct) [(αtγt αrγr)z + (γrcr γtct)(θ x)] , θEz,x θ(xt, t) θ (xr, r)2 2 = 2(1 γtct)(γrcr γtct)(θ µ) , θL(θ) = Et 2(1 γtct)(γrcr γtct) ˆω(t) (θ µ) = C(θ µ), = Et 2(1 γtct)(γrcr γtct) ˆω(t) . Use gradient descent to update θ during training: dθ(s) ds = θL(θ) = C(θ µ) . The generalization loss thus evolves as: dθ(s) µ2 ds = θ(s) µ, dθ(s) ds = θ(s) µ, C(θ(s) µ) = Cθ(s) µ2 , = θ(s) µ2 = θ(0) µ2eCs . B.1.6 Unified Sampling Process Deterministic sampling. When the stochastic ratio ρ = 0, lets analyze apecial case where the coefficients satisfying ˆα(t) = dα(t) . Let = ti+1 ti, for the core updating rule we dt have: = α(ti+1) ˆz + γ(ti+1) ˆx , ˆγ(t) = dγ(t) dt (ti)t + o(t)) ˆz + (γ(ti) + γ = (α(ti) + α = (α(ti)ˆz + γ(ti)ˆx) + (ˆα(ti)ˆz + ˆγ(ti)ˆx) + o(t) = (α(ti)f z(F , x, ti) + γ(ti)f x(F , x, ti)) + (ˆα(ti)f z(F , x, ti) + ˆγ(ti)f x(F , x, ti)) + o(t) (ti)t + o(t)) ˆx = (α(ti) + (ˆα(ti) ˆγ(ti) γ(ti) (x, ti) α(ti) ˆγ(ti) ˆα(ti) γ(ti) ˆγ(ti) γ(ti) (x, ti) α(ti) ˆγ(ti) ˆα(ti) γ(ti) + γ(ti) + ˆγ(ti) α(ti) (x, ti) ˆα(ti) xt α(ti) ˆγ(ti) ˆα(ti) γ(ti) α(ti) (x, ti) ˆα(ti) xt α(ti) ˆγ(ti) ˆα(ti) γ(ti) ) ) + o(t) = + (x, ti) + o(t) In this case (, ) tries to predict the velocity field of the flow model, and we can see that the term + (x, ti) corresponds to the sampling rule of the Euler ODE solver. Stochastic sampling. As for case when the stochastic ratio ρ = 0, follow the Euler-Maruyama numerical methods of SDE, the noise injected should be Gaussian with zero mean and variance proportional to t, so when the updating rule is = α(ti+1) ( ρ z) + γ(ti+1) ˆx, the coefficient of should satisfy 1 ρ ˆz + α(ti+1) ρ t, ρ α2(ti+1) In practice, we set ρ = 2t α(ti) α2(ti+1) . which corresponds to g(t) = (cid:112)2α(t) for the SDE dx = (x, t)dt + g(t)dw. 29 Extrapolating. Below we analyze why extrapolating the estimation ˆxi ˆxi + κ(ˆxi ˆxi1), ˆzi ˆzi + κ(ˆzi ˆzi1) help, and what does it actually do mathematically. Denote at each time step as xi, and correspondingly ˆxi = x(F , xi, ti), ˆzi = z(F , xi, ti) . From the analysis of the deterministic sampling, we can see that the updating rule is actually xi+1 = xi + v(xi, ti) + h2 ϵi , = ti+1 ti for κ = 0, where v(, ) is the vector field of the ODE, and ϵi is an error term. Further denote v(xi, ti) as vi, since ˆxi and ˆzi affect the updating linearly, using extrapolating means xi+1 = xi + (vi + κ(vi vi1)) + h2 ϵi . When κ = 0.5, we recover the Adams-Bashforth method which is second-order xi + ( 3 2 vi 1 2 vi1) By Taylors theorem, expand vi vi1 = vi v(xi, ti) + O(h2) , thus we have xi+1 = xi + (vi + κ(vi vi1)) + h2 ϵi = xi + (vi + κ(vi vi + v(xi, ti) + O(h2))) + h2 ϵi = xi + vi + κh2 v(xi, ti) + h2 ϵi + O(h3) . Whereas the true solution x(ti+1) is x(ti+1) = xi + vi + 1 2 h2 v(xi, ti) + O(h3) , so the local truncation error(LTE) can be computed as x(ti+1) xi+1 = h2 (cid:20) ( 1 κ)v(xi, ti) ϵi (cid:21) + O(h3) . Since ϵi is not known, emprically we set κ within the range [0.2, 0.6], which can possibly help to reduce the LTE and improve the sampling quality. B.2 Other Techniques B.2.1 Beta Transformation We utilize three representative cases to illustrate how the Beta transformation fBeta(t; θ1, θ2) generalizes time warping mechanisms for [0, 1]. 1 Standard logit-normal time transformation [46, 7]. For U(0, 1), the logit-normal transformation flognorm(t; 0, 1) = 1+exp(Φ1(t)) generates symmetric density profile peaked at = 0.5, consistent with the central maximum of the logistic-normal distribution. Analogously, the Beta transformation fBeta(t; θ1, θ2) (with θ1, θ2 > 1) produces density peak at = θ11 θ1+θ22 . When θ1 = θ2 > 1, this reduces to = 0.5, mirroring the logit-normal case. Both transformations concentrate sampling density around critical time regions, enabling importance sampling for accelerated training. Notably, this effect can be equivalently achieved by directly sampling Beta(θ1, θ2). Uniform time distribution [46, 48, 29, 25]. The uniform limit case emerges when θ1 = θ2 = 1, reducing fBeta(t; 1, 1) to an identity transformation. This corresponds to flat density p(t) = 1, reflecting no temporal preferencea baseline configuration widely adopted in diffusion and flowbased models. 30 Approximately symmetrical time distribution [37, 36, 18, 20]. For near-symmetric configurations where θ1 θ2 > 1, the Beta transformation induces quasi-symmetrical densities with tunable central sharpness. For instance, setting θ1 = θ2 = 2 yields parabolic density peaking at = 0.5, while θ1 = θ2 1+ asymptotically approaches uniformity. This flexibility allows practitioners to interpolate between uniform sampling and strongly peaked distributions, adapting to varying requirements for temporal resolution in training. Such approximate symmetry is particularly useful in consistency models where balanced gradient propagation across time steps is critical. B.2.2 Kumaraswamy Transformation Setting and notation. Fix positive real number > 0 and consider the shift function fshift(t; s) = 1 + (s 1)t , [0, 1]. For a, b, > 0, define the Kumaraswamy transform as fKuma(t; a, b, c) = 1 (cid:18) (cid:16) 1 ta(cid:17)b(cid:19)c , [0, 1]. Notice that when = = = 1 one obtains fKuma(t; 1, 1, 1) = 1 (cid:0)1 t1(cid:1)1 = t, so that the identity function appears as special case. We work in the Hilbert space L2([0, 1]) with the inner product f, = (cid:90) 1 0 (t)g(t) dt. Accordingly, we introduce the error functional J(a, b, c) := (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13)fKuma(; a, b, c) fshift(; s) (cid:13) 2 and Jid := (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13)id fshift(; s) (cid:13) 2 . It is known that for = 1 one has inf a,b,c J(a, b, c) < Jid. The goal is to quantify this improvement by optimally adjusting all three parameters (a, b, c). Quadratic approximation around the identity. Since the interesting behavior occurs near the identity (a, b, c) = (1, 1, 1), we reparameterize as θ := := α β γ , 1 1 1 with θl1. Thus, we study the function fKuma(t; 1 + α, 1 + β, 1 + γ) in small neighborhood of (1, 1, 1). Writing (a, b, c; t) := fKuma(t; a, b, c) = (cid:16) 1 (1 ta)b(cid:17)c , secondorder Taylor expansion around (a, b, c) = (1, 1, 1) gives fKuma(t; 1 + α, 1 + β, 1 + γ) = + 3 (cid:88) i=1 θi gi(t) + 1 2 3 (cid:88) i,j= θiθj hij(t) + O(θ3), (8) where gi(t) = θi fKuma(t; 1 + θ) (cid:12) (cid:12) (cid:12)θ=0 and hij(t) = 2 θiθj (cid:12) (cid:12) fKuma(t; 1 + θ) (cid:12)θ=0 . short calculation yields: 31 (a) With respect to (noting that for = = 1 one has fKuma(t; a, 1, 1) = ta): g1(t) = fKuma (t; 1, 1, 1) = da ta(cid:12) (cid:12) (cid:12)a=1 = ln t. (b) With respect to (since for = 1, = 1 we have fKuma(t; 1, b, 1) = 1 (1 t)b): fKuma (c) With respect to (noting that for = = 1 we have fKuma(t; 1, 1, c) = tc): (t; 1, 1, 1) = (1 t) ln(1 t). g2(t) = Thus, we observe that g3(t) = fKuma (t; 1, 1, 1) = ln t. g1(t) = g3(t), which indicates an inherent redundancy in the three-parameter model. In consequence, the Gram matrix (defined below) will be of rank at most two. Next, define the difference between the identity and the shift functions: g(t) := id(t) fshift(t; s) = 1 + (s 1)t = (1 s) t(1 t) 1 + (s 1)t . Then, Jid = g, g. Also, introduce the first-order moments and the Gram matrix: vi := g, gi, Gij := gi, gj, i, = 1, 2, 3. Inserting the expansion (8) into the error functional gives J(1 + θ) = (cid:13) (cid:13)fKuma(; 1 + θ) fshift(; s)(cid:13) 2 2 = Jid 2 (cid:13) 3 (cid:88) i=1 θi vi + 3 (cid:88) i,j= θiθj Gij + O(θ3). Thus, the quadratic approximation (or model) of the error is (cid:98)J(θ) := Jid 2 θv + θG θ. Since the Gram matrix is positive semidefinite (and has nontrivial null-space due to g1 = g3), the minimizer is determined only up to the null-space. To select the unique (minimumnorm) minimizer, we choose θ = Gv, where denotes the Moore-Penrose pseudoinverse. The quadratic model is then minimized at (cid:98)Jmin = Jid vGv. scaling argument now shows that for any sufficiently small ε > 0 one has J(1 + ε θ) (cid:98)J(ε θ) = Jid ε2 vGv < Jid, so that the full nonlinear functional is improved by following the direction of θ. For convenience we introduce the explicit improvement factor ρ3(s) := vGv Jid(s) (0, 1), = 1, (9) so that our main bound can be written succinctly as min a,b,c> J(a, b, c) (cid:16) 1 ρ3(s) (cid:17) Jid(s). (s > 0, = 1) (10) 32 Computation of the Gram matrix G. We now compute the inner products Gij = gi, gj, i, = 1, 2, 3. Since the functions g1 and g3 are identical, only two independent functions appear in the system. standard fact from Beta-function calculus is that Thus, one has (cid:90) 1 0 tn ln2 dt = 2 (n + 1)3 , > 1. g1, g1 = g2, g2 = (cid:90) 1 0 (cid:90) 1 t2 ln2 dt = 2 33 = 2 27 , (1 t)2 ln2(1 t) dt = 2 , since the change of variable = 1 yields the same result. g1, g2 = (cid:90) 1 0 t(1 t) ln ln(1 t) dt = 25 216 . It is now convenient to express the Gram matrix with an overall factor: (Notice that = 2 1 25 16 1 25 16 1 25 . 1 25 16 1 25 16 2 27 = 25 216 , which is consistent with the computed value of g1, g2.) Since g1 = g3, it is clear that the columns (and rows) corresponding to parameters and are identical, so that rank(G) = 2. One can compute the Moore-Penrose pseudoinverse by eliminating one of the redundant rows/columns, inverting the resulting 2 2 block, and then re-embedding into R33. One obtains 1 0 1 = 27 0 1 25 9 0 . 1 Computation of the first-order moments vi. Recall that g(t) = id(t) fshift(t; s) = 1 + (s 1)t . This expression can be rewritten as g(t) = (1 s) t(1 t)Ds(t), with Ds(t) := 1 1 + (s 1)t . Then, the firstorder moments read v1 = v3 = (1 s) (cid:90) 1 0 t(1 t)Ds(t) ln dt, v2 = (1 s) (cid:90) 1 t(1 t)Ds(t) (1 t) ln(1 t) dt. These integrals can be expressed in closed form (involving logarithms and powers of (s 1)); in the case = 1 at least one of the vi is nonzero so that ρ3(s) > 0. 33 universal numerical improvement. Since projecting onto the three-dimensional subspace spanned by {g1, g2, g3} is at least as effective as projecting onto any one axis, we immediately deduce that ρ3(s) ρ1(s), where the one-parameter improvement factor is defined by ρ1(s) := v1(s)2 g1, g1 Jid(s) . By an elementary (albeit slightly tedious) estimate for example, using the bounds 1 valid for 1 1 one can show that 2 Ds(t) 2 ρ1(s) 49 . Hence, one deduces that ρ3(s) 49 1536 0.0319, for 1 1. In particular, for [0.5, 2] {1} the optimal three-parameter Kumaraswamy transform reduces the squared L2 error by at least 3.19% compared with the identity mapping. Analogous bounds can be obtained on any compact subset of (0, ) {1}. Interpretation of the bound. the three-parameter model can outperform the identity mapping) in two important respects: Inequality (10) strengthens the known qualitative result (namely, that (a) Quantitative improvement: The explicit factor ρ3(s) is computable via one-dimensional integrals, providing concrete measure of the error reduction. (b) Utilization of all three parameters: Even though the redundancy (i.e. g1 = g3) implies that the Gram matrix is singular, the full three-parameter model still offers strict improvement; indeed, one has ρ3(s) ρ1(s) > 0 for = 1. (Equality would require, hypothetically, that v2(s) = 0, which does not occur in practice.) Summary. For every shift parameter > 0 with = 1 there exist parameters (a, b, c) (in neighborhood of (1, 1, 1)) such that (cid:13) (cid:13) (cid:13)fKuma(; a, b, c) fshift(; s) (cid:16) (cid:13) 2 (cid:13) (cid:13) 2 1 ρ3(s) (cid:17) (cid:13) (cid:13) (cid:13)id fshift(; s) (cid:13) 2 (cid:13) (cid:13) 2 , with the improvement factor ρ3(s) defined in (9) and satisfying ρ3(s) 0.0319 on [0.5, 2] {1}. Thus, the full three-parameter Kumaraswamy transform not only beats the identity mapping but does so by quantifiable margin."
        }
    ],
    "affiliations": [
        "Westlake University",
        "Zhejiang University"
    ]
}