{
    "paper_title": "Rethinking Chain-of-Thought Reasoning for Videos",
    "authors": [
        "Yiwu Zhong",
        "Zi-Yuan Hu",
        "Yin Li",
        "Liwei Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video."
        },
        {
            "title": "Start",
            "content": "Rethinking Chain-of-Thought Reasoning for Videos Yiwu Zhong1, Zi-Yuan Hu1, Yin Li2, Liwei Wang1* 1The Chinese University of Hong Kong, 2University of Wisconsin-Madison 5 2 0 2 0 1 ] . [ 1 6 1 6 9 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances video MLLMs reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaViLab/Rethink_CoT_Video. 1. Introduction Chain-of-Thought (CoT) [61, 80] aims to solve complex tasks by generating explicit, step-by-step intermediate reasoning traces before producing final answer. CoT has been key driver of the strong reasoning capabilities in latest large language models (LLMs) [15, 19]. Building on this success, several recent efforts [12, 18, 59, 68, 71] have extended CoT to multimodal large language models (MLLMs), demonstrating improved reasoning over visual inputs, including both images and videos. Despite its success in vision, CoT reasoning in MLLMs incurs major overhead in both inference and training. On the inference side, visual inputs, especially videos, often *Corresponding author. Figure 1. CoT Reasoning, with dense prefilling and lengthy decoding, incurs substatial computation load at both training and inference. In contrast, Concise Reasoning coupled with token compression is significantly more efficient, thanks to sparse prefilling and concise decoding. expand into thousands of visual tokens with high redundancy, while CoT produces long reasoning sequences. Together, these factors compound to sharply increased memory usage, compute cost, and carbon footprint during deployment. Additionally, CoT training relies on supervised fine-tuning (SFT) with heavily-labeled reasoning traces, followed by reinforcement learning (RL) over long sequences [4, 12, 18, 49, 59, 68, 71, 76]. This pipeline not only requires costly annotation, but also leads to prolonged training cycles. In this paper, we aim to reduce the inference and training overhead of reasoning-oriented video MLLMs. With Transformer-based architecture  (Fig. 1)  , this overhead can be decomposed into: (1) the cost of decoding phrase, which scales with the number of output tokens for reasoning and the total number of input tokens, and (2) the cost of prefilling phrase, which is determined by the number of input tokens. Therefore, reducing CoT overhead requires either shortening the output chain, which promotes more concise reasoning, or decreasing the number of input tokens, which is dominated by redundant visual tokens and can be reduced 1 via token compression [3, 42, 52, 79, 86]. With this intuition, we conduct systematic benchmark of MLLMs with and without CoT across suite of video datasets, covering general, long-form, and complex video understanding tasks. Our benchmark leads to several surprising observations. First, despite the overhead, adding CoT yields only modest gains over the base pre-trained MLLM using direct answering. Indeed, we observe that CoT outputs frequently contain human-like pondering patterns (e.g., Hmm, Lets think, or Wait) that contribute little to reasoning. Second, prompting the base pre-trained MLLM to generate concise reasoning chains leads to major performance drop, significantly worse than direct answering. We conjecture that the model possesses the necessary knowledge to answer the questions, yet is not well aligned with the concise reasoning paradigm. Third, we find that token compression, although previously shown to be effective for video MLLMs [86], causes notably larger performance degradation when the model is prompted to produce concise or CoTstyle reasoning, compared with direct answering. Collectively, these observations motivate rethinking of how CoT inference and training should be designed for video reasoning. While existing MLLMs with CoT typically rely on lengthy reasoning chains and large number of input visual tokens, we hypothesize that concise reasoning combined with reduced set of visual tokens can be sufficient for effective video reasoning. To verify this, we design and validate an efficient post-training and inference framework that enhances video MLLMs reasoning capability. Specifically, we directly fine-tune pre-trained video MLLM via RL using Group Relative Policy Optimization (GRPO) [44]. We further integrate visual token compression with GRPO, allowing the model to learn to reason effectively from reduced set of tokens. At inference time, the model operates on compressed visual tokens and produces brief reasoning traces before its final answer. In doing so, our framework substantially reduces inference overhead, demonstrates improved accuracy across wide range of benchmarks, and additionally removes the need for costly CoT annotations and SFT training. Contributions. Our benchmark, framework, and results challenge the prevailing assumption that long, human-like Instead, CoT reasoning is essential for video reasoning. we demonstrate an alternative approach that operates on compressed visual tokens and produces concise reasoning traces. The resulting models achieve improved inference efficiency, deliver competitive performance, and can be learned solely through RL-based post-training. Scope and limitations. The objective of this work is not to introduce new method, but rather to illuminate parallel solution that deviates from widespread view in the field. We benchmark existing methods, draw empirical observations, formulate hypothesis, and design framework to test that hypothesis. In doing so, we do not claim algorithmic superiority over prior methods, in spite of better results. Our work is grounded in empirical evidence rather than theoretical guarantees, and our conclusions are constrained by the scope of current video benchmarks. Nonetheless, we believe the insights revealed in our study may generalize beyond these datasets and potentially extend to other forms of visual reasoning. 2. Related Work Multi-Modal LLMs. Significant progress has been made in developing multi-modal LLMs for image understanding [6, 29, 31, 89], video understanding [28, 35, 73, 77, 88], and long-video reasoning [17, 46, 47, 50, 64, 69]. More recently, unified multimodal models [1, 25, 34, 53, 58, 90] have emerged, exhibiting strong generalization across both images and videos. Building on these advances, we explore whether the concise reasoning capability learned through large-scale pre-training can be effectively elicited via reinforcement learning for general video understanding, without relying on additional CoT annotations. Multi-modal Chain-of-Thought Reasoning. Inspired by CoT prompting in text-based LLMs [23, 62], early efforts in multi-modal reasoning focus on generating intermediate reasoning steps for visual understanding [11, 16, 37, 39, 43, 81, 84, 85]. Recent breakthroughs such as OpenAIo1 [19] and DeepSeek-R1 [15] demonstrate that reinforcement learning algorithms (e.g., RLHF [38], DPO [40], and GRPO [44]) can significantly enhance reasoning capability. Among them, GRPO, featuring rule-based rewards and group-based sampling, has become scalable framework for post-training and has been widely extended to vision tasks [33, 45] and vision-language tasks, including image understanding [9, 18, 48, 49, 57, 66, 68, 71, 76] and video reasoning [4, 12, 27, 54, 59, 70]. common practice in these methods is two-stage posttraining pipeline [4, 12, 18, 20, 48, 49, 54, 59, 68, 71, 76]: first generating CoT annotations for supervised fine-tuning (SFT), followed by GRPO-based reinforcement learning (RL). Specifically, Video-R1 [12] designs temporal-aware reward for GRPO fine-tuning and Video-RFT [59] proposes semantic consistency reward to align CoT generation and video descriptions. LongVILA-R1 [4] builds sequence parallelism for long videos and Ego-R1[54] trains tool-using agent for ego-centric long videos. VideoChat-R1.5 [70] learns to iteratively refine the focused regions by the collected spatio-temporal supervision. In this work, we challenge the prevailing paradigm of CoT reasoning and its twostage SFT-GRPO fine-tuning. Our experiments suggest that direct GRPO training with concise reasoning is already sufficient to elicit strong multi-modal reasoning, removing the heavy overhead for CoT annotation or CoT SFT. 2 Figure 2. Overview of CoT models. After pre-training, they are typically post-trained via SFT stage using CoT annotations and RL stage. For both training and inference, the models suffer from heavy prefilling with dense visual tokens, and lengthy decoding due to human-like thinking generation. Token Compression. Token compression methods aim to reduce the number of tokens processed by Transformerbased models, thereby lowering computational overhead. Such techniques have been explored in NLP models [14, 21, 22, 55, 72], vision models [2, 8, 10, 24, 32, 41, 51, 56, 63, 75], and multi-modal LLMs [3, 30, 42, 52, 67, 74, 79, 82, 86]. Among them, several approaches operate without re-training and can be directly applied during inference [3, 52, 79, 86]. For instance, AIM [86] merges visually similar tokens before the LLM and prunes uninformative tokens within the LLM, achieving substantial token reduction with minimal impact on pre-trained capability. Building on these characteristics, we incorporate token compression to improve the efficiency of both training and inference, particularly for processing long videos. 3. Benchmarks and Analysis In this section, we provide analysis on prevalent pre-trained models and typical chain-of-thought models, through accuracy and efficiency evaluation across diverse video benchmarks. These analysis motivate our final model design. 3.1. Background Before the anlysis, we first provide the background of our analysis as shown in Figure 2. Model training: MLLMs are usually pre-trained on largescale image-text and video-text datasets. Building on the capabilities acquired during pre-training, CoT models undergo two-stage post-training process: (1) Supervised fine-tuning steers the models to learn the reasoning patterns derived from CoT annotations, and (2) Reinforcement finetuning seeks to incentivize reasoning capability through sparse reward signals and the GRPO algorithm [15]. Model inference: Both post-training and inference stages 3 Figure 3. Statistics of training and inference overhead. (a) Training overhead shows the training runtime of CoT model (i.e., Video-R1 [12]), which is measured via four A800-SXM4-80GB GPUs. (b) Inference overhead reports the inference statistics (i.e., decoding length and inference runtime) of different reason modes, which is measured through single A800-SXM4-80GB GPU. share the same model forward process, consisting of prefilling phrase and decoding phrase. In the prefilling phrase, the model processes input visual and text tokens to compute their key-value (KV) cashes at each LLM layer. The decoding phrase then auto-regressively generates tokens one by one. Both phrases are computationally impacted by the number of processed or generated tokens. In the following, we describe more details on the different modes of decoding phrase. Chain-of-thought reason decoding: CoT models are trained to generate long text traces that include multiple reasoning steps: = CoT(x) = {r1, r2, . . . , rT , ˆa}, (1) where denotes the input video and textual query, is the generated text response, {rt} denotes intermediate reasoning steps, and ˆa is the final answer. These reasoning steps are typically human-like pondering patterns (e.g., Hmm,, Lets think,, or Wait,) which are learned from CoT annotations. This decoding mode incurs high computational cost due to its lengthy token generation process. Concise reason decoding: This mode produces compact reasoning response rather than full CoT trace: = ConciseReason(x) = {r, ˆa}. (2) Such behavior is first acquired from pre-training and can be further enhanced by post-training (e.g., RL). By reducing the number of generated tokens, concise reasoning achieves faster inference while preserving response accuracy. Direct answer decoding: The simplest decoding mode generates only the final answer: = DirectAnswer(x) = {ˆa}. This mode minimizes computation cost for decoding, yet lacking explainability behind the answer. Model Decoding Mode General Video Benchmarks Long Video Benchmarks Complex Video Benchmarks VideoMME MVBench MLVU LVBench LongVideoBench EgoSchema VideoHolmes Video-TT MMVU Qwen2.5-VL-7B [1] direct answer 55.5 63.2 55.4 34.2 52. 52.5 35.7 35.2 63.4 Qwen2.5-VL-7B + Video-R1 [12] chain-of-thought 54.9 (-0.6) 64.9 (+1.7) 58.9 (+3.5) 35.4 (+1.2) 54.6 (+2.1) 47.6 (-4.9) 39.4 (+3.7) 39.9 (+4.7) 62.4 (-1.0) Qwen2.5-VL-7B [1] concise reason 52. 55.2 47.3 32.2 47.5 53.3 32. 31.4 55.2 Table 1. Inference results on video benchmarks. The CoT model improves the pre-trained model on some benchmarks yet not the others. This improvement requires substantial computations, including: CoT annotations, supervised fine-tuning, and lengthy decoding. Model Decoding General Video Benchmarks Long Video Benchmarks Complex Video Benchmarks Mode VideoMME MVBench MLVU LVBench LongVideoBench EgoSchema VideoHolmes Video-TT MMVU Qwen2.5-VL-7B [1] direct answer 52.5 (-3.0) 60.0 (-3.2) 54.6 (-0.8) 31.3 (-2.9) 51.1 (-1.4) 51.3 (-1.2) 34.7 (-1.0) 31.2 (-4.0) 63.8 (-0.4) Qwen2.5-VL-7B + Video-R1 [12] chain-of-thought 52.8 (-2.1) 60.0 (-4.9) 57.4 (-1.5) 32.5 (-2.9) 50.4 (-4.2) 45.5 (-2.1) 37.8 (-1.6) 38.0 (-1.9) 61.8 (-0.6) Qwen2.5-VL-7B [1] concise reason 46.7 (-6.1) 52.0 (-3.2) 39.4 (-7.9) 29.5 (-2.7) 44.4 (-3.1) 50.7 (-2.6) 26.9 (-5.2) 30.2 (-1.2) 48.2 (-7.0) Avg. -1.9 -2. -4.3 Table 2. Training-free token compression method applied during inference [86]. The accuracy delta (colored) represents the performance degradation compared to the model without using token compression. Concise reason is largely impacted by token compression. 3.2. Analysis In the following analysis, we summarize the key finding at the beginning of each part. We first measure the efficiency of CoT model during training and inference. Then we compare its reasoning capability to the pre-trained model which CoT model was initialized from. Finally, we explore token compression to improve the efficiency of CoT model. Tested Models: We consider two typical types of models. The first is the pre-trained model, such as Qwen2.5-VL [1]. It is milestone work of multi-model LLM that demonstrate strong performance on video reasoning. The second type is the chain-of-thought (CoT) model, such as Video-R1 [12]. It is initialized by the pre-trained Qwen2.5-VL model and then post-trained to learn CoT reasoning. It requires extensive annotations of chain-of-thought and trains the model via supervised fine-tuning (SFT) and reinforcement learning (RL) with GRPO. Benchmarks: We evaluate the models on diverse and commonly-used video benchmarks, including (1) general video benchmarks (VideoMME [13], MVBench [26]): they are created to evaluate the reasoning capability of video LLMs; (2) long video benchmarks (MLVU [87], LVBench [60], LongVideoBench [65], Egoschema [36]): they are designed for long video understanding; (3) complex video benchmarks (VideoHolmes [5], Video-TT [78], MMVU [83]): they emphasize complex event reasoning or multi-discipline understanding. Finding 1: CoT model incurs substantial computation overhead during both training and inference, yet yields modest improvements over the pre-trained model which it was initialized from. Fig. 3 (a) reports the training cost of CoT model. To imitate human-like reasoning, post-training requires over 30 hours on four high-end GPUs, not to mention the additional expense for collecting CoT annotations. Further, as shown in Fig. 3 (b), CoT model spends around ten times in inference runtime due to long sequence decoding, when compared to direct answer and concise reason (e.g., 11.9 vs. 0.47 and 1.71). Such high cost hinders the deployment of video LLMs, especially in the resource-constrained environments (e.g., robots). By closer inspection at the generated text, there are frequent self-reflection or verification expressions, such as Hmm, and Wait,. We hypothesize that these patters are not necessary for general video understanding, which typically require perception and reasoning on the objects, human-object interactions, and social events over time. More importantly, despite its significant computation cost, CoT model fails to exhibit benefits on video benchmarks accordingly. As shown in Table 1, the pre-trained model, which the CoT model was initialized from, can perform favorably to CoT model and event outperforms it on several benchmarks (e.g., VideoMME and EgoSchema). Together, these results intrigue our re-thinking about CoT reasoning and its post-training paradigm. Finding 2: Concise reasoning, learned from pretraining, can largely reduce decoding cost, yet its performance is unsatisfactory. In Table 1, we notice that the pre-trained model already possesses the capability to provide concise reasoning. Such reasoning can save substantial computation for decoding. However, there is clear performance gap between concise reason and direct answer. We conjecture that the pre-trained model is not well aligned with concise reasoning paradigm and could be aligned by post-training. Finding 3: Training-free token compression method can reduce prefilling overhead, but fails to well preserve the performance of concise reasoning. 4 Figure 4. Framework of our method. (1) Typical CoT models are trained via three stages and perform long reasoning during inference. (2) In comparison, our method does not require the stage of supervised fine-tuning and the annotations at this stage, and generates concise reasoning during inference. (3) We further reduce computation overhead by trainable token compression. In parallel to shorten the text length at decoding phrase, one alternative to improve the efficiency is reducing the overhead at prefilling phrase. We explore the recent advances from training-free token compression which do not require further training. For example, [86] reduces prefilling workload substantially during inference, with minimum loss of pre-trained capability. We apply it to the pre-trained and CoT models in plug-and-play manner. The results are shown in Table 2. Concise reason mode experiences larger accuracy drops, whereas direct-answer or CoT models are less affected. We conjecture that despite the concise reasoning capability acquired from pre-training, the model is not optimized to such decoding mode and thus being more fragile and vulnerable to the unseen input intervention (e.g., token compression). Remark: Motivated by these findings, it is essential to design solution that can enhance the reasoning capability of pre-trained models in an efficient way. 4. Methods Our Hypothesis: Concise reasoning, combined with compressed visual tokens and optimized via RL finetuning, is sufficient for effective video reasoning. Motivated by our empirical findings, we synthesize an efficient post-training framework tailored for video reasoning. As illustrated in Figure 4, our method differs from existing approaches in three aspects: (1) Post-Training: it directly performs RL post-training, eliminates the need for SFT and its CoT annotations, (2) Prefilling: it integrates token compression to reduce the cost of training and inference, and (3) Decoding: it generates native, concise reasoning response instead of long CoT text during inference. We elaborate each component in the following sections. 4.1. Post-Training with GRPO Given pre-trained MLLM, most prior works (e.g., VideoR1 [12]) adopt two-stage pipeline that first applies SFT on annotated CoT responses, followed by RL finetuning. However, we hypothesize that directly fine-tuning the model with RL is sufficient to enhance reasoning ability and achieve strong performance. Thus, we bypass the costly SFT stage and train the model solely with RL. We employ Group Relative Policy Optimization (GRPO) [44] during RL training, variant of policy gradient methods. By comparing groups of sampled responses, GRPO removes the need for critic model and reduces training overhead. Formally, given an input (a video and textual query), GRPO samples candidate responses = {y1, . . . , yG} from the policy. reward function assigns scores {R1, . . . , RG}, which are normalized by their mean and standard deviation: , (3) i=1) Ai = Ri mean({Ri}G std({Ri}G i=1) where Ai denotes the relative advantage of the i-th response. This encourages the model to favor higher-scoring responses within the group. In our implementation, we adopt the same reward functions used in most existing works, including format reward and accuracy reward. To prevent large deviations from the pre-trained parameters πref , KL-divergence term is applied to regularize the model weights. The final optimization objective is: (cid:104) (cid:88) (cid:16) min Ai πθ(yi) πθold (yi) , Ai clip( πθ(yi) πθold (yi) , (cid:105) , β DKL(πθ πref) i=1 1 ϵ, 1 + ϵ) (cid:17) (4) max πθ Eyπθold 5 Model Decoding Mode General Video Benchmarks Long Video Benchmarks Complex Video Benchmarks VideoMME MVBench MLVU LVBench LongVideoBench EgoSchema VideoHolmes Video-TT MMVU Qwen2.5-VL-7B [1] direct answer 55.5 63.2 55.4 34.2 52.5 52. 35.7 35.2 63.4 Qwen2.5-VL-7B + GRPO [15] direct answer 56.9 (+1.4) 64.0 (+0.8) 57.7 (+2.3) 36.9 (+2.7) 54.3 (+1.8) 55.7 (+3.2) 38.6 (+2.9) 38.5 (+3.3) 63.7 (+0.3) Qwen2.5-VL-7B [1] concise reason 52.8 55. 47.3 32.2 47.5 53.3 32.1 31. 55.2 Qwen2.5-VL-7B + GRPO [15] concise reason 55.4 (+2.6) 65.2 (+10.0) 58.4 (+11.1) 36.0 (+3.8) 54.4 (+6.9) 53.1 (-0.2) 40.0 (+7.9) 40.1 (+8.7) 65.0 (+9.8) Qwen2.5-VL-7B + Video-R1 [12] chain-of-thought 54.9 64.9 58.9 35. 54.6 47.6 39.4 39.9 62.4 Table 3. Validation 1: Direct reinforcement learning with GRPO consistently improves pre-trained model and fill the gap of concise reason, without the need for CoT annotations and CoT supervised fine-tuning. Model Decoding General Video Benchmarks Long Video Benchmarks Complex Video Benchmarks Mode VideoMME MVBench MLVU LVBench LongVideoBench EgoSchema VideoHolmes Video-TT MMVU Qwen2.5-VL-7B [1] concise reason 46.7 (-6.1) 52.0 (-3.2) 39.4 (-7.9) 29.5 (-2.7) 44.4 (-3.1) 50.7 (-2.6) 26.9 (-5.2) 30.2 (-1.2) 48.2 (-7.0) Qwen2.5-VL-7B + GRPO [15] concise reason 51.8 (-3.6) 61.2 (-4.0) 57.0 (-1.4) 33.8 (-2.2) 50.5 (-3.9) 50.5 (-2.6) 39.0 (-1.0) 40.1 (-0.0) 64.0 (-1.0) Avg. -4.3 -2.2 Table 4. Validation 2: Direct reinforcement learning with GRPO can mitigate the accuracy gap introduced by token compression during inference. The delta (colored) represents the accuracy degradation compared to corresponding model without using token compression. where πθold is the old policy before gradient update, πref denotes the reference policy (e.g., the model before GRPO training), and β is regularization coefficient controlling divergence from the reference policy. 4.2. Prefill with Token Compression Videos often contain redundant visual tokens, significantly increasing the computational cost of LLM prefilling and response decoding. Inspired by the recent method AIM [86], we perform token merging and token pruning to reduce the number of visual tokens. Unlike AIM designed for inference, we incorporate token compression to training and let the model learn to adapt the compressed information. Existing token compression methods, including AIM, typically do not physically remove tokens from the model. Instead, they manipulate attention masks to indicate the retained or pruned tokens. It is easy to implement, yet leaves both wall time and GPU memory consumption unchanged and still large. In contrast, to support efficient training, we develop the token pruning that physically discards unimportant tokens to reduce memory footprint and runtime. Moreover, many existing token compression methods require explicit computation of attention weights and thus are not compatible with FlashAttention [7]. To handle this incompatibility, we selectively disable FlashAttention in only small number of layers that perform pruning (e.g., 5 layers). This hybrid strategy preserves the efficiency benefits from both FlashAttention and token compression. Hence, our final implementation achieves substantial memory and runtime savings during both training and inference. Once token compression is applied, the number of tokens and LLM computation are largely reduced. This allows the model to take more video frames as inputs and thus better understand long videos, using comparable computation as the original network forward without token compression. 4.3. Decode with Concise Reasoning Mode As introduced in Section 3.1, we adopt the decoding mode as concise reasoning. It provides explainability to the final answer and meanwhile maintains reasonable decoding overhead, making both training and inference efficient. 5. Experiments and Results In this section, we start with the experiments validating our hypothesis and then present the results of our final model. Further, we provide ablation study and output visualization. 5.1. Avoid Overthinking Direct GRPO training consistently enhances the pretrained model and bridges the gap in concise reasoning, without requiring CoT annotations or SFT. We directly fine-tune the pre-trained model through RL with GRPO algorithm. The results are reported at Table 3. It turns out that consistent improvements are surprisingly observed across benchmarks (e.g., +1.4/+2.6 on VideoMME, +2.3/+11.1 on MLVU, +2.9/+7.9 on VideoHolmes, over direct answer/concise reason mode). Further, the GRPO finetuned model even surpasses the heavily post-trained CoT model (e.g., 55.4 vs. 54.9 on VideoMME, 40.0 vs. 39.4 on VideoHolmes). These consistent improvements suggest that GRPO can generally incentive the reasoning capability, eliminating the need for CoT annotations or CoT SFT. In Table 3, we also observe that, for the pre-trained model, there exists noticeable performance gap between direct answer and concise reason modes (e.g., 2.7 on VideoMMME, 8.1 on MLVU). We conjecture that this is caused by the pre-training data which emphasizes on answer accuracy rather than explicit reasoning. Interestingly, after GRPO fine-tuning, this gap is mitigated. Concise reason mode can even perform better (e.g., 65.2 vs. 64. 6 Model Training Decoding General Video Benchmarks Long Video Benchmarks Complex Video Benchmarks Stages Mode VideoMME MVBench MLVU LVBench LongVideoBench EgoSchema VideoHolmes Video-TT MMVU Qwen2.5-VL-7B [1] Qwen2.5-VL-7B [1] PT PT direct answer concise reason Video-R1-7B [12] PT, SFT, RL chain-of-thought Our Final Model PT, RL concise reason Qwen3-VL-8B [53] Qwen3-VL-8B [53] PT PT direct answer concise reason 55.5 52.8 54.9 60.6 60.3 57. 63.2 55.2 64.9 65.6 66.8 63. 55.4 47.3 58.9 67.0 58.2 52. 34.2 32.2 35.4 38.9 35.8 32. 52.5 47.5 54.6 55.7 56.2 52. 52.5 53.3 47.6 55.0 63.1 62. 35.7 32.1 39.4 41.6 40.7 37. 35.2 31.4 39.9 40.4 39.6 34. 63.4 55.2 62.4 63.6 66.4 67. Table 5. Final model results on video benchmarks. Our final solution performs token compression at both training and inference, and fine-tunes the pre-trained model Qwen2.5-VL-7B with concise reasoning mode using GRPO. Ours not only achieves the best results across benchmarks, but also requires much less computation during training (i.e., no SFT) and inference (i.e., no heavy CoT)."
        },
        {
            "title": "Decoding",
            "content": "General Video Benchmarks Long Video Benchmarks Complex Video Benchmarks"
        },
        {
            "title": "Mode",
            "content": "VideoMME MVBench MLVU LVBench LongVideoBench EgoSchema VideoHolmes Video-TT MMVU Qwen2.5-VL-7B + GRPO [15] concise reason inference w. T.C. concise reason inference w. T.C. & 6x frames concise reason + training w. T.C. & 6x frames concise reason 55. 51.8 59.5 60.6 65.2 61.2 65. 65.6 58.4 57.0 62.9 67.0 36. 33.8 38.0 38.9 54.4 50.5 54. 55.7 53.1 50.5 54.8 55.0 40. 39.0 40.8 41.6 40.1 40.1 39. 40.4 65.0 64.0 65.3 63.6 Table 6. Ablation study on token compression and the number of video frames. T. C. denotes token compression method. on MVBench, 58.4 vs. 57.7 on MLVU, 40.0 vs. 38.6 on VideoHolmes). These results validate our hypothesis that concise reasoning, optimized by only RL fine-tuning without overthinking, can be sufficient for video reasoning. 5.2. Look Less, See Further Direct GRPO training improves the compatibility of concise reasoning with token compression, allowing for more input frames with improved accuracy. In Table 4, we observe that concise reasoning can be further integrated with token compression after GRPO finetuning (e.g., performance drops shrink from 4.3 to 2.2). This in turn allows us to input more frames for improved performance, which is discussed in our Ablation Study (Sec. 5.4). We conjecture that GRPO training strengthens the models behavior for concise reasoning, making it more capable of operating effectively in this decoding mode. The degree to which the model acquires this capability could directly influence its robustness to unseen input perturbations, such as token compression. 5.3. Results of Final Model Implementation Details: We use the same training dataset and training hyper-parameters as Video-R1 [12]. Specifically, we train models via GRPO for 2,000 steps on VideoR1-260k. For efficiency consideration, during both training and inference, we keep each frame with resolution as 128 28 28 and each video with the maximum number of frames as 16, unless otherwise noted. Setup: We compare the performance of our final model with multiple baselines, including pre-trained Qwen2.5VL [1] with direct answer or concise reason mode, and CoT model Video-R1 [12]. For fair comparison, we maintain the same level of computation budget for prefilling. Our final model thus takes more input frames during training and inference (i.e., 96 frames), since token compression technique reduces the number of visual tokens for each frame. Our model achieves strong results across benchmarks. As shown in Table 5, our model achieves the best results on all benchmarks (i.e., +5.7 on VideoMME, +8.1 on MLVU, +2.2 on VideoHolmes, over Video-R1). Notably, using only the resources for GRPO fine-tuning, our model can even outperform or compare favorably to Qwen3-VL which takes much more resources during pre-training to improve beyond Qwen2.5-VL. These results suggest that our method can effectively enhance the reasoning capability on general videos, long videos, and complex videos. Our method is efficient at both training and inference. For training, our method only requires RL post-training by inheriting the concise reasoning capability derived from pre-training. It eliminates the need for CoT annotations and CoT SFT. For inference, our method performs concise reasoning rather than generating long CoT traces. 5.4. Ablation Study As shown in Table 6, incorporating token compression during training yields clear performance gains compared to applying it only at inference time (e.g., +1.1 on VideoMME, +4.1 on MLVU, and +0.8 on VideoHolmes). These results indicate that the model can learn to adapt to compressed visual tokens during training. 7 Figure 5. Visualization of generated text from CoT reasoning (Video-R1) and concise reasoning (Ours). Partial text chunks are colored through human validation. Green: ground-truth or correct predicted answers. Blue: correct intermediate reasoning steps. Purple: unnecessary intermediate reasoning steps. Red: incorrect intermediate reasoning steps or final predictions. Moreover, token compression reduces the number of visual tokens, allowing the model to process more video frames within comparable prefilling computation budget. This expanded temporal coverage significantly enhances performance, particularly for long video understanding tasks (e.g., +5.9 on MLVU and +4.2 on LVBench). 5.5. Visualization Figure 5 presents qualitative comparisons between CoT reasoning (Video-R1) and concise reasoning (our final model). Given video, question, and multiple-choice options, the model is instructed to generate the reasoning process / final anser within the think / answer brackets. Long reasoning traces are often unnecessary and even distracting. Across the three examples, CoT reasoning tends to overthink and overcomplicate the problem, for instance, elaborating on each option with human-like phrases such as Oh, see or Hmm. These expressions are format-oriented rather than content-oriented, indicating that the model focuses more on mimicking human reasoning style than on actual reasoning content. Such verbose reasoning not only increases computational cost, but can also divert the reasoning trajectory, leading to incorrect answers. Dense frames help. With token compression, the model can process densely sampled video frames without increasing prefilling overhead. Dense frame coverage preserves critical visual details that may be missed under sparse sampling. Take the middle sample in Figure 5 as the instance. key frame directly reveals the correct answer. This frame is missed by sparse sampling and CoT reasoning is unable to compensate such missing information. 6. Conclusion In this work, we revisit the role of chain-of-thought reasoning in video MLLMs and challenge the assumption that long, human-like reasoning traces and large sets of video tokens are necessary for strong performance. Through extensive benchmarking, we find that conventional CoT provides limited benefits while incurring significant computational overhead, and that pre-trained models are poorly aligned with concise reasoning despite possessing sufficient knowledge. Motivated by these observations, we hypothesize and empirically validate that effective video reasoning can emerge from concise reasoning traces operating on compressed video tokens. Our post-training and inference framework realizes this paradigm, substantially improving inference efficiency while maintaining competitive accuracy across diverse video understanding tasks, without reliance on CoT annotations or heavy SFT. We hope these findings inspire rethinking of how reasoning should be modeled in MLLMs and encourage future exploration of efficient approaches to video reasoning."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2, 4, 6, 7 [2] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your ViT but faster. In International Conference on Learning Representations, 2023. 3 [3] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, pages 1935. Springer, 2024. 2, 3 [4] Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, and Song Han. Scaling rl to long videos. In Advances in Neural Information Processing Systems (NeurIPS), 2025. 1, 2 [5] Junhao Cheng, Yuying Ge, Teng Wang, Yixiao Ge, Jing Liao, and Ying Shan. Video-holmes: Can mllm think like holmes for complex video reasoning? arXiv preprint arXiv:2505.21374, 2025. 4 [6] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023. 2 [7] Tri Dao. Flashattention-2: Faster attention with betarXiv preprint ter parallelism and work partitioning. arXiv:2307.08691, 2023. 6 [8] Peiyan Dong, Mengshu Sun, Alec Lu, Yanyue Xie, Kenneth Liu, Zhenglun Kong, Xin Meng, Zhengang Li, Xue Lin, Zhenman Fang, et al. HeatViT: Hardware-Efficient Adaptive Token Pruning for Vision Transformers. In Proceedings of the IEEE International Symposium on High-Performance Computer Architecture, pages 442455, 2023. 3 [10] Mohsen [9] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 90629072, 2025. 2 Soroush Abbasi Koohpayegani, Fayyaz, Farnoush Rezaei Jafari, Sunando Sengupta, Hamid Reza Vaezi Joze, Eric Sommerlade, Hamed Pirsiavash, and Jurgen Gall. Adaptive Token Sampling for Efficient Vision Transformers. In Proceedings of the European Conference on Computer Vision, pages 396414. Springer, 2022. 3 [11] Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, and Wynne Hsu. Video-of-thought: Step-by-step video reasoning from perception to cognition. In Proceeding of the ICML, 2024. 2 [12] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. 1, 2, 3, 4, 5, 6, 9 [13] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 4 [14] Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan Chakaravarthy, Yogish Sabharwal, and Ashish Verma. PoWER-BERT: Accelerating BERT Inference via In Proceedings of Progressive Word-Vector Elimination. the International Conference on Machine Learning, pages 36903699. PMLR, 2020. 3 [15] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 1, 2, 3, 6, 7 [16] Songhao Han, Wei Huang, Hairong Shi, Le Zhuo, Xiu Su, Shifeng Zhang, Xu Zhou, Xiaojuan Qi, Yue Liao, and Si Liu. Videoespresso: large-scale chain-of-thought dataset for fine-grained video reasoning via core frame selection. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2618126191, 2025. 2 [17] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model In Proceedings of the for long-term video understanding. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1350413514, 2024. [18] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. 1, 2 [19] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 1, 2 [20] Lingjie Jiang, Xun Wu, Shaohan Huang, Qingxiu Dong, Zewen Chi, Li Dong, Xingxing Zhang, Tengchao Lv, Lei Cui, and Furu Wei. Think only when you need with large hybrid-reasoning models. arXiv preprint arXiv:2505.14631, 2025. 2 [21] Gyuwan Kim and Kyunghyun Cho. Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search. arXiv preprint arXiv:2010.07003, 2020. 3 [22] Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, and Kurt Keutzer. Learned Token Pruning for Transformers. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 784794, 2022. 3 [23] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. [24] Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei Niu, Mengshu Sun, Xuan Shen, Geng Yuan, Bin Ren, Hao Tang, et al. SPViT: Enabling Faster Vision Transformers Latency-Aware Soft Token Pruning. In Proceedings of the European Conference on Computer Vision, pages 620640. Springer, 2022. 3 [25] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2 [26] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multimodal video understanding benchmark, 2023. 4 [27] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal arXiv preprint perception via reinforcement fine-tuning. arXiv:2504.06958, 2025. 2 [28] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual represenarXiv preprint tation by alignment before projection. arXiv:2311.10122, 2023. [29] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2668926699, 2024. 2 [30] Zhihang Lin, Mingbao Lin, Luxi Lin, and Rongrong Ji. Boosting multimodal large language models with visual tokens withdrawal for rapid inference. Proceedings of the AAAI Conference on Artificial Intelligence, 2025. 3 [31] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 2 [32] Xiangcheng Liu, Tianyi Wu, and Guodong Guo. Adaptive Sparse ViT: Towards Learnable Adaptive Token PrunarXiv preprint ing by Fully Exploiting Self-Attention. arXiv:2209.13802, 2022. 3 [33] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visualrft: Visual reinforcement fine-tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025. [34] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, et al. Nvila: Efficient frontier visual language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 41224134, 2025. 2 [35] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), 2024. 2 [36] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding, 2023. 4 [37] Chancharik Mitra, Brandon Huang, Trevor Darrell, and Roei Herzig. Compositional chain-of-thought prompting for large multimodal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 [38] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 2 [39] Haiyi Qiu, Minghe Gao, Long Qian, Kaihang Pan, Qifan Yu, Juncheng Li, Wenjie Wang, Siliang Tang, Yueting Zhuang, and Tat-Seng Chua. Step: Enhancing video-llms compositional reasoning by spatio-temporal graph-guided selftraining. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 32843294, 2025. 2 [40] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. [41] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification. Advances in Neural Information Processing Systems, 34:1393713949, 2021. 3 [42] Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025. 2, 3 [43] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. Advances in Neural Information Processing Systems, 37:86128642, 2024. 2 [44] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2, 5 [45] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. 2 [46] Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2616026169, 2025. [47] Enxin Song, Wenhao Chai, Tian Ye, Jenq-Neng Hwang, Xi Li, and Gaoang Wang. Moviechat+: Question-aware sparse memory for long video question answering. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. 2 [48] Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2025. 2 10 [49] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning. In Advances in Neural Information Processing Systems (NeurIPS), 2025. 1, 2 [50] Reuben Tan, Ximeng Sun, Ping Hu, Jui-hsien Wang, Hanieh Deilamsalehy, Bryan Plummer, Bryan Russell, and Kate Saenko. Koala: Key frame-conditioned long video-llm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1358113591, 2024. 2 [51] Quan Tang, Bowen Zhang, Jiajun Liu, Fagui Liu, and Yifan Liu. Dynamic Token Pruning in Plain Vision Transformers for Semantic Segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 777 786, 2023. 3 [52] Keda Tao, Can Qin, Haoxuan You, Yang Sui, and Huan Wang. Dycoke: Dynamic compression of tokens for fast In Proceedings of the Comvideo large language models. puter Vision and Pattern Recognition Conference, pages 1899219001, 2025. 2, 3 [53] Qwen3-VL Team. Qwen3-vl, 2025. 2, 7 [54] Shulin Tian, Ruiqi Wang, Hongming Guo, Penghao Wu, Yuhao Dong, Xiuying Wang, Jingkang Yang, Hao Zhang, Hongyuan Zhu, and Ziwei Liu. Ego-r1: Chain-of-toolthought for ultra-long egocentric video reasoning, 2025. [55] Hanrui Wang, Zhekai Zhang, and Song Han. SpAtten: Efficient Sparse Attention Architecture with Cascade Token In Proceedings of the IEEE Internaand Head Pruning. tional Symposium on High-Performance Computer Architecture, pages 97110, 2021. 3 [56] Hongjie Wang, Bhishma Dedhia, and Niraj Jha. Zerotprune: Zero-shot token pruning through leveraging of the attention graph in pre-trained transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1607016079, 2024. 3 [57] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2025. 2 [58] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2 [59] Qi Wang, Yanrui Yu, Ye Yuan, Rui Mao, and Tianfei Zhou. Videorft: Incentivizing video reasoning capability in mllms via reinforced fine-tuning. In Advances in neural information processing systems, 2025. 1, 2 [60] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Ming Ding, Xiaotao Gu, Shiyu Huang, Bin Xu, et al. Lvbench: An extreme long video understanding benchmark. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2295822967, 2025. [61] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 1 [62] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837, 2022. 2 [63] Siyuan Wei, Tianzhu Ye, Shen Zhang, Yao Tang, and Jiajun Liang. Joint Token Pruning and Squeezing Towards More In ProAggressive Compression of Vision Transformers. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20922101, 2023. 3 [64] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. In European Conference on Computer Vision, pages 453470. Springer, 2025. 2 [65] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. 4 [66] Jiaer Xia, Yuhang Zang, Peng Gao, Yixuan Li, and Kaiyang Zhou. Visionary-r1: Mitigating shortcuts in visual reasoning with reinforcement learning. arXiv preprint arXiv:2505.14677, 2025. [67] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, and Dahua Lin. Conical visual concentration for In Proceedings of efficient large vision-language models. the Computer Vision and Pattern Recognition Conference (CVPR), pages 1459314603, 2025. 3 [68] Guowei Xu, Peng Jin, Ziang Wu, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 20872098, 2025. 1, 2 [69] Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context arXiv preprint visual language models for long videos. arXiv:2408.10188, 2024. 2 [70] Ziang Yan, Xinhao Li, Yinan He, Zhengrong Yue, Xiangyu Zeng, Yali Wang, Yu Qiao, Limin Wang, and Yi Wang. Videochat-r1.5: Visual test-time scaling to reinforce mulIn Advances in timodal reasoning by iterative perception. Neural Information Processing Systems (NeurIPS), 2025. 2 [71] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. 1, 2 [72] Deming Ye, Yankai Lin, Yufei Huang, and Maosong Sun. Tr-BERT: Dynamic Token Reduction for Accelerating BERT Inference. arXiv preprint arXiv:2105.11618, 2021. 3 [73] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug11 owl3: Towards long image-sequence understanding in multimodal large language models, 2024. ical Methods in Natural Language Processing, pages 6876 6911. Association for Computational Linguistics, 2024. 2 [86] Yiwu Zhong, Zhuoming Liu, Yin Li, and Liwei Wang. Aim: Adaptive inference of multi-modal llms via token merging and pruning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025. 2, 3, 4, 5, 6 [87] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding, 2024. 4 [88] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, et al. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. arXiv preprint arXiv:2310.01852, 2023. 2 [89] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 2 [90] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [74] Weihao Ye, Qiong Wu, Wenhao Lin, and Yiyi Zhou. Fit and prune: Fast and training-free visual token pruning for multimodal large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2212822136, 2025. 3 [75] Hongxu Yin, Arash Vahdat, Jose M. Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov. A-ViT: Adaptive ToIn Proceedings of kens for Efficient Vision Transformer. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1080910818, 2022. 3 [76] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025. 1, 2 [77] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024. 2 [78] Yuanhan Zhang, Yunice Chew, Yuhao Dong, Aria Leo, Bo Hu, and Ziwei Liu. Towards video thinking test: holistic benchmark for advanced video reasoning and understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2062620636, 2025. 4 [79] Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, et al. Sparsevlm: Visual token sparsification for efficient vision-language model inference. In International Conference on Machine Learning, 2025. 2, [80] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022. 1 [81] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-ofarXiv preprint thought reasoning in language models. arXiv:2302.00923, 2023. 2 [82] Zeliang Zhang, Phu Pham, Wentian Zhao, Kun Wan, Yu-Jhe Li, Jianing Zhou, Daniel Miranda, Ajinkya Kale, and Chenliang Xu. Treat visual tokens as text? but your mllm only needs fewer efforts to see. arXiv preprint arXiv:2410.06169, 2024. 3 [83] Yilun Zhao, Haowei Zhang, Lujing Xie, Tongyan Hu, Guo Gan, Yitao Long, Zhiyuan Hu, Weiyuan Chen, Chuhan Li, Zhijian Xu, et al. Mmvu: Measuring expert-level multidiscipline video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 84758489, 2025. 4 [84] Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei Yang. Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models. Advances in Neural Information Processing Systems, 36, 2024. 2 [85] Yiwu Zhong, Zi-Yuan Hu, Michael Lyu, and Liwei Wang. Beyond embeddings: The promise of visual table in visual reasoning. In Proceedings of the 2024 Conference on Empir-"
        }
    ],
    "affiliations": [
        "The Chinese University of Hong Kong",
        "University of Wisconsin-Madison"
    ]
}