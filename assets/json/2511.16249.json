{
    "paper_title": "Controllable Layer Decomposition for Reversible Multi-Layer Image Generation",
    "authors": [
        "Zihao Liu",
        "Zunnan Xu",
        "Shi Shu",
        "Jun Zhou",
        "Ruicheng Zhang",
        "Zhenchao Tang",
        "Xiu Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work presents Controllable Layer Decomposition (CLD), a method for achieving fine-grained and controllable multi-layer separation of raster images. In practical workflows, designers typically generate and edit each RGBA layer independently before compositing them into a final raster image. However, this process is irreversible: once composited, layer-level editing is no longer possible. Existing methods commonly rely on image matting and inpainting, but remain limited in controllability and segmentation precision. To address these challenges, we propose two key modules: LayerDecompose-DiT (LD-DiT), which decouples image elements into distinct layers and enables fine-grained control; and Multi-Layer Conditional Adapter (MLCA), which injects target image information into multi-layer tokens to achieve precise conditional generation. To enable a comprehensive evaluation, we build a new benchmark and introduce tailored evaluation metrics. Experimental results show that CLD consistently outperforms existing methods in both decomposition quality and controllability. Furthermore, the separated layers produced by CLD can be directly manipulated in commonly used design tools such as PowerPoint, highlighting its practical value and applicability in real-world creative workflows. Our project is available at https://monkek123king.github.io/CLD_page/."
        },
        {
            "title": "Start",
            "content": "Controllable Layer Decomposition for Reversible Multi-Layer Image Generation Zihao Liu1*, Zunnan Xu1*, Shi Shu1, Jun Zhou1, Ruicheng Zhang1,2, Zhenchao Tang2, Xiu Li1 1Tsinghua University, 2Sun Yat-sen University 5 2 0 2 5 2 ] . [ 2 9 4 2 6 1 . 1 1 5 2 : r Figure 1. Given an image and its bounding boxes, our framework generates clean foreground layer for each box and single, coherent background. Our method can cleanly separate every foreground layer even when objects are crowded and heavily overlapping, while keeping boundaries sharp, depth order intact, and the composite visually coherent."
        },
        {
            "title": "Abstract",
            "content": "This work presents Controllable Layer Decomposition (CLD), method for achieving fine-grained and controllable multi-layer separation of raster images. In practical workflows, designers typically generate and edit each RGBA layer independently before compositing them into final raster image. However, this process is irreversible: once composited, layer-level editing is no longer possible. Existing methods commonly rely on image matting and inpainting, but remain limited in controllability and segmentation precision. To address these challenges, we propose two key modules: LayerDecompose-DiT (LD-DiT), which decouples image elements into distinct layers and enables *Equal Contribution. Corresponding author. 1 fine-grained control; and Multi-Layer Conditional Adapter (MLCA), which injects target image information into multilayer tokens to achieve precise conditional generation. To enable comprehensive evaluation, we build new benchmark and introduce tailored evaluation metrics. Experimental results show that CLD consistently outperforms existing methods in both decomposition quality and controllability. Furthermore, the separated layers produced by CLD can be directly manipulated in commonly used design tools such as PowerPoint, highlighting its practical value and applicability in real-world creative workflows. Our project is available at CLD. 1. Introduction In the design of posters, advertisements, and other visual media, designers typically do not directly work on single raster image. Instead, they create and edit foreground elements across multiple RGBA layers. Specifically, designers use tools such as Adobe Photoshop or PowerPoint to construct foreground elements at the layer level and then overlay multiple foreground layers onto background layer to produce the final image. However, this compositing process is irreversible: once multiple RGBA layers are merged into single raster image, the original multi-layer information cannot be recovered. Consequently, when only raster image is available, precise layer-level editing and adjustments become extremely challenging. Accurate layer decomposition from raster images can effectively address this issue, enabling designers to perform efficient and controllable edits on the separated layers. In this work, we investigate the problem of usercontrollable layer decomposition, aiming to split single raster image into set of independent layers based on userprovided bounding boxes. Existing approaches, such as decomposition methods for natural images [1, 2, 16, 43], often suffer from incomplete foreground extraction or unwanted artifacts, including messy edges or background color leaking into foreground layers. Some studies [4, 45, 60] adopt modular, multi-stage pipeline, dividing the task into object detection [3840], segmentation [12, 13, 21, 48, 49], image matting [20, 41, 54], and image inpainting [5, 17, 31], with each stage handled by dedicated pre-trained model. However, such pipelines are prone to error propagation: mistakes in early stages can adversely affect subsequent stages, degrading overall layer decomposition quality. To address these limitations, LayerD [42] proposed fully automatic graphic layer decomposition method, iteratively performing image matting for top-layer extraction and image inpainting for background completion. However, LayerD entirely relies on the matting model to identify top-layer elements, leaving users with no control over the final decomposition results. To overcome these limitations, we propose controllable layer decomposition method, which allows users to specify desired decomposition outcomes via bounding boxes. Our core model uses pre-trained DiT models (e.g., FLUX.1[dev] [23]), fully exploiting their strong image generation capabilities to produce layer decomposition results of higher quality. Figure 1 shows the results of our method. It can be seen that the model is able to generate the corresponding foreground and background layers based on the provided bounding boxes, achieving excellent overall visual quality. Moreover, previous evaluation metrics for layer decomposition were limited by uncontrollable generated results, lacking standardized benchmarks. By introducing bounding boxes, we can define ideal target layers and propose new benchmark. Specifically, we establish benchmark for controllable layer decomposition, built on the PrismLayersPro [3] dataset, with new metric dimensions tailored for controllable settings. Our main contributions are summarized as follows: We propose CLD (Controllable Layer Decomposition), controllable framework for raster image layer decomposition that leverages DiTs generation and reasoning capabilities to achieve high-quality layer separation. We introduce new benchmark to assess the performance of layer decomposition from multiple dimensions. Experimental results demonstrate that CLD outperforms baseline methods in both controllability and generation quality, and separated layers can be applied directly to downstream graphic design and editing tasks. 2. Related work Image Layer Decomposition. Image layer decomposition aims to divide an image into set of composable layers that can be re-synthesized to reconstruct the original image. Early approaches are mostly color-based, grouping pixels by color similarity and focusing on digital painting or natural images [1, 2, 9, 22, 43, 44]. Other studies explore objectlevel decomposition in natural scenes [15, 27, 28, 32, 50, 52, 55, 58, 59]. Recent studies have explored multi-layer image generation, using generative models to directly synthesize and disentangle foreground and background layers in unified framework [3, 11, 14, 29, 30, 35, 36, 46, 56]. For example, Huang et al. [14] introduced DreamLayer, which explicitly models the interaction between transparent foreground and background layers for coherent text-driven generation. However, this method is designed for text-toimage synthesis and is not directly applicable to our layer decomposition task. In design images such as posters and advertisements, multiple visual elements coexist, making decomposition challenging in terms of fine-grained separation and controllability. Recent works [4, 42] have explored this direction. Chen et al.[4] proposed pipeline combining visual language models (VLM) with SAM[21], while Suzuki et al. [42] introduced LayerD, which separates layers via image matting and inpainting. However, LayerD relies entirely on the matting model to determine top-layer elements, offering no user control and tending to produce coarse, element-level separations. To overcome these limitations, we propose DiT-based framework that enables controllable, fine-grained layer generation guided by userprovided bounding boxes. Image matting and Element extraction. Natural image matting optimizes the compositing equation = αF + (1 α)B for α, F, using inputs like trimaps or scribbles [25, 34, 47]. Although trimap-free methods [19, 53] reduce manual effort, they are optimized for photographic statistics and fail on the clean edges, solid colors, and repet2 itive textures common in graphic designs. Zero-shot segmentation foundation models offer an alternative. SAM [21] and SAM 2 [37] produce binary masks from prompts but ignore partial transparency. HQSAM [18] refines boundaries but remains at the instance level, failing to model soft transitions. ZIM [20] adapts SAM for zero-shot matting, but it is trained for natural objects and hallucinates alpha on solid-color graphics. Furthermore, none of these models support sequential decomposition; users cannot extract nested elements (e.g., segment the red subtitle) without providing new manual trimap. Our framework solves this. We treat the bounding box as layer request rather than rough separator. The network predicts hard binary mask (for crisp boundaries) and residual alpha map (for optional soft transitions), enabling the extraction of both solid text and semi-transparent shadows. Because the model is conditioned on the box, it can be applied recursively: after removing the top element, the model can process the revealed region, yielding full layer stack. Extensive experiments (Sec. 4) show our strategy outperforms matting-based and SAM-based baselines on graphic-design benchmarks in edge accuracy, layer consistency, and user controllability. 3. Method 3.1. Preliminaries Flow Matching [7, 26] is an alternative to diffusion models by directly learning the velocity field that transports noise to data. Unlike noise-prediction diffusion models (e.g., DDPM [8]), it formulates generation as continuous dynamical system, enabling more stable and efficient sampling process. Specifically, given real sample x0 and noise sample x1, Flow Matching learns velocity field vθ() to approximate the optimal transport between them. The objective aligns the predicted velocity with the true derivative dxt dt along the interpolated path xt = (1 t)x0 + tx1. By modeling the velocity rather than the noise residual, it achieves smoother optimization and more stable gradients in high-dimensional spaces. Due to its faster convergence and higher generation quality, we adopt Flow Matching as the training paradigm for CLD and fine-tune the backbone via LoRA [10]. The overall training objective is: LFM = EtU (0,1) (cid:104) vθ(xt, t, ctext, cimg) (x1 x0)2(cid:105) , (1) where ctext and cimg denote the text and image conditions. 3.2. Problem Formulation Given an raster image RHW 3, such as composited design image, we regard it as consisting of background layer Ibg RHW 4 and multiple foreground layers {Ii i=1 , where denotes the total fg RHiWi4}N 1 i, yl i, xr , yr number of layers. Each layer is represented in RGBA format to preserve transparency information. To enable controllable layer decomposition, the user can provide set of bounding boxes Bu = {B1, . . . , BN 1}, where each Bi = (xl ) specifies the top-left and bottom-right coordinates of target region. The background layer is defined by B0 = (0, 0, H, ), and together they form the complete bounding box set = concat(B0, Bu). Our objective is to generate collection of disentangled layers = {D0, D1, . . . , DN 1}, where each Di represents an independent RGBA layer aligned with its corresponding bounding box Bi. The decomposition aims to ensure spatial alignment and visual consistency across all layers, thereby achieving fine-grained and controllable layer separation. 3.3. LayerDecompose-DiT LayerDecompose-DiT (LD-DiT) is designed to simultaneously generate visual tokens for the background layer and multiple foreground layers, enabling fine-grained disentanglement and hierarchical modeling of image structures. At the architectural level, we build upon FLUX.1[dev] [23], which adopts the Multimodal Diffusion Transformer (MMDiT) as its backbone. MMDiT employs two distinct sets of network weights to separately process text tokens and image tokens, enabling efficient multimodal interaction. However, the original MMDiT is limited to singleimage generation and cannot directly support multi-layer synthesis. To address this limitation, we introduce several key enhancements. Specifically, the input visual tokens are cropped according to user-provided bounding boxes, and the target regional tokens are concatenated into unified token sequence, which is then processed by MMDiT for denoising multi-layer generation. Moreover, to enable the model to better capture hierarchical dependencies within this sequence, we incorporate Layer-Aware Rotary Position Embedding (LA-RoPE) to jointly encode spatial and inter-layer positional relationships. As shown in Figure 2, the conditional inputs to LD-DiT include the bounding boxes B, the original image I, and text prompt T. The text prompt can either be user-provided to semantically guide the generation process or automatically extracted from the original image using VLM. The noisy inputs are obtained by adding Gaussian noise to the cropped multi-layer latent sequence, which is then denoised by MMDiT and decoded into set of layered RGBA images. To enhance global consistency in multi-layer generation, we introduce the composite image as an auxiliary generation target. Unlike layerwise blending, it serves as direct reconstruction of the input image. During training, the input image is embedded using the same visual encoder as the layer tokens and prepended to the layer-specific sequence, with bounding box (0, 0, H, ) covering the entire image. Formally, given layer tokens Xlayers = {x0, x1, . . . , xN 1} and the com3 Figure 2. Our framework utilizes main backbone and parallel control module for precise layer decomposition. (a) The overall CLD architecture, showing the LayerDecompose-DiT (LD-DiT) backbone responsible for generating the multi-layer latent. (b) The detailed structure of the Multi-Layer Conditional Adapter (MLCA). MLCA additively fuses features from the conditional image with the LD-DiTs hidden states, then performs hierarchical cropping based on the input bounding boxes to create multi-layer guidance token sequence. Figure 3. Overview of our adapted Multi-Layer RGBA image Decoder Architecture and Layer-Aware Rotary Position Encoding. posite token xcomp, the input to MMDiT is: Xinput = [xcomp; x0; x1; . . . ; xN 1], (2) where [; ] denotes sequence concatenation. This setup enables the model to propagate global context through the composite token, reinforcing visual and structural coherence across layers. Latent Decoder. Conventional VAEs are limited to processing RGB inputs and are unable to capture transparency information. To address this limitation, our model explicitly incorporates transparency-aware encoding. We adapt two alternative designs into our framework: the MultiLayer Transparent Image Autoencoder from ART [35] and the Latent Transparency module proposed in LayerDiffuse [56]. Through systematic comparison, we find that the former achieves better preservation of alpha information and stronger inter-layer coherence. Based on these observations, we choose to adapt the Multi-Layer Transparent Image Autoencoder to fit our multi-layer generation pipeline, enabling LD-DiT to accurately maintain consistent structures across layers. Figure 3 presents the decoder architecture used in our framework. LA-RoPE. Traditional positional encodings typically model spatial relations within single layer and fail to capture hierarchical dependencies across layers. To address this, we propose Layer-Aware Rotary Position Embedding (LA-RoPE), which jointly encodes both spatial and interlayer positional information in unified hierarchical form (see Fig. 3). Each visual token is indexed by [l, h, w], denoting its layer, height, and width positions. By integrating these hierarchical indices into the queries and keys of selfattention, LA-RoPE enables reasoning across layers and enhances structural coherence and controllability in multilayer generation. Formally, let the n-th query and m-th key be qn, km Rdhead , each split along the channel dimension into three parts: qn = {ql n, qw The (n,m)-th element of the attention matrix is: (cid:88) km = {kl m, kw m, kh n, qh m}. }, (3) Re(cid:2)qc n(kc m)ei(pc npc m)θ(cid:3), A(n, m) = (4) c{l,h,w} where pn = {ln, hn, wn} denotes the 3D position index of the n-th token, (kc m) is the complex conjugate of kc m, θ is preset nonzero constant, and Re[] represents the real part of complex number. 4 3.4. Multi-Layer Conditional Adapter 3.5. Dual-Condition Classifier-Free Guidance Since the input to MMDiT consists of multi-layer sequence of visual tokens, existing conditional control frameworks, such as ControlNet [57], are primarily designed for single-layer image generation and therefore struggle to capture the complex dependencies among multiple visual layers. In particular, ControlNet performs conditioning through global residual pathway, which offers only coarse-level guidance and lacks the ability to maintain structural alignment and inter-layer consistency. To overcome these limitations, we introduce the MultiLayer Conditional Adapter (MLCA), dedicated conditioning module tailored for multi-layer visual generation. MLCA explicitly models hierarchical relationships between layers and dynamically injects layer-specific conditions into the diffusion process. By aligning conditional features with each layers token representation, MLCA enables precise, fine-grained control while preserving inter-layer coherence throughout generation. Specifically, the input conditional image is first encoded into its latent representation zimg via VAE encoder EVAE(): zimg = EVAE(I) RH C. (5) Next, linear mapping network Linear() projects the latent feature into the same feature space as the hidden states RLD of the main MMDiT model, obtaining ˆzimg: ˆzimg = Linear(zimg) RH D. (6) Subsequently, we crop ˆzimg according to the input bounding boxes B, slicing the corresponding regions and flattening them into multi-layer guidance token sequence himg. This sequence is structurally aligned with the MMDiT input, enabling parallel modeling across layers within the Transformers self-attention mechanism for both explicit alignment and implicit coordination: himg = Flatten(Crop(ˆzimg, B)) RLD. (7) We then perform additive conditioning by summing the mapped conditional features with the MMDiT hidden states to obtain the fused feature representation ˆh: ˆh = + himg. (8) Afterward, ˆh is fed into LD-DiT as crucial guidance signal during the denoising process. This design allows the Transformer to dynamically coordinate inter-layer interactions while maintaining the individual objectives of each layer. By decoupling layer-wise conditioning and enabling effective cross-layer communication, MLCA delivers precise control, stronger inter-layer coherence, and more consistent multi-layer decomposition compared to traditional single-layer control frameworks. Building on the fine-grained guidance from MLCA, we further introduce dual-condition Classifier-Free Guidance (CFG) strategy to improve inter-layer consistency and semantic controllability. The model takes two conditional inputs: text description ctext and reference image cimg. During inference, the process splits into conditional and an unconditional branch. Unlike conventional CFG that discards all conditions in the unconditional branch, we retain cimg while nullifying ctext. The reason for retaining cimg in the unconditional branch is twofold. (1) It provides structural anchor, ensuring that both conditional and unconditional branches remain aligned in terms of spatial layout and hierarchical structure. (2) It enables semantic incremental separation: by subtracting the unconditional prediction from the conditional one, shared structural information is canceled, isolating the text-driven semantic contribution. The predicted flow velocity ˆv is computed as: ˆv = vθ(xt, t, , cimg) (cid:16) + vθ(xt, t, ctext, cimg) vθ(xt, t, , cimg) (cid:17) (9) , where denotes the CFG scaling factor, and vθ is the flow velocity predicted by the model under different conditions. 4. Experiment 4.1. Experiment Setting Dataset Preparation. We employ PrismLayersPro [3], one of the latest and largest high-quality multi-layer image datasets, as the primary source for both training and evaluation. This dataset contains approximately 20K groups of high-quality layered samples, where each group consists of complete composite image, multiple corresponding transparent layers (in RGBA format), and an associated textual description. PrismLayersPro spans 21 distinct categories, encompassing wide range of visual styles such as 3D, cartoon, and others, exhibiting high diversity and visual complexity. For data partitioning, we divide PrismLayersPro into training, validation, and testing sets with ratio of 90% / 5% / 5%, respectively. Based on this partitioning, we define set of dedicated evaluation metrics and construct new benchmark for multi-layer decomposition. Implementation details. Our backbone is built upon the latest FLUX.1[dev] model, state-of-the-art Multimodal Diffusion Transformer (MMDiT) architecture. We fine-tune this model on the partitioned PrismLayersPro training set using LoRA adaptation, with the LoRA rank set to 64. The training is performed with the Prodigy optimizer, learning rate of 1, and batch size of 4. The model is trained for 25K iterations on images with resolution of 1024 1024. 5 Table 1. Comparison with LayerD [42] on the Crello [51] dataset test set. LayerD Metrics refers to the evaluation metrics proposed in LayerD, while Q-Insight Metrics denotes the evaluation metrics we introduce based on the Q-Insight [24] model. Method LayerD Metrics Alpha Soft IoU RGB L1 LayerD[ICCV 25] Ours 0.0653 0.0474 0.7055 0. Unified Score 0.1799 0.1352 Q-Insight Metrics Visual Semantic Consistency Fidelity Editability User Study Content Completeness Semantic Consistency Visual Quality 3.8658 3.9157 3.6773 3.7334 4.0011 4. 19% 81% 24% 76% 9% 91% Table 2. Ablation study on different model variants. We examine four configurations: (1) using the decoder adapted from LayerDiffuse [56], (2) disabling the image condition in the CFG unconditional branch, (3) removing the composite image prediction objective, and (4) the full model. The ART [35] adapted decoder is used for all experiments except the first configuration. Variant (1) Using adapted LayerDiffuse decoder (2) w/o image condition in CFG-Uncond (3) w/o composite image prediction (4) Full Model Layer-level SSIM 0.822 0.767 0.845 0.874 FID 24.016 69.910 21.453 19.413 Mask-level F1 IoU 0.847 0.576 0.867 0. 0.902 0.692 0.918 0.920 PSNR 24.429 21.691 26.211 27.646 Reconstruction SSIM FID PSNR 22.570 20.658 27.240 29.825 0.824 0.819 0.926 0.945 36.127 103.250 15.638 11.464 4.2. Metrics Since there is currently no established benchmark for evaluating deterministic multi-layer image separation tasks, we propose comprehensive, multi-dimensional evaluation to assess model performance across layer quality, transparency accuracy, and overall reconstruction fidelity. Our evaluation system is divided into three main categories: Layer-level: These metrics assess the visual quality of each individual layer, using PSNR and SSIM for pixellevel and structural consistency, and FID [6] for perceptual similarity between generated and real images. Mask-level: We use IoU (Intersection over Union) and F1 score to evaluate the alignment between generated and ground-truth alpha masks, reflecting accuracy in modeling transparent regions and boundaries. Reconstruction: All generated RGBA layers are composited back into an RGB image. We then evaluate this reconstructed image with PSNR, SSIM, and FID to assess global coherence and overall fidelity restoration. All generated images are in RGBA format, while evaluation metrics (PSNR, SSIM, FID) are defined only for RGB images. To ensure metric compatibility and fair comparison, we first convert the RGBA image into RGB format. Specifically, We use fixed neutral gray background Igrey with pixel values (0.5, 0.5, 0.5) within the range [0, 1], and then composite it into Itgt with the RGBA images RGB channels Irgb and alpha channel Iα as follows: Itgt = Irgb Iα + Igrey (1 Iα). (10) The use of neutral gray background ensures robust blending with minimal color distortion, preserving transparent regions and enabling consistent evaluation across methods. 4.3. Quantitative Result Currently, there is no existing work that performs fully comparable multi-layer generative decomposition under user-guided conditions. Therefore, we select LayerD [42], state-of-the-art method for automatic layer separation, as our primary baseline for comparison since it is the most recent and directly comparable approach in both task definition and output representation. LayerD [42] performs layer separation using image matting and inpainting. Operating on single composite image, it relies entirely on the models inference, offering no explicit control over layer structure. To address layer correspondence, LayerD employs order-aware alignment via DTW [33] and computes evaluation metrics based on the matched layers. It is important to note that our method differs from LayerD in that it incorporates user-provided bounding boxes to guide the layer decomposition. This bounding-box guidance introduces explicit structural control over the generated layers, which LayerD does not utilize. Consequently, the two methods may produce different generation styles due to the additional structural guidance in our approach. We perform this comparison to demonstrate how the combination of bounding-box guidance and the proposed model architecture affects controllability, precision, and visual quality. Table 1 presents comparison between LayerD and our proposed method under the evaluation metrics defined by LayerD, using the Crello [51] dataset as the test set, which we adopt for fair comparison since it was also used in 6 Figure 4. Unlike LayerD [42], which offers coarse separation and lacks user control, our method uses bounding boxes to guide more fine-grained and controllable process. This results in better precision, visual quality, and hierarchical consistency in complex scenarios. LayerDs experiments. To adapt to our task, the RGB L1 metric is calculated by first converting RGBA images into RGB format according to Equation 10, and then applying the L1 distance calculation. The Unified Score metric jointly considers the performance of both RGB L1 and Alpha soft IoU, and is formulated as follows: Unified Score = RGB L1 + (1 Alpha soft IoU) 2 . (11) Experimental results show that our method achieves better results than LayerD across all evaluation metrics, benefiting from the introduction of bounding-box guidance and the powerful image generation capability of DiT. To further assess our models performance, we employ Q-Insight [24], state-of-the-art image quality evaluation model that leverages MLLM reasoning and reinforcement learning to provide interpretable, zero-shot perceptual quality assessment. We design three evaluation dimensions: Semantic Consistency, Visual Fidelity, and Editability. As shown in Table 1, our method achieves better performance than LayerD across all three dimensions, confirming its comprehensive advantages in semantic preservation, visual quality, and editability. It is worth noting that the LayerD model is trained on the Crello [51] training set, while our model is trained solely on the PrismLayerPro [3] dataset. Despite this, our model still achieves better results on the Crello test set, further demonstrating its strong generalization ability and robustness across datasets. 4.4. Qualitative Result Comparison with LayerD. Figure 4 shows visual comparison of layer separation between our method and LayerD, and the results of the user study are presented in Table 1. Our model produces more detailed and fine-grained layer structures, benefiting from user-specified bounding boxes that guide the decomposition process. This additional guidance allows higher controllability and flexibility, particularly in complex scenes. Compared to LayerDs automatic separation, our method preserves semantic and structural alignment across layers, and produces results that better align with user preferences. Comparison with Image Matting and Segmentation Methods. Recent layer separation methods, including our baseline LayerD [42], largely rely on image matting or segmentation models, whose performance directly impacts subsequent layer separation and background inpainting. These models often struggle in scenarios containing with numerous foreground objects or complex hierarchical relationships, limiting the quality of the decomposition. To illustrate this, we compare our approach with widely used matting and segmentation methods, ZIM [20] and SAM2 [37]. Figure 6 shows that in scenes with overlapping elements, complex layouts, or text-rich regions, these methods tend to produce blurred boundaries, fragmented regions, or mis-segmented objects. In contrast, our method performs multi-layer decomposition in unified generative process. Leveraging the global reasoning and contextual 7 Figure 5. Ablation study on the impact of decoder choices and the CFG unconditional image condition. modeling of DiT, it accurately infers layer boundaries and maintains visual and semantic coherence, even under heavy occlusion. The comparison results indicate that combining generative end-to-end decomposition with user-guided structural control provides inherent advantages over existing layer decomposition methods based on image matting or segmentation, offering higher fidelity, clearer layer separation, and stronger inter-layer consistency. 4.5. Ablation Study We conducted systematic ablation studies on our proposed layer decomposition model to evaluate the contribution of each component to the overall performance. The experiments were performed on test set derived from the PrismLayersPro [3] dataset, and model performance was analyzed using the multi-dimensional evaluation metrics defined in Sec. 4.2. Incorporating the composite image into the generation target. In our model design, we include the composite image as an auxiliary generation target to enable information flow between the overall image and individual RGBA layers, improving global consistency and reconstructability. As shown in Table 2, this design enhances Layer-level and Mask-level metrics, with the largest gains in the Reconstruction metric, demonstrating improved visual coherence Figure 6. Unlike segmentation (SAM2 [37]) and matting (ZIM [20]) models, which fail on complex design images, our generative approach produces clearer, more coherent layering. and layer fidelity. CFG unconditional branch. Our framework uses two main conditions: text description and the original image. During inference, we apply Classifier-Free Guidance (CFG), dropping the text condition in the unconditional branch but retaining the original image. Keeping the image in the unconditional branch significantly improves results  (Table 2)  and prevents background leakage in foreground layers (Figure 5), highlighting its importance for spatial consistency and semantic coherence. RGBA image decoder. Traditional VAE decoders only handle RGB. We adapted two RGBA strategies for CLD: (1) LayerDiffuse [56], which reconstructs RGB and alpha separately, and (2) ART [35], which directly generates RGBA from ViT-based latent. As shown in Figure 5 and Table 2, the ART-adapted decoder achieves better metrics and cleaner foreground boundaries. [2] Yagiz Aksoy, Tunc Ozan Aydin, Aljoˇsa Smolic, and Marc Pollefeys. Unmixing-based soft color segmentation for image manipulation. ACM Transactions on Graphics (TOG), 36(2):119, 2017. 2 [3] Junwen Chen, Heyang Jiang, Yanbin Wang, Keming Wu, Ji Li, Chao Zhang, Keiji Yanai, Dong Chen, and Yuhui Yuan. Prismlayers: Open data for high-quality multilayer transparent image generative models. arXiv preprint arXiv:2505.22523, 2025. 2, 5, 7, 8 [4] Jingye Chen, Zhaowen Wang, Nanxuan Zhao, Li Zhang, Difan Liu, Jimei Yang, and Qifeng Chen. Rethinking layered graphic design generation with top-down approach. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1686116870, 2025. 2, 1 [5] Ciprian Corneanu, Raghudeep Gadde, and Aleix Martinez. Latentpaint: Image inpainting in latent space with diffusion models. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 4334 4343, 2024. 2 [6] DC Dowson and BV666017 Landau. The frechet distance between multivariate normal distributions. Journal of multivariate analysis, 12(3):450455, 1982. 6 [7] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [8] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [9] Daichi Horita, Kiyoharu Aizawa, Ryohei Suzuki, Taizan Yonetsuji, and Huachun Zhu. Fast nonlinear image unblending. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 20512059, 2022. 2 [10] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 3 [11] Dingbang Huang, Wenbo Li, Yifei Zhao, Xinyu Pan, Yanhong Zeng, and Bo Dai. Psdiffusion: Harmonized multilayer image generation via layout and appearance alignment. arXiv preprint arXiv:2505.11468, 2025. 2 [12] Jiaqi Huang, Zunnan Xu, Ting Liu, Yong Liu, Haonan Han, Kehong Yuan, and Xiu Li. Densely connected parameterIn Proefficient tuning for referring image segmentation. ceedings of the AAAI Conference on Artificial Intelligence, pages 36533661, 2025. 2 [13] Jiaqi Huang, Zunnan Xu, Jun Zhou, Ting Liu, Yicheng Xiao, Mingwen Ou, Bowen Ji, Xiu Li, and Kehong Yuan. Sam-r1: Leveraging sam for reward feedback in multimodal arXiv preprint segmentation via reinforcement learning. arXiv:2505.22596, 2025. [14] Junjia Huang, Pengxiang Yan, Jinhang Cai, Jiyang Liu, Zhao Wang, Yitong Wang, Xinglong Wu, and Guanbin Li. Dreamlayer: Simultaneous multi-layer generation via diffusion mode. arXiv preprint arXiv:2503.12838, 2025. 2, 1 Figure 7. Application: Direct Editing on Decomposed Layers. This example showcases direct editing operations on separated layers, including Remove Layers, Copy & Add Layers, and Change Layout. All operations are performed within PowerPoint. 4.6. Application As shown in Figure 7, we demonstrate practical application of our method. By manipulating the separated layers, users can conveniently perform various editing operations in common office software such as PowerPoint, including removing layers, copying and adding layers, and adjusting layouts. This layer-based editing paradigm greatly enhances the editability and flexibility of image content, enabling even non-professional users to easily accomplish complex element rearrangement and layout adjustments. This highlights the practical value of our method in realworld design and content creation scenarios. 5. Conclusion In this paper, we present CLD, method to separate raster images into fine-grained layers, solving key limitation in post-production. We use bounding boxes for precise, userguided separation and introduce new multi-dimensional benchmark for this task. Experiments show CLD outperforms existing methods in decomposition quality, controllability, and visual fidelity. The generated layers are directly usable for downstream editing (e.g., removal, recomposition), offering practical and high-quality solution for creative design and future multi-layer extensions."
        },
        {
            "title": "References",
            "content": "[1] Naofumi Akimoto, Huachun Zhu, Yanghua Jin, and Yoshimitsu Aoki. Fast soft color segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 82778286, 2020. 2 9 [15] Phillip Isola and Ce Liu. Scene collaging: Analysis and synthesis of natural images with semantic layers. In Proceedings of the IEEE International Conference on Computer Vision, pages 30483055, 2013. 2 [16] Xiaoyu Jin, Zunnan Xu, Mingwen Ou, and Wenming Yang. Alignment is all you need: training-free augmentation strategy for pose-guided video generation. arXiv preprint arXiv:2408.16506, 2024. 2 [17] Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: plug-and-play image inpainting model with decomposed dual-branch diffusion. In European Conference on Computer Vision, pages 150168. Springer, 2024. 2 [18] Lei Ke, Mingqiao Ye, Martin Danelljan, Yu-Wing Tai, ChiKeung Tang, Fisher Yu, et al. Segment anything in high quality. Advances in Neural Information Processing Systems, 36: 2991429934, 2023. [19] Zhanghan Ke, Jiayu Sun, Kaican Li, Qiong Yan, and Rynson WH Lau. Modnet: Real-time trimap-free portrait matIn Proceedings of the ting via objective decomposition. AAAI Conference on Artificial Intelligence, pages 1140 1147, 2022. 2 [20] Beomyoung Kim, Chanyong Shin, Joonhyun Jeong, Hyungsik Jung, Se-Yun Lee, Sewhan Chun, Dong-Hyun Hwang, and Joonsang Yu. Zim: Zero-shot image matting for anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2382823838, 2025. 2, 3, 7, 8 [21] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 2, 3 [22] Yuki Koyama and Masataka Goto. Decomposing images into layers with advanced color blending. In Computer Graphics Forum, pages 397407. Wiley Online Library, 2018. 2 [23] Black Forest Labs. https : / / huggingface . co / black - forest - labs / FLUX . 1-dev, 2024. Last accessed 7 March, 2025. 2, Flux.1 [dev]. [24] Weiqi Li, Xuanyu Zhang, Shijie Zhao, Yabin Zhang, Junlin Li, Li Zhang, and Jian Zhang. Q-insight: Understanding image quality via visual reinforcement learning. arXiv preprint arXiv:2503.22679, 2025. 6, 7, 1 [25] Yaoyi Li and Hongtao Lu. Natural image matting via guided contextual attention. In Proceedings of the AAAI conference on artificial intelligence, pages 1145011457, 2020. 2 [26] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations. 3 [27] Zhengzhe Liu, Qing Liu, Chirui Chang, Jianming Zhang, Daniil Pakhomov, Haitian Zheng, Zhe Lin, Daniel Cohen-Or, and Chi-Wing Fu. Object-level scene deocclusion. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 2 [28] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: Poseguided text-to-video generation using pose-free videos. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 41174125, 2024. 2 [29] Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, et al. Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation. In SIGGRAPH Asia 2024 Conference Papers, pages 112, 2024. 2 [30] Yue Ma, Kunyu Feng, Zhongyuan Hu, Xinyu Wang, Yucheng Wang, Mingzhe Zheng, Xuanhua He, Chenyang Zhu, Hongyu Liu, Yingqing He, et al. Controllable video arXiv preprint arXiv:2507.16869, generation: survey. 2025. 2 [31] Yue Ma, Yingqing He, Hongfa Wang, Andong Wang, Leqi Shen, Chenyang Qi, Jixuan Ying, Chengfei Cai, Zhifeng Li, Heung-Yeung Shum, et al. Follow-your-click: Open-domain regional image animation via motion prompts. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 60186026, 2025. 2 [32] Tom Monnier, Elliot Vincent, Jean Ponce, and Mathieu Aubry. Unsupervised layered image decomposition into obIn Proceedings of the IEEE/CVF interject prototypes. national conference on computer vision, pages 86408650, 2021. 2 [33] Meinard Muller. Information retrieval for music and motion. Springer, 2007. [34] GyuTae Park, SungJoon Son, JaeYoung Yoo, SeHo Kim, and Nojun Kwak. Matteformer: Transformer-based image matting via prior-tokens. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1169611706, 2022. 2 [35] Yifan Pu, Yiming Zhao, Zhicong Tang, Ruihong Yin, Haoxing Ye, Yuhui Yuan, Dong Chen, Jianmin Bao, Sirui Zhang, Yanbin Wang, et al. Art: Anonymous region transformer for In Provariable multi-layer transparent image generation. ceedings of the Computer Vision and Pattern Recognition Conference, pages 79527962, 2025. 2, 4, 6, 8, 1, 3 [36] Guocheng Gordon Qian, Ruihang Zhang, Tsai-Shien Chen, Yusuf Dalva, Anujraaj Argo Goyal, Willi Menapace, Ivan Skorokhodov, Meng Dong, Arpit Sahni, Daniil OstaInteractive personalized shev, et al. arXiv preprint t2i via spatially-aware layered canvas. arXiv:2510.20820, 2025. 2 Layercomposer: [37] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. In The Thirteenth International Conference on Learning Representations. 3, 7, 8 [38] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018. 2 [39] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779788, 2016. [40] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015. [41] Yanan Sun, Chi-Keung Tang, and Yu-Wing Tai. Semantic image matting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11120 11129, 2021. 2 [42] Tomoyuki Suzuki, Kang-Jun Liu, Naoto Inoue, and Kota Yamaguchi. Layerd: Decomposing raster graphic designs into layers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1778317792, 2025. 2, 6, 7, 1, 3 [43] Jianchao Tan, Jyh-Ming Lien, and Yotam Gingold. Decomposing images into layers via rgb-space geometry. ACM Transactions on Graphics (TOG), 36(1):114, 2016. 2 [44] Jianchao Tan, Jose Echevarria, and Yotam Gingold. Efficient palette-based decomposition and recoloring of images via rgbxy-space geometry. ACM Transactions on Graphics (ToG), 37(6):110, 2018. 2 [45] Petru-Daniel Tudosiu, Yongxin Yang, Shifeng Zhang, Fei Chen, Steven McDonagh, Gerasimos Lampouras, Ignacio Iacobacci, and Sarah Parisot. Mulan: multi layer annotated dataset for controllable text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2241322422, 2024. 2 [46] Zitong Wang, Hang Zhao, Qianyu Zhou, Xuequan Lu, Xiangtai Li, and Yiren Song. Diffdecompose: Layer-wise decomposition of alpha-composited images via diffusion transformers. arXiv preprint arXiv:2505.21541, 2025. 2 [47] Ning Xu, Brian Price, Scott Cohen, and Thomas Huang. Deep image matting. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2970 2979, 2017. 2 [48] Zunnan Xu, Zhihong Chen, Yong Zhang, Yibing Song, Xiang Wan, and Guanbin Li. Bridging vision and language encoders: Parameter-efficient tuning for referring image segIn Proceedings of the IEEE/CVF international mentation. conference on computer vision, pages 1750317512, 2023. [49] Zunnan Xu, Jiaqi Huang, Ting Liu, Yong Liu, Haonan Han, Kehong Yuan, and Xiu Li. Enhancing fine-grained multimodal alignment via adapters: parameter-efficient trainIn 2nd ing framework for referring image segmentation. Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ ICML 2024), 2024. 2 [50] Zunnan Xu, Zhentao Yu, Zixiang Zhou, Jun Zhou, Xiaoyu Jin, Fa-Ting Hong, Xiaozhong Ji, Junwei Zhu, Chengfei Cai, Shiyu Tang, et al. Hunyuanportrait: Implicit condition control for enhanced portrait animation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1590915919, 2025. 2 [51] Kota Yamaguchi. Canvasvae: Learning to generate vector graphic documents. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 54815489, 2021. 6, 7, 2 [52] Jinrui Yang, Qing Liu, Yijun Li, Soo Ye Kim, Daniil Pakhomov, Mengwei Ren, Jianming Zhang, Zhe Lin, Cihang Xie, and Yuyin Zhou. Generative image layer decomposition with visual effects. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 76437653, 2025. 2 [53] Jingfeng Yao, Xinggang Wang, Shusheng Yang, and Baoyuan Wang. Vitmatte: Boosting image matting with pretrained plain vision transformers. Information Fusion, 103: 102091, 2024. 2 [54] Qihang Yu, Jianming Zhang, He Zhang, Yilin Wang, Zhe Lin, Ning Xu, Yutong Bai, and Alan Yuille. Mask guided matting via progressive refinement network. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11541163, 2021. [55] Xiaohang Zhan, Xingang Pan, Bo Dai, Ziwei Liu, Dahua Lin, and Chen Change Loy. Self-supervised scene deocclusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 37843792, 2020. 2 [56] Lvmin Zhang and Maneesh Agrawala. Transparent image layer diffusion using latent transparency. ACM Transactions on Graphics (TOG), 43(4):115, 2024. 2, 4, 6, 8, 1 [57] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. 5 [58] Ruicheng Zhang, Jun Zhou, Zunnan Xu, Zihao Liu, Jiehui Huang, Mingyang Zhang, Yu Sun, and Xiu Li. Zero-shot 3daware trajectory-guided image-to-video generation via testtime training. arXiv preprint arXiv:2509.06723, 2025. 2 [59] Chuanxia Zheng, Duy-Son Dao, Guoxian Song, Tat-Jen Cham, and Jianfei Cai. Visiting the invisible: Layer-bylayer completed scene decomposition. International Journal of Computer Vision, 129(12):31953215, 2021. 2 [60] Jun Zhou, Jiahao Li, Zunnan Xu, Hanhui Li, Yiji Cheng, Fa-Ting Hong, Qin Lin, Qinglin Lu, and Xiaodan Liang. Fireedit: Fine-grained instruction-based image editing via region-aware vision language model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1309313103, 2025. 2 Controllable Layer Decomposition for Reversible Multi-Layer Image Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Further Explanation of the Baseline Currently, no existing work fully matches our task setting. Recent approaches [4, 42] are mostly based on image matting or vision-language models (VLMs), but these methods cannot handle user-specified bounding box constraints. Other related works [14, 35, 56] that use DiT focus on textto-image generation rather than layer decomposition, making them unsuitable as direct baselines. Figure 9 illustrates the differences between our task setup and those of related works. Among existing research, the most relevant method is the recently proposed LayerD [42]. This work investigates three architectural variants: the matting-base model, the YOLO-base model, and the VLM-base model. Among them, the matting-base design achieves the strongest performance. In addition, LayerD systematically examines multiple combinations of matting models and inpainting models and selects the best-performing configuration. As result, LayerD can be regarded as the current state of the art among traditional approaches based on matting/segmentation/VLM. For these reasons, we adopt LayerD as our primary baseline. Experimental results demonstrate that our method significantly outperforms LayerD in both quantitative metrics and visual quality. This highlights the advantages of our diffusion-based layered generation framework, which provides stronger representational capacity and better disentanglement under complex scenarios compared to traditional matting/segmentation/VLM-based solutions. Figure 8. Visualization of RGBARGB Conversion. 7. Q-Insight Additional Details To provide more objective and comprehensive comparison of our method with LayerD [42] in terms of generation quality, we further employ Q-Insight [24], the current 1 state-of-the-art image evaluation model, for automated assessment. We evaluate the generated images from multiple perspectives, complementing traditional metrics by assessing semantic alignment and editability. Specifically, we structure the assessment into three dimensions: Semantic Consistency: Measures the alignment between the generated image and the textual description or target semantics; Visual Fidelity: Assesses visual quality, detail preservation, and overall realism; Editability: Evaluates the manipulability of the image for subsequent editing, modifications, and local adjustments. The evaluation prompts used are as follows. Q-Insight Prompt You are evaluating the quality of layered image decomposition. The input consists of multiple images: [Original Image], [Layer 1], [Layer 2], ..., [Reconstructed Image]. # THINKING STEP First, reason about the decomposition quality step-by-step inside the <think> tags. Analyze the semantic completeness of each layer, the visual fidelity of the reconstruction, and the practical editability of the layer structure based on the criteria below. # RATING STEP Second, based on your reasoning, provide three separate ratings. All ratings should be floats between 1.00 and 5.00, rounded to two decimal places. # CRITERIA 1. **semantic consistency:** (1.00 = fragmented, meaningless layers; 5.00 = all layers are semantically whole and independent). Each layer should represent complete, logical object or part (e.g., whole text block, distinct background element, person). 2. **visual fidelity:** (1.00 = major artifacts, poor reconstruction; 5.00 = layers reconstruct the original perfectly). **Note: You must ignore the inherent artistic style of the image (e.g., photo vs. cartoon).** Your score should *only* reflect artifacts from the decomposition process, such as halos, incorrect background inpainting, color bleeding, or missing pixels. 3. **editability:** (1.00 = useless, underor over-decomposed; 5.00 = perfect granularity for editing). **Preference: finer-grained decomposition (more layers) is preferred and should receive higher score,** as long as the individual layers still represent complete semantic parts. Do not penalize for over-decomposition if the resulting finegrained layers are logical and useful for an editor (e.g., separating title from body text is better than keeping them as one layer). # FORMATTING Return the result in JSON format with the following keys: { semantic consistency: <score>, visual fidelity: <score>, editability: <score> } ) 8. Additional Dataset Specifications The dataset used for training and in the proposed benchmark is based on PrismLayersPro, constructed by Chen et al[3]. It contains 21 visual style categories, including 3D, Pokemon, anime, cartoon, doodle art, furry, ink, kid crayon drawing, line draw, melting gold, melting silver, metal textured, neon graffiti, papercut art, pixel art, pop art, sand painting, steampunk, toy, watercolor painting, and wood carving. The dataset comprises 20K samples, each providing complete layer information along with corresponding textual annotations. For each data sample, the dataset contains: 1. global textual description: Tglobal; 2. composite image: Icomp; 3. RGBA layers: global statistical sense. Neutral gray, as the midpoint of the color space, naturally satisfies both criteria and provides balanced fill, reducing systematic color shifts and worstcase error, leading to stable and fair evaluations. From visual perspective, neutral gray avoids artificial color or brightness shifts in transparent areas, ensuring comparability across methods. Moreover, in opaque regions (where α is close to 1), the influence of the background becomes negligible, and thus this choice does not interfere with foreground content. Figure 8 illustrates the visual results after conversion. In summary, using neutral gray (0.5, 0.5, 0.5) as placeholder is statistically robust, visually neutral, and practically convenient, reducing metric bias and enabling consistent, reliable RGB-based comparisons. {I(0), I(1), . . . , I(n1)}, (12) 10. Additional Results where I(0) is the background layer, and I(1) I(n1) are foreground layers; 4. Text descriptions for each layer: {T(0), T(1), . . . , T(n1)}; (13) 5. Bounding boxes for foreground layers: For the k-th foreground layer (k 1), the bounding box is b(k) = (x(k) , y(k) , x(k) , y(k) ), (14) while the background layer does not provide bounding box, defaulting to cover the entire image: b(0) = (0, 0, H, ). (15) PrismLayersPro provides sufficient data, complete layer annotations, and textual descriptions, allowing direct use for training and evaluation. We split each style category independently for our experiments: 090% for training, 9095% for testing, and 95100% for validation, ensuring balanced and consistent partitions. In Figure 10 and 11, we present additional examples of layer decomposition results. All samples shown in this figure are taken from the test split of our curated PrismLayersPro dataset. These examples further illustrate the effectiveness and consistency of our decomposition pipeline across diverse scenes and appearance variations. 11. In-The-Wild Case In Figure 12 and 13, we present the layer decomposition results of our method on in-the-wild examples. These samples are drawn from the test split of the Crello [51] dataset, which contains diverse, professionally designed graphic layouts with rich stylistic variations. As shown in the figure, our model generalizes well beyond the controlled training distribution and is able to separate complex visual elements, including icons, text segments, and composite shapes. This demonstrates the robustness of our framework and its ability to handle real-world design assets encountered in practical editing scenarios. 9. RGBA-to-RGB Conversion Method 12. Failure Case We describe our RGBA-to-RGB conversion process in Sec. 4.2. fixed solid background is used, and the input RGBA image is decomposed into RGB channels Irgb and the alpha channel Iα, with the final RGB image obtained via linear color compositing (Eq. 10). In this paper, we adopt neutral gray (pixel values (0.5, 0.5, 0.5) in the normalized range [0, 1]) as the default background color. Since RGBA layers contain no real background, transparent regions must be filled with placeholder color solely for computing standard RGB metrics (PSNR, SSIM, FID). The placeholder should satisfy two criteria: (1) it should not introduce additional color bias, and (2) it should minimize the evaluation error induced by background filling in Figure 14 shows representative failure cases of our method, which fall into two main categories. The first occurs with very small, thin, or intricate content, such as fine text or tiny details, where limited spatial resolution prevents the model from capturing sufficient local cues. We believe that increasing input resolution or using multi-scale features could mitigate this issue. The second involves complex hierarchical occlusions. When objects are heavily occluded, the image provides insufficient information for accurate reconstruction. Although textual descriptions are used as auxiliary guidance, they are often insufficient to recover hidden content. Addressing this may require more advanced use of text cues or explicit structural priors. Figure 9. Comparison with related work. The figure illustrates the distinctions between our task and prior methods: ART [35] generates multilayer images without target-image constraints, and LayerD [42] decomposes layers without fine control. Our approach, by incorporating user-provided bounding boxes, achieves accurate and controllable layer separation. 3 Figure 10. More qualitative results. 4 Figure 11. More qualitative results. Figure 12. In-The-Wild Case. 6 Figure 13. In-The-Wild Case. 7 Figure 14. Failure case."
        }
    ],
    "affiliations": [
        "Sun Yat-sen University",
        "Tsinghua University"
    ]
}