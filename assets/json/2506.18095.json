{
    "paper_title": "ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation",
    "authors": [
        "Junying Chen",
        "Zhenyang Cai",
        "Pengcheng Chen",
        "Shunian Chen",
        "Ke Ji",
        "Xidong Wang",
        "Yunjin Yang",
        "Benyou Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in multimodal generative models have unlocked photorealistic, instruction-aligned image generation, yet leading systems like GPT-4o-Image remain proprietary and inaccessible. To democratize these capabilities, we present ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and 46K text-and-image-to-image data, all synthesized using GPT-4o's image generation capabilities for distilling its advanced image generation abilities. Leveraging this dataset, we develop Janus-4o, a multimodal large language model capable of both text-to-image and text-and-image-to-image generation. Janus-4o not only significantly improves text-to-image generation over its predecessor, Janus-Pro, but also newly supports text-and-image-to-image generation. Notably, it achieves impressive performance in text-and-image-to-image generation from scratch, using only 91K synthetic samples and 6 hours of training on an 8 A800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will foster open research in photorealistic, instruction-aligned image generation."
        },
        {
            "title": "Start",
            "content": "ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation Junying Chen, Zhenyang Cai, Pengcheng Chen, Shunian Chen, Ke Ji, Xidong Wang, Yunjin Yang, Benyou Wang The Chinese University of Hong Kong, Shenzhen wangbenyou@cuhk.edu.cn https://github.com/FreedomIntelligence/ShareGPT-4o-Image"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in multimodal generative models have unlocked photorealistic, instruction-aligned image generation, yet leading systems like GPT-4o-Image remain proprietary and inaccessible. To democratize these capabilities, we present ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and 46K text-and-image-to-image data, all synthesized using GPT-4os image generation capabilities for distilling its advanced image generation abilities. Leveraging this dataset, we develop Janus-4o, multimodal large language model capable of both text-to-image and textand-image-to-image generation. Janus-4o not only significantly improves text-to-image generation over its predecessor, Janus-Pro, but also newly supports text-and-image-to-image generation. Notably, it achieves impressive performance in text-and-image-to-image generation from scratch, using only 91K synthetic samples and 6 hours of training on an 8A800 GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will foster open research in photorealistic, instruction-aligned image generation. 5 2 0 2 2 2 ] . [ 1 5 9 0 8 1 . 6 0 5 2 : r Figure 1: Overview of the ShareGPT-4o-Image. The dataset comprises 91K synthetic samples from GPT-4o-Image, capturing its advanced capabilities for both text-to-image and text-andimage-to-image generation tasks. Displayed prompts are simplified. Equal Contribution. Corresponding author. 1 Figure 2: Image Generation Gains from ShareGPT-4o-Image. Fine-tuning Janus-Pro with ShareGPT-4o-Image yields Janus-4o, which shows notable improvements in image generation. Janus-4o also supports text-and-image-to-image generation, outperforming other baselines with just 91K training samples."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in large-scale generative models (Song & Ermon, 2019; Rombach et al., 2022; Ho et al., 2020; Ramesh et al., 2021; Esser et al., 2021; Rombach et al., 2022; Ho & Salimans, 2022) have significantly improved the quality and controllability of multimodal synthesis. Among these, image generation has emerged as core capability in multimodal AI systems, enabling powerful tools for creative design, visual reasoning, and interactive agents. As large language models (LLMs) continue to evolve into natively multimodal architectures (Achiam et al., 2023; Liu et al., 2023; Bai et al., 2025; Team et al., 2023; Chen et al., 2024e), the boundary between language understanding and visual generation is rapidly dissolving, resulting in models that can follow complex instructions and produce highly contextualized, photorealistic images. Recently, GPT-4o-Image (Open AI, 2025), the image generation variant of GPT-4o (OpenAI, 2024), has demonstrated unprecedented capabilities in both text-to-image and text-and-imageto-image generation (Chen et al., 2025b). It leverages unified architecture that jointly learns from both modalities, resulting in superior instruction alignment, semantic coherence, and photorealism in image generation (Open AI, 2025). Notably, such proprietary models are characterized by highly opaque implementation processes, with even their architectures remaining undisclosed. In contrast, existing open-source image generation models, such as Janus-Pro (Chen et al., 2025c), still fall significantly behind (Yan et al., 2025). To make such capabilities more accessible to the community, we introduce ShareGPT-4oImage, synthesized dataset designed to transfer GPT-4o-level image generation skills to open multimodal models. The dataset includes 45K text-to-image and 46K text-and-image-toimage prompts, carefully curated for diversity and quality. Using GPT-4os image generation, we synthesized the corresponding 91K image outputs generated by GPT-4o. The resulting dataset covers wide range of styles and grounded visual reasoning, and reflects GPT-4os strengths in instruction-following and visual aesthetics. 2 We fine-tune Janus-Pro (Chen et al., 2025c) on ShareGPT-4o-Image to develop Janus-4o, new multimodal large language model (MLLM) that supports both text-to-image and text-and-image-to-image generation. In text-to-image generation, Janus-4o improves on JanusPro by 4 and 1.6 points on EvalGen and DPG-Bench benchmarks, respectively. Unlike its predecessor, which only handled text-to-image tasks, Janus-4o introduces text-and-imageto-image capabilities through innovative enhancements. On the ImgEdit-Bench, it excels among open-source image editing models, despite being trained on only 91K samples. Human evaluations also show stronger preference for Janus-4os outputs, highlighting the effectiveness of ShareGPT-4o-Image dataset. We are releasing both ShareGPT-4o-Image and Janus-4o to the research community to foster progress in aligning open-source multimodal models with cutting-edge generative capabilities. Our contributions are summarized as follows: We introduce ShareGPT-4o-Image, the first dataset comprising high-quality text-toimage and text-and-image-to-image pairs, distilled from GPT-4o image generation. Based on ShareGPT-4o-Image, we develop Janus-4o, multimodal large language model capable of both text-to-image and text-and-image-to-image generation. Experiments show that Janus-4o significantly improves upon Janus-Pro in image generation and enables text-and-image-to-image generation. Remarkably, impressive performance in text-and-image-to-image generation is achieved from scratch using only 91K synthetic samples and 6 hours of training on single 8A800 GPU machine."
        },
        {
            "title": "2 ShareGPT-4o-Image",
            "content": "Figure 3: The flow diagram for the dataset construction process. The prompt in the diagram is the simplified version. To distill the advanced generative capabilities of GPT-4o-Image (Open AI, 2025), we construct ShareGPT-4o-Image, large-scale dataset comprising 45k text-to-image pairs and 46k instruction-guided image editing triplets. The entire data generation process, illustrated in Figure 3, leverages Gemini-Pro-2.5 (Team et al., 2023) as our primary Large Language Model (LLM) for all text synthesis tasks. 2.1 Text-to-Image Data We generate text-to-image pairs via two complementary pipelines: prompt-first approach for controlled attribute coverage and an image-first approach to ground prompts in realistic visual content. Prompt-First Pipeline. This pipeline synthesizes structured prompts and then generates corresponding images. First, we define six-dimensional attribute space (Objects, Background, Style, etc.) populated with rich vocabulary, including 1,000 object categories from ImageNet (Appendix B). For each sample, we sample attributes from these dimensions and use our LLM to compose them into coherent, natural-language prompt. Finally, each prompt is passed to GPT-4o-Image to generate paired image. This systematic process yields diverse prompts with controlled complexity. Image-First Pipeline. To complement the synthetic prompts, we source real-world, diverse, high-quality images from the ALLaVA dataset (Chen et al., 2024a). For each image, we prompt our LLM to generate detailed, descriptive text prompt that accurately reflects the visual content (see Appendix C.1 for the meta-prompt). The resulting (generated prompt, original image) pairs ensure our datasets text distribution also captures the language needed to describe naturally occurring scenes. 2.2 Instruction-Guided Image Editing Data To capture instruction-following editing capabilities, we generate data triplets of (source image, instruction, edited image). The generation process is as follows: First, we define taxonomy of 14 image editing tasks, grouped into 5 high-level categories such as object manipulation and style transfer (Appendix B.1.1). For each data point, we begin with source image, drawn either from our newly generated text-to-image collection or from curated set of real-world photos. We then sample an editing task from our taxonomy. Based on the source image content and the selected task, our LLM synthesizes specific, natural-language editing instruction (meta-prompts in Appendix C.3). Finally, GPT-4oImage executes this instruction on the source image to produce the edited output. This structured pipeline yields diverse dataset of 46k triplets covering wide range of common image editing scenarios. 3 Janus-4o: Fine-Tuning with ShareGPT-4o-Image Figure 4: Overview of Janus-4o model. Built upon Janus-Pro, it is constructed via fine-tuning on ShareGPT-4o-Image. It incorporates enhancements to support text-and-image-to-image generation. Both text-to-image and text-and-image-to-image tasks are jointly trained. To assess the effectiveness of ShareGPT-4o-Image, we fine-tuned the ShareGPT-4o-Image dataset on Janus-Pro to develop Janus-4o. The training methodology of Janus-4o is illustrated in Figure 4. 3.1 Text-to-Image Fine-Tuning Training For text-to-image training, we fine-tune Janus-Pro under the original text-to-image generation setup. text prompt is tokenized into = (s0, s1, . . . , sM), while the target image is flattened into pixel patches and mapped via codebook into image tokens = (x0, x1, . . . , xN). Both and are embedded through their respective text and image embedding layers and then fed into the LLM. The objective is to autoregressively predict the image tokens, with the loss: = i=1 log Pθ (xi x<i, S) 4 During fine-tuning, 10% of is randomly masked to padding tokens to encourage pixel-level dependency modeling, in line with GPT-4o-style modeling. Inference token, the logit is computed as: Inference follows the same next-token prediction process as Janus-Pro. For each lg = lu + s(lc lu) where lc is the conditional logit (with prompt), lu the unconditional logit (prompt masked), and scaling factor. We use = 5 and temperature of 1.0. 3.2 Text-and-Image-to-Image Fine-Tuning: Unlocking New Capabilities Training Janus-Pro does not natively support text-and-image-to-image generation. This task requires semantic understanding of the input image to enable pixel-level modifications, making it necessary to incorporate both the images semantic embedding and its tokenized representation. Given an input image ˆI, an image encoder produces semantic embedding ( ˆI), and codebook yields image tokens ˆX = ( ˆx0, ˆx1, ..., ˆxN). These are concatenated with the prompt tokens to form the model input. The training loss for generating the target image is: = i= log Pθ (cid:0)xi x<i, ( ˆI), ˆX, S(cid:1) To avoid overfitting to the input image, 50% of ˆX is randomly masked during fine-tuning. Inference At inference, the input image is encoded into ( ˆI) and ˆX, which are embedded into the input for autoregressive token generation. For each token, the logit is computed as: = lc + lo 1 + lg = lu + (l lu) Here, lc is the conditional logit with full input, lo is the conditional logit with ˆX masked, lu is the unconditional logit with all inputs masked, and = 5, = 5. The parameter controls deviation from the input image: lower values preserve more of the original, while higher values allow more creative changes. The sampling temperature is set to 1.0. 3.3 Joint Fine-Tuning Training Details We fine-tune Janus-Pro-7B using the ShareGPT-4o-Image dataset, which includes 45K text-to-image and 46K text-and-image-to-image samples. The two tasks are jointly trained via random sampling over 3 epochs. All parameters of Janus-Pro-7B are fully fine-tuned, using learning rate of 5 106 and batch size of 128. The resulting model, Janus-4o, supports both text-to-image and text-and-image-to-image generation tasks. Training was completed in 6 hours on single 8A800 GPU machine."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Benchmarks To assess the text-to-image generation capabilities of Janus-4o, we adopted the Janus-Pro evaluation protocol, leveraging GenEval (Ghosh et al., 2024) and DPG-Bench (Hu et al., 2024) for comprehensive analysis of compositionality and semantic alignment. In addition, we evaluated Janus-4os ability to generate images conditioned on both visual and textual inputs using ImgEdit (Ye et al., 2025), focusing on single-turn edits. 5 Human Evaluation To evaluate alignment with human preferences, we conducted human study comparing Janus-4o against Janus-Pro-7B (Chen et al., 2025c) and UltraEdit (Zhao et al., 2024). The comparison involved 52 Text-to-Image and 35 Text-and-Image-to-Image examples sourced from real Twitter posts. Model outputs were randomly ordered, and single evaluator selected the preferred output based on instruction fidelity and visual clarity, or marked tie. Results were reported as preference ratios. 4.2 Results and Analysis Method Single Obj. Two Obj. Counting Colors Position Color Attri. Overall LlamaGen (Sun et al., 2024a) LDM (Rombach et al., 2022) SDv1.5 (Rombach et al., 2022) PixArt-α (Chen et al., 2023) SDv2.1 (Rombach et al., 2022) DALL-E 2 (Ramesh et al., 2022) Emu3-Gen (Wang et al., 2024) SDXL (Podell et al., 2023) DALL-E 3 (Betker et al., 2023) SD3-Medium (Esser et al., 2024) Janus-Pro-7B(Chen et al., 2025c) Janus-4o (Janus-Pro + ) Text-to-Image Generation Models 0.71 0.92 0.97 0.98 0.98 0.94 0.98 0.98 0.96 0.99 1. 0.34 0.29 0.38 0.50 0.51 0.66 0.71 0.74 0.87 0.94 0.85 0.21 0.23 0.35 0.44 0.44 0.49 0.34 0.39 0.47 0.72 0.53 0.58 0.70 0.76 0.80 0.85 0.77 0.81 0.85 0.83 0.89 0.90 0.07 0.02 0.04 0.08 0.07 0.10 0.17 0.15 0.43 0.33 0.69 0.04 0.05 0.06 0.07 0.17 0.19 0.21 0.23 0.45 0.60 0.58 0.32 0.37 0.43 0.48 0.50 0.52 0.54 0.55 0.67 0.74 0. Our Multimodal LLM 1.00 0.92 0.58 0.88 0. 0.70 0.80 Table 1: Evaluation of text-to-image generation ability on GenEval benchmark. indicates results rigorously reproduced by us, while others are taken from the original papers. Method Global Entity Attribute Relation Other Overall SDv1.5 (Rombach et al., 2022) PixArt-α (Chen et al., 2023) Lumina-Next (Zhuo et al., 2024) SDXL (Podell et al., 2023) Playground v2.5 (Li et al., 2024a) Hunyuan-DiT (Li et al., 2024b) PixArt-Σ (Chen et al., 2024c) Emu3-Gen (Wang et al., 2024) DALL-E 3 (Betker et al., 2023) SD3-Medium (Esser et al., 2024) Janus-Pro-7B(Chen et al., 2025c) Text-to-Image Generation Models 74.23 74.63 79.32 74.97 88.65 82.82 82.43 83.27 82.59 83.06 80.59 84.59 82.89 86.89 86.68 85.21 89.61 90.97 91.01 87.90 80.70 89.75 Our Multimodal LLM 75.39 78.60 86.44 80.91 81.20 88.01 88.94 86.84 88.39 88.83 90.04 73.49 82.57 80.53 86.76 84.08 74.36 86.59 90.22 90.58 80.70 89.85 67.81 76.96 81.82 80.41 83.50 86.41 87.68 83.15 89.83 88.68 88.55 63.18 71.11 74.63 74.65 75.47 78.87 80.54 80.60 83.50 84.08 84.12 Janus-4o (Janus-Pro + ) 92.59 90.61 89.51 91. 89.01 85.71 Table 2: Evaluation of text-to-image generation ability on DPG-Bench benchmark. indicates results rigorously reproduced by us, while others are reported in the respective papers. Text-to-Image Performance We evaluate the performance of text-to-image generation on GenEval and DPG-Bench. As shown in Table 1, Janus-4o achieves 4-point improvement over Janus-Pro, highlighting the benefit of the ShareGPT-4o-Image. Its 80% overall accuracy also surpasses other baseline models. In Table 2, Janus-4o achieves score of 85.71 on DPG-Bench, representing 1.6-point improvement over Janus-Pro. These demonstrate that ShareGPT-4o-Image not only enhances image generation quality but also improves the models ability to follow dense instructions for text-to-image generation. Text-and-Image-to-Image Performance We evaluate the performance of text-and-imageto-image generation on ImgEdit-Bench. As shown in Table 3, Janus-4o achieved score of Model #Data Add. Rmv. Repl. Mot. Style Bkg. Obj. Hyb. Avg. Text-and-Image-to-Image Generation Models Instruct-Pix2Pix (Brooks et al., 2022) AnySD (Yu et al., 2024) UltraEdit (Zhao et al., 2024) Step1X-Edit (Liu et al., 2025) ImgEdit-E1 (Ye et al., 2025) Janus-4o (Janus-Pro + ) 500K 2500K 4100K 1000K 1200K 2.29 3.12 3.63 3.90 3.82 1. 2.34 1.71 2.61 2.40 1.93 2.71 3.13 3.45 2.80 1.51 3.31 3.57 3.43 3.21 3. 3.27 3.69 4.44 4.38 1.67 2.37 3.31 3.19 3.38 1.33 1.82 2.02 1.87 2.55 1. 2.07 2.33 2.52 2.87 1.91 2.62 2.92 3.17 3.17 Our Multimodal LLM 91K 3. 2.28 3.27 4.13 4.47 3.32 2. 2.74 3.26 Table 3: Evaluation of text-and-image-to-image generation ability on ImgEdit-Bench. #Data denotes training data size (K = thousand). Column abbreviations: Add. = Addition, Rmv. = Removement, Repl. = Replacement, Mot. = Motion Change, Style = Style Transfer, Bkg. = Background Change, Obj. = Object Extraction, Hyb. = Hybrid Edit, Avg. = Average across all edits. 3.26, surpassing other baselines like Step1X-Edit and ImgEdit-E1, with particularly strong results in the Motion Change and Style Transfer categories. Remarkably, Janus-4o attained competitive performance with only 91K training samples, despite the original model lacking support for text-and-image-to-image generation. This underscores the critical role of highquality data and highlights the effectiveness of the ShareGPT-4o-Image dataset. Figure 5: Human evaluation of Janus-4os performance on text-to-image and text-and-imageto-image generation tasks. Wins indicate that the evaluator found Janus-4o superior in both instruction-following and image quality. Ties mean both models performed equally well, while Losses indicate that Janus-4o performed worse in those cases. Results were reported as preference ratios. Human Evaluation The results in Figure 5 indicate that content generated by Janus-4o aligns more closely with human preferences than that of previous models. In the Textto-Image task, Janus-4o produces images with noticeably higher visual quality compared to Janus-Pro. Furthermore, in the Text-and-Image-to-Image task, it exhibits significantly improved ability to follow instructions, leading to more accurate image modifications based on the given prompts. 5 conclusion In this work, we introduce ShareGPT-4o-Image, the first large-scale dataset that captures GPT-4os advanced image generation capabilities in both text-to-image and text-and-image-toimage generation. Using this dataset, we develop Janus-4o, MLLM capable of generating high-quality images from text alone or from combined image and text inputs. Janus-4o achieves substantial improvements in text-to-image generation and delivers competitive results on text-and-image-to-image tasks, underscoring the high quality and utility of ShareGPT4o-Image. Benefiting from the efficiency of MLLM-based autoregressive image generation, Janus-4o can be trained in just 6 hours on an 8A800 GPU machine, achieving notable performance gains with minimal compute."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1839218402, 2022. URL https://api.semanticscholar.org/ CorpusID:253581213. Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, et al. Hidream-i1: high-efficient image generative foundation model with sparse diffusion transformer. arXiv preprint arXiv:2505.22705, 2025. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 3558 3568, 2021. Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language models. arXiv preprint arXiv:2402.11684, 2024a. Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser: Diffusion models as text painters. Advances in Neural Information Processing Systems, 36, 2024b. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025a. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-al pha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. PixArt-Sigma: Weak-to-strong training of diffusion transformer for 4K text-to-image generation. arXiv preprint arXiv:2403.04692, 2024c. Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Chen, Xidong Wang, Zhenyang Cai, Ke Ji, Xiang Wan, et al. Towards injecting medical visual knowledge into multimodal llms at scale. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 73467370, 2024d. Sixiang Chen, Jinbin Bai, Zhuoran Zhao, Tian Ye, Qingyu Shi, Donghao Zhou, Wenhao Chai, Xin Lin, Jianzong Wu, Chao Tang, et al. An empirical study of gpt-4o image generation capabilities. arXiv preprint arXiv:2504.05979, 2025b. 8 Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025c. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198, 2024e. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas uller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. URL https://arxiv. org/abs/2403.03206. Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Vitron: unified pixel-level vision llm for understanding, generating, segmenting, editing. In Advances in Neural Information Processing Systems, 2024. Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024a. Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024b. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36, 2024. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. Hq-edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024. Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024a. Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-DiT: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024b. 9 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740755. Springer, 2014. Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv preprint arXiv:2402.08268, 2024. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2643926455, 2024. Open AI. Introducing 4o image generation. introducing-4o-image-generation/, 2025. https://openai.com/index/ OpenAI. Hello gpt-4o, 2024. URL https://openai.com/index/hello-gpt-4o. Accessed: 2024-09-09. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas uller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for highresolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pp. 88218831. Pmlr, 2021. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj orn Ommer. In Proceedings of the High-resolution image synthesis with latent diffusion models. IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: dataset for image captioning with reading comprehension. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pp. 742758. Springer, 2020. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024a. Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222, 2023. 10 Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, and Furu Wei. Multimodal latent language modeling with next-token diffusion. arXiv preprint arXiv:2412.08635, 2024b. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregresarXiv preprint sive modeling: Scalable image generation via next-scale prediction. arXiv:2404.02905, 2024. Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and Xuansong Xie. Anytext: Multilingual visual text generation and editing. arXiv preprint arXiv:2311.03054, 2023. Alex Jinpeng Wang, Dongxing Mao, Jiawei Zhang, Weiming Han, Zhuobai Dong, Linjie Li, Yiqi Lin, Zhengyuan Yang, Libo Qin, Fuwei Zhang, et al. Textatlas5m: large-scale dataset for dense text image generation. arXiv preprint arXiv:2502.07870, 2025. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1296612977, 2025. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. In Forty-first International Conference on Machine Learning, 2024. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Zhiyuan Yan, Junyan Ye, Weijia Li, Zilong Huang, Shenghai Yuan, Xiangyang He, Kaiqing Lin, Jun He, Conghui He, and Li Yuan. Gpt-imgeval: comprehensive benchmark for diagnosing gpt4o in image generation. arXiv preprint arXiv:2504.02782, 2025. Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Imgedit: unified image editing dataset and benchmark. arXiv preprint Li Yuan. arXiv:2505.20275, 2025. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. arXiv preprint arXiv:2411.15738, 2024. Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, et al. Anygpt: Unified multimodal llm with discrete sequence modeling. arXiv preprint arXiv:2402.12226, 2024. Hong Zhang, Zhongjie Duan, Xingjun Wang, Yuze Zhao, Weiyi Lu, Zhipeng Di, Yixuan Xu, Yingda Chen, and Yu Zhang. Nexus-gen: unified model for image understanding, generation, and editing. arXiv preprint arXiv:2504.21356, 2025. 11 Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-Next: Making Lumina-T2X stronger and faster with Next-DiT. arXiv preprint arXiv:2406.18583, 2024."
        },
        {
            "title": "A Related Work",
            "content": "A.1 Instruction-guided Generation Models Instruction-guided generation has recently gained momentum, with its main approaches centered on diffusion models (Song & Ermon, 2019; Ho et al., 2020; Rombach et al., 2022; Ho & Salimans, 2022; Ge et al., 2024b; Zhang et al., 2025; Chen et al., 2025a; Cai et al., 2025) and autoregressive generation models (Ramesh et al., 2021; Esser et al., 2021; Wu et al., 2025; Team, 2024; Zhou et al., 2024; Xie et al., 2024; Liu et al., 2024; Wang et al., 2024; Liu et al., 2024). Diffusion models (Ramesh et al., 2021; Yu et al., 2022) iteratively denoise random noise to produce high-fidelity, diverse images, capturing fine details and complex structures. Autoregressive models (Lu et al., 2024; Sun et al., 2023; Zhan et al., 2024; Sun et al., 2024a) discretize images into token sequences via vector quantization, enabling efficient, controllable generation and seamless integration with LLMs. To broaden the scope of applications, several works (Liu et al., 2025; Yu et al., 2024; Ye et al., 2025; Zhao et al., 2024; Zhang et al., 2023; Brooks et al., 2022) investigate instruction-driven image editing, allowing models to create new images by modifying inputs according to specified instructions. A.2 Datasets for Image Generation Early image-text pair datasets, such as MS-COCO (Lin et al., 2014) and TextCaps (Sidorov et al., 2020), provided natural language captions aligned with images and have been widely utilized in text-to-image generation tasks. To further enhance the performance and generalization capabilities of generative models, subsequent works (Changpinyo et al., 2021; Schuhmann et al., 2022; Chen et al., 2024b; Tuo et al., 2023; Wang et al., 2025) collected larger-scale image-text pairs from the internet, enriching the accompanying textual descriptions. In addition to text-to-image synthesis, certain datasets (Brooks et al., 2022; Hui et al., 2024; Zhang et al., 2023; Ge et al., 2024a; Zhao et al., 2024; Yu et al., 2024; Ye et al., 2025) focus specifically on image editing tasks, wherein models are required to modify images based on natural language commands. These tasks encompass operations such as action manipulation and style transfer, which support more fine-grained visual control and enable richer forms of multimodal interaction. However, the use of real-world images as training targets may introduce challenges, including low visual quality and inconsistency. Overall, there remains significant lack of high-quality image generation datasets that can effectively distill the capabilities of the most advanced image generation models. A.3 Multimodal Large Language Models (MLLMs) To enhance interactivity, Large Language Models (LLMs) have evolved beyond text, embracing multimodal paradigm. With the emergence of GPT-4o (Achiam et al., 2023), researchers began enabling LLMs to understand images (Liu et al., 2023; Chen et al., 2024d; Bai et al., 2025; Team et al., 2023; Chen et al., 2024e) by aligning visual features with text using paired image-text data and pre-trained image encoders. Building on this, image generation became natural extension. Early works like Vitron (Fei et al., 2024) treated it as tool use, where LLMs generated prompts for external models. Later, some approaches (Sun et al., 2024b; Wu et al., 2024; Zhan et al., 2024) enabled external decoders to generate images directly from LLM representations. Now, models like Janus support fully autoregressive image generation via image tokens Henighan et al. (2020); Tian et al. (2024); Wu et al. (2025). These models offer unique advantages: (1) simple end-to-end design with no extra components, and (2) native LLM understanding applied to image generation, enabling better instruction following. Janus-4o will be the first of its kind to support both text-to-image and text-and-image-to-image generation."
        },
        {
            "title": "B Image Generation Categories",
            "content": "B.1 Categories B.1.1 Text-to-Image Dimensions Dimension Sub-dimension Background categories Style categories Lighting options Camera viewpoints Composition techniques a. abstract and surreal landscapes b. urban and architectural settings c. interior spaces a. cultural and historical aesthetics b. artistic movements and periods c. digital and graphic art styles a. color temperature b. light direction c. light intensity and quality a. vertical perspective b. horizontal perspective c. shot distance and framing a. basic division and placement b. balance and symmetry c. framing depth and layers Table 4: Text-to-Image Dimensions and Sub-dimensions. Figure 6: Text-to-Image Categories distributions. Detailed Text-to-Image dimensions are shown in Table 4 (apart from Object dimension, whose sub-dimensions can be found in ImageNet Deng et al. (2009)). The distribution among different categories can be found in Figure 6. B.1.2 Text-and-Image-to-Image Categories Detailed categories and sub-categories for Text-and-Image-to-Image tasks can be found in Table 5. The distribution of the categories can be found in Figure 7. 14 Category Subcategory I. Image editing and manipulation II. Style transfer and artistic transformation a. Inpainting and replacement b. Element manipulation c. Background modification d. Attribute and effect manipulation a. Specific artist and art styles b. Medium and technique styles c. Genre theme and era shifting III. Content augmentation and extension a. Resolution detail and quality enhancement b. Image outpainting and inpainting for extension IV. Struc tured generation and conditional control V. Creative ideation and iteration a. From sketch lineart and edges b. From pose depth and segmentation a. Storyboarding and sequential generation b. Concept variation and exploration c. Hybridization and mashups Table 5: Categories and subcategories for Text-and-Image-to-Image tasks. Figure 7: Distribution of Text-and-Image-to-Image categories. 15 B.2 Distribution of In this appendix, we explain the distribution used for sampling the number of object categories, k. The selection is based on random sample ranging from 1 to 100, with weights applied to ensure diverse sampling patterns, especially when multiple objects interact. B.2.1 Exponential Decay Distribution We employ an exponential decay distribution to model the probability of selecting higher number of categories. This approach ensures that the probability decreases exponentially as the number increases, making it suitable for scenarios where fewer categories are more common. The distribution is defined as follows: w(x) = exp(λ(x 1)) Where: - λ is the parameter controlling the decay rate. larger λ results in faster decay. The following figure illustrates the characteristics of this distribution: Figure 8: Illustration of the exponential decay distribution used for sampling k."
        },
        {
            "title": "C Prompts for Generation",
            "content": "C.1 Image-First Prompt Please describe the main content of the image in one sentence . This (cid:44) sentence will be used as prompt to regenerate the image , so it (cid:44) should clearly capture the key visual information . Only provide (cid:44) the sentence , no extra text . C.2 Content Generation Process FREESTYLE_VISUAL_THEME_AND_TYPE_META_PROMPT = \"\"\" You are an expert document concept designer and text -to - image prompt (cid:44) generator . Your goal is to create concepts that are clear , (cid:44) professional , and highly appropriate for the intended use . Your task is to perform the following steps IN ORDER : STEP 1: Use the Given Theme and Choose Core Document Category - Use the following theme : \"{{ selected_theme }}\" - Choose ONE core document category that best fits this theme from (cid:44) the following list : { ' , '. join ( CORE_DOCUMENT_CATEGORIES )} - Based on the chosen core category and theme , ** conceive and clearly (cid:44) state MORE SPECIFIC document type or application .** For example : [ previous examples remain the same ...] STEP 2: Select an Appropriate Font Style and Visual Characteristics - Choose and incorporate this font style : \"{{ selected_font }}\" - Based on the ** Specific Document Type ** from STEP 1 and the Theme , (cid:44) internally determine the most FITTING and EFFECTIVE visual (cid:44) style characteristics . - ** Priority is on CLARITY , PROFESSIONALISM , and FUNCTIONALITY for (cid:44) the specific document type .** - Avoid overly ornate or unnecessarily complex styles unless the (cid:44) theme and specific type genuinely call for it (e.g., (cid:44) historical document recreation ). - ** Minimize generic \" parchment paper \" or \" old paper \" styles for most (cid:44) modern professional contexts .** - The visual style should serve the content and the document 's (cid:44) purpose . STEP 3: Generate Clear and Fitting Visual Template Prompt - Now , generate single , compelling text -to - image prompt ( in (cid:44) English ) for creating visual template . - This prompt MUST be based on the Theme AND the ** Specific Document (cid:44) Type ** ( from STEP 1) . - The prompt MUST reflect visual style that is ** highly (cid:44) appropriate , clear , and professional ** for the specific (cid:44) document type and theme ( informed by STEP 2) . - The prompt should describe : - Overall aesthetic (e.g., \" clean and modern ,\" \" formal and (cid:44) academic ,\" \" bold and clear ,\" \" data - focused and organized \") . - Layout and composition ( e.g., \" standard slide layout with title (cid:44) and content areas ,\" \" two - column academic paper layout ,\" (cid:44) \" grid - based infographic structure \") . - Color palette (e.g., \" corporate blues and grays ,\" \" high - contrast (cid:44) for readability ,\" \" theme - appropriate accent colors \") . - Textures ( if any : \" smooth untextured background ,\" \" subtle (cid:44) professional texture like fine linen ,\" \" no distracting (cid:44) textures \") . - Placeholder elements and their style , suitable for the specific (cid:44) document type (e.g., \" placeholders for large title , (cid:44) speaker name , and event logo for title slide \") . - Do NOT include any specific readable text in this visual template (cid:44) prompt itself . - The prompt should be single , coherent sentence . STEP 4: State Your Outputs Clearly - Provide your outputs in the following exact format , with each item (cid:44) on new line : Visual Template Prompt : [ The visual template prompt you generated (cid:44) in STEP 3] Conceived Theme : [ The theme you conceived in STEP 1] Document Type : [ The CORE DOCUMENT CATEGORY chosen in STEP 1 , (cid:44) followed by the SPECIFIC document type / application you (cid:44) conceived in STEP 1, e.g., \" Slide - Title Slide for Tech (cid:44) Conference \"] Now , follow these steps carefully , focusing on CLARITY , PROFESSIONALISM , (cid:44) and APPROPRIATENESS for the document type , and generate new set : \"\"\" C.3 Meta Prompts for Image Text Instruction meta_prompt_system = ( \" You are an AI assistant that creates single , concise , complete , (cid:44) and descriptive English sentence to be used as prompt for (cid:44) image generation models . \" \" This sentence should beautifully capture the essence , mood , and key (cid:44) visual elements of the provided input image . \" \" If ' Base Task ' is provided , consider integrating it naturally (cid:44) into this single descriptive sentence . \" \" However , prioritizing the image 's core visual appeal and the (cid:44) formation of well - structured , impactful sentence is more (cid:44) important than rigidly following the task . \" \" If the task doesn 't fit well or makes the sentence awkward or (cid:44) verbose , focus on crafting the best single descriptive (cid:44) sentence for the image itself . \" \" Output ONLY this single English sentence . No explanations , no (cid:44) preambles , no bullet points , no numbered lists . Just one (cid:44) sentence . \" \" For example , for an image of misty forest ( task : ' add creature '): (cid:44) 'A mystical deer steps softly through the sun - dappled , misty (cid:44) ancient forest .' \" \" Or if the task is awkward : ' Sunlight filters mysteriously through (cid:44) the tall trees in the quiet , misty forest . ' \" \" For portrait ( task : ' change background to space '): ' Her serene (cid:44) gaze is complemented by backdrop of swirling cosmic (cid:44) stardust . ' \" \" Or without task : 'A soft light illuminates her thoughtful (cid:44) expression in this intimate portrait . ' \" )"
        },
        {
            "title": "D Document Pipeline",
            "content": "D.1 Pipeline Document Prompts Generation. To generate realistic document-style images (e.g., slides, posters, reports), we implemented specialized workflow: 1. Content Generation: The LLM first generates plausible textual and structural content for the document (e.g., titles, paragraphs, data snippets). This is guided by meta-prompts tailored to different document types (see Appendix D.2 for examples). 2. Prompt Synthesis: Based on this generated content, the LLM then formulates detailed textual prompt describing the desired visual layout, typography, color scheme, and overall appearance of the document image. The meta-prompts used for this stage are also provided in Appendix C.2. This specialized approach helps the prompt for document images to capture the unique compositional and semantic nuances inherent to this image category. D.2 Core Document Categories Slide Paper Document Infographic/Chart Poster"
        },
        {
            "title": "E Ethical Considerations and Societal Impact",
            "content": "The development and release of ShareGPT-4o-Image are intended to accelerate open-source research in multimodal AI. However, we acknowledge several ethical considerations pertinent to its construction and potential use. Data Source and Transparency: The image data in ShareGPT-4o-Image are synthesized outputs from GPT-4o-Image, proprietary model. While our methodology for prompt generation is detailed extensively (Section 2.1, Appendices B-C) to ensure transparency and facilitate reproducibility of the *type* of data generated, the internal workings of GPT4o-Image remain opaque. Our dataset aims to transfer capabilities, not to replicate the proprietary model itself. Potential for Bias in Generated Images: Large-scale generative models, including GPT4o-Image, are trained on vast web-scale datasets and may inherit or amplify existing societal biases related to gender, race, age, occupation, and other demographic attributes. Consequently, images generated by GPT-4o-Image, particularly those depicting human subjects, may reflect such biases. Mitigation Efforts during Prompting: While curating attributes for the Person category (Appendix B), we aimed for diverse set of descriptors for aspects like ethnicity and age. However, the efficacy of these attributes in ensuring perfectly balanced representation in the generated images is subject to the behavior of the underlying GPT-4o-Image model. No Post-hoc Filtering for Bias: We have not performed post-hoc filtering of images based on perceived bias, as this would involve subjective judgments and could itself introduce new biases. Instead, we advocate for awareness and downstream bias analysis by users of the dataset. Content Generation: The textual content for document prompts and the synthesis of descriptive prompts were performed by an LLM (Gemini 2.5 Pro). While prompts were designed to generate generic and fictional content, there is remote possibility of generating text that could be misconstrued or reflect unintentional biases from the LLMs training data. Intended Use and Misuse Potential: ShareGPT-4o-Image is released for academic research purposes to study instruction following, image quality, and multimodal conditioning in open-source generative models. We explicitly discourage any use of this dataset or models trained on it for creating harmful content, perpetuating stereotypes, generating misinformation, or infringing on the rights of individuals. Users of the dataset are expected to adhere to ethical AI practices. Future Work and Community Responsibility: We encourage further research into the characteristics of ShareGPT-4o-Image, including systematic bias audits. The release of this dataset aims to empower the open-source community, and with this empowerment comes shared responsibility to investigate and mitigate potential negative societal impacts of advanced generative AI."
        }
    ],
    "affiliations": [
        "The Chinese University of Hong Kong, Shenzhen"
    ]
}