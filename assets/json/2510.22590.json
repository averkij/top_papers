{
    "paper_title": "ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs",
    "authors": [
        "Yassir Lairgi",
        "Ludovic Moncla",
        "Khalid Benabdeslem",
        "Rémy Cazabet",
        "Pierre Cléau"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In today's rapidly expanding data landscape, knowledge extraction from unstructured text is vital for real-time analytics, temporal inference, and dynamic memory frameworks. However, traditional static knowledge graph (KG) construction often overlooks the dynamic and time-sensitive nature of real-world data, limiting adaptability to continuous changes. Moreover, recent zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance on prebuilt ontologies often suffer from instability across multiple runs, as well as incomplete coverage of key facts. To address these challenges, we introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that builds and continuously updates Temporal Knowledge Graphs (TKGs) from unstructured texts. ATOM splits input documents into minimal, self-contained \"atomic\" facts, improving extraction exhaustivity and stability. Then, it constructs atomic TKGs from these facts while employing a dual-time modeling that distinguishes when information is observed from when it is valid. The resulting atomic TKGs are subsequently merged in parallel. Empirical evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17% better stability, and over 90% latency reduction compared to baseline methods, demonstrating a strong scalability potential for dynamic TKG construction."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 0 9 5 2 2 . 0 1 5 2 : r ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs Yassir LAIRGI1,2, Ludovic MONCLA1, Khalid BENABDESLEM1, Rémy CAZABET1, and Pierre CLÉAU2 1LIRIS, INSA Lyon, Université Claude Bernard Lyon 1, France 2GAUC, Lyon, France {ludovic.moncla, khalid.benabdeslem, remy.cazabet}@liris.cnrs.fr {yassir.lairgi, pierre.cleau}@auvalie.com"
        },
        {
            "title": "Abstract",
            "content": "In todays rapidly expanding data landscape, knowledge extraction from unstructured text is vital for real-time analytics, temporal inference, and dynamic memory frameworks. However, traditional static knowledge graph (KG) construction often overlooks the dynamic and time-sensitive nature of real-world data, limiting adaptability to continuous changes. Moreover, recent zeroor few-shot approaches that avoid domain-specific fine-tuning or reliance on prebuilt ontologies often suffer from instability across multiple runs, as well as incomplete coverage of key facts. To address these challenges, we introduce ATOM1 (AdapTive and OptiMized), few-shot and scalable approach that builds and continuously updates Temporal Knowledge Graphs (TKGs) from unstructured texts. ATOM splits input documents into minimal, self-contained atomic facts, improving extraction exhaustivity and stability. Then, it constructs atomic TKGs from these facts while employing dual-time modeling that distinguishes when information is observed from when it is valid. The resulting atomic TKGs are subsequently merged in parallel. Empirical evaluations demonstrate that ATOM achieves 18% higher exhaustivity, 17% better stability, and over 90% latency reduction compared to baseline methods, demonstrating strong scalability potential for dynamic TKG construction."
        },
        {
            "title": "Introduction",
            "content": "Unstructured data is expanding at an unprecedented rate (Dresp-Langley et al., 2019), and given that the majority of big data is inherently unstructured (Trugenberger, 2015), there is an urgent need for robust information extraction and data modeling techniques to unlock its potential and derive insights across broad spectrum of applications (Cetera 1The code, prompts, and dataset are available at https: //github.com/AuvaLab/itext2kg. ATOM is available as an open-source Python library. et al., 2022). prominent model for converting this unstructured data into structured, actionable knowledge is the Knowledge Graph (KG) (Zhong et al., 2023). KG construction involves identifying entities, relationships, and attributes from diverse data sources to create structured knowledge representations. Traditionally, many approaches have focused on static Knowledge Graphs (KGs), which provide snapshots of knowledge without incorporating temporal dynamics. However, as real-world phenomena are inherently dynamic, static KGs, rarely or never updated, struggle to remain relevant and accurate (Jiang et al., 2023). In contrast, Temporal Knowledge Graphs (TKGs) integrate time dimensions by associating timestamps or time intervals with facts (e.g., (Einstein, was awarded, the Nobel Prize, in 1921)), making them particularly wellsuited for analyzing changes, trends, and enabling temporal reasoning. GraphRAG (Edge et al., 2024) and agent-based architectures (Xi et al., 2023) have demonstrated the potential of TKGs in retrieving and modeling dynamic information (Wu et al., 2024). Additionally, TKGs have been effectively used to model the memory of agents within agentic systems (Anokhin et al., 2024), highlighting their role in capturing the evolving nature of knowledge for adaptive and responsive systems. Traditional methods for KG construction, often reliant on entity recognition and relation extraction, face several limitations. They typically depend on predefined ontologies and supervised learning techniques that require extensive human annotation (Al-Moslmi et al., 2020). Recent advances in Large Language Models (LLMs) (Jin et al., 2023) and zeroor few-shot techniques (Zhang et al., 2024; Carta et al., 2023; Hu et al., 2023) have paved the way for more flexible KG construction approaches that reduce dependency on extensive training datasets. Despite these advances, current zeroor fewshot methods for KG construction often suffer from several limitations. They can be non-exhaustive, omitting key relationships, and prone to instability, where multiple construction runs on the same text yield different results. Moreover, many of these approaches overlook the temporal dimension of the input data and struggle to adapt to real-life scenarios with dynamic, evolving data, leading to false positives and lack of scalability (Cai et al., 2024). In this paper, we propose ATOM (AdapTive and OptiMized), few-shot and scalable dynamic TKG construction approach from unstructured text, ensuring stability and exhaustivity. ATOM introduces strategy that decomposes unstructured text into atomic facts. Rather than processing these atomic facts sequentially, ATOM proposes an architecture with parallel 5-tuple extraction, followed by parallel atomic merging mechanism. In the rest of the paper, we present related work in Section 2, our proposed approach ATOM in Section 3, experimental evaluation in Section 4, conclusion in Section 5, and present the limitations in Section 6."
        },
        {
            "title": "2 Related work",
            "content": "Current zeroand few-shot approaches to KG construction, such as AttacKG+ (Zhang et al., 2024), iterative LLM prompting pipelines (Carta et al., 2023), LLM-Tikg (Hu et al., 2023), LLM Builder 2, and LLM Graph Transformer3 aim to build KGs without requiring task-specific training. However, these methods suffer from inconsistencies such as unresolved entities and relations. That is why iText2KG (Lairgi et al., 2024) introduces an incremental, zero-shot architecture that constructs KGs iteratively by comparing newly extracted entities and relations with existing ones using embeddings and cosine similarity, achieving performance gains over some state-of-the-art LLM-based methods. However, it produces nonexhaustive and non-stable KGs due to the stochastic nature of LLMs (Atil et al., 2024). Moreover, it fails to incorporate the temporal dimension, and scalability remains significant challenge when applying it to real-world scenarios due to its incremental nature. Graphiti (Rasmussen et al., 2025) proposed 2https://llm-graph-builder.neo4jlabs.com/ 3https://python.langchain.com/docs/how_to/ graph_constructing/ dynamic TKG construction approach for agents memory with an exclusively LLM-based entity/relation and temporal resolution framework. key limitation is that it relies solely on prompting the LLM across all its modules, making it heavily dependent on LLM calls. The system prompts the LLM with all previous entities for entity resolution, which becomes impractical as the graph scales to millions of nodes. Similarly, time conflicts are resolved exclusively through LLM calls, resulting in high computational costs and scalability challenges when applied to large-scale datasets. Moreover, they do not handle the exhaustivity and stability of the constructed TKGs. AriGraph (Anokhin et al., 2024) integrated semantic and episodic memories to support reasoning, planning, and decision-making in LLM agents. However, their entity resolution method leads to semantic drift in the temporal KG, where, for example, reference to Apple might ambiguously denote either the company or the fruit. Furthermore, scalability becomes problematic as the volume of unstructured data increases. Despite these advances, current zeroand fewshot TKG construction methods face three key limitations: (1) they struggle to maintain exhaustive fact coverage when processing longer texts, (2) they often produce unstable TKGs across multiple runs, and (3) they lack scalable architectures for dynamic temporal updates. To address these challenges, we propose ATOM, framework that combines atomic fact decomposition for exhaustive and stable extraction and parallel merging for scalability."
        },
        {
            "title": "3 Proposed approach: ATOM",
            "content": "In this section, we first present some notations and definitions used throughout the paper and then introduce the formulation of our proposed framework."
        },
        {
            "title": "3.1 Problem statement",
            "content": "ATOM incorporates dual-time modeling, differentiating between when facts are observed and the temporal information conveyed by the facts themselves, which is characterized by validity period. This approach better reflects real-world data (Rasmussen et al., 2025; Chekol and Stuckenschmidt, 2018; Meijer, 2022). This separation ensures TKG dynamism and proper inference of relative times by providing the observation time as context to the LLM (eg, month ago). Figure 1: ATOMs architecture, running in parallel, ensuring scalability, speed, and continuous updates. Unstructured texts observed at time are denoted by Dt, the i-th temporal atomic fact observed at time is denoted by ft,i, the i-th atomic TKG observed at time is denoted by Gt , the TKG snapshot observed at time is denoted by Gt s, and the updated TKG at time is denoted by Gt. Definition 1 (Dynamic-Temporal KG with Dual Time Modeling). Let Tobs be an ordered set of observation timestamps at which the KG is updated, and let Tstart, Tend be sets of timestamps used to label inherent validity period of facts defined by their start and end times, respectively. For each observation time Tobs, TKG snapshot is defined as: same fact. Validity start and end timestamps can be unknown, in which case their respective lists are empty (denoted as [.]). Dynamic Temporal Knowledge Graph (DTKG), updated at t, is defined as the parallel pairwise merge of these TKG snapshots via the merge operator (as described later in Section 3.2.3). (cid:16) Gt = where: t, Rt, start, end, t(cid:17) (1) Gt = (cid:77) Gt = Gt1 Gt (2) tTobs={...,t1,t} is the set of entities known at observation time t, Rt is the set of relations known at observation time t, start Tstart is the set of validity start times referenced by facts in this snapshot, end Tend is the set of validity end times referenced by facts in this snapshot, t Rt t end is the set of temporal facts (5-tuples) observed in the snapshot at observation time t. start Definition 2 (Temporal Atomic Fact with Dual Time Modeling). Let Tobs be set of observation timestamps at which new data is ingested, and let Tstart and Tend be the sets of validity periods mentioned within the data. For each observation time Tobs, let Dt be an unstructured text that becomes available at t. temporal atomic fact ft,i is short, self-contained snippet derived from Dt that conveys exactly one piece of information, extracted by the LLM. Depending on the content of the snippet, an atomic fact may or may not explicitly contain validity period. Formally, ExtractAFactsLLM(Dt) = {ft,1, . . . , ft,mt} (3) fact in this snapshot is 5-tuple (quintuple ) (es, rp, eo, tstart, tend) indicating the relation rp Rt holds between the subject entity es and the object entity eo t. Technically, tstart and tend are chosen to be lists to aggregate start and end validity timestamps to track the history of the An example is provided in Section in the Appendices. In what follows, the term atomic fact is used for conciseness. Definition 3 (Atomic Temporal KG). Given an atomic fact ft,i observed at time t, its atomic temporal KG Gt is the set of 5-tuples extracted by the LLM: Gt = ExtractQuintuplesLLM(ft,i) P(cid:0)E Rt t start end (cid:1) (4) Gt Concretely, 5-tuples (es, rp, eo, tstart, tend) derived from single atomic fact ft,i at observation time t. the set of is Given the definitions 1 and 3, the DTKG, updated at t: Gt = (cid:77) Gt = (cid:77) tTobs tTobs ( (cid:77) 1,mt (cid:74) (cid:75) Gt ) (5) Figure F.1 in the Appendices illustrates detailed example of ATOMs pipeline. 3.2 ATOMs Framework Given continuous stream of unstructured texts, our goal is to construct and maintain consistent and dynamic TKG, ensuring for each Tobs: (C1) Exhaustivity: the constructed TKG snapshot ideally captures every 5-tuple that is seGt mantically present in Dt. (C2) Stability across multiple runs: when the identical 5-tuple extraction prompt is executed repeatedly on the same input text using the same LLM, the resulting TKG snapshots should be nearly identical. In the following, we detail the different modules of ATOMs architecture (Figure 1)."
        },
        {
            "title": "3.2.1 Module-1: Atomic fact decomposition\nATOM does not construct TKGs directly from raw\ninput documents but first decomposes them into\natomic facts (Figure 1). This decomposition ad-\ndresses a fundamental limitation of LLMs:\nthe\n\"forgetting effect\" where models prioritize salient\ninformation in longer contexts while omitting key\nrelationships, leading to incomplete knowledge ex-\ntraction (Liu et al., 2023). Following (Hosseini\net al., 2024; Chen et al., 2023; Raina and Gales,\n2024), ATOM uses LLM-based prompting for de-\ncomposition with an optimal chunk size to main-\ntain high exhaustivity (determined experimentally\nin Section 4.3). However, while prior work focused\non information retrieval applications, ATOM applies\natomic decomposition specifically for TKG con-\nstruction. This strategy addresses both conditions:",
            "content": "it ensures exhaustivity (C1) by preventing information loss that occurs when LLMs process complex, multi-fact paragraphs, and enhances stability (C2) by providing clear, unambiguous contexts that reduce output variance across multiple runs. Each atomic fact is related to an observation time, and it is necessary to encapsulate the relative validity period presented in the context. The primary computational challenge of this approach is scale: single document can yield hundreds or thousands of atomic facts. Sequential processing of each fact for 5-tuple extraction, followed by entity/relation and temporal resolution, becomes time-consuming. To address this challenge, ATOM employs parallel architecture for both extraction and merging phases, as detailed in the subsequent modules. 3.2.2 Module-2: Atomic TKGs construction 5-tuples are extracted from each atomic fact in parallel using an LLM, producing atomic TKGs Gt while embedding nodes and relations following (Lairgi et al., 2024). To facilitate temporal resolution in Module-3, ATOM preprocesses 5-tuples during their extraction. It prevents separate quintuples describing the same temporal fact from coexisting in the same TKG such as (John_Doe, and (John_Doe, is_ceo, X, [01-01-2025], [.]) is_no_longer_ceo, X, [01-01-2026], [.]), which should be resolved into (John_Doe, is_ceo, X, [0101-2025], [01-01-2026]). During the extraction, few-shot examples are provided as context to the LLM to transform end validity facts into affirmative counterparts while modifying only the tend time. For instance, the statement John Doe is no longer the CEO of on 01-01-2026 is converted into the 5-tuple (John_Doe, is_ceo, X, [.], [01-01-2026]), ensuring direct matching with the corresponding validity start time 5-tuple during the merge. For relative temporal expressions (e.g., month ago), the observation time is provided as context, enabling the LLM to infer the validity period."
        },
        {
            "title": "TKGs and DTKG update",
            "content": "ATOM then employs the binary merge algorithm (Algorithm A.1 in the Appendices) to merge pairs of atomic TKGs. The algorithm proceeds in three phases: first, entity resolution searches for exact and Gt matches between Gt i+1 based on name and label. When no exact match exists, cosine similarity is computed, merging entities if similarity exceeds θE. Second, relation resolution merges relation names regardless of endpoints and timestamps (e.g., owns possesses has) using threshold θR. Third, temporal resolution merges observation and validity time sets for relations with similar (es, rp, eo), detecting and aligning end-action facts with their corresponding beginning facts. Unlike Graphiti, ATOM avoids LLM calls during merging, improving scalability and preventing context overflow as the graph expands. The preprocessing of end-actions during extraction enables this LLMindependent merging approach. Subsequently, the binary merge function is extended to handle the entire set of atomic TKGs through iterative pairwise merging in parallel until single consolidated TKG is obtained (Algorithm A.2 in the Appendices). Atomic TKGs are organized into pairs, with each pair merged in parallel. If the number of TKGs is odd, the remaining TKG carries forward to the next iteration. This process continues iteratively, reducing the number of TKGs at each step, until convergence to single merged TKG. This parallel strategy scales with the number of available threads and addresses the computational challenge from Module-1, enabling ATOM to maintain low latency while preserving the exhaustivity and stability benefits of atomic decomposition. After the merge of all atomic TKGs, the snapshot Gt is obtained, and it is merged with the previous DTKG Gt1 using Algorithm A.1 to yield the DTKG updated at t, Gt."
        },
        {
            "title": "4 Experiments",
            "content": "Our evaluation addresses the following research questions: RQ1: How does exhaustivity deteriorate as the LLM context increases, and what degree of information loss could occur? RQ2: How does ATOMs atomic fact decomposition ensure stability, exhaustivity, and improve the quality of the 5-tuples? 4.1 Metrics To assess (C1), we adopt the metrics Hallucination (HALL), Omission (OM), and Match (MATCH) introduced by Ghanem and Cruz (2024). Given gold-standard KG, MATCH denotes correctly extracted triplets, OM refers to triplets that exist in the source but are missing from the extracted KG, and HALL represents unsupported triplets. These definitions are extended from factual triplets (es, rp, eo) to temporal 5-tuples (es, rp, eo, tstart, tend). 5tuple may exhibit correctness at the factual level while introducing temporal hallucination or omission. Hence, for 5-tuples whose factual components match the gold standard, the following definitions are established: MATCHt: 5-tuples whose (tstart, tend) is also correct. HALLt: 5-tuples containing temporal values not present in the gold standard. OMt: 5-tuples whose the (tstart, tend) are in the gold standard but are not reproduced in the extraction. Exhaustivity (RQ1 & RQ2). The exhaustivity of the factual component is measured as: RMATCH = MATCH MATCH + OM (6) The exhaustivity of the temporal component RMATCHt is computed analogously with MATCHt in the numerator. 5-tuples Quality (RQ2). It is assessed through the factual/temporal hallucination and omission rates: RHALL = HALL MATCH + HALL ROM = OM MATCH + OM (7) (8) RQ3: How does ATOM scale with the number of atomic facts provided as input, and what is its time complexity compared to baseline methods? RQ4: How does ATOM perform on DTKG construction consistency compared to baseline methods? The ROMt is computed similarly to ROM with OMt in the numerator. And RHALLt is simply RMATCH - RMATCHt - ROMt. Stability (RQ2). It is measured using the cosine similarity between the centroid embeddings of 5tuple sets obtained across independent runs. Let c(1) denote the centroid vector obtained during the baseline run (RUN 1) and let c(r) denote the centroid obtained at repetition r. Formally, the stability score is computed as: Sr = cos(cid:0)c(1), c(r)(cid:1) = (cid:10)c(1), c(r)(cid:11) c(1) c(r) (9) This score is computed for = 2, 3. Time complexity (RQ3). Time complexity is evaluated by progressively increasing the number of atomic facts provided as input and measuring the total wall-clock latency required to construct the complete DTKG. DTKG consistency (RQ4). For entity/relation resolution, the false discovery rate (1-precision) is defined in (Lairgi et al., 2024). Since they overlook recall and F1-score, we extend the evaluation to include precision, recall, and F1-score for both entity resolution (ER) and relation resolution (RR), denoted as Metric-ER and Metric-RR, respectively. For temporal resolution, qualitative comparison is provided."
        },
        {
            "title": "4.2 Datasets and baseline methods",
            "content": "DocRed (Yao et al., 2019) is unsuitable for temporal extraction due to inconsistencies (Tan et al., 2022), and TempDocRed (Zhu et al., 2025), though temporally enriched, focuses only on event start dates and named entities. CS-GS and Music-GS (Kabal et al., 2024), also used in iText2KG evaluations, lack temporal data. Therefore, the NYT News dynamic and temporal dataset (Singh, 2023), containing lead paragraphs from two million news articles since 2000, is adopted. From this, the 2020-COVID-NYT subset comprising 1,076 articles that focus on COVID-19 dynamics during 2020 is extracted. This subset is enriched with human-verified atomic facts and 5-tuples. Publication dates are used as observation dates (details on observation time modeling in Section in the Appendices). More information about this dataset is provided in Table T.1 in the Appendices. To the best of our knowledge, an approach for resolving duplicate entities and relations while maintaining KG consistency among the SOTA methods for zeroand few-shot KG construction is supported only by iText2KG and Graphiti. The temporal aspect is handled by Graphiti only. Hence, it is the primary comparator of ATOM. In all experiments, the temperature is set to 0 to reduce hallucinations. text-embedding-large-34 is used for embeddings. θE = 0.8 and θR = 0.7 are estimated following (Lairgi et al., 2024) (details are in Section in the Appendices). 4.3 Exhaustivity deterioration in longer contexts is lead evaluated by paragraphs Exhaustivity iteratively concatenating (increasing context size) and testing five SOTA claude-sonnet-4-2025-01-315, LLMs mistral-large-24117, gpt-4o-2024-11-206, gpt-4.1-2025-04-148, o3-mini-2025-01-319. Figure 2 shows clear \"forgetting effect\". decreased factual and temporal exhaustivity as token count increases across all models except claude-Sonnet-4-2025-01-31, which maintains the highest exhaustivity for atomic facts but degrades for 5-tuples. This indicates that LLMs prioritize salient facts in longer texts. Moreover, all models show higher exhaustivity for atomic fact decomposition than 5-tuple extraction. Atomic facts require surface-level parsing, while 5-tuples demand deeper semantic understanding of entities, relationship identification, and temporal extraction (tstart, tend). This added complexity causes greater information loss. To mitigate information loss in the atomic fact decomposition, we empirically determine the optimal chunk size at < 400 tokens to keep the exhaustivity > 0.8. For subsequent evaluations, we use claude-sonnet-4-2025-01-31 for atomic fact decomposition based on its superior performance. For 5-tuple extraction, both gpt-4.1-2025-04-14 and perform comparably, hence gpt-4.1-2025-04-14 is used due to its lower cost. Section 4.5 evaluates exhaustivity gains from using atomic facts as input. claude-sonnet-4-2025-01-"
        },
        {
            "title": "4.4 TKG stability",
            "content": "Rerunning the same prompt multiple times leads to variations in output. To evaluate the stability score, 5-tuples are initially extracted from both atomic 4https://platform.openai.com/docs/models/ text-embedding-3-large 5https://docs.claude.com/en/docs/about-claude/ models/overview 6https://platform.openai.com/docs/models/ gpt-4o-2024-11-20 7https://docs.mistral.ai/getting-started/ models/models_overview/ 8https://openai.com/index/gpt-4-1/ 9https://openai.com/index/openai-o3-mini/ 4.5 The exhaustivity and quality of the 5-tuples 5-tuples are extracted from both atomic facts (denoted as (F) in Table 2) and lead paragraphs (L). As shown in Table 2, higher values are maintained by the RMATCH and RMATCHt rates when atomic facts are provided as input compared to when lead paragraphs are used, with gain of 18% achieved on temporal exhaustivity and 31% on factual exhaustivity, along with gain of approximately 31% on factual omission. Hence, atomic fact decomposition improves temporal and factual exhaustivity, and this addresses (C1). However, an increase in RHALL by 9% is observed, which is attributed to the addition of inferred atomic facts from the lead paragraphs by the LLM during the atomic fact decomposition. Consequently, the LLM extracts 5-tuples that do not exist in the gold standard. Furthermore, higher ROMt is observed, which is attributed to imperfections that may occur during atomic fact decomposition, where temporal information may not be assigned to atomic facts by the LLM. This error propagates to 5-tuples extraction in ROMt. It is trade-off that is discussed further in Section 6. Table 2: 5-tuple quality metrics. The extraction is performed on (F) atomic facts and (L) lead paragraphs. Metric RMATCH RMATCHt ROMt RHALLt ROM RHALL 2020-COVID-NYT (L) 2020-COVID-NYT (F) 0.405 0.150 0.176 0.123 0.229 0.131 0.000 0.000 0.595 0.150 0.333 0.172 0.720 0.143 0.354 0.165 0.366 0.158 0.000 0.000 0.280 0.143 0.428 0. 4.6 ATOMs time complexity Given the demonstrated benefits of atomic fact decomposition in improving exhaustivity and stability, all subsequent experiments utilize atomic facts as input rather than lead paragraphs. All baseline methods are run using gpt-4.1-2025-04-14. ATOM employs 8 threads and batch size of 40 atomic facts for 5-tuples extraction, which respects OpenAI rate limits. iText2KG and Graphiti separate entity and relation extraction, increasing latency. Graphitis incremental entity/relation resolution, which relies on the LLM, limits parallel requests and significantly increases latency as the graph expands. Similarly, iText2KG is incremental, restricting parallel requests. Although iText2KG Figure 2: Exhaustivity vs. token count as context for (a) the atomic fact decomposition (b) 5-tuples extraction. facts (denoted as (F) in Table 1) and lead paragraphs individually, establishing baseline named Run 1. This extraction process is subsequently repeated twice more without altering any parameters. As shown in Table 1, centroids of the extracted 5-tuples embeddings from atomic facts remain nearly constant across runs 1 and 2, exhibiting very low standard deviations and high stability scores. In contrast, the centroids of the extracted 5-tuples embeddings from the lead paragraphs demonstrate greater variability. This addresses (C2) and reflects the effect of atomic facts in maintaining stable construction of TKGs with gain of 17%. Table 1: Stability Sr evaluated by rerunning the 5-tuples extraction process multiple times without any modifications, using gpt-4.1-2025-04-14 with Run 1 as baseline. The extraction is performed on (F) atomic facts and (L) lead paragraphs. Dataset Run 2 Run 3 2020-COVID-NYT (F) 2020-COVID-NYT (L) 0.944 0.024 0.776 0.214 0.943 0.025 0.773 0. uses distance metric for resolution, reducing LLM dependency, its separate extraction steps double the number of LLM calls and induce isolated entities that require further LLM iterations. Conversely, ATOMs architecture facilitates (1) parallel LLM calls, (2) parallel merge of atomic TKGs, (3) LLMindependent merging, and (4) temporal resolution. This design reduces latency by 93.8% compared to Graphiti and 95.3% compared to iText2KG (Figure 3). ATOMs Module-3 accounts for only 13% of its total latency, with the remainder attributed to API calls, which can be further minimized through either increasing the batch size (by upgrading the API tier) or scaling hardware for local LLM deployment. Figure 3: Latency comparison of the baseline methods as function of the number of atomic facts as input. 4.7 ATOMs DTKG construction Table 3 shows that ATOM and iText2KG demonstrate comparable entity and relation resolution performance, as both employ distance metric for merging. ATOM shows an improvement over Graphiti, whose incremental, LLM-based entity and relation resolution degrades with graph expansion (increasing context size), which is consistent with findings in Section 4.3. Beyond entity and relation resolution, temporal resolution reveals more significant differences between ATOM and Graphiti. The examples in Figures F.3 and F.4 in the Appendices illustrate atomic facts observed at different times that refer to the same information but with different validity periods. In these examples, Graphiti creates separate relations for 5-tuples with different validity periods (tstart, tend), while ATOM detects these similar relations and extends their validity period history. This temporal resolution serves two functions: tracking events that naturally appear and disappear over time, and matching relations with only tstart or only tend to complete their validity periods as additional information becomes available. Additionally, Graphiti incorporates validity periods only and does not allow for observation time modeling, treating observation time as tstart. This can induce errors. For example, if news article observed on \"January 23, 2020\" states \"The mysterious respiratory virus spread to at least 10 other countries,\" Graphiti would set tstart = 23-01-2020, while the statement does not specify validity period and the true validity could be weeks or days before its publication. In contrast, ATOM separately models observation time and validity periods, allowing it to recognize atomic facts without validity period and avoid incorrect temporal assignments (Example in Figure F.2 in the Appendices). Table 3: Performance on DTKG construction. Metric ATOM Graphiti iText2KG Precision-ER Recall-ER F1-Score-ER Precision-RR Recall-RR F1-Score-RR 0.994 0.993 0.994 1 1 1 0.967 0.952 0.959 0.917 0.888 0.902 0.974 0.980 0.977 0.991 0.988 0."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we presented ATOM, few-shot and scalable approach for constructing and dynamically updating TKGs from unstructured texts. Experimental results indicate that ATOMs atomic fact decomposition effectively addresses the exhaustivity and stability challenges often observed in LLMbased TKG construction methods. Its parallel architecture accelerates TKG construction and enables scalability for larger unstructured texts. Potential directions for future work include quantitative evaluation of temporal resolution and fine-tuning an LLM specifically for refining atomic fact decomposition. In summary, ATOM enables fast and continuous updates of TKGs."
        },
        {
            "title": "6 Limitations",
            "content": "ATOM has some limitations that warrant consideration. First, the atomic fact decomposition can introduce error propagation: the LLM may generate inferred facts not present in the source text, leading to increased hallucination rates, and may fail to properly assign temporal information to atomic facts, resulting in temporal omissions in the extracted 5tuples (Section 4.5). potential improvement consists of fine-tuning an LLM model specifically for this task. Second, the distance-metric-based merging approach, while scalable and efficient, can occasionally merge semantically distinct named entities that exhibit high similarity (e.g., gpt-5:model and gpt-3.5:model). supervised entity/relation resolution classifier trained on labeled entity pairs could replace the threshold-based approach."
        },
        {
            "title": "References",
            "content": "Tareq Al-Moslmi, Marc Gallofré Ocaña, Andreas L. Opdahl, and Csaba Veres. 2020. Named entity extraction for knowledge graphs: literature overview. IEEE Access, 8:3286232881. Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Mikhail Burtsev, and Evgeny Burnaev. 2024. Arigraph: Learning knowledge graph world models with episodic memory for llm agents. arXiv preprint arXiv:2407.04363. Berk Atil, Sarp Aykent, Alexa Chittams, Lisheng Fu, Rebecca Passonneau, Evan Radcliffe, Guru Rajan Rajagopal, Adam Sloan, Tomasz Tudrej, Ferhan Ture, and 1 others. 2024. Non-determinism of\" deterministic\" llm settings. arXiv preprint arXiv:2408.04667. Li Cai, Xin Mao, Yuhao Zhou, Zhaoguang Long, Changxu Wu, and Man Lan. 2024. survey on temporal knowledge graph: Representation learning and applications. arXiv preprint arXiv:2403.04782. Salvatore Carta, Alessandro Giuliani, Leonardo Piano, Alessandro Sebastian Podda, Livio Pompianu, and Sandro Gabriele Tiddia. 2023. Iterative zero-shot LLM prompting for knowledge graph construction. arXiv preprint arXiv:2307.01128. Wiesław Cetera, Włodzimierz Gogołek, Aleksander Zołnierski, and Dariusz Jaruga. 2022. Potential for the use of large unstructured data resources by public innovation support institutions. Journal of Big Data, 9(1):46. Melisachew Wudage Chekol and Heiner Stuckenschmidt. 2018. Towards probabilistic bitemporal knowledge graphs. In Companion Proceedings of the The Web Conference 2018, pages 17571762. Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Hongming Zhang, and Dong Yu. 2023. Dense retrieval: What retrieval granularity should we use? arXiv preprint arXiv:2312.06648. Birgitta Dresp-Langley, Ole Kristian Ekseth, Jan Fesl, Seiichi Gohshi, Marc Kurz, and Hans-Werner Sehring. 2019. Occams razor for big data? on detecting quality in large unstructured datasets. Applied Sciences, 9(15):3065. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024. From local to global: graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130. Hussam Ghanem and Christophe Cruz. 2024. Enhancing knowledge graph construction: Evaluating with emphasis on hallucination, omission, and graph similarity metrics. In International Knowledge Graph and Semantic Web Conference, pages 3246. Springer. Mohammad Javad Hosseini, Yang Gao, Tim Baumgärtner, Alex Fabrikant, and Reinald Kim Amplayo. Scalable and domain-general abstrac2024. arXiv preprint tive proposition segmentation. arXiv:2406.19803. Yuelin Hu, Futai Zou, Jiajia Han, Xin Sun, and Yilei Wang. 2023. LLM-Tikg: Threat intelligence knowledge graph construction utilizing large language model. Available at SSRN 4671345. Xuhui Jiang, Chengjin Xu, Yinghan Shen, Xun Sun, Lumingyuan Tang, Saizhuo Wang, Zhongwu Chen, Yuanzhuo Wang, and Jian Guo. 2023. On the evolution of knowledge graphs: survey and perspective. arXiv preprint arXiv:2310.04835. Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han. 2023. Large language models on graphs: comprehensive survey. arXiv preprint arXiv:2312.02783. Othmane Kabal, Mounira Harzallah, Fabrice Guillet, and Ryutaro Ichise. 2024. Enhancing domainindependent knowledge graph construction through openie cleaning and llms validation. Procedia Computer Science, 246:26172626. Yassir Lairgi, Ludovic Moncla, Rémy Cazabet, Khalid Benabdeslem, and Pierre Cléau. 2024. itext2kg: Incremental knowledge graphs construction using large language models. In International Conference on Web Information Systems Engineering, pages 214 229. Springer. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172. Lost Lisa Meijer. 2022. Bi-vaks: Bi-temporal versioning approach for knowledge graphs. Delft University of Technology. Vatsal Raina and Mark Gales. 2024. Question-based retrieval using atomic units for enterprise rag. arXiv preprint arXiv:2405.12363. Preston Rasmussen, Pavlo Paliychuk, Travis Beauvais, Jack Ryan, and Daniel Chalef. 2025. Zep: temporal knowledge graph architecture for agent memory. arXiv preprint arXiv:2501.13956. Aryan Singh. 2023. NYT Articles: (2000-Present) Daily Updated. //www.kaggle.com/datasets/aryansingh0909/ nyt-articles-21m-2000-present. 2025-06-01. Accessed: 2.1M+ https: Qingyu Tan, Lu Xu, Lidong Bing, Hwee Tou Ng, and Sharifah Mahani Aljunied. 2022. Revisiting docred addressing the false negative problem in relation extraction. arXiv preprint arXiv:2205.12696. Carlo Trugenberger. 2015. Scientific discovery by machine intelligence: new avenue for drug research. arXiv preprint arXiv:1506.07116. Yuxia Wu, Yuan Fang, and Lizi Liao. 2024. Retrieval augmented generation for dynamic graph modeling. arXiv preprint arXiv:2408.14523. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, and 1 others. 2023. The rise and potential of large language model based agents: survey. arXiv preprint arXiv:2309.07864. Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou, and Maosong Sun. 2019. Docred: large-scale document-level relation extraction dataset. arXiv preprint arXiv:1906.06127. Yongheng Zhang, Tingwen Du, Yunshan Ma, Xiang Wang, Yi Xie, Guozheng Yang, Yuliang Lu, and EeChien Chang. 2024. AttacKG+: Boosting attack knowledge graph construction with large language models. arXiv preprint arXiv:2405.04753. Lingfeng Zhong, Jia Wu, Qian Li, Hao Peng, and Xindong Wu. 2023. comprehensive survey on automatic knowledge graph construction. ACM Computing Surveys, 56(4):162. Jun Zhu, Yan Fu, Junlin Zhou, and Duanbing Chen. 2025. temporal knowledge graph generation dataset supervised distantly by large language models. Scientific Data, 12(1):734. ATOMs Algorithms ATOMs framework is based on two main algorithms, presented below: Algorithm A.1 for merging pairs of TKGs and Algorithm A.2 for parallelizing the merge of lists of TKGs."
        },
        {
            "title": "B Example of the atomic fact",
            "content": "decomposition Example: (Observed in 01-01-2025) On June 18, 2024, Real Madrid won the Champions League final with 2-1 victory. Following the triumph, fans of Real Madrid celebrated the Champions League victory across the city. Real Madrid won the Champions League final match on June 18, 2024. (Observation tobs = [01 01 2025], tstart = [18 06 2024], tend = [.]) The Champions League final match ended with 2-1 victory for Real Madrid on June 18, 2024. (Observation tobs = [01 01 2025], tstart = [18 06 2024], tend = [.]) Fans of Real Madrid celebrated the Champions League final match victory across the city on June 18, 2024. (Observation tobs = [01 01 2025], tstart = [18 06 2024], tend = [.])"
        },
        {
            "title": "C Estimating the merging thresholds",
            "content": "The merging thresholds θR and θE were estimated by (Lairgi et al., 2024), based on the mean cosine similarity of 1,500 pairs of similar entities and relation names generated by gpt-4-061310; however, because entity typology is not considered, hybrid similarity measure is proposed, combining the entity name embedding and the entity label embedding as: λ embeddingsname + β embeddingslabel. Using this measure, 1,200 pairs of similar entities incorporating typology were generated, and λ is optimized to maximize the resulting cosine similarity, after which it is determined that λ = 0.8, β = 0.2, and θE = 0.8, while θR = 0.7 is retained as previously estimated in (Lairgi et al., 2024)."
        },
        {
            "title": "D Observation time modeling",
            "content": "Modeling the observation time is essential both for capturing the dynamism of the TKG and for inferring relative times. For historical or retrospective unstructured text streams (e.g., past news articles or archive documents), the observation time should correspond to the original publication time rather than the time at which the document was processed and ingested into the DTKG. This distinction is essential for preserving the correct temporal ordering of events and enabling reliable inference of relative times. Conversely, for prospective or continuously monitored sources, where data is ingested automatically as it becomes available, the observation time can be treated as the ingestion time. In such settings, ingestion reflects the earliest feasible moment at 10https://platform.openai.com/docs/models/ gpt-4 Cosine similarity of and embeddings r.endEntity (r.endEntity) Cosine similarity of and names embeddings Algorithm A.1 Binary Merge of TKGs 1: function BINARYMERGE(T KG1 = (E1, R1), KG2 = (E2, R2), θE, θR) if there exists E2 such that e.name = e.name and e.label = e.label then 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: // Entity Resolution // Initialize mapping for all entity E1 do (e) else Compute max eE2 Let arg max eE2 (cid:16) cos (cid:17) e.v, e.v (cid:17) e.v, e.v (cid:16) cos if θE then (e) else (e) end if end if 1 end for Emerged E2 {e (e) / E2} // Relations Name Resolution // Rupdated for all relation R1 do Update endpoints: Compute sr max rR (cid:16) Let arg max rR2 cos r.v, r.v r.startEntity (r.startEntity), cos (cid:16) r.v, r.v (cid:17) (cid:17) if sr θR then Update names: Update r.name r.name end if // Temporal Resolution // if there exists R2 such that is similar to then // For similar relations, their times are merged Update start time: Update end time: Update observation time: r.tstart r.tstart r.tstart r.tend r.tend r.tend r.tobs r.tobs r.tobs 26: 27: 28: 29: 30: 31: 32: 33: 34: end function end if Rupdated 1 Rupdated 1 {r} end for Rmerged R2 Rupdated return (Emerged, Rmerged) 1 which the information could be known by the system. The granularity of observation time is application-dependent and may be defined according to user requirements. For instance, COVID-19 news was simulated using daily observation snapshots, whereas social media streams may require per-post snapshot. Algorithm A.2 Parallel Merge of TKGs 1: function PARALLELMERGE(T KGs, θE, θR) Input: list of temporal knowledge graphs 2: KGs = {T KG1, KG2, . . . , KGn} 3: 4: 5: 6: 7: current KGs while current > 1 do mergedResults Let current Form pairs: pairs { (current[2i], current[2i + 1]) 0 < n/2 } if is odd then lef tover current[n 1] lef tover null end if for all each pair (T KGa, KGb) in pairs in parallel do merged BINARYMERGE(T KGa, KGb, θE, θR) Add merged to mergedResults end for if lef tover = null then Add lef tover to mergedResults end if current mergedResults end while return current[0] else 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: end function Figure F.1: Example overview of ATOMs pipeline. It begins with atomic fact decomposition, followed by the extraction of atomic TKGs from these facts, which are then merged in parallel. When an incoming update arrives, ATOM handles the temporal resolution by transforming the end action into the affirmative part while modifying only the tend, then merges the resulting atomic TKG with the existing DTKG. Table T.1: 2020-COVID-NYT Statistics Analysis. We use lead paragraphs as they encapsulate the articles key facts, while the full article text is often unavailable in research datasets due to licensing restrictions and would introduce unnecessary verbosity without proportional information gain."
        },
        {
            "title": "Basic Dataset Information",
            "content": "Total Articles Grouped Articles (by pub. date) Average Tokens per Group Date Range"
        },
        {
            "title": "Knowledge Graph Structure",
            "content": "Total 5-tuples Number of atomic TKGs Avg number of 5-tuples per atomic TKG 1,076 274 206 156 2020-01-09 to 2020-12-30 4,223 2,037 2,186 7,210 4,223 2 Figure F.2: Two DTKGs constructed using ATOM and Graphiti from 09-01-2020 (in UNIX, 1578524400) to 23-012020 (in UNIX, 1579734000) from 2020-COVID-NYT dataset. Left (ATOM): Preserves observation times (tobs) separately from validity periods, with timestamps encoded in UNIX format to eliminate overhead associated with string parsing operations and timezone conversion calculations. Right (Graphiti): Treats observation time as validity start time. valid_at corresponds to tstart in Graphitis time modeling. The highlighted fact The mysterious respiratory virus spread to at least 10 other countries is observed on 23-01-2020, but this does not guarantee the spread occurred at that time. ATOMs dual-time modeling prevents such temporal misattribution. Figure F.3: Temporal resolution comparison between ATOM and Graphiti. Two atomic facts observed on January 28, 2020, report death counts from January 24 (26 deaths) and January 27 (at least 80 deaths). Left (ATOM): performs temporal resolution by detecting similar relations and extending their validity period history (tend in the figure). Right (Graphiti): creates separate relations for each atomic fact, resulting in duplication. Moreover, Graphiti misinterprets By January 24, 2020 and By January 27, 2020 as validity start times rather than validity end times, leading to temporal misattribution. Figure F.4: Temporal resolution comparison between ATOM and Graphiti. Two atomic facts observed on different dates (April 16 and April 19, 2020) describe protest activities during two time periods (the week of April 13 and the week of April 19). Left (ATOM): merges similar relations and extend their validity periods (tstart and tend in the figure). Right (Graphiti): maintains separate relations for each atomic fact. Moreover, Graphiti fails to translate In the week of April 13, 2020 and In the week of April 19, 2020 into proper validity periods as ATOM does."
        }
    ],
    "affiliations": [
        "GAUC, Lyon, France",
        "LIRIS, INSA Lyon, Université Claude Bernard Lyon 1, France"
    ]
}