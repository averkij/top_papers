{
    "paper_title": "Hallucination Detox: Sensitive Neuron Dropout (SeND) for Large Language Model Training",
    "authors": [
        "Shahrad Mohammadzadeh",
        "Juan David Guerra",
        "Marco Bonizzato",
        "Reihaneh Rabbany",
        "Golnoosh Farnadi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As large language models (LLMs) become increasingly deployed across various industries, concerns regarding their reliability, particularly due to hallucinations-outputs that are factually inaccurate or irrelevant to user input-have grown. Our research investigates the relationship between the training process and the emergence of hallucinations to address a key gap in existing research that focuses primarily on post hoc detection and mitigation strategies. Using models from the Pythia suite (70M-12B parameters) and several hallucination detection metrics, we analyze hallucination trends throughout training and explore LLM internal dynamics. We introduce SEnsitive Neuron Dropout (SeND), a novel training protocol designed to mitigate hallucinations by reducing variance during training. SeND achieves this by deterministically dropping neurons with significant variability on a dataset, referred to as Sensitive Neurons. In addition, we develop an unsupervised hallucination detection metric, Efficient EigenScore (EES), which approximates the traditional EigenScore in 2x speed. This efficient metric is integrated into our protocol, allowing SeND to be both computationally scalable and effective at reducing hallucinations. Our empirical evaluation demonstrates that our approach improves LLM reliability at test time by up to 40% compared to normal training while also providing an efficient method to improve factual accuracy when adapting LLMs to domains such as Wikipedia and Medical datasets."
        },
        {
            "title": "Start",
            "content": "Preprint. Under review. HALLUCINATION DETOX: SENSITIVE NEURON DROPOUT (SEND) FOR LARGE LANGUAGE MODEL TRAINING Shahrad Mohammadzadeh1, 4, , Juan David Guerra2, 4, Rabbany1, 4, and Golnoosh Farnadi1, 3, 4 , Marco Bonizzato2, 3, 4, Reihaneh 4 2 0 O 0 2 ] . [ 1 0 6 4 5 1 . 0 1 4 2 : r 1McGill University, Montreal, Canada 2Polytechnique Montreal, Montreal, Canada 3Universite de Montreal, Montreal, Canada 4Mila - Quebec AI Institute, Montreal, Canada {shahrad.mohammadzadeh, juan.guerra, marco.bonizzato, reihaneh.rabbany, farnadig}@mila.quebec"
        },
        {
            "title": "ABSTRACT",
            "content": "As large language models (LLMs) become increasingly deployed across various industries, concerns regarding their reliability, particularly due to hallucinationsoutputs that are factually inaccurate or irrelevant to user inputhave grown. Our research investigates the relationship between the training process and the emergence of hallucinations to address key gap in existing research that focuses primarily on post hoc detection and mitigation strategies. Using models from the Pythia suite (70M12B parameters) and several hallucination detection metrics, we analyze hallucination trends throughout training and explore LLM internal dynamics. We introduce SEnsitive Neuron Dropout (SeND), novel training protocol designed to mitigate hallucinations by reducing variance during training. SeND achieves this by deterministically dropping neurons with significant variability on dataset, referred to as Sensitive Neurons. In addition, we develop an unsupervised hallucination detection metric, Efficient EigenScore (EES), which approximates the traditional EigenScore in 2x speed. This efficient metric is integrated into our protocol, allowing SeND to be both computationally scalable and effective at reducing hallucinations. Our empirical evaluation demonstrates that our approach improves LLM reliability at test time by up to 40% compared to normal training while also providing an efficient method to improve factual accuracy when adapting LLMs to domains such as Wikipedia and Medical datasets."
        },
        {
            "title": "INTRODUCTION",
            "content": "1.1 MOTIVATION In the age of increasingly sophisticated and ever growing Large Language Models (LLMs), we see these models being utilized in every industry imaginable. However, concerns surrounding their reliability and safety have escalated due to their exploitation and the errors that arise when they are used by the general public. One of these concerning areas discovered by the scientific community is the phenomenon of hallucinations - LLMs producing content that may not align with real-world facts, the users input, or training data it has seen in the past (Huang et al., 2023a). In our research, we target specific field of hallucinations called confabulations. Confabulations are type of hallucination where the LLM generates different responses given the same or similar inputs. This can be harmful when the generations alter between correct and factually incorrect responses. Previous research has largely focused on identifying and addressing hallucinations in large language models (LLMs), but the impact of the training process on hallucinations remains under-explored Equal Contribution 1 Preprint. Under review. (Huang et al., 2023a; Rawte et al., 2023; Ye et al., 2023; Hong et al., 2024; Xu et al., 2024; Chen et al., 2024; Li et al., 2024; Gao et al., 2024b). This paper addresses this gap by investigating how the iterative learning process in LLMs leads to significant variance in hallucination behavior during training. This variability indicates that the models factual confidence fluctuates, making it challenging to pinpoint checkpoint at which the model has confidently learned facts. As LLMs are deployed in high-risk industries, ensuring their reliability is crucial for user safety. However, this is not always achieved, leading to serious consequences, such as an Air Canada lawsuit over an LLM-generated incorrect policy (Garcia, 2024). Addressing such issues requires deeper understanding of how hallucinations arise during training, enabling more reliable and efficient mitigation strategies beyond post-processing methods. To explore these hallucination trends, we analyze models ranging from 70 million to 12 billion parameters within EleutherAIs Pythia suite (Biderman et al., 2023), assessing them across various training checkpoints and tasks. Our goal is to validate the oscillatory behavior observed in prior studies (Li et al., 2024) through evaluation metrics from HuggingFace and EleutherAI (Hong et al., 2024; Gao et al., 2024a), and to explore the correlation between model size, training progression, and hallucination patterns. In response to the identified variance, we introduce novel training protocol called Sensitive Neuron Dropout (SeND). SeND is designed to emphasize confident learning of facts, and in turn reduce the likelihood of confabulations, rather than solely minimizing the stochastic gradient descent (SGD) loss (e.g., cross-entropy). By selectively dropping Sensitive Neuronsthose that exhibit significant fluctuations in contextual embeddings throughout trainingSeND acts as regularization technique that reduces hallucination variance and enhances the models factual certainty. This provides more reliable criterion for determining training termination, ensuring models not only achieve loss convergence but also display stable factual confidence. To maintain efficiency as model size and inference count increase, we propose the Efficient EigenScore (EES), an approximation metric for hallucination detection. EES replaces EigenScore (Chen et al., 2024), the primary metric used in our experiments, offering scalable solution with high correlation to the original EigenScore. Our contributions to the field can be summarized as follows, emphasizing that SeND enhances the training process but does not replace post-hoc solutions, which may still be required after training:1 1. Empirical verification of the oscillatory nature of hallucinations in LLMs training across various model scales and detection metrics. 2. Sensitive Neuron Dropout (SeND), training-time method designed to reduce hallucination variance and increase model factual confidence during training. 3. Efficient EigenScore (EES), an efficient hallucination detection metric used to keep SeND efficient, achieving up to 2x speedup with minimal effects on accuracy. 1.2 RELATED WORK The majority of research on hallucinations in language models has focused on detecting and mitigating this phenomenon rather than explaining its underlying causes. Recent techniques can be categorized into two main approaches: those that rely on output text or model probabilities at inference time (Manakul et al., 2023; Joshi et al., 2017; Li et al., 2023) and those that utilize internal representations or hidden layers of the model (Su et al., 2024; Chen et al., 2024; Kossen et al., 2024). While the former has demonstrated effectiveness, the latter offers deeper insights but often comes with computational trade-offs. Additionally, methods like Reinforcement Learning with Human Feedback (RLHF) have gained traction for enhancing model reliability (Yu et al., 2024). However, many of these post-hoc solutions enhance factual accuracy by layering algorithms atop pre-trained models, which can be inefficient. Our work addresses this gap by focusing on the internal dynamics of the model that contribute to hallucinations. We use several metrics for evaluation, including Halueval (Li et al., 2023), FactScore (Min et al., 2023), SelfCheckGPT (Manakul et al., 2023), and XSum (Narayan et al., 2018), to validate our findings across different tasks. Given that the internal dynamics of the model have proven to be 1For the code and datasets used, refer to our GitHub repository at: https://anonymous.4open. science/r/SeND-Pythia/README.md. 2 Preprint. Under review. reliable candidates for assessing certainty and hallucination likelihood, we leverage methodologies such as EigenScore (Chen et al., 2024) and Semantic Entropy (Kossen et al., 2024), which detect hallucination risk by analyzing the variability in multiple high-temperature outputs. In our experiments, we use the EigenScore metric alongside the HELM dataset created by Su et al. (2024) to detect hallucinations during training and in the development of SeND. Regularization techniques have been introduced to fix the issue of variability, notably random neuron dropout, used to reduce the variance and ensure that no neuron is overpowering others (Srivastava et al., 2014; Baldi & Sadowski, 2013). Work such as that done by Santra et al. (2020); Ba & Frey (2013) aims to modify random neuron dropout to change the way neurons are dropped to more deterministic, precise manner. This has allowed the authors to drop unimportant connections in deep neural network to ensure that class discriminative information is propagated through the model correctly (Santra et al., 2020). Inspired by this, our aim is to target hallucinatory neurons in our models to ensure that factual information is propagated through. significant drawback of state-of-the-art detection techniques, particularly those relying on internal model dynamics, is their efficiency. Existing methods often necessitate multiple inferences and embedding generations, making the spectral analysis of embedding matrices computationally intensive and increasingly impractical as models and datasets grow (Chen et al., 2024; Su et al., 2024). To address these challenges, we propose the use of spectral theory for efficient approximation. This approach enables scalable hallucination detection while maintaining performance. By utilizing tools such as the Density of States (DOS) and the kernel polynomial method (KPM) for approximating EigenScore (Huang et al., 2023b; Lin et al., 2014), we aim to enhance the efficiency of our analysis in the context of confabulations, which we will demonstrate empirically with EES and SeND."
        },
        {
            "title": "2 OSCILLATORY BEHAVIOUR VALIDATION",
            "content": "The training epochs of transformer model can be vital in understanding the dynamics of how the model learns, particularly when trained on an unsupervised loss with stochastic gradient. Beyond the models architecture and the data itself, numerous factors influence the learning process such as: Whether the loss function penalizes the learner for factual mistakes it makes or if it primarily tries to force the model to memorize the data. While our paper does not aim to address the broader debate on whether language models truly understand language or rely heavily on memorization, our analysis of training dynamics through multiple epochs shows that reducing stochastic gradient loss does not necessarily correspond to reducing hallucinations, verifying the results Li et al. (2024) showed for the oscillatory behaviour of LLMs in hallucination during training. This highlights the need for further investigation into the relationship between optimization and factual accuracy in LLMs. 2.1 METHODS In our study, we utilize Eleuther AIs Pythia and LMEval tools (Biderman et al., 2023; Gao et al., 2024a), to examine the development and evolution of LLMs throughout the training process. Pythia comprises suite of 16 LLMs, all trained on public data in the same sequential order, with sizes ranging from 70 million to 12 billion parameters, 8 of which we use for our experiments. We used 20 equally spaced training checkpoints from the start to the finish for our analysis. We chose Pythia as it is based on GPT-Neo (Black et al., 2022), which shares similar foundational architecture to other state-of-the-art language models. Pythias comprehensive package of models makes it particularly suitable for our analysis, allowing us to conduct thorough examination of the development and evolution of LLMs throughout the training process. These models are evaluated at each checkpoint on variety of hallucination/fact-checking metrics. To do this, we leverage the HuggingFace Hallucination Leaderboard (Hong et al., 2024), which offers comprehensive benchmarks for our experiments. There are two main components to our evaluations: Summarization and Self-consistency. For summarization, models are evaluated under the XSum dataset Narayan et al. (2018) where the model is given dataset of BBC news articles and must give summaries of each article. higher Rouge1 score on XSum means the data is aligning better with the provided reference summary. SelfCheckGPT (Manakul et al., 2023) is used 3 Preprint. Under review. (a) Self-Consistency (b) Summarization Figure 1: Visualization of Oscillatory Behavior Across Varying LLM Sizes. Hallucination metrics are evaluated at equidistant checkpoints of the Pythia models, with sizes 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, 12B. Part (a) presents the performance of the Pythia models under the SelfCheckGPT metric. Average performance is indicated by solid lines, while the shaded regions represent the standard deviation. Higher SelfCheckGPT score indicates higher probability of self-contradiction and higher probability of confabulation. Part (b) depicts the same experimental setup, but hallucination measured on the XSum v2 dataset, where Rouge1 is used as the performance metric. higher Rouge1 score suggests better alignment of the generated text to that of the reference summary. For all model sizes, we observe pronounced trend of high variance and oscillatory behavior in hallucination rates. This fluctuation highlights the models uncertainty at specific time stamps and emphasizes the need for robust mitigation strategy to stabilize performance during training. to see if the model is uncertain with respect to dataset of prompts for self-consistency. When the SelfCheckGPT score is high, it means the model is more likely to contradict itself on the given input and therefore, more likely to hallucinate. 2.2 HOW DO THE ESTABLISHED ITERATIVE TRAINING PROCESSES INFLUENCE LLM HALLUCINATIONS? The analysis of hallucination oscillations, as shown in Figure 1, indicates consistent pattern across different models: oscillations persist throughout training from the initial to the final checkpoint. This finding highlights the uncertainty of halting training solely based on the convergence of training loss to its minimum to minimize hallucination. For example, in the evaluation plot of XSUM with the 12B model (1b), the optimal value for the hallucination metric occurs at the 60000th checkpoint, despite the models unsupervised loss converging by step 140000, leading to the termination of training. This evidence challenges the notion that optimizing solely for unsupervised loss in SGD guarantees learning the most accurate representation of the data. This observation is seen more drastically in 1a, where model size has no effect on the performance of SelfCheckGPT. Instead, we observe exaggerated oscillatory behaviour within self-consistency, meaning that model size is even less effective at tackling the issue of confabulations. One potential solution to mitigate this issue could involve incorporating regularization term into the unsupervised loss based on hallucination detection metric discussed in Section 4. 2.3 HOW DOES MODEL COMPLEXITY AFFECT THE EMERGENCE OF HALLUCINATIONS THROUGHOUT TRAINING? An analysis of hallucination detection metrics reveals diminishing rate of improvement with increased model scaling, particularly up to the 12B parameter size (Figure 1b). This suggests that beyond certain point, even though there is improvement in the hallucinations, larger models do not significantly reduce hallucinations, indicating that scaling alone is not sufficient for building robust models. Instead, more refined approaches are needed to address the underlying variability in model behavior. For the following experiments, we focus on the Pythia 1B model."
        },
        {
            "title": "INTERNAL TRAINING DYNAMICS",
            "content": "Following our investigation of the oscillatory behaviour in training, we look into the internal states of the Pythia 1B model to see what information we are able to extract. In doing so, we define series 4 Preprint. Under review. of terms and formulas in order to understand the internal processes during the training of LLMs. This information is later used in sections 3.3 and 4 to assist us in deriving methods for improving the variance in the hallucinatory behaviour of models during training. 3.1 SENSITIVE NEURONS To start our analysis of the internal states, we convert the activation matrix of the model into sentence embedding vector 3.1 which turns an Rn,m activation matrix into sentence embedding vector ak for input with dimension Rn. Given its demonstrated success in hallucination detection by Su et al. (2024), we employ this sentence embedding extraction approach. Definition 3.1 (Sentence Embedding Vector). The Sentence Embedding Vector is way to convert the large Rn,m activation matrix into smaller, easier to manage vector with dimension Rn. (cid:88) (1) ek = (( 1 2 1 i= N 1) + 1) Where ek is the activation of one input k, is the number of tokens in the sequence, and 1 is the subtraction to get the penultimate layer index and the formula is adapted from Su et al. (2024). The penultimate layer of the LLM, being the layer closest to the output probabilities, is our primary focus for hallucination analysis due to its rich information about output certainty. Next, we define the Net Change Formula 3.2 as way to extract information from the model indicative of oscillatory behaviour between checkpoints from the sentence embedding vector. Definition 3.2 (Net Change Formula). Let et the contextual embedding after checkpoint/epoch t. Then we define the net change formula as = et denote the embedding of data point at neuron of et1 et (2) With these definitions, we can now describe the crux of our investigation: Sensitive Neurons. These Sensitive Neurons give us key parts of the model that we will prove contribute to the hallucination of LLM models. They can be used to adapt training procedures for lowering hallucination variation during training and better overall confidence at inference time. In essence, Sensitive Neurons are embedding indices in the sentence embedding from definition 3.1 that experience drastic changes between checkpoints/epochs of the training, something we believe is related to the oscillatory behaviour in hallucination performance. When finding the most Sensitive Neurons, we typically want to select the top K% neurons for specific data points representation. In our investigation we set = 20. Definition 3.3 (Sensitive Neurons). Indices of the contextual embedding for data point which exhibit the highest net change across the last checkpoints of training, indicating overall high variability during this period. This is calculated by Vi = ar(ei) (cid:88) et t=T C+1 (3) where Vi is the total variability during the last checkpoints and the most Sensitive Neurons are = arg max 1iN {Vi Vi percentile(V, 100 k)} (4) where is the embedding vector size and is the desired percentile threshold. The above definition of Sensitive Neurons is then applied to LLM hallucinations through analyses of the EigenScores. In their paper, Chen et al. (2024) define new metric for detecting confabulations, subclass of hallucinations. They do this by calculating an EigenScore 3.4 based on determinant calculations from multiple outputs of an LLM with high-temperature setting (temperature set to 0.5) to encourage the LLM to produce variety of different outputs. They propose that if an LLM is set to hallucinate on that output, the generated texts will show higher semantic variability and produce higher EigenScore. This method achieves SOTA performance and is unsupervised as it only relies on the representations learned by the model. In the forthcoming sections, we will analyze the correlation between the EigenScore of data points during training checkpoints and the most Sensitive Neurons associated with them. 5 Preprint. Under review. Definition 3.4 (EigenScore). The EigenScore of data point indicates the degree of hallucination on input by the average logarithm of the eigenvalues on the covariance matrix of the multiple output generations (typically 10 in our experiments). ES = E(Y x, θ) = 1 (cid:88) i=1 log(λi) (5) where λ = {λ1, . . . , λK} denotes the eigenvalues of the regularized covariance matrix Σ + α I. we advise referring to Chen et al. (2024) for more detailed analysis of this formula. 3.2 SENSITIVE NEURON IMPACT ON EIGENSCORES To assess the correlation between Sensitive Neurons and other neurons in the embedding matrix of 10 generated outputs at specific checkpoint, we conduct experiments aimed to determine if the presence of Sensitive Neurons indicates higher uncertainty and greater likelihood of hallucinations. We evaluate the Sensitive Neuron effect on the HELM dataset (Su et al., 2024), which includes outputs and internal states from six open-source LLMs based on inference over 50,000 Wikipedia articles, with human annotators labeling passages as factual or hallucinatory. To assess the impact of Sensitive Neurons on hallucination, we adapt the EigenScore method by applying it to sentence embeddings from the penultimate layer of EleutherAIs Pythia 1B model, focusing on checkpoints between 133,000 and 143,000 training steps, where embeddings are more stable and the model has higher degree of language understanding compared to initial checkpoints. We perform sensitive neuron dropout, removing the top 10% of Sensitive Neurons at each checkpoint, and compare the results to baseline where 10% of neurons are randomly dropped. Additionally, we analyze the impact on hallucination-prone inputs versus non-hallucination-prone inputs to determine if Sensitive Neurons play critical role during hallucination, without negatively affecting correct outputs. 3.2.1 WHAT IS THE EFFECT OF SENSITIVE NEURONS ON HALLUCINATION METRICS? Since reduction in the EigenScore metric can be used as proxy to show the reduction in likelihood of hallucination and an increase in the uncertainty of the model about its response, we keep using this metric in our investigations. We are able to show through our comparison of the baseline random neuron dropout and Sensitive Neuron dropout that Sensitive Neurons significantly reduce the EigenScore metric and in turn, reduce the possibility of confabulation (Figure 2a) implying their highly important role in determining models certainty. Not only do we observe this in hallucinatory outputs, we also observe smaller reduction in EigenScore when applying this technique to correctly answered queries (Figure 2b). This result indicates that our methodology has significant effect on the uncertainty shown by an LLM. We observe that looking at the internal states of the model is an effective way to eliminate confabulating text generation in various model sizes. (a) Model Size (b) Output Type Figure 2: Comparison of sensitive neuron dropout on inference of Eleuther AIs Pythia various model sizes with random neuron dropout. (a) Average sensitive neuron dropout with standard deviation plotted as scale of the model increases. (b) Average sensitive neuron dropout for hallucinatory inputs and non-hallucinatory inputs. Input size for each test is 80 I.I.D. texts. Sensitive neuron dropping presents clear, significant reduction in EigenScore compared to that of random neuron dropping across model sizes. Hallucinatory generations experience larger drop in EigenScore, meaning that our protocol scales with likelihood of hallucination. 6 Preprint. Under review. 3.3 EFFICIENT EIGENSCORE APPROXIMATION To address the computational complexity of EigenScore calculations, particularly as LLM hidden layer sizes increase and more generations occur at higher temperatures, we develop an approximation method. This approximation, detailed in Algorithm 1, leverages the properties of Spectral Density or Density of States (DOS) to estimate EigenScore without explicitly constructing the covariance matrix. While this approximation provides general overview of EigenScore trends, it is important to note that the output scales differ: EigenScore ranges from [0, ), whereas the approximation, referred to as Efficient EigenScore (EES), outputs values between [1, 1]. Since the spectrum of the matrix is altered to make EES computable and operates on its own scale, EES can be seen as standalone metric for hallucination detection. Algorithm 1 Efficient EigenScore (EES) Computation Algorithm Require: Embedding matrix RdmodelK, number of Chebyshev terms , number of stochastic trace estimation samples Nz Ensure: Approximated EigenScore EES 1: Standardize and Scale the Embedding Matrix E: 2: Emean = 1 (cid:113) 1 i=1 E[:, i] i=1(E[:, i] Emean) (cid:80)K (cid:80)K 3: Estd = 4: Enormalized = EEmean 5: σmax = Power Method(Enormalized) Estd Compute mean of Compute standard deviation of Standardize Compute the largest singular value using the power Scale by σmax Initialize dm coefficients Initialize cm coefficients Sample zj (0, I) Compute Chebyshev polynomial using the recurrence relation Sample random vectors for stochastic trace estimation method σmax 6: Enormalized Enormalized 7: Initialize: 8: dm = 0 {0, 1, . . . , } 9: cm = 0 {0, 1, . . . , } 10: Compute DOS coefficients dm: 11: for = 0 to do 12: 13: 14: end for 15: Compute Chebyshev coefficients cm: 16: for = 0 to do cm (cid:82) 1 17: approximation 0 log(λ)T m(λ) dλ 18: end for 19: Compute EigenScore: 20: EES 1 m=0 dmcm 21: return EES (cid:80)M Using Equation 27 and Gaussian Quadrature for Approximate EigenScore using DOS coefficients Return the approximated EigenScore The computation of the Efficient EigenScore (EES) is based on two fundamental concepts: Chebyshev Polynomials and Density of States (DOS). detailed introduction to these concepts is provided in Appendix sections B.1 and B.2. Below, we outline brief sketch of the derivation of EES. Since Chen et al. (2024) use the covariance matrix of the embedding matrix of 10 generated sequences by the model in their methods, we represent it with and use it in our derivation. Lemma 1. Let = log. Then, for covariance matrix with eigenvalues λi, we have trace(log(H)) = (cid:88) i=1 log(λi), where λi are the eigenvalues of H. Proposition 1. Using the property of the density of states (DOS), we have: (cid:90) log(λ) µ(λ) dλ = log (cid:32) (cid:89) (cid:33) λi , which follows from Lemma 1 since (cid:80)N i=1 log(λi) = log i=1 (cid:16)(cid:81)N i=1 λi (cid:17) . 7 (6) (7) Preprint. Under review. Note that from Proposition 1, the integral is equal to N.EigenScore(H) or in our application, given the integral equals K.EigenScore(C), being the number of model generations. Our objective is to simplify the integral and approximate its value, avoiding the direct computation of the covariance matrix. This approach is intended to mitigate the computational complexity and associated costs of explicitly handling the covariance matrix. Further utilizing Chebyshev Polynomials, DOS, and KPM (as introduced in Appendix B.2), we can simplify the integral mentioned in Equation 7 to (cid:80)M m=0 dmcm, where dm term in DOS is approximated using Stochastic Trace Estimation and cm mth Chebyshev Polynomial coefficient. Appendices B.3 and B.4 provide the derivation of this equation. Note that the simplified integral is ultimately used to approximate the EigenScore of the matrix which is ultimately equivalent to 1 m=0 dmcm. Performance of EES approximaK tion is closely correlated with that of the original EigenScore and can be seen to closely track the progress of EigenScore through training of Pythia 1B on the HELM dataset in Figure 7. (cid:80)M 3.4 HOW DOES EFFICIENT EIGENSCORE APPROXIMATION SCALE COMPARED TO REGULAR EIGENSCORE? The efficiency of EES is compared to that of the regular EigenScore calculation with respect to scaling matrix sizes. These tests are imperative to the application of our training protocol on LLMs in Section 4, as with increasing model size, we get ever larger matrix sizes to decompose for the EigenScore calculation. To elucidate these scaling effects, we conduct grid search over two important parameters: Matrix size (Figure 3) and Moments used for EES calculation (Figure 6). The difference between EES time in comparison to EigenScore when increasing the number of columns and rows is visualized in Figure 3 using moments value of 20. It is evident that EES provides significant computational advantage when increasing the number of columns or rows. Remarkably, at matrix size R1e8, EES nearly halves the computation time of regular EigenScore calculation at around 4 seconds whereas EigenScore takes approximately 7 seconds to calculate. We can then deduce that given good enough approximation, EES provides significant reduction in computational complexity as model and matrix size increase. Figure 3: Efficient EigenScore approximation scaling investigation. The figure shows the difference in computation time between regular EigenScore calculation and EES with moments value of 20. The x-axis represents the product of the matrixs rows and columns, and the y-axis shows the computation time. As matrix size increases, EES consistently reduces computation time, making it practical choice for large LLMs."
        },
        {
            "title": "4 SENSITIVE NEURON DROPOUT (SEND)",
            "content": "Building on the findings from Section 3.2, and aiming to reduce variance in the factual uncertainty of LLMs during training, this section introduces SeND, an efficient and transferable framework for training LLMs. SeND integrates the EES method discussed in Section 3.3 to enhance computational efficiency while addressing variance in sensitive neuron behavior. By identifying sensitive neurons, which contribute to the oscillatory behavior of hallucinations during training, SeND deterministi8 Preprint. Under review. cally drops these neurons based on small subset of the training data. This approach ensures an increase in the models factual certainty by the end of training as explained in Algorithm 2. Algorithm 2 Sensitive Neuron Dropout Require: ϵ denotes the acceptable range for loss convergence and δ denotes acceptable range for confabulation (EES) convergence Refer to Algorithm 1 for EES denotes the number of epochs per sensitive neuron calculation for in do end for for 1 do Train LLM for one epoch over Yt Record penultimate layer representations Rt of LLM over Ys 1: Initialize dataset with α% training Yt and (100 α)% tracking Ys 2: while Loss > ϵ and EES > δ do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: end while end for Take average Variability Vavg = 1 Ns = most sensitive neurons Vavg Drop neurons for next epochs Calculate variability Vt between Rt to Rt+1 i=0 Vi (cid:80)Ns Refer to Equation 3 Refer to Equation 4.1 SEND EXPERIMENT SETUP To evaluate SeND, we use Eleuther AIs Pythia 1B model, continuing its training on specific datasets rather than restarting pretraining to maintain efficiency. We continue the training of the fully trained model on two datasets: HELM, consisting of Wikipedia text (Su et al., 2024), and MedHALT, medical dataset emulating real-world entrance exam questions (Pal et al., 2023). Due to the importance of factual accuracy in the medical domain, MedHALT was chosen to assess SeNDs impact on hallucination mitigation. Both datasets were tested in two sizes: 200 and 2,000 points (referred to as 2k). SeND implements the EigenScore reduction technique from Section 3.2 and detects Sensitive Neurons using 3-epoch window on specialized hallucination tracking dataset. Sensitive Neurons in the penultimate layer are identified based on their variability across epochs and are deterministically dropped for the subsequent 3 training epochs. This dropout process is repeated at each 3-epoch interval until the training loss converges, effectively mitigating hallucination tendencies and refining the model. While we acknowledge that the number of subsequent epochs for dropout could be adjusted, we limit our approach to 3 for ease of computation. 4.2 PERFORMANCE OF SEND ON PYTHIA 1B The results of training Pythia 1B on HELM and MedHALT 200 are illustrated in Figure 4. To validate that the EES method accurately approximates the EigenScore metric; we compare the models progress during training (up to loss convergence) and assess whether the resulting graphs are similar. These results are detailed in Appendix B.6. Upon confirming that EES provides reliable approximation of the EigenScore, we proceed to compare the performance of Pythia 1B trained using standard training with SeND on HELM and MedHALT 2k (Figure 4). In the case of training on HELM with the regular protocol, we observe results consistent with previous findings: while the model successfully reduces loss, it fails to optimize for hallucination, as evidenced by the increasing EES metric (green line in Figure 4a). Conversely, training with SeND reveals consistent trajectory toward reducing both EES and loss, as depicted by the blue line. To assess the effectiveness of SeND in comparison to other state-of-the-art factuality metrics, we employ the FactScore metric from Min et al. (2023), which quantifies the factual accuracy of content generated by large language models (LLMs). The fact-checking is conducted using the HELM dataset on the final trained models (1B SeND and 1B Normal Training). higher FactScore indicates improved factual precision. When evaluated on 100 data points from the HELM dataset, the 1B SeND model achieves FactScore of 0.07, whereas the 1B Normal Training model attains 0.05, demonstrating 40% improvement in factual accuracy, even during test time. This highlights the efficacy of SeND in enhancing the factual certainty of the model. Note that SeND is not replacement for post-hoc methods like RAG (Gao et al., 2024b), but rather to complement them. Preprint. Under review. (a) HELM (b) MedHALT 2k Figure 4: Regular finetuning vs. SeND on two datasets. (a) presents the results of training Pythia 1B on HELM with regular finetuning and SeND. (b) uses the same training setup as (a), but the LLM is trained on MedHALT 2k. In both plots, performance is reported as the average EES over 5 runs on the validation set. Models are trained until loss convergence. Training with SeND shows more controlled reduction in EES compared to regular finetuning, suggesting that SeND optimizes for hallucinations as well as loss, with less overall confidence variability during training. Since finetuning without dropout consistently achieves better factual consistency than random dropout in our experiments, we compare SeND directly with standard finetuning. The robustness of this training protocol is essential as we aim for it to be applied across many different fields. In light of this, we present the results of finetuning on MedHALT 2k with and without SeND in Figure 4b. We observe similar trend in 4b as shown in Figure 4a, where standard finetuning increases the EES score throughout training, showing that the model is not taking into account hallucinations and factuality during its training. In Figure 4b, there is an improvement in the trajectory of EES as training continues, showing that our model is in fact able to incorporate factuality as metric to account for during training of the model. Although the decrease in EES is marginal for MedHALT 2k, we can attribute this to the dataset it is being trained on. The relatively small difference observed between the training protocol behaviours could be due to MedHALT 2k data never being seen before the finetuning phase whereas HELM data has been seen. In this case, it may be beneficial to delay the onset of SeND, as high variability between checkpoints on new training instances is expected."
        },
        {
            "title": "5 CONCLUSION & FUTURE WORK",
            "content": "In this paper, we presented protocol to refine the current training methods of LLMs based on experiments showing oscillatory behaviour with respect to hallucinations throughout training (Figure 1). To do this we used the internal states of LLMs, specifically the penultimate layer activations during inference on specialized dataset. We present an initial method of reducing hallucinations based on the principles of EigenScore metrics introduced by Chen et al. (2024). We showed empirically that our Sensitive Neuron detection method significantly reduces the EigenScore on inference of LLMs throughout various stages of training (Figure 2). Following the success of the Sensitive Neuron method, we moved on to the application of hallucination reduction method on training. We show through finetuning that we are able to fix the oscillatory behaviour initially seen throughout training and reduce the EES of finetuned models as shown in Figure 4 by modifying the internal mechanics of training with SEnsitive Neuron Dropout. At test time we achieve 40% increase in FactScore performance, verifying that SeND provides substantial improvement to current training protocols. To further advance our work, we plan to scale SeND to larger datasets and models, as current experiments were limited by compute constraints with larger LLMs. Demonstrating SeNDs effectiveness on larger open-source models like Metas LLaMA 3.1 (Dubey et al., 2024) will provide crucial evidence for organizations developing state-of-the-art LLMs to incorporate SeND into their training protocols, ultimately improving model safety. Given that SeND targets variance reduction during training, we anticipate even greater gains on larger LLMs, where the higher inherent variance may amplify the regularization effect and lead to more significant improvements. 10 Preprint. Under review."
        },
        {
            "title": "REFERENCES",
            "content": "Jimmy Ba and Brendan Frey. training deep neural networks. In Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL https://papers.nips.cc/paper_files/paper/2013/hash/ 7b5b23f4aadf9513306bcd59afb6e4c9-Abstract.html. Adaptive dropout for Pierre Baldi and Peter Sadowski. Information Processing Systems, ral URL 71f6278d140af599e06ad9bf1ba03cb0-Abstract.html. In Advances in Neu2013. Inc., https://papers.nips.cc/paper_files/paper/2013/hash/ volume 26. Curran Associates, Understanding Dropout. Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: Suite for Analyzing Large Language Models Across Training and Scaling, May 2023. URL http://arxiv.org/abs/ 2304.01373. arXiv:2304.01373 [cs]. Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: An Open-Source Autoregressive Language Model, April 2022. URL http://arxiv.org/ abs/2204.06745. arXiv:2204.06745 [cs]. Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. INSIDE: LLMs Internal States Retain the Power of Hallucination Detection, February 2024. URL http: //arxiv.org/abs/2402.03744. arXiv:2402.03744 [cs]. Kun Dong, Austin R. Benson, and David Bindel. Network Density of States. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 11521161, July 2019. doi: 10.1145/3292500.3330891. URL http://arxiv.org/abs/ 1905.09758. arXiv:1905.09758 [cs, math]. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana 11 Preprint. Under review. Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzman, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vıtor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi 12 Preprint. Under review. Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The Llama 3 Herd of Models, July 2024. URL https://arxiv.org/abs/2407.21783v2. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, July 2024a. URL https://zenodo.org/records/ 12608602. Version Number: v0.4.3. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-Augmented Generation for Large Language Models: Survey, March 2024b. URL http://arxiv.org/abs/2312.10997. arXiv:2312.10997 [cs]. Marisa Garcia. What Air Canada Lost In Remarkable Lying AI Chatbot Case, February 2024. URL https://www.forbes.com/sites/marisagarcia/2024/02/19/ what-air-canada-lost-in-remarkable-lying-ai-chatbot-case/. Section: Aerospace & Defense. Giwon Hong, Aryo Pradipta Gema, Rohit Saxena, Xiaotang Du, Ping Nie, Yu Zhao, Laura PerezBeltrachini, Max Ryabinin, Xuanli He, Clementine Fourrier, and Pasquale Minervini. The Hallucinations Leaderboard An Open Effort to Measure Hallucinations in Large Language Models, April 2024. URL http://arxiv.org/abs/2404.05904. arXiv:2404.05904 [cs]. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions, November 2023a. URL http://arxiv.org/abs/2311.05232. arXiv:2311.05232 [cs]. Shenyang Huang, Jacob Danovitch, Guillaume Rabusseau, and Reihaneh Rabbany. Fast and Attributed Change Detection on Dynamic Graphs with Density of States, May 2023b. URL http://arxiv.org/abs/2305.08750. arXiv:2305.08750 [cs]. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16011611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/ P17-1147. Jannik Kossen, Jiatong Han, Muhammed Razzak, Lisa Schut, Shreshth Malik, and Yarin Gal. Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs, June 2024. URL http://arxiv.org/abs/2406.15927. arXiv:2406.15927 [cs] version: 1. Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. HaluEval: LargeScale Hallucination Evaluation Benchmark for Large Language Models, October 2023. URL http://arxiv.org/abs/2305.11747. arXiv:2305.11747 [cs]. Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models, January 2024. URL http://arxiv.org/abs/2401.03205. arXiv:2401.03205 [cs]. Lin Lin, Yousef Saad, and Chao Yang. Approximating spectral densities of large matrices, October 2014. URL http://arxiv.org/abs/1308.5467. arXiv:1308.5467 [math]. Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models, October 2023. URL http: //arxiv.org/abs/2303.08896. arXiv:2303.08896 [cs]. 13 Preprint. Under review. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation, October 2023. URL http://arxiv.org/ abs/2305.14251. arXiv:2305.14251 [cs]. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Dont Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization, August 2018. URL http://arxiv.org/abs/1808.08745. arXiv:1808.08745 [cs]. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Med-HALT: Medical Domain Hallucination Test for Large Language Models, October 2023. URL http://arxiv.org/ abs/2307.15343. arXiv:2307.15343 [cs, stat]. Vipula Rawte, Amit Sheth, and Amitava Das. Survey of Hallucination in Large Foundation Models, September 2023. URL http://arxiv.org/abs/2309.05922. arXiv:2309.05922 [cs]. Bikash Santra, Angshuman Paul, and Dipti Prasad Mukherjee. Deterministic dropout for deep neural networks using composite random forest. Pattern Recognition Letters, 131:205212, ISSN 0167-8655. doi: 10.1016/j.patrec.2019.12.023. URL https://www. March 2020. sciencedirect.com/science/article/pii/S0167865519303988. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research, 15(56):19291958, 2014. ISSN 1533-7928. URL http://jmlr.org/ papers/v15/srivastava14a.html. Weihang Su, Changyue Wang, Qingyao Ai, Yiran HU, Zhijing Wu, Yujia Zhou, and Yiqun Liu. Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models, June 2024. URL http://arxiv.org/abs/2403.06448. arXiv:2403.06448 [cs]. Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is Inevitable: An Innate Limitation of Large Language Models, January 2024. URL http://arxiv.org/abs/2401.11817. arXiv:2401.11817 [cs]. Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. Cognitive Mirage: Review of Hallucinations in Large Language Models, September 2023. URL http://arxiv.org/ abs/2309.06794. arXiv:2309.06794 [cs]. Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, and Tat-Seng Chua. RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback, March 2024. URL http://arxiv.org/abs/2312.00849. arXiv:2312.00849 [cs]."
        },
        {
            "title": "A ADDITIONAL EXPERIMENTS",
            "content": "A.1 DRASTIC EMBEDDING CHANGES LEADING TO SENSITIVE NEURONS Looking at internal states of the model allows getting deeper understanding of the dynamics that could be leading to the oscillatory behaviour seen in Figure 1. To do this, we record the net change (Definition 3.2) between checkpoints of the penultimate layer where one checkpoint would be the correct answer and the next would hallucinate. This net change with respect to various different input texts is plotted in Figure 5. It can be observed that there were specific embedding activations that experienced drastically more change relative to the rest of the embeddings. This is the main source of motivation to further define Sensitive Neurons (Definition 3.3). 14 Preprint. Under review. Figure 5: Net change of sentence embeddings between checkpoints 125,000 and 143,000. Each different colour is different input text. As depicted, there are specific neurons that go through drastic changes between the two checkpoints of the training regardless of the input. EFFICIENT EIGENSCORE (EES) DERIVATION B.1 BACKGROUND: CHEBYSHEV POLYNOMIALS Chebyshev polynomials are sequence of orthogonal polynomials in the interval [1, 1] orthogonality property shown in equation 8 that are widely used in numerical analysis, approximation theory, and other areas of applied mathematics. In this work, we are mainly concerned with the Chebyshev polynomials of the first kind with the recurrence relation shown in equation 9. Note that this recurrence could also be applied to matrices. Any function defined in the interval [1, 1] can be approximated with the Chebyshev expansion as shown in 10. (cid:90) 1 1 2 (1 + δ0n)π 1 x2 Tm(x)Tn(x) dx = δmn, where δmn = (cid:26)1 0 if = n, if = n, T0(x) = 1, T1(x) = x, Tn+1(x) = 2x Tn(x) Tn1(x), for 1. (cid:88) (x) = cnTn(x), where cn = (cid:90) 1 n=0 2 π 1 (cid:90) 1 1 dx for > 0, (x)Tn(x) 1 x2 (x) 1 x2 dx. c0 = 1 π B.2 BACKGROUND: DOS AND KPM (8) (9) (10) (11) (12) Let be symmetric matrix RN with an eigendecomposition = QΛQT , where Λ = diag(λ1, , λN ) and = [q1, , qN ] is orthogonal. The spectral density induced by is the generalized function: δ(λ λi), (13) µ(λ) = 1 (cid:88) i= 15 Preprint. Under review. where δ is the Dirac delta function. For any analytic test function , the integral of with respect to µ is: (cid:90) (λ)µ(λ) dλ = trace(f (H)). (14) Dong et al. (2019) introduced KPM as numerical technique to approximate DOS. KPM approximates DOS by expanding it in terms of chebyshev polynomials. Requiring the matrixs spectrum to be supported in the interval [1, 1], KPM approximates DOS with the following formula, λ being the eigen value of the matrix and dm approximated by Stochastic Trace Estimation: (cid:88) µ(λ) = dmT m(λ), where dm = and dm and m(x) = m=1 1 trace(Tm(H)), Nz(cid:88) zT Tm(H)zj, 1 1 Nz j=1 2 (1 + δ0m)π 1 Tm(x). (15) (16) (17) (18) In the application for hallucination detection, we can use equation 14 to derive formula for the EigenScore approximation using the properties of Chebyshev polynomials and DOS. B.3 STOCHASTIC TRACE ESTIMATION ON EMBEDDING MATRIX We are interested in computing the dm term of DOS relying solely on the embedding matrix therefore we need to rewrite dm as follows: dm = 1 1 Nz (cid:88) j=0 Tm(ET E)zj zT (19) where Tm can be computed using the Chebyshev polynomials of matrix = ET E. T0(ET E)zj = Izj = zj, T1(ET E)zj = ET Ezj, Tm+1(ET E)zj = 2ET ETm(ET E)zj Tm1(ET E)zj Each term can be computed with matrix-vector multiplication. B.4 EES INTEGRAL CALCULATION Given the orthogonality of the Chebyshev polynomials, we can simplify the integral mentioned in proposition 1. To approximate the EigenScore, we will expand log(λ) in terms of Chebyshev polynomials and use their orthogonality to simplify the integral. Expanding and Integrating To approximate the integral: (cid:90) 1 log(λ)µ(λ) dλ 16 (20) Preprint. Under review. Substitute the Chebyshev Expansion for DOS: µ(λ) (cid:88) m=0 dmT m(λ) (21) where: m(λ) = w(λ)Tm(λ) = π 2 1 λ2(1 + δ0m) Tm(λ) Distribute log(λ) in the integral: (cid:90) 1 log(λ) (cid:32) (cid:88) m=0 (cid:33) dmT m(λ) dλ = 1 M (cid:88) m=0 (cid:90) dm log(λ)T m(λ) dλ (22) Evaluate the Integral Using Orthogonality: To simplify the integral, (cid:90) log(λ)T m(λ) dλ First, express log(λ) as series of Chebyshev polynomials: log(λ) = (cid:88) m=0 cmTm(λ) Then: (cid:90) 1 0 log(λ)T m(λ) dλ = (cid:90) 1 (cid:32) (cid:88) 0 m=0 (cid:33) cmTm(λ) Tm(λ) dλ Note: The lower bound of the integral is 0 as the matrix is defined in the spectrum [0, 1]. Using the orthogonality, we get: So the integral simplifies to: cm = (cid:90) 1 0 log(λ)T m(λ) dλ 1 (cid:88) m=0 dmcm (23) (24) (25) (26) (27) (28) B.5 EFFICIENT EIGENSCORE MOMENTS Figure 6 presents the effect of using different moment values as the number of matrix rows increases with respect to time. This is an important hyperparameter to tune as increasing the number of moments on EES correlates to having more accurate and representative approximation of the EigenScore. We observe that as moments in EES increase, the time to calculate EES increases. From this result, we conclude that selecting moment value of under 50 would provide balanced trade-off between accuracy and calculation time. 17 Preprint. Under review. Figure 6: Effect of changing number of moments on EES calculation time (seconds). More moments gives more accurate approximation but higher computation time. Figure 7: Performance of SeND on Pythia 1B wih HELM dataset computed with both EES and regular EigenScore. EES is able to closely track the true EigenScore performance metric, showing that it is good approximator. B.6 EIGENSCORE AND EES TRAINING TRAJECTORIES To demonstrate that our EigenScore approximation method, EES, is good metric, we record the progress of Pythia 1B finetuning on the HELM dataset using both EigenScore and EES hallucination performance metrics (Figure 7). Albeit different scale and window, the trajectories, magnitude and shape of the graphs are nearly identical while EES takes only 4 minutes to calculate and EigenScore takes approximately 8, an astounding 2x increase in compute speed. These results show that our metric closely resembles the target metric while greatly reducing the required computational resources."
        }
    ],
    "affiliations": [
        "McGill University, Montreal, Canada",
        "Mila - Quebec AI Institute, Montreal, Canada",
        "Polytechnique Montreal, Montreal, Canada",
        "Universite de Montreal, Montreal, Canada"
    ]
}