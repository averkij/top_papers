{
    "paper_title": "WaterDrum: Watermarking for Data-centric Unlearning Metric",
    "authors": [
        "Xinyang Lu",
        "Xinyuan Niu",
        "Gregory Kang Ruey Lau",
        "Bui Thi Cam Nhung",
        "Rachael Hwee Ling Sim",
        "Fanyu Wen",
        "Chuan-Sheng Foo",
        "See-Kiong Ng",
        "Bryan Kian Hsiang Low"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language model (LLM) unlearning is critical in real-world applications where it is necessary to efficiently remove the influence of private, copyrighted, or harmful data from some users. However, existing utility-centric unlearning metrics (based on model utility) may fail to accurately evaluate the extent of unlearning in realistic settings such as when (a) the forget and retain set have semantically similar content, (b) retraining the model from scratch on the retain set is impractical, and/or (c) the model owner can improve the unlearning metric without directly performing unlearning on the LLM. This paper presents the first data-centric unlearning metric for LLMs called WaterDrum that exploits robust text watermarking for overcoming these limitations. We also introduce new benchmark datasets for LLM unlearning that contain varying levels of similar data points and can be used to rigorously evaluate unlearning algorithms using WaterDrum. Our code is available at https://github.com/lululu008/WaterDrum and our new benchmark datasets are released at https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax."
        },
        {
            "title": "Start",
            "content": "WaterDrum: Watermarking for Data-centric Unlearning Metric Xinyang Lu * 1 Xinyuan Niu * 1 2 Gregory Kang Ruey Lau * 1 3 Bui Thi Cam Nhung 1 Rachael Hwee Ling Sim 1 Fanyu Wen 1 Chuan-Sheng Foo 2 See-Kiong Ng 1 Bryan Kian Hsiang Low 1 5 2 0 2 8 ] . [ 1 4 6 0 5 0 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language model (LLM) unlearning is critical in real-world applications where it is necessary to efficiently remove the influence of private, copyrighted, or harmful data from some users. However, existing utility-centric unlearning metrics (based on model utility) may fail to accurately evaluate the extent of unlearning in realistic settings such as when (a) the forget and retain set have semantically similar content, (b) retraining the model from scratch on the retain set is impractical, and/or (c) the model owner can improve the unlearning metric without directly performing unlearning on the LLM. This paper presents the first data-centric unlearning metric for LLMs called WaterDrum that exploits robust text watermarking for overcoming these limitations. We also introduce new benchmark datasets for LLM unlearning that contain varying levels of similar data points and can be used to rigorously evaluate unlearning algorithms using WaterDrum. Our code is available at GitHub and our new benchmark datasets are released at HuggingFace. 1. Introduction The capabilities of large language models (LLMs) have drastically improved in recent years, prompting increased efforts to deploy LLMs in real-world applications. However, accompanying this push for practical LLM deployment are growing concerns around data issues regarding LLMs that may threaten to derail such developments, especially since LLMs typically require large amounts of training *Equal contribution 1Department of of Singapore, 2Centre for Singapore Science, National University 117417 A*STAR, Way, Create <{xinyang.lu,rachael.sim,wenfanyu}@u.nus.edu, {niux,greglau,btcnhung,lowkh}@comp.nus.edu.sg, seekiong@nus.edu.sg, foo chuan sheng@i2r.a-star.edu.sg>. Frontier AI Research 3CNRS@CREATE, 1 Singapore #08Tower, Computer Singapore (CFAR), Create 138602. data. Data owners have raised intellectual property (IP) infringement concerns: For example, the New York Times has sued OpenAI over its LLMs use of their copyrighted work (Grynbaum and Mac, 2023). Many jurisdictions are also paying increased scrutiny over data privacy concerns, e.g., with regulations such as the General Data Protection Regulation (GDPR, 2016) and the California Consumer Privacy Act (CCPA, 2018) mandating the right to be forgotten that allow users to request the erasure of their data from the trained models. Furthermore, it is also not uncommon for public data to become outdated or to be found erroneous/harmful, e.g., the retraction of public scientific papers with fabricated data (Hu et al., 2024). These data concerns have sparked considerable research efforts on LLM unlearning algorithms, which aim to efficiently remove the influence of subset of the models original training data (called the forget set) while avoiding the prohibitively expensive alternative of retraining the model from scratch on the retain set. However, due to the size and complexity of LLMs, existing unlearning algorithms cannot yet achieve complete unlearning: They may not be able to fully remove the influence of all data in the forget set, and may also inadvertently remove the influence of data in the retain set that should be preserved (Maini et al., 2024; Shi et al., 2024b). This brings up natural question: How can we measure the extent to which these algorithms have unlearned given set of data? Existing works have largely proposed utility-centric unlearning metrics that evaluate unlearning based on model utility (performance) indicators, such as the perplexity or accuracy on downstream tasks. After unlearning, the model utility indicators related to the forget set are expected to worsen. We provide an overview of unlearning metrics and position our work in App. A. However, are these utility-centric metrics effective in the face of practical challenges with real-world datasets? In real-life settings, it is (a) common for the forget and retain set to have semantically similar content, (b) typical to be prohibitively expensive to retrain an LLM, and (c) possible that an LLM owner might attempt to improve the metric without directly performing LLM unlearning to reduce cost. In Sec. 5, we will show that utility-centric metrics fall short and we have identified three reasons. First, expecting worse WaterDrum: Watermarking for Data-centric Unlearning Metric utility on the forget set after unlearning ignores the ability of the LLMs to generalize from the retain set (Liu et al., 2024a). Second, these metrics require the retrained LLM to obtain reference values to evaluate the success of unlearning, which is not obtainable in practice. Finally, these metrics are also not resilient as model owner can improve them without directly performing unlearning on the LLM with the threat model in Sec. 2.3. modify the original model φT based on DF to generate an unlearned model φR that approximates φR. Such an unlearned model may not have fully unlearned the forget set and could be intuitively viewed as retaining the influence of some subset of the forget set data DG DF and hence still be effectively influenced by its approximate retain set DR = DR DG. The best unlearned models should have the size DG be as small as possible. In this work, we first take into consideration the above limitations to (a) define clear desiderata that an effective, practical, and resilient unlearning metric should satisfy (Sec. 2). Next, we (b) propose novel data-centric approach to evaluating LLM unlearning instead. Specifically, we develop Watermarking for Data-centric Unlearning Metric (WaterDrum) that satisfies these desiderata, based on robust text watermarking framework Waterfall (Lau et al., 2024) that is capable of verifying multiple data owners watermarks in LLM outputs when the LLM is trained on their watermarked text data (Sec. 3). Our key insight is that using watermarked data creates clear counterfactual model that is not trained on watermarked data would not contain the watermark signal. As existing benchmark datasets are insufficient to verify our desiderata, we (c) propose new empirical evaluation methods and an accompanying new benchmark dataset WaterDrum-Ax that includes data from multiple parties and contains duplicates with varying degrees of similarity. This benchmark could pave the way for future work to develop more effective and practical unlearning metrics and algorithms. Finally, in Sec. 5, we (d) empirically show that our proposed unlearning metric WaterDrum significantly outperforms existing metrics at satisfying our desiderata. We (e) also use WaterDrum to benchmark unlearning algorithms to illustrate their strengths and weaknesses. 2. Problem Formulation and Desiderata We consider setting with data owners, , where each data owner possesses dataset Di. These datasets may contain similar data instances (e.g., news articles on the same event or blogs on the same topic, with example in App. G.3). The model owner aggregates their data DT := (cid:83)N i=1 Di and trains an LLM model φT , which is deployed as service. We consider the unlearning scenario where subset of data owners, F, requests to erase their data, DF := (cid:83) iF Di (the forget set), from the LLM due to concerns about privacy, IP protection or erroneous content. Ideally, the model owner would retrain new model, φR, on the remaining set of data, DR := DT DF (the retain set), to comply with these unlearning requests. However, full retraining is impractical in reality due to the prohibitive computational cost, especially when DR is large. Instead, the model owner will resort to unlearning algorithms that In most practical scenarios, data owners have only query access to the model. Let denote the query function, which maps data point or dataset to corresponding text/set of queries, q(d) or q(D), formed based on the given data. For ease of notation, we abbreviate q(D) as q. For example, qF denotes the queries formed based on DF . To analyze whether the model owner has unlearned their dataset Di, the data owner could rely on some LLM output, such as φ(q(d)) or φ(qi), to compute an unlearning metric that quantifies the extent to which their data remains present in the output. Specifically, we define an unlearning metric where (φ(q(d)); i) and (φ(qi); i), respectively, measure the influence of is data (second term) detectable in the LLM output to queries q(d) or q(Di). Additionally, to ease notation, we also use to measure the influence of set of owners, for instance, (φ(q(d)); F) measures the influence of the forget set Fs data detectable in the LLM output. The metric should satisfy the following non-exhaustive desiderata. 2.1. Effectiveness First, the metric must effectively measure the extent of To assess potential metrics on this, we unlearning. benchmark them against the ground truth unlearning algorithm, i.e., retraining the model on only the retained dataset to obtain φR. By construction, φR is guaranteed not to contain any influence of the forget set DF and fully contain that of the retained data DR. Naturally, the metric evaluated for DR on the retrained LLM φR should be approximately the same as that of the original LLM φT , i.e., (φR(qR); R) (φT (qR); R). Beyond this baseline requirement, we propose two key effectiveness desiderata: D1 Separability. The metric should be able to classify whether an owner still has influence on an unlearned LLM model. Specifically, when evaluated on the retrained LLM φR, the metric should, with high probability, produce higher values when measured on output based on queries related to the retain set DR (which influences φR) than queries related to the forget set DF (which does not). For any data point dr Dr DR by owner and df Df DF by owner , the probability P[M (φR(q(dr)); r) > (φR(q(df )); )] 1 . (1) 2 WaterDrum: Watermarking for Data-centric Unlearning Metric Equation (1) implies that there exists threshold κ such that, for any data point di Di DT by owner i, small value (φR(q(di)); i) < κ indicates that Di is unlikely part of the retain set DR. Similarly, when considering an unlearned model, small value ( φR(q(di)); i) indicates that Di is unlikely part of the approximate retain set DR. In other words, the metric serves as good classifier for whether an owners data still influences the model, with higher AUROC indicating better separability (Fawcett, 2006). The next desideratum helps to identify κ used for classification. D2 Calibration. In Sec. 1, we highlight that the existing unlearning algorithms cannot yet achieve complete unlearning. Thus, our unlearning metric should be calibrated to the extent of imperfect unlearning. For example, we can simulate imperfect unlearning by retraining with different-sized subsets of the forget set. The metric should be proportional to the size of DG, the subset of the forget set that is not unlearned in the LLM φRG. Specifically, given random subset DG DF that is retained, unlearning, the metric should only depend on the queried output instead of full access to the weights or token probabilities of unlearned model φF . D4 Robustness to similar data. The effectiveness desiderata D1-D2 should hold for any DR and DF , including typical scenarios where DR and DF have similar data points, e.g., new agencies have different articles reporting on the same event. Let denote that text and have large similarity score (SS), SS(a, b), e.g., computed with some semantic text similarity (STS) score, and Di Dj denote sets where di Di, there is corresponding dj Dj where di dj. The desiderata D4 is challenging because the similarity of data points dr and df in the retain and forget set often implies that the corresponding LLM model outputs will also be similar, i.e., φ(q(dr)) φ(q(df )). This will make it hard for many model utility-centric metrics to satisfy both the separability and the calibration desiderata and further motivate the need to adopt more data-centric unlearning metrics, as we will see in Sec. 5. [M (φRG(qF ); F)] DG DF . (2) 2.3. Resilience Note that Equation (2) implies that fully unlearned model such as φR should have (φR(qF ); F) = 0. This means that the classification threshold κ in D1 should be close to 0, i.e., when evaluating unlearning algorithms, we identify successful complete unlearning of the forget set by looking for ( φR(qF ); F) 0. In addition, the magnitude of the metric could be intuitively interpreted as the extent to which the forget set has not been unlearned. This enables the unlearning metric to go beyond being just binary indicator of whether an entire forget set has been unlearned, to meaningful continuous score of unlearning. Further discussion is given in App. C. 2.2. Practicality To be viable metric for deployment, the metric must also satisfy the following additional feasibility and robustness desiderata that account for challenges faced in common real-life scenarios: D3 Feasibility. (a) When the metric is used to evaluate the unlearned model φR and produce ( φR(qi); i), it should not require the retrained model φR. The premise of unlearning is that retraining the model on the retain set is prohibitively expensive. Hence, metrics cannot depend on φR in practice. However, as we will see in Sec. 3.1 and Sec. 5.3, many existing metrics cannot satisfy D2 without access to φR, which limits their practicability. (b) In addition, to enable data owners with only query-access to the model to evaluate Finally, we need to consider the realistic scenario in which the model owners interests may not align with those of the data owners. As full unlearning is costly, the model owner is incentivized to avoid it while appearing to fulfil the data owners erasure requests. As the model owner is aware of the metric used, they can attempt to improve the metric through threat model without directly performing unlearning if doing so is less costly. To analyze this, we consider the scenario where the model owner continues to use the existing model φT instead of spending resources to unlearn DF (and produce φR). Threat model. The model owner implements the threat model that involves simulating decoy unlearned model ˆφF with gating functionto intercept any query qi that is received. For metrics that it could compute exactly, the model owner would filter queries that result in output with signals that indicate that the underlying model is still the full model φT with influence from the forget set DF , e.g., queries qi where (φT (qi); F) > κ, and replace them with some text k(qi, DF ) that minimizes the metric signal. For metrics that the model owner cannot compute exactly (e.g., metrics with computation that require some information that is private to the data owner), the model owner can only resort to proxy indicator SS that measures how similar query qi is to the forget set DF , for the filter: (cid:40) ˆφF (qi) = k(qi, Df ) φT (qi) if Df DF , SS(qi, qf ) > , otherwise. (3) 3 WaterDrum: Watermarking for Data-centric Unlearning Metric In practice, for k(qi, DF ), the model owner can generate an output that minimizes the score of metric , such as by replacing it with output from another untrained model. Note that in situations where Equation (3) is applied, the model owner will realistically only intercept queries with large SS threshold B. Performing this for small threshold will harm overall model performance with more decoy output replacements and will be more costly in the extreme scenario, this approach intercepts all queries and would essentially be comparable to full unlearning algorithm. The final desideratum is for the metric to be resilient against such threat model. D5 Resilience. The metric should meet all the above desiderata, despite the model owner potentially implementing threat model in Equation (3). 3. Methodology In this section, we first analyze how existing unlearning effectiveness metrics face challenges in meeting the desiderata described in Sec. 2, before presenting WaterDrum, our data-centric unlearning metric based on watermarking that satisfies them. Moreover, in App. A, we provide deeper introduction of utility-centric and other unlearning metrics. 3.1. Challenges for utility-centric metrics Utility-centric unlearning metrics evaluate unlearning effectiveness based on model utility (performance) indicators, such as verbatim memorization, perplexity, and accuracy on downstream tasks. Performance indicators compare the output on queries (e.g., φR(qF ) about the forget set) to the original data (e.g., DF ). For instance, ROUGE-L (Maini et al., 2024) compares the output phrasing/longest common subsequence of φR(qF ) and the training data DF . As another example, some membership inference attacks (MIA) based unlearning metrics (Shokri et al., 2017), such as (Shi et al., 2024a), are utility-centric as they may depend on the log-likelihood of tokens of the original text data. However, such performance indicators do not meet our required desiderata for the metric . First, D3(a) does not allow retraining the model. Without retraining, the value (φR(qF ), DF ) cannot be known and thus cannot be used to ensure that the metric produces value close to 0 when the forget set is fully unlearned (e.g., it is not possible to define and compute as ( φR(qF ), DF ) (φR(qF ), DF )). Thus, without retraining, does not satisfy D2, making it difficult to identify successful unlearning of the forget set. Next, when there are similar data present in the forget and retain set (D4), we observe that any unlearned model φR (e.g., the retrained model φR) tend to produce similar Table 1: Comparison of unlearning metrics based on the proposed desiderata. We enforce D3, thus the metrics cannot rely on the retrained model. D1 and D2 consider the setting with no similar data and with an honest model owner. ROUGE (Maini et al., 2024) Truth Ratio (Maini et al., 2024) KnowMem (Shi et al., 2024b) MIA (Shi et al., 2024a) WaterDrum (ours) D1 D2 D4 D5 outputs on queries on both sets, that is, φR(qF ) φR(qR) (as empirically verified in App. F.2). As the performance indicators largely depend on direct comparisons with the model outputs, the indicator values will also be similar, i.e., ( φR(qF ), DF ) ( φR(qR), DR). In Sec. 5, we show that this leads to utility-centric metrics failing to satisfy D1 when the data from the forget and retain set have high or moderate similarities. The failure arises because expecting poor predictions on the forget set and low ( φR(qF ), DF ) overlooks the generalization capability of LLMs (Liu et al., 2024a). Lastly, the model owner can directly measure the performance (φT (qf ), Df ) for any query qf on data from in the forget set, such as that of the ROGUE-L score. Hence, under the threat model T, the model owner can fully intercept any queries carrying the signal that the forget set remains in the model. Thus, utility-centric metrics may not satisfy D5 and an alternative metric that depends on private information is needed. In Table 1, we compare existing metrics and our metric based on the desiderata. 3.2. Watermarking as Unlearning Metric To overcome these challenges and satisfy our desiderata, we propose to adopt novel data-centric approach to evaluating unlearning instead. Instead of relying on utility-centric metrics that indirectly infer unlearning via model performance, we directly track the presence of data by actively embedding data-specific signals in the LLM output that are designed to be orthogonal to its performance. In App. A, we highlight how WaterDrum differs from existing watermarking-based metrics for image classification tasks. In Sec. 3.3, we start by outlining desiderata required by watermarking framework (and its verification operator) to meet our unlearning metric desiderata laid out in Sec. 2. 3.3. Watermarking desiderata In our watermarking framework, each data owner is assigned unique private watermark key µi. There are two key operators that they can perform: (1) watermarking operator W(di, µi) that takes any text data di 4 WaterDrum: Watermarking for Data-centric Unlearning Metric"
        },
        {
            "title": "Unlike",
            "content": "Figure 1: existing utility-centric metrics, WaterDrum satisfy the desiderata in Sec. 2. WaterDrum is robust to similar data as Waterfall embed orthogonal data-specific signals in the LLM output that are W1 verifiable. Di and produces corresponding text data uniquely associated with watermark µi, and (2) verification operator V(g, µi) that takes in any text data such as an output from an LLM model and reflects the likelihood that contains the watermark µi. To meet our unlearning metric desiderata in Sec. 2, the watermark and verification operators used in the framework above will need to satisfy the following desiderata: W0 Fidelity. The watermarking should have minimal impact on the semantic similarity of the original data, i.e., W(d, µ) for any µ and data DT . While this does not directly impact the unlearning desiderata, W0 ensures that the watermarking process preserves the value of the data for the model owner and the metric can be deployed in practice. W1 Verifiability. (a) The watermark should be verifiable if and only if the watermarked content is present in the model. In our setting, this implies that the retrained model should not contain the watermark of an owner in who requested to erase its data, i.e., V(φR(qf ), µf ) = 0. In contrast, model that has been trained on owner data Df DF should have verifiable watermark µf , i.e., V(φF (qf ), µf ) > 0. (b) If every text data in DF is watermarked with the same key µF , the value V(φRG(qF ), µF ) should be proportional to the size of the data DG DF . Together, (a) and (b) support D1 and D2. W2 Overlap verifiability. The verifiability desiderata W1 is satisfied despite the presence of other watermarks (e.g., µr from another owner r) in the training dataset of the model. This allows for multiple watermarks to be evaluated from the output of the same model. We will also need additional desiderata on the watermarking process to meet the rest of the unlearning metric desiderata: Figure 2: Overview of the watermarking process of WaterDrum W3 Query-access constraint. The data owners should be able to verify the watermark with only query-access to the model. This supports D3, allowing for feasible and efficient evaluation of unlearning. W4 Unique key. Each data owner is watermark key µi should be unique. When forget set data owner requests full erasure of its data, the forget and retain sets will have different watermarks, with different strengths, thus supporting D1. Furthermore, the unique keys ensure that similar or even identical data from different owners will have different watermarks, which supports D4. W5 Private key. Each data owner watermark key µi should be private and unknown by the model owner. This provides some defense against the threat model described in Sec. 2 and supports D5. Figure 1 summarizes how framework that satisfies these desiderata can satisfy the unlearning metric desiderata of Sec. 2. 3.4. Overview of WaterDrum To satisfy the watermarking desiderata presented above in Sec. 3.3, we propose WaterDrum, an unlearning metric built on top of our adaptation of the scalable and robust Waterfall framework (Lau et al., 2024) which can successfully verify multiple owners watermarks in LLM outputs when the LLM is trained on their watermarked text. Specifically, we adopt the watermarking W(, µ) and verification V(, µ) operators as defined in Waterfall, and define the WaterDrum metric as: (φ(qi); i) := 1 i (cid:88) dD V(φ(q(d)), µi), (4) where is dataset watermarked by data owner with key µi, and φ is any model that is being evaluated. For composite datasets, such as , that may consist of data from multiple owners i, we overload the WaterDrum metric notation to be the weighted average across the 5 WaterDrum: Watermarking for Data-centric Unlearning Metric different subsets: (φ(qF ); F) := 1 (cid:88) iF i (φ(qi); i) (5) Waterfalls watermarking and verification approaches satisfy the watermarking desiderata W0, W1(a) and W2, as elaborated and demonstrated in (Lau et al., 2024) (we verify W0 in App. F.1). We empirically verified that the Waterfall method satisfies W1(b) on calibration in Sec. 5.3. The rest of the watermarking process desiderata can be satisfied by an appropriate design of the unlearning and evaluation process, which we illustrate in Figure 2 and present below: fixed forget DF and retain DR datasets, and do not represent practical scenarios where there are multiple data owners who could decide independently whether to erase their data; and (b) Similar data. Both datasets do not measure and control for range of data similarity across DF and DR, and hence cannot support evaluations on unlearning metrics for D4 and unlearning algorithms on their ability to unlearn data in DF that are similar to those in DR. In fact, Thaker et al. (2024) have also identified that in these popular benchmark datasets, the forget and retain sets are disjoint (the queries on the forget set are related only to the forget set and are unrelated to the retain set) and the performance of the unlearning methods declines sharply if dependencies between both sets are introduced. This underscores the importance of considering less disjoint and more similar datasets. P1 Watermarking setup. Each data owner first watermarks its data Di with unique private key µi to generate watermarked dataset = W(di, µi) di Di}, before the model owner aggregates their watermarked data i=1 i, trains model φ on it, and offer to clients (including data owners) query-access to it. := (cid:83)N := {d := (cid:83) P2 Unlearning. subset of data owners requests that their data iF Di be erased from the model φ . The model owner will claim to have done the unlearning, and offer query-access to new model φ R. P3 Unlearning verification. The verification operator takes the role of the uncertainty metric in WaterDrum, as per Equation (4). In most cases, each data owner in can query the unlearned model φ with queries based on and apply the verification operator V( φR(q to measure the extent that their data has been unlearned. Other queries can be performed for more challenging situations, such as under threat model, as described later in Sec. 5.4. ), µf ) with queries Note that WaterDrum in Equation (4) applied during P3 only requires query-access to the model, hence satisfying W3. Watermarking desiderata W4 and W5 are also satisfied by the setup in P1 and the fact that the model owner never requires the data owners keys, including in P2. In App. G.1, we explain why the process is practical and discuss other deployment details. 4. The WaterDrum-Ax Dataset Apart from good unlearning metrics, suitable unlearning benchmark datasets are also critical for evaluating and developing practical unlearning algorithms. However, existing benchmark datasets such as TOFU (Maini et al., 2024), MUSE (Shi et al., 2024b) and WMDP (Li et al., 2024b) may not represent the realistic challenges outlined in our problem setting (Sec. 2) as they lack: (a) Realistic forget-retain splits. Both TOFU and MUSE only have 6 dataset, benchmark To address these limitations, we introduce complementary WaterDrum-Ax. unlearning WaterDrum-Ax, comprising ArXiv paper abstracts across various categories published after the release of the Llama-2 model, includes (a) abstracts from the 20 most popular academic subject categories to represent 20 different data owners that can be freely assigned to define DF and DR; and (b) varying levels of data similarity ranging from exact duplicates to paraphrased versions of the abstracts that can be used across DF and DR to support evaluation of the practicality and resilience of the unlearning metrics, especially the assessment of D4 on robustness to similar data. Overall, WaterDrum-Ax contains 400 abstracts for each category, aggregating to total of 8000 data points in WaterDrum-Ax. These abstracts have an average length of 260 tokens, which is considerably longer than that of (Maini et al., 2024) (59 tokens). The WaterDrum-Ax benchmark dataset can be used to: (i) evaluate unlearning metrics based on the desiderata introduced in Sec. 2, and (ii) evaluate unlearning algorithms on effective and practical metrics identified in (i). The empirical evaluations in Sec. 5 focus on (i) but include some preliminary results on (ii) in Sec. 5.5. We leave more systematic investigations of (ii) to future work. 5. Experiments primary setup. Our Experimental experiments were conducted on the WaterDrum-Ax (Sec. 4) and WaterDrum-TOFU 1 (derived from TOFU (Maini et al., 2024), details in App. B) benchmark datasets, with the pre-trained Llama-2-7B (Touvron et al., 2023) as the base model. This model was finetuned with different data subsets under various settings. For the following experiments, we consider the last 1 class from WaterDrum-Ax and 1https://huggingface.co/datasets/Glow-AI/ WaterDrum-TOFU WaterDrum: Watermarking for Data-centric Unlearning Metric the last 10% data from WaterDrum-TOFU as the forget sets. We also conducted experiments on Phi-1.5 (Li et al., 2023) detailed in App. E.2. For baselines, we compare WaterDrum against recent and commonly adopted unlearning metrics: ROUGE-L (Lin, 2004), Truth Ratio Maini et al. (2024), KnowMem (Shi et al., 2024b) and MIA (Shi et al., 2024a). For ease of comparability, all metrics are scaled such that their score when evaluated on the original model φT (which is accessible to the data owners before unlearning) is 1.0. As our WaterDrum framework involves watermarking the training data DT (P1), the models finetuned on this watermarked dataset differ slightly from the dataset used for other metrics. their performance remains comparable, as However, Waterfall satisfies desiderata W0. Additional details on the datasets, other models considered, unlearning metrics, inference parameters, and implementation are presented in App. D. 5.1. Practicality desiderata (D3, D4) We first evaluate WaterDrum and the baseline metrics on the effectiveness and practicality desiderata, D1-D4, as we have outlined in Sec. 2. To do so, we establish experimental settings that mimic the real-life scenarios described in the practicality desiderata D3 and D4. Then, under these settings, we evaluate the effectiveness of various metrics based on D1 and D2, by considering how they evaluate the ground truth unlearning algorithm retraining the model on only the retained dataset to generate φR, which is guaranteed to contain no influence from the forget set DF by construction. Feasibility (D3). All of the baseline metrics (ROUGE-L, Truth Ratio, KnowMem and MIA) typically require retraining model φR with just the retain set DR to get reference values (φR(qF ); F), and hence violate D3(a). In our experiments, we show how the effectiveness of these metrics gets significantly impacted without access to φR. In contrast, WaterDrum does not require φR as it naturally has (φR(qF ); F) = 0 since it satisfies W1. In addition, the computation of the MIA metric requires logit-access, which violates D3(b). However, for evaluation purposes, we grant MIA logit-access in our experiments. Robustness to similar data (D4). We establish the settings to assess the robustness of the unlearning metrics to similar data by injecting small amount of data Ds DF into DR, i.e., the retain set is augmented (Ds = Ds DR) with some data points that are similar to DF . We consider two such scenarios: (a) Exact duplication. Data points in Ds are exact copies of those in DF , (Ds = DF ) and (b) Semantic duplication. Data points in Ds are paraphrased version of DF , (Ds DF ). In addition, we consider the 7 case where there is (c) no duplication of DF data points in DR, (Ds = ). Additional implementation details are in App. D.4. 5.2. Separability desiderata (D1) To assess whether the unlearning metrics satisfy the D1 desiderata, note that the lefthand side expression P[M (φR(q(dr)); r) > (φR(q(df )); )] in Equation (1) corresponds to the definition of the AUROC of the metric in distinguishing between and F(Fawcett, 2006). Hence, we can compute the AUROC of various unlearning metrics with the retrained model φR, and assess if the AUROC 1. Note that we exclude MIA from this experiment because it focuses solely on assessing privacy leakage based on distributional differences between forget and holdout sets, without considering the retain set. the metrics on the Table 2 shows the AUROC of WaterDrum-TOFU dataset under various duplicate settings. Notably, WaterDrum is the only metric that consistently achieves AUROC > 0.9 and close to 1, hence satisfying D1. In contrast, the other metrics performance degrades significantly in the exact and duplicate settings, with AUROC dropping to around 0.5, which means the metrics are no better than random assignment in separating the forget and retain sets. Furthermore, note that Truth Ratio only achieves an AUROC of about 0.75 even in the no duplicate setting, indicating that it does not satisfy D1 under normal settings. ROUGE is also relatively less reliable than the other metrics as can be observed from the occasional large variation in AUROC values across trials over different retrained models2 on the same retain set (e.g., the semantic duplicate setting) ROUGE is more reliant on the retrained model being trained to memorize specific phrases from the forget set. Empirical results on WaterDrum-Ax  (Table 2)  show similar trends, with WaterDrum consistently performing well and KnowMem encountering difficulties in all settings. ROUGE performs poorly under the exact duplicate setting where only just 5% of the augmented retain set are exact duplicates of the forget set. While it performs well for the semantic duplicate settings in this experiment, this occurs because the ROUGE score between Ds and DF is still low ( 0.65) although the semantic similarity of Ds and DF is high (STS 1). The lower ROUGE score implies that the text has already been heavily paraphrased such that the semantic duplicate setting is effectively closer to the no duplicate setting for ROUGE in this experiment. Milder forms of perturbation for this dataset would likely make its degradation of performance on D1 more apparent. 2The stochasticity comes from the training process of the retrained model. WaterDrum: Watermarking for Data-centric Unlearning Metric Table 2: AUROC ( 5, 95 percentile range) of metrics for different levels of similarity for the WaterDrum-TOFU dataset (left) and WaterDrum-Ax dataset (right). WaterDrums AUROC remains near 1.0 even when similar data exists. Similarity ROUGE Truth Ratio WaterDrum Similarity ROUGE KnowMem WaterDrum Exact Duplicate 0.4860.016 0.5080.014 0.9260.049 Semantic Duplicate 0.8020.424 0.4720.054 0.9540.001 0.9300.115 0.7470.011 0.9280. No Duplicate Exact Duplicate 0.3340.010 0.4920.010 0.9570.015 Semantic Duplicate 0.9600.003 0.4500.012 0.9630.002 0.9740.001 0.4910.014 0.9650.001 No Duplicate at 0, since calibrated metric should be proportional to and have (φR(qF ); F) = 0. R2 values close to 1 imply that the metrics are well calibrated, while large negative values occur when the metrics produce similar, instead of proportional, values for varying percentages. Figure 3 shows the calibration curves for the various unlearning metrics, and Table 3 the corresponding R2 values, under the various duplicate settings for the WaterDrum-Ax. Note that WaterDrum is the only metric that is calibrated across all settings, and can represent the percentage of forgotten data remaining in the unlearned model. In fact, the rest of the unlearning metrics perform poorly across all settings, including the basic no duplicate setting they cannot be used to tell when DF is fully unlearned, as (φR(qF ); F) = 0. The results demonstrate the strong reliance of the baseline methods on access to the retrained model. This reliance is impractical as unlearning algorithms were designed precisely to approximate retrained models that are infeasible to obtain. Figure 11 and Table 8 in App. H.2.1 show similar results for the WaterDrum-TOFU dataset, where all baseline metrics fail to meet the calibration desiderata for all settings, including the no duplicate setting. 5.4. Resilience desiderata (D5) We assess whether our WaterDrum metric satisfies the resilience desiderata where the model owner attempts to avoid unlearning by building decoy unlearned model ˆφF (Equation (3)). To create the impression of successful unlearning, the model owner can compute the forget set data owner Fs unlearning metric on any model output, and adjust any output with high scores to an alternative output with low scores (e.g., output from decoy model). Such an attack would work well for all baseline metrics, since the model owner can replicate any metric computation process that is done by data owner . However, the key advantage of WaterDrum is that the model owner does not have the private key µf of data owner to compute the metric (Equation (4)) when building their decoy model. The model owner can only resort to some proxy indicator of similarity SS between received queries qi and the forget set DF to decide which output it should replace to lower the WaterDrum metric score. The lower the threshold it sets, the higher the chances Figure 3: Plots of unlearning metrics against the % of DF remaining in the retrained model, under settings with different levels of data similarity for the WaterDrum-Ax dataset. Note that except WaterDrum, no other metrics are calibrated and satisfy D2. Associated R2 are in Table 3. Table 3: R2 of the best fit line (dotted in Figure 3) for metrics under different levels of similarity on the WaterDrum-Ax dataset. WaterDrum is very well linearly calibrated across the settings, with the highest R2 value. Similarity ROUGE KnowMem MIA WaterDrum Exact Duplicate Semantic Duplicate No Duplicate -37.47 0.693 0.650 -498.1 -276.5 -252.9 -1220 -90.21 -7. 0.987 0.991 0.963 5.3. Calibration desiderata (D2) Next, we assess whether the unlearning metrics meet the calibration desiderata, as defined in Equation (2). Not meeting this desideratum implies that the metrics cannot indicate the extent to which the forget set has been unlearned in given model. We evaluate this by first producing retrained models with varying percentage of the forget set included, i.e., φRG, where DG DF is randomly sampled and = DG/DF . We then compute the unlearning metrics for each retrained model and plot calibration curves showing how the metrics vary with different k. To quantify how well the metrics satisfy Equation (2), we can compute the R2 value for the best-fit line with the vertical intercept 8 WaterDrum: Watermarking for Data-centric Unlearning Metric of reducing the WaterDrum score, but the more output it would need to replace, increasing the cost of this attack and discouraging the model owner to avoid actual unlearning. Generating coherent replacement text without µf is costly, as Waterfall watermarks are robust to modification attacks (Lau et al., 2024) the model owner may have to replace any intercepted output with unwatermarked text from other sources (e.g., another model) with lower quality, impacting its service to its users. In response to the threat model, data owner can prepare beforehand set of queries that it assesses to have watermark signal above an unlearning threshold κ, i.e. = {q(di)M (φT (q(di)); ) > κ}. In our experiment, DF is set of Arxiv abstracts from the math.PR dataset, and consists of other such abstracts not3 in DF . The model owner uses the STS score as SS, computed between the models generated text and all text in df DF . As the model owner increases B, it potentially reduces the average watermarking score via 2 effects: (1) diluting the score by replacing the output with watermark signal by the output from unwatermarked sources, and (2) expecting lower watermark signal from the remaining unfiltered queries that are semantically further away from the original watermarked DF . Figure 4 plots the WaterDrum metric against the percentage of intercepted queries in Q, as the threshold is increased. Note that the unlearning metric decreases almost 1:1 with the percentage of intercepted queries, implying that the model is only relying on effect (1) with no help from effect (2). This makes it very costly for the model owner to carry out the attack. For example, reducing the forget watermark strength to 0.2 requires rejecting more than 70% of the non-relevant queries the model owner may favor performing actual unlearning instead. 5.5. Benchmarking unlearning algorithms Finally, we provide basic illustration of how we could use WaterDrum to benchmark unlearning algorithms. WaterDrum evaluation plot shows the unlearning algorithms evaluated on two axes: ( φR(qR); R) on the x-axis and ( φR(qF ); F) on the y-axis that measure the retain and forget watermark strength, respectively, on an unlearned model φR. The original model φT , which contains both DF and DR, is at the top right corner, while the ground truth retrained model φR, which only contains DR, is at the bottom right corner. The closer the algorithms are to the retrained model, the better they are at both unlearning DF while retaining the influence of DR. Figure 5 shows the WaterDrum evaluation plot for several unlearning algorithms (Finetune, KL Minimization 3For simplicity, in our experiments the data owner does not include queries based on DF in as it can assume that the model owner would definitely filter any output φT (qF ) based on it. Figure 4: Plot of forget watermark strength (WaterDrum metric) over % of queries in intercepted, as the model owner increases its filtering threshold under the threat model T. The best possible unlearning metric against will have its score decrease only proportionally (dotted orange diagonal line). WaterDrum achieves similar performance, implying that the threat model requires intercepting large proportion of queries to reduce the metric detectable by the forget set data owner. Watermark strength is scaled to 1.0 for before the threat model is implemented. Figure 5: Benchmark of existing unlearning methods with WaterDrum on the WaterDrum-Ax. The green lines represent the optimal unlearning values. (KL) (Maini et al., 2024), Task Vector (TV) (Ilharco et al., 2023), SCRUB (Kurmanji et al., 2024); details are in App. D.2). Note that most algorithms are still far from reaching the retrained model performance. The KL and TV algorithms achieve good unlearning quality but significantly compromise the retain sets influence and models overall utility, while Finetune and SCRUB maintain some retain performance but do not achieve the best unlearning quality. In addition, Finetune and SCRUB only achieve AUROC (D1) of 0.568 and 0.439, respectively. We also performed some preliminary experiments for the scenario with multiple parties and duplicate data in App. H.3. 6. Conclusion In this work, we (1) defined clear desiderata that an effective, practical, and resilient unlearning metric should satisfy, (2) proposed novel data-centric LLM unlearning metric, WaterDrum, based on watermarking that meets these desiderata, unlike existing metrics, and (3) introduced benchmark dataset, WaterDrum-Ax, which can be used with WaterDrum to benchmark unlearning algorithms. 9 WaterDrum: Watermarking for Data-centric Unlearning Metric"
        },
        {
            "title": "References",
            "content": "While our desiderata may be non-exhaustive and watermark strength is just one aspect of unlearning effectiveness, we believe that our work is the first step towards developing more effective and practical unlearning algorithms and deriving theoretical results. Future work could conduct more comprehensive and systematic evaluation of existing LLM unlearning algorithms and adapt theoretical insights from the watermarking community to analyze LLM unlearning metrics based on our new connection. Alexander Becker and Thomas Liebig. Evaluating machine unlearning via epistemic uncertainty. arXiv:2208.10836, 2022. Lucas Bourtoule, Varun Chandrasekaran, Christopher Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In IEEE S&P), pages 141159, 2021. Yinzhi Cao and Junfeng Yang. Towards making systems forget with machine unlearning. In IEEE S&P, pages 463480, 2015. CCPA. California consumer privacy act of 2018, 2018. URL https://leginfo.legislature.ca.gov/ faces/billTextClient.xhtml?bill id= 201720180AB375. California Civil Code Title 1.81.5. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. survey on evaluation of large language models. ACM TIST, pages 145, 2024. Vikram Chundawat, Ayush Tarun, Murari Mandal, and Mohan Kankanhalli. Can bad teaching induce forgetting? unlearning in deep networks using an incompetent teacher. In Proc. AAAI, 2023a. Vikram Chundawat, Ayush Tarun, Murari Mandal, and Mohan Kankanhalli. Zero-shot machine unlearning. IEEE TIFS, 2023b. Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, and Hannaneh Hajishirzi. Do membership inference attacks work on large language models? arXiv:2402.07841, 2024. Tom Fawcett. An introduction to ROC analysis. Pattern recognition letters, 27(8):861874, 2006. Xiangshan Gao, Xingjun Ma, Jingyi Wang, Youcheng Sun, Bo Li, Shouling Ji, Peng Cheng, and Jiming Chen. Verifi: Towards verifiable federated unlearning. IEEE TDSC, 2024. GDPR. General data protection regulation, article 17: Right to erasure (right to be forgotten). Official Journal of the European Union, 2016. Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forgetting In Proc. IEEE CVPR, pages in deep networks. 93049312, 2020. 10 WaterDrum: Watermarking for Data-centric Unlearning Metric Aditya Golatkar, Alessandro Achille, Avinash Ravichandran, Marzia Polito, and Stefano Soatto. In Proc. Mixed-privacy forgetting in deep networks. IEEE CVPR, 2021. Michael M. Grynbaum and Ryan Mac. The Times sues OpenAI and Microsoft over A.I. use of copyrighted work, URL https://www.nytimes.com/ Dec 2023. 2023/12/27/business/media/new-yorktimes-open-ai-microsoft-lawsuit.html. Yu Guo, Yu Zhao, Saihui Hou, Cong Wang, and Xiaohua Jia. Verifying in the dark: Verifiable machine unlearning by using invisible backdoor triggers. IEEE Transactions on Information Forensics and Security, 2023. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LORA: Low-rank adaptation of large language models. In Proc. ICLR, 2022. Yiming Hu, Chenyu Wu, Qingyan Pan, Yinghua Jin, Rui Lyu, Vikina Martinez, Shaofeng Huang, Jingyi Wu, Lacey J. Waymentand Noel A. Clark, Markus B. Raschke, Yingjie Zhao, and Wei Zhang. Retraction note: Synthesis of γ-graphyne using dynamic covalent chemistry. Nature Synthesis, 3:1311, 2024. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. In Proc. ICLR, 2023. Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou. Towards unbounded machine unlearning. In Proc. NeurIPS, 2024. Gregory Kang Ruey Lau, Xinyuan Niu, Hieu Dao, Jiangwei Chen, Chuan-Sheng Foo, and Bryan Kian Hsiang Low. Waterfall: Scalable framework for robust text watermarking and provenance for llms. In Proc. EMNLP, 2024. Na Li, Chunyi Zhou, Yansong Gao, Hui Chen, Anmin Fu, Zhi Zhang, and Yu Shui. Machine unlearning: Taxonomy, metrics, applications, challenges, and prospects. arXiv:2403.08254, 2024a. Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin Li, Ann-Kathrin Dombrowski, Shashwat Goel, Gabriel Mukobi, et al. The wmdp benchmark: Measuring and reducing malicious use with unlearning. In Proc. ICML, pages 2852528550, 2024b. Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023. Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In ACL, pages 7481, 2004. Aiwei Liu, Leyi Pan, Yijian Lu, Jingjing Li, Xuming Hu, Xi Zhang, Lijie Wen, Irwin King, Hui Xiong, and Philip Yu. survey of text watermarking in the era of large language models. ACM Computing Surveys, pages 136, 2024a. Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush Varshney, et al. Rethinking machine unlearning for language models. arXiv:2402.08787, 2024b. large Aengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, and Dylan Hadfield-Menell. Eight methods to evaluate robust unlearning in llms. arXiv:2402.16835, 2024. Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary Chase Lipton, and Zico Kolter. TOFU: task of fictitious unlearning for LLMs. In ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models, 2024. Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and Quoc Viet Hung Nguyen. survey of machine unlearning. arXiv:2209.02299, 2022. Lip Yee Por, KokSheik Wong, and Kok Onn Chee. Unispach: text-based data hiding method using unicode space Journal of Systems and Software, pages characters. 10751082, 2012. Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. Detecting pretraining data from large language models. In Proc. ICLR, 2024a. Weijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Malladi, Jieyu Zhao, Ari Holtzman, Daogao Liu, Luke Zettlemoyer, Noah Smith, and Chiyuan Zhang. Muse: Machine unlearning six-way evaluation for language models. arXiv:2407.06460, 2024b. Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In Proc. IEEE S&P, pages 318, 2017. David Marco Sommer, Liwei Song, Sameer Wagh, and Prateek Mittal. Towards probabilistic verification of machine unlearning. arXiv:2003.04247, 2020. Ayush Tarun, Vikram Chundawat, Murari Mandal, and Mohan Kankanhalli. Fast yet effective machine unlearning. IEEE TNNLS, 2023. WaterDrum: Watermarking for Data-centric Unlearning Metric Pratiksha Thaker, Shengyuan Hu, Neil Kale, Yash Maurya, Zhiwei Steven Wu, and Virginia Smith. Position: Llm unlearning benchmarks are weak measures of progress. arXiv:2410.02879, 2024. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288, 2023. Wenbo Wan, Jun Wang, Yunming Zhang, Jing Li, Hui Yu, and Jiande Sun. comprehensive survey on robust image watermarking. Neurocomputing, 2022. Qizhou Wang, Bo Han, Puning Yang, Jianing Zhu, Tongliang Liu, and Masashi Sugiyama. Towards effective evaluations and comparisons for llm unlearning methods. In Proc. ICLR, 2025. Ruihan Wu, Chhavi Yadav, Russ Salakhutdinov, and Kamalika Chaudhuri. Evaluating deep unlearning in large language models. arXiv:2410.15153, 2024. Xi Yang, Jie Zhang, Kejiang Chen, Weiming Zhang, Zehua Ma, Feng Wang, and Nenghai Yu. Tracing text provenance via context-aware lexical substitution. In Proc. AAAI, pages 1161311621, 2022. Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, and Xiang Yue. Machine unlearning of pre-trained large language models. arXiv:2402.15159, 2024. A. Related Works Unlearning metrics. Unlearning algorithms are often evaluated based on their a) unlearning effectiveness, b) utility preservation, and c) unlearning efficiency (Li et al., 2024a). We briefly discuss b) and c) as they are not the focus of this work. b) Utility preservation refers to how well the LLM maintains its performance and usability after unlearning, which can be measured with performance indicators (e.g., perplexity, accuracy) on the retain set and various downstream tasks (Chang et al., 2024). The c) efficiency of an unlearning algorithm can be assessed based on how much time and resources it saves compared to retraining from scratch (Nguyen et al., 2022; Li et al., 2024a). a) Unlearning effectiveness metrics. Broadly, unlearning effectiveness (or forget quality) refers to how well the LLM has removed the presence/influence of the forget set. There are few classes of such metrics. Utility based metrics are form of utility-centric metrics that expect the model utility (performance indicators) when evaluated on the forget set to worsen after unlearning. LLM utility based unlearning metrics include ROUGE-L (Lin, 2004), Truth Ratio (Maini et al., 2024), and KnowMem (Shi et al., 2024b). More details of their definitions can be found in App. D.3 and we have described the disadvantages of utility-centric metrics in Sec. 3.1. Membership inference attacks (MIA) based metrics the ability or probability to infer expect the membership of data sample in the forget set to reduce significantly after unlearning. Some MIA-based metrics are also utility-centric, as membership inference may depend on performance indicators, such as perplexity and the log-likelihood of tokens in text data (Shi et al., 2024a). However, MIA attacks (Shokri et al., 2017) have demonstrated limited success against LLMs (Duan et al., 2024), and their performance is adversely affected by the presence of similar data in the forget and retain set. Watermarking based metrics embed signals in the forget set and expect the strength of these signals to decrease after unlearning (Li et al., 2024a). Our algorithm WaterDrum falls under the category but is the first metric that works for LLMs. Existing watermarking-based unlearning metrics are designed and work only for image datasets and classification models. For example, Guo et al. (2023) embedded invisible backdoors in images with incorrect target labels to assess the success of unlearning, measured by drop in the success rate of backdoor attacks. Sommer et al. (2020) introduced 12 WaterDrum: Watermarking for Data-centric Unlearning Metric probabilistic verification framework for backdoors, in which users modified their data prior to submission. We highlight the key differences of our work: (a) These methods rely on label-based predictions and face challenges such as generalization effects, conflicting backdoor patterns, or backdoor defences. In contrast, our work focuses on adapting watermarking to LLMs, where longer and more complex output sequences provide richer signals for unlearning verification. (b) These models compromise model utility even before unlearning, especially when the forget set is large. In contrast, our framework does not significantly degrade model utility. (c) Most importantly, existing watermarking and backdoor attack-based metrics are limited to image data and cannot be directly applied as unlearning metrics for textual data due to additional challenges such as in preserving data fidelity (Guo et al., 2023; Sommer et al., 2020). Text watermarking. Watermarking is an extensively studied technique for copyright protection, fingerprinting, and authentication (Wan et al., 2022; Liu et al., 2024a). Watermarking consists of two main stages: embedding and detection, where the watermark must remain imperceptible and robust against removal attacks (Wan et al., 2022). Unlike digital images, where continuous signals facilitate imperceptible watermark embedding, text watermarking is more difficult due to its discrete nature and susceptibility to text modifications (Liu et al., 2024a). Existing methods, such as inserting Unicode characters (Por et al., 2012) or synonym replacement (Yang et al., 2022), are often easily detectable and susceptible to word replacement. On the other hand, syntactic-based watermarking methods are often constrained by the limited choices of syntactic structures and require prior linguistic knowledge (Wan et al., 2022). Recently, LLMs have emerged as promising watermarking tool as they can generate natural-looking text and improve watermarking robustness. Lau et al. (2024) proposed robust text watermarking approach capable of embedding watermarks across data from multiple data owners, preserving the semantic content of the original text, and also achieving watermark robustness such that watermarks in the training data of LLMs remain detectable in the model output. We build on Lau et al. (2024) framework in our work to develop our unlearning metric. Future work can consider other watermarking frameworks. Retraining-based vs. non-retraining evaluation. This section is adapted from the survey by Liu et al. (2024b). Retraining is commonly viewed as the gold standard in classical unlearning settings (Cao and Yang, 2015; Golatkar et al., 2020; Bourtoule et al., 2021). This has led to various evaluation metrics that assert how closely an unlearned model approximates retrained one, e.g. via matching performance on the forget set (Golatkar et al., 2020; Chundawat et al., 2023b) or measuring distances in weights and activations (Tarun et al., 2023; Golatkar et al., 2021; Chundawat et al., 2023a). However, retraining LLMs is often infeasible due to the scale of model parameters and the volume of pretraining data. In addition, retraining-based metrics contradict the purpose of unlearning that emphasizes the unavailability of retrained model. Therefore, non-retraining metrics are now more important and aligned with the growing trend of commercial LLMs that only provide black-box access. Chundawat et al. (2023a) proposes the ZRF score that captures the randomness in model predictions as an indicator of unlearning, while Becker and Liebig (2022) proposes to utilize model epistemic uncertainty. Yao et al. (2024) propose that surrogate subset with the same distribution as the forget set can be employed to approximate the performance of the retrained model. However, these metrics often overlook the models ability to generalize from pre-training or the remaining retain set. To address this, synthetic datasets, such as TOFU dataset (Maini et al., 2024), are carefully crafted to ensure sufficient separation between the forget and retain set. Nonetheless, such separation and low similarity is rarely achievable in real-world scenarios. In this work, we address these limitations by proposing non-retraining metric that works despite greater similarity between the forget and retain set and the generalization ability of LLMs. Additionally, our metric would work for multiple unlearning requests. Specifically, we propose to use watermarking (Sommer et al., 2020; Guo et al., 2023; Gao et al., 2024) to handle potential similarities due to its ability to make each data point uniquely identifiable. Comparison with other LLM unlearning evaluations. Maini et al. (2024); Shi et al. (2024b) have proposed new unlearning metrics and benchmark datasets. Li et al. (2024b) proposes multiple-choice question benchmark dataset, WMDP, to evaluate the models knowledge in biosecurity, cybersecurity, and chemical security. This benchmark dataset is different from TOFU, MUSE, and ours in nature because it is specifically for knowledge editing and only contains testing data instead of training data. Wang et al. (2025) suggest that an unlearning metric should be robust against (unchanged by) red teaming scenarios (such as recovering knowledge by jail-breaking, probing, relearning) and unlearning algorithms should be compared when they achieve the same retain quality, which is realized by mixing the parameters of the model before and after unlearning. Wu et al. (2024) proposes new perspective of fact unlearning and an accompanying synthetic dataset. In contrast, we propose and satisfy novel set of desiderata to address 13 WaterDrum: Watermarking for Data-centric Unlearning Metric realistic settings, such as when the forget and retain sets have semantically similar content and when retraining is impractical. Our desiderata are not intended to be exhaustive and can complement existing benchmarks. Lynch et al. (2024) proposes suite of adversarial metrics to resurface forget set-related knowledge that exists in the unlearned LLMs, e.g., jailbreaking prompts, relearning (via fine-tuning and in-context learning), and latent knowledge extraction. While these metrics employ the textual similarity to the forget set in adversarial scenarios to evaluate the unlearning success, watermarking uses the signal carried in model outputs to detect the presence of data from the forget set. Miscellaneous. See Section 4 of (Liu et al., 2024b) for more discussion about other unlearning effectiveness, utility preservation, efficiency, and scalability metrics. B. Details on Watermarking with Waterfall Watermarking and verification of the training text data was done using the Waterfall algorithm (Lau et al., 2024), using the code available on https://github.com/aoi3142/Waterfall. The text were watermarked with the default model meta-llama/Llama-3.1-8B-Instruct, with watermark strength κ = 2 and perturbation key kp = 1. When watermarking for WaterDrum-Ax, the different data owners were assigned consecutive IDs µ, starting from 0 and incrementing by 1 for each data owner (0, 1, 2, ...). For experiments involving duplicate data, we watermarked with the ID 1 higher than the owner index instead (i-th owner watermarked with µi = + 1, where is zero-indexed). The watermark ID for the duplicate of the last owners data is wrapped around, using µ1 = 0. For the experiments with multiple data owners requesting to have their data unlearned, this simulates the situation where some owners only request for portion of their data to be unlearned, while retaining the remaining portion of their data. When watermarking for WaterDrum-TOFU, the data from the retain set was watermarked with ID µ = 0 while data from the forget set was watermarked with ID µ = 1. Duplicate data of the forget set were watermarked with the retain watermark, ID µ = 0. C. Further Discussion on D2 Calibration D2 (calibration) enables unlearning metrics to go beyond being just binary indicator of whether an entire dataset has been unlearned, to be meaningful continuous score of how much of forget set DF has been unlearned. of D2 captures the desire that the unlearning metric can be directly interpreted as indicating the proportion of DF that has not been unlearned, given just single calibration datapoint (i.e., the forget set metric evaluated on the original model). Surprisingly, as seen in our experiments (Fig. 3 and Tab 3), WaterDrum can satisfy D2, enabling this intuitive and simple interpretation in the ground truth scenario of models retrained with data including varying fractions of the forget set DG DF . As corollary, D2 is needed to easily define the threshold for classifying if total unlearning is successful, without the impractical requirement of retrained model. Specifically, D1 (Equation (1)) implies that there exists threshold κ to decide whether data point di Di DT from owner belongs to the retain set or not: Equation (2) from D2 implies that fully unlearned model should have (φR(qF ); F) = 0. Thus, the threshold κ should be close to 0. We also discuss practical use cases for D2 in App. G.2. D. Details on Experimental Setup We conduct our experiments on NVIDIA L40 and H100 GPUs. Evaluation is averaged across 3 random seeds {41, 42, 43}. Text generation from the different models used temperature = 1, top-p = 1, top-k left as the LLM vocabulary size. More details of our experimental setup are presented below. D.1. Training Hyperparameters WaterDrum-Ax. We finetune the bfloat16-pretrained Llama-2-7B model from Hugging Face4 using LoRA (r = 8, α = 32) with batch size 128 , 20 training epochs, learning rate 1e3. Additionally, we finetune the bfloat16-pretrained Phi-1.5 model (detailed in App. E.2) with the same settings. We have considered these two models as they are representative of the recent LLMs, different in terms of model architectural details, and span different model scales. WaterDrum-TOFU. We finetune the bfloat16-pretrained Llama-2-7B-chat model from Hugging Face5 using LoRA (r = 8, α = 32) with batch size 128 , 10 training epochs, learning rate 1e4. Subsequently, for unlearning, we use batch size of 32. While we conduct the main experiments using LoRA as 4https://huggingface.co/meta-llama/Llama2-7b-hf. 5https://huggingface.co/meta-llama/LlamaThe proposed linear proportional form (Equation (2)) 2-7b-chat-hf. WaterDrum: Watermarking for Data-centric Unlearning Metric in other LLM unlearning works (Maini et al., 2024; Shi et al., 2024b), we also affirm that WaterDrum applies to full parameter fine-tuning in App. E.1. D.2. Baseline Unlearning Algorithms In our experiments, we have adopted several popular baseline unlearning algorithms detailed as follows: Retrain: Directly retraining the model from scratch on the retain set. The retrained model usually serves as the golden standard for other unlearning methods. Finetune: Continually training the model on the retain set for 1 or several epochs. This method assumes that the model naturally forgets about the forget set as learning progresses on the retain set. In this paper, we finetune for 1 epoch using learning rate of .0001. KL Minimization (KL) (Maini et al., 2024): Concurrently maximizing the prediction loss on the forget set and minimizing the Kullback-Leibler divergence of predictions on the retain set to the original model. We ran KL minimization for 5 unlearning epochs. SCRUB (Kurmanji et al., 2024): Maximizing the Kullback-Leibler divergence of predictions on the forget set to the original model, while minimizing the prediction loss and divergence on the retain set. The optimization process alternates between maximization steps and minimization steps. In our experiments, we ran 3 maximization and minimization epochs. Direct Preference Optimization (DPO) (Maini et al., 2024): For question-answering tasks, encouraging responses such as dont know on the forget set, while simultaneously minimizing the prediction loss on the retain set. Note that this method is not suitable for completion tasks, and is omitted for the WaterDrum-Ax dataset. We ran 5 unlearning epochs for DPO. Task Vector (TV) (Ilharco et al., 2023): Subtracting the parameters of the model trained only on the forget set from the model to be unlearned. In the experiments, we finetune the model on the forget set for 5 epochs. D.3. Baseline Unlearning Metrics the longest ROUGE-L: measures common subsequence between the generated text and This serves as surrogate for reference text. the WaterDrum-Ax the generation quality for accuracy dataset the For for the WaterDrum-TOFU dataset. question-answering and the WaterDrum-Ax dataset, we prompted the model with the first 50 tokens of the training dataset for the model to perform completion generation. For the WaterDrum-TOFU dataset, we prompted the model with the questions, using the models prompt format. To calculate the metric score, we follow Shi et al. (2024b); Maini et al. (2024) in computing the ROUGE-L recall scores (Lin, 2004) to compare the model response with the training data as ground truth. We generated 10 outputs for each prompt, and the mean score for the 10 generations was taken. Truth Ratio: measures the probability of generating correct answer versus wrong answer as an indicator of whether the model still memorizes the knowledge to be unlearned on the WaterDrum-TOFU dataset. Following Maini et al. (2024), for each given question, we compute the ratio by dividing the averaged probabilities of multiple wrong answers by the probability of paraphrased true answer. KnowMem: measures the ROUGE score of QA pairs related to the training data to measure the the knowledge on the model memorization of WaterDrum-Ax dataset. Following (Shi et al., 2024b), we use GPT-4 to create question-answering evaluation set with 8000 QA pairs based on the abstracts in the WaterDrum-Ax dataset and measure the ROUGE score between the models generated response to the questions and the ground truth answers. MIA: measures the difference in predictive distribution between two models to measure privacy leakage from unlearning. Specifically, we employ the state-of-the-art Min-40% attack (Shi et al., 2024a) based on the loss on the forget set and holdout set, and compute AUROC of discriminating the losses. and compare WaterDrum: We also use our proposed watermark metric the the above-mentioned baseline evaluation metrics. We used the same generation setup as that in ROUGE-L for WaterDrum, and evaluated the watermark strength of only the generated output excluding the prompt. against results D.4. Duplication Details As discussed in Sec. 5.1, we examine 3 representative scenarios where there exists extra data Ds that is similar to DF with different SS: (a) Exact duplication: Ds is an exact copies of DF , hence we make Ds as copy of DF . This marks the highest similarity with STS = 1.00 and ROUGE = 1.00. (b) Semantic duplication: Ds is paraphrased version of DF with the same semantic meaning. We use GPT-4 to paraphrase DF and obtain Ds. In this case, Ds has STS = 0.97, ROUGE = 0.69 on WaterDrum-Ax, 15 WaterDrum: Watermarking for Data-centric Unlearning Metric Table 4: AUROC of metrics for different levels of similarity for the WaterDrum-Ax dataset (right). WaterDrum AUROC remains near 1.0 even when similar data exists. Similarity Exact Duplicate Semantic Duplicate No Duplicate ROUGE KnowMem WaterDrum Full LoRA Full LoRA Full LoRA 0.335 0.334 0.965 0. 0.984 0.974 0.497 0.492 0.447 0. 0.481 0.491 0.990 0.957 0.990 0. 0.991 0.965 Table 5: R2 of the best fit line for various metrics under different levels of similarity for the WaterDrum-Ax dataset. WaterDrum is very well linearly calibrated across the settings, with the highest R2 value. Similarity Exact Duplicate Semantic Duplicate No Duplicate ROUGE KnowMem MIA WaterDrum Full -5059 -981.5 -4. LoRA -37.47 -498.1 -1220 Full 0.545 -139. -35.57 LoRA 0.693 -276.5 -90.21 Full 0. -103.8 -3.937 LoRA 0.650 -252.9 -7.553 0. 0.987 0.989 0.991 0.940 0.963 and STS = 0.96, ROUGE = 0.60 on WaterDrum-TOFU. We also consider the standard scenario when there is (c) No duplication at all in the dataset. We then finetune 3 models on the WaterDrum-Ax dataset which includes Ds in its DR during finetuning, corresponding to the 3 different levels of similarity. Note that since Ds is from different data owner to DF , we embed different watermarks for Ds and DF for the evaluation of our WaterDrum. Subsequently, we adopt the set of considered unlearning methods (including retraining the model on just the retain set DR) to remove DF while retaining Ds. E. Ablations E.1. Evaluation on full parameter fine-tuning The majority of the experiments were conducted using LoRA (Hu et al., 2022), as is the setting in other LLM unlearning works (Maini et al., 2024; Shi et al., 2024b). To affirm that WaterDrum is applicable when used for full parameter fine-tuning, we conducted experiments for the separability (D1) and calibration (D2) desiderata for varying levels of similarity for the WaterDrum-Ax dataset. For full parameter fine-tuning, we used learning rate of 1e-4 and trained for 10 epochs. Note that due to the high computational cost of full parameter fine-tuning, we only report the results for one seed, while the results for LoRA are the averaged across three different seeds. Table 4 and Table 5 shows WaterDrum performs better than other metrics, for both LoRA and full parameter fine-tuning. The LoRA and full-parameter fine-tune results are very similar for WaterDrum across the experiments. Figure 6: Plots of our WaterDrum against the % of DF remaining in the retrained model, under settings with no duplication and exact duplication using Phi-1.5 for the WaterDrum-Ax dataset. E.2. Evaluation on other models We have also evaluated our WaterDrum on Phi-1.56 on WaterDrum-Ax to verify its adaptability to different LLM models. Figures 6 and 7 illustrate the calibration and AUROC for the settings of no duplicate and exact duplicate. The result on Phi-1.5 aligns with our main experiments using Llama2-7B and meets the proposed desiderata. This validates our WaterDrums adaptability to different LLMs, which guarantees its real application potential. F. Additional experimental results F.1. Quantitative evidence that watermarking with Waterfall does not degrade model performance Our WaterDrum framework lays out desiderata for compatible watermarking methods (Sec. 3.3), including 6https://huggingface.co/microsoft/phi-1 5 16 WaterDrum: Watermarking for Data-centric Unlearning Metric Figure 7: AUROC plots of our WaterDrum for Phi-1.5 model on the WaterDrum-Ax dataset. Table 6: Semantic similarity of qf and qs from the WaterDrum-Ax dataset. For reference, the STS score of texts from the same category is 0.67. Similarity of query STS score of query output Exact Duplicate Semantic Duplicate 0.96 0.87 fidelity (W0). We chose to use Waterfall (Lau et al., 2024) as their paper already presented extensive empirical results showing that its watermarking process has minimal degradation on model performance (App H.3). Nonetheless, we have confirmed Waterfalls fidelity for our experiments by comparing the models performance when trained on the un/watermarked data using truth ratio (Maini et al., 2024), which computes each models probability of generating the correct answer compared to set of wrong answers perturbed from the correct answer. Our results show that on the WaterDrum-TOFU dataset, the truth ratio of un/watermarked models are very similar, at 0.5143 and 0.5163, respectively, showing that watermarking has minimal impact on the models performance. F.2. Similarity of output in retrained model Under the setting where the retain set (Ds = Ds DR) contains some data points that are similar to the forget set (Ds DF ), we verify that output of the model trained on the retrained set are similar for the duplicate queries R(qF ) φs φs R(qs). We empirically verify the similarity by evaluating the STS score between the outputs of the forget query qF and the retain query qs. As shown in Table 6, the mean STS scores are 0.96 and 0.87 for exact and semantic duplicates, respectively. For comparison, the STS score of query outputs from the same WaterDrum-Ax category (e.g., outputs for queries from the same arXiv category) only have mean STS score of 0.67. This shows that the query outputs from the duplicate queries are very similar, much more than queries from the same subject. 17 Figure 8: Count of data with different watermark strengths measured on Df and Ds (with similar semantics) for the WaterDrum-Ax dataset when unlearning 1 class. The result shows that metric scores from the two sets have similar distribution. F.3. Similar metrics score across data We verify that data points from Ds and Df with similar semantics will have similar metric scores (M (φR(qs); s) (φR(qF ); F)). We use our WaterDrum to measure the metric scores on data points from Ds and Df for the WaterDrum-Ax dataset when unlearning 1 class. Figure 8 illustrates the count of different metric scores across two subsets with similar semantics. This verifies that the distributions of metric scores from the two subsets are similar. G. Practical considerations for real-world deployment of WaterDrum G.1. Practical deployment pipeline for WaterDrum key strength of WaterDrum is its real-world feasibility, especially when dealing with closed-sourced LLM providers, where other LLM unlearning metrics fail. Unlike other methods, WaterDrum can be easily implemented in practice with just additional lightweight data preprocessing and no other changes to existing pipelines. Specifically, WaterDrum offers the following advantages for real-world deployment: Data owners can quickly watermark their data before sharing them with the model owners or releasing important data publicly. This not only facilitates unlearning verification but also allows them to detect whether their data has been used by model owners without authorization (Maini et al., 2024). No changes are required on the model owners end. They can continue training their closed-source LLMs WaterDrum: Watermarking for Data-centric Unlearning Metric and provide API access, or even release open-source models. Data owners can then detect whether their data has been used for fine-tuning of any model based on just model output (even closed-source), submit an unlearning request, and verify whether unlearning has been done via WaterDrum. Verification is very efficient (Lau et al., 2024) and can even be run on CPU (about 3 seconds per 1000 query outputs). In comparison, other LLM unlearning metrics face severe limitations that rule out practical deployment, such as requiring retrained model (D3), which even cooperative model owner cannot provide due to computational costs. G.2. Practical real-life use case for D2 (Calibration) in WaterDrum Although it is ideal for unlearning to delete the forget set fully, in practice, partial unlearning (as an outcome of imperfect unlearning) may be inevitable due to the size and complexity of LLMs. This is because a) exact unlearning involving retraining from scratch is prohibitively expensive and impractical, and b) perfect unlearning on LLMs is not yet achievable with current approximate unlearning algorithms without significantly harming model performance (e.g., on the retain set). In Sec. 5.5, we demonstrate this by testing various SOTA unlearning methods: all methods only achieve partial unlearning except when the model is destroyed (i.e., has no presence of both the forget and retain set), or when new model is retrained from scratch. With D2 (Calibration), the characterization of partial unlearning becomes possible, and this is important across various stages of the unlearning pipeline in practical, real-life scenarios: 1. Deployment: In practice, model owners may only be able to achieve partial unlearning of the forget set while preserving the utility of their model offering to customers. calibrated continuous score unlearning metric satisfying D2 such as ours can serve as an objective proxy for negotiations with data owners on the needed extent of unlearning and the corresponding amount of compensation required. The negotiated targeted extent of unlearning can then be used as an objective to guide the actual implementation of unlearning, e.g. the selection of the most suitable unlearning algorithm which may each achieve different forget-retain performance trade-offs (e.g., from reference plot like Figure 5, choosing the method that achieves the highest retain score for fixed forget threshold), or suitable hyperparameters for given method. Figure 9: News agencies (Reuters and The Straits Times) both report soccer match with high semantic similarity (STS=0.90). 2. Evaluation and development: For research and development, calibrated metric satisfying D2 enables evaluation beyond binary success/failure and instead quantifies partial success. This supports more realistic and granular assessment of theoretical unlearning algorithms. In summary, perfect unlearning may not be achievable in practice due to the limitations of current LLM unlearning algorithms, which necessitate continuous evaluation that goes beyond binary decision. D2 (Calibration) provides an interpretable way to measure partial unlearning, enabling practical evaluation and considerations of trade-offs between model performance and compensations. Until perfect unlearning is feasible, continuous and calibrated metric satisfying D2 will be valuable. G.3. Practical real-life scenario for data owners with similar data As discussed in Sec. 2, it is common for the data owners to have semantically similar instances, such as news articles on the same event. Here, we identify real-life scenario where two news agencies provide semantically similar articles, as shown in Figure 9. The two articles from two data owners exhibit high semantic similarity with an STS score of 0.90. In this case, one agency may request unlearning, which matches our problem setting in D4. H. Additional unlearning evaluation results Here we provide additional evaluation results in the main paper on both WaterDrum-Ax and WaterDrum-TOFU datasets. 18 WaterDrum: Watermarking for Data-centric Unlearning Metric Figure 10: Plots of unlearning metrics against the % of DF remaining in the retrained model, scaled by referencing the original and retrained model with different levels of data similarity for the WaterDrum-Ax dataset."
        },
        {
            "title": "Plots of unlearning metrics against",
            "content": "Figure 11: the % of DF remaining in the retrained model, under settings with different levels of data similarity for the WaterDrum-TOFU dataset. Table 7: R2 of the best fit line (dotted in Figure 10 and scaled by referencing the original and retrained model) for various metrics under different levels of similarity for the WaterDrum-Ax dataset. Similarity ROUGE KnowMem MIA WaterDrum Exact Duplicate Semantic Duplicate No Duplicate 0.923 0.997 0. -0.331 0.101 0.006 0.273 -0.011 0.990 0.994 0.995 0.957 H.1. Evaluation on WaterDrum-Ax H.1.1. ROBUSTNESS TO SIMILAR DATA Relaxation of Feasibility. In Sec. 5.3, we have demonstrated the calibration of the metrics without access to φR. Here, we explore relaxing the restriction by allowing metrics to use φR as reference. By referencing the fully retrained model as the baseline 0 point for (φR(qF ); F), we visualize the scaled calibration of the baseline metrics in Figure 10, and present the R2 values in Table 7. The results imply that, under the relaxed condition by referencing φR, the calibration of the baseline metrics generally improves. Notably, ROUGE achieves good calibration across various similarity levels, though it underperforms in the exact duplicate settings. In contrast, our WaterDrum consistently demonstrates strong calibration, with robust R2 values across all settings. Despite these, it is important to emphasize that the retrained models are not available in practical scenarios, and their availability will eliminate the need to perform unlearning in the first place. Table 8: R2 of the best fit line for various metrics under different levels of similarity for the WaterDrum-TOFU dataset. Similarity ROUGE Truth Ratio MIA WaterDrum Exact Duplicate Semantic Duplicate No Duplicate -175.6 -75.96 -0.610 -8643 -10910 -12. -3.480 -41.15 -0.838 0.889 0.947 0.923 H.2. Evaluations on WaterDrum-TOFU As supplement to the main experiments, here we present additional results on the WaterDrum-TOFU dataset. As described in Sec. 5.1, we consider the exact duplication, semantic duplication, and no duplication settings, and finetune the models on the WaterDrum-TOFU dataset. While Sec. 5.2 discusses separability results with similar data, we report here the evaluation of calibration (D2) with similar data as follows: H.2.1. CALIBRATION WITH SIMILAR DATA. Figure 11 visualizes the calibration on WaterDrum-TOFU and Table 8 displays the R2 values. Similar to Sec. 5.3, our WaterDrum outperforms the baseline metrics by ensuring (φR(qF ); F) = 0 and maintaining strong calibration, with high R2 values without referencing retrained models across all settings. H.3. Benchmarking Unlearning Algorithms for More Classes and Duplicate Data In addition to the results in Sec. 5.5, here we consider the WaterDrum-Ax with 1, 3 and 5 data owners (out of 20 19 WaterDrum: Watermarking for Data-centric Unlearning Metric Figure 13: Benchmark of existing unlearning methods with WaterDrum on the WaterDrum-Ax duplicate data (DT = DR DF ), for 1, 3, and 5 data owners requesting their data to be removed. Figure 12: Plots of unlearning metrics against the % of DF remaining in the retrained model, scaled by referencing the original and retrained model with different levels of data similarity for the WaterDrum-TOFU dataset. Table 9: R2 of the best fit line (scaled by referencing the original and retrained model) for various metrics under different levels of similarity for the WaterDrum-TOFU dataset. Similarity ROUGE Truth Ratio MIA WaterDrum Exact Duplicate Semantic Duplicate No Duplicate 0.964 0.994 0. -0.074 0.596 0.995 -0.018 -0.417 0.608 0.997 0.996 0.997 total data owners) requesting for their data to be unlearned from the model (Figure 13). Additionally, we also consider duplicate data in both forget and retain set (Figure 14). We can observe that except for Finetune, all the other unlearning algorithms perform poorly. However, note that Finetune requires the most amount of computation resources as the retain set is likely to be significantly larger than the forget set. The retain watermark strength for the retraining model when considering unlearning of 5 classes increases slightly beyond 1.0. We hypothesize that this is due to the large proportion of forget set out of the whole dataset when removing 5 out of the total 20 classes (25% of the training data). The high proportion means that the retain set DR used for training the retraining model is much smaller than the full dataset DT , which could have resulted in the retraining model becoming more specialized in the smaller retraining dataset containing the retain set, resulting in higher retain watermark strength. I. Other Questions 1. What is the difference with watermarking-based unlearning metric? discussion on watermark based metrics in App. A. existing See 2. Existing works (Lynch et al., 2024; Liu et al., 2024b) have already identified similar limitations about existing unlearning metrics. What is the novelty of the work? We formally define clear desiderata and propose non-retraining based metric thar works despite greater similarity between the forget and retain set and the generalization ability of LLMs. See more discussion in App. A. 3. Why do we only run experiments on TOFU and WaterDrum-Ax instead of other datasets such as WMDP? TOFU and WaterDrum-Ax already cover both LLM question-answering and generation tasks, which are representative of LLM tasks. WMDP is different from TOFU and WaterDrum-Ax in nature because it is specifically for knowledge editing and only contains testing data instead of training data. In this work, we are more concerned about verifying the removal of specific data owners contributions instead of removing specific knowledge. 4. Can our conclusion be generalized to other datasets or other models? Why do we not run experiments on other models? Results on Phi-1.5 (see App. E.2) show that the conclusions can be generalized to other models as well. The two models considered in our paper are representative of recent LLMs, different in terms of model architectural details, and span different model scales. These two models are also the only models considered in (Maini et al., 2024; Wang et al., 2025). 5. Beyond unlearning our watermark metric be used to measure utility preservation/retention? Our metric can be used to effectiveness, can WaterDrum: Watermarking for Data-centric Unlearning Metric 8. Why do we not consider robustness (e.g., recovering knowledge about the forget set by relearning on the retain set) as in (Wang et al., 2025)? We view our work as complementary and do not claim that our desiderata are exhaustive. Our focus is on the most essential desiderata (effectiveness desiderata) and more practical/realistic settings. Figure 14: Benchmark of existing unlearning methods with WaterDrum on the WaterDrum-Ax with duplicate data (DT = DRDF DS , where DF and DS are the duplicated data in the forget and retain sets respectively). For the x-axis, the top figures show WaterDrum scores for the retain set excluding duplicates DR, while the bottom figure shows WaterDrum scores for only the duplicates within the retain set DS . The y-axis for both figures are the same, showing DF . verify that the metric on the retain set in the unlearned model is similar to that in the original model. Hence, by verifying the retain watermark, our metric can also guarantee that there is no catastrophic forgetting and removal of the influence of retain set. 6. Practical significance of unlearning from finetuning data vs pretraining data. In real-life applications, LLM finetuning is performed to enhance the model in specific downstream tasks, which is more likely to make use of task-specific datasets. These datasets are more concerned with privacy/safety issues, and are hence more significant for unlearning than public datasets. 7. What new insights can be gained from the proposed framework? (a) We showed that existing metrics fail on our necessary desiderata (Sec. 3.1), prompting caution on metrics design. (b) Using WaterDrum to benchmark LLM unlearning algorithms (Sec. 5.5) shows that they perform poorly on unlearning and retaining performance. WaterDrum can serve as an optimization criterion for future LLM unlearning algorithms. (c) By emphasizing practical conditions, WaterDrum encourages future LLM unlearning algorithms to consider realistic constraints."
        }
    ],
    "affiliations": [
        "A*STAR, Way, Create",
        "CNRS@CREATE, 1 Singapore #08Tower, Computer Singapore (CFAR), Create",
        "Centre for Singapore Science, National University",
        "Department of of Singapore",
        "Frontier AI Research"
    ]
}