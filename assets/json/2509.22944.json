{
    "paper_title": "SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights",
    "authors": [
        "Lorenz K. Müller",
        "Philippe Bich",
        "Jiawei Zhuang",
        "Ahmet Çelik",
        "Luca Benfenati",
        "Lukas Cavigelli"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Post-training quantization has emerged as the most widely used strategy for deploying large language models at low precision. Still, current methods show perplexity degradation at bit-widths less than or equal to 4, partly because representing outliers causes precision issues in parameters that share the same scales as these outliers. This problem is especially pronounced for calibration-free, uniform quantization methods. We introduce SINQ to augment existing post-training quantizers with an additional second-axis scale factor and a fast Sinkhorn-Knopp-style algorithm that finds scales to normalize per-row and per-column variances, thereby minimizing a novel per-matrix proxy target for quantization: the matrix imbalance. Our method has no interactions between layers and can be trivially applied to new architectures to quantize any linear layers. We evaluate our method on the Qwen3 model family and DeepSeek-V2.5. SINQ improves WikiText2 and C4 perplexity significantly against uncalibrated uniform quantization baselines and can be further enhanced by combining it with calibration and non-uniform quantization levels. Code to reproduce the results of this work and to easily quantize models using SINQ is available at https://github.com/huawei-csl/SINQ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 2 4 4 9 2 2 . 9 0 5 2 : r SINQ: Sinkhorn-Normalized Quantization for LLMs SINQ: SINKHORN-NORMALIZED QUANTIZATION FOR CALIBRATION-FREE LOW-PRECISION LLM WEIGHTS Lorenz K. Muller, Philippe Bich, Jiawei Zhuang, Ahmet Celik, Luca Benfenati & Lukas Cavigelli Computing Systems Lab, Huawei Zurich Research Center lorenz.mueller@huawei.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Post-training quantization has emerged as the most widely used strategy for deploying large language models at low precision. Still, current methods show perplexity degradation at bit-widths 4, partly because representing outliers causes precision issues in parameters that share the same scales as these outliers. This problem is especially pronounced for calibration-free, uniform quantization methods. We introduce SINQ to augment existing post-training quantizers with an additional second-axis scale factor and fast SinkhornKnoppstyle algorithm that finds scales to normalize per-row and per-column variances, thereby minimizing novel per-matrix proxy target for quantization: the matrix imbalance. Our method has no interactions between layers and can be trivially applied to new architectures to quantize any linear layers. We evaluate our method on the Qwen3 model family and DeepSeek-V2.5. SINQ improves WikiText2 and C4 perplexity significantly against uncalibrated uniform quantization baselines and can be further enhanced by combining it with calibration and non-uniform quantization levels. Code to reproduce the results of this work and to easily quantize models using SINQ is available at https://github.com/huawei-csl/SINQ."
        },
        {
            "title": "INTRODUCTION",
            "content": "Post-training quantization (PTQ) is powerful approach to reducing the cost of neural network inference. Weight quantization reduces the storage, memory, and data movement required to run neural network. As such, it is useful on its own whenever any of these components bottleneck the performance of an inference system. When integer (INT) or floating-point (FP) weight quantization is further combined with INT or FP activation quantization, it can also be used to reduce compute requirements by executing MatMul operations at low-precision. Potential speed-ups are substantial: For example, moving from bfloat16 to int4 weights yields potential speedup of 4x in memory-bound scenarios. Weight-only quantization is especially popular in LLM deployment because accelerator memory capacity and data movement are often the initial performance bottlenecks in this scenario. In this paper, we demonstrate that carefully chosen uncalibrated, uniform quantizer can approach the end-to-end output quality of calibrated quantizers or non-uniform formats while being appreciably simpler: Calibration (and even more so end-to-end optimization) is an intuitive approach to improving the output quality of quantized models, but comes with the inherent downsides of possible bias and overfitting (Lin et al. (2024b)) and additional compute time required at quantization time (for models under large-scale deployment, this is not concerning as the quantization cost can be amortized over time, but for small-scale scenarios, this cost can be prohibitive). Similarly, nonuniform formats can offer an improvement over integer quantization (Dettmers et al. (2023)), but require potentially costly look-ups during inference and cannot be combined with activation quantization in compute-limited scenarios. In brief, if uncalibrated uniform quantization were to reach the same output quality, it would be preferable for these reasons. This paper takes step towards closing the gap between these different approaches to quantization. The key contributions of this paper are: We propose adding scaling factor along the second axis of to-be-quantized matrix tiles. 1 SINQ: Sinkhorn-Normalized Quantization for LLMs Figure 1: If we have scales along both dimensions of matrix that is to be quantized, we can trade off the impact of outliers between rows and columns, which is impossible in single-scale quantization. Left: Conceptual illustration of quantization error distributions with single or dual-scaling. Right: Example on small matrix. We propose new proxy metric for ease of quantization of matrix, the matrix imbalance (Eq. 4). We propose fast algorithm based on Sinkhorn-Knopp iterations for finding these dual weight scales to minimize the matrix imbalance (Sec. 2.2.1). In numerous experiments across different model scales, we show that our method improves over state-of-the-art baselines for calibration-free quantization methods."
        },
        {
            "title": "2 METHODS",
            "content": "We divide our method into two parts: Firstly, the quantized parameterization, i.e. the mathematical expression used to map between the full precision and the quantized matrix. All quantization methods used in practice, have some set of auxiliary parameters to use in this mapping. Secondly, the representation space, i.e. the space in which we instantiate the full precision matrix when quantizing it."
        },
        {
            "title": "2.1 QUANTIZED PARAMETRIZATION",
            "content": "Typically, one does not simply replace the weight matrix with, for example, an INT4 matrix, but rather divides it into tiles and assigns some higher-precision auxiliary parameters to each tile. Here, we describe different possibilities for the type of auxiliary parameters to use and how to tile the matrix."
        },
        {
            "title": "2.1.1 PARAMETERIZATION PER TILE",
            "content": "Scales + Shifts The most widely used approach uses scale and shift vector (e.g., Badri & Shaji (2023)), like so: Wapprox = (Q + z) where Wapprox is matrix (or matrix tile), is 1 vector, is 1 vector and is quantized matrix. Also, the transpose of this with 1 vectors is commonly used. (1) Dual-Scales In this paper, we propose new parameterization based on an idea we call dualscaling: Given matrix (or tile of matrix), instead of supplying single vector of scales along one dimension of the matrix, we supply two vectors, one along each dimension. Formulaically, we propose: Wapprox = t (2) where is 1 vector, is 1 vector and the rest is as above. The key benefit of Eq. 2 can be illustrated as follows: Say Wij is an outlying large value. By scaling up si and scaling down tj we can trade off quantization errors that will occur in row for errors in column j. See Fig. 1 for an illustration. Dual-Scales + Shifts improvement justifies it), we can also add shifts to the dual scales: If we do not mind the potential additional overhead (or rather, if an accuracy 2 SINQ: Sinkhorn-Normalized Quantization for LLMs Wapprox = (Q + z) (3)"
        },
        {
            "title": "2.1.2 TILING",
            "content": "Typically, (e.g., Badri & Shaji (2023); Lin et al. (2024b)) tiling for quantization is implemented along one dimension of the matrix that is to be quantized. By consequence, these tiles have rectangular shapes; e.g., matrix tiled with tile-size would yield tiles of shape . This could cause problem with the dual-scale parameterization. Namely, the standard parameterization has 2 M/T scale and shift parameters, while the dual-scaled only has M/T + . To ensure that the dual-scale parameterization has approximately the same number of additional parameters, we can use 2D tiling that divides the matrix into square tiles, e.g., of shape . For square matrices, this yields the same number of auxiliary parameters as the single-scale + shift approach with rectangular tiling. Alternatively, we may use dual-scale parameterization together with shift (as in Eq. 3). With 1D rectangular tiling, dual-scale + shift parameterization has small additional overhead compared to single-scale + shift parameterization; the total auxiliary parameters are 2 M/T + ."
        },
        {
            "title": "2.2 REPRESENTATION SPACE",
            "content": "Before assigning values to the parameters from which we will reconstruct our matrix, we may want to transform the space in which the matrix is represented, to make the reconstruction better aligned with some quality metric (like weight MSE or end-to-end accuracy on some validation data). The two most common among such transformations of the weight space are rotations (like the Hadamard transform (Ashkboos et al. (2024)), or even learned rotations (Liu et al.)), and channel-wise scaling (like in activation aware quantization (AWQ, Lin et al. (2024b)) or Smoothquant (Xiao et al. (2023))). Here, we propose new transformation of the weight matrix using our dual-scaling parameterization."
        },
        {
            "title": "2.2.1 PROXY METRIC AND SINKHORN NORMALIZATION",
            "content": "First, let us give an intuition of why dual-scaling is useful. Our dual-scaling representation offers kind of flexibility in parameter assignment missing in other formats (e.g., Eq. 1): In single scaling formats, an outlier at position (i, j) necessarily causes all values either in column or row to have higher error, because they share large scale (that is needed to represent the outlier). With dualscaling we may choose whether we distribute errors into column or row by assigning higher scale either on the row or the column (see Fig. 1 for an illustration). To find scale factors that balance the impact of outliers between rows and columns, we propose to minimize what we term the imbalance of the matrix. We define the imbalance as I(W) = σmax(W) σmin(W) = maxi{0,1} [W.std(dim=i).max()] mini{0,1} [W.std(dim=i).min()] , (4) where σmax(W) is the maximum across the standard deviations of all rows and columns of the matrix and σmin(W) the corresponding minimum (in pseudo-pytorch notation). Note that the matrix imbalance is inconvenient to optimize with gradient descent, because of the sparse gradients that result from the maximum and minimum operations. Instead, to find such doubly normalizing scale-factors, we propose modified Sinkhorn-Knopp iteration (Sinkhorn & Knopp (1967)), where the goal is not to normalize all column and row sums (as in the standard algorithm), but all column and row standard deviations instead. The central idea is to alternatingly divide the rows and columns by their current standard deviations, see Alg. 1. Note that, in practice, we accumulate the scale factors in the log-domain for numerical stability, clip update values to avoid large jumps, and implement an early-stopping measure that keeps track of the imbalance. We term this approach Sinkhorn Normalized Quantization (SINQ). Akhondzadeh et al. suggest the kurtosis as local proxy metric and optimization target for making matrices more easily quantizable, in the context of finding optimal rotations to apply to each layer. We find that 1) our imbalance optimization substantially reduces the average kurtosis of both rows SINQ: Sinkhorn-Normalized Quantization for LLMs (a) Opt. imbalance (b) Opt. imbalance (c) End-to-end (d) Opt. kurtosis (e) Opt. kurtosis (f) Per Layer Error Figure 2: Results on Qwen3-1.7B. Minimizing the imbalance with our algorithm (a and b) decreases both the imbalance and the kurtosis. Minimizing the kurtosis directly with gradient descent (d and e) yields lower kurtosis, but causes large imbalance; note the log-scale on (d). Finally (c) and (f) show the end-to-end perplexity on WikiText2 and per-layer RTN MSE improvement when optimizing imbalance or kurtosis, respectively. Algorithm 1 SINQ: Alternatingly normalize the standard deviation of the rows and columns of the matrix to be quantized. Then apply standard quantization method (e.g., RTN). Require: Rmn, niter, bits Ensure: Zmn, Rm, Rn 1: σmin min(W.std(dim=0). min(), W.std(dim=1). min()) 2: ˆW 3: for 1 to niter do 4: 5: 6: 7: 8: end for 9: Q, z, Quantize( ˆW) 10: return Q, z, σ1, σ0 ˆW has std. dev. σmin on all rows and columns omit in case of symmetric quantization the quantized matrix, optional shifts, and the two scale vectors σ0 max( ˆW.std(dim=0), σmin) ˆW ˆW/σ0 σ1 max( ˆW.std(dim=1), σmin) ˆW ˆW/σ1 and columns and 2) that directly minimizing kurtosis (while increasing imbalance) in our setting decreases end-to-end accuracy, see Fig. 2. This indicates that for the dual-scaling setting, imbalance is better proxy target for ease of quantization than kurtosis."
        },
        {
            "title": "2.2.2 ACTIVATION-AWARE CALIBRATION: FROM AWQ TO A-SINQ",
            "content": "AWQ (Lin et al. (2024b)) finds vector of scales for each input of linear layer, by minimizing the 2-norm between the linear layers output with the original and the scaled, quantized weight matrix. Formulaically, α = arg min α (cid:13) (cid:13)x WT x/ µx α dq(q( µx α W))T (cid:13) (cid:13)2 , (5) 4 SINQ: Sinkhorn-Normalized Quantization for LLMs where is set of inputs, µx is the sample mean of the absolute value of x, q() is the quantization function, dq() is the dequantization function and α is per-layer parameter (a scalar).1 Notably, AWQ scaling can be combined with SINQ. However, naıve approach does not work. Suppose we feed an awq-scaled matrix into the SINQ algorithm. In that case, the iterated normalization can remove the awq-scales. Instead, we first normalize the matrix as in Alg. 1, then scale the normalized matrix with the awq-scales, and finally quantize. In this ordering of operations, the awq-scales fulfill their purpose of weighting matrix entries by importance. The awq-scales can be absorbed into one of the dual-scales. 2."
        },
        {
            "title": "IMPLEMENTATION CONSIDERATIONS",
            "content": "When using 1D tiling, the second scale can be applied as scale vector to the output of the quantized linear layer, rather than when reconstructing the weight (see Eq. 6). In this formulation, the forward complexity of the dual-scaling approach becomes very similar to AWQ: The term inside the square bracket is the RTN dequantization, and for each linear layer, we need to do one additional element-wise scaling of activations (just like in AWQ). Wapprox = (cid:2)s (Q + z) t(cid:3) = (x [s (Q + z)]) (6)"
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "We evaluate our proposed methods against several strong baselines in 4-bit (and to lesser extent 3-bit) quantization using the permissively licensed and powerful Qwen3 family of models by Yang et al. (2025). We use the evaluation settings of Zheng et al. (2025). In accordance with Dutta et al. (2024), we report perplexities for language modeling and flip percentages for QA tasks. Flip percentages indicate how often the quantized model predicts different result from the original full-precision model. Additionally, results for reasoning benchmarks are provided in the appendix. We highlight here that our method and implementation are architecture agnostic; i.e., there is no interdependency between the quantization of different layers (unlike, e.g., in methods using Hadamard transformations). For all models we tried, it works out of the box. Wherever there is no mention to the contrary, we set the group size to 64, batch-size to 8, and for SINQ use 1D tiling and dual-scaling + shift parameterization. To account for the overhead of different parameterizations and tiling strategies fairly, in our experiments, we report the total memory use (including activations) and look for Pareto-optimal parameterizations in the output quality vs. memory trade-off."
        },
        {
            "title": "3.1 UNCALIBRATED UNIFORM QUANTIZATION",
            "content": "In Tab. 1, our method outperforms the baselines in every uncalibrated case in terms of C4 (Raffel et al. (2020)) and WikiText2 perplexity, sometimes reducing the residual difference to the 16-bit baseline by more than half. Similarly, our method performs best in terms of the average number of flips (see Tab. 2). Fig. 3 shows the memory-perplexity Pareto plot for different quantization methods across wide range of Qwen3 models. Because the Qwen3 models are available in many different sizes, our method can dominate the bfloat16 baselines across large range of available memory, from ca. 1.5 GB to 65 GB. Some additional perplexity results, including on Llama models (Sec. A.3), DeepSeek-V3 (Sec. A.4), and Mixture-of-Experts (MoE, Fedus et al. (2022)) models (Sec. A.7)."
        },
        {
            "title": "3.1.1 RESULTS ON LARGE MODELS",
            "content": "We further evaluate our method on two large models, Qwen3-235B-A22B by Yang et al. (2025) and DeepSeek-V2.5-236B DeepSeek-AI (2024), see Tab. 3. Notably, these are both MoE models, and the latter uses Multi-head Latent Attention (MLA). This underlines the robustness of SINQ to different architectures. 1For results in combination with our method, we modify this formula by changing the norm to 1-norm, which we observe to give slightly better results in combination with SINQ. 5 SINQ: Sinkhorn-Normalized Quantization for LLMs Table 1: Weight-only uncalibrated uniform PTQ on Qwen3 models with 3-bit and 4-bit quantization, reporting perplexity and actual memory usage (GB). Lower is better for all metrics. The best result for given setting is marked in bold. Qwen3-1.7B Qwen3-14B Qwen3-32B Method Mem. Wiki2 C4 Mem. Wiki2 C4 Mem. Wiki2 C4 Original (BF16) RTN Hadamard + RTN HQQ SINQ (ours) RTN Hadamard + RTN HQQ SINQ (ours) - 3 B - 4 3.44 1.28 1.28 1.28 1.28 1.42 1.42 1.42 1.42 16.67 19.21 29. 8.64 12.01 65.52 7.60 10.77 32.43 32.40 32.10 22.39 18.74 19.10 18.96 17.14 31.10 31.07 30.54 24. 9.23 9.23 9.23 9.25 20.81 10.54 20.70 10.54 22.10 10.54 19.83 10.56 10.50 10.60 10.73 9.33 8.95 8.85 8.78 8.76 14.88 17.61 15.10 17.61 14.39 17.62 12.90 17.61 12.50 20.78 12.35 20.78 12.36 20.78 12.21 20. 30.78 35.83 11.26 14.83 9.09 12.58 8.79 11.83 8.92 12.80 8.28 11.60 8.62 12.20 7.74 10.96 Baseline result obtained by running our own implementations. (a) (b) Figure 3: Pareto plot in terms of memory vs. WikiText2 perplexity for Qwen3-0.6B to 32B for different uncalibrated quantization methods. (a) compares different 4-bit methods (including FP4, INT4, and NF4 where available). The maximum distance from the 4-bit pareto front of our method is < 0.01ppl. Note that the difference to the baseline is small. (b) allows bit widths of 4, 6, 8. For 8-bit quantization we inlcude LLM.int8() from Dettmers et al. (2022) as reference method. Both plots include the BF16 model as baseline. For these plots we allow group sizes 64 and 128 for all methods."
        },
        {
            "title": "3.2 UNCALIBRATED NON-UNIFORM QUANTIZATION",
            "content": "SINQ is compatible with non-uniform quantization levels, for example, NF4 as defined by Dettmers et al. (2023). In Tab. 4 we compare to various non-uniform 4-bit quantization methods. We simply replace the quantization function in Alg.1 with the NF4 quantizer. Also here the SINQ method improves over the NF4 baseline. We note that for the 32B model, SINQ with INT4 slightly outperforms SINQ with NF4."
        },
        {
            "title": "3.3 CALIBRATED UNIFORM QUANTIZATION",
            "content": "To demonstrate compatibility with calibration approaches, in Tab. 5 we consider the combination of SINQ and AWQ (see Sec. 2.2.2 for the methodology). For better match to the original AWQ implementation, we quantize our s, to 8 bits in these calibrated experiments. In several cases, even our uncalibrated method outperforms the calibrated baselines, but the addition of AWQ calibration brings further improvements. 6 SINQ: Sinkhorn-Normalized Quantization for LLMs Table 2: Flip rates (%) (as proposed by Dutta et al. (2024)) on HellaSwag, PIQA, and MMLU for Qwen3 models with 3-bit and 4-bit quantization. Lower is better. The best result for given setting is marked in bold. Method HellaSwag PIQA MMLU Avg. HellaSwag PIQA MMLU Avg. Qwen3-14B Qwen3-32B F - T I D R I T - 3 - 4 RTN Hadamard + RTN HQQ SINQ (ours) RTN BnB (FP4) BnB (NF4) Hadamard + RTN HQQ SINQ (ours) GPTQ - 3 - 4 Hadamard + GPTQ A-SINQ (ours) GPTQ Hadamard + GPTQ AWQ A-SINQ (ours) 8.44 8.60 10.68 10.93 7.94 7.99 7.02 5.34 2.92 4.21 2.66 3.63 2.81 2.36 5.18 5.14 5.13 2.24 2.22 2.23 2.20 4.57 5.71 3.10 5.55 4.35 3. 7.83 7.56 7.18 4.13 3.54 3.26 3.11 10.97 16.21 14.28 10.82 4.89 6.72 4.70 4.88 5.17 4.65 11.17 11.15 10.36 4.56 4.53 4.10 4. 9.34 12.60 10.07 7.73 4.13 5.55 3.49 4.69 4.11 3.46 8.06 7.95 7.56 3.64 3.43 3.20 3.18 22.84 17.08 19.83 13.17 9.30 7.13 7.23 5. 4.18 12.32 3.73 4.01 5.83 2.52 6.33 5.52 5.23 2.78 2.70 2.59 2.57 6.31 9.14 3.48 6.02 5.18 3.59 8.76 8.71 7.62 3.48 3.54 4.13 3. 10.61 12.81 10.98 10.21 5.28 6.25 4.76 5.32 4.98 4.69 10.25 10.08 10.15 4.80 4.79 4.44 4.38 16.84 15.27 9.17 7.63 5.26 9.24 3.99 5.12 5.33 3. 8.45 8.10 7.67 3.69 3.68 3.72 3.60 Baseline result obtained by running our own implementations. Table 3: Weight-only PTQ on DeepSeek-V2.5-236B and Qwen3-235B-A22B MoE models with 3-bit and 4-bit quantization, reporting perplexity and actual memory usage (GB). Lower is better for all metrics. The best result for given setting is marked in bold. DeepSeek-V2.5-236B Qwen3-235B-A22B Setting Baseline Method Mem. Wiki2 C4 Mem. Wiki2 Original (BF16) 471. Calibration-free (3-bit) RTN HQQ SINQ (ours) Calibration-free (4-bit) RTN BnB (FP4) BnB (NF4) HQQ SINQ (ours) 110.90 110.92 110.91 134.24 134.52 134.52 134.25 134.51 5. 5.91 5.89 5.82 5.49 5.55 5.49 5.49 5.48 8.15 470.19 8.84 8.76 8.74 8.27 8.41 8.28 8.27 8. 110.98 114.43 110.99 134.03 134.10 134.10 134.03 134.06 5.37 10.11 13.07 6.27 5.65 6.67 5.60 5.60 5.58 C4 9.30 13.92 16.38 10.03 9.49 10.21 9.49 9.46 9."
        },
        {
            "title": "3.4 QUANTIZATION TIME",
            "content": "Quantization with SINQ is fast. On identical hardware, SINQ has an average runtime 1.1 our RTN baseline. This is faster than the already efficient HQQ, at > 2, or calibrated methods like AWQ, at > 30 the RTN baseline. Further details are given in Tab. 7 and Fig. 5 in the appendix."
        },
        {
            "title": "3.5 ABLATION STUDIES",
            "content": "In this section we compare several variants of our method, namely we compare the conditions 1) with and without shifts, 2) 1D and 2D tiling, 3) quantized (int8) and half precision (fp16) auxiliary variables. In Fig. 4, we see that in general, both tilings and precisions work well; differences are minor, and both settings have their sections of the Pareto front. The use of shifts does improve the 7 SINQ: Sinkhorn-Normalized Quantization for LLMs Table 4: Weight-only uncalibrated PTQ on Qwen3 models with 4-bit non-uniform quantization, reporting perplexity and actual memory usage (GB). Lower is better for all metrics. The best nonuniform result for given setting is marked in bold, the results where SINQ with uniform quantization outperforms the non-uniform baselines are marked red. Qwen3-1.7B Qwen3-14B Qwen3-32B Method Mem. Wiki2 C4 Mem. Wiki2 C4 Mem. Wiki2 C4 Original (BF16) - BnB (FP4) BnB (NF4) HIGGS (non-uniform) SINQ (NF4) (ours) SINQ (ours, uniform) 3.44 1.42 1.42 1.51 1.42 1.42 16.67 19.21 29.54 8.64 12.01 65.52 7.60 10. 24.05 23.44 10.59 18.00 20.43 10.59 23.98 25.27 10.28 16.94 19.83 10.56 17.14 19.83 10.56 8.88 12.54 20.67 8.89 12.27 20.67 9.13 12.56 19.88 8.72 12.13 20.73 8.76 12.21 20.73 11.93 16.90 7.94 11.21 8.02 11.24 7.83 10.97 7.74 10.96 Table 5: Weight-only PTQ on Qwen3 models with 3-bit and 4-bit quantization, reporting perplexity and actual memory usage (GB). Lower is better for all metrics. The best result for given setting is marked in bold, the calibration-free results that outperform all calibrated baselines at equal bits (other than our own) are marked red. Method Original (BF16) B - 3 - 4 GPTQ Hadamard + GPTQ A-SINQ (ours) SINQ (ours, calibration-free) GPTQ Hadamard + GPTQ AWQ A-SINQ (ours) SINQ (ours, calibration-free) Qwen3-1.7B Qwen3-14B Qwen3-32B Mem. Wiki2 C4 Mem. Wiki2 C4 Mem. Wiki2 C4 3.44 1.26 1.26 1.26 1. 1.38 1.38 1.38 1.38 1.42 16.67 19.21 29.54 32.21 24.70 22.30 22.39 19.70 18.12 16.90 16.67 17.14 31.05 25.37 24.00 24. 9.28 9.28 8.90 9.25 21.51 10.24 20.38 10.24 19.95 10.25 19.73 10.21 19.83 10.58 8.64 9.54 9.61 9.31 9.33 8.81 8.81 8.78 8.71 8.76 12.01 65. 13.03 17.70 12.92 17.70 12.71 16.68 12.90 17.61 12.22 19.99 12.19 19.99 12.24 20.00 12.13 19.83 12.21 20.73 7.60 9.03 8.51 8.45 8.79 7.80 7.78 7.79 7.78 7.74 10. 12.38 11.63 11.54 11.83 10.99 10.95 10.96 10.93 10.96 Baseline result obtained by running our own implementations. Pareto front appreciably in some places. Based on these results, we choose 1D tiling with shifts as good default setting and quantize the auxiliaries to match the methods we are comparing against."
        },
        {
            "title": "4.1 UNCALIBRATED, UNIFORM INTEGER QUANTIZATION",
            "content": "Most closely related to our approach are works focusing on quantization to uniform integer values without the use of calibration set. Beyond the trivial (but effective) round-to-nearest (RTN) method with scales and shifts chosen to cover the full range of the input weights, there have been two major innovations in this domain. Firstly, half-quadratic quantization (HQQ, Badri & Shaji (2023)) proposes optimizing the values of the shifts found by RTN, so that p-norm (usually = 0.7) error between the original and the quantized matrix becomes minimal. Secondly, applying Hadamard transform to all weights in network has been observed to normalize the weight distributions (Tseng et al. (2024)), which often eases quantization. The Hadamard approach has high-level similarity to our approach, in that we also transform the weight matrices to find an easier-to-quantize format."
        },
        {
            "title": "4.2 NON-UNIFORM QUANTIZATION",
            "content": "After training, neural network weights are usually not uniformly distributed. Therefore, quantization incurs lower errors when the quantization levels are also non-uniform, to match the distribution 8 SINQ: Sinkhorn-Normalized Quantization for LLMs (a) (b) (c) Figure 4: Ablation experiments in the form of memory-perplexity Pareto-fronts across the Qwen3 family on WikiText2. (a) Auxiliary variable precision (b) Tiling dimension (c) With/without shifts. of the trained weights. Dettmers et al. (2023) proposes quantiles of the normal distribution as preferable set of quantization levels resulting in the normal-float-4 (NF4) format (in the 4-bit case). The variance between optimal levels across different layers in network is reduced when the weights of the network have been Hadamard transformed. This is used in HIGGS by Malinovskii et al. (2025) together with non-uniform quantization: Non-uniform quantization levels can be synergistic with weight matrix transformations. SINQ is orthogonal to the uniformity of the quantization levels; we show that it is compatible with non-uniform quantization in NF4-based experiments."
        },
        {
            "title": "4.3 CALIBRATION",
            "content": "If quantization time and potential overfitting can be tolerated, using some data to calibrate the quantized value assignments can be practical approach. highly influential work is GPTQ Frantar et al. (2022) that considers the Hessian for given layer to find weight pairs that can compensate for each other, if their quantization errors have opposite signs. second approach, as seen in AWQ Lin et al. (2024b), is to minimize the prediction error of each linear layer (separately) under quantization (for more details see Sec. 2.2.2). This per-layer prediction error minimization has been further developed by Shao et al. and Ma et al.. Elhoushi & Johnson (2025) combine non-uniform quantization with calibration to learn optimal non-uniform quantization levels. SINQ is orthogonal to calibration; we demonstrate its compatibility with calibration in AWQ-based experiments."
        },
        {
            "title": "4.4 WEIGHT SPACE TRANSFORMATIONS",
            "content": "The concept of weight space transformation, such as applying the Hadamard transform, random rotation, or scaling with diagonal matrix, can be further improved by combining it with calibration and/or non-uniform quantization. HIGGS (Malinovskii et al. (2025)) applies Hadamard transforms and matches non-uniform quantization levels to the typically resulting distribution. QuaRot (Ashkboos et al. (2024)), SpinQuant (Liu et al.), and FlatQuant (Sun et al.) combine various calibration methods with rotations (including the Hadamard transform). Duquant (Lin et al. (2024a)) combines learned rotations with permutations for further flexibility. In Kurtail, Akhondzadeh et al. optimize rotations on kurtosis proxy target. Several of these methods specifically target joint activation and weight quantization. The key differences to our method are that we use the dual-scaling and minimize the matrix imbalance, allowing the method to be uniform, calibration-free and, compared to rotated models, architecture agnostic (similar to HQQ (Badri & Shaji (2023)) and BnB (Dettmers et al. (2023))) in the sense that each linear layer can be treated independently (which is helpful for generalization to new architectures)."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We have proposed using scaling factors in both matrix dimensions when representing weight matrices at low precision, along with an effective method for finding good values for these scaling factors, by simultaneously normalizing the row and column standard deviation through modified Sinkhorn iteration. We show in numerous experiments that this method is fast and outperforms state-of-theart methods for uniform quantization without calibration, and can be combined with widely used calibrated and/or non-uniform methods. 9 SINQ: Sinkhorn-Normalized Quantization for LLMs"
        },
        {
            "title": "REFERENCES",
            "content": "Mohammad Sadegh Akhondzadeh, Aleksandar Bojchevski, Evangelos Eleftheriou, and Martino Dazzi. Kurtail: Kurtosis-based llm quantization. In Sparsity in LLMs (SLLM): Deep Dive into Mixture of Experts, Quantization, Hardware, and Inference. Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian Croci, Bo Li, Pashmina Cameron, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. Advances in Neural Information Processing Systems, 37:100213100240, 2024. Hicham Badri and Appu Shaji. Half-quadratic quantization of large machine learning models, November 2023. URL https://mobiusml.github.io/hqq_blog/. DeepSeek-AI. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model, 2024. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3.int8 (): 8-bit matrix multiplication for transformers at scale. Advances in neural information processing systems, 35: 3031830332, 2022. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in neural information processing systems, 36:1008810115, 2023. Abhinav Dutta, Sanjeev Krishnan, Nipun Kwatra, and Ramachandran Ramjee. Accuracy is not all you need. Advances in Neural Information Processing Systems, 37:124347124390, 2024. Mostafa Elhoushi and Jeff Johnson. any4: Learned 4-bit numeric representation for llms. arXiv preprint arXiv:2507.04610, 2025. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Haokun Lin, Haobo Xu, Yichen Wu, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun, and Ying Wei. Duquant: Distributing outliers via dual transformation makes stronger quantized llms. Advances in Neural Information Processing Systems, 37:8776687800, 2024a. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of machine learning and systems, 6:87100, 2024b. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. Spinquant: Llm quantization with learned rotations. In The Thirteenth International Conference on Learning Representations. Yuexiao Ma, Huixia Li, Xiawu Zheng, Feng Ling, Xuefeng Xiao, Rui Wang, Shilei Wen, Fei Chao, and Rongrong Ji. Affinequant: Affine transformation quantization for large language models. In The Twelfth International Conference on Learning Representations. Vladimir Malinovskii, Andrei Panferov, Ivan Ilin, Han Guo, Peter Richtarik, and Dan Alistarh. Higgs: Pushing the limits of large language model quantization via the linearity theorem. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 1085710886, 2025. 10 SINQ: Sinkhorn-Normalized Quantization for LLMs Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models. In The Twelfth International Conference on Learning Representations. Richard Sinkhorn and Paul Knopp. Concerning nonnegative matrices and doubly stochastic matrices. Pacific Journal of Mathematics, 21(2):343348, 1967. Yuxuan Sun, Ruikang Liu, Haoli Bai, Han Bao, Kang Zhao, Yuening Li, Xianzhi Yu, Lu Hou, Chun Yuan, Xin Jiang, et al. Flatquant: Flatness matters for llm quantization. In Forty-second International Conference on Machine Learning. Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip #: Even better llm quantization with hadamard incoherence and lattice codebooks. In International Conference on Machine Learning, pp. 4863048656. PMLR, 2024. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: In International Accurate and efficient post-training quantization for large language models. conference on machine learning, pp. 3808738099. PMLR, 2023. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Yixin Ye, Yang Xiao, Tiantian Mi, and Pengfei Liu. Aime-preview: rigorous and immediate evaluation framework for advanced mathematical reasoning. https://github.com/ GAIR-NLP/AIME-Preview, 2025. GitHub repository. Xingyu Zheng, Yuye Li, Haoran Chu, Yue Feng, Xudong Ma, Jie Luo, Jinyang Guo, Haotong Qin, Michele Magno, and Xianglong Liu. An empirical study of qwen3 quantization. arXiv preprint arXiv:2505.02214, 2025."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 RESULTS ON REASONING In Tab. 6 we show results on reasoning benchmarks (Ye et al. (2025)). Here, we include the length of reasoning traces to ensure that lengthened reasoning does not negate some of the upside of quantization. Note that these are preliminary pass@1 results. These preliminary findings seem to suggest that the proposed method sustains robust reasoning capabilities while avoiding an increase in reasoning trace length, which is crucial for preserving the efficiency gains achieved through quantization. Table 6: Reasoning performance on Qwen3-14B with 4-bit weight-only PTQ. Method AIME 2024 Qwen3-14B AIME 2025 Avg. Tok. Acc. (%) Tok. Acc. (%) Tok. Acc. (%) Original (FP16) 11 464 10 973 RTN 11 500 BnB (FP4) BnB (NF4) 12 132 Hadamard + RTN 11 210 11 862 HQQ SINQ 11 660 F - T B A B - 4 76.70 66.70 60.00 70.00 70.00 70.00 73.30 12 12 642 12 455 12 899 12 989 12 991 12 305 63.30 50.00 53.30 56.70 53.30 56.70 63.30 0 -242 -72 +930 +99 +367 -67 70. 58.35 56.65 63.35 61.65 63.35 68.30 11 SINQ: Sinkhorn-Normalized Quantization for LLMs A.2 TIMING RESULTS In Tab. 7 we report quantization time results on single GPU for various models. Although precise timings may vary with hardware, our method achieves times comparable to the RTN baseline and even surpasses HQQ, which is already regarded as fast quantization technique. Furthermore, the calibrated version, A-SINQ, is substantially faster than popular state-of-the-art calibrated methods like GPTQ and AWQ. Fig. 5 shows the distribution of quantization times over 10 runs for various popular quantization methods on Qwen3-32B on GPU. Table 7: Average quantization time (seconds) across 10 runs for some Qwen3 models on GPU, comparing different quantization methods. The rightmost column reports the relative average slowdown with respect to RTN. Method RTN HQQ GPTQ AWQ A-SINQ (ours) SINQ (ours) Qwen3-1.7B Qwen3-4B Qwen3-8B Qwen3-14B Qwen3-32B Avg. cost 2.91 0.11 6.32 0.06 11.35 0.31 20.61 0. 46.79 2.52 1.00 3.65 0.13 10.15 0.27 122.45 2.45 24.06 1.54 193.33 1.68 426.89 0.75 669.06 0.84 1160.37 1.68 3064.62 24.33 1613.75 9.79 104.63 9.26 225.27 3.91 392.51 2.86 411.95 0.57 92.17 0.33 49.81 0.13 23.86 0.17 51.56 2.00 13.23 0.64 6.33 0.52 3.03 0.29 695.29 1.19 173.93 0.38 21.38 2. 43.62 0.54 2.32 62.68 34.46 8.54 1.09 Figure 5: Distribution of quantization times for each method for Qwen3-32B. A.3 RESULTS ON LLAMA MODELS In Tab. 8 we report quantization results on Llama family models. These findings further validate the effectiveness of SINQ also on this type of architecture. SINQ: Sinkhorn-Normalized Quantization for LLMs Table 8: Weight-only PTQ on Llama models with 3-bit and 4-bit quantization, reporting perplexity and actual memory usage (GB). Lower is better for all metrics. In bold is the best result for given setting. Llama 2-7B Llama 3-8B Llama 3-70B Method Mem. Wiki2 C4 Mem. Wiki2 C4 Mem. Wiki2 C4 Original (BF16) 14.08 - 3 - T B A B - 4 RTN Hadamard + RTN HQQ SINQ (ours) RTN BnB (FP4) Hadamard + RTN HQQ SINQ (ours) BnB (NF4) SINQ (NF4) (ours) 3.54 3.54 3.62 3.54 4.17 4.17 4.17 4.22 4.19 4.17 4.18 5.47 6.40 6.31 7.05 6.14 5.67 5.76 5.65 5.68 5. 5.65 5.58 6.90 17.45 6.13 9.61 141.11 2. 7.30 8.05 7.89 9.03 7.72 7.14 7.24 7.10 7.13 7.04 7.09 7.03 5.25 5.25 5.24 5.35 6.06 6.06 6.06 6.06 6. 6.07 6.07 10.18 9.97 9.55 8.04 6.61 6.93 6.72 6.58 6.53 6.56 6.51 15.27 15.25 14.68 12.32 10.25 10.75 10.23 10.22 10. 10.20 10.09 35.93 35.93 36.16 35.93 42.71 42.71 42.71 42.71 42.81 42.71 42.81 5.26 4.99 85.64 4.52 3.56 3.58 3.54 3.26 3. 3.22 3.16 10.80 10.45 23.32 8.48 10.58 8.23 9.95 8.13 7.51 7.68 7.50 A.4 RESULTS ON DEEPSEEK-V3 In Tab. 9 we compare HQQ to SINQ on WikiText2 perplexity for DeepSeek-V3 Liu et al. (2024). Table 9: Weight-only PTQ on DeepSeek-V3-685B with 4-bit quantization. We report perplexity on WikiText-2 (lower is better). Best per setting in bold. Setting Method Wiki2 Calibration-free (4-bit) HQQ SINQ 5.38 5.31 A.5 ACCURACY RESULTS In Fig. 6 and Tab. 10 we report accuracy results on various QA tasks. Note that flips (as reported in the main paper) are the more reliable (and less easily manipulated) metric than accuracy for QA tasks, as shown in Dutta et al. (2024). Fig. 6 closely follows the analysis presented in prior work Dutta et al. (2024), further confirming the alignment of our findings with existing literature. 13 SINQ: Sinkhorn-Normalized Quantization for LLMs Figure 6: Comparison of baseline accuracy, accuracy changes, and flip rates across different 4-bit quantization methods (similar to Dutta et al. (2024)). On QA tasks, flips have been shown to be the more consistent quality metric of LLM quantization. Table 10: Accuracy (%) on HellaSwag, PIQA, and MMLU for Qwen3 models with 3-bit and 4-bit quantization. Higher is better. Method HellaSwag PIQA MMLU Avg. HellaSwag PIQA MMLU Avg. Qwen3-14B Qwen3-32B - 3 B - 4 Original (BF16) RTN Hadamard + RTN HQQ SINQ (ours) RTN BnB (FP4) BnB (NF4) Hadamard + RTN HQQ SINQ (ours) SINQ (NF4) (ours) GPTQ B - 3 - 4 Hadamard + GPTQ A-SINQ (ours) GPTQ Hadamard + GPTQ AWQ A-SINQ (ours) F - I B C A L 60.95 56.99 49.66 55.50 58. 60.11 59.41 60.47 58.45 60.24 60.05 60.35 58.34 57.41 58.16 60.55 60.27 60.48 60.84 80.20 78.83 77.80 73.45 77.91 77. 79.11 79.38 79.71 78.67 79.76 79.54 79.72 76.71 77.75 77.15 79.43 79.60 79.38 79.22 75.01 67.53 72.92 75.82 78.44 77.62 78.23 76.58 78.24 78.00 78.37 74.75 75.10 75. 78.11 77.85 78.01 78.07 73.33 69.93 63.55 68.78 70.35 72.55 72.14 72.80 71.23 72.75 72.53 72.81 69.93 70.08 70.24 72.70 72.57 72.62 72. 63.85 46.98 50.43 59.87 60.65 61.22 56.97 63.12 62.90 62.33 63.20 63.18 61.16 61.26 61.47 63.22 63.01 63.51 63.43 80. 81.88 71.82 75.41 77.75 79.49 78.78 77.91 79.98 78.94 79.92 80.85 80.52 77.86 78.45 79.22 80.20 81.01 79.90 80.03 78.53 78.10 78.17 78. 81.78 81.20 81.60 80.99 81.68 81.63 81.32 78.94 78.78 79.00 81.36 80.98 81.38 81.61 75.56 65.78 67.98 71.93 73.11 73.93 72.03 74.90 74.28 74.64 75.23 75. 72.65 72.83 73.23 74.93 75.00 74.93 75.02 A.6 FURTHER COMPARISON TO HIGGS For fairer comparison to the HIGGS method, in Tab. 11 compare it to SINQ with quantized auxiliaries to better match the HIGGS setup. 14 SINQ: Sinkhorn-Normalized Quantization for LLMs Table 11: Comparison to HIGGS method with quantized auxiliary variables to better match the HIGGS memory use. Method Original (BF16) HIGGS (non-uniform) SINQ (NF4) (ours) SINQ (NF4) (ours, q. aux.) - Qwen3-1.7B Qwen3-14B Qwen3-32B Mem. Wiki2 C4 Mem. Wiki2 C4 Mem. Wiki2 C4 3.44 1.51 1.42 1. 16.67 19.21 29.54 8.64 12.01 65.52 7.60 10.77 23.98 25.27 10.28 16.94 19.83 10.56 16.92 19.84 10.19 9.13 12.56 19.88 8.72 12.13 20.73 8.72 12.13 19.80 8.02 11.24 7.83 10.97 7.82 10. A.7 ADDITIONAL RESULTS ON MOE MODELS In Tab. 12 we show some perplexity results on MoE models to underline the flexibility of our method. These results further demonstrate that SINQ is able to outperform state-of-the-art calibration-free methods for weight quantization. Table 12: Weight-only PTQ on DeepSeek-V2-Lite and Qwen3-30B-A3B MoE models with 3-bit and 4-bit quantization, reporting perplexity and actual memory usage (GB). Lower is better for all metrics. In bold is the best result for given setting. DeepSeek-V2-Lite Qwen3-30B-A3B Method Mem. Wiki2 C4 Mem. Wiki2 C4 6.31 7.94 8.36 7.45 6.59 6.82 6.61 6.49 8.83 10.98 11.74 10. 9.19 9.49 9.18 9.07 61.06 15.10 15.10 15.13 18.07 18.08 18.07 18.13 8.70 12.28 10.52 10. 9.04 9.68 9.14 9.02 12.15 15.89 14.39 13.62 12.64 12.93 12.64 12.41 Setting Baseline Original (BF16) 32.55 Calibration-free (3-bit) RTN HQQ SINQ (ours) Calibration-free (4-bit) RTN BnB HQQ SINQ (ours) 9.12 9.12 9.02 10.63 10.63 10.85 10."
        }
    ],
    "affiliations": [
        "Computing Systems Lab, Huawei Zurich Research Center"
    ]
}