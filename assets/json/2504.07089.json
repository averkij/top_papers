{
    "paper_title": "OmniCaptioner: One Captioner to Rule Them All",
    "authors": [
        "Yiting Lu",
        "Jiakang Yuan",
        "Zhen Li",
        "Shitian Zhao",
        "Qi Qin",
        "Xinyue Li",
        "Le Zhuo",
        "Licheng Wen",
        "Dongyang Liu",
        "Yuewen Cao",
        "Xiangchao Yan",
        "Xin Li",
        "Botian Shi",
        "Tao Chen",
        "Zhibo Chen",
        "Lei Bai",
        "Bo Zhang",
        "Peng Gao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 9 8 0 7 0 . 4 0 5 2 : r OMNICAPTIONER: One Captioner to Rule Them All Yiting Lu2,, Jiakang Yuan1,3,, Zhen Li4, Shitian Zhao1, Qi Qin1, Xinyue Li1 Le Zhuo1, Licheng Wen1, Dongyang Liu1, Yuewen Cao1, Xiangchao Yan1 Xin Li2 Botian Shi1, Tao Chen3, Zhibo Chen2,(cid:66), Lei Bai1, Bo Zhang1,,(cid:66), Peng Gao1 1 Shanghai Artificial Intelligence Laboratory, 2 University of Science and Technology of China, 3 Fudan University, 4 The Chinese University of Hong Kong luyt31415@mail.ustc.edu.cn, jkyuan22@m.fudan.edu.cn, chenzhibo@ustc.edu.cn, zhangbo@pjlab.org.cn https://alpha-innovator.github.io/OmniCaptioner-project-page https://github.com/Alpha-Innovator/OmniCaptioner https://huggingface.co/U4R/OmniCaptioner Figure 1: OMNICAPTIONER: the top section demonstrates its capability to process diverse visual domains. The bottom section highlights its applications in visual reasoning (associated with reasoning LLM), image generation (integrated with T2I generation models), and efficient downstream SFT tasks adaptation."
        },
        {
            "title": "Abstract",
            "content": "We propose OMNICAPTIONER, versatile visual captioning framework for generating fine-grained textual descriptions across wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides unified solution for captioning natural Equal contribution, Project Lead, (cid:66) Corresponding authors. Preprint. Technical report. images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OMNICAPTIONER can offer new perspective for bridging the gap between language and visual modalities."
        },
        {
            "title": "Introduction",
            "content": "Pretraining of Multimodal Large Language Models (MLLMs) [23, 18, 6, 39, 2], particularly in bridging the gap between visual and textual domains, has gained significant attention in recent years. Substantial progress has been achieved in image captioning and visual question answering, enabling models to serve as universal visual assistants through large-scale Supervised Fine-Tuning (SFT). However, MLLMs still face limitations in perceptual accuracy in the visual-text and structured image domains, particularly when handling synthesized images that exhibit substantial domain gap from natural images, as illustrated in Fig. 3 (c). Recent research has increasingly emphasized the role of image captioning in aligning modalities during multimodal pretraining, aiming to enhance both perception and reasoning across diverse domains through the SFT process. Meanwhile, domain-specific studies, such as those focusing on document understanding MLLMs [26, 15] and mathematical MLLMs [33, 54, 41], have leveraged domainspecific caption data to further improve modality alignment and advance multimodal pretraining. These advancements highlight the need for unified framework for multimodal pretraining centered on image captioning. Also, despite progress in MLLMs, their multimodal reasoning capabilities still fall short of the textual reasoning abilities of LLMs. As shown in Fig. 2, when provided only with question and no visual input on the MathVision and MathVerse benchmarks, DeepSeek-DistillQwen-7B (orange) significantly outperforms Qwen2-VL-Instruct (blue), demonstrating the strength of LLM-driven reasoning in multimodal tasks. In this work, we bridge this gap by introducing the first OMNICAPTIONER framework, designed to generate fine-grained textual descriptions across diverse visual domains as shown in Fig. 1. Unlike prior approaches that focus on specific visual categories (i.e., natural or geometry images), our approach enables unified solution for diverse image types, paving the way for broader multimodal understanding. We focus on converting low-level pixel features into semantically rich textual representations, which preserve crucial visual details while bridging the modality gap between vision and language. OMNICAPTIONER has two characteristics: i) Diverse Visual Domain Coverage: We present unified framework that supports diverse visual content, including natural images, visual text images (e.g., poster, UI, textbook) and structured images (e.g., geometry, equation, tables, charts). ii) Pixel-to-Text Mapping: By pairing these diverse image types with detailed captions, we convert low-level pixel information into semantically rich, fine-grained textual descriptions, enabling deeper understanding of visual content, which effectively bridges the gap between visual and textual modalities. To evaluate the effectiveness of OMNICAPTIONER, we conduct systematic assessments across both image understanding (e.g., visual reasoning) and image generation tasks (e.g., text-to-image generation). Our results reveal several key advantages: i) Improved Visual Reasoning with LLMs: Our detailed, long-context captions can be directly incorporated into powerful LLMs to address challenging visual reasoning questions, particularly for models like the DeepSeek-R1 [13] series. This approach enables LLMs to perform visual reasoning tasks in training-free manner, leveraging rich textual descriptions without requiring additional fine-tuning. ii) Enhanced Image Generation and Conversion: The detailed captions produced by our framework significantly improve image generation tasks, such as image-to-text generation and image conversion, owing to their near-complete iii) Efficient SFT Process: Leveraging the knowledge from pixel-to-text mapping capability. pretraining on OMNICAPTIONER, the SFT process becomes more efficient, requiring less training data and achieving faster convergence. 2 Figure 2: Performance comparison across different visual benchmarks for different LLMs/MLLMs (7B) with or without visual input. The bar with dashed borders denotes Qwen2-VL-Instruct, indicating it has pixel-level visual input, while others do not. Qwen2-VL-Ins.(NA) refers to setting where only the question is provided as input. We divide the MME score by 100 to have the same scale as other benchmarks. Figure 3: Illustration of OMNICAPTIONERs plug-and-play applications (Sub-figure a, b) and comparison between OMNICAPTIONER and LLava-OneVision-7B on non-natural image captioning (Sub-figure c). Sub-figure (a) shows that OMNICAPTIONER leverages LLMs strong reasoning abilities to perform multimodal reasoning tasks. Sub-figure (b) highlights how hallucinated or inaccurate captionslike those from LLava-OneVision-7B can lead to inconsistent image conversion, revealing weakened alignment capabilities in text-to-image models when captions dont faithfully represent the original content. Sub-figure (c) highlights that LLaVA-OneVision-7B, due to limited exposure to non-natural images during pretraining, struggles with perception in such domains, often leading to hallucinations, whereas OMNICAPTIONER provides more accurate descriptions. Furthermore, the contributions of this paper are summarized below: Unified Visual Captioning Framework: We present OMNICAPTIONER, unified framework for generating captions across diverse domains. Our approach seamlessly integrates captioning capabilities for natural images, visual text images (e.g., posters, UI, and textbooks), and structured visual images (e.g., tables, charts, equations, and geometric diagrams). OMNICAPTIONER sets new standard for generalized visual captioning, enabling more effective and scalable vision-language understanding. Comprehensive Pixel-to-Text Conversion: Our framework leverages detailed captions to convert low-level pixel information into semantically rich, fine-grained textual descriptions, effectively bridging the gap between visual and textual modalities. Particularly, this enhances text-to-image generation by providing more precise and context-aware textual guidance, leading to improved visual fidelity and alignment with the intended semantics. Improved Visual Reasoning with LLMs: By incorporating detailed, long-context captions, our approach enables enhanced visual reasoning capabilities, especially when integrated into LLMs such as the DeepSeek-R1 series. Leveraging the perceptual information provided by OMNICAPTIONER, LLMs can infer and reason within the textual space to effectively solve visual reasoning tasks."
        },
        {
            "title": "2 Related Works",
            "content": "Image Captioning. Image captioning tasks can be broadly classified into two categories. The first approach focuses on generating high-quality captions for natural images. Notably, ShareGPT4V [3] improves vision-language alignment by collecting high-quality, attribute-specific captions through targeted prompts to GPT-4V for natural images, while models like Densefusion [19] leverage multiple expert models to synthesize captions for natural images. The second approach, exemplified by CompCap [5], tackles the challenge of domain diversity during pretraining by incorporating synthetic images to enhance performance on underrepresented domains. However, the first approaches are often constrained by its focus on specific domains, while the second faces challenges due to the relatively small quantity of synthetic images used during training. Multimodal Large Language Models. With the development of LLMs [44, 13, 37, 48], integrating visual perception capability into LLMs (i.e., MLLMs) has received increasing attention. To address the gap between different modalities, most of works [39, 2, 6, 42, 23, 18, 21, 22] first pretrain on image captioning data to obtain vision-language connector (e.g., MLP-based or cross-attention based) and followed by SFT. To better integrate information from multiple modalities, several works [20, 27, 8, 35] try to explore new architectures to process different modalities in single Transformer model. In addition to model architecture, some works [40] try to boost models reasoning ability through post-training (e.g., reinforcement learning) [40] or test-time scaling (e.g., monte-carlo tree search) [45, 28, 9]. Furthermore, recent studies [50, 31, 5, 7] have systematically investigated the influence of data quality on on both the pretraining and SFT phases of MLLMs. MM1 [50] reveals that model capabilities induced through pretraining with high-quality data are effectively preserved after SFT. Most existing open-source MLLMs [23, 18] primarily focus on pretraining with natural images, while domain-specific MLLMs (e.g., math, chart) are trained exclusively on domain-specific caption data. In contrast, we propose more unified pretraining approach that integrates diverse domain knowledge during pretraining. In addition, current MLLMs generally exhibit inferior reasoning capabilities compared to text-only LLMs, OMNICAPTIONER can generate detailed, long-context captions of different domains and use LLMs to address challenging visual reasoning tasks."
        },
        {
            "title": "3 OMNICAPTIONER",
            "content": "To achieve unified multimodal pretraining paradigm and handle diverse visual domains, we first construct diverse caption dataset as shown in Sec. 4. We will provide the dataset description and then detail the dataset construction process in Sec. 3.1 and Sec. 3.2, respectively. And the pertaining process is described in Sec. 3.3. 3.1 Diverse Visual Caption Dataset The diversity of our visual caption dataset is characterized by two dimensions: domain diversity (diverse data sources) and caption formula diversity. To achieve effective unified pretraining, the dataset needs to encompass wider range of domains. For example, when acting as documentation assistant, MLLMs need to comprehend tables and charts, while as GUI agent, they are required to understand elements in web pages. As illustrated in the data distribution section of Fig. 4, our caption dataset is composed of four major categories: natural images, structured images (including chart, table, and so on), visual text images (including UI images, posters, and so on), and video. This comprehensive data coverage enables our model to serve as multi-domain assistant and further enhance the performance on downstream tasks. Furthermore, diverse types of captions may be necessary even for the same visual input. For instance, chart image may require both structured tabular conversion and comprehensive analytical descriptions. To address this requirement, we define diverse caption formulas for each domain. This approach enables our model to generate diverse caption formats, including multilingual (Chinese and English) descriptions, varying granularity levels (from comprehensive to concise), and so on. 3.2 Dataset Construction To generate high-quality captions for images across diverse domains, we propose two-step caption generation pipeline. The design of our pipeline takes into account the need for accurate visual Figure 4: OMNICAPTIONERs diverse visual captioning pipeline. The pipeline consists of SeedCaption Generation to ensure precise pixel-to-word mapping, and Caption Extension to enrich caption styles to support image generation and visual reasoning tasks. OMNICAPTIONER utilizes 21Mcaption dataset, covering diverse domains beyond natural images, enabling more comprehensive captioning capabilities. For further details about dataset composition, please refer to Fig. 7 in Appendix A. descriptions, the flexibility to support different stylistic outputs, the ability to perform reasoning and logic extrapolation, as well as bilingual captioning. Seed Caption Generation. In the first stage, we focus on seed caption generation. The goal is to produce an initial caption that is as accurate as possible, with comprehensive textual description of all relevant visual elements present in the visual signal. This stage leverages carefully designed prompts to guide the powerful closed-source multimodal model GPT-4o to describe all possible visual elements in natural images and visual-text images, ensuring an accurate pixel-to-word mapping. For structured images generated via code, the description is generated as accurately as possible using predefined code rules. The generated seed caption serves as reliable foundation for further refinement in the subsequent stage. Caption Extension. The second stage, caption extension, is responsible for enhancing and diversifying the generated caption. Here, the focus shifts from purely accuracy to incorporating stylistic variation and domain-specific reasoning. The seed caption is extended by introducing bilingual outputs (Chinese and English), with variations ranging from detailed to medium-length, short, and tag-style captions. Additionally, we inject reasoning knowledge relevant to specific domains to enrich the semantic depth of the captions. This allows the captions to not only reflect the visual content but also accommodate nuanced understanding in different contexts. Specially, for Natural Images, we leverage the open-source LLM, Qwen2.5-32B, to adjust the caption length through different prompts, allowing captions to range from medium-length to short and tag-style. Additionally, these varied captions are translated into Chinese, facilitating the creation of bilingual prompts for image generation. The benefit of this approach is to enable more flexible and effective bilingual prompt extraction for image generation tasks. For Visual Text Images, we use open source LLM Qwen2.532B to translate the detailed subtitles generated by GPT-4o into the corresponding Chinese versions to ensure cross-language consistency. For Structured Images, which often relate to mathematical or document-based reasoning (e.g., Chain-of-Thought (CoT) analysis), we prioritize the accuracy of the seed caption. After confirming the seed captions accuracy, we input both the seed caption and the original image into the open-source multimodal model Qwen2-VL-76B for CoT-style caption 5 generation. This approach allows us to condition the captioning process on both the seed captions code (e.g., Markdown, LaTeX) and the image content, reducing hallucinations and improving the reliability of the generated captions. Additionally, we collect structured images without seed captions and directly input them into the same multimodal model for CoT-style caption generation. By decoupling the caption generation process into these two stages, we ensure both high accuracy in representing visual content and flexibility in producing diverse, contextually appropriate captions. 3.3 Unified Pretraining Process To effectively handle the multi-domain nature of the OMNICAPTIONER dataset, which spans broad range of image types and captioning tasks, we propose practical approach utilizing distinct system prompts. These prompts help minimize task conflicts and improve task coordination during training. By customizing system prompts for specific image categories and using fixed set of question templates for various captioning styles, we differentiate between tasks and data types in the pretraining process. This approach facilitates efficient multi-domain training, ensuring robust model performance across diverse tasks and domains. To address the challenge of handling images with large variations in resolution and arbitrary aspect ratios, we leverage the powerful visual understanding capabilities of the Qwen2-VL-7B [39] model. Given that the Qwen2-VL-Instruct model is inherently powerful in managing multi-domain image data, we initialize our model with the Qwen2-VL-Instruct weights. This initialization allows us to effectively fine-tune on the OMNICAPTIONER dataset, ensuring robust performance across wide range of image resolutions and aspect ratios while benefiting from the models ability to generalize across diverse domains."
        },
        {
            "title": "4 One Captioner to Rule Them All",
            "content": "Improved Visual Reasoning Tasks with LLMs. Current MLLMs lag behind LLMs in reasoning capabilities. This discrepancy motivates us to investigate whether LLMs can directly perform visual reasoning without modality-alignment losses that may degrade their reasoning ability while still effectively handling diverse visual reasoning tasks. In this work, we integrate image captioning with large language models (LLMs) to enable seamless visual reasoning in textual space. As illustrated in Fig. 3 (a), firstly, our captioner converts input images (spanning natural images, charts, equations, and beyond) into linguistically dense descriptions that explicitly encode pixel-level structures (e.g., spatial layouts, symbolic operators, tabular hierarchies) into textual space. These captions, acting as lossless semantic proxies, are then directly processed by powerful LLMs (e.g., DeepSeek-R1 [13], Qwen2.5 series [44]) to perform task-agnostic visual reasoning, including geometric problem-solving and spatial analysis. Just shown in Fig. 3, OMNICAPTIONER can transform geometric images into detailed and precise visual descriptions. OMNICAPTIONER accurately describes geometric images, such as circle with diameter and circumferential angles, detailing spatial relationships among points. This enables LLMs to perform logical inferences, like calculating angles, without direct pixel-level perception. There are three key advantages to this approach: i) Decoupled Perception and Reasoning By separating perception (handled by MLLMs) from reasoning (handled by LLMs), our method avoids conflicts between the two capabilities, leading to more effective and accurate visual reasoning. ii) Elimination of Modality-Alignment Training Instead of requiring complex modality-alignment losses, our approach translates visual inputs into linguistic representations, allowing LLMs to process them naturally. This removes the need for additional multimodal training while preserving the reasoning strengths of LLMs. iii) Flexibility and Generalization The plug-and-play design enables seamless integration of LLMs into diverse visual reasoning tasks without domain-specific tuning. This ensures broad applicability across different types of visual inputs, from geometric diagrams to complex tabular structures. Enhanced Image Generation and Conversion. Detailed and accurate image captions play pivotal role in both the training and inference stages of Text-to-Image (T2I) tasks. During training, such captions offer fine-grained supervision by explicitly aligning low-level/high-level visual patterns (e.g., textures, spatial arrangements, object attributes) with precise linguistic semantics. At inference time, as shown in Fig. 3(b), detailed and precise captions substantially enhance image generation quality by guiding the model to follow instructions more faithfullycapturing spatial relationships, object 6 Table 1: Performance comparison on various visual benchmarks between our OMNICAPTIONERinserted LLMs and previous SOTA MLLMs. We would like to emphasize that by utilizing OMNICAPTIONER, LLMs can function as MLLMs without requiring additional training. Moreover, we have observed that, particularly in mathematical scenarios, caption-integrated LLMs surpasses MLLMs with comparable parameter sizes, where MLLMs have undergone rigorous data preparation and GPU-intensive training. Model MME MMMU MathVision MathVerse Olympiad GPT-4V GPT-4o (2024-05) Claude3.5-Sonnet Qwen2-VL-2B [39] InternVL2-2B [6] MinniCPM-V2.0 [46] OMNICAPTIONER + Qwen2.5-3B-Instruct Frontier Models - - - 63.1 69.1 68.3 3B-Level Models 1872 1876 1808 1599 41.1 36.3 38.2 43.0 7B-Level Models Qwen2-VL-7B [39] InternVL2-8B [6] MiniCPM-Llama-V-2.5-8B [47] 2024 Cambrain-1-8B [36] LLava-Onevision-7B [18] MiniCPM-V2.6 [47] OMNICAPTIONER + Qwen2.5-7B-Instruct OMNICAPTIONER + DS-R1-Distill-Qwen-7B 2327 2210 45.8 - 1998 2348 1824 1942 54.1 52.6 - 42.7 48.8 49.8 54.5 47.5 InternVL-Chat-V1.5 [6] InternVL2-26B [6] Cambrian-34B [36] VILA-1.5-40B InternVL2-40B OMNICAPTIONER + Qwen2.5-32B-Instruct OMNICAPTIONER + DS-R1-Distill-Qwen-32B Qwen2-VL-72B [39] InternVL2-76B [6] LLaVA-OneVision-72B [18] OMNICAPTIONER + DS-R1-Distill-Llama-70B 32B-Level Models 46.8 51.2 49.7 55.1 55.2 59.7 59.2 2194 2260 - - 2307 1831 72B-Level Models 64.5 62.7 56.8 64.6 2482 2414 2261 2025 24.0 30.4 - 12.4 12.1 - 16.0 16.3 18.4 - - - 18.3 26.4 36.2 15.0 17.0 - - 16.9 32.1 43. 25.9 23.6 - 42.9 32.8 50.2 - 21.0 25.3 - 22.2 31.9 37.0 - - 26.2 25.7 37.4 40.5 28.4 31.1 - - 36.3 39.7 43.7 - 42.8 39.1 42. 18.0 25.9 - - 0.4 - 7.24 - 1.9 - - - - 10.9 7.8 0.6 3.5 - - 3.9 13.1 13.2 11.2 5.5 - 13.7 interactions, and semantic details with higher fidelity. These benefits highlight the critical role of captions as dense supervisory signal, enabling more precise instruction-following in T2I generation. Efficient SFT Process. The training paradigm of MLLMs typically consists of two sequential phases: pretraining on image-caption data, followed by Supervised Fine-Tuning (SFT). Empirical studies [5, 17, 31] have demonstrated that diverse and high-quality image-caption data (e.g., composite images) can significantly enhance image-language alignment and subsequently promote performance on downstream tasks, such as Visual Question Answering (VQA). OMNICAPTIONER leverages diverse and high-quality domain data (e.g., table, chart, and so on) during the pretraining phase, enabling the model to acquire multi-domain knowledge. During the SFT phase, the multi-domain knowledge serves as crucial foundation for rapid adaptation to downstream tasks across different domains."
        },
        {
            "title": "5 Experiment",
            "content": "To evaluate OMNICAPTIONER, we conduct three primary experiments. The first experiment focuses on visual reasoning with Caption-inserted Large Language Model. In this setup, detailed captions and corresponding questions are provided to the LLM, and its ability to answer the questions is evaluated. We use five benchmark datasets to assess the models performance on this downstream task: MME [10], Mathverse [52], Mathvision [38], MMMU [49] and Olympiad bench [14]. For the LLMs, we select Qwen2.5-3B-Instruct [44], Qwen2.5-7B-Instruct [44], Qwen2.57 Table 2: Performance comparison of models trained with different captioners on GenEval [12] (Resolution: 1024 1024). Methods Color Attri. Sin. Obj. Pos. Colors Counting Overall GenEval SANA-1.0-1.6B [43] SANA-1.0-1.6B + Qwen2-VL [39] SANA-1.0-1.6B + OMNICAPTIONER 38.50 44.29 46.00 98.75 98.44 99. 21.25 26.64 29.50 86.70 86.97 84.57 65.31 57.81 64.06 64.61 65.27 67.58 32B-Instruct [44], DeepSeek-R1-Distill-Qwen-7B [13], DeepSeek-R1-Distill-Qwen-32B [13], and DeepSeek-R1-Distill-LLaMA-70B [13], all chosen for their strong reasoning capabilities. The second experiment evaluates the efficiency of the SFT process. For this, we select the LLaVAOneVision [18] data from the OV stage with chain-of-thought enhancement to assess the SFT version of OMNICAPTIONER across multiple commonly-used benchmarks [10, 49, 29, 30, 38, 52, 25]. The third experiment involves finetuning the text-to-image generation model [34, 11, 43] such as SANA1.0-1.6B [43] with image-caption pairs generated by different captioners (i.e., Qwen2-VL [39], OMNICAPTIONER). The training setting uses resolution of 1024 1024. The models generative performance is then evaluated on the GenEval [12]. 5.1 Main Results Improved Visual Reasoning with LLMs. Our experimental results demonstrate that integrating captions into reasoning-enhanced Large Language Models (LLMs), without any additional fine-tuning, achieves state-of-the-art performance across multiple reasoning benchmarks, including MathVision [38], MathVerse [52], MMMU [49], and Olympiad bench [14]. This highlights the power of OMNICAPTIONER in boosting reasoning capabilities for multiple visual tasks. Specifically, OMNICAPTIONER-inserted LLMs significantly outperform existing models in MathVision across multiple model sizes, underscoring the enhancement of reasoning ability for complex visual and mathematical tasks. Notably, OmniCaptioner + DS-R1-Distill-Qwen-7B and OmniCaptioner + DS-Distill-Qwen32B demonstrate exceptional performance on MathVerse benchmark, significantly outperforming previous models. These results further validate the efficacy of caption-based pretraining in bridging the LLMs comprehension of visual geometry content. In the MMMU benchmark, OmniCaptioner + DS-R1-Distill-Qwen-72B approaches the performance of Qwen2-VL-72B, with minimal gap between them. This result serves as strong evidence that caption integration with reasoning-enhanced LLMs leads to significant visual understanding and reasoning for multidisciplinary content. The successful integration of captions with LLMs across scales, from 3B to 72B, underscores that OMNICAPTIONER consistently enhances LLMs reasoning abilities for visual tasks, yielding improvements irrespective of model size. These results highlight that our unified pretraining methodology, leveraging large-scale caption data, is highly effective strategy for advancing visual reasoning across diverse tasks, outperforming existing approaches even when compared to large-scale fine-tuning methods. Enhanced Image Generation. As illustrated in Tab. 2, to validate the importance of caption accuracy in T2I generation, our model demonstrates significant performance improvements over the Qwen2-VL-Instruct [39] caption and original SANA, on GenEval benchmark. The original SANA model achieves 64.61 overall score on GenEval, which is significantly improved to 65.27 with Qwen2-VL-Instruct and further to 67.58 withOMNICAPTIONER. This +2.97 absolute gain over the vanilla SANA model underscores the effectiveness of high-quality captions in guiding T2I generation. Also, our OMNICAPTIONER outperforms Qwen2-VL-Instruct across various aspects (except colors), showcasing the enhanced accuracy of our caption generation. Efficient SFT. In Tab. 3, we compare the performance of several models on visual perception and reasoning tasks, including LLaVA-OV-7B(SI), LLaVA-OV-7B, Qwen2-VL-base+OV SFT, and our proposed OmniCaptioner+OV SFT model. While LLaVA-OV-7B (SI) and LLaVA-OV-7B use significantly larger datasets for SFT 3.2M and 4.8M examples, respectively our OmniCaptioner+OV SFT achieves comparable results with just 1.6M SFT examples used during the one-vision (OV) stage. key difference lies in the unified pretraining phase of OMNICAPTIONER, which utilizes diverse caption-based dataset prior to the SFT stage. This step equips the model with richer domain knowledge, enabling it to excel in visual instruction-following tasks despite fewer SFT exam8 Table 3: SFT performance comparison across diverse evaluation benchmarks. OmniCaptioner + OV SFT denotes the SFT model based on OMNICAPTIONER, while Qwen2-VL-base + OV SFT is based on Qwen2-VL-Base. LLaVA-OV-7B (SI) represents the model after the single-image training in LLaVA-OneVision [18]. SFT Model Data MME MMMU MathVerse MathVista DocVQA ChartQA LLaVA-OV-7B (SI) [18] LLaVA-OV-7B [18] Qwen2-VL-Base+OV SFT OMNICAPTIONER+OV SFT 3.2M 2109 4.8M 1998 1.6M 1905 1.6M 2045 47.3 48.8 44.4 46.6 26.9 26.2 24.9 25. 56.1 63.2 53.8 57.4 89.3/86.9 90.2/87.5 84.2/- 91.2/- 78.8 80.0 53.5 79.0 Table 4: Comparing different captioners through experiments with captioner-inserted LLM on multiple visual benchmarks. Captioner Selection LLM MME MMMU MathVision MathVerse llava-onevision-qwen2-7b-ov DS-R1-Distill-Qwen-7B DS-R1-Distill-Qwen-7B InternVL2-8B DS-R1-Distill-Qwen-7B Qwen2-VL-7B-Instruct DS-R1-Distill-Qwen-7B OMNICAPTIONER (ours) 1646 1789 1914 1942 22.4 23.1 42.4 47.5 31.7 34.4 31.6 36.2 36.6 39.9 33.0 40. ples. It also reveals that Qwen2-VL-base + SFT lags behind OmniCaptioner + OV SFT, indicating OMNICAPTIONERs superior visual perception capabilities. 5.2 Discussions and Findings We consider conducting three important experiments when combining OMNICAPTIONER with reasoning-enhanced LLMs. First, we evaluate effectiveness using different Qwen versions. Second, we aim to explore the extent to which Qwen2-VL-Instruct (without image input) and mainstream reasoning-enhanced LLMs rely on visual modality information to solve visual reasoning tasks. Third, we compare OMNICAPTIONER to Qwen2-VL-Instruct by modifying the captions provided to the reasoning-enhanced LLMs. For more visualization results of image captioning, video captioning, and text-to-image generation task, please refer to Appendix and Appendix E. Effect of Different Qwen-Family Versions. Fig. 5 illustrates the performance progression of combining OMNICAPTIONER with different versions of Qwen on MMMU and MathVerse. As Qwen evolves from Qwen1-8B-chat to Qwen2.5-7B-Instruct, there is steady improvement in visual reasoning capabilities, driven by the pixel-to-word captioning ability of OMNICAPTIONER. As illustrated in Fig. 2, the performance comparison between OmniCaptioner + Qwen2.5-7B-Base, OmniCaptioner + Qwen2.5-7B-Instruct and OmniCaptioner + DS-R1-Distill-Qwen-7B highlights the advantage of integrating the DeepSeek Distilled Qwen2.5, which excels in mathematical reasoning. The distilled variant (DS-R1-Distill-Qwen-7B) achieves the highest accuracy across MME (1942), Figure 5: Integrate OMNICAPTIONER into different versions of LLMs, enabling them to handle tasks in multimodal scenarios. MathVision (36.2), and MathVerse (40.5), emphasizing the benefits of distilled reasoning ability. In contrast, Qwen2.5-7B-Instruct is better suited for general world knowledge tasks, as reflected in its improved performance on the MMMU (54.5). Impact of Visual Modality on Reasoning-Enhanced LLMs. From Fig. 2, the performance of Qwen2-VL-7B (NA) and DeepSeek-Distill-Qwen-7B suggests that the absence of image input significantly restricts their ability to solve visual reasoning tasks. In contrast, OmniCaptioner + DS-R1-Distill-Qwen-7B, which retains visual modality, achieves substantially higher accuracy than its non-visual input LLM, highlighting the critical role of visual information in enhancing reasoning capabilities. Furthermore, non-visual input LLM DS-R1-Distilled-Qwen-7B significantly outperforms the no-image MLLM (i.e., Qwen2-VL-Instruct-7B) on MathVision and MathVerse, demonstrating the superior reasoning ability of R1 Serious model. Effect of Different Captioners. Tab. 4 presents comparative analysis of different captioners on multiple perception and reasoning benchmarks. Our model, incorporating DeepSeek-Distill-Qwen2.57B, achieves superior performance across all evaluated tasks, significantly outperforming previous approaches. These results highlight the effectiveness of OMNICAPTIONER, whose captions provide more precise and contextually accurate descriptions than those generated by Qwen2-VL-7B-Instruct. The enhanced caption quality contributes to improved visual reasoning tasks, particularly in tasks requiring multi-step inference and detailed visual understanding."
        },
        {
            "title": "6 Conclusion",
            "content": "We have introduced OMNICAPTIONER, unified framework that bridges visual and textual modalities through fine-grained pixel-to-text mapping across diverse domains, including natural images, visual-text images and structured images. By converting low-level visual patterns into semantically rich captions, our approach empowers reasoning-enhanced LLMs (e.g., DeepSeek-R1) to achieve enhanced visual reasoning, and enables precise text-to-image generation through comprehensive semantic preservation. This work pioneers scalable paradigm for multimodal alignment and reasoning, achieving seamless visual-language interoperability without costly label-supervised fine-tuning."
        },
        {
            "title": "Acknowledgement",
            "content": "The research was supported by Shanghai Artificial Intelligence Laboratory, the Shanghai Municipal Science and Technology Major Project, and Shanghai Rising Star Program (Grant No. 23QD1401000)."
        },
        {
            "title": "References",
            "content": "[1] Anas Awadalla, Le Xue, Manli Shu, An Yan, Jun Wang, Senthil Purushwalkam, Sheng Shen, Hannah Lee, Oscar Lo, Jae Sung Park, et al. Blip3-kale: Knowledge augmented large-scale dense captions. arXiv preprint arXiv:2411.07461, 2024. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. In European Conference on Sharegpt4v: Improving large multi-modal models with better captions. Computer Vision, pages 370387. Springer, 2024. [4] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. [5] Xiaohui Chen, Satya Narayan Shukla, Mahmoud Azab, Aashu Singh, Qifan Wang, David Yang, ShengYun Peng, Hanchao Yu, Shen Yan, Xuewen Zhang, et al. Compcap: Improving multimodal large language models with composite captions. arXiv preprint arXiv:2412.05243, 2024. [6] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. 10 [7] Xueqing Deng, Qihang Yu, Ali Athar, Chenglin Yang, Linjie Yang, Xiaojie Jin, Xiaohui Shen, and Liang-Chieh Chen. Coconut-pancap: Joint panoptic segmentation and grounded captions for fine-grained understanding and generation. arXiv preprint arXiv:2502.02589, 2025. [8] Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, and Xinlong Wang. Unveiling encoder-free vision-language models. arXiv preprint arXiv:2406.11832, 2024. [9] Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, and Ji-Rong Wen. Progressive multimodal reasoning via active retrieval. arXiv preprint arXiv:2412.14835, 2024. [10] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. [11] Peng Gao, Le Zhuo, Dongyang Liu, Ruoyi Du, Xu Luo, Longtian Qiu, Yuhang Zhang, Chen Lin, Rongjie Huang, Shijie Geng, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. [12] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. [13] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [14] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [15] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, et al. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding. arXiv preprint arXiv:2403.12895, 2024. [16] Zihan Huang, Tao Wu, Wang Lin, Shengyu Zhang, Jingyuan Chen, and Fei Wu. Autogeo: Automating geometric image dataset creation for enhanced geometry understanding. arXiv preprint arXiv:2409.09039, 2024. [17] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. [18] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [19] Xiaotong Li, Fan Zhang, Haiwen Diao, Yueze Wang, Xinlong Wang, and Ling-Yu Duan. Densefusion-1m: Merging vision experts for comprehensive multimodal perception. arXiv preprint arXiv:2407.08303, 2024. [20] Xi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Ghosh, Luke Zettlemoyer, and Armen Aghajanyan. Moma: Efficient early-fusion pre-training with mixture of modality-aware experts. arXiv preprint arXiv:2407.21770, 2024. [21] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023. [22] Dongyang Liu, Renrui Zhang, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, Kaipeng Zhang, et al. Sphinx-x: Scaling data and parameters for family of multi-modal large language models. arXiv preprint arXiv:2402.05935, 2024. [23] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [24] Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, and Xiang Yue. Harnessing webpage uis for text-rich visual understanding. arXiv preprint arXiv:2410.13824, 2024. 11 [25] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [26] Chuwei Luo, Yufan Shen, Zhaoqing Zhu, Qi Zheng, Zhi Yu, and Cong Yao. Layoutllm: Layout instruction tuning with large language models for document understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1563015640, 2024. [27] Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jifeng Dai, Yu Qiao, and Xizhou Zhu. Mono-internvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pretraining. arXiv preprint arXiv:2410.08202, 2024. [28] Ruilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu, Xinzhe Ni, Zicheng Lin, Jin Zeng, and Yujiu Yang. Ursa: Understanding and verifying chain-of-thought reasoning in multimodal mathematics. arXiv preprint arXiv:2501.04686, 2025. [29] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [30] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [31] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Anton Belyi, et al. Mm1: methods, analysis and insights from multimodal llm pre-training. In European Conference on Computer Vision, pages 304323. Springer, 2024. [32] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-to-video generation. arXiv preprint arXiv:2407.02371, 2024. [33] Tianshuo Peng, Mingsheng Li, Hongbin Zhou, Renqiu Xia, Renrui Zhang, Lei Bai, Song Mao, Bin Wang, Conghui He, Aojun Zhou, et al. Chimera: Improving generalist model with domain-specific experts. arXiv preprint arXiv:2412.05983, 2024. [34] Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, et al. Lumina-image 2.0: unified and efficient image generative framework. arXiv preprint arXiv:2503.21758, 2025. [35] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [36] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. [37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [38] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804, 2024. [39] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [40] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, et al. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024. [41] Renqiu Xia, Mingsheng Li, Hancheng Ye, Wenjie Wu, Hongbin Zhou, Jiakang Yuan, Tianshuo Peng, Xinyu Cai, Xiangchao Yan, Bin Wang, et al. Geox: Geometric problem solving through unified formalized vision-language pre-training. arXiv preprint arXiv:2412.11863, 2024. [42] Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi Yan, et al. Chartx & chartvlm: versatile benchmark and foundation model for complicated chart reasoning. arXiv preprint arXiv:2402.12185, 2024. 12 [43] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution text-to-image synthesis with linear diffusion transformers. In The Thirteenth International Conference on Learning Representations, 2025. [44] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [45] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. [46] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [47] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [48] Jiakang Yuan, Xiangchao Yan, Botian Shi, Tao Chen, Wanli Ouyang, Bo Zhang, Lei Bai, Yu Qiao, and Bowen Zhou. Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback. arXiv preprint arXiv:2501.03916, 2025. [49] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [50] Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, et al. Mm1. 5: Methods, analysis & insights from multimodal llm fine-tuning. arXiv preprint arXiv:2409.20566, 2024. [51] Liang Zhang, Anwen Hu, Haiyang Xu, Ming Yan, Yichen Xu, Qin Jin, Ji Zhang, and Fei Huang. Tinychart: Efficient chart understanding with visual token merging and program-of-thoughts learning. arXiv preprint arXiv:2404.16635, 2024. [52] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [53] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Ziyu Guo, Shicheng Li, Yichi Zhang, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, et al. Mavis: Mathematical visual instruction tuning with an automatic data engine. arXiv preprint arXiv:2407.08739, 2024. [54] Shan Zhang, Aotian Chen, Yanpeng Sun, Jindong Gu, Yi-Yu Zheng, Piotr Koniusz, Kai Zou, Anton van den Hengel, and Yuan Xue. Open eyes, then reason: Fine-grained visual mathematical understanding in mllms. arXiv preprint arXiv:2501.06430, 2025. [55] Mingyu Zheng, Xinwei Feng, Qingyi Si, Qiaoqiao She, Zheng Lin, Wenbin Jiang, and Weiping Wang. Multimodal table understanding. arXiv preprint arXiv:2406.08100, 2024."
        },
        {
            "title": "A OmniCaptioner Dataset Composition",
            "content": "As shown in Fig. 7, the OMNICAPTIONER dataset is large-scale multimodal benchmark comprising images, tables, charts, mathematical geometry/equations, posters, PDFs, UI elements, and videos, with captions available in both English and Chinese. The dataset includes natural images sourced from in-house collections, BLIP3Kale [1], and DenseFusion [19]. Tabular data are collected from the arXiv website and the open-source MMTab dataset [55], while chart data originate from arXiv website and TinyChart [51]. Mathematical content, including equations and geometric structures, is sourced from arXiv and generated from datasets such as MAVIS [53] and AutoGeo [16]. UI data are obtained from the MultiUI dataset [24], while poster images feature OCR-based captions. Video captions are derived from OpenVid [32] and Panda [4], covering multiple attributes such as detailed descriptions, style, background, tags, camera angles, and object information. Fig. 6 illustrates the token length distribution for different caption types associated with natural images, categorized into detailed, medium, short, and tag captions."
        },
        {
            "title": "B Experimental Setup",
            "content": "We fine-tune the Qwen2-VL-7B-Instruct model on large-scale captioning dataset using 64 A100 GPUs. The training process is distributed using torchrun with the DeepSpeed ZeRO-3 optimization strategy. Hyperparameters: Batch Size: 256 (1 per device, with gradient accumulation of 8) Learning Rate: 1e-5 (base model), 1e-5 (merger module), 2e-6 (vision tower) Weight Decay: 0.0 Warmup Ratio: 3% Scheduler: Cosine decay Precision: BF16 enabled, Gradient Checkpointing: Enabled Training Details: Image Resolution: From 22828 to 64002828 pixels Epochs: Figure 6: Token length distribution for natural images."
        },
        {
            "title": "C System Prompt Example",
            "content": "Fig. 8 presents different system prompts used in OMNICAPTIONER for various image types. It categorizes prompts into three sections: natural images, visual text images, and structured images. These prompts guide the models captioning style and task-specific adaptations. Figure 7: Dataset composition for pretraining OMNICAPTIONER."
        },
        {
            "title": "D Caption Visualization",
            "content": "As illustrated in Fig. 9 to Fig. 15, we present comprehensive visualization of captioning results across multiple tasks using OMNICAPTIONER, including natural images, table images, chart images, math images, poster images, and videos. For natural images, we demonstrate the impact of different system prompts on caption generation, showcasing how specific prompts can elicit world knowledge in the models responses in Fig. 16. In the case of structured images from Fig. 17, different system prompts lead to distinct stylistic variations in captioning, reflecting the adaptability of the model to various formatting requirements. Additionally, we visualize how OmniCaptioner-generated captions can enhance DeepSeek-R1-Distill-LLaMA-70B in Fig. 18, Fig. 19 and Fig. 20, enabling it to tackle visual tasks more effectively. These visualizations highlight the versatility and robustness of OMNICAPTIONER in handling diverse multimodal data, demonstrating its potential for improving vision-language understanding. 15 Text-to-Image Generation The visualization from Fig. 21 demonstrates that OMNICAPTIONERs detailed captions significantly enhance the text-to-image (T2I) alignment in models like SANA 1.0 [43]. By providing precise and richly descriptive textual caption, the generated images exhibit improved fidelity to the original prompts. We also present some image conversion examples in Fig. 22 to illustrate the pixel-toword ability of our OMNICAPTIONER. All the generated images shown above are produced by the generation model trained on image data labeled by OMNICAPTIONER, fine-tuned using SANA 1.0 with 1.6B parameters. Figure 8: Different system prompts used for OMNICAPTIONER. Figure 9: Natural image captioning. 17 Figure 10: Table/Chart image captioning. 18 Figure 11: Visual-Text image captioning. Figure 12: Math image captioning. 20 Figure 13: UI captioning. 21 Figure 14: PDF captioning. Figure 15: Video captioning. 23 Figure 16: Natural image captioning with different system prompts. 24 Figure 17: Structured image captioning with different system prompts. Figure 18: Visualization of thinking process with OMNICAPTIONER for natural images. 26 Figure 19: Visualization of thinking process with OMNICAPTIONER. 27 Figure 20: Visualization of thinking process with OMNICAPTIONER for math images. Figure 21: The detailed caption from OMNICAPTIONER enhances the alignment capability of text-toimage generation by providing precise descriptions, ensuring that the generated image accurately reflects the intended concepts, attributes, and relationships. The generation model here is fine-tuned on images labeled by OMNICAPTIONER, using the SANA 1.0 model with 1.6B parameters. 29 Figure 22: Image Conversion through OMNICAPTIONER and SANA-1.0. The generation model, SANA-1.0, is fine-tuned on images annotated by OMNICAPTIONER, enabling more accurate and semantically aligned image generation."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai Artificial Intelligence Laboratory",
        "The Chinese University of Hong Kong",
        "University of Science and Technology of China"
    ]
}