{
    "paper_title": "Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry",
    "authors": [
        "Zhuochun Li",
        "Yong Zhang",
        "Ming Li",
        "Yuelyu Ji",
        "Yiming Zeng",
        "Ning Cheng",
        "Yun Zhu",
        "Yanmeng Wang",
        "Shaojun Wang",
        "Jing Xiao",
        "Daqing He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are widely used as reference-free evaluators via prompting, but this \"LLM-as-a-Judge\" paradigm is costly, opaque, and sensitive to prompt design. In this work, we investigate whether smaller models can serve as efficient evaluators by leveraging internal representations instead of surface generation. We uncover a consistent empirical pattern: small LMs, despite with weak generative ability, encode rich evaluative signals in their hidden states. This motivates us to propose the Semantic Capacity Asymmetry Hypothesis: evaluation requires significantly less semantic capacity than generation and can be grounded in intermediate representations, suggesting that evaluation does not necessarily need to rely on large-scale generative models but can instead leverage latent features from smaller ones. Our findings motivate a paradigm shift from LLM-as-a-Judge to Representation-as-a-Judge, a decoding-free evaluation strategy that probes internal model structure rather than relying on prompted output. We instantiate this paradigm through INSPECTOR, a probing-based framework that predicts aspect-level evaluation scores from small model representations. Experiments on reasoning benchmarks (GSM8K, MATH, GPQA) show that INSPECTOR substantially outperforms prompting-based small LMs and closely approximates full LLM judges, while offering a more efficient, reliable, and interpretable alternative for scalable evaluation."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 3 ] . [ 1 8 8 5 2 2 . 1 0 6 2 : r RETHINKING LLM-AS-A-JUDGE: REPRESENTATIONAS-A-JUDGE WITH SMALL LANGUAGE MODELS VIA SEMANTIC CAPACITY ASYMMETRY Zhuochun Li1,2,, Yong Zhang1,, Ming Li3, Yuelyu Ji2, Yiming Zeng4, Ning Cheng1,*, Yun Zhu1, Yanmeng Wang1, Shaojun Wang1, Jing Xiao1, Daqing He2 1Ping An Technology (Shenzhen) Co., Ltd. 2University of Pittsburgh 3University of Maryland, College Park 4University of Connecticut {zhl163,yuj49,dah44}@pitt.edu {zhangyong203,chengning211}@pingan.com.cn {minglii}@umd.edu {yiming.zeng}@uconn.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) are widely used as reference-free evaluators via prompting, but this LLM-as-a-Judge paradigm is costly, opaque, and sensitive to prompt design. In this work, we investigate whether smaller models can serve as efficient evaluators by leveraging internal representations instead of surface generation. We uncover consistent empirical pattern: small LMs, despite with weak generative ability, encode rich evaluative signals in their hidden states. This motivates us to propose the Semantic Capacity Asymmetry Hypothesis: evaluation requires significantly less semantic capacity than generation and can be grounded in intermediate representations, suggesting that evaluation does not necessarily need to rely on large-scale generative models but can instead leverage latent features from smaller ones. Our findings motivate paradigm shift from LLM-asa-Judge to Representation-as-a-Judge, decoding-free evaluation strategy that probes internal model structure rather than relying on prompted output. We instantiate this paradigm through INSPECTOR, probing-based framework that predicts aspect-level evaluation scores from small model representations. Experiments on reasoning benchmarks (GSM8K, MATH, GPQA) show that INSPECTOR substantially outperforms prompting-based small LMs and closely approximates full LLM judges, while offering more efficient, reliable, and interpretable alternative for scalable evaluation. The code and data are available at: https: //github.com/zhuochunli/Representation-as-a-judge"
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) have demonstrated remarkable capabilities in generation, reasoning, and alignment tasks (Achiam et al., 2023; Touvron et al., 2023). growing number of works leverage the paradigm of LLM-as-a-Judge, wherein powerful LLMs are prompted to assess the quality of generated outputs without access to ground-truth references (Chang et al., 2024; Prasad et al., 2023). This approach has achieved strong empirical results in reference-free evaluation across domains such as summarization and complex reasoning (He et al., 2023; Zhang et al., 2024). However, this prompt-based evaluation paradigm has important limitations. First, it requires autoregressive decoding, making it computationally expensive even for single-point evaluations. Second, it relies on large proprietary models (e.g., GPT-4), whose internal mechanisms remain opaque and unverifiable. Lastly, its effectiveness depends heavily on prompt engineering, raising concerns about reproducibility, robustness, and scaling (Polo et al., 2024; Voronov et al., 2024; Mizrahi et al., 2024). Equal contribution. * Corresponding author. Work was done during Zhuochun Lis internship at Ping An Technology. 1 Figure 1: Illustration of Representation-as-a-Judge. natural goal is to use smaller open-source LMs as evaluators, as they offer more lightweight and accessible alternative to large proprietary models. However, when prompted directly, their evaluation performance is poor and highly inconsistent compared to large LLMs. Prior work (Li et al., 2024a; Waldis et al., 2024) has shown that small models, despite weaker generation, often possess semantic competence comparable to large models. This suggests that their poor evaluation performance may stem from limitations in surface generation rather than fundamental lack of understanding. Building on this, we ask more granular question: do small models encode evaluation-relevant signals in their internal representations, even when generation is poor? the ability to evaluate may place lower demands on This question represents broader insight: semantic capacity than the ability to generate. Even when generation fails, compact internal representations may already encode the features needed for judgment. We therefore formalize the Semantic Capacity Asymmetry Hypothesis: The semantic capacity required for accurate evaluation is significantly lower than that required for generation. Evaluation can be grounded in compressed internal representations of small language models, even when generation requires full-model decoding. Building on this hypothesis, we advance an alternative perspective on evaluation: Representationas-a-Judge. Rather than relying on prompted text generation, evaluative signals can be extracted directly from the latent structure. This addresses key bottlenecks of prompt-based evaluation and opens the door to lightweight, interpretable, and scalable systems. To explore this perspective, we introduce INSPECTOR ( INternal Signal Probing and EvaluaTion Of Representations), probing-based framework for reference-free evaluation. Given (prompt, response) pair, INSPECTOR performs the following steps: (1) obtain aspect-level evaluation scores from strong LLM judge across multiple aspects (e.g., logicality, fluency, consistency); (2) input the same evaluation prompt to small LM and extract internal representations; (3) identify informative layers and train lightweight classifiers to approximate evaluation scores using latent embeddings. Empirically, we validate this framework on reasoning benchmarks such as GSM8K, MATH, and GPQA, and find that internal representations from small models (e.g., 1.7B) achieve high predictive performance, substantially outperforming prompting-based baselines and in many cases approaching the fidelity of full-scale LLM judges. Furthermore, we show that classifiers trained with this method can be used to filter noisy reasoning data, leading to measurable gains in downstream supervised fine-tuning (Xu et al., 2024; Albalak et al., 2024; Li et al., 2024b). Our contributions are threefold: We identify and analyze consistent empirical phenomenon: small LMs, despite weak generation, encode evaluation-relevant signals in their internal representations. We formalize this insight as the Semantic Capacity Asymmetry Hypothesis, positing that evaluation requires less semantic capacity than generation and can be grounded in intermediate representations. We advance new perspective, Representation-as-a-Judge, and instantiate it through our probing-based framework, INSPECTOR, demonstrating high-fidelity evaluation and efficient data filtering for reasoning tasks. To the best of our knowledge, this is the first work to investigate LLM probing for evaluation tasks. 2 Figure 2: Overview of our proposed INSPECTOR. We freeze the small LMs and probe their representations, training only lightweight probing classifier to fit the proprietary LLM evaluations."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Our work builds on two research directions: (i) data evaluation methods, particularly the emerging LLM-as-a-Judge paradigm, and (ii) probing techniques for analyzing LLM representations. Below we summarize progress in each line and position our contribution. LLM Evaluation Recent studies in NLP have introduced reference-based evaluation metrics such as BERTScore (Zhang et al., 2019) and BARTScore (Yuan et al., 2021), which leverage pretrained language models to better align with human judgments. ROSCOE further develops unsupervised, reference-free metrics that outperform traditional n-gram methods like ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), as well as several model-based metrics. With the emergence of LLMs, many works have begun to treat LLMs themselves as evaluators (Zheng et al., 2023; Liu et al., 2023; Li et al., 2024c), identifying and filtering high-quality data samples. variety of techniques enhance this LLM-as-a-Judge paradigm: Chain-of-Thought reasoning (Wei et al., 2022) to improve decision quality, RECEVAL (Prasad et al., 2023) to assess reasoning chains via correctness and informativeness, and SOCREVAL (He et al., 2023) to let LLMs generate their own answers before evaluation. However, these methods often rely on prompt engineering and proprietary LLMs, limiting both their reliability and interpretability. Alternatively, we propose to probe and analyze critical internal representations of small LMs, training lightweight classifiers to approximate state-of-the-art evaluations from powerful LLMs. This yields more efficient and explainable approach to data evaluation. Probing LLM Representations There is growing interest in probing the internal representations of LLMs to uncover interpretable features. Early work by Shi et al. (2016) introduced probing by training logistic regression classifier on top of machine translation encoders to study the extent of syntactic information. Building on this idea, later studies investigate more complex linguistic phenomena. For example, Starace et al. (2023) examine how linguistic categories such as part-ofspeech (POS) and dependency relations are jointly encoded across layers of LLMs, revealing shared and hierarchical structures in their representations. Beyond linguistic categories, recent research has explored whether LLMs capture higher-level abstractions of world knowledge and state. Zhang et al. (2025) propose Sentinel, which probes decoder attention in small proxy LLMs to extract relevance signals for context compression, framing probing as lightweight understanding task rather than linguistic diagnostic. More works further probe the internal representations of world states in Transformer models when processing game scripts and embodied sequences (Li et al., 2023; Jin & Rinard, 2024; Di Palma et al., 2025). In contrast to prior work, which mainly seeks to understand what knowledge LLMs encode, we leverage probing in new direction: extracting critical internal representations that are predictive of evaluation quality. To the best of our knowledge, this is the first work to bridge probing with the LLM-as-a-Judge paradigm, enabling evaluation that is both more efficient and more interpretable."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "The Semantic Capacity Asymmetry Hypothesis suggests that evaluation can be grounded in compact intermediate representations of small LMs, without requiring full decoding. To operationalize and test this idea, we propose REPRESENTATION-AS-A-JUDGE: new evaluation paradigm that directly probes latent semantic structure rather than relying on surface-level outputs. We instantiate this paradigm through INSPECTOR. Illustrated in Figure2, it consists of three components: LLMs evaluation annotation, small LMs probing, and building probing classifiers, which we will discuss in Sec. 4.1, 4.2, and 4.3, respectively."
        },
        {
            "title": "3.1 LLMS EVALUATION ANNOTATION",
            "content": "Following established rubric definitions from prior work, including ROSCOE Golovneva et al. (2022) and Socreval He et al. (2023), we adopt five widely used evaluation aspects to assess the quality of reference-free rationales, with the corresponding few-shot prompts provided in Appendix K: Semantic Consistency: Do the solution steps and final answer remain faithful to the problem facts (no invented events, omitted givens, or unstated assumptions)? Logicality: Does each inference and arithmetic step follow valid rules and correctly apply operations? Informativeness: Does the rationale include the essential steps and intermediate calculations needed to verify the final answer? Fluency: Is the text grammatical, clear, and easy to follow, with proper punctuation, sentence flow, notation, and presentation? Factuality: Are the claims, facts, evidence, references, and concrete assertions in the rationale factually correct and supported? Let denote task instruction and the corresponding model-generated response. We first employ medium-scale language model Mmed (10-50B) to generate responses for all benchmark queries. Unlike state-of-the-art LLMs Mlarge (50100B+), Mmed is less powerful, which is desirable because it produces more diverse evaluation distributions, containing both good and bad quality samples, across different aspects, thereby facilitating the subsequent probing process. This will construct the response dataset = {(xi, yi)}N i=1, where xi is benchmark question and yi = Mmed(xi) is the corresponding response. For each evaluation aspect K, we construct an aspect-specific instruction Ik and integrate (x, y) into it, yielding Ik(x, y) as the evaluation prompt. We then query state-of-the-art LLM Mlarge to obtain evaluation scores along each aspect. For given sample (x, y), we prompt Mlarge with Ik(x, y) and obtain scalar score sk [1, 5]:"
        },
        {
            "title": "Collecting these pairs yields the probing dataset",
            "content": "si,k = Mlarge (cid:0)Ik(xi, yi)(cid:1). Dprob = (cid:91) kK { (Ik(xi, yi), si,k) }N i=1. (1) To avoid bias toward over-represented quality levels, we construct balanced probing dataset for each aspect: we first determine the minimum number of samples among the five score levels (15), and then randomly downsample the other levels to this size. This ensures that the multiclass probing tasks are balanced across labels and not dominated by frequent scores. Specifically, original scores (15) are treated as multiclass classification tasks, while responses with scores higher than threshold τ are labeled high quality and the rest low quality, forming simpler binary classification task. Since these aspects have been validated and the effectiveness of powerful LLMs has been extensively studied in prior work, we treat the resulting scores as gold labels for our subsequent LM probing experiments."
        },
        {
            "title": "3.2 SMALL LMS PROBING",
            "content": "We use probing to test whether small LMs encode linearly recoverable evaluative cues in their hidden states, with the probes minimal capacity ensuring that any predictive signal reflects the models own semantics. In our tasks, we observe that prompt-based evaluation inference from small LMs Msmall (0-10B) yields large gap compared to the gold scores from Mlarge, primarily due to differences in model capacity. To address this, instead of relying solely on the final decoded text, we analyze internal representations layer by layer to identify those most predictive of the gold evaluation scores. Specifically, we convert each prompt Ik(xi, yi) Dprob into collection of per-layer, pooled representation features, and fit these features with simple, cross-validated linear probes. More explanations could be found in Appendix B. 4 Extraction and pooling. For sample i, let Si denote the sequence length and = 1, ..., Si index tokens, we input prompt Ik(xi, yi) and run Msmall in evaluation mode, obtaining per-layer ℓ hidden states H(ℓ) we compute small but expressive set of pooled vectors: mean, last, min, max, and concat. These pooling variants capture complementary tokenlevel and global signals across layers. For example: and attention weights A(ℓ) . From H(ℓ) i r(ℓ) i,mean = 1 Si Si(cid:88) H(ℓ) [t, :] Rd i,last = H(ℓ) r(ℓ) t=1 , :] Rd, [t = max{t : token non-pad} (2) Attention and statistical features. For each head of layer ℓ, we compute an attention-entropy statistic e(ℓ) i,h and compress them into the number of attention heads per layer: µ(ℓ) ="
        },
        {
            "title": "1\nR",
            "content": "(cid:88) e(ℓ) i,h, σ(ℓ) = stdh (cid:0)e(ℓ) i,h (cid:1), max e(ℓ) i,h. (3) For each pooled vector we also compute compact statistics, including its norm norm(r), variance var(r), and entropy E(r). Feature assembly. For each layer ℓ and pooling type {mean,last,min,max,concat}, we construct feature matrix by concatenating multiple components along the feature dimension: (ℓ,p) = (cid:2) PCAd(r(ℓ) i,p) (cid:12) (cid:12) [norm, var, E()] (cid:123)(cid:122) (cid:125) statistics where PCAd denotes an optional dimensionality-reduction to components. All transforms that depend on the data (imputer, PCA, scaler) are applied inside cross-validation pipeline to avoid information leakage. , max (4) i= (cid:124) (cid:12) (cid:12) [µ(ℓ) , σ(ℓ) i,h] (cid:3)N e(ℓ) Probing and layer ranking. We treat gold labels si,k {1, . . . , 5} from Dprob as both (i) i,k = I[ si,k τ ] with threshold multiclass prediction target ymulti i,k = si,k and (ii) binary target ybin τ (high vs. low quality). For each (ℓ,p) we fit logistic probe and evaluate generalization with stratified cross-validation. Based on the fitting results, we rank layerpoolfeature configurations by chosen criterion (binary or multiclass performance) for different downstream tasks. The probing stage produces: (1) ranked list of layerpoolfeature configurations with crossvalidated predictive performance, (2) per-layer progression plots that visualize where evaluative signal accumulates inside Msmall. These results inform the subsequent section, where we build final probing classifiers using the selected features."
        },
        {
            "title": "3.3 BUILDING PROBING CLASSIFIERS",
            "content": "The probing stage produces ranked list of layerpoolfeature configurations with predictive performance. In this section we describe how we use that ranked list to assemble multi-layer feature sets, train final probing classifiers, and select an optimal evaluator. From ranked configurations to candidate layer sets. Let π denote the ranked list of layerpoolfeature tuples produced after the previous probing stage. We take the Top-K unique layers in π and start from the highest-ranked single layer, iteratively adding the next-ranked layer only if the addition improves performance. Feature concatenation for multi-layer probes. For chosen subset of layers = {ℓ1, . . . , ℓS} and pooling method p, we construct multi-layer feature matrix by horizontally concatenating the per-layer features defined in Eq. equation 4. Concretely, for sample we form the concatenated feature vector x(S,p) = (cid:2) r(ℓ1) i,p ; r(ℓ2) i,p ; . . . ; (5) where dp is the dimensionality of the pooled vector for pooling p. If attention summaries are included, we append the per-layer attention summaries to form the final feature vector. The assembled , . . . , x(S,p) dataset for examples is (cid:101)X (S,p) = [ x(S,p) ]. (cid:3) RSdp , (ℓS) i,p 1 5 Classifier training and selection. For each candidate feature assembly (cid:101)X (S,p), we train family of simple, interpretable classifiers to predict both the multiclass target ymulti and the binary target ybin. We select the final probing classifier by maximizing task-specific performance criterion over the candidate set and probe families. Formally, letting denote the set of classifier hyperparameterizations tested, we choose (S, p, θ) = argmax (S,p,clf) a(S,p,clf) γ , (S, p, clf) C. (6) where γ {bin, multi} denotes the classification task type (binary vs. multiclass), and θ are the learned classifier parameters for the selected configuration. In case of ties, we prefer the configuration with smaller σ (more stable performance) and with fewer layers. This search yields compact, high-performing probing classifier that uses small subset of layers identified by the probing stage, which is tuned for robust generalization via cross-validation and grid search. It can serve as an efficient surrogate evaluator approximating Mlarge judgments while remaining orders of magnitude cheaper at inference time."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "Datasets. To prove the effectiveness on reasoning evaluation tasks, we picked three popular benchmarks in Mathematics & Science Tasks: GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), and GPQA (Rein et al., 2024). Datasets statistics are shown in Appendix C. In our experiments, we select Llama-3-8B-Instruct (Dubey et al., 2024) as Mmed for reModels. sponse generation, as it achieves reasonable performance on the chosen benchmarks while producing diverse evaluation scores. For LLM-based evaluation annotation, we employ DeepSeek-V3 (Liu et al., 2024) as Mlarge owing to its strong reasoning ability and relatively low cost. To validate the effectiveness of our pipeline across different small LMs, we consider Qwen3-0.6B (Yang et al., 2025), Qwen3-1.7B (Yang et al., 2025), Llama-3.2-1B-Instruct (Dubey et al., 2024), and Llama-3.18B-Instruct (Dubey et al., 2024) as Msmall. Baselines. To the best of our knowledge, this is the first study to investigate LLM probing for evaluation tasks, and therefore related work is limited. Accordingly, we compare our approach against three baselines: (1) direct prompt-based inference on small LMs, (2) fine-tuning small LMs (Qwen3-0.6B), and (3) RoBERTa (Liu et al., 2019) on probing datasets to fit the evaluation scores. Implementation Details. We set the score threshold τ = 4, labeling samples with scores 4 as high-quality and those with scores < 4 as low-quality. For dimensionality reduction, we apply PCA with = 50 in the probing process. We use = 5 for Top-K aggregation and experiment with variety of classifiers, including Logistic Regression, Random Forests, small Multilayer Perceptrons (MLPs), and linear Support Vector Machines (SVMs). Since all pooling, layer, and classifier variants operate on cached hidden representations, exploring these configurations incurs negligible computational overhead. All test results are reported on the zero-shot prompt and weighted average F1 score. Additional implementations are provided in Appendix D."
        },
        {
            "title": "4.2 PROBING CLASSIFIERS RESULTS",
            "content": "We present the main results of our optimal probing classifiers across Mathematics & Science benchmarks in Figure 3. Explicit statistics for the probing datasets and the numbers of main results can be found in Appendix E, F. Notably, our strong results are obtained by only training on small probing datasets (typically fewer than 100 samples per score) due to the downsampling strategy (3.1). Probing is much more effective than prompt-based inference. Figure 3 shows substantial improvements of our methods over baselines, with average F1 scores increasing by more than 20% on most tasks. This suggests that poor outputs from LLMs do not necessarily imply that the models lack the underlying knowledge to solve the task. Instead, crucial information may be 6 Figure 3: Weighted average F1 score (%) across reasoning benchmarks with multiclass classification and binary classification tasks. Our probing method (colored bars with hatch marks) significantly outperforms prompting on the same models (colored bars without hatch marks) across all tasks. already embedded in the internal representations, but remains hidden after the final decoding process. Probing allows us to uncover and leverage these latent understandings, whereas direct text generation can introduce noise and degrade human-interpretable performance. Importantly, the effectiveness of our approach is consistently observed across all evaluation aspects and for different sizes of Msmall across multiple model families, including results after fine-tuning. This provides strong indication that our method can be applied to more general scenarios. Larger LLMs do not necessarily provide stronger evaluation, either through inference or probing. Although Llama-3.1-8B-Instruct achieves the best results on several tasks, Qwen3-1.7B still outperforms it on certain benchmarks despite being much smaller. Within the same model family, both the Qwen3 and Llama3 series demonstrate that larger models do not consistently surpass their smaller counterparts across all aspects. For instance, on MATH, Qwen3-0.6B outperforms Qwen31.7B in prompt-based inference for logicality (18.18% vs. 15.06%), and Llama-3.2-1B-Instruct surpasses Llama-3.1-8B-Instruct in binary probing for fluency (96.32% vs. 92.65%). These results highlight that different models can exhibit distinct strengths across evaluation aspects, cautioning against blind reliance on scaling laws. Probing classifiers for binary classification serve as highly reliable data filters. While the performance of probing classifiers on multiclass prediction remains modest (approximately 5060%), this is expected given the difficulty of approximating Mlarge that is hundreds of times larger with Msmall. In contrast, binary classification performance reaches 8090%, making it sufficiently reliable to function as reference-free coarse filtering mechanism. This capability is particularly valuable for common NLP applications such as curating high-quality data for supervised fine-tuning, where the filter can efficiently separate high and low quality samples during initial screening and thereby reduce the cost of further fine-grained annotation."
        },
        {
            "title": "5.1 ABLATION STUDY OF POOLING AND CLASSIFIER METHODS",
            "content": "For each probing result in Figure 3, we dont report the combinations of various pooling methods (r(ℓ) in Equation 2) and classifiers (clf in Equation 6). To better illustrate the effect of different pooling and classifier choices, we conduct an ablation study on the Informativeness aspect of the 7 MATH dataset, using Qwen3-0.6B and Llama-3.2-1B-Instruct on binary classification, with results shown in Figure 4. The results demonstrate that mean pooling consistently outperforms other strategies, which is intuitive since averaging preserves critical information while producing compact feature representations. Among classifiers, Logistic Regression achieves the best results. With limited data and potentially noisy labels from LLMs scoring, calibrated probabilities and regularization from Logistic Regression can lead to better overall F1 score. These patterns match findings from recent studies: Kantamneni et al. (2025) show that complex probes often do not strongly outperform simpler pooling + linear classifiers; Lee et al. (2023) demonstrates that averaging over all token vectors can beat using only CLS or last token embeddings. Finally, these findings are consistent with the detailed results shown in Table 11, where top-performing configurations most often involve mean pooling combined with Logistic Regression classifiers. Figure 4: Ablation study of different pooling and classifier methods on binary classification tasks. Left: logistic regression fixed. Right: mean pooling fixed."
        },
        {
            "title": "5.2 DATA FILTERING AND SUPERVISED FINE-TUNING (SFT)",
            "content": "To assess the effectiveness of our data filtering approach, we apply the trained probing classifiers of Qwen3-1.7B in knowledge distillation setup, with Llama-3-8B-Instruct (Dubey et al., 2024) as the teacher and Llama-2-7B-Chat (Touvron et al., 2023) as the student. For each benchmark, the classifiers assign binary scores (0 or 1) to every response across five aspects, which are then summed to yield total score between 0 and 5. Responses are ranked by this score from high to low to construct the training set, and the student is trained on progressively larger subsets in 10% increments. We directly compare our probing-based filtering against two baselines: (i) the gold DeepSeek-V3 filtering and (ii) random filtering. The result curves are summarized in Figure 5. Figure 5: Llama-2-7B-Chat SFT performance under different data filtering methods (probing classifier, DeepSeek-V3, and random) with incremental training subsets. Our findings can be summarized as follows: (1) filtering training data with our probing classifiers yields SFT performance comparable to using powerful LLM as the filter, indicating that our approach can approximate LLM-level data quality judgments; (2) both probing and DeepSeek-V3 consistently outperform random filtering, despite occasional mid-range fluctuations where random 8 sampling may incidentally capture higher-quality data, reinforcing the importance of quality-aware data selection for downstream training; and (3) the observed updownup trend suggests that initial gains derive from training on high-quality data, performance then declines as lower-quality data is introduced, and subsequently recovers once the volume of training data becomes sufficiently large. This supports claims in prior studies (Sajith & Kathala, 2024; Iyer et al., 2024): data quality plays the primary role in low-resource settings, whereas data quantity may become dominant factor as training data scales."
        },
        {
            "title": "6.1 EVALUATIVE SIGNALS IN INTERMEDIATE REPRESENTATIONS",
            "content": "To better understand why small models can support accurate evaluation, we analyze the internal representations used in our probing classifiers (4). Specifically, we examine how evaluative signals vary across layers and what types of features most effectively capture them. We present representative results in Figure 6, with additional analysis provided in Appendix I. (1) Representations Encode Strong Evaluative Signals. Across reasoning datasets, we observe that hidden representations show substantial correlation with evaluation scores from the strong LLM judge. These signals are especially strong in mid-to-upper layers, indicating that evaluative information is embedded throughout intermediate layers, rather than being restricted to the output stage. (2) Evaluative Features Reside in Structured Feature Subspaces. Probing feature spaces derived from PCA-based subspaces often reveals evaluative signals more effectively than scalar or attentionderived features. These observations suggest that evaluative signals are present across structured feature subspaces, with PCA-based projections revealing them more clearly than simpler features. Figure 6: Layer-wise probing accuracy for Factuality (left) and Semantic Consistency (right) using Qwen3-1.7B on the MATH dataset. PCA features perform best, peaking in upper layers."
        },
        {
            "title": "6.2 THE SEMANTIC CAPACITY ASYMMETRY HYPOTHESIS",
            "content": "These patterns observed above support the hypothesis that accurate evaluation requires much less capacity than generation and can rely on intermediate representations. This reflects an intrinsic asymmetry between the tasks: generation involves discourse planning and long dependencies that demand substantial capacity and often require full decoding, while evaluation focuses on identifying inconsistencies or content errors, which are already accessible in intermediate model states. Prior work has shown that internal representations often encode much richer and more reliable semantic information than what is reflected in surface outputs. Probing studies demonstrate that hidden states capture fine-grained linguistic and semantic structure (Waldis et al., 2024; Rogers et al., 2020). Complementing this, latent-knowledge analyses reveal that task-relevant information can remain present in intermediate representations even when generated text is unreliable or intentionally misled (Kadavath et al., 2022; Burns et al., 2023; Mallen et al., 2023). Building on these insights, we show that intermediate representations not only encode general semantic structure but also contain evaluation-relevant signals that can be directly leveraged for effective reference-free assessment."
        },
        {
            "title": "7 CONCLUSION",
            "content": "Despite suboptimal generation, small LMs retain strong evaluative signals in their internal representations. We validate the Semantic Capacity Asymmetry Hypothesis and introduce INSPECTOR, probing-based pipeline that extracts high-fidelity judgments from these latents. Experiments on various reasoning benchmarks show the capability of our method, suggesting that Representationas-a-Judge can serve as scalable and interpretable solution for evaluation and data curation."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "All datasets and models used in our experiments are publicly accessible. We also provide additional details in the Appendix, including dataset statistics, model parameters, and training hyperparameters. We believe that the information provided is sufficient to reproduce our methods and results. Furthermore, we will release all data and code in public repository upon acceptance of the paper. USE OF LARGE LANGUAGE MODELS (LLMS) We used large language models (LLMs) only for minor text polishing, such as grammar and phrasing. All ideas, experiments, analyses, and discussions were conducted solely by the authors. The LLM did not contribute to the design and interpretation of our research."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. survey on data selection for language models. arXiv preprint arXiv:2402.16827, 2024. Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in language models without supervision. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=ETKGuby0hcs. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. survey on evaluation of large language models. ACM transactions on intelligent systems and technology, 15(3):145, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dario Di Palma, Alessandro De Bellis, Giovanni Servedio, Vito Walter Anelli, Fedelucio Narducci, and Tommaso Di Noia. Llamas have feelings too: Unveiling sentiment and emotion representations in llama models through probing. arXiv preprint arXiv:2505.16491, 2025. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Yann Dubois, Balazs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam FazelZarandi, and Asli Celikyilmaz. Roscoe: suite of metrics for scoring step-by-step reasoning. arXiv preprint arXiv:2212.07919, 2022. 10 Hangfeng He, Hongming Zhang, and Dan Roth. Socreval: Large language models with the socratic method for reference-free reasoning evaluation. arXiv preprint arXiv:2310.00074, 2023. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Vivek Iyer, Bhavitvya Malik, Pavel Stepachev, Pinzhen Chen, Barry Haddow, and Alexandra Birch. Quality or quantity? on data scale and diversity in adapting large language models for lowresource translation. arXiv preprint arXiv:2408.12780, 2024. Charles Jin and Martin Rinard. Emergent representations of program semantics in language models trained on programs. In Forty-first International Conference on Machine Learning, 2024. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022. Subhash Kantamneni, Joshua Engels, Senthooran Rajamanoharan, Max Tegmark, and Neel Nanda. Are sparse autoencoders useful? case study in sparse probing. arXiv preprint arXiv:2502.16681, 2025. Kihoon Lee, Gyuho Choi, and Chang Choi. Use all tokens method to improve semantic relationship learning. Expert Systems with Applications, 233:120911, 2023. Kenneth Li, Aspen Hopkins, David Bau, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. Emergent world representations: Exploring sequence model trained on synthetic task. ICLR, 2023. Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, and Tianyi Zhou. Superfiltering: Weak-to-strong data filtering for fast instruction-tuning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1425514273, August 2024a. Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 7595 7628, Mexico City, Mexico, June 2024b. Association for Computational Linguistics. URL https://aclanthology.org/2024.naacl-long.421. Zhuochun Li, Yuelyu Ji, Rui Meng, and Daqing He. Learning from committee: Reasoning distillation from mixture of teachers with peer-review. arXiv preprint arXiv:2410.03663, 2024c. Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pp. 7481, 2004. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Alex Mallen, Madeline Brumley, Julia Kharchenko, and Nora Belrose. Eliciting latent knowledge from quirky language models. arXiv preprint arXiv:2312.01037, 2023. 11 Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky. State of what art? call for multi-prompt LLM evaluation. Transactions of the Association for Computational Linguistics, 12:933949, 2024. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311318, 2002. Felipe Maia Polo, Ronald Xu, Lucas Weber, Mırian Silva, Onkar Bhardwaj, Leshem Choshen, Allysson Flavio Melo de Oliveira, Yuekai Sun, and Mikhail Yurochkin. Efficient multi-prompt evaluation of LLMs. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=jzkpwcj200. Archiki Prasad, Swarnadeep Saha, Xiang Zhou, and Mohit Bansal. Receval: Evaluating reasoning chains via correctness and informativeness. arXiv preprint arXiv:2304.10703, 2023. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Anna Rogers, Olga Kovaleva, and Anna Rumshisky. primer in BERTology: What we know about how BERT works. Transactions of the Association for Computational Linguistics, 8:842866, 2020. Aryan Sajith and Krishna Chaitanya Rao Kathala. Is training data quality or quantity more impactful to small language model performance? arXiv preprint arXiv:2411.15821, 2024. Xing Shi, Inkit Padhi, and Kevin Knight. Does string-based neural mt learn source syntax? In Proceedings of the 2016 conference on empirical methods in natural language processing, pp. 15261534, 2016. Giulio Starace, Konstantinos Papakostas, Rochelle Choenni, Apostolos Panagiotopoulos, Matteo Rosati, Alina Leidinger, and Ekaterina Shutova. Probing llms for joint encoding of linguistic categories. arXiv preprint arXiv:2310.18696, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Anton Voronov, Lena Wolf, and Max Ryabinin. Mind your format: Towards consistent evaluation of in-context learning improvements. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 62876310, August 2024. Andreas Waldis, Yotam Perlitz, Leshem Choshen, Yufang Hou, and Iryna Gurevych. Holmes Σ benchmark to assess the linguistic competence of language models. Transactions of the Association for Computational Linguistics, 12:16161647, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. survey on knowledge distillation of large language models. arXiv preprint arXiv:2402.13116, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Weizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore: Evaluating generated text as text generation. Advances in neural information processing systems, 34:2726327277, 2021. 12 Qiyuan Zhang, Yufei Wang, Tiezheng Yu, Yuxin Jiang, Chuhan Wu, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, et al. Reviseval: Improving llm-as-a-judge via responseadapted references. arXiv preprint arXiv:2410.05193, 2024. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019. Yong Zhang, Yanwen Huang, Ning Cheng, Yang Guo, Yun Zhu, Yanmeng Wang, Shaojun Wang, and Jing Xiao. Sentinel: Attention probing of proxy models for llm context compression with an understanding perspective, 2025. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023."
        },
        {
            "title": "A LIMITATIONS",
            "content": "While the proposed method demonstrates competitive performance compared to other baselines, we acknowledge that there are still potential limitations: Following prior work, we adopt five evaluation aspects and construct corresponding prompt templates for our experiments. However, in the absence of standardized evaluation criteria, our chosen definitions may not represent the optimal formulation for all tasks. Furthermore, we observe that the difficulty of the selected evaluation aspects varies. For instance, fluency appears to be the easiest, as the majority of responses consistently receive high scores relative to other aspects. This suggests the need for further investigation into the design and selection of evaluation aspects. Although we conducted experiments on various datasets, future work can explore more general reasoning fields, such as commonsense and code generation tasks. We only use the DeepSeek-V3 as our rating model, due to its cost and availability. However, this may cause evaluation bias and affect our downstream probing classifier training process. Exploring diverse rating LLMs from different organizations could be valuable direction for future research."
        },
        {
            "title": "B SMALL LMS PROBING DETAILS",
            "content": "Notation and extraction. For sample i, we input prompt Ik(xi, yi) and run Msmall in evaluation mode, obtaining H(ℓ) RSid, A(ℓ) RRSiSi, for layers ℓ = 1, . . . , L. Here Si is the token length, the hidden dimension, and the number of attention heads. Pooling and representation. From H(ℓ) These pooling variants capture complementary token-level and global signals across layers: we compute small but expressive set of pooled vectors. r(ℓ) i,mean = 1 Si Si(cid:88) H(ℓ) [t, :] Rd, i,last = H(ℓ) r(ℓ) r(ℓ) i,min = min i,concat = (cid:2)r(ℓ) r(ℓ) t=1 , :] Rd, [t H(ℓ) [t, :], i,min; r(ℓ) i,max; r(ℓ) i,mean = max{t : token non-pad}, r(ℓ) i,max = max (cid:3) R3d. H(ℓ) [t, :], Attention and statistical features. For each head of layer ℓ we compute an attention-entropy statistic (with small constant ϵ > 0 for numerical stability): e(ℓ) i,h ="
        },
        {
            "title": "1\nSi",
            "content": "Si(cid:88) Si(cid:88) s=1 t=1 i,h,s,t log (cid:0)A(ℓ) A(ℓ) i,h,s,t + ϵ(cid:1). We compress head-wise entropies into low-dimensional summaries per layer: µ(ℓ) ="
        },
        {
            "title": "1\nR",
            "content": "(cid:88) e(ℓ) i,h, σ(ℓ) = stdh (cid:0)e(ℓ) i,h (cid:1), max e(ℓ) i,h. For each pooled vector we also compute compact statistics: (cid:88) var(r) = Var(r), E(r) = norm(r) = r2, softmax(r)m log softmax(r)m, Feature assembly. For each layer ℓ and pooling type {mean,last,min,max,concat} we assemble candidate feature matrices: (ℓ,p) = (cid:2) PCAd(r(ℓ) i,p) (cid:12) (cid:12) [norm, var, E()] (cid:123)(cid:122) (cid:125) statistics where PCAd denotes an optional dimensionality-reduction to components. All transforms that depend on the data (imputer, PCA, scaler) are applied inside cross-validation pipeline to avoid information leakage. , max i=1 (cid:124) (cid:12) (cid:12) [µ(ℓ) , σ(ℓ) i,h] (cid:3)N e(ℓ) Probing and layer ranking. We treat gold labels si,k {1, . . . , 5} from Dprob as both (i) i,k = I[ si,k τ ] with threshold τ multiclass prediction target ymulti i,k = si,k and (ii) binary target ybin (high vs. low quality). For each (ℓ,p) we fit logistic probe (linear logistic regression; one-vs-rest for multiclass) and evaluate generalization with stratified cross-validation. Each candidate probe yields performance tuple , σ(ℓ,p) bin (cid:0)a(ℓ,p) bin multi , σ(ℓ,p) where a(ℓ,p) multi denote the mean accuracies of the binary and multiclass probes, respectively, bin and σ(ℓ,p) their corresponding standard deviations across cross-validation folds. We rank layerpoolfeature configurations by chosen criterion (binary or multiclass accuracy mean) for different downstream tasks. and a(ℓ,p) , σ(ℓ,p) multi , a(ℓ,p) multi bin (cid:1)"
        },
        {
            "title": "C DATASETS STATISTICS",
            "content": "We download datasets GSM8K and GPQA from Huggingface, MATH from their official project website: https://github.com/hendrycks/math. GSM8K dataset is split according to the official original split ratio. We use the official training set for Math and MATH-500 for the test set due to its high representation and low cost. Since there is no official train/test split for GPQA, we use gpqa main and gpqa extended as the training set, gpqa diamond as the test set. Table 1 shows the statistics of all datasets."
        },
        {
            "title": "Type",
            "content": "#Train #Test GSM8K Mathematics MATH Mathematics GPQA"
        },
        {
            "title": "Science",
            "content": "7473 7500 994 1319 500 198 Table 1: Dataset statistics."
        },
        {
            "title": "D IMPLEMENTATION DETAILS",
            "content": "Primary experiments are conducted on eight NVIDIA Quadro RTX 8000 and eight NVIDIA RTX A6000 GPUs. 14 D.1 SMALL LMS PARAMETERS The parameter settings for Msmall: Qwen3-0.6B (Yang et al., 2025), Qwen3-1.7B (Yang et al., 2025), Llama-3.2-1B-Instruct (Dubey et al., 2024), and Llama-3.1-8B-Instruct (Dubey et al., 2024)."
        },
        {
            "title": "Parameter",
            "content": "temperature max new tokens do sample output hidden states output attentions torch dtype attn implementation"
        },
        {
            "title": "Value",
            "content": "0.6 256 True True True float16 eager Table 2: Small LMs parameter settings. D.2 MEDIUM LMS PARAMETERS The parameter settings for Mmed: Llama-3-8B-Instruct (Dubey et al., 2024)."
        },
        {
            "title": "Parameter",
            "content": "temperature max new tokens do sample torch dtype top p"
        },
        {
            "title": "Value",
            "content": "0 512 True float16 1.0 Table 3: Medium LM parameter settings. D.3 LARGE LMS PARAMETERS DeepSeek-V3 (Liu et al., 2024) is required by the official API: https://api-docs. deepseek.com."
        },
        {
            "title": "Parameter",
            "content": "temperature max new tokens do sample model response format"
        },
        {
            "title": "Value",
            "content": "0 2048 True deepseek-chat {type: json object} Table 4: Large LM parameter settings. D.4 MODEL TUNING HYPERPARAMETER We list the training hyperparameters for Llama-2-7B-Chat (Touvron et al., 2023) in Section 6.4. D.5 PROBING CLASSIFIER DETAILS We train several probing classifiers on internal features to fit gold scores. All classifiers are implemented in scikit-learn pipelines with StandardScaler for feature normalization. We perform hyperparameter search via 5-fold cross-validation using macro F1 as the scoring metric. Table 6 below summarizes each classifiers fixed parameters and the hyperparameter ranges considered in the grid search."
        },
        {
            "title": "Value",
            "content": "epoch batch size learning rate warmup steps max seq length gradient accumulation steps lora lora alpha lora dropout target modules 8 8 1e-4 100 1024 4 16 32 0.1 [\"q proj\", \"v proj\"] Table 5: Student LM training hyperparameter settings."
        },
        {
            "title": "Random Forest",
            "content": "Fixed Parameters max iter=2000, class weight=\"balanced\", solver=\"lbfgs\" class weight=\"balanced\", random state=42 Multi-layer Perceptron"
        },
        {
            "title": "Linear SVM",
            "content": "hidden layer sizes=(256,128), activation=\"relu\", alpha=1e-4, learning rate init=1e-3, max iter=1000, early stopping=True, random state=42 kernel=\"linear\", class weight=\"balanced\", probability=True, random state=42 Hyperparameter Grid C: penalty: [l2] [0.001,0.01,0.1,1]; estimators: [100,300,500]; max depth: [None,10,20]; min samples leaf: [1,2,5] alpha: learning rate init: [1e-4,1e-3,1e-2]; hidden layer sizes: [(200,100),(100,),(200,100,50)] [1e-4,1e-3,1e-2]; C: [0.001,0.01,0.1,1,10,100] Table 6: Probing classifiers including fixed parameters and grid search ranges for hyperparameter tuning."
        },
        {
            "title": "E PROBING DATASETS STATISTICS",
            "content": "As explained in Section 4.1, we first determine the minimum number of samples among the five score levels (15), and then randomly downsample the other levels to this size. This ensures that the multiclass probing tasks are balanced across labels and not dominated by frequent scores. Thus, after downsampling, Dprob contains total of 5 samples, and we split the dataset into training and test sets with an 80:20 ratio. In the following tables, we mark the minimum number of each aspect in bold. These score distributions can reflect the difficulty evaluation levels among various dataset questions and aspects. Dprob statistics for GSM8K: Dprob statistics for MATH: Dprob statistics for GPQA: 16 #score= #score=2 #score=3 #score=4 #score="
        },
        {
            "title": "Semantic Consistency\nLogicality\nInformativeness\nFluency\nFactuality",
            "content": "184 118 46 17 377 491 842 81 23 300 369 174 111 70 479 34 26 49 86 85 6435 6353 7226 7317 6272 Table 7: Number of LLM-judge scores across five aspects of probing datasets on GSM8K. #score=1 #score=2 #score=3 #score=4 #score="
        },
        {
            "title": "Semantic Consistency\nLogicality\nInformativeness\nFluency\nFactuality",
            "content": "441 224 75 132 609 467 1288 180 27 945 1879 1604 748 746 1818 121 98 2062 2857 34 4575 4269 4418 3721 4077 Table 8: Number of LLM-judge scores across five aspects of probing datasets on MATH. #score=1 #score=2 #score=3 #score=4 #score="
        },
        {
            "title": "Semantic Consistency\nLogicality\nInformativeness\nFluency\nFactuality",
            "content": "257 267 197 135 306 529 463 164 26 475 60 145 327 68 119 22 31 163 210 46 125 87 142 554 47 Table 9: Number of LLM-judge scores across five aspects of probing datasets on GPQA."
        },
        {
            "title": "F DETAILED MAIN RESULTS",
            "content": "We show the specific numbers of results in Table 10. Method Semantic Consistency Informativeness GSM8K MATH GPQA GSM8K MATH GPQA GSM8K MATH GPQA GSM8K MATH GPQA GSM8K MATH GPQA Factuality Logicality Fluency 28.34 7.03 24.21 16.38 40.92 RoBERTa (Liu et al., 2019) Qwen3-0.6B (Yang et al., 2025) Prompt Inference Tuning Probing Classifier Llama-3.2-1B-Instruct (Dubey et al., 2024) Prompt Inference Probing Classifier Qwen3-1.7B (Yang et al., 2025) Prompt Inference Probing Classifier Llama-3.1-8B-Instruct (Dubey et al., 2024) Prompt Inference Probing Classifier 6.33 38. 14.72 39.31 16.18 42.98 19.33 24.76 47.73 17.40 48.47 21.98 58.45 27.97 51. 45.40 43.57 42.71 42.71 79.28 RoBERTa (Liu et al., 2019) Qwen3-0.6B (Yang et al., 2025) Prompt Inference Tuning Probing Classifier Llama-3.2-1B-Instruct (Dubey et al., 2024) Prompt Inference Probing Classifier Qwen3-1.7B (Yang et al., 2025) Prompt Inference Probing Classifier Llama-3.1-8B-Instruct (Dubey et al., 2024) Prompt Inference Probing Classifier 38.95 76.13 34.02 76. 43.57 82.35 51.13 45.40 76.07 43.48 68.07 53.88 79.21 58.01 75.11 5. 8.65 23.92 16.00 40.48 11.82 40.69 17.42 40.93 20.78 36.41 17.46 16.64 51. 9.55 47.16 25.00 49.86 12.75 50.00 Multiclass Classification (score=1-5) 11.65 26.18 17.18 6. 28.14 8.96 8.08 8.65 6.73 7. 8.50 18.18 17.68 52.00 13.32 53.47 15.06 59.05 26.04 59.63 22.12 29.03 64. 11.73 55.81 21.81 64.79 25.34 58.42 14.62 14.63 45.18 23.54 42.33 13.88 46. 30.20 43.85 15.09 16.01 61.07 12.34 51.27 18.34 51.42 37.21 61.78 14.53 28.14 57. 12.12 60.34 17.27 59.79 27.35 62.28 14.66 18.18 51.46 17.11 53.29 22.27 47. 19.33 42.56 20.47 21.91 60.97 17.52 55.16 21.45 60.97 48.94 56.15 10.10 19.84 54. 8.45 50.13 9.77 45.02 12.69 45.36 24.50 29.35 43.80 11.76 43.29 30.46 48. 14.07 44.06 14.65 15.23 55.18 10.74 47.80 18.40 58.34 9.63 63.94 15.96 15.16 50. 12.83 49.21 5.66 54.37 22.53 49.56 43.90 46.89 45. 46.58 46.06 45.00 44.83 43.57 44. 46.89 45.00 43.57 46.06 Binary-Classification (high vs low quality) 40.27 45.45 72. 35.12 63.64 34.66 68.38 45.10 77.42 57.22 65.85 68.43 43.08 65.00 61.54 72. 29.22 68.43 51.99 45.25 78.80 45.45 78.79 51.34 83.67 64.80 79.77 49.93 64.75 71. 74.35 80.85 63.28 74.47 58.81 80.65 49.71 38.98 65.48 42.79 72.08 38.94 72. 42.32 69.91 59.74 45.00 84.16 41.03 86.80 56.28 85.48 86.67 88.12 48.21 44.83 78. 41.31 76.93 50.26 80.96 65.63 80.41 35.29 47.43 82.35 37.65 82.35 41.58 88. 69.56 76.63 56.08 50.73 92.65 22.93 96.32 50.73 92.65 85.27 92.65 61.95 53.85 92. 52.75 87.94 59.17 96.11 84.80 88.33 57.41 45.00 73.80 44.44 71.42 69.85 81. 38.02 70.51 57.26 57.26 70.16 46.28 73.36 58.22 73.36 52.26 76.47 55.99 63.21 76. 71.17 80.52 49.90 87.11 58.12 74.21 Table 10: Average F1 score (%) across various reasoning benchmarks with multiclass classification and binary classification tasks. The best performance among different classification tasks in each benchmark is marked in bold. We also report the specific layers, pooling, and classifier methods that achieve the best probing performance, as summarized in Table 10, across different benchmarks in Table 11. GSM8K MATH Multiclass Classification (score=1-5)"
        },
        {
            "title": "GPQA",
            "content": "Semantic Consistency Qwen3-1.7B, layers=[14,25],"
        },
        {
            "title": "Factuality",
            "content": "layers=[17], layers=[16,17], pool=mean, LR Qwen3-0.6B, pool=mean, LR Qwen3-1.7B, pool=mean, LS Llama-3.2-1B-Instruct, ers=[1], pool=last, LS Qwen3-1.7B, pool=mean, LR layers=[14,18], laylaylayers=[15], Qwen3-1.7B, pool=mean, LS Llama-3.1-8B-Instruct, ers=[17], pool=last, LR Llama-3.1-8B-Instruct, ers=[10], pool=mean, LS Qwen3-1.7B, pool=mean, LS Llama-3.1-8B-Instruct, ers=[21], pool=mean, LR laylaylaylayers=[18], layers=[2], layers=[13], Qwen3-1.7B, pool=mean, LS Qwen3-1.7B, pool=mean, LS Llama-3.1-8B-Instruct, ers=[12], pool=mean, LS Qwen3-0.6B, pool=last, LR Qwen3-1.7B, pool=last, LR layers=[18], layers=[15], layBinary-Classification (high vs low quality)"
        },
        {
            "title": "Informativeness",
            "content": "Semantic Consistency Llama-3.2-1B-Instruct, ers=[3], pool=last, LS Qwen3-1.7B, pool=last, LR Qwen3-1.7B, pool=mean, LS Qwen3-1.7B, pool=last, LS Qwen3-1.7B, pool=mean, LR"
        },
        {
            "title": "Fluency",
            "content": "layers=[24], layers=[21], layers=[1,11], layers=[16], layers=[15], layers=[18], Qwen3-1.7B, pool=mean, LR Qwen3-1.7B, pool=last, LR Llama-3.1-8B-Instruct, ers=[20], pool=last, LS Llama-3.2-1B-Instruct, ers=[13], pool=last, LR Llama-3.1-8B-Instruct, ers=[28], pool=mean, LR laylaylaylay- layLlama-3.1-8B-Instruct, ers=[16], pool=last, LR Llama-3.2-1B-Instruct, ers=[3], pool=last, LR Qwen3-1.7B, pool=mean, LR Qwen3-1.7B, pool=mean, LR Qwen3-1.7B, pool=last, LS layers=[17], layers=[14], layers=[18], Table 11: Detailed layers, pooling and classifiers selection for best probing performances. pool denotes different pooling methods in Equation 2. LR denotes Logistic Regression and LS denotes linear SVM. OUT-OF-DISTRIBUTION (OOD) PROBING To evaluate the generalization abilities of our probing methods on Out-of-Distribution (OOD) data, we conducted experiments using one mathematical reasoning dataset as the training set and another dataset as the test set. Table 12 highlights the probing performance with Qwen3-1.7B as the Msmall in OOD scenarios. Our analysis reveals two complementary patterns. First, multiclass (15) probing demonstrates limited transferability, with F1 scores dropping to approximately 1025%, suggesting that fine-grained score prediction is dataset-specific. In particular, transferring from more challenging datasets (MATH) to simpler ones (GSM8K) proves more difficult than the reverse. Second, binary probing demonstrates substantially greater robustness under distribution shift: out-ofdistribution F1 scores range from 3562%, comparable to in-distribution results in Table 10. These findings suggest that PCA-based linear probing reliably captures coarse, domain-general quality signals, whereas fine-grained distinctions are too difficult to transfer. Consequently, for claims of OOD robustness, we emphasize binary evaluations and recommend controlling probe capacity when reporting cross-dataset transfer."
        },
        {
            "title": "Method",
            "content": "Semantic Consistency GSM8K"
        },
        {
            "title": "Factuality",
            "content": "GSM8K MATH GSM8K MATH GSM8K MATH GSM8K MATH Multiclass Classification (score=1-5) Qwen3-1.7B Probing Classifier Qwen3-1.7B Probing Classifier 10.31 14.26 10."
        },
        {
            "title": "11.65\nBinary-Classification (high vs low quality)",
            "content": "16.47 23.50 13.35 11.60 16.63 27. 35.76 61.04 27.20 62.90 60.39 45. 41.58 23.59 36.70 62.19 Table 12: Average F1 score (%) of test results across mathematics reasoning benchmarks in Outof-Distribution (OOD) scenarios. Specifically, we conducted experiments by training on GSM8K and testing on MATH, as well as training on MATH and testing on GSM8K. EXTENDED RESULTS ON OPEN-ENDED BENCHMARK While our primary focus is on reasoning evaluation, we additionally evaluated our approach on the open-ended generation benchmark AlpacaEval 2.0 (Dubois et al., 2024), using the same experimental settings as the three reasoning benchmarks in Table 10. The results of AlpacaEval 2.0 are shown in Table 13. The results further supports the effectiveness and robustness of our approach on broader range of domains and tasks, including reference-free open-ended benchmarks."
        },
        {
            "title": "Factuality",
            "content": "Multiclass Classification (score=1-5) 7.56 RoBERTa (Liu et al., 2019) Qwen3-0.6B (Yang et al., 2025) 22.40 Prompt Inference 14.29 Tuning Probing Classifier 36.84 Llama-3.2-1B-Instruct (Dubey et al., 2024) 12.50 Prompt Inference Probing Classifier 35.71 Qwen3-1.7B (Yang et al., 2025) 7.14 Prompt Inference Probing Classifier 35.12 Llama-3.1-8B-Instruct (Dubey et al., 2024) 14.92 Prompt Inference Probing Classifier 34.69 8.08 11.11 11.11 42.59 13.33 55. 20.11 45.93 17.99 53.70 8.32 16.94 9.93 46.71 8.37 46.08 22.70 42. 18.34 45.90 Binary-Classification (high vs low quality) 41.56 RoBERTa (Liu et al., 2019) Qwen3-0.6B (Yang et al., 2025) Prompt Inference 55.24 Tuning 35.24 61.42 Probing Classifier Llama-3.2-1B-Instruct (Dubey et al., 2024) 49.20 Prompt Inference Probing Classifier 67.14 Qwen3-1.7B (Yang et al., 2025) 50.24 Prompt Inference 67.14 Probing Classifier Llama-3.1-8B-Instruct (Dubey et al., 2024) 39.77 Prompt Inference 70.16 Probing Classifier 39.68 43.00 43.00 55. 34.19 70.42 65.80 75.93 48.15 77.78 46.58 49.93 45.41 62.76 59.26 67. 45.03 70.71 58.34 69.96 11.69 11.82 10.61 61.21 16.97 61.21 25.19 65. 23.38 57.32 49.49 46.36 28.48 91.06 64.24 81.82 45.45 91.06 62.42 82. 8.08 8.89 12.70 23.33 0.20 28.57 20.00 23.70 14.29 22.96 39. 44.44 27.78 55.56 39.68 65.80 55.56 77.78 27.35 64.07 Table 13: Average F1 score (%) on AlpacaEval 2.0 with multiclass classification and binary classification tasks."
        },
        {
            "title": "I ADDITIONAL ANALYSIS OF EVALUATIVE SIGNALS",
            "content": "To complement the main findings in Figure 6, we include detailed layer-wise analyses for additional evaluation dimensions: Factuality, Informativeness, Logicality, and Semantic Consistency. These results are based on Qwen3-1.7B evaluated on the MATH dataset. Each figure reports probing accuracy across layers using three feature types: PCA-projected embeddings, statistical summaries, and attention-derived vectors. Figure 7: Layer-wise diagnostic results for Informativeness. Peak correlation appears near Layer 25. PCA accuracy increases steadily, while stats features decline in upper layers. 19 Figure 8: Layer-wise diagnostic results for Logicality. Signals concentrate around Layers 17 and 27. PCA features show rising accuracy toward the upper layers. Figure 9: Layer-wise diagnostic results for Fluency. Evaluative signals show local peaks at Layer 17."
        },
        {
            "title": "J CASE ANALYSIS",
            "content": "This case compares three evaluators scoring the same model-generated response: strong LLM judge (DeepSeek-V3), probing-based judge that accesses intermediate representations of Qwen31.7B, and prompt-based judge that relies on the same models generated evaluation text. Consistent with our main hypothesis, the probing-based scores closely match the strong judge, indicating that the semantic and factual signals required for accurate evaluation are already encoded in the small models latent states. In contrast, the prompt-based evaluation from the same small model diverges: although its numeric scores remain plausible, the generated justifications introduce spurious concerns, overinterpret minor details, or provide unreliable reasoning. These deviations reflect the higher capacity demands of generating coherent evaluation explanation. The pattern illustrates core capacity asymmetryevaluation depends on localized semantic cues accessible from intermediate representations, whereas producing well-formed meta-evaluative text requires long-range structuring and discourse planning that small models struggle to express reliably. GSM8K EXAMPLE Question: Keaton climbed 30 feet ladder twenty times while working at the construction site. Reece, also working at the same site, climbed ladder 4 feet shorter than Keatons ladder 15 times. Whats the total length of the ladders that both workers climbed in inches? 20 Gold Answer: Model Prediction: Lets break it down step by step. Keaton climbed 30 feet ladder 20 times. To find the total length of the ladder he climbed, we need to multiply the height of the ladder by the number of times he climbed it: 30 feet/ladder 20 times = 600 feet Since there are 12 inches in 1 foot, we can convert the total length from feet to inches: 600 feet 12 inches/foot = 7200 inches Now, lets calculate the total length of the ladder Reece climbed. Reece climbed ladder that was 4 feet shorter than Keatons ladder, which means Reeces ladder was: 30 feet - 4 feet = 26 feet Reece climbed the ladder 15 times, so the total length of the ladder he climbed is: 26 feet/ladder 15 times = 390 feet Converting this to inches: 390 feet 12 inches/foot = 4680 inches Now, lets add the total length of the ladders climbed by both workers: 7200 inches (Keaton) + 4680 inches (Reece) = 11,880 inches So, the total length of the ladders that both workers climbed is 11,880 inches. DEEPSEEK-V3 OUTPUT Semantic Consistency (score 5): Every step strictly follows the problem facts without inventing events, dropping givens, or adding unstated assumptions. The response correctly processes Keatons 30-foot ladder climbed 20 times, calculates Reeces ladder as 4 feet shorter (26 feet) climbed 15 times, converts both totals from feet to inches using the standard conversion (12 inches per foot), and sums the results accurately. The final answer is fully grounded in the given problem. Logicality (score 5): All inferences and arithmetic are valid; each step follows logically from prior steps. The response correctly interprets the problem, performs accurate calculations for both workers (including the conversion from feet to inches), and sums the results appropriately. Informativeness (score 5): Full, verifiable step-by-step derivation showing all essential calculations: Keatons total feet (30 20 = 600), conversion to inches (600 12 = 7200), Reeces ladder height (30 - 4 = 26), Reeces total feet (26 15 = 390), conversion to inches (390 12 = 4680), and final sum (7200 + 4680 = 11880). Anyone can re-check the answer from these clear, complete steps. Fluency (score 5): Clear, grammatical, well-punctuated, and easy to follow; uses readable notation and step-by-step presentation that flows logically from one calculation to the next. Factuality (score 5): All factual claims are correct and well-supported: accurate arithmetic (3020=600, 30-4=26, 2615=390, 60012=7200, 39012=4680, 7200+4680=11880), correct unit conversion (12 inches per foot), and proper application of the problems conditions. No errors, hallucinations, or unsupported assertions. QWEN3-1.7B PROBING EVALUATION Predicted scores: Semantic Consistency: 4 Logicality: 5 Informativeness: 5 Fluency: 5 Factuality: 21 QWEN3-1.7B PROMPT-BASED EVALUATION OUTPUT Semantic Consistency (score 3): Solution follows the problem facts. Minor consistency lapse: the rationale never states the tacit assumption that each climb is single ascent. Also, unit management is done per person instead of showing single feet-to-inches conversion. These are small omissions that do not change the numeric result, so mid-level credit is appropriate. Logicality (score 3): Arithmetic steps are correct. Logical inefficiency exists: the chain converts each person to inches separately rather than adding feet then converting once harmless but unnecessary detour. Because all inferences are valid but the chain is slightly untidy, middle score fits. Informativeness (score 3): Key calculations are present. However, the explanation omits one explicit grouped intermediate (explicitly writing total = 990 ft before conversion is not shown in every version), and some steps are shown only implicitly. Enough detail to verify the answer, but not fully exhaustive hence partial credit. Fluency (score 4): Language is clear, grammatical, and easy to follow. Sentences read naturally and numeric steps are separated. Minor repetition and slightly verbose conversion phrasing prevent perfect score, but overall readability is high. Factuality (score 2): While the final numeric answer is correct, there are factual/imprecision concerns that lower confidence: (1) the rationale implicitly assumes climb means one ascent and does not state this, which could be misread in other contexts; (2) converting per-person instead of converting total feet introduces an avoidable chance for unit-handling mistakes in variants of the problem. These represent multiple factual/imprecision issues significant enough to warrant low factuality rating. Figure: Complete evaluation outputs for the GSM8K case across three judges: DeepSeek-V3, Qwen3-1.7B (probing), and Qwen3-1.7B (prompting)."
        },
        {
            "title": "K PROMPT TEMPLATES",
            "content": "We provide the one-shot evaluation prompt templates for five aspects defined in Section 4.1 across three selected benchmarks: GSM8K, MATH, and GPQA. We use these prompts to get LLM evaluation results. Some parts of the prompt design are inspired by SOCREVAL(He et al., 2023). Its noticeable that when we use these same prompts to prob small LMs, we found the prompts are too complex for small LMs to understand, which makes it difficult to acquire accurate probing results. Thus, for probing prompts, we remove the one-shot example and ask it to directly output the score, instead of JSON output. Specifically, for the probing evaluation prompt, we remove the Example question, Example generated response, and Example representation. We also change the final output requirement to: Now evaluate the Question and Generated response above based on the instruction. Return only the score. 22 Figure 10: Evaluation prompt of Semantic Consistency on GSM8K and MATH. Figure 11: Evaluation prompt of Logicality on GSM8K and MATH. Figure 12: Evaluation prompt of Logicality on GPQA. Figure 13: Evaluation prompt of Informativeness on GSM8K and MATH. 24 Figure 14: Evaluation prompt of Informativeness on GPQA. Figure 15: Evaluation prompt of Fluency on GSM8K and MATH. Figure 16: Evaluation prompt of Fluency on GPQA. Figure 17: Evaluation prompt of Factuality on GSM8K and MATH. 26 Figure 18: Evaluation prompt of Factuality on GPQA."
        }
    ],
    "affiliations": [
        "Ping An Technology (Shenzhen) Co., Ltd.",
        "University of Connecticut",
        "University of Maryland, College Park",
        "University of Pittsburgh"
    ]
}