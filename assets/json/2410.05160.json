{
    "paper_title": "VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks",
    "authors": [
        "Ziyan Jiang",
        "Rui Meng",
        "Xinyi Yang",
        "Semih Yavuz",
        "Yingbo Zhou",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering. Recently, there has been a surge of interest in developing universal text embedding models that can generalize across tasks (e.g., MTEB). However, progress in learning universal multimodal embedding models has been relatively slow despite their importance. In this work, we aim to explore the potential for building universal embeddings capable of handling a wide range of downstream tasks. Our contributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark), which covers 4 meta-tasks (i.e. classification, visual question answering, multimodal retrieval, and visual grounding) and 36 datasets, including 20 training and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model -> Vector), a contrastive training framework that converts any state-of-the-art vision-language model into an embedding model via training on MMEB. Unlike previous models such as CLIP and BLIP, VLM2Vec can process any combination of images and text to generate a fixed-dimensional vector based on task instructions. We build a series of VLM2Vec models on Phi-3.5-V and evaluate them on MMEB's evaluation split. Our results show that VLM2Vec achieves an absolute average improvement of 10% to 20% over existing multimodal embedding models on both in-distribution and out-of-distribution datasets in MMEB."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 1 ] . [ 2 0 6 1 5 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Manuscript",
            "content": "VLM2VEC: TRAINING VISION-LANGUAGE MODELS FOR MASSIVE MULTIMODAL EMBEDDING TASKS Ziyan Jiang1, Rui Meng2, Xinyi Yang2, Semih Yavuz2, Yingbo Zhou2, Wenhu Chen1 1University of Waterloo, 2Salesforce Research ziyanjiang528@gmail.com, ruimeng@salesforce.com, wenhuchen@uwaterloo.ca https://tiger-ai-lab.github.io/VLM2Vec/"
        },
        {
            "title": "ABSTRACT",
            "content": "Embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering. Recently, there has been surge of interest in developing universal text embedding models that can generalize across tasks (e.g., MTEB). However, progress in learning universal multimodal embedding models has been relatively slow despite their importance. In this work, we aim to explore the potential for building universal embeddings capable of handling wide range of downstream tasks. Our contributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark), which covers 4 meta-tasks (i.e. classification, visual question answering, multimodal retrieval, and visual grounding) and 36 datasets, including 20 training and 16 evaluation datasets, and (2) VL M2VE (Vision-Language Model Vector), contrastive training framework that converts any state-of-the-art vision-language model into an embedding model via training on MMEB. Unlike previous models such as CLIP and BLIP, VL M2VE can process any combination of images and text to generate fixed-dimensional vector based on task instructions. We build series of VL M2VE models on Phi-3.5-V and evaluate them on MMEBs evaluation split. Our results show that VL M2VE achieves an absolute average improvement of 10% to 20% over existing multimodal embedding models on both in-distribution and out-of-distribution datasets in MMEB."
        },
        {
            "title": "INTRODUCTION",
            "content": "encode inputs (whether text or images) as Embeddings, or distributed representations, fixed-dimensional vectors, enabling range of downstream tasks. Since the advent of Word2Vec (Mikolov, 2013) and GloVe (Pennington et al., 2014), substantial research efforts have focused on learning textual embeddings (Kiros et al., 2015; Conneau et al., 2017) and image embeddings (Radford et al., 2021; Li et al., 2022; Jia et al., 2021; Yu et al., 2022). These embeddings facilitate variety of applications, including textual and visual semantic similarity (Agirre et al., 2012; Marelli et al., 2014; Chechik et al., 2010; Cer et al., 2017), information retrieval (Mitra et al., 2017; Karpukhin et al., 2020; Lin et al., 2014), automatic evaluation (Zhang et al., 2020; Sellam et al., 2020), prompt retrieval for in-context learning (Liu et al., 2022; Rubin et al., 2022; Hongjin et al., 2022), and retrieval-augmented generation (Lewis et al., 2020; Guu et al., 2020; Izacard & Grave, 2020). recent shift in research has focused on developing universal embeddings that can generalize across wide range of tasks. For instance, Muennighoff et al. (2023) introduced MTEB (Massive Text Embedding Benchmark) to comprehensively assess text embeddings across tasks such as classification and clustering. MTEB has become the standard for evaluating universal text embeddings. Recent works (Wang et al., 2022a; Su et al., 2023; Wang et al., 2024; Springer et al., 2024; BehnamGhader et al., 2024) have demonstrated promising results on the MTEB benchmark. However, progress in multimodal embeddings has been relatively slower. Despite advancements in text embeddings, the lack of both benchmarks and methodologies in the multimodal embedding domain remains challenge. Work done during an internship at University of Waterloo in collaboration with Salesforce Research. Corresponding authors are Ziyan Jiang, Rui Meng and Wenhu Chen"
        },
        {
            "title": "Manuscript",
            "content": "Figure 1: We develop universal multimodal embedding benchmark, MMEB, along with VL M2VE C, an embedding model adapted from vision-language models (VLMs). VL M2VE is capable of following instructions and performing various multimodal embedding tasks, accommodating any combination of image and text modalities. Current research in multimodal embeddings faces two primary limitations: (1) existing studies typically evaluate visual embeddings on isolated tasks, such as ImageNet classification (Deng et al., 2009; Hendrycks et al., 2021a;b) or MSCOCO/Flickr retrieval (Lin et al., 2014; Plummer et al., 2015); (2) most existing models, such as CLIP (Radford et al., 2021), BLIP (Li et al., 2022), and SigLIP (Zhai et al., 2023), either process text and images separately or perform shallow fusion of visual and textual information (Wei et al., 2023), limiting their ability to fully capture the relationships between text and image modalities. Furthermore, these models exhibit limited reasoning and generalization capabilities, particularly in zero-shot scenarios for complex reasoning tasks. In this paper, we attempt to build an universal multimodal embedding framework to pave road for the future research, which consists of two efforts: - MMEB: We introduce novel benchmark, MMEB (Massive Multimodal Embedding Benchmark), which includes 36 datasets spanning four meta-task categories: classification, visual question answering, retrieval, and visual grounding. MMEB provides comprehensive framework for training and evaluating embedding models across various combinations of text and image modalities. All tasks are reformulated as ranking tasks, where the model follows instructions, processes query, and selects the correct target from set of candidates. The query and target can be an image, text, or combination of both. MMEB is divided into 20 in-distribution datasets, which can be used for training, and 16 out-of-distribution datasets, reserved for evaluation. - VL M2VE C: We adopt the pre-trained vision-language model Phi-3.5-V (Abdin et al., 2024) as the backbone for VL M2VE C. In contrast to other multimodal embedding models like UniIR (Wei et al., 2023) and MagicLens (Zhang et al., 2024), which rely on late fusion of CLIP (Radford et al., 2021) features, our approach leverages the deep integration of vision and language features within transformer architecture. There are several advantages to this approach: (1) VLMs are trained on massive multimodal datasets and can handle any combination of images and text, as well as high-resolution images and long text inputs; (2) vision and language features are deeply fused in the transformer model, improving the models ability to capture cross-modal relationships; and (3) these models are well-suited for generalizing across diverse tasks, particularly those requiring instructionfollowing capabilities. These factors make VL M2VE an ideal choice for task generalization. We trained VL M2VE on the 20 MMEB training datasets using contrastive learning and compared its performance with various baselines. Following extensive contrastive training, VL M2VE can handle any combination of images and text, producing fixed-dimensional vectors. We evaluate VL M2VE against wide array of multimodal embedding models, including CLIP (Radford et al., 2021), BLIP2 (Li et al., 2023a), SigLIP (Zhai et al., 2023), MagicLens (Zhang et al., 2024), UniIR (Wei et al., 2023) and E5-"
        },
        {
            "title": "Manuscript",
            "content": "V (Jiang et al., 2024), demonstrating consistent improvements across all task categories. Notably, VL M2VE achieves 17.3-point improvement (from 42.8 to 60.1) across all 36 MMEB datasets and 11.6-point increase (from 40.4 to 52.0) on 16 out-of-distribution datasets in zero-shot evaluation, highlighting the effectiveness of our proposed framework. We are committed to releasing all data, training code, and models to facilitate future research in this area."
        },
        {
            "title": "2.1 DATASET OVERVIEW",
            "content": "We present MMEB (Massive Multimodal Embedding Benchmark), comprehensive benchmark designed to evaluate multimodal embeddings across diverse set of tasks. MMEB consists of 36 datasets organized into four meta-tasks: classification, visual question answering, retrieval, and visual grounding. Each task is reformulated as ranking problem, where the model is provided with an instruction and query (which may consist of text, images, or both) and is tasked with selecting the correct answer from set of candidates. These candidates could be text, images, or additional instructions. The datasets are divided into two categories: 20 in-distribution datasets for training and 16 out-of-distribution datasets for evaluation. We report performance metrics across all 36 tasks. An overview of MMEB is provided in Figure 2 and the dataset statistics are provided in Table 1. The embedding models are supposed to compress the query side into vector and the target candidates into set of vectors. The candidate with the highest dot-product will be selected as the prediction for evaluation. We measure the Precision@1 to reflect the percentage of top candidate matching the groundtruth. To ensure the task difficulty, we introduce large amount of candidates. MMEB offers wide range of tasks from various domains, such as common, news, Wikipedia, web, and fashion. The benchmark incorporates diverse combinations of modalities for both queries and targets, including text, images, and text-image pairs. Additionally, tasks are designed to follow different types of instructions. For instance, tasks may involve object recognition (e.g., Identify the object shown in the image.), retrieval (e.g., Find an image that matches the given caption.), or visual grounding (e.g., Select the portion of the image that answers the question.). Examples for each dataset in MMEB are provided in Tables 6, 7, 8 and 9. The diversity in MMEB makes it an ideal testbed for universal embeddings. 2.2 META-TASK AND DATASET DESIGN MMEB is organized into four primary meta-task categories: Classification This category comprises 5 in-distribution and 5 out-of-distribution datasets. Queries consist of instructions and images, optionally accompanied by related text. Targets are class labels, and the number of class labels corresponds to the number of classes in the dataset. Visual Question Answering This category includes 6 in-distribution and 4 out-of-distribution datasets. The query consists of an instruction, an image, and piece of text as the question, while the target is the answer. Each query has 1,000 target candidates: 1 ground truth and 999 distractors. Information Retrieval This category contains 8 in-distribution and 4 out-of-distribution datasets. Both the query and target sides can involve combination of text, images, and instructions. Similar to the VQA task, each query has 1,000 candidates, with 1 ground truth and 999 distractors. Visual Grounding This category includes 1 in-distribution and 3 out-of-distribution datasets, which are adapted from object detection tasks. Queries consist of an instruction, an image, and text referring to specific region or object within the image. The target may include cropped image of the object or text describing the same region. Each query includes 1,000 candidates: 1 ground truth and 999 distractors. These distractors may include hard negatives from the same object class, other objects in the image, or random objects from different images. This task evaluates the models ability to recognize and represent the same object or concept across different modalities (image or text) and from varying perspectives. The task involves correctly identifying or referring to specific regions or objects within an image based on the provided instructions or language expressions."
        },
        {
            "title": "Manuscript",
            "content": "Table 1: The statistics of MMEB: 36 datasets across 4 meta-task categories, with 20 in-distribution datasets used for training and 16 out-of-distribution datasets used exclusively for evaluation. Meta-Task Dataset Query Target OOD? #Training #Eval #Candidates Classification (10 Tasks) VQA (10 Tasks) Retrieval (12 Tasks) ImageNet-1K N24News HatefulMemes VOC2007 SUN397 Place365 ImageNet-A ImageNet-R ObjectNet CountryOK-VQA A-OKVQA DocVQA InfographicVQA ChartQA Visual7W ScienceQA VizWiz GQA TextVQA VisDial CIRR VisualNews t2i VisualNews i2t MSCOCO t2i MSCOCO i2t NIGHTS WebQA OVEN FashionIQ EDIS Wiki-SS-NQ Visual Grounding (4 Tasks) MSCOCO Visual7W-Pointing RefCOCO RefCOCO-Matching + I I + + + + + + + + + + T + T I + + I + + + + T T T T T I I + + + I + 100K 49K 8K 8K 20K - - - - - 9K 17K 40K 24K 28K 70K - - - - 123K 26K 100K 100K 100K 113K 16K 17K - - - - 100K - - - 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 24 2 20 397 365 1000 200 313 211 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 For each evaluation split within the MMEB datasets, 1,000 samples are randomly selected. Further details on dataset processing can be found in Section A.1."
        },
        {
            "title": "3 VL M2VE C: TRANSFORMING LVMS TO EMBEDDERS",
            "content": "3.1 CONTRASTIVE TRAINING We develop VL M2VE C, contrastive training framework designed to convert any state-of-the-art vision-language model into an embedding model, as illustrated in Figure 3. relevant query-target pair is denoted as (q, t+). Both and t+ could be either single image, text or single image + text. We define : (qt, qi) and t+ : (t+ We then apply the instruction to the original query to generate new one qinst: , t+ ). qinst = [IMAGE TOKEN]Instruct: {task definition} Query: {q} (1) where {task definition} is placeholder for one-sentence description of the embedding task. To enhance the embedding models generalizability by better understanding instructions, we have crafted task-specific instructions, as shown in Tables 6, 7, 8 and 9."
        },
        {
            "title": "Manuscript",
            "content": "Figure 2: An overview of the tasks and datasets in MMEB. MMEB includes four meta-tasks and 36 datasets: 20 in-distribution datasets (blue) used for training and 16 out-of-distribution (orange) datasets used exclusively for evaluation. Given pretrained VLM, we feed query and target into it to obtain the query and target embeddings (hqinst, ht+) by taking the last layer vector representation of the last token. To train the embedding model, we adopt the standard InfoNCE loss over the in-batch negatives and hard negatives: min = log ϕ(hqinst, ht+) (cid:88) ϕ(hqinst, ht+) + (ϕ(hqinst, ht)) tN (2) where denotes the set of all negatives, and ϕ(hq, ht) is function that computes the matching score between query and target t. In this paper, we adopt the temperature-scaled cosine similarity function as follows: cos(hq, ht)) (3) τ is temperature hyper-parameter. ϕ(hq, ht) = exp( 1 τ 3.2 INCREASING BATCH SIZE THROUGH GRADCACHE Since hard negatives are often difficult or ambiguous to collect for most multimodal datasets, using larger batch sizes becomes crucial. This increases the number of in-batch random negatives, which in turn helps improve the performance of the embedding model. bottleneck lies in the GPU memory that limits us from increasing the batch size and the number of in-batch random negatives during training, as each training instance may include one image (either from the query or target side) or multiple images (from both query and target sides), resulting in substantial memory consumption. We apply GradCache (Gao et al., 2021a), gradient caching technique that decouples backpropagation between contrastive loss and the encoder, removing encoder backward pass data dependency along the batch dimension. Mathematically, supposed we have large batch of queries Q, and we divide it into set of subbatches, each of which can fit into memory for gradient computation: = { ˆQ1, ˆQ2, . . . }. There are two major steps: Representation Gradient Computation and Caching and Sub-batch Gradient Accumulation. First, gradient tensors within each subbatch is calculated and stored: ui = f (qi) . Then gradients are accumulated for encoder parameters across all sub-batches: (qi) Θ (qi) Θ f (qi) Θ (cid:88) (cid:88) (cid:88) (cid:88) ui = = ˆQj qi ˆQj ˆQj qi ˆQj (4)"
        },
        {
            "title": "Manuscript",
            "content": "Figure 3: VL M2VE uses VLM as the backbone to deeply integrate image and text features. It is trained with contrastive loss between the query and target, following task-specific instructions. The training data consists of diverse combinations of modalities on both the query and target sides, which may include images, text, or image-text pairs."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENT SETTINGS In this paper, we adopt Phi-3.5-V (Abdin et al., 2024) as our backbone VLM, with training conducted via either full model fine-tuning or LoRA. The temperature is set to 0.02. Multiple experimental configurations are explored to assess the impact of key hyperparameters, such as batch size and input length. In the basic setting, we use batch size of 256, 2,000 training steps, 4 image crops, and maximum text length of 256 tokens. Detailed ablation studies on these parameters are discussed in Section 4.4. For the 20 training datasets, if dataset contains more than 50K samples, we randomly select 50K for consistency, resulting in total training set of 662K data points. When using GradCache, we set sub-batch size of 4 to enable full model tuning, with the total batch size accumulated to 1,024. All experiments were run on 8 H100 GPUs. 4.2 BASELINES Four groups of baselines are reported in this study. CLIP-family: We utilize vision/language encoders such as CLIP (Radford et al., 2021), OpenCLIP (Cherti et al., 2023), SigLIP (Zhai et al., 2023), and BLIP2 (Li et al., 2023a) as our baseline. Due to the length limitations of the text encoder, some queries or target text in certain tasks may be truncated. We apply score-level fusion by combining multimodal features using element-wise addition with equal weights (w1 = w2 = 1). As found in UniIR (Wei et al., 2023), we do not use instructions, as this was observed to potentially degrade performance. UniIR: UniIR (Wei et al., 2023) is unified, instruction-guided multimodal retriever designed to handle eight different retrieval tasks across multiple modalities. The model builds on CLIP and BLIP, employing shallow fusion techniques such as score-level and feature-level fusion to integrate modalities. In this study, we use the BLIP FF variation as baseline. MagicLens: MagicLens (Zhang et al., 2024) is self-supervised image retrieval model capable of handling open-ended instructions. It utilizes dual-encoder architecture with shared parameters, initializing the vision and language encoders with either CoCa or CLIP. The model uses multihead attention pooler to unify multimodal inputs into single embedding. For this study, we report results using the CLIP-Large backbone. Since MagicLens requires image inputs, we represent pure text inputs by using the output vector of the text encoder."
        },
        {
            "title": "Manuscript",
            "content": "E5-V: E5-V (Jiang et al., 2024) is contemporary model that also leverages vision-language models for multimodal embedding tasks. It proposes single-modality training approach, where the model is trained exclusively on text pairs. E5-V demonstrated strong performance on three multimodal retrieval tasks. In contrast, we propose MMEB, extending the evaluation to larger and more diverse set of tasks. Additionally, our model is trained on multimodal pairs, which include various combinations of image and text modalities on both the query and target sides."
        },
        {
            "title": "4.3 MAIN RESULT",
            "content": "Table 2: Results on the MMEB benchmark. The scores are averaged per meta-task. For detailed scores per dataset, see Table 5. Model Per Meta-Task Score Average Score Classification VQA Retrieval Grounding IND OOD Overall # of datasets 10 10 12 20 16 36 CLIP (Radford et al., 2021) BLIP2 (Li et al., 2023a) SigLIP (Zhai et al., 2023) OpenCLIP (Cherti et al., 2023) UniIR (Wei et al., 2023) E5-V (Jiang et al., 2024) Magiclens (Zhang et al., 2024) LoRA (bs=1024) Full Fine-Tuning (bs=1024) - Best baseline Baseline Models 9.1 4.2 8.4 10.9 15.0 4.9 8.3 53.0 33.9 31.6 52.3 60.1 11.5 35.4 Ours (VL M2VE C) 54.9 50.3 +39.9 62.3 57.8 +2.2 42.8 27.0 40.3 47.8 42.1 21.8 38. 54.8 52.8 +7.0 51.8 47.0 59.5 53.3 62.2 19.0 26.0 79.5 72.3 +17.3 37.1 25.3 32.3 39.3 44.7 14.9 31.0 38.7 25.1 38.0 40.2 40.4 11.5 23.7 37.8 25.2 34.8 39.7 42.8 13.3 27. 66.5 62.8 +21.8 52.0 47.4 +11.6 60.1 55.9 +17.3 We report Precision@1 for all models in Table 2. It measures the ratio of positive candidates being ranked in the top place for all queries. For VL M2VE C, both the LoRA and fully fine-tuned variants follow the same setup of 1024 training batch size, 2K training steps and 4 sub-image crops. The LoRA variant uses rank of 8. From Table 2, the LoRA version of VL M2VE is the best variant, achieving an average precision@1 of 60.1% across all 36 datasets from MMEB. Additionally, it maintains an average precision@1 of 52.0% on 16 out-of-distribution tasks in zero-shot evaluation, suggesting strong generalization ability. This indicates that our model, when well-trained on datasets from diverse task categories, domains, and modality combinations, can effectively follow instructions to align the visual and text spaces and generalize well to unseen tasks. The full fine-tuning variation achieves slightly lower scores than the LoRA version. For detailed discussion comparing full fine-tuning and LoRA, please refer to Section 4.4.1. Compared to other baseline models, we observe consistent improvements in our model across all meta-task categories. Notably, our model achieves 17.3-point improvement (from 42.8 to 60.1) across all 36 MMEB datasets and 11.6-point increase (from 40.4 to 52.0) on 16 out-of-distribution datasets for zero-shot evaluation. Additionally, unlike the baseline models, which fail to demonstrate reasonable performance across all different task categories, VL M2VE achieves relatively strong performance (at least 50%) across all four meta-task categories. This highlights its capability to handle wide range of multimodal embedding tasks effectively. Its worth noting that our contemporary work E5-V (Wang et al., 2022a), which is also based on vision-language models, performs much worse than VL M2VE C. This is due to the fact that E5-V has been trained exclusively on text data without using multimodal data. 4.4 RESULT ANALYSIS To train an effective and generalizable multimodal embedding, various factors need to be considered, ranging from the data to the training setup. In this section, we present detailed ablation studies on these factors. We will discuss two training setups: Full Fine-Tuning vs. LoRA, along with Training"
        },
        {
            "title": "Manuscript",
            "content": "parameters, and two topics related to data: Meta-task generalization and Impact of instructions in this section."
        },
        {
            "title": "4.4.1 FULL FINE-TUNING VS. LORA",
            "content": "When fine-tuning the VLMs, key decision is whether to conduct full fine-tuning, which updates all parameters in the model, or to use parameter-efficient method such as LoRA. We compare the performance of fully fine-tuned VL M2VE with its LoRA variants at different ranks. The training and data setups are kept consistent across all models. We observe that LoRA achieves better performance when the rank is appropriately configured. Table 3: We compare the performance of fully fine-tuned VL M2VE with its LoRA variants at different ranks. LoRA can achieve better performance when the rank is appropriately configured. Model Meta-Task Average Score Average Score Classification VQA Retrieval Grounding IND OOD Overall # of datasets Full Fine-Tuning (bs=256) LoRA = 4 (bs=256) LoRA = 8 (bs=256) LoRA = 16 (bs=256) LoRA = 32 (bs=256) 10 50.4 52.7 52.9 51.1 50.6 46.4 53.6 52.5 40.5 47.8 12 52.6 60.1 60.3 52.0 53.9 4 68.6 80.2 80.0 72.5 72.5 57.9 64.9 64.2 54.9 58.9 16 44.7 50.4 50.8 45.8 46.5 36 52.0 58.4 58.2 50.8 53.4 4.4.2 TRAINING PARAMETERS During our experiments, we identified three key parameters that significantly impact the performance of VL M2VE C: training batch size, the number of sub-image crops, and the number of training steps. In Figure 4, we observe that the final performance gradually improves as we increase the batch size, training step size, and number of sub-image crops. We particularly want to highlight the impact of batch size. Due to the lack of hard negatives, using large batch size with plenty of random negatives, supported by the GradCache technique, plays crucial role in enhancing the performance of VL M2VE C, as discussed in Section 3.2. Figure 4: The figures demonstrate the influence of the training setup on VL M2VE Cs final performance. Here, we examine the effects of training batch size, the number of sub-image crops, and the number of training steps. 4.4.3 META-TASK GENERALIZATION We have demonstrated that VL M2VE has the potential to transfer to out-of-distribution datasets after being trained on diverse range of in-distribution datasets, with the instruction-following settings. An interesting question arises as to whether focusing on specific meta-task can enhance the models overall generalizability. We have trained three models, each focused solely on one meta-task (classification, visual question answering, and retrieval). Visual grounding was not included due to"
        },
        {
            "title": "Manuscript",
            "content": "the limited number of datasets. We then evaluated the models transferability to other meta-tasks. We refer to these three models as VLM2VecRET, trained on 8 retrieval tasks, VLM2VecVQA, trained on 6 visual question answering tasks, and VLM2VecCLS, trained on 5 classification tasks. Figure 5 illustrates the generalizability of these three models on unseen meta-tasks. We could observe that VLM2VecRET has better generalizablilty on other meta-task, compared with other two models, especially on visual grounding categories. The reason is that retrieval tasks involve more diverse combination of text and visual modalities from both the query and target sides, which helps the model generalize better to unseen meta-tasks. This observation highlights the benefits of using more diverse tasks in the VL M2VE training process. Figure 5: The figures show the generalization ability of models trained on one meta-task to other unseen meta-tasks. Models trained on retrieval tasks demonstrate better generalization ability because retrieval tasks involve more diverse combination of text and visual modalities from both the query and target sides. 4.4.4 IMPACT OF INSTRUCTIONS Previous studies have shown the influence of instructions on addressing various tasks. VL M2VE C, which leverages VLM as its backbone and is trained on large-scale datasets with instructions, is expected to better generalize across tasks and improve performance in multimodal embedding tasks. In this section, we evaluate the performance of VL M2VE with and without task-specific instructions to quantify the impact of incorporating instructions into the embedding process. As shown in Table 4, excluding instructions leads to an average performance drop of around 30%, highlighting the importance of instruction-guided embeddings. Table 4: Comparison of VL M2VE with and without task-specific instructions. The percentage decrease highlights the significant performance drop when instructions are not incorporated. Model Meta-Task Average Score Average Score Classification VQA Retrieval Grounding # of datasets w/ instruction w/o instruction 10 50.4 36.7 46.4 33.5 12 52.6 31.1 4 68.6 44.3 IND 20 57.9 37.3 OOD Overall 16 44.7 31. 36 52.0 34.8 -27.2% -27.8% -40.9% -35.4% -35.6% -29.3% -33.1%"
        },
        {
            "title": "5 RELATED WORK",
            "content": "5.1 TEXT EMBEDDING Text embeddings have demonstrated significant potential in powering downstream applications such as information retrieval (Karpukhin et al., 2020; Xiong et al., 2020), text similarity (Gao et al., 2021b), prompt retrieval for in-context learning (Hongjin et al., 2022), and classification (Logeswaran & Lee, 2018; Reimers & Gurevych, 2019). Early work focused on creating effective"
        },
        {
            "title": "Manuscript",
            "content": "embeddings for specific tasks. With the rise of pretrained language models, efforts have shifted toward developing universal embedding models capable of handling wide range of embedding tasks. Studies such as GTR (Ni et al., 2022) and E5 (Wang et al., 2022a) leveraged large amounts of noisy paired data to pretrain and fine-tune dense retrievers. More recent works like TART (Asai et al., 2022) and InstructOR (Su et al., 2023) introduced natural language prompts to guide embedding models in producing task-relevant embeddings. Building on this, models like E5Mistral(Wang et al., 2024), SFR-Embedding(Meng et al., 2024), RepLLaMA(Ma et al., 2024b), GTE-Qwen2(Li et al., 2023b), and NV-Embed (Lee et al., 2024) have utilized pretrained large language models (LLMs) as their backbone, fine-tuning them with multi-task data and instructions. These models have delivered significant improvements over earlier approaches that did not use LLMs for initialization or instruction tuning. However, these advances come at cost: both the model size and the dimensionality of output vectors have increased substantially, leading to higher costs."
        },
        {
            "title": "5.2 MULTIMODAL EMBEDDINGS",
            "content": "Multimodal embeddings have long been significant research challenge. Early works like CLIP (Radford et al., 2021), BLIP (Li et al., 2022; 2023a), Align (Jia et al., 2021), SigLIP (Zhai et al., 2023), SimVLM Wang et al. (2022b) and CoCa (Yu et al., 2022) primarily focused on learning universal representations from large-scale, weakly supervised image-text pairs. These models generally encode images and text separately, projecting them into shared space. This approach has laid the groundwork for more recent multimodal models like LLaVA (Liu et al., 2024). Most research on universal multimodal embeddings involves fine-tuning models like CLIP or BLIP, typically using simple fusion mechanisms to combine visual and language information. For instance, UniIR (Wei et al., 2023) creates multimodal embeddings by simply adding text and visual features, while MagicLens (Zhang et al., 2024) employs shallow self-attention layers to integrate these features more effectively. The study most similar to ours is E5-V (Jiang et al., 2024), which converts multimodal generative model into an embedding model. We compare our approach with E5-V and find that our models achieve significant improvements in performance. 5.3 EMBEDDING BENCHMARKS Significant efforts have been made to develop benchmarks for evaluating retrieval systems. For text retrieval models, MS MARCO (Nguyen et al., 2016) and Natural Questions (Kwiatkowski et al., 2019b) are two of the most widely used benchmarks in general domains. To broaden the evaluation across more diverse domains, BEIR (Thakur et al.) was introduced, incorporating 18 datasets from various fields. Building on this, MTEB (Muennighoff et al., 2023) further expands BEIRs scope by adding more tasks, such as classification, clustering, and semantic textual similarity (STS), to assess the generalization capabilities of embedding models. For multimodal retrieval, several benchmarks have been introduced to evaluate model performance across different modalities. MBEIR (Wei et al., 2023) includes 8 tasks and 16 datasets, designed to test models ability to retrieve information based on various forms of queries (text, image, or combination) and instructions that span multiple modalities."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we aim to build the first large-scale multimodal embedding framework, comprising two main components: MMEB and VL M2VE C. MMEB includes 36 datasets across four meta-task categories, providing comprehensive and diverse framework for training and evaluating embedding models. VL M2VE leverages VLMs as backbone to deeply fuse visual and textual spaces, enhancing generalization to unseen tasks through instruction following."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. SemEval-2012 task 6: pilot on semantic textual similarity. In Eneko Agirre, Johan Bos, Mona Diab, Suresh Manandhar, Yuval Marton, and Deniz Yuret (eds.), *SEM 2012: The First Joint Conference on Lexical and Computational Semantics Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pp. 385393, Montreal, Canada, 7-8 June 2012. Association for Computational Linguistics. URL https://aclanthology.org/S12-1051. Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, HanarXiv preprint naneh Hajishirzi, and Wen-tau Yih. Task-aware retrieval with instructions. arXiv:2211.09260, 2022. Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: large-scale bias-controlled dataset for pushing the limits of object recognition models. Advances in neural information processing systems, 32, 2019. Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. Llm2vec: Large language models are secretly powerful text encoders. arXiv preprint arXiv:2404.05961, 2024. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Steven Bethard, Marine Carpuat, Marianna Apidianaki, Saif M. Mohammad, Daniel Cer, and David Jurgens (eds.), Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval2017), pp. 114, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001. URL https://aclanthology.org/S17-2001. Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. Webqa: Multihop and multimodal qa. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1649516504, 2022. Gal Chechik, Varun Sharma, Uri Shalit, and Samy Bengio. Large scale online learning of image similarity through ranking. Journal of Machine Learning Research, 11(3), 2010. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 28182829, 2023. Alexis Conneau, Douwe Kiela, Holger Schwenk, Loıc Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 670680, 2017. Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose MF Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 326335, 2017. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams, John M. Winn, and Andrew Zisserman. The pascal visual object classes challenge: retrospective. International Journal of Computer Vision, 111:98 136, 2014. URL https://api.semanticscholar. org/CorpusID:207252270."
        },
        {
            "title": "Manuscript",
            "content": "Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344, 2023. Luyu Gao, Yunyi Zhang, Jiawei Han, and Jamie Callan. Scaling deep contrastive learning batch size under memory limited setup. arXiv preprint arXiv:2101.06983, 2021a. Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 68946910, 2021b. Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 36083617, 2018. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pp. 39293938. PMLR, 2020. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 83408349, 2021a. Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1526215271, 2021b. SU Hongjin, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, et al. Selective annotation makes language models better few-shot learners. In The Eleventh International Conference on Learning Representations, 2022. Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina Toutanova, and Ming-Wei Chang. Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1206512075, 2023. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67006709, 2019. Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering, 2020. URL https://arxiv.org/abs/2007.0128. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pp. 49044916. PMLR, 2021. Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. E5-v: Universal embeddings with multimodal large language models. arXiv preprint arXiv:2407.12580, 2024. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 67696781, 2020. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 787798, 2014."
        },
        {
            "title": "Manuscript",
            "content": "Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. Advances in neural information processing systems, 33:26112624, 2020. Ryan Kiros, Yukun Zhu, Russ Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-thought vectors. Advances in neural information processing systems, 28, 2015. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019a. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019b. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428, 2024. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 94599474, 2020. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International conference on machine learning, pp. 1288812900. PMLR, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023a. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281, 2023b. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In Computer Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740755. Springer, 2014. Fuxiao Liu, Yinghan Wang, Tianlu Wang, and Vicente Ordonez. Visual news: Benchmark and challenges in news image captioning. arXiv preprint arXiv:2010.03743, 2020. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? DeeLIO 2022, pp. 100, 2022. Siqi Liu, Weixi Feng, Tsu-jui Fu, Wenhu Chen, and William Yang Wang. Edis: Entity-driven image search over multimodal web content. arXiv preprint arXiv:2305.13631, 2023. Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and Stephen Gould. Image retrieval on real-life images with pre-trained vision-and-language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 21252134, 2021. Lajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence repreIn International Conference on Learning Representations, 2018. URL https: sentations. //openreview.net/forum?id=rJvJXZb0W."
        },
        {
            "title": "Manuscript",
            "content": "Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. Unifying multimodal retrieval via document screenshot embedding. arXiv preprint arXiv:2406.11251, 2024a. Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for multi-stage text retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 24212425, 2024b. Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. SICK cure for the evaluation of compositional distributional semantic models. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis (eds.), Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC14), pp. 216 223, Reykjavik, Iceland, May 2014. European Language Resources Association (ELRA). URL http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual In Proceedings of the IEEE/cvf question answering benchmark requiring external knowledge. conference on computer vision and pattern recognition, pp. 31953204, 2019. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 16971706, 2022. Rui Meng, Ye Liu, Shafiq Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. Sfr-embedding-2: Advanced text embedding with multi-stage training, 2024. URL https://huggingface. co/Salesforce/SFR-Embedding-2_R. Tomas Mikolov. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. Bhaskar Mitra, Fernando Diaz, and Nick Craswell. Learning to match using local and distributed representations of text for web search. In Proceedings of the 26th international conference on world wide web, pp. 12911299, 2017. Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. Mteb: Massive text emIn Proceedings of the 17th Conference of the European Chapter of the bedding benchmark. Association for Computational Linguistics, pp. 20142037, 2023. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. Ms marco: human generated machine reading comprehension dataset. November 2016. URL https://www.microsoft.com/en-us/research/publication/ ms-marco-human-generated-machine-reading-comprehension-dataset/. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, et al. Large dual encoders are generalizable retrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 98449855, 2022. Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 15321543, 2014."
        },
        {
            "title": "Manuscript",
            "content": "Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models. In Proceedings of the IEEE international conference on computer vision, pp. 26412649, 2015. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERTIn Proceedings of the 2019 Conference on Empirical Methods in Natural Lannetworks. guage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 39823992, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/ D19-1410. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 26552671, 2022. Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. In European A-okvqa: benchmark for visual question answering using world knowledge. conference on computer vision, pp. 146162. Springer, 2022. Thibault Sellam, Dipanjan Das, and Ankur Parikh. Bleurt: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 78817892, 2020. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, In Proceedings of the IEEE/CVF and Marcus Rohrbach. Towards vqa models that can read. conference on computer vision and pattern recognition, pp. 83178326, 2019. Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi Raghunathan. Repetition improves language model embeddings. arXiv preprint arXiv:2402.15449, 2024. Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned text embeddings. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 11021121, 2023. Nandan Thakur, Nils Reimers, Andreas Ruckle, Abhishek Srivastava, and Iryna Gurevych. Beir: heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2022a. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368, 2024. Zhen Wang, Xu Shan, Xiangxie Zhang, and Jie Yang. N24news: new dataset for multimodal news classification. arXiv preprint arXiv:2108.13327, 2021. Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: SimIn International Conference on ple visual language model pretraining with weak supervision. Learning Representations, 2022b. Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. Uniir: Training and benchmarking universal multimodal information retrievers. arXiv preprint arXiv:2311.17136, 2023."
        },
        {
            "title": "Manuscript",
            "content": "Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, and Rogerio Feris. Fashion iq: new dataset towards retrieving images by natural language feedback. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pp. 1130711317, 2021. Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pp. 34853492. IEEE, 2010. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808, 2020. Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. Transactions on Machine Learning Research, 2022. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language In Proceedings of the IEEE/CVF International Conference on Computer image pre-training. Vision, pp. 1197511986, 2023. Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, and Ming-Wei Chang. Magiclens: Self-supervised image retrieval with open-ended instructions. arXiv preprint arXiv:2403.19651, 2024. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2020. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017. Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 49955004, 2016."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 DATASET DETAILS A.1.1 CLASSIFICATION There are total of 10 datasets for classification tasks. ImageNet-1K (Deng et al., 2009) The dataset is large-scale dataset commonly used in image classification, consisting of over 1 million images across 1K different classes. ImageNet-A (Hendrycks et al., 2021b) The dataset contains images from distribution unlike the ImageNet training distribution. ImageNet-A examples belong to ImageNet classes, but the examples are harder and can cause mistakes across various models. They cause consistent classification mistakes due to scene complications encountered in the long tail of scene configurations and by exploiting classifier blind spots. ImageNet-R (Hendrycks et al., 2021a) The dataset contains set of images labeled with ImageNet labels obtained by collecting art, cartoons, deviantart, graffiti, embroidery, graphics, origami, paintings, patterns, plastic objects, plush objects, sculptures, sketches, tattoos, toys, and video game renditions of ImageNet classes. VOC2007 (Everingham et al., 2014) The dataset focuses on recognizing objects in realistic scenarios and contains 20 object classes. N24News (Wang et al., 2021) The dataset is sourced from the New York Times and consists of 24 categories, with each news article containing both text and image information. The task is to classify the given news image and its accompanying text into one of the 24 categories. HatefulMemes (Kiela et al., 2020) The dataset proposes new challenge set for multimodal classification, focusing on detecting hate speech in multimodal memes. Place365 (Zhou et al., 2017) The dataset is repository of 10 million scene photographs, labeled with scene semantic categories, comprising large and diverse list of the types of environments encountered in the world. SUN397 (Xiao et al., 2010) The dataset is dataset for scene recognition consisting of 397 categories. ObjectNet (Barbu et al., 2019) The dataset is crowd-sourced test set of 50K images featuring objects in unusual poses and cluttered scenes, designed to challenge recognition performance. It includes controls for rotation, background, and viewpoint, and covers 313 object classes. Country-211 (Radford et al., 2021) The dataset is designed to assess the geolocation capability of visual representations. It filters the YFCC100M dataset to find 211 countries that have at least 300 photos with GPS coordinates. A.1.2 VISUAL QUESTION ANSWERING (VQA) There are total of 10 datasets for VQA tasks. OK-VQA (Marino et al., 2019) The dataset includes questions that require external resources for answers. A-OKVQA (Schwenk et al., 2022) The dataset is an augmented successor of OK-VQA, requiring broad base of commonsense and world knowledge to answer. The questions generally cannot be answered by simply querying knowledge base, and instead require some form of commonsense reasoning about the scene depicted in the image. DocVQA (Mathew et al., 2021) The dataset contains questions for document analysis and recognition over document images of various types and content. InfographicsVQA (Mathew et al., 2022) The dataset comprises diverse collection of infographics accompanied by natural language question and answer annotations. The questions require methods capable of jointly reasoning over the document layout, textual content, graphical elements, and data visualizations."
        },
        {
            "title": "Manuscript",
            "content": "ChartQA (Masry et al., 2022) The dataset is designed for question answering about charts, with focus on visual and logical reasoning applied to real-world charts. ScienceQA (Lu et al., 2022) The dataset contains questions with diverse science topics and annotations of their answers with corresponding lectures and explanations. Visual7W-telling (Zhu et al., 2016) The dataset establishes semantic link between textual descriptions and image regions through object-level grounding. It has two types of questions: telling and pointing. It leverages the six questions (what, where, when, who, why, and how) to systematically examine models capability for visual understanding through telling questions. Additionally, seventh which question is appended for visual answers as pointing questions. We use Visual7W-telling in our VQA category and Visual7W-pointing in our visual grounding category. VizWiz (Gurari et al., 2018) The dataset originates from natural visual question answering scenario, where blind individuals captured images and recorded spoken questions about them, along with 10 crowdsourced answers for each visual question. For our task, we select only the answerable questions. TextVQA (Singh et al., 2019) The dataset is designed to benchmark visual reasoning based on text within images. Models need to read and reason about the text in images to answer related questions. GQA (Hudson & Manning, 2019) The dataset is designed for real-world visual reasoning and compositional question answering. It uses real images from the Visual Genome dataset. Each image is accompanied by scene graph annotations that describe the classes and attributes of objects in the scene, as well as their pairwise relationships. A.1.3 RETRIEVAL There are total of 12 datasets for retrieval tasks. VisDial (Das et al., 2017) The dataset features dialogues created by two Amazon Mechanical Turk workers. One worker takes the role of the questioner, who only sees the text description of an image, while the other plays the answerer, who has access to the image. They engage in 10round Q&A session about the image. We repurpose this dataset as retrieval task, where the goal is to retrieve the image based on the given dialogue. CIRR (Liu et al., 2021) The dataset is designed for the task of composed image retrieval. It consists of pairs of real-life reference and target images, along with modification sentence that describes the changes made between the two images. FashionIQ (Wu et al., 2021) The dataset contains images of fashion products with crowd-sourced descriptions highlighting the differences between these products. Similar to CIRR, FashionIQ can also be used for the task of composed image retrieval, where each test case consists of pair of reference and target images, along with modification sentence that describes the changes between the two images. VisualNews (Liu et al., 2020) The dataset contains publicly available news image paired with captions. We split this task into two setups: VisualNews i2t, which retrieves the caption given the news image and VisualNews t2i, which retrieves the news image given the caption. MSCOCO (Lin et al., 2014) The dataset is well-known image caption dataset. Similar to VisualNews, WE split this task into two setups: MSCOCO i2t, which retrieves the caption given the image and MSCOCO t2i, which retrieves the image given the caption. WebQA (Chang et al., 2022) The dataset is multihop, multimodal QA dataset that requires retrieving Wikipedia page to answer given question. We use the Wikipedia pages image and text descriptions as the candidates for retrieval. NIGHTS (Fu et al., 2023) The dataset contains human similarity judgments on image pairs that are alike in various ways. The original dataset consists of triplets: reference image and two perturbed versions, along with human judgments indicating which version is most similar to the reference. Following M-BEIR (Wei et al., 2023), we refactor this dataset into retrieval task to match pairwise images, where the reference image serves as the query, and the perturbed version that aligns with human judgment is the target."
        },
        {
            "title": "Manuscript",
            "content": "OVEN (Hu et al., 2023) The dataset contains instances that include an image and visual recognition text question. Additionally, each instance provides related Wikipedia image along with its corresponding text description (the Wikipedia title and the first 100 tokens of its summary) as reference for answering the question, which we treat as the target candidate. EDIS (Liu et al., 2023) The dataset is cross-modal image search in the news domain. This dataset contains entity-rich queries, requiring the model to understand both entities and events from the text queries. The candidate consists of the news image and its accompanying headline. Wiki-SS-NQ (Ma et al., 2024a) The dataset is another retrieval-based VQA dataset. Unlike the original Natural Questions dataset (Kwiatkowski et al., 2019a), which uses Wikipedia paragraph to answer the question, this dataset leverages Wiki-SS, utilizing Wikipedia page screenshots as the corpus. The screenshot provides more comprehensive information than plain Wikipedia paragraph. For CIRR, FashionIQ, VisualNews, MSCOCO, WebQA, NIGHTS, OVEN and EDIS, we use the processed versions from M-BEIR (Wei et al., 2023). A.1.4 VISUAL GROUNDING There are total of 4 datasets for visual grounding tasks. MSCOCO (Lin et al., 2014) The dataset includes an object detection task, which involves recognizing an object from given class in an image. We have repurposed this task into ranking problem within the MMEB format. The query consists of the image and the object name, while the target is the cropped image of the specified object. We gather distractors from other objects in the same image as well as from different images. We discard test cases where the object is too small. RefCOCO (Kazemzadeh et al., 2014) The dataset includes an object detection task that requires more reasoning than MSCOCO. Unlike simply identifying the object class, the RefCOCO dataset uses language expressions to refer to specific objects within an image. In our MMEB, we have two tasks related to RefCOCO: RefCOCO and RefCOCO-Matching. In RefCOCO, the query consists of the image and the language expressions referring to specific object, while the target is the cropped image of that object. In RefCOCO-Matching, both the query and the target contain the image and the language expressions referring to specific object, where the two objects are identical. Visual7W-pointing (Zhu et al., 2016) The dataset establishes semantic link between textual descriptions and image regions through object-level grounding. It has two types of questions: telling and pointing. It leverages the six questions (what, where, when, who, why, and how) to systematically examine models capability for visual understanding through telling questions. Additionally, seventh which question is appended for visual answers as pointing questions. We use Visual7W-telling in our VQA category and Visual7W-pointing in our visual grounding category."
        },
        {
            "title": "Manuscript",
            "content": "Table 5: The detailed results of the baselines and our VL M2VE on MMEB, which includes 20 indistribution datasets and 16 out-of-distribution datasets. The out-of-distribution datasets are highlighted with yellow background in the table. We only include the best version of VL M2VE in the table, which was trained using LoRA. CLIP OpenCLIP SigLIP BLIP2 MagicLens E5-V UniIR VL M2VE Classification (10 tasks) ImageNet-1K N24News HatefulMemes VOC2007 SUN397 Place365 ImageNet-A ImageNet-R ObjectNet Country-211 All Classification VQA (10 tasks) OK-VQA A-OKVQA DocVQA InfographicsVQA ChartQA Visual7W ScienceQA VizWiz GQA TextVQA All VQA Retrieval (12 tasks) VisDial CIRR VisualNews t2i VisualNews i2t MSCOCO t2i MSCOCO i2t NIGHTS WebQA FashionIQ Wiki-SS-NQ OVEN EDIS All Retrieval Visual Grounding (4 tasks) MSCOCO RefCOCO RefCOCO-matching Visual7W-pointing All Visual Grounding Final Score (36 tasks) All All IND All OOD 55.8 34.7 51.1 50.7 43.4 28.5 25.5 75.6 43.4 19.2 42.8 7.5 3.8 4.0 4.6 1.4 4.0 9.4 8.2 41.3 7.0 9.1 30.7 12.6 78.9 79.6 59.5 57.7 60.4 67.5 11.4 55.0 41.1 81.0 53.0 33.8 56.9 61.3 55.1 51.8 37.8 37.1 38. 63.5 38.6 51.7 52.4 68.8 37.8 14.2 83.0 51.4 16.8 47.8 11.5 3.3 5.3 4.6 1.5 2.6 10.2 6.6 52.5 10.9 10.9 25.4 15.4 74.0 78.0 63.6 62.1 66.1 62.1 13.8 44.6 45.0 77.5 52.3 34.5 54.2 68.3 56.3 53.3 39.7 39.3 40.2 10.3 36.0 49.6 52.1 34.5 21.5 3.2 39.7 20.6 2.5 27. 8.7 3.2 2.6 2.0 0.5 1.3 6.8 4.0 9.7 3.3 4.2 18.0 9.8 48.1 13.5 53.7 20.3 56.5 55.4 9.3 28.7 39.5 54.4 33.9 28.9 47.4 59.5 52.0 47.0 25.2 25.3 25.1 48.0 33.7 49.0 51.6 57.0 31.5 8.0 70.9 31.6 6.2 38.8 12.7 2.9 3.0 5.9 0.9 2.5 5.2 1.7 43.5 4.6 8. 24.8 39.1 50.7 21.1 54.1 40.0 58.1 43.0 11.2 18.7 1.6 62.6 35.4 22.1 22.8 35.6 23.4 26.0 27.8 31.0 23.7 9.6 23.4 49.7 49.9 33.1 8.6 2.0 30.8 7.5 3.1 21.8 8.9 5.9 1.7 2.3 2.4 5.8 3.6 2.6 7.8 8.2 4.9 9.2 6.1 13.5 8.1 20.7 14.0 4.2 17.7 2.8 8.6 5.9 26.8 11. 10.8 11.9 38.9 14.3 19.0 13.3 14.9 11.5 53.7 33.9 51.0 62.7 61.7 38.0 12.9 61.6 37.1 8.8 42.1 24.5 10.6 5.6 5.0 1.8 12.3 11.6 19.2 49.3 10.3 15.0 37.6 53.2 63.6 68.8 72.0 74.1 69.7 86.3 39.3 11.3 66.6 78.2 60.1 46.6 67.8 62.9 71.3 62. 42.8 44.7 40.4 65.6 79.5 67.1 88.6 72.7 42.6 19.3 70.2 29.5 13.0 54.8 63.2 50.2 78.4 40.8 59.0 47.7 43.4 39.2 60.7 66.1 54.9 73.3 47.8 67.2 70.7 70.6 66.5 66.1 88.1 12.9 56.6 47.3 79.9 62.3 67.3 84.7 79.2 86.8 79.5 60.1 66.5 52. 45.4 13.9 47.2 64.3 39.6 20.0 42.6 75.0 40.3 14.2 40.3 2.4 1.5 4.2 2.7 3.0 1.2 7.9 2.3 57.5 1.0 8.4 21.5 15.1 51.0 52.4 58.3 55.0 62.9 58.1 20.1 55.1 56.0 23.6 31.6 46.4 70.8 50.8 70.1 59.5 34.8 32.3 38."
        },
        {
            "title": "Manuscript",
            "content": "Table 6: Examples of datasets in MMEB (Part 1 of 4). Instructions are written in italic font style. Category Dataset Query Text Query Image Target Text Target Image ImageNet-1K (Deng et al., 2009) Represent the given image for classification Italian greyhound ImageNet-A (Hendrycks et al., 2021b) Represent the given image for classification. sea anemone, anemone ImageNet-R et al., 2021a) (Hendrycks Represent the given image for classification. baseball player N24News 2021) (Wang et al., Represent the given news image with the following caption for domain classification. Ms. Goodman styled Amber Valletta with wings for 1993 shoot by Peter Lindbergh for Harpers Bazaar. Classification VOC2007 et al., 2014) (Everingham Identify the object shown in the image. Style bus SUN397 (Xiao et al., 2010) Identify the scene shown in the image. firing range indoor ObjectNet 2019) (Barbu et al., Identify the object shown in the image. Country-211 et al., 2021) (Radford Identify the country depicted in the image. HatefulMemes (Kiela et al., 2020) Represent the given image for binary classification to determine whether it constitutes hateful speech or not. mug China No Place365 2017) (Zhou et al., Identify the scene shown in the image. Airport Terminal - - - - - - - - - -"
        },
        {
            "title": "Manuscript",
            "content": "Table 7: Examples of datasets in MMEB (Part 2 of 4). Instructions are written in italic font style. Category Dataset Query Text Query Image Target Text Target Image OK-VQA (Marino et al., 2019) Represent the given image with the following question. What breed of dog is this? A-OKVQA (Schwenk et al., 2022) Represent the given image with the following question. What is the metal basket near the net used to hold? chihuahua tennis balls DocVQA (Mathew et al., 2021) Represent the given image with the following question. What is name of university? university of california InfographicsVQA (Mathew et al., 2022) ChartQA (Masry et al., 2022) ScienceQA (Lu et al., 2022) VQA Represent the given image with the following question. Which social platform has heavy female audience? Represent the given image with the following question. How many food item is shown in the bar graph? Represent the given image with the following question. Which of these states is farthest north? Visual7W-telling (Zhu et al., 2016) Represent the given image with the following question. Where is the man sitting? VizWiz (Gurari et al., 2018) GQA (Hudson & Manning, 2019) Represent the given image with the following question. Can you tell me what this medicine is please? Represent the given image with the following question. What is under the utensil on the left? pinterest 14 South Carolina At the computer night time The napkin is under the utensil. TextVQA (Singh et al., 2019) Represent the given image with the following question. What is the brand of this camera? dakota - - - - - - - - - -"
        },
        {
            "title": "Manuscript",
            "content": "Table 8: Examples of datasets in MMEB (Part 3 of 4). Instructions are written in italic font style. Category Dataset Query Text Query Image Target Text Target Image VisDial (Das et al., 2017) VisualNews t2i (Liu et al., 2020) MSCOCO t2i (Lin et al., 2014) Retrieval WebQA (Chang et al., 2022) EDIS (Liu et al., 2023) Represent the given dialogue about an image, which is used for image retrieval. Q:do you see lot of people A:just 3 Q:what is the tennis player wearing A:white tennis dress Q:what color is her tennis racket A:black Q:is she wearing hat A:a visor Q:is she close to the net A:no Q:do you see another player A:no Q:do you see tennis bag A:no Retrieve an image of this news caption. US goalkeeper Hope Solo makes save. Find me an everyday image that matches the given caption. Man riding motor bike on dirt road on the countryside. Find Wikipedia image-passage pair that answers this question. Do both the Hays County Courthouse in San Marcos, Texas and the Ike Wood House at 227 Mitchell Street in San Marcos, Texas have six columns on their front entrance? Find news image that matches the provided caption. Tom Holland makes his debut in the Spidey suit in Captain America Civil War. Wiki-SS-NQ (Ma et al., 2024a) Find the document screenshot that can answer the given query. VisualNews i2t (Liu et al., 2020) Find caption for the news in the given photo. - - - - - - Represent the given image. Represent the given image. Represent the given image. Represent the given Wikipedia image with related text information. Hays County Courthouse (2018), San Marcos, TX The Hays County Courthouse in San Marcos, Texas. Listed on the National Register of Historic Places. 227 Mitchell, San Marcos, Texas Ike Wood House at 227 Mitchell Street in San Marcos, Texas. Represent the given image with related text information. Comic RiffsJon Favreau is set to reprise his Iron Man role for Spider Man: Homecoming. Represent screenshot. the given document Canadian Prime Minister Stephen Harper shakes hands with President Obama during the North American Leaders Summit in Toluca Mexico in February 2014. MSCOCO i2t (Lin et al., 2014) Find an image caption describing the given everyday image. man on bicycle riding next to train. - -"
        },
        {
            "title": "Manuscript",
            "content": "Table 9: Examples of datasets in MMEB (Part 4 of 4). Instructions are written in italic font style. Category Dataset Query Text Query Image Target Text Target Image CIRR (Liu et al., 2021) FashionIQ (Wu et al., 2021) Given an image, find similar everyday image with the described changes. Show three bottles of soft drink. Find an image to match the fashion image and style note. Is shiny and silver with shorter sleeves and fit and flare. Represent the given image. Represent the given image. Retrieval NIGHTS (Fu et al., 2023) Find day-to-day image that looks similar to the provided image. Represent the given image. OVEN (Hu et al., 2023) pair Wikipedia that imageRetrieve description provides evidence for the question of this image. What is the name of this place? MSCOCO (Lin et al., 2014) Visual7W-Pointing (Zhu et 2016) al., Select the portion of the image that isolates the object of the given label The lable of the object is stop sign. Select the portion of the image that answers the given question. Which door is behind person sitting on bench? Represent the given Wikipedia image with related text information. Titisee. The Titisee is lake in the southern Black Forest in BadenWurttemberg. It covers an area of 1.3 (km2) and is an average of 20 (m) deep. It owes its formation to the Feldberg glacier, the moraines of which were formed in the Pleistocene epoch and nowadays form the shores of the lake. The lakes outflow, at 840 (m) above sea level, is the River Gutach, which merges with the Haslach stream below Kappel to form the Wutach. The waters of the Titisee thus drain eventually into the Upper Rhine between Tiengen and Waldshut. On the north shore lies the. Represent the given cropped image of the object. Represent the given cropped image of the object. Grounding RefCOCO (Kazemzadeh et al., 2014) Select the portion of the image that follows the language expressions. man in black coat Represent the given cropped image of the object. RefCOCO-Matching (Kazemzadeh et al., 2014) Select the portion of the image that follows the language expressions. kid on right in back, blondish hair Select the portion of the image that follows the language expressions. top right kid"
        }
    ],
    "affiliations": [
        "Salesforce Research",
        "University of Waterloo"
    ]
}