{
    "paper_title": "Graph-Enhanced Deep Reinforcement Learning for Multi-Objective Unrelated Parallel Machine Scheduling",
    "authors": [
        "Bulent Soykan",
        "Sean Mondesire",
        "Ghaith Rabadi",
        "Grace Bochenek"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Unrelated Parallel Machine Scheduling Problem (UPMSP) with release dates, setups, and eligibility constraints presents a significant multi-objective challenge. Traditional methods struggle to balance minimizing Total Weighted Tardiness (TWT) and Total Setup Time (TST). This paper proposes a Deep Reinforcement Learning framework using Proximal Policy Optimization (PPO) and a Graph Neural Network (GNN). The GNN effectively represents the complex state of jobs, machines, and setups, allowing the PPO agent to learn a direct scheduling policy. Guided by a multi-objective reward function, the agent simultaneously minimizes TWT and TST. Experimental results on benchmark instances demonstrate that our PPO-GNN agent significantly outperforms a standard dispatching rule and a metaheuristic, achieving a superior trade-off between both objectives. This provides a robust and scalable solution for complex manufacturing scheduling."
        },
        {
            "title": "Start",
            "content": "GRAPH-ENHANCED DEEP REINFORCEMENT LEARNING FOR MULTI-OBJECTIVE UNRELATED PARALLEL MACHINE SCHEDULING 6 2 0 2 8 ] . [ 1 2 5 0 8 0 . 2 0 6 2 : r Bulent Soykan Institute for Simulation and Training University of Central Florida Orlando, FL, USA Bulent.Soykan@ucf.edu Sean Mondesire, Ghaith Rabadi, Grace Bochenek School of Modeling, Simulation, and Training University of Central Florida Orlando, FL, USA {Sean.Mondesire, Ghaith.Rabadi, Grace.Bochenek}@ucf.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "The Unrelated Parallel Machine Scheduling Problem (UPMSP) with release dates, setups, and eligibility constraints presents significant multi-objective challenge. Traditional methods struggle to balance minimizing Total Weighted Tardiness (TWT) and Total Setup Time (TST). This paper proposes Deep Reinforcement Learning framework using Proximal Policy Optimization (PPO) and Graph Neural Network (GNN). The GNN effectively represents the complex state of jobs, machines, and setups, allowing the PPO agent to learn direct scheduling policy. Guided by multi-objective reward function, the agent simultaneously minimizes TWT and TST. Experimental results on benchmark instances demonstrate that our PPO-GNN agent significantly outperforms standard dispatching rule and metaheuristic, achieving superior trade-off between both objectives. This provides robust and scalable solution for complex manufacturing scheduling. Source code and data are available at: github.com/bulentsoykan/GNN-DRL4UPMSP. Keywords Proximal Policy Optimization Machine Scheduling Problem Graph Neural Networks"
        },
        {
            "title": "INTRODUCTION",
            "content": "Advanced manufacturing environments operate under intense pressure to improve efficiency, reduce operational costs, and maintain responsiveness to dynamic market demands [1]. The implementation of efficient production scheduling is crucial for achieving these goals. It involves finding the best way to allocate constrained resources, such as machines and personnel, to competing tasks or jobs over specific timeframe. Optimal scheduling decisions directly impact key performance indicators (KPIs), including production throughput, resource utilization, adherence to delivery deadlines, and overall system productivity. Consequently, developing adaptive scheduling methodologies is important for maintaining competitiveness in todays globalized industrial landscape [2]. fundamental class of scheduling problems encountered in various industrial settings is the Parallel Machine Scheduling Problem (PMSP). In this context, set of jobs must be processed on set of available machines. This paper focuses on particularly challenging variant known as the Unrelated Parallel Machine Scheduling Problem (UPMSP). In the UPMSP, the processing time required for specific job depends not only on the job itself but also distinctly on the machine assigned to process it. This scenario accurately reflects real-world situations where machines, even if performing similar functions, may differ in age, capability, or specialization. Such problems are prevalent in diverse sectors, including semiconductor manufacturing [3], textile production [4], where heterogeneity among processing units is common. Real-world UPMSP applications are further complicated by several interacting constraints addressed in this research. These include job-specific release dates, machine eligibility restrictions, and setup times that depend on both the sequence of jobs and the specific machine. Handling these complexities simultaneously significantly increases the difficulty of finding optimal or near-optimal schedules. In addition, effective scheduling must often balance conflicting objectives, such as minimizing total weighted tardiness (TWT) to meet deadlines and minimizing total setup time (TST) TESO: Tabu-Enhanced Simulation Optimization for Noisy Black-Box Problems for operational efficiency. The inherent conflictwhere prioritizing one objective negatively impacts the othermakes finding high-performing solutions challenging. Given the NP-hard nature of this multi-objective UPMSP [5], traditional approaches face limitations in effectively managing this trade-off. In this paper, we propose novel method based on Deep Reinforcement Learning (DRL) in order to address the limitations of existing methods and tackle the complexities of the multi-objective UPMSP. DRL agents learn decisionmaking policies through interaction with an environment, making them well-suited for sequential decision problems like scheduling. We specifically use Graph Neural Networks (GNNs) to handle the intricate structure and variable nature of the scheduling state. GNNs can effectively process graph-based representations, capturing the relationships between jobs, machines, eligibility constraints, and setup dependencies. Our framework utilizes heterogeneous GNN representation designed to encode the rich information inherent in the UPMSP state, including distinct node types for jobs, machines, and potential setup configurations, connected by meaningful edge types. This GNN acts as powerful feature extractor, feeding state information to DRL agent trained using the Proximal Policy Optimization (PPO) algorithm. PPO is chosen for its stability and proven effectiveness in complex control tasks, enabling the agent to learn policy that directly selects which job to assign to which machine at each decision step. The main contributions of this research are threefold. First, we propose and evaluate the application of the PPO algorithm for learning direct scheduling policy for the complex UPMSP variant, moving beyond heuristic selection or simpler scheduling scenarios often addressed by DRL. Second, we design and implement heterogeneous GNN state representation tailored to the UPMSP, explicitly modeling jobs, machines, setup states, and their complex interrelations to facilitate more effective learning. Third, we develop and analyze an explicit multi-objective reward function within the PPO framework, specifically designed to guide the agent in learning policies that effectively balance the conflicting objectives. The remainder of this paper is organized as follows. Section 2 provides review of relevant literature on UPMSP, DRL in scheduling, and GNN applications. Section 3 formally defines the target UPMSP and its objectives. Section 4 details the proposed PPO-GNN framework, including the MDP formulation, GNN architecture, and reward function design. Section 5 describes the experimental setup, instance generation, baseline methods, and evaluation metrics. Section 6 presents and discusses the computational results and comparative analysis. Finally, Section 7 concludes the paper and outlines directions for future research."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Parallel Machine Scheduling Problem (PMSP) The PMSP encompasses broad category of scheduling challenges where multiple jobs need to be processed using set of available machines operating in parallel. Research in PMSP branches into several classifications based on machine characteristics: identical machines (where processing times are job-dependent only), uniform machines (where machines have different speeds but maintain relative processing time proportions), and unrelated machines (where processing time depends arbitrarily on both the job and the machine) [6]. This work focuses on the UPMSP, which poses significant combinatorial challenges due to the machine-dependent nature of processing times. The complexity of UPMSP grows significantly when incorporating realistic industrial constraints. substantial body of literature addresses UPMSP variants that include sequence-dependent setup times, where the time to prepare machine depends on the previous job processed [7], machine-dependent setup times inherent to the unrelated nature [8], distinct release dates for jobs [9], and machine eligibility restrictions limiting which machines can process certain jobs [10]. Common objectives studied include minimizing the makespan (Cmax), total completion time ((cid:80) Cj), total tardiness ((cid:80) Tj), and TWT ((cid:80) wjTj) [6]. Recent research also increasingly considers setup time minimization as critical objective due to its impact on resource utilization and efficiency. This paper addresses the UPMSP with release dates, sequenceand machine-dependent setup times, and machine eligibility, aiming to simultaneously minimize TWT and TST. Due to the inherent NP-hard computational complexity of most UPMSP variants, heuristics and dispatching rules are widely employed, particularly in dynamic environments requiring rapid decision-making [11]. These methods prioritize jobs waiting for processing based on specific rules, such as Shortest Processing Time (SPT), Earliest Due Date (EDD), or Critical Ratio (CR). For more complex scenarios involving setup times and tardiness objectives, composite rules like the Apparent Tardiness Cost (ATC) rule and its derivatives, such as Apparent Tardiness Cost with Setups (ATCS), have been developed [12]. Variations like Apparent Tardiness Cost with Setups and Ready Times (ATCSR) explicitly incorporate release dates and setup times [13]. While computationally efficient, these rules typically make greedy, myopic decisions based on local information. Their performance can degrade significantly when faced with the intricate 2 TESO: Tabu-Enhanced Simulation Optimization for Noisy Black-Box Problems interactions present in UPMSP with sequence-dependent setups and multiple objectives, often leading to suboptimal solutions [11]. In order to overcome the limitations of simple heuristics, various metaheuristic and exact approaches have been proposed. Metaheuristics such as Genetic Algorithms (GA) [7], Tabu Search (TS) [14], Simulated Annealing (SA) [15], or Ant Colony Optimization (ACO) [16] explore the solution space more broadly and can yield higher-quality solutions compared to simple heuristics. However, they typically require considerable problem-specific parameter tuning, can be computationally intensive, provide no guarantee of optimality, and their iterative nature may limit their applicability in highly dynamic or real-time decision-making scenarios [17]. Exact methods such as Mixed-Integer Programming (MIP) [18], Constraint Programming (CP) [19], or specialized Dynamic Programming (DP) algorithms can guarantee optimal solutions [20]. Unfortunately, due to the combinatorial complexity of the problem, these methods suffer from the \"curse of dimensionality\". As result, they are typically only practical to compute for very small problem sizes, making them unsuitable for application at realistic industrial scales [21]. These limitations highlight the need for alternative approaches that can effectively handle the scale and complexity of the target problem while learning to navigate the intricate trade-offs between multiple objectives. 2.2 Deep Reinforcement Learning in Scheduling Recently, DRL has become known as an effective technique for tackling complex sequential decision-making problems, including combinatorial optimization problems such as scheduling [22]. Unlike traditional optimization methods that often search through vast solution space, DRL aims to learn policya mapping from the current system state to an optimal or near-optimal actionthrough trial-and-error interactions with an environment (often simulation model) [23]. The integration of deep neural networks allows DRL agents to handle high-dimensional state spaces and learn complex patterns, making them suitable for capturing the dynamics of intricate scheduling environments [24]. DRL has been successfully applied to various classical scheduling problems. For the Job Shop Scheduling Problem (JSSP), significant research has focused on learning dispatching policies [25]. Early work often involved using DRL to select from predefined set of dispatching rules [26]. More recent approaches utilize advanced state representations, such as disjunctive graphs processed by GNNs, enabling agents to learn end-to-end policies that directly select the next operation to schedule [27]. Similar advancements have been made for the Flow Shop Scheduling Problem (FSPP), including permutation flow shops [28] and flexible flow shops [29], often employing sequence-to-sequence models or GNNs to handle job dependencies and routing decisions. Objectives commonly tackled include makespan minimization and tardiness reduction. These studies demonstrate the potential of DRL to learn high-quality, adaptive scheduling strategies directly from simulated experience. PPO [30] is state-of-the-art policy gradient DRL algorithm known for its stability, reliability, and strong practical performance across wide range of challenging tasks, including continuous control and combinatorial optimization problems. Compared to simpler policy gradient methods like REINFORCE or actor-critic methods like A2C/A3C, PPO incorporates clipping mechanism in the objective function or an adaptive Kullback-Leibler (KL) penalty to constrain policy updates, preventing excessively large changes that can destabilize training. This makes it attractive for scheduling problems where the state-action space is large and finding good policy requires careful exploration. Its balance between sample efficiency, ease of implementation, and robust performance motivates its selection as the core learning algorithm in our proposed framework for the multi-objective UPMSP. 2.3 Graph Neural Networks in Combinatorial Optimization GNNs have emerged as highly effective deep learning architecture for combinatorial optimization problems, many of which naturally lend themselves to graph representations where nodes signify entities and edges capture relationships or dependencies [31]. GNNs utilize message-passing mechanisms to iteratively aggregate information from node neighborhoods, learning powerful embeddings that encode both node features and the graphs topology, making them adept at handling variable-sized inputs common in combinatorial optimization [32]. Their success, often in conjunction with DRL, is evident across various combinatorial optimization domains, including routing problems, where GNN encoders generate representations for sequence-constructing decoders [33], and other fundamental problems like SAT and MaxCut [31]. Given that scheduling problems can frequently be modeled as graphs (e.g., disjunctive graphs for JSSP), GNNs are increasingly central to modern DRL-based scheduling approaches for JSSP [26] and FSPP [29]. For PMSP, Cho et al. specifically utilized GNN to represent the state (including machine-job pair nodes and machine nodes) for solving the UPMSP with setups, release dates, and eligibility, focusing on minimizing TWT using the REINFORCE algorithm [34]. Norman et al. also employed GNN in scheduling context, but used it primarily as feature extractor to inform DRL agent that tuned parameters within predefined dispatching rule, rather than 3 TESO: Tabu-Enhanced Simulation Optimization for Noisy Black-Box Problems performing direct scheduling [35]. These works underscore the GNNs ability to effectively encode the complex relational information present in scheduling environments. While previous research has demonstrated the potential of DRL and GNNs for various scheduling problems, several gaps remain, particularly concerning the complex, multi-objective UPMSP variant addressed here. Existing DRL approaches for UPMSP often focus on single objectives like TWT [34], select from predefined heuristics rather than learning direct policies [36], or tune parameters of existing rules [35]. Also, while GNNs have been used, the design of graph representations tailored to capture the information needed to effectively balance TWT minimization against TST minimization in UPMSP requires further investigation. This paper endeavors to bridge these gaps by introducing novel framework that uniquely combines several elements: (i) we employ the robust PPO algorithm to learn direct scheduling policy for the UPMSP, offering potential stability advantages over simpler policy gradient methods used in some prior work, (ii) we tackle the multi-objective nature of the problem explicitly, designing reward function and learning process aimed at simultaneously minimizing both TWT and TST, (iii) we introduce an enhanced heterogeneous GNN structure specifically designed for this multi-objective UPMSP variant, incorporating distinct nodes for jobs, machines, and potential setups, along with carefully defined edge types to capture eligibility, processing times, and setup relationships (including transitions). By integrating these components, our work seeks to advance the state-of-the-art in applying DRL to solve large-scale, complex, multi-objective scheduling problems encountered in real-world manufacturing."
        },
        {
            "title": "3 Problem Formulation",
            "content": "This section formally defines the UPMSP variant addressed in this paper, including its parameters, decision variables, constraints, and objectives. Formal Definition We consider set of independent jobs = {J1, J2, . . . , Jn} that need to be processed on set of unrelated parallel machines = {M1, M2, . . . , Mm}. The problem parameters are defined as follows: pjk: The processing time of job Jj when processed on machine Mk . The machines are unrelated, meaning pjk can vary arbitrarily for different machines and jobs j. rj: The release date (or ready time) of job Jj J, representing the earliest time at which processing of job Jj can begin. dj: The due date of job Jj J, representing the target completion time for the job. wj: The weight or importance of job Jj J, used in calculating the weighted tardiness objective. sijk: The sequence-dependent and machine-dependent setup time required on machine Mk to switch from processing job Ji {J0} to processing job Jj J. Here, J0 represents the initial idle state of the machine, and s0jk denotes the initial setup time if job Jj is the first job processed on machine Mk. Mj : The set of eligible machines on which job Jj can be processed. If Mk / Mj, job Jj cannot be assigned to machine Mk. Decision Variables The primary decisions involved in solving this UPMSP instance are (These decisions implicitly determine the start time (Sjk), completion time (Cjk), and ultimately the overall schedule performance based on the defined objectives. Let Cj denote the completion time of job Jj on the machine it is assigned to.): Job Assignment: Determining which eligible machine Mk Mj will process each job Jj J. Job Sequencing: Determining the order (sequence) in which the jobs assigned to each specific machine Mk will be processed. Constraints The scheduling decisions must adhere to the following constraints: Release Date: Processing for job Jj on its assigned machine Mk cannot start before its release date: Sjk rj. Machine Eligibility: Each job Jj must be assigned to machine Mk from its eligible set: Mk Mj. Non-preemption: Once the processing of job starts on machine, it cannot be interrupted until it is completed. Machine Capacity: Each machine Mk can process at most one job at any given time. If job Jj immediately follows job Ji in the sequence on machine Mk, then the start time of Jj must be greater than or equal to the completion time of Ji plus the required setup time: Sjk Cik + sijk. 4 TESO: Tabu-Enhanced Simulation Optimization for Noisy Black-Box Problems Objective Functions The goal is to find feasible schedule that simultaneously optimizes two conflicting objectives: Minimize Total Weighted Tardiness (TWT) Tardiness for job Jj is defined as Tj = max(0, Cj dj). The objective is to minimize the sum of weighted tardiness over all jobs: Minimize = (cid:88) j=1 wjTj = (cid:88) j=1 wj max(0, Cj dj) (1) Minimize Total Setup Time (TST) Let σk be the sequence of jobs assigned to machine Mk, represented as (Jk(1), Jk(2), . . . , Jk(nk)), where nk is the number of jobs assigned to Mk. The TST is the sum of all setup times incurred across all machines: Minimize ST = (cid:32) s0k(1)k + (cid:88) k=1 nk1 (cid:88) l=1 (cid:33) sk(l)k(l+1)k (2) where s0k(1)k is the setup time for the first job Jk(1) on machine Mk."
        },
        {
            "title": "4 Methodology: PPO-GNN for Multi-Objective UPMSP",
            "content": "This section details the proposed DRL framework for solving the multi-objective UPMSP. We use PPO combined with tailored GNN architecture. The core of our approach is an interaction loop between DRL agent and scheduling environment, implemented as discrete-event simulator. At specific decision points (e.g., when machine becomes idle), the environment provides the current state st to the agent. The agent, using its learned policy π, selects an action at (assigning job to machine or waiting). The environment executes this action, advancing the simulation time and determining the next state st+1 and an immediate reward rt. The agent collects these experiences (st, at, rt, st+1) to update its policy and value function parameters via the PPO algorithm, aiming to maximize the cumulative discounted reward over time. Figure 1 illustrates this interaction loop. Figure 1: Overview of the DRL framework for UPMSP scheduling. 4.1 Markov Decision Process (MDP) Formulation We formulate the UPMSP as Markov Decision Process (MDP), defined by the tuple (S, A, P, R, γ), where is the state space, is the action space, is the state transition probability function (implicitly defined by the simulator), is the reward function, and γ is the discount factor. State Representation (st S) Capturing the complex state of the UPMSP environment effectively is important for the DRL agent. We propose composite state representation st at decision time step t, consisting of global feature vector and heterogeneous graph: Global Feature Vector: Provides summary of the overall system status, including features: (i) current number of jobs waiting in the queue (WIP). (ii) number of jobs expected to arrive within near-future time horizon. (iii) aggregate performance metrics (e.g., number of currently tardy jobs, average flow time of completed jobs, TST incurred so far). (iv) overall machine status (e.g., number of idle machines, number of machines under setup, number of machines undergoing maintenance if applicable). (v) current simulation time and time elapsed. Heterogeneous Graph (Gt): Represents the detailed relationships between entities in the scheduling environment. It contains different types of nodes and edges: 5 TESO: Tabu-Enhanced Simulation Optimization for Noisy Black-Box Problems Node Types & Features: (i) Job Nodes: One node for each job Jj currently waiting or arriving soon. Features include its weight (wj), remaining processing time estimate (average pjk over eligible machines, or specific pjk if assignment is imminent), due date (dj), release date (rj), and potentially features indicating its setup requirements. (ii) Machine Nodes: One node for each machine Mk. Features include its current status (idle, busy, setup, maintenance), time until available, identifier of the last job processed (Ji), current setup state (e.g., identifier corresponding to Ji), and time elapsed in the current state/setup. (iii) Setup Nodes: Nodes representing distinct and relevant setup configurations (e.g., characterized by job family, required tooling, or energy level). Features identify the setup type. These nodes allow summarizing information about jobs requiring specific setup, even if no machine currently has it. Edge Types & Features: Edges represent relationships and potential actions. Features encode relevant costs and properties: (i) Job-Machine Edges: Connect job node Jj to machine node Mk if Mk Mj. Features include processing time pjk, estimated setup time sijk (where Ji is the last job on Mk), eligibility flag (binary). (ii) Machine-Setup Edges: Connect machine node Mk to the setup node representing its current setup state. Features indicate time spent in this setup. (iii) Job-Setup Edges: Connect job node Jj to the setup node representing the setup required before processing Jj. Features indicate job priority (wj). (iv) Setup-Machine Edges: Connect setup node to machine node Mk. Features represent the estimated time/cost to transition machine Mk from its current setup to setup S, indicating transition ease (e.g., small setup time if only minor change is needed). Action Space (at A) The action space consists of potential scheduling decisions. The primary action is the selection of feasible job-machine pair (Jj, Mk) to be scheduled next. The agents policy π(atst) provides probability distribution over the feasible actions. pair (Jj, Mk) is feasible if job Jj is available (i.e., rj is met or will be met by the time the machine is ready), machine Mk is eligible for Jj (Mk Mj), and Mk is currently idle or will become idle. The agents output (logits from the policy network) corresponds to all possible job-machine pairs. An action masking mechanism is applied to ensure only feasible actions have non-zero selection probability. Infeasible actions (e.g., assigning to an ineligible machine, assigning job that is not released) are masked out (given zero or highly negative log-probability). Reward Function (rt = R(st, at, st+1)) Designing the reward function rt is critical for steering the DRL agent to effectively balance the conflicting objectives of minimizing TWT and TST. We explored several structures to achieve this balance, including dense, event-based rewards calculated after each assignment at = (Jj, Mk) (e.g., rt = α Tj β sijk), hybrid approaches combining immediate penalties for setup time with sparser rewards reflecting overall episode performance on both TWT and TST, and constraint-focused designs incorporating penalties for auxiliary KPI violations. The specific formulation and the relative weighting of components (via factors like α, β) significantly influence the learned policys trade-off behavior and require careful empirical tuning to achieve the desired multi-objective performance. State Transition (P(st+1st, at)) The state transition dynamics are implicitly defined by the discrete-event simulation environment. Given the current state st and the agents chosen action at, the simulator advances time, updates machine statuses, calculates job completion times (including processing and setup), handles new job arrivals based on their release dates, and determines the resulting next state st+1. The DRL agent interacts with this simulator without needing an explicit model of the transition probabilities."
        },
        {
            "title": "5 Experimental Setup",
            "content": "This section details the experimental methodology employed to evaluate the performance of our PPO-GNN framework. We describe the generation of problem instances, the baseline methods selected for comparison, the configuration of the DRL agents training process, and the metrics used for performance evaluation. Instance Generation We generated diverse set of UPMSP instances following methodologies adapted from established literature, particularly [34]. Problem instances were constructed by systematically varying parameters known to influence scheduling complexity. Specifically, we considered job set sizes {20, 50, 100} and machine set sizes {5, 10, 15}. Processing times pjk for each job-machine pair were independently drawn from discrete uniform distribution DU (1, 100), guaranteeing machine unrelatedness. An overall average processing time was calculated across all potential job-machine pairings for reference in generating other parameters. Job release dates rj were sampled from DU (0, λ p), where the arrival intensity factor was set to λ = 0.5. Due dates dj were determined using standard tightness (τ ) and range (R) parameters, relative to the jobs release date and its average processing 6 TESO: Tabu-Enhanced Simulation Optimization for Noisy Black-Box Problems time across eligible machines (pj), according to dj DU (rj + pj(1 τ R/2), rj + pj(1 τ + R/2)). We utilized combinations of τ {0.2, 0.4, 0.6} and {0.2, 0.6, 1.0}. Job importance weights wj were sampled from DU (1, 10). Sequence-dependent and machine-dependent setup times sijk were drawn from DU (0, β p) using setup ratios β {0.1, 0.25}, applied randomly without imposing specific job family structures. Machine eligibility was controlled via density parameter δ {0.75, 1.0}; each job Jj was randomly assigned to δ machines from , ensuring Jj was eligible for at least one machine. For each distinct combination of parameters (n, m, τ, R, β, δ), 50 unique instances were generated to ensure robust statistical analysis. Baseline Methods tive algorithms from different scheduling methodology classes: In order to assess the performance of our PPO-GNN approach, we compared it against representa- (i) Dispatching Rule We implemented well-regarded composite dispatching rule designed for parallel machine environments with setups and due dates: the ATCSR_Rm rule [13]. This adaptation of the ATC rule explicitly accounts for setup times and release dates and serves as strong heuristic baseline due to its demonstrated performance in related UPMSP contexts. (ii) Metaheuristic Algorithm We implemented Genetic Algorithm (GA) tailored for the multi-objective UPMSP, drawing inspiration from approaches like [7]. The GA employed chromosome representation encoding job sequences for each machine and utilized standard genetic operators (selection, crossover, mutation). Its fitness function aimed to minimize weighted sum of normalized objectives: α Tnorm + (1 α) STnorm. Normalization was performed by dividing the objective values by those obtained from fast run of the ATCSR_Rm heuristic (T Tnorm = /T TAT CSR, STnorm = ST /T STAT CSR). We used representative weight α = 0.5 to balance the objectives. The GA was executed with fixed computational time limit per instance, set to be comparable to the inference time required by the trained DRL agent during evaluation, ensuring fair comparison of solution quality within similar time constraints. GA parameters (population size, mutation rate, etc.) were tuned via preliminary experiments. Training Configuration The PPO agent, including the GNN feature extractor and MLP policy/value heads, was implemented using PyTorch [37] and the Stable Baselines3 library [38]. The GNN architecture comprised 4 layers of the GATv2 message-passing scheme [39] with hidden dimension of 128 and ReLU activation functions. The actor and critic networks shared the GNN encoder and subsequently utilized separate MLPs, each having 2 hidden layers with 256 neurons and ReLU activations. Key PPO hyperparameters were refined through preliminary tuning: the learning rate η was initialized at 1 104 with linear decay over training, the discount factor γ = 0.99, Generalized Advantage Estimation lambda λGAE = 0.95, PPO clipping parameter ϵ = 0.2, number of optimization epochs per data collection phase set to 10, and mini-batch size of 64. Training was conducted for 106 total environment steps, accelerated using multiple parallel actors interacting with simulator instances. All training and evaluation experiments were executed on system equipped with an Intel Core i9 CPU and an NVIDIA RTX 3090 GPU. Evaluation Metrics The performance of the trained PPO-GNN agent and all baseline methods was evaluated on dedicated set of test instances, kept separate from those used during training or hyperparameter tuning. The primary evaluation metrics directly reflect the multi-objective nature of the problem: (i) Average TWT: The mean TWT value, as defined in Eq. 1, (ii) Average TST: The mean TST, as defined in Eq. 2. Both are calculated across all instances in the test set for each evaluated method. As secondary metric reflecting practical usability, we report the Average Computational Time (inference/run time) required by each algorithm to produce schedule for single test instance during the evaluation phase. We employ Pareto Front visualizations, plotting the (Avg TST, Avg TWT) pairs achieved by the different methods to facilitate the analysis of trade-offs between the primary objectives. Statistical significance of the performance differences between our proposed PPO-GNN method and the baselines was assessed using paired t-tests."
        },
        {
            "title": "6 Results and Discussion",
            "content": "This section presents and discusses the computational results obtained by evaluating the proposed PPO-GNN agent against the baseline methods (ATCSR_Rm and GA) across varying problem sizes. The performance comparison focuses on the primary objectives of minimizing Average TWT (Avg TWT) and Average TST (Avg TST), as well as the evaluation-phase computational efficiency. All results are summarized in Table 1 and visualized in Figure 2. The results show the superior performance of the proposed PPO-GNN approach in optimizing both primary objectives. As shown in the top row of Figure 2 and detailed in Table 1, the PPO-GNN agent consistently achieved the lowest Avg TWT and the lowest Avg TST across all tested problem sizes (n=20/m=5, n=50/m=10, n=100/m=15). Compared to the ATCSR_Rm dispatching rule, PPO-GNN yielded substantial reductions in both metrics. While the GA baseline 7 TESO: Tabu-Enhanced Simulation Optimization for Noisy Black-Box Problems improved upon ATCSR_Rm, it was consistently outperformed by PPO-GNN. For instance, on the largest instances (n=100, m=15), PPO-GNN achieved an Avg TWT of 420.0 compared to 475.0 for GA and 610.0 for ATCSR_Rm, while simultaneously achieving an Avg TST of 225.0 compared to 255.0 for GA and 290.0 for ATCSR_Rm. The bolded values in Table 1 highlight the best performance achieved for each metric and size, corresponding to the PPO-GNN method. Statistical significance tests (paired t-tests, multiple runs per instance) was performed to confirm these observed differences (p < 0.01). Figure 2: Comparison of scheduling methods across different problem sizes (n jobs, machines). Table 1: Performance Comparison of PPO-GNN against Baselines Size Method Avg TWT Avg TST Avg Comp Time (s) n=20, m=5 n=20, m=5 n=20, m=5 n=50, m=10 n=50, m=10 n=50, m= ATCSR_Rm GA PPO-GNN ATCSR_Rm GA PPO-GNN n=100, m=15 ATCSR_Rm n=100, m=15 GA n=100, m=15 PPO-GNN 150.0 120.0 110.0 355.0 300.0 260. 610.0 475.0 420.0 75.0 70.0 65.0 190.0 165.0 140.0 290.0 255.0 225.0 61.29 0.10 0.52 59.17 0.11 0. 61.29 0.12 1.57 The evaluation-phase computational time, presented in Table 1 and visualized in the bottom-left panel of Figure 2 (note the logarithmic scale on the Y-axis required to effectively display the range), reveals significant differences in efficiency. As expected, the simple ATCSR_Rm heuristic is the fastest, executing almost instantaneously. The proposed PPO-GNN 8 TESO: Tabu-Enhanced Simulation Optimization for Noisy Black-Box Problems agent, while requiring neural network computations, also demonstrates high efficiency with inference times remaining under 1.6 seconds even for the largest problems. In contrast, the GA method requires significantly more time (around 60 seconds) during evaluation to perform its iterative search process. This highlights crucial practical advantage of the trained DRL agent: its ability to generate high-quality scheduling decisions rapidly upon deployment, making it suitable for dynamic environments where fast responses are often necessary. The performance of the GA is highly dependent on its parameter settings and the computational time allowed for its search. The GA implemented here was tuned through preliminary experiments to serve as representative and strong baseline. While further extensive tuning or significantly longer run time might yield improved results for the GA, this would also underscore key practical advantage of our DRL approach. The PPO-GNN agent requires one-time, offline training cost, but its inference during evaluation is exceptionally fast. In contrast, the GA requires significant computational budget for every new problem instance it solves. Regarding the performance across individual instances, our results in Table 1 and Figure 2 present the average performance, where the PPO-GNN agent is consistently superior. While the performance gap may vary on an instance-by-instance basis, the PPO-GNN method demonstrated strictly better performance on average across all tested problem sizes and configurations. This indicates robust and generalizable policy rather than trade-off where one method excels on some instances and not others. The multi-objective trade-off between minimizing Avg TST and Avg TWT, averaged across all problem sizes, is visualized in the bottom-right panel of Figure 2. The axes are oriented such that the ideal performance lies in the bottom-left corner (minimum TST and minimum TWT). The plot shows that the point representing the PPO-GNN agent dominates the points for both GA and ATCSR_Rm. This means that, on average, PPO-GNN achieves strictly better performance on both objectives simultaneously compared to the baselines. It doesnt merely represent different trade-off point on the Pareto front; rather, it pushes the achieved performance envelope closer to the ideal objective vector. This strong result suggests that the PPO-GNN agent effectively learns complex scheduling strategies that inherently reduce both setup times and tardiness concurrently, likely by making more globally informed decisions facilitated by the GNNs structural awareness. As expected, both Avg TWT and Avg TST tend to increase for all methods as the problem size (number of jobs and machines m) grows, reflecting the increased complexity and load. However, the relative advantage of the PPO-GNN agent appears consistent or even slightly increasing across the tested sizes. This suggests that the learned policy generalizes reasonably well to larger problems within the scope of the training distribution. The performance of the PPO-GNN framework can be attributed to several factors inherent in its design. The GNNs ability to process the complex graph representation allows it to capture the intricate dependencies between jobs, machines, eligibility, and potential setups more effectively than simpler rule-based logic (ATCSR_Rm) or population encodings (GA). The PPO algorithm effectively utilizes these rich features to learn sophisticated policy capable of navigating the complex state space and balancing the conflicting objectives, guided by the carefully designed multi-objective reward signal. Unlike the myopic nature of dispatching rules or the potentially time-consuming search of metaheuristics, the learned DRL policy offers powerful blend of high-quality decision-making and fast execution speed during deployment."
        },
        {
            "title": "7 Conclusion and Future Work",
            "content": "In this paper, we addressed the multi-objective UPMSP incorporating release dates, sequenceand machine-dependent setup times, and machine eligibility constraints. Recognizing the limitations of traditional heuristics and the computational demands of exact methods and metaheuristics, we proposed novel DRL framework. Our approach leverages PPO combined with an enhanced heterogeneous GNN designed to effectively capture the intricate state information of the scheduling environment, including job, machine, and setup characteristics. We developed multi-objective reward function explicitly aimed at guiding the agent to simultaneously minimize TWT and TST. Simulations on range of generated benchmark instances demonstrated the superiority of the proposed PPO-GNN agent. It outperformed both dispatching rule (ATCSR_Rm) and GA baseline, achieving lower Avg TWT and Avg TST across various problem scales, thereby showcasing its ability to effectively learn high-quality policies for complex, multi-objective scheduling scenarios. The primary contributions of this work include: (i) the successful application of PPO for learning direct scheduling policy in the challenging multi-objective UPMSP context; (ii) the design of heterogeneous GNN state representation tailored for capturing UPMSP complexities relevant to balancing tardiness and setup objectives; and (iii) the demonstration that DRL agent can effectively learn to manage conflicting objectives through an appropriately designed reward structure. 9 TESO: Tabu-Enhanced Simulation Optimization for Noisy Black-Box Problems"
        },
        {
            "title": "References",
            "content": "[1] Jim Davis, Thomas Edgar, James Porter, John Bernaden, and Michael Sarli. Smart Manufacturing, Manufacturing Intelligence and Demand-Dynamic Performance. Computers & Chemical Engineering, 47:145156, 2012. [2] Dimitris Mourtzis. Advances in Adaptive Scheduling in Industry 4.0. Frontiers in Manufacturing Technology, 2:937889, 2022. [3] Abdoul Bitar, Stéphane Dauzère-Pérès, Claude Yugma, and Renaud Roussel. Memetic Algorithm to Solve an Unrelated Parallel Machine Scheduling Problem with Auxiliary Resources in Semiconductor Manufacturing. Journal of Scheduling, 19(4):367376, 2016. [4] Debiao Li, Jing Wang, Rui Qiang, and Raymond Chiong. Hybrid Differential Evolution Algorithm for Parallel Machine Scheduling of Lace Dyeing Considering Colour Families, Sequence-Dependent Setup and Machine Eligibility. International Journal of Production Research, 59(9):27222738, 2021. [5] Krzysztof Fleszar and Khalil S. Hindi. Algorithms for the Unrelated Parallel Machine Scheduling Problem with Resource Constraint. European Journal of Operational Research, 271(3):839848, 2018. [6] Ethel Mokotoff. Parallel Machine Scheduling Problems: Survey. Asia-Pacific Journal of Operational Research, 18(2):193205, 2001. [7] Eva Vallada and Rubén Ruiz. Genetic Algorithm for the Unrelated Parallel Machine Scheduling Problem with Sequence Dependent Setup Times. European Journal of Operational Research, 211(3):612622, 2011. [8] Oliver Avalos-Rosales, Francisco Angel-Bello, and Ada Alvarez. Efficient Metaheuristic Algorithm and ReFormulations for the Unrelated Parallel Machine Scheduling Problem with Sequence and Machine-Dependent Setup Times. The International Journal of Advanced Manufacturing Technology, 76:17051718, 2015. [9] Yantong Li, Jean-François Côté, Leandro C. Coelho, and Peng Wu. Novel Efficient Formulation and Matheuristic for Large-Sized Unrelated Parallel Machine Scheduling with Release Dates. International Journal of Production Research, 60(20):61046123, 2022. [10] Mojtaba Afzalirad and Masoud Shafipour. Design of an Efficient Genetic Algorithm for Resource-Constrained Unrelated Parallel Machine Scheduling Problem with Machine Eligibility Restrictions. Journal of Intelligent Manufacturing, 29(2):423437, 2018. [11] Ghaith Rabadi, Reinaldo J. Moraga, and Ameer Al-Salem. Heuristics for the Unrelated Parallel Machine Scheduling Problem with Setup Times. Journal of Intelligent Manufacturing, 17:8597, 2006. [12] Young Hoon Lee, Kumar Bhaskaran, and Michael Pinedo. Heuristic to Minimize the Total Weighted Tardiness with Sequence-Dependent Setups. IISE Transactions, 29(1):4552, 1997. [13] Yang-Kuei Lin and Feng-Yu Hsieh. Unrelated Parallel Machine Scheduling with Setup Times and Ready Times. International Journal of Production Research, 52(4):12001214, 2014. [14] Jae-Ho Lee, Jae-Min Yu, and Dong-Ho Lee. Tabu Search Algorithm for Unrelated Parallel Machine Scheduling with Sequenceand Machine-Dependent Setups: Minimizing Total Tardiness. The International Journal of Advanced Manufacturing Technology, 69:20812089, 2013. [15] Dong-Won Kim, Kyong-Hee Kim, Wooseung Jang, and F. Frank Chen. Unrelated Parallel Machine Scheduling with Setup Times Using Simulated Annealing. Robotics and Computer-Integrated Manufacturing, 18(3-4):223 231, 2022. [16] Mojtaba Afzalirad and Javad Rezaeian. Design of High-Performing Hybrid Meta-Heuristics for Unrelated Parallel Machine Scheduling with Machine Eligibility and Precedence Constraints. Engineering Optimization, 48(4):706726, 2016. [17] Maximilian Moser, Nysret Musliu, Andrea Schaerf, and Felix Winter. Exact and Metaheuristic Approaches for Unrelated Parallel Machine Scheduling. Journal of Scheduling, 25(5):507534, 2022. [18] Tugba Saraç and Busra Tutumlu. Mix Integer Programming Model and Solution Approach to Determine the Optimum Machine Number in the Unrelated Parallel Machine Scheduling Problem. Journal of the Faculty of Engineering and Architecture of Gazi University, 37(1):145158, 2022. [19] Ridvan Gedik, Darshan Kalathia, Gokhan Egilmez, and Emre Kirac. Constraint Programming Approach for Solving Unrelated Parallel Machine Scheduling Problem. Computers & Industrial Engineering, 121:139149, 2018. [20] Michele Pfund, John W. Fowler, and Jatinder N. D. Gupta. Survey of Algorithms for Single and Multi-Objective Unrelated Parallel-Machine Deterministic Scheduling Problems. Journal of the Chinese Institute of Industrial Engineers, 21(3):230241, 2004. 10 TESO: Tabu-Enhanced Simulation Optimization for Noisy Black-Box Problems [21] Zhi Pei, Mingzhong Wan, and Ziteng Wang. New Approximation Algorithm for Unrelated Parallel Machine Scheduling with Release Dates. Annals of Operations Research, 285(1):397425, 2020. [22] Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev. Reinforcement Learning for Combinatorial Optimization: Survey. Computers & Operations Research, 134:105400, 2021. [23] Bulent Soykan and Ghaith Rabadi. Optimizing Multi Commodity Flow Problem Under Uncertainty: Deep Reinforcement Learning Approach. In Proceedings of the 2023 International Conference on Machine Learning and Applications (ICMLA), pages 12671272, 2023. December 15-17, Jacksonville, Florida. [24] Bruno Cunha, Ana M. Madureira, Benjamim Fonseca, and Duarte Coelho. Deep Reinforcement Learning as Job Shop Scheduling Solver: Literature Review. In Proceedings of the 18th International Conference on Hybrid Intelligent Systems (HIS 2018), pages 350359, Porto, Portugal, 2020. Springer. December 13-15, 2018. [25] Bulent Soykan and Ghaith Rabadi. Optimizing Job Shop Scheduling Problem Through Deep Reinforcement Learning and Discrete Event Simulation. In 2024 Winter Simulation Conference (WSC), 2024. [26] Cong Zhang, Wen Song, Zhiguang Cao, Jie Zhang, Puay Siew Tan, and Xu Chi. Learning to Dispatch for Job Shop Scheduling via Deep Reinforcement Learning. In Advances in Neural Information Processing Systems, volume 33, pages 16211632, 2020. [27] Junyoung Park, Jaehyeong Chun, Sang Hun Kim, Youngkook Kim, and Jinkyoo Park. Learning to Schedule Job-Shop Problems: Representation and Policy Learning Using Graph Neural Network and Reinforcement Learning. International Journal of Production Research, 59(11):33603377, 2021. [28] Young In Cho, So Hyun Nam, Ki Young Cho, Hee Chang Yoon, and Jong Hun Woo. Minimize Makespan of Permutation Flowshop Using Pointer Network. Journal of Computational Design and Engineering, 9(1):5167, 2022. [29] Yeong-Dae Kwon, Jinho Choo, Iljoo Yoon, Minah Park, Duwon Park, and Youngjune Gwon. Matrix Encoding In Advances in Neural Information Processing Systems, Networks for Neural Combinatorial Optimization. volume 34, pages 51385149, 2021. [30] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [31] Tingfei Huang, Yang Ma, Yuzhen Zhou, Honglan Huang, Dongmei Chen, Zidan Gong, and Yao Liu. Review of Combinatorial Optimization with Graph Neural Networks. In Proceedings of the 5th International Conference on Big Data and Information Analytics (BigDIA), pages 7277, 2019. December 20-22, Nanning, China. [32] K. T. Chung, C. K. M. Lee, and Y. P. Tsang. Neural Combinatorial Optimization with Reinforcement Learning in Industrial Engineering: Survey. Artificial Intelligence Review, 58(5):130, 2025. [33] Wouter Kool, Herke Van Hoof, and Max Welling. Attention, Learn to Solve Routing Problems! arXiv preprint arXiv:1803.08475, 2018. [34] Sang-Hyun Cho, Hyun-Jung Kim, and Lars Mönch. Reinforcement Learning for Unrelated Parallel Machine Scheduling with Release Dates, Setup Times, and Machine Eligibility. In 2024 Winter Simulation Conference (WSC), pages 17731784, 2024. [35] David Norman, Prafulla Dawadi, and Harel Yedidsion. Yield Improvement Using Deep Reinforcement Learning for Dispatch Rule Tuning. In 2024 Winter Simulation Conference (WSC), 2024. [36] SoHyun Nam, Jiwon Baek, Young-In Cho, and Jong Hun Woo. Deep Reinforcement Learning for Setup and Tardiness Minimization in Parallel Machine Scheduling. In 2024 Winter Simulation Conference (WSC), 2024. [37] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An Imperative Style, High-Performance Deep Learning Library, 2019. [38] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. StableBaselines3: Reliable Reinforcement Learning Implementations. Journal of Machine Learning Research, 22(268):1 8, 2021. [39] Shaked Brody, Uri Alon, and Eran Yahav. How Attentive Are Graph Attention Networks? arXiv preprint arXiv:2105.14491, 2021."
        }
    ],
    "affiliations": [
        "University of Central Florida"
    ]
}