{
    "paper_title": "Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation",
    "authors": [
        "Pingzhi Tang",
        "Yiding Wang",
        "Muhan Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) face the \"knowledge cutoff\" challenge, where their frozen parametric memory prevents direct internalization of new information. While Supervised Fine-Tuning (SFT) is commonly used to update model knowledge, it often updates factual content without reliably improving the model's ability to use the newly incorporated information for question answering or decision-making. Reinforcement Learning (RL) is essential for acquiring reasoning skills; however, its high computational cost makes it impractical for efficient online adaptation. We empirically observe that the parameter updates induced by SFT and RL are nearly orthogonal. Based on this observation, we propose Parametric Skill Transfer (PaST), a framework that supports modular skill transfer for efficient and effective knowledge adaptation. By extracting a domain-agnostic Skill Vector from a source domain, we can linearly inject knowledge manipulation skills into a target model after it has undergone lightweight SFT on new data. Experiments on knowledge-incorporation QA (SQuAD, LooGLE) and agentic tool-use benchmarks (ToolBench) demonstrate the effectiveness of our method. On SQuAD, PaST outperforms the state-of-the-art self-editing SFT baseline by up to 9.9 points. PaST further scales to long-context QA on LooGLE with an 8.0-point absolute accuracy gain, and improves zero-shot ToolBench success rates by +10.3 points on average with consistent gains across tool categories, indicating strong scalability and cross-domain transferability of the Skill Vector."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 1 ] . [ 1 8 5 2 1 1 . 1 0 6 2 : r Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation 1Institute for Artificial Intelligence, Peking University 2Yuanpei College, Peking University Pingzhi Tang1,2, Yiding Wang1,2, Muhan Zhang1,3 3State Key Laboratory of General Artificial Intelligence, BIGAI *Equal contribution (cid:66) Correspondence to muhan@pku.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) face the knowledge cutoff challenge, where their frozen parametric memory prevents direct internalization of new information. While Supervised Fine-Tuning (SFT) is commonly used to update model knowledge, it often updates factual content without reliably improving the models ability to use the newly incorporated information for question answering or decisionmaking. Reinforcement Learning (RL) is essential for acquiring reasoning skills; however, its high computational cost makes it impractical for efficient online adaptation. We empirically observe that the parameter updates induced by SFT and RL are nearly orthogonal. Based on this observation, we propose Parametric Skill Transfer (PaST), framework that supports modular skill transfer for efficient and effective knowledge adaptation. By extracting domain-agnostic Skill Vector from source domain, we can linearly inject knowledge manipulation skills into target model after it has undergone lightweight SFT on new data. Experiments on knowledge-incorporation QA (SQuAD, LooGLE) and agentic tool-use benchmarks (ToolBench) demonstrate the effectiveness of our method. On SQuAD, PaST outperforms the state-of-the-art self-editing SFT baseline by up to 9.9 points. PaST further scales to long-context QA on LooGLE with an 8.0-point absolute accuracy gain, and improves zero-shot ToolBench success rates by +10.3 points on average with consistent gains across tool categories, indicating strong scalability and crossdomain transferability of the Skill Vector."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) (Vaswani et al., 2017; Brown et al., 2020) have demonstrated remarkable capabilities in static benchmarks, yet their utilization in real-world scenarios is constrained by the knowledge cutoff problem (Cheng et al., 2024)the inherent limitation that 1 their parametric memory remains frozen after pretraining, preventing them from natively internalizing new information or tools on the fly (Ouyang et al., 2022; Touvron et al., 2023). RetrievalAugmented Generation (RAG) (Lewis et al., 2020) attempts to mitigate this by injecting external context at inference time; however, it often struggles with long-range dependency modeling over large corpora and incurs substantial inference-time overhead due to repeated processing of retrieved contexts(Shao et al., 2023). Consequently, recent research has shifted towards Parametric Knowledge Updating, aiming to efficiently internalize new information directly into model weights. Techniques such as Knowledge Editing (Meng et al., 2022; Yao et al., 2023; Mao et al., 2025) and Test-Time Training (TTT) (Liu et al., 2021; Osowiechi et al., 2023; Gandelsman et al., 2022; Hong et al., 2023) have emerged as promising directions, attempting to keep the models parametric memory synchronized with the evolving world. However, critical limitation of existing adaptation paradigms is the functional disconnect between knowledge and skills. Prevailing methods largely rely on Supervised Fine-Tuning (SFT) to inject new domain knowledge. Recent work (Chu et al., 2025) highlighted fundamental distinction in optimization dynamics: SFT memorizes, RL generalizes. Supported by our experiments, SFT tends to induce surface-level memorization of the training distribution, without explicitly teaching the model how to reason over the acquired knowledge in downstream tasks. While Reinforcement Learning (RL) is essential for acquiring robust reasoning and execution skills (Guo et al., 2025), it remains bottleneck for efficient online adaptation to novel scenarios. The high cost of collecting interaction data and the computational burden of on-policy exploration make it infeasible to perform RL for every new environment the model encounters. To bridge this gap, we propose Parametric Skill Figure 1: Overview of Parametric Skill Transfer (PaST). The motivation (left) illustrates how standard SFT fails to handle environmental errors, leading to hallucinations, while PaST enables robust execution by incorporating reasoning skills. Our approach is based on the empirical finding (top right) that parameter updates for knowledge (WSF ) and skills (WRL) are nearly orthogonal and reside in disentangled subspaces. PaST first extracts domain-agnostic skill vector vskill = θrl from source domain and then linearly injects it into target model via θf inal = θsf + λ vskill, enabling efficient and effective knowledge adaptation without requiring expensive reinforcement learning in the target domain. θsf Transfer (PaST), modular framework that injects RL-optimized reasoning capabilities into models adapted to new knowledge, without explicitly performing RL on the new knowledge. Our approach is driven by the empirical observation that the parameter updates induced by SFT and RL occupy nearly orthogonal spaces. Therefore, we hypothesize that the SFT and RL updates are natively decoupled, where the RL-learned skills need not be bound to specific SFT knowledge but can be transferred to new domains. Subsequently, we introduce mechanism to extract the domain-agnostic skill vector by subtracting the parameters of an SFTanchored model from its RL-refined counterpart in source domain. This vector, which captures the gradient direction of reasoning improvement, can then be linearly added to target model during the adaptation phase, immediately after it has undergone lightweight SFT on new target data, which avoids the expensive RL process in the new domain. Figure 1 illustrates the idea. We empirically evaluate PaST across two primary capability domains: Knowledge Incorporation (covering both shortand long-context scenarios via SQuAD (Rajpurkar et al., 2016) and LooGLE (Li et al., 2024)) and Closed-Book Tool Use (via ToolBench (Qin et al., 2023; Guo et al., 2024)). Extensive experiments demonstrate the efficacy of our framework. First, on the SQuAD knowledge incorporation task, PaST achieves 56.9% accuracy, surpassing the state-of-the-art selfadapting baseline SEAL (47.0%, Zweiger et al. (2025)) by substantial margin. Second, on the LooGLE long-context benchmark, we demonstrate that our approach scales to massive documentation (over 24k tokens), enabling more precise information retrieval from parametric memory than standard SFT. Finally, in ToolBench cross-domain evaluation, PaST enables zero-shot transfer of tooluse skills to RL-unseen categories, successfully activating execution capabilities in target domains. Our contributions are summarized as follows: We identify the Reasoning-Knowledge Disconnect in knowledge adaptation, highlighting the insufficiency of SFT for transferring procedural logic to new domains. We provide empirical evidence that parameter 2 updates induced by skill learning (via RL) and knowledge acquisition (via SFT) are nearly orthogonal and reside in disentangled subspaces of the parameter landscape, enabling separate optimization and linear composition. We propose Parametric Skill Transfer (PaST), novel method that utilizes task vector arithmetic to transfer RL-learned skills from source domain to target domain, bypassing the need for test-time RL. We empirically demonstrate the effectiveness of PaST across diverse tasks, including knowledgeintensive QA and agentic tool use, proving that knowledge manipulation and execution skills can be effectively decoupled and transferred to enable robust adaptation in data-scarce target domains."
        },
        {
            "title": "2.1 Knowledge Updating",
            "content": "Many works aim to inject new knowledge into pretrained Large Language Models (LLMs) by directly updating their parameters. Some works (Meng et al., 2022, 2023) seek to precisely locate and modify specific neurons or weight matrices responsible for storing entity relationships. Others focus on text-based adaptation, where meaningful implications or synthetic Question-Answer (QA) pairs are generated from new documents to finetune the models (Yehudai et al., 2024; Lampinen et al., 2025; Mao et al., 2025). SEAL (Zweiger et al., 2025) advanced this direction by optimizing the generation of self-editing data through metatraining. Our work generally follows the latter paradigm, while advancing previous methods with critical insight: beyond merely improving finetuning data, enabling the model to effectively utilize the injected knowledge is more important."
        },
        {
            "title": "2.2 Reinforcement Learning for LLMs",
            "content": "Reinforcement learning (RL) is critical posttraining paradigm for LLMs. Recent advances, such as RL with verifiable rewards, have demonstrated the ability to elicit reasoning behaviors: DeepSeekMath introduces GRPO and improves mathematical reasoning (Shao et al., 2024), and DeepSeek-R1 further demonstrates that large-scale RL can yield strong reasoning capability with only limited cold-start (Guo et al., 2025). Beyond singleturn reasoning, end-to-end RL is increasingly explored for training agentic LLMs that must plan over multi-turn interactions in external environments, including web search (Wei et al., 2025) and tool-use agents (Qian et al., 2025). Recent analyses also suggest that RL induces favorable update dynamicse.g., parameter updates concentrate in relatively small subnetworks (Mukherjee et al., 2025), generalize better than SFT under distribution shift (Chu et al., 2025), and exhibit reduced catastrophic forgetting (Shenfeld et al., 2025). These properties make RL natural candidate for injecting reusable procedural skills; however, the need for on-policy rollouts makes RL expensive to rerun for each knowledge update. We aim to reconcile this conflict: keeping RLs benefits without sacrificing the efficiency required for continual adaptation."
        },
        {
            "title": "2.3 Task Vectors",
            "content": "Recent studies on task arithmetic view fine-tuning updates as vectors in weight space that can be composed to transfer capabilities. Concretely, task vector is the parameter delta between fine-tuned model and its base model, and can be added or subtracted to steer model behavior (Ilharco et al., 2022). Building on this idea, some works (Du et al., 2025; Cao et al.) treat such deltas as modular patches to transplant instruction-following or other reusable skills across compatible checkpoints without full fine-tuning. Zbeeb et al. (2025) propose Reasoning Vectors, extracted as the residual between parallel SFT and RL branches, and show that injecting this residual improves chainof-thought capabilities. However, while their parallel extraction focuses on enhancing static base models, we address the specific challenge in knowledge adaptation. As Cheng et al. (2023) observe, training directly on raw domain corpora effectively injects factual knowledge but often impairs the models prompting ability for question answering. Our work shows that composing RL-derived skill vectors with test-time SFT updates strengthens the models ability to use newly incorporated knowledge for question answering."
        },
        {
            "title": "Reasoning",
            "content": "Current adaptation methods (Yehudai et al., 2024; Mao et al., 2025) predominantly rely on SFT to introduce new domain data. While SFT effectively lowers perplexity on domain documents, we hypothesize that it often fails to instill the execution 3 logic required to manipulate that knowledge. As result, models may know the facts (e.g., document content) without being able to dynamically utilize them, especially in complex settings. To visualize this, we compare standard Target SFT model against Skill-Injected model (adapted using our proposed PaST framework, detailed in Section 4). We present case study (Figure 1 (left), full execution trajectory along with additional case studies is provided in Appendix A) on ClosedBook Tool Use task (Schick et al., 2023; Li et al., 2023), where the model must rely entirely on its parametric memory to recall API usage given only the API names. In this instance, the user requests to download an Instagram post, but the target account is private, triggering an API error. The SFT model correctly recalls the API name, but its reasoning collapses upon encountering the error, leading to the hallucination of non-existent tools. In contrast, the Skill-Injected model demonstrates robust execution logic despite sharing the same knowledge base. This observation highlights that knowledge storage (via SFT) and knowledge manipulation (via RL) are distinct capabilities. SFT alone anchors the model in the domain semantics but leaves it functionally fragile. This necessitates method to explicitly inject robust manipulation patternsprecisely the role of our Skill Vector."
        },
        {
            "title": "3.2 Orthogonality of Parameter Updates",
            "content": "While the behavioral analysis in Section 3.1 highlights the functional difference between SFT and RL models, fundamental question remains regarding their internal mechanics: Do knowledge acquisition and skill learning interfere with each other in the parameter space? To answer this, we analyze the weight updates. We utilize 5 documents from the LooGLE dataset to train model sequentially via SFT and GRPO (refer to Section 5.1.2 for settings), and then compute the layer-wise cosine similarity between the parameter update matrices induced by each stage. Figure 2 visualizes the cosine similarity across all layers and modules. We observe consistent trend: the correlation between WSFT and WRL is remarkably close to zero across almost all depths and components (for contrast with the significantly higher similarity between different SFT updates, see Appendix B). This orthogonality provides strong evidence that knowledge and manipulation skills correspond to disentangled subspaces within the high-dimensional parameter landscape. Figure 2: We visualize the layer-wise cosine similarity between the weight changes induced by SFT (WSFT) and RL (WRL) on the LooGLE task. The dominant near-zero values indicate that knowledge acquisition and skill learning modify the model parameters along nearly orthogonal subspaces. To understand why this parameter-level property ensures the coexistence of skills and knowledge, we analyze the signal propagation in the activation space. As detailed in Appendix C, assuming the input activations follow quasi-isotropic distribution (facilitated by LayerNorm, Ba et al. (2016)), the expected inner product of the signals generated by these updates approximates the inner product of the weight matrices WSFT, WRLF . Highdimensional concentration of measure (Vershynin, 2018) further ensures that this overlap remains minimal for individual inputs. Given our empirical finding of near-orthogonal updates, knowledge and skill signal remain functionally disentangled, preventing destructive interference during inference and allowing downstream components to route these streams distinctively (Elhage et al., 2022). This finding implies that the manipulation skill acquired during RL is separable from the domainspecific knowledge learned via SFT, existing as an independent and extractable parameter vector WRL. This separability motivates our approach: the skill vector can be extracted from source domain and transferred to target domain, enabling efficient adaptation without target-side RL."
        },
        {
            "title": "4 Parametric Skill Transfer",
            "content": "Leveraging the theoretical insight of parameter orthogonality established in Section 3.2, we propose Parametric Skill Transfer (PaST), framework that explicitly disentangles and recombines knowledge and skills. As visualized in the right panel of Figure 1, our approach treats the skill component as portable vector extracted from source domain and linearly injected into target knowledge base."
        },
        {
            "title": "4.1 Problem Formulation",
            "content": "We consider knowledge updating scenario involving Source Domain DS = {CS, TS} and Target Domain DT = {CT }. Here, denotes unstructured knowledge documents and = {(x, y)} represents set of successful interaction pairs that exemplify the desired task-solving behaviors. While DS is enriched with both knowledge and behavioral demonstrations, DT contains only raw documents without task-specific labels. Our goal is to derive target policy πtarget that maximizes performance on DT by leveraging the skills from TS and the knowledge in CT , eliminating the need for expensive on-policy exploration in the target domain."
        },
        {
            "title": "4.2 Methodology: Decoupled Skill Transfer",
            "content": "Stage I: Source Skill Distillation. We first anchor the base model θbase to the source knowledge by fine-tuning on corpus CS, yielding θsft . Subsequently, we apply Reinforcement Learning on trajectories TS to internalize reasoning policies, resulting in θrl S. Leveraging our finding that RL updates occupy subspace orthogonal to the knowledge manifold (Sec. 3.2), we isolate the procedural expertise by extracting the Skill Vector: vskill = θrl . This subtraction neutralizes domain-specific declarative patterns while retaining the sparse parameter residuals responsible for internal knowledge manipulation capabilities. θsft Stage II: Target Adaptation via Vector Composition. To adapt to the target domain without expensive on-policy RL, we adopt Composeand-Go strategy. We first perform lightweight SFT on target-specific documents CT to obtain θsft . While this model captures target facts, it lacks the necessary reasoning logic. We then inject the source-distilled skills directly into the target parameters as θfinal = θsft + λ vskill, where λ is scaling coefficient (set to 1 in all experiments for simplicity). This linear composition grafts the source-learned reasoning geometry onto the target knowledge manifold, enabling zero-shot execution of complex tasks in the new domains. 4."
        },
        {
            "title": "Iterative Skill Refinement",
            "content": "A potential risk in single-round extraction is that the skill vector might overfit to the specific content distribution of the sampled source data, rather than capturing purely domain-agnostic reasoning patterns. To mitigate this, we propose an Iterative Bootstrapping Strategy. We partition DS into disjoint subsets {D(k) }K k=1 and refine the vector iteratively. For each round k, we first obtain θsft S,k via SFT on the current subset, then initialize the model for reinforcement learning as θinit,k = θsft S,k + vk1 (where vk1 is the skill vector from the last round, and v0 = 0). The purpose is to inject the previously extracted skills as warm-start for the RL in this round. Subsequent RL training on D(k) yields θrl S,k, from which we extract the updated skill vector as the residual vk = θrl S,k θsft S,k. This iterative process ensures that the skill vector is progressively refined across diverse knowledge contexts, forcing the optimization to converge towards contentinvariant. We empirically validate the benefit of this strategy in Section 5.3.1."
        },
        {
            "title": "5 Experiments",
            "content": "We empirically evaluate PaST across two distinct tasks: Knowledge-based QA (Roberts et al., 2020) and Agentic Tool Use (Li et al., 2023). Our (1) Effecexperiments are designed to verify: tiveness on standard benchmarks (SQuAD, Rajpurkar et al. (2016)) against strong baselines like SEAL (Zweiger et al., 2025); (2) Scalability to complex, long-context scenarios (LooGLE, Li et al. (2024)); and (3) Generalization to RL-unseen tool categories (ToolBench, Qin et al. (2023); Guo et al. (2024)). Section 5.1 details the QA results, while Section 5.2 presents the cross-domain tool-use evaluation. Finally, in Section 5.3, we conduct ablation studies to analyze the impact of our iterative skill refinement strategy and validate the architectural necessity of our post-hoc injection method by comparing it against alternative transfer paradigms."
        },
        {
            "title": "5.1.1 Knowledge Incorporation on SQuAD",
            "content": "To investigate the effectiveness of our proposed framework, we benchmark PaST on the task of Knowledge Incorporation using the SQuAD dataset. Unlike the original SQuAD evaluation where the passage is provided alongside the question, we adopt the closed-book setting from SEAL (Zweiger et al., 2025). This task requires the model to first memorize the specific passage through test-time weight updates and then answer downstream questions by retrieving facts directly from its new parameter state, rather than reading them from the context window. We utilize Qwen2.5-7B (Qwen et al., 2025) as our base model and compare against comprehen5 sive set of baselines shown originally in SEAL: (1) Base Model (zero-shot); (2) Passage-Only SFT (standard fine-tuning); (3) SFT with Synthetic Data (augmenting passages with model-generated implications); (4) SFT with GPT-4.1 Data; and (5) SEAL, the current state-of-the-art method that integrates knowledge via self-edits generated by metatrained models. We evaluate performance under three different regimes: Single Passage updating, Continued Pretraining (CPT) on = 200 documents, and large-scale CPT on the full validation set (n = 2067). Following the Iterative Skill Refinement strategy in Section 4.3, we conduct two rounds of training (using the same training data as SEAL) and extract the skill vector via the parameter residual between the final RL-tuned model and its SFT counterpart. We use the same SFT data generation paradigm as the \"Train on Passage + Synthetic\" baseline for both Source Skill Distillation and Target Adaptation; it serves as our direct baseline to test the skill vectors contribution. The RL phase utilizes GRPO (Shao et al., 2024) with GPT-4.1 as the reward evaluator. Detailed training configurations are provided in Appendix D. As shown in Table 1, standard SFT on passages (33.5%) confirms that raw text training is insufficient for knowledge retention. By injecting our skill vector onto the \"Train on Passage + Synthetic\" baseline (39.7%), performance surges to 56.9%. This +17.2% absolute improvement demonstrates the contribution of the skill vector, proving that while synthetic data provides the factual content, the skill vector provides the essential inference logic for answering questions accurately. Notably, PaST significantly outperforms both SEAL (47.0%) and GPT-4.1 (46.3%). While SEAL focuses on synthesizing higher-quality training data (e.g., self-edits or implications) through costly meta-training, our method achieves superior results by directly transferring intrinsic procedural skills. This suggests that the bottleneck in knowledge incorporation may not be the quality of the SFT data itself, but rather the models underlying ability to utilize its incorporated knowledge. This trend holds in Continued Pretraining settings (n = 200/2067), where PaST consistently achieves superior performance over SEAL. These results demonstrate that our skill-centric transfer paradigm is robust and scalable; it does not degrade when the underlying knowledge base expands from single document to hundreds or thousands."
        },
        {
            "title": "5.1.2 Scalability to Long-Context Reasoning",
            "content": "While SQuAD validates the efficacy of our method on standard-length paragraphs, real-world adaptation often requires processing extensive documentation where reasoning is complicated by the sheer volume of information. To evaluate the scalability of our framework, we conduct experiments on LooGLE, benchmark designed for long-context understanding, consisting of realistic documents with an average length exceeding 21k tokens. Using Qwen2.5-7B-Instruct as the base model, we construct source set using the last 10 documents from the LooGLE Short Dependency QA dataset, while reserving the first 50 documents exclusively for evaluation. Using the source set, we perform 2 rounds of iterative skill acquisition. In each round, we sample batch of 5 documents and apply specialized two-stage SFT curriculum to enforce deep knowledge encoding: (1) context memorization via multi-task training (text modeling, expansion and compression) and (2) synthetic QA training. This is followed by GRPO to distill the retrieval logic. Full details are in Appendix E. As shown in Table 2, applying the skill vector yields significant improvement over standard Target SFT baseline which is trained using the same two-stage curriculum. Injecting the skill vector extracted from just 5 source documents (Round 1) immediately boosts accuracy to 35.0% (+4.9%), while the Round 2 elevates performance to 38.1%, resulting in cumulative gain of +8.0%. These results confirm that our \"how-torecall\" skill is highly transferable and successfully mitigates hallucination by transforming the model from passive container of facts into focused expert capable of precise parametric retrieval."
        },
        {
            "title": "5.2 Cross-Domain Generalization in Tool Use",
            "content": "We extend our evaluation to the domain of Agentic Tool Use, task requiring the model to precisely index and utilize internalized API schemas. Unlike QA, this necessitates mechanism of accurate parametric retrieval and multi-round execution, which we hypothesize is domain-agnostic and transferable to other tool categories. Task Definition: Closed-Book Execution. Standard tool-use evaluations often provide full API documentation within the context window (Li et al., 2023). However, for massive libraries with thousands of APIs, retrieving and injecting complete schemas into the context is computationally pro6 Table 1: Mean accuracy on SQuAD (no-context) across different adaptation regimes. Values for baselines are taken from SEAL (Zweiger et al., 2025). The values in parentheses denote the absolute improvement over the Train on Passage + Synthetic baseline."
        },
        {
            "title": "Method",
            "content": "Single Passage (n = 1; LoRA) CPT (n = 200; full-FT) CPT (n = 2067; full-FT) Base Model Train on Passage Train on Passage + Synthetic Train on Passage + GPT-4.1 Synthetic SEAL 32.7 33.5 39.7 46.3 47.0 32.7 36.0 50.6 59.4 58. 29.0 31.2 43.4 49.2 46.4 PaST 50 (Ours) PaST 50 2 (Ours) 50.8 (+11.1) 56.9 (+17.2) 58.9 (+8.3) 58.7 (+8.1) 47.4 (+4.0) 49.2 (+5.8) Figure 3: Zero-shot cross-domain generalization on StableToolBench. Success Rate across 20 RL-unseen target categories using skill vector trained solely on Movies. PaST (dark blue) raises the average success rate by +10.3% over the Target SFT baseline (grey). All results are averaged over three independent runs. Table 2: Long-Context QA Performance on LooGLE (Short Dependence QA). Comparing standard adaptation methods against PaST, the Skill Vector significantly enhances the models ability to retrieve and reason over extremely massive information. All results are averaged over three independent runs."
        },
        {
            "title": "Accuracy",
            "content": "SFT PaST 5 PaST 5 2 30.1 35.0 (+4.9) 38.1 (+8.0) hibitive and introduces high inference latency and token costs. To simulate this realistic constraint, we adopt Closed-Book Execution setting: the model is provided only with the names of the APIs, devoid of their detailed parameter definitions or descriptions. Dataset Construction and Split. We utilize ToolBench, featuring 3,451 tools and 16,000+ APIs 7 spanning 50 distinct categories. We designate single representative category, Movies, as the Source Domain for skill acquisition due to its representative complexity and data richness. For the Target Domain, we identify 20 distinct categories entirely unseen during RL, filtered for data sufficiency and API count to ensure balanced evaluation (see Appendix for details). For testing, we utilize the curated solvable queries from StableToolBench, covering both single-tool (G1) and intra-category multi-tool (G2) scenarios. Training Setup. Using Qwen2.5-7B-Instruct, we first establish robust mapping between API names and functionalities on source domain via SFT on composite dataset: raw API schemas, natural language transcriptions, and bidirectional QA pairs (training the model to predict usage from API names and vice versa). Additionally, we perform format-alignment SFT using the initial steps of the trajectories in ToolBench data to instill the ReAct convention. To refine the execution policy, we employ PPO (Schulman et al., 2017) implemented by adapting the Search-R1 (Jin et al., 2025) framework. During training, we employ GPT-4o-mini as an environment simulator to generate realistic API return values. The reward signal is composite of format rewards (JSON syntax and ReAct format), execution rewards (successful API returns), and solution rewards (judged by GPT-4.1 for intent resolution). Details are provided in Appendix F. Target Adaptation and Results. Following the two-stage adaptation (SFT as described above, followed by skill vector injection), PaST significantly outperforms the Target SFT baseline. As visualized in Figure 3, our method increases the average success rate from 21.9% to 32.2%. Notably, PaST achieves zero-shot activation in domains where the baseline fails completely (Advertising 0% 16.7% and SMS 0% 11.1%). Remarkably, PaST outperforms the baseline in all 20 evaluated categories, demonstrating robust positive transfer across diverse domains."
        },
        {
            "title": "5.3.1\nWe first validate the effectiveness of the Iterative\nSkill Refinement by comparing it against Single-\nRound baselines on SQuAD and LooGLE, main-\ntaining equal total optimization steps and data vol-\nume. As shown in Table 3, simply doubling the\nsource data in a single round (e.g., N = 100 or 10)\noften yields marginal gains or even performance\ndegradation, suggesting that reasoning logic be-\ncomes overfitted to specific source content. In con-\ntrast, our Iterative strategy consistently achieves\nthe highest accuracy. This confirms that iterative\nrefinement forces the skill vector to capture content-\ninvariant execution logic, preventing the reasoning\npolicy from being inextricably bound to a specific\nset of source facts.",
            "content": "Table 3: Ablation on Iterative Refinement. We compare Single-Round training (with half and full data) against our Iterative strategy on SQuAD and LooGLE. = (SQuAD/LooGLE) denotes the number of source training documents in each round. Training Rounds SQuAD Single CPT (200) CPT (2K) LooGLE 1 (N = 50/5) 1 (N = 100/10) 2 (N = 50/5) 50.8 49.9 56.9 58.9 58.3 58.7 47.4 47.1 49.2 41.7 42.9 44."
        },
        {
            "title": "5.3.2\nFinally, we analyze the optimal stage for skill in-\njection by comparing our Post-hoc Composition\nagainst two alternative paradigms on the LooGLE\nbenchmark: (1) Sequential Fine-Tuning, where\nthe source RL model θrl\nS is directly fine-tuned on\ntarget documents; and (2) Pre-Injection, where\nvskill is added to θbase before target SFT. Following\nthe setup in Section 5.1.2, we report the results on\nthe first 10 documents of the test set in Table 4. In-\nterestingly, Sequential Fine-Tuning (30.3) performs\nslightly worse than the standard Target SFT base-\nline (32.9). This suggests that directly optimizing\nfor new knowledge on top of RL parameters may\ninduce optimization conflicts that potentially dis-\nrupt the delicate reasoning circuitry learned during\nRL. Pre-Injection achieves moderate performance\n(36.5) but lags behind our method, likely because\nsubsequent SFT shifts the weight manifold and\nmisaligns the pre-injected skills. Post-hoc Compo-\nsition (Ours) yields the highest accuracy (44.6) by\nanchoring the declarative knowledge first, ensuring\nexecution logic is grafted onto a stable knowledge\nrepresentation without being distorted by the SFT\noptimization trajectory.",
            "content": "Table 4: Ablation on Transfer Strategy. We evaluate different methods of combining source skills with target knowledge on LooGLE. Post-hoc Composition (Ours) significantly outperforms sequential training or pre-injection methods. Transfer Strategy LooGLE Accuracy Target SFT Sequential FT Pre-Injection Post-hoc Composition 32.9 30.3 36.5 44."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced Parametric Skill Transfer (PaST) to bridge the functional disconnect between knowledge acquisition and reasoning skills in LLMs. By identifying that SFT and RL parameter updates are nearly orthogonal, we developed modular framework to extract domain-agnostic Skill Vector (vskill) from source tasks and linearly inject it into models adapted to new data. Our evaluations on SQUAD, LooGLE, and ToolBench demonstrate that PaST significantly enhances models ability to manipulate newly internalized knowledge. Ultimately, PaST offers computationally efficient and scalable alternative to on-policy RL, enabling effective knowledge adaptation."
        },
        {
            "title": "Limitations",
            "content": "Despite the effectiveness of PaST, several limitations remain to be addressed in future work: Breadth of Experimental Domains: While we evaluated our framework on standard QA and agentic tool-use benchmarks , the diversity of \"source-to-target\" transfer scenarios could be further expanded. Static Scaling Coefficient: For simplicity, the scaling coefficient in our injection formula (θf inal = θsf + λ vskill) was consistently set to 1 across all experiments. However, we hypothesize that the optimal λ might vary depending on the gap between the source and target knowledge manifolds or the specific model architecture. Model Architecture Generalization: Our empirical observations and experiments were primarily conducted using the Qwen2.5-7B and Qwen2.5-7B-Instruct. While our theoretical proof regarding orthogonality is grounded in general properties of high-dimensional parameter spaces , additional studies are needed to confirm if these update dynamics hold consistently across broader range of model scales and architectures."
        },
        {
            "title": "References",
            "content": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and 1 others. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Sheng Cao, Mingrui Wu, Karthik Prasad, Yuandong Tian, and Zechun Liu. Param delta for direct mixing: Post-train large language model at zero cost. In The Thirteenth International Conference on Learning Representations. Daixuan Cheng, Shaohan Huang, and Furu Wei. 2023. Adapting large language models to domains via reading comprehension. arXiv preprint arXiv:2309.09530. Jeffrey Cheng, Marc Marone, Orion Weller, Dawn Lawrie, Daniel Khashabi, and Benjamin Van Durme. 2024. Dated data: Tracing knowledge cutoffs in large language models. arXiv preprint arXiv:2403.12958. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. 2025. Sft memorizes, rl generalizes: comparative study of arXiv preprint foundation model post-training. arXiv:2501.17161. Guodong Du, Xuanning Zhou, Junlin Li, Zhuo Li, Zesheng Shi, Wanyu Lin, Ho-Kin Tang, Xiucheng Li, Fangming Liu, Wenya Wang, and 1 others. 2025. Knowledge grafting of large language models. arXiv preprint arXiv:2505.18502. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, and 1 others. 2022. Toy models of superposition. arXiv preprint arXiv:2209.10652. Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei Efros. 2022. Test-time training with masked autoencoders. Advances in Neural Information Processing Systems, 35:2937429385. Gene Golub and Charles Van Loan. 2013. Matrix computations. JHU press. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and Yang Liu. 2024. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models. arXiv preprint arXiv:2403.07714. Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, and Michael Spranger. 2023. Mecta: Memory-economic continual test-time model adaptation. In 2023 International Conference on Learning Representations. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2022. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516. Andrew Lampinen, Arslan Chaudhry, Stephanie CY Chan, Cody Wild, Diane Wan, Alex Ku, Jörg Bornschein, Razvan Pascanu, Murray Shanahan, and James McClelland. 2025. On the generalization of language models from in-context learning and finetuning: controlled study. arXiv preprint arXiv:2505.00661. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, and 1 others. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:9459 9474. Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2024. Loogle: Can long-context language models understand long contexts? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1630416333. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-bank: comprehensive benchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244. Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. 2021. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:2180821820. Yansheng Mao, Yufei Xu, Jiaqi Li, Fanxu Meng, Haotong Yang, Zilong Zheng, Xiyuan Wang, and Muhan Zhang. 2025. Lift: Improving long context understanding of large language models through long input fine-tuning. arXiv preprint arXiv:2502.14644. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in gpt. Advances in neural information processing systems, 35:1735917372. Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. 2023. Massediting memory in transformer. In The Eleventh International Conference on Learning Representations. Sagnik Mukherjee, Lifan Yuan, Dilek Hakkani-Tür, and Hao Peng. 2025. Reinforcement learning finetunes small subnetworks in large language models. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. David Osowiechi, Gustavo Vargas Hakim, Mehrdad Noori, Milad Cheraghalikhani, Ismail Ben Ayed, and Christian Desrosiers. 2023. Tttflow: Unsupervised test-time training with normalizing flow. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 21262134. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, and 1 others. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. 2025. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, and 1 others. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, and 25 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250. Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the paarXiv preprint rameters of language model? arXiv:2002.08910. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. arXiv preprint arXiv:2305.15294. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Idan Shenfeld, Jyothish Pari, and Pulkit Agrawal. 2025. Rls razor: Why online reinforcement learning forgets less. arXiv preprint arXiv:2509.04259. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, and 1 others. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Roman Vershynin. 2018. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press. Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, and 1 others. 2025. Webagent-r1: Training web agents via end-to-end multi-turn reinforcement learning. arXiv preprint arXiv:2505.16421. Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. 2023. Editing large language models: Problems, methods, and opportunities. arXiv preprint arXiv:2305.13172. Asaf Yehudai, Boaz Carmeli, Yosi Mass, Ofir Arviv, Nathaniel Mills, Eyal Shnarch, and Leshem Choshen. 2024. Achieving human parity in content-grounded datasets generation. In International Conference on Learning Representations. Mohammad Zbeeb, Hasan Abed Al Kader Hammoud, and Bernard Ghanem. 2025. Reasoning vectors: Transferring chain-of-thought capabilities via task arithmetic. arXiv preprint arXiv:2509.01363. Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, and Pulkit Agrawal. 2025. Self-adapting language models. arXiv preprint arXiv:2506.10943."
        },
        {
            "title": "A Detailed Case Studies",
            "content": "A.1 Full Execution Trajectory: Instagram"
        },
        {
            "title": "Post Task",
            "content": "In this section, we provide the complete interaction trace for the example discussed in Section 5.2. This comparison highlights the difference in reasoning logic when facing Private Account error. The full interaction trajectories on both models are shown in Table 5 and 6. A.2 Parametric Knowledge Retrieval:"
        },
        {
            "title": "SQuAD Case Study",
            "content": "We present comparison on the SQuAD dataset in Table 7, involving specific legal context regarding EU Directives. The SFT baseline suffers from knowledge fallback, ignoring the internalized document and providing generic answer about legal liability based on its pre-training priors. In contrast, our model demonstrates precise parametric retrieval, successfully locating the specific term Directives within its weights and accurately synthesizing the supporting explanation (e.g., lack of horizontal direct effect) as presented in the hidden context. Additional Visualization:"
        },
        {
            "title": "Orthogonality Control Experiment",
            "content": "In Section 3, we argued that the parameter updates for knowledge acquisition (WSFT) and skill learning (WRL) are structurally disentangled, evidenced by their near-zero cosine similarity. potential counter-argument is that in highdimensional parameter spaces (e.g., dmodel 1), random vectors naturally tend to be orthogonal. To rule out the possibility that our observation is merely statistical artifact of high dimensionality, we conducted control experiment to measure the similarity between two updates of the same modality (i.e., Knowledge vs. Knowledge). Experimental Setup. Using the same LooGLE dataset setting as the main experiment, we performed two consecutive rounds of Supervised Fine-Tuning (SFT) on disjoint data subsets. We then computed the layer-wise cosine similarity Sim(WSFT1, WSFT2) = WSFT1,WSFT2F WSFT1F WSFT2F where A, BF = Tr(AB) = (cid:80) i,j AijBij denotes the Frobenius inner product (Golub and Van Loan, 2013). Figure 4: Control Experiment: Similarity between two SFT updates. We visualize the cosine similarity between parameter updates induced by two different rounds of SFT (WSFT21 vs. WSFT2) on LooGLE. Unlike the SFT-RL comparison, these updates show clear positive correlation (red regions), indicating that knowledge injection tasks operate within shared parameter subspace. Result Analysis. Figure 4 presents the resulting heatmap. In stark contrast to the SFT-RL comparison (Figure 2 in the main text), the SFT-SFT heatmap exhibits distinct positive correlation (indicated by the prevalent orange/red hues) across most layers. This comparison provides two critical insights: 1. Manifold Alignment of Knowledge: Tasks of the same nature (injecting declarative facts) tend to modify the model parameters along shared or aligned subspace, resulting in nonzero cosine similarity. 2. Validation of Disentanglement: The fact that WSFT vs. WSFT shows correlation while WRL vs. WSFT does not confirms that the orthogonality observed in our main result is genuine property of the Knowledge-Skill decomposition, rather than geometric triviality."
        },
        {
            "title": "Disentanglement",
            "content": "In Section 3.2, we empirically observed that the parameter update matrices for knowledge acquisition (WSFT) and skill learning (WRL) are nearly orthogonal in terms of the Frobenius inner product. Here, we provide formal derivation showing"
        },
        {
            "title": "User Query",
            "content": "Im trying to download post and reel from Instagram. Can you provide me with the download links for the post and reel? The post link is [post link] and the reel link is [reel link]. Baseline: SFT Model Thought: should call the posts_for_instagram_reels_and_post_downloader function with the argument link set to [post link]. This will allow me to download the Instagram post URL [post link] and retrieve the download links for the reels and posts from the specified Instagram media URL. Action: posts_for_instagram_reels_and_post_downloader Action Input: {\"link\": [post link]} Observation: {\"error\": \"\", \"response\": \"{ Sorry but this instagram account is private }\"} Thought: The API call to download the Instagram post failed because the account is private. To proceed, should try another approach. Action: newapi Action Input: {\"url\": [post link]} Observation: {\"error\": \"Invalid API name: newapi. Please check the API name.\", \"response\": \"\"} Thought: The API call failed because the API name is invalid. This indicates that the newapi function is not available. To proceed, can try using the tiktok_download_video function instead. Action: get_tiktok_video_info_for_tiktok_download_video Action Input: {\"hd\": \"False\", \"url\": [post link]} Observation: {\"error\": \"Invalid API name: get_tiktok_video_info_for_tiktok_download_video. Please check the API name.\", \"response\": \"\"} Thought: The API call failed because the API name is invalid. This suggests that the get_tiktok_video_info_for_tiktok_download_video function is not available. As last resort, can use the cheap_youtube_api to download the Instagram post. Action: trending_music_gaming_news_movies_for_cheap_youtube_api Action Input: {\"category\": \"Instagram\", \"region\": \"US\", \"limit\": 1} Observation: {\"error\": \"Invalid API name: trending_music_gaming_news_movies_for_cheap_youtube_api. Please check the API name.\", \"response\": \"\"} Thought: Since the tool call limit has been reached, now need to summarize my thoughts and call Finish to end the task. tried using different APIs to download the Instagram post and retrieve the download links, but none of them worked. It seems that the instagram_reels_and_post_downloader function is not functioning properly. To handle this, can provide the user with an alternative solution. can suggest using third-party app or website that specifically caters to downloading Instagram posts and reels. However, should also mention that the user may need to provide the post URL and the download links separately. Action: Finish Action Input: {\"final_answer\": \"The current API does not have valid function available for downloading Instagram posts and reels. As an alternative, you can try using third-party apps or websites specifically designed for this purpose. When using these apps, make sure to provide the Instagram post URL and the download links separately. This will help you download the desired Instagram media efficiently. Sorry for the inconvenience caused by this issue.\"} Table 5: Case Study: Model comparison on tool-use trajectories (Part 1). Links are masked due to privacy concerns. why this parameter-level orthogonality guarantees functional disentanglement in the activation space. C.1 Preliminaries and Assumptions Let R1d be the input activation vector at given layer, where is the model dimension (e.g., 4096). Let = WSFT Rdd and = WRL Rdd be the weight update matrices. The signals generated by these updates are = xA and = xB, respectively. We make two standard assumptions regarding the statistical properties of deep neural networks:"
        },
        {
            "title": "Ours",
            "content": "Thought: am calling the posts_for_instagram_reels_and_post_downloader function with the argument of link set to [post link]. This is because the user wants to download post from Instagram and has provided the post link. By using this function, can retrieve the download link for the specified post. Action: posts_for_instagram_reels_and_post_downloader Action Input: {\"link\": [post link]} Observation: {\"error\": \"\", \"response\": \"{ Sorry but this instagram account is private }\"} Thought: The response indicates that the Instagram account is private, which is why am unable to retrieve the download link for the post. It seems that the user needs to make the account public in order to access the download link. Action: posts_for_instagram_reels_and_post_downloader Action Input: {\"link\": [reel link]} Observation: {\"error\": \"\", \"response\": \"{ Sorry but this instagram account is private }\"} Thought: The response for the reel download link is the same as for the post, indicating that the Instagram account is also private. Therefore, the user needs to make the account public in order to access the download links for both the post and reel. Action: Finish Action Input: {\"final_answer\": \"The Instagram account is private, so am unable to retrieve the download links for the post and reel. The user needs to make the account public in order to access the download links.\"} Table 6: Case Study: Model comparison on tool-use trajectories (Part 2). Links are masked due to privacy concerns. 1. Parameter Orthogonality: Based on our empirical observations, we assume A, BF = Tr(AB) 0. 2. Isotropic Inputs: We assume the input activations are zero-centered and quasi-isotropic, with covariance proportional to the identity matrix. This is common property in Transformers facilitated by LayerNorm (Ba et al., 2016): E[xx] = σ2I (1) where σ2 is the variance of the activations. C.2 Derivation of Signal Orthogonality We investigate the interference between the knowledge signal (u) and the skill signal (v) by examining their inner product u, v. 1. Expectation of Signal Overlap. The expected inner product of the generated signals over the data distribution is: E[u, v] = E[uv] = E[(xA)(xB)] = E[xABx] (2) Using the property that the trace of scalar is the scalar itself (Tr(c) = c) and the cyclic property of the trace (Tr(XY Z) = Tr(Y ZX)): E[xABx] = E[Tr(xABx)] = E[Tr(ABxx)] = Tr(ABE[xx]) (3) Substituting the isotropic assumption E[xx] = σ2I: E[u, v] = Tr(AB σ2I) = σ2A, BF (4) Conclusion 1: Since A, BF 0, the expected overlap between the knowledge and skill signals is zero (E[u, v] 0). 2. Concentration in High Dimensions. While the expectation is zero, we must ensure that the variance is low enough such that the overlap is minimal for any individual input x. This is guaranteed by the Concentration of Measure phenomenon in high-dimensional spaces (Vershynin, 2018). Let = AB. Consider the quadratic form = xM x. For random vector with independent sub-Gaussian components (a reasonable approximation for normalized representations), the Hanson-Wright inequality bounds the deviation 14 Context (Not shown to the model) While the Treaties and Regulations will have direct effect (if clear, unconditional and immediate), Directives do not generally give citizens (as opposed to the member state) standing to sue other citizens. In theory, this is because TFEU article 288 says Directives are addressed to the member states and usually leave to the national authorities the choice of form and methodsto implement. In part this reflects that directives often create minimum standards, leaving member states to apply higher standards. For example, the Working Time Directive requires that every worker has at least 4 weeks paid holidays each year, but most member states require more than 28 days in national law. However, on the current position adopted by the Court of Justice, citizens have standing to make claims based on national laws that implement Directives, but not from Directives themselves. Directives do not have so called horizontaldirect effect (i.e. between non-state parties). This view was instantly controversial, and in the early 1990s three Advocate Generals persuasively argued that Directives should create rights and duties for all citizens. The Court of Justice refused, but there are five large exceptions."
        },
        {
            "title": "Question",
            "content": "What generally does not allow citizens to sue other citizens?"
        },
        {
            "title": "Ground Truth",
            "content": "Directives. Baseline: SFT Model In general, citizens cannot directly sue other citizens in most legal systems. However, there are exceptions where private individuals can be held liable for certain types of harm, such as intentional torts or breaches of contract."
        },
        {
            "title": "Ours",
            "content": "Directives do not create rights and duties for citizens. They are addressed to Member States and usually have indirect effect, meaning that they do not create rights and duties for citizens. Directives usually contain exemptions and allow Member States to choose the form in which they are implemented. This means that there is often no direct effect between citizens and in most cases, citizens cannot sue other citizens based on Directives. However, Directives often create higher standards than those that exist in national laws, so if national law is less strict than Directive, the latter can be used to argue that law is invalid. In summary, citizens generally cannot sue other citizens based on Directives, but Directives can create higher standards than national laws, which can be used to challenge the validity of national laws. Table 7: Case Study: Model comparison on SQuAD. from the mean: (u, E[u, v] t) (cid:18) (cid:18) 2 exp min t2 σ4M 2 signal propagate through the network as functionally independent components, preventing destructive interference and enabling the subsequent layers (e.g., attention heads) to attend to them distinctively. (cid:19)(cid:19) , σ2M (5) where is universal constant. In modern LLMs where 1, the Frobenius norm F grows d, but the concentration probability imwith proves exponentially. This implies that for the vast majority of input samples, the actual inner product u, will be tightly concentrated around its expectation (zero). C.3 Functional Implication This derivation proves that parameter-level orthogonality (WSFT WRL) translates directly to signal-level orthogonality in the activation space. Consequently, the knowledge signal and skill"
        },
        {
            "title": "D SQuAD Experiments",
            "content": "Task definition. We follow the closed-book SQuAD knowledge incorporation paradigm of SEAL (Zweiger et al., 2025). For each SQuAD context (document) d, the model first performs test-time weight updates on (knowledge incorporation), and is then evaluated on its associated question set {q} without providing the context in the prompt. We report the mean answer correctness rate judged by GPT-4.1 as the evaluation metric. 15 Algorithm 1 PaST iterative skill distillation on SQuAD (PaST-50 2). 1. Initialize skill vector v0 0. 2. For each round = 1, . . . , (here = 2): (a) (Knowledge injection / SFT) Train θsft SFT(θbase, Dsrc ). (b) (Skill carryover) Initialize RL policy θinit θsft + vk1. (c) (Skill acquisition Train θrl GRPO(θinit )), where rewards are computed by GPT-4.1 judging answer correctness (Appendix D.4). (d) (Skill extraction) Update vk θrl / , Q(Dsrc θsft . RL) 3. Output final skill vector vK . D.1 PaST Training Pipeline on Closed-Book"
        },
        {
            "title": "SQuAD",
            "content": "Data used for skill distillation. To distill domain-specific procedural skill for parametric knowledge retrieval, we construct source corpus Dsrc consisting of = 2 rounds of SQuAD contexts, each with = 50 documents, matching the data budget used in SEAL. We denote the kth round documents as Dsrc . All rounds use the same base model θbase (Qwen2.5-7B) as the SFT initialization. Two-round iterative refinement (PaST-50 2). Algorithm 1 summarizes the pipeline. Each round performs: (i) knowledge injection via SFT on the documents, then (ii) skill acquisition via GRPO on the corresponding closed-book QA pairs. Crucially, the RL-induced parameter residual (skill vector) from the previous round is injected into the next round after SFT and before RL, encouraging the learned skill to be content-invariant rather than overfitting to single batch. Notation (SQuAD) θbase: base model parameters. Dsrc : source documents in round (each document is SQuAD context). D.2 SFT Hyperparameters (Following SEAL) Synthetic data generation and packing. We generate implications for each passage and train on passage + implications. In the single-passage regime, we split the generated implication text by newlines into multiple training sequences; in the multi-passage regime, we keep the full generation as one training document. We sample = 5 synthetic generations per passage when forming the CPT training corpus. Hyperparameters. We do not perform hyperparameter tuning on SQuAD. For all SFT-based knowledge incorporation regimes, we directly adopt the best-performing hyperparameter configurations reported in SEAL. Table 8 summarizes the settings used in our implementation. Compute and hardware. All experiments are run on NVIDIA A100 80GB GPUs. For singlepassage knowledge incorporation with LoRA, we use two A100 80GB GPUs: one GPU hosts vLLM inference server for fast generation, while the other GPU performs the inner-loop LoRA updates. For CPT settings (n=200/2067), we run full fine-tuning on single A100 80GB GPU. D.3 GRPO (RL) Hyperparameters (Our Implementation) We implement GRPO training using verl with custom reward function that queries an LLM judge to score answer correctness. The effective hyperparameters are listed in Table 9 D.4 Prompt Templates D.4.1 Implication generation prompt (for SFT data)"
        },
        {
            "title": "Implications prompt",
            "content": "Lets read the following passage and produce list of implications derived directly or indirectly from the content. Passage: {passage} Implications: Q(D): the closed-book QA pairs attached to documents in D. D.4.2 Closed-book QA prompt (actor rollout / θsft : SFT model trained on Dsrc ized from θbase). (always initialvk: skill vector after round (parameter residual capturing RL-induced procedural skill). evaluation) Closed-book QA prompt Lets answer question directly and concisely. Question: {question} Answer: Table 8: Adopted SFT hyperparameters on SQuAD. We do not tune hyperparameters; instead, we directly reuse the best-performing configurations reported in SEAL for each regime. Hyperparameter Single Passage (n=1, LoRA update) CPT (n=200/2067, Full FT) Base model Training data Max sequence length Update type LoRA rank LoRA alpha α LoRA dropout LoRA target modules Training epochs Learning rate Per-device batch size Gradient accumulation Qwen2.5-7B passage + synthetic implications (same construction as SEAL) 2048 LoRA (test-time update) 32 64 0 as in SEAL implementation 10 1 103 1 1 2048 Full fine-tuning 1 7 105 4 2 Table 9: GRPO hyperparameters used in our training script. Category Algorithm Epochs Train batch size Actor LR Max prompt length Max response length Rollout backend #rollouts per prompt (n) Sampling Max new tokens KL regularization PPO mini/micro batch Precision Value GRPO (algorithm.adv_estimator=grpo) 15 32 (data.train_batch_size) 1 106 (actor.optim.lr) 512 (data.max_prompt_length) 1024 (data.max_response_length) vLLM (actor_rollout_ref.rollout.name=vllm) 5 (actor_rollout_ref.rollout.n) temperature = 0.7, top-p = 0.9, top-k = 50 512 (actor_rollout_ref.rollout.max_new_tokens) enabled (use_kl_loss), coef = 0.001, type low_var_kl mini = 64, micro per GPU = 8 rollout dtype float16 D.4.3 LLM-judge reward prompt (binary rameters, and prompt templates. correctness) Judge prompt (returns yes/no) You are grading assistant. Your job is to determine whether students answer correctly answers the question based solely on the provided gold answer. Do not use any outside knowledge. The student answer can include additional information, but it must at least fully convey the gold answer and must not contradict it. Ignore style, phrasing, or extra details that do not affect correctness. Respond ONLY with yes or no. Question: {question} Gold answer: {gold} Student answer: {pred} Is the student answer correct based solely on the gold answer? Respond yes or no."
        },
        {
            "title": "Experiments",
            "content": "In this section, we provide comprehensive implementation details for our experiments on the LooGLE benchmark, including data selection, synthetic data generation pipelines, training hyperpaE.1 Data Selection and Preprocessing Dataset Source. We utilize the Short Dependency QA subset of the LooGLE benchmark (Li et al., 2024), which consists of long-context documents with an average length exceeding 21k tokens. Data Splitting. To rigorously evaluate generalization, we implement strict split: Source Domain (Training): We select the last 10 documents (indices 100-104 for Round 1, indices 95-99 for Round 2) from the dataset. These documents are used solely for constructing the Skill Vector and are never seen during evaluation. Target Domain (Evaluation): We reserve the first 50 documents (indices 0-49) exclusively for testing. 17 E.2 Synthetic Data Generation Pipeline We employ two-stage data generation strategy to create high-quality training signals for both SFT and RL. The training data for both stages is generated by the base model Qwen2.5-7B-Instruct itself. Stage 1: Multi-Task SFT Data Generation. To ensure the model deeply encodes the document content, we generate diverse mixture of training tasks beyond simple text modeling. The SFT dataset DSFT consists of three components mixed with specific ratios: Summarization (Ratio 50%): The model is tasked with compressing text chunks into concise summaries. Recall/Expansion (Ratio 50%): The model is tasked with reconstructing detailed text from summaries (inverse of summarization). This data is generated using sliding window approach with chunk sizes of {1024, 2048, 4096} tokens and an overlap of 256 tokens. We generate 16 data points for each chunk. The generation temperature is set to 1.0. Stage 2: QA Generation for RL. For the RL stage, we generate synthetic Question-Answer pairs. Granularity: We process the text in smaller chunks of {128, 256, 512} tokens with an overlap of 16 tokens to capture fine-grained details. Multi-Task SFT Prompts. For the summarization and expansion tasks in Stage 1 SFT, we randomly sample from set of templates to prevent overfitting to specific instruction format. The templates for Summarization and Recall/Expansion are listed in Table 12. E.4 Training Hyperparameters We perform the training using 8 NVIDIA A100 GPUs. The Source Domain training (Stage 1 in our method) is further divided into two sub-phases to ensure stability: 1. SFT Phase 1 (Knowledge Encoding): High learning rate training on the mixed dataset (Summarization, Recall, Verbatim) to enforce document memorization. 2. SFT Phase 2 (QA Adaptation): Lower learning rate training specifically on the synthetic QA pairs to bridge the gap to the RL format. 3. RL Phase (Skill Sharpening): GRPO training to refine the retrieval logic. Checkpoint Selection Strategy. To avoid overfitting to the source documents, we employ an independent validation set consisting of LooGLE documents with indices 90-94. We evaluate checkpoints every 40 steps. Based on the validation accuracy, we selected the checkpoint at Step 120 for Round 1 and Step 160 for Round 2 as the final models for skill extraction. Table 13 details the specific hyperparameters used in each phase. Density: We generate 8 pairs per chunk. E.5 Evaluation Prompt Diversity: We employ set of 6 distinct \"Proposer\" prompts (detailed in Sec. E.3) to ensure questions cover various aspects: factual details, reasoning (why/how), definitions, comparisons, lists, and significance. E.3 Prompt Templates To ensure the reproducibility of our synthetic data generation pipeline, we provide the exact content of the prompt templates used. QA Generation Prompts. We utilize 6 distinct variations of prompts to generate diverse QuestionAnswer pairs. All variations share common template to enforce strict formatting (XML tags) and specificity requirements. The full content of the instruction template and the 6 variations are detailed in Table 10 and 11. Given the open-ended nature of the generated answers in the LooGLE benchmark, standard metrics like Exact Match or ROUGE are often insufficient to capture semantic correctness. Therefore, we employ Model-Based Evaluation paradigm using GPT-4.1 as an impartial judge to determine if the predicted answer matches the ground truth. Judge Prompts. To ensure the evaluation is strictly binary and easy to parse, we enforce strict output format. The exact prompts used for the judge model are provided below:"
        },
        {
            "title": "Evaluation Prompt",
            "content": "You are precise evaluator. Your task is to deter-mine if the 'Predicted Answer' is semantically the same as the 'Ground Truth' for the given 'Question'. Your entire response MUST 18 Shared Instruction Template (Common to all QA Prompts) You are an AI assistant tasked with generating single, high-quality question-answer pair from given text. **Your instructions are critical:** **Generate one Q&A pair** based *only* on the provided text. 1. 2. **Specificity is Key:** The question *must* be self-contained and unambiguous. It must include specific names or key terms from the text (e.g., \"What is Project Helios?\" instead of \"What is this project?\"). 3. <answer>Your answer here</answer>. Do not include any other text before or after the tags. 4. [Specific Task Instruction: Refer to Variations 1-6 below] **Format:** Your output *must* use XML-style tags: <question>Your question here</question> **Example of Good, Specific Q&A:** [Task-specific Example: Refer to Variations 1-6 below] **Text Fragment:** {text_segment} **Your Output:** Variation 1: Factual Detail **Task:** Focus on specific, factual detail: name, date, key term, or specific 4. component. **Example of Good, Specific Q&A:** <question> What consensus mechanism does the Helios architecture pioneer? </question> <answer> It pioneers decentralized consensus mechanism called Proof-of-History (PoH). </answer> Variation 2: Reasoning (Why/How) **Task:** Focus on the *reason* (Why) or the *method* (How) behind concept or event 4. described in the text. **Example of Good, Specific Q&A:** <question> Why does the Proof-of-History (PoH) mechanism successfully reduce latency? </question> <answer> Because it creates verifiable, sequential record of time, which avoids the need for solving complex computational puzzles like in Proof-of-Work. </answer> Table 10: QA Generation Prompts (Part 1). The shared system instruction and the first two task variations used to generate synthetic QA pairs for LooGLE. be only the single word 'True' or the single word 'False'. Do not provide any explanation or punctuation. Question: {question} Ground Truth: {reference} Predicted Answer: {pred} Robustness via Multi-Pass Sampling. To account for generation stochasticity and ensure the robustness of our reported metrics, we perform 3 independent generation runs with temperature of = 1.0 for every question in the test set.This protocol ensures that our results reflect the models consistent capability rather than lucky generations."
        },
        {
            "title": "F ToolBench Experiments",
            "content": "F.1 Category Filtering and Selection Criteria To ensure robust and balanced evaluation of crossdomain generalization in agentic tool use, we perform multi-stage filtering process on the original ToolBench and StableToolBench datasets. We apply the following constraints to select the evaluation categories: Data Sufficiency: We exclude categories with fewer than 3 solvable queries in the StableToolBench test set to ensure that the evaluation results are statistically meaningful. Category Complexity: We select categories with an API count between 75 and 350. This range ensures that the domain is sufficiently complex to require parametric internalizing (avoiding trivial domains with too few APIs) while remaining manageable for the environment simulator. Based on these criteria, 21 categories were retained. We designate Movies as the Source Domain for skill acquisition because its API count (111) represents the median complexity of the filtered set, and it contains relatively high number of test queries (35), allowing for stable monitoring of the RL training progress. The remaining 20 categories serve as the Target Domains for zero-shot generalization testing. Statistics of Evaluated Categories Table 14 summarizes the statistics for the selected domains. The \"APIs\" column denotes the number of unique API schemas the model must internalize, and the \"Test Queries\" column denotes the number of solvable scenarios used for final evaluation. F."
        },
        {
            "title": "Implementation Details for SFT",
            "content": "Our SFT process is divided into two stages: Knowledge Internalization (Stage 1) and Format Alignment (Stage 2). All experiments are conducted using the verl framework with FSDP2 strategy on 8A100 (80GB) GPUs. Teacher Prompts. We use three distinct templates to generate descriptions from JSON schemas to ensure linguistic diversity."
        },
        {
            "title": "Teacher Generation Prompts",
            "content": "Transform the following API JSON into coherent, natural language paragraph describing how to use it. CRITICAL: You MUST explicitly mention the API name ({API_NAME}) and its purpose at the beginning. You are technical documentation expert. Convert the provided API JSON definition into comprehensive technical reference. Rules: 1. Identity: Explicitly state the API Name. 2. Purpose: Explain functionality. 3. Inputs: Detail parameters. Convert the provided API definition into structured natural language summary. Format: 1. Tool Identifier: The exact API name string. 2. Intent: User goal. 3. Action: Parameters. Training Pair Construction. Based on the generated descriptions, we construct four types of training pairs to build robust mapping between API names, intents, and schemas: Type (Name Usage): Queries like \"How do use the {name} API?\" mapped to NL descriptions. Type (Intent Usage): Queries like \"Identify the API defined by: {description} and explain its parameters.\" Type (Intent Raw JSON): Requesting the underlying JSON schema based on description. Type (Name Raw JSON): Directly mapping the API name to its original JSON definition. F.2.2 Training Configurations and"
        },
        {
            "title": "Hyperparameters",
            "content": "We utilize different optimization strategies for the two stages. Stage 1 focuses on broad knowledge acquisition with larger batch size and higher learning rate, while Stage 2 performs fine-grained format alignment. The detailed configurations are shown in Table 15. F.3 Details of Reinforcement Learning for F.2.1 Data Generation for Stage"
        },
        {
            "title": "Tool Use",
            "content": "To internalize the parametric knowledge of tools, we use the base model Qwen2.5-7B-Instruct to transform raw JSON schemas into diverse natural language (NL) descriptions and QA pairs. We implement the reinforcement learning phase based on the Search-R1 (Jin et al., 2025) framework, extending it to support multi-turn agentic tool-use trajectories and environment interactions. 20 (+1.0), Partially Solved (+0.5), or Unsolved (0.0). F.4 Hyperparameters for Reinforcement"
        },
        {
            "title": "Learning",
            "content": "The reinforcement learning phase is conducted using the PaST framework on 4A100 GPUs. Table 19 summarizes the detailed hyperparameter settings used for the ToolBench experiments."
        },
        {
            "title": "G Declaration of AI Usage",
            "content": "Generative AI tools were used for grammar refinement and language polishing to enhance the readability of the manuscript. AI assistance was also employed during the coding and implementation phases of the project. All AI-assisted outputs were reviewed by the authors to ensure the technical quality and accuracy of the final paper. Agent Prompting. The model is prompted to rely on its internalized knowledge acquired during the SFT phase. The system prompt specifies the ReAct format (Thought, Action, Action Input) and lists only the names of available APIs. The exact prompt template is provided in Table 16. Environment Simulator. Since real-world API execution can be unstable or costly during RL, we employ gpt-4o-mini as an API Simulator. The simulator is provided with the full API documentation and examples (from the original ToolBench) and is tasked to validate the agents generated Action Input against the ground-truth schema. It returns either realistic JSON response or specific error message (e.g., missing required parameters, type mismatch). The simulators instructions are detailed in Table 17. F.3.1 Reward Design The reward signal for trajectory is composed of three components, designed to encourage format adherence, successful tool invocation, and task resolution. Format and Execution Reward. For each intermediate turn j, we assign step-wise reward Rstep,j based on the validity of the generated action and its execution outcome. Specifically: (i) positive reward of +0.1 is granted if the action follows the correct ReAct format and the API call is successfully executed by the simulator; (ii) penalty of 0.1 is applied if the format is correct but the API call fails (e.g., due to missing required parameters or type mismatches); (iii) heavier penalty of 0.2 is imposed if the model fails to follow the output format or invokes hallucinated API name not present in the available toolset. Termination Reward. To prevent infinite loops and encourage concise solutions, we apply maximum of 5 turns and apply reward at the final turn where the model calls the Finish tool: Active Finish: If the model successfully calls Finish, it receives +0.2. Forced Termination: If the model reaches the maximum turn limit without calling Finish, it receives 0.5. Solution Reward. The final Pass Reward is determined by GPT-4.1 judge, which evaluates the entire trajectory. The judge assigns one of three statuses based on the rules in Table 18: Solved 21 Variation 3: Definitions 4. **Task:** Generate \"What is...\" or \"What does... stand for?\" question about key concept. **Example of Good, Specific Q&A:** <question> What is Proof-of-History (PoH) as described in the context of the Helios architecture? </question> <answer> It is decentralized consensus mechanism that creates verifiable, sequential record of time. </answer> Variation 4: Comparisons **Task:** Focus on the *difference* or *similarity* between two specific concepts, methods, 4. or entities in the text. **Example of Good, Specific Q&A:** <question> What is the key difference between the Proof-of-History (PoH) mechanism and Proof-of-Work (PoW)? </question> <answer> Proof-of-History (PoH) creates verifiable, sequential record of time, whereas Proof-of-Work (PoW) relies on solving complex computational puzzles. </answer> Variation 5: Lists/Processes **Task:** Generate question that asks to list steps, components, or stages of specific 4. system or method. **Example of Good, Specific Q&A:** <question> What are the three main stages of the Project Nova deployment pipeline? </question> <answer> The three main stages are Build, Test, and Verify. </answer> Variation 6: Significance/Impact **Task:** Generate \"What is the significance of...\" or \"What is the main advantage of...\" 4. question. **Example of Good, Specific Q&A:** <question> What is the main advantage of the Helios architectures Proof-of-History mechanism? </question> <answer> Its main advantage is drastic reduction in latency. </answer> Table 11: QA Generation Prompts (Part 2). Additional task variations (3-6) used to ensure diversity in the synthetic LooGLE training data."
        },
        {
            "title": "Summarization Templates",
            "content": "Create concise and objective summary of the text below. Focus on the main ideas and most important information, presenting them clearly in your own words.nnText to summarize:n{text_segment}nnNow generate your summary: Distill the following text to its absolute essence. Provide one-paragraph summary that captures the core argument and key takeaways. Avoid any fluff or secondary details.nnOriginal Text:n{text_segment}nnYour distilled summary: Break down the following text for someone completely unfamiliar with the topic. Your summary should be simple, clear, and use easy-to-understand language, as if you were explaining it to 5th grader. Focus on the main concepts without getting lost in technical jargon.nnText:n{text_segment}nnYour simple explanation: Provide summary of the following passage. Your summary should not only capture the key information but also reflect the original authors tone and style (e.g., formal, persuasive, humorous, critical). nnOriginal Passage:n{text_segment}nnYour summary, in the original tone: Provide strictly neutral and objective summary of the provided text. Your goal is to recount the main points and arguments without injecting any personal opinion, interpretation, or evaluative language. Simply state what the text says.nnText:n{text_segment}nnNeutral Summary: Recall/Expansion Templates Below is summary of larger text. Your task is to expand on this summary, creating detailed and comprehensive paragraph that could have been the original source. Flesh out the main points with explanations, smooth transitions, and supporting details.nnSummary:n{summary}nnNow generate the detailed original text: You are given concise summary below. Your objective is to generate more detailed and fully-formed text based on it. Elaborate on the ideas presented in the summary, ensuring the resulting text is coherent, well-structured, and reads naturally.nnSummary:n{summary}nnGenerated Text: The following summary is high-level overview of piece of information. Your task is to reconstruct plausible original text by filling in the necessary details, examples, and explanations that might have been removed during summarization.nnSummary:n{summary}nnReconstructed Text with Details: Consider the summary below as set of conclusions or key statements. Your goal is to write text that logically leads to these points. Build coherent argument or explanation that culminates in the information provided in the summary.nnKey Points / Summary:n{summary}nnText with Logical Development: Take the core ideas presented in the summary below and weave them into single, seamless paragraph. Focus on creating smooth transitions between the points so they flow together as unified piece of writing, rather than list of facts.nnCore Ideas:n{summary}nnCoherent Paragraph: Table 12: Multi-Task SFT Prompts. Templates used for the Summarization and Recall tasks in Stage 1 training to enhance knowledge encoding. 23 Table 13: Detailed Hyperparameters for LooGLE Experiments. We employ multi-stage curriculum: SFT Phase 1 enforces deep encoding of the 21k-token documents via mixed tasks; SFT Phase 2 adapts the model to the QA format; and the RL stage (GRPO) refines the retrieval logic using closed-book setting (short prompt, long generation). Hyperparameter SFT Phase 1 (Knowledge Encoding) SFT Phase 2 (QA Adaptation) RL (GRPO) (Skill Sharpening) General Configuration Base Model Precision Gradient Checkpointing Number of GPUs Optimization Optimizer Learning Rate LR Scheduler Global Batch Size Micro Batch Size (per GPU) Total Epochs Data & Context Max Sequence Length Max Prompt Length Max Response Length Data Source RL Specifics (GRPO) Group Size (G) KL Coefficient (β) KL Reference Model Reward Function bf16 True 8 AdamW 1e-4 Cosine 16 2 Qwen2.5-7B-Instruct bf16 True 8 bf16 True 8 AdamW 2e-5 Cosine 64 2 2 AdamW 1e-6 Constant 128 8 10 (Selected Best Step) 8192 - - Mixed (Sum/Recall) 8192 - - Synthetic QA - 128 (Closed-Book) 1024 Synthetic QA - - - - - - - - 5 0.001 SFT Phase 2 Checkpoint GPT-4.1 Judge Table 14: Statistics of the 21 selected categories from ToolBench. The Source Domain is used for RL training, while Target Domains are used for zero-shot evaluation."
        },
        {
            "title": "Movies",
            "content": "Advertising AI & ML Business Software Communication Database Education Email Financial Food Health and Fitness Location Mapping Media SMS Science Search Text Analysis Video Images Weather eCommerce"
        },
        {
            "title": "Domain Role",
            "content": "# APIs # Test Queries 111 118 108 199 250 260 214 143 224 208 90 328 113 159 75 99 103 98 207"
        },
        {
            "title": "Target\nTarget\nTarget\nTarget\nTarget\nTarget\nTarget\nTarget\nTarget\nTarget\nTarget\nTarget\nTarget\nTarget\nTarget\nTarget\nTarget\nTarget\nTarget\nTarget",
            "content": "24 35 3 4 5 8 7 9 5 11 9 7 11 7 12 3 4 13 4 44 8 15 Table 15: Hyperparameters for the two stages of SFT in ToolBench. Hyperparameter Stage 1: Internalization Stage 2: Alignment Base Model Max Sequence Length Optimizer Precision Learning Rate Total Batch Size Micro Batch Size (per GPU) Total Epochs Parallel Strategy Liger Kernel Qwen2.5-7B-Instruct 8192 8192 AdamW BF 5 105 64 2 3 2 105 32 2 3 FSDP2 (verl) Enabled Table 16: The system prompt for the agent in ToolBench Experiments."
        },
        {
            "title": "Agent System Prompt",
            "content": "You are an intelligent agent designed to handle real-time user queries using variety of tools. First, you will receive task description. Then, you will enter loop of reasoning and acting to complete the task. At each step, follow this process: 1. **Thought**: Analyze the current status and determine the next logical step. 2. **Action**: Select the appropriate tool to execute that step and output the function name directly. 3. **Action Input**: Provide the arguments for the tool as STRICT valid JSON object. Output Format: Thought: <your reasoning> Action: <function_name> Action Input: <function_arguments_as_a_valid_JSON_object> After the action is executed, you will receive the result (Observation: <observation>). Based on the new state, continue the loop until the task is complete. Constraints & Rules: 1. **Action Field**: The \"Action\" output must be the EXACT name of the function. Do NOT include parentheses (), words like \"call\" or \"use\", or any punctuation. 2. **Finishing**: You MUST call the \"Finish\" function to submit your final answer. Available Tools: 1. **General Tools**: You have been trained on specific set of APIs: {api_names}. You must rely on your **internal knowledge** to recall the correct parameter schemas for these tools. 2. **Termination Tool**: You MUST use the following tool to finish the task. Its definition is provided below: {\"name\": \"Finish\", \"description\": \"If you believe that you have obtained result that can answer the task, please call this function to provide the final answer. Remember: you must ALWAYS call this function at the end of your attempt, and the only part that will be shown to the user is the final answer, so it should contain sufficient information.\", \"parameters\": { \"properties\": {\"final_answer\": {\"type\": \"string\", \"description\": \"The final answer you want to give the user.\"}}}, \"required\": [\"final_answer\"], \"optional\": []} 25 Table 17: The simulators instructions in ToolBench Experiments."
        },
        {
            "title": "API Simulator Instructions",
            "content": "You are an advanced API Simulator and Validator. Your role is to act as real API server, strictly adhering to the provided documentation to process requests. ### 1. Input Structure Explanation The user will provide input in the following specific format: API Documentation: Contains the APIs URL, method, description, and parameter definitions (required/optional). API Examples: Contains reference calls (Note: This section is truncated to 2048 chars and may be incomplete; use it for style reference but rely on Documentation for logic). API Input: The specific arguments/payload you need to process. ### 2. Processing Logic 1. **Analyze:** Read the API Documentation to understand the schema and constraints (types, required fields). 2. **Validate:** Check the API Input against the API Documentation. - Are all required_parameters present? - Do the data types match (e.g., string vs int)? 3. **Execute:** - **If Valid:** Generate realistic, rich JSON response. - **If Invalid:** Generate JSON response where error describes the specific validation failure. ### 3. Output Format You must output ONLY valid JSON object. No Markdown code blocks. No conversational text. **JSON Schema:** { \"error\": \"String describing the error (if any), otherwise empty string\", \"response\": <The_Simulated_Response_Object_or_Null> } ### 4. Behavior Rules - **Length Constraints:** - Keep the response **concise and lightweight**. Keep the entire JSON output shorter than 300 words. Do not generate excessively large payloads. If the API returns list or array, **limit it to maximum 1-2 items**. - **Source of Truth:** Do not blindly copy \"API Examples\" if they contradict the \"API Input\". The \"API Input\" is your priority. 26 Table 18: The evaluation prompt in ToolBench experiments. GPT-4.1 Judge Prompt Giving the query and the corresponding execution trajectory (including thoughts, tool calls, and observations), evaluate the answer_status based on these rules: 1. **Solved**: The tool calls were successful. The final answer is strictly grounded in the real \"Observation\" data and fully addresses the query. 2. **Partially Solved**: The model used real \"Observation\" data, but the task is only halfway finished or the final answer missed some details from the observations. 3. **Unsolved**: - The model fabricated information not found in the Observations. - The tool calls failed, and the model failed to solve the query or made up result. - The answer is incorrect or irrelevant. Output JSON object with the following fields: - \"reason\": very brief explanation (less than 20 words). - \"answer_status\": One of [\"Solved\", \"Partially Solved\", \"Unsolved\"]. <Target_Query> {query} </Target_Query> <Model_Execution_Trajectory> {trajectory} </Model_Execution_Trajectory> Table 19: Detailed hyperparameters for PPO training in ToolBench experiments. Category Hyperparameter Data & Environment Optimization PPO Algorithm Max Prompt Length Max Response Length Max Simulation Turns Actor Learning Rate Critic Learning Rate Actor LR Warmup Ratio Critic LR Warmup Ratio LR Scheduler Optimizer Advantage Estimator PPO Epochs per Batch Mini-batch Size Clip Range (ϵ) KL Penalty Coefficient (β) Entropy Coefficient Training Schedule Total Training Steps Compute Precision Value 8192 1024 1 106 1 105 0.285 0.015 Constant AdamW GAE 1 64 0.2 0.001 0.001 180 BF"
        }
    ],
    "affiliations": [
        "Institute for Artificial Intelligence, Peking University",
        "State Key Laboratory of General Artificial Intelligence, BIGAI",
        "Yuanpei College, Peking University"
    ]
}