{
    "paper_title": "Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance",
    "authors": [
        "Ahmed Alajrami",
        "Xingwei Tan",
        "Nikolaos Aletras"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction-tuning plays a vital role in enhancing the task-solving abilities of large language models (LLMs), improving their usability in generating helpful responses on various tasks. However, previous work has demonstrated that they are sensitive to minor variations in instruction phrasing. In this paper, we explore whether introducing perturbations in instruction-tuning data can enhance LLMs' resistance against noisy instructions. We focus on how instruction-tuning with perturbations, such as removing stop words or shuffling words, affects LLMs' performance on the original and perturbed versions of widely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics and potential shifts in model behavior. Surprisingly, our results suggest that instruction-tuning on perturbed instructions can, in some cases, improve downstream performance. These findings highlight the importance of including perturbed instructions in instruction-tuning, which can make LLMs more resilient to noisy user inputs."
        },
        {
            "title": "Start",
            "content": "Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance Ahmed Alajrami Xingwei Tan Nikolaos Aletras Department of Computer Science University of Sheffield, UK {ajsalajrami1, xingwei.tan, n.aletras}@sheffield.ac.uk 5 2 0 2 3 ] . [ 1 8 2 5 3 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Instruction-tuning plays vital role in enhancing the task-solving abilities of large language models (LLMs), improving their usability in generating helpful responses on various tasks. However, previous work has demonstrated that they are sensitive to minor variations in instruction phrasing. In this paper, we explore whether introducing perturbations in instruction-tuning data can enhance LLMs resistance against noisy instructions. We focus on how instruction-tuning with perturbations, such as removing stop words or shuffling words, affects LLMs performance on the original and perturbed versions of widelyused benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics and potential shifts in model behavior. Surprisingly, our results suggest that instruction-tuning on perturbed instructions can, in some cases, improve downstream performance. These findings highlight the importance of including perturbed instructions in instruction-tuning, which can make LLMs more resilient to noisy user inputs."
        },
        {
            "title": "Introduction",
            "content": "Instruction-tuning is widely adopted to enable LLMs to follow complex instructions and respond properly (Sanh et al., 2022; Zhao et al., 2023; Chang et al., 2023; Minaee et al., 2024; Zhang et al., 2024). During instruction-tuning, LLMs are fine-tuned on datasets comprising various task instructions and their corresponding responses. LLMs have been shown to be sensitive to prompt variability, producing inconsistent responses when given semantically equivalent prompts (Sun et al., 2024; Zhao et al., 2024; Yan et al., 2024). To remedy this, recent instruction datasets are often generated with extensive paraphrasing using LLMs to 1Code is available here: https://github.com/aajrami/ finetuning-on-noisy-instructions/ Figure 1: Instruction-tuning on perturbed instructions can enhance LLMs resilience to noisy inputs. increase data diversity (Peng et al., 2023). However, this paraphrased data is of high quality with minimal noise in the instructions. different line of work has explored the robustness of instructiontuned models to instruction variations during inference by introducing different types of noise, such as deleting words (Gu et al., 2023). However, how noisy data may affect LLMs during training has yet to be explored. In this paper, we focus on answering the following research question: Can fine-tuning of base models on perturbed instructions improve their resilience to noisy user inputs? Our question is theoretically motivated by previous work that has shown that introducing noise during training acts as form of regularization (Bishop, 1995), which can prevent overfitting and improve generalization.2 To evaluate the impact of instruction perturbation and simulate noisy user inputs, we employ five strategies inspired by Gu et al. (2023): (1) delete stop words, (2) shuffle words, (3) delete words, (4) replace words, and (5) insert words. We further introduce sixth perturbation strategy by adding misspellings. These strategies allow us to 2We may interchangeably use terms such as resilience or robustness to refer to the models capacity to withstand and adapt to noisy user inputs without substantial drop in performance. The term generalization refers to the models capacity to perform given task when presented with novel, previously unseen phrasings or formats. simulate and analyze various forms of perturbed instructions, including both structural and semantic changes. We construct four perturbed instructiontuning datasets where GPT4-Alpaca (Peng et al., 2023), Super-Natural (Wang et al., 2022), and Dolly (Conover et al., 2023) are combined and then perturbed. We include versions of the data where 25%, 50%, 75%, and 100% of the instructions are perturbed respectively, which allows us to compare how LLMs are affected by different proportions of noisy instructions presented in instruction-tuning. We evaluate performance across three widely-used language understanding benchmarks: Massive Multitask Language Understanding (Hendrycks et al., 2021), Big-Bench Hard (Suzgun et al., 2022), and Grade School Math (Cobbe et al., 2021). Figure 1 shows the process of generating noisy instructiontuning data and LLM fine-tuning. Contributions. We make two key contributions. First, we conduct systematic study of how noisy instructions, by fundamentally altering their syntactic and semantic structure during training, impact LLM performance on downstream tasks. Second, our empirical analysis suggests that fine-tuning on noisy instructions may offer simple approach to enhance robustness. The results of our study appear to offer insights into the nature of LLM learning during instruction-tuning. They prompt re-examination of the widely held assumption that complete instruction comprehension is always necessary for effective task learning. Specifically, our findings suggest that LLMs can derive benefit from instruction modifications that do not strictly preserve meaning, indicating more nuanced relationship between instruction and task performance than previously assumed."
        },
        {
            "title": "2.1 Analyzing Instruction-tuning",
            "content": "Instruction fine-tuning enables LLMs to follow user instructions and reduces the need for few-shot incontext examples (Ouyang et al., 2022; Wei et al., 2022a; Touvron et al., 2023a; Chung et al., 2024). The instruction-tuning datasets contain instructions of various tasks and their corresponding responses which can be human-annotated (Mishra et al., 2022) or synthetically generated (Taori et al., 2023; Peng et al., 2023). hardly comprehensible by humans, indicating that language models have vastly different way to understand instructions. Recent studies have investigated the internal mechanisms of instruction fine-tuning and their influence on LLMs. By observing the output token distribution shift of models before and after instruction-tuning, Lin et al. (2024) found that most shifts occur with stylistic tokens (e.g. discourse markers and transitional words), and knowledge content originates from untuned LLMs. By introducing knowledge interventions, Ren et al. (2024) also showed that instruction-tuning is process of self-aligning the instructions with existing parametric knowledge rather than introducing new knowledge into the model."
        },
        {
            "title": "2.2 Robustness of Instruction-tuned Models",
            "content": "Gu et al. (2023) investigated how instruction-tuned models handle instruction perturbations and paraphrasing. After fine-tuning model on the original instructions training set and evaluating it on the perturbed instructions test set, they found the model was relatively robust in few-shot settings but notably sensitive in zero-shot scenarios. Similarly, Sun et al. (2024) showed that paraphrased instructions can disrupt model consistency and proposed mitigation strategy using soft prompt embeddings to align semantically similar instructions. Wang et al. (2024) examined errors from speech recognition and OCR, finding such noise significantly degrades LLMs performance. They also explored using LLMs for zero-shot correction of noisy instructions. Yan et al. (2024) proposed contrastive instruction-tuning which align the hidden representations of instruction-instance pairs that are semantically equivalent but textually different and to differentiate those that are semantically distinct. Zhao et al. (2024) proposed consistency alignment framework including instruction augmentation by paraphrasing and automatic self reward. Lou et al. (2024) introduced dataset curation scheme by diversifying the task inputs with various facets. Kim et al. (2024) proposed instructive decoding, which improves instruction-following in instruction-tuned LLMs by contrasting the decoding process without additional tuning. AutoPrompt (Shin et al., 2020), which applied gradient-based search to optimize the prompt for various tasks, usually finds prompts that are Unlike previous studies, we investigate how training on noisy instructions affects their ability to adapt to instruction perturbations."
        },
        {
            "title": "Perturbation",
            "content": "No (Original)"
        },
        {
            "title": "Add Misspelling",
            "content": "Users Content <instruction> Rewrite the given paragraph in shorter, easier to understand form. <instruction> Input: Although it is generally accepted that the internet has allowed us to connect with people all . . . <instruction> Rewrite the given paragraph in shorter, easier to understand form. <instruction> Input: Although it is generally accepted that the internet has allowed us to connect with people all . . . <instruction> Rewrite shorter given paragraph in easier , the to understand form.<instruction> Input: Although it is generally accepted that the internet has allowed us to connect with people all . . . <instruction> Rewrite the given paragraph in shorter, easier to understand form. <instruction> Input: Although it is generally accepted that the internet has allowed us to connect with people all . . . <instruction> Rewrite the previous paragraph in new , easier to understand it . <instruction> Input: Although it is generally accepted that the internet has allowed us to connect with people all . . . <instruction> Rewrite the given paragraph in shorter form , easier than to understand form better . <instruction> Input: Although it is generally accepted that the internet has allowed us to connect with people all ... <instruction> Rewrite the givdn paragraphu in shorter, easier to understand frm . <instruction> Input: Although it is generally accepted that the internet has allowed us to connect with people all . . . Table 1: Example of task instruction from the GPT4-Alpaca dataset and the corresponding perturbed instruction generated by each perturbation strategy."
        },
        {
            "title": "Instruction Perturbation Strategies",
            "content": "We investigate the impact of instruction fine-tuning on the performance of LLMs when subjected to noisy input conditions. Following Gu et al. (2023), we employ five instruction perturbation strategies: delete stop words, shuffle words, delete words, replace words, and insert words. Furthermore, we introduce misspelling as an additional noise injection approach. Table 1 shows an example instruction from the GPT4-Alpaca dataset (Peng et al., 2023). The model input consists of an instruction and associated context. The perturbation strategies are applied exclusively to the instruction component. Delete Stop Words. Stop words, such as the, is, and of are functional words that mainly contribute to grammatical structure but have limited effects on semantic content. Removing stop words leads to syntactically incomplete instructions, allowing us to evaluate the models reliance on syntactic cues and its ability to infer meaning from partial input. For instance, the instruction Translate the sentence into French becomes Translate sentence French. Shuffle Words. The second perturbation strategy is to randomly shuffle the words. By changing the original word order, we aim to introduce both syntactic and semantic alterations. We employ 25% word shuffling of words within the instruction, while the other words maintain their relative order. We cap shuffling at 25%, enough to mimic the partial mix-up would be seen from hurried typing, and realistic enough to test robustness without turning the prompt into total gibberish. For instance, given the instruction Summarize the following paragraph, this might result in following the Summarize paragraph. This perturbation provides an assessment of how sensitive models are to changes in word order and to what extent they rely on the original structure to comprehend the instruction. Delete Words. In this perturbation strategy, 25% of the words within each instruction are randomly deleted. In contrast to the targeted removal of stop words, this approach introduces more significant distortions by potentially eliminating both functional and semantic words, thereby disrupting the semantic coherence of the instructions to greater extent. This strategy assesses the models capacity to infer task intent when presented with incomplete syntactic and semantic structures. Replace Words. We randomly select 25% of the words from an instruction and replace them using predictions from pretrained BERT model (Devlin et al., 2019). Following Gu et al. (2023), we use BERTs masked language modeling head to generate contextually plausible substitute for all selected words. The selected words are replaced by [MASK] tokens, then BERT predicts replacement words in forward pass. This strategy introduces minimal semantic shifts to the instructions without relying on lexicon, which typically requires manual effort. This perturbation may or may not alter the core meaning of the instruction, depending on the replaced words and their context. The strategy allows us to investigate the models sensitivity to nuanced lexical variations and its ability to generalize under slightly altered task phrasing. Insert Words. We introduce additional words into the instruction by leveraging pretrained BERT model (Devlin et al., 2019). Specifically, we"
        },
        {
            "title": "Instruction Dataset",
            "content": "# Samples GPT4-Alpaca Super-Natural Instruction Dolly"
        },
        {
            "title": "Total",
            "content": "52,002 55,793 15,011 122,806 Table 2: Number of samples in each dataset. randomly select positions between existing words, covering approximately 25% of the total word count, and insert [MASK] token at each selected position. We then replace each [MASK] with words predicted by BERT, resulting in an augmented instruction containing additional, contextually plausible tokens. This perturbation may introduce noise, redundancy, or shifts in meaning, challenging the models ability to extract the core task intention from more verbose or distorted input. Add Misspelling. Finally, to simulate noisy input that may more closely resemble user-generated errors, we introduce typographical errors. We randomly select 25% of the words within each instruction and introduce typo into each of these words. We apply simple character-level edits, such as deleting random letter, transposing adjacent letters, inserting random vowel, or substituting character with randomly selected one. This strategy enables us to evaluate the models sensitivity to spelling errors in the instructions and its ability to discern the intended meaning from noisy input."
        },
        {
            "title": "4.1 Models",
            "content": "We experiment with two open-weight base LLMs in two sizes: Qwen-2.5 (7B and 72B) (Yang et al., 2024); and Llama-3.1 (8B and 70B) (Dubey et al., 2024). 4."
        },
        {
            "title": "Instruction Datasets",
            "content": "We fine-tune all the base models using combination of three standard instruction datasets with distinct characteristics. GPT4-Alpaca (Peng et al., 2023) is derived from Alpaca (Taori et al., 2023), where the original examples are replaced with responses generated by GPT-4. Super-Natural Instruction (Wang et al., 2022) contains diverse tasks, including text classification and translation, with corresponding instructions. It is designed to evaluate the LLM abilities across wide range of linguistic and functional contexts. Dolly (Conover et al., 2023) consists of instruction-following examples that reflect practical, real-world tasks like brainstorming and creative writing. The prompt-response pairs are high-quality and human-generated. Table 2 summarizes the number of samples in these datasets. Perturbation Settings. To simulate real-world settings where the perturbations could appear altogether, we construct five different dataset mixtures, each containing different proportion of perturbed instructions: (1) the original, unmodified instruction samples from all three datasets considered as baseline (0% Perturbation), (2) 25% of the instruction samples are perturbed, while the remaining 75% are left unaltered (25% Perturbation), (3) half of the instruction samples are perturbed (50% Perturbation), (4) 75% of the samples are perturbed (75% Perturbation), and (5) all instruction samples across the three datasets are perturbed (100% Perturbation). In all mixtures involving perturbations, the altered samples are evenly distributed across the six different perturbation strategies (Section 3). 4."
        },
        {
            "title": "Implementation Details",
            "content": "We apply parameter-efficient fine-tuning methods for all experiments. Specifically, we use LoRA (Hu et al., 2022) to fine-tune the 7B and 8B models, and QLoRA (Dettmers et al., 2023) for the larger 70B and 72B models. Each model is fine-tuned for one epoch on each dataset mixture to ensure consistency across experiments. All fine-tuning runs were performed on single NVIDIA H100 GPU. Full details on the fine-tuning hyperparameters are provided in Appendix A."
        },
        {
            "title": "4.4 Evaluation",
            "content": "General Benchmarks. We assess downstream performance using: Massive Multitask Language Understanding (MMLU; Hendrycks et al. 2021): MMLU evaluates models factual knowledge and reasoning across 57 subjects, ranging from elementary to professional-level difficulty, using multiple-choice questions. We follow the original MMLU setup, evaluating in 0-shot and 5-shot settings, and report average test accuracy. Big-Bench Hard (BBH; Suzgun et al. 2022): challenging subset of 23 tasks from the original BIG-Bench (Srivastava et al., 2023), aimed at evaluating advanced reasoning in language models. We IT VAN 0% MMLU (5-shot) 50% 75% 25% 100% 0% 25% BBH (CoT) 50% 75% 100% 0% GSM8K (CoT) 50% 25% 75% 100% 74.3 0.0 73.0 0.1 71.5 0.1 70.0 0. 68.6 0.6 66.7 0.1 63.9 0.4 60.8 0.4 57.7 0.5 54.9 0. 79.9 0.2 12.5 0.3 22.9 0.6 33.0 1.4 42.7 1.2 w 8 l 2 7 Q 0 a 74.3 0.0 0% 74.4 0.0 25% 74.4 0.0 50% 75% 74.3 0.0 100% 74.3 0.0 73.0 0.1 73.0 0.1 73.1 0.1 73.0 0.1 73.1 0.0 71.7 0.1 71.8 0.1 71.9 0.1 71.8 0.2 71.9 0.1 70.2 0.4 70.3 0.5 70.5 0.5 70.4 0.5 70.5 0.5 68.9 0.7 69.1 0.6 69.1 0.7 69.1 0.7 69.2 0. 66.8 0.0 66.7 0.0 67.0 0.0 67.4 0.0 66.6 0.0 62.7 0.2 63.3 0.3 64.0 0.2 63.9 0.3 63.4 0.2 58.7 0.2 59.7 0.2 61.1 0.3 60.7 0.2 60.3 0.3 54.3 0.5 55.9 0.5 57.7 0.6 57.6 0.4 56.8 0.4 50.6 0.6 52.4 0.6 54.8 0.6 54.7 0.6 53.8 0.7 80.6 0.0 81.1 0.0 80.6 0.0 80.5 0.0 80.0 0. 12.6 0.5 12.6 0.4 12.6 0.5 12.8 0.5 12.6 0.2 24.5 0.5 24.7 0.5 24.6 0.5 24.4 0.6 24.8 0.6 34.6 1.0 34.5 1.2 34.3 0.7 34.0 0.9 34.3 1.1 44.6 1.2 44.2 1.4 44.5 1.1 44.4 0.9 45.1 0.8 VAN 65.8 0. 64.5 0.1 63.1 0.1 62.1 0.3 60.8 0.5 64.5 0.1 62.5 0. 60.2 0.4 57.5 1.0 55.0 0.9 56.3 0.3 9.0 0.7 16.3 0. 23.5 0.6 30.5 1.2 65.8 0.0 0% 65.9 0.0 25% 65.9 0.0 50% 75% 65.7 0.0 100% 66.0 0.0 64.6 0.2 64.8 0.3 64.8 0.2 64.7 0.2 64.8 0.3 63.3 0.1 63.4 0.2 63.6 0.1 63.6 0.1 63.7 0.1 62.2 0.2 62.3 0.2 62.5 0.2 62.5 0.2 62.5 0. 60.7 0.5 60.9 0.7 61.0 0.6 61.2 0.5 61.2 0.5 63.0 0.4 66.0 0.1 62.7 0.0 62.9 0.4 66.2 0.1 63.4 0.1 60.5 0.4 64.4 0.3 64.1 0.3 64.2 0.5 61.1 0.3 60.0 1.9 62.0 0.5 61.8 0.3 62.0 0.5 58.7 0.7 59.1 0.4 59.3 0.5 59.1 0.5 59.2 0.8 56.5 0.6 56.5 0.6 56.7 0.5 56.3 0.5 56.8 0. 58.4 0.0 58.5 0.0 58.2 0.0 57.4 0.0 58.4 0.0 9.2 0.1 9.4 0.2 9.2 0.2 9.1 0.2 9.2 0.2 16.6 0.7 16.8 0.8 16.9 0.6 16.9 0.7 17.1 0.6 23.8 1.0 23.9 0.7 24.0 0.9 23.8 0.8 23.7 1.2 28.1 1.0 27.7 0.9 27.8 1.0 27.6 1.2 27.8 1.5 VAN 85.7 0.0 84.5 0.2 83.0 0.3 81.8 0.3 80.3 0.4 82.7 0. 79.2 0.2 75.4 0.2 71.7 0.4 68.1 0.8 88.8 0.2 14.9 0. 28.1 0.7 40.8 1.0 53.0 1.5 85.8 0.0 0% 85.7 0.0 25% 85.7 0.0 50% 85.8 0.0 75% 100% 85.8 0.0 84.6 0.2 84.6 0.3 84.7 0.3 84.7 0.3 84.8 0.2 83.1 0.3 83.1 0.3 83.1 0.3 83.2 0.3 83.2 0. 82.0 0.2 82.0 0.3 82.0 0.3 82.1 0.3 82.1 0.4 80.5 0.5 80.5 0.6 80.7 0.6 80.7 0.5 80.6 0.6 83.8 0.1 83.8 0.1 83.3 0.1 83.6 0.0 83.6 0.0 80.5 0.2 80.4 0.2 80.2 0.2 80.3 0.2 80.4 0.2 77.3 0.3 77.4 0.4 77.2 0.2 77.2 0.3 77.3 0.2 73.8 0.5 74.0 0.8 73.7 0.6 73.9 0.6 73.8 0. 70.8 1.1 70.8 1.0 70.6 0.8 70.4 0.8 70.8 0.8 90.0 0.2 89.9 0.2 89.9 0.2 90.3 0.3 90.2 0.1 15.3 0.4 15.3 0.5 15.4 0.4 15.6 0.3 15.6 0.5 29.0 0.4 29.6 0.4 29.3 0.5 29.6 0.7 29.7 0.7 42.7 1.1 43.0 1.1 42.8 1.4 42.9 1.7 43.4 1.8 55.0 1.7 55.5 1.6 55.5 1.9 55.5 2.3 55.9 2. VAN 75.8 0.0 74.1 0.1 72.2 0.2 70.2 0.4 68.5 0. 78.3 0.1 75.7 0.2 73.3 0.2 70.3 0.4 68.1 0.4 80.2 0. 13.0 0.3 24.1 0.5 34.7 1.5 43.9 0.9 78.1 0.0 0% 25% 77.9 0.0 78.0 0.0 50% 75% 78.0 0.0 100% 78.6 0.0 76.7 0.3 76.5 0.2 76.6 0.3 76.8 0.3 77.3 0. 74.9 0.3 74.8 0.4 74.8 0.4 75.1 0.3 75.6 0.3 73.0 0.4 72.8 0.3 73.0 0.4 73.4 0.4 74.1 0.4 71.4 0.5 71.2 0.4 71.6 0.4 71.8 0.4 72.8 0.3 81.8 0.1 81.4 0.1 81.2 0.1 81.5 0.1 81.7 0.1 78.9 0.1 78.7 0.1 78.5 0.2 78.9 0.3 79.0 0.2 75.9 0.2 76.0 0.3 75.9 0.3 76.3 0.2 76.4 0. 72.7 0.4 72.8 0.3 73.1 0.4 73.3 0.4 73.3 0.5 70.1 0.4 70.2 0.5 70.6 0.5 70.6 0.7 70.8 0.6 82.3 0.2 82.1 0.2 80.2 0.3 81.6 0.2 82.0 0.2 13.7 0.7 14.0 0.6 13.5 0.6 13.8 0.4 13.7 0.4 25.4 0.4 25.5 0.8 25.6 0.7 25.6 0.7 26.1 0.6 37.0 1.0 37.0 1.3 37.0 1.4 37.3 1.1 38.1 1. 47.5 1.0 47.7 0.5 47.6 1.0 48.2 0.7 48.8 1.2 Table 3: Results of evaluating the vanilla non-instruction-tuned baselines (VAN) and the fine-tuned models under various instruction perturbations using the MMLU, BBH and GSM8K evaluation benchmarks. Results are reported on both the original evaluation instructions (0%) and the various perturbed evaluation instructions with standard deviations over three runs. Bold values denote the best performance across each model. assess both direct prompting and chain-of-thought (CoT) (Wei et al., 2022b), using official prompts with three in-context examples, and report average exact match across sub-tasks. Grade School Math (GSM8K; Cobbe et al. 2021): benchmark of 8.5K grade school-level word problems for testing multi-step mathematical reasoning in language models. We evaluate with direct prompting and CoT using eight in-context few-shot examples, and we report the exact match. We follow the same approach as in fine-tuning and create five evaluation instruction sets with different perturbation settings: original instructions (0%), 25% of the instructions are perturbed, 50% are perturbed, 75% are perturbed, and all instructions are perturbed (100%). The perturbed instructions are evenly distributed across the six different approaches (see Section 3). Safety and Bias. Additionally, to analyze potential side effects of instruction-tuning on noisy instructions, such as changes in toxicity or misinformation generation, we evaluate the models on: (1) ToxiGen (Hartvigsen et al., 2022) which measures the extent to which models generate toxic language and hate speech when explicitly prompted to do so across various demographic groups. We report the percentage of toxic outputs identified using RoBERTa model (Liu et al., 2020) fine-tuned for toxicity detection, as described by Hartvigsen et al. (2022); and (2) TruthfulQA proposed by Lin et al. (2022) which assesses how effectively models can refrain from generating known falsehoods caused by misconceptions or false beliefs, while still generating informative and useful content. We use two off-the-shelf task-specific judge models developed by AllenAI based on Llama-2 (7B) (Touvron et al., 2023b) for measuring truthfulness3 and informativeness, following the setup of Groeneveld et al. (2024). In both datasets, we evaluate using only the original, unaltered prompts to measure model toxicity and truthfulness."
        },
        {
            "title": "5 Results",
            "content": "Table 3 presents the performance across three benchmarks: MMLU (5-shot), BBH (CoT) and GSM8K (CoT) for each of our models. Full suite of results including 0-shot and direct prompting are presented in Appendix B. Fine-tuning on perturbed instructions may enhance robustness under noisy prompts. We 3https://huggingface.co/allenai/ truthfulqa-truth-judge-llama2-7B first observe that incorporating instruction perturbation during fine-tuning often appears to enhance model robustness across the three evaluation benchmarks and the various evaluation settings. For instance, the Qwen-7B model fine-tuned with 50% perturbed instructions achieves 0.5% higher accuracy on MMLU compared to its vanilla (VAN) counterpart when evaluated using 75% perturbed instructions. In the BBH (CoT) benchmark, it is notable that the Llama-70B model fine-tuned with 100% perturbed instructions achieves the highest scores when evaluated using 25%, 50%, 75% and 100% perturbed instructions. However, there are exceptions where the model fine-tuned with the original unaltered instructions still achieves slightly better performance. For example, in the BBH (CoT), the Qwen-72B model fine-tuned on the original unaltered instructions achieves the best overall performance when evaluated using 25% perturbed instructions. Higher proportions of perturbed instructions appear to be beneficial in some contexts. The results also surprisingly suggest that using larger number of perturbed instructions in the training mix can lead to improved performance. For example, in both MMLU (5-shot) and BBH (CoT), Qwen-7B, Llama-8B and Llama-70B models achieve their peak performance when fine-tuned with 50% or more noisy instructions. However, for GSM8K (CoT), smaller models such as Qwen-7B and Llama-8B appear to respond more favorably to less perturbed instructions. One possible explanation for this is that the GSM8K benchmark evaluates multi-step mathematical reasoning, where incomplete or ambiguous instructions can be particularly challenging and harmful for smaller models. Observed gains on standard unperturbed benchmarks from noisy fine-tuning. Moreover, the results across all the three benchmarks suggest that fine-tuning on perturbed instructions not only can improve models performance under perturbed test conditions but also sometimes yields gains when evaluated on the original, unaltered instructions. For example, in the MMLU (5-shot), when evaluating using the original unaltered instructions, both Llama-8B and Llama-70B models fine-tuned with 100% perturbed instructions achieve their best performance of 66.0% and 78.6% respectively. Similarly, the Qwen-7B model fine-tuned with 75% perturbed instructions achieves 0.6% higher performance than the model variant fine-tuned on the original unaltered instructions when evaluated on the BBH (CoT) benchmark. CoT remains more effective than direct prompting. Prior work has shown that CoT prompting outperforms direct prompting on benchmarks like BBH and GSM8K (Kojima et al., 2022; Wei et al., 2022b), and our results confirm this, especially under instruction perturbation. On BBH (CoT), models like Llama-70B fine-tuned with fully perturbed instructions achieve the highest scores of 79.0% and 76.4% when evaluated with 25% and 50% perturbed instructions, respectively, indicating that CoT prompting benefits from increased robustness introduced during training. While Qwen-72B performs best when fine-tuned with less perturbed instructions, its relatively weaker performance under direct prompting suggests that incorporating perturbation during fine-tuning still contributes to improved generalization and reasoning robustness. Instruction-tuning yields uneven gains across tasks. We observe that the impact of instructionFor example, tuning seems to vary by task. MMLU shows relatively modest improvement from instruction-tuning, regardless of whether or not perturbations are applied. In contrast, BBH appears to consistently benefit from instructiontuning, especially for Llama models. This observation aligns with findings from Sun and Dredze (2025), who suggest that certain tasks are already well-represented in models pre-training data, leaving limited room for further gains through instruction-tuning. On the other hand, tasks that are underrepresented or poorly learned during pretraining can potentially see more substantial improvements as the model acquires new task-specific capabilities during instruction-tuning."
        },
        {
            "title": "6.1 Safety and Bias",
            "content": "Figures 2a and 2b show model toxicity and truthfulness on the ToxiGen and TruthfulQA benchmarks respectively, under various instruction perturbations. Fine-tuning with perturbed instructions appears to be associated with enhanced safety and truthfulness across most models. On ToxiGen, models like Qwen-7B and Llama-8B exhibit lower average toxicity when fine-tuned with 100% perturbed instructions, while Llama-70B sees improved results with 75% perturbation. However, Qwen-72B performs better with original instructions, which IT VAN ORIG D STOP SHFL MMLU 5-shot BBH CoT GSM8K CoT TruthfulQA ToxiGen % Info+True () % Toxic () 65.8 0.0 65.8 1.5 55.8 2.3 64.2 0.0 64.6 0.0 64.3 0.0 62.5 0.8 63.1 1.1 60.6 0. 60.9 1.3 61.0 0.9 60.5 1.3 62.4 1.5 62.3 1.1 50.0 1.6 50.5 2.1 50.5 1.8 56.0 2.0 59.5 2.3 60.5 1.9 56.5 2.1 58.0 2.0 33.5 0.0 32.8 0.0 35.0 0.0 36.2 0.0 60.2 0.0 59.5 0.0 58.5 0.0 58.9 0.0 61.3 0. 85.4 0.1 87.4 0.1 85.2 0.3 84.0 0.1 91.1 0.1 90.6 0.3 90.1 0.1 90.0 0.1 90.2 0.2 (a) ToxiGen ORIG l - 4 64.4 0.0 64.5 0.0 SHFL 25% 64.8 0.0 SHFL 50% 64.8 0.0 SHFL 75% SHFL 100% 64.8 0. Table 4: Llama-8B trained on Dolly using single perturbation strategy, removing stop words (STOP) or shuffling 25% of the words (SHFL) (top). The same model trained on GPT4-Alpaca under varying levels of word shuffling in the instructions (bottom). All instructions were perturbed in each case. results. Despite the disruption introduced by shuffling 25% of the words, the model achieves the highest scores on both the TruthfulQA and ToxiGen benchmarks. These findings suggest that the model may be able to infer the intended task largely from the input data itself, even when the instructions are perturbed. While we initially hypothesized that introducing noise, through stop-word removal or partial word shuffling, would hinder performance, the results indicate surprising degree of robustness. In some cases, performance even appears to improve under perturbation, suggesting that the model may not rely as heavily on surface-level instruction cues."
        },
        {
            "title": "6.3 Perturbation Intensity Ablation",
            "content": "To investigate how the degree of instruction degradation affects model performance, we perform an ablation study that systematically varies the intensity of word-shuffling perturbation. Specifically, we fine-tune Llama-8B on GPT4-Alpaca with instructions in which 25%, 50%, 75%, and 100% of the words are randomly shuffled. As control, we also fine-tune the model on the original, unmodified instructions. All models are then evaluated on the same set of original, unperturbed benchmarks. The results in Table 4 suggest counterintuitive yet noteworthy trend: as the intensity of perturbation increases, we sometimes observe an improvement in the models performance. Fine-tuning on instructions with 50% or more of the words shuffled often yields strong results across all benchmarks. In some cases, the model trained on fully shuffled instructions, where no coherent phrasing remains, performs better than the model trained on the original unperturbed instructions. These results are broadly consistent with the broader pattern we (b) TruthfulQA Figure 2: Vanilla non-instruction-tuned baselines (VAN) and the fine-tuned models under various instruction perturbations on ToxiGen benchmark (a) and TruthfulQA (b). Lower is better for toxicity, while higher is better for informative and truthful. may indicate higher sensitivity to perturbations. Similarly, on TruthfulQA, three out of four models, including Llama-70B, achieve higher truthfulness and informativeness when fine-tuned on fully perturbed data. possible interpretation is that noisy instructions encourage LLMs to rely less on surface-level patterns and more on robust reasoning. An exception is Qwen-7B, where the vanilla model outperforms all fine-tuned variants, suggesting model-specific sensitivity to instruction noise. 6."
        },
        {
            "title": "Individual Perturbation Strategies",
            "content": "To better understand the impact of specific perturbation strategies on model performance, we conduct an ablation study using two methods: removing stop words (STOP) and shuffling 25% of the words in each instruction (SHFL). We fine-tune Llama8B on Dolly, applying each perturbation strategy to all instructions in the dataset. As baseline, we also fine-tune the model on the original, unmodified instructions. We evaluate the fine-tuned models on the original, unaltered evaluation benchmarks. The results in Table 4 are broadly consistent with the patterns observed in our main findings (Section 5). Notably, the model trained on STOPperturbed instructions showed improved performance over the one trained on the original dataset across several benchmarks, including MMLU (5shot), BBH (CoT), and GSM8K (CoT). Interestingly, the SHFL strategy also yields encouraging Perturbation (1) Del. Stop. (2) Repl. Wor. (3) Add Missp. (4) Del. Wor. Input <instruction> sentence correct adjective order: <instruction> Options: (A) fiberglass old surfboard (B) old fiberglass surfboard <instruction> Which sentence has the following adjective order: <instruction> Options: (A) driving blue car (B) blue driving car <instruction> Which sntnc has the correct adjectivee order: <instruction> Options: (A) lovely midsize green Filipino sock (B) Filipino midsize lovely green sock <instruction> Which has the correct adjective: <instruction> Options: (A) plastic grey old-fashioned small sock (B) small oldfashioned grey plastic sock True Pred (B) (B) (B) (A) (A) (A) (B) (A) Table 5: Generated answers by Llama-70B fine-tuned with 100% perturbed instructions for the \"Which sentence has the correct adjective order\" perturbed question from the BBH benchmark. observed in our main experiments and in the earlier ablation of individual perturbation strategies: performance can improve as superficial instruction cues are degraded. It is possible that heavy instruction noise nudges the model toward the core semantics of the task, which may reduce its dependence on any particular wording and discourage overfitting to fixed prompt templates."
        },
        {
            "title": "6.4 Qualitative Analysis",
            "content": "Table 5 presents example responses from the Llama-70B model, fine-tuned with fully perturbed instructions, for sample question from the BBH benchmark. We observe that the model can often produce correct answers even when instructions are altered by removing stop words or introducing misspellings as in examples (1) and (3). However, its performance appears to deteriorate when key words in the instruction are replaced or deleted. For instance, substituting the word correct with following as in example (2), or deleting the words sentence and order as in example (4), seems to hinder the models ability to respond correctly."
        },
        {
            "title": "7 Theoretical Grounding of Fine-Tuning",
            "content": "on Noisy Instructions Our findings suggest that incorporating perturbations during instruction-tuning may not only enhance model robustness to noisy or perturbed inputs but may also yield improvements on standard, unperturbed instructions. plausible explanation for this effect is that noisy instruction-tuning serves as an implicit form of regularization (Bishop, 1995), potentially encouraging models to move beyond reliance on superficial linguistic patterns. Exposure to wide spectrum of instruction formulations, including those containing syntactic or semantic anomalies, may discourage overfitting to narrow or canonical phrasing. These perturbations effectively broaden the training distribution, functioning as form of data augmentation (Dao et al., 2019; Hernández-García and König, 2018; Vaibhav et al., 2019), and may thereby help LLMs to learn more robust and generalizable task representations. particularly noteworthy observation is that models fine-tuned with 100% perturbed instructions often achieve high accuracy, even when evaluated on standard instructions. This may suggest that the perturbations could act not merely as noise, but as potential source of useful inductive bias that enhances generalization across prompt formats. However, the effectiveness of instruction perturbation appears not to be uniform across all models. While larger models like Llama (70B) and Qwen (72B) exhibit substantial benefits, smaller models, such as Llama (8B) and Qwen (7B), show inconsistent gains. These variations underscore the importance of model-specific calibration of perturbation levels. There may be an upper limit beyond which perturbations become detrimental, particularly for models with limited capacity or for tasks requiring precise instruction-following behavior."
        },
        {
            "title": "8 Conclusion",
            "content": "We explored the impact of instruction perturbation on the robustness and generalization capabilities of instruction-tuned LLMs. By systematically evaluating models of varying sizes across diverse benchmarks, our findings suggest that fine-tuning on structurally perturbed instructions can enhance model performance, particularly under noisy evaluation conditions. Our results indicate that models trained on highly perturbed instructions tend to perform better not only under noisy test conditions but also with standard prompts, suggesting that instruction perturbation encourages more flexible task representations. These findings point to instruction perturbation as simple yet potentially effective strategy for enhancing model resilience, particularly in realworld scenarios where user instructions may be inconsistent or ambiguous. By questioning the assumption that clean instructions are always optimal for tuning, this work offers practical step toward improving instruction-following reliability in large language models. Future research could explore adaptive or semantically-aware perturbation techniques. Such direction may help refine instruction-tuning practices."
        },
        {
            "title": "Limitations",
            "content": "Our experiments were conducted solely with English instructions and downstream tasks due to wide availability of diverse and publicly available instruction-tuning data. We acknowledge that languages differ in their sensitivity to word order and stop words, factor not explored in the current work. Chinese, for example, has fewer stop words and less rigid syntactic structure than English, allowing for greater flexibility in word order. Therefore, the effects of perturbation should be investigated with respect to the specific linguistic characteristics of each language under consideration in future work."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Atsuki Yamaguchi and the anonymous reviewers for their invaluable feedback. AA is supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation grant EP/S023062/1."
        },
        {
            "title": "References",
            "content": "Chris Bishop. 1995. Training with noise is equivalent to Tikhonov regularization. Neural computation, 7(1):108116. Yu-Chu Chang, Xu Wang, Jindong Wang, Yuanyi Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Weirong Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qian Yang, and Xingxu Xie. 2023. survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15:1 45. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. ArXiv, abs/2110.14168. the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 15281537. PMLR. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. QLoRA: Efficient finetuning of quantized LLMs. In Thirty-seventh Conference on Neural Information Processing Systems. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, A. Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke S. Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hanna Hajishirzi. 2024. Olmo: Accelerating the science of language models. In Annual Meeting of the Association for Computational Linguistics. Jiasheng Gu, Hongyu Zhao, Hanzi Xu, Liangyu Nie, Hongyuan Mei, and Wenpeng Yin. 2023. Robustness of learning from task instructions. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1393513948. Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. 2022. Toxigen: large-scale machine-generated dataset for adversarial and implicit hate speech detection. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 33093326. Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023. Free dolly: Introducing the worlds first truly open instructiontuned llm. Company Blog of Databricks. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In International Conference on Learning Representations. Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De Sa, and Christopher Re. 2019. kernel theory of modern data augmentation. In Proceedings of Alex Hernández-García and Peter König. 2018. Data regularization. augmentation instead of explicit arXiv preprint arXiv:1806.03852. Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah A. Smith, Yejin Choi, and Hannaneh Hajishirzi. 2024. Unpacking DPO and PPO: Disentangling best practices for learning from preference feedback. In The Thirtyeighth Annual Conference on Neural Information Processing Systems. Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023. Camels in changing climate: Enhancing lm adaptation with tulu 2. Preprint, arXiv:2311.10702. Taehyeon Kim, Joonkee Kim, Gihun Lee, and Se-Young Yun. 2024. Instructive decoding: Instruction-tuned large language models are self-refiner from noisy instructions. In The Twelfth International Conference on Learning Representations. Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems, volume 35, pages 2219922213. Nathan Lambert, Jacob Daniel Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Validad Miranda, Alisa Liu, Nouha Dziri, Xinxi Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hanna Hajishirzi. 2024. Tülu 3: Pushing frontiers in open language model post-training. ArXiv, abs/2411.15124. Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. 2024. The unlocking spell on base LLMs: Rethinking alignment via In The Twelfth International in-context learning. Conference on Learning Representations. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32143252. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2020. RoBERTa: robustly optimized BERT pretraining approach. Renze Lou, Kai Zhang, Jian Xie, Yuxuan Sun, Janice Ahn, Hanzi Xu, Yu Su, and Wenpeng Yin. 2024. MUFFIN: Curating multi-faceted instructions for improving instruction following. In The Twelfth International Conference on Learning Representations. Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. 2024. Large language models: survey. arXiv preprint arXiv:2402.06196. Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 34703487, Dublin, Ireland. Association for Computational Linguistics. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277. Mengjie Ren, Boxi Cao, Hongyu Lin, Cao Liu, Xianpei Han, Ke Zeng, Wan Guanglu, Xunliang Cai, and Le Sun. 2024. Learning or self-aligning? rethinking instruction fine-tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6090 6105, Bangkok, Thailand. Association for Computational Linguistics. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander Rush. 2022. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations. Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 42224235, Online. Association for Computational Linguistics. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research. Featured Certification. et al. 2023. How far can camels go? exploring the state of instruction tuning on open resources. Advances in Neural Information Processing Systems, 36:7476474786. Jiuding Sun, Chantal Shaib, and Byron Wallace. 2024. Evaluating the zero-shot robustness of instructiontuned language models. In The Twelfth International Conference on Learning Representations. Kaiser Sun and Mark Dredze. 2025. Amuro & char: Analyzing the relationship between pre-training and fine-tuning of large language models. In Proceedings of the 10th Workshop on Representation Learning for NLP (RepL4NLP-2025), pages 131151, Albuquerque, NM. Association for Computational Linguistics. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. In Annual Meeting of the Association for Computational Linguistics. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. Preprint, arXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, and Graham Neubig. 2019. Improving robustness of machine translation with synthetic noise. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 19161920, Minneapolis, Minnesota. Association for Computational Linguistics. Bin Wang, Chengwei Wei, Zhengyuan Liu, Geyu Lin, and Nancy F. Chen. 2024. Resilience of large lanIn Findings guage models for noisy instructions. of the Association for Computational Linguistics: EMNLP 2024, pages 1193911950, Miami, Florida, USA. Association for Computational Linguistics. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah Smith, Iz Beltagy, Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al. 2022. Supernaturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 50855109. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc Le. 2022a. Finetuned language models are zero-shot learners. In International Conference on Learning Representations. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022b. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA. Curran Associates Inc. Tianyi Yan, Fei Wang, James Y. Huang, Wenxuan Zhou, Fan Yin, Aram Galstyan, Wenpeng Yin, and Muhao Chen. 2024. Contrastive instruction tuning. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1028810302, Bangkok, Thailand. Association for Computational Linguistics. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. 2024. Instruction tuning for large language models: survey. Preprint, arXiv:2308.10792. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. survey of large language models. arXiv preprint arXiv:2303.18223. Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Shuaiqiang Wang, Chong Meng, Zhicong Cheng, Zhaochun Ren, and Dawei Yin. 2024. Improving the robustness of large language models In Proceedings of the via consistency alignment. 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 89318941, Torino, Italia. ELRA and ICCL."
        },
        {
            "title": "Appendix",
            "content": "A Instruction-tuning Hyperparameters For instruction-tuning and evaluation, we adopt the implementation from Open Instruct (Wang et al., 2023; Ivison et al., 2023, 2024; Lambert et al., 2024). Table 6 shows the hyperparameters used in our instruction fine-tuning experiments."
        },
        {
            "title": "Value",
            "content": "learning rate lr scheduler type warmup ratio weight decay # train epochs gradient acc. steps max. seq. length LoRA rank LoRA alpha LoRA dropout 1e-5 linear 0.03 0 1 128 4,096 64 16 0.1 Table 6: The instruction fine-tuning hyperparameters."
        },
        {
            "title": "B Full Results",
            "content": "In addition to the results on MMLU (5-shot), BBH (CoT) and GSM8K (CoT) presented in Table 3, we present the full results on MMLU  (Table 7)  , BBH  (Table 8)  and GSM8K  (Table 9)  . IT 0% MMLU (0-shot) 50% 75% 25% 100% 0% MMLU (5-shot) 50% 25% 75% 100% 7 Q 8 l 2 7 Q 0 a VAN. 72.0 0.0 70.6 0.0 69.2 0.3 67.8 0. 66.3 0.4 74.3 0.0 73.0 0.1 71.5 0.1 70.0 0.4 68.6 0. 72.3 0.0 0% 72.3 0.0 25% 72.2 0.0 50% 72.4 0.0 75% 100% 72.4 0.0 71.0 0.0 71.1 0.1 71.0 0.1 71.1 0.2 71.1 0.1 69.8 0.1 69.9 0.2 69.8 0.2 70.0 0.2 70.0 0.2 68.4 0.6 68.3 0.5 68.4 0.6 68.5 0.6 68.6 0.5 66.9 0.4 67.1 0.4 67.1 0.5 67.3 0.3 67.4 0.3 74.3 0.0 74.4 0.0 74.4 0.0 74.3 0.0 74.3 0. 73.0 0.1 73.0 0.1 73.1 0.1 73.0 0.1 73.1 0.0 71.7 0.1 71.8 0.1 71.9 0.1 71.8 0.2 71.9 0.1 70.2 0.4 70.3 0.5 70.5 0.5 70.4 0.5 70.5 0.5 68.9 0.7 69.1 0.6 69.1 0.7 69.1 0.7 69.2 0.6 VAN. 64.2 0. 62.6 0.1 60.9 0.2 59.4 0.0 57.5 0.2 65.8 0.0 64.5 0. 63.1 0.1 62.1 0.3 60.8 0.5 65.1 0.0 0% 64.4 0.0 25% 50% 64.3 0.0 75% 64.9 0.0 100% 64.7 0.0 63.6 0.2 63.0 0.2 62.9 0.2 63.5 0.2 63.4 0.2 61.9 0.4 61.5 0.4 61.6 0.4 62.0 0.5 61.9 0. 60.3 0.1 60.0 0.3 60.2 0.2 60.5 0.1 60.6 0.1 58.5 0.3 58.4 0.4 58.6 0.3 58.8 0.4 58.7 0.3 65.8 0.0 65.9 0.0 65.9 0.0 65.7 0.0 66.0 0.0 64.6 0.2 64.8 0.3 64.8 0.2 64.7 0.2 64.8 0.3 63.3 0.1 63.4 0.2 63.6 0.1 63.6 0.1 63.7 0.1 62.2 0.2 62.3 0.2 62.5 0.2 62.5 0.2 62.5 0. 60.7 0.5 60.9 0.7 61.0 0.6 61.2 0.5 61.2 0.5 VAN. 83.1 0.0 81.7 0.2 80.5 0.2 78.9 0. 77.3 0.3 85.7 0.0 84.5 0.2 83.0 0.3 81.8 0.3 80.3 0. 83.8 0.0 0% 25% 83.7 0.0 83.6 0.0 50% 75% 83.7 0.0 100% 83.8 0.0 82.4 0.1 82.3 0.1 82.3 0.1 82.4 0.2 82.4 0.1 81.1 0.2 81.1 0.3 81.1 0.3 81.2 0.2 81.2 0.2 79.5 0.3 79.6 0.4 79.7 0.3 79.9 0.3 80.0 0.3 77.9 0.3 78.0 0.2 78.2 0.2 78.3 0.2 78.3 0.2 85.8 0.0 85.7 0.0 85.7 0.0 85.8 0.0 85.8 0. 84.6 0.2 84.6 0.3 84.7 0.3 84.7 0.3 84.8 0.2 83.1 0.3 83.1 0.3 83.1 0.3 83.2 0.3 83.2 0.3 82.0 0.2 82.0 0.3 82.0 0.3 82.1 0.3 82.1 0.4 80.5 0.5 80.5 0.6 80.7 0.6 80.7 0.5 80.6 0.6 VAN. 74.4 0. 72.8 0.2 71.1 0.1 69.0 0.2 67.3 0.3 75.8 0.0 74.1 0. 72.2 0.2 70.2 0.4 68.5 0.4 75.7 0.0 0% 75.5 0.0 25% 75.4 0.0 50% 75% 75.6 0.0 100% 76.6 0.0 74.0 0.3 73.8 0.3 73.9 0.3 74.1 0.2 75.1 0.2 72.6 0.2 72.4 0.3 72.7 0.1 72.8 0.2 73.7 0. 70.8 0.1 70.7 0.1 70.9 0.1 71.0 0.1 72.0 0.0 69.5 0.3 69.2 0.3 69.6 0.3 69.7 0.3 70.7 0.3 78.1 0.0 77.9 0.0 78.0 0.0 78.0 0.0 78.6 0.0 76.7 0.3 76.5 0.2 76.6 0.3 76.8 0.3 77.3 0.2 74.9 0.3 74.8 0.4 74.8 0.4 75.1 0.3 75.6 0.3 73.0 0.4 72.8 0.3 73.0 0.4 73.4 0.4 74.1 0. 71.4 0.5 71.2 0.4 71.6 0.4 71.8 0.4 72.8 0.3 Table 7: Results of evaluating the fine-tuned models under various instruction perturbations using the MMLU evaluation benchmark. Accuracy is reported on both the original evaluation instructions (0%) and the various perturbed evaluation instructions. Bold values denote the best performance across each model. IT 0% 25% BBH (CoT) 50% 75% 100% 0% 25% BBH (direct) 50% 75% 100% VAN. 66.7 0.1 63.9 0.4 60.8 0.4 57.7 0. 54.9 0.5 31.0 0.1 27.0 0.1 22.9 0.4 18.6 0.2 14.5 0. 66.8 0.0 0% 66.7 0.0 25% 67.0 0.0 50% 67.4 0.0 75% 100% 66.6 0.0 62.7 0.2 63.3 0.3 64.0 0.2 63.9 0.3 63.4 0.2 58.7 0.2 59.7 0.2 61.1 0.3 60.7 0.2 60.3 0.3 54.3 0.5 55.9 0.5 57.7 0.6 57.6 0.4 56.8 0.4 50.6 0.6 52.4 0.6 54.8 0.6 54.7 0.6 53.8 0.7 50.5 0.0 50.2 0.0 50.2 0.0 50.4 0.0 50.0 0. 48.9 0.2 48.4 0.1 48.3 0.2 48.7 0.2 48.2 0.2 47.0 0.3 46.5 0.2 46.3 0.2 46.9 0.3 46.4 0.2 45.1 0.6 44.4 0.7 43.9 0.5 44.9 0.7 44.1 0.3 43.2 0.7 42.3 0.7 41.9 0.7 43.2 0.7 42.1 0.5 VAN. 64.5 0. 62.5 0.3 60.2 0.4 57.5 1.0 55.0 0.9 45.7 0.1 44.4 0. 43.0 0.2 41.4 0.3 40.1 0.7 63.0 0.4 0% 66.0 0.1 25% 62.7 0.0 50% 75% 62.9 0.4 100% 66.2 0.1 63.4 0.1 60.5 0.4 64.4 0.3 64.1 0.3 64.2 0.5 61.1 0.3 60.0 1.9 62.0 0.5 61.8 0.3 62.0 0. 58.7 0.7 59.1 0.4 59.3 0.5 59.1 0.5 59.2 0.8 56.5 0.6 56.5 0.6 56.7 0.5 56.3 0.5 56.8 0.4 45.1 0.4 47.4 0.1 46.1 0.1 45.3 0.5 47.3 0.1 46.0 0.3 44.3 0.3 46.3 0.2 45.8 0.1 45.8 0.3 44.2 0.3 43.6 0.8 44.6 0.2 44.3 0.4 44.5 0.2 42.3 0.1 42.6 0.3 42.8 0.3 42.4 0.1 42.6 0. 40.7 0.5 41.3 0.4 41.5 0.3 40.9 0.3 41.2 0.5 VAN. 82.7 0.1 79.2 0.2 75.4 0.2 71.7 0. 68.1 0.8 26.4 0.1 23.8 0.4 21.2 0.4 18.4 0.4 15.8 0. 83.8 0.1 0% 83.8 0.1 25% 50% 83.3 0.1 83.6 0.0 75% 100% 83.6 0.0 80.5 0.2 80.4 0.2 80.2 0.2 80.3 0.2 80.4 0.2 77.3 0.3 77.4 0.4 77.2 0.2 77.2 0.3 77.3 0.2 73.8 0.5 74.0 0.8 73.7 0.6 73.9 0.6 73.8 0.4 70.8 1.1 70.8 1.0 70.6 0.8 70.4 0.8 70.8 0.8 66.6 0.1 66.5 0.0 66.5 0.1 66.5 0.0 67.1 0. 64.5 0.2 64.5 0.2 64.4 0.3 64.5 0.2 65.0 0.3 62.2 0.2 62.2 0.2 62.2 0.5 62.2 0.5 62.7 0.5 59.7 0.4 59.7 0.6 59.9 0.6 60.0 0.6 60.3 0.4 57.6 0.8 57.5 0.8 57.6 0.7 57.6 0.8 58.0 0.6 VAN. 78.3 0. 75.7 0.2 73.3 0.2 70.3 0.4 68.1 0.4 58.1 0.1 56.5 0. 54.9 0.5 53.0 0.8 51.3 0.8 81.8 0.1 0% 81.4 0.1 25% 81.2 0.1 50% 75% 81.5 0.1 100% 81.7 0.1 78.9 0.1 78.7 0.1 78.5 0.2 78.9 0.3 79.0 0.2 75.9 0.2 76.0 0.3 75.9 0.3 76.3 0.2 76.4 0. 72.7 0.4 72.8 0.3 73.1 0.4 73.3 0.4 73.3 0.5 70.1 0.4 70.2 0.5 70.6 0.5 70.6 0.7 70.8 0.6 63.9 0.1 63.2 0.1 63.1 0.1 63.4 0.1 64.7 0.0 61.7 0.2 61.1 0.2 61.2 0.3 61.4 0.3 62.7 0.3 59.2 0.5 58.9 0.6 58.9 0.6 59.1 0.7 60.3 0.8 56.3 0.7 56.5 0.4 56.2 0.5 56.6 0.5 57.9 0. 53.7 0.4 54.2 0.5 54.0 0.7 54.4 0.7 55.4 0.7 7 Q 8 l 2 w 0 7 l Table 8: Results of evaluating the fine-tuned models under various instruction perturbations using the BBH evaluation benchmark. The average exact match (EM) is reported on both the original evaluation instructions (0%) and the various perturbed evaluation instructions. Bold values denote the best performance across each model. IT 0% 25% GSM8K (CoT) 50% 75% 100% 0% GSM8K (direct) 50% 75% 25% 100% 7 Q 8 l B 2 7 Q 0 7 l VAN. 79.9 0.2 12.5 0. 22.9 0.6 33.0 1.4 42.7 1.2 22.6 0.0 5.2 0.3 8.1 0. 10.6 1.0 13.9 0.6 80.6 0.0 0% 81.1 0.0 25% 80.6 0.0 50% 75% 80.5 0.0 100% 80.0 0.0 12.6 0.5 12.6 0.4 12.6 0.5 12.8 0.5 12.6 0.2 24.5 0.5 24.7 0.5 24.6 0.5 24.4 0.6 24.8 0.6 34.6 1.0 34.5 1.2 34.3 0.7 34.0 0.9 34.3 1. 44.6 1.2 44.2 1.4 44.5 1.1 44.4 0.9 45.1 0.8 25.0 0.0 24.9 0.0 25.3 0.0 24.9 0.0 25.7 0.0 4.9 0.3 4.9 0.3 4.9 0.2 4.8 0.3 5.0 0.3 8.3 0.6 8.4 0.6 8.4 0.7 8.6 0.6 8.7 0.5 VAN. 56.3 0. 9.0 0.7 16.3 0.6 23.5 0.6 30.5 1.2 14.3 0.1 3.9 0. 5.6 0.7 58.4 0.0 0% 58.5 0.0 25% 58.2 0.0 50% 75% 57.4 0.0 100% 58.4 0.0 9.2 0.1 9.4 0.2 9.2 0.2 9.1 0.2 9.2 0.2 16.6 0.7 16.8 0.8 16.9 0.6 16.9 0.7 17.1 0.6 23.8 1.0 23.9 0.7 24.0 0.9 23.8 0.8 23.7 1.2 28.1 1.0 27.7 0.9 27.8 1.0 27.6 1.2 27.8 1. 14.3 0.0 14.6 0.0 14.1 0.0 14.1 0.0 14.2 0.0 3.6 0.2 3.9 0.1 3.5 0.1 3.7 0.1 3.6 0.0 5.0 0.3 5.0 0.4 5.0 0.4 5.2 0.4 5.3 0.3 11.8 0.5 11.8 0.4 11.7 0.4 12.1 0.3 12.1 0.3 7.3 0.6 6.7 0.5 6.6 0.4 6.6 0.4 6.6 0.4 6.9 0. 16.2 0.9 16.1 1.1 16.1 0.9 16.2 1.1 16.3 0.9 8.1 0.5 7.4 1.0 7.3 1.0 7.5 1.1 7.5 1.1 7.8 1.0 VAN. 88.8 0.2 14.9 0. 28.1 0.7 40.8 1.0 53.0 1.5 43.2 0.0 7.9 0.3 14.0 0. 19.7 1.0 25.5 1.0 90.0 0.2 0% 89.9 0.2 25% 89.9 0.2 50% 90.3 0.3 75% 100% 90.2 0.1 15.3 0.4 15.3 0.5 15.4 0.4 15.6 0.3 15.6 0.5 29.0 0.4 29.6 0.4 29.3 0.5 29.6 0.7 29.7 0.7 42.7 1.1 43.0 1.1 42.8 1.4 42.9 1.7 43.4 1. 55.0 1.7 55.5 1.6 55.5 1.9 55.5 2.3 55.9 2.0 44.8 0.2 44.7 0.1 44.2 0.1 43.7 0.1 44.0 0.2 8.3 0.7 8.4 0.6 8.3 0.5 8.5 0.6 8.4 0.5 15.3 0.8 15.1 0.5 15.1 0.4 15.4 0.5 15.3 0.5 21.3 0.9 21.1 0.7 21.4 0.6 21.4 0.6 21.4 0.7 27.6 1.0 27.5 1.0 27.5 0.9 27.6 0.8 27.5 1. VAN. 80.2 0.1 13.0 0.3 24.1 0.5 34.7 1.5 43.9 0. 34.4 0.2 6.6 0.6 10.9 0.6 14.7 1.0 19.1 1.2 82.3 0.2 0% 82.1 0.2 25% 80.2 0.3 50% 75% 81.6 0.2 100% 82.0 0. 13.7 0.7 14.0 0.6 13.5 0.6 13.8 0.4 13.7 0.4 25.4 0.4 25.5 0.8 25.6 0.7 25.6 0.7 26.1 0.6 37.0 1.0 37.0 1.3 37.0 1.4 37.3 1.1 38.1 1.8 47.5 1.0 47.7 0.5 47.6 1.0 48.2 0.7 48.8 1.2 35.3 0.1 35.0 0.1 34.3 0.2 34.4 0.1 34.5 0.2 6.8 0.5 7.0 0.5 7.0 0.5 7.1 0.6 7.0 0. 11.8 0.4 11.7 0.7 11.8 0.8 11.9 0.5 12.1 0.9 16.2 0.6 16.3 0.7 16.4 0.5 16.6 0.9 16.4 0.6 20.7 0.8 20.8 0.8 20.8 0.8 20.8 0.8 21.4 0.7 Table 9: Results of evaluating the fine-tuned models under various instruction perturbations using the GSM8K evaluation benchmark. The exact match (EM) is reported on both the original evaluation instructions (0%) and the various perturbed evaluation instructions. Bold values denote the best performance across each model."
        }
    ],
    "affiliations": [
        "Department of Computer Science University of Sheffield, UK"
    ]
}