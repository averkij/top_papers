{
    "paper_title": "Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation",
    "authors": [
        "Qingwen Bu",
        "Hongyang Li",
        "Li Chen",
        "Jisong Cai",
        "Jia Zeng",
        "Heming Cui",
        "Maoqing Yao",
        "Yu Qiao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The increasing demand for versatile robotic systems to operate in diverse and dynamic environments has emphasized the importance of a generalist policy, which leverages a large cross-embodiment data corpus to facilitate broad adaptability and high-level reasoning. However, the generalist would struggle with inefficient inference and cost-expensive training. The specialist policy, instead, is curated for specific domain data and excels at task-level precision with efficiency. Yet, it lacks the generalization capacity for a wide range of applications. Inspired by these observations, we introduce RoboDual, a synergistic dual-system that supplements the merits of both generalist and specialist policy. A diffusion transformer-based specialist is devised for multi-step action rollouts, exquisitely conditioned on the high-level task understanding and discretized action output of a vision-language-action (VLA) based generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in real-world setting and 12% gain on CALVIN by introducing a specialist policy with merely 20M trainable parameters. It maintains strong performance with 5% of demonstration data only, and enables a 3.8 times higher control frequency in real-world deployment. Code would be made publicly available. Our project page is hosted at: https://opendrivelab.com/RoboDual/"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 1 ] . [ 2 1 0 0 8 0 . 0 1 4 2 : r Preprint. TOWARDS SYNERGISTIC, GENERALIZED AND EFFICIENT DUAL-SYSTEM FOR ROBOTIC MANIPULATION Qingwen Bu1,4 Hongyang Li2,4 Li Chen2 Jisong Cai4 Jia Zeng4 Heming Cui2 Maoqing Yao3 Yu Qiao4 1Shanghai Jiao Tong Univeristy 2The University of Hong Kong 3AgiBot 4Shanghai AI Lab Figure 1: Overview of RoboDual. Our objective is to develop synergistic dual-system framework which supplements the generalizability of large-scale pre-trained generalist with the efficient and task-specific adaptability of specialist. (a) The fast specialist policy obsesses real-time and accurate control by aid of the slow yet generalized output from the generalist one trained with large-scale data. (b) RoboDual exhibits significant improvement in terms of performance and efficiency over single standalone option and surpasses previous state-of-the-arts in the real-robot setting."
        },
        {
            "title": "ABSTRACT",
            "content": "The increasing demand for versatile robotic systems to operate in diverse and dynamic environments has emphasized the importance of generalist policy, which leverages large cross-embodiment data corpus to facilitate broad adaptability and high-level reasoning. However, the generalist would struggle with inefficient inference and cost-expensive training. The specialist policy, instead, is curated for specific domain data and excels at task-level precision with efficiency. Yet, it lacks the generalization capacity for wide range of applications. Inspired by these observations, we introduce RoboDual, synergistic dual-system that supplements the merits of both generalist and specialist policy. diffusion transformer-based specialist is devised for multi-step action rollouts, exquisitely conditioned on the high-level task understanding and discretized action output of vision-languageaction (VLA) based generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in real-world setting and 12% gain on CALVIN by introducing specialist policy with merely 20M trainable parameters. It maintains strong performance with 5% of demonstration data only, and enables 3.8 higher control frequency in real-world deployment. Code would be made publicly available. Our project page is hosted at https://opendrivelab.com/RoboDual/. 1 Preprint."
        },
        {
            "title": "INTRODUCTION",
            "content": "The pursuit of versatile and adaptive robotic intelligence has been central objective in the robotics community for decades (Franklin, 1997; Kunze et al., 2018; Duan et al., 2022). Conventional robot learning methods typically develop policies through datasets curated for the designated robot and its specific task. The yielding policy could be deemed as specialist, including the popular ACT (Zhao et al., 2023) and Diffusion Policy (Chi et al., 2023). It exhibits high precision in dedicated scenarios and tasks, and yet often demonstrates limited generalization ability (Brohan et al., 2023a). As robots are increasingly employed in open-ended and multi-task environments, the demand for systems capable of handling diverse tasks and adapting seamlessly across various embodiments has surged. This has fueled the development of the generalist, such as RT-2 (Brohan et al., 2023a) and Octo (Ghosh et al., 2024). They leverage extensive, heterogeneous datasets to enhance cross-domain generalizability and aim to transfer web knowledge to robotic control. Recent advances on VisionLanguage-Action (VLA) approaches (Li et al., 2024; Brohan et al., 2023a; Kim et al., 2024; Covariant, 2024) exemplify the potential of generalist policy to meet the ever-evolving demands. VLAs integrate vast cross-embodiment data with pre-trained large (vision-)language models, facilitating capabilities such as common-sense reasoning and instruction following (Zhao et al., 2024). While VLA-based generalists excel at knowledge transfer and generalization across diverse scenarios, several limitations remain: 1) They cannot be directly deployed to new embodiments or environments out-of-the-box without adaptation (Wang et al., 2024b). The finetuning process is more data and training intensive compared to specialist policies (Fu et al., 2024). 2) Though VLAs are skilled in high-level decision making, their large model nature leads to extremely high inference latency (Brohan et al., 2023a). This pivotal bottleneck makes them unsuitable for fine-grained control in dynamic environments. 3) Current generalist models support single-frame RGB observations only, which, while enabling training on larger-scale datasets, restricts their effectiveness in tasks where additional sensory inputs such as depth or tactile feedback play pivotal roles. In the meantime, incorporating these extra modalities requires resource-intensive re-training (Han et al., 2024) and runs the risk of catastrophic forgetting (Zhai et al., 2023b). Significant efforts, such as model quantization and fine-tuning of the generalist with multimodal data, have been made to address the aforementioned limitations (Kim et al., 2024; Zhen et al., 2024). Nonetheless, these approaches still face inevitable performance declines or data scarcity-related issues. This raises the question of whether it is sufficient to rely solely on enhancements to generalists to resolve these challenges. We recall that generalist model offers broad generalizability and benefits from web-scale pre-training, while specialist policy is competent at efficiency and fast adaptation to specific tasks. Based on the insights, we propose generalized and efficient framework in which both policies complement each other for improved manipulation, as illustrated in Figure 1(a). Our work introduces novel dual-system1 synergy approach, namely RoboDual. It is designed to harness the advantages of both parties and facilitate the practical deployment of large generalist policies. We start with the large-scale pre-trained OpenVLA (Kim et al., 2024) to establish our generalist policy. For seamless cooperation between the two models, we implement the specialist model as lightweight and scalable diffusion transformer (Peebles & Xie, 2023) policy. The specialist learns the multimodal action distribution by utilizing any sensory inputs and the generalist outputs adaptively through unified conditioning mechanism. Latent representations and discretized action outputs from the generalist enable our specialist to adapt to new tasks or environments efficiently with minimal data and training costs. During inference, the generalist provides deliberate yet comparatively slower conditioning, which supports multistep roll-outs of the fast-reacting specialist to achieve precise and generalized control. In this way, RoboDual is rendered with high-level task understanding and generalizability from the generalist, combined with efficient action refinement of the specialist, achieving outstanding performance across diverse array of tasks. As demonstrated in Figure 1(b), it realizes 12% performance gain over the generalist-only variant on CALVIN (Mees et al., 2022b) with minimal training cost. In real-robot setting, RoboDual outperforms both specialist and generalist baselines by significant margin. To summarize, our contributions are threefold: 1In cognitive science, the dual-system framework distinguishes between System-1 that engages in rapid, automatic processing, and System-2, which involves slow, deliberate thinking with intentional effort (Kahneman, 2011). By analogy, it can be suggested that our specialist model may operate in manner akin to System-1, while the generalist model might reflect the characteristics of System-2. Preprint. We bring in novel approach that integrates generalist and specialist policies into synergistic framework, dubbed as RoboDual, following dual-system spirit. Our methodology leverages the merits of each party and paves the way for the practical application of VLA models. We propose diffusion transformer-based specialist policy that enables real-time control, adaptively conditioned by generalist outputs and various sensory inputs. The framework facilitates the flexible integration of diverse modalities and allows for the deconstruction of the two models on the aspect of training data, thereby enhancing their individual strengths and capabilities. We demonstrate that the dual-system approach surpasses both specialistand generalist-only models in various tasks, through extensive real-world and simulation experiments."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Generalist policy. The RT-X series of works (Brohan et al., 2023b;a) have sparked significant progress in the development of multi-task generalist policies (Yang et al., 2023; Ghosh et al., 2024) by leveraging extensive cross-embodiment datasets (Padalkar et al., 2024; Walke et al., 2023; Khazatsky et al., 2024). Octo (Ghosh et al., 2024) employs transformer-based policy trained on 800k trajectories from the Open-X-Embodiment dataset (Padalkar et al., 2024), enabling flexible fine-tuning for novel robotic configurations. OpenVLA (Kim et al., 2024), which represents the most advanced generalist policy to date, directly integrates pre-trained vision-language models to generate robotic actions by treating them as tokens within the language models vocabulary. Unlike Octo, which relies solely on pre-trained language encoder T5 (Raffel et al., 2020) and derives generalization primarily from large-scale in-domain policy training, OpenVLA harnesses world knowledge from much broader vision-language dataset. Despite its demonstrated generalizability, OpenVLAs model size, which comprises billions of parameters, hinders both data and inference efficiency. Hence it would confine the deployment on heterogeneous robot setup as generalist policy. In contrast, our approach introduces novel and cost-effective dual-system framework to address these caveats, instead of developing another larger generalist model reliant on extensive datasets. Specialist policy. We regard specialist models as policies specifically trained to execute narrow set of tasks or functions with high precision (Goyal et al., 2023; 2024). These models typically utilize curated datasets tailored to their specific applications (Zhao et al., 2023; Chi et al., 2023). While many specialist policies excel in few-shot imitation learning, they often lack the integration of language inputs, necessitating the training of distinct models for different tasks. notable trend in prior studies is the reliance on 3D representations (Shridhar et al., 2023; Gervet et al., 2023; Ze et al., 2023; Yan et al., 2024; Goyal et al., 2024) to improve performance on low-dimensional control tasks. Transformer-based models have emerged as powerful tools for extracting multimodal features, enhancing manipulation capabilities through their flexibility in processing heterogeneous observations (Kim et al., 2021; Dasari & Gupta, 2021; Goyal et al., 2023; Simeonov et al., 2023). The recent development of the Diffusion Policy (Chi et al., 2023), along with subsequent works (Ze et al., 2024; Prasad et al., 2024), has proven effective in managing multimodal action distributions for robotic manipulation while exhibiting improved training stability. In our work, the specialist model is designed as scalable diffusion transformer (Peebles & Xie, 2023) policy, adept at handling multimodal inputs and generalist outputs as conditioning in unified framework. Furthermore, we demonstrate how collaboration with generalist model enhances generalization and enables the execution of multi-instruction tasks that previous specialist-only models struggle to address. Hierarchical control with LLMs. The rise of Large Language Models (LLMs) and their ability to interpret prompts and perform reasoning has sparked interest in their application to robotics (Wang et al., 2024a). key area is high-level task planning, where LLMs decompose tasks using natural language, as demonstrated in SayCan (Ahn et al., 2022), PaLM-E (Driess et al., 2023), and HiP (Ajay et al., 2024). This enables robots to translate abstract commands into concrete actions. This is followed by works using structured code for control (Liang et al., 2023; Singh et al., 2023), which map user instructions into executable programs. Another line of work, including RoboFlamingo (Li et al., 2024) and LCB (Shentu et al., 2024), involves hierarchical control via latent space representations, where they employ an additional action decoder network on top of LLM latent outputs to regress actions directly. Recent works also explore action tokenization through VQVAE techniques (Van Den Oord et al., 2017) to better bridge LLMs and actions (Wang et al., 2024c; Szot et al., 2024). Although our framework shares similar hierarchical philosophy, it does not explicitly decompose tasks and does not require end-to-end optimization of decoupled policies. Instead, we utilize both discretized outputs 3 Preprint. Figure 2: The overall architecture of RoboDual. (a) Generalist. The generalist takes as inputs RGB images and language prompts, generating conditioning sources for the specialist model, including latent representations and discretized actions. (b) Specialist. Comprising stacked Diffusion Transformer (DiT) blocks, the specialist is conditioned on multiple sensory inputs and the generalists output through cross-attention mechanism. It predicts noise injected into ground truth actions, providing fast, precise control by leveraging the slower, high-level guidance of the generalist. and latent embeddings as interfaces to connect two models, rather than pre-defined low-level skills. The generalist and specialist can be decoupled on the aspect of training data to develop their respective capabilities, which also brings flexibility to utilizing various modal inputs."
        },
        {
            "title": "3 ROBODUAL: GENERALIST-SPECIALIST SYNERGY",
            "content": "Our goal is to develop synergistic and generalized framework that leverages the strengths of both generalist and specialist policies while simultaneously addressing their respective limitations beyond mere integration. We introduce the generalist policy, which forms meticulous and robust planner in our framework in Section 3.1. In Section 3.2, we provide design principles of the specialist policy, which is optimized for precise, real-time control and allows unified conditioning. Finally, we describe our training and inference protocols in Section 3.3. The systematic diagram is illustrated in Figure 2. 3.1 GENERALIST: AN AUTO-REGRESSIVE VISION-LANGUAGE-ACTION MODEL Figure 2(a) demonstrates our generalists architecture. Our generalist model is built upon OpenVLA (Kim et al., 2024), 7B-parameter autoregressive vision-language-action model trained with large corpus of robotic manipulation data, including Open-X-Embodiment (Padalkar et al., 2024), Bridge V2 (Walke et al., 2023), DROID (Khazatsky et al., 2024)), etc. The generalist model follows the architecture of Prismatic-7B (Karamcheti et al., 2024) vision-language model (VLM), which consists of fused visual encoder from different backbones (Zhai et al., 2023a; Oquab et al., 2024), projection layer to align visual embeddings with language modality, and large language model LLaMA2 (Touvron et al., 2023). Despite extensive training on large-scale cross-embodiment dataset, OpenVLA is not capable of functioning in zero-shot manner in new environments or embodiments (Wang et al., 2024b). Adaptation to our specific robotic setup and test environments (with novel coordination system, camera angle, etc.) remains necessary, which we accomplish through LoRA (Hu et al., 2022) fine-tuning. Nonetheless, we intend to leverage the massive pre-trained knowledge embedded in OpenVLA to endow our dual-system framework with certain generalizability. Autoregressive generation of action chunking. Following RT-2 (Brohan et al., 2023a) and OpenVLA (Kim et al., 2024), we map the least used 256 words in the LLaMA tokenizer vocabulary into uniformly distributed action bins within [1, 1]. This approach allows us to detokenize language tokens into discretized actions based on their corresponding indices in the vocabulary. The generalist model decodes every degree-of-freedom of actions in an auto-regressive manner, where the decoding for the current token is dependent upon input prompts and previously decoded tokens. We further extend the original OpenVLA to predict action chunks with temporal length of kg. This longer-range 4 Preprint. planning on the generalist side enhances its own ability to capture non-Markovian behavior in human demonstrations, and also facilitates more informative conditioning provided to the specialist model. The action output corresponding to each time step is separated by [space] token in the tokenizer vocabulary. However, action chunking increases the inference latency of VLA due to the generation of greater number of tokens. This further prompts the need for specialist model that runs at higher frequency between consecutive VLA outputs to achieve more responsive control."
        },
        {
            "title": "3.2 SPECIALIST: A CONTROLLABLE DIFFUSION TRANSFORMER POLICY",
            "content": "Built on top of pre-trained generalist policy, the specialist is to achieve improved performance while reducing control latency, even with limited training data and compute. To fully exploit the multimodal sensory inputs necessary for effective manipulation, as well as the privileged knowledge from the generalist policy, we design the specialist based on Diffusion Transformer (DiT) (Peebles & Xie, 2023), to perform controllable action sequences denoising. Base architecture. Figure 2(b) illustrates the architecture of our specialist model, which is primarily composed of stacked DiT blocks. Each block includes causal self-attention layer to process temporal actions, cross-attention layer to fuse information, and point-wise feedforward network that performs non-linear transformations. Drawing parallels with image diffusion models (Saharia et al., 2022), we treat 7-DoF action as pixel with seven channels, which is linearly projected into single token and processed by the diffusion model. This formulation facilitates seamless temporal expansion of action tokens, enabling action chunk prediction (Zhao et al., 2023) with flexible temporal length of ks. We employ Vision Transformers (ViT) (Dosovitskiy et al., 2021) as generalized sensory encoders to encode all possible input modalities (e.g., RGB, depth, and tactile), with minor modifications on the patchify layer given different number of channels. DINO (Caron et al., 2021) pre-trained model is leveraged to encode the RGB inputs, which is frozen during training. Encoders for other modalities are constrained to six layers with hidden size of 256 to ensure efficiency. Beyond what we have explored, our framework is also adaptable to non-image inputs that can be encoded into sequence of embeddings. Action denoising with multimodal conditioning. The specialist model leverages multiple sources of conditioning and their corresponding conditioning approaches to enhance decision-making: 1) proprioceptive states (Proprio.) of the robot, 2) multimodal sensory inputs, 3) generalists discretized action outputs, and 4) latent representations (refer to Figure 2(a)) from the generalist model. Each source contributes distinct information, facilitating more informed and robust policy. The proprioceptive states are processed through two-layer MLP and combined with time-step embedding to enable adaptive sample-wise conditioning. Beyond regressing γ and β parameters for adaptive layer normalization (Perez et al., 2018), scaling parameter α, is introduced in the residual connections to ensure stable conditioning and improve training robustness (Peebles & Xie, 2023). For sensory inputs, we incorporate perceiver resampler (Alayrac et al., 2022), consisting of multihead attention pooling module followed by an MLP layer, to selectively distill key features from observation embeddings generated by ViTs while reducing token length. Specifically, we employ eight learnable queries for every sensory input. The resampler preserves performance and accelerates the multistep denoising process, particularly when dealing with multi-source inputs advantageous for manipulation tasks, such as multiview observations, historical frames, and multimodal data. To condition the specialist on the discretized actions from the generalist, we concatenate them with the noised action of the corresponding time step, and project the concatenated inputs into shared latent space through linear layers. This approach is inspired by video prediction models (Blattmann et al., 2023), which concatenate initially known frames with noised inputs to predict future states. Conditioning our specialist model on the task and action latents derived from the generalist involves utilizing linear projection on the generalist tokens to align their hidden spaces. Despite simplicity, it is parameter-efficient and preserves the original positional encoding within VLA. Finally, the projected generalist latents, along with the resampled observation embeddings, are concatenated and utilized as keys and values in the cross-attention layer. Our diverse conditioning enables the specialist model to process comprehensive contextual data effectively, prompting more informed decision-making. Given that the generalist and specialist models operate asynchronously during inference (with single generalist inference supporting multiple specialist rollouts), we implement shifted-window 5 Preprint. conditioning mechanism. Specifically, following τs steps of specialist inference, only the most recent kg τs generalist actions are sampled as conditioning before second update. We also apply this mechanism as an augmentation during training to ensure latency robustness of RoboDual. The specialist model is optimized to denoise future trajectories by conditioning on the lagged generalists output, with the specialists observation input leading the generalists by τ [0, kg] steps."
        },
        {
            "title": "3.3 TRAINING & INFERENCE PROTOCOL",
            "content": "Generalist training. Disparate from recent studies (Li et al., 2024; Szot et al., 2024) that directly applies action regression loss to the output tokens of VLMs, we follow OpenVLA (Kim et al., 2024) and use discrete token prediction, which naturally aligns the next-token prediction approach of decoder-only LLMs (Chen et al., 2021). The model gϕ is fed with prompts and prefixes of the ground truth actions a<i, and trained to minimize the sum of next-token negative log-probabilities: (cid:34) Lgen = Ep,a<i Na(cid:88) i= log gϕ(ˆai = ai p, a<i) , (1) (cid:35) where Na represents the total length of action tokens. During the inference stage, the generalist decodes ˆai based on previously decoded tokens ˆa<i, instead of ground truth actions. Specialist training. Following Diffusion Policy (Chi et al., 2023), we train our specialist with an action denoising objective. Given an action trajectory of temporal length ks from dataset a0 Da, randomly sampled noise ϵ (0, I), and an arbitrary timestamp U(1, ), where Z, = 100, the forward diffusion process is formulated in closed form as at = 1 αtϵ. αt denotes noise schedule that performs one-step noise adding (Ho et al., 2020). We optimize the following training objective to train the specialist model πθ as follows: αta0 + Lspec = Et,c,a0,ϵ (cid:2)ϵ πθ( αta0 + 1 αtϵ, c, t)2(cid:3) , (2) where denotes the set of conditioning sources. We explore training lightweight specialist model from scratch, conditioned on pre-trained generalist, and promote synergistic interactions between the two systems. Introducing merely 20M trainable parameters and one-hour training with our specialist model, the resulting dual-system demonstrates more significant performance improvement compared to the gains achieved from several days of additional training on the VLA alone (17% v.s. 10%). We delve into this in Section 4.4. Further details regarding training hyperparameters and architecture design are provided in Appendix B."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We conduct extensive experiments to evaluate the performance of our method and to highlight its notable attributes concerning generalizability, efficiency, and adaptability. We intend to study the following research questions: I. (Section 4.2) Could RoboDual demonstrate higher success rates in simulation and real-world tests compared to previous methods? II. (Section 4.3) Does RoboDual perform generalizable manipulation? III. (Section 4.4) How is the adaptation and inference efficiency of RoboDual? IV. (Section 4.5) What are the key factors contributing to the dual-system synergy? 4.1 EVALUATION SUITE Simulation experiments on CALVIN. CALVIN (Mees et al., 2022b) is widely recognized simulation benchmark for assessing long-horizon language-conditioned manipulation tasks. Our objective is to demonstrate the generalizability of our system in multitask learning using free-form language instructions. Additionally, we investigate how the specialist model can leverage multiple input modalities, beyond the third-view RGB input of the generalist, to enhance manipulation performance. Further information regarding the benchmark and implementation details are provided in Appendix A. Real-world robot experiments. All real-world experiments are conducted with an ALOHA platform featuring 7-DoF action space and third-view RGB camera. We evaluate policies on both singleinstruction tasks (Lift the pod did, Pour shrimp into bowl, and Push block Left) and multiinstruction tasks (Put <obj> into basket and Knock <obj> over). Additionally, we propose comprehensive set of evaluation tasks that cover various axes of generalization: 1) position variation, 6 Preprint. Table 1: Language-conditioned visuomotor control on CALVIN ABCD. We report success rates along with the average length of completed tasks (out of the whole 5 tasks) per evaluation sequence. Lang and All denote whether models are trained only with the subset vision-language data pairs. Method MCIL (Lynch & Sermanet, 2021) HULC (Mees et al., 2022a) RT-1 (Brohan et al., 2023b) MDT (Reuss et al., 2024) RoboFlamingo (Li et al., 2024) SuSIE (Black et al., 2024) GR-1 (Wu et al., 2024) 3D Diffuser Actor (Ke et al., 2024) RoboDual (Ours) Train sets All All Lang Lang Lang All Lang Lang Lang 1 30.4 41.8 53.3 61.7 82.4 87.0 85.4 92.2 94.4 Task completed in row (%) 3 4 1.3 16.5 22.2 41.6 61.9 69.0 71.2 78.7 82.7 0.2 5.7 9.4 23.8 46.6 49.0 59.6 63.9 72.1 0.0 1.9 3.8 14.7 33.1 38.0 49.7 51. 62.4 5 0.0 1.1 1.3 8.7 23.5 26.0 40.1 41.2 54.4 Avg. Len. 0.31 0.67 0.90 1.54 2.48 2.69 3.06 3. 3.66 2) visual distractions, 3) unseen background, and 4) novel objects. Each task is collected with teleportation for 20-120 demonstrations based on their complexity. To establish our baselines, we take the most advanced and widely adopted specialist policies, ACT (Zhao et al., 2023) and Diffusion Policy (Chi et al., 2023), alongside generalist models, Octo (Ghosh et al., 2024) and OpenVLA (Kim et al., 2024), for comparative analysis. Specialists are trained in single-task manner, while the generalists are first trained with the combination of all tasks and then tuned on specific scenarios to optimize their performance. For clearer comparison, we implement our method by first training the generalist across all tasks, followed by training the specialist separately using either taskspecific data (Ours-single-task) or multi-task data (Ours-multi-task). To enable specialist baselines, which do not use language inputs, to effectively learn multi-instruction tasks, we incorporate FiLM conditioning (Perez et al., 2018) into the visual backbone as RT-1 (Brohan et al., 2023b). For all tasks, we report the average success rate over 15 independent runs. More details are in Appendix A. 4.2 COMPARISON TO STATE-OF-THE-ARTS Method Task completed in row (%) 5 3 1 Table 2: Evaluations on robustness to free-form language instructions. RoboDual shows exceptional instruction-following capability. CALVIN benchmark. We compare the performance of RoboDual with other stateof-the-art methods on CALVIN ABCD. The results are given in Table 1. We yield an improvement from 3.27 to 3.66 on the average length of completed tasks. The success rate of accomplishing consecutive 5 tasks is elevated by 13.2%. Additionally, we further investigate the robustness to free-form task instructions of various methods, as shown in Table 2. We incorporate RoboFlamingo and LCB (Shentu et al., 2024), both of which also utilize LLMs (MPT-1B (MosaicML, 2023) and LLaVA-7B (Liu et al., 2024) respectively), as our baseline approaches. All methods are trained exclusively on the ABC split using the original language annotations and are evaluated with GPT-4 enriched ones. While the performance of baseline methods decreases significantly compared to their results in Table 1, our method exhibits minimal impact and nearly doubles the average length compared to LCB. This improvement can be attributed to both the semantic understanding capability of the generalist and the specialist models robustness to variations in conditioning latents. RoboFlamingo 3D Diffuser Actor LCB RoboDual (Ours) 0.40 1.42 1.78 63.0 65.2 73.6 33.0 39.1 50. 16.4 20.3 28.5 8.6 11.7 16.0 3.6 6.1 9.9 Avg. Len. 81.8 68. 48.8 3.47 91.8 57.8 4 Real-world experiments. The results are presented in Figure 3. The state-of-the-art specialist policy, Diffusion Policy, achieves smoother control with high success rate on more dexterous, yet narrowly defined tasks. However, it struggles significantly with multi-instruction tasks, achieving only 20% success rate in Put <obj> into basket. In contrast, OpenX-pretrained generalist models (Octo and OpenVLA) perform better on diverse tasks involving multiple objects and requiring language conditioning. Regarding OpenVLA, its data and inference efficiency could be primary bottlenecks that constrain its overall performance. In the Push block left task, OpenVLA overfits to specific trajectories, despite demonstrations involving block placements in three distinct positions. Its high inference latency also induces jittering and pauses, undermining performance in dexterous control tasks. RoboDual harnesses the high-level reasoning capabilities of the generalist to guide the 7 Preprint. Figure 3: Real-world robot experiments. We report the success rates of each method on three singleinstruction tasks and two multi-instruction tasks, along with the aggregate performance. RoboDual outperforms all specialist and generalist baselines by notable margin across all tasks. Figure 4: Setting on generalizability evaluation. We evaluate four axes of generalizability with different tasks: (a) Position Variation: The red block will be randomly placed in 20cm 10cm area; (b) Visual Distractor: We change the color of drawer, with added plush toys, yellow bowls and clay as distractors; (c) Unseen Background: We replace the checkered tablecloth with solid white one; and (d) Novel Object: Manipulated objects are replaced with unseen ones (banana eggplant). Table 3: Generalizability evaluation. RoboDual excels at all evaluated tasks that require generalization capability from high-level semantic understanding to low-level position variation. Framework Method Position Variation Visual Distractor Unseen Background Novel Object Average Specialist Generalist RoboDual ACT (Zhao et al., 2023) Diffusion Policy (Chi et al., 2023) Octo (Ghosh et al., 2024) OpenVLA (Kim et al., 2024) Ours-single-task Ours-multi-task 46.7 53.3 20.0 26.7 93.3 86.7 26.7 40.0 60.0 73. 80.0 73.3 0 26.7 6.7 20.0 60.0 53.3 13.3 40.0 6.7 46. 46.7 60.0 21.7 40.0 23.4 41.7 70.0 68.3 specialist in executing smooth control, demonstrating strong performance across both singleand multi-instruction tasks. Overall, RoboDual exhibits an improvement of +20% compared to the most competitive baseline and consistently achieves minimum success rate of 60% across all tasks. 4.3 GENERALIZABILITY EVALUATION We investigate the generalizability of RoboDual and baseline methods from four different aspects, as illustrated in Figure 4. Detailed quantitative results are given in Table 3. In task Put block into bowl that requires position generalization, specialist policies and Octo exhibit noticeable bias towards certain tested positions, likely due to the inherent imbalance in the training data. Although OpenVLA can move in the correct direction, its control frequency prevents swift adjustments to the end effector, often causing the object to be pushed away before it can be grasped. Conversely, both the multi-task and single-task learning variants of RoboDual demonstrate strong performance on this challenging task. It is also noted that RoboDual demonstrates error-correction capabilities by attempting to 8 Preprint. re-grasp the block following failed initial attempt, as shown in our video demos. Moreover, thanks to the large, pre-trained VLMs leveraged, RoboDual outperforms both ACT and Diffusion Policy in tasks necessities high-level generalizability (e.g., visual distractor and novel object). These results demonstrate RoboDuals superior ability to generalize across various scenarios."
        },
        {
            "title": "4.4 EFFICIENCY ANALYSIS",
            "content": "Training efficiency. We first investigate the efficient adaptation of RoboDual to new environments, as illustrated in Figure 5. The generalist-only and specialist-only variants are implemented precisely as utilized within our framework to ensure fair comparison. To facilitate multitask learning on CALVIN using our specialist model, we employ DistillBERT (Sanh, 2019) to encode task instructions and concatenate the language embeddings with visual observations for conditioning. The performance of the generalist-only variant stabilizes at 3.27 after approximately 1,400 GPU hours of training on CALVIN. Based on this, RoboDual, which further incorporates specialist model with only 20M trainable parameters, enhances performance to 3.52 following just one hour of training on node equipped with eight A100 GPUs. Our approach ultimately improves the performance of fully-trained generalist by an additional 12%. Notably, when applied to an inadequately trained generalist, just one hour of adaptation with RoboDual achieves greater improvement than several days of further training with the VLA alone (3.44 v.s. 3.27, denoted as Ours in Figure 5). These results demonstrate that RoboDual is cost-effective approach that provides substantial performance gains with minimal training costs. Figure 5: Training efficiency. We report the training duration (GPU hours) of the different methods and the model performance in different training phases. RoboDual achieves notable performance gains with minimum training time. R.F. Avg. Len. Data Scale (a) CALVIN benchmark. (b) Real-world experiments. Table 4: Data efficiency. We explore the adaptability of our specialist model to novel tasks with constrained data availability. RoboDuals performance advantage becomes more pronounced with limited data. R.F.: RoboFlamingo. Data efficiency. We then investigate how the specialist model can efficiently adapt to new environments and improve the overall performance with limited in-domain data. Results on CALVIN are listed in Table 4(a). We take RoboFlamingo, which also utilizes large VLM, to conduct comparative analysis. We initialize RoboFlamingo with its official checkpoint trained on the full set of CALVIN dataset and reinitialize its LSTM-based action decoder head to train it with sub-proportion of data. While RoboFlamingos results diminish by nearly half when utilizing only 5% of demonstrations, RoboDual exhibits robust performance with limited data and maintains an average length of 3.59, demonstrating exceptional data efficiency. The superiority of our method is further evidenced by real-world experiments. As presented in Table 4(b), we select put block into bowl as exemplar task. Training with just five demonstrations, RoboDual achieves notable success rate of 73.3%, which is more than three times the performance of the diffusion policy (D.P.). In contrast, ACT completely fails to grasp the block with such limited demonstrations. Additionally, our method demonstrates the ability to succeed in novel block positions which are not included in the training set with only 5 episodes. Such an observation indicates that the specialist can effectively leverage privileged knowledge from the generalist and extrapolate with limited data. 5% 1.35 10% 1.71 2.48 Full No. Demos 20.0 20.0 53. 3.59 3.62 3.66 0 6.7 46.7 73.3 80.0 93.3 Success Rate ACT D.P. 5 10 RoboDual RoboDual Inference latency analysis. In real-world experiments, we constrain the generalist model to produce single action (specifically the 8th action), which is then followed by eight steps of rapid specialist inference with latency of 0.035 seconds. Correspondingly, RoboDual achieves control frequency of 15 Hz in our real-world setup using NVIDIA A5000 Ada GPUs, facilitating deployment in more dexterous tasks. Notably, inference latency is primary factor contributing to the performance degradation of OpenVLA. Operating at only 3.9 Hz within our system, it significantly alters the system dynamics compared to the 20 Hz non-blocking controller used in our real-world tasks. It can be observed that OpenVLA often struggles to adjust the end effector precisely before initiating 9 Preprint. Figure 6: Ablations on the factors affecting system synergy. We investigate: (a) The importance of each output from the generalist as conditioning sources, (b) The effectiveness of incorporating additional sensory inputs, and (c) Comparison between different conditioning methods. grasp, resulting in lower success rates for tasks requiring fine control, such as Put block into bowl  (Table 3)  . Therefore, we believe that RoboDual is an effective and simple solution for deploying large VLA models in diverse robotics setups. We include video demos in our anonymous project page to offer more intuitive glimpse into RoboDuals performance in real-world scenarios. 4.5 ABLATION STUDY The competitive results presented above position our approach favorably compared to specialist and generalist-only policies. In the following, we examine factors that pay credit towards more desirable synergistic framework for RoboDual. Generalist outputs as conditioning. Figure 6(a) describes that each conditioning source from the generalist model plays an essential role in achieving synergy and enhancing overall performance. Although discretized trajectories may not be perfect, they offer plausible directions that can be iteratively refined by the specialist model, resulting in an improvement of 0.8. Furthermore, both the action and task latents (refer to Figure 2) encapsulate high-level task understanding, which is crucial for multi-task learning, particularly in the context of long-horizon manipulation in CALVIN. Additional sensory inputs. Incorporating additional sensory inputs into the specialist model for specific scenarios proves to be cost-effective strategy. Our approach eliminates the necessity for further fine-tuning of the VLA, without compromising the inference efficiency of the specialist model. As demonstrated in Figure 6(b), RoboDual leverages additional modalities (e.g., depth and tactile) and extra viewpoints (e.g., gripper camera) effectively to enhance overall performance. Conditioning method. The firm bridges connecting generalist and specialist models are constructed through stable conditioning mechanisms. We assess three well-established methods: FiLM (Perez et al., 2018), in-context conditioning (Wang et al., 2023), and cross-attention, to process additional conditional information. Results are shown in Figure 6(c). Cross-attention-based conditioning enables dynamic information weighting among diverse modalities and performs best in our framework."
        },
        {
            "title": "5 CONCLUSION AND FUTURE WORK",
            "content": "We present RoboDual, synergistic dual-system for robotic manipulation that capitalizes the generalizability of Vision-Language-Action (VLA) models alongside the efficiency and adaptability of specialist policies. Our proposed diffusion transformer-based specialist can achieve finer-grained control, efficiently incorporate any sensory input, and adapt to diverse tasks and heterogeneous environments with minimal data and training costs. By fostering synergistic cooperation, RoboDual effectively addresses several limitations inherent in existing VLAs, offering cost-effective and widely adaptable solution for the practical deployment of large generalist policies. Limitations and future work. In RoboDual, we assume that the inference time for both the generalist and specialist remains constant. Consequently, during deployment, each generalist inference step is associated with fixed number of specialist steps. Besides, current generalist outputs discretized 10 Preprint. actions only. Given the scalability of auto-regressive VLAs, there is potential for the generalist to be trained to perform task decomposition (Ahn et al., 2022), affordance grounding (Qian et al., 2024; Lai et al., 2024), and image goal generation (Sun et al., 2024). These capabilities may serve as effective complements, establishing better bridges between the generalist and specialist models."
        },
        {
            "title": "REFERENCES",
            "content": "Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. In CoRL, 2022. 3, 11 Anurag Ajay, Seungwook Han, Yilun Du, Shuang Li, Abhi Gupta, Tommi Jaakkola, Josh Tenenbaum, Leslie Kaelbling, Akash Srivastava, and Pulkit Agrawal. Compositional foundation models for hierarchical planning. In NeurIPS, 2024. 3 Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. In NeurIPS, 2022. 5 Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Rich Walke, Chelsea Finn, Aviral Kumar, and Sergey Levine. Zero-shot robotic manipulation with pre-trained image-editing diffusion models. In ICLR, 2024. 7 Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable Video Diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 5 Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. RT-2: Vision-language-action models transfer web knowledge to robotic control. In CoRL, 2023a. 2, 3, Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. RT-1: Robotics transformer for real-world control at scale. In RSS, 2023b. 3, 7 Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 5 Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision Transformer: Reinforcement learning via sequence modeling. In NeurIPS, 2021. 6 Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion Policy: Visuomotor policy learning via action diffusion. In RSS, 2023. 2, 3, 6, 7, 8, 17 Introducing Covariant. soning introducing-rfm-1-giving-robots-human-like-reasoning-capabilities/, 2024. reahttps://covariant.ai/insights/ capabilities. human-like RFM-1: Giving robots Sudeep Dasari and Abhinav Gupta. Transformers for one-shot visual imitation. In CoRL, 2021. 3 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 5 Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-E: An embodied multimodal language model. In ICML, 2023. 3 11 Preprint. Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, and Cheston Tan. survey of embodied AI: From simulators to research tasks. TETCI, 2022. 2 Stan Franklin. Autonomous agents as embodied AI. Cybernetics & Systems, 1997. 2 Letian Fu, Huang Huang, Gaurav Datta, Lawrence Yunliang Chen, William Chung-Ho Panitch, Fangchen Liu, Hui Li, and Ken Goldberg. In-context imitation learning via next-token prediction. arXiv preprint arXiv:2408.15980, 2024. 2 Theophile Gervet, Zhou Xian, Nikolaos Gkanatsios, and Katerina Fragkiadaki. Act3D: 3D feature field transformers for multi-task robotic manipulation. In CoRL, 2023. 3 Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. 2, 3, 7, 8, Ankit Goyal, Jie Xu, Yijie Guo, Valts Blukis, Yu-Wei Chao, and Dieter Fox. RVT: Robotic view transformer for 3D object manipulation. In CoRL, 2023. 3 Ankit Goyal, Valts Blukis, Jie Xu, Yijie Guo, Yu-Wei Chao, and Dieter Fox. RVT-2: Learning precise manipulation from few demonstrations. arXiv preprint arXiv:2406.08545, 2024. 3 Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. OneLLM: One framework to align all modalities with language. In CVPR, 2024. 2 Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 16, 17 Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 6 Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022. 4, 16 Daniel Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, 2011. 2 Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic VLMs: Investigating the design space of visually-conditioned language models. In ICML, 2024. 4, Tsung-Wei Ke, Nikolaos Gkanatsios, and Katerina Fragkiadaki. 3D Diffuser Actor: Policy diffusion with 3D scene representations. arXiv preprint arXiv:2402.10885, 2024. 7 Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. DROID: large-scale in-the-wild robot manipulation dataset. In RSS, 2024. 3, 4 Heecheol Kim, Yoshiyuki Ohmura, and Yasuo Kuniyoshi. Transformer-based deep imitation learning for dual-arm robot manipulation. In IROS, 2021. 3 Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. OpenVLA: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 2, 3, 4, 6, 7, 8, Lars Kunze, Nick Hawes, Tom Duckett, Marc Hanheide, and Tomáš Krajník. Artificial intelligence for long-term robot autonomy: survey. RA-L, 2018. 2 Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. LISA: Reasoning segmentation via large language model. In CVPR, 2024. 11 Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, Hang Li, and Tao Kong. Vision-language foundation models as effective robot imitators. In ICLR, 2024. 2, 3, 6, 7 Preprint. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as Policies: Language model programs for embodied control. In ICRA, 2023. 3 Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 2024. 7 Corey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured data. In RSS, 2021. Oier Mees, Lukas Hermann, and Wolfram Burgard. What matters in language conditioned robotic imitation learning over unstructured data. RA-L, 2022a. 7 Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. CALVIN: benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. RA-L, 2022b. 2, 6, 16 MosaicML. Introducing MPT-7B: new standard for open-source, commercially usable LLMs. www.mosaicml.com/blog/mpt-7b, 2023. 7 Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning robust visual features without supervision. TMLR, 2024. Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, et al. Open X-Embodiment: Robotic learning datasets and RT-X models. In ICRA, 2024. 3, 4, 16 William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 2, 3, 5 Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. FiLM: Visual reasoning with general conditioning layer. In AAAI, 2018. 5, 7, 10 Aaditya Prasad, Kevin Lin, Jimmy Wu, Linqi Zhou, and Jeannette Bohg. Consistency policy: Accelerated visuomotor policies via consistency distillation. arXiv preprint arXiv:2405.07503, 2024. Shengyi Qian, Weifeng Chen, Min Bai, Xiong Zhou, Zhuowen Tu, and Li Erran Li. AffordanceLLM: Grounding affordance from vision language models. In CVPR, 2024. 11 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. JMLR, 2020. 3 Moritz Reuss, Ömer Erdinç Yagmurlu, Fabian Wenzel, and Rudolf Lioutikov. Multimodal Diffusion Transformer: Learning versatile behavior from multimodal goals. In ICRA Workshops, 2024. 7 Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022. 5 Sanh. DistilBERT, distilled version of BERT: Smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. 9 Yide Shentu, Philipp Wu, Aravind Rajeswaran, and Pieter Abbeel. From LLMs to Actions: latent codes as bridges in hierarchical robot control. arXiv preprint arXiv:2405.04798, 2024. 3, 7 Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-Actor: multi-task transformer for robotic manipulation. In CoRL, 2023. 3 Anthony Simeonov, Ankit Goyal, Lucas Manuelli, Lin Yen-Chen, Alina Sarmiento, Alberto Rodriguez, Pulkit Agrawal, and Dieter Fox. Shelving, Stacking, Hanging: Relational pose diffusion for multi-modal rearrangement. arXiv preprint arXiv:2307.04751, 2023. 13 Preprint. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. ProgPrompt: Generating situated robot task plans using large language models. In ICRA, 2023. 3 Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 16 Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive Model Beats Diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Andrew Szot, Bogdan Mazoure, Harsh Agrawal, Devon Hjelm, Zsolt Kira, and Alexander Toshev. Grounding multimodal large language models in actions. arXiv preprint arXiv:2406.07904, 2024. 3, 6 Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 4 Aaron Van Den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NeurIPS, 2017. 3 Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe HansenEstruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. BridgeData v2: dataset for robot learning at scale. In CoRL, 2023. 3, 4 Jiaqi Wang, Zihao Wu, Yiwei Li, Hanqi Jiang, Peng Shu, Enze Shi, Huawen Hu, Chong Ma, Yiheng Liu, Xuhui Wang, et al. Large language models for robotics: Opportunities, challenges, and perspectives. arXiv preprint arXiv:2401.04334, 2024a. Zhendong Wang, Yifan Jiang, Yadong Lu, Pengcheng He, Weizhu Chen, Zhangyang Wang, Mingyuan Zhou, et al. In-context learning unlocked for diffusion models. In NeurIPS, 2023. 10 Zhijie Wang, Zhehua Zhou, Jiayang Song, Yuheng Huang, Zhan Shu, and Lei Ma. Towards testing and evaluating vision-language-action models for robotic manipulation: An empirical study. arXiv preprint arXiv:2409.12894, 2024b. 2, 4 Zihao Wang, Shaofei Cai, Zhancun Mu, Haowei Lin, Ceyao Zhang, Xuejie Liu, Qing Li, Anji Liu, Xiaojian Ma, and Yitao Liang. OmniJARVIS: Unified vision-language-action tokenization enables open-world instruction following agents. arXiv preprint arXiv:2407.00114, 2024c. 3 Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. In ICLR, 2024. 7 Ge Yan, Yueh-Hua Wu, and Xiaolong Wang. DNAct: Diffusion guided multi-task 3D policy learning. arXiv preprint arXiv:2403.04115, 2024. 3 Jonathan Yang, Dorsa Sadigh, and Chelsea Finn. Polybot: Training one policy across robots while embracing variability. In CoRL, 2023. 3 Yanjie Ze, Ge Yan, Yueh-Hua Wu, Annabella Macaluso, Yuying Ge, Jianglong Ye, Nicklas Hansen, Li Erran Li, and Xiaolong Wang. GNFactor: Multi-task real robot learning with generalizable neural feature fields. In CoRL, 2023. 3 Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, and Huazhe Xu. 3D diffusion policy. In RSS, 2024. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023a. 4 Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. Investigating the catastrophic forgetting in multimodal large language models. arXiv preprint arXiv:2309.10313, 2023b. 2 14 Preprint. Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. In RSS, 2023. 2, 3, 5, 7, 8, 17 Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledge for large-scale task planning. In NeurIPS, 2024. 2 Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3D-VLA: 3D vision-language-action generative world model. arXiv preprint arXiv:2403.09631, 2024. 2 15 Preprint."
        },
        {
            "title": "A EXTENDED DETAILS ON EVALUATION SUITES",
            "content": "CALVIN Benchmark. CALVIN (Mees et al., 2022b) encompasses 34 distinct tasks, characterized by unconstrained task instructions that cover range of skills, from basic pick-and-place operations to articulated object manipulation. The benchmark comprises four different environments (A, B, C, and D), each featuring Franka Panda robotic arm for tabletop manipulation. In our study, we adopt the challenging evaluation setting, where policies are trained with demonstrations from environments A, B, and C, followed by zero-shot evaluations in environment D. The evaluation protocol includes test set of 1,000 unique instruction chains, each composed of five consecutive tasks. The results of the baselines are directly referenced from the official benchmark. For implementation on CALVIN, we train the generalist model with LoRA (Hu et al., 2022) for 50 epochs. The specialist model is trained for 100k iterations with batch size of 64 (8 epochs). We set the action chunk size to kg = ks = 8 for both the generalist and specialist policy. Unless specified otherwise, our specialist model takes as input RGB and depth images from static (third-view) and gripper view cameras. Both the generalist and specialist operate on images of size 224 224. Real-world experiments. We propose an array of tasks to evaluate different manipulation skills, such as basic pick and place (put the block into bowl), non-prehensile manipulation (push block left), and articulated object manipulation (open the drawer), targeting at comprehensive assessment of different policies. Due to limitations of our real-world setup, both the generalist and specialist take as input RGB images from single camera view, with size 192 256. Notably, all generalist baselines (i.e., Octo (Ghosh et al., 2024) and OpenVLA (Kim et al., 2024)) show zero success rate if we directly deploy them in zero-shot manner. In addition to the challenges posed by novel camera views and embodiment configurations, current generalist policies face limitations in direct deployment due to being trained in normalized action space. The statistics of normalization are tailored to each dataset in the OpenX collection (Padalkar et al., 2024), resulting in an implicit, dataset-specific mapping between action spaces and corresponding observations. The reasons mentioned above raise the necessity of further adapting generalist policies with our self-collected demonstrations. We train the specialist baselines from scratch, while the generalists benefit from initialization with pre-trained checkpoints. All models undergo extensive training to ensure clear convergence over time. The implementation of RoboDual follows two-stage training pipeline, where the generalist is trained with multi-task data using LoRA (Hu et al., 2022) finetuning, followed by the efficient training of our specialist model with either multi-task or task-specific demonstrations. We set the action chunk size ks = 8 for the specialist and use temporal aggregation during inference to achieve smoother control. Specifically, action outputs are temporally aggregated with an exponential weighting scheme wi = exp(m i), where [0, ks] and w0 is the weight for the oldest action. We set to 0.1 by default. DDIM scheduler (Song et al., 2021) is employed with the timestamps set to 100 for training and only 5 for inference. Directly predicting samples shows to be more robust than noise prediction. We find that increasing the denoising steps does not necessarily bring performance improvement, while using smaller steps allows faster inference and more responsive control. Classifier-free guidance (Ho & Salimans, 2022) is applied on generalist latents and proprioceptive states with guidance scale of wg = 3 for better controllability. We also explore the capability of the generalist to anticipate and plan ahead, generating actions ks steps beyond its current observation. Consequently, we observe that the specialist is able to accurately interpolate actions based on conditioning from the generalist, guiding the robotic arm to the intended position and enabling precise, fine-grained control."
        },
        {
            "title": "B ARCHITECTURE DESIGN AND TRAINING HYPERPARAMETERS",
            "content": "Generalist. The architecture of our generalist policy is identical to Prismatic-7B (Karamcheti et al., 2024). Regarding training of the generalist, we follow OpenVLA (Kim et al., 2024), and use the AdamW optimizer with constant learning rate of 2e-5 and weight decay of 0.01. We find that achieving robust convergence on the CALVIN benchmark, with its 34 distinct tasks and diverse environments, necessitates sufficiently large batch size of 2048. We leverage gradient accumulation 16 Preprint. to achieve large batch size with constrained compute. However, for real-world experiments, smaller batch size of 64 remains feasible, offering greater flexibility in practical applications. Specialist. We provide the detailed architecture information of our specialist policy in Table 5. Rotary position embeddings are incorporated into each DiT block to ensure coherence among chunked actions. In general, it is more lightweight than previous popular specialist policies (Zhao et al., 2023; Chi et al., 2023) that contains 50M to 80M parameters. Incorporating new sensory input entails an individual ViT encoder (6.4M) and perceiver resampler module (1.1M), which introduces additional 7.5M parameters in total. Considering an input size of 224, sensory encoders generate 196 tokens with patch size of 16. Observation embeddings are subsequently downsampled to only 8 tokens by the resampler. Notably, conditioning latents in the cross-attention module only need to be computed once for the multistep denoising process, leading to more efficient control. During training, we leverage the AdamW optimizer with learning rate of 1e-4 and weight decay of 1e-3, mostly following Chi et al. (2023). cosine-annealing scheduler with warm-up steps of 1,000 is employed to improve training stability. We apply classifier-free guidance (Ho & Salimans, 2022) on generalist latents and proprioceptive states with condition drop chance of 0.1, allowing the specialist policy to imitate without privileged information from the generalist. No specific modifications of training parameters are made for any tasks or environments. Table 5: Architecture details of our diffusion transformer-based specialist policy. Architecture of Specialist Policy Diffusion Transformer Perceiver Resampler Sensory Encoders (ViT) Layers Heads Hidden Size MLP Ratio Action Steps Layers No. Tokens MLP Ratio Layers Heads Hidden Size MLP Ratio Patch Size 6 8 256 4 8 1 8 4 6 8 256 4 16 Parameters 16.2M"
        }
    ],
    "affiliations": [
        "AgiBot",
        "Shanghai AI Lab",
        "Shanghai Jiao Tong University",
        "The University of Hong Kong"
    ]
}