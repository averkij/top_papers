{
    "paper_title": "Training Software Engineering Agents and Verifiers with SWE-Gym",
    "authors": [
        "Jiayi Pan",
        "Xingyao Wang",
        "Graham Neubig",
        "Navdeep Jaitly",
        "Heng Ji",
        "Alane Suhr",
        "Yizhe Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present SWE-Gym, the first environment for training real-world software engineering (SWE) agents. SWE-Gym contains 2,438 real-world Python task instances, each comprising a codebase with an executable runtime environment, unit tests, and a task specified in natural language. We use SWE-Gym to train language model based SWE agents , achieving up to 19% absolute gains in resolve rate on the popular SWE-Bench Verified and Lite test sets. We also experiment with inference-time scaling through verifiers trained on agent trajectories sampled from SWE-Gym. When combined with our fine-tuned SWE agents, we achieve 32.0% and 26.0% on SWE-Bench Verified and Lite, respectively, reflecting a new state-of-the-art for open-weight SWE agents. To facilitate further research, we publicly release SWE-Gym, models, and agent trajectories."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 3 ] . [ 1 9 3 1 1 2 . 2 1 4 2 : r Training Software Engineering Agents and Verifiers with SWE-Gym Jiayi Pan 1 Xingyao Wang 2 Graham Neubig 3 Navdeep Jaitly 4 Heng Ji 2 Alane Suhr 1 Yizhe Zhang"
        },
        {
            "title": "Abstract",
            "content": "We present SWE-Gym, the first environment for training real-world software engineering (SWE) agents. SWE-Gym contains 2,438 real-world Python task instances, each comprising codebase with an executable runtime environment, unit tests, and task specified in natural language. We use SWE-Gym to train language model based SWE agents, achieving up to 19% absolute gains in resolve rate on the popular SWE-Bench Verified and Lite test sets. We also experiment with inference-time scaling through verifiers trained on agent trajectories sampled from SWE-Gym. When combined with our fine-tuned SWE agents, we achieve 32.0% and 26.0% on SWE-Bench Verified and Lite, respectively, reflecting new stateof-the-art for open-weight SWE agents. To facilitate further research, we publicly release SWEGym, models, and agent trajectories.1 1. Introduction Language models (LMs) have remarkable promise in automating software engineering (SWE) tasks, as most clearly measured by recent progress on recent benchmarks like SWE-Bench (Jimenez et al., 2024) and Commit0 (Zhao et al., 2024). While LM-based SWE agents have shown significant performance gains through improving agentcomputer interfaces (Yang et al., 2024) and prompting strategies (Wang et al., 2024c), advances in SWE agents have been limited by reliance on proprietary models, rather than improving underlying LM itself. Unlike other domains where supervised fine-tuning and reinforcement learning have significantly improved LM capabilities, for example in general chat (Ouyang et al., 2022), mathematical reasoning (Shao et al., 2024; Yuan Equal contribution. Equal supervision. 1UC Berkeley 2UIUC 3CMU 4Apple. Correspondence to: Jiayi Pan <jiayipan@berkeley.edu>, Xingyao Wang <xingyao6@illinois.edu>, Alane Suhr <suhr@berkeley.edu>, Yizhe Zhang <yizzhang@apple.com>. 1https://github.com/SWE-Gym/SWE-Gym Figure 1: SWE-Gym enables scalable improvements for software engineering agents. Top: Training time scaling shows consistent performance improvements as we obtain more training trajectories, with no signs of saturation at 491 trajectories. We use temperature = 0. Bottom: For inference time scaling, we generate number of candidate trajectories per task and select the best using verifier trained on SWE-Gym. This approach demonstrates roughly logarithmic gains with the number of sampled solutions. = 0 (excluded from regression) is used as the first hypothesis to be consistent with the top figure; later rollouts use = 0.5. et al., 2024), and web navigation (Pan et al., 2024), software engineering agents currently lack suitable training environments. Creating such an environment for SWE agents is uniquely challenging. Real-world software engineering requires interaction with an executable runtime that has been prepared with the appropriate software dependencies and reproducible test suites, among other requirements. These challenges are reflected in the existing resources (Tab. 1). For example, the SWE-Bench (Jimenez et al., 2024) training split contains only the solutions (git patches that solve the task), and lacks executable environments and reward signals. R2E (Jain et al., 2024) uses synthetic instructions that are 1 Training Software Engineering Agents and Verifiers with SWE-Gym Dataset (split) Repository-Level Executable Environment Real task # Instances (total) # Instances (train) CodeFeedback (Zheng et al., 2024b) APPS (Hendrycks et al., 2021a) HumanEval (Chen et al., 2021) MBPP (Tao et al., 2024) R2E (Jain et al., 2024) SWE-Bench (train) (Jimenez et al., 2024) SWE-Gym Raw SWE-Bench (test) (Jimenez et al., 2024) SWE-Gym 66,383 10,000 164 974 246 19,008 66,894 2,294 2,438 66,383 5,000 0 374 0 19,008 66,894 0 2,438 Table 1: SWE-Gym is the first publicly-available training environment combining real-world software engineering tasks from GitHub issues with pre-installed dependencies and executable test verification. Repository-level: whether each task is situated in sophisticated repository; Executable Environment: whether each instance in the resource comes with an executable environment with all relevant dependencies pre-installed; Real task: whether the instruction for each instance is collected from human developers. very far from real-world problems, while datasets such as CodeContests (Li et al., 2022) and APPS (Hendrycks et al., 2021a) focus only on isolated tasks rather than realistic repository-level coding problems. To bridge this gap, we present SWE-Gym, the first training environment combining real-world software engineering tasks from GitHub issues with pre-installed dependencies and executable test verification.2 SWE-Gym contains 2,438 Python tasks sourced from 11 popular open-source repositories (Tab. 2), providing useful environments for training LMs as agents and verifiers. SWE-Gym supports training state-of-the-art openweight SWE agents. Based on the OpenHands (Wang et al., 2024c) agent scaffold for general-purpose software development (5.1), we fine-tune 32B Qwen-2.5 coder model (Hui et al., 2024b) using only 491 agent-environment interaction trajectories sampled using SWE-Gym, and achieve substantial absolute improvements of +12.3% (to 15.3%) and +13.6% (to 20.6%) on SWE-Bench Lite and SWE-Bench Verified respectively (3.2). SWE-Gym is effective across agent scaffolds. In another agent scaffold based on SWE-Bench specialized workflow (Moatless, Orwall 2024, 5.1), we observe an improvement to 19.7% (32B model) and 10.0% (7B model) on SWEBench Lite through self-improvement, where the LM interacts with SWE-Gym, receives reward from it, and learns to improve itself through rejection sampling fine-tuning. SWE-Gym supports training verifier models that enable inference-time scaling. We use the test suite executions provided by SWE-Gym to determine whether sampled agent trajectories are successful or not. Given these samples, we train verifier model (i.e., an outcome-supervised reward model Cobbe et al. (2021)) that estimates trajectorys probability of success, and thus enables inference-time scal2We discuss concurrent work by Golubev et al. (2024) in A. ing. Specifically, at inference time we sample multiple agent trajectories, and select the one with the highest estimated reward according to the verifier. This approach further improves the resolve rate to 32.0% (+11.4% absolute improvement) on SWE-Bench Verified (4.1.1, Fig. 1 bottom) and 26.0% on SWE-Bench Lite (4.1.2), establishing new state-of-the-art among systems with publicly accessible weights (Tab. 9). Our baseline training and inference-time scaling methods on SWE-Gym yield continuously improved results with increasing compute  (Fig. 1)  . In the training phase, performance scales with the number of sampled trajectories up to our current limit of 491 trajectories, suggesting that performance is currently limited by sampling compute rather than SWE-Gyms size. Similarly, using the agent and verifier trained by SWE-Gym, the bottom panel shows that adding more compute during the inference time steadily improves the results. 2. SWE-Gym Environment SWE-Gym comprises 2,438 real-world software engineering tasks sourced from pull requests in 11 popular Python repositories, with pre-configured executable environments and expert-validated test cases, constructed in close alignment with SWE-Bench (Jimenez et al., 2024). These repositories are separate from those used in SWE-Bench to avoid contamination. These tasks require SWE agents to develop test-passing solutions for real-world GitHub issues using provided codebases and executable environments. Such agents must map from natural language descriptions of the issue, as well as the initial state of the repository, to pull request represented as git patch. To solve the task, agents must edit or create files, and can execute code in the environment for debugging and testing. The standard SWE-Gym includes 2,438 tasks. We also identify subset of 230 tasks, SWE-Gym Lite, which contains 2 Training Software Engineering Agents and Verifiers with SWE-Gym Category Metric SWE-Gym SWE-Gym Lite"
        },
        {
            "title": "Size",
            "content": "# Instances # Repos"
        },
        {
            "title": "Length by Words",
            "content": "2,438 (2,294) 11 (12) 239.8 (195.1) 230 (300) 11 (12) 186.2 (175.9)"
        },
        {
            "title": "Codebase",
            "content": "# Non-test Files # Non-test Lines 971.2 (2944.2) 340675.0 (363728.4) 818.8 (2988.5) 340626.2 (377562.4)"
        },
        {
            "title": "Tests",
            "content": "# Lines edited # Files edited # Func. edited # Fail to Pass # Total 69.8 (32.8) 2.5 (1.7) 4.1 (3.0) 10.0 (9.0) 760.8 (132.5) 10.6 (10.1) 1.0 (1.0) 1.4 (1.34) 2.04 (3.5) 99.9 (85.2) Table 2: Statistics comparing SWE-Gym with the SWE-Bench test split (in parenthesis). Except for size metrics, we report the average value across instances. Figure 2: Repository distribution of SWE-Gym instances. generally easier and more self-contained tasks that are suitable for rapid prototyping, in alignment with SWE-Bench Lite (Jimenez et al., 2024). To support future research in SWE agent development and automatic dataset synthesis, we also release large set of Python GitHub issues without executable environments, which we refer to as SWE-Gym Raw. SWE-Gym Raw includes 66,894 instances spanning 358 Python repositories. 2.1. Dataset Construction Identify Repositories To identify repositories as the basis of SWE-Gym, we first use SEART GitHub search3 to filter list of initial repositories. Unlike SWE-Bench, which focuses on the top 5k most downloaded PyPI libraries (Jimenez et al., 2024), we select Python repositories that were created before 7/1/2022 and have more than 500 stars, with at least 300 lines of code, more than 500 pull requests (PRs) and 100 contributors. This results in 358 repositories. Extract Training Instances from Repositories. We then use SWE-Benchs instance extraction script to convert these repositories into SWE-Bench-style instances, each corresponding to Github issue including the natural language description of the issue, snapshot of the repository in which the issue was created, and set of unit tests. Over the 358 repositories, we extract 64,689 task instances. We refer to this dataset as SWE-Gym Raw, which is over three times larger than the 19K instances gathered in previous work (Jimenez et al., 2024) and includes nearly ten times as many repositories. While SWE-Gym Raw instances contain code, issue descriptions and the solution, they do not contain executable environments. And its unclear if the unit tests are effective 3https://seart-ghs.si.usi.ch/ in evaluating the correctness of solution. We focus on subset of repositories for which we semi-manually create such executable environments. Of the 358 total repositories collected, we select subset of 11 repositories with high number of instances. Version Training Instances. Associating instances with their respective version numbers (e.g. 1.2.3) and setting up environments version-by-version makes the environment collection process more practical by avoiding redundant setup work. We generalize SWE-Benchs versioning script to support versioning via script execution, and semi-automatically collect versions for each instance based on information available in the repository (e.g., pyproject.toml, git tag, etc). Setup Executable Environments and Verify Instances An executable environment with pre-installed dependencies is essential for developing software engineering agents, as it most closely reflects an agents setup at deployment time, and enables incremental unit test feedback during development. However, configuring dependencies for specific version of the codebase can be particularly challenging due to the absence of universal installation method for Python packages. This issue is further compounded by backward compatibility problems where an older version of codebase requires an older version of dependency, making environment setup even more difficult in old GitHub issues. Despite these challenges, ignoring these difficult environments might cause distribution bias that decreases the utility of SWE-Gym, so we take on this challenge for the instances from these 11 selected repositories. We manually configure the dependencies for each task instance given on the relevant configuration files (e.g., requirements.txt), continuous integration scripts, or documentation provided in the repository snapshot at the time of the instances issue creation. We then run the execution-based validation script 3 Training Software Engineering Agents and Verifiers with SWE-Gym from SWE-Bench to verify that the gold patch (i.e., the code diff submitted by human developer) passes more unit tests than the original code. We estimate that the entire process takes around 200 human annotation hours and 10k CPU core hours. After validation and filtering out failed instances, we obtain 2,438 unit-test-validated instances from 11 different repositories. We publicly release the pre-built Docker images for each instance to allow full reproducibility. Each image takes up an average of 2.6 GB, totaling 6 TB. 2.2. SWE-Gym Lite Solving software engineering tasks is hard and computationally intensive, costing usually $1 or more per task with frontier models (Wang et al., 2024c). To improve research efficiency via faster agent evaluation, Jimenez et al. (2024) introduce SWE-Bench Lite, canonical subset of 300 instances from SWE-Bench. Following the SWE-Bench Lite filtering pipeline,4 we delineate the SWE-Gym Lite split, comprising 230 instances. Similar to SWE-Bench Lite, this subset excludes tasks that require editing more than one file, tasks with poorly described problem statements, those with excessively complex ground-truth code diffs, and tests focused on error message validation. 2.3. Dataset Statistics Fig. 2 illustrates that the task distribution across repositories exhibits long-tail pattern. Notably, tasks associated with pandas comprise nearly one-third of the total, whereas tasks related to bokeh represent mere one percent. Our analysis suggests that tasks and repositories included in SWE-Gym are more challenging than those included in SWE-Bench. Tab. 2 shows that overall, SWE-Gym has statistics similar to SWE-Bench, with several key differences. Codebases in SWE-Gym, on average, have relatively fewer files than SWE-Bench, but similar number of lines of code. However, SWE-Gym has significantly more lines and files edited per gold patch when compared to SWEBench. Longer issue descriptions and larger number of edits in the gold patch may loosely correlate with higher task complexity. Additionally, our experiments find consistently lower model performance on SWE-Gym compared to SWE-Bench.5 Beyond models and scaffolds over-fitting to SWE-Bench, one explanation for this decreased performance on SWE-Gym could be our deliberate inclusion of sophisticated repositories, such as pandas and machinelearning libraries like MONAI. 4For details on its construction process, see https://www. swebench.com/lite.html. 5B.3 contains details of these experiments. 3. Training LMs as SWE Agents with SWE-Gym With SWE-Gym, we train language model agents. Our primary objective is to establish reasonable baselines for SWE-Gym using two agent scaffolds (OpenHands, Wang et al. 2024c, 3.2; Moatless Tools, Orwall 2024, 3.3) and validate the effectiveness of our dataset. 3.1. Setting Agent Scaffolds. We experiment with two types of agents scaffolds general-purpose prompting and specialized workflows, demonstrated through OpenHands CodeAct (Wang et al., 2024c) and MoatlessTools ( Orwall, 2024). Policy Improvement Algorithm. As baseline, we employ simple policy improvement algorithm: rejection sampling fine-tuning (a.k.a. filtered behavior cloning). This method is widely used as baseline in language model and reinforcement learning literature (Zhou et al., 2024; Snell et al., 2022). At each iteration, policy is sampled to interact with the environment. This policy may either be stronger teacher policy or the student policy itself. Only trajectories that achieve rewards exceeding predefined threshold are retained. The student model is then fine-tuned to mimic these high-reward trajectories using the standard negative log-likelihood loss. In our setting, the reward threshold is set to include only successful trajectories. Evaluation Metrics. We perform evaluation on SWEBench Lite and SWE-Bench Verified (Jimenez et al., 2024). We measure: (1) Resolve Rate, RR, (%), the percentage of resolved problems; (2) Empty Patch, EP, (%), the percentage of trajectories where none of the code in the repository is edited. We use OpenHandss remote runtime (Neubig & Wang, 2024) feature to parallelize evaluation (e.g., execute unit tests). Training Setup. We use Qwen-2.5-Coder-Instruct (Hui et al., 2024a) 7B, 14B, and 32B variants as our base models. For hyper-parameters and details of the training runs, please refer to B.2. 3.2. Training General-Purpose Prompting Agents In this subsection, we use OpenHands (version CodeActAgent 2.1, Wang et al. 2024c;b) as our agent scaffold. OpenHands CodeActAgent uses ReAct-style (Yao et al., 2023) general-purpose prompting without specialized workflow (5.1) and relies on the underlying LM to perform action and interpret observation. It equips an LM with bash terminal and file editor. We disable the browser feature of OpenHands in this work. We use OpenHandss remote runtime (Neubig & Wang, 2024) feature to roll out agents in parallel (i.e., execute agents actions) on SWE-Gym. 4 Training Software Engineering Agents and Verifiers with SWE-Gym Model Size Empty Patch (%, ) Stuck in Loop (%, ) zero-shot fine-tuned zero-shot fine-tuned zero-shot Avg. Turn(s) fine-tuned Resolve Rate (%, ) zero-shot fine-tuned 7B 14B 32B 7B 14B 32B 40.3 49.7 27.0 45.8 44.9 9.5 29.7 10.7 18.1 31.6 8.9 18. 33.8 12.0 14.5 30.4 +4.3 13.8 47.0 31.7 16.7 39.6 32.1 29.4 SWE-Bench Lite (300 instances) 31.0 16.0 4.6 27.1 +1.5 18.1 SWE-Bench Verified (500 instances) 20.3 23.2 15. +1.9 22.2 21.4 1.8 29.3 +13.9 1.0(1.0) 2.7(1.9) 3.0(1.4) 10.0(2.4) +9.0 12.7(2.3) +10.0 15.3(2.5) +12.3 21.0 18.6 21.3 10.7 5.6 23.8 21.9 25.5 24.6 35.3 +13.4 +4.6 30.1 +7.0 31. 1.8(1.1) 4.0(1.6) 7.0(1.3) 10.6(2.1) +8.8 16.4(2.0) +12.4 20.6(2.1) +13.6 Table 3: Model performance (fine-tuned on 491 SWE-Gym-sampled trajectories) on SWE-Bench (Jimenez et al., 2024) using OpenHands (Wang et al., 2024c) as agent scaffold. We use Qwen-2.5-Coder-Instruct as the base model. We set temperature = 0 for evaluation. Trajectory Collection. We roll out 491 successful trajectories where the agent successfully solves the given task from SWE-Gym. These trajectories are sampled from two models (gpt-4o-2024-08-06 and claude-3-5-sonnet-20241022) with different temperature settings. Each success trajectory, on average, has 39.9 LM completion messages (roughly 19 turns) and 18, 578 tokens. Please refer to Tab. 8 for more details. While SWE-Gym contains many more tasks for trajectory collection and allows repeated sampling, our current set of 491 trajectories was primarily limited by computational budget constraints rather than the availability of tasks. Please refer to Tab. 10 and B.3 for details on the temperature setting and the resolve rate in the training set. Additional Evaluation Metrics & Settings. In addition to the metrics presented in 3.1, we include the following for further analysis: (3) Stuck in Loop (%), the percentage of trajectories where the agent is stuck in loop by repeating the exact same action for the last three turns; (4) Avg. Turn(s), the average number of action-observation turns for these trajectories. Our evaluation of the trained LM is bounded by either 100 interaction turns or the 32k context window length, whichever is reached first. We use sampling temperature of 0 (i.e., greedy) unless otherwise specified. Training on SWE-Gym trajectories turns LM into effective agents to fix issues. As shown in Tab. 3, despite the base model Qwen-2.5-Coder-Instruct-32B only performs 3.0% and 7.0% on SWE-Bench Lite and Verified, the model fine-tuned on SWE-Gym-sampled trajectories is able to achieve consistent improvements, up to 12.3% (3.0% 15.3%) and 13.6% (7.0% 20.6%) absolute performance (i.e., more Github issue resolved) with the largest 32B model. Training help reduce agents stuck-in-loop behavior. Open-weight LMs, often suffer from stuck-in-loop, especially when prompted with general-purpose prompts (5.1): the LM would repeat exactly the same action for multiple turns and never get out of this loop. This is evident in the zero-shot stuck in loop statistics in Tab. 3, where even the largest 32B model gets stuck in loop 30% of the time. The fine-tuned model consistently reduces the stuck in loop rate ranging from 4.6% to 18.6% for problems in both SWEBench Lite and Verified, with the exception of 32B model on SWE-Bench Lite (+1.5%). The reduced stuck-in-loop rate likely encourages the agent to execute more code edits, as evident by the decreased empty patch rate across different scales (except for 32B on SWE-Bench Verified). Performance scales with model size. Rather unsurprisingly, we see the resolve rate, the empty patch, and the stuckin-loop rate improves consistently in both SWE-Bench Lite and Verified as we switch to bigger base models (Tab. 3). Self-improvement is not yet working. We use the finetuned 32B model to rollout trajectories from SWE-Gym for 6 times and obtain 868 successful trajectories (i.e. on-policy trajectories) using temperature = 0.5. We further fine-tune the base 32B model on mixture of 868 on-policy trajectories and the previously collected 491 off-policy trajectories. When evaluating the model on SWE-Bench Lite, we observe performance drop from 15.3% to 8.7% compared to the model that was fine-tuned using only off-policy trajectories from GPT-4 and Claude. We hypothesize improved results could be achieved with more advanced optimization methods, such as Proximal Policy Optimization (PPO) (Schulman et al., 2017), or by employing stronger base model. These directions remain promising avenues for future investigation. 3.3. Self-improvement with Specialized Workflow Agents We then explore the opposite end of the scaffold spectrumagents with specialized workflowsusing the MoatlessTools Agent (version 0.0.2; Orwall 2024) for our experiments. Unlike OpenHands CodeAct, which offers extensive freedom in long-horizon planning, this scaffold constrains the models action space with pre-defined workflows, effec5 Training Software Engineering Agents and Verifiers with SWE-Gym tively reducing task horizons. As demonstrated in Tab. 3 and Tab. 5, we observe that open-weight models consistently deliver better zero-shot performance when paired with the MoatlessTools scaffold. Given the improved zero-shot performance and shorter task horizon with the MoatlessTools scaffold, we explore if SWEGym allows agents to self-improve without reliance on strong teacher. With limited compute budget, we conduct this experiment with only 7B and 34B models, using LoRA (Hu et al., 2022) for the 34B models for improved efficiency. We start our experiments with ablations on 7B model and scale it to 32B. We sample the model for 30 rounds on SWE-Gym-Lite with high temperature of 1.0, adding successful trajectories to the fine-tuning dataset. This process is repeated twice, after which the improvements are marginal. Easy Data Bias Degrades Model Performance. During repeated sampling, similar to the observation in Brown et al. (2024) , we find that the success probability for each instance follows long-tail distribution as shown in Fig. 3, with more instances being solved simply by increasing the number of samples. While having broader coverage of tasks should be beneficial for training, as first observed in math reasoning (Tong et al., 2024), repeated sampling introduces distribution bias toward easier tasks, making it suboptimal to naively train on all successful trajectories. through data selection. We propose per-instance capping - simple method limiting the maximum instances per task. As shown in Fig. 3, this approach balances dataset bias and size. Too low cap limits dataset size and hurts performance (see 4.2), while too high cap skews distribution toward easier tasks. As shown in Tab. 4, empirically, we find threshold of 2 achieves good balance between dataset size and difficulty bias, performing marginally better than the full dataset while improving training speed. We rank trajectories by the rounds of model responses they need, fewer the better. Cap # Traj Empty Patch (%, ) Resolve Rate (%, ) 0 (Zero-shot) 1 2 3 No Cap (All) 0 36 62 82 172 56.3 37.3 29 43.7 30.7 7.0 9.0 9.7 7.7 9. Table 4: Resolve rate and empty patch rate on SWE-Bench Lite after 7B model trained with with data from different instance capping strategies (Cap) and therefore different number of trajectories (Traj). Setting 7B Model 32B Model EP(%, ) RR(%, ) EP(%, ) RR(%, ) Zero-Shot Iteration 1 Iteration 2 56.3% 29.0% 23.3% 7.0% 9.0% 10.0% 24.3% 18.3% 9.7% 19.0% 19.7% 19.7% Table 5: Resolve rate (RR) and Empty patch rate (EP) on SWE-Bench Lite with MoatlessTools Scaffold after online rejection sampling fine-tuning, evaluated at temperature = 0. RR shown in highlighted columns. Figure 3: Success distribution over 30 rounds on SWEGym Lite with 7B model in zero-shot. The distribution is naturally biased toward easy tasks. Per instance capping reduces this bias but lowers the total trajectory count for training. We set temperature = 1 during sampling. Mitigate Easy Data Bias with Per Instance Capping. To mitigate such bias, Tong et al. (2024) propose to keep the training set size same but obtain equal or more number of positive instances on difficult tasks. However, obtaining positive instances from harder tasks is inherently difficult and on average requires more rounds of sampling and therefore more compute. We consider compute-bounded scenario with fixed sampling budget to optimize model performance Results. With capping of 2, we perform two rounds of policy improvements for both 7B and 32B models and present results in Tab. 5. For the 7B model, the resolve rate improves significantly, rising from 7.0% in the zeroshot setting to 9.0% after the first iteration and reaching 10.0% after the second iteration, indicating notable benefits from training on SWE-Gym. In contrast, the 32B model demonstrates strong initial performance with zero-shot resolve rate of 19.0%, but shows only marginal improvement to 19.7% after the first iteration and no additional gains in subsequent iterations. We hypothesize that this plateau may stem from limitations inherent to the agent scaffold and the rejection sampling finetuning algorithm we employed to improve the policy. 6 Training Software Engineering Agents and Verifiers with SWE-Gym 4. Scaling Agent Improvements with SWE-Gym 4.1. Inference-Time Scaling with SWE-Gym-powered Verifier Beyond training the agent, trajectories sampled from SWEGym also allow us to train verifier (a.k.a. reward model). As baseline, we train an outcome-supervised reward model (ORM) (Cobbe et al., 2021) that, given the relevant context of the task execution (e.g., problem statement, trajectories, git diff), generates score representing the probability if the agent solves the problem. We show that such learned verifiers enable effective inference-time scaling for further performance improvement. Using the same underlying LM, one can sample multiple solutions for the same problem and use the verifier to pick the best solution. 4.1.1. VERIFIER FOR AGENT SCAFFOLD WITH GENERAL-PURPOSE PROMPTING For OpenHands CodeActAgent (Wang et al., 2024c;b) that uses general-purpose prompting (5.1), we consider verifier (ORM) setting where the input is an interleaved trajectory τ and output is scalar reward r: τ = [o1, a1, o2, a2, . . . , on, an], [0, 1] where observation ok can be the problem-statement, command execution output, error messages, etc; action ak can be bash command or file operations (e.g., edit, view) from the agent. We include verifier prompt template in B.5. Training Setup. Following the same training hyperparameters as described in 3, we train 32B Qwen2.5-Coder-Instruct as verifier. We train the LM to predict YES or NO regarding if the agent has successfully solved the request based on the agent trajectory (see the verifier prompt in B.5). At inference time, conditioned on the prompt and the agent trajectory τ , we use SGLang (Zheng et al., 2024a) to obtain the log probability of the next token being YES (ly) or NO (ln). We then calculate the probability of success as ORM by normalizing the log probability: pyes = exp(ly)/(exp(ly) + exp(ln)). Verifier Training Data. We re-use two sets of trajectories we sampled for agent training in 3.2: (1) off-policy trajectories which contain 443 successful trajectories that are sampled from gpt-4o and claude-3-5-sonnet; (2) on-policy trajectories which contain 875 successful trajectories sampled from the agent model (fine-tuned Qwen2.5-Coder-Instruct-32B). Note that we only keep trajectories that fit in the context window (32k tokens) for training. We combine both on-policy and off-policy trajectories, randomly sample the same amount of unsuccessful trajectories from each subset (1318 in total), and combine them as our dataset for verifier training. Metrics. We measure (1) Pass@K, which represents the percentage of tasks where at least one successful solution is found among sampled trajectories, and (2) Best@K, which selects the trajectory with the highest verification score (pyes) among samples and reports the percentage of these selected trajectories that are successful. Pass@K evaluates the models ability to find any working solution (i.e., the upper bound for Best@K), while Best@K assesses our verifiers capability to identify the most promising solution. We calculate the mean and variance for each data point following Lightman et al. (2023), please see B.1 for details on the calculation. Figure 4: Scaling inference-time compute improves performance on SWE-Bench Verified using finetuned verifier. Both the agent and the verifier are Qwen2.5-Coder-Instruct-32B model fine-tuned on the corresponding dataset (4.1.1). OpenHands (Wang et al., 2024c) is used as the agent scaffold. The first rollout was performed with temperature = 0, and = 0.5 was used for the rest. Result. Fig. 4 shows how Pass@K and Best@K metrics scale with the number of sampled agent trajectories using the fine-tuned 32B model as the agent model. Pass@K demonstrates strong improvement, rising from 20.6% to 37.8% as increases from 1 to 8, and to 42.8% at k=16. The Best@K metric, which relies on our verifiers ability to select the best trajectory, demonstrates more modest but steady progress, improving from 20.6% at k=1 to 29.8% at k=8, and to 32.0% at k=16. The gap between Pass@K and Best@K reveals room for more advanced reward modeling techniques for coding agents. Surprisingly, we found that fine-tuning the model using LoRA (Hu et al., 2022) (29.8% @8) via Unsloth library (Unsloth Team, 2024) performs better than full-parameter fine-tuning for verifier training (27.2%@8), potentially due to the effect of regularization. Furthermore, as shown in Fig. 1 (bottom), the Best@K curve exhibits strong linearity on logarithmic scale, indicating promising scaling behavior. 7 Training Software Engineering Agents and Verifiers with SWE-Gym 5: verifier study are"
        },
        {
            "title": "Abaltion\nPerformances",
            "content": "training for Figure (4.1.1). evaluated on SWEBench Verified. Both the agent and the verifier are Qwen2.5-Coder-Instruct-32B model fine-tuned on the corresponding dataset (4.1.1). OpenHands (Wang et al., 2024c) is used as the agent scaffold. Ablation Study: Training Data matters for Verifier. As shown in Fig. 5, our ablation study demonstrates that the choice of training data can significantly impact verifier performance. Different from the LoRAtrained model in the previous paragraph, all ablation studies perform full-parameter fine-tuning on 32B Qwen2.5-Coder-Instruct for the verifier. Training with mixture of off-policy and on-policy data yields the best results (our default setting), with Best@k scaling from 20% to approximately 27% at k=8. In contrast, using only on-policy data from the fine-tuned model shows moderate but limited improvement, while training exclusively on offpolicy data from Claude and GPT leads to early performance plateaus around 22%. Notably, using off-policy data with twice the number of negative examples performs slightly worse with lower number of k, suggesting that overweighting negative examples during training could make the verifier overly conservative. These findings indicate that verifier training benefits most from diverse dataset combining both off-policy and on-policy examples, as well as balanced positive and negative data, enabling better generalization and more accurate identification of successful solutions during inference time scaling. 4.1.2. VERIFIER FOR AGENT SCAFFOLD WITH SPECIALIZED WORKFLOW Figure 6: Scaling inference-time compute for MoatlessTools Agents with learned verifiers. We set temperature = 0.5 during sampling. dence token indicating task success.. We adapt the context extractor from Zhang et al. (2024a) and provide the prompt template in B.4. We train 7B and 32B verifiers using on-policy trajectories from the last round of sampling, applying LoRA (Hu et al., 2022) for regularization. To address data bias, we cap positive data points per instance at 2 and balance the dataset by subsampling failure cases to match successful ones. Result. We evaluate the verifiers by sampling the policy 8 times at temperature 0.5. As shown in Fig. 6, these verifiers enable effective scaling across model sizes - the 7B agentverifier system improves from 10% to 13.3% success rate on SWE-Bench Lite, while the 32B system improves from 19.7% to 26.3%. The 7B verifier plateaus after N=4 samples when ranking trajectories from both 7B and 32B agents. In contrast, the 32B verifier continues improving even at N=8, suggesting model size affects scaling behavior. For MoatlessTools Agents with specialized workflows, given that it doesnt have turn-taking action-observation trajectory like OpenHands CodeActAgent, we prepare verifier inputs through parsing process. This process combines task descriptions, relevant agent context, and generated patches, training the model to output single confi4.2. Training-Time Scaling with Data In this subsection, we study the impact of training data scaling on agent performance. We identify three types of data scaling for sampled trajectories: (1) Random Scaling (No Dedup.), which simply increases the total number of Training Software Engineering Agents and Verifiers with SWE-Gym training trajectories regardless of uniqueness  (Fig. 7)  ; (2) Unique Instance Scaling (Dedup.), which scales the number of distinct instances by randomly selecting one success trajectory per instance  (Fig. 8)  ; and (3) Repository Scaling (Dedup.), which samples trajectories by sequentially including all instances from one repository at time, sorted by repository name, to study the impact of repository-level diversity. approaches, Repository Scaling shows stronger initial performance at 25% data, suggesting that complete repository coverage may provide more coherent learning signals early in training. These results suggest that the repository and instance diversity of SWE-Gym is not yet bottleneck - further improvements could likely be achieved by simply sampling more agent trajectory data for traning, regardless of duplication or repository distribution. Setup. Using OpenHands (Wang et al., 2024c), we evaluate these scaling approaches on SWE-Bench Verified: random scaling on the full trajectory dataset from 3.2 (491 trajectories), unique instance scaling on these trajectories deduplicated by instance ID (294 trajectories), and repository-based scaling where we sort repositories alphabetically and include all trajectories from each repository in order (e.g., first 25% contains complete trajectories from the first repositories). We compare models trained on 25%, 50%, and 100% of the corresponding dataset for each approach. Please refer to Tab. 7 for detailed statistics of these datasets. Model performance scales well with more training examples. As illustrated in Fig. 7, we observe favorable scaling behavior: there is consistent improvements in model resolve rate as training data increases, particularly for the 32B model. These results suggest that SWE-Gyms current size and repository diversity are likely not performance bottlenecks - further improvements could likely be achieved by allocating more computing resources to sampling more training trajectories. Figure 7: Model performance scaling with training data size. The x-axis shows the percentage of training data used in log base 2 scale. Similar scaling trends suggest instance and repository diversity is not yet bottleneck. Fig. 7 reveals comparable overall performance between different scaling approaches up to where deduplication takes effect. While Random Scaling (No Dedup.) achieves higher final performance, this is likely due to having more trajectories (491 vs 294) rather than better scaling efficiency. Among deduplicated 9 Figure 8: Comparison of three data sampling approaches: without deduplication, repository-based sampling, and random sampling (4.2). All variants use the 32B model evaluated on SWE-Bench Verified. 5. Related Work 5.1. Agents that Solve Github Issues In this paper, we focus on software engineering agents designed to automatically resolve GitHub issues, specifically in the context of SWE-Bench (Jimenez et al., 2024). These agents operate in well-defined setting: given GitHub issue and its associated code repository, they generate valid code modification (i.e., git diff patch) to address the issue. The correctness of these modifications is verified through the execution of test suite written by human developers. Existing agent designs can be categorized by the number of human priors injected into the agent workflow: (1) specialized workflow: Xia et al. (2024); Orwall (2024); Zhang et al. (2024b); Chen et al. (2024) build specialized workflow with human-defined stages (e.g., localization, code edit, patch re-ranking selection), where an LM is iteratively prompted to produce the final results. While restricting the LM to work on pre-defined subtasks, this approach effectively reduces the task horizon and alleviates the need for long-term planning. However, these specialized workflows face limitations: they require significant human engineering to define appropriate stages and transitions, may not generalize well to novel issue types outside their designed workflow, and can be brittle when intermediate steps fail. (2) general-purpose prompting: Systems such as Yang Training Software Engineering Agents and Verifiers with SWE-Gym et al. (2024); Wang et al. (2024c) rely on general prompting approaches, including methods like ReAct (Yao et al., 2023) and CodeAct (Wang et al., 2024b). These systems primarily depend on the LM itself to plan through long horizons and produce the next action based on an evolving history of actions and observations, without enforcing fixed workflow or relying heavily on human pre-defined stages. While more flexible, these general approaches place higher demands on the underlying LMs capabilities and can be computationally expensive due to multiple rounds of interaction with growing interaction history. Most existing successful methods are based on prompting proprietary models and crafting specialized workflow to workaround existing limitations of those models. This starkly contrasts the success of learning-based approaches in other domains (Silver et al., 2017; Akkaya et al., 2019), where system learns from prior interactions and rewards to develop task competency progressively. We argue that proper training environments and baselines can help accelerate research in this direction. We verify that we can use SWE-Gym to build strong agents through learning. 5.2. Environments for Training Software Agents There is no existing dataset suitable for training software engineering agents. Existing datasets often exhibit distinct constraints. SWE-Bench (Jimenez et al., 2024) is widely used for evaluating software engineering performance. The training split lacks executable environments and success signals present in the evaluation split, making its usefulness limited for model training. HumanEval (Chen et al., 2021) is designed for standalone code generation tasks, akin to coding competitions. Therefore, it falls short of addressing the complex challenges inherent in real-world, repositorylevel software engineering tasks, which involve thousands of files, millions of lines of code, and tasks such as bug fixing, feature development, and system optimization. Similarly, R2E (Jain et al., 2024) is small evaluation dataset with 246 instances and, due to its synthetic nature, lacks the realism and complexity in real world software engineering scenario. There is also limited empirical evidence if this environment is useful for training. Our proposed SWE-Gym instead uses real-world GitHub issue bodies as task instructions, and uses the associated unit tests for validation. This approach introduces realistic and complex task formulations, aligning closely with real-world software engineering challenges. 5.3. Post-training: From Chatbots, Reasoners to Agents Post-training, which finetunes pre-trained language model, usually through supervised or reinforcement learning, has proven highly effective in improving model performance across various domains. RLHF (Ouyang et al., 2022) has become standard method for adapting language models into chatbots, improving both performance and alignment In mathematical reasoning, of the model (Team, 2024). two standard datasets, MATH (Hendrycks et al., 2021b) and GSM-8K (Cobbe et al., 2021), have question and answer pairs for both training and evaluation, which enables researchers to explore methods to train both policy, and verifiers (reward models) (Cobbe et al., 2021; Wang et al., 2024a). Earlier works (Wang et al., 2024b; Chen et al., 2023; Zeng et al., 2023; Wu et al., 2024) shows distilling agent trajectories from stronger models consistently improves weaker models. Recent works shift towards self-improving methods. Xi et al. (2024); Zhai et al. (2024); Bai et al. (2024) show how RL or rejection sampling fine-tuning, guided by reward signal, can enable models to self-improve without reliance on more capable teacher model. Despite the wide success, post-training generally rely on either expert demonstration data or training environment with reliable reward signals, which is largely missing in the software engineering domain. This limitation has contributed to reliance on prompting-based methods with proprietary language models, as reflected in most top-performing approaches on SWE-bench (Jimenez et al., 2024). Our work addresses this gap with SWE-Gym, training environment grounded in real-world software engineering tasks, using associated expert written tests as the reward signal. Our experiments verify that we can use SWE-Gym to build strong SWE agents without any prompt engineering. Concurrent with our work, Ma et al. (2024) and Golubev et al. (2024) both study the training of software engineering agents, with the latter also exploring verifiers training and dataset construction. We discuss the key differences between these concurrent works in A. 6. Conclusions In this paper, we introduce SWE-Gym, the first open training environment that bridges critical gaps in enabling scalable learning for software engineering agents. By combining real-world Python tasks with repository-level context, pre-configured execution environments, and test verifications, SWE-Gym will be foundation for advancing LM agent training research. Through extensive experiments, we demonstrate that SWE-Gym enables both agent and verifier models to achieve significant improvements in resolving complex software tasks. Our findings highlight the scalability of these approaches, revealing potential for continuous performance gains with increased compute. 7. Acknowledgments We thank John Yang and Ofir Press for helpful discussions, and John Yang for assistance in reproducing data analysis re10 Training Software Engineering Agents and Verifiers with SWE-Gym sults from SWE-Bench. We thank Modal Labs6 for the GPU compute support through its Academic Credits Program."
        },
        {
            "title": "References",
            "content": "Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., Petron, A., Paino, A., Plappert, M., Powell, G., Ribas, R., et al. Solving rubiks cube with robot hand. arXiv preprint arXiv:1910.07113, 2019. Badertdinov, I., Trofimova, M., Anapolskiy, Y., Abramov, S., Zainullina, K., Golubev, A., Polezhaev, S., Litvintseva, D., Karasik, S., Fisin, F., Skvortsov, S., Nekrashevich, M., Shevtsov, A., and Yangel, B. Scaling data collection for training software engineering agents. Nebius blog, 2024. Bai, H., Zhou, Y., Cemri, M., Pan, J., Suhr, A., Training Levine, S., and Kumar, A. in-the-wild device-control agents with autonomous reinforcement abs/2406.11896, 2024. URL https://api.semanticscholar. org/CorpusID:270562229. learning. Digirl: ArXiv, Brown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., Re, C., and Mirhoseini, A. Large language monkeys: Scaling inference compute with repeated sampling. ArXiv, abs/2407.21787, 2024. URL https: //api.semanticscholar.org/CorpusID: 271571035. Chen, B., Shu, C., Shareghi, E., Collier, N., Narasimhan, K., and Yao, S. Fireact: Toward language agent fine-tuning. ArXiv, abs/2310.05915, 2023. URL https: //api.semanticscholar.org/CorpusID: 263829338. Chen, D., Lin, S., Zeng, M., Zan, D., Wang, J.-G., Cheshkov, A., Sun, J., Yu, H., Dong, G., Aliev, A., Wang, J., Cheng, X., Liang, G., Ma, Y., Bian, P., Xie, T., and Wang, Q. Coder: Issue resolving with multi-agent and task graphs. CoRR in ArXiv, abs/2406.01304, 2024. Chen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D. W., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Babuschkin, I., Balaji, S., Jain, S., Carr, A., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M. M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. ArXiv, abs/2107.03374, 2021. URL https://api.semanticscholar. org/CorpusID:235755472. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, 6https://modal.com/ 11 Training Software Engineering Agents and Verifiers with SWE-Gym R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. ArXiv, abs/2110.14168, 2021. URL https://api.semanticscholar. org/CorpusID:239998651. Golubev, A., Polezhaev, S., Zainullina, K., Trofimova, M., Badertdinov, I., Anapolskiy, Y., Litvintseva, D., Karasik, S., Fisin, F., Skvortsov, S., Nekrashevich, M., Shevtsov, A., Abramov, S., and Yangel, B. Leveraging training and search for better software engineering agents. Nebius blog, 2024. https://nebius.com/blog/posts/training-andsearch-for-software-engineering-agents. Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. R. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/ forum?id=VTF8yNQM66. Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A., et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and Steinhardt, J. Measuring coding challenge competence with APPS. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021a. URL https: //datasets-benchmarks-proceedings. neurips.cc/paper/2021/hash/ c24cd76e1ce41366a4bbe8a49b02a028-Abstract-round2. html. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. ArXiv, abs/2305.20050, 2023. URL https: //api.semanticscholar.org/CorpusID: 258987659. Ma, Y., Cao, R., Cao, Y., Zhang, Y., Chen, J., Liu, Y., Liu, Y., Li, B., Huang, F., and Li, Y. Lingma swe-gpt: An open development-process-centric language model for automated software improvement. arXiv preprint arXiv:2411.00622, 2024. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D. X., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. ArXiv, abs/2103.03874, 2021b. URL https://api.semanticscholar. org/CorpusID:232134851. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptaIn The Tenth Internation of large language models. tional Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9. Hui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Dang, K., et al. Qwen2. 5coder technical report. arXiv preprint arXiv:2409.12186, 2024a. Hui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Dang, K., et al. Qwen2. 5coder technical report. arXiv preprint arXiv:2409.12186, 2024b. Jain, N., Shetty, M., Zhang, T., Han, K., Sen, K., and Stoica, I. R2E: turning any github repository into programming agent environment. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=kXHgEYFyf3. 12 Modal. Modal: High-performance AI infrastructure. https://modal.com/, 2024. Accessed: 2024-1218. Neubig, G. and Wang, X. on as Coding Agents All Hands AI blog, 2024. Speed!). https://www.all-hands.dev/blog/ evaluation-of-llms-as-coding-agents-on-swe-bench-at-30x-speed. SWE-Bench (at Evaluation of LLMs 30x URL Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Pan, J., Zhang, Y., Tomlin, N., Zhou, Y., Levine, S., and Suhr, A. Autonomous evaluation and refinement of digital agents. ArXiv, abs/2404.06474, 2024. URL https://api.semanticscholar. org/CorpusID:269009430. PyTorch Team. PyTorch native posttraining library. https://github.com/pytorch/ torchtune, 2024. torchtune: Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. ArXiv, abs/1707.06347, 2017. URL https://api. semanticscholar.org/CorpusID:28695052. Training Software Engineering Agents and Verifiers with SWE-Gym Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap, T. P., Simonyan, K., and Hassabis, D. Mastering chess and shogi by self-play with general reinforcement learning algorithm. ArXiv, abs/1712.01815, 2017. URL https://api.semanticscholar. org/CorpusID:33081038. Snell, C. B., Kostrikov, I., Su, Y., Yang, M., and Levine, S. Offline rl for natural language generation with implicit language learning. ArXiv, abs/2206.11871, 2022. URL https://api.semanticscholar. org/CorpusID:249954054. Tao, N., Ventresque, A., Nallur, V., and Saber, T. Enhancing program synthesis with large language models using many-objective grammar-guided genetic programming. Algorithms, 17(7):287, 2024. doi: 10.3390/A17070287. URL https://doi.org/10.3390/a17070287. Team, Q. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm.github.io/ blog/qwen2.5/. Tong, Y., Zhang, X., Wang, R., Wu, R. M., and He, J. Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving. ArXiv, abs/2407.13690, 2024. URL https://api.semanticscholar. org/CorpusID:271270574. Unsloth Team. Easily finetune and train LLMs. Get faster with unsloth. https://unsloth.ai/, 2024. Wang, P., Li, L., Shao, Z., Xu, R., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.510. URL https: //aclanthology.org/2024.acl-long.510. Wang, X., Li, B., Song, Y., Xu, F. F., Tang, X., Zhuge, M., Pan, J., Song, Y., Li, B., Singh, J., Tran, H. H., Li, F., Ma, R., Zheng, M., Qian, B., Shao, Y., Muennighoff, N., Zhang, Y., Hui, B., Lin, J., Brennan, R., Peng, H., Ji, H., and Neubig, G. OpenHands: An Open Platform for AI Software Developers as Generalist Agents. CoRR in ArXiv, abs/2407.16741, 2024c. Wu, Z., Bai, H., Zhang, A., Gu, J., Vinod Vydiswaran, V., Jaitly, N., and Zhang, Y. Divide-or-conquer? which part should you distill your llm? ArXiv, 2024. Xi, Z., Ding, Y., Chen, W., Hong, B., Guo, H., Wang, J., Yang, D., Liao, C., Guo, X., He, W., Gao, S., Chen, L., Zheng, R., Zou, Y., Gui, T., Zhang, Q., Qiu, X., Huang, X., Wu, Z., and Jiang, Y.-G. Agentgym: Evolving large language model-based agents across diverse environments. ArXiv, abs/2406.04151, 2024. URL https://api.semanticscholar. org/CorpusID:270285866. Xia, C. S., Deng, Y., Dunn, S., and Zhang, L. Agentless: Demystifying llm-based software engineering agents. CoRR, abs/2407.01489, 2024. doi: 10.48550/ARXIV. 2407.01489. URL https://doi.org/10.48550/ arXiv.2407.01489. Yang, J., Jimenez, C. E., Wettig, A., Lieret, K., Yao, S., Narasimhan, K., and Press, O. Swe-agent: Agentcomputer interfaces enable automated software engineering. CoRR, abs/2405.15793, 2024. doi: 10.48550/ARXIV. 2405.15793. URL https://doi.org/10.48550/ arXiv.2405.15793. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and In The Eleventh Internaacting in language models. tional Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum? id=WE_vluYUL-X. Yuan, L., Cui, G., Wang, H., Ding, N., Wang, X., Deng, J., Shan, B., Chen, H., Xie, R., Lin, Y., Liu, Z., Zhou, B., Peng, H., Liu, Z., and Sun, M. Advancing LLM reasoning generalists with preference trees. CoRR, abs/2404.02078, 2024. doi: 10.48550/ARXIV.2404.02078. URL https: //doi.org/10.48550/arXiv.2404.02078. Wang, X., Chen, Y., Yuan, L., Zhang, Y., Li, Y., Peng, H., and Ji, H. Executable code actions elicit better LLM agents. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024b. URL https: //openreview.net/forum?id=jJ9BoXAfFa. Zeng, A., Liu, M., Lu, R., Wang, B., Liu, X., Dong, Agenttuning: Enabling generY., and Tang, J. In Annual Meetalized agent abilities for llms. ing of the Association for Computational Linguistics, 2023. URL https://api.semanticscholar. org/CorpusID:264306101. 13 Training Software Engineering Agents and Verifiers with SWE-Gym Zhai, Y., Bai, H., Lin, Z., Pan, J., Tong, S., Zhou, Y., Suhr, A., Xie, S., LeCun, Y., Ma, Y., and Levine, S. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. ArXiv, abs/2405.10292, 2024. URL https://api.semanticscholar. org/CorpusID:269790773. Zhang, K., Yao, W., Liu, Z., Feng, Y., Liu, Z., Murthy, R., Lan, T., Li, L., Lou, R., Xu, J., Pang, B., Zhou, Y., Heinecke, S., Savarese, S., Wang, H., and Xiong, C. Diversity empowers intelligence: Integrating expertise of software engineering agents. ArXiv, abs/2408.07060, 2024a. URL https://api.semanticscholar. org/CorpusID:271860093. Zhang, Y., Ruan, H., Fan, Z., and Roychoudhury, A. Autocoderover: Autonomous program improvement. In ISSTA, 2024b. Zhao, W., Jiang, N., Lee, C., Chiu, J. T., Cardie, C., Galle, M., and Rush, A. M. Commit0: Library generation from scratch, 2024. URL https://arxiv.org/abs/ 2412.01769. Zheng, L., Yin, L., Xie, Z., Sun, C., Huang, J., Yu, C. H., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., Barrett, C., and Sheng, Y. Sglang: Efficient execution of structured language model programs, 2024a. URL https://arxiv.org/abs/2312.07104. Zheng, T., Zhang, G., Shen, T., Liu, X., Lin, B. Y., Fu, J., Chen, W., and Yue, X. Opencodeinterpreter: Integrating code generation with exArXiv, abs/2402.14658, ecution and refinement. 2024b. URL https://api.semanticscholar. org/CorpusID:267782452. Zhou, Y., Zanette, A., Pan, J., Levine, S., and Kumar, A. Archer: Training language model agents via hierarchical multi-turn rl. ArXiv, abs/2402.19446, 2024. URL https://api.semanticscholar. org/CorpusID:268091206. Orwall, A. Moatless Tool. https://github. com/aorwall/moatless-tools, 2024. Accessed: 2024-10-22. 14 Training Software Engineering Agents and Verifiers with SWE-Gym A. Comparison with Concurrent Works Ma et al. (2024) trains an LM agent, Lingma SWE-GPT, using method similar to our rejection sampling fine-tuning baseline, with dataset comparable to our SWE-Gym Raw splits. Without executable unit test feedback, they rely on manually defined heuristics to filter out low-quality trajectories, such as comparing similarity between submitted patches and edit locations with gold patches. The model weights are publicly accessible but not the training pipeline or the dataset. Most relevant to our work are two consecutive blog posts by Golubev et al. (2024) and Badertdinov et al. (2024), who also construct an executable training environment with real-world tasks from GitHub. Instead of manual configuration, they employ general environment setup script and simply discard instances that fail the setup process. This approach leads to key differences in dataset size and distribution: while it biases the environment away from tasks with complex dependencies, they successfully collect 6,415 instances, about 1.5 times larger than our dataset. In Golubev et al. (2024), they also study training agents and verifiers with the environment. Additionally, they explore lookahead setting where trained verifier ranks and selects the best next action. With substantially large collection of agent trajectories (80,036 compared to thousands in our experiments) and model size (72B compared to 32B), Their best system achieves 40% accuracy on SWE-Bench Verified. While their dataset and agent trajectories are publicly accessible, the model is not. In comparison, with comparable dataset size, our SWE-Gym has executable feedback, avoids potential dataset bias through manual configuration of environments, while providing comprehensive analysis of agent and verifier training, their scaling behaviors, and positive results on agent self-improvement. Our system achieves competitive results with significantly lower compute and smaller model size (32B vs 72B). Lastly, we open source all artifacts of the project, including dataset, model weights, agent trajectory data and the training pipeline. Model Name, Model Size Ma et al. (2024), 72B Golubev et al. (2024) Agent and Verifier, 72B Our SWE-Gym Agent and Verifier, 32B Openness SWE-Bench Lite Verified Model Environment 22.0 - 26. 30.2 40.6 32.0 Table 6: Comparison of model performance on SWE-Bench benchmark and if the model weights and environments are publically accessible (openness). B. Experiment Details B.1. Mean and Variance for Pass@N and Best@N. We mostly follow (Lightman et al., 2023) for obtaining the mean and variance for the Pass@N and Best@N curve. Given total of rounds of rollouts, for < , we calculate the mean and variance across 100 randomly selected sub-samples of size from the rollouts. For the OpenHands CodeActAgent inference-time scaling curve at 4, we exclude this calculation for N=1 , as we use temperature of 0 for the first attempt. B.2. Training Details. OpenHands Agent Experiment. We use torchtune (PyTorch Team, 2024) for full parameter fine-tuning with learning rate of 1e-4, maximum 5 epochs, global batch size of 8, max context length of 32768. We fine-tuned both 7B, 14B, and 32B variant of the model, and experiments were performed with 2-8x NVIDIA H100 80G GPU on modal (Modal, 2024). The only exception is in the main experiment of 4.1.1, where we use LoRA (Hu et al., 2022) (29.8% @8) via Unsloth library (Unsloth Team, 2024) to train the verifier for max 2 epochs, while other hyper-parameter stays the same. MoatlessTools Agent Experiment. All MoatlessTools models are trained with context window of 10240. For experiments with the 7B model, we use torchtune to train the policy model with full-finetuning using 4 H100 GPUs. We set batch size to 8, learning rate to 2 105, and train for 5 epochs. For the 32B model, we use Unsloth (Unsloth Team, 2024) with single H100 GPU for LoRA fine-tuning. We set the number of epochs to 5, batch size to 8, LoRA rank to 64, and learning rate to 5 104. We use the same configuration for verifier training. 15 Training Software Engineering Agents and Verifiers with SWE-Gym getmoto/moto Project-MONAI/MONAI pandas-dev/pandas python/mypy dask/dask iterative/dvc conan-io/conan pydantic/pydantic facebookresearch/hydra bokeh/bokeh modin-project/modin"
        },
        {
            "title": "Total",
            "content": "Original Dedup. 155 95 70 46 45 36 20 11 7 3 3 491 72 53 61 27 29 24 12 7 5 2 2 294 Sorted by Random (Dedup.) First 25% First 50% First 25% Sorted by Repo (Dedup.) First 50% 12 17 14 7 8 8 1 2 2 1 1 73 33 25 30 12 17 12 7 4 5 1 1 0 53 0 0 6 0 12 0 0 2 0 73 46 53 0 0 29 0 12 0 5 2 0 147 Table 7: Distribution of success trajectories used in training-time scaling experiments (4.2). Dedup. denotes that the trajectories are deduplicated by randomly select ONE success trajectory per instance ID; Sorted by random (repo) X% (Dedup.) denotes subset of trajectories taken from the first X% from dedup. instances that are sorted randomly (by repository name). Num. of Messages Num. of Tokens Resolved 5, 557.0 491.0 5, 557.0 491.0 Count Mean 39.2 39.9 Std 31.9 19.9 Min 7.0 13. Max 101.0 101.0 5% 9.0 19.0 10% 9.0 21. Percentiles 25% 9.0 25.0 50% 29.0 33.0 75% 61.0 47.5 90% 100.0 65.0 95% 101.0 87.0 17, 218.3 18, 578. 17, 761.6 11, 361.4 1, 615.0 2, 560.0 167, 834.0 81, 245.0 1, 833.0 5, 813.0 1, 907.0 8, 357.0 2, 268.0 11, 559. 12, 305.0 15, 999.0 26, 434.0 22, 040.5 41, 182.2 31, 632.0 51, 780.6 39, 512.5 Table 8: Statistics of SWE-Gym-sampled trajectories. We use the tokenizer from Qwen-2.5-Coder-Instruct-7B to estimate the number of tokens. Agent RAG RAG Lingma Agent (Ma et al., 2024) Lingma Agent (Ma et al., 2024) Training Software Engineering Agents and Verifiers with SWE-Gym Model Model Size Training Data Resolved (%) SWE-Bench Verified (500 instances) SWE-Llama (Jimenez et al., 2024) SWE-Llama (Jimenez et al., 2024) Lingma SWE-GPT (v0925) Lingma SWE-GPT (v0925) 7B 13B 7B 72B 32B 10K instances 10K instances 90K PRs from 4K repos 90K PRs from 4K repos 491 agent trajectories from 11 repos 491 agent trajectories from 11 repos for agent + 1318 2 success/failure agent trajectories for verifier 1.4 1.2 18. 28.8 20.6 32.0 OpenHands (Wang et al., 2024c) (Ours) fine-tuned Qwen2.5-Coder-Instruct OpenHands w/ Verifier (Wang et al., 2024c) (Ours) fine-tuned Qwen2.5-Coder-Instruct 32B (Agent & Verifier) Table 9: Performance comparison with SWE-Bench (Jimenez et al., 2024) baselines with publicly accessible weights. Data source: https://www.swebench.com/, Accessed on Dec 21, 2024. Trajectory Set Sampled from Model Sampled on Dataset Temperature Max Turns Success trajectories D0 gpt-4o-2024-08-06 SWE-Gym Lite 0 19 (8.26%) D1 D0 D2 D1 gpt-4o-2024-08-06 gpt-4o-2024-08-06 gpt-4o-2024-08-06 gpt-4o-2024-08-06 gpt-4o-2024-08-06 SWE-Gym Lite SWE-Gym Lite SWE-Gym Lite SWE-Gym Lite SWE-Gym Lite gpt-4o-2024-08-06 claude-3-5-sonnet-20241022 gpt-4o-2024-08-06 gpt-4o-2024-08SWE-Gym Lite SWE-Gym Lite SWE-Gym Full SWE-Gym Full (Cumulative) Total D0 0.2 0.3 0.4 0.5 0.8 30 30 30 30 30 (Cumulative) Total D1 0 0 0 50 50 50 50 (Cumulative) Total D2 19 11 (4.78%) 17 (7.39%) 21 (9.13%) 18 (7.83%) 20 (8.70%) 106 19 (8.26%) 67 (29.1%) 111 (4.55%) 188 (7.71%) 491 * Run into infrastructure-related error where some instances failed to complete, this number might be under estimate of actual number of success trajectories. Table 10: Summary of trajectories sampled from SWE-Gym. For MoatlessAgent experiments, we serve the agent with FP8 quantization for improved throughput, which we found to have minimal effects on model performance. B.3. Details of OpenHands Trajectory Sampling As detailed in Tab. 10, we collect few sets of trajectories for fine-tuning experiments. We collect dataset D0 by sample gpt-4o-2024-08-06 on SWE-Gym Lite with temperature 0 and collected 19 trajectories that eventually solve the task (evaluated by unit test in SWE-Gym). We then varied the temperatures (setting t={0.2, 0.3, 0.4, 0.5, 0.8}) and sample on SWE-Gym Lite. Combining these instances with D0, we get 106 trajectories that solve the given problem (D1). We set the maximum number of turns to be 30 for both D0 and D1. To experiment on the effect of max turn, we set max number of turns to 50 and sample gpt-4o-2024-08-06 (19 resolved out of 230) and claude-3-5-sonnet-20241022 (67 resolved out of 230) with temperature 0 on SWE-Gym Lite, and sample gpt-4o-2024-08-06 (temperature t={0, 1}) on SWE-Gym full set (in total 299 resolved out of 4876 instances). This gives us in in total 106 + 19 + 67 + 299 = 491 success trajectories, which forms our final training trajectories D2. B.4. MoatlessTools ORM Prompt The following is pseudo-code that generates prompt for MoatlessTools Verifier (ORM), which is modified from (Zhang et al., 2024a). Unlike (Zhang et al., 2024a), which relies on proprietary models like Claude-3.5-Sonnet for context extraction, we obtain context directly from the agents trajectory being evaluated. 17 Training Software Engineering Agents and Verifiers with SWE-Gym (cid:44) SYSTEM_MESSAGE = \"\"\"You are an expert in python for software engineering and code review. Your responsibility is to review the patches generated by language models to fix some issues and provide feedback on the quality of their code.\"\"\" (cid:44) (cid:44) USER_MESSAGE=\"\"\"I want you to evaluate an LLM-generated candidate patch that (cid:44) tries to resolve an issue in codebase. To assist you in this task, you are provided with the following information: - You are given an issue text on github repository (wrapped with <issue_description></issue_description>). (cid:44) - You are also given some identified code spans that are relevant to the issue. Each code span is wrapped with <code_span file_path=FILE_PATH (cid:44) (cid:44) span_id=SPAN_ID></code_span> tags, where FILE_PATH is the path to the file containing the code span, and SPAN_ID is the unique identifier for the code span. (cid:44) Each code span also comes with the line numbers for you to better understand (cid:44) (cid:44) the context. It's possible that the code span are not sufficient to fix the issue, adjust your score accordingly. - You are given the candidate patch that tries to resolve the target issue. For your convenience, you are given the hunks of original code and the code after applying the patch. (cid:44) The code before the patch is wrapped with <before_patch></before_patch> and the code after the patch is wrapped with <after_patch></after_patch>. (cid:44) Note that the file names in before_patch starts with 'a/' and the file names (cid:44) in after_patch starts with 'b/'. <issue_description> {issue_text} </issue_description> <before_patch> {before_patch} </before_patch> <after_patch> {after_patch} </after_patch> {code_spans} Response in \"True\" or \"False\" for whether the patch has resolved the issue.\"\"\" B.5. OpenHands ORM Prompt The following is pseudo-code that generates prompt for OpenHands Verifier (ORM). SYSTEM_MESSAGE = '''You are an expert judge evaluating AI assistant interactions. (cid:44) (cid:44) Your task is to determine if the assistant successfully resolved the user's request. Key evaluation criteria: 1. Did the assistant complete the main task requested by the user? 18 Training Software Engineering Agents and Verifiers with SWE-Gym 2. Did the assistant handle all edge cases and requirements specified? 3. Were there any errors or issues in the final solution? 4. Did the assistant verify the solution works as intended? Respond only with \"<judgement>YES</judgement>\" or \"<judgement>NO</judgement>\".''' USER_MESSAGE = '''Please evaluate the following interaction between an AI (cid:44) assistant and user: === INTERACTION LOG === ''' + traj_str + ''' === END INTERACTION === Based on the above interaction, did the assistant successfully resolve the user's (cid:44) initial request? Respond with YES or NO.''' messages = [ {'role': 'system', 'content': SYSTEM_MESSAGE}, {'role': 'user', 'content': USER_MESSAGE}, {'role': 'assistant', 'content': '<judgement>' + (\"YES\" if resolved else (cid:44) \"NO\") + '</judgement>'} ] The last assistant messages that contains judgement is only provided during training time. At inference time, the trained verifier is responsible predicting the probability of Yes and No."
        }
    ],
    "affiliations": [
        "Apple",
        "CMU",
        "UC Berkeley",
        "UIUC"
    ]
}