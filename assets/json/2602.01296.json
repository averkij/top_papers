{
    "paper_title": "Interacted Planes Reveal 3D Line Mapping",
    "authors": [
        "Zeran Ke",
        "Bin Tan",
        "Gui-Song Xia",
        "Yujun Shen",
        "Nan Xue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch. We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\\&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research."
        },
        {
            "title": "Start",
            "content": "IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Interacted Planes Reveal 3D Line Mapping Zeran Ke, Bin Tan, Gui-Song Xia, Yujun Shen, Nan Xue Abstract3D line mapping from multi-view RGB images provides compact and structured visual representation of scenes. We study the problem from physical and topological perspective: 3D line most naturally emerges as the edge of finite 3D planar patch. We present LiP-Map, lineplane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research. Index Terms3D Line Mapping, 3D Planar Primitives, Interacted Planes, Structured Visual Geometry, Camera Relocalization"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "Structured 3D reconstruction from multi-view images using simple primitives [1], [2], [3], [4], [5], [6], [7], [8] is compelling, as it parallels how humans intuitively build complex 3D geometries from fundamental elements such as points, lines, curves, and planes, leveraging the parsimony of geometric composition. The symbolic nature of structured 3D scene representations could arguably enhance spatial intelligence, enabling more efficient understanding of our 3D world. Yet this perspective is controversial, since many in the community remain unconvinced, citing the limited reconstruction quality of current methods. In this paper, we focus on two key primitives, rectangular planes and line segments, with the goal of achieving accurate, complete, and detailed 3D line mapping1 for man-made environments. The problem of 3D line mapping has been extensively studied through two-view matching [9], [10] and multi-view tracking of 2D line segments, using pipeline analogous to 3D point mapping. However, due to the fundamental differences between line segments and points, establishing reliable line correspondences remains challenging. recent approach, LIMAP [1], attempts to mitigate this issue by considering the top line matches instead of single match, thereby improving the chance of correct correspondences while pruning incorrect ones. Although LIMAP [1] significantly outperforms Line3D++ [3], it is limited by strong dependence on dense viewpoints to achieve successful line triangulation. Other studies [2], [4] aim to bypass 2D line matching by leveraging neural implicit fields, demonstrat- (: equal contribution. : corresponding author) Z.-R. Ke is with the School of Computer Science, Wuhan University, Wuhan 430072, China. He is currently research intern at Ant Group. Gui-Song Xia is with the School of Computer Science and the School of Artificial Intelligence, Wuhan University, Wuhan 430072, China. B. Tan, Y. Shen, and N. Xue are with Ant Group, Hangzhou 310000, China. 1. This terminology is borrowed from LIMAP [1], and this task is also referred to as 3D line reconstruction. ing the potential for matching-free formulations. However, they suffer from the slow optimization process due to their reliance on neural fields. To construct reliable 3D line map from 2D line detections across multiple views, two key ingredients are required: robust cross-view correspondences and accurate spatial placement of the reconstructed 3D lines. common strategy is to utilize an existing surface reconstruction from multi-view inputs with known camera poses and to project detected 2D lines onto the surface using depth maps. However, this approach often fails in practice, as illustrated in Fig. 1. On one hand, view-dependent parallax can cause single image line to correspond to multiple 3D line segments at different depths. On the other hand, inconsistencies in the depth maps can introduce spurious structural lines. Because true 3D lines are inherently inseparable from structural surface edges, joint recovery of surfaces and edges is indispensable, motivating novel formulation of 3D line mapping that explicitly models the geometric relationship and interaction between planes and line segments. Planar representations offer promising opportunity. PlanarSplatting [8] reconstructs structured scene surfaces by optimizing set of learnable planar primitives. This formulation is attractive because plane boundaries are naturally formed by lines, revealing direct geometric and topological link between surfaces and 3D line structures. However, PlanarSplatting is designed for surface reconstruction only, without explicitly connecting multi-view 2D line detections to the learned primitives. Thus, we explore how to associate 2D line segments with 3D planar edges and leverage these associations to reconstruct reliable, consistent 3D line maps. We therefore propose LiP-Map, which explicitly exploits the interaction between planes and lines to recover reliable and consistent 3D line maps. The name stands for LinePlane Joint Optimization for 3D line mapping. Our approach leverages the geometric synergy between line segments and the edges of 3D planar primitives. By applying any existing 2D line segment detector [11], [12], 6 2 0 2 1 ] . [ 1 6 9 2 1 0 . 2 0 6 2 : r IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2 Fig. 1: Comparison of different 3D line maps on scenes of three public datasets. First row: lift 2D detected lines into 3D lines using depths rendered from the GT mesh. Second row: lift 2D detected lines into 3D lines using depths rendered by PlanarSplatting [8]. Third row: extract planar edges from PlanarSplatting [8]. Last row: our 3D line maps. 2D line segments are detected by DeepLSD [11]. lift means back-project 2D detected lines into 3D using the sensor/predicted depth maps. [13], [14] to multi-view input images, set of 3D planes is optimized with two key learning objectives: The 3D planes should align as closely as possible with the 2.5D depth/normal maps from the input images. The edges (or boundaries) of the 3D planes should be consistent with the observed 2D line segments. As illustrated in Fig. 2, given set of posed multi-view images, our LiP-Map can reconstruct the planar surface of the scene by optimizing set of learnable 3D planar primitives, with the supervision of depth and normal maps. Meanwhile, the geometric structure of the scenes planar surface model is progressively refined under the supervision of the multi-view detected 2D line segments. By formulating joint optimization that enforces the two above objectives, LiP-Map achieves state-of-the-art accuracy and completeness in 3D line mapping. Extensive experiments on public benchmarks, including 50 scenes from the ScanNetV2 [15] dataset, 30 scenes from the ScanNet++ [16] dataset, and 10 scenes from the Hypersim [17] dataset, demonstrate its advantages. Furthermore, qualitative comparisons on 7Scenes [18] and Tanks&Temples [19] using camera poses from VGGT [20] highlight the robustness of LiP-Map for pose-free inputs. Beyond reconstruction, our 3D line maps provide practical gains in line-assisted visual localization: the lifted 3D primitives offer stable, geometrically meaningful correspondences that complement point features and remain robust in textureless or repetitive regions. On the 7Scenes [18] dataset, integrating them into standard point-only localization pipeline yields substantially improved pose accuracy and robustness over both point-only baselines and existing pointline joint methods. In addition, LiP-Map also enhances the quality of planar 3D reconstruction, further underscoring its versatility."
        },
        {
            "title": "2 RELATED WORK",
            "content": "3D Line Mapping. Mapping 2D features from multi-view images to 3D is fundamental task with numerous applications in 3D vision, augmented reality, and robotics [21], [22], [23], [24]. However, in indoor scenes, where structures dominate and surfaces often lack texture, traditional 2D keypoint-based mapping methods [25], [26], [27], [28] tend to be suboptimal. Consequently, 3D line mapping has emerged as compelling alternative to conventional 3D point mapping. Recent advances in 2D line segment detection [11], [13], [14], [29], [30], [31] and matching [10], [32] have enabled the adaptation of 3D point mapping pipelines to 3D line mapping [1], [3], [33]. However, because of the fundamental differences between 2D line features and point features, the matching process often introduces inherent uncertainties, sometimes leading to failures. To address this, several studies [1], [3], [34] leverage geometric constraints in the search for line correspondence to improve reconstruction. More recently, neural field-based approaches [2], [4] have sought to eliminate explicit line matching, achieving improved mapping results. Structured 3D Surface Reconstruction. In addition to 3D line mapping, many studies explore 3D reconstruction using different types of primitives, such as surfels [35], [36], [37], planes [5], [6], [8], [38], [39], [40], [41], [42], [43], polygonal meshes [44], [45], [46], [47], sketches [48], and implicit surfaces [49], [50], [51], among others. Among these, planar 3D reconstruction via PlanarSplatting [8] shows great potential for indoor scenes, as it explicitly optimizes surface geometry with set of 3D planes derived from 2.5D cues at ultrafast speed. Explicit modeling and optimization of planar surface geometry bring several benefits over other methods: (1) the expected geometry is always accessible in differentiable manner without the need for proxies, and (2) planar surfaces have deterministic geometric meaning and approximate scene geometry with linear structures. Our study falls into the category of matching-free solutions, and we adopt 3D planar splatting [8] for its computational efficiency and the intrinsic relationship between plane and line representations. To the best of our knowledge, this IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE Fig. 2: Overview of our proposed reconstruction pipeline, novel 3D line mapping method by exploring the structural synergies between 3D planes and lines. Fig. 3: Illustration of surfaces (left) and 3D lines (right) in ScanNetV2 scene [15]. The 3D lines align closely with physical surface boundaries, highlighting the strong correlation between the two structures. is the first work to explicitly optimize the synergy between 3D line segments and planes, offering novel perspective on structured 3D reconstruction."
        },
        {
            "title": "3 LiP-Map: LINE-PLANE JOINT MAPPING",
            "content": "Given set of posed multi-view images with monocular 2D and 2.5D sketches, = {(Ii, Ki, ξi, Di, Ni, Li)}N i=1 (1) where Ii is the i-th image, Ki and ξi are the intrinsic and extrinsic matrices, Di and Ni are the depth and normal maps (either provided as ground-truth or predicted by pretrained 3D foundation models [52], [53]), and Li is the set of 2D line segments detected by any line segment detectors [11], [12], [13], [14], denoted by (cid:110) Li = i,j = (x1 ldet i,j, x2 i,j) R2 R2(cid:111) . (2) Our goal is to reconstruct the 3D line map of the corresponding scene using these 2D/2.5D geometric cues. The final line mapping result is denoted by = (cid:110) = (u3d l3d , v3d ) R3 R3(cid:111) . (3) Fig. 4: Representation of the 3D rectangular plane with learnable shape parameters. 3.1 Representation of Planar Surface We follow the recent PlanarSplatting [8], which reconstructs scene using set of 3D rectangular planes Π = {πi}K i=1 from I. This is motivated by the observation (see Fig. 3) that 3D rectangular planes naturally link surfaces and 3D lines: the interiors of the planes approximate scene surfaces, while their edges encompass the target 3D lines. 3D Plane Parameterization. As shown in the Fig. 4 and described in PlanarSplatting [8], each 3D rectangular plane π is equipped with learnable parameters, including the plane center pπ R3, the plane rotation qπ R4 (in quaterπ } R4. nion), and the plane radii rπ = {rx+ Here, the symbol +/ represents the positive/negative direction of the X-axis/Y-axis of the rectangle. Then, the π R3 positive direction of the X-axis of the 3D rectangular plane can be further calculated as: π R3 and Y-axis π , rx π , ry+ π , ry π = R(qπ)[1, 0, 0], x (4) where R(qπ) R33 is the rotation matrix converted from qπ. The normal of the 3D rectangular plane nπ R3 can be calculated as: π = R(qπ)[0, 1, 0], nπ = R(qπ)[0, 0, 1]. These learnable 3D planes Π are optimized with 2.5D geometric cues (i.e., monocular depth and normal maps) to reconstruct the scene with the differentiable rasterization. Please refer to [8] or our Appendix for more details. (5) IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 4 Plane Edges as Lines. Based on the above representation, we further sequentially define the set of four planar vertices π R3}4 Vπ = {vi i=1 of the 3D plane π as: π π v1 π =pπ + rx+ π, π π v2 π =pπ + rx+ π, v3 π π π =pπ rx π, π π π =pπ rx v4 π. π + ry+ π ry π ry π + ry+ (6) Then, the set of four planar edges Eπ = {ei plane π can be represented as: π}4 i=1 of the 3D e1 π =(v1 π =(v3 π, v2 π, v4 π), π), e2 π = (v2 π = (v4 e4 π, v3 π, v1 π), π). (7) For scene represented with 3D planes Π = {πi}K i=1, the target 3D lines is the subset of the edges of all 3D planes: {Eπi}K i=1. (8) This simplifies 3D line mapping into the task of optimizing plane edges aligned with 2D line detections, jointly with the optimization of plane surfaces. 3.2 2D Line to 3D Plane Edge Assignment To reconstruct 3D lines within the plane edges {Eπi}K i=1, we have to find the plane edges that potentially correspond to 3D lines. To avoid ambiguity, we refer to these kinds of edges as line-edge, denoted as = {ej }, where πi {1, 2, ..., K}, {1, 2, 3, 4}, and {Eπi }K i=1. We apply the 2D line segments Li from arbitrary line detectors to find line-edge. Because all planes are under optimization, directly associating any plane edges Eπi with 2D observations ldet i,j Li would be fragile. We tackle this issue from the attraction field representations [13] of the 2D line segments Li. Denote Ri,j as the 1-pixel region of the line segment ldet i,j , in which the distance of any pixel Ri,j to ldet i,j is not greater than 1 pixel, and we pixel-wise associate the most possible planar primitive with respect to the pixel and its supported line segment ldet i,j . Fig. 5 shows case of detected line segments using DeepLSD [11] and all 1 pixel regions of the detection results. Taking pixel from the 1-pixel region of ldet i,j , we cast its corresponding ray which starts from the camera center of image Ii, and find the first hit 3D plane π pixel-wise. Next, to make sure which 3D edge of the 3D plane π is the target line-edge, we project all four 3D edges Eπ = {ev v=1 to the 2D image. The projected 2D edges can be defined as: π}4 E2d π = (cid:110) e2d π,v (cid:111)4 v=1 . (9) Since the 3D plane is optimized on the fly, the projected 2D edges E2d π may not be well aligned with the detected 2D line segment ldet i,j before convergence. To address this, we first compare the angular distances between the edges in E2d π and the corresponding detected 2D line, and discard the two edges with the largest angular distances. We then apply the orthogonal distance between the remaining edges and the corresponding detected 2D line to select the best candidate e2d π . With the best candidate e2d π,v E2d π,v Illustration of 2D line segments detected by Fig. 5: DeepLSD [11] and their corresponding 1-pixel regions in single view of the Hypersim [17] scene ai 001 001. Left: the detected 2D lines. Right: the corresponding 1-pixel regions. Fig. 6: Illustration of 2D line to 3D plane edge assignment. The best candidate of the line-edge is filtered from the perspective of angle and distance. regarding to ldet at pixel as i,j , we define the selected 2D/3D line-edge pπ := e2d e2d π,v = (v1 π,v , v2 π,v ), epπ := ev π , (10) where (v1 edge e2d π,v . π,v , v2 π,v ) are the two vertices of the projected 2D i,j , e2d We now obtain an assignment between the 2D line segment and 2D/3D line-edge from pixel p, denoted as S(p) = (ldet pπ, epπ). After searching throughout all 1pixel regions of all detected 2D line segments Li on image Ii, we finally obtain the assignments of all 2D lines to 3D plane edges with regard to meaningful pixels. These assignments are then used to jointly optimize the 3D lines (line-edge) and 3D planes, which will be described in the next section. For clarity, we illustrate this process in Fig. 7. As it is shown, rays are cast from all pixels in the 1-pixel regions to find the first intersected planar primitive. To achieve this, we first compute the intersection points between all rays and the planes underlying the planar primitives. We then check whether each intersection lies within the bounds of the primitive and whether its depth value is positive, thereby identifying primitives that are validly intersected by the rays. Among these valid intersections, we select the nearest planar primitive with the minimal depth value along each ray. In this way, we identify the 3D planes that potentially contain 3D line segments. Finally, the selected planar primitives are jointly optimized to reconstruct the corresponding 3D lines as described in Sec. 3.3. Their edges IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 5 where Deuc(ldet i,j , e2d pπ) = min(x1 i,j v1 i,j v2 π,v + x2 π,v + x2 i,j v2 i,j v1 π,v , π,v ), Dort(ldet i,j , e2d pπ) = + (v1 (v2 π,v x2 x1 π,v x2 x1 i,j) (x1 i,j x2 i,j i,j) (x1 i,j x2 i,j i,j x2 i,j) i,j x2 i,j) (14) (15) . i,j, x2 Here (x1 ldet i,j , and (v1 line-edge e2d i,j) are two endpoints of the 2D line segment π,v , v2 π,v ) are two endpoints of the projected pπ. Besides the above two losses for constraining each lineedge in both 2D and 3D space individually, we also introduce group loss Lgroup which encourages all line-edge assigned to the same 2D line segment to be close enough in the 3D space and can be calculated as: Lgroup = (cid:88) (cid:88) Dort(ep1π, ep2π), (16) ldet i,j Li p1,p2R(ldet i,j ) where p1, p2 are the pixels within the 1-pixel region R(l) of line segment l. The final loss can then be calculated as: = αΠLΠ + αLLE, (17) where LE = (L2d + Lgroup), αΠ = 10, and αL = 0.1 in our experiments. 3.4 Line Mapping Finalization After the optimization process converges, extracting 3D lines is straightforward, since each line corresponds to an edge of an optimized 3D plane. To ensure the accuracy of the mapping results, we filter inliers by computing both the angular distance dang = Dang(ldet pπ) and the orthogonal distance dorth = Dort(ldet pπ) between the 2D projection π,v and its associated 2D line detection ldet e2d i,j . If dang τα and dorth τd, then the 3D planar edge ev π is considered correct interpretation of the detected 2D line segment ldet i,j and is retained in the final mapping results. We set τd = 1 pixel and τα = 0.01 in our experiments. i,j , e2d i,j , e2d Line Track Builder. After the final 3D line map is obtained, we then build the 3D-2D line correspondences between and all the 2D detection results over image set I. Denoted by (l3d ) = {(v1, ı1), . . . , (vT , ıT )} the tracked 2D line , any detected 2D line segment ldet segments of l3d Lvk should satisfy the following criteria between the 2D projection lvk vk,ık , The angular distance dang = Dang(ldet , lvk ) should be and ldet of l3d vk,ık vk,ık smaller than τa = 0.01; The orthogonal distance ddist between ldet vk,ık and lvk should be smaller than τd = 2 pixels; The overlap ratio doverlap between ldet vk,ık and lvk should be greater than τo = 0.2. We provide the diagrams and mathematical formulas for the computations of dang, ddist, and doverlap in Appendix B. Fig. 7: Illustration of the assignment process from one view. Red: rays. Blue: planar primitives. Black: planar edges. Top left: the association of rays and planar primitives. Top right: the selected planar primitives. Bottom left: the selected planar edges. Bottom right: the optimized planar edges. align well with the detected 2D lines, enabling for reliable and plausible reconstruction. 3.3 Plane-Line Optimization Given an image with 2D line to 3D plane edge assignments SI, our goal is to optimize the assigned 3D line-edge for line mapping. We achieve this by constraining the lineedge in both 3D and 2D space. In 3D space, we have to ensure that these line-edge are located on the scene surface. Note that the line-edge themselves are part of the 3D planes. Thus, the on-surface constraint can be directly implemented by optimizing the 3D planes with the plane rendering loss LΠ supervised by 2.5D monocular depths & normals, formed as: LΠ = αDLDepth + αN LNormal, (11) where αD = 5 and αN = 1. This rendering process has been introduced in PlanarSplatting [8], and we also make an introduction in Appendix A. In 2D space, we have to ensure that the line-edge are aligned with the 2D detected line segments on image I. Specifically, given all 2D line to 3D plane edge assignments SI, the 2D alignment loss can be calculated as: L2d = L2d euc + L2d ort, euc and 2D lines loss L2d ort are (12) and the 2D endpoints loss L2d calculated by: L2d euc = L2d ort = (cid:88) (cid:88) ldet i,j Li (cid:88) pR(ldet i,j ) (cid:88) ldet i,j Li pR(ldet i,j ) Deuc(ldet i,j , e2d pπ), Dort(ldet i,j , e2d pπ), (13) IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 6 TABLE 1: Quantitative results of 3D line mapping on the ScanNetV2 [15] dataset and the ScanNet++ [16] dataset with lines detected by 4 different detectors. All metrics are reported at 5 mm along with the average number of reconstructed lines. ScanNetV2 [15]: 50 scenes Line-Level Metrics Junction-Level Metrics Statistics Detector Method ACC-L COMP-L PREC-L RECAL-L F-SCORE-L ACC-J COMP-J PREC-J RECAL-J F-SCORE-J #Lines LSD [12] HAWPv3 DeepLSD ScaleLSD LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours 0.0748 0.1083 0.0692 0. 0.0791 0.0833 0.0782 0.0735 0.0733 0.0783 0.0748 0.0776 0.0828 0.0829 0.0827 0.0742 0.4094 0.5499 0.3326 0.1260 0.4510 0.4092 0.3955 0.1168 0.3673 0.3469 0.2963 0. 0.3217 0.2782 0.2711 0.1384 0.5912 0.4853 0.5893 0.7069 0.6699 0.5402 0.7027 0.7424 0.6451 0.5097 0.6624 0.7308 0.6413 0.5437 0.6939 0.7422 0.0807 0.0253 0.1333 0. 0.0628 0.0553 0.0895 0.3521 0.1064 0.0804 0.1610 0.3504 0.1294 0.1182 0.1765 0.3399 0.1436 0.0454 0.2228 0.4438 0.1139 0.0960 0.1575 0.4744 0.1814 0.1297 0.2585 0. 0.2155 0.1840 0.2822 0.4430 0.0753 0.1137 0.0702 0.0774 0.0811 0.0853 0.0799 0.0743 0.0736 0.0786 0.0758 0.0781 0.0843 0.0840 0.0840 0.0743 0.4183 0.5540 0.3494 0. 0.4636 0.4285 0.4090 0.1350 0.3816 0.3615 0.3185 0.1388 0.3356 0.2977 0.2831 0.1537 0.5873 0.4904 0.5837 0.7068 0.6547 0.5326 0.6927 0.7398 0.6436 0.5165 0.6569 0. 0.6326 0.5411 0.6863 0.7431 0.0502 0.0161 0.0728 0.2083 0.0293 0.0231 0.0472 0.2156 0.0571 0.0420 0.0798 0.2158 0.0710 0.0589 0.1121 0.2078 0.0927 0.0298 0.1302 0. 0.0555 0.0426 0.0873 0.3301 0.1038 0.0732 0.1407 0.3292 0.1263 0.1004 0.1915 0.3204 328 241 529 2974 172 237 389 2942 397 658 613 499 733 1186 2934 ScanNet++ [16]: 30 scenes Line-Level Metrics Junction-Level Metrics Statistics Detector Method ACC-L COMP-L PREC-L RECAL-L F-SCORE-L ACC-J COMP-J PREC-J RECAL-J F-SCORE-J #Lines LSD HAWPv3 DeepLSD ScaleLSD LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours 0.0718 0.1084 0.0648 0.0759 0.0870 0.1337 0.0783 0.0890 0.0785 0.1145 0.0729 0.0828 0.0825 0.1201 0.0813 0.0801 1.1532 1.5770 0.8684 0. 1.2963 0.5728 1.0479 0.2428 1.0347 0.8208 0.8035 0.2466 1.0197 0.3798 0.8639 0.2520 0.5283 0.4355 0.5308 0.5653 0.4876 0.3699 0.5578 0.5724 0.4114 0.3629 0.5034 0. 0.4507 0.3907 0.5663 0.5889 0.0281 0.0025 0.0689 0.2900 0.0194 0.0343 0.0346 0.2664 0.0416 0.0151 0.0863 0.3056 0.0402 0.0920 0.0708 0.2895 0.0515 0.0050 0.1183 0. 0.0375 0.0617 0.0635 0.3636 0.0771 0.0282 0.1512 0.3996 0.0752 0.1453 0.1272 0.3882 0.0721 0.1038 0.0633 0.0744 0.0878 0.1333 0.0792 0.0906 0.0769 0.1114 0.0715 0. 0.0824 0.1191 0.0812 0.0811 1.1604 1.5790 0.8821 0.2718 1.3043 0.5918 1.0561 0.2568 1.0465 0.8335 0.8220 0.2621 1.0280 0.4039 0.8714 0.2665 0.5396 0.4593 0.5403 0. 0.4902 0.3743 0.5635 0.5761 0.4278 0.3685 0.5179 0.5669 0.4498 0.3965 0.5677 0.5959 0.0141 0.0014 0.0344 0.1432 0.0081 0.0144 0.0186 0.1741 0.0185 0.0071 0.0396 0. 0.0195 0.0445 0.0433 0.2039 0.0266 0.0029 0.0689 0.2279 0.0160 0.0273 0.0360 0.2673 0.0356 0.0137 0.0741 0.3157 0.0376 0.0782 0.0807 0.3038 73 16 218 41 228 141 2691 102 108 280 3948 110 740"
        },
        {
            "title": "4 EXPERIMENTS\nThis section presents experimental results on public datasets\nand ablation studies to demonstrate the effectiveness of our\nmethod and its components. More details and additional\nresults are provided in our Appendix.",
            "content": "TABLE 2: Quantitative results of 3D line mapping on the ScanNetV2 [15] dataset and the ScanNet++ [16] dataset with lines detected by 4 different detectors. Rτ and Pτ are reported at 5 mm, 10 mm, 50 mm along with the average number of supporting images/lines. 4.1 Implementation Details Initialization. We first use the provided depth maps from all views to generate coarse scene mesh. From this mesh, we randomly sampled 2,000 points to serve as the initial centers of our 3D planar primitives. The initial radius of each primitive π is set to half the minimum distance from π to its nearest neighboring primitive. The plane rotation is initialized using the surface normal of the coarse mesh. Optimization. Our LiP-Map is implemented in PyTorch [54] and optimized with Adam [55]. For each scene, we jointly optimize lines and planes from scratch for 60 epochs, with each epoch traversing all available viewpoints once. The learning rates of the learnable parameters, including plane centers, radii, and rotations, are fixed at 0.001. Plane Pruning and Splitting. During optimization, we adopt the plane-splitting operation introduced in PlanarSplatting [8], guided by the gradients of the plane radii to better fit the geometry of the scene. If the average radius gradients along the X-axis (rx+ π ) exceed 0.2, the π plane is split along the Y-axis. Similarly, if the radius gradients along the Y-axis (ry+ π ) are greater than 0.2, the plane is split along the X-axis. Unlike PlanarSplatting, and rx and ry π Detector Method LSD HAWPv DeepLSD ScaleLSD LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours ScanNetV2 [15]: 50 scenes R5 1.04 0.19 1.50 6.23 0.90 0.80 1.63 5.26 1.67 0.96 1.97 4.63 2.08 1.78 4.09 6. R10 R50 6.41 1.24 14.20 40.23 5.61 2.94 14.80 33.12 10.41 6.45 18.81 29.74 12.94 11.49 38.01 42. 31.07 13.97 85.05 240.29 28.25 19.11 80.68 183.61 53.78 46.18 118.22 168.16 64.40 54.33 204.94 224.81 Detector Method R5 R10 R50 ScanNet++ [16]: 30 scenes LSD HAWPv DeepLSD ScaleLSD LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours 2.07 0.07 5.63 12.87 1.33 0.24 4.09 4.33 3.05 0.41 8.37 11.04 2.81 0.61 10.04 14.31 5.69 0.23 16.29 48.64 3.78 0.69 11.86 16. 8.96 1.42 24.13 45.22 7.99 1.93 28.73 43.91 17.27 1.03 49.65 61.20 11.23 3.06 33.80 40.67 28.36 7.11 76.47 59.61 24.36 9.50 80.88 99. P5 25.8 7.3 24.2 27.6 28.5 17.6 28.3 30.9 30.0 10.8 24.5 31.7 31.3 15.0 27.6 30.4 33.8 12.3 36.8 29.1 33.5 18.1 32.4 34.3 38.8 15.4 38.1 39.9 39.8 16.7 40.4 36.5 P10 45.4 14.4 44.6 43. 48.4 27.8 47.7 47.6 48.3 19.2 46.2 47.1 49.9 25.3 50.7 48.6 P10 50.1 18.8 52.4 64.0 58.2 27.1 59.3 72. 53.8 22.6 63.6 73.8 56.6 23.1 67.3 71.7 P50 80.2 54.9 79.6 81.7 82.0 64.7 81.1 84.6 81.3 60.1 81.8 85. 86.2 64.5 85.0 87.4 P50 69.1 51.6 71.5 74.1 78.9 57.1 82.8 81.5 74.7 58.7 79.5 76.6 80.3 59.4 83.1 81. # supports 5.6 / 5.8 4.2 / 4.5 8.6 / 8.8 10.7 / 14.1 7.1 / 12.1 4.4 / 5.8 8.5 / 9.1 6.3 / 8.1 6.4 / 6.8 4.3 / 4.5 9.4 / 9.6 6.2 / 6.4 7.2 / 10.2 4.5 / 4.4 8.4 / 9.1 8.2 / 12.8 # supports 4.7 / 5.0 4.1 / 4.2 5.8 / 6.2 4.9 / 5.4 5.1 / 6.2 4.2 / 4.3 5.8 / 6.4 4.7 / 7.1 5.2 / 5.9 4.0 / 4.2 6.3 / 6.6 4.9 / 5.3 5.3 / 6.1 4.1 / 4.0 5.9 / 6.6 4.6 / 6.7 IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE Fig. 8: Qualitaitve comparison on the ScanNetV2 dataset [15] (left 3 scenes with ScaleLSD [14] detector) and the ScanNet++ dataset [16] (right 3 scenes with DeepLSD [11] detector). First row: LIMAP w/ depth [1]. Second row: LIMAP [1]. Third row: CLMAP [34]. Last row: Ours. which performs pruning and splitting every 1,000 iterations, we apply these operations once per epoch. normal maps. Following LIMAP [1] and CLMAP [34], we take the first ten scenes for evaluation. 4.2 Datasets, Metrics and Baselines Datasets. We test our method on the ScanNetV2 dataset [15], the ScanNet++ dataset [16] and the Hypersim dataset [17]. The ScanNetV2 and the ScanNet++ are two realistic indoor datasets that provide posed videos. Following the test protocol according to PlanarSplatting [8], we randomly sample 50 scenes from the ScanNetV2 dataset and 30 scenes from the ScanNet++ dataset, and sample one image every 10 frames from the video of each scene for evaluation. Similarly to PlanarSplatting [8], we use Metric3Dv2 [53] for depth estimation and Ominidata [52] for normal estimation. The Hypersim is photorealistic synthetic dataset for holistic indoor scene understanding. Each scene of this dataset provides maximum of 100 posed images, depth maps, and Metrics. As there are no ground-truth (GT) 3D lines, we evaluate the 3D line mapping with either GT mesh models or point clouds. To comprehensively evaluate the quality of the reconstructed 3D line maps, we consider two evaluation metrics: one is the widely used standard benchmark for the evaluation of reconstructed 3D models, and the other is the evaluation benchmark proposed by LIMAP [1]. The widely used metrics (M1) adopted in 3D reconstructions [4], [56], [57], including accuracy (ACC), completeness (COMP), precision (PREC), recall (RECAL), and F-SCORE for evaluation. We assess these five metrics at both the junction level (-J) and the line level (-L). For junction-level results, we evaluate the performance of the two endpoints (junctions) of the 3D lines. For line-level results, we uniformly sample 100 points on each line to measure performance. Additionally, we IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 8 TABLE 3: Quantitative results of 3D line mapping on the Hypersim [17] dataset with lines detected by 4 different detectors. All metrics are reported at 5 mm along with the average number of reconstructed lines. Hypersim [17]: First 10 scenes Line-Level Metrics Statistics Detector Method ACC-L COMP-L PREC-L RECAL-L F-SCORE-L #Lines LSD [12] HAWPv3 DeepLSD ScaleLSD LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours 0.0731 0.0088 0.1045 0. 0.0435 0.0097 0.0591 0.0077 0.0861 0.0093 0.1088 0.0076 0.0520 0.0096 0.0664 0.0090 0.5336 0.5800 0.5010 0.3989 0.7133 0.7907 0.6678 0.3531 0.5174 0.7168 0.4745 0. 0.6432 0.5397 0.5602 0.3410 0.9302 0.9969 0.9136 0.9921 0.9030 0.9896 0.9286 0.9923 0.8964 0.9934 0.9044 0.9924 0.8774 0.9901 0.9161 0.9857 0.1890 0.2024 0.2028 0. 0.0912 0.1262 0.1087 0.2785 0.2154 0.2465 0.2330 0.3387 0.1341 0.1794 0.1593 0.2944 0.3085 0.3297 0.3248 0.4424 0.1648 0.2214 0.1935 0.4295 0.3412 0.3868 0.3622 0. 0.2296 0.2982 0.2679 0.4465 886 890 1209 2538 263 238 641 2461 665 786 1105 3355 458 439 1225 2968 TABLE 4: Quantitative results of 3D line mapping on the Hypersim [17] dataset with lines detected by 4 different detectors. Rτ and Pτ are reported at 1 mm, 5 mm, 10 mm along with the average number of supporting images/lines. Hypersim [17]: First 10 scenes Detector Method R1 R5 LSD HAWPv3 DeepLSD ScaleLSD LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours LIMAP [1] LIMAP w/ depth [1] CLMAP [34] Ours 70.5 268.0 112.3 245.3 15.4 82.2 52.7 184.3 65.3 261.8 138.0 396.3 24.5 136.2 107.9 223.6 282.5 432.6 403.0 585. 66.1 182.3 205.0 512.9 244.2 492.8 455.8 886.5 100.6 275.1 391.1 639.6 R10 351.5 434.1 492.7 678.7 95.1 189.9 272.7 594. 304.4 503.1 551.1 1010.5 142.4 287.9 533.9 754.1 P1 65.7 84.0 61.9 81.7 65.0 80.8 63.3 82.8 66.4 84.2 63.8 82. 64.2 79.2 62.9 80.2 P5 84.0 99.9 80.6 94.6 81.0 99.7 83.7 95.7 80.7 99.8 80.7 95.1 80.9 99.7 82.2 95. P10 89.6 100 87.9 98.7 88.4 100 89.6 99.2 85.6 100 87.5 98.7 87.0 100 89.1 98.7 # supports 15.6 / 17.7 16.6 / 22.2 19.7 / 21.6 12.3 / 20.8 12.3 / 23.5 15.8 / 33.0 13.7 / 14.7 9.3 / 18.2 17.2 / 23.5 17.5 / 24.9 20.5 / 21.9 13.1 / 21.9 14.0 / 29.3 15.6 / 35.6 14.7 / 15.9 11.0 / 32.0 Fig. 9: Qualitative comparison on the Hypersim dataset [17] using 3 different line detectors. First row: LIMAP w/ depth [1]. Second row: LIMAP [1]. Third row: CLMAP [34]. Last row: Ours. apply the number of reconstructed 3D line segments as statistical metric for completeness and robustness. LIMAPs evaluation metrics (M2) are built on the line tracking and use the following metrics: Length recall (in meters) at threshold τ (Rτ ) means the sum of the lengths of the line portions within τ mm from the GT model. Inlier percentage at threshold τ (Pτ ) means the percentage of tracks that are within τ mm from the GT model. Average supports means the average number of image supports and 2D line supports across all line tracks. The complete formulation of the evaluation metrics is provided in the Appendix B. Baselines. We use the current state-of-the-art method, LIMAP [1] as the baseline, which achieves 3D line mapping through carefully crafted scoring and tracking, as well as automatically identifying and exploiting structural priors. Since we use depth maps for 3D line mapping and LIMAP provides optimization with depth maps, we compare with depth-based LIMAP in all experiments. We also compare with the recent CLMAP [34], which is built upon LIMAP and is proposed to improve the consistency of the structure of the line map by jointly optimizing points, lines, and planes. Different from our learnable planar primitives, CLMAP statically applies planes detected by RSPD [58] for coplanar constraints of 3D lines. 4.3 3D Line Mapping Detectors. To systematically investigate the impact of detected 2D line segments on the final 3D line mapping, we conduct comprehensive experiments using four distinct line detectors: LSD [12], HAWPv3 [13], DeepLSD [11], and ScaleLSD [14], all within unified quantitative evaluation. ScanNetV2 & ScanNet++. We first evaluate our approach on the ScanNetV2 dataset [15] and the ScanNet++ dataset [16]. We use the corresponding plane mesh of each scene provided by these two datasets for the computation of metrics M1 and M2. The average results of metric M1 are illustrated in Table 1. As can be seen that our method achieves superior balance between accuracy and completeness of reconstructed 3D lines, with consistently lower completeness error and comparable accuracy error while maintaining higher precision, recall, and F1 scores. Matching-based LIMAP and CLMAP are robust to reconstruct more accurate 3D line maps on the whole, and our method tends to reconstruct more complete line structures of the scene with much larger number of 3D lines. Due to the motion blur of images and the not-perfect depth maps estimated from Metric3dv2 [53], LIMAP with depth would fit unreliable 3D lines from the detected 2D lines and get worse reconstruction results finally. But this problem can be greatly alleviated in our method since the planar primitive can average the depth error of each region, so that our IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 9 TABLE 5: Ablation study of our proposed loss functions in metric M1 on the Hypersim dataset [15] with lines detected by LSD [12]. All metrics are reported at 5 mm along with the average number of reconstructed lines. L2d euc L2d ort Lgroup ACC-L COMP-L PREC-L RECAL-L F-SCORE-L #Lines 0.0082 0.0088 0.0083 0.0082 0.0087 0.0088 0.0081 0.0079 0.8447 0.4648 0.4472 0.4689 0.4516 0.4232 0.4566 0.3989 0.9551 0.9421 0.9441 0.9446 0.9585 0.9534 0.9662 0.9921 0.1269 0.1512 0.1619 0.1330 0.2180 0.2011 0.2504 0.2908 0.2240 0.2606 0.2764 0.2332 0.3552 0.3321 0.3977 0. 532 1358 1654 1301 3109 2929 3439 2538 TABLE 6: Ablation study of our proposed loss functions in metric M2 on the Hypersim dataset [15] with lines detected by LSD [12]. Rτ and Pτ are reported at 1 mm, 5 mm, 10 mm along with the average number of supporting images/lines. L2d euc L2d ort Lgroup 11.7 31.5 39.5 29.7 143.7 135.3 115.2 245.3 R5 32.8 99.4 117.6 97.2 476.7 468.0 335.5 585.6 54.1 121.1 141.8 118.5 569.2 566.2 389.4 678.7 P1 81.0 81.1 81.4 80.9 81.5 81.7 81.3 81.7 P5 92.6 92.0 92.5 91.9 93.0 93.2 94.0 94.6 98.6 97.5 97.9 98.2 98.4 98.1 97.9 98.7 # supports 5.3 / 6.2 10.0 / 11.3 10.4 / 11.8 10.2 / 11.4 10.8 / 13.1 10.9 / 13.1 11.3 / 13.3 12.3 / 20.8 TABLE 7: Ablation study of the proposed 2D line-3D edge assignment in metric M1 on the Hypersim dataset [15] with lines detected by LSD [12]. All metrics are reported at 5 mm along with the average number of reconstructed lines. theta(θ) dist(d) ACC-L COMP-L PREC-L RECAL-L F-SCORE-L #Lines 0.0082 0.0087 0.0082 0.0079 0.8447 0.4051 0.3542 0.3989 0.9551 0.9879 0.9900 0.9921 0.1269 0.2461 0.2776 0. 0.2240 0.3866 0.4260 0.4424 532 2125 2561 2538 TABLE 8: Ablation study of the proposed 2D line-3D edge assignment in metric M2 on the Hypersim dataset [15] with lines detected by LSD [12]. Rτ and Pτ are reported at 1 mm, 5 mm, 10 mm along with the average number of supporting images/lines. theta(θ) dist(d) 11.7 138.9 177.4 245.3 R5 32.8 351.2 421.6 585. R10 54.1 418.7 492.9 678.7 P1 81.0 76.2 76.9 81.7 P5 92.6 92.2 92.0 94. P10 98.6 97.4 96.9 98.7 # supports 5.3 / 6.2 10.8 / 13.3 11.5 / 14.1 12.3 / 20.8 tracks than LIMAP and CLMAP. The qualitative comparison of 3D line mapping is shown in Fig. 9. We take reconstructed line maps of 3 scenes with 3 different line detectors for illustration. Our method reconstructs clearer and more complete line structures than other methods. LIMAP with depth also outperforms LIMAP and CLMAP. But LIMAP with depth still has the risk of reconstructing unreasonable lines because it would fit unreliable 3D lines from detected 2D lines with incorrect depth maps, as indicated by the red square in Fig. 9. This kind of case will increase the reconstruction error of LIMAP with depth. 4.4 Ablation Studies 4.4.1 The Design of Hybrid Losses We primarily conduct our ablation experiments on the first ten scenes of the Hypersim dataset [17] to validate the Fig. 10: Illustration on the ablation of our proposed 2D line3D edge assignment. First row: w/o θ & d. Second row: only w/ θ. Third row: only w/ d. Last row: w/ θ & d. method achieves lower accuracy errors than LIMAP with depth. The average results of metric M2 are reported in Table 2. The length recall of our method is significantly higher than that of other methods, albeit with competitive precision scores. LIMAP and CLMAP are better at building long line tracks and can track more image supports and line supports than our method. The qualitative comparison of 3D line mapping is shown in Fig. 8. The scene name, number of used images, and the used line detector are tagged under each column. Our method excels not only in capturing fine structures but also in reconstructing more comprehensive line map overall. LIMAP and CLMAP can basically reconstruct the structure of the whole scene on the ScanNetV2 dataset while using dense views with over 100 images, but they failed to reconstruct the complete structures of scenes on the ScanNet++ dataset with relatively sparse views. Hypersim. We further present comparisons of all methods on the first ten scenes of the Hypersim dataset [17]. Following LIMAP [1], we use the point cloud recovered from the depth maps of each scene for the computation of the metrics M1 and M2. The average results of metric M1 are reported in Table 3. As illustrated, our method and LIMAP with depth outperform LIMAP and CLMAP in terms of much lower accuracy error and higher precision, since Hypersim provides accurate depth maps. Additionally, our method significantly outperforms other methods in terms of lower completeness error and higher recall and F1 scores. Our method can reconstruct more complete structures with more 3D lines. The average results of metric M2 are reported in Table 4. Our method also achieves higher length recall than other methods, and slightly lower precision than LIMAP with depth, while being less capable of building long line IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 10 TABLE 9: Quantitative results of reconstruction with different plane splitting thresholds on Hypersim [17]. Splitting Line-Level Metric Line-Track Metric M2 Threshold ACC-L COMP-L PREC-L RECAL-L F-SCORE-L #Lines R1 R5 R10 0.02 0.1 0.2 1.0 2.0 10.0 0.0083 0.0079 0.0076 0.0104 0.0118 0. 0.5031 0.5223 0.5269 0.6757 0.5753 0.6148 0.9899 0.9862 0.9924 0.9284 0.9127 0.8907 0.3372 0.3214 0.3387 0.2189 0.1919 0.1740 0.4933 0.4766 0.4963 0.3442 0.3066 0.2775 8232 4053 3355 1145 888 744 549.0 413.4 396.3 69.9 51.2 40. 1336.0 987.9 886.5 190.9 147.1 114.4 2033.3 1364.1 1010.5 223.2 173.2 137.8 P1 82.0 78.8 82.2 68.7 67.1 66.3 P5 94.1 91.5 95.1 82.7 81.5 81. P10 97.4 95.7 98.7 88.6 87.9 86.9 # supports 10.5 / 11.9 11.9 / 16.5 13.1 / 21.9 11.7 / 13.5 11.6 / 13.4 11.4 / 13.1 TABLE 10: Quantitative results of reconstruction with different numbers of initial planes on Hypersim [17]. Initial Line-Level Metric M1 Line-Track Metric M2 Number ACC-L COMP-L PREC-L RECAL-L F-SCORE-L #Lines R1 R5 R10 200 2000 5000 10000 20000 40000 0.0098 0.0076 0.0074 0.0072 0.0073 0.0072 0.0071 0.9547 0.5269 0.5685 0.5598 0.5471 0.5331 0.5321 0.9284 0.9924 0.9738 0.9746 0.9743 0.9948 0.9956 0.2323 0.3387 0.3085 0.3169 0.3232 0.3401 0.3471 0.3571 0.4963 0.4604 0.4701 0.4776 0.4988 0.5068 2265 3355 3337 3312 3614 4016 223.3 396.3 368.2 343.5 377.6 420.8 417.7 622.8 886.5 873.2 791.8 802.3 954.1 978.5 876.0 1010.5 940.1 959.2 969.4 1213.6 1343.0 P1 80.6 82.2 81.6 82.6 82.0 83.3 82.6 93.2 95.1 94.9 94.2 94.7 95.7 95.2 P10 95.5 98.7 98.1 97.5 98.3 98.9 98.9 # supports 12.4 / 18.7 13.1 / 21.9 12.9 / 21.6 12.3 / 20.5 11.9 / 19.7 12.8 / 18.4 12.6 / 18.2 TABLE 11: Quantitative results of reconstruction with different pairs of loss weights on Hypersim [17]. Loss weights Line-Level Metric M1 Line-Track Metric M2 αΠ 0.1 1 10 10 10 αL 0.1 0.1 0.1 1 10 ACC-L COMP-L PREC-L RECAL-L F-SCORE-L #Lines R1 R5 R10 0.0146 0.0127 0.0076 0.0098 0.0078 0.5882 0.5188 0.5269 0.5421 0. 0.9428 0.9576 0.9924 0.9784 0.9901 0.2453 0.2098 0.3387 0.2791 0.3415 0.3804 0.3282 0.4963 0.4239 0.5003 1348 842 3355 1876 6141 74.3 51.9 396.3 109.7 678.6 155.0 106.3 886.5 218.8 1487. 177.3 121.4 1010.5 246.6 1842.8 P1 86.7 88.4 82.2 87.6 84.2 P5 96.5 96.9 95.1 96.9 96.2 98.5 98.9 98.7 99.1 98.6 # supports 10.4 / 12.2 10.7 / 12.2 13.1 / 21.9 11.2 / 13.0 10.2 / 15.8 effectiveness of each design of our hybrid losses through controlled comparisons. The choosing checkmark denotes the use of the designed loss, and the case in which no loss terms were employed indicates the use of the baseline method of PlanarSplatting [8]. Comparative results under evaluation metric M1 are reported in Table 5. As can be observed, our loss design facilitates the reconstruction of greater number of 3D line segments, significantly reducing the overall completeness error. The improvement in completeness is directly reflected in higher recall and F1 scores. Since our method builds upon the baseline of PlanarSplating and employs depth maps to supervise the positions of planes and lines, it achieves comparable performance in terms of reconstruction error and precision. Nevertheless, our approach demonstrates superior accuracy in 3D line mapping compared to the baseline. We also report comparative results under evaluation metric M2 in Table 6. Consistent with the conclusions in Table 5, all ablation experiments achieve comparable reconstruction precision. However, compared to the original baseline of PlanarSplatting, each proposed loss component significantly improves the length recall of the built line tracks and enables tracking of longer image and line segment trajectories. Furthermore, combining different loss functions leads to further improvements in length recall and increases the number of supported line segments. The above ablation results validate the effectiveness of the proposed design of hybrid losses. 4.4.2 The Design of 2D Line to 3D Plane Edge Assignment In the Sec. 3.2, our assignment strategy adopts both the angle between the projected plane edge and the detected line segment and the perpendicular distance from the endpoints of the projected plane edge to the detected line as two complementary metrics to identify the optimal correspondence. Here, we conduct ablation studies considering only the angle or only the perpendicular endpoint distance, respectively, to verify the necessity of combining both strategies. Not any checkmark chosen denotes the use of the original PlanarSplatting [8]. We only apply these different strategies during the process of per-scene optimization, and the standard step of 3D line assignment (as described in Sec. 3.4) is finally adopted. The quantitative results of metric M1 are reported in Table 7 and the quantitative results of metric M2 are reported in Table 8. As reported in the results, each individual assignment strategy enhances both the accuracy and completeness of the final 3D line reconstruction, while promoting the formation of longer line tracks. The joint strategy, which integrates both angular consistency and geometric proximity, yields the optimal performance. Qualitative comparisons are provided in Fig. 10. As can be seen, constructing correspondences using only angle or only distance each improves the quality of 3D line reconstruction. Our strategy searches for 2D line and 3D edge correspondences by considering both angular and spatial distance relationships, yielding the most complete and accurate 3D line maps. 4.4.3 The Sensitivity to Heuristics and Hyperparameters We conducted an ablation study on several key hyperparameters and heuristics used in our method on the standard Hypersim dataset [17]. Both metrics of M1 and M2 are reported in all results. The quantitative results about the impact of different plane splitting thresholds on the final line mapping are reported in Table 9. It can be seen that the threshold for plane splitting directly affects the final number of line segments, and that extra-large or oversmall thresholds both degrade reconstruction quality. The quantitative results about the impact of the number of initial planes on the final line mapping are reported in IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE TABLE 12: Per-scene results of visual localization on 7Scenes [18]. We report the median translation and rotation error in cm and degrees, as well as the pose accuracy at 5 cm / 5 deg threshold. Matching-based Methods Learning-based Methods Scene Hlocpoint [59] PtLinepoint & line [60] Limappoint & line [1] Ourspoint & line Pl2Mappoint [61] Pl2Mappoint & line [61] Ourspoint & line Chess Fire Heads Office Pumpkin RedKitchen Stairs 2.4 / 0.84 / 93.0 2.3 / 0.89 / 88.9 1.1 / 0.75 / 95.9 3.1 / 0.91 / 77.0 5.0 / 1.32 / 50.4 4.2 / 1.39 / 58.9 5.2 / 1.46 / 46.8 2.4 / 0.85 / 92.7 2.3 / 0.91 / 87.9 1.2 / 0.81 / 95.2 3.2 / 0.96 / 74.5 5.1 / 1.35 / 49.0 4.3 / 1.42 / 58.0 4.8 / 1.33 / 51.9 2.5 / 0.85 / 92.3 2.1 / 0.84 / 95.5 1.1 / 0.76 / 95.9 3.0 / 0.89 / 78.4 4.7 / 1.23 / 52.9 4.1 / 1.39 / 60.2 3.7 / 1.02 / 71.1 2.3 / 0.82 / 93.2 2.0 / 0.83 / 93.4 1.0 / 0.74 / 95.4 3.0 / 0.90 / 78.6 4.7 / 1.24 / 52.7 4.1 / 1.36 / 60.7 3.9 / 0.97 / 65.9 2.0 / 0.65 / 95.5 2.0 / 0.81 / 93.3 1.2 / 0.74 / 97.8 2.8 / 0.78 / 82.3 3.5 / 0.96 / 63.1 3.8 / 1.13 / 66.7 8.5 / 2.4 / 27.8 1.9 / 0.63 / 96.0 1.9 / 0.80 / 94.0 1.1 / 0.71 / 98.2 2.7 / 0.74 / 84.3 3.4 / 0.93 / 64.1 3.7 / 1.10 / 68.9 7.6 / 2.0 / 33. 1.9 / 0.63 / 96.2 1.9 / 0.80 / 93.8 1.1 / 0.71 / 98.5 2.7 / 0.73 / 83.9 3.5 / 0.92 / 64.3 3.7 / 1.10 / 68.5 7.4 / 2.1 / 34.1 localization on Stairs from Fig. 11: Line-assisted visual 7Scenes [18] based on Hloc [59]. Blue: Detected 2D lines. Red: Projected 2D lines from ours 3D line map. Left: Projection by point-based estimated pose. Right: Projection by our line-assisted estimated pose. localization on Stairs from Fig. 12: Line-assisted visual 7Scenes [18] based on Pl2Map [61]. Blue: Detected 2D lines. Red: Projected 2D lines from ours 3D line map. Left: Projection by point-based estimated pose. Right: Projection by our line-assisted estimated pose. Table 10. It can be seen that too few initial planes have significant impact on the final number of line segments and the reconstruction quality. The quantitative results about the impact of different settings of loss weights on the final line mapping are reported in Table 11. It can be seen that both loss weights affect the final number of line segments, but reconstruction quality is influenced more strongly by the plane loss weight. 4.5 Line-Assisted Visual Localization Omnidata [52] model for our 3D reconstruction. The input to our visual localization pipeline is set of 2D-3D point correspondences (from point-based methods, e.g., the matching-based SfM model or the learning-based model) and 2D-3D line correspondences (from our LiP-Map). For the matching-based SfM model, we utilize the 2D-3D point correspondences processed by the point triangulator from COLMAP [21] with SuperPoint [62] from HLoc [59]. For the learning-based model, we employ the pretrained model from Pl2Map [61] to obtain 2D3D point correspondences. In this section, we present our experimental results for line-assisted visual localization using both point and line features on the 7Scenes dataset [18]. The classic LSD [12] detector is adopted to detect 2D lines for 3D line mapping. We use the ground-truth depths and poses provided by the dataset and normal maps predicted by the pretrained For query images with unknown camera poses, we first use PoseLib [63] to estimate initial poses from 2D-3D point correspondences. Then, we query each image with the initial pose to generate 2D-3D line correspondences from our reconstructed 3D line map (as described in Sec. 3.2 and Sec. 3.4). Finally, we use PoseLib [63] to estimate refined IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE TABLE 13: The analysis of time and numbers of planes on the Hypersim dataset [17]. All results of runtime are reported in seconds (s). Hypersim Plane Time(/iter) Line Time(/iter) Summary(60 epochs) Dataset Rasterizer Loss Total Assignment Loss Total Average Total Init. #Planes Final #Planes ai 001 001 ai 001 002 ai 001 003 ai 001 004 ai 001 005 ai 001 006 ai 001 007 ai 001 008 ai 001 009 ai 001 010 Average 0.0036 0.0036 0.0032 0.0041 0.0034 0.0040 0.0038 0.0036 0.0037 0.0033 0.0036 0.0029 0.0029 0.0028 0.0036 0.0031 0.0035 0.0032 0.0033 0.0029 0.0034 0.0032 0.0067 0.0066 0.0061 0.0078 0.0067 0.0076 0.0072 0.0070 0.0067 0.0068 0.0069 0.0221 0.0206 0.0192 0.0263 0.0223 0.0230 0.0224 0.0211 0.0214 0.0222 0. 0.0009 0.0010 0.0009 0.0010 0.0009 0.0010 0.0010 0.0010 0.0010 0.0009 0.0010 0.0241 0.0226 0.0211 0.0285 0.0242 0.0252 0.0245 0.0231 0.0235 0.0240 0.0241 5.3799 5.1115 4.9280 6.2292 5.7705 5.9670 5.6613 5.3834 5.6190 5.4591 5.5509 322.7921 306.6906 295.6779 373.7514 346.2297 358.0211 339.6774 323.0037 337.1397 327.5479 333.0532 2000 1955 1640 1897 1834 1767 1954 1794 1946 1943 1873 12900 7448 6337 19704 13549 6384 6888 6608 7105 13718 Fig. 13: The analysis of time and numbers of planes on the Hypersim dataset [17]. We report how the number of planes and the optimization time evolve over optimization epochs. poses via both 2D-3D point correspondences and 2D-3D line correspondences. The quantitative results for each scene are summarized in Table 12. We report the median translation error (in cm), the median rotation error (in degrees), and the pose accuracy (defined as the percentage of poses with errors below 5 cm / 5 degrees) of the estimated poses over all test images of each scene. As shown, the inclusion of our reconstructed 3D line map substantially reduces pose estimation error and improves the accuracy of point-based methods. Compared with other line-assisted point-and-line joint pose estimation methods, our LiP-Map also achieves highly competitive performance. To clearly demonstrate the improvement provided by our reconstructed 3D line map for visual localization, we present additional details for the most challenging Stairs scene from 7Scenes [18] in Fig. 11 and Fig. 12. We first use the ground-truth (GT) pose to retrieve the corresponding set of 3D lines from our reconstructed model, and then respectively project these 3D lines onto the image plane using the pose estimated by the point-based method (left panel) and the line-assisted pose (right panel). The pose estimated by the point-based method often projects 3D lines to displaced locations in the image, while the pose refined with our line-assisted pipeline achieves significantly more accurate projections, resulting in better visual consistency with the observed scene structures. 4.6 Analysis of Efficiency and Robustness TABLE 14: Optimization time and number of planes versus the number of input views. 12 Input Views 2 5 10 40 60 80 100 Iterations Time(s) Init. #Planes Final. #Planes 1000 32.6058 1982 1000 35.3922 1947 3065 2000 101.3842 1929 4571 2000 104.9645 1925 6102 3000 165.1081 1865 7622 3000 162.7495 1890 7747 4000 221.6956 1874 5000 275.3702 1875 8676 TABLE 15: Optimization time and number of planes versus the initial number of planar primitives. Numbers 200 2000 10000 20000 40000 100000 Iterations Time(s) Init. #Planes Final. #Planes 6000 331.9121 184 6000 333.0532 1873 10064 6000 345.8118 4811 11677 6000 346.6410 9676 12579 6000 356.7424 19315 14810 6000 391.7404 38399 18940 6000 443.6078 95517 of optimization and the per-epoch runtime for each scene in Table 13. The reported Init. #Planes denotes the number of planar primitives remaining after the initial pruning step, which removes planes that are not visible under any viewpoints. The reported Final. #Planes denotes the number of planar primitives after the optimization converges. The number of planes and the runtime at every epoch for each scene are reported in Fig. 13. Scalability with the number of input views. We first investigate how the optimization time and the number of planes scale with the number of input views in Table 14. It should be noted that when the input views are too sparse (e.g., only 2 views), 60 epochs (120 iterations) are not sufficient for reconstruction, so we run more iterations (e.g., 1000) to achieve high-quality results. In practice, the total runtime is dominated by the number of optimization iterations and is partly influenced by the number of planar primitives. The qualitative comparison of results is illustrated in Fig. 14, taking the ai 001 001 scene as an example. It can be seen that our method is robust to the reconstruction across different numbers of input views. Scalability with the number of planes. We also investigate how the optimization time and the number of planes scale with the initial number of planar primitives. As reported in Table 15, the optimization time does not increase sharply even when the number of initial planes rises substantially, and the final number of planes after convergence remains within relatively stable range. The qualitative comparison of results are visualized in Fig. 15, taking the ai 001 007 scene as an example. These results indicate that our method is robust to the number of planes used for initialization. Notably, although initializing with 200 planar primitives yields poor initial planar mesh at the beginning, the optimization can still converge to relatively complete reconstruction in the end. In this section, we analyze the efficiency and robustness of our optimization framework on the first ten scenes of Hypersim [17], using the DeepLSD [11] detector, on single NVIDIA RTX A6000 with 48GB of memory. Details of per-scene optimization. We first fix the number of initial planar primitives to 2000 for each scene, use 100 input views, and optimize for 60 epochs. We provide detailed breakdown of the average runtime cost of the two core modules during each iteration and report the total runtime 4.7 Generalization to Non-Planar Structures To further investigate the generalization ability of our method on generic non-planar structures, we conducted experiments using only multi-view images from scenes in some common datasets, including DTU [64], Tanks and Temples [19], BlenderMVS [65], and MipNerf360 [66]. To explore this capability as thoroughly as possible, we use noisy poses, depth maps, and normal maps predicted by VGGT for optimization. As shown in Fig. 16, these scenes IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 13 Fig. 14: Qualitative comparison of results with different numbers of input views. First row: initial mesh. Second row: initial planar mesh. Third row: final planar mesh. Last row: final 3D lines. Fig. 15: Qualitative comparison of results with different numbers of initial planar primitives. First row: initial planar mesh. Second row: final planar mesh. Last row: final 3D lines. include non-planar and irregular structures such as curved chairs, circular tables, trees, slender fence railings, street lamps, sculptures, and other objects with complex geometry. But our method can largely meet the reconstruction needs of these scenes by organizing and optimizing the topological relationships among large number of planes. Although some planes in the reconstructed planar surface may be inaccurate, the strict thresholds in our assignment strategy (as described in Sec. 3.4) enforce strong geometric consistency between 2D and 3D lines, allowing these inaccurate plane edges to be filtered out. 4.8 More Results with VGGT We additionally conduct visual experiments on the 7Scenes dataset [18] and the Tanks and Temples dataset [19]. Different from using the ground-truth poses and depth maps provided by the dataset, we apply the recent VGGT [20] to regress poses, depth maps, and normal maps for optimization. This is more challenging setting to reconstruct 3D lines with imperfect poses and depth maps. Since all hyperparameters in our optimization process are scale-invariant, no modifications are required when integrating VGGT, as the overall framework remains compatible under scale variations. We present visual comparison of reconstruction results with Pl2Map [61] in Fig. 17. Our method can also achieve more complete and accurate 3D line maps, even with outputs from VGGT. We also show some additional results of our 3D line maps on the 7Scenes dataset [18] and the Tanks and Temples dataset [19] in Fig. 18. IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 14 Fig. 16: Qualitative results of reconstructed planar mesh and 3D line maps on non-planar structural scenes. sketches of depth and normal representations from posed multi-view images to optimize the geometric synergy between 3D planes and 3D line segments. Through an online optimization process, we simultaneously achieve highperformance 3D line mapping alongside 3D plane reconstruction with prominent topological structure. Our proposed LiP-Map produces accurate, complete, and detailed 3D line mapping, seamlessly integrating with any 2D line segment detector, including both classical (e.g., LSD) and learning-based approaches. By bypassing the challenging 2D line segment matching step, our method significantly outperforms state-of-the-art approaches on both real-world and synthetic indoor scene benchmarks. We hope and believe that our structured 3D reconstruction approach will contribute to compact spatial representations, shaping the next era of spatial intelligence in the near future."
        },
        {
            "title": "REFERENCES",
            "content": "[1] S. Liu, Y. Yu, R. Pautrat, M. Pollefeys, and V. Larsson, 3d line mapping revisited, in IEEE Conf. Comput. Vis. Pattern Recog. IEEE, 2023, pp. 21 44521 455. 1, 2, 6, 7, 8, 9, 11, 20 [2] L. Li, S. Peng, Z. Yu, S. Liu, R. Pautrat, X. Yin, and M. Pollefeys, 3d neural edge reconstruction, in IEEE Conf. Comput. Vis. Pattern Recog. IEEE, 2024, pp. 21 21921 229. 1, 2 [3] M. Hofer, M. Maurer, and H. Bischof, Efficient 3d scene abstraction using line segments, Comput. Vis. Image Underst., vol. 157, pp. 167178, 2017. 1, 2 [4] N. Xue, B. Tan, Y. Xiao, L. Dong, G. Xia, T. Wu, and Y. Shen, NEAT: distilling 3d wireframes from neural attraction fields, in IEEE Conf. Comput. Vis. Pattern Recog. IEEE, 2024, pp. 19 968 19 977. 1, 2, [5] Z. Yu, J. Zheng, D. Lian, Z. Zhou, and S. Gao, Single-image piecewise planar 3d reconstruction via associative embedding, in IEEE Conf. Comput. Vis. Pattern Recog., 2019, pp. 10291037. 1, 2 [6] B. Tan, N. Xue, S. Bai, T. Wu, and G. Xia, Planetr: Structure-guided transformers for 3d plane recovery, in Int. Conf. Comput. Vis., 2021, pp. 41664175. 1, 2 Fig. 17: Qualitative comparison with Pl2Map [61] on 7scenes [18]. Top: the ground-truth mesh. Middle: Pl2Map with ground-truth. Bottom: Our results with VGGT."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we present LiP-Map, line-plane joint optimization framework for 3D line mapping. Our method leverages 2D primal sketches of line segments and 2.5D IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 15 [12] R. G. von Gioi, J. Jakubowicz, J. Morel, and G. Randall, LSD: fast line segment detector with false detection control, IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 4, pp. 722732, 2010. 1, 3, 6, 8, 9, 11 [13] N. Xue, T. Wu, S. Bai, F. Wang, G. Xia, L. Zhang, and P. H. S. Torr, Holistically-attracted wireframe parsing: From supervised to self-supervised learning, IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 12, pp. 14 72714 744, 2023. 1, 2, 3, 4, 8 [14] Z. Ke, B. Tan, X. Zheng, Y. Shen, T. Wu, and N. Xue, Scalelsd: Scalable deep line segment detection streamlined, in IEEE Conf. Comput. Vis. Pattern Recog., June 2025, pp. 63276336. 1, 2, 3, 7, [15] A. Dai, A. X. Chang, M. Savva, M. Halber, T. A. Funkhouser, and M. Nießner, Scannet: Richly-annotated 3d reconstructions of indoor scenes, in IEEE Conf. Comput. Vis. Pattern Recog., 2017, pp. 24322443. 2, 3, 6, 7, 8, 9, 21, 22, 23, 24, 30, 31 [16] C. Yeshwanth, Y. Liu, M. Nießner, and A. Dai, Scannet++: highfidelity dataset of 3d indoor scenes, in Int. Conf. Comput. Vis., 2023, pp. 1222. 2, 6, 7, 8, 22, 25 [17] M. Roberts, J. Ramapuram, A. Ranjan, A. Kumar, M. A. Bautista, N. Paczan, R. Webb, and J. M. Susskind, Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding, in Int. Conf. Comput. Vis., 2021. 2, 4, 7, 8, 9, 10, 12, 20, 21, 22, 23, 26, 32, 33, 34, 35 [18] J. Shotton, B. Glocker, C. Zach, S. Izadi, A. Criminisi, and A. Fitzgibbon, Scene coordinate regression forests for camera relocalization in rgb-d images, in IEEE Conf. Comput. Vis. Pattern Recog., 2013, pp. 29302937. 2, 11, 12, 13, 14, 15 [19] A. Knapitsch, J. Park, Q.-Y. Zhou, and V. Koltun, Tanks and temples: benchmarking large-scale scene reconstruction, ACM Trans. Graph., vol. 36, no. 4, Jul. 2017. 2, 12, 13, 15 [20] J. Wang, M. Chen, N. Karaev, A. Vedaldi, C. Rupprecht, and D. Novotny, Vggt: Visual geometry grounded transformer, in IEEE Conf. Comput. Vis. Pattern Recog., 2025. 2, 13, [21] J. L. Sch onberger and J. Frahm, Structure-from-motion revisited, IEEE Computer Society, in IEEE Conf. Comput. Vis. Pattern Recog. 2016, pp. 41044113. 2, 11 [22] D. Chekhlov, A. P. Gee, A. Calway, and W. W. Mayol-Cuevas, Ninja on plane: Automatic discovery of physical planes for augmented reality using visual SLAM, in IEEE/ACM International Symposium on Mixed and Augmented Reality, 2007, pp. 153156. 2 [23] S. Wang, V. Leroy, Y. Cabon, B. Chidlovskii, and J. Revaud, Dust3r: Geometric 3d vision made easy, in IEEE Conf. Comput. Vis. Pattern Recog., 2024, pp. 20 69720 709. 2 [24] Z. Xue, Z. Yuan, J. Wang, X. Wang, Y. Gao, and H. Xu, USEEK: unsupervised se(3)-equivariant 3d keypoints for generalizable manipulation, in Int. Conf. on Robot. and Auto., 2023, pp. 17151722. 2 [25] D. G. Lowe, Distinctive image features from scale-invariant keypoints, Int. J. Comput. Vis., vol. 60, no. 2, pp. 91110, 2004. 2 [26] J. Morel and G. Yu, ASIFT: new framework for fully affine invariant image comparison, SIAM J. Imaging Sci., vol. 2, no. 2, pp. 438469, 2009. [27] P. Sarlin, D. DeTone, T. Malisiewicz, and A. Rabinovich, Superglue: Learning feature matching with graph neural networks, in IEEE Conf. Comput. Vis. Pattern Recog., 2020, pp. 49374946. 2 [28] H. Bay, A. Ess, T. Tuytelaars, and L. V. Gool, Speeded-up robust features (SURF), Comput. Vis. Image Underst., vol. 110, no. 3, pp. 346359, 2008. 2 [29] N. Xue, T. Wu, S. Bai, F. Wang, G. Xia, L. Zhang, and P. H. S. Torr, Holistically-attracted wireframe parsing, in IEEE Conf. Comput. Vis. Pattern Recog., 2020, pp. 27852794. 2 [30] R. Pautrat, J. Lin, V. Larsson, M. R. Oswald, and M. Pollefeys, SOLD2: self-supervised occlusion-aware line description and detection, in IEEE Conf. Comput. Vis. Pattern Recog., 2021, pp. 11 368 11 378. 2 [31] S. Huang, F. Qin, P. Xiong, N. Ding, Y. He, and X. Liu, TP-LSD: tri-points based line segment detector, in Eur. Conf. Comput. Vis., vol. 12372, 2020, pp. 770785. 2 [32] H. Li, K. Chen, J. Zhao, J. Wang, P. Kim, Z. Liu, and Y. Liu, Learning to identify correct 2d-2d line correspondences on sphere, in IEEE Conf. Comput. Vis. Pattern Recog., 2021, pp. 11 74311 752. 2 [33] D. Wei, Y. Wan, Y. Zhang, X. Liu, B. Zhang, and X. Wang, ELSR: efficient line segment reconstruction with planes and points guidance, in IEEE Conf. Comput. Vis. Pattern Recog., 2022, pp. 15 786 15 794. [34] X. Bai, H. Cui, and S. Shen, Consistent 3d line mapping, in European Conference on Computer Vision (ECCV), 2024. 2, 6, 7, 8 Fig. 18: Qualitative results of our 3D line maps on Tanks and Temples [19] and 7scene [18]. Barn, Truck, Meetingroom and Temple are from the Tanks and Temples dataset. Office, Redkitchen, Chess and Heads are from the 7scenes dataset. [7] W. Ma, B. Tan, N. Xue, T. Wu, X. Zheng, and G. Xia, How-3d: Holistic 3d wireframe perception from single image, in Int. Conf. 3D Vis., 2022, pp. 596605. 1 [8] B. Tan, R. Yu, Y. Shen, and N. Xue, Planarsplatting: Accurate planar surface reconstruction in 3 minutes, in IEEE Conf. Comput. Vis. Pattern Recog., 2025. 1, 2, 3, 5, 6, 7, 10, 21, 22 [9] L. Zhang and R. Koch, An efficient and robust line segment matching approach based on LBD descriptor and pairwise geometric consistency, J. Vis. Commun. Image Represent., vol. 24, no. 7, pp. 794805, 2013. 1 [10] R. Pautrat, I. Suarez, Y. Yu, M. Pollefeys, and V. Larsson, GlueStick: Robust image matching by sticking points and lines together, in Int. Conf. Comput. Vis., 2023, pp. 97069716. 1, 2 [11] R. Pautrat, D. Barath, V. Larsson, M. R. Oswald, and M. Pollefeys, Deeplsd: Line segment detection and refinement with deep image gradients, in IEEE Conf. Comput. Vis. Pattern Recog. IEEE, 2023, pp. 17 32717 336. [Online]. Available: https://doi.org/10.1109/CVPR52729.2023.01662 1, 2, 3, 4, 7, 8, IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 16 [35] H. Pfister, M. Zwicker, J. van Baar, and M. H. Gross, Surfels: surface elements as rendering primitives, in ACM SIGGRAPH, 2000, pp. 335342. 2 [36] Y. Wang, F. Serena, S. Wu, C. Oztireli, and O. Sorkine-Hornung, Differentiable surface splatting for point-based geometry processing, ACM Trans. Graph., vol. 38, no. 6, pp. 230:1230:14, 2019. 2 [37] B. Huang, Z. Yu, A. Chen, A. Geiger, and S. Gao, 2d gaussian splatting for geometrically accurate radiance fields, in ACM SIGGRAPH Conference Papers, A. Burbano, D. Zorin, and W. Jarosz, Eds., 2024, p. 32. 2 [38] B. Tan, N. Xue, T. Wu, and G. Xia, NOPE-SAC: neural one-plane RANSAC for sparse-view planar 3d reconstruction, IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 12, pp. 15 23315 248, 2023. 2 [39] C. Liu, K. Kim, J. Gu, Y. Furukawa, and J. Kautz, Planercnn: 3d plane detection and reconstruction from single image, in IEEE Conf. Comput. Vis. Pattern Recog., 2019, pp. 44504459. [40] J. Shi, S. Zhi, and K. Xu, Planerectr: Unified query learning for 3d plane recovery from single view, in Int. Conf. Comput. Vis., 2023, pp. 93439352. 2 [41] L. Jin, S. Qian, A. Owens, and D. F. Fouhey, Planar surface reconstruction from sparse views, in Int. Conf. Comput. Vis., 2021, pp. 12 97112 980. 2 [42] S. Agarwala, L. Jin, C. Rockwell, and D. F. Fouhey, Planeformers: From sparse view planes to 3d reconstruction, in Eur. Conf. Comput. Vis., vol. 13663, 2022, pp. 192209. 2 [43] J. Liu, P. Ji, N. Bansal, C. Cai, Q. Yan, X. Huang, and Y. Xu, Planemvs: 3d plane reconstruction from multi-view stereo, in IEEE Conf. Comput. Vis. Pattern Recog., 2022, pp. 86558665. 2 [44] T. Shen, J. Gao, K. Yin, M. Liu, and S. Fidler, Deep marching tetrahedra: hybrid representation for high-resolution 3d shape synthesis, in Adv. Neural Inform. Process. Syst., 2021, pp. 6087 6101. 2 [45] Z. Chen, A. Tagliasacchi, and H. Zhang, Bsp-net: Generating compact meshes via binary space partitioning, in IEEE Conf. Comput. Vis. Pattern Recog., 2020, pp. 4251. 2 [46] Z. Chen, A. Tagliasacchi, T. A. Funkhouser, and H. Zhang, Neural dual contouring, ACM Trans. Graph., vol. 41, no. 4, pp. 104:1 104:13, 2022. [47] T. Monnier, J. Austin, A. Kanazawa, A. A. Efros, and M. Aubry, Differentiable blocks world: Qualitative 3d decomposition by rendering primitives, in Adv. Neural Inform. Process. Syst., 2023. 2 [48] C. Gao, X. Wang, Q. Yu, L. Sheng, J. Zhang, X. Han, Y.-Z. Song, and D. Xu, 3d reconstruction from single sketch via view-dependent depth sampling, IEEE Trans. Pattern Anal. Mach. Intell., vol. 46, no. 12, pp. 96619676, 2024. 2 [49] L. Yariv, J. Gu, Y. Kasten, and Y. Lipman, Volume rendering of neural implicit surfaces, in Adv. Neural Inform. Process. Syst., 2021, pp. 48054815. 2 [50] Z. Yu, S. Peng, M. Niemeyer, T. Sattler, and A. Geiger, Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction, in Adv. Neural Inform. Process. Syst., 2022. 2 [51] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, Neural 3d scene reconstruction with the manhattan-world assumption, in IEEE Conf. Comput. Vis. Pattern Recog. IEEE, 2022, pp. 55015510. 2 [52] A. Eftekhar, A. Sax, J. Malik, and A. Zamir, Omnidata: scalable pipeline for making multi-task mid-level vision datasets from 3d scans, in Int. Conf. Comput. Vis., 2021, pp. 10 76610 776. 3, 7, 11, 19, 23, 30 [53] M. Hu, W. Yin, C. Zhang, Z. Cai, X. Long, H. Chen, K. Wang, G. Yu, C. Shen, and S. Shen, Metric3d v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation, CoRR, vol. abs/2404.15506, 2024. 3, 7, 8, 19, [54] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. opf, E. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, Pytorch: An imperative style, high-performance deep learning library, in Adv. Neural Inform. Process. Syst., 2019, pp. 80248035. 6 [55] D. P. Kingma and J. Ba, Adam: method for stochastic optimization, in Int. Conf. Learn. Represent., 2015. 6 [56] J. Watson, F. Aleotti, M. Sayed, Z. Qureshi, O. M. Aodha, G. J. Brostow, M. Firman, and S. Vicente, Airplanes: Accurate plane estimation via 3d-consistent embeddings, in IEEE Conf. Comput. Vis. Pattern Recog., 2024, pp. 52705280. 7 [57] M. Sayed, J. Gibson, J. Watson, V. Prisacariu, M. Firman, and C. Godard, Simplerecon: 3d reconstruction without 3d convolutions, in Eur. Conf. Comput. Vis., vol. 13693, 2022, pp. 119. [58] A. M. Araujo and M. M. Oliveira, robust statistics approach for plane detection in unorganized point clouds, Pattern Recognition, vol. 100, p. 107115, 2020. 8 [59] P.-E. Sarlin, C. Cadena, R. Siegwart, and M. Dymczyk, From coarse to fine: Robust hierarchical localization at large scale, in CVPR, 2019. 11 [60] S. Gao, J. Wan, Y. Ping, X. Zhang, S. Dong, Y. Yang, H. Ning, J. Li, and Y. Guo, Pose refinement with joint optimization of visual points and lines, in IROS, 2022, pp. 28882894. 11 [61] B.-T. Bui, H.-H. Bui, D.-T. Tran, and J.-H. Lee, Representing 3d sparse map points and lines for camera relocalization, in IROS, 2024. 11, 13, 14 [62] D. DeTone, T. Malisiewicz, and A. Rabinovich, Superpoint: Selfsupervised interest point detection and description, in IEEE Conf. Comput. Vis. Pattern Recog., 2018, pp. 224236. 11 [63] V. Larsson and contributors, PoseLib - Minimal Solvers for [Online]. Available: https: Camera Pose Estimation, 2020. //github.com/vlarsson/PoseLib 11 [64] H. Aanæs, R. R. Jensen, G. Vogiatzis, E. Tola, and A. B. Dahl, Large-scale data for multiple-view stereopsis, Int. J. Comput. Vis., pp. 116, 2016. 12 [65] Y. Yao, Z. Luo, S. Li, J. Zhang, Y. Ren, L. Zhou, T. Fang, and L. Quan, Blendedmvs: large-scale dataset for generalized multi-view stereo networks, IEEE Conf. Comput. Vis. Pattern Recog., pp. 1787-1796, 2020. 12 [66] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman, Mip-Nerf 360: Unbounded anti-aliased neural radiance fields, in IEEE Conf. Comput. Vis. Pattern Recog., 2022, pp. 5470 5479. 12 [67] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, Scikit-learn: Machine learning in Python, Journal of Machine Learning Research, vol. 12, pp. 28252830, 2011. 20 [68] R. Wang, S. Xu, Y. Dong, Y. Deng, J. Xiang, Z. Lv, G. Sun, X. Tong, and J. Yang, Moge-2: Accurate monocular geometry with metric scale and sharp details, in Adv. Neural Inform. Process. Syst., 2025."
        },
        {
            "title": "REFERENCES",
            "content": "[1] S. Liu, Y. Yu, R. Pautrat, M. Pollefeys, and V. Larsson, 3d line mapping revisited, in IEEE Conf. Comput. Vis. Pattern Recog. IEEE, 2023, pp. 21 44521 455. 1, 2, 6, 7, 8, 9, 11, 20 [2] L. Li, S. Peng, Z. Yu, S. Liu, R. Pautrat, X. Yin, and M. Pollefeys, 3d neural edge reconstruction, in IEEE Conf. Comput. Vis. Pattern Recog. IEEE, 2024, pp. 21 21921 229. 1, 2 [3] M. Hofer, M. Maurer, and H. Bischof, Efficient 3d scene abstraction using line segments, Comput. Vis. Image Underst., vol. 157, pp. 167178, 2017. 1, 2 [4] N. Xue, B. Tan, Y. Xiao, L. Dong, G. Xia, T. Wu, and Y. Shen, NEAT: distilling 3d wireframes from neural attraction fields, in IEEE Conf. Comput. Vis. Pattern Recog. IEEE, 2024, pp. 19 968 19 977. 1, 2, [5] Z. Yu, J. Zheng, D. Lian, Z. Zhou, and S. Gao, Single-image piecewise planar 3d reconstruction via associative embedding, in IEEE Conf. Comput. Vis. Pattern Recog., 2019, pp. 10291037. 1, 2 [6] B. Tan, N. Xue, S. Bai, T. Wu, and G. Xia, Planetr: Structure-guided transformers for 3d plane recovery, in Int. Conf. Comput. Vis., 2021, pp. 41664175. 1, 2 [7] W. Ma, B. Tan, N. Xue, T. Wu, X. Zheng, and G. Xia, How-3d: Holistic 3d wireframe perception from single image, in Int. Conf. 3D Vis., 2022, pp. 596605. 1 [8] B. Tan, R. Yu, Y. Shen, and N. Xue, Planarsplatting: Accurate planar surface reconstruction in 3 minutes, in IEEE Conf. Comput. Vis. Pattern Recog., 2025. 1, 2, 3, 5, 6, 7, 10, 21, 22 [9] L. Zhang and R. Koch, An efficient and robust line segment matching approach based on LBD descriptor and pairwise geometric consistency, J. Vis. Commun. Image Represent., vol. 24, no. 7, pp. 794805, 2013. 1 IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 17 [10] R. Pautrat, I. Suarez, Y. Yu, M. Pollefeys, and V. Larsson, GlueStick: Robust image matching by sticking points and lines together, in Int. Conf. Comput. Vis., 2023, pp. 97069716. 1, 2 [11] R. Pautrat, D. Barath, V. Larsson, M. R. Oswald, and M. Pollefeys, Deeplsd: Line segment detection and refinement with deep image gradients, in IEEE Conf. Comput. Vis. Pattern Recog. IEEE, 2023, pp. 17 32717 336. [Online]. Available: https://doi.org/10.1109/CVPR52729.2023.01662 1, 2, 3, 4, 7, 8, 12 [12] R. G. von Gioi, J. Jakubowicz, J. Morel, and G. Randall, LSD: fast line segment detector with false detection control, IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 4, pp. 722732, 2010. 1, 3, 6, 8, 9, 11 [13] N. Xue, T. Wu, S. Bai, F. Wang, G. Xia, L. Zhang, and P. H. S. Torr, Holistically-attracted wireframe parsing: From supervised to self-supervised learning, IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 12, pp. 14 72714 744, 2023. 1, 2, 3, 4, 8 [14] Z. Ke, B. Tan, X. Zheng, Y. Shen, T. Wu, and N. Xue, Scalelsd: Scalable deep line segment detection streamlined, in IEEE Conf. Comput. Vis. Pattern Recog., June 2025, pp. 63276336. 1, 2, 3, 7, 8 [15] A. Dai, A. X. Chang, M. Savva, M. Halber, T. A. Funkhouser, and M. Nießner, Scannet: Richly-annotated 3d reconstructions of indoor scenes, in IEEE Conf. Comput. Vis. Pattern Recog., 2017, pp. 24322443. 2, 3, 6, 7, 8, 9, 21, 22, 23, 24, 30, 31 [16] C. Yeshwanth, Y. Liu, M. Nießner, and A. Dai, Scannet++: highfidelity dataset of 3d indoor scenes, in Int. Conf. Comput. Vis., 2023, pp. 1222. 2, 6, 7, 8, 22, [17] M. Roberts, J. Ramapuram, A. Ranjan, A. Kumar, M. A. Bautista, N. Paczan, R. Webb, and J. M. Susskind, Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding, in Int. Conf. Comput. Vis., 2021. 2, 4, 7, 8, 9, 10, 12, 20, 21, 22, 23, 26, 32, 33, 34, 35 [18] J. Shotton, B. Glocker, C. Zach, S. Izadi, A. Criminisi, and A. Fitzgibbon, Scene coordinate regression forests for camera relocalization in rgb-d images, in IEEE Conf. Comput. Vis. Pattern Recog., 2013, pp. 29302937. 2, 11, 12, 13, 14, 15 [19] A. Knapitsch, J. Park, Q.-Y. Zhou, and V. Koltun, Tanks and temples: benchmarking large-scale scene reconstruction, ACM Trans. Graph., vol. 36, no. 4, Jul. 2017. 2, 12, 13, 15 [20] J. Wang, M. Chen, N. Karaev, A. Vedaldi, C. Rupprecht, and D. Novotny, Vggt: Visual geometry grounded transformer, in IEEE Conf. Comput. Vis. Pattern Recog., 2025. 2, 13, 23 [21] J. L. Sch onberger and J. Frahm, Structure-from-motion revisited, IEEE Computer Society, in IEEE Conf. Comput. Vis. Pattern Recog. 2016, pp. 41044113. 2, [22] D. Chekhlov, A. P. Gee, A. Calway, and W. W. Mayol-Cuevas, Ninja on plane: Automatic discovery of physical planes for augmented reality using visual SLAM, in IEEE/ACM International Symposium on Mixed and Augmented Reality, 2007, pp. 153156. 2 [23] S. Wang, V. Leroy, Y. Cabon, B. Chidlovskii, and J. Revaud, Dust3r: Geometric 3d vision made easy, in IEEE Conf. Comput. Vis. Pattern Recog., 2024, pp. 20 69720 709. 2 [24] Z. Xue, Z. Yuan, J. Wang, X. Wang, Y. Gao, and H. Xu, USEEK: unsupervised se(3)-equivariant 3d keypoints for generalizable manipulation, in Int. Conf. on Robot. and Auto., 2023, pp. 17151722. 2 [25] D. G. Lowe, Distinctive image features from scale-invariant keypoints, Int. J. Comput. Vis., vol. 60, no. 2, pp. 91110, 2004. 2 [26] J. Morel and G. Yu, ASIFT: new framework for fully affine invariant image comparison, SIAM J. Imaging Sci., vol. 2, no. 2, pp. 438469, 2009. 2 [27] P. Sarlin, D. DeTone, T. Malisiewicz, and A. Rabinovich, Superglue: Learning feature matching with graph neural networks, in IEEE Conf. Comput. Vis. Pattern Recog., 2020, pp. 49374946. 2 [28] H. Bay, A. Ess, T. Tuytelaars, and L. V. Gool, Speeded-up robust features (SURF), Comput. Vis. Image Underst., vol. 110, no. 3, pp. 346359, 2008. 2 [29] N. Xue, T. Wu, S. Bai, F. Wang, G. Xia, L. Zhang, and P. H. S. Torr, Holistically-attracted wireframe parsing, in IEEE Conf. Comput. Vis. Pattern Recog., 2020, pp. 27852794. [30] R. Pautrat, J. Lin, V. Larsson, M. R. Oswald, and M. Pollefeys, SOLD2: self-supervised occlusion-aware line description and detection, in IEEE Conf. Comput. Vis. Pattern Recog., 2021, pp. 11 368 11 378. 2 [31] S. Huang, F. Qin, P. Xiong, N. Ding, Y. He, and X. Liu, TP-LSD: tri-points based line segment detector, in Eur. Conf. Comput. Vis., vol. 12372, 2020, pp. 770785. 2 [32] H. Li, K. Chen, J. Zhao, J. Wang, P. Kim, Z. Liu, and Y. Liu, Learning to identify correct 2d-2d line correspondences on sphere, in IEEE Conf. Comput. Vis. Pattern Recog., 2021, pp. 11 74311 752. 2 [33] D. Wei, Y. Wan, Y. Zhang, X. Liu, B. Zhang, and X. Wang, ELSR: efficient line segment reconstruction with planes and points guidance, in IEEE Conf. Comput. Vis. Pattern Recog., 2022, pp. 15 786 15 794. 2 [34] X. Bai, H. Cui, and S. Shen, Consistent 3d line mapping, in European Conference on Computer Vision (ECCV), 2024. 2, 6, 7, 8 [35] H. Pfister, M. Zwicker, J. van Baar, and M. H. Gross, Surfels: surface elements as rendering primitives, in ACM SIGGRAPH, 2000, pp. 335342. 2 [36] Y. Wang, F. Serena, S. Wu, C. Oztireli, and O. Sorkine-Hornung, Differentiable surface splatting for point-based geometry processing, ACM Trans. Graph., vol. 38, no. 6, pp. 230:1230:14, 2019. [37] B. Huang, Z. Yu, A. Chen, A. Geiger, and S. Gao, 2d gaussian splatting for geometrically accurate radiance fields, in ACM SIGGRAPH Conference Papers, A. Burbano, D. Zorin, and W. Jarosz, Eds., 2024, p. 32. 2 [38] B. Tan, N. Xue, T. Wu, and G. Xia, NOPE-SAC: neural one-plane RANSAC for sparse-view planar 3d reconstruction, IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 12, pp. 15 23315 248, 2023. 2 [39] C. Liu, K. Kim, J. Gu, Y. Furukawa, and J. Kautz, Planercnn: 3d plane detection and reconstruction from single image, in IEEE Conf. Comput. Vis. Pattern Recog., 2019, pp. 44504459. 2 [40] J. Shi, S. Zhi, and K. Xu, Planerectr: Unified query learning for 3d plane recovery from single view, in Int. Conf. Comput. Vis., 2023, pp. 93439352. 2 [41] L. Jin, S. Qian, A. Owens, and D. F. Fouhey, Planar surface reconstruction from sparse views, in Int. Conf. Comput. Vis., 2021, pp. 12 97112 980. 2 [42] S. Agarwala, L. Jin, C. Rockwell, and D. F. Fouhey, Planeformers: From sparse view planes to 3d reconstruction, in Eur. Conf. Comput. Vis., vol. 13663, 2022, pp. 192209. 2 [43] J. Liu, P. Ji, N. Bansal, C. Cai, Q. Yan, X. Huang, and Y. Xu, Planemvs: 3d plane reconstruction from multi-view stereo, in IEEE Conf. Comput. Vis. Pattern Recog., 2022, pp. 86558665. 2 [44] T. Shen, J. Gao, K. Yin, M. Liu, and S. Fidler, Deep marching tetrahedra: hybrid representation for high-resolution 3d shape synthesis, in Adv. Neural Inform. Process. Syst., 2021, pp. 6087 6101. [45] Z. Chen, A. Tagliasacchi, and H. Zhang, Bsp-net: Generating compact meshes via binary space partitioning, in IEEE Conf. Comput. Vis. Pattern Recog., 2020, pp. 4251. 2 [46] Z. Chen, A. Tagliasacchi, T. A. Funkhouser, and H. Zhang, Neural dual contouring, ACM Trans. Graph., vol. 41, no. 4, pp. 104:1 104:13, 2022. 2 [47] T. Monnier, J. Austin, A. Kanazawa, A. A. Efros, and M. Aubry, Differentiable blocks world: Qualitative 3d decomposition by rendering primitives, in Adv. Neural Inform. Process. Syst., 2023. 2 [48] C. Gao, X. Wang, Q. Yu, L. Sheng, J. Zhang, X. Han, Y.-Z. Song, and D. Xu, 3d reconstruction from single sketch via view-dependent depth sampling, IEEE Trans. Pattern Anal. Mach. Intell., vol. 46, no. 12, pp. 96619676, 2024. 2 [49] L. Yariv, J. Gu, Y. Kasten, and Y. Lipman, Volume rendering of neural implicit surfaces, in Adv. Neural Inform. Process. Syst., 2021, pp. 48054815. 2 [50] Z. Yu, S. Peng, M. Niemeyer, T. Sattler, and A. Geiger, Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction, in Adv. Neural Inform. Process. Syst., 2022. 2 [51] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, Neural 3d scene reconstruction with the manhattan-world assumption, in IEEE Conf. Comput. Vis. Pattern Recog. IEEE, 2022, pp. 55015510. [52] A. Eftekhar, A. Sax, J. Malik, and A. Zamir, Omnidata: scalable pipeline for making multi-task mid-level vision datasets from 3d scans, in Int. Conf. Comput. Vis., 2021, pp. 10 76610 776. 3, 7, 11, 19, 23, 30 [53] M. Hu, W. Yin, C. Zhang, Z. Cai, X. Long, H. Chen, K. Wang, G. Yu, C. Shen, and S. Shen, Metric3d v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation, CoRR, vol. abs/2404.15506, 2024. 3, 7, 8, 19, 23 [54] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 18 A. opf, E. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, Pytorch: An imperative style, high-performance deep learning library, in Adv. Neural Inform. Process. Syst., 2019, pp. 80248035. [55] D. P. Kingma and J. Ba, Adam: method for stochastic optimization, in Int. Conf. Learn. Represent., 2015. 6 [56] J. Watson, F. Aleotti, M. Sayed, Z. Qureshi, O. M. Aodha, G. J. Brostow, M. Firman, and S. Vicente, Airplanes: Accurate plane estimation via 3d-consistent embeddings, in IEEE Conf. Comput. Vis. Pattern Recog., 2024, pp. 52705280. 7 [57] M. Sayed, J. Gibson, J. Watson, V. Prisacariu, M. Firman, and C. Godard, Simplerecon: 3d reconstruction without 3d convolutions, in Eur. Conf. Comput. Vis., vol. 13693, 2022, pp. 119. 7 [58] A. M. Araujo and M. M. Oliveira, robust statistics approach for plane detection in unorganized point clouds, Pattern Recognition, vol. 100, p. 107115, 2020. 8 [59] P.-E. Sarlin, C. Cadena, R. Siegwart, and M. Dymczyk, From coarse to fine: Robust hierarchical localization at large scale, in CVPR, 2019. [60] S. Gao, J. Wan, Y. Ping, X. Zhang, S. Dong, Y. Yang, H. Ning, J. Li, and Y. Guo, Pose refinement with joint optimization of visual points and lines, in IROS, 2022, pp. 28882894. 11 [61] B.-T. Bui, H.-H. Bui, D.-T. Tran, and J.-H. Lee, Representing 3d sparse map points and lines for camera relocalization, in IROS, 2024. 11, 13, 14 [62] D. DeTone, T. Malisiewicz, and A. Rabinovich, Superpoint: Selfsupervised interest point detection and description, in IEEE Conf. Comput. Vis. Pattern Recog., 2018, pp. 224236. 11 [63] V. Larsson and contributors, PoseLib - Minimal Solvers for [Online]. Available: https: Camera Pose Estimation, 2020. //github.com/vlarsson/PoseLib 11 [64] H. Aanæs, R. R. Jensen, G. Vogiatzis, E. Tola, and A. B. Dahl, Large-scale data for multiple-view stereopsis, Int. J. Comput. Vis., pp. 116, 2016. [65] Y. Yao, Z. Luo, S. Li, J. Zhang, Y. Ren, L. Zhou, T. Fang, and L. Quan, Blendedmvs: large-scale dataset for generalized multi-view stereo networks, IEEE Conf. Comput. Vis. Pattern Recog., pp. 1787-1796, 2020. 12 [66] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman, Mip-Nerf 360: Unbounded anti-aliased neural radiance fields, in IEEE Conf. Comput. Vis. Pattern Recog., 2022, pp. 5470 5479. 12 [67] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, Scikit-learn: Machine learning in Python, Journal of Machine Learning Research, vol. 12, pp. 28252830, 2011. 20 [68] R. Wang, S. Xu, Y. Dong, Y. Deng, J. Xiang, Z. Lv, G. Sun, X. Tong, and J. Yang, Moge-2: Accurate monocular geometry with metric scale and sharp details, in Adv. Neural Inform. Process. Syst., 2025. 23 IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE"
        },
        {
            "title": "APPENDIX A\nMORE IMPLEMENTATION DETAILS",
            "content": "Ray-to-Plane Intersection. We first calculate the intersections between planar primitives and the rays emitted from pixels of the image. Specifically, given ray = {o, d} starting from the camera center R3 with direction π R3 to one planar primitive R3, its intersection xr π can be calculated as: xr π = + (pπ nπ) nπ d, (18) where pπ and nπ are the center and the normal of the planar primitive π. Plane Splatting Function. After achieving the ray-to-plane intersection xr π, we then calculate its splatting weight with our plane splatting function, which will be used for rendering. For given intersection xr π between the ray and the planar primitive π, we first calculate its projection distance PX , PY to the X-axis and Y-axis of the planar primitive as: PX = (xr π pπ) vx π, PY = (xr π pπ) vy π. (19) Then, we calculate the splatting weight along the X-axis of the plane π as: wX (xr π) = (cid:40) 2σ(5λ(rx+ 2σ(5λ(rx π PX )), π PX )), if PX > 0 otherwise , (20) where σ() is the Sigmoid function and λ is the hyperparameter to control the splatting weight. Similarly, we continue to calculate the splatting weight along the Y-axis of the plane π as: wY (xr π) = (cid:40) 2σ(5λ(ry+ 2σ(5λ(ry π PY )), π PY )), if PY > 0 otherwise , (21) π , ry π π , ry+ π , rx where rx+ are the radii parameters of the plane π. The value of λ is increased with an exponential function during optimization up to the maximum value of 300 as: λ = min(20e((10.001ite)), 300), (22) where ite means the iteration number during optimization. At last, the final splatting weight can be calculated as: w(xr π) = (cid:40) wX , wY , if wX < wY otherwise . (23) Planar Blending Composition. For all ray-to-plane intersections, we filter them with splatting weight lower than 0.0001 and then sort the remaining intersections according to their depth from near to far. Then, nearest intersections of each ray are selected to render (M = 5 in this paper). Denote the selected intersections of ray as = {xr }M j=1. Here, τ (j) indicates the index of the plane among all planar primitives. Finally, we render the depth and normal map of certain image as: πτ (j) Fig. 19: Geometric consistency measures. We propose three scale-invariant measures for ensuring the geometric consistency between 3D planar edges and 2D detected lines. NΠ render(r) = (cid:88) j= Tjw(xr πτ (j) )nπτ (j) , where Tj = j1 (cid:89) (1 w(xr )). πτ (i) (25) (26) i= is the depth of the intersection and nπτ (j) Here, tj is the normal of the planar primitive πτ (j). To supervise the rendered depth and normal map, we use the groundtruth depth/normal maps of the given dataset or the depth/normal maps predicted by the pretrained model of Metric3Dv2 [53] and Omnidata [52] to serve as pseudo labels. The depth and normal maps used for supervision are denoted as Dpre(r) and Npre(r). Finally, the render loss can be calculated as: LΠ render(r)Npre(r))1+ render =α1 1 NΠ (cid:88) α1 α2 rI (cid:88) rI (cid:88) rI NΠ render(r) Npre(r))1+ (27) DΠ render(r) Dpre(r)1, where α1 = 5.0, α2 = 1.0, α3 = 2.0, is the ray emitted from pixel of image I. APPENDIX GEOMETRIC CONSISTENCY MEASURES. We have defined some geometric constraints in our paper and applied these constraints to extract the final 3D line maps and to build line tracks. Here, we introduce the computation of these geometric constraints and give toy example in Fig. 19 for illustration. Assume the green region is projected from the 3D planar primitive, (x1 1) are the endpoints of the projected 2D line L1 from the 3D planar edge, and (x1 2) are the endpoints of the detected 2D line L2. We compute three geometric measures as follows: 2, x2 1, x2 The angle distance dang (denoted as θ) between the projected 2D lines and each detected 2D line. As shown in Fig. 19(a), the θ is computed as: (cid:32) θ = arccos clamp( (cid:13) (cid:0)x2 (cid:13) (cid:1) (cid:0)x2 1 x1 1 1 x1 (x2 2 x1 2 1) (cid:1)(cid:13) (cid:13) (cid:33) , 1, 1) . DΠ render(r) = (cid:88) j= Tjw(xr πτ (j) )tj, (24) (28) The angle distance used in Sec. 3.2 and Sec. 3.4 is also computed in this way. IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 20 The maximum orthogonal distance ddist from the endpoints of the projected 2D line to the detected 2D line. As shown in Fig. 19(b), the ddist is computed as: ddist = max (d1, d2) , di = (cid:13) (cid:0)xi (cid:13) 1 x1 2 (x2 (cid:1) (cid:0)xi 1 x2 2 2 x1 2) (cid:1)(cid:13) (cid:13) . (29) The orthogonal distance ddist defined in Sec. 3.4 is also computed in this way. The overlap ratio doverlap between the projected 2D line and the detected 2D line. As shown in Fig. 19(C), the doverlap is computed as: doverlap = L2 , (30) where is the inner projection portion of L1 on L2. The overlap ratio used in Sec. 3.4 is computed in this way."
        },
        {
            "title": "APPENDIX C\nCOMPUTATION OF EVALUATION METRICS",
            "content": "Given the ground-truth model (mesh or point cloud) and the reconstructed 3D lines, we introduce how to compute the widely-used standard metric M1 and the line tracks-based metric M2. The basic step is to calculate the closest distance between the 3D line segments and the model. To this end, we first uniformly sample points along each 3D line segment and then build KD-Tree over the ground-truth model or the sampled points for efficient computation of the distance between the sampled points and the ground-truth model. Metric M1. To construct the input points Ppre, we uniformly sample 100 points along each 3D line segment for linelevel metrics and collect the two endpoints of each 3D line segment for junction-level metrics. Given the ground-truth point cloud Pgt, we build the KD-tree as: KDTpre = KDTree(Ppre), KDTgt = KDTree(Pgt), (31) where algorithm KDTree is adopted from the library Scikitlearn [67]. Then we query the KDTgt with points Ppre and query the KDTpre with point cloud Pgt to get the distance: Dpre = KDTgt(Ppre), Dgt = KDTpre(Pgt). (32) The accuracy and completeness are computed by the following: ACC = COMP = n1(cid:88) Di gt, 1 n1 i=1 1 n2 n2(cid:88) i=1 (33) Di pre, where n1 is the number of points in the group of points Ppre and n2 is the number of points in the point cloud Pgt. Given the distance threshold τd (0.05m in our paper), the precision, recall, and F1 score are computed by: n1(cid:88) 1(Di pre <= τd), PREC = 1 n1 RECALL = n2(cid:88) i=1 1 F1 = 2 i=1 PREC RECALL PREC + RECALL 1(Di gt <= τd), (34) where 1() is the indicator function. Metric M2. Given the reconstructed 3D lines, following LIMAP [1], we uniformly sample 1000 points along each 3D line segment and build the KD-tree to obtain the closest distances Dpre. Given the threshold τd of distance (1/5/10 mm or 5/10/50 mm in our paper), the proportion of each 3D line segment that has points with distance less than τd is calculated as: ratiosi = 1 (cid:88) j=1 1(D(i,j) pre <= τd) (35) where = 1000 is the number of sampled points of each line. The sum of the lengths of the line portions within τd mm from the GT model is computed as: Rτd = (cid:88) i= Leni ratiosi, (36) where Leni is the length of the i-th 3D line. The percentage of tracks that are within τd mm from the GT model is computed as: Pτd = 100 1 (cid:88) i=1 1(ratiosi > 0). (37) APPENDIX GLOBAL LINE MERGING Our pipeline may inherit errors from the 2D line detector, which often manifest as fragmented detections, duplicated line hypotheses, or slight endpoint misalignments across views. As lightweight remedy, we provided global merging strategy to consolidate redundant 3D line segments while preserving geometric consistency. Baseline and local merging: Taking our original method without merging as baseline, our local merging strategy groups 3D lines corresponding to the same detected line and merges them into single line segment. Global merging: Our global merging strategy uses DBSCAN to cluster 3D lines associated with each 2D detected line across all views, propagates unique identifiers through these clusters to establish global correspondences, and merges lines sharing the same identifier using PCA-based alignment to determine the final merged line segments. The algorithm is shown in Algorithm 1. We provide the corresponding implementation in our code repository and the threshold is set to τdbscan = 0.01 in our implementation. The qualitative comparison of results on ai 001 001 and ai 001 010 scenes from the Hypersim dataset [17] is shown in Fig. 20. Compared to the original results without merging, the local merging strategy can clean up incorrect lines and IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE Fig. 20: Qualitative results of our line merging strategies on two scenes from the Hypersim dataset [17]. We crop some patches for detailed visualization. Red: original LiP-Map w/o merging. Blue: LiP-Map w/ local merging. Orange: LiP-Map w/ global merging. merge corresponding lines, which would also incorporate many more redundant lines. The global merging strategy further consolidates corresponding lines, removes redundant lines, and ultimately improves the quality and completeness of the line mapping. Due to the complexity, the global merging strategy is more computationally expensive than the local merging strategy, which is acceptable based on the final results. Algorithm 1: Global Line Merging Strategy Input: Optimized 3D planes P, detected 2D lines {Li}N views, perpendicular distance threshold τdbscan i=1 from all Output: Merged 3D line map Lmerged // Step 1: Multi-view line association foreach view {1, . . . , } do foreach detected 2D line li,j Li do Find all 3D lines Li,j by casting rays from the 1-pixel region of li,j and collecting edges from intersected 3D planes; // Step 2: Local DBSCAN clustering Compute pairwise perpendicular distances for all line pairs in Li,j by sampling 10 points per line; Apply DBSCAN with threshold τdbscan to obtain clusters {Ci,j,k}; // Step 3: Global identifier propagation foreach cluster Ci,j,k do if any line in Ci,j,k has global identifier gid then Assign gid to all lines in Ci,j,k; else Create new global identifier gnew id lines in Ci,j,k; and assign to all // Step 4: PCA-based line merging Group all 3D lines by their global identifiers {Gg}; foreach global group Gg do Apply PCA to obtain main direction dg and mean point pg; Project all endpoints in Gg onto the main line to find tmin and tmax; Construct merged line: lg = (pg + tmin dg, pg + tmax dg); Add lg to Lmerged; return Lmerged TABLE 16: Quantitative comparison of planar reconstruction results on the ScanNetV2 dataset [15]. PlanarSplatting [8] Ours VOI 2.489 2.469 RI 0.951 0. SC 0.530 0.532 APPENDIX ENHANCEMENT FOR 3D PLANAR RECONSTRUCTION Since our LiP-Map enhances the reconstruction of plane edges, leading to more accurate perception of physical boundaries, it can further improve performance in 3D planar reconstruction. Following PlanarSplatting [8], we use the metrics including Variation of Information (VOI), Rand Index (RI), and Segmentation Covering (SC). As shown in Table 16, our LiP-Map achieves superior performance compared to PlanarSplatting [8]. We also illustrate comparison of reconstruction results in Fig. 21. As evident from the comparison, our method effectively aligns the edges of planar primitives with the line structures in the scene, resulting in reconstructed meshes with more prominent geometric topology. This leads to more accurate and plausible reconstructions, enabling the extraction of complete and precise 3D line map from the scene."
        },
        {
            "title": "APPENDIX F\nLIMITATIONS",
            "content": "Despite its advantages, our method has certain limitations that warrant further study. Due to errors in 2D line detection, duplicate 3D line segments may appear, potentially IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 22 TABLE 17: Optimization time and number of planes versus the splitting threshold. Threshold 0. 0.1 0.2 1.0 2.0 10 Time(s) Init. #Planes Final. #Planes 948.5209 1873 115615 388.9839 1873 19876 333.0532 1873 10064 312.5148 1873 2071 305.0371 1873 1369 301.3305 1873 method demonstrates clear advantages."
        },
        {
            "title": "APPENDIX G\nMORE QUALITATIVE RESULTS OF OUR LINE MAPS",
            "content": "We visualize more qualitative results of our 3D line maps on scenes from the ScanNetV2 dataset [15](Fig 22), the ScanNet++ dataset [16]  (Fig. 23)  , and the Hypersim dataset [17]  (Fig. 24)  ."
        },
        {
            "title": "APPENDIX H\nQUALITATIVE RESULTS ON THE SENSITIVITY TO\nHEURISTICS AND HYPERPARAMETERS",
            "content": "We have reported the quantitative results on the sensitivity to heuristics and hyperparameters in Sec.4.4.3. In this section, we would like to provide more detailed discussions and the corresponding qualitative results. The plane splitting threshold (based on radius gradients). This parameter is fixed to 0.2 in all experiments. On the one hand, we keep it consistent with the setting in PlanarSplatting [8]. On the other hand, small threshold makes planes split too aggressively, causing the number of planes to explode, whereas large threshold slows down splitting, leaving too few planes to support fine-grained reconstruction. We validate this claim by reporting statistics on the optimization time and the number of planes in Table 17 and illustrating some details of reconstruction in Fig. 25. The initial number of random planar primitives. We have reported detailed results about how the optimization time varies with the number of initial planes in Table 15, and provided the corresponding qualitative comparisons in Fig. 15. Notably, larger number of planes for initialization is indeed necessary to obtain better reconstruction results when meeting large-scale scenes (as shown in Fig. 36(b)). Therefore, the most straightforward choice is to use large initialization value for all scenes (e.g., 50,000), and the only trade-off is slight increase in optimization time (as reported in Table 15). The 2000 initial planes are sufficient for general indoor scenes. The assignment thresholds for 3D line mapping. The assignment thresholds are only used in the inference stage to achieve the final line mapping after optimization, as described in Sec. 3.4. We use pair of very strict thresholds, τd = 1 pixel and τα = 0.01 rad (about 0.57 degrees), in our paper to ensure accurate 3D line mapping. Relaxing the thresholds will yield many more 3D lines and output more complete 3D line maps, as shown in Fig. 26, which also introduces lot of noisy lines. The balancing weights αΠ,αL in the joint optimization. Our choice of balancing weights, with αΠ = 10,αL = 0.1, is Fig. 21: Illustration of the comparison of planar mesh/edges reconstruction with baseline PlanarSplatting [8] on ai 001 001 from Hypersim [17]. Left column: PlanarSplatting. Right column: ours. First row: planar surface. Second row: planar mesh. Third row: planar edges. Last row: 3D line maps. violating the parsimony principle in 3D line mapping. Additionally, the plane-based representation of 3D line segments is particularly well-suited for reconstructing architectural structures in man-made environments, though it has limited capability in reconstructing non-planar, unstructured objects. Nevertheless, we argue that 3D line segment reconstruction primarily aims to perceive geometric features in the environment and efficiently capture the outlines and topological structures of buildings, generating lightweight, semantically rich vectorized modelsareas in which our IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 23 intended to bring the planar surface optimization loss and the 3D line optimization loss to comparable numerical scale. This balance also makes the optimization process highly stable. We are also pleased to provide qualitative visualizations of some results in Fig. 27. It can be analyzed that the large weight αL for 3D line optimization can cause the number of planes to grow rapidly, thereby increasing the optimization time. In contrast, the small weight αΠ for the planar-surface optimization will reduce the number of planes, which compromises complete and fine-grained scene reconstruction. APPENDIX DEPENDENCE ON INITIALIZATION QUALITY: We agree that it is crucial to understand whether the performance gains are primarily due to the proposed joint optimization framework or simply inherited from highquality priors. ScanNetV2. To address this, we have conducted additional experiments on ScanNetV2 [15] using different quality depth maps, such as raw sensor depth maps, monocular depth maps predicted by Metric3D [53], and depth maps from the recent SOTA method MoGe-2 [68]. Some qualitative results are illustrated in Fig. 28 for detailed comparison. The results show that while the quality of initial depth maps does influence the final outcome, our joint optimization framework is still able to significantly improve the geometry and can generate more complete and smoother meshes compared to the initial meshes. In particular, the line map accuracy and surface consistency are notably enhanced through the refinement process, even when starting from noisy or sparse depth estimates. Furthermore, we conducted experiments on ScanNetV2 [15] to investigate the impact of normal maps on our method. We use sensor depth maps and perform optimization with normal maps from Metric3D [53], MoGe-2 [68], and Omnidata [52], respectively. The results are shown in Fig. 29. We per-scene illustrate the normal map of one view and the corresponding part of the reconstructed mesh as concise comparison to show in detail the impacts of normal maps on the optimization results. It can be seen that normal maps may have some influence on the final optimized mesh, but all settings still produce similarly high-quality 3D line maps in the end. Hypersim. Similarly, regarding the impact of depth maps on the optimization results, we conducted extensive experiments on the Hypersim dataset [17] using ground-truth depth maps, depth maps from Metric3D, and depth maps from the recent SOTA method MoGe-2, while keeping all other settings identical. Fig. 30 compares initial meshes from Metric3D [53] and MoGe-2 [68] against our optimized planar meshes, demonstrating that our method recovers highquality planar meshes even from low-quality initialization. Fig. 31 shows that our optimization framework produces high-quality line maps comparable to those initialized with ground-truth meshes, even when starting from poor-quality initial meshes. Additional results on other Hypersim scenes are shown in Fig. 32 and Fig. 33. The extensive results above provide evidence that our optimization framework is designed to be robust to certain levels of noise and sparsity in the input depth maps and normal maps, and demonstrate that the proposed optimization contributes meaningfully to the final output, beyond merely fitting the input priors. APPENDIX MORE RESULTS ABOUT GENERALIZATION TO NONPLANAR STRUCTURES We have investigated the generalization ability of our method on generic non-planar structures and conducted experiments using only multi-view images from scenes in some common datasets. In this section, we select several views from Hypersim [17] scenes that contain such non-planar geometry, and visualize 2D line detection, the meshes, and line maps reconstructed by our method in Fig. 34. It can be seen that our method can approximate curved surfaces by fitting these non-planar structures with dense set of planar primitives, which demonstrates the satisfaction of the needs for reconstructing many common non-planar structures in man-made indoor scenes. Additionally, we illustrate some local failure cases of non-planar scenes in Figure 35. Red boxes highlight inaccurate or hallucinated planes that often arose to satisfy the supervision from the noisy depth maps or normal maps. However, the strict thresholds used in our assignment strategy (as described in Sec. 3.4) enforce strong geometric consistency between 2D and 3D lines, allowing these inaccurate plane edges to be filtered out. APPENDIX FAILURE CASES We have summarized some failure cases under four different conditions and illustrated qualitative results in Fig. 36, including completely failed cases and partially unsatisfactory cases. To explore the boundaries of our method as thoroughly as possible, all results are reconstructed with VGGTs [20] imperfect outputs. (a) Pure rotation: This is classic ill-posed problem. We render 100 views for each of two scenes in Blender by rotating the camera without any translation. Purely rotational sequences make outputs from VGGT unreliable, causing the initial mesh to exhibit severe geometric drift and ultimately leading to reconstruction failure. (b) Initial number of planes: Given the same initial mesh, initializing the planar surface with insufficient plane primitives can also lead to the reconstruction failing for large-scale scenes. (c) Non-Planar Scene: Reconstructing non-planar structured scenes remains challenging for our method as well, and results are less accurate and less clean than those obtained for indoor scenes. (d) Initial mesh/depth: Using the same number of planar primitives for initialization, noisy or inaccurate initial meshes or depth maps can also degrade the reconstruction quality. IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 24 (a) scene0084 00 (b) scene0086 00 (c) scene0217 (d) scene0304 00 (e) scene0462 00 (f) scene0488 00 (g) scene0651 00 (h) scene0664 00 (i) scene0693 Fig. 22: More qualitative results of the 3D line maps recovered by our method on the ScanNetV2 dataset [15]. IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 25 (a) 8a20d62ac0 (b) 9b74afd2d2 (c) 9f21bdec (d) 45b0dac5e3 (e) bc2fce1d81 (f) f3685d06a9 Fig. 23: More qualitative results of the 3D line maps recovered by our method on the ScanNet++ dataset [16]. IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (a) ai 001 001 (b) ai 001 003 (c) ai 001 004 (d) ai 001 007 (e) ai 001 008 (f) ai 001 Fig. 24: More qualitative results of the 3D line maps recovered by our method on the Hypersim dataset [17]. IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 27 Fig. 25: Qualitative results of reconstruction with different plane splitting thresholds. Some patches are cropped for detailed comparison, and the final numbers of planes are attached below. IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE Fig. 26: Qualitative results of reconstruction with different pairs of assignment thresholds for 3D line mapping. IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 29 Fig. 27: Qualitative results of reconstruction with different pairs of loss weights for 3D line mapping. IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE Fig. 28: Qualitative comparison of initial meshes (first row), our planar meshes (second row), and our 3D line maps (last row) on ScanNetV2 [15]. Normal maps predicted by Omnidata [52] are adopted in this experiment. IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 31 Fig. 29: Qualitative comparison of our planar meshes and 3D line maps reconstructed with normal maps obtained from different models on ScanNetV2 [15]. The raw sensor depth maps are adopted in this experiment. IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE Fig. 30: Qualitative comparison of initial meshes and our planar meshes on Hypersim [17]. Fig. 31: Qualitative comparison of 3D line mapping with different quality of initial meshes on Hypersim [17]. IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 33 Fig. 32: Additional qualitative comparison of initial meshes and our planar meshes on Hypersim [17]. IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 34 Fig. 33: Additional qualitative comparison of 3D line mapping with different quality of initial mesh/depth on Hypersim [17]. IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 35 Fig. 34: Qualitative results of non-planar structure reconstruction on Hypersim [17] scenes. We show 2D line detection, reconstructed meshes, and 3D line maps for curved structures such as chairs and circular tables. Fig. 35: Qualitative results of locally non-planar structure. The 2D line detection results, along with the reconstructed 3D planar surfaces and 3D line segments, are illustrated for reference. IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 36 Fig. 36: Visualization of our fully/partially failure cases."
        }
    ],
    "affiliations": [
        "Ant Group",
        "School of Computer Science and the School of Artificial Intelligence, Wuhan University, Wuhan 430072, China",
        "School of Computer Science, Wuhan University, Wuhan 430072, China"
    ]
}