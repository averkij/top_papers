{
    "paper_title": "Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis",
    "authors": [
        "Jinbin Bai",
        "Tian Ye",
        "Wei Chow",
        "Enxin Song",
        "Qing-Guo Chen",
        "Xiangtai Li",
        "Zhen Dong",
        "Lei Zhu",
        "Shuicheng Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models, such as Stable Diffusion, have made significant strides in visual generation, yet their paradigm remains fundamentally different from autoregressive language models, complicating the development of unified language-vision models. Recent efforts like LlamaGen have attempted autoregressive image generation using discrete VQVAE tokens, but the large number of tokens involved renders this approach inefficient and slow. In this work, we present Meissonic, which elevates non-autoregressive masked image modeling (MIM) text-to-image to a level comparable with state-of-the-art diffusion models like SDXL. By incorporating a comprehensive suite of architectural innovations, advanced positional encoding strategies, and optimized sampling conditions, Meissonic substantially improves MIM's performance and efficiency. Additionally, we leverage high-quality training data, integrate micro-conditions informed by human preference scores, and employ feature compression layers to further enhance image fidelity and resolution. Our model not only matches but often exceeds the performance of existing models like SDXL in generating high-quality, high-resolution images. Extensive experiments validate Meissonic's capabilities, demonstrating its potential as a new standard in text-to-image synthesis. We release a model checkpoint capable of producing $1024 \\times 1024$ resolution images."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 1 ] . [ 1 1 6 2 8 0 . 0 1 4 2 : r Technical Report MEISSONIC: REVITALIZING MASKED GENERATIVE TRANSFORMERS FOR EFFICIENT HIGH-RESOLUTION TEXT-TO-IMAGE SYNTHESIS Jinbin Bai1,2, Tian Ye3, Wei Chow5, Enxin Song5, Qing-Guo Chen1, Xiangtai Li2, Zhen Dong6, Lei Zhu3,4, Shuicheng Yan2 Model: https://huggingface.co/MeissonFlow/Meissonic Vision: Unleash creativity, Create freely, Just like Pro."
        },
        {
            "title": "ABSTRACT",
            "content": "Diffusion models, such as Stable Diffusion, have made significant strides in visual generation, yet their paradigm remains fundamentally different from autoregressive language models, complicating the development of unified language-vision models. Recent efforts like LlamaGen have attempted autoregressive image generation using discrete VQVAE tokens, but the large number of tokens involved renders this approach inefficient and slow. In this work, we present Meissonic, which elevates non-autoregressive masked image modeling (MIM) text-to-image to level comparable with state-of-the-art diffusion models like SDXL. By incorporating comprehensive suite of architectural innovations, advanced positional encoding strategies, and optimized sampling conditions, Meissonic substantially improves MIMs performance and efficiency. Additionally, we leverage highquality training data, integrate micro-conditions informed by human preference scores, and employ feature compression layers to further enhance image fidelity and resolution. Our model not only matches but often exceeds the performance of existing models like SDXL in generating high-quality, high-resolution images. Extensive experiments validate Meissonics capabilities, demonstrating its potential as new standard in text-to-image synthesis. We release model checkpoint capable of producing 1024 1024 resolution images."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) (Touvron et al., 2023; Bai et al., 2023; Su et al., 2024) have showcased amazing advancements in natural language processing (NLP) tasks by adhering to established scaling laws. The combination of next-token prediction pre-training and supervised fine-tuning, has unlocked the potential of LLMs across diverse range of language tasks, suggesting that similar methodologies might be applicable to text-to-image synthesis (T2I). Simultaneously, diffusion models, such as Stable Diffusion (Rombach et al., 2022a; Podell et al., 2023; Desync, 2024; Art, 2023), have rapidly advanced to become the dominant paradigm in visual generation. However, their operation significantly diverges from autoregressive models, posing challenges in achieving unified approach to both language and vision tasks. Some effects have explored bridging this gap through connectors between LLMs and diffusion models. Recent developments like LlamaGen (Sun et al., 2024) have ventured into autoregressive image generation using discrete image tokens derived from VQVAE (Yu et al., 2022a). Despite progress, the substantial number of image tokens compared to text tokens makes autoregressive generation Equal contribution. Corresponding authors. 1Alibaba Group 2Skywork AI 3HKUST(GZ) 4HKUST 5Zhejiang University 6UC Berkeley 2Proof of Concept done during Jinbins internship in Skywork AI. 1 Technical Report Figure 1: Images produced by Meissonic exhibit exceptional image quality. More samples can be found in Appendix J. Notably, Meissonic can effortlessly produce images with solid-color backgrounds without requiring any additional modifications. inefficient. For example, tokenizing one 1024 1024 image using 16 downsampled VQVAE yields 4096 tokens, where sequential generation process is prohibitively slow. 2 Technical Report On the other hand, MaskGIT (Chang et al., 2022) introduced more efficient, non-autoregressive alternative, where all image tokens are predicted simultaneously in parallel, iterative refinement process. Then, MUSE (Chang et al., 2023) extended this technique to higher resolutions, achieving 512 512 resolution T2I generation. These non-autoregressive methods offer around 99% reduction in decoding steps compared to autoregressive methods. However, despite their efficiency, non-autoregressive transformers remain limited in performance compared to advancing diffusion or autoregressive models, particularly in high-quality, high-resolution text-to-image synthesis. In this work, we address these challenges and introduce two key innovations to make masked image modeling (MIM) competitive with advanced diffusion models: Enhanced Transformer Architecture: Previous MIM methods (Chang et al., 2023; 2022) predominantly utilized naive transformer architectures, potentially limiting their capabilities. We discovered that combination of multi-modal and single-modal transformer layers can significantly boost MIM training efficiency and performance. Language and vision representations are inherently different. The multi-modal transformer can effectively capture cross-modal interactions, extracting information from unpooled text representations and effectively bridging the gap between these distinct modalities. This allows the model to harness useful signals from noisy data. Additionally, subsequent single-modal transformer layers refine the visual representation, improving performance and training stability. Empirically, 1 : 2 ratio between these two types of transformer layers yields optimal performance. Advanced Positional Encoding & Masking Rate as Sampling Condition: We incorporate Rotary Position Embedding (RoPE) (Su et al., 2024) for encoding positional information in queries and keys, which helps maintain detail in high-resolution images. RoPE effectively addresses the issue of context disassociation in transformers as the number of tokens increases. Traditional absolute positional encoding methods lead to distortions and loss of detail at 512 512 resolutions, whereas RoPE significantly mitigates these issues. Additionally, we introduce the masking rate as dynamic sampling condition throughout the generation process. Previous MIM methods (Chang et al., 2023; 2022) have overlooked this aspect, resulting in suboptimal image details. This issue arises because the number of tokens predicted by the MIM model changes dramatically throughout the sampling loop. With the masking rate condition, the model can ascertain the current stage of the sampling period by leveraging conditional information from the masking rate. Note that merely relying on attention masks is insufficient to bridge this gap. We achieve effective conditional encoding by discretizing the continuous masking rate into 1000 levels. This approach enables the model to adapt to different stages of the sampling process, significantly improving image detail and overall quality. Beyond these architectural improvements, to achieve comparable performance with SDXL for highresolution generation, we adopt effects in three additional aspects: High-Quality Training Data: The quality of training data is crucial. While LAION (Schuhmann et al., 2022) offers diverse visual dataset, its captions can be subpar (Chen et al., 2024). We curated high-quality internal dataset with accurate captions, which, combined with our training strategy, significantly improved the generative capabilities of the base model. Micro-Conditioning: We identified that incorporating original image resolution, crop coordinates, and human preference score (Wu et al., 2023) as micro-conditions greatly enhances model stability during high-resolution aesthetic training. Feature Compression Layers: To efficiently generate high-resolution images, we integrated feature compression layers, maintaining computational efficiency even at 1024 1024 resolution. Our contributions culminate in Meissonic, next-generation T2I model based on masked discrete image token modeling. Unlike larger diffusion models such as SDXL (Podell et al., 2024) and DeepFloyd-XL (Liu et al., 2024a), Meissonic, with just 1B parameters, offers comparable or superior 10241024 high-resolution, aesthetically pleasing images while being able to run on consumergrade GPUs with only 8GB VRAM without the need for any additional model optimizations. Moreover, Meissonic effortlessly generates images with solid-color backgrounds, feature that usually demands model fine-tuning or noise offset adjustments in diffusion models. Advancement of Meissonic represents significant stride towards high-resolution, efficient, and accessible T2I MIM models. We evaluate Meissonic using various qualitative and quantitative metrics, 3 Technical Report including HPS, MPS, GenEval benchmarks, and GPT4o assessments, demonstrating its superior performance and efficiency."
        },
        {
            "title": "2 METHOD",
            "content": "2.1 MOTIVATION Recent breakthroughs in text-to-image synthesis have been largely propelled by diffusion models, such as Stable Diffusion XL, which have set de facto standards for image quality, detail, and conceptual fidelity. However, these models operate significantly differently from autoregressive language models, posing challenges in achieving unified approach for vision and language tasks. This divergence not only complicates the integration of these modalities but also highlights the need for innovative methods that can bridge the gap between them. In particular, non-autoregressive Masked Image Modeling (MIM) techniques, exemplified by MaskGIT and MUSE, have shown potential for efficient image generation. Yet, despite their promise, MIM approaches face two critical limitations: (a) Resolution Constraint. Current MIM methods are limited to generating images at maximum resolution of 512 512 pixels. This limitation hinders their broader adoption and advancement, particularly as the text-to-image synthesis community increasingly adopts 1024 1024 resolution as the standard. (b) Performance Gap. Existing MIM techniques have not yet achieved the level of performance exhibited by leading diffusion models like SDXL. They notably underperform in key areas such as image quality, intricate detailing, and conceptual representation, which are critical for practical applications. These challenges necessitate the exploration of new approaches. Our objective is to empower MIM to efficiently generate high-resolution images (e.g., 1024 1024), while narrowing the gap with toptier diffusion models, and ensuring computational efficiency suitable for consumer-grade hardware. Through our work, Meissonic, we aim to push the boundaries of MIM methods and bring them to the forefront of text-to-image synthesis. 2.2 MODEL ARCHITECTURE The Meissonic model is architected to facilitate efficient high-performance text-to-image synthesis through an integrated framework comprising CLIP text encoder (Radford et al., 2021), vectorquantized (VQ) image encoder and decoder (Esser et al., 2021a), and multi-modal Transformer backbone. Figure 2 illustrates the overall structure of the model. Vector-quantized Image Encoder and Decoder. We employ VQ-VAE model (Esser et al., 2021a) to convert raw image pixels into discrete semantic tokens. This model comprises an encoder, decoder, and quantization layer that maps input images into sequences of discrete tokens using learned codebook. For an image of size , the encoded token size is , where represents the downsampling ratio. In our implementation, we utilize downsampling ratio of = 16 and codebook size of 8192, allowing 1024 1024 image to be encoded into sequence of 64 64 discrete tokens. Flexible and Efficient Text Encoder. Instead of using large language model encoders, such as T5XXL1 (Raffel et al., 2020) or LLaMa (Touvron et al., 2023), which are prevalent in previous works (Chen et al., 2024; Esser et al., 2024), we utilize single text encoder from the state-of-the-art CLIP model with latent dimension of 1024, and fine-tune for optimal T2I performance. While this decision may limit the models capacity to fully comprehend lengthy text prompts, our observations indicate that excluding large-scale text encoders like T5 does not diminish visual quality. Moreover, this approach significantly reduces GPU memory requirements and computational cost. Notably, offline extraction of T5 features would entail approximately 11 times more processing time and 6 times more storage than employing the CLIP text encoder, underscoring the efficiency of our design. 1Many works indicate that the T5 text encoder is the key factor in obtaining the ability to synthesize words, we still show the ability to synthesize letters in Figure 8. We leave this future improvement. 4 Technical Report Figure 2: The architecture of Meissonic. During the image generation process, discrete tokens are created randomly according to predefined schedule. Meissonic then applies masking and performs predictions over several steps to reconstruct all tokens and decode the resulting image. In the case of image editing, the original image is converted into discrete tokens, which are masked according to specified masking strategy. After series of processing steps, the masked tokens are reconstructed and utilized to decode the target image. Text prompt and other conditions are incorporated to control the synthesis process. represents the masking rate condition, and represents the micro conditions. Comp. and Decomp. denotes feature compression layers and feaure decompression layers, respectively. More details about Multi-modal Transformer Block can be found in Appendix E. Multi-modal Transformer Backbone for Masked Image Modeling. Our transformer architecture builds upon the Multi-modal Transformer framework (Sauer et al., 2024), incorporating sampling parameters to encode sampling parameters and Rotary Position Embeddings (RoPE) (Su et al., 2024) for spatial information encoding. We introduce feature compression layers to efficiently handle high-resolution generation with numerous discrete tokens. These layers compress embedding features from 64 64 to 32 32 before processing through the transformer, and followed by feature decompression layers to 6464, thereby alleviating computational burdens. To enhance training stability and mitigate the NaN Loss issue, we follow the training strategy from LLaMa (Touvron et al., 2023), implementing gradient clipping and checkpoint reloading during distributed training and integrating QK-Norm layers into the architecture. We elaborate on the designs of our transformer in the subsequent section. Diverse Micro Conditions. To augment generation performance, we incorporate additional conditions such as original image resolution, crop coordinates, and human preference score (Wu et al., 2023). These conditions are transformed into sinusoidal embeddings and concatenated as additional channels to the final pooled hidden states of the text encoder. Masking Strategy. Following the approach established in Chang et al. (2023), we employ variable masking ratio with cosine scheduling. Specifically, we randomly sample masking ratio [0, 1] from truncated arccos distribution characterized by the following density function: p(r) = 2 π (1 r2) 1 In contrast to autoregressive models that learn conditional distributions (xi x<i) for fixed token orders, our approach utilizes random masking with variable ratios to enable the model to learn (xi xΛ) for arbitrary subsets of tokens Λ. This flexibility is pivotal for our parallel sampling strategy and facilitates various zero-shot image editing capabilities, which will be demonstrated in Section 3. 2.3 MULTI-MODAL TRANSFORMER FOR MASKED IMAGE MODELING Meissonic employs the Multi-modal Transformer as its foundational architecture and innovatively customizes the modules to address the distinctive challenges inherent in high-resolution masked image modeling. We introduce several specialized designs for MIM as follows: 5 Technical Report Rotary Position Embeddings. RoPE (Su et al., 2024) has demonstrated exceptional performance within in LLMs (Su et al., 2024; Touvron et al., 2023; Ding et al., 2024; Bai et al., 2023). Some studies (Lu et al., 2024; Lin et al., 2023; Zhuo et al., 2024) have attempted to extend 1D RoPE (Su et al., 2024) to 2D or 3D for image diffusion models. Our findings reveal that, due to the high-quality image tokenizer used for converting images into discrete tokens, the original 1D RoPE yields promising results. This 1D RoPE facilitates seamless transition from the 256 256 stage to the 512 512 stage, simultaneously enhancing the generative performance of the model. Deeper Model with Single-modal Transformer. Although the Multi-modal Transformer block demonstrated commendable performance, our experiments reveal that reducing the number of multi-modal blocks to single-modal block configuration offers more stable and computationally efficient approach for training T2I models. Therefore, we opt to employ Multi-modal Transformer blocks in the initial stages of the network, transitioning to exclusively Single-modal Transformer blocks in the latter half. Our findings suggest an optimal block ratio of about 1:2. Micro Conditions with Human Preference Score. Our experiments reveal that incorporating three micro-conditions is pivotal for achieving stable and reliable High-resolution MIM Model: original image resolution, crop coordinates, and human preference score. The original image resolution effectively aids the model in implicitly filtering out low-quality data and learning the properties of high-quality, high-resolution data, while crop coordinates enhance training stability, likely due to improved consistency between image conditions and semantic conditions during cropped patch coordination. In the final stage, we leverage the Human Preference Score (Wu et al., 2023) to effectively enhance image quality, using signals provided by the Human Preference Model to guide the models outputs in mimicking and approximating human preferences. Feature Compression Layers. Existing multi-stage approaches, such as MUSE (Chang et al., 2023) and DeepFloyd-XL (DeepFloyd, 2023), employ cascading multiple subnetworks to achieve higher-resolution image generation. We argue that such multi-stage training introduces unnecessary complexity and hampers the generation of high-fidelity, highresolution images. Instead, we advocate integrating streamlined feature compression layers during the fine-tuning stage to facilitate efficient high-resolution generation process learning. This approach functions akin to lightweight high-resolution adapter (Guo et al., 2024), module extensively explored and integrated within Stable Diffusion. By incorporating 2D convolution-based feature compression layers into the transformer backbone, we compress the feature maps prior to the transformer layers and subsequently decompress them after the transformer layers, effectively addressing the challenges of efficiency and resolution transition. 2.4 TRAINING DETAILS et encoder Meissonic is constructed using CLIP-ViT-H-142 text (Ilharco et al., 2021), pre-trained VQ image encoder and decoder al., (Patil 2024), and customized Transformer-based (Esser et al., 2024) backbone. We employ classifier-free guidance (Ho & Salimans, 2022) and cross-entropy loss to train Meissonic. Training occurs resolution across both stages, three leveraging (CFG) Table 1: Comparison of training data and time for various models. Model Params (B) Training Images (M) 8A100 GPU Daysa Wurstchen (Pernias et al., 2024) SD-1.5 (Rombach et al., 2022b) SD-2.1 (Rombach et al., 2022b) Imagen (Saharia et al., 2022) Dall-E 2 (Ramesh et al., 2022) GigaGAN (Kang et al., 2023) SDXL (Podell et al., 2024) 1.0 0.9 0.9 3.0 6.5 0.9 2.6 1420 4800 3900 860 650 980 unknown 128.1 781.2 1041.6 891.5 5208.3 597.8 unknown 19b Meissonic Data collected from Sehwag et al. (2024). FP16 Tensor Core of A100 is 312 TFLOPS and H100 is 989 TFLOPS. GPU 210 1.0 hours are adjusted from 48 H100 days based on this rate. 2We utilize laion/CLIP-ViT-H-14-laion2B-s32B-b79K from OpenCLIP as our initial weights. Technical Report Table 2: HPS v2.0 benchmark. Scores are collected from https://github.com/tgxs002/ HPSv2. We highlight the best. HPS v2.0 Model Animation Concept-art Painting Photo Averaged GLIDE (Nichol et al., 2022) LAFITE (Zhou et al., 2022) VQ-Diffusion (Gu et al., 2022) Latent Diffusion (Rombach et al., 2022b) DALLE mini VQGAN + CLIP (Esser et al., 2021b) CogView2 (Ding et al., 2022) Versatile Diffusion (Xu et al., 2023) DALLE 2 (Ramesh et al., 2022) Stable Diffusion v1.4 (Rombach et al., 2022a) Stable Diffusion v2.0 (Rombach et al., 2022a) Epic Diffusion DeepFloyd-XL (DeepFloyd, 2023) Openjourney MajicMix Realistic ChilloutMix Deliberate (Desync, 2024) SDXL Base 0.9 (Podell et al., 2024) Realistic Vision (SG 161222, 2024) SDXL Refiner 0.9 (Podell et al., 2024) Dreamlike Photoreal 2.0 (Art, 2023) SDXL Base 1.0 (Podell et al., 2024) SDXL Refiner 1.0 (Podell et al., 2024) Meissonic-512 Meissonic 23.34 24.63 24.97 25.73 26.10 26.44 26.50 26.59 27.34 27.26 27.48 27.57 27.64 27.85 27.88 27.92 28.13 28.42 28.22 28.45 28.24 28.88 28.93 28.90 29.57 23.08 24.38 24.70 25.15 25.56 26.53 26.59 26.28 26.54 26.61 26.89 26.96 26.83 27.18 27.19 27.29 27.46 27.63 27.53 27.66 27.60 27.88 27.89 28.15 28.58 23.27 24.43 25.01 25.25 25.56 26.47 26.33 26.43 26.68 26.66 26.86 27.03 26.86 27.25 27.22 27.32 27.45 27.60 27.56 27.67 27.59 27.92 27. 28.22 28.72 24.50 25.81 25.71 26.97 26.12 26.12 26.44 27.05 27.24 27.27 27.46 27.49 27.75 27.53 27.64 27.61 27.62 27.29 27.75 27.46 27.99 28.31 28.38 28.04 28.45 23.55 24.81 25.10 25.78 25.83 26.39 26.47 26.59 26.95 26.95 27.17 27.26 27.27 27.45 27.48 27.54 27.67 27.73 27.77 27.80 27.86 28.25 28.27 28.33 28.83 public datasets and our curated data. First, we train Meissonic-256 with batch size of 2,048 for 100,000 steps. Second, we continue training Meissonic-512 with batch size of 512 for an additional 100,000 steps. Third, we continue training Meissonic with batch size of 256 for 42,000 steps with resolution of 1024 1024. The performance results of Meissonic-512 and Meissonic are reported in Table 2. All experiments are carried out with fixed learning rate of 1 104 except Stage 4. Further details are elaborated in Section 2.5. All inferences in this paper are performed with CFG = 9 and 48 steps. Its crucial to highlight the resource efficiency of our training process. Our training is considerably more resource-efficient compared to Stable Diffusion (Podell et al., 2023). Meissonic is trained in approximately 48 H100 GPU days, demonstrating that production-ready image synthesis foundation model can be developed with considerably reduced computational costs. Additional details on this comparison can be found in Table 1. 2.5 PROGRESSIVE AND EFFICIENT TRAINING STAGE DECOMPOSITION Our approach systematically decomposes the training process into four carefully designed stages, allowing us to progressively build and refine the models generative capabilities. These stages, combined with precise enhancements to specific components, contribute to continual improvements in synthesis quality. Given that SDXL has not disclosed details regarding its training data, our experience is particularly valuable for guiding the community in constructing SDXL-level text-toimage models. We present images generated by Meissonic at each of the four training stages in Appendix to support our claims. Stage 1: Understanding Fundamental Concepts from Extensive Data. Previous studies (Chen et al., 2024; Yu et al., 2024) indicate that raw captions from LAION are insufficient for training textto-image models, often requiring the caption refinement provided by MLLMs such as LLaVA (Liu et al., 2024b). However, this solution is computationally demanding and time-intensive. While some studies (Chen et al., 2024; Sehwag et al., 2024) utilize the extensively annotated SA-10M (Kirillov et al., 2023) dataset, our findings reveal that SA-10M does not comprehensively cover fundamental 7 Technical Report Figure 3: Qualitative Comparisons with SD 1.5, SD 2.1, DeepFloyd-XL, Deliberate, and SDXL. concepts, particularly regarding human faces. Thus, we adopt balanced strategy that leverages the original high-quality LAION data for foundational concepts learning in the initial training phase, utilizing reduced resolution to enhance efficiency. Specifically, we carefully curated the deduplicated LAION-2B dataset by filtering out images with aesthetic scores below 4.5, watermark probabilities exceeding 50%, and other criteria outlined in Kolors (2024). This meticulous selection resulted in approximately 200 million images, which were employed for training at resolution of 256 256 in this initial stage. Stage 2: Aligning Text and Images with Long Prompts. In the first stage, our approach does not rely on high-quality image-text paired data. Therefore, in the second stage, we focus on improving the models capability to interpret long, descriptive prompts. We filtered the initial LAION set more rigorously, retaining only images with aesthetic scores above 8, and other criteria outlined in Kolors (2024). Additionally, we incorporate 1.2 million synthetic image-text pairs with refined captions exceeding 50 words, primarily derived from publicly available high-quality synthetic datasets, complemented by additional high-quality images from our internal 6 million dataset. This aggregation results in around 10 million image-text pairs. Notably, we maintain the model architecture while increasing the training resolution to 512 512, enabling the model to capture more intricate image details. We observed significant boost in the models ability to capture abstract concepts and respond accurately to complex prompts, including diverse styles and fantasy characters. Stage 3: Mastering Feature Compression for Higher-resolution Generation. High-resolution generation remains an unexplored area within MIM (Chang et al., 2023; 2022; Patil et al., 2024). Unlike methods such as MUSE (Chang et al., 2023) or DeepFloyd-XL (DeepFloyd, 2023), which rely on external super-resolution (SR) modules, we demonstrate that efficient 10241024 generation is feasible through feature compression for MIM. By introducing feature compression layers, we achieve seamless transition from 512512 to 10241024 generation with minimal computational cost. In this stage, we further refine the dataset by filtering based on resolution and aesthetic score, selecting approximately 100K high-quality, high-resolution image-text pairs from the LAION subset utilized in Stage 2. This, combined with the remaining high-quality data, results in approximately 6 million samples for training at 1024 resolution. Stage 4: Refining High-Resolution Aesthetic Image Generation. In the final stage, we fine-tune the model using small learning rate, without freezing the text encoder, and incorporate human Technical Report preference score as micro condition. This can significantly enhance the models performance in high-resolution image generation. This targeted adjustment significantly enhances the models performance in generating high-resolution images, while also improving diversity. The training data remains the same as in Stage 3. Table 3: GenEval benchmark. We highlight the best result. Model Overall DALL-E mini SD v1.5 SD v2.1 DALL-E 2 SD XL Meissonic 0.23 0.43 0.50 0.52 0.55 0.54 Objects Single Two 0.73 0.97 0.98 0.94 0. 0.99 0.11 0.38 0.51 0.66 0.74 0.66 Counting Colors Position Color Attribution 0.12 0.35 0.44 0.49 0.39 0. 0.37 0.76 0.85 0.77 0.85 0.86 0.02 0.04 0.07 0.10 0.15 0.10 0.01 0.06 0.17 0.19 0.23 0."
        },
        {
            "title": "3 RESULTS",
            "content": "3.1 QUANTATIVE COMPARISON Classic evaluation metrics for image generation models, such as FID and CLIP Score, have limited relevance to visual aesthetics, as highlighted by Podell et al. (2024); Chen et al. (2024); Kolors (2024); Sehwag et al. (2024). Therefore, we report our models performances using Human Preference Score v2 (HPSv2) (Wu et al., 2023), GenEval (Ghosh et al., 2024), and Multi-Dimensional Human Preference Score (MPS)3 (Zhang et al., 2024b), as illustrated in Table 2,3,6. Table 6: MPS scores on RealUser-800 Prompts. We highlight the best result. Model VQ-Diffusion (Gu et al., 2022) Latent Diffusion (Rombach et al., 2022b) DALLE mini (Dayma et al., 2021) VQGAN + CLIP (Esser et al., 2021b) CogView2 (Ding et al., 2022) Versatile Diffusion (Xu et al., 2023) Stable Diffusion v1.4 (Rombach et al., 2022a) Stable Diffusion v2.0 (Rombach et al., 2022a) DeepFloyd-XL (DeepFloyd, 2023) SDXL Base 0.9 (Podell et al., 2024) SDXL Refiner 0.9 (Podell et al., 2024) SDXL Base 1.0 (Podell et al., 2024) SDXL Refiner 1.0 (Podell et al., 2024) MPS 9.70 10.56 11.32 11.50 12.39 12.61 13.89 14.39 15.22 16.37 16.64 16.46 16. In our pursuit of making Meissonic accessible to the broader community, we optimized our model to 1 billion parameters, ensuring that it runs efficiently on 8GB VRAM, making inference and fine-tuning both convenient. Figure 4 provides comparative analysis of GPU memory consumption4 across different inference batch sizes against SDXL. Additionally, Figure 5 details the inference time per step5. Meissonic 17.34 We also present qualitative comparisons of image quality and text-image alignment in Figure 3, with additional comparisons provided in the Appendix I. Furthermore, Figure 5 illustrates Meissonics proficiency in generating text-driven style art image. To complement these analyses, we conduct human evaluation by K-Sort Arena (Li et al., 2024) with internal checkpoint, we also conduct GPT-4o to evaluate the performance between Meissonic and other models in Figure 4. All Figures and Tables demonstrate that Meissonic achieves competitive performance in human performance and text alignment compared to DALL-E 2 and SDXL, as well as showcasing its efficiency. 3Given that the KolorsPrompts benchmark was unavailable, we curated diverse prompt dataset consisting of 800 real user-generated prompts spanning various concepts and themes for the MPS evaluation. 4GPU memory usage was gauged using torch.cuda.memory reserved(). While this method might yield higher values, all models are measured under identical settings to maintain fairness. 5Inference time is assessed using an A100 GPU with fp16 models. Notably, the reported times contributions from the VAE and text encoder, meaning that multi-step inferences do not scale linearly. 9 Technical Report Table 4: GPU Memory Cost for for Different Models and Batch Sizes. Table 5: Models and Batch Sizes. Inference Time Comparison for Different Figure 4: GPT4o Preference Evaluation of Meissonic against current open Text-to-image Models. 3.2 ZERO-SHOT IMAGE-TO-IMAGE EDITING Model CLIP-I CLIP-T DINO InstructPix2Pix (Brooks et al., 2023) MagicBrush (Zhang et al., 2024a) PnP (Tumanyan et al., 2023) Null-Text Inv. (Mokady et al., 2023) EMU-Edit (Sheynin et al., 2024) Meissonic image editing tasks, we For benchmark Meissonic against state-of-the-art models using the EMU-Edit dataset (Sheynin et al., 2024), which includes operations: seven different combackground alteration, prehensive changes, image style alteration, object removal, object addition, localized modifications, color/texture and alterations. We present results in Table 7. Additionally, examples from our internal image editing dataset6, including mask-guided editing in Figure 6 and mask-free editing in Figure 7, further showcase Meissonics versatility. Remarkably, Meissonic achieved this performance without any training or fine-tuning on image editing-specific data or instruction dataset. Table 7: Results on the EMU-Edit (Sheynin et al., 2024) test set. We highlight the best result. 0.762 0.776 0.153 0.678 0.819 0.760 0.834 0.838 0.521 0.761 0.859 0.871 0.219 0.222 0.089 0.236 0.231 0.266 6This dataset will be released in separate work. 10 Technical Report Figure 5: Evaluating the ability to generate diverse styles. The enlarged samples of (d) Meissonic are provided in Appendix H. Prompt: garden full of [Y] illustrated in [X] style. Figure 6: Examples of image editing with mask on internal Image Editing Dataset"
        },
        {
            "title": "4 CONCLUSION AND IMPACT",
            "content": "In this work, we have significantly advanced masked image modeling (MIM) for text-to-image (T2I) synthesis by introducing several key innovations: transformer architecture blends multi-modal and single-modal layers, advanced positional encoding strategies, and an adaptive masking rate as the sampling condition. These innovations, coupled with high-quality curated training data, progressive and efficient training stage decomposition, micro-conditions, and feature compression layers, have 11 Technical Report Figure 7: Examples of image inpainting, outpainting, and mask-free image editing on internal Image Editing Dataset culminated in Meissonic, 1B parameter model that outperforms larger diffusion models in highresolution, aesthetically pleasing image generation while remaining accessible on consumer-grade GPUs. Our evaluations demonstrate Meissonics superior performance and efficiency, marking significant step towards accessible and efficient high-resolution non-autoregressive MIM T2I models. Boarder Impact. Recently, offline text-to-image applications on mobile devices have emerged, such as Pixel Studio from Google Pixel 9 and Image Playground from Apple iPhone 16. These innovations reflect growing trend toward enhancing user experience and privacy. As pioneering resource-efficient foundation model, Meissonic represents significant advancement in this field, delivering state-of-the-art image synthesis capabilities with strong emphasis on user privacy and offline functionality. This development not only empowers users with creative tools but also ensures the security of sensitive data, marking notable leap forward in mobile imaging technology. Acknowledgement. We would like to express our gratitude to all those who contributed their time, expertise, and insights during the development of Meissonic. Listed in no particular order: Jingjing Ren, Sixiang Chen from HKUST(GZ), Wenhao Chai from University of Washington, Donghao Zhou from CUHK, and other anonymous friends. We are profoundly grateful for their commitment and the unique perspectives they brought to this project. 12 Technical Report"
        },
        {
            "title": "A MODEL NAME ORIGIN",
            "content": "The name Meissonic is derived from combination of the renowned French painter Ernest Meissonier and the term sonic. Ernest Meissonier is celebrated for his meticulous attention to detail and his ability to capture dynamic moments in art. The addition of sonic evokes sense of speed and modernity, highlighting the models capabilities in efficient image synthesis and transformation."
        },
        {
            "title": "B RELATED WORK",
            "content": "Diffusion-based Image Generation. Diffusion models have achieved remarkable advances in image generation, with notable contributions like Stable Diffusion (Rombach et al., 2022b), and the more recent SDXL (Podell et al., 2024), often driven by large-scale datasets. These models move beyond pixel-level operations by working within compressed latent spaces, forming what we now recognize as latent diffusion models (Luo et al., 2023; Podell et al., 2024; Wu et al., 2024a; Shi et al., 2024; Zhou et al., 2024; Yi et al., 2024; Wu et al., 2024b). SDXL represents significant leap in this domain, introducing micro-conditions and multi-aspect training to gain greater control over image generation, which has inspired wide range of derivative models in the community, such as Deliberate (Desync, 2024) and RealVisXL (SG 161222, 2024). The integration of transformer architectures has also become more prevalent, with models like DiT (Peebles & Xie, 2023) and U-ViT (Bao et al., 2023) demonstrating the potential of diffusion transformers in this field. SD3 (Esser et al., 2024), which combines diffusion transformers with flow matching at an impressive scale of 8B parameters, underscores the scalability and potential of the multimodal transformer-based diffusion backbone. Despite these advances, diffusion models still face challenges, particularly their reliance on acceleration techniques (Sauer et al., 2023; Luo et al., 2023; Yin et al., 2024) to speed up inference, making them cumbersome for real-time applications. Additionally, the quantization of diffusion transformers has proven less straightforward than with large language models (Li et al., 2023). The research community continues to explore better paradigms for image generation. Addressing these limitations, our work aims to contribute an efficient, high-quality alternative in the form of Meissonic. Token-based Image Generation. Token-based autoregressive transformers (Lee et al., 2022; Chen et al., 2018; Yu et al., 2022b), first validated by VQ-GAN (Esser et al., 2021b), have shown considerable promise for image generation. However, these methods are inherently computationally demanding, requiring the prediction of hundreds to thousands of tokens to form single image. As pioneering work, MaskGIT (Chang et al., 2022) challenged this paradigm by introducing masked image modeling (MIM) approach, achieving competitive fidelity and diversity in class-conditional image generation. Building on this, MUSE (Chang et al., 2023) extended MIM to text-to-image synthesis, scaling up to 3B parameters and achieving remarkable performance. MUSE demonstrates the viability of non-autoregressive token-based models, but it encountered limitations in generating high-resolution images, capping at 512512, and lagging behind SDXL (Podell et al., 2023) in terms of fidelity and text-image alignment. Meissonic advances the performance of token-based models beyond what latent diffusion methods have achieved, effectively pushing the envelope in terms of both quality and resolution in the text-to-image synthesis landscape with the MIM method. Figure 8: Zero-shot generation of stylized letters. Meissonic can synthesize individual letters to form the word MEISSONIC. Prompt: post featuring [COLOR] [LETTER] painted on top."
        },
        {
            "title": "C APPLICATIONS",
            "content": "We present the letter synthesis capability of Meissonic in Figure 8. 13 Technical Report Figure 9: Memes generated by Meissonic. Figure 10: Cartoon Stickers generated by Meissonic. We present the combination capability of complex concepts of Meissonic in Figure 1. We present meme generation in Figure 9. We present cartoon sticker generation in Figure 10."
        },
        {
            "title": "D ABLATION STUDY",
            "content": "Detailed roadmap to build Meissonic. We present ablation studies during training Meissonic-512 in Table. 11. The HPS v2.1 (Wu et al., 2023) scores are calculated for verifying the effectiveness of each compoment. Our ablations are based on training stage 2, ensuring consistency with the training dataset scale, model scale, and other training configurations. Figure 11: HPS v2.1 Score on internal 1000 prompts 14 Technical Report Figure 12: Multi-modal Transformer For Meissonic. Figure 13: Word cloud image of our RealUser-800 prompts benchmark."
        },
        {
            "title": "E MULTIMODAL TRANSFORMER BLOCK FOR MEISSONIC",
            "content": "We present detailed structure of our Multi-modal Transformer Block for Meissonic in Figure 12. Specifically, denotes image embedding inputs, denotes text embedding inputs, and denotes conditions inputs. 15 Technical Report Figure 14: Images generated using the same prompt across Meissonics four training stages. The resolutions for stages 1 and 2 are 2562 and 5122, respectively, while stages 3 and 4 are 10242. For clarity and comparison, all images are displayed in consistent layout. 16 Technical Report WORD CLOUD OF OUR REALUSER800 BENCHMARK We present word cloud image that illustrates the diverse concepts, styles, and themes encompassed within our RealUser-800 prompts benchmark in Figure 13."
        },
        {
            "title": "G IMAGES GENERATED DURING DIFFERENT TRAINING STAGES",
            "content": "We present images generated using the same prompt across Meissonics four training stages in Figure 14."
        },
        {
            "title": "H ENLARGED EXAMPLES FROM GENERATING DIVERSE STYLES",
            "content": "We present enlarged samples from Figure 5 (d) Meissonic in Figure 15. Figure 15: Enlarged Examples from generating diverse styles with Meissonic. Prompt: garden full of [Y] illustrated in [X] style."
        },
        {
            "title": "I MORE EXAMPLES OF QUALITATIVE COMPARISONS",
            "content": "We present more examples of qualitative comparisons in Figure 16."
        },
        {
            "title": "J MORE IMAGES PRODUCED BY MEISSONIC",
            "content": "We present additional images generated by Meissonic using CC3M (Sharma et al., 2018) items, with detailed captions provided by VILA-1.5 (Lin et al., 2023) and Morph (Pan et al., 2024). These images can be found in Figure 17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35. We present additional images generated by Meissonic using HPS (Wu et al., 2023) benchmark prompts. These images can be found in Figure 36,37,38,39,40,41. 17 Technical Report Figure 16: Qualitative Comparisons with SD 1.5, SD 2.1, DeepFloyd-XL, Deliberate, and SDXL. Technical Report Figure 17: High Quality Samples Produced by Meissonic. 19 Technical Report Figure 18: High Quality Samples Produced by Meissonic. Technical Report Figure 19: High Quality Samples Produced by Meissonic. 21 Technical Report Figure 20: High Quality Samples Produced by Meissonic. Technical Report Figure 21: High Quality Samples Produced by Meissonic. 23 Technical Report Figure 22: High Quality Samples Produced by Meissonic. Technical Report Figure 23: High Quality Samples Produced by Meissonic. 25 Technical Report Figure 24: High Quality Samples Produced by Meissonic. Technical Report Figure 25: High Quality Samples Produced by Meissonic. 27 Technical Report Figure 26: High Quality Samples Produced by Meissonic. Technical Report Figure 27: High Quality Samples Produced by Meissonic. 29 Technical Report Figure 28: High Quality Samples Produced by Meissonic. Technical Report Figure 29: High Quality Samples Produced by Meissonic. 31 Technical Report Figure 30: High Quality Samples Produced by Meissonic. Technical Report Figure 31: High Quality Samples Produced by Meissonic. 33 Technical Report Figure 32: High Quality Samples Produced by Meissonic. Technical Report Figure 33: High Quality Samples Produced by Meissonic. 35 Technical Report Figure 34: High Quality Samples Produced by Meissonic. Technical Report Figure 35: High Quality Samples Produced by Meissonic. 37 Technical Report Figure 36: High Quality Samples Produced by Meissonic. Technical Report Figure 37: High Quality Samples Produced by Meissonic. 39 Technical Report Figure 38: High Quality Samples Produced by Meissonic. Technical Report Figure 39: High Quality Samples Produced by Meissonic. 41 Technical Report Figure 40: High Quality Samples Produced by Meissonic. Technical Report Figure 41: High Quality Samples Produced by Meissonic. 43 Technical Report"
        },
        {
            "title": "REFERENCES",
            "content": "Dreamlike Art. Dreamlike photoreal 2.0. https://huggingface.co/dreamlike-art/ dreamlike-photoreal-2.0, 2023. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth In Proceedings of the IEEE/CVF conference on words: vit backbone for diffusion models. computer vision and pattern recognition, pp. 2266922679, 2023. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1839218402, 2023. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1131511325, 2022. Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. Junsong Chen, Jincheng YU, Chongjian GE, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-$alpha$: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In The Twelfth International Conference on Learning Representations (ICLR), 2024. Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. Pixelsnail: An improved auIn International conference on machine learning (ICML), pp. toregressive generative model. 864872. PMLR, 2018. Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah, Tanishq Abraham, Phuc Le Khac, Luke Melas, and Ritobrata Ghosh. Dall mini. https://huggingface.co/spaces/ dallemini/dalle-mini, 2021. IF DeepFloyd. Deepfloyd if, 2023. https://huggingface.co/DeepFloyd, 2023. Desync. Perfect deliberate. perfectdeliberate, 2024. https://civitai.com/models/24350/ Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via hierarchical transformers. Advances in Neural Information Processing Systems (NeurIPS), 35:1689016902, 2022. Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens. arXiv preprint arXiv:2402.13753, 2024. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1287312883, 2021a. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pp. 1287312883, 2021b. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 44 Technical Report Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pp. 1069610706, 2022. Lanqing Guo, Yingqing He, Haoxin Chen, Menghan Xia, Xiaodong Cun, Yufei Wang, Siyu Huang, Yong Zhang, Xintao Wang, Qifeng Chen, et al. Make cheap scaling: self-cascade diffusion model for higher-resolution adaptation. arXiv preprint arXiv:2402.10491, 2024. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1012410134, 2023. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 40154026, 2023. Kolors. Kolors: Effective training of diffusion model for photorealistic text-to-image synthesis. arXiv preprint, 2024. Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1152311532, 2022. Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. Q-diffusion: Quantizing diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1753517545, 2023. Zhikai Li, Xuewen Liu, Dongrong Fu, Jianquan Li, Qingyi Gu, Kurt Keutzer, and Zhen Dong. K-sort arena: Efficient and reliable benchmarking for generative models via k-wise human preferences. arXiv preprint arXiv:2408.14468, 2024. Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023. Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-to-image alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695, 2024a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b. Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, Xihui Liu, Wanli Ouyang, and Lei Bai. Fit: Flexible vision transformer for diffusion model. arXiv preprint arXiv:2402.12376, 2024. Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 60386047, 2023. 45 Technical Report Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and In International Conference on Machine Learning editing with text-guided diffusion models. (ICML), pp. 1678416804. PMLR, 2022. Kaihang Pan, Siliang Tang, Juncheng Li, Zhaoyu Fan, Wei Chow, Shuicheng Yan, Tat-Seng Chua, Yueting Zhuang, and Hanwang Zhang. Auto-encoding morph-tokens for multimodal llm. arXiv preprint arXiv:2405.01926, 2024. Suraj Patil, William Berman, Robin Rombach, and Patrick von Platen. amused: An open muse reproduction, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Pablo Pernias, Dominic Rampas, Mats Leon Richter, Christopher Pal, and Marc Aubreville. In The Wurstchen: An efficient architecture for large-scale text-to-image diffusion models. Twelfth International Conference on Learning Representations (ICLR), 2024. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations (ICLR), 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1068410695, June 2022a. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pp. 1068410695, 2022b. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems (NeurIPS), 35:3647936494, 2022. Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023. Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. arXiv preprint arXiv:2403.12015, 2024. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text In Thirty-sixth Conference on Neural Information Processing Systems Datasets and models. Benchmarks Track, 2022. 46 Technical Report Vikash Sehwag, Xianghao Kong, Jingtao Li, Michael Spranger, and Lingjuan Lyu. Stretching each dollar: Diffusion training from scratch on micro-budget. arXiv preprint arXiv:2407.15811, 2024. SG 161222. Realvisxl v5.0. realvisxl-v50, 2024. https://civitai.com/models/139562/ Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL, 2018. Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8871 8879, 2024. Qingyu Shi, Lu Qi, Jianzong Wu, Jinbin Bai, Jingbo Wang, Yunhai Tong, Xiangtai Li, and MingarXiv Hsuan Yang. Relationbooth: Towards relation-aware customized object generation. preprint, 2024. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19211930, 2023. Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. Motionbooth: Motion-aware customized text-to-video generation. Advances in Neural Information Processing Systems (NeurIPS), 2024a. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-toimage synthesis. arXiv preprint arXiv:2306.09341, 2023. Zike Wu, Pan Zhou, Xuanyu Yi, Xiaoding Yuan, and Hanwang Zhang. Consistent3d: Towards consistent high-fidelity text-to-3d generation with deterministic sampling prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 98929902, 2024b. Xingqian Xu, Zhangyang Wang, Gong Zhang, Kai Wang, and Humphrey Shi. Versatile diffusion: Text, images and variations all in one diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR), pp. 77547765, 2023. Xuanyu Yi, Zike Wu, Qingshan Xu, Pan Zhou, Joo-Hwee Lim, and Hanwang Zhang. Diffusion timestep curriculum for one image to 3d generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 99489958, 2024. Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. arXiv preprint arXiv:2405.14867, 2024. Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQGAN. In International Conference on Learning Representations (ICLR), 2022a. 47 Technical Report Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for contentrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022b. Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Yue Cao, Xinlong Wang, and Jingjing Liu. Capsfusion: Rethinking image-text data at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1402214032, 2024. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36, 2024a. Sixian Zhang, Bohan Wang, Junqiang Wu, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan Wang. Learning multi-dimensional human preference for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 80188027, 2024b. Donghao Zhou, Jiancheng Huang, Jinbin Bai, Jiaze Wang, Hao Chen, Guangyong Chen, Xiaowei Hu, and Pheng-Ann Heng. Magictailor: Component-controllable personalization in text-to-image diffusion models. arXiv preprint arXiv, 2024. Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. Towards language-free training for text-to-image generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pp. 1790717917, 2022. Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. arXiv preprint arXiv:2406.18583, 2024."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "HKUST",
        "HKUST(GZ)",
        "Skywork AI",
        "UC Berkeley",
        "Zhejiang University"
    ]
}