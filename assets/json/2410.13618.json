{
    "paper_title": "LoLDU: Low-Rank Adaptation via Lower-Diag-Upper Decomposition for Parameter-Efficient Fine-Tuning",
    "authors": [
        "Yiming Shi",
        "Jiwei Wei",
        "Yujia Wu",
        "Ran Ran",
        "Chengwei Sun",
        "Shiyuan He",
        "Yang Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid growth of model scale has necessitated substantial computational resources for fine-tuning. Existing approach such as Low-Rank Adaptation (LoRA) has sought to address the problem of handling the large updated parameters in full fine-tuning. However, LoRA utilize random initialization and optimization of low-rank matrices to approximate updated weights, which can result in suboptimal convergence and an accuracy gap compared to full fine-tuning. To address these issues, we propose LoLDU, a Parameter-Efficient Fine-Tuning (PEFT) approach that significantly reduces trainable parameters by 2600 times compared to regular PEFT methods while maintaining comparable performance. LoLDU leverages Lower-Diag-Upper Decomposition (LDU) to initialize low-rank matrices for faster convergence and orthogonality. We focus on optimizing the diagonal matrix for scaling transformations. To the best of our knowledge, LoLDU has the fewest parameters among all PEFT approaches. We conducted extensive experiments across 4 instruction-following datasets, 6 natural language understanding (NLU) datasets, 8 image classification datasets, and image generation datasets with multiple model types (LLaMA2, RoBERTa, ViT, and Stable Diffusion), providing a comprehensive and detailed analysis. Our open-source code can be accessed at \\href{https://github.com/SKDDJ/LoLDU}{https://github.com/SKDDJ/LoLDU}."
        },
        {
            "title": "Start",
            "content": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 LoLDU: Low-Rank Adaptation via Lower-Diag-Upper Decomposition for Parameter-Efficient Fine-Tuning Yiming Shi, Jiwei Wei, Yujia Wu, Ran Ran, Chengwei Sun, Shiyuan He, Yang Yang 4 2 0 2 7 ] . [ 1 8 1 6 3 1 . 0 1 4 2 : r AbstractThe rapid growth of model scale has necessitated substantial computational resources for fine-tuning. Existing approach such as Low-Rank Adaptation (LoRA) has sought to address the problem of handling the large updated parameters in full fine-tuning. However, LoRA utilize random initialization and optimization of low-rank matrices to approximate updated weights, which can result in suboptimal convergence and an accuracy gap compared to full fine-tuning. To address these issues, we propose LoLDU, Parameter-Efficient Fine-Tuning (PEFT) approach that significantly reduces trainable parameters by 2600 times compared to regular PEFT methods while maintaining comparable performance. LoLDU leverages LowerDiag-Upper Decomposition (LDU) to initialize low-rank matrices for faster convergence and orthogonality. We focus on optimizing the diagonal matrix for scaling transformations. To the best of our knowledge, LoLDU has the fewest parameters among all PEFT approaches. We conducted extensive experiments across 4 instruction-following datasets, 6 natural language understanding (NLU) datasets, 8 image classification datasets, and image generation datasets with multiple model types (LLaMA2, RoBERTa, ViT, and Stable Diffusion), providing comprehensive and detailed analysis. Our open-source code can be accessed at https://github.com/SKDDJ/LoLDU. Index TermsParameter-Efficient Fine-Tuning, Low-Rank Adaptation, Domain Adaptation, Large Models I. INTRODUCTION"
        },
        {
            "title": "W ITHIN the era of exponentially increasing the scale",
            "content": "of models, fine-tuning these large models for new domains (e.g., Visual Instruction Tuning [1]), applying advanced learning techniques (e.g., Representation Learning [2] [4]), or adapting to downstream tasks (e.g., Text-to-Image Customization [5], [6], Object Tracking [7], [8]) requires substantial computational resources. To address this challenge, Parameter-Efficient Fine-Tuning (PEFT) techniques such as LoRA [9], VeRA [10],QLoRA [11], and PiSSA [12] have been developed to mitigate the bottleneck by reducing the number of trainable parameters, memory (VRAM), and storage costs. Despite advancements in PEFT, the process of fine-tuning large models remains prohibitively expensive in terms of both computational resources and storage requirements. For Yiming Shi, Jiwei Wei, Yujia Wu, Ran Ran, Chengwei Sun, Shiyuan He and Yang Yang are with the Center for Future Media and School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu 611731, China (e-mail: yimingshi666@gmail.com; mathematic6@gmail.com; ranran@std.uestc.edu.cn; suncw10@126.com). 202322080314@std.uestc.edu.cn; Corresponding author: Jiwei Wei. Email: mathematic6@gmail.com. 1Kindly note that the parameter count reported does not include the classification head, as it must be trained using all methods. Figure 1. Performance vs log-scaled trainable parameters for FGVC (left) and StanfordCars (right) on ViT Base. Our LoLDU methods with = {1, 8, 16, 32, 64, 128, 256, 512, 768} exhibit superior parameter efficiency and performance when contrasted with Linear Probing [13] (LP, fine tuning the classifier head only1), FourierFT [14] (n = {3000, 10000}), LoRA [9] (r = 16), and Full Fine-Tuning. LoLDU r=768 outperforms LoRAr=16 with 96.837% fewer trainable parameters. Particularly noteworthy is that LoLDU with = 1 achieves competitive scores with just 24 trainable parameters, while LoLDU with = 768 attains the highest accuracy: 42.15% for FGVC and 66.66% for StanfordCars, showcasing the scalability and effectiveness of our approach. Full Fine-Tuning (85.8M parameters) and Linear Probing represent the upper and lower performance bounds, respectively. instance, fine-tuning model with 7 billion parameters, such as LLaMA2 [15], on instruct-following tasks [16], [17] incurs substantial costs. These costs are not limited to the training phase but extend to the storage of multiple fine-tuned model checkpoints, each consuming gigabytes of storage, thus leading to significant storage overhead. Approaches like Low-Rank Adaptation (LoRA) [9] and Vector-based Random Matrix Adaptation (VeRA) [10] have been developed to address these challenges by reducing the number of updated parameters. LoRA [9] achieves this by randomly initializing two lowrank matrices and optimizing them to approximate the models updated weights. Similarly, VeRA [10] involves the random initialization and freezing of two matrices while training only two vectors for scale transformation. Recent research has revealed LoRAs limitations in data memorization due to low-rank updates. MoRA [18] addresses this issue through input dimension reshaping and square linear layer application. However, these methods often result in suboptimal convergence due to random initialization, as proposed by [19], [20], thus yielding provably small hyperspherical energy [21]. Furthermore, there is an accuracy gap compared to full finetuning, underscoring the need for more effective ParameterEfficient Fine-Tuning strategies. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2 Thus, OFT [21] proposes that maintaining orthogonality is crucial for preserving pre-trained knowledge, which enhances generalization [22].Building on this insight, we observe that Lower-Diag-Upper (LDU) decomposition inherently possesses orthogonal properties in its lower and upper triangular matrices. Additionally, we incorporate heuristic initialization constrain the range of initialized values, resulting in more stable training process. In contrast to other PEFT approaches [9], [10], [12], [18], which require fine-tuning O(n2) level parameters, for the first is possible to optimize only it time, we demonstrate that 0.00025% of parameters without any performance degradation. Our method, LoLDU, operates at O(n) level and employs the LDU decomposition technique to extract the core model parameters, which are then fine-tuned for downstream tasks. To demonstrate the efficiency of LoLDU across various model architectures, scales, and task types, we conduct an extensive set of experiments on tasks including instruction following [16], [17], [23], natural language understanding (NLU) [24], image classification [25][30], and image generation [6]. These experiments involved models with architectures such as LLaMA2-7B (decoder-only) [15], RoBERTa-Base (encoderdecoder) [31], ViT-Base (encoder-only) [32], and Stable Diffusion [33], with model scales ranging from 86 million to 7 billion parameters. This comprehensive evaluation verifies the effectiveness of our method across diverse scenarios. In summary, this paper makes three key contributions: We introduce novel approach to Parameter-Efficient Fine-Tuning (PEFT) by firstly attempting to leverage Lower-Diag-Upper (LDU) decomposition, offering solution that maintains model performance while drastically reducing trainable parameters to as low as 0.00025% of the original model. We present LoLDU, PEFT technique that harnesses Low-Rank Adaptation via Lower-Diag-Upper Decomposition, which operates with complexity of O(n). The LoLDU method employs orthogonal lower and upper triangular matrices to preserve pre-trained knowledge and enhance generalization, incorporating heuristic initialization and scaling factor to optimize the diagonal matrix. LoLDU demonstrates the effectiveness and versatility through comprehensive experiments across various model architectures, scales, and task types. It offers pioneering approach for efficient model adaptation across diverse scenarios in both NLP and CV domains. II. RELATED WORK Parameter-Efficient Fine-Tuning (PEFT) is designed to mitigate the significant computational and storage costs associated with Full Fine-Tuning (FT). Among the various PEFT approaches, Low-Rank Adaptation (LoRA) [9] offers more flexible and generalized re-parameterization framework for fine-tuning, achieved by training two low-rank matrices to approximate the updated parameters. However, studies [19], [20] have indicated that random initialization for reparameterization can be bottleneck, leading to suboptimal convergence. In this work, we present the first attempt to Figure 2. Comparison of LoRA (left) and our LoLDU (right) method. In LoRA, tunable parameters are low-rank (r) matrices and B, with = BA. For each weight , there are (din + dout) trainable parameters. LoLDU, however, optimizes diagonal matrix for scale transformation, preserving original model knowledge during tuning. The weight update in LoLDU is = σ (Lr, diag(zr), Ur), involving + 1 trainable parameters. The permutation matrix , while omitted in this figure for simplicity, is included in Figure address this issue by leveraging the Lower-Diag-Upper (LDU) decomposition technique for initialization. In Figure 2, we provide comparison between LoRA and our LoLDU method. Parameter efficient fine tuning (PEFT). To date, existing PEFT approaches can be divided into three categories: (1) Additive PEFT: This approach introduces new tunable parameters or modifies model representations. Examples include adapters [34][37] and prefix-tuning [38], which add small, trainable components to the model for efficient task-specific learning. (2) Selective PEFT [39][42]: This method finetunes only subset of the models parameters, such as specific layers or neurons. Techniques like BitFit [43] aims to only update bias parameters b, while maintaining fixed weights , the models conditional distribution p(yx; θ) towards the target domain distribution ptarget(yx), where θ denotes the model parameters. (3) Re-parameterized PEFT [9], [44], [45]. This technique usually reconstructs model parameters in low-dimensional space as new knowledge is often represented in low-rank form [46]. to shift Low-Rank Adaptation. LoRA [9] decomposes parameter matrices into low-rank forms, maintaining performance while reducing the number of parameters to be fine-tuned. Previous studies have credited LoRA for its efficiency in inference and storage, albeit at an expensive training cost due to the random initialization, which causes the model to saturate more slowly. Recent studies [47] have attempted to bridge this gap by exploring the development of new initialization methods to create LoRA parameters instead of starting from scratch. Advancing the initialization strategies for LoRA parameters is imperative for enhancing the quality and adaptability of downstream tasks. Therefore, Section IV delves into the exploration of various initialization methodologies. Re-parameterization. Singular Value Decomposition (SVD) is widely utilized for re-parameterization in ParameterEfficient Fine-Tuning (PEFT) methods. Recent studies [12], [37], [48][50] have explored various SVD-based approaches for low-rank matrix initialization. These include fine-tuning singular values of reshaped weight matrices [50], initializing JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Figure 3. Schematic representation of our LoLDU method. The left diagram illustrates the forward pass, demonstrating the transformation of the input Rdin into the output Rdout via residual subspace matrix L[r:]D[r:]U[r:] and decomposed subspace matrix σLrDrUr. The right diagram shows the initialization process, where the residual matrix is obtained by performing LDU decomposition on the pre-trained weights, then subtracting the top-r submatrices (top-r rows and columns) from the permutation matrix (P), lower triangular (L), scaled diagonal (D), and upper triangular (U) matrices. Diagonal matrix is trainable (orange), while the other matrices remain fixed (blue). LoLDU enables efficient adaptation of pre-trained models via low-rank updates, reducing both computational cost and parameter count. adapter matrices with principal components [12], introducing intermediate matrices between frozen principal components matrices, and updating weights as sparse combinations of singular vector outer products [49]. However, SVDs computational complexity O(mn2 + n3) for an matrix remains constraint compared to LDU decomposition O(mn2 n3/3). Furthermore, LDU decomposition offers more interpretable representation of matrix structure through elementary row operations and pivoting strategies. III. METHOD We present LoLDU (depicted in Figure 3), parameterefficient-fine-tuning method utilizing Lower-Diag-Upper (LDU) decomposition. LoLDU builds upon the principle proposed by LoRA [9], focusing on learning the changes to LoRA, which in pre-trained weights. employs the LDU decomposition for initialization. We then compute the Residual Subspace Matrix (RSM) by applying element-wise subtraction of the Decomposition Subspace Matrix (DSM) from the original matrix. The DSM is constructed using the first entries, which are selected to maintain low-rank formation while remaining trainable. In contrast random initialization, LoLDU leverages A. Initialization and Orthogonal Space Preservation Previous works have shown that maintain the orthogonality nature is crucial to improve the representation quanlity [21]. The advantage of LDU decomposition is the factorization that preserves the orthogonality of the lower and upper triangular matrices. We leverage this property to initialize the low-rank matrices. The LDU decomposition factorizes matrix W0 Rmn into four matrices: W0 = diag(z) U, where Rmm is permutation matrix, Rmk is lower triangular with ones on the diagonal, diag(z) Rkk (1) is the diagonal formation of vector z, and Rkn is upper triangular with ones on the diagonal, where = min(m, n). This property is essential for obtaining an equivalent formation to the original model weight W0. Specifically, we optimize only the diagonal entries of matrix diag(z) and dynamically adjust the scaling factor σ to align updated parameters with the target matrix, wherein the σ is initialized to 1.0. B. Low-Rank Approximation In the realm of learning weight changes, our approach aligns with the principles of LoRA-based methods [9], [18], [51], [52], which mitigate inference latency by merging pre-trained weights with the learned adapter matrices. Formally, let W0 Rmn represent the pre-trained weight matrix, and Rmn denote the weight changes introduced during fine-tuning. LoRA parameterizes using low-rank decomposition in the forward pass: = W0x + = W0x + BAx, where Rmr and Rrn are trainable matrices, with the rank min(m, n). (2) In contrast, our proposed method, LoLDU, decomposes the weight matrix W0 using an LDU (Lower-Diag-Upper) decomposition which breaks down W0 into four matrices: diag(z) . We take the insprition from [46], [53] that learned adapter matrices reside in low intrinsic dimension. Therefore, we extract the top components from the LDU decomposition, which helps in maintaining an intrinsic subspace to adapt to downstream tasks. These components are represented as follows: = Lr = L[:,:r] Rmr, diag(zr) = D[:r,:r] Rrr, = Ur = U[:r,:] Rrn, (3) (4) (5) JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4 where Lr represents the first columns of the lower triangular matrix L, D[:r,:r] denotes the top by block of the diagonal matrix D, and Ur is the first rows of the upper triangular matrix . These components capture the essential structure of the original weight matrix in reduced form. C. LoLDU Weight Adaptation Procedure Using these components, we define the Decomposed Subspace Matrix (DSM), which reconstructs part of the original weight matrix using the top components. The DSM is formulated as: DSM = σ (Lr, diag(zr), Ur), (6) where σ is introduced to control the magnitude of the weight updates as scaling factor. Next, we obtain the Residual Subspace Matrix (RSM) by subtracting the DSM from the original weight matrix W0, which ensures that the RSM captures the information not represented by the top components, thereby preserving the full knowledge encoded in W0: RSM = W0 DSM. (7) The weight change is parameterized as: = DSM = σ (Lr, diag(zr), Ur), (8) by parameterizing in this manner, efficient updates to the model weights are enabled without significantly increasing the parameter count. The advantage of LoLDU lies in its use of orthogonal, lower, and upper triangular matrices, which help preserve the inherent knowledge of the model. The orthogonal nature of these matrices ensures that the decomposed components maintain their properties during transformations as proposed by [22], thereby preserving the information integrity. Moreover, we initialize diag(zr) using heuristic methods such as Constant (Dr.mean), Uniform, Normal, or Regular LDU, to enhance training stability. The proposed forward pass can be expressed as follows: = RSM + = RSM + DSM = RSM + σ (Lr, diag(zr), Ur)x. (9) D. Optimization Process The fine-tuning phase of LoLDU employs sophisticated optimization strategy, focusing on the diagonal matrix Dr and the scaling factor σ. This approach represents departure from conventional fine-tuning methods, offering more granular control over parameter updates while preserving the integrity of pre-trained knowledge."
        },
        {
            "title": "The optimization problem is formulated as a constrained",
            "content": "minimization: Algorithm 1 Low-Rank LDU Decomposition and Optimization for Layer Weight Adaptation Input: Weight matrix Rmn, rank r, alpha α, learning rate η, number of iterations , projection operator Output: Decomposed components P, Lr, Dr, Ur, residual Wresidual, scaling factor σ, optimized Dr, optimized σ 1: Phase 1: Initial Decomposition 2: P, L, LU decomposition(W) // Perform standard"
        },
        {
            "title": "LU decomposition",
            "content": "// Extract diagonal matrix // Normalize 3: diag(U) 4: D1U 5: Phase 2: Low-Rank Approximation 6: Lr L:,1:r 7: Dr D1:r,1:r 8: Ur U1:r,: 9: Phase 3: Scaling Factor and Residual Computation 10: σ α/r 11: Wapprox σPLrDrUr // Extract first columns of // Extract top-left submatrix of // Extract first rows of // Compute scaling factor // Compute low-rank approximation // Compute residual matrix 12: Wresidual Wapprox 13: Phase 4: Heuristic Initialization 14: Apply heuristic initialization to Dr // Choose from methods: Constant((Dr.mean)), Uniform, Normal, or Regular LDU 15: Phase 5: Optimization with Projected Gradient Descent 16: for 1 to do 17: Compute gradients Dr and σL Dr P(Dr η Dr L) σ P(σ η σL) 18: 19: 20: end for 21: return P, Lr, Dr, Ur, Wresidual, σ where denotes the task-specific loss function, fW0+W denotes the model with updated weights, (x, y) are the inputoutput pairs from the fine-tuning dataset, represents the Frobenius norm, and ϵ is set constraint threshold."
        },
        {
            "title": "To address the constrained nature of",
            "content": "the optimization problem, we employ projected gradient descent method, ensuring that updates to Dr and σ remain within the feasible region defined by the constraints. This is achieved through projection operator P: D(t+1) = (cid:18) D(t) ηt σ(t+1) = (cid:18) σ(t) ηt (cid:19) D(t) (cid:19) σ(t) , , (11) (12) where ηt is the learning rate at iteration t, adaptively adjusted using techniques such as Adam [54] or RMSprop [55] to account for the geometry of the parameter space. Please refer to Algorithm 1 for additional detailed information. minimize Dr,σ L(fW0+W (x), y) subject to DrF ϵ, 0 < σ 1, E. Computational Complexity Analysis (10)"
        },
        {
            "title": "The computational efficiency of LoLDU can be evaluated",
            "content": "in terms of both space and time complexity: JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5 Table RESULTS FOR DIFFERENT ADAPTATION METHODS ON THE GLUE BENCHMARK. THE TERM PARAMS REFERS TO THE NUMBER OF PARAMETERS UPDATED DURING FINE-TUNING. WE REPORT MATTHEWS CORRELATION FOR COLA, PEARSON CORRELATION FOR STS-B, AND ACCURACY FOR THE REMAINING TASKS. HIGHER VALUES INDICATE BETTER PERFORMANCE. EXCEPT LOLDU, ALL RESULTS ARE FROM PRIOR WORK. LOLDU PERFORMS ON PAR WITH LORA WHILE USING SIGNIFICANTLY FEWER PARAMETERS. THE baseline ROW SHOWS THE PERCENTAGE INCREASE OR DECREASE IN PERFORMANCE COMPARED TO OUR METHOD."
        },
        {
            "title": "Model Method",
            "content": "# Params SST-2 MRPC CoLA QNLI RTE acc acc acc acc cor STS-B Avg. cor B - E R"
        },
        {
            "title": "FT\nBitFit\nLoRA\nPiSSA\nVeRA",
            "content": "125M 94.8 0.1M 93.7 0.3M 95.1 0.707M 94.6 0.043M 94.6 LoLDU baseline 0.0184M 94.8 6.13% -0.3 90.2 92.7 89.7 88.4 89.5 89.9 +0.2 63.6 62.0 63.4 63.0 65. 63.8 +0.4 92.8 91.8 93.3 93.1 91.8 92.9 -0.4 78.7 81.5 78.4 85.9 78.7 81.3 +2.9 91.2 90.8 91.5 91.2 90. 92.3 +0.8 85.2 85.4 85.2 86.0 85.2 85.8 +0.6 Space complexity: The storage requirement for LoLDU is O(r + 1), which is considerably lower than the O(mr + rn) required by methods such as LoRA. This reduction in parameter count not only leads to significant memory savings but improves efficiency during both the training and inference phases. Time complexity: The forward pass of LoLDU requires O(mnr) operations with minor linear term O(r). In contrast to methodologies that necessitate recurrent complex iterations [52], [56], LoLDU performs the LDU decomposition only once during initialization, with time complexity of O(mn2 n3/3), and utilizing direct updates via projected gradient descent without iterative refinement, ensuring efficient parameter optimization and rapid convergence. In summary, LoLDU leverages LDU decomposition to efficiently parameterize weight changes, reducing the number of tunable parameters and maintaining high performance. This method provides more efficiency and effective alternative to traditional LoRA-based approaches. IV. EXPERIMENTS This section presents an evaluation of LoLDU within the fields of natural language processing (NLP) and computer vision (CV). For NLP, LoLDU is applied for fine-tuning: (1) RoBERTa Base [31] on natural language understanding (GLUE [24]), and (2) LLaMA-2 7B [15] on instruction tuning (Alpaca [16], Vicuna [17]). For CV, we apply LoLDU to fine-tune: (1) Vision Transformers (ViT) Base [32] on image classification [25][30], and (2) Stable Diffusion v1.5 [33] on customized image generation [6]. We compare our LoLDU method with widely used Parameter-Efficient Fine-Tuning (PEFT) methods. To ensure fair comparison, we replicate the setups from previous studies [9], [14], [57] and utilize their reported results. The baselines considered are: Full Fine-Tuning (FT): FT trains all model parameters on the task-specific data. LoRA [9]: LoRA updates weights by injecting two tunable low-rank matrices for parameterization. MELoRA [57]: MELoRA trains group of mini LoRAs to maintain higher rank. FourierFT [14]: FourierFT learns small fraction of spectral coefficients using the Fourier transform. Finally, we perform ablation studies to examine the impact of initialization methods, scaling factors, and rank. Further results concerning the learning rate and rank are detailed in Appendix E1 and Appendix E2. We conduct all experiments on single NVIDIA RTX A6000 (48G) GPU. Table II COMPARATIVE ANALYSIS OF VARIOUS METHODS ON IMAGE CLASSIFICATION DATASETS USING VIT BASE MODELS. THE TABLE REPORTS THE MEAN ACCURACY (%) AFTER 10 EPOCHS, ALONGSIDE PARAMETERS EFFICIENCY AND APPROACH FEATURES. Method Mean Acc. Params (%) Keep Orthogonal No random Init. No extra Infer. cost Faster convergence FullFT LP 88.20 68.38 100 - LoRA 76.22 6.77 FourierFT 79.29 2.79 LoLDU 82.79 0. A. Natural Language Understanding a) Models and Datasets: We evaluate LoLDU on the GLUE benchmark (General Language Understanding Evaluation [24]), which comprises nine NLU tasks. These tasks include single-sentence classification (CoLA, SST-2), similarity and paraphrasing (MRPC, STS-B, QQP), and natural language inference (MNLI, QNLI, RTE, WNLI). For evaluation, we fine-tune pre-trained RoBERTa Base models [31]. b) Implementation Details: We adopt the experimental setup of VeRA [10], tuning the hyperparameters for learning rates and the scaling factor values across six datasets in the GLUE benchmark. Following the approach of LoRA [9], we fully fine-tune the classification head. We apply LoLDU to the weight matrices Wq, Wk, Wv, and Wo in each transformer block. Hyperparameters are provided in Table VII in the Appendix. c) Results: Results are summarized in Table I. Following [9], [52], and [58], we specify the number of trainable parameters for the fine-tuned layers excluding the classification head. We report the median of five random seed results, selecting the best epoch for each run. In general, LoLDU achieves better or on-par performance compared to baseline methods with significantly fewer trainable parameters. Notably, LoLDU outperforms all baselines including fully fine-tuning the RoBERTa JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Base on STS-B. As mentioned in Section III, the parameter count of LoRA is dependent on both the width and depth of models, resulting in larger count growth (LoRA: 0.3M; ours: 0.0184M) compared to LoLDU. B. Instruction Tuning a) Models and Datasets: Instruction tuning [16], [59], [60] is technique that involves fine-tuning large language models (LLMs) on paired data consisting of instructions and their corresponding outputs to enhance the quality of the models responses. In our study, we apply LoRA [9] and LoLDU to fine-tune the LLaMA2 model [15]. Specifically, we use LLaMA2-7B as the base model, which is then fine-tuned on the Alpaca dataset [16]. This dataset comprises 52,000 instruction-output pairs generated by OpenAIs text-davinci003 model. For evaluation, we conduct rigorous and holistic assessment of the fine-tuned model using INSTRUCTEVAL [23], allowing us to systematically analyze the models performance in problem-solving, writing ability, and alignment to human values. b) Implementation Details: In the implementation of LoRA, rank of = 64 is employed, with focus on updating all linear layers, excluding the language modeling head (lm_head), and specifically targeting the WQ and WV matrices. For LoLDU, the training process spans three epochs, and we present the average performance scores across all evaluated responses. Hyperparameter configuration is detailed in Table VIII in Appendix B. c) Results: The results, as presented in Table III, demonstrate that LoLDU achieves slight improvement over the performance of LoRA, while employing merely 0.05% of the parameters required by LoRA. Table III RESULTS ON INSTRUCTEVAL FOR INSTRUCTION-FOLLOWING TASKS: EXACT MATCH FOR MMLU, DROP, AND BBH, PASS@1 FOR HUMANEVAL. HIGHER VALUES ARE PREFERABLE. BOLDFACE INDICATES THE BEST METRIC VALUES. THE baseline ROW DISPLAYS THE PERFORMANCE CHANGE PERCENTAGE COMPARED TO OUR METHOD. Model Method # Params MMLU DROP HEval BBH 7 - 2 L w/o FT LoRA AdaLoRA MELoRA LoLDU baseline - 33.6M 33.6M 0.5M 45.96 45.64 45.96 46.46 0.016M 46.21 0.05% +0. 31.55 32.46 31.94 32.65 32.71 +0.25 12.20 15.09 14.02 16.16 15.11 +0.02 32.04 32.40 32.85 33.01 33.12 +0. C. Image Classification a) Models and Datasets: We assess our approach on image classification utilizing the Base version of the Vision Transformer (ViT) [32], pre-trained on ImageNet-21K [61]. Fine-tuning is performed on datasets such as CIFAR10 (10) [25], EuroSAT (10) [30], as well as StanfordCars (196) [28], FLOWERS102 (102) [27], FGVC (100) [29], and CIFAR100 (100) [26], covering both small and large label spaces. For detailed information, refer to Appendix C. b) Implementation Details: We include three baselines for evaluation: Full Fine-Tuning (FT), Linear Probing [13] (LP, fine-tuning the classification head only), and LoRA [9]. We adhere to the experimental configurations established by FourierFT [14]. For both LoRA and our method, only the WQ and WV matrices of ViT are updated. We use = 16 for LoRA and = {64, 768} for LoLDU. Detailed hyperparameter configurations are available in Table IX in the Appendix C. c) Results: Table IV presents the results for six image classification datasets using the ViT Base model. LoRA and LoLDU demonstrate superior performance compared to Linear Probing [13], showcasing their efficacy in image classification tasks within the computer vision domain. Notably, our approach achieves comparable outcomes while utilizing merely 3.173% of LoRAs parameters. LoLDU exhibits particularly impressive gains, surpassing LoRA by 15.28% and 16.99% in FGVC and StanfordCars tasks, respectively, effectively narrowing the accuracy gap with Full Fine-Tuning, as depicted in Figure 1. Furthermore, LoLDU outperforms all baselines, including Fully Fine-Tuning, on EuroSAT and Flowers datasets. D. Image Generation a) Models and Datasets: We assess our method in the domain of image generation. Recent research [5], [6] highlights the necessity for customization in this field, which holds significant practical implications. The goal is to finetune text-to-image model using limited set (typically 35) of images representing an unique concept (e.g., scene, individual, pet, or object) to effectively capture and reproduce the novel concept. For this study, we employ the v1.5 version of Stable Diffusion (SD) [33], widely-adopted computer vision foundation model. SD is pre-trained on LAION-5B [62], dataset consists of 5.85 billion image-text pairs filtered using CLIP [63]. b) Implementation Details: We conduct our experiments on seven different concepts, including persons, pets, and objects, using the CustomConcept101 dataset [64] and the human-centric FFHQ dataset [65]. We select two concurrent works as baselines: Textual Inversion [5] and DreamBooth [6]. Textual Inversion learns new concept by mapping it from the image to the textual modality, encoding them as rare token in the embedding space. DreamBooth, utilizes semantic prior (e.g., class-specific) to maintain the subjects key features. We provide the datasets in Figure 6 and hyperparameters in Table in Appendix D. c) Results: We present the visual results in Figure 6, while Table VI provides quantitative comparison. We assess our methods efficacy through DINO, CLIP-T and CLIPI metrics. DINO [66] is computed as the average pairwise cosine similarity between the ViT-S/16 DINO embeddings of generated and real images. CLIP-I measures the average pairwise cosine similarity between CLIP [63] embeddings of generated and real images, while CLIP-T evaluates prompt fidelity by measuring the average cosine similarity between prompt and image CLIP embeddings. LoLDU achieves the highest average score across metrics. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7 Figure 4. Comprehensive Analysis of Rank Ablation Study Results. This figure presents the performance of the ViT-base model on various image classification tasks using the LoLDU method with different ranks. The x-axis shows ranks (1 to 768), and the y-axis indicates accuracy for datasets: FGVC, StanfordCars, CIFAR10, CIFAR100, EuroSAT, and Flowers. and efficiency of our method across variety of tasks. a) Effect of Initialization: The initialization of the entries in the diagonal matrix diag(z) (Eq. 1) plays crucial role in LoLDUs performance. We evaluate several initialization policies on the ViT Base model across six image classification datasets. Table presents our findings. Empirical results indicate that Uniform initialization consistently outperforms other strategies, achieving the highest average accuracy by stabilizing the training loop and enhancing convergence. Thus, LoLDU with Uniform initialization is optimal for applications requiring stable dynamics and high accuracy. Additionally, both Uniform and Normal initialization contribute to training stability. b) Impact of Scaling Factor: The scaling factor within LoLDU is crucial for assessing the efficacy of low-rank updates in augmenting model performance. This ablation study is dedicated to examining the necessity of integrating scaling factor, specifically fixed at value of 1, to evaluate its impact on enhancing model accuracy and ensuring training stability. Table presents comprehensive comparative analysis of performance metrics with and without the incorporation of scaling factor across various datasets. The empirical findings reveal that the absence of scaling factor, as denoted by the Figure 5. Concept Learning Progression In Text-to-Image Generation. Top row: target concept. Subsequent rows: generated images using LoLDU (our method), DreamBooth [6], and Textual Inversion [5], respectively, at training steps 0-600. LoLDU exhibits accelerated convergence, achieving concept acquisition within 100 steps, surpassing baseline methods in efficiency. E. Analysis In this section, we conduct comprehensive analysis of the hyperparameters associated with LoLDU, specifically focusing on initialization, scaling factor, and rank. We systematically investigate the influence of these parameters on the performance JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8 Table IV WE CONDUCTED COMPARISON ON IMAGE CLASSIFICATION DATASETS USING VIT BASE MODELS. THE ACCURACY (%) AFTER 10 EPOCHS IS REPORTED. FOURIERFT WAS EVALUATED USING DIFFERENT TRAINABLE PARAMETERS FOR EACH LAYER, INDICATED BY SYMBOLS: () FOR 3000 AND () FOR 10000. baseline REPRESENTS THE PERFORMANCE GAP BETWEEN OUR LOLDU METHOD AND THE BASELINE METHOD LORA. BOLD DENOTES THE BEST RESULTS."
        },
        {
            "title": "Model Method",
            "content": "# Params"
        },
        {
            "title": "FGVC\nacc",
            "content": "StanfordCars CIFAR10 CIFAR100 acc acc acc B - LP FT LoRA(r16) FourierFT() FourierFT() LoLDU(r64) LoLDU(r768) baseline - 17.44 85.8M 54.84 25.16 581K 27.51 72K 32.44 239K 1.5k 18k 32.31 42.15 3.173% +16.99 25.76 79.78 45.38 46.11 56.36 50.99 66.66 +21. 96.41 98.92 98.78 98.58 98.69 97.96 98.59 -0.19 84.28 92.38 92.02 91.20 91.45 89.60 91.21 -0."
        },
        {
            "title": "Flowers\nacc",
            "content": "88.72 99.05 98.44 98.29 98.78 97.60 99.21 +0.77 97.64 98.43 97.55 98.14 98.04 98.53 98.92 +1.37 Avg. 68.38 87.23 76.22 76.64 79. 77.83 82.79 +6.57 Table ABLATION STUDY OF DIFFERENT INITIALIZATION METHODS ACROSS SIX IMAGE CLASSIFICATION DATASETS. WE SET RANK UP TO 768 AND LEARNING RATE TO 3E-3 AND TEST ON THE VIT BASE MODEL. THE DATASETS INCLUDE FGVC, STANFORDCARS, CIFAR10, CIFAR100, EUROSAT, AND FLOWERS. THE UNIFORM INITIALIZATION METHOD IS INDICATED BY SYMBOLS: FOR (A=-1, B=1) AND FOR (A=-Z.MEAN/2, B=Z.MEAN/2). THE NORMAL INITIALIZATION METHOD IS INDICATED BY SYMBOLS: FOR (MEAN=0, STD=1) AND FOR (MEAN=Z.MEAN, STD=Z.STD). FOR EACH ENTRY, THE LEFT VALUE REPRESENTS RESULTS WITH SCALING FACTOR, WHILE THE RIGHT VALUE IN GRAY REPRESENTS RESULTS WITHOUT SCALING FACTOR. THE AVERAGE PERFORMANCE (AVG.) ACROSS ALL DATASETS IS ALSO REPORTED. BOLD DENOTES THE BEST RESULTS FOR EACH DATASET AND THE AVERAGE."
        },
        {
            "title": "StanfordCars\nacc",
            "content": "CIFAR10 acc CIFAR100 acc"
        },
        {
            "title": "Flowers\nacc",
            "content": "Avg. ViT-Base Initialization Ablation Study 1.17 / 1.38 Uniform() 19.33 / 16.63 14.22 / 9.71 2.37 / 2.37 Normal() 39.60 / 39.12 65.17 / 65.00 98.02 / 98.33 90.27 / 90.54 99.00 / 99.03 98.63 / 98.63 81.78 / 81.78 Normal() 16.72 / 15.10 10.11 / 7.91 2.10 / 2.13 Constant(z.mean) 42.21 / 41.16 65.41 / 63.86 98.38 / 98.21 90.77 / 90.21 99.16 / 98.99 98.63 / 98.43 82.43 / 81.81 Zeros 72.43 / 72.13 46.00 / 43.27 96.44 / 96.05 41.08 / 40.49 45.59 / 45.05 16.51 / 14.83 50.95 / 46.61 30.89 / 26.26 Ones 9.30 / 9.24 2.01 / 1.95 8.27 / 9.09 1.16 / 1. 52.98 / 48.49 57.81 / 52.95 35.92 / 28.93 29.17 / 26.54 10.29 / 8.60 4.51 / 4. 4.61 / 4.41 3.73 / 4.41 1.34 / 1.12 Regular LDU Uniform() 40.50 / 40.44 65.12 / 62.37 98.28 / 98.20 90.61 / 90.61 99.04 / 98.95 98.92 / 98.92 82.08 / 81.58 42.15 / 39.72 66.66 / 64.54 98.59 / 98.28 91.21 / 90.48 99.21 / 98.97 98.63 / 98.82 82.74 / 81.80 Table VI COMPARISON OF IMAGE GENERATION METHODS. PERFORMANCE METRICS (DINO, CLIP-T, AND CLIP-I) FOR DREAMBOOTH, TEXTUAL INVERSION, AND LOLDU METHODS. HIGHER VALUES INDICATE BETTER PERFORMANCE. BOLD VALUES INDICATE BEST PERFORMANCE FOR EACH METRIC. Model Method DINO CLIP-T CLIP-I Avg. 4 . 1 - DreamBooth Textual Inversion LoLDU 0.679 0.649 0.723 0.323 0.313 0. 0.801 0.801 0.830 0.601 0.588 0.750 gray values, consistently leads to diminished accuracy and compromises the stability of the convergence process. This highlights the pivotal role of the scaling factor in optimizing the performance of LoLDU, thereby enabling robust and efficient learning dynamics across diverse range of image classification tasks. c) Influence of Rank: The rank parameter within LoLDU is pivotal in determining the models complexity and expressiveness. We conducted an extensive analysis by varying the rank across diverse tasks, as detailed in Table XII. Additionally, the visual results of this analysis are presented in Figure 4. Our findings indicate that an increase in rank consistently enhances performance across all datasets, especially at lower ranks, but stabilizes beyond 256, indicating diminishing returns. Thus, selecting an optimal rank balances expressiveness and efficiency. In practical applications of LoLDU, our findings suggest that adopting rank approximately one-third of the full rank ensures an optimal balance between performance and resource efficiency, thereby providing broader applicability across various scenarios. d) Parameter Efficiency vs. Performance Trade-off: Finally, we explore the nuanced relationship between parameter efficiency and performance, focusing on the capabilities of LoLDU in comparison to other established methodologies. Table II provides compelling insight into the efficiency of LoLDU , which achieves mean accuracy of 82.79% while utilizing mere 0.21% of the parameters. This is stark contrast to methods like FullFT, which, despite achieving higher accuracy of 88.20%, require the full parameter set, and LoRA, which uses 6.77% of the parameters for lower accuracy of 76.22%. These data underscore LoLDUs JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9 Figure 6. Visualized Results of the Image Generation Task. From left to right: target reference images, outputs from LoLDU (ours), DreamBooth, and Textual Inversion. Each row represents distinct category with specified prompt (annotated under each row). LoLDU demonstrates efficacy in generating diverse, prompt-adherent images while preserving key attributes from the reference set. exceptional capacity to deliver competitive performance with substantially reduced parameter footprint. to traditional methods, offering strategic advantage in both research and practical applications. LoLDUs efficiency in parameter usage not only reduces computational and memory demands but also enhances the models adaptability to various deployment scenarios, particularly those with limited resources. This efficiency is achieved without compromising on key performance metrics, as evidenced by the methods ability to maintain orthogonality, avoid random initialization, eliminate extra inference costs, and ensure faster convergence. These attributes collectively position LoLDU as highly effective and resource-efficient alternative V. CONCLUSION In conclusion, LoLDU represents significant advancement in Parameter-Efficient Fine-Tuning (PEFT), offering novel approach with the Lower-Diag-Upper (LDU) decomposition technique. By optimizing just 0.00025% of parameters while maintaining performance across diverse tasks and model architectures, LoLDU addresses the prohibitive computational and storage costs associated with fine-tuning large models. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 10 Its preservation of orthogonality in triangular matrices and precise diagonal matrix optimization ensure efficient scale transformation and robust convergence. Our extensive evaluation, spanning various tasks and model scales up to 7 billion parameters, validates LoLDUs effectiveness and superiority over traditional fine-tuning methods, underscoring its potential for broad applicability and impact in advancing efficient model customization practices."
        },
        {
            "title": "REFERENCES",
            "content": "[1] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, in Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [2] L. Zhang, W. Wei, Q. Shi, and et al., Accurate tensor completion via adaptive low-rank representation, IEEE Transactions on Neural Networks and Learning Systems, no. 10, 2020. [3] L. Zhang, J. Fu, S. Wang, and et al., Guide subspace learning for unsupervised domain adaptation, IEEE Transactions on Neural Networks and Learning Systems, no. 9, 2020. [4] R. Zhang, H. Zhang, X. Li, and F. Nie, Adaptive robust low-rank 2d reconstruction with steerable sparsity, IEEE Transactions on Neural Networks and Learning Systems, no. 9, 2020. [5] R. Gal, Y. Alaluf, Y. Atzmon, O. Patashnik, A. H. Bermano, G. Chechik, and D. Cohen-Or, An image is worth one word: Personalizing text-toimage generation using textual inversion, in The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023, 2023. [6] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman, Dreambooth: Fine tuning text-to-image diffusion models for subjectdriven generation, in IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, 2023. [7] S. Lai, C. Liu, D. Wang, and H. Lu, Refocus the attention for parameterefficient thermal infrared object tracking, IEEE Transactions on Neural Networks and Learning Systems, 2024. [8] Z. Zheng, X. Wang, N. Zheng, and Y. Yang, Parameter-efficient person re-identification in the 3d space, IEEE Transactions on Neural Networks and Learning Systems, no. 6, 2024. [9] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, Lora: Low-rank adaptation of large language models, in The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. [10] D. J. Kopiczko, T. Blankevoort, and Y. M. Asano, VeRA: Vector-based random matrix adaptation, in The Twelfth International Conference on Learning Representations, 2024. [11] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, Qlora: Efficient finetuning of quantized llms, in Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [12] F. Meng, Z. Wang, and M. Zhang, PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models, 2024. [13] X. Chen, S. Xie, and K. He, An empirical study of training selfsupervised vision transformers, in 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, 2021. [14] Z. Gao, Q. Wang, A. Chen, and et al., Parameter-Efficient Fine-Tuning with Discrete Fourier Transform, 2024. [15] H. Touvron, L. Martin, K. Stone, and et al., Llama 2: Open Foundation and Fine-Tuned Chat Models, 2023. [16] R. Taori, I. Gulrajani, T. Zhang, and et al., Stanford alpaca: An instruction-following llama model, 2023. [17] W.-L. Chiang, Z. Li, Z. Lin, and et al., Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. [18] T. Jiang, S. Huang, S. Luo, and et al., MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning, 2024. [19] X. Glorot and Y. Bengio, Understanding the difficulty of training deep feedforward neural networks, in Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, 2010. [20] I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton, On the importance of initialization and momentum in deep learning, in Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, 2013. [21] Z. Qiu, W. Liu, H. Feng, Y. Xue, Y. Feng, Z. Liu, D. Zhang, A. Weller, and B. Scholkopf, Controlling text-to-image diffusion by orthogonal finetuning, in Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [22] W. Liu, R. Lin, Z. Liu, J. M. Rehg, L. Paull, L. Xiong, L. Song, and A. Weller, Orthogonal over-parameterized training, in IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, 2021. [23] Y. K. Chia, P. Hong, L. Bing, and S. Poria, InstructEval: Towards holistic evaluation of instruction-tuned large language models, in Proceedings of the First edition of the Workshop on the Scaling Behavior of Large Language Models (SCALE-LLM 2024), 2024. [24] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, GLUE: multi-task benchmark and analysis platform for natural language understanding, in 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019. [25] A. Krizhevsky, V. Nair, and G. Hinton, CIFAR-10 (Canadian Institute for Advanced Research). [26] , CIFAR-100 (Canadian Institute for Advanced Research). [27] A. Gurnani, V. Mavani, V. Gajjar, and Y. Khandhediya, Flower Categorization using Deep Convolutional Neural Networks, 2017. [28] J. Krause, M. Stark, J. Deng, and L. Fei-Fei, 3D Object Representations for Fine-Grained Categorization, in 2013 IEEE International Conference on Computer Vision Workshops, 2013. [29] S. Maji, E. Rahtu, J. Kannala, and et al., Fine-Grained Visual Classification of Aircraft, 2013. [30] P. Helber, B. Bischke, A. Dengel, and D. Borth, EuroSAT: Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification, 2017. [31] Y. Liu, M. Ott, N. Goyal, and et al., RoBERTa: Robustly Optimized BERT Pretraining Approach, 2019. [32] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, An image is worth 16x16 words: Transformers for image recognition at scale, in 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. [33] R. Rombach, A. Blattmann, D. Lorenz, and et al., High-resolution image synthesis with latent diffusion models, 2021. [34] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, Parameter-efficient transfer learning for NLP, in Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, 2019. [35] T. Lei, J. Bai, S. Brahma, J. Ainslie, K. Lee, Y. Zhou, N. Du, V. Y. Zhao, Y. Wu, B. Li, Y. Zhang, and M. Chang, Conditional adapters: Parameter-efficient transfer learning with fast inference, in Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [36] R. Zhang, J. Han, C. Liu, and et al., LLaMA-Adapter: Efficient Finetuning of Language Models with Zero-init Attention, 2023. [37] F. Zhang and M. Pilanci, Spectral Adapter: Fine-Tuning in Spectral Space, 2024. [38] X. L. Li and P. Liang, Prefix-tuning: Optimizing continuous prompts for generation, in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021. [39] D. Guo, A. Rush, and Y. Kim, Parameter-efficient transfer learning with diff pruning, in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021. [40] S. S. S. Das, H. Zhang, P. Shi, W. Yin, and R. Zhang, Unified lowresource sequence labeling by sample-aware dynamic sparse finetuning, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023. [41] A. Ansell, I. Vulic, H. Sterz, and et al., Scaling Sparse Fine-Tuning to Large Language Models, 2024. [42] Y. Sung, V. Nair, and C. Raffel, Training neural networks with fixed sparse masks, in Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, 2021. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11 P. Schramowski, S. Kundurthy, K. Crowson, L. Schmidt, R. Kaczmarczyk, and J. Jitsev, LAION-5B: an open large-scale dataset for training next generation image-text models, in Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. [63] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, Learning transferable visual models from natural language supervision, in Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, 2021. [64] N. Kumari, B. Zhang, R. Zhang, E. Shechtman, and J. Zhu, Multiconcept customization of text-to-image diffusion, in IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, 2023. [65] T. Karras, S. Laine, and T. Aila, Flickr faces hq (ffhq) 70k from stylegan, CoRR, 2018. [66] M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski, and A. Joulin, Emerging properties in self-supervised vision transformers, in 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, 2021. [43] E. Ben Zaken, Y. Goldberg, and S. Ravfogel, BitFit: Simple parameterefficient fine-tuning for transformer-based masked language-models, the Association for the 60th Annual Meeting of in Proceedings of Computational Linguistics (Volume 2: Short Papers), 2022. [44] S.-Y. Liu, C.-Y. Wang, H. Yin, and et al., DoRA: Weight-Decomposed Low-Rank Adaptation, in Proceedings of the 41st International Conference on Machine Learning, July 2024. [45] D. Vander Mijnsbrugge, F. Ongenae, and S. Van Hoecke, Parameter efficient neural networks with singular value decomposed kernels, IEEE Transactions on Neural Networks and Learning Systems, no. 9, 2023. [46] A. Aghajanyan, S. Gupta, and L. Zettlemoyer, Intrinsic dimensionality explains the effectiveness of language model fine-tuning, in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021. [47] J. Phang, Y. Mao, P. He, and W. Chen, Hypertuning: Toward adapting large language models without back-propagation, in International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, 2023. [48] C. Feng, M. He, Q. Tian, and et al., TriLoRA: Integrating SVD for Advanced Style Personalization in Text-to-Image Generation, 2024. [49] V. Lingam, A. Tejaswi, A. Vavre, and et al., SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors, 2024. [50] L. Han, Y. Li, H. Zhang, P. Milanfar, D. N. Metaxas, and F. Yang, Svdiff: Compact parameter space for diffusion fine-tuning, in IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, 2023. [51] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, Qlora: Efficient finetuning of quantized llms, in Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [52] Q. Zhang, M. Chen, A. Bukharin, and et al., AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning, 2023. [53] C. Li, H. Farkhoor, R. Liu, and J. Yosinski, Measuring the intrinsic dimension of objective landscapes, in 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. [54] D. P. Kingma and J. Ba, Adam: method for stochastic optimization, in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. [55] T. Tieleman and G. Hinton, Lecture 6.5-rmsprop: Divide the gradient by running average of its recent magnitude, COURSERA: Neural networks for machine learning, no. 2, 2012. [56] N. Ding, X. Lv, Q. Wang, Y. Chen, B. Zhou, Z. Liu, and M. Sun, Sparse low-rank adaptation of pre-trained language models, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023. [57] P. Ren, C. Shi, S. Wu, M. Zhang, Z. Ren, M. Rijke, Z. Chen, and J. Pei, MELoRA: Mini-ensemble low-rank adapters for parameterefficient fine-tuning, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), August 2024. [58] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and A. Ghodsi, DyLoRA: Parameter-efficient tuning of pre-trained models using dynamic searchfree low-rank adaptation, in Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, 2023. [59] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei, and A. Roberts, The flan collection: Designing data and methods for effective instruction tuning, in International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, 2023. [60] A. Kopf, Y. Kilcher, D. von Rutte, S. Anagnostidis, Z. R. Tam, K. Stevens, A. Barhoum, D. Nguyen, O. Stanley, R. Nagyfi, S. ES, S. Suri, D. Glushkov, A. Dantuluri, A. Maguire, C. Schuhmann, H. Nguyen, and A. Mattick, Openassistant conversations - democratizing large language model alignment, in Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [61] T. Ridnik, E. Ben-Baruch, A. Noy, and L. Zelnik-Manor, ImageNet21K Pretraining for the Masses, 2021. [62] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST"
        },
        {
            "title": "This appendix provides supplementary material",
            "content": "to support the methodologies and findings presented in the main manuscript. It is organized into five key areas: Natural Language Understanding, Instruction Tuning, Image Classification, Image Generation, and Ablation Studies. Each section offers detailed insights into datasets, experimental protocols, and hyperparameter settings, ensuring the replicability and validation of our results. Section A: Analysis of the GLUE benchmark and hyperparameters for Natural Language Understanding tasks. Section B: Examination of the Alpaca dataset and LLaMA-2 model fine-tuning hyperparameters for Instruction Tuning. Section C: Overview of image classification datasets and Vision Transformer (ViT) fine-tuning configurations. Section D: Exploration of datasets for image generation and Stable Diffusion hyperparameters. Sections E: Ablation studies on learning rate and rank variations affecting model performance. A. Natural Language Understanding 1) GLUE Benchmark Details: The GLUE benchmark is framework for evaluating NLP models across nine tasks, such as CoLA, SST-2, and MRPC, focusing on grammaticality, sentiment, and semantic similarity. It includes diagnostic dataset for assessing linguistic phenomena, aiding in the development of robust NLP systems through transfer learning. For more details, see the GLUE Benchmark Overview. 2) Hyperparameters for GLUE Experiments: Table VII details the hyperparameters for GLUE experiments. Table VII HYPERPARAMETERS FOR GLUE TASKS"
        },
        {
            "title": "Max Length",
            "content": "3e-4 MNLI SST-2 4e-4 MRPC 3e-4 CoLA 2e-4 2e-4 QNLI 3e-4 QQP RTE 4e-4 STS-B 2e-4 10 10 20 20 10 20 20 30 128 128 512 128 512 512 512 512 Base: roberta-base, Batch: 32, Rank: 768, Alpha: 768 Modules: query, value, Warmup: 0.06 B. Instruction Tuning for 1) Alpaca Dataset Overview: The Alpaca dataset serves as crucial asset instruction tuning, consisting of 52,000 instruction-output pairs generated using OpenAIs text-davinci-003 engine. Its primary goal is to improve the instruction-following capabilities of language models by providing diverse array of instructional scenarios. The dataset is produced through the Self-Instruct framework, which includes modifications such as employing text-davinci-003 for instruction generation and implementing aggressive batch decoding to enhance efficiency. The Alpaca datasets diversity and high-quality annotations make it valuable resource for training models to perform well across various tasks. This section explores the distinctive features of the Alpaca dataset, highlighting its role in the fine-tuning process of language models. For more details, refer to the Hugging Face dataset card for Alpaca. 2) Hyperparameters for LLaMA-2 Fine-tuning: Table VIII provides comprehensive overview of the hyperparameter settings employed during the fine-tuning of the LLaMA-2 model. These parameters are critical for optimizing model performance and ensuring robust convergence across various tasks. Table VIII HYPERPARAMETERS FOR INSTRUCTION TUNING"
        },
        {
            "title": "Base Model\nPrecision\nBatch Size\nMicro Batch Size\nLearning Rate\nNumber of Epochs\nRank\nAlpha\nTarget Modules\nCutoff Length\nSeed",
            "content": "LLaMA2-7B BF16 128 1 1e-3 3 1024 1024 proj, proj 256 42 C. Image Classification section introduces 1) Dataset Descriptions: This the datasets employed for image classification tasks, which include CIFAR10 [25], EuroSAT [30], StanfordCars [28], FLOWERS102 [27], FGVC [29], and CIFAR100 [26]. These datasets are selected to represent broad spectrum of visual concepts and complexities, ranging from small to large label spaces. 2) Hyperparameters for ViT Fine-tuning: The hyperparameter settings utilized for the fine-tuning of the Vision Transformer (ViT) model are detailed in Table IX. D. Image Generation 1) Dataset Details: The CustomConcept101 and FlickrFaces-HQ (FFHQ) datasets provide concept images for fine tuning our image generation model. FFHQ contains 70,000 high-resolution images (10241024) with diverse attributes such as age, ethnicity, and accessories. Images were sourced from Flickr, aligned, and cropped using dlib, excluding nonhuman subjects. For more information, see the FFHQ Dataset. 2) Hyperparameters for Stable Diffusion Fine-tuning: The hyperparameter settings utilized for the fine-tuning of the Stable Diffusion model are detailed in Table X. E. Ablation Studies 1) Learning Rate: This section provides an academic analysis of the impact of varying learning rates on model training. The visual representation, as detailed in 7, illustrates JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Table IX HYPERPARAMETERS FOR IMAGE CLASSIFICATION"
        },
        {
            "title": "Model\nLearning Rate\nBatch Size\nMax Epochs\nPrecision\nOptimizer\nLR Scheduler\nWarmup Steps\nTarget Modules\nRank\nAlpha\nSeed",
            "content": "vit-b16-224-in21k 3e-3 128 10 bf16 AdamW Linear 30 query, value 768 768 42 Table HYPERPARAMETERS FOR IMAGE GENERATION"
        },
        {
            "title": "Base Model\nVAE\nLearning Rate\nPrecision\nResolution\nTrain Batch Size\nOptimizer\nLR Scheduler\nLR Warmup Steps\nMax Train Steps\nRank\nAlpha\nSeed\nAdam Weight Decay\nTarget Modules",
            "content": "stable-diffusion-v1-5 sd-vae-ft-mse 5e-4 fp16 512 1 AdamW constant 15 1000 32 32 42 0.01 to k, to v, to q, to out Figure 7. Learning Rate Ablation Study. The figure demonstrates the effect of different learning rates on ViT-base model accuracy across FGVC, StanfordCars, CIFAR10, CIFAR100, EuroSAT, and Flowers datasets. the outcomes of the learning rate ablation study, while the accompanying table, referenced in XI, provides comprehensive quantitative data. 2) Rank Ablation: This subsection presents an analysis of the rank ablation study, examining the impact of different parameter ranks on model performance. Table XII summarizes the results. Table XI LR ABLATION FOR VIT-BASE: COMPARISON ON FGVC, STANFORDCARS, CIFAR10, CIFAR100, EUROSAT, AND FLOWERS. ALL RANKS SET TO 768. BOLD INDICATES BEST RESULTS. LR FGVC StanfordCars CIFAR10 CIFAR100 EuroSAT Flowers Avg. acc acc acc acc acc acc ViT-Base LR Ablation 1e-1 5e-2 8e-3 5e-3 3e-3 6e-4 3e-4 1e-5 6.54 9.69 38.37 41.13 40.44 27.51 21.42 2.25 0.85 3.69 63.38 65.25 62.37 41.57 31.55 2.10 26.21 32.96 96.86 97.84 98.20 98.28 98.20 96. 6.71 18.28 89.30 89.89 90.61 90.05 89.56 73.53 48.70 61.06 97.69 98.50 98.95 98.73 98.23 72.53 48.31 95.49 97.75 98.53 98.92 97.65 94.51 0.88 22.89 36.86 80.56 81.86 81.58 75.63 72.25 41.22 Table XII VIT RANK ABLATION STUDY ON FGVC, STANFORDCARS, CIFAR10, CIFAR100, EUROSAT, AND FLOWERS DATASETS. DIFFERENT RANKS INDICATE VARYING PARAMETER COUNTS. #PARAMS: TUNABLE PARAMETERS (M). THE FIRST SECTION SHOWS THE BASE VERSION, FOLLOWED BY THE LARGE-SCALE ABLATION. BOLD DENOTES OPTIMAL LOLDU RESULTS. Rank Params FGVC StanfordCars CIFAR10 CIFAR100 EuroSAT Flowers ViT-Base Rank Ablation 1 8 16 32 64 128 256 512 768 24 192 384 768 1536 3072 6144 12288 18456 FT LoRA 85.8 581 27.59 28.28 31.13 32.75 34.01 34.91 36.38 38.48 42.15 54.84 25.16 43.95 48.40 50.87 53.00 55.09 58.20 61.44 63.68 66.66 79.78 45.38 96.81 97.47 97.76 97.82 97.96 98.07 98.06 98.17 98. 98.92 98.78 86.67 89.84 88.48 88.76 89.60 89.89 90.18 90.30 91.21 92.38 92.02 95.25 96.14 96.74 97.28 97.60 98.20 98.62 98.83 99.21 99.05 98.44 98.33 98.53 98.53 98.63 98.53 98.53 98.63 98.63 98. 98.43 97."
        }
    ],
    "affiliations": [
        "Center for Future Media and School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu 611731, China"
    ]
}