{
    "paper_title": "Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal Skills",
    "authors": [
        "Yuquan Xie",
        "Zaijing Li",
        "Rui Shao",
        "Gongwei Chen",
        "Kaiwen Zhou",
        "Yinchuan Li",
        "Dongmei Jiang",
        "Liqiang Nie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent efforts to leverage the Multi-modal Large Language Model (MLLM) as GUI agents have yielded promising outcomes. However, these agents still struggle with long-horizon tasks in online environments, primarily due to insufficient knowledge and the inherent gap between offline and online domains. In this paper, inspired by how humans generalize knowledge in open-ended environments, we propose a Hierarchical Multimodal Skills (HMS) module to tackle the issue of insufficient knowledge. It progressively abstracts trajectories into execution skills, core skills, and ultimately meta-skills, providing a hierarchical knowledge structure for long-horizon task planning. To bridge the domain gap, we propose the Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm, which efficiently leverages skills acquired in offline environments to reduce the action search space during online tree exploration. Building on HMS, we propose Mirage-1, a multimodal, cross-platform, plug-and-play GUI agent. To validate the performance of Mirage-1 in real-world long-horizon scenarios, we constructed a new benchmark, AndroidLH. Experimental results show that Mirage-1 outperforms previous agents by 32\\%, 19\\%, 15\\%, and 79\\% on AndroidWorld, MobileMiniWob++, Mind2Web-Live, and AndroidLH, respectively. Project page: https://cybertronagent.github.io/Mirage-1.github.io/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 7 8 3 0 1 . 6 0 5 2 : r Mirage- : Augmenting and Updating GUI Agent with Hierarchical Multimodal Skills Yuquan Xie1, Zaijing Li1 2, Rui Shao1, Gongwei Chen1 Kaiwen Zhou3, Yinchuan Li3, Dongmei Jiang2, Liqiang Nie1 1Harbin Institute of Technology, Shenzhen 2Peng Cheng Laboratory 3Huawei Noahs Ark Lab {lzj14011,xieyuquan20016}@gmail.com, {shaorui,nieliqiang}@hit.edu.cn https://cybertronagent.github.io/Mirage-1.github.io/"
        },
        {
            "title": "Abstract",
            "content": "Recent efforts to leverage the Multi-modal Large Language Model (MLLM) as GUI agents have yielded promising outcomes. However, these agents still struggle with long-horizon tasks in online environments, primarily due to insufficient knowledge and the inherent gap between offline and online domains. In this paper, inspired by how humans generalize knowledge in open-ended environments, we propose Hierarchical Multimodal Skills (HMS) module to tackle the issue of insufficient knowledge. It progressively abstracts trajectories into execution skills, core skills, and ultimately meta-skills, providing hierarchical knowledge structure for longhorizon task planning. To bridge the domain gap, we propose the Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm, which efficiently leverages skills acquired in offline environments to reduce the action search space during online tree exploration. Building on HMS, we propose Mirage-1, multimodal, crossplatform, plug-and-play GUI agent. To validate the performance of Mirage-1 in real-world long-horizon scenarios, we constructed new benchmark, AndroidLH. Experimental results show that Mirage-1 outperforms previous agents by 32%, 19%, 15%, and 79% on AndroidWorld, MobileMiniWob++, Mind2Web-Live, and AndroidLH, respectively."
        },
        {
            "title": "Introduction",
            "content": "The GUI agent that can automate operations on graphical user interfaces, such as mobile devices, represents one of the hallmark applications of artificial general intelligence [11]. Recently, GUI agents driven by Large Language Models (LLMs) have achieved remarkable progress in practical applications [47, 24, 21, 46]. Despite these advancements, these agents [6, 35, 31] still struggle to execute long-horizon tasks effectively in online environments. There are two main challenges that arise: (1) Existing GUI agents lack explicit task planning, which limits their ability to perform complex, multi-step operations in dynamic environments. Long-horizon tasks consist of multiple interdependent sub-goals, each demanding that the agent possess sufficient knowledge to grasp the purpose, implementation, and interconnections of Preprint. Under review. Figure 1: Compared with existing GUI agents, Mirage1 improves performance by: (1) Hierarchical Multimodal Skills (HMS) module that enhances the agents capability in long-horizon task planning, and (2) SkillAugmented MCTS algorithm that leverages the HMS to search feasible trajectories in online environments. these sub-goals. However, as illustrated in Figure 1, existing agents make decisions conditioned only on the final goal, lacking fine-grained task planning to guide their actions [35, 6, 39, 3]. (2) domain gap exists between offline and online environments, undermining the agents generalization ability. In the online environment, app layouts and button functionalities can change with version updates. Moreover, while apps may share certain similarities, their inherent differences often cause agents to fail when attempting to apply previously acquired knowledge to new applications [42, 44, 7, 43]. As result, agents trained solely on offline datasets often exhibit subpar performance in online environments [6, 35, 39]. In this paper, we propose agent Mirage-1, which tackles the aforementioned challenges in two aspects. (1) Hierarchical Multimodal Skills (HMS) module for long-horizon planning. It first summarizes offline trajectories into Execution Skills, then abstracts similar Execution Skills into general Core Skills, and finally further abstracts them into Meta Skills. By clearly outlining the implementation (Execution Skills) and functionality (Core Skills) of each sub-goal while establishing their interrelationships (Meta Skills), HMS provides the comprehensive hierarchical knowledge for long-horizon task planning. (2) Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm for knowledge exploration in online settings. To enable the agent to adapt to online environments, we introduce Monte Carlo Tree Search (MCTS) to facilitate exploration and acquisition of online knowledge. Further, we propose Skill-Augmented MCTS (SA-MCTS), which leverages the skills in HMS to perform planning during the search phase. This integration reduce the search space during tree exploration and enhance the reliability of candidate actions to better align with task objectives. Through the SA-MCTS algorithm, the agent quickly acquires skills from the online environment. By integrating and updating skills from both offline and online domains, the domain gap is bridged, leading to improved performance in the online environment. We conducted extensive experiments in online environments of mobile and web, and the results substantiate Mirage-1s superior performance. Compared to current state-of-the-art (SOTA) methods, Mirage-1 achieves average performance gains of 32%, 19%, and 15% on AndroidWorld, MobileMiniWob++, and Mind2Web-Live, respectively. Furthermore, to verify the GUI agents ability to execute long-horizon tasks in real-world scenarios, we propose an online mobile benchmark, named AndroidLH, which includes 30 long-horizon tasks derived from 12 real-world Android applications. The experimental results show that Mirage-1 achieves success rate exceeding 79% on AndroidLH, outperforming previous agents. In summary, our contributions are as follows: We propose cross-platform, plug-and-play GUI agent, Mirage-1. The experimental results demonstrate that Mirage-1 exhibits superior performance on AndroidWorld, MobileMiniWob++, and Mind2Web-Live. Moreover, on our proposed long-horizon tasks benchmark, AndroidLH, Mirage-1 also achieved the best performance. To broaden the knowledge base of general MLLMs within the GUI domain, we propose Hierarchical Multimodal Skills (HMS). Summarizing historical trajectories into hierarchical skills efficiently enriches the skill of GUI agents, ultimately leading to significant improvement in online long-horizon tasks. To address the domain gap between offline and online environments, we introduce the SA-MCTS algorithm. It leverages the HMS along with the MCTS algorithm to explore unfamiliar tasks, thereby rapidly expanding Mirage-1s knowledge in online environments."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 GUI Agents Graphical User Interface (GUI) agents are designed to autonomously interact with digital systems through the emulation of human-like interactions, including clicking and typing. Previous approaches to GUI automation have predominantly relied on HTML or Accessibility Trees for element grounding and action execution [47, 24, 13, 4, 8, 46, 36]. However, there are differences between this approach and the way humans use digital devices. Recently, several studies [35, 6, 39, 20, 37, 3, 5, 18, 31] have proposed the development of GUI agent based on pure vision. These approaches utilize multimodal planner to plan the current task and generate actions, alongside grounding model to locate elements. Despite their effectiveness on offline datasets, these approaches exhibit limitations in 2 Figure 2: The Mirage-1 framework comprises Hierarchical Planner, an Operator, Decision Reflector, and Hierarchical Multimodal Skills Module (HMS). To bridge the offline-online domain gap, Skill-Augmented Monte Carlo Tree Search (SA-MCTS) is employed for unseen task exploration, with successful trajectories expanding HMS capabilities. The Hierarchical Planner retrieves Core Skills from HMS and decomposes task goals into sub-goals for Operator execution. The Decision Reflector leverages Execution Skills to assess task execution feasibility. completing long-horizon tasks within complex and dynamic online environments. Mirage-1 addresses long-horizon task planning by integrating HMS, which provides hierarchical knowledge. Mirage-1 utilizes the SA-MCTS algorithm to explore unknown tasks in an online environment, thereby reducing the domain gap between offline and online settings. 2.2 Memory in Agents In agent-environment interactions, memory mechanisms are crucial for planning, reasoning, and reflection. Recent studies have focused on textual memory representations for their interpretability and efficiency [45]. Notable approaches include Agent Workflow Memory [34], leveraging recurring patterns for planning, Synapse [47] utilizing abstracted state-action trajectories, and Agent [1] incorporating task experiences in hierarchical planning. Despite their effectiveness, text-based approaches exhibit limitations in processing multimodal information. Although [16] introduced multimodal memory in Optimus-1, its graph-based representation faces scalability challenges in GUI environments. These limitations are addressed by the hierarchical multimodal skills module in Mirage-1, which progressively abstracts historical trajectories into structured, hierarchical skills. The hierarchical multimodal skills module provides comprehensive hierarchical knowledge for long-horizon task planning."
        },
        {
            "title": "3 Mirage-1",
            "content": "In this section, we first define the task formulation, then introduce the framework of Mirage-1, followed by an in-depth overview of Hierarchical Multimodal Skills (HMS), and finally introduce the Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm. 3.1 Task Formalization Formally, we denote the Environment as E, the Observation Space as O, the Action Space as A, and the Multimodal Agent as M. Given task goal G, at time step t, the agent receives the 3 observation ot and context Ct as input. Ct consists of historical actions ai and summary si, defined as: Ct = {g, a1, s1, . . . , at1, st1}. The agent predicts the action at and its corresponding description dt at time t, where {at, dt} = M(ot, Ct). The action at interacts with the environment E, resulting in the observation ot+1 at the next time step. Through continuous interactions, we obtain trajectory = {(o1, a1), (o2, a2), (o3, a3), . . . , (oT , aT )}, where represents the length of the trajectory. 3.2 Framework of MirageAs illustrated in Figure 2, Mirage-1 consists of four modules: Hierarchical Planner, Operator, Decision Reflector, and Hierarchical Multimodal Skills Module. Hierarchical Planner. Given task goal G, the Hierarchical Planner AP first retrieves the candidate Meta Skills from HMS by cosine similarity calculation: m1, . . . , mk CoSim(G, mi) (1) where CoSim() denotes the cosine similarity, mi represents the Meta Skills in HMS, and indicates the number of candidates. Subsequently, AP evaluates these candidates to determine the most appropriate Meta Skill mc: mc = AP (G, m1, . . . , mk) (2) Finally, AP leverages the Core Skills associated with mc as conditions to generate sequence of sub-goals: g1, g2, . . . , gn = AP (G, c1, . . . , cr) (3) where and indicate the number of Core Skills and the number of sub-goals, respectively. We implement AP using GPT-4o [12] as the reasoning model and text-embedding-3-small for cosine similarity computations. Operator. Given the current sub-goal gi generated by AP , along with observation ot and historical summary S1...t at step t, the Operator AO generates an action at and its description dt: at, dt = AO(gi, ot, S1...t) (4) For interactive actions such as click, double-click, long press, text input, and scroll, AO invokes visual grounding model to locate the target element coordinates: coordt = AO(dt, ot) (5) For predefined actions such as scrolling up or returning to previous screen, AO directly invokes the corresponding system API to execute the action. For both mobile and web environments, we implement our Operator by combining agents with grounding models. Specifically, we use M3A Agent [24] for mobile environments and WebCanvas Agent [21] for web environments. To demonstrate the generalization of the proposed method, we employ various grounding models, including OS-Atlas [35], UGround [6], Aria-UI [39], and UI-TARS [23]. Decision Reflector. The GUI environment presents unique challenges where each action can significantly impact subsequent states. The Decision Reflector AR addresses this challenge by assessing the effectiveness of the action and preventing potential state conflicts. Operating at fixed frequency, AR analyzes each predicted action at within its context. It takes the task goal G, the current sub-goal gi, observation ot, and historical summary S1...t as input while leveraging the Execution Skills ε in HMS. AR then evaluates potential state changes and assigns quality score (range 0-10): p, = AR(G, gi, ot, at, S1...t, ε) (6) When falls below 5, the Operator AO receives regeneration signal to produce an alternative action. Experimental results in Section 4.6 validate that this reflection mechanism substantially improves task success rates in online settings. Algorithm 1 Skill-Augmented MCTS Input: M: Multimodal Agent, DT : task pool, : number of iterations, : MCTS tree depth, S: number of sub-goals to sample, B: replay buffer, K0: hierarchical multimodal skill module initialized by offline datasets Output: KN , the latest hierarchical multimodal skill module for = 1 to do Ki Ki1 for each task in DT do Initialize MCTS tree REE with root h0. for = 1 to do Iterative skill module refinement loop Inherit skills from previous iteration Process each task to gather experience lt SelectLeafNode(T REE, h0) Sub-Goal Expansion: , . . . , gS {g1 State {r1 , . . . , rS Select sub-goal Sub-Goal Rollout: (τt, Rrollout Backpropagation: } SampleSubGoals(Ki, lt, S) } EstimateValues(M, lt, {gj = arg maxj rj }S j=1) ) ExecuteSubGoal(M, Ki, lk, k) Select leaf node lk Generate sub-goals via Ki Evaluate sub-goals Choose best sub-goal Execute using skills Update value estimates along the path from lt to root Store (g ) in node , τt, Rrollout Update (), Q() from lk to root end for Human evaluates whether success or not and adds successful trajectories to the replay buffer end for Ki RefineSkillModule(Ki1, B) end for End of task processing loop Refine Ki End of skill refinement iterations 3.3 Hierarchical Multimodal Skills Humans exhibit remarkable generalization capabilities when learning skills across different graphical user interface (GUI) environments, due to they are able to abstract specific tasks into generalizable skills [30]. When faced with new task, humans draw on previously learned skills as guidance. For example, once person learns to perform the task Search for the weather in London on April 1st, they can abstract it into the skill, search for the weather of given location and date. The process typically involves fundamental steps such as launching browser, entering query, and executing search, these steps remain consistent across different systems. Consequently, the person can effortlessly apply this skill to similar tasks in new contexts, such as Search for the weather in New York on March 2nd. Inspired by this, we propose the Hierarchical Multimodal Skills (HMS) module, which abstracts specific tasks into generalizable skills and provides structured guidance for task planning. This enables the agent to leverage prior experiences as reusable planning references when encountering new tasks. As shown in Figure 2, we divide the Hierarchical Multimodal Skills into three levels: Meta Skill, Core Skill, and Execution Skill. Execution Skill represents the summarization and formalization of historical trajectories, which enhances the efficiency and convenience of knowledge development and retrieval during reflection. To identify the intention (referred to as the step-goal) behind each action in trajectory J, we provide GPT-4o with the temporal sequence of observations (ot1 and ot) encompassing the executed action at, in conjunction with the current sub-goal g. GPT-4o is then instructed to analyze the changes in these observations to infer the corresponding step-goal si for the current action at. Execution Skill is represented as ε = (g, o1, s1, a1, . . . , ot1, st1, at1, ot). For example, Search for the weather in London on April 1st is an Execution Skill, as it specifies particular date and city. Core Skill is general function derived from multiple Execution Skills. It is represented as function, and the agent can obtain step goals for completing specific task by invoking this function. For example, search for the weather of given location and date can be represented as Core Skill: search_weather(location, date). We employ GPT-4o to consolidate functionally similar Execution Skills into generalized Core Skill. Thus, Core Skill represents universal pattern for class of tasks, while the associated Execution Skills summarize the specific task trajectories related to it. 5 Table 1: Performance comparison on AndroidWorld, MobileMiniWob++, and AndroidLH. means the results are reproduced under the same prompt setting. Method Operator AndroidWorld MobileMiniWob++ AndroidLH SR SR CR SR Rawles et al. GPT-4-Turbo+Choice Gemini 1.5Pro+Choice GPT-4o+Choice 30.6 19.4 41.9 Rawles et al. Zheng et al. GPT-4-Turbo + SoM 25.4 Gemini 1.5 Pro + SoM 22.8 36.6 GPT-4o + SoM 15.5 GPT-4-Turbo + SoM Wu et al. Mirage-1-O Gou et al. Mirage-1-U Yang et al. Mirage-1-A Qin et al. Mirage-1-T GPT-4o + OS-Atlas GPT-4o + OS-Atlas GPT-4o + UGround GPT-4o + UGround GPT-4o + Aria-UI GPT-4o + Aria-UI GPT-4o + UI-TARS GPT-4o + UI-TARS 31.9 42.2 32.8 40.5 37.1 41.4 36.2 43.1 - - - - - - - AxTree 59.7 57.4 - - - - Image+AXTree 67.7 40.3 - 66. Image - - - - - - 51.1 32.3% 60.9 48.4 23.4% 56.5 52.2 11.6% 62.0 53.3 19.1% 60.9 - - - 19.1% - 16.7% - 18.8% - 14.3% 33.3 34.9 38.1 34.9 36.5 31.7 30.2 27.0 50.8 39.7 55.6 31.7 49.2 42.9 55.2 - - - - - - - 13.3 16.7 20.0 16.7 20.0 23.3 13.3 - - - - - - - - - 16.7 88.1% 30.0 26.7 40.1% 36.7 23.3 55.2% 36.7 26.7 28.6% 40.0 - - - 79.6% - 37.4% - 57.5% - 49.8% Meta Skill is high-level aggregation of Core Skills. For example, the Meta Skill search_web() encompasses Core Skills such as search_news(query, date), search_weather(location, date), and search_wiki(query). We employed GPT-4o to consolidate Core Skills into Meta Skills. Notably, the same Core Skill may be applicable to different Meta Skills, thereby implicitly establishing the interrelationships between Meta Skills. The HMS module starts empty. Upon encountering new Execution Skill, it is first analyzed at the Meta Skill level. If no suitable Meta Skill category exists, GPT-4o is utilized to generate new Meta Skill, accompanied by comprehensive documentation. Subsequently, the existing skills associated with this Meta Skill are reviewed. GPT-4o evaluates whether these skills sufficiently abstract the tasks characteristics. If the existing skills are deemed inadequate, new Core Skill is created and categorized under the appropriate Meta Skill. To maintain memory retrieval efficiency, the Core Skill layer is regularly refined. Similar Core Skills are merged to enhance both retrieval accuracy and the overall representation of knowledge. 3.4 Skill-Augmented MCTS Due to the initial construction of the HMS based on offline trajectories, when deployed in online environments, the agent encounters significant domain gap. This occurs because offline trajectories often contain outdated information that diverges from the dynamic nature of online content. To address this challenge, we propose Skill-Augmented Monte Carlo Tree Search (SA-MCTS) approach for online exploration. Our HMS module improves the MCTS [9, 22] process by providing generalized planning strategies, while online exploratory techniques simultaneously enrich the skill base. This bidirectional interaction improves both the agents real-time adaptability and the universal applicability of knowledge. SA-MCTS is illustrated in Algorithm 1. During exploration, the agent generates sub-goals through hierarchical process: it first identifies relevant Meta Skills for the current task and then selects appropriate Core Skills in Meta Skills to generate specific sub-goals. This knowledge-guided approach improves both exploration efficiency and the agents ability to generalize across complex tasks. Successful trajectories are systematically processed: they are first formatted as Execution Skills, then abstracted into Core Skills, and finally categorized under appropriate Meta Skills. In summary, the SA-MCTS algorithm enhances MCTS exploration by incorporating offline-initialized HMS in two key aspects. First, instead of directly generating actions, the agent utilizes existing Meta Skills and Core Skills to decompose the task goal into sub-goals. Second, these skill-guided sub-goals direct the agent towards more promising exploration paths, significantly reducing ineffective searches. See Appendix for more details."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Settings We evaluate the proposed agent, Mirage-1, in both Mobile and Web online environments to demonstrate its cross-platform generalization. For Mobile, we evaluate Mirage-1s performance on AndroidWorld [24], an online benchmark within an Android emulator environment. We report task success rate as an evaluation metric. For Web, we employ Mind2Web-Live [21] as the evaluation benchmark. We use task success rate and micro-completion rate (measures the proportion of key nodes completed in all tasks) as evaluation metrics. For more details, please refer to Appendix F.1. 4.2 Baselines We conducted comprehensive comparison with various baselines in mobile and web environments. Mobile. We compare the baselines of three input modes: AXTree-only, image-AXTree, and imageonly. For AXTree-only, we employ GPT-4-Turbo, Gemini-1.5-Pro [28] and GPT-4o as Planner with choice-based selection [24]. For Image-AXTree, we compare GPT-4-Turbo, Gemini-1.5-Pro, GPT-4o, and SeeAct [46], all implementing SoM grounding methods [38]. For image-only, we employ GPT-4o as the Planner and utilize four grounding models trained in offline environments: OS-Atlas [35], UGround [6], Aria-UI [39], and UI-TARS [23]. Web. For text-only input, we employ the WebCanvas agent [21] with various Planner, including GPT-4-Turbo, GPT-4o, Gemini-1.5-Pro, and GPT-3.5-Turbo with choice-based selection. For image-only input, we remove the DOM tree components in the observation. We compare the performance of GPT-4o with OS-Atlas [35], UGround [6] and Aria-UI [39]. See Appendix F.2 for more details. 4.3 Implementation Details Mobile. We build Mirage-1 based on vision-only M3A Agent [24]. We used the trajectories in AITW [25] to build the initial HMS. We prompt GPT-4o to generate 200 unseen tasks and use SA-MCTS to expand HMS. See Appendix F.3 for more details. Web. We build Mirage-1 based on the vision-only WebCanvas agent [21]. We use the trajectories of Multimodal-Mind2Web [46] to build the initial HMS, which is the multimodal version of Mind2Web [4]. We prompt GPT-4o to generate 200 unseen tasks and use SA-MCTS to update HMS. 4.4 Experiments Result Mobile. As illustrated in Table 1, Mirage1 demonstrates performance improvements on the AndroidWorld benchmark, achieving relative gains of 32.3%, 23.4%, 11.6%, and 19.1% in task success rates compared to the OS-Atlas, UGround, Aria-UI, and UI-TARS, respectively. In MobileMiniWob++, Mirage-1 achieves improvements of 19.1%, 16.7%, and 18.8% over the current SOTA agent with OS-Atlas, UGround, and Aria-UI, respectively. Experimental results in Table 1 demonstrate that Mirage-1 significantly improves the performance of offlinetrained agents in online environments. Table 2: Performance comparison on Mind2Web-Live. We report the completion rate (CR) and the task success rate (SR). means the results are reproduced under the same prompt setting. Method Operator CR SR HTML Tree Pan et al. GPT-4-Turbo + Choice GPT-4o + Choice Gemini-1.5-Pro + Choice GPT-3.5-Turbo + Choice 44.3 47.6 44.6 40.2 21.1 22.1 22.3 16.5 Image Wu et al. GPT-4o + OS-Atlas Mirage-1-O GPT-4o + OS-Atlas Gou et al. GPT-4o + UGround Mirage-1-U GPT-4o + UGround Yang et al. GPT-4o + Aria-UI Mirage-1-A GPT-4o + Aria-UI 50.4 51.2 50.8 51.4 51.9 53.3 18.4 21.315.8% 19.2 22.115.1% 22.1 24.08.6% 7 Web. As shown in Table 2, Mirage-1 achieved superior performance across various Operators compared to the baselines. Specifically, we observed improvements in task success rates of 15.8%, 15.1%, and 8.6% for OS-Atlas, UGround, and Table 3: Ablation study on different sources of HMS and components of HMS Ablation Setting AndroidWorld Ablation Setting AndroidWorld Offline Skills Online Skills SR 31.9 34.5 37.9 42.2 Execution. Core. Meta. SR 31.9 32.7 35.3 38.8 42.2 (a) Ablation study on different sources of HMS (b) Ablation study on the components of HMS Table 4: Experiments about the inference time and efficiency of memory mechanisms Method Mobile-Agent-v2 [31] Mobile-Agent-E [33] Mirage-1 AndroidLHSR 30.0 33.3 36.7 Inference time (s) / step 43.5 30.1 23.4 Memory Mechanisms AndroidWorldSR exemplar memory [47] workflow [34] HMS 34.5 37.9 42. (a) Comparison of inference time (b) Comparison of different memory mechanisms Aria-UI, respectively. These consistent improvements across diverse web-based tasks suggest our HMS exhibits strong generalization capabilities in web environments. The experimental results in Tables 1 and 2 demonstrate Mirage-1s cross-platform generalization and its adaptability to various Operators, highlighting the flexibility of the Mirage-1 architecture. 4.5 Long-horizon Benchmark: AndroidLH While existing online benchmarks [24] focus on short-horizon, single-app tasks, we introduce AndroidLH, an Android benchmark for long-horizon, multi-app operations. AndroidLH includes 30 diverse tasks across multiple applications, generated via GPT-4o to mirror real-world application scenarios. For evaluation, we adopt AndroidWorlds system state-based completion verification method. See Appendix for more details. As shown in Table 1, Mirage-1 achieves substantial and consistent improvements across multiple Operators in AndroidLH. Specifically, Mirage-1 exhibits significant performance gains with mean improvements of 53.3% in completion rate and 56.1% in success rate compared to the baseline agents. We attribute this to the fact that HMS provides Mirage-1 with the essential skills required for long-horizon planning in the online environment. 4.6 Ablation Study Sources of HMS. As shown in Table 3a, our ablation experiments demonstrate the relative contributions of different source skills. Removing offline skills led to the most significant performance degradation of 22.3%, highlighting their fundamental importance. The subsequent removal of online skills resulted in an 11.3% decrease. Experimental results reveal that offline skills, serving as the foundation of HMS, provide the necessary knowledge for long-horizon planning. Meanwhile, online skills further improve Mirage-1s capabilities in online environments. Components of HMS. Table 3b illustrates the contributions of different types of skills within HMS. When Execution Skills were removed, performance decrease of 8.7% was observed. The removal of Meta Skills resulted in more substantial performance degradation of 19.5%, indicating their fundamental role in skill organization and contextual coherence. When Core Skills and Meta Skills were removed, the performance dropped significantly by 29.1%. The experimental results Figure 3: We compared three online exploration methods: direct exploration, MCTS, and SA-MCTS on 30 GPT-4o generated tasks. Results show that SA-MCTS acquires more skills than the other methods, demonstrating its superior effectiveness in online exploration. Figure 4: Case study example where Mirage-1 completes long-horizon task. First, Mirage-1 retrieval Meta Skills, Core Skills, and Execution Skills from HMS. Then Mirage-1 generates plans for long-horizon tasks. With the help of Core Skills, Mirage-1 can easily achieve sub-goals. indicate that Core Skills and Meta Skills play critical role within HMS, providing Mirage-1 with abstract and comprehensive knowledge. Superiority of HMS. We compare Mirage-1 with popular agent frameworks in terms of performance and inference efficiency. As shown in the Table 4a, Mirage-1 outperforms Mobile-Agent-v2 [31] and Mobile-Agent-E [33] on AndroidLH, while also achieving faster inference speed. We attribute this to Mirage-1 benefiting from the HMS module, which enables efficient retrieval and planning, rather than performing multi-round planning and decision-making at every step like other agents. Moreover, we compare the proposed HMS module with other memory mechanisms. As shown in Table 4b, experimental results demonstrate that HMS outperforms existing memory mechanisms in the AndroidWorld benchmark. Online Exploration Strategy. As shown in Figure 3, the SA-MCTS achieved remarkable improvements in skill acquisition efficiency, demonstrating 2.8 increase compared to the Direct approach and 41% improvement over standard MCTS. The results show that HMS can significantly improve the success rate of MCTS exploration, demonstrating the strong adaptability of HMS. 4.7 Case Study To show the Mirage-1s performance in executing long-horizon tasks in real-world scenarios, we conducted case study, as shown in Figure 4. To accomplish the task goal, Mirage-1 first needs to create new contact and accurately fill in the contacts information. It then must also learn how to send message to the specified contact. The qualitative case in Figure 4 shows that Mirage-1 effectively retrieves and leverages skills in HMS to complete the long-horizon task."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose Hierarchical Multimodal Skills module (HMS) that addresses the challenge of insufficient prior knowledge in long-horizon task planning. To address the domain gap between offline and online, Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm is proposed. This algorithm effectively utilizes offline-acquired skills to reduce the action search space during online tree exploration. On top of HMS, we propose multimodal agent Mirage-1. Experimental results demonstrate that Mirage-1 achieves superior performance compared to SOTA GUI agents, particularly in long-horizon tasks."
        },
        {
            "title": "References",
            "content": "[1] Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s: An open agentic framework that uses computers like human. arXiv preprint arXiv:2410.08164, 2024. [2] Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, and Liqiang Nie. Lion: Empowering multimodal large language model with dual-level visual knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2654026550, 2024. [3] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. [4] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36, 2024. [5] Zhiqi Ge, Juncheng Li, Xinglei Pang, Minghe Gao, Kaihang Pan, Wang Lin, Hao Fei, Wenqiao Zhang, Siliang Tang, and Yueting Zhuang. Iris: Breaking gui complexity with adaptive focus and self-refining. arXiv preprint arXiv:2412.10342, 2024. [6] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. [7] Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, et al. Studying large language model generalization with influence functions. arXiv preprint arXiv:2308.03296, 2023. [8] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023. [9] Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. Reasoning with language model is planning with world model. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 81548173, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.507. URL https://aclanthology.org/2023.emnlp-main.507/. [10] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. WebVoyager: Building an end-to-end web agent with large multimodal models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 68646890, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.371. URL https://aclanthology.org/2024.acl-long. 371/. [11] Siyuan Hu, Mingyu Ouyang, Difei Gao, and Mike Zheng Shou. The dawn of gui agent: preliminary case study with claude 3.5 computer use. arXiv preprint arXiv:2411.10323, 2024. [12] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [13] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, et al. Autowebglm: large language modelbased web navigating agent. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 52955306, 2024. [14] Hao Li, Qi Lv, Rui Shao, Xiang Deng, Yinchuan Li, Jianye Hao, and Liqiang Nie. Star: Learning diverse robot skill abstractions through rotation-augmented vector quantization. arXiv preprint arXiv:2506.03863, 2025. 10 [15] Wei Li, Bing Hu, Rui Shao, Leyang Shen, and Liqiang Nie. Lion-fs: Fast & slow videolanguage thinker as online video assistant. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 32403251, 2025. [16] Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, and Liqiang Nie. Optimus-1: Hybrid multimodal memory empowered agents excel in long-horizon tasks. arXiv preprint arXiv:2408.03615, 2024. [17] Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, and Liqiang Nie. Optimus-2: Multimodal minecraft agent with goal-observation-action conditioned policy. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 90399049, 2025. [18] Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for generalist gui agent. In NeurIPS 2024 Workshop on Open-World Agents, 2024. [19] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. In International Conference on Learning Representations (ICLR), 2018. URL https://arxiv.org/abs/1802.08802. [20] Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent. arXiv preprint arXiv:2408.00203, 2024. [21] Yichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang, Shuyan Zhou, Tongshuang Wu, et al. Webcanvas: Benchmarking web agents in online environments. arXiv preprint arXiv:2406.12373, 2024. [22] Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents. arXiv preprint arXiv:2408.07199, 2024. [23] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. [24] Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024. [25] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36, 2024. [26] Leyang Shen, Gongwei Chen, Rui Shao, Weili Guan, and Liqiang Nie. Mome: Mixture of multimodal experts for generalist multimodal large language models. arXiv preprint arXiv:2407.12709, 2024. [27] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. [28] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [29] Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali Ahmed, Tyler Jackson, Shibl Mourad, and Doina Precup. Androidenv: reinforcement learning platform for android. arXiv preprint arXiv:2105.13231, 2021. [30] Avinash Vaidya, Henry Jones, Johanny Castillo, and David Badre. Neural representation of abstract task structure during generalization. eLife, 10:e63226, mar 2021. ISSN 2050-084X. doi: 10.7554/eLife.63226. URL https://doi.org/10.7554/eLife.63226. 11 [31] Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. arXiv preprint arXiv:2406.01014, 2024. [32] Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158, 2024. [33] Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, and Heng Ji. Mobile-agent-e: Self-evolving mobile assistant for complex tasks. arXiv preprint arXiv:2501.11733, 2025. [34] Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory. arXiv preprint arXiv:2409.07429, 2024. [35] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. [36] Bin Xie, Rui Shao, Gongwei Chen, Kaiwen Zhou, Yinchuan Li, Jie Liu, Min Zhang, and Liqiang Nie. Gui-explorer: Autonomous exploration and mining of transition-aware knowledge for gui agent. arXiv preprint arXiv:2505.16827, 2025. [37] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. [38] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. [39] Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Aria-ui: Visual grounding for gui instructions. arXiv preprint arXiv:2412.16256, 2024. [40] Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023. [41] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. [42] Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Qingwei Lin, Saravan Rajmohan, et al. Large language model-brained gui agents: survey. arXiv preprint arXiv:2411.18279, 2024. [43] Xingxuan Zhang, Jiansheng Li, Wenjing Chu, Junjia Hai, Renzhe Xu, Yuqing Yang, Shikai Guan, Jiazheng Xu, and Peng Cui. On the out-of-distribution generalization of multimodal large language models. arXiv preprint arXiv:2402.06599, 2024. [44] Yanzhe Zhang, Tao Yu, and Diyi Yang. Attacking vision-language computer agents via pop-ups. arXiv preprint arXiv:2411.02391, 2024. [45] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. survey on the memory mechanism of large language model based agents. arXiv preprint arXiv:2404.13501, 2024. [46] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. [47] Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. Synapse: Trajectory-as-exemplar prompting with memory for computer control. In The Twelfth International Conference on Learning Representations, 2023."
        },
        {
            "title": "A Limitations",
            "content": "Multimodal large language models (MLLMs) possess inherent strengths and limitations [2, 26]. Leveraging their capabilities in instruction understanding and multimodal perception [17], agents have achieved promising results across various domains [15, 14]. However, issues related to the interpretability of their outputs and potential safety risks remain significant concerns that cannot be overlooked. Moreover, although demonstrating promising results on the online evaluation benchmark, the current implementation exhibits computational overhead in inference processing. critical architectural limitation exists in the integration of planner and grounding components: the grounding models interpretation accuracy of planner outputs directly impacts task execution success. Misinterpretation of planning results by the grounding model can lead to imprecise spatial positioning and subsequent task failures."
        },
        {
            "title": "B Broader Impact",
            "content": "The large language model exhibits an inherently uncontrollable output in the planning, action generation, and reflection phases, potentially producing outputs that violate safety constraints or ethical guidelines. Our evaluation framework is implemented within web and mobile environments. Furthermore, model inputs may contain sensitive user information, introducing potential privacy vulnerabilities in inference time."
        },
        {
            "title": "C GUI environment",
            "content": "C.1 Mobile Following the settings of AndroidWorld [24], we create an Android emulator with the hardware set as Pixel 6, the System Image as Tiramisu, and the API level as 33. AndroidWorld determines task completion by utilizing predefined evaluation functions that assess system states, including database records and device information. This automated verification mechanism uses system-level indicators to validate the execution status of the task. Observation Space. AndroidWorld includes three observation types: Screenshots, Accessibility tree, and UI elements. Mirage-1 only receives the current screenshot of the device. The screenshot resolution is 2400 1080 3. Action Space. The action space of Mirage-1 includes: Click: Simulates touch events as specified coordinates Text input: Simulates typing in focused text fields Navigation: Sends navigate home/ navigate back key events Scrolling: Executes swipes in four directions(up, down, left, right) App launching: Starts specified applications Status: Reports if the task is in progress, complete, or infeasible. Answer: Provides responses, which are needed for information retrieval tasks Wait: No-op useful for loading screens and UI transitions Clear Text: helpful operation to clear text in text fields C.2 Web Following the settings of Mind2Web-Live [21], we use playwright 1 to simulate the web environment. Mind2Web-Live validates task completion through predetermined key nodes that are guaranteed to execute. The system evaluates the current task status by analyzing both the active URL address and the interaction metadata of clicked elements. 1https://github.com/microsoft/playwright 13 Observation Space. Mind2Web-Live includes two observation types: Screenshots and Accessibility tree. Mirage-1 only receives current webpage capture. The screenshot resolution is 1080 720 3. Action Space. The action space of Mirage-1 includes: Goto: Goto specific URL Google Search: use Google to search content with keywords Click: Mouse click event with specified coordinates Hover: Mouse hover event with specified coordinates Fill: Typing text in focused text fields Go Back: Go to the previous webpage"
        },
        {
            "title": "D AndroidLH",
            "content": "AndroidLH comprises 30 long-horizon tasks spanning 12 applications. As shown in Table 5, the table below lists all tasks. We have leveraged AndroidWorlds task template and evaluation mechanisms to create logical combinations of different tasks that reflect real-world usage scenarios. For evaluation, we adopt AndroidWorlds system state-based completion verification method while introducing an additional metric, Completion Rate (CR), which measures the proportion of completed subtasks. This metric provides finer-grained insights into task progress, particularly for complex, multi-step tasks. SA-MCTS SA-MCTS consists of four phases: selection, expansion, simulation, and backpropagation. E.1 Selection The selection phase uses the Upper Confidence Bound (UCB1) formulation of MCTS to select nodes which aims to balance exploration and exploitation. We denote the agent state with ht. We consider the value function Q(ht, g) which represents the estimated value of executing sub-goal at state ht. At each new node lt, we sample proposal sub-goals g1 . We initialize all values Q(ht, g) to zero. We use use GPT-4o to produce feedback score for each sub-goal by asking it to rank the generated sub-goals by its perceived utility in helping the agent complete the user task. After the initial selection, we select the sub-goal to explore based on the standard MCTS UCB1 formulation: , . . . , gS (cid:34) = arg max ,...,lS l1 Q(ht, g) + cexp (cid:115) (cid:35) log (ht) 1 + (ht+1) (7) where (ht) is the visitation frequency of state ht, and cexp is an exploration constant. E.2 Expansion and Simulation Based on the preceding section, we select and execute sub-goals in the environment to reach new node. Beginning from the selected state nodes trace, we roll out the trajectory until terminal state is reached. The environment returns reward at the end of the trajectory, R, where = 1 if the agent completes the sub-goal and = 0 otherwise. E.3 Backpropagation We then backpropagate this reward by updating the values of each node bottom-up from the leaf node to the root as follows: Q(ht, gi t) (ht, gi t) + Q(ht, gi t)N (ht, gi t) + 1 t) + 1 (ht, gi t) (ht, gi 14 (8) (9)"
        },
        {
            "title": "F Experiments",
            "content": "F.1 Experimental Settings To demonstrate the generalization of Mirage-1 across diverse domains, we evaluated it both in mobile and web online environments following previous work ([32], [31], [40], [10]). To further verify the effectiveness of the method, we selected various grounding models, including OS-Atlas [35], UGround [6], Aria-UI [39], and UI-TARS [23]. All experiments were conducted on 8x NVIDIA L40 GPUs. Mobile. We evaluate on AndroidWorld [24], an online benchmark within an Android emulator environment. It consists of 116 tasks that span 20 applications and assesses the success or failure of the task by examining the final state of the virtual devices system. We also include 92 MobileMiniWob++ tasks provided by AndroidWorld, which adapts the MiniWob++ Web agent environment [19] to AndroidEnv [29], the same environment as AndroidWorld. We report task success rate as an evaluation metric, which represents the proportion of successfully completed tasks relative to the total number of tasks. Web. We utilize the test set from Mind2Web-Live [21], an online evaluation benchmark designed for dynamic web environments. It includes 104 tasks and adds evaluation functions that automatically assess task success or failure. Specifically, it defines and annotates key nodes for each task, which represent critical steps that must be completed for task to be deemed successful, regardless of the trajectory taken by the agent. We use standard metrics: micro completion rate, which measures the proportion of completed key nodes across all the tasks, and task success rate, which represents the proportion of successfully completed tasks relative to the total number of tasks. F.2 Baselines Mobile. We conducted comprehensive comparative analysis of multiple agents and their variants across different input modalities. For AXTree-only input, we compare the performance of GPT-4Turbo, GPT-4o, and Gemini 1.5 Pro as operators that use choice-based element grounding. In the combined Image and AXTree scenario, we examine GPT-4-Turbo, Gemini 1.5 Pro, GPT-4o, and SeeAct, all implementing SoM grounding methods [38]. For image-only inputs, we mainly removed SoM images and textual list of elements from the AXTree in the observation and compared GPT-4o with different grounding models (OS-Atlas, UGround, Aria-UI, UI-TARS). All agents employed ReAct-style reasoning process [41] for target element selection or localization, enhanced by self-reflection [27] methodology after each execution step to optimize subsequent decision-making. Web. We evaluate agents in different configurations for web interaction tasks. For text-based inputs, we test the original WebCanvas agent [21] with multiple language models including GPT-4-Turbo, Gemini 1.5 Pro, and GPT-3.5-Turbo, where the agent perceives web pages through textual HTML elements with choice-based selection. For image-only inputs, we remove DOM tree components from the original prompts to work purely with visual inputs. We evaluated the performance of GPT-4o with different grounding models (OS-Atlas, UGround, Aria-UI). In all image-based scenarios, the agents interact with HTML elements using coordinate-based localization rather than DOM references. F.3 Implementation Details Mobile. We build Mirage-1 based on M3A Agent [24]. We mainly removed SoM images and textual list of elements from the AXTree in the observation. Only tasks and screenshots are accepted as input. We used GPT-4o as the planning and reflection model. Separately, we used OS-Atlas, UGround, Aria-UI, and UI-TARS as our grounding models. To initiate HMS, we randomly select the 1000 trajectories in the general data of AITW [25]. We use the method discussed in Section 3.3. To expand HMS, we provide GPT-4o with information about the app, including the developers description of the apps functions and several app screenshots. Then we prompt GPT-4o to generate 200 unseen tasks. After that, we use the SA-MCTS algorithm for online exploration and utilize the successful trajectories to expand our HMS. Finally, we obtained 93 Meta Skills, 463 Core Skills, and 1063 Execution Skills. Web. We build Mirage-1 based on the WebCanvas agent [21]. We mainly removed the textual list of elements from the DOM tree in the observation and only used webpage screenshots as inputs. Like Figure 5: Case study example where Mirage completes long-horizon task. First, Mirage retrieval Meta Skills, Core Skills, and Execution Skills from HMS. Then Mirage generates plans for longhorizon tasks. With the help of Core Skill, Mirage can easily achieve sub-goals. Mobile, we used GPT-4o as the planning and reflection models and used OS-Atlas, UGround, and Aria-UI as our grounding models. To initiate HMS, we randomly select the 1000 trajectories of Multimodal-Mind2Web [46], which is the multimodal version of Mind2Web [4]. We use the method discussed in Section 3.3. To expand HMS, similar to Mobile, we provide GPT-4o with several web page screenshots and the main content of the web pages. Then, we prompt GPT-4o to generate 200 unseen task descriptions for the web pages. We then SA-MCTS algorithm for exploration and then utilize the successful trajectories to expand our HMS. Finally, we obtained 125 Meta Skills, 643 Core Skills, and 1049 Execution Skills."
        },
        {
            "title": "G Case Study",
            "content": "We present more complex example. As shown in 5, after creating contact, the agent needs to create note. After entering the text, the text should then be sent to someone via text message. Mirage-1 first retrieves HMS and gets 2 relevant core skills: add_contact and send_text_message. Then Mirage-1 prompts GPT-4o to generate plans for the task."
        },
        {
            "title": "H Prompts",
            "content": "You are skilled assistant specializing in analyzing and interpreting tasks on mobile devices. Based on user's goal, agent's action and two screenshots (before and after the action), you need to transform the action excuted by agent in the following action list. You need to output the action in the correct JSON format. Note you must describe the goal of action in the ' description' field of the JSON format. You can perform the following actions: {action space} will give some examples to help you understand the task better: Example 1: {example1} Example 2: {example2} 16 Now it's your turn. Input: - Goal: {task}. - Action: {action}. - Screenshots: Two images before and after the action. your answer should look like: Action: {{\"action_type\": ..., \"description\": ...}} Your Answer: Listing 1: Prompt for Extract Execution Skill from Trajectory. Create new skill function using given task, action sequence, and existing skill functions. Ensure arguments and actions are relevant and general enough for reuse. The skill function only supportes these actions: - `Agent.click(element)`: Click on an element on the screen. - `Agent.double_tap(element)`: Double tap on an element on the screen. - `Agent.long_press(element)`: Long press on an element on the screen. - `Agent.input_text(text)`: Type text into text field. - `Agent.scroll(direction)`: Scroll the screen or scrollable UI element in one of the four directions. - `Agent.swipe(direction)`: Swipe the screen in one of the four directions. - `Agent.open_app(app_name)`: Open an app. - `Agent.wait()`: Wait for the screen to update. - `Agent.keyboard_enter()`: Press the Enter key. - `Agent.navigate_home()`: Navigate to the home screen. - `Agent.navigate_back()`: Navigate back. Tips: - Use `Agent.open_app(app_name)` if app opening is primary initial step. # Steps 1. Analyze the provided task and the sequence of actions to understand the goal. 2. Reference existing skill functions as templates for creating new ones. 3. Generate skill function, ensuring its actions and arguments match the provided task and actions. 4. Ensure the skill function's arguments are reusable for similar tasks. # Output Format - Provide the new skill function in Python format. - Ensure the skill function includes docstring describing its parameters and purpose. # Examples {example1} {example2} {example3} Your Turn: Input: - Task: {task} - Actions: {actions} - Skill: {skill} Your Answer: Reason: <reason> New skill: <new skill> Listing 2: Prompt for Generate New Core Skill. You are an expert in abstract thinking and plan optimization. Your task is to classify given task into suitable category based on list of existing skills. Each skill is defined by its name and description in the format `skill_name: skill_description`. - Identify and Match: Start by thoroughly analyzing the task and the available skills. Match the task with the skill descriptions to determine the most appropriate category. - Create New Skills: If no existing skill entirely covers the task, synthesize new skill by combining existing skills or creating completely new description that captures the necessary abilities. - Consider Multiple Skills: Be aware that some tasks may require combination of skills, so consider all relevant skills when classifying the task. ### New Skill Template Skill name: Provide concise, descriptive name for the skill. Skill description: Write detailed description that covers the essence of the skill. Skill combination: If applicable, list the combined skills in the format `<skill1>, <skill2>, < skill3>`. 17 ### Steps 1. Analyze the Task: Start by considering the main actions or objectives involved in the given task. 2. Review Existing Skills: Go through each skill description to find potential matches or overlaps with the task requirements. 3. Decide on Category: Determine whether the task can be fully categorized under an existing skill, needs the combination of several skills, or demands the creation of new skill. 4. Define and Document: Clearly write out the reasoning and final categorization, including new skill descriptions if necessary. ### Output Format Reason: Provide the thought process and analysis that led to the classification. Category: Specify the appropriate existing skill or indicate \"New Skill\" with details. ### Examples {example1} {example2} {example3} {example4} {example5} ### Notes - Consider potential overlaps between skills. - Always detail your decision-making process clearly. - Generate new skills only when necessary. Input: Task: {task} Skills: {skills} Output: Reason: <reason> Category: <category> Skill name: <skill name>(if new skill) Skill description: <skill description>(if new skill) Skill combination: <skill combination>(if new skill) Listing 3: Prompt for Generate Select or Generate Meta Skill. You are an Android Phone Agent that generates high-level task plans. Given task and skill functions, generate plans in either of these formats: - Call skill function with specific arguments: `Category.skill_function(arg1, arg2, ...)`. - Use natural language to describe the plan. You should use this format: In order to complete <goal >, need to <do something> in <some context>. Replace <goal> with the specific goal, <do something> with the high-level task, and <some context> with the relevant information or condition. There are some nesseray rules for you to follow: - When using natural language, the description should be clear and easy to understand. - Write code in single line without line breaks - Only use provided skill functions, do not create new ones - Use natural language for tasks that don't match existing functions - Ensure arguments match function specifications exactly - DO not add reasoning process in the Plans. Example: Task: \"Create note in Markor named abcs.txt. Perform paste operation in the note and save the note.\" Category: NoteManager Skill Functions: def add_text_to_bottom(note_name, additional_text): \"\"\"Edit specified note in the Markor app by adding text to the bottom. Args: note_name (str): The name of the note to be edited. additional_text (str): The text to add to the bottom of the note. \"\"\" def create_note_in_markor(note_name, note_content): \"\"\"Create new note in the Markor app with the specified name and content. 18 Args: note_name (str): The name of the note to be created. note_content (str): The content to be added to the new note. \"\"\" Result: Reason: The user wants to create note in Markor named abcs.txt, perform paste operation in the note, and save the note. don't need to call the skill function `add_text_to_bottom` because has performed the paste operation in the note. Plans: 1. NoteManager.create_note_in_markor(\"abcs.txt\", \"\") 2. Perform paste operation in the note. 3. Check the note and save the note. Think carefully about the task and skill functions to generate the most accurate and feasible plans. Input: Task: {task} {context} Output must be in the following format: Reason: <reason> Plans: 1. plan1 2. plan2 Listing 4: Prompt for Planning with HMS. You are an instructor in the Android Phone scenario, guiding the Agent to better complete users' tasks. Given the user's task, the current screenshot of the Android Phone, and an action that the Agent intends to take, assess whether the action can help to complete the goal. Predict the new state that the action may lead to, based on your previous experience, and assign score to each action. The Agent will then choose the action with the highest score to execute. will provide examples to help you predict the new state. {exampl_trace_skill1} {exampl_trace_skill2} {exampl_trace_skill3} Input: - The current user goal is: {goal} - The current action that the Agent intends to take is: {action} - The summary of the Agent's historical trajectory is as follows: {history} - will give the first screenshot and current screenshot of the Android Phone to help you predict the new state that action may lead to based on your previous experience. Think step by step to predict the new state, evaluate if the action aligns with the goal, and rank the action accordingly. Output: You must output the structured answer based on the above information. - caption: Describe the screenshot using less than 30 words. - reason: Think step by step to predict the new state and evaluate if the proposed action aligns with the goal based on your previous experience. - state_change: Predict the new state that action may lead to based on your previous experience. Output only the state_change. - score: Assign score (0-10) to the action based on the following criteria: 1) Goal Alignment: Does the action help achieve the user's goal? 2) How well the action brings the user closer to their goal. 3) Likelihood of Success: How likely is it that the action will succeed based on the current context and UI state. 4) Efficiency: How efficient is the action in terms of user effort and time required. just output the score as number between 0-10. Output format: {{\"caption\": ..., \"reason\": ..., \"state_change\": ..., \"score\": ...}} Listing 5: Prompt for Reflection with Execution Skill. 19 Name Task Template AddRecipeAndCleanDuplicates Add the following recipes into the Broccoli app:{recipe}. Then delete all but one of any recipes in the Broccoli app that are exact duplicates, ensuring at least one instance of each unique recipe remains. OptimizeBrightnessForPhoto Turn the screen brightness up to the maximum to ensure the shooting environment is well-lit, then take photo. OptimizeBrightnessForVideo Turn the screen brightness up to the maximum to ensure the shooting environment is well-lit, then take video. SimpleCalendarAddTomorrowAnd DeleteEvents In Simple Calendar Pro, create calendar event for tomorrow at {hour}h with the title {event_title} and the description {event_description}. The event should last for {duration_mins} mins. Then, delete all events in the calendar on {year}-{month}-{day}. Apps broccoli app settings, camera settings, camera simple calendar pro SimpleCalendarDeleteEvents OnRelativeDayAndAddOneEventRelativeDay In Simple Calendar Pro, delete all events scheduled for this {day_of_week}.Then create calendar event for this {day_of_week} at {hour}h with the title {event_title} and the description {event_description}. The event should last for {duration_mins} mins. simple calendar pro ClockStopWatchRunning AndBrowserDraw ClockStopWatchRunning AndBrowserMaze Run the stopwatch. Open the file task.html in Downloads in the file manager; when prompted open it with Chrome. Then create drawing using the three colors shown at the top and hit submit. Run the stopwatch. Open the file task.html in Downloads in the file manager; when prompted open it with Chrome. Then navigate the to the bottom-right cell, by using the\" \" direction buttons. ClockStopWatchRunning AndBrowserMultiply Run the stopwatch. Then click the button 5 times, remember the numbers displayed, and enter their product in the form. ClockStopWatchRunning AndBrowserMazeAndPause Run the stopwatch. Open the file task.html in Downloads in the file manager; when prompted open it with Chrome. Then navigate the to the bottom-right cell, by using the\" \" direction buttons. After that, pause the stopwatch. ClockStopWatchRunning AndBrowserDrawAndPause Run the stopwatch. Open the file task.html in Downloads in the file manager; when prompted open it with Chrome. Then create drawing using the three colors shown at the top and hit submit. After that, pause the stopwatch. chrome, clock chrome, clock chrome, clock chrome, clock chrome, clock ClockStopWatchRunningAnd BrowserMultiplyAndPause Run the stopwatch. Then click the button 5 times, remember the numbers displayed, and enter their product in the form. After that, pause the stopwatch. chrome, clock ClockStopWatchRunningAndPause Run the stopwatch. And pause it after while. clock ConnectBluetoothAndOpenMusicTurn on bluetooth. Then open the retro music app. Clear any pop-ups that may appear by settings, retro music granting\" \" all permissions that are required. Enjoy the music! AddContactAndCallAndSms Create new contact for {name}. His number is {number}. Then call him. After that, send text message to him using Simple SMS Messenger: nice to meet to you. AddContactAndCall Create new contact for {name}. His number is {number}. Then call him. AddContactAndMarkowAndSms Create new contact for {name}. His number is {number}. Then create new note in Markor named {file_name} with the following text: {text}. Share the entire content of the note with the phone number {number} via SMS using Simple SMS Messenger. AddContactAndSms Create new contact for {name}. His number is {number}. Send text message using Simple SMS Messenger to him with message: {text}. CopyToClipboardAndCreateNote Copy the following text to the clipboard: {clipboard_content}. Then create note in Markor named {file_name}. Perform paste operation in the note and save the note. SystemCopyToClipboardAndSms Copy the text {clipboard_content} to your clipboard, then send it to {number} using simple sms messenger app. contacts, dialer, simple sms messenger contacts, dialer contacts, markor, simple sms messenger contacts, simple sms messenger markor simple sms messenger MarkorCreateFolderAndMoveFile Create new folder in Markor named {folder_name}.Then move the note {file_name} markor from {source_folder} to {destination_folder}. MarkorCreateNoteAndSms Create new note in Markor named {file_name} with the following text: {text}. Share the entire content of the note with the phone number {number} via SMS using Simple SMS Messenger markor, simple sms messenger TurnOffWifiAndAudioRecorder Turning off Wifi. Then Record an audio clip using Audio Recorder app and save it. TurnOffWifiAnd ConnectBluetooth AndOpenMusic TurnOffWifiAnd TurnOffBluetooth AudioRecorder Turn off wifi and turn on bluetooth. Then open the retro music app. Clear any pop-ups that may appear by granting all permissions that are required. Enjoy the music! Turning off Wifi and turn off bluetooth. Then Record an audio clip using Audio Recorder app and save it. settings, audio recorder TurnOnWifiAndBrowserMaze Turning on Wi-Fi. Open the file task.html in Downloads in the file manager; when prompted open it with Chrome. Then navigate the to the bottom-right cell, by using the direction buttons. settings, chrome CameraTaskPhotoAndCreate Drawing Take photo with the camera. Then, Create new drawing in Simple Draw Pro. Name it {file_name}. Save it in the Pictures folder within the sdk_gphone_x86_64 storage area. camera, simple draw pro TurnOnWifiAndOpenApp Turn on Wifi, then open the {app_name} app TurnOffWifiAndTurnOnBluetooth TurnOffWifiAndTurnOffBluetooth Turn off WiFi, then enable bluetooth Turn off WiFi, then turn off bluetooth UpdateExpensesWithCleanup Add the following expenses into the arduia pro expense: {expense}. Then, delete all but one of any expenses in arduia pro expense that are exact duplicates, ensuring at least one instance of each unique expense remains. settings settings settings pro expense Table 5: AndroidLH Benchmark Task list. 20 settings, audio recorder settings, retro music"
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology, Shenzhen",
        "Huawei Noahs Ark Lab",
        "Peng Cheng Laboratory"
    ]
}