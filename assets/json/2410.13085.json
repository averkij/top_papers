{
    "paper_title": "MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models",
    "authors": [
        "Peng Xia",
        "Kangyu Zhu",
        "Haoran Li",
        "Tianze Wang",
        "Weijia Shi",
        "Sheng Wang",
        "Linjun Zhang",
        "James Zou",
        "Huaxiu Yao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Artificial Intelligence (AI) has demonstrated significant potential in healthcare, particularly in disease diagnosis and treatment planning. Recent progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new possibilities for interactive diagnostic tools. However, these models often suffer from factual hallucination, which can lead to incorrect diagnoses. Fine-tuning and retrieval-augmented generation (RAG) have emerged as methods to address these issues. However, the amount of high-quality data and distribution shifts between training data and deployment data limit the application of fine-tuning methods. Although RAG is lightweight and effective, existing RAG-based approaches are not sufficiently general to different medical domains and can potentially cause misalignment issues, both between modalities and between the model and the ground truth. In this paper, we propose a versatile multimodal RAG system, MMed-RAG, designed to enhance the factuality of Med-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an adaptive retrieved contexts selection method, and a provable RAG-based preference fine-tuning strategy. These innovations make the RAG process sufficiently general and reliable, significantly improving alignment when introducing retrieved contexts. Experimental results across five medical datasets (involving radiology, ophthalmology, pathology) on medical VQA and report generation demonstrate that MMed-RAG can achieve an average improvement of 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available in https://github.com/richard-peng-xia/MMed-RAG."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 1 ] . [ 1 5 8 0 3 1 . 0 1 4 2 : r Preprint. MMED-RAG: VERSATILE MULTIMODAL RAG SYSTEM FOR MEDICAL VISION LANGUAGE MODELS Peng Xia1, Kangyu Zhu5, Haoran Li6, Tianze Wang3, Weijia Shi4, Sheng Wang4, Linjun Zhang3, James Zou2, Huaxiu Yao1 1UNC-Chapel Hill, 2Stanford University, 3Rutgers University, 4University of Washington, 5Brown University, 6PloyU {pxia,huaxiu}@cs.unc.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Artificial Intelligence (AI) has demonstrated significant potential in healthcare, particularly in disease diagnosis and treatment planning. Recent progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new possibilities for interactive diagnostic tools. However, these models often suffer from factual hallucination, which can lead to incorrect diagnoses. Fine-tuning and retrieval-augmented generation (RAG) have emerged as methods to address these issues. However, the amount of high-quality data and distribution shifts between training data and deployment data limit the application of fine-tuning methods. Although RAG is lightweight and effective, existing RAG-based approaches are not sufficiently general to different medical domains and can potentially cause misalignment issues, both between modalities and between the model and the ground truth. In this paper, we propose versatile multimodal RAG system, MMed-RAG, designed to enhance the factuality of Med-LVLMs. Our approach introduces domain-aware retrieval mechanism, an adaptive retrieved contexts selection method, and provable RAG-based preference fine-tuning strategy. These innovations make the RAG process sufficiently general and reliable, significantly improving alignment when introducing retrieved contexts. Experimental results across five medical datasets (involving radiology, ophthalmology, pathology) on medical VQA and report generation demonstrate that MMed-RAG can achieve an average improvement of 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available in https://github.com/richard-peng-xia/MMed-RAG."
        },
        {
            "title": "INTRODUCTION",
            "content": "Artificial Intelligence (AI) has already transformed healthcare and still has lot of potential for further advancements (Tautan et al., 2021; Wang et al., 2019; Ye et al., 2021; Tu et al., 2024; Xia et al., 2024b; Hu et al., 2024a;b; Li et al., 2024). Recently, Medical Large Vision-Language Models (Med-LVLMs) have shown great promise for advancing interactive and intelligent diagnosis (Li et al., 2023a; Moor et al., 2023; Zhang et al., 2023b; Wu et al., 2023b). Despite this potential (Li et al., 2023b; Wu et al., 2023a; Shi et al., 2024), current Med-LVLMs still face significant reliability issues, particularly their tendency to generate non-factual medical responses (Xia et al., 2024a; Royer et al., 2024; Chen et al., 2024a; Jiang et al., 2024), making them unreliable in critical medical applications. These factuality issues raise serious concerns when deploying such models in clinical settings, where even small diagnostic errors could lead to severe consequences for patient care. Recently, researchers have begun to focus on improving the factuality of Med-LVLMs through various techniques, including fine-tuning (Li et al., 2023a; Moor et al., 2023; Thawkar et al., 2023; Zhang et al., 2023b; Chen et al., 2024b) and retrieval-augmented generation (RAG) (Xia et al., 2024c; He et al., 2024; Sun et al., 2024). Fine-tuning is direct method to improve model performance, but it faces several limitations in the medical field. First, there is lack of sufficient high-quality labeled data for fine-tuning in the medical domain. Additionally, distribution gap often exists between the training data and the real-world deployment data (Schrouff et al., 2022), leading to significantly worse model performance during deployment. Hence, RAG has emerged as viable alternative by providing external references during the inference stage, enhancing the factuality of Med-LVLMs (Wu et al., 2023c; Gao et al., 2023). However, despite its advantages, current RAG implementations in Med-LVLMs have significant limitations. First, these methods tend to be dataset-specific, reducing their generalizability across various medical domains. Second, these 1 Preprint. models are still facing misalignment issues that lead to factuality problems. This misalignment may arise from the impact of adding RAG on the original Med-LVLMs cross-modality alignment, as well as on the overall alignment between the model and ground truth. To address these challenges, we propose versatile factual Multimodal Medical RAG system called MMed-RAG. Specifically, MMed-RAG first introduces domain-aware retrieval mechanism, designed to handle different domains of medical images more effectively. Here, we design domain identification module to adaptively select corresponding retrieval model given the input medical image. Secondly, we include adaptive calibration approach for selecting the number of retrieved contexts. Lastly, MMed-RAG incorporates RAG-based preference fine-tuning to enhance crossmodality alignment and overall alignment with ground truth. The preference pairs are designed to achieve two goals: first, to improve cross-modality alignment by encouraging the model to avoid generating responses without utilizing input medical images, even the responses are correct; second, to improve overall alignment by encouraging the model to understand retrieved contexts when unsure, while avoiding interference from irrelevant retrieved information. The primary contribution of this paper is MMed-RAG, versatile multimodal RAG system designed specifically for Med-LVLMs to generate more factual responses. Under mild assumptions, our theoretical analysis demonstrates that MMed-RAG mitigates both cross-modality misalignment and overall misalignment with ground truth. Furthermore, empirical results on five medical multimodal datasets, covering three medical image modalities (radiology, pathology, and ophthalmology), show that MMed-RAG significantly improves the factual accuracy of Med-LVLMs, achieving improvements of 18.5% and 69.1% on Medical VQA and report generation tasks, respectively, compared to the original Med-LVLM. These empirical findings further demonstrate the effectiveness of our proposed components and support the theoretical analysis in addressing misalignment issues."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "In this section, we will provide brief overview of Med-LVLMs and preference optimization. Medical Large Vision Language Models. Med-LVLMs bridge LLMs with medical visual modules, allowing the model to take medical image xv and clinical query xt as input x, and autoregressively predict the probability distribution of the next token. The text output is denoted as y. Preference Optimization. Preference optimization has achieved remarkable results in LLM alignment. Give an input x, language model policy πθ can produce conditional distribution πθ(y x) with as the output text response. The recently popular DPO (Rafailov et al., 2023) utilizes preference data achieve objective alignment in LLMs. The preference data is defined as = {x(i), y(i) represent preferred and dispreferred responses given an input prompt x. The probably of obtaining each preference pair is p(yw yl) = σ(r(x, yw) r(x, yl)), where σ() is the sigmoid function. In DPO, the optimization can be formulated as classification loss over the preference data as: i=1, where y(i) and y(i) , y(i) }N LDPO(πθ; πref) = E(x,yw ,yl)D (cid:104) log σ (cid:16) α log πθ (yw x) πref(yw x) α log πθ (ylx) πref(ylx) (cid:17)(cid:105) . (1) where πθ represents the reference policy, which is the LLM fine-tuned through supervised learning."
        },
        {
            "title": "3 MMED-RAG: A VERSATILE MEDICAL RAG SYSTEM",
            "content": "In this section, as illustrated in Figure 1, we will propose MMed-RAG, versatile RAG system for improving the factuality of Med-LVLMs. Specifically, MMed-RAG consists of three complementary modules. First, we design domain-aware retrieval mechanism to select the optimal retriever by feeding each given medical image to the domain identification module. Second, to select an optimal number of retrieved contexts and filter out low-quality information, MMed-RAG adopts adaptive method by filtering out low-quality information using the similarity scores during the RAG phase. Lastly, we use RAG-based preference fine-tuning approach to improve the cross-modality alignment and the overall alignment between groundtruth. We detail these steps as follows: 3.1 DOMAIN-AWARE RETRIEVAL MECHANISM In MMed-RAG, we introduce domain-aware retrieval mechanism to efficiently handle medical images from different sources (e.g., radiology, pathology, ophthalmology). Specifically, we first 2 Preprint. Figure 1: Overview of MMed-RAG, versatile factual multimodal RAG system designed to enhance the reliability of Med-LVLMs. It introduces domain-aware retrieval mechanism that effectively handles different domains of medical images by selecting suitable retrieval models. Additionally, it uses an adaptive context selection approach to determine the optimal number of retrieved contexts and employs preference fine-tuning to improve both cross-modality and overall alignment. employ domain identification module that assigns domain label to each input medical image. To achieve this, we create small dataset with medical images as inputs and their corresponding domain labels as outputs, using this dataset to fine-tune the BiomedCLIP model (Zhang et al., 2023a) to improve its domain awareness. Formally, for given medical image xv, we predict its domain = F(xv). Based on the assigned domain label d, the image xv is fed into the corresponding multimodal retriever Rd() for knowledge retrieval. Here, each multimodal retriever Rd() for each domain is trained through contrastive learning (Radford et al., 2021). Specifically, the visual and textual information Ximg, Xtxt are processed by their corresponding encoders Eimg(), Etxt() to generate textual and visual embeddings Vtxt = Etxt(Xtxt), Vimg = Eimg(Ximg). Contrastive learning loss is then applied to maximize the similarity between text and image embeddings representing the same example, while minimizing the similarity between embeddings representing different examples, as defined below: = Limg + Ltxt 2 , where Limg = 1 N (cid:88) log i=1 exp(Si,i) j=1 exp(Si,j) (cid:80)N , Ltxt = 1 (cid:88) log i=1 exp(Si,i) j=1 exp(Sj,i) (cid:80)N , (2) where RN represents the similarity matrix between image and text modalities, calculated as: = Vimg Vtxt )T , where each element Si,j represents the similarity between the image representation of example and the text representation of example j. Vimg ( Vtxt Finally, for the input image xt, after feeding into the corresponding multimodal retriever Rd(), the multimodal retriever will retrieves the top-k most similar reports for the image. These retrieved reports xr = Rd(xv) are then provided to the Med-LVLM M() as references to guide the generation. 3.2 ADAPTIVE RETRIEVED CONTEXT SELECTION Following the domain-aware retrieval mechanism, the next step is to determine the optimal amount of context to retrieve. Retrieving too much or too little information can result in hallucinations (Xia et al., 2024c). Current RAG methods applied to Med-LVLMs generally rely on empirical results or fixed values based on validation sets to select the optimal value of the number of retrieved contexts (Xia et al., 2024c; He et al., 2024; Sun et al., 2024). However, the distribution of similarity scores varies depending on the complexity of the image and its 3 Figure 2: Relations between selected contexts and similarity. Preprint. alignment with the textual information from the data source. These fixed-k methods do not guarantee optimal performance on target data, as they overlook the similarity scores generated during the retrieval process. To address this, we propose an adaptive method that dynamically selects based on the similarity scores of the retrieved contexts. Specifically, during the domain-aware retrieval mechanism phase, the retrieved information is denoted as xr(k) = Rd(xv; k), where represents the number of retrieved contexts, and the corresponding similarity scores are denoted as Sk. For simplicity, when there is no ambiguity, we will refer to xr(k) as xr. As illustrated in Figure 2, our method is based on key observation: the similarity scores (CLIP score in this case) between retrieved contexts often exhibit sharp decline after certain number of results (nearly top-9 in this case). This suggests that lower-quality information can still be included among the top-k retrieved contexts when using fixed-k strategy, especially in cases where the fixed value of is too large. These lower-quality retrievals introduce noise and irrelevant information, which can significantly impair the models ability to generate factual and coherent responses. To mitigate this issue, we draw inspiration from the Gap statistic method used in clustering (Tibshirani et al., 2001) and extend this concept to RAG for Med-LVLMs. Specifically, after retrieving the top-k contexts, we perform an additional round of optimization by analyzing the similarity ratios between consecutive retrievals. These similarity ratios are denoted as ui = log(Si/Si+1) for 0 < k, where Si represents the similarity score of the i-th retrieved context. When ui exceeds predefined threshold γ, this indicates substantial drop in relevance, suggesting that the remaining retrievals are less likely to contribute preferredly to the models output. At this point i, we truncate k, effectively discarding the less relevant retrievals that follow. This adaptive truncation mechanism ensures that only the most relevant contexts are retained for generating the final response, reducing the risk of hallucination and improving the factual accuracy of the outputs. Although the threshold γ is fixed, this approach provides adaptive way to balance the bias and variance in retrieved contexts. By adapting to the characteristics of each input xv, our method enhances the robustness of the retrieval process and ensures that the selection of is tailored to the specific data at hand, thereby improving overall performance across diverse contexts and tasks. 3.3 RAG-BASED PREFERENCE FINE-TUNING After context selection, MMed-RAG supplies Med-LVLM with reliable retrieved information as external knowledge to aid in generating factual responses. However, incorporating this retrieved knowledge may potentially disrupt the original alignment within the existing Med-LVLM, concern we will elaborate on below: Alignment Analysis. In the alignment analysis, we aim to explore how incorporating retrieved context impacts the original alignment in Med-LVLMs, focusing on two key aspects: (1) cross-modality alignment and (2) overall alignment with the ground truth. To evaluate cross-modality alignment, we conduct two tests on LLaVA-Med-1.5 (Li et al., 2023a) using the Harvard-FairVLMed (Luo et al., 2024) dataset. First, when replacing the original image with highly noisy image associated with different ground truth, the original model gives incorrect answers (the ground truth being the response for the original image). After incorporating RAG, where context is retrieved based on the original image, 55.08% of these cases return correct answers. This indicates that the model directly references the retrieved knowledge without considering the input image, highlighting significant cross-modal misalignment issues. Furthermore, 43.31% of the questions that were originally answered correctly are answered incorrectly after incorporating RAG, suggesting interference from incorrect retrieval information, which leads to overall misalignment with the ground truth. To address cross-modality misalignment and the overall misalignment introduced by incorporating retrieved knowledge, as shown in Algorithm 1, we propose RAG-based preference fine-tuning (RAG-PT) approach to fine-tune the target Med-LVLM M(). Specifically, RAG-PT constructs two types of preference pairs designed to mitigate both categories of misalignment. Preference Pairs for Cross-Modality Alignment. We first construct preference pairs aimed at improving cross-modality alignment. In this dataset, we select samples from = {x(i) i=1, where xv, xt, and represent the input medical image, clinical query, and ground-truth answer, respectively. For simplicity, we omit the sample index (i) in the following sections. models correct response using retrieved knowledge, i.e., M(xv, xt + xr) = y, is considered preferred response pi, where xr is the retrieved information. dispreferred response ni is selected from cases , x(i) , y(i)}N 4 Preprint. Algorithm 1: Versatile Multimodal RAG System (MMed-RAG) Input: = {x(i) , y(i)}N , x(i) i=1: Dataset; πθ: Parameters of the Med-LVLM; Med-LVLM: M(, ); Domain Identification: F(); Retriever: R(); Noisy Function: I(). Output: πref: Parameters of the reference model. 1 Training Stage 2 Initialize Dcm with an empty set 3 foreach (xv, xt, y) do 4 5 7 8 9 10 11 13 14 15 16 Generate retrieved contexts with an assigned domain label xr RF (xv )(xv) Generate the noisy image Cross-Modality Alignment if M(xv, (xt, xr)) = and M(x v, (xt, xr)) = then Select the preferred response yw,o1 y, dispreferred response yl,o1 M(x Put {(xv, xt), yw,o1, yl,o1} into Dcm I(xv) Overall Alignment Initialize D1 oa and D2 if M(xv, (xt, xr)) = and M(xv, xt) = then oa with empty set Select the preferred response yw,o2 y, dispreferred response yl,o2 M(xv, xt) Put {(xv, xt), yw,o2, yl,o2} into D1 oa if M(xv, xt) = and M(xv, (xt, xr)) = then v, (xt, xr)) Select the preferred response yw,o3 y, dispreferred response yl,o3 M(xv, (xt, xr)) Put {(xv, xt), yw,o3, yl,o3} into D2 oa 17 18 Dpt = Dcm Doa, Doa = D1 oa D2 oa 19 foreach ((xv, xt), yw,o, yl,o) Dpt do 20 21 Inference Stage 22 foreach test sample (xv, xt) do 23 Compute the losses Lpt following equation 4 and update πref Select top-k retrieved contexts with an assigned domain label xr RF (xv )(xv) Get the predictions of the model w/ RAG-PT M(xv, (xt, xr)) 24 where the model makes correct inference based on an unrelated image, i.e., M(x v, xt) = y, but M(x v, xt + xr) = y, reflecting the models reliance on the retrieved knowledge. The unrelated are generated through two-step process: first, we use the retriever to select an image images with the lowest similarity to the target image; then, we introduce diffusion noise into the selected unrelated image. We define the noise step as s, and the noised image at step is expressed as: + (cid:112)1 ξs ϵ, where ξs = (cid:81)s i=0 ξi and ξs (0, 1) is hyperparameter. The preference pairs constructed in this stage are denoted as Dcm. By comparing the preferred and dispreferred responses in Dcm, we encourage the model to prioritize the input medical image when generating responses. = (cid:112)ξs x (3) Preference Pairs for Overall Alignment. Second, we construct preference pairs to improve overall alignment, focusing on enhancing the models ability to effectively leverage retrieved knowledge when generating responses. The preference pairs in this stage are constructed from two subsets. The first subset, D1 oa, is designed to strengthen the models comprehension and reasoning abilities regarding the retrieved knowledge. Preferred responses are selected where the model correctly answers based on both the original image and the retrieved information, i.e., M(xv, xt + xr) = y, while dispreferred responses represent cases where the model answers incorrectly based on the image without using retrieval, i.e., M(xv, xt) = y. Comparing these preferred and dispreferred responses enhances the models understanding of the retrieved information and improves the overall effectiveness of RAG. In the second subset, D2 oa, the goal is to mitigate interference from the retrieved knowledge. Preferred responses are selected where the model correctly answers based solely on the original image without using retrieved knowledge, i.e., M(xv, xt) = y, while dispreferred responses occur when the model answers incorrectly using both the image and retrieved information, i.e., M(xv, xt + xr) = y. This helps the model learn when to rely on its internal knowledge versus retrieved knowledge. Finally, we combine the first and second subsets to form the second set of preference pairs, Doa = D1 Finally, we merge the first and second preference set and denote the preference dataset as Dpt = Dcm Doa = {x(i), y(i) l,o are represented as preferred and dispreferred i=1, where y(i) oa w,o, y(i) w,o, y(i) l,o }N oa. 5 Preprint. responses, respectively. Based on the curated preferences, we fine-tune Med-LVLM using direct preference optimization (Rafailov et al., 2023) with the following loss: Lpt = E(x,yw,o,yl,o)D (cid:104) log σ (cid:16)"
        },
        {
            "title": "4 THEORETICAL ANALYSIS",
            "content": "α log πθ (yw,ox) πo(yw,ox) α log πθ (yl,ox) πo(yl,ox) (cid:17)(cid:105) . (4) In this section, we provide theoretical analysis of the model obtained from equation 4 and examine how the image input and retrieved context influences the model. Recall that xv, y, xt, xr denotes input medical image, groundtruth answer, question, and retrieved information, respectively."
        },
        {
            "title": "4.1 THE IMPROVEMENT ON CROSS-MODALITY ALIGNMENT",
            "content": "We first consider the loss for cross-modality alignment, Lcm = E(x,yw,o,yl,o)Dcm (cid:104) log σ (cid:16) α log πθ (yw,ox) πo(yw,ox) α log πθ (yl,ox) πo(yl,ox) (cid:17)(cid:105) . (5) where (xw, yw,o) qw(xw, yw,oxt, xr) and (xl, yl,o) ql(xl, yl,oxt, xr) represent distributions of the preferred responses and dispreferred responses on Dcm, respectively. Let denote (xv, xr, xt) Definition 4.1 Define the weight of xv with respect to log πθ(yx) as wt(xv, πθ) := Eyπθ (x) (cid:20) xv (cid:21)2 log πθ(yx) (6) Definition 4.1 describes how log πθ(yx) changes with respect to xv, and the weight is always nondispreferred. We demonstrate that this is reasonable definition through Lemma 4.1. Lemma 4.1 For linear model = θ1xv + θ2xt + ϵ such that ϵ (0, 1), wt(xv, πθ) = θ2 1 Assumption 4.1 Let h(x, y), abbreviate as h, be := (cid:34) (cid:88) πo(yx) (cid:19) 1 α (cid:18) qw(yx) ql(yx) (cid:35)1 (cid:18) qw(yx) ql(yx) (cid:19) 1 α Assume that wt(xv, πo) < c2, where (cid:115)(cid:13) (cid:13) (cid:13) (cid:13) = (cid:112)πo(yx) xv (cid:13) (cid:13) (cid:13) (cid:13) 2 2 + (cid:90) (cid:18) xv (cid:19)2 πo(yx) dy (cid:13) (cid:13) (cid:13) (cid:13) (cid:112)πo(yx) xv (cid:13) (cid:13) (cid:13) (cid:13) (7) (8) Assumption 4.1 requires that xv has small weight in log πo(yx). model πo(yx) independent of xv could satisfy Assumption 4.1. In this case, the reference model generates answers without using information from the image. Theorem 4.1 Suppose that Assumption 4.1 holds, cross-modality loss increase the weight of xv. wt(xv, πθ) > wt(xv, πo) (9) Theorem 4.1 indicates that when the weight of xv is too small in the initial model πo(yx), the cross-modality loss function adjusts the model to place greater emphasis on images, informed by the retrieved data. Intuitively, for any sample (x, y), generating unrelated images causes the policy to rely less on images. By using samples from this distribution as negative samples, the new model diverges from the initial model, increasing its reliance on images. 4.2 THE IMPROVEMENT ON OVERALL ALIGNMENT In this section, we analyze the improvement on overall alignment. Let q1 w(xv, yw,oxt, xr) and q1 (xv, yl,oxt) represent distributions of the preferred responses and dispreferred responses on D1 oa, respectively; q2 w(xv, yw,oxt) and q2 (xv, yl,oxt, xr) represent distributions of the preferred responses and dispreferred responses on D2 oa, respectively. Overall loss is defined by (cid:17)(cid:105) (cid:16) (cid:104) Loa = E(x,yw,o,yl,o)Doa log σ α log πθ(yw,ox) πo(yw,ox) α log πθ(yl,ox) πo(yl,ox) . (10) 6 Preprint. Consider π as the generative distribution underlying M, construction of D1 oa indicate that there is significant gap between π(yxv, xt, xr) and π(yxv, xt, xr) for xr generates true answer while xr generate false one. oa and D2 Assumption 4.2 Assume that π(yxx, xr, xt) : is L-lipschitz continuous on xr for all (xv, xt, y) such that π(yxv, xt, xr) π(yxv, xt, xr) dx(xr, xr), where dx is any distance metric on the text space. Based on Assumption 4.2, xr can be viewed as being far from the meaningful retrieved information xr, resulting in different weight in the model. Then, we claim in the following theorem that the overall loss in equation 10 can effectively leverage retrieved knowledge while training. Assumption 4.3 Let h1(xv, xt, xr, y), abbreviate as h1, be h1 := (cid:34) (cid:88) πo(yx) (cid:18) q1 w(yxv, xt, xr) + q2 (yxv, xt) + q2 q1 w(yxv, xt) (yxv, xt, xr) (cid:19) 1 α (cid:35)1 (cid:18) q1 w(yxv, xt, xr) + q2 (yxv, xt) + q2 q1 w(yxv, xt) (yxv, xt, xr) Assume that wt(xr, πo) < 1 and wt(xr, πo) > c2 (cid:19)2 πo h1 (cid:90) (cid:18) h1 xr 2, where (cid:13) (cid:13) (cid:13) (cid:13) h1 xr dy (cid:13) (cid:13) (cid:13) (cid:13) + 2 2 πo πo h1 xr (cid:13) (cid:13) (cid:13) (cid:13)2 c1 = c2 = (cid:115)(cid:13) (cid:13) (cid:13) (cid:13) (cid:115)(cid:13) (cid:13) (cid:13) (cid:13) πo h1 xr (cid:13) (cid:13) (cid:13) (cid:13) 2 + (cid:90) (cid:18) h1 xr (cid:19)2 πo h1 + (cid:18) πo xr (cid:19)2 h1 πo dy + πo (cid:13) (cid:13) (cid:13) (cid:13) h1 xr (cid:13) (cid:13) (cid:13) (cid:13) (cid:19) 1 α (11) (12) Theorem 4.2 Suppose that Assumption 4.3 holds, then overall loss 10 increase the weight of xr and decrease the weight of xr. wt(xr, πθ) > wt(xr, πo), wt(xr, πθ) < wt(xr, πo) (13) Theorem 4.2 suggests that the model tend to improve the overall alignment. When xr generates false answer, the training procedure tends to reduce the reliance on xr, resulting in decrease in the weight assigned to xr. Conversely, if xr is helpful for generating the true answer, πθ(yx) tend to enhance its use of xr."
        },
        {
            "title": "5 EXPERIMENT",
            "content": "In this section, we evaluate the performance of MMed-RAG, aiming to answer the following questions: (1) Can MMed-RAG effectively improve the factuality of Med-LVLMs compared to decoding-based and RAG-based baselines? (2) How effective is each proposed component on performance? (3) What is the effect of preference data for different alignment goals? and (4) Does MMed-RAG actually improve cross-modality alignment and overall alignment? 5.1 EXPERIMENTAL SETUPS Implementation Details. We use LLaVA-Med-1.5 7B (Li et al., 2023a) as the backbone model. During the preference fine-tuning process, we adapt LoRA fine-tuning (Hu et al., 2021). For the training of retriever, the vision encoder is ResNet-50 (He et al., 2016), and the text encoder is bio-BioClinicalBERT (Alsentzer et al., 2019). We use the AdamW optimizer with learning rate of 103, weight decay of 102 and batch size of 32. The model is trained for 360 epochs. For more detailed information on training hyperparameters and training data, please see Appendix A.1.1. Baseline Methods. We compare MMed-RAG with two types of LVLM hallucination mitigation methods that show promising results in natural image understanding. 1) Decoding-based methods, including Greedy Decoding, Beam Search (Sutskever et al., 2014), DoLa (Chuang et al., 2023), OPERA (Huang et al., 2023), VCD (Leng et al., 2023). These methods manipulate the logits of the models output tokens to enhance factual accuracy. 2) Multimodal RAG-based methods, including MedDr (He et al., 2024), FactMM-RAG (Sun et al., 2024), RULE (Xia et al., 2024c). Furthermore, we compare the performance with other open-source Med-LVLMs, including Med-Flamingo (Moor et al., 2023), MedVInT (Zhang et al., 2023b), RadFM (Wu et al., 2023b). 7 Preprint. Table 1: Model performance (%) of different methods based on LLaVA-Med-1.5 on medical VQA task. Notably, we report the accuracy, F1 score and AUROC. The best results and second best results are highlighted in red and blue , respectively. Models Radiology Ophthalmology Pathology IU-Xray MIMIC-CXR Harvard-FairVLMed Quilt-1M PMC-OA (Pathology) Acc AUC Acc F1 AUC Acc AUC Acc F1 AUC Acc AUC LLaVA-Med-1.5 75.47 64.04 67.46 75. 80.49 68.84 63.03 74.11 63.05 62. 72.90 60.03 59.28 71.98 54.19 + Greedy + Beam Search + DoLa + OPERA + VCD 76.88 76.91 78.00 70.59 68.99 + MedDr 83.33 + FactMM-RAG 84.51 87.84 + RULE 65.59 66.06 66.75 61.54 54.35 67.80 68.51 78.00 68.74 68.77 72.19 63.22 61.08 77.15 77.07 85. 78.32 81.56 81.35 69.34 70.89 55.16 77.58 83.92 86.75 86.36 85.73 76.66 75.57 56.18 81.86 87.49 71.13 73.79 72.73 62.46 64.61 58.47 70.09 83. 82.54 80.93 76.87 71.41 65.88 70.17 83.67 87.12 85.98 88.08 85.53 81.37 77.20 80.72 87.21 92.89 70.09 68.94 67.10 65.59 64.16 64.15 72.20 77. 64.72 63.52 63.47 60.51 61.43 68.15 69.25 68.97 70.12 69.33 69.10 66.32 67.39 73.23 73.62 73.80 58.75 57.65 57.58 54.79 55.72 67.01 68.15 68. 58.61 56.29 57.71 55.32 55.10 59.97 60.49 61.41 70.42 69.84 70.27 68.30 67.94 69.19 69.38 70.36 53.10 52.89 52.95 51.86 51.62 57.01 57.31 58. MMed-RAG 89.54 80.72 87.13 83.57 88. 85.08 87.94 92.78 80.81 72.95 76. 72.25 64.54 73.09 61.42 Table 2: Model performance (%) of different methods based on LLaVA-Med-1.5 on report generation task. Notably, we report the average BLEU, ROUGE-L, METEOR. Models Radiology IU-Xray MIMIC-CXR Ophthalmology Harvard-FairVLMed BLEU ROUGE-L METEOR BLEU ROUGE-L METEOR BLEU ROUGE-L METEOR LLaVA-Med-1.5 9.64 + Greedy + Beam Search + DoLa + OPERA + VCD + MedDr + FactMM-RAG + RULE MMed-RAG 11.47 12.10 11.79 10.66 10.42 12.37 14.70 27.53 31.38 12.26 15.38 16.21 15.82 14.70 14.14 16.45 18.05 23. 25.59 8.21 12.69 13.17 12.72 12.01 11.59 13.50 15.92 27.99 32.43 12. 16.63 16.97 17.11 15.40 15.18 18.59 18.71 18.61 23.25 13.05 14.26 14.74 14.89 12.52 12.30 15.72 15.84 15. 12.34 11.16 14.19 14.43 14.81 13.72 13.38 16.77 16.82 17.42 20.47 18. 17.98 18.37 18.26 16.59 16.73 19.82 20.82 22.35 24.82 11.36 11.49 12.62 12.51 11.47 11.38 13.72 14.17 14. 16.59 10.75 13.77 14.50 14.51 13.63 13.89 15.40 15.31 17.74 19.85 Evaluation Datasets. We utilize five medical vision-language datasets for medical VQA and report generation tasks, i.e., MIMIC-CXR (Johnson et al., 2019), IU-Xray (Demner-Fushman et al., 2016), Harvard-FairVLMed (Luo et al., 2024), PMC-OA (Lin et al., 2023a) (we only select the pathology part) and Quilt-1M (Ikezogwo et al., 2024). These datasets cover radiology, ophthalmology, and pathology. To construct the VQA benchmarks, following (Xia et al., 2024a), we generate questionanswer pairs from medical reports using GPT-4 (OpenAI, 2023), with answers formatted as yes or no. Pathology images are excluded from the report generation task due to their brief and insufficient descriptions. The detailed dataset descriptions are provided in the Appendix A.2. Evaluation Metrics. Following (Jing et al., 2017; Lin et al., 2023b), we use Accuracy, F1 Score and AUROC for evaluating medical VQA task, and BLEU Score (Papineni et al., 2002), ROUGE-L (Lin, 2004) and METEOR (Banerjee & Lavie, 2005) for evaluating report generation task. 5.2 MAIN RESULTS In this section, we provide comprehensive comparison with various baseline methods and other open-source Med-LVLMs on medical VQA and report generation tasks. Comparison with Baselines. We compare MMed-RAG with baseline methods on medical VQA and report generation tasks, with the results presented in Table 1 and Table 2, respectively. Overall, MMed-RAG outperforms all baselines across nearly all metrics and datasets. Specifically, MMedRAG demonstrates significant performance boost, improving by 18.5% and 69.1% over the original Med-LVLM in medical VQA and report generation tasks, respectively. When compared to baseline methods, MMed-RAG surpasses decoding-based approaches, achieving improvements of 11.5% and 44.2% in the two tasks. Furthermore, recent RAG-based methods show substantial improvements over earlier techniques, yet our approach still outperforms RAG-based baselines by 2.8% and 16.1% in the medical VQA and report generation tasks, respectively. This indicates that MMed-RAG effectively mitigates misalignment issues introduced by RAG. Notably, MMed-RAG 8 Preprint. achieves more pronounced gains in report generation, likely due to the higher complexity of the task and the greater influence of retrieved contexts in guiding open-ended generation. Comparison with Other Med-LVLMs. To provide comprehensive comparison, we evaluate MMed-RAG against other open-source Med-LVLMs to demonstrate the superiority of our approach. We assess the performance of these models across different medical image modalities, reporting the average results for medical VQA and report generation tasks in Table 3 (see Appendix A.6 for detailed results). Our findings show that MMed-RAG significantly outperforms Med-LVLMs pre-trained on large-scale datasets across various domains. This reinforces the generalizability and effectiveness of our approach across diverse image domains and medical multimodal tasks. Table 3: Performance comparison with several Med-LVLMs. Rad: Radiology, Opt: Ophthalomology, Pat: Pathology. Model Rad Opt Pat Med-Flamingo MedVInT RadFM miniGPT-Med MMed-RAG 27.42 33.17 35.82 36.66 56.94 22.50 29.40 27.07 25.28 56.38 29.11 25.33 24.82 23.16 54."
        },
        {
            "title": "5.3 ANALYSIS",
            "content": "In this section, we provide detailed analysis of each modules performance, along with series of analytical experiments, to better understand the performance gains of MMed-RAG. Additionally, we demonstrate the compatibility of our approach, achieving consistent 40.3% performance improvement on different backbone, i.e., LLAVA-Med-1.0 (see details in Appendix A.6). Table 4: Ablation results on two datasets covering different domains. RG: report generation, FairVLMed: Harvard-FairVLMed. Ablation Studies. We conduct series of ablation experiments to evaluate the impact of each component in MMed-RAG. The results for both medical VQA and report generation tasks on the IU-Xray and Harvard-FairVLMed datasets are summarized in Table 4. According to the results, we can see that: (1) The domain-aware retrieval mechanism (DR) significantly improves the factuality of Med-LVLM, with an average performance increase of 17.9% and 16.1% on the IU-Xray and FairVLMed datasets, respectively. Here, the retrieved knowledge aids the model in generating more factual responses. (2) Building on this, the introduction of adaptive retrieval context selection (RCS) further filters out unreliable retrieved contexts, yielding an additional performance boost of 19.3% and 6.3% on the IU-Xray and FairVLMed datasets. (3) The inclusion of RAG-based preference fine-tuning (RAG-PT) enhances the models understanding of the retrieved knowledge, leading to substantial performance gains of 37.1% and 16.9% on the respective datasets. This demonstrates that RAG-PT effectively addresses misalignment issues. LLaVA-Med-1.5 +DR +RCS +RAG-PT (Ours) FairVLMed RG 13.41 15.89 17.22 20. 68.99 77.12 79.56 85.80 10.04 13.23 17.92 29.80 66.63 72.69 75.74 87.18 IU-Xray Model VQA VQA RG Table 5: Performance using RAG-PT based on subsets of preference data. Impact of the Preference Data in RAG-PT. To better understand how RAG-PT mitigates the misalignment issue and improves performance, we conducted detailed study on the training preference data composition of RAG-PT. As described in Section 3.3, the RAG-PT data is designed to address both cross-modality alignment and overall alignment objectives, with the latter focusing on enhanced understanding of retrieved knowledge and minimizing retrieval interference. The detailed experimental results in Table 5 demonstrate that the preference data tailored for different alignment objectives positively impacts the models performance, showing the effectiveness of RAG-PT. LLaVA-Med-1.5 +RAG-PT 1 +RAG-PT 2 +RAG-PT 3 FairVLMed RG 10.04 19.38 20.16 19.43 13.41 18.37 18.66 18.92 66.63 79.42 79.35 80.07 68.99 80.19 80.27 81.30 IU-Xray Model VQA VQA RG How Effective is MMed-RAG in Mitigating Misalignment Issues? To gain more intuitive understanding of the effectiveness of MMed-RAG in addressing misalignment issues: 1) we calculate the proportion of errors caused by RAG and compare it to the proportion after incorporating MMedRAG. 2) We visualize the attention maps of image and text tokens with and without RAG-PT. First, as mentioned in Section 3.3, the model may directly copy reference information, referred to as Copy-Reference (CR) rate. After applying MMed-RAG, as shown in Figure 3, the CR rate drops to 28.19%. Additionally, the proportion of errors affected by RAG interference, referred to as OverReliance (OR) rate, which is initially 43.31%, decreased to 8.38% after incorporating MMed-RAG. 9 Preprint. Furthermore, as shown in Figure 4, the original Med-LVLM tends to rely more heavily on text while ignoring visual information. When retrieval information is introduced, the original Med-LVLM focused more on the retrieved answers, even if the content is incorrect. After RAG-PT, the model significantly increases its attention to visual information and reduces the interference of RAG, thus better aligning the models knowledge with the fundamental facts. Figure 3: Alignment analysis with and without RAG. OR: Over-Reliance; CR: CopyReference."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Figure 4: Visualization of attention map. The red box region is labeled with the attentions that can be enhanced by MMed-RAG. Factuality in Med-LVLMs. The rapid advancements in Large Vision-Language Models (LVLMs) (Liu et al., 2024a;b; Xia et al., 2023; Zhou et al., 2023; 2024; Chen et al., 2024d) are beginning to influence the field of medical image analysis. Several Med-LVLMs (Li et al., 2023a; Moor et al., 2023; Zhang et al., 2023b; Wu et al., 2023b), have emerged, showing remarkable performance across different medical imaging modalities. Despite these advances, Med-LVLMs continue to present notable factual hallucination (Xia et al., 2024a; Royer et al., 2024), generating textual outputs that contradict medical visual information. This raises concerns about potential misdiagnoses or overlooked conditions. Recently, benchmarks have been developed to assess the accuracy of MedLVLMs in tasks such as visual question answering (VQA) and report generation (Xia et al., 2024a; Royer et al., 2024). However, research aimed at enhancing the factual accuracy of Med-LVLMs remains relatively unexplored. Retrieval Augmented Generation in Med-LVLMs. Retrieval-Augmented Generation (RAG) has proven to be powerful technique for enhancing factual accuracy in language modeling (Gao et al., 2023; Wu et al., 2023c; Chen et al., 2024c; Qu et al., 2024). In the biomedical domain, RAG leverages external knowledge to guide the generation of Med-LVLMs, offering clear advantages in tasks such as medical VQA and report generation (Yuan et al., 2023; Kumar & Marttinen, 2024; Tao et al., 2024; He et al., 2024; Sun et al., 2024). However, these works mainly focus on enhancing the relevance of the retrieved contexts without considering the models understanding of retrieved knowledge. Recently, RULE (Xia et al., 2024c) is proposed to use preference fine-tuning to reduce the models over-reliance on retrieved contexts. However, it still overlooks misalignment issues caused by RAG, as well as the generalizability of the retriever given the diverse domains of input images. In response, we propose MMed-RAG to mitigate these risks, enhancing the factuality of Med-LVLMs by addressing these overlooked factors. This can lead to better cross-modality and overall alignment to enhance the understanding of retrieved knowledge and visual information, ensuring more consistent and reliable performance across tasks."
        },
        {
            "title": "7 CONCLUSION",
            "content": "This paper introduces MMed-RAG, versatile multimodal RAG system designed to address the critical issue of factual hallucination in Med-LVLMs. MMed-RAG employs domain-aware retrieval mechanism, adaptive calibration for selecting the optimal number of retrieved contexts, and RAG-based preference fine-tuning to improve both cross-modality alignment and overall alignment with the ground truth. These enhancements significantly boost the factual accuracy of Med-LVLMs. Experimental results demonstrate MMed-RAG effectiveness in enhancing factual accuracy across various imaging domains, including radiology, ophthalmology, pathology. Our findings underscore the importance of incorporating robust multimodal RAG mechanism to ensure that Med-LVLMs can serve as dependable tools in clinical settings. 10 Preprint."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "This work is partially supported by Cisco Faculty Research Award."
        },
        {
            "title": "REFERENCES",
            "content": "Asma Alkhaldi, Raneem Alnajim, Layan Alabdullatef, Rawan Alyahya, Jun Chen, Deyao Zhu, Ahmed Alsinan, and Mohamed Elhoseiny. Minigpt-med: Large language model as general interface for radiology diagnosis. arXiv preprint arXiv:2407.04106, 2024. Emily Alsentzer, John Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, arXiv preprint Publicly available clinical bert embeddings. and Matthew McDermott. arXiv:1904.03323, 2019. Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pp. 6572, 2005. Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, and Lihua Zhang. Detecting and evaluating medical hallucinations in large vision language models. arXiv preprint arXiv:2406.10185, 2024a. Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, et al. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale. arXiv preprint arXiv:2406.19280, 2024b. Zhanpeng Chen, Chengjin Xu, Yiyan Qi, and Jian Guo. Mllm is strong reranker: Advancing multimodal retrieval-augmented generation via knowledge-enhanced reranking and noise-injected training. arXiv preprint arXiv:2407.21439, 2024c. Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou, Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Qinglan Huang, et al. Mj-bench: Is your multimodal reward model really good judge for text-to-image generation? arXiv preprint arXiv:2407.04842, 2024d. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models, 2024e. URL https://arxiv.org/ abs/2401.01335. Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. arXiv preprint arXiv:2309.03883, 2023. Dina Demner-Fushman, Marc Kohli, Marc Rosenman, Sonya Shooshan, Laritza Rodriguez, Sameer Antani, George Thoma, and Clement McDonald. Preparing collection of radiology examinations for distribution and retrieval. Journal of the American Medical Informatics Association, 23(2):304310, 2016. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2023. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770778, 2016. Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, and Hao Chen. Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning. arXiv preprint arXiv:2404.15127, 2024. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, arXiv preprint and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv:2106.09685, 2021. Preprint. Ming Hu, Lin Wang, Siyuan Yan, Don Ma, Qingli Ren, Peng Xia, Wei Feng, Peibo Duan, Lie Ju, and Zongyuan Ge. Nurvid: large expert-level video database for nursing procedure activity understanding. Advances in Neural Information Processing Systems, 36, 2024a. Ming Hu, Peng Xia, Lin Wang, Siyuan Yan, Feilong Tang, Zhongxing Xu, Yimin Luo, Kaimin Song, Jurgen Leitner, Xuelian Cheng, et al. Ophnet: large-scale video benchmark for ophthalmic surgical workflow understanding. arXiv preprint arXiv:2406.07471, 2024b. Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. arXiv preprint arXiv:2311.17911, 2023. Wisdom Ikezogwo, Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, and Linda Shapiro. Quilt-1m: One million image-text pairs for histopathology. Advances in neural information processing systems, 36, 2024. Yue Jiang, Jiawei Chen, Dingkang Yang, Mingcheng Li, Shunli Wang, Tong Wu, Ke Li, and Lihua Zhang. Medthink: Inducing medical large-scale visual language models to hallucinate less by thinking more. arXiv preprint arXiv:2406.11451, 2024. Baoyu Jing, Pengtao Xie, and Eric Xing. On the automatic generation of medical imaging reports. arXiv preprint arXiv:1711.08195, 2017. Alistair EW Johnson, Tom Pollard, Nathaniel Greenbaum, Matthew Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger Mark, Seth Berkowitz, and Steven Horng. Mimic-cxr-jpg, large publicly available database of labeled chest radiographs. arXiv preprint arXiv:1901.07042, 2019. Yogesh Kumar and Pekka Marttinen. Improving medical multi-modal contrastive learning with expert annotations. arXiv preprint arXiv:2403.10153, 2024. Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. arXiv preprint arXiv:2311.16922, 2023. Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large language-and-vision assistant for biomedicine in one day. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023a. Wenxue Li, Xinyu Xiong, Peng Xia, Lie Ju, and Zongyuan Ge. Tp-drseg: Improving diabetic retinopathy lesion segmentation with explicit text-prompts assisted sam. arXiv preprint arXiv:2406.15764, 2024. Yingshu Li, Yunyi Liu, Zhanyu Wang, Xinyu Liang, Lingqiao Liu, Lei Wang, Leyang Cui, Zhaopeng Tu, Longyue Wang, and Luping Zhou. comprehensive study of gpt-4vs multimodal capabilities in medical imaging. arXiv preprint arXiv:2310.20381, 2023b. Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pp. 7481, 2004. Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-clip: Contrastive language-image pre-training using biomedical documents. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 525536. Springer, 2023a. Zhihong Lin, Donghao Zhang, Qingyi Tao, Danli Shi, Gholamreza Haffari, Qi Wu, Mingguang He, and Zongyuan Ge. Medical visual question answering: survey. Artificial Intelligence in Medicine, 143:102611, 2023b. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024a. Preprint. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b. Yan Luo, Min Shi, Muhammad Osama Khan, Muhammad Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu Tian, Luo Song, Ava Kouhana, Tobias Elze, et al. Fairclip: Harnessing fairness in vision-language learning. arXiv preprint arXiv:2403.19949, 2024. Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: multimodal medical fewshot learner. In Machine Learning for Health (ML4H), pp. 353367. PMLR, 2023. OpenAI. Gpt-4 technical report, 2023. https://arxiv.org/abs/2303.08774. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311318, 2002. Xiaoye Qu, Qiyuan Chen, Wei Wei, Jishuo Sun, and Jianfeng Dong. Alleviating halluciarXiv preprint nation in large vision-language models with active retrieval augmentation. arXiv:2408.00555, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Corentin Royer, Bjoern Menze, and Anjany Sekuboyina. Multimedeval: benchmark and toolkit for evaluating medical vision-language models. arXiv preprint arXiv:2402.09262, 2024. Jessica Schrouff, Natalie Harris, Sanmi Koyejo, Ibrahim Alabdulmohsin, Eva Schnider, Krista Opsahl-Ong, Alexander Brown, Subhrajit Roy, Diana Mincu, Christina Chen, et al. Diagnosing failures of fairness transfer across distribution shift in real-world medical settings. Advances in Neural Information Processing Systems, 35:1930419318, 2022. Congzhen Shi, Ryan Rezai, Jiaxi Yang, Qi Dou, and Xiaoxiao Li. survey on trustworthiness in foundation models for medical image analysis. arXiv preprint arXiv:2407.15851, 2024. Liwen Sun, James Zhao, Megan Han, and Chenyan Xiong. Fact-aware multimodal retrieval augmentation for accurate medical radiology report generation. arXiv preprint arXiv:2407.15268, 2024. Ilya Sutskever, Oriol Vinyals, and Quoc Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 31043112, 2014. Yitian Tao, Liyan Ma, Jing Yu, and Han Zhang. Memory-based cross-modal semantic alignment network for radiology report generation. IEEE Journal of Biomedical and Health Informatics, 2024. Alexandra-Maria Tautan, Bogdan Ionescu, and Emiliano Santarnecchi. Artificial intelligence in neurodegenerative diseases: review of available tools with focus on machine learning techniques. Artificial Intelligence in Medicine, 117:102081, 2021. Omkar Thawkar, Abdelrahman Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Jorma Laaksonen, and Fahad Shahbaz Khan. Xraygpt: Chest radiographs summarization using medical vision-language models. arXiv preprint arXiv:2306.07971, 2023. Robert Tibshirani, Guenther Walther, and Trevor Hastie. Estimating the number of clusters in Journal of the Royal Statistical Society: Series (Statistical data set via the gap statistic. Methodology), 63(2):411423, 2001. 13 Preprint. Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical ai. NEJM AI, 1(3):AIoa2300138, 2024. Chunhao Wang, Xiaofeng Zhu, Julian Hong, and Dandan Zheng. Artificial intelligence in radiotherapy treatment planning: present and future. Technology in cancer research & treatment, 18: 1533033819873922, 2019. Chaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin, Xiaoman Zhang, Xiao Zhou, Ziheng Zhao, Ya Zhang, Yanfeng Wang, et al. Can gpt-4v (ision) serve medical applications? case studies on gpt-4v for multimodal medical diagnosis. arXiv preprint arXiv:2310.09909, 2023a. Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for radiology. arXiv preprint arXiv:2308.02463, 2023b. Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun Shum, Cheng Niu, Randy Zhong, Juntong Song, and Tong Zhang. Ragtruth: hallucination corpus for developing trustworthy retrieval-augmented language models. arXiv preprint arXiv:2401.00396, 2023c. Peng Xia, Xingtong Yu, Ming Hu, Lie Ju, Zhiyong Wang, Peibo Duan, and Zongyuan Ge. Hgclip: Exploring vision-language models with graph representations for hierarchical understanding. arXiv preprint arXiv:2311.14064, 2023. Peng Xia, Ze Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, et al. Cares: comprehensive benchmark of trustworthiness in medical vision language models. arXiv preprint arXiv:2406.06007, 2024a. Peng Xia, Ming Hu, Feilong Tang, Wenxue Li, Wenhao Zheng, Lie Ju, Peibo Duan, Huaxiu Yao, and Zongyuan Ge. Generalizing to unseen domains in diabetic retinopathy with disentangled representations. In arXiv preprint arXiv:2406.06384, 2024b. Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, and Huaxiu Yao. Rule: Reliable multimodal rag for factuality in medical vision language models. arXiv preprint arXiv:2407.05131, 2024c. Qing Ye, Chang-Yu Hsieh, Ziyi Yang, Yu Kang, Jiming Chen, Dongsheng Cao, Shibo He, and Tingjun Hou. unified drugtarget interaction prediction framework based on knowledge graph and recommendation system. Nature communications, 12(1):6775, 2021. Zheng Yuan, Qiao Jin, Chuanqi Tan, Zhengyun Zhao, Hongyi Yuan, Fei Huang, and Songfang Huang. Ramm: Retrieval-augmented biomedical visual question answering with multi-modal pre-training. In Proceedings of the 31st ACM International Conference on Multimedia, pp. 547 556, 2023. Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, et al. Biomedclip: multimodal biomedical arXiv preprint foundation model pretrained from fifteen million scientific image-text pairs. arXiv:2303.00915, 2023a. Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415, 2023b. Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. arXiv preprint arXiv:2310.00754, 2023. Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. Calibrated self-rewarding vision language models. arXiv preprint arXiv:2405.14622, 2024. 14 Preprint."
        },
        {
            "title": "A EXPERIMENT",
            "content": "A.1 EXPERIMENTAL SETUP A.1.1 DATA STATISTICS The data quantities used in this study are presented in Table 6, Table 7 and Table 8. We clarify that for training the retriever, the data refers to the number of image-text pairs, while for fine-tuning, it refers to the number of QA items. The All category represents the total amount of data used to construct the preference dataset for RAG-PT. The training of RAG-PT includes three types of samples: (a) clean samples with originally correct answers that remain correct even after adding noise to the images, (b) clean image samples with originally incorrect answers that become correct, and (c) clean image samples with originally correct answers that become incorrect. Table 6: Data statistics for medical VQA task. Train (DR) refers to the number of image-text pairs for retriever training, All (RAG-PT) refers to the total data for RAG-PT, and Train (RAG-PT)- a/b/c refer to the respective subsets for RAG-PT training. Dataset Train (DR) All (RAG-PT) Train (RAG-PT)-a Train (RAG-PT)-b Train (RAG-PT)-c Ophthalomology Radiology Pathology 7000 4034 5000 3247 4836 1082 1612 663 1030 1989 523 1135 1235 804 Table 7: Data statistics for report generation. Train (DR) refers to the number of image-text pairs for retriever training, All (RAG-PT) refers to the total data for RAG-PT, and Train (RAG-PT)- a/b/c refer to the respective sample categories for RAG-PT training. Dataset Train (R) All (RAG-PT) Train (RAG-PT)-a Train (RAG-PT)-b Train (RAG-PT)-c Ophthalmology Radiology 7000 4034 3247 142 233 78 126 207 342 Table 8: Data statistics for various datasets. The rows represent the number of images and QA pairs for each dataset. Harvard-FairVLMed IU-Xray MIMIC-CXR PMC-OA Quilt-1M # Images # QA Items 713 4285 589 2573 700 3470 530 3124 559 A.1.2 HYPERPARAMETER SETTINGS Following the settings of CLIP (Radford et al., 2021), we adopt the same architecture and hyperparameters for the vision and text encoders. The vision encoder is ResNet-50 (He et al., 2016), and the text encoder is bio-bert-based model (Alsentzer et al., 2019). We use the AdamW optimizer with learning rate of 104 and batch size of 512. The model is trained for 360 epochs. For the first phase, we trained for 3 epochs, and for the second phase, the training was conducted for 1 epoch. Training for 20 hours on one A100 80G GPU. For the RAG-PT phase, we adjust the diffusion noise level, symbolized by ξ through specific formula: ξ = Sigmoid(lt) (0.5 102 105) + 105, where ϵ is drawn from normal distribution. The reports available for retrieval are from the training set of the corresponding dataset. In our experiments, we apply cross-validation to tune all hyperparameters with grid search. All the experiments are implemented on PyTorch 2.1.2 using four NVIDIA RTX A6000 GPUs. It takes roughly 3 and 4 hours for fine-tuning CLIP and LLaVA-Med1.5 7B, respectively. A.2 EVALUATED DATASETS We utilize five open-source medical vision-language datasets, i.e., MIMIC-CXR (Johnson et al., 2019), IU-Xray (Demner-Fushman et al., 2016), Harvard-FairVLMed (Luo et al., 2024), PMCOA (Lin et al., 2023a) and Quilt-1M (Ikezogwo et al., 2024). 15 Preprint. MIMIC-CXR (Johnson et al., 2019) is large publicly available dataset of chest X-ray images in DICOM format with associated radiology reports. IU-Xray (Demner-Fushman et al., 2016) is dataset that includes chest X-ray images and corresponding diagnostic reports. Harvard-FairVLMed (Luo et al., 2024) focuses on fairness in multimodal fundus images, containing image and text data from various sources. It aims to evaluate bias in AI models on this multimodal data comprising different demographics. PMC-OA (Lin et al., 2023a) is large-scale dataset comprising figure-caption pairs extracted from PubMed Central. It covers 2,478,267 papers and includes total of 12,211,907 figure-caption pairs. We only use the pathology subset filtered by GPT-4 based on the captions. Quilt-1M (Ikezogwo et al., 2024) is the largest vision-language dataset in histopathology, containing 1 million image-text pairs sourced from platforms such as YouTube, Twitter, research papers, and other parts of the internet. A.3 EVALUATED MODELS i.e., LLaVA-Med (Li et al., 2023a), MedWe evaluate five open-source Med-LVLMs, Flamingo (Moor et al., 2023), MedVInT (Zhang et al., 2023b), RadFM (Wu et al., 2023b), miniGPTMed (Alkhaldi et al., 2024). The selected models are all at the 7B level. LLaVA-Med (Li et al., 2023a) is vision-language conversational assistant, adapting the generaldomain LLaVA (Liu et al., 2024b) model for the biomedical field. The model is fine-tuned using novel curriculum learning method, which includes two stages: aligning biomedical vocabulary with figure-caption pairs and mastering open-ended conversational semantics. It demonstrates excellent multimodal conversational capabilities. Med-Flamingo (Moor et al., 2023) is multimodal few-shot learner designed for the medical domain. It builds upon the OpenFlamingo, continuing pre-training with medical image-text data from publications and textbooks. This model aims to facilitate few-shot generative medical visual question answering, enhancing clinical applications by generating relevant responses and rationales from minimal data inputs. RadFM (Wu et al., 2023b) serve as versatile generalist model in radiology, distinguished by its capability to adeptly process both 2D and 3D medical scans for wide array of clinical tasks. It integrates ViT as visual encoder and perceiver module, alongside the MedLLaMA language model, to generate sophisticated medical insights for variety of tasks. This design allows RadFM to not just recognize images but also to understand and generate human-like explanations. MedVInT (Zhang et al., 2023b), which stands for Medical Visual Instruction Tuning, is designed to interpret medical images by answering clinically relevant questions. This model features two variants to align visual and language understanding: MedVInT-TE and MedVInT-TD. Both MedVInT variants connect pre-trained vision encoder ResNet-50 adopted from PMC-CLIP (Lin et al., 2023a), which processes visual information from images. It is an advanced model that leverages novel approach to align visual and language understanding. miniGPT-Med (Alkhaldi et al., 2024) is vision-language model derived from large-scale language models and tailored for radiology diagnosis applications. It handles various medical visionlanguage task using distinct task identifiers, demonstrating advanced performance in disease grounding, medical report generation, and medical VQA. A.4 OVERVIEW OF THE BASELINES We compare MMed-RAG with two types of LVLM hallucination mitigation methods that show promising results in natural image understanding. 1) Decoding-based methods, including Greedy Decoding, Beam Search (Sutskever et al., 2014), DoLa (Chuang et al., 2023), OPERA (Huang et al., 2023), VCD (Leng et al., 2023). These methods manipulate the logits of the models output tokens to enhance factual accuracy. 2) Multimodal RAG-based methods, including MedDr (He et al., 2024), FactMM-RAG (Sun et al., 2024), RULE (Xia et al., 2024c). 16 Preprint. Instruction [Round1] You are professional medical expert. will provide you with some medical reports. Please generate some questions with answers (the answer should be yes or no) based on the provided report. The subject of the questions should be the medical image or patient, not the report. Below are the given report: [REPORT] Instruction [Round2] Please double-check the questions and answers, including how the questions are asked and whether the answers are correct. You should only generate the questions with answers and no other unnecessary information. Below are the given report and QA pairs in round1: [REPORT] [QA PAIRS R1] Table 9: The instruction to GPT-4 for generating QA pairs. Greedy decoding involves selecting the most probable next token at each step of generation. While it is efficient and straightforward, it can lead to suboptimal outcomes by getting stuck in repetitive or less creative patterns. Beam search (Sutskever et al., 2014) expands on greedy decoding by maintaining multiple candidate sequences (or beams) at each step, allowing for broader exploration of possible outputs. This approach balances quality and diversity by selecting the top-k sequences based on their probabilities, resulting in more coherent and creative text generation compared to greedy decoding. DoLa (Chuang et al., 2023) derives the next-token distribution by contrasting the logits projected from later layers against those from earlier layers, leveraging the fact that factual knowledge in LLMs is typically localized within specific transformer layers. OPERA (Huang et al., 2023) is LVLMs decoding method based on an Over-trust Penalty and Retrospection-Allocation strategy The key insight is that hallucinations are closely tied to knowledge aggregation patterns in the self-attention matrix, where MLLMs tend to focus on summary tokens, neglecting image tokens and resulting in content hallucination. VCD (Leng et al., 2023) is decoding method that tackles the object hallucination issue in LVLMs. It contrasts output distributions derived from original and distorted visual inputs to calibrate the models output without the usage of external tools, reducing the the over-reliance on statistical bias and unimodal priors. MedDr (He et al., 2024) is medical foundation model built upon generated diagnosis-based datasets, demonstrating advanced capabilities in various data modalities. Meddr also integrates retrieval-augmented medical diagnosis strategy during inferencing to enhance factual accuracy. FactMM-RAG (Sun et al., 2024) is fact-aware multimodal retrieval-augmented pipeline for radiology report generation. It utilize RadGraph to annotate chest radiograph reports and mine clinically relevant pairs to train universal multimodal retriever. RULE (Xia et al., 2024c) is an advanced medical retrieval-augmented generation strategy designed to enhance the factuality of Med-LVLMs. First, it introduces robust strategy for controlling factuality risk through the calibrated selection of retrieved contexts. Second, RULE develops preference optimization strategy to balance Med-LVLMs intrinsic knowledge and the retrieved information. A.5 PROMPTS We convert the medical reports into series of closed-ended questions with yes or no answers. To ensure the quality of the VQA data, we perform round of self-checks using GPT-4 (OpenAI, 2023). Finally, we conduct an round of manual filtering to remove questions with obvious issues or those related to multiple images or patient histories. The prompt templates used are shown in Table 9. A.6 ADDITIONAL RESULTS Generalization on Different Backbones. To demonstrate the compatibility of our approach across different backbone models, we apply it to LLaVA-Med-1.0. As shown in Table 10, our method 17 Preprint. delivers an average improvement of 40.3% over the original LLaVA-Med-1.0, further highlighting its effectiveness in enhancing RAG performance and its adaptability to various backbones. MMedRAG can be transferred to different Med-LVLMs, yielding consistent improvements across various domains, demonstrating the compatibility of our method. Table 10: Performance on different backbones. Model LLaVA-Med-1.0 +MMed-RAG IU-Xray VQA 61.73 80.32 RG 8.74 22.63 FairVLMed RG VQA 59.54 78.49 10.59 15.88 Detailed Results of Other Med-LVLMs. As shown in Table 11, we illustrate the detailed performance simply shown in Table 3. PROOFS FOR THEORETICAL RESULTS IN SECTION 4 Here we provide proofs for the results in Section 4. B.1 NOTATIONS Let xv, y, xt, xr be input medical image, ground-truth answer, question, and retrieved information, respectively. Denote (xw, yw,o) qp(xw, yw,oxt, xr) and (xl, yl,o) ql(xl, yl,oxt, xr) as distributions of the preferred responses and dispreferred responses. Let denote (xv, xr, xt). We aim to fine-tune generative model πθ(yx, xt) through DPO loss (Rafailov et al., 2023): (cid:19) (cid:18) arg min πθ E(xw,xl,yw,o,yl,o)DU α log πθ(yw,ox) πo(yw,ox) α log πθ(yl,ox) πo(yl,ox) . (14) where (t) = log(1 + exp(t)). Define the weight of xv with respect to log πθ(yx) as wt(xv, πθ) := Eyπθ(x) (cid:20) xv (cid:21)2 log πθ(yx) (15) B.2 ASSUMPTIONS Assumption B.1 (Large parameter space) Assume that π(xv, yxt, xr) lies in the optimization space {πθ, θ Θ} such that π(xv, yxt, xr) πo(xv, yxt, xr) (cid:16) qw(xv,yxt,xr) ql(xv,yxt,xr) (cid:17) 1 α Assumption B.1 requires that the parameter space sufficiently large to ensure that πθ can achieve its global optimum, allowing us to represent the optimizer with closed form. Assumption B.2 Let h(x, y), abbreviate as h, be := (cid:34) (cid:88) πo(yx) (cid:19) 1 α (cid:18) qw(yx) ql(yx) (cid:35)1 (cid:18) qw(yx) ql(yx) (cid:19) 1 α Assume that wt(xv, πo) < c2, where (cid:115)(cid:13) (cid:13) (cid:13) (cid:13) = (cid:112)πo(yx) xv (cid:13) (cid:13) (cid:13) (cid:13) 2 + (cid:90) (cid:18) xv (cid:19)2 πo(yx) dy (cid:13) (cid:13) (cid:13) (cid:13) (cid:112)πo(yx) xv (cid:13) (cid:13) (cid:13) (cid:13)2 Assumption B.3 Let h1(xv, xt, xr, y), abbreviate as h1, be h1 := (cid:34) (cid:88) πo(yx) (cid:18) q1 w(yxv, xt, xr) + q2 (yxv, xt) + q2 w(yxv, xt) (yxv, xt, xr) 18 (cid:19) 1 α (cid:35)1 (cid:18) q1 w(yxv, xt, xr) + q2 (yxv, xt) + q2 q1 w(yxv, xt) (yxv, xt, xr) (16) (17) (cid:19) 1 α (18) Preprint. Table 11: Model performance (%) of different Med-LVLMs based on LLaVA-Med-1.5 on medical VQA task. Models LLaVA-Med-1.5 MMed-RAG Med-Flamingo MedVInT RadFM miniGPT-Med Radiology Ophthalmology Pathology IU-Xray MIMIC-CXR Harvard-FairVLMed Quilt-1M PMC-OA (Pathology) 75.47 89.54 26.74 73.34 26.67 54.87 75.79 83.57 61.27 66.06 69.30 53.92 63.03 87. 42.06 35.92 52.47 66.73 62.80 72.95 27.11 26.81 27.02 26.82 59.28 64.54 32.62 27.77 25.12 27.03 Assume that wt(xr, πo) < 1 and wt(xr, πo) > c2 2, where c1 = c2 = (cid:115)(cid:13) (cid:13) (cid:13) (cid:13) (cid:115)(cid:13) (cid:13) (cid:13) (cid:13) πo πo h1 xr h1 xr (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2 2 2 2 + + (cid:90) (cid:18) h1 xr (cid:19)2 πo h1 dy πo (cid:13) (cid:13) (cid:13) (cid:13) h1 xr (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:90) (cid:18) h1 xr (cid:19)2 πo h1 + (cid:18) πo xr (cid:19)2 h1 πo dy + πo (cid:13) (cid:13) (cid:13) (cid:13) h1 xr (cid:13) (cid:13) (cid:13) (cid:13)2 B.3 PROOFS Lemma B.1 Suppose that Assumption B.1 hold, optimizing equation 14 gives πθ(yx) πo(yx) (cid:19) α (cid:18) qw(yx) ql(yx) (19) (20) Lemma B.1 indicates that the model tends to increase πo(yx) if qw(yx) > ql(yx), which is more likely to occur when (xv, y) represents preferred sample given xt and xr. Below, we provide an application of Lemma B.1 using linear regression example. Lemma B.1 is proved with Lemma B.2 and Lemma B.3. Lemma B.2 (Lemma C.1 in Chen et al. (2024e)) For a, > 0, the following inequality holds (t) + (t) log(1 + b/a) + log(1 + a/b) and equality holds if and only if = log(a/b) Lemma B.3 Denote (cid:26) p1(xw, yw,o, xl, yl,oxt, xr) = qw(xw, yw,oxt, xr) ql(xl, yl,oxt, xr) p2(xw, yw,o, xl, yl,oxt, xr) = ql(xw, yw,oxt, xr) qw(xl, yl,oxt, xr) and abbreviated as p1 and p2 for notational convenience. Then, 2ED [U (f (xw, yw,o, xt, xr) (xl, yl,o, xt, xr))] p1 + p2 2 log 2 DKL DKL p1 + p2 2 p2 p1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:19) (cid:18) (cid:18) Equality holds if and only if (x, y) = g(x) + log qw(xv, yxt, xr) ql(xv, yxt, xr) where g(x) is any function that is possibly dependent on xv, xt and xr. 19 (cid:19) (21) (22) Preprint. Proof B.1 2ED [U (f (xw, yw,o, xt, xr) (xl, yl,o, xt, xr))] (cid:90) q(xt, xr) p1 (f (xw, yw,o, xt, xr) (xl, yl,o, xt, xr)) dxdy + (cid:90) (cid:90) q(xt, xr) p2 (f (xl, yl,o, xt, xr) (xw, yw,o, xt, xr)) dxdy q(xt, xr) (cid:20) p1 log (cid:18) 1 + (cid:18) + p2 log 1 + (cid:19) p2 p1 (cid:19)(cid:21) p1 p2 dxdy (23) = (cid:90) =2 log 2 + (cid:20) q(xt, xr) p1 log =2 log 2 KL (cid:18) (cid:13) (cid:13) p1 p1 + p2 2 (cid:19) (cid:19) (cid:18) p1 + p2 2p1 (cid:18) KL p2 + p2 log (cid:19) (cid:13) (cid:13) p1 + p2 (cid:19)(cid:21) (cid:18) p1 + p2 2p2 dxdy where the first inequality follows from Lemma B.2. For equivalence, (x, yw,o, xt, xr) (xl, yl,o, xt, xr) = log qw(xw, yw,oxt, xr) ql(xl, yl,oxt, xr) ql(xw, yw,oxt, xr) qw(xl, yl,oxt, xr) (24) Thus, for any xw, yw,o, xl, yl,o, xt, xr, (xw, yw,o, xt, xr) log qw(xw, yw,oxt, xr) ql(xw, yw,oxt, xr) = (xl, yl,o, xt, xr) log qw(xl, yl,oxt, xr) ql(xl, yl,oxt, xr) (25) Therefore, equation 25 holds if and only if there exists some g(xv, xt, xr) such that (xv, xt, xr, y) = g(xt, xr) + log qw(xv, yxt, xr) ql(xv, yxt, xr) (26) Lemma B.3 provides closed-form solution to equation 14 if the parameter space is sufficiently large. This lemma is crucial for the proof Lemma B.1, which follows below Proof B.2 According to the Assumption B.1, we have π(xv, yxt, xr) = ˆg(xt, xr)πo(xv, yxt, xr) (cid:18) qw(xv, yxt, xr) ql(xv, yxt, xr) (cid:19) 1 α After reparameterization, α log (cid:18) π(xv, yxt, xr) πo(xv, yxt, xr) (cid:19) = α log[ˆg(xt, xr)] + log qw(xv, yxt, xr) ql(xv, yxt, xr) which is the global minimum of arg min ED [U (f (xw, yw,o, xt, xr) (xl, yl,o, xt, xr))] by Lemma B.3. Since π(xv, yxt, xr) {πθ, θ Θ} lies in the optimization space, we have EDU (f (xw, yw,o, xt, xr) (xl, yl,o, xt, xr)) min = min πθ EDU (cid:18) α log πθ(yw,oxw, xt, xr) πo(yw,oxw, xt, xr) α log πθ(yl,oxl, xt, xr) πo(yl,oxl, xt, xr) (cid:19) and πθ(xv, yxt, xr) is the optimizer of equation 30, which gives (27) (28) (29) (30) α log (cid:18) πθ(xv, yxt, xr) πo(xv, yxt, xr) (cid:19) = g(xt, xr) + log qw(xv, yxt, xr) ql(xv, yxt, xr) =πθ(xv, yxt, xr) = πo(xv, yxt, xr) (cid:18) qw(xv, yxt, xr) ql(xv, yxt, xr) (cid:19) 1 α exp (cid:18) 1 α (31) (cid:19) g(xt, xr) 20 Preprint."
        },
        {
            "title": "Then",
            "content": "πθ(yx) = πθ(xv, yxt, xr) πθ(xxt, xr) = πo(xv, yxt, xr) (cid:80) πo(xv, yxt, xr) (cid:17) 1 α (cid:16) qw(xv,yxt,xr) ql(xv,yxt,xr) (cid:16) qw(xv,yxt,xr) ql(xv,yxt,xr) exp (cid:0) 1 (cid:17) 1 α α (g(xt, xr)(cid:1) exp (cid:0) 1 α (g(xt, xr)(cid:1) πo(yx) = (cid:80) πo(yx) (cid:17) 1 α (cid:16) qw(xv,yxt,xr) ql(xv,yxt,xr) (cid:16) qw(xv,yxt,xr) ql(xv,yxt,xr) = (cid:17) 1 α πo(yx) (cid:80) πo(yx) (cid:17) 1 α (cid:16) qw(yxv,xt,xr) ql(yxv,xt,xr) (cid:16) qw(yxv,xt,xr) ql(yxv,xt,xr) (cid:17) 1 α (32) Corollary B.1 Suppose that preferred responses (xw, yw) and dispreferred responses (xl, yl) satisfy yw = βxw + ϵ1 and yl = βxl + ϵ2 respectively. DPO for = θxv + ϵ3 is based on reference model = θoxv + ϵ4, where ϵis are independent and follow standard normal distribution. Then, θ = θo + (β β) 1 α (33) Corollary B.1 is direct application of Lemma B.1, indicating that the model updates coefficient θo towards the direction of β for preferred responses and away from β for dispreferred responses. Proof B.3 Let ϕ() denote the probability density function of standard normal, by Lemma B.1, ϕ(y θx) ϕ(y θox) = exp (cid:19) y2 θ1xy (cid:18) 1 2 (cid:19) 1 α (cid:18) ϕ(y βx) ϕ(y βx) (cid:18) 1 exp y2 θoxy (cid:19) (cid:18) exp 1 α (cid:19) (β β)xy = exp (θ1xy) exp (θoxy) exp (cid:19) (β β)xy (cid:18) 1 α =θ = θo + (β β) 1 α Lemma B.4 For linear model = θ1xv + θ2xt + ϵ such that ϵ (0, 1), wt(xv, πθ) = θ2 1 Proof B.4 Let ϕ() denote the probability density function of standard normal, (cid:90) (cid:18) wt(xv, πθ) = (cid:90) (cid:90) (cid:90) = θ2 = θ2 1 = θ2 1 1 2 xv (y θ1xv θ2xt) (cid:19)2 ϕ(y θ1xv θ2xt)dy (y θ1xv θ2xt)2 ϕ(y θ1xv θ2xt)dy (θ1xv + θ2xt y) dϕ(y θ1xv θ2xt) dy dy ϕ(y θ1xv θ2xt)dy = θ2 1 (34) (35) Theorem B.2 Suppose that Assumption B.2 holds, then cross-modality increase the weight of xv. wt(xv, πθ) > wt(xv, πo) (36) Proof B.5 By Lemma B.1, we have πθ(yx) = πo(yx) h(x, y), (cid:90) πo(yx) h(x, y)dy = 1 (37) Abbreviate h(x, y) and πo(yxv, xt) as and πo respectively, we have wt(xv, πθ) wt(xv, πo) (cid:90) (cid:32) xv πo πo + xv (cid:33)2 πoh dy wt(xv, πo) (cid:90) (cid:20) xv (cid:21)2 πo dy 2(cid:112)wt(xv, πo) (cid:13) (cid:13) (cid:13) (cid:13) πo xv (cid:13) (cid:13) (cid:13) (cid:13)2 wt(xv, πo) (38) 21 Preprint. the second inequality follows from CauchySchwarz inequality (cid:90) xv πo xv dy = (cid:90) xv πo πo πo xv dy (cid:112)wt(xv, πo) (cid:13) (cid:13) (cid:13) (cid:13) πo xv (cid:13) (cid:13) (cid:13) (cid:13)"
        },
        {
            "title": "Denote c as",
            "content": "(cid:115)(cid:13) (cid:13) (cid:13) (cid:13) πo := xv (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 + (cid:90) (cid:18) xv (cid:19)2 πo dy (cid:13) (cid:13) (cid:13) (cid:13) πo xv (cid:13) (cid:13) (cid:13) (cid:13)2 the last term in equation 38 is equivalent to (cid:16) (cid:17) (cid:112)wt(xv, πo) (cid:18) (cid:112)wt(xv, πo) + + 2 (cid:13) (cid:13) (cid:13) (cid:13) πo (cid:19) xv (cid:13) (cid:13) (cid:13) (cid:13)2 Thus, wt(xv, πθ) > wt(xv, πo) if (cid:112)wt(xv, πo) < c. (39) (40) (41) Theorem B.3 Suppose that Assumption B.3 holds, the overall loss increase the weight of xr and decrease the weight of xr. wt(xr, πθ) > wt(xr, πo), wt(xr, πθ) < wt(xr, πo) (42) Proof B.6 The distribution of preferred responses can be considered as mixture distribution: q1 w(xv, yw,oxt, xr) + q2 w(xv, yw,oxt). Similarly, for dispreferred responses, the distribution is represented as l (xv, yl,oxt, xr). By Lemma B.1, (xv, yl,oxt) + q2 πθ(yx) = πo(yx) h1(x, y), (cid:90) πo(yx) h1(x, y)dy = 1 (43) Abbreviate h1(x, y) as h1. Follow the same procedure in the proof of Theorem B.2, wt(xr, πθ) wt(xr, πo) (cid:90) (cid:20) xr h1 (cid:21)2 πo h1 dy 2(cid:112)wt(xr, πo) (cid:13) (cid:13) (cid:13) (cid:13) πo (cid:16) (cid:17) c1 (cid:112)wt(xr, πo) = (cid:18) (cid:112)wt(xr, πo) + c1 + 2 wt(xr, πo) h1 (cid:13) (cid:13) (cid:13) (cid:13) xr (cid:13) (cid:13) (cid:13) (cid:13) πo xr h1 (cid:13) (cid:13) (cid:13) (cid:13)2 where we apply CauchySchwarz inequality in equation 44. c1 = (cid:115)(cid:13) (cid:112)πo(yx) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 Thus, wt(xr, πθ) > wt(xr, πo) if (cid:112)wt(xr, πo) < c1. Again, by CauchySchwarz inequality (cid:13) (cid:112)πo(yx) (cid:13) (cid:13) (cid:13) (cid:90) (cid:18) xr (cid:19)2 πo(yx) xr xr (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 dy h1 h1 h1 + wt(xr, πθ) wt(xr, πo) (cid:18) πo (cid:19)2 πo (cid:90) (cid:18) h1 xr h1 xr (cid:17) (cid:16)(cid:112)wt( xr, πo) c2 + = (cid:19)2 h1 πo (cid:112)wt( xr, πo) c2 + 2 dy + 2(cid:112)wt( xr, πo) (cid:13) (cid:13) (cid:13) (cid:13) (cid:18) (cid:13) (cid:13) (cid:13) (cid:13) πo πo xr (cid:13) h1 (cid:13) (cid:13) xr (cid:13)2 (cid:13) (cid:19) (cid:13) (cid:13) (cid:13)2 wt(xr, πo) where c2 = (cid:115)(cid:13) (cid:13) (cid:13) (cid:13) πo xr h1 (cid:13) (cid:13) (cid:13) (cid:13) 2 2 + (cid:90) (cid:18) xr h1 (cid:19)2 πo h1 + (cid:18) xr πo (cid:19)2 h1 πo dy + πo (cid:13) (cid:13) (cid:13) (cid:13) xr h1 (cid:13) (cid:13) (cid:13) (cid:13)2 (47) Thus, wt(xr, πθ) < wt(xr, πo) if (cid:112)wt(xr, πo) > c2. 22 (cid:19) (44) (45) (46) Preprint."
        },
        {
            "title": "C MORE CASES",
            "content": "Figure 5: Case 1. Figure 6: Case 2. Figure 7: Case 3. Figure 8: Case 4. Figure 9: Case 5."
        }
    ],
    "affiliations": [
        "Brown University",
        "PloyU",
        "Rutgers University",
        "Stanford University",
        "UNC-Chapel Hill",
        "University of Washington"
    ]
}