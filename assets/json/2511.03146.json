{
    "paper_title": "MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity",
    "authors": [
        "Kaiyuan Zhang",
        "Chenghao Yang",
        "Zhoufutu Wen",
        "Sihang Yuan",
        "Qiuyue Wang",
        "Chaoyi Huang",
        "Guosheng Zhu",
        "He Wang",
        "Huawenyu Lu",
        "Jianing Wen",
        "Jianpeng Jiao",
        "Lishu Luo",
        "Longxiang Liu",
        "Sijin Wu",
        "Xiaolei Zhu",
        "Xuanliang Zhang",
        "Ge Zhang",
        "Yi Lin",
        "Guang Shi",
        "Chaoyou Fu",
        "Wenhao Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As reasoning models scale rapidly, the essential role of multimodality in human cognition has come into sharp relief, driving a growing need to probe vision-centric cognitive behaviors. Yet, existing multimodal benchmarks either overemphasize textual reasoning or fall short of systematically capturing vision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs insufficiently assessed. To address this limitation, we introduce MME-CC (Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded benchmark that organizes 11 representative reasoning tasks into three fundamental categories of visual information: spatial, geometric, and knowledge-based reasoning, and provides fine-grained analyses of MLLMs' cognitive capacity across these dimensions. Based on MME-CC, we conduct extensive experiments over 16 representative MLLMs. Our study reveals that closed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs. 30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak (less than or equal to 30%). We further identify common error patterns, including orientation mistakes, fragile cross-view identity persistence, and poor adherence to counterfactual instructions, and observe that Chain-of-Thought typically follows a three-stage process (extract -> reason -> verify) with heavy reliance on visual extraction. We hope this work catalyzes a shift toward treating the cognitive capacity of MLLMs as central to both evaluation and model design."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 6 4 1 3 0 . 1 1 5 2 : r MME-CC: Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity ByteDance Seed, Nanjing University Full author list in Contributions"
        },
        {
            "title": "Abstract",
            "content": "As reasoning models scale rapidly, the essential role of multimodality in human cognition has come into sharp relief, driving growing need to probe vision-centric cognitive behaviors. Yet, existing multimodal benchmarks either overemphasize textual reasoning or fall short of systematically capturing vision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs insufficiently assessed. To address this limitation, we introduce MME-CC (Multi-Modal Evaluation benchmark of Cognitive Capacity), vision-grounded benchmark that organizes 11 representative reasoning tasks into three fundamental categories of visual informationspatial, geometric, and knowledgebased reasoningand provides fine-grained analyses of MLLMs cognitive capacity across these dimensions. Based on MME-CC, we conduct extensive experiments over 16 representative MLLMs. Our study reveals that closed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs. 30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak (30%). We further identify common error patternsincluding orientation mistakes, fragile cross-view identity persistence, and poor adherence to counterfactual instructionsand observe that Chain-of-Thought typically follows three-stage process (extract reason verify) with heavy reliance on visual extraction. We hope this work catalyzes shift toward treating the cognitive capacity of MLLMs as central to both evaluation and model design. Github: https://randomtutu.github.io/MME-CC"
        },
        {
            "title": "Introduction",
            "content": "Vision is crucial means for humans to perceive the world. Naturally, multimodal large language models (MLLMs) have become key research direction for researchers in their pursuit of Artificial General Intelligence (AGI). Various analytical studies focused on multimodal understanding are thriving, aiming to fully uncover the potential flaws of MLLMs and guide the iteration of them. growing number of benchmarks have been introduced to evaluate multimodal language models (MLLMs) across diverse visual reasoning tasks, encompassing general image understanding [3, 10], multi-disciplinary multimodal reasoning [11, 24, 25], and open-domain scenarios [6, 9, 22]. Notably, state-of-the-art MLLMs often demonstrate strong performance on these benchmarks. While bunch of MLLM benchmarks claim to evaluate the cognitive capacity of MLLMs, they have various flaws. These flaws make them either lean too much toward textual capabilities or lack sufficient coverage of vision-based cognitive behaviors when assessing MLLMs cognitive capacity. MathVista [11] and MMMU Series [24, 25] are overly biased toward the text-space-based reasoning capabilities of MLLMs. On the other hand, other benchmarks (e.g. ZeroBench [21]) measure the cognitive capacity of MLLM by enumerating various vision-based reasoning tasks, but lack well-established classification system and in-depth analysis of 1 Figure 1 Task taxonomy of MME-CC. Three major task categories are definedSpatial Reasoning, Geometric Reasoning, and Visual Knowledge Reasoningeach with representative subtasks and one illustrative input example. the cognitive capacity of MLLM. To remedy the gap, we introduce MME-CC (Multi-Modal Evaluation Benchmark of Cognitive Capacity), benchmark of vision-based cognitive tasks for MLLMs. Specifically, MME-CC firstly introduces set of vision-based cognitive tasks that examine three essential dimensions of MLLMs reasoning: Spatial Information, Geometric Information, and Visual Knowledge Reasoning. In addition, these tasks are organized into 11 distinct representative visual reasoning problems, as shown in Figure 1, each attributed to one of the three dimensions. Moreover, built with the extensive efforts of 10-person annotation team, including dedicated subtask leads and task lead, MME-CC underwent multiple stages of human review and cross-checking to ensure validity. On this basis, using 1,173 questions carefully annotated by human experts, MME-CC conducts detailed analysis of the current cognitive capacity of MLLMs. Finally, MME-CC also reveals the strengths and weaknesses, Chain-of-Thought (CoT) patterns, and error patterns of 16 representative MLLMs in different dimensions of vision-based cognitive capacity. Our contributions are as follows: We provide MME-CC, high-quality, language-independent visual reasoning benchmark that fills the gap in MLLM cognitive capacity categorization and systematic analysis. We uncover several key insights into current MLLM cognitive capacity: Closed-source models consistently outperform open-source counterparts, with Gemini-2.5-Pro achieving the best performance (42.66) compared to the strongest open-source model, GLM-4.5V (30.45). Spatial and geometric reasoning remain broadly weak, with both categories scoring at or below 30%. Common error patterns include orientation and reference-frame mistakes, poor cross-view identity persistence, and limited adherence to counterfactual instructions. CoT reasoning typically follows three-stage layered processextraction, reasoning, and verificationwith visual extraction involved throughout."
        },
        {
            "title": "2 MME-CC",
            "content": "We construct MME-CC benchmark to systematically evaluate the visual cognitive and reasoning abilities of MLLMs. This benchmark is designed to facilitate comprehensive and rigorous assessment of model capabilities through series of meticulously crafted tasks. The core of this benchmark comprises three 2 Figure 2 Overview of the data construction and quality control pipeline. The pipeline comprises four stages: (1) Task definition and preliminary evaluation subtasks are defined with clear objectives, and small-scale pilots are conducted to validate prompt design and calibrate difficulty; (2) Data acquisition and manual verification images from license-compliant sources are annotated and cross-checked to ensure quality; (3) Post-processing standardized procedures (e.g., cropping, resolution checks, identifier assignment) are applied to unify formatting; (4) Model-based filtering items that are overly simple, redundant, or ambiguous are removed based on MLLM performance, and the remaining samples form the final benchmark. primary task categories: Spatial Reasoning, Geometric Reasoning, and Visual Knowledge Reasoning, which respectively serve to examine model performance within each corresponding dimension."
        },
        {
            "title": "2.1 Data Construction Pipeline",
            "content": "As illustrated in Figure 2, our human-in-the-loop pipeline produces high-quality evaluation data through iterative definition, acquisition, processing, and filtering. Annotators. We employed structured 10-person team (6 annotators, 3 subtask leads, 1 task lead), all with prior multimodal evaluation experience. All annotators were required to review detailed instruction manual and pass corresponding qualification test before commencing the annotation tasks (see Appendix for the full guidelines). Each subtask had dedicated lead responsible for design, pipeline coordination, and multi-stage quality review, ensuring consistent annotation and rigorous quality control across the data lifecycle. Table 1 Annotator roles and responsibilities for the evaluation set (N = 10). Participant Role Job Description Annotators A1A6 Question construction and multi-round quality checks Subtask Leads B1B3 Task Lead C1 Subtask design and end-to-end oversight; with over 3 years of experience in relevant fields (e.g., NLP, CV) and prior work on dataset creation. Overall benchmark owner; extensive evaluation experience; with documented history of leading the development of 3+ public benchmarks. Task Definition & Preliminary Evaluation Each subtask has clear objective and concise, task-specific prompt template. To validate the design, we built pilot set of 510 examples from carefully selected images for each subtask. Rather than ad-hoc sampling, construction followed structured design process specifying evaluation dimensions and steps. We piloted with two models (Doubao and Gemini) to test clarity 3 Table 2 Task taxonomy and dataset statistics of MME-CC. Source indicates the data origin, #Q is the number of samples, and I./O. represent the average input and output token lengths measured with Doubao-Seed-1.6-vision-0815. Task Description Spatial Reasoning Satellite Image Matching Indoor Directional Reasoning Indoor Deduplication Counting Geometric Reasoning Gomoku Variation Unblock Me Maze Jigsaw Puzzle Match street view images to their satellite map locations using visual clues. Determine object orientations using spatial reference points. Count unique furniture instances across multiple views. Total / Avg. Source #Q I. O. (3 tasks, 319 samples) 101 4,335 2,768 Google Maps Real Estate 10,168 5,090 Real Estate 105 10,091 4,370 319 4,076 (5 tasks, 605 samples) 8,198 Solve visual logic problems based on Gomoku with mixed game pieces. Find the shortest move sequence to unblock target block. Identify the correct number on the shortest path in maze. Complete missing puzzle pieces in partial layout. Photography 141 Game Screenshots Autogenerated Photography 122 194 1,411 4,700 3,483 11,741 336 9, 1,765 2,330 Chart Modification Modify chart values based on instructions. Web Images 49 2,497 Visual Knowledge Reasoning Total / Avg. 605 6,204 (3 tasks, 249 samples) 1, Sandbagging Counterfactual Instruction Finding Wrong Answer Total Follow adversarial instructions to answer partially correctly. Provide the reversed answer based on depicted facts. Detect incorrect answer choices from grouped Q&As. Web Images Web Images Internal Dataset Total / Avg. 41 1,753 672 60 1,379 148 5,122 2,691 249 2,751 1, 1,173 4,166 3,904 and feasibility, then iteratively refined the task scope, prompts, and data handling. Reliability was further ensured through explicit controls. Detailed construction procedures and reliability controls for each task are documented in Appendix A. Data Collection. After definitions stabilize, we collect and annotate images from diverse, license-compliant sources: (1) Real-world photographs, such as board games (e.g., Gomoku) and puzzle scenes. (2) Targeted screen captures from online platforms, including comics, real-estate listings (e.g., Lianjia), Google Street View, and video content (e.g., YouTube, Bilibili). (3) Game screenshots that cover representative in-game reasoning cases. (4) Additional types of images as required by specific subtasks. Image Post-processing & Model-based Filtering. All collected images undergo unified post-processing pipeline, including ID assignment, cropping, and related adjustments (see Appendix A). We then apply leading models (e.g., Gemini-2.5-Pro) to filter the data. This process removes items that are trivially easy (defined as achieving model accuracy score above 95%, semantically redundant, or lacking discriminative value. In total, this filtering stage removed approximately 50% of the initial data pool. The remaining pool serves as the foundation for constructing the final benchmark set used in downstream evaluation. 4 Table 3 Comparison of representative benchmarks. Vision-based indicates that all task information is derived from images rather than text, Output specifies the answer format (MC = multiple-choice, FF = free-form), and Source denotes the dataset origin. Benchmark MME [3] MMMU [24] MMBench [10] MMStar [2] Zerobench [21] MME-CC (Ours) Input Vision-based Output Source"
        },
        {
            "title": "1 Image\n≥1 Image\n1 Image\n1 Image\n≥1 Image",
            "content": "1 Image MC MC / FF MC MC FF"
        },
        {
            "title": "Existing\nDiverse\nDiverse\nExisting\nDiverse",
            "content": "FF"
        },
        {
            "title": "2.2 Data Quality Assurance",
            "content": "In addition to the quality controls embedded throughout the data collection process, the finalized dataset undergoes the following further checks: Manual Verification. Each sample is double-checked: one annotator provides the reference answer and another independently verifies it. In cases of disagreement, the sample was escalated to the respective subtask lead for final binding decision. Every item undergoes at least two rounds of review, and dedicated QA team conducts periodic audits to ensure accuracy and consistency. Exception Handling. Samples that yield zero accuracy across screened models are randomly reviewed to identify potential annotation issues or hidden shortcuts; problematic items are corrected or removed."
        },
        {
            "title": "2.3 Benchmark Statistics",
            "content": "Table 2 summarizes the taxonomy and statistics of MME-CC, which organizes 11 subtasks into three reasoning categories: Spatial, Geometric, and Visual Knowledge Reasoning. For each subtask, it reports the data source, sample size, and the average input and output token lengths, where the input length includes both text and image tokens extracted by Doubao-Seed-1.6-vision-0815. These statistics indicate the reasoning complexity, as longer sequences require deeper inference, and the considerable output length reflects the need for non-trivial reasoning chains. In addition, Table 3 compares MME-CC with representative benchmarks. Notably, MME-CC emphasizes vision-based reasoning, since the textual input does not contain any task-specific solution information, thereby ensuring that solving relies primarily on visual understanding and reasoning."
        },
        {
            "title": "3.1 Experimental Setup\nModel Configuration. We evaluate a range of large language and vision–language models, including both\nproprietary and open-source systems. For proprietary models, we use the official inference APIs with default\nsettings. For open-source models, we adopt a unified decoding configuration with temperature set to 1.0 and\ntop-p to 0.7, while all other hyperparameters follow their respective defaults.",
            "content": "Evaluation Metrics. We employ an LLM-as-a-judge protocol, wherein language model is prompted to compare the model-generated response against gold reference and assign correctness score. Specifically, we adopt DeepSeek-V3-0324 as the judge model. To verify its reliability, we conduct manual evaluation of 99 randomly sampled items (33 from each category), achieving scoring agreement rate of 95% with human judgments. The scoring prompt used for this evaluation is provided in the Appendix C. For each question, DeepSeek-V3-0324 compares the final answer with the reference and outputs score in {0, 1}. 15 students who did not participate in the question setting, with higher degrees 5 Table 4 Results on the MME-CC benchmark across three core dimensions: Spatial Reasoning (SR), Geometric Reasoning (GR), and Visual Knowledge Reasoning (VKR). Shaded entries indicate the best performance, bold the second-best, and underlined the third-best in each column. Model Human1 (n=99, sampled) Reasoning Overall 95.86 Closed-Source Models Gemini-2.5-Pro [5] GPT-5 (high) [13] Doubao-Seed-1.6-vision-0815 (Think) [1] Gemini-2.5-Flash [4] o4-mini (high) [16] GPT-4.1 [15] GPT-4o-1120 [14] Doubao-Seed-1.6-vision-0815 (Nonthink) [1] GLM-4.5V [27] Qwen2.5-VL-72B-Instruct [19] MiMo-VL-7B-RL [12] GLM-4.1V-9B-Thinking [26] Qwen2.5-VL-32B-Instruct [18] InternVL3-8B [17] Keye-VL-8B-Preview [7] Qwen2.5-VL-7B-Instruct [20]"
        },
        {
            "title": "3.2 Main Results",
            "content": "Open-Source Models 42. 40.25 40.08 37.57 35.00 32.14 26.88 25. 30.45 23.59 20.90 19.30 14.39 11. 9.65 7.50 SR 95.83 23.80 30. 22.03 18.60 25.00 27.90 22.60 23.63 13. 12.47 9.30 8.73 9.03 7.30 7. 4.70 GR 95.83 29.56 23.64 31.50 21. 21.96 12.22 10.12 23.82 13.34 8. 11.10 9.22 8.56 2.38 2.04 3. VKR 95.92 74.63 66.47 66.70 72.90 58. 56.30 47.93 30.43 64.73 49.33 42. 39.93 25.57 24.40 19.20 14.57 We conduct comprehensive evaluation of 16 MLLMs on MME-CC, covering three core reasoning dimensions: Spatial Reasoning, Geometric Reasoning, and Visual Knowledge Reasoning. The results are shown in Table 4. MLLMs remain limited in visually grounded reasoning tasks. The state-of-the-art model Gemini-2.5-Pro achieves an overall accuracy of only 42.66% on MME-CC, with score of 74.63% in VKR tasks, while its performance in Geometric Reasoning and Spatial Reasoning remains below 30%. These results suggest that current models lack the ability to conduct comprehensive and fine-grained reasoning under purely visual inputs. Their better performance in basic perceptual tasks, such as entity recognition or object detection (e.g., VKR), mainly results from the fact that these tasks are well covered during training, whereas the complex tasks we design require more advanced spatial and geometric reasoning and therefore expose clear limitations. Reasoning-oriented models exhibit advantages over non-reasoning models. In Spatial and Geometric Reasoning tasks, reasoning-oriented models consistently outperform their non-reasoning counterparts. Notably, within the GPT series, GPT-5 (high) achieves superior performance compared to GPT-4.1 across SR and GR tasks. This trend indicates that longer Chain-of-Thought reasoning chains provide additional opportunities for iterative verification of recognition outcomes and intermediate inferences, thereby contributing to improved problem-solving in complex scenarios. more detailed analysis of this phenomenon is presented in the Discussion section. Scaling laws remain valid for visual reasoning tasks. Within the Qwen2.5 family, performance improves consistently as the parameter scale increases from 7B to 32B and 72B. This observation indicates that complex 6 Figure 3 Detailed CoT analysis of Doubao-Seed-1.6-vision-0815 on the Satellite Image Matching task. The analysis reveals three key findings: (1) hierarchical reasoning with distinct phases, (2) continuous and task-dependent visual extraction, and (3) frequent self-interruptions that reduce reasoning efficiency. visual perception and reasoning tasks require broader knowledge capacity to support effective inference, while smaller-scale models face inherent limitations in achievable performance."
        },
        {
            "title": "4 Discussion",
            "content": "Based on the results presented in Table 4, this paper discusses the following research questions: RQ1: What are the differences in model performance? RQ2: How does the model reason in visual tasks? RQ3: What error patterns do models exhibit in visual reasoning?"
        },
        {
            "title": "4.1 RQ1: Models excel at different tasks, yet overall performance remains unsatisfac-",
            "content": "tory Notably, GPT-5 (high) achieves the highest performance in Spatial Reasoning with score of 30.3%, result that appears attributable to its capabilities in sub-tasks requiring complex spatial orientation and object counting (e.g., Indoor Directional Reasoning and Indoor Deduplication Counting). Gemini-2.5-Pro, in contrast, demonstrates clear advantage in Visual Knowledge Reasoning by attaining leading score of 70.7%. In the domain of Geometric Reasoning, the Doubao models performance advantage appears localized to the Jigsaw Puzzle task, whereas Gemini-2.5-Pro shows more robust results across other geometric reasoning challenges  (Table 4)  ."
        },
        {
            "title": "4.2 RQ2: MLLMs remain far from “thinking like humans”\nWe analyze the chain-of-thought (CoT) behavior of Doubao-Seed-1.6-vision-0815 on MME-CC and\nsummarize the following observations.",
            "content": "Layered, stage-wise reasoning The reasoning chain follows three stages: Stage 1: Problem Understanding & Information Collection, where the model reads the prompt, scans the image for key objects and relations, and 7 Table 5 Performance on MME-CC subtasks. Each cell reports the ablation score; in parentheses we list the main-experiment score and the signed difference () from the base, i.e., ablation = main . For ablations, we include only subtasks without instruction conflicts with the experimental variable; conflicted subtasks (e.g., Chart Modification, Visual Knowledge Reasoning) are omitted. Task Gemini-2.5-Pro Doubao-Seed-1.6-visiono4-mini-high Spatial Reasoning Satellite Image Matching Indoor Directional Reasoning Indoor Deduplication Counting Average (Spatial) Geometric Reasoning Maze Gomoku Variation Jigsaw Puzzle Unblock Me Average (Geometric) 30.5 (28.3 + 2.2) 15.4 (14.3 + 1.1) 29.1 (28.8 + 0.3) 25.0 (23.8 + 1.2) 1.9 (1.1 + 0.9) 31.0 (34.8 3.8) 30.8 (30.4 + 0.4) 27.7 (26.8 + 0.9) 22.9 (23.3 0.4) 41.0 (39.6 + 1.4) 4.8 (5.7 0.9) 19.0 (20.8 1.8) 21.6 (22.0 0.4) 0.6 (0.8 0.2) 16.7 (14.9 + 1.8) 72.1 (70.6 + 1.5) 29.6 (28.8 + 0.8) 29.8 (28.8 + 1.0) 31.9 (30.3 + 1.6) 11.2 (11.2 + 0.0) 33.2 (33.5 0.3) 25.4 (25.0 + 0.4) 1.5 (1.2 + 0.3) 12.5 (12.0 + 0.5) 26.7 (27.0 0.3) 21.6 (21.4 + 0.2) 15.6 (15.4 + 0.2) Overall Average 23.9 (23.5 +0.4) 25.7 (25.4 +0.3) 20.5 (20.2 +0.3) restates the goal; Stage 2: Core Analysis & Reasoning, where it proposes options, checks them against visual evidence and rules, and updates assumptions; and Stage 3: Conclusion Formation & Verification, where it assembles the confirmed evidence, gives an answer with brief rationale, and performs final consistency check. Although task-specific tactics vary, the overall structure remains stable, as shown in Figure 3. Image revisiting throughout the process Visual information extraction is not confined to the beginning but occurs as needed throughout the reasoning process. In Satellite Image Matching, for example, the model repeatedly inspects the original images to re-check building orientation and relative layout, thereby revising earlier spatial judgments. Excessive verification reduces efficiency The model frequently employs wait style pauses for reflection and re-checks. Moderate pausing can reduce errors; excessive pausing, however, leads to stalling and repetitive verification, which is particularly evident in complex spatial relations. On MME-CC spatial and geometric tasks, most models achieve only 20%-30%; in Maze, which requires continued rule-based simulation and path planning, no model exceeds 2%. We hypothesize that long reasoning chains dilute attention, obscure crucial visual details, and ultimately degrade outcomes. Textual guidance yields consistent gains To address this, we add the instruction You should first describe the relevant content in the image according to the prompt, and then answer the question. As shown in Table 5, most tasks exhibit consistent improvements. The gains indicate that an initial textual description stabilizes subsequent reasoning by anchoring visual perception; in addition, the improvements mainly arise from better textual alignment rather than stronger intrinsic visual reasoning."
        },
        {
            "title": "4.3 RQ3: MLLMs exhibit recurring failures in orientation judgment, entity consistency,",
            "content": "and instruction following We analyze failure cases in MME-CC and observe several recurring errors that appear across tasks and reasoning dimensions. Orientation judgment and reference-frame alignment. The model often fails to preserve object orientation across views, and viewpoint changes induce mismatches that hinder the establishment of consistent global 8 (a) In the Satellite Image Matching task, the model fails to capture critical visual cues (e.g., steps and entrance), focusing instead on irrelevant details. (b) In the Indoor Deduplication Counting task, the model fails to deduplicate entities, leading to redundant counting. Figure 4 Representative error cases of Doubao-Seed-1.6-vision-0815. reference frame, thus producing orientation errors during reasoning; the issue is notably salient in tasks that require reasoning over indoor layouts or aligning orientations across multiple views, as shown in Figure 4a. Entity identity consistency under multi-view settings. When reasoning over multiple views, the model frequently fails to maintain identity consistency for the same entity in the scene, which leads to double counting or omission, as illustrated in Figure 4b. Over-reliance on literal descriptions under instruction constraints. Faced with non-literal or counterfactual instructions, the model tends to prioritize the literal visual content while ignoring task-specific counterfactual constraints expressed in text, thereby producing answers that conflict with the required instruction, as shown in Figure 14. Further analyses and complete error cases are provided in Appendix E."
        },
        {
            "title": "5 Related Work",
            "content": "General multimodal benchmarks. broad line of benchmarks evaluates VLMs on perception and languagemediated reasoning. MMBench [10], SEED-Bench [8], and MMStar [2] provide large-scale multiple-choice evaluation with fine-grained skill categorization. MMMU [24], MathVista [11], and MMMU-Pro [25] target multi-disciplinary reasoning across STEM and humanities. Open-domain settings such as RealWorldQA [22], OlympiadBench [6], and VisualWebBench [9] broaden task diversity. While these resources are valuable for overall capability profiling, many items permit solutions that rely on textual cues, format priors, or OCR, which makes it difficult to isolate visual reasoning. Language-independent visual reasoning. Recent analyses report shortcut use in multimodal evaluations, where models exploit answer-bearing text instead of reasoning over images (e.g., NaturalBench, EasyARC, VLSBench). To better probe visual inference, several works explore spatial relations and visual puzzles. ZeroBench [21] stresses spatial and commonsense limits with carefully designed queries, and VisuLogic [23] 9 offers human-verified problems spanning spatial relations, geometric abstraction, and visual planning. However, many existing tasks are constrained by synthetic data, repetitive templates, or narrow formats, which limits novelty and reduces the headroom for assessing language-independent reasoning."
        },
        {
            "title": "6 Conclusion",
            "content": "We present MME-CC as vision-grounded benchmark that organizes eleven representative tasks into spatial, geometric, and visual-knowledge dimensions, and we provide fine-grained analyses of multimodal models cognitive capacity across these dimensions. We evaluate sixteen representative models and observe that closed-source systems currently lead overall (42.66 for Gemini-2.5-Pro vs. 30.45 for GLM-4.5V), while spatial and geometric reasoning remain comparatively weak (both 30%). We further identify recurring error patternsorientation/reference-frame confusion, limited cross-view identity persistence, and reduced adherence to counterfactual instructionsand we find that Chain-of-Thought typically follows three-stage pattern (extract reason verify) with visual extraction throughout; in addition, prompting that first verbalizes key visual content yields consistent gains, indicating reliance on explicit textual grounding. MMECC reduces textual shortcuts and surfaces vision-centric behaviors, thereby enabling taskand dimension-level diagnostics that are actionable for evaluation and model design; we expect these analyses to inform training signals and architectures that better couple visual perception with structured reasoning and to support systematic progress on cognitively grounded visual reasoning."
        },
        {
            "title": "7 Contributions",
            "content": "Core Contributors (α-β order) Kaiyuan Zhang, Chenghao Yang, Zhoufutu Wen, Sihang Yuan, Qiuyue Wang, Chaoyi Huang ({liniuniu, yuansihang.24, chaoyihuang}@bytedance.com, zhangkaiyuan.103@jiyunhudong.com) Contributors (α-β order) Guosheng Zhu, He Wang, Huawenyu Lu, Jianing Wen, Jianpeng Jiao, Lishu Luo, Longxiang Liu, Sijin Wu, Xiaolei Zhu, Xuanliang Zhang Advisors Chaoyou Fu (Nanjing University) Wenhao Huang (huang.wenhao@bytedance.com) Ge Zhang, Yi Lin, Guang Shi denotes corresponding authors. Contributors without explicit affiliations are from ByteDance Seed. During the work, Chenghao Yang is an intern at ByteDance Seed."
        },
        {
            "title": "References",
            "content": "[1] ByteDance. Seed1.6 tech introduction. https://seed.bytedance.com/en/seed1_6, 2025. Accessed: September 2025. [2] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large vision-language models? In NeurIPS, 2024. [3] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. MME: comprehensive evaluation benchmark for multimodal large language models. CoRR, abs/2306.13394, 2023. [4] Google. Gemini-2.5-Flash: large language model. https://cloud.google.com/vertex-ai/generative-ai/ docs/models/gemini/2-5-flash, 2025. Accessed: September 2025. [5] Google. Gemini-2.5-Pro(preview 05-06): large language model. https://cloud.google.com/vertex-ai/ generative-ai/docs/models/gemini/2-5-pro, 2025. Accessed: September 2025. [6] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In ACL (1), pages 38283850. Association for Computational Linguistics, 2024. [7] Kwai-Keye. Keyevl-8b: Vision-language model. https://huggingface.co/Kwai-Keye/Keye-VL-8B-Preview, 2025. Accessed: September 2025. [8] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In CVPR, pages 1329913308. IEEE, 2024. [9] Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding? CoRR, abs/2404.05955, 2024. [10] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? In ECCV (6), volume 15064 of Lecture Notes in Computer Science, pages 216233. Springer, 2024. [11] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR. OpenReview.net, 2024. [12] MiMo Team. Mimo-vl-7b: vision-language model. https://arxiv.org/abs/2506.03569v1, 2025. Accessed: September 2025. [13] OpenAI. Introducing gpt-5. https://openai.com/index/introducing-gpt-5/, 2024. [14] OpenAI. Hello gpt4-o. https://openai.com/index/hello-gpt-4o/, 2024. URL https://openai.com/index/ hello-gpt-4o/. [15] OpenAI. Introducing gpt-4.1 in the api. https://openai.com/index/gpt-4-1/, 2025. Accessed: September 2025. [16] OpenAI. Introducing openai o3 and o4-mini. https://openai.com/index/introducing-o3-and-o4-mini/, 2025. Accessed: September 2025. [17] OpenGVLab. Internvl-3: Vision-language model. https://huggingface.co/OpenGVLab/InternVL3-8B, 2025. Accessed: September 2025. [18] Qwen Team. Qwen2.5-vl-32b: 5-VL-32B-Instruct, 2025. Accessed: September 2025. vision-language model. [19] Qwen Team. Qwen2.5-vl-72b: 5-VL-72B-Instruct, 2025. Accessed: September 2025. vision-language model. [20] Qwen Team. Qwen2.5-vl-7b: 5-VL-7B-Instruct, 2025. Accessed: September 2025. vision-language model. https://huggingface.co/Qwen/Qwen2. https://huggingface.co/Qwen/Qwen2. https://huggingface.co/Qwen/Qwen2. [21] Jonathan Roberts, Mohammad Reza Taesiri, Ansh Sharma, Akash Gupta, Samuel Roberts, Ioana Croitoru, Simion-Vlad Bogolin, Jialu Tang, Florian Langer, Vyas Raina, Vatsal Raina, Hanyi Xiong, Vishaal Udandarao, Jingyi Lu, Shiyang Chen, Sam Purkis, Tianshuo Yan, Wenye Lin, Gyungin Shin, Qiaochu Yang, Anh Totti Nguyen, Kai Han, and Samuel Albanie. Zerobench: An impossible visual benchmark for contemporary large multimodal models. CoRR, abs/2502.09696, 2025. [22] xAI. RealWorldQA. https://huggingface.co/datasets/xai-org/RealworldQA, 2024. Accessed: 2025-07-27. [23] Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, Wenhai Wang, Jifeng Dai, and Jinguo Zhu. Visulogic: benchmark for evaluating visual reasoning in multi-modal large language models. CoRR, abs/2504.15279, 2025. [24] Xiang Yue, Yuansheng Ni, Tianyu Zheng, Kai Zhang, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. In CVPR, pages 95569567. IEEE, 2024. [25] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. In ACL (1), pages 1513415186. Association for Computational Linguistics, 2025. [26] Zai-org. Glm-4.1v: Reasoning-centric vision-language model. https://arxiv.org/abs/2507.01006v1, 2025. Accessed: September 2025. [27] Zai-org. GLM-4.5V: large language model. https://huggingface.co/zai-org/GLM-4.5V, 2025. Accessed: September 2025."
        },
        {
            "title": "A Detailed Benchmark Task Construction",
            "content": "This section provides detailed breakdown of the construction methods and reliability controls for each task within the MME-CC benchmark. A.1 Spatial Reasoning A.1.1 Satellite Image Matching Construction Method (i) Select map tiles with salient landmark. (ii) Pair them with two Google Street View (GSV) images from the same area (camera poses are recorded). (iii) Mark seven mutually confusable candidate locations (AG) on the map and insert the two ground truth locations among them. (iv) Each sample consists of one map, two query GSV images, and seven options. The model must output one letter per query. (i) Candidate locations must be topologically valid and visually confusable to prevent Reliability Controls easy elimination. (ii) We double-check for landmark visibility and viewpoint consistency between GSV and the map. (iii) All images are standardized via cropping/resizing, and overlays (routes, compass, text, metadata) are removed. (iv) The task design ensures low chance of random success; for two different correct answers, blind guessing accuracy is 1/(7 6) 2.4%. A.1.2 Indoor Directional Reasoning Construction Method (i) Utilize Lianjia VR tours which provide floorplan location and camera facing direction. (ii) Capture short sequences of adjacent views within tour. (iii) For each sample, present an anchor view (with given orientation) and query view from the same sequence. The model must identify the orientation of the query view from fixed set (e.g., N/E/S/W). (i) Remove compasses, icons, and text from images; instruct models to ignore lighting Reliability Controls cues. (ii) Verify smooth viewpoint continuity (no \"teleports\") and cross-check orientations against the floorplan data. (iii) Maintain near-uniform class balance across different room types and directions. A.1.3 Indoor Deduplication Counting Construction Method (i) From single apartments VR tour, extract coherent set of views. (ii) Specify two target object categories with brief, disambiguating definitions. (iii) The model must return counts of unique instances for each category, correctly deduplicated across all provided views. (i) Provide clear inclusion/exclusion rules regarding occlusion or stacking of objects. (ii) Reliability Controls Normalize crops and strip all overlays and metadata. (iii) Use definitions and illustrative examples to reduce ambiguity. (iv) Requiring counts for two categories per sample reduces the chance of lucky guess. (v) manual review process enforces identity consistency of objects across multiple views. A.2 Geometric reasoning A.2.1 Gomoku Variation Construction Method (i) Create hybrid game boards that preserve the five-in-a-row objective of Gomoku but use piece shapes from other games (e.g., Chinese Chess). (ii) Curate endgame positions that have exactly one winning move. (iii) Filter out positions with multiple optimal solutions. (iv) Standardize coordinates and formatting; filter out duplicate or symmetric board states. 13 (i) Manually and programmatically verify the uniqueness of the forced win. (ii) Exclude Reliability Controls duplicate or symmetric layouts. (iii) Standardized formatting and coordinate systems suppress shortcut cues. A.2.2 Unblock Me Construction Method (i) Mask nonessential UI elements from screenshots of the game. (ii) Ask for both the minimum number of moves and the ordered sequence of moves (using standardized block IDs). (iii) Keep only levels where unique minimal solution is verified by two independent human annotators and the in-game solver. (iv) Adjudicate any disagreements and discard ambiguous cases. (i) Hide UI hints like move counters or suggestions. (ii) Confirm the uniqueness of Reliability Controls minimal solutions via both human annotators and an automated solver. (iii) Adjudicate any conflicts and remove levels that do not have single, unique minimal solution. A.2.3 Maze Construction Method (i) Generate fixed-size mazes and overlay digits at selected empty cells. (ii) The query asks the model to list all digits on the shortest path from entrance to exit, reported in ascending order. (iii) Ensure unique shortest path exists through dual human tracing and algorithmic checks. (iv) Remove any revealing artifacts. (i) Guarantee unique shortest path for every maze. (ii) Employ dual human verification, Reliability Controls supplemented with algorithmic checks where available. (iii) Remove start/end arrows, solution traces, and other visual artifacts that could give away the answer. A.2.4 Jigsaw Puzzle Construction Method (i) Physically assemble jigsaw puzzles and then remove 36 pieces. (ii) Photograph the board (with labeled empty slots) and the set of candidate pieces. (iii) The model is asked to provide one-to-one mapping of each piece to its correct slot. (iv) Exclude symmetric or visually ambiguous pieces/slots. (v) Normalize lighting and crop images. (i) physical test-fit confirms all piece-to-slot mappings are correct. (ii) Ambiguous Reliability Controls or symmetric items are removed during curation. (iii) Image normalization suppresses non-content cues like shadows or lighting gradients. A.3 Visual Knowledge Reasoning A.3.1 Sandbagging Construction Method (i) Pair one image with four ordered sub-questions and provide the instruction: answer exactly one correctly and three incorrectly. (ii) Randomize the index of the single correct answer. (iii) The ground truth includes canonical answers for all questions and the required correctness pattern (e.g., [Incorrect, Correct, Incorrect, Incorrect]). (iv) Evaluation programmatically enforces the 1-right/3-wrong constraint. (i) The position of the correct answer is randomized. (ii) Automatic checks enforce the Reliability Controls 1-right/3-wrong output pattern. (iii) Prompts explicitly forbid staged \"first will answer correctly, then will answer incorrectly\" outputs, requiring direct final answer. A.3.2 Counterfactual Instruction Construction Method (i) Choose images with unambiguous facts (e.g., object presence, color, count, left-right position). (ii) Specify an explicit inversion mapping (e.g., presence absence, left right, higher lower, color color B). (iii) Keep only items where the counterfactual state is well-defined and checkable. 14 (i) Use pre-specified, deterministic inversion maps. (ii) Filter out ambiguous cases where Reliability Controls the \"opposite\" is not clear. (iii) Retain only items with clear, verifiable answers in the counterfactual world. A.3.3 Chart Modification Construction Method (i) Collect chart images and manually annotate the underlying data table. (ii) Apply deterministic edits to the data (e.g., swap categories, replace month, add/subtract constant, scale values, convert counts to percentages). (iii) Compute target values directly from the annotated data and cross-check programmatically. (i) Target answers are generated by deterministic operations on ground-truth data. (ii) Reliability Controls Programmatic validation ensures correctness. (iii) Clean charts to remove any UI hints or interactive elements. A.3.4 Finding the Wrong Answer Construction Method (i) Present one image with four question-and-answer pairs. (ii) Exactly one of the answers is deliberately flawed (e.g., an attribute, count, or relation is swapped). (iii) The other three answers are independently solvable and correct. (iv) Balance error categories and avoid underspecified questions. (v) Create wrong answers via minimal, precise edits to correct answer. (i) Balance the types of errors across the dataset. (ii) Verify that the three \"correct\" Reliability Controls answers are indeed independently verifiable. (iii) Generate the single \"wrong\" answer via minimal, controlled perturbation. (iv) Remove any items with underspecified or ambiguous questions."
        },
        {
            "title": "B Data Quality Assurance Protocol",
            "content": "Our quality control (QC) process is guided by core philosophy: ensuring every item is correct, unambiguous, and possesses sufficient difficulty to differentiate model capabilities. To achieve this, we implement two-tiered, adaptive validation strategy that leverages the distinct strengths of our annotation team. For the majority of sub-tasks, we follow scalable, two-stage protocol. First, our sub-task leadsthe most senior and experienced members of our teamconduct pilot review on random sample (e.g., 15 items). From this review, they distill common pitfalls and complex edge cases into detailed set of QC guidelines. These codified rules then empower our primary annotation pool to perform comprehensive, full-scale validation of the remaining data. However, for subset of tasks that are particularly cognitively demanding and require nuanced judgment, such as Indoor Directional Reasoning and Unblock Me, we adopt more stringent, expert-only protocol. The complexity of these tasks makes their quality difficult to guarantee via simple rule-based checking. Therefore, 100% of the validation for these specific sub-tasks is conducted directly by our senior sub-task leads, ensuring the highest possible standard of quality and consistency where it matters most. This hybrid strategy allows us to maintain rigorous quality across the entire benchmark in scalable yet meticulous manner. B.1 Task-Specific QC Guidelines and Examples Our principle of adaptive validation means that each sub-task has its own unique set of quality criteria. The following examples illustrate how our general principles are translated into concrete, task-specific rules. Satellite Image Matching Goal: Determine the locations of two Google Street View images on satellite map based on landmark. QC Rules: 1. Prevent Information Leakage: Ensure that screenshots of Street View or satellite maps are free of auxiliary UI elements (e.g., map pins, business labels, watermarks) that could directly reveal the location. 15 2. Ensure Landmark Uniqueness: Landmarks must possess distinct, asymmetrical features. Symmetrical buildings or uniform landscapes that appear similar from multiple angles are rejected, as they introduce ambiguity in determining precise orientation and location."
        },
        {
            "title": "Indoor Deduplication Counting",
            "content": "Goal: Count the number of unique furniture pieces, where each item may appear in multiple photos. QC Rules: 1. Guarantee Non-Trivial Difficulty: Problems must involve sufficient number of images and overlapping items to pose real deduplication challenge. Trivial cases (e.g., counting two items from two photos) are discarded. 2. Resolve Categorical Ambiguity: The annotation guidelines must pre-emptively resolve potential ambiguities. For example, rules explicitly define whether an empty flowerpot is counted as \"pottery,\" or if \"lounge chair\" is distinct category from \"dining chair.\""
        },
        {
            "title": "Gomoku Variation",
            "content": "Goal: new twist on Gomoku (Five-in-a-Row) played using game pieces from other rulesets, such as Chinese Chess or Checkers. QC Rules: 1. Manage Solution Ambiguity: For strategic games like Gomoku, multiple moves can be equally optimal (e.g., critical defensive block vs. strong offensive setup). In such cases, the ground truth is expanded to accept all valid optimal solutions. Indoor Directional Reasoning Goal: Infer the orientation of objects (e.g., windows, screens) based on spatial continuity and given reference orientation. QC Rules: 1. Expert-Only Validation: This task falls under our stringent protocol, with 100% of items validated by senior leads due to the subtlety of the required spatial reasoning. 2. Eliminate Descriptive Ambiguity: Vague natural language descriptions (e.g., \"the direction the bed head is facing\") are disallowed. All directional references must be precise, using either cardinal directions, relative positioning (e.g., \"parallel to the north wall\"), or clearly defined coordinate systems. Unblock Me Goal: Move the red block to the exit using the minimum number of steps. QC Rules: 1. Dual-Validation for Ground Truth: The correctness of the optimal step count is enforced through two-fold process: (a) programmatic validation against the canonical optimal solution data for each puzzle, and (b) final manual review by senior lead to ensure the visual representation is clear, unambiguous, and matches the puzzle state. This task is also part of our expert-only validation protocol. LLM-as-a-Judge Prompts We use Deepseek-v3-0324 based judge for evaluating open-ended answers. The following prompts are used:"
        },
        {
            "title": "General Scoring Prompt",
            "content": "You are grading teacher tasked with reviewing and scoring student answers based on the reference answer. During the grading process, you must adhere to the following important points: The scoring is based solely on the correctness of the students final answer compared to the reference answer. There is no need to assess whether the intermediate steps in the solution are correct. 16 First, extract the final answer provided by the student and display it in your analysis result. Then, judge the correctness of the extracted answer based on the reference answer. Assign score based on your analysis. When explaining the scoring analysis, the explanation should be broken down logically into sections. At the end of your explanation, summarize the analysis and format it as: \"In conclusion, the students answer should receive points\" (where indicates the specific score awarded). Keep your explanation concise, limited to 200 words. Provide the final score in \"JSON\" format using code block. Your output format should be: [Scoring analysis]: [Score]: points [JSON]: json { \"answer_score\": [[score]] } Scoring Criteria: The final answer is assessed according to the reference answer key and assigned one of two levels: 1 Point: Maximum score. The students final answer matches the reference answer exactly. For questions with multiple subparts, all subparts must be correct to receive 1 point. If the students answer is mathematically equivalent to the reference answer (e.g., student writes 1 + 1 2 while reference is 1 + 0.5x), this is acceptable. 0 Points: Minimum score. The students final answer does not match the reference answer. The students answer is empty. <Question>: {prompt} <Reference Answer>: {response_reference} <Students Answer>: {response}"
        },
        {
            "title": "Unblock Me Grading Prompt",
            "content": "Please determine if the students answer is correct based on the reference answer, and score it according to the following criteria: 1 point: The students answer matches the reference answer in terms of: the minimum number of steps, AND the set of blocks that need to be moved (excluding the red block), where the block set must be exactly the same (order does not matter). 0 points: The students answer is incorrect in terms of: the minimum number of steps, OR the set of blocks to be moved (excluding the red block), OR the student misses one of the criteria above, OR the students answer is empty. Please first determine if the students answer is correct in terms of the minimum number of steps, then determine whether the answer contains the correct set of blocks that need to be moved. Finally, present the score in the following JSON format: json { \"answer_score\": [[score]] } Example 1 (Correct): <Reference Answer>: Blocks that need to be moved: b, c, d; Minimum number of steps: 4 <Students Answer>: The minimum number of steps is 4. You need to move blocks d, c, and b. <Your output>: Is the students answer correct in terms of the minimum number of steps: Correct. For the blocks that need to be moved, the students answer (excluding the red block) is d, c, b, and the reference answer is b, c, they are consistent. Score: json { \"answer_score\": [[1]] } Example 2 (Incorrect): <Reference Answer>: Blocks that need to be moved: a, b, c, e; Minimum number of steps: 6 <Students Answer>: The minimum number of steps required is 6. The blocks to move are: c, a, d, e, b. <Your output>: Is the students answer correct in terms of the minimum number of steps: Correct. For the blocks that need to be moved, the students answer (excluding the red block) is c, a, d, e, b, and the reference answer is a, b, c, they are inconsistent. Score: json { \"answer_score\": [[0]] } <Question>: {prompt} <Reference Answer>: {response_reference} <Students Answer>: {response}"
        },
        {
            "title": "D Details of Error Cases",
            "content": "This section provides detailed illustrations of representative error cases across the subtasks of MME-CC. Figures 514 highlight typical failure patterns observed in Doubao-1.6-Pro-Vision, covering spatial reasoning, geometric reasoning, and instruction-dependent reasoning tasks. The examples reveal recurring issues such as insufficient cross-view geometric grounding (Satellite Image Matching, Indoor Directional Reasoning, Indoor Deduplication Counting), inadequate spatial planning and constraint simulation (Maze, Gomoku Variation, Jigsaw Puzzle, Unblock Me), and over-reliance on literal visual descriptions when specific instruction following is required (Sandbagging, Counterfactual Instruction). These cases complement the error pattern analysis in Section 4.3, providing concrete evidence of how current VLLMs fail to integrate visual features, maintain 18 global consistency, and adapt reasoning strategies under different task conditions."
        },
        {
            "title": "E Detailed Error Case Analyses by Error Pattern",
            "content": "This appendix provides extended analyses of representative failure cases for the recurring error patterns identified in Section 4.3."
        },
        {
            "title": "Incorrect Orientation and Reference Frame Alignment",
            "content": "E.1 Satellite Image Matching: The model fails to match ground-level and satellite views by overlooking geometric cues, relying instead on superficial textures (Figure 5). Indoor Directional Reasoning: The model does not propagate directional constraints across rooms, defaulting to linguistic heuristics and producing incorrect orientation judgments (Figure 6). E.2 Lack of Cross-View Object Identity Persistence Indoor Deduplication Counting: The model fails to maintain object identity across multiple views, producing redundant counts or omissions (Figure 7). E.3 Insufficient Spatial Planning and Constraint Simulation Maze: The model fails to construct globally optimal paths, relying instead on local adjacency (Figure 8). Gomoku Variation: The model misidentifies piece positions and ignores opponent strategies, hallucinating alignments (Figures 9, 10). Jigsaw Puzzle: The model chooses pieces by color similarity, ignoring spatial alignment (Figure 11). Unblock Me: The model applies rigid interpretations of constraints and fails to simulate feasible rearrangements (Figure 12). E.4 Over-Reliance on Literal Descriptions in Instruction-Conditioned Reasoning Sandbagging: The model misidentifies visual logos and fails to follow constrained output rules (Figure 13). Counterfactual Instruction: The model defaults to literal visual outputs instead of counterfactual answers (Figure 14). Chart Modification: The model outputs complete tables instead of adhering to logical constraints in instructions."
        },
        {
            "title": "F Ablation experiment setup and analysis",
            "content": "Table 5 presents the results of representative models on the eleven subtasks of MME-CC. Each entry is formatted as ablation (base delta). The ablation score corresponds to the setting where the original instruction is augmented with an additional clause: You should first describe the relevant content in the image according to the prompt, and then answer the question. The value inside the parentheses indicates the accuracy in the base setting, while delta shows the difference between the ablation and the base. 19 Figure 5 Error Case in Satellite Image Matching 20 Figure 6 Error Case in Indoor Directional Reasoning Figure 7 Error Case in Indoor Deduplication Counting 22 Figure 8 Error case in Maze 23 Figure 9 Error case 1 in Gomoku Variation Figure 10 Error case 2 in Gomoku Variation 25 Figure 11 Error case in Jigsaw Puzzle 26 Figure 12 Error case in Unblock Me Figure 13 Error case in Sandbagging 28 Figure 14 Error case in Counterfactual Instruction"
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "Nanjing University"
    ]
}