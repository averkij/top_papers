{
    "paper_title": "BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions",
    "authors": [
        "Nan Huo",
        "Xiaohan Xu",
        "Jinyang Li",
        "Per Jacobsson",
        "Shipei Lin",
        "Bowen Qin",
        "Binyuan Hui",
        "Xiaolong Li",
        "Ge Qu",
        "Shuzheng Si",
        "Linheng Han",
        "Edward Alexander",
        "Xintong Zhu",
        "Rui Qin",
        "Ruihan Yu",
        "Yiyao Jin",
        "Feige Zhou",
        "Weihao Zhong",
        "Yun Chen",
        "Hongyu Liu",
        "Chenhao Ma",
        "Fatma Ozcan",
        "Yannis Papakonstantinou",
        "Reynold Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated remarkable performance on single-turn text-to-SQL tasks, but real-world database applications predominantly require multi-turn interactions to handle ambiguous queries, execution errors, and evolving user requirements. Existing multi-turn benchmarks fall short by treating conversation histories as static context or limiting evaluation to read-only operations, failing to reflect production-grade database assistant challenges. We introduce BIRD-INTERACT, a benchmark that restores this realism through: (1) a comprehensive interaction environment coupling each database with a hierarchical knowledge base, metadata files, and a function-driven user simulator, enabling models to solicit clarifications, retrieve knowledge, and recover from errors without human supervision; (2) two evaluation settings consisting of a pre-defined conversational protocol (c-Interact) and an open-ended agentic setting (a-Interact) where models autonomously decide when to query the user simulator or explore the environment; (3) a challenging task suite covering the full CRUD spectrum for business-intelligence and operational use cases, guarded by executable test cases. Each task features ambiguous and follow-up sub-tasks requiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600 tasks, up to 11,796 interactions) for comprehensive performance assessment, and BIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed behavioral analysis and rapid method development. Our empirical results highlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in c-Interact and 17.00% in a-Interact. Analysis via memory grafting and Interaction Test-time Scaling validates the importance of effective interaction for complex, dynamic text-to-SQL tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 8 1 3 5 0 . 0 1 5 2 : r BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions BIRD-INTERACT: RE-IMAGINING TEXT-TO-SQL EVALUATION FOR LARGE LANGUAGE MODELS VIA LENS OF DYNAMIC INTERACTIONS Xiaohan Xuα,γ Binyuan Huiγ Nan Huoα,γ Bowen Qinγ Linheng Hanγ Yiyao Jinγ Chenhao Maγ αThe University of Hong Kong Feige Zhouγ Fatma Ozcanβ Jinyang Liα,γ Xiaolong Liα,γ Per Jacobssonβ Ge Quα,γ Shipei Linγ Shuzheng Siγ Edward Alexanderγ Xintong Zhuγ Rui Qinγ Ruihan Yuγ Weihao Zhongγ Yun Chenγ Yannis Papakonstantinouβ Hongyu Liuγ Reynold Chengα,γ βGoogle Cloud γThe BIRD Team bird.bench25@gmail.com https://bird-interact.github.io"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) have demonstrated remarkable performance on single-turn text-to-SQL tasks, but real-world database applications predominantly require multi-turn interactions to handle ambiguous queries, execution errors, and evolving user requirements. Existing multi-turn benchmarks fall short of capturing this complexity, either by treating conversation histories as static context or by limiting evaluation to narrow, read-only (SELECT-ONLY) operations, thereby may potentially failing to reflect the challenges encountered in production-grade database assistant. In this work, we introduce BIRD-INTERACT, benchmark that restores this missing realism through: (1) comprehensive interaction environment that couples each database with hierarchical knowledge base, metadata files, and function-driven user simulator, enabling models to solicit clarifications, retrieve knowledge, and recover from execution errors without human supervision; (2) two evaluation settings reflecting real-world interaction settings which contain pre-defined conversational protocol (c-Interact) and more open-ended agentic setting (a-Interact) in which the model autonomously decides when to query the user simulator or explore the DB environment; (3) challenging task suite that covers the full CRUD spectrum for both business-intelligence and operational use cases, guarded by executable test cases. Each task features ambiguous and follow-up sub-tasks, requiring LLMs to engage in dynamic interaction. The suite is organized into two sets: full set (BIRD-INTERACT-FULL) of 600 tasks which unfold up to 11,796 dynamic interactions for comprehensive overview of performance and lite set (BIRD-INTERACT-LITE) of 300 tasks, with simplified databases for detailed behavioral analysis of interactions, and fast development of methods. Our empirical results highlight the difficulty of BIRD-INTERACT: the most recent flagship model GPT-5 completes only 8.67% of tasks in the cInteract setting and 17.00% in the a-Interact setting on the full task suite. Further analysis via memory grafting and Interaction Test-time Scaling (ITS) validates the importance of effective interaction for achieving success in complex, dynamic text-to-SQL tasks."
        },
        {
            "title": "INTRODUCTION",
            "content": "Data-driven decision-making has become indispensable across modern enterprises, prompting surge of interest in Natural Language Interfaces to Databases (NLIDB) that empower non-technical users to extract insights from relational databases using natural language (Shi et al., 2024). Motivated by Equal contribution. 1 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Figure 1: Task overview of BIRD-INTERACT showing the evaluated system interacting with DB Environment and User Simulator to complete the user task with sequence of sub-tasks. this vision, wave of methods (Pourreza et al., 2025a;b; Pourreza & Rafiei, 2023; Liu et al., 2025; Qu et al., 2024; Li et al., 2025b; Maamari et al., 2024; Sheng & Xu, 2025; Li et al., 2025a; Talaei et al., 2024; Caferoglu & Ulusoy, 2024; Cao et al., 2024; Lee et al., 2025) based on large language models (LLMs) has recently achieved impressive text-to-SQL performance on popular single-turn benchmarks such as Spider (Yu et al., 2018) and BIRD (Li et al., 2023b). However, real-world data interaction is rarely single, perfectly-formed query (Li et al., 2025c; Dinan et al., 2019). It is an iterative, stateful dialogue characterized by ambiguity (Chen et al., 2025b) and evolving goals (Wu et al., 2025). The task in Figure 1 exemplifies this complexity. To succeed, the text-to-SQL system must first engage the user to resolve the ambiguity of the term urgent care. Only with this clarified context can it generate the correct SQL. If its initial code fails an execution test, LLM must debug and revise its SQL solution based on the error feedback. After the user confirms the SQL is correct, they may proceed with follow-up question that depends on its intermediate results. Therefore, evaluation on true practical utility LLMs with these multi-faceted aspects requires benchmark containing complete interactive problem-solving process, rather than isolated, single-turn SQL generation, but the entire interactive problem-solving loop. Although existing interactive text-to-SQL datasets (Yu et al., 2019b;a; Chen et al., 2025b; Guo et al., 2021; Dahl et al., 1994) have been developed, they inadequately model this reality for two primary reasons. First, most multi-turn text-to-SQL benchmarks rely on static conversation transcripts (Yu et al., 2019a; Chen et al., 2025b; Yu et al., 2019b; Guo et al., 2021). They present models with clean interaction history without recording the failed attempts, digressions, and clarifications that occur in practice. This design may introduce fundamental limitation: every LLM is evaluated against the same predetermined dialogue trajectory, regardless of how it would have naturally guided the interaction. This setup fails to reward intelligent interaction strategies and cannot effectively penalize conversational mess up. Second, existing benchmarks suffer from narrow task scope, overwhelmingly focusing on read-only (SELECT-only) queries typical of business intelligence (BI) reporting. This ignores vast and critical range of database management (DM) operations, including data manipulation (INSERT, UPDATE, DELETE), schema modifications (ALTER TABLE), and transactional control, which are also common operations in the normal DBA cycle (Chen et al., 2024). To address these critical limitations, we introduce BIRD-INTERACT, new benchmark designed to evaluate LLMs in dynamic text-to-SQL environment. Our work makes the following contributions: (1) High-Fidelity Interactive Environment: We develop comprehensive sandbox upon an open-source project LIVESQLBENCH (BIRD-Team, 2025) for each task, including hierarchical knowledge base (HKB) with domain-specific facts, metadata files, an executable database environment, and most critically, an interactive user simulator as recent research (Wu et al., 2025; Yao et al., 2025; Wang et al., 2024). This simulator can respond to clarification questions, provide feedback on proposed actions, and guide the model through complex tasks, enabling end-to-end evaluation without human intervention. However, recognizing that traditional simulators, even those powered by 2 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions advanced models like GPT-4o, exhibit unfair behaviors such as ground-truth leakage, we propose novel two-stage function-driven approach that maps model questions to constrained symbolic actions before generating controlled simulator responses. (2) Two Evaluation Settings: We propose two popular evaluation settings. c-Interact (protocol-guided) presents tasks with clear conversational protocol, testing models ability to follow structured conversation with the user. In contrast, a-Interact (agentic) provides only high-level goal, requiring the model to autonomously plan strategy, decide when to query the database, consult documentation, or ask the user simulator for help. (3) Comprehensive and Challenging Task Suite: BIRD-INTERACT expands the scope of evaluation to include the full spectrum of CRUD operations. Tasks are drawn from both analytical and operational domains and are accompanied by executable test cases that verify functional correctness. Each task features an ambiguous initial priority sub-task, dynamic clarification requirements, follow-up sub-tasks, and environmental uncertainties, which can only be resolved through dynamic interaction. The suite consists of two parts: full set (BIRD-INTERACT-FULL) of 600 tasks, unfolding up to 11,796 dynamic interactions for comprehensive evaluation of performance, and lite set (BIRD-INTERACT-LITE) of 300 tasks with cleaner databases, enabling finer-grained behavioral analysis and faster deployment. Our experiments show that state-of-the-art models struggle with BIRD-INTERACT, with GPT-5 achieving only 8.67% success in c-Interact and 17% in a-Interact. We identify distinct challenges across interaction modes: communication effectiveness often determines success in c-Interact, while a-Interact suffers from bias toward costly trial-and-error over strategic resource exploration. We also observe Interaction Test-time Scaling (ITS), where performance improves monotonically with additional interaction opportunities across multiple models. These findings support our hypothesis that developing strategic interaction capabilities is key to improving LLM performance on complex database reasoning."
        },
        {
            "title": "2 PROBLEM DEFINITION",
            "content": "Task Definition. We formalize interactive text-to-SQL as multi-turn collaboration between text-to-SQL system Sθ and user simulator Uγ operating over database environment = {D, M, K}, where is the executable database, contains schema metadata, and represents external knowledge (Lee et al., 2021; Dou et al., 2022; Li et al., 2023b). Given sequence of related sub-tasks = {q1, q2, . . . , qn}, the goal is for to generate SQL solutions {σ1, . . . , σn} through interactions. For each sub-task qi, the interaction proceeds through interaction turn = 1, 2, . . . until completion: = Uγ(ht1 ut , qi, E), = Sθ(ht1 st , ut i, E), = ht1 ht ut i, st (1) where ht represents the interaction history up to turn and denotes text concatenation in prompt. The user simulator Uγ manages the interaction by presenting sub-tasks, answering clarification questions for ambiguous queries, and providing feedback on submitted SQL. Critically, subsequent sub-tasks are released only after successful completion of first sub-tasks. Metrics. Each sub-task qi is annotated with ground-truth SQL σ and executable test cases Ti that define correctness. predicted solution σi is correct if it passes all associated test cases, ensuring functional equivalence with σ . In our implementation, each task consists of two related sub-tasks (n = 2): an initial priority sub-task q1 containing ambiguities requiring resolution, and (2) subsequent follow-up sub-task q2. We evaluate system performance using: (1) Success Rate (SR): The proportion of sub-tasks completed successfully, with each sub-task scored 0 or 1. We report SR separately for sub-task 1 and sub-task 2 as an online evaluation during interaction. (2) Normalized Reward: Defined as normalized scoring according to priority weighting as designed in Appendix to [0, 1] for analyzing system behaviors after interaction (offline evaluation) (Yao et al., 2022)."
        },
        {
            "title": "3 BENCHMARK CONSTRUCTION",
            "content": "This section details the methodology for the construction of BIRD-INTERACT benchmark. We begin by outlining the overall benchmark setup (Section 3.1), and then elaborate on how we convert clear single-turn tasks into ones requiring interactions (Section 3.2). 3 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions"
        },
        {
            "title": "3.1 SETUP AND RESOURCES",
            "content": "We build our benchmark on the text-to-SQL tasks and infrastructure of LIVESQLBENCH (BIRDTeam, 2025). We select this foundation due to several key advantages. First, LIVESQLBENCH provides comprehensive evaluation environment. It supports the full spectrum of SQL operations, including DML and DDL, which allows for dynamic database states that reflect real-world usage. Furthermore, its permissive license and ready-to-use artifacts, including an executable database sandbox and metadata files, facilitate extension and reproducibility. Third, it features Hierarchical Knowledge Base (HKB) that organizes external knowledge as nodes in directed acyclic graph (DAG), as shown in Figure 1, where \"AVS\" depends on \"IF\" and \"CPI\". This structure explicitly models dependencies between facts that require multi-hop reasoning to connect isolated information. Despite these strengths, LIVESQLBENCH is fundamentally single-turn benchmark. This design fails to capture the interactive and often ambiguous nature of real-world data analysis scenarios. Our primary contribution is to convert this static benchmark into dynamic, interactive setting. 3."
        },
        {
            "title": "INTERACTIVE TASK ANNOTATION",
            "content": "To maintain the integrity and quality of our benchmark, we recruit 12 expert annotators through rigorous multi-stage selection process detailed in Appendix B. We describe systematically the conversion from single-turn tasks of LIVESQLBENCH into multi-turn interactive scenarios through two key annotation strategies: ambiguity injection and follow-up sub-task generation: Ambiguity Injection. Ambiguities in daily life require interactions to seek clarification. To make annotation and evaluation controllable, we design methods to inject ambiguities into single-turn queries and the environment from LIVESQLBENCH, pairing each with unique clarification. (1) Superficial user query ambiguities: we target surface-level ambiguity in the user request. These include intent-level ambiguities, where the user language is vague (e.g., \"elderly people\"), and implementation-level ambiguities, where the users intent is clear but the implementation details (e.g., decimal precision) are under-specific. (2) Knowledge ambiguities: we inject incompleteness into the external knowledge. This category includes two subtypes: (i) one-shot knowledge ambiguity, where isolated knowledge entries are removed. (ii) knowledge chain breaking, where intermediate nodes in multi-hop knowledge chains are masked. For example, consider the chain \"urgent care\" \"AVS\" \"IF/CPI\" in Figure 2. By masking the intermediate node, i.e., the fact \"AVS\" in HKB, we deliberately break the inferential chain, rendering knowledge ambiguous and requiring user clarification to proceed. (3) Environmental ambiguities: LIVESQLBENCH databases already contain natural noise, such as NULL in critical fields, which further introduces uncertainty in how these cases should be handled. Figure 2: Knowledge chain breaking ambiguity. Each injected ambiguity is paired with corresponding SQL snippet from the ground-truth query as clarification source, which guides our user simulator in generating consistent and contextually appropriate clarifications. Quality control ensures that ambiguous queries are unsolvable without clarification yet fully reconstructable once clarifications are provided. Complete details are given in Appendix G. Follow-Up Sub-tasks Annotation. User intents frequently evolve throughout an interactive session (Taylor, 2015), with users modifying, filtering conditions, or exploring related aspects of their queries. Therefore, we also extend each initial priority sub-task with one additional follow-up sub-task to resonate with this scenario. These follow-up sub-tasks are designed carefully using principled 5-category taxonomy detailed in Appendix G.5. key contribution of our benchmark is the introduction of state dependency between sub-tasks, different with other datasets (Yu et al., 2019a;b; Lee et al., 2021; Zhong et al., 2017; Li et al., 2025d). System models must reason over modified database states or the newly created objects (e.g. tables) from preceding queries to write SQLs for follow-up sub-tasks. 4 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions"
        },
        {
            "title": "3.3 FUNCTION-DRIVEN USER SIMULATOR",
            "content": "Evaluating interactive text-to-SQL systems requires user interactions, such as multi-turn requests and responses to clarification questions. Conducting such human-in-the-loop evaluations at scale is impractical. To make large-scale evaluations feasible, recent interactive benchmarks, such as MINT (Wang et al., 2024), employ LLMs to simulate human users (Li et al., 2025c; Yu et al., 2019a;b). However, we observe that there are two major issues among these simulators: (1) they sometimes leak information from ground-truth SQL query, and (2) they may deviate from the original task requirements (Barres et al., 2025; Kazi et al., 2024). STATISTIC Table 1: Data Statistics Two-Stage Strategy. To ensure more robust evaluation, we introduce two-stage function-driven user simulator, as illustrated in Figure 3(c). In the first stage, an LLM functions as semantic parser. It maps the systems clarification request into one of three predefined allowed actions: AMB(), LOC(), or UNA(). AMB() is invoked for queries related to ambiguities that have been pre-annotated with the key SQL snippet. LOC() handles reasonable clarification requests that fall outside our pre-annotated ambiguities, such as questions about SQL formatting or specific sub-components. In these cases, the simulator uses an AST-based retrieval step to locate the relevant SQL fragment (detailed in Appendix M). Finally, UNA() rejects any inappropriate requests, such as attempts to elicit ground-truth answers. In the second stage, the user simulator generates final response based on the chosen action and the annotated GT SQL with clarification source. This two-stage approach, ensures the simulators behavior remains predictable and controllable, while still permitting diverse and context-aware interactions. Detailed prompts are provided in Appendix Q. Total Tasks # BI tasks # DM tasks # Distinct Test Cases # Tokens / User Query # Tokens / SQL # Ambiguities / Task # sub-tasks / Task # Interactions / Task 300 195 105 135 40.22 600 410 190 191 32.95 252.21 3.89 2 13.64 361.52 5.16 2 13.04 Inter-Agreement FULL 93.33 93.50 LITE 3.4 DATA STATISTICS Table 1 reports key properties of BIRD-INTERACT. The resulting benchmark comprises total of 900 interactive text-to-SQL tasks, each featuring an ambiguous initial priority sub-task, dynamic clarification requirements, follow-up sub-tasks, and environmental uncertainties, collectively spanning the full CRUD spectrum (Create, Read, Update, and Delete). In Appendix D, we also conduct comprehensive comparison against other relevant benchmarks, showing that BIRD-INTERACT is among the most open, challenging, and long-horizon interactive benchmarks in text-to-SQL scenarios."
        },
        {
            "title": "4 EVALUATION SETTINGS",
            "content": "Two Evaluation Settings. The interactive framework of BIRD-INTERACT supports evaluation in two scenarios: LLMs as conversational assistants (c-Interact) (Dinan et al., 2019) and as agents (a-Interact) (Schluntz & Zhang, 2024). Budget-Constrained Awareness Testing. The application of LLMs is limited by computational resources and user patience (Wen et al., 2025; Li et al., 2025e). We introduce budget-constrained awareness mechanism to both evaluation settings, where interactions are capped by an adaptive budget and systems are informed of the remaining budget. This enables evaluation under varying budgets, including stress-testing (Ahmad et al., 2025; Hubinger, 2024) in low-budget conditions to assess the systems ability to ask the right questions and plan effectively. The specific budget settings are detailed in the following sections. 4.1 c-INTERACT EVALUATION Interaction Setup. The c-Interact evaluation establishes multi-turn dialogue between user simulator and system S. The session unfolds in two sequential phases of sub-tasks: First, presents an underspecified sub-task q1 alongside database metadata and knowledge base K. System may engage in clarification dialogue before generating SQL σ1. Upon successful validation against test cases T1, issues contextually coherent follow-up sub-task q2, prompting to respond with SQL σ2. Each sub-task incorporates single debugging opportunity: following query failure, may BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Figure 3: Two evaluation settings for BIRD-INTERACT: c-Interact, where the system engages in conversation with the user, and a-Interact, where the system interacts flexibly. At the end of the task, the system will receive reward [0, 1]. submit one revised query after receiving execution feedback from U. Each debugging attempt incurs reward penalty to account for the additional computational cost, details can be found in Figure 3. The evaluation episode concludes when both sub-tasks are successfully completed or all attempts are exhausted. Notably, failure in the initial priority sub-task immediately terminates the entire session. Budget Constraints. The budget is implemented as constraint on the number of clarification turns. The total allowed turns, τclar, are calculated as follows: τclar = mamb + λpat, Here, mamb represents the minimum budget required to resolve the ambiguities, which is equal to the number of annotated ambiguities in the user task. The parameter λpat is tunable variable that simulates different levels of user patience, granting the evaluated system extra turns for clarification. 4.2 a-INTERACT EVALUATION Interaction Setup. The a-Interact provides LLMs with autonomous planning and execution within pre-defined action space, following REACT paradigm (Yao et al., 2023). We model the complete database environment as set of callable tools, containing the target database, metadata, HKB, and User Simulator, allowing the agent to determine optimal invocation strategies dynamically. In this work, we summarize and define 9 discrete actions common to text-to-SQL with details in Appendix I. BIRD-INTERACT also supports customized scaffolds, details can be found in Appendix I.2. Budget Constraints. To reflect the varying computational costs of different actions, we implement budget-constrained evaluation framework where each action consumes predetermined amount of budget, encouraging cost-effective action sequences. The total budget for each task is = Bbase + 2 mamb + 2 λpat, where Bbase = 6 is the base budget, mamb is the number of annotated ambiguity points, and λpat is the user patience parameter, maintaining consistency with the c-Interact framework. This setting evaluates the agents ability to achieve high performance under resource constraints while balancing thoroughness with efficiency. Further details of action costs are provided in Appendix I. This setting can evaluate agent performance under realistic constraints that present practical database interaction scenarios, where users have limited patience and computational resources are finite. 6 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Table 2: Success Rate and Final Normalized Reward of different models on BIRD-INTERACT-FULL. The success rate is cumulative; Reward* is the normalized reward. The values reported in c-Interact are after debugging phase, and (+n) means the performance gained via debugging. Avg. Cost is the cost for one task on average in USD. Our user simulator has an avg. cost of 0.03 USD. BI = Business Intelligence User Queries, DM = Data Management User Queries. Model Priority Question (Success Rate %) Overall DM BI Follow Ups (Success Rate %) DM BI Overall Reward* Avg. Cost 9.49 ( +0.00) GPT-5 10.71 ( +4.62) Claude-Sonnet-3.7 Deepseek-Chat-V3.1 11.44 ( +0.73) Qwen-3-Coder-480B 16.30 ( +2.68) Claude-Sonnet-4 16.06 ( +4.87) 17.76 ( +2.92) O3-Mini 18.73 ( +4.38) Gemini-2.5-Pro 25.40 ( +2.12) 33.86 ( +7.41) 33.86 ( +3.17) 34.39 ( +5.29) 35.98 ( +10.58) 37.57 ( +11.11) 38.62 ( +10.05) Qwen-3-Coder-480B Deepseek-Chat-V3.1 O3-Mini Gemini-2.5-Pro Claude-Sonnet-3.7 Claude-Sonnet-4 GPT-5 8.05 10.49 12.20 10.49 11.46 15.85 15.61 24.74 31.58 36.32 41.58 41.58 53.68 58. c-Interact Text-to-SQL 14.50 ( +0.67) 18.00 ( +5.50) 18.50 ( +1.50) 22.00 ( +3.50) 22.33 ( +6.67) 24.00 ( +5.50) 25.00 ( +6.17) 5.84 ( +0.24) 4.62 ( +0.49) 4.62 ( +0.24) 8.03 ( +0.97) 10.46 ( +1.22) 11.44 ( +0.73) 12.41 ( +1.22) a-Interact Text-to-SQL 13.33 17.17 19.83 20.33 21.00 27.83 29.17 3.90 4.63 5.85 5.85 5.61 8.05 10. 14.81 ( +0.53) 16.40 ( +3.17) 16.93 ( +1.06) 16.93 ( +4.23) 22.22 ( +3.70) 25.40 ( +4.23) 24.87 ( +5.29) 8.67 ( +0.33) 8.33 ( +1.33) 8.50 ( +0.50) 10.83 ( +2.00) 14.17 ( +2.00) 15.83 ( +1.83) 16.33 ( +2.50) 4.74 5.26 14.21 20.00 16.84 22.63 30.00 4.17 4.83 8.50 10.33 9.17 12.67 17.00 12.58 13.87 15.15 17.75 18.35 20.27 20.92 10.58 13.47 16.43 17.33 17.45 23.28 25. $ 0.08 $ 0.29 $ 0.12 $ 0.11 $ 0.29 $ 0.07 $ 0.04 $ 0.07 $ 0.06 $ 0.06 $ 0.22 $ 0.60 $ 0.51 $ 0."
        },
        {
            "title": "5 EXPERIMENT",
            "content": "We benchmark 7 recent and powerful LLMs (2 open-source, 5 closed-source) as system models via fresh PostgreSQL 14 Docker instance for more stable evaluation. We set the user patience to 3 by default and a-Interact base budget of 6. All models use temperature=0 and top_p=1, with default reasoning settings, conducting single runs due to cost (full details in Appendix H.2 and H.3). 5.1 MAIN RESULTS Table 2 summarizes the success rate (SR) and normalized reward (NR) obtained by 7 representative frontier LLMs on BIRD-INTERACT-FULL. The full experimental results of BIRD-INTERACT-LITE can be found in Table 10. We can observe: BIRD-INTERACT remains challenging, leaving ample room for future improvement. Even the strongest models in our study, GPT-5 and Gemini-2.5-Pro, capture only 20.92% and 25.52% of the available reward respectively, in the c-Interact and a-Interact mode. Absolute success rates reveal similar limitations: no more than 16.33% of tasks are solved end-to-end in c-Interact and 17.00% in a-Interact, with most models falling in substantially lower rates. Evolving User Intent is Challenge in Online Assessment. Follow-up sub-tasks are noticeably more challenging, likely because the longer, concatenated context in these turns remains bottleneck for LLMs in interactive text-to-SQL tasks. Offline Reward v.s. Online SR Evaluation. Table 2 shows that offline normalized reward (NR) and online success rate (SR) generally correlate positively, though notable divergences occur due to the reward structure allocating 70% to the primary sub-task and 30% to follow-up sub-tasks. These complementary metrics capture different aspects of model performance. Success rate measures holistic task completion across multi-turn interactions, relevant when users prioritize successful outcomes regardless of path. Normalized reward assesses performance on users critical initial objectives while crediting challenging follow-up sub-tasks. Together, they provide comprehensive evaluation of the distinct capabilities required for advanced interactive text-to-SQL systems. Business Intelligence versus Data Management. Business intelligence (BI) queries pose significantly greater challenges for LLMs compared to data management (DM) tasks since DM operations typically follow standardized, predictable patterns that LLMs can effectively learn (Li et al., 2025d), 7 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Figure 4: The performance of different LLMs with different user patience on BIRD-INTERACT-LITE. The red line denotes a-Interact mode (-a); the blue line denotes c-Interact mode (-c). And the dotted line (Idealized Performance) denotes the performance under ambiguity-free single-turn text-to-SQL. whereas BI queries demand nuanced understanding of complex, domain-specific business logic and analytical reasoning that varies substantially across contexts. Interaction Mode Emerged as the Decisive Factor for Successful Outcome. Furthermore, we observe that different models demonstrate varying aptitudes for different interaction paradigms, with each model showing relative strengths in specific modes. For example, GPT-5 performs poorly in the constrained, predefined flow designed personally of the c-Interact mode by achieving only 14.50% SR (worst) but excels in the a-Interact setting with 29.17% SR (best), which affords more flexible and exploratory space. This evidence demonstrates the critical importance of matching interaction modes to model-specific capabilities, which we hypothesize stem from differences in training data distributions and architectural inductive biases (Liu et al., 2024; Gao et al., 2024b). 5.2 INTERACTION ANALYSIS The Impact of Communication on Task Success in c-Interact. notable finding is the underperformance of the flagship model, GPT-5, on the c-Interact, despite its strong performance on many single-turn tasks (Phan et al., 2025; Glazer et al., 2024; Rein et al., 2024). Therefore, we hypothesize that this stems from deficiency in its interactive communication abilities rather than its core generation capability. To test this hypothesis, we conduct an experiment termed Memory Grafting. In this setup, we provide GPT-5 with the ambiguity resolution histories from two other better models, Qwen-3-Coder and O3-mini, before asking it to generate the final SQL query. The results, presented in Figure 5, show that GPT-5s performance improves significantly when leveraging the interaction history from either model. This finding indicates that while GPT-5 possesses robust SQL generation capabilities, more effective communication schema is required to help it achieve satisfactory outcomes for user tasks. We also further analyze the patterns for effective communication in Appendix O. Figure 5: SR of GPT-5 with memory grafting. Interaction Test-Time Scaling. To investigate the relationship between interaction frequency and model performance, we conduct an Interaction Test-Time Scaling (ITS) experiment in BIRDINTERACT-LITE where results are shown in Figure 4. We simulate varying levels of user patience by allowing different numbers of interaction turns for both c-Interact and a-Interact. As baseline, we include single-turn task performance for each model, where all necessary context is provided to BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions create unambiguous tasks. This single-turn condition represents an idealized scenario that, while potentially requiring significant user effort to ensure complete information provided (Li et al., 2025d), eliminates the need for further clarification. As demonstrated in the figure, Claude-3.7-Sonnet exhibits clear scaling behavior with respect to increasing interaction opportunities. This pattern shows that the model can steadily improve by transforming additional interaction chances into valuable information gains through efficient interaction. ITS Law: model satisfies this law if, given enough interactive turns, its performance can match or even surpass that of the idealized single-turn task. Action Distribution Patterns in a-Interact. We analyze action distributions across 7 system models and find concentration in two primary actions: submit (direct code execution with error feedback) and ask (user clarification requests), which together comprise 60.87% of all actions. Despite being the most computationally expensive actions (Figure 3), models favor these over systematic exploration behaviors like knowledge and schema retrieval. This suggests LLMs prefer direct trial-and-error execution over comprehensive environment exploration, likely due to pretraining biases. Future work should incentivize broader tool utilization for complex interactive tasks. Additional analysis on the FULL set appears in Appendix I."
        },
        {
            "title": "6 USER SIMULATOR ANALYSIS",
            "content": "This section presents comprehensive evaluation of our function-driven user simulator compared to conventional user simulators and their respective impacts on dynamic interactive text-to-SQL benchmarks through both objective and subjective experiments. Evaluation on USERSIM-GUARD. To provide an objective and comprehensive observation of different user simulator mechanisms, we construct static dataset called USERSIM-GUARD, comprising 1,989 questions with reference actions labeled by human experts. Detailed information regarding the distribution and annotation procedures can be found in Appendix N. We employed an LLM-as-Judge (Zheng et al., 2023) evaluation framework using Qwen-2.5-72B and Llama-3.1-70B as independent evaluators to mitigate potential self-enhancement bias. Our analysis reveals significant reliability concerns with conventional user simulator designs. Specifically, as shown in Figure 6, when confronted with Unanswerable (UNA) questions, baseline user simulators consistently fail to implement safeguards, resulting in unfair or inappropriate feedback generation with over 34% failure rate. In contrast, our proposed functiondriven approach demonstrates substantially improved reliability, with only 5.9% of responses falling into problematic categories. This represents significant improvement in user simulator robustness and reliability compared to baseline approaches. Figure 6: The accuracy of different user simulators on USERSIM-GUARD. Alignment with Human User. We evaluate alignment between our user simulators and actual human behavior by having human experts interact with 7 system models on 100 randomly sampled tasks across BI and DM domains. We then compute correlations (Ivey et al., 2024; Kong et al., 2024) between success rates (SR) achieved by human users versus our simulators across the same tasks. As shown in Table 3, function-driven simulators demonstrate significantly stronger alignment with human behavior: GPT-4o with function calling achieves 0.84 Pearson correlation (p = 0.02) compared to 0.61 without function calling (p = 0.14), while Gemini-2.0-Flash shows similar improvements. Table 3: Correlation analysis between AI and human users. User Simulator Pearson (p-value) GPT-4o - w/ Func. (Ours) Gemini-2.0-Flash - w/ Func. (Ours) GPT-4o - Baseline Gemini-2.0-Flash - Baseline 0.84 (p = 0.02) 0.79 (p = 0.03) 0.61 (p = 0.14) 0.54 (p = 0.21) 9 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions These results confirm that incorporating our designed mechanism produces more realistic user simulators that better reflect actual human-AI interaction patterns (detailed analysis in Appendix N)."
        },
        {
            "title": "7 RELATED WORK",
            "content": "Text-to-SQL. Text-to-SQL has emerged as an attractive interface to relational databases because it frees users from learning intricate schema details and SQL syntax. The advent of large language models (LLMs) (OpenAI, 2025; Team et al., 2023; Team, 2024; Guo et al., 2025; Li et al., 2023a; Qu et al., 2025) with strong reasoning and cross-domain generalization has accelerated this progress. Few-shot systems such as DIN-SQL (Pourreza & Rafiei, 2023) and DAIL-SQL (Gao et al., 2024a) exploit in-context learning to decouple the task into schema-linking and SQL-generation stages, while methods like CodeS (Li et al., 2024a) and DTS-SQL (Pourreza & Rafiei, 2024) improve smaller models through carefully curated, high-quality training subsets. Concurrently, agent-based frameworks that interleave thought, action, and observation, which are exemplified by MAC-SQL (Wang et al., 2025), demonstrate that iterative interaction with the environment can further raise SQL accuracy. Despite these advances, virtually all existing systems are evaluated only in single-turn settings; their effectiveness in conversational, multi-turn text-to-SQL scenarios remains an open question. Multi-turn Text-to-SQL. Multi-turn Text-to-SQL addresses the reality that user queries are often ambiguous or underspecified; without clarification the system may return incorrect or empty results. Benchmarks such as COSQL and LEARN-TO-CLARIFY extend the Spider (Yu et al., 2018) dataset with dialogue turns to probe this challenge (Yu et al., 2019a; Chen et al., 2025b; Li et al., 2024b). However, these resources presuppose static, noise-free dialogue history shared by all models, ignoring that different systems might ask different follow-up questions (Yao et al., 2025; Barres et al., 2025). More recent evaluations of autonomous agents, for example, MINT, introduce dynamic interaction histories (Wang et al., 2024), yet they have not been adapted to the text-to-SQL setting. Constructing realistic user simulator for databases is non-trivial because it must respect complex schema constraints while keeping the answer space fair and controllable (Zhou et al., 2025; Barres et al., 2025). In this work, we fill this gap by proposing an interactive benchmark that is implemented with an optimized user simulator, new databases, and knowledge, and we analyze the behaviour of state-of-the-art reasoning models rigorously to make contributions for realistic and uncertain text-to-SQL systems."
        },
        {
            "title": "8 FUTURE WORK",
            "content": "While BIRD-INTERACT establishes comprehensive framework for evaluating interactive textto-SQL systems, several directions remain for future investigation. First, we plan to develop post-trained, human-aligned local user simulator via post-training, aiming to capture more reliable response patterns while maintaining controllability and reducing API cost. Second, our current a-Interact setting imposes strict budget constraints that create stress-mode evaluation environment, placing considerable pressure on LLM agents to make optimal decisions under resource scarcity. To complement these findings, we will conduct experiments in free-mode setting without the budgetconstrained awareness testing (Section 4). This could allow us to observe natural interaction strategies when models are unconstrained, identify whether more sophisticated exploration patterns emerge, and characterize the relationship between interaction thoroughness and task success. Comparing stress-mode and free-mode performance will provide deeper insights into efficiency-effectiveness trade-offs in interactive text-to-SQL systems."
        },
        {
            "title": "9 CONCLUSION",
            "content": "We present BIRD-INTERACT, benchmark for evaluating interactive text-to-SQL systems through dynamic, multi-turn interactions that better reflect real-world usage scenarios. Our benchmark features function-driven user simulator, dual evaluation settings for conversational and autonomous planning modes, and totally 900 challenging tasks designed to test LLM abilities to handle ambiguities and maintain state across turns. Comprehensive evaluation demonstrates critical gap between existing SQL generation capabilities and the strategic interaction skills required for effective human-AI collaboration in database querying. 10 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions"
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "We would like to express our sincere gratitude to Irina Saparina, Mohammadreza Pourreza, Mehdi Bouzouina, Hailong Li, Jiatong Shi, and Professor Shinji Watanabe for their fruitful discussions and valuable insights that helped improve this work."
        },
        {
            "title": "REFERENCES",
            "content": "Lama Ahmad, Sandhini Agarwal, Michael Lampe, and Pamela Mishkin. Openais approach to external red teaming for ai models and systems. arXiv preprint arXiv:2503.16431, 2025. Victor Barres, Honghua Dong, Soham Ray, Xujie Si, and Karthik Narasimhan. tau2-bench: Evaluating conversational agents in dual-control environment. arXiv preprint arXiv:2506.07982, 2025. Adithya Bhaskar, Tushar Tomar, Ashutosh Sathe, and Sunita Sarawagi. Benchmarking and improving text-to-SQL generation under ambiguity. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 70537074, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.436. BIRD-Team. Livesqlbench: dynamic and contamination-free benchmark for evaluating llms on real-world text-to-sql tasks. https://github.com/bird-bench/livesqlbench, 2025. Accessed: 2025-05-22. Hasan Alp Caferoglu and Özgür Ulusoy. E-sql: Direct schema linking via question enrichment in text-to-sql. arXiv preprint arXiv:2409.16751, 2024. Zhenbiao Cao, Yuanlei Zheng, Zhihao Fan, Xiaojin Zhang, Wei Chen, and Xiang Bai. Rsl-sql: Robust schema linking in text-to-sql generation. arXiv preprint arXiv:2411.00073, 2024. Chongyan Chen, Yu-Yun Tseng, Zhuoheng Li, Anush Venkatesh, and Danna Gurari. Accounting for focus ambiguity in visual questions. arXiv preprint arXiv:2501.02201, 2025a. Maximillian Chen, Ruoxi Sun, Tomas Pfister, and Sercan Ö. Arik. Learning to clarify: Multi-turn conversations with action-based contrastive self-training. In ICLR, 2025b. Xi Chen, Jinguo You, Kun Li, and Xiang Li. Beyond read-only: Crafting comprehensive Chinese text-to-SQL dataset for database manipulation and query. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, pp. 33833393, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-naacl.214. Deborah A. Dahl, Madeleine Bates, Michael Brown, William Fisher, Kate Hunicke-Smith, David Pallett, Christine Pao, Alexander Rudnicky, and Elizabeth Shriberg. Expanding the scope of the ATIS task: The ATIS-3 corpus. In Human Language Technology: Proceedings of Workshop held at Plainsboro, New Jersey, March 8-11, 1994, 1994. Bryan L. M. de Oliveira, Luana G. B. Martins, Bruno Brandão, and Luckeciano C. Melo. Infoquest: Evaluating multi-turn dialogue agents for open-ended conversations with hidden context. arXiv preprint arXiv:2502.12257, 2025. Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In ICLR (Poster). OpenReview.net, 2019. Zhongjun Ding, Yin Lin, and Tianjing Zeng. Ambisql: Interactive ambiguity detection and resolution for text-to-sql. arXiv preprint arXiv:2508.15276, 2025. Mingwen Dong, Nischal Ashok Kumar, Yiqun Hu, Anuj Chauhan, Chung-Wei Hang, Shuaichen Chang, Lin Pan, Wuwei Lan, Henghui Zhu, Jiarong Jiang, et al. Practiq: practical conversational text-to-sql dataset with ambiguous and unanswerable queries. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 255273, 2025. 11 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Longxu Dou, Yan Gao, Xuqi Liu, Mingyang Pan, Dingzirui Wang, Wanxiang Che, Dechen Zhan, MinYen Kan, and Jian-Guang Lou. Towards knowledge-intensive text-to-SQL semantic parsing with formulaic knowledge. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 52405253, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.350. Avrilia Floratou, Fotis Psallidas, Fuheng Zhao, Shaleen Deep, Gunther Hagleither, Wangda Tan, Joyce Cahoon, Rana Alotaibi, Jordan Henkel, Abhik Singla, Alex Van Grootel, Brandon Chow, Kai Deng, Katherine Lin, Marcos Campos, K. Venkatesh Emani, Vivek Pandit, Victor Shnayder, Wenjing Wang, and Carlo Curino. NL2SQL is solved problem... not! In CIDR. www.cidrdb.org, 2024. Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql empowered by large language models: benchmark evaluation. Proc. VLDB Endow., 17(5):11321145, January 2024a. ISSN 2150-8097. doi: 10.14778/3641204.3641221. Jie Gao, Simret Araya Gebreegziabher, Kenny Tsu Wei Choo, Toby Jia-Jun Li, Simon Tangi Perrault, and Thomas Malone. taxonomy for human-llm interaction modes: An initial exploration. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, CHI EA 24, New York, NY, USA, 2024b. Association for Computing Machinery. ISBN 9798400703317. doi: 10.1145/3613905.3650786. Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, Olli Järviniemi, Matthew Barnett, Robert Sandler, Jaime Sevilla, Qiuyu Ren, Elizabeth Pratt, Lionel Levine, Grant Barkley, Natalie Stewart, Bogdan Grechuk, Tetiana Grechuk, Shreepranav Varma Enugandla, and Mark Wildon. Frontiermath: benchmark for evaluating advanced mathematical reasoning in ai. arXiv preprint, arXiv:2411.04872, 2024. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jiaqi Guo, Ziliang Si, Yu Wang, Qian Liu, Ming Fan, Jian-Guang Lou, Zijiang Yang, and Ting Liu. Chase: large-scale and pragmatic Chinese dataset for cross-database context-dependent text-toSQL. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 23162331, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.180. Moshe Hazoom, Vibhor Malik, and Ben Bogin. Text-to-SQL in the wild: naturally-occurring dataset based on stack exchange data. In Royi Lachmy, Ziyu Yao, Greg Durrett, Milos Gligoric, Junyi Jessy Li, Ray Mooney, Graham Neubig, Yu Su, Huan Sun, and Reut Tsarfaty (eds.), Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021), pp. 7787, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.nlp4prog-1.9. Zezhou Huang, Pavan Kalyan Damalapati, and Eugene Wu. Data ambiguity strikes back: How documentation improves gpts text-to-sql. arXiv preprint arXiv:2310.18742, 2023. Evan Hubinger. Introducing alignment stress-testing at anthropic. In AI Alignment Forum, January, 2024. Nan Huo, Reynold Cheng, Ben Kao, Wentao Ning, Nur Al Hasan Haldar, Xiaodong Li, Jinyang Li, Mohammad Matin Najafi, Tian Li, and Ge Qu. Zeroea: zero-training entity alignment framework via pre-trained language model. Proceedings of the VLDB Endowment, 17(7):17651774, 2024. 12 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Nan Huo, Jinyang Li, Bowen Qin, Ge Qu, Xiaolong Li, Xiaodong Li, Chenhao Ma, and Reynold Cheng. Micro-act: Mitigate knowledge conflict in question answering via actionable self-reasoning. arXiv preprint arXiv:2506.05278, 2025. Jonathan Ivey, Shivani Kumar, Jiayu Liu, Hua Shen, Sushrita Rakshit, Rohan Raju, Haotian Zhang, Aparna Ananthasubramaniam, Junghwan Kim, Bowen Yi, et al. Real or robotic? assessing whether llms accurately simulate qualities of human responses in dialogue. arXiv preprint arXiv:2409.08330, 2024. Taaha Kazi, Ruiliang Lyu, Sizhe Zhou, Dilek Hakkani-Tür, and Gokhan Tur. Large language models as user-agents for evaluating task-oriented-dialogue systems. In SLT, pp. 913920. IEEE, 2024. Chuyi Kong, Yaxin Fan, Xiang Wan, Feng Jiang, and Benyou Wang. Platolm: Teaching llms in multi-round dialogue via user simulator. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 78417863, 2024. Maya Larbi, Amal Akli, Mike Papadakis, Rihab Bouyousfi, Maxime Cordy, Federica Sarro, and Yves Le Traon. When prompts go wrong: Evaluating code model robustness to ambiguous, contradictory, and incomplete task descriptions. arXiv preprint arXiv:2507.20439, 2025. Accessed: 2025-09-23. Chia-Hsuan Lee, Oleksandr Polozov, and Matthew Richardson. Kaggledbqa: Realistic evaluation of text-to-sql parsers. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 22612273, 2021. Dongjun Lee, Choongwon Park, Jaehyuk Kim, and Heesoo Park. MCS-SQL: leveraging multiple In COLING, pp. 337353. prompts and multiple-choice selection for text-to-sql generation. Association for Computational Linguistics, 2025. Fangyu Lei, Jixuan Chen, Yuxiao Ye, Ruisheng Cao, Dongchan Shin, SU Hongjin, ZHAOQING SUO, Hongcheng Gao, Wenjing Hu, Pengcheng Yin, et al. Spider 2.0: Evaluating language models on real-world enterprise text-to-sql workflows. In The Thirteenth International Conference on Learning Representations, 2025. Boyan Li, Jiayi Zhang, Ju Fan, Yanwei Xu, Chong Chen, Nan Tang, and Yuyu Luo. Alpha-SQL: Zero-shot text-to-SQL using monte carlo tree search. In Forty-second International Conference on Machine Learning, 2025a. Haoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xiaokang Zhang, Jun Zhu, Renjie Wei, Hongyan Pan, Cuiping Li, and Hong Chen. Codes: Towards building open-source language models for text-to-sql. Proceedings of the ACM on Management of Data (PACMMOD), 2(3):128, 2024a. doi: 10.1145/3654930. Haoyang Li, Shang Wu, Xiaokang Zhang, Xinmei Huang, Jing Zhang, Fuxin Jiang, Shuai Wang, Tieying Zhang, Jianjun Chen, Rui Shi, Hong Chen, and Cuiping Li. Omnisql: Synthesizing high-quality text-to-sql data at scale. Proc. VLDB Endow., 18(11):46954709, September 2025b. ISSN 2150-8097. doi: 10.14778/3749646.3749723. Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, and Yongbin Li. Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sql parsing. In Proceedings of the AAAI conference on artificial intelligence, pp. 1307613084, 2023a. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin Chen-Chuan Chang, Fei Huang, Reynold Cheng, and Yongbin Li. Can LLM already serve as database interface? big bench for large-scale database grounded text-to-sqls. In NeurIPS, 2023b. Jinyang Li, Nan Huo, Yan Gao, Jiayi Shi, Yingxiu Zhao, Ge Qu, Yurong Wu, Chenhao Ma, JianGuang Lou, and Reynold Cheng. Tapilot-crossing: Benchmarking and evolving llms towards interactive data analysis agents. arXiv preprint arXiv:2403.05307, 2024b. BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Jinyang Li, Nan Huo, Yan Gao, Jiayi Shi, Yingxiu Zhao, Ge Qu, Bowen Qin, Yurong Wu, Xiaodong Li, Chenhao Ma, Jian-Guang Lou, and Reynold Cheng. Are large language models ready for multi-turn tabular data analysis? In Forty-second International Conference on Machine Learning, 2025c. Jinyang Li, Xiaolong Li, Ge Qu, Per Jacobsson, Bowen Qin, Binyuan Hui, Shuzheng Si, Nan Huo, Xiaohan Xu, Yue Zhang, et al. Swe-sql: Illuminating llm pathways to solve user sql issues in real-world applications. arXiv preprint arXiv:2506.18951, 2025d. Junyan Li, Wenshuo Zhao, Yang Zhang, and Chuang Gan. Steering llm thinking with budget guidance. arXiv preprint arXiv:2506.13752, 2025e. Zongxi Li, Yang Li, Haoran Xie, and Joe Qin. Condambigqa: benchmark and dataset for conditional ambiguous question answering. arXiv preprint arXiv:2502.01523, 2025f. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating LLMs as agents. In The Twelfth International Conference on Learning Representations, 2024. Yifu Liu, Yin Zhu, Yingqi Gao, Zhiling Luo, Xiaoxia Li, Xiaorong Shi, Yuntao Hong, Jinyang Gao, Yu Li, Bolin Ding, et al. Xiyan-sql: novel multi-generator framework for text-to-sql. arXiv preprint arXiv:2507.04701, 2025. Karime Maamari, Fadhil Abubaker, Daniel Jaroslawicz, and Amine Mhedhbi. The death of schema linking? text-to-sql in the age of well-reasoned language models. arXiv preprint arXiv:2408.07702, 2024. Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. Ambigqa: Answering ambiguous open-domain questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 57835797, 2020. OpenAI. Openai o3 and o4-mini system card, 2025. Accessed: 2025-05-15. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. Mohammadreza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-to-sql with self-correction. Advances in Neural Information Processing Systems, 36:3633936348, 2023. Mohammadreza Pourreza and Davood Rafiei. Dts-sql: Decomposed text-to-sql with small large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, 2024. Mohammadreza Pourreza, Hailong Li, Ruoxi Sun, Yeounoh Chung, Shayan Talaei, Gaurav Tarlok Kakkar, Yu Gan, Amin Saberi, Fatma Ozcan, and Sercan Ö. Arik. CHASE-SQL: multi-path reasoning and preference optimized candidate selection in text-to-sql. In ICLR. OpenReview.net, 2025a. Mohammadreza Pourreza, Shayan Talaei, Ruoxi Sun, Xingchen Wan, Hailong Li, Azalia Mirhoseini, Amin Saberi, Sercan Arik, et al. Reasoning-sql: Reinforcement learning with sql tailored partial rewards for reasoning-enhanced text-to-sql. arXiv preprint arXiv:2503.23157, 2025b. Ge Qu, Jinyang Li, Bowen Li, Bowen Qin, Nan Huo, Chenhao Ma, and Reynold Cheng. Before generation, align it! novel and effective strategy for mitigating hallucinations in text-to-sql generation. In ACL (Findings), pp. 54565471. Association for Computational Linguistics, 2024. Ge Qu, Jinyang Li, Bowen Qin, Xiaolong Li, Nan Huo, Chenhao Ma, and Reynold Cheng. SHARE: an slm-based hierarchical action correction assistant for text-to-sql. In ACL (1), pp. 1126811292. Association for Computational Linguistics, 2025. 14 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Irina Saparina and Mirella Lapata. AMBROSIA: benchmark for parsing ambiguous questions into database queries. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. Erik Schluntz and Barry Zhang. Building effective agents, December 2024. Engineering at Anthropic. Lei Sheng and Shuai-Shuai Xu. Slm-sql: An exploration of small language models for text-to-sql. arXiv preprint arXiv:2507.22478, 2025. Liang Shi, Zhengju Tang, Nan Zhang, Xiaotong Zhang, and Zhi Yang. survey on employing large language models for text-to-sql tasks. ACM Computing Surveys, 2024. Shayan Talaei, Mohammadreza Pourreza, Yu-Chen Chang, Azalia Mirhoseini, and Amin Saberi. Chess: Contextual harnessing for efficient sql synthesis. arXiv preprint arXiv:2405.16755, 2024. Robert Taylor. Question-negotiation and information seeking in libraries. College & Research Libraries, 76(3):251267, 2015. DeepSeek-AI Team. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. Rat-sql: Relation-aware schema encoding and linking for text-to-sql parsers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 75677578, 2020. Bing Wang, Changyu Ren, Jian Yang, Xinnian Liang, Jiaqi Bai, Linzheng Chai, Zhao Yan, Qian-Wen Zhang, Di Yin, Xing Sun, and Zhoujun Li. Mac-sql: multi-agent collaborative framework for text-to-sql. In Proceedings of the 31st International Conference on Computational Linguistics (COLING 2025), pp. 540557, 2025. Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. Mint: In The Twelfth Evaluating llms in multi-turn interaction with tools and language feedback. International Conference on Learning Representations, 2024. Hao Wen, Xinrui Wu, Yi Sun, Feifei Zhang, Liye Chen, Jie Wang, Yunxin Liu, Ya-Qin Zhang, and Yuanchun Li. Budgetthinker: Empowering budget-aware llm reasoning with control tokens. arXiv preprint arXiv:2508.17196, 2025. Shirley Wu, Michel Galley, Baolin Peng, Hao Cheng, Gavin Li, Yao Dou, Weixin Cai, James Zou, Jure Leskovec, and Jianfeng Gao. Collabllm: From passive responders to active collaborators. arXiv preprint arXiv:2502.00640, 2025. John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing and benchmarking interactive coding with execution feedback. Advances in Neural Information Processing Systems, 36:2382623854, 2023. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2074420757. Curran Associates, Inc., 2022. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. {$tau$}-bench: benchmark for underline{T}ool-underline{A}gent-underline{U}ser interaction in real-world domains. In The Thirteenth International Conference on Learning Representations, 2025. 15 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, Oleksandr Polozov, and Charles Sutton. Natural language to code generation in interactive data science notebooks. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 126173, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.9. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir R. Radev. Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In EMNLP, pp. 39113921. Association for Computational Linguistics, 2018. Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander R. Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter S. Lasecki, and Dragomir R. Radev. Cosql: conversational text-to-sql challenge towards cross-domain natural language interfaces to databases. In EMNLP/IJCNLP (1), pp. 19621979. Association for Computational Linguistics, 2019a. Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Jonathan Kraft, Vincent Zhang, Caiming Xiong, Richard Socher, and Dragomir R. Radev. Sparc: Cross-domain semantic parsing in context. In ACL (1), pp. 45114523. Association for Computational Linguistics, 2019b. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprint arXiv:1709.00103, 2017. Yifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine, Sainbayar Sukhbaatar, and Xian Li. Sweet-rl: Training multi-turn llm agents on collaborative reasoning tasks. arXiv preprint arXiv:2503.15478, 2025. BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions"
        },
        {
            "title": "APPENDIX CONTENTS",
            "content": "A Limitations Annotation Group Details B.1 Annotator Entrance Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Training Tutorials . B.3 Qualification Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Benchmark Design Principles Comparison with Related Benchmarks D.1 Task Comparison . . . D.2 Database Comparison . Evaluation Metrics E.1 Success Rate (SR) . . E.2 Normalized Reward . Test Scripts F.1 BI Queries . . F.2 DM Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Ambiguity and Follow-up Annotation Details G.1 User Query Ambiguity Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . G.2 Knowledge and Environmental Ambiguity Annotation . . . . . . . . . . . . . . . G.3 Ambiguity Chain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.4 User Query Ambiguity Taxonomy . . . . . . . . . . . . . . . . . . . . . . . . . . G.5 Follow-up Sub-Task Taxonomy . . . . . . . . . . . . . . . . . . . . . . . . . . . . Experiment Details H.1 Choice of PostgreSQL as the Evaluation Database System . . . . . . . . . . . . . H.2 Model Alias . . . . H.3 Experiment Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Action Space and Selection Patterns in a-Interact I.1 Action Space in a-Interact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I.2 Universal Cost Scheme for Custom Agents . . . . . . . . . . . . . . . . . . . . . I.3 Action Selection Patterns and Their Impact (Full Set) . . . . . . . . . . . . . . . . Performance on Different Ambiguity Types 17 19 19 19 20 20 20 20 22 22 22 23 23 23 24 24 25 25 27 28 28 29 29 29 32 33 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Experiments on BIRD-INTERACT-LITE Error Analysis User Simulator Design Details Evaluating the Function-Driven User Simulator N.1 UserSim-Guard: Benchmark for Simulator Robustness . . . . . . . . . . . . . . N.2 Experimental Setup . N.3 Results and Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Pathways to Effective Communication Human Evaluation of Dataset Quality Prompts Q.1 System Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Q.2 User Simulator Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 35 35 36 36 37 39 40 40 44 18 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Figure 7: Examples of training materials by screenshots for BIRD-Interact annotators."
        },
        {
            "title": "A LIMITATIONS",
            "content": "Our work has centered on the text-to-SQL domain, but we believe our proposed interaction evaluation is not inherently limited to it. Instead, it can cover generalizable human-AI collaboration. Exploring the adaptation of this framework to other generative domains, such as Python code synthesis or API call generation, is promising direction for future research. But at this time, we think its representative scenario since it also features long-context, hierarchical knowledge, and AI coding problem."
        },
        {
            "title": "B ANNOTATION GROUP DETAILS",
            "content": "To ensure the high quality of annotations for the BIRD-INTERACT benchmark, we designed rigorous, multi-stage process for annotator selection, training, and qualification. This process aimed to ensure that all annotators possessed strong SQL expertise and followed consistent, reproducible workflow. B.1 ANNOTATOR ENTRANCE TEST All potential annotators were required to complete structured training program before contributing to the benchmark. We began by recruiting pool of 33 candidates, including students, engineers, and text-to-SQL researchers with prior database experience. Each candidate underwent weeklong training period consisting of tutorials and guided exercises (detailed below), followed by qualification exam. This exam tested proficiency in SQL generation, schema understanding, and annotation of interactive tasks. Only candidates who achieved passing score of at least 90% were admitted as official annotators, resulting in final team of 12 highly qualified contributors. B.2 TRAINING TUTORIALS Candidates participated in an intensive tutorial program covering essential aspects of interactive text-to-SQL, including: Database environment setup Database schema analysis and comprehension Reproduction of single-turn text-to-SQL examples from LIVESQLBENCH Ambiguity taxonomy, injection procedures, and clarification annotation Follow-up sub-task taxonomy and construction, with solution SQL and test scripts Solution validation and evaluation script development BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions The tutorials contain the DB sandbox, code suite, detailed procedures, examples, and hands-on exercises that mirror the interactive feature of real-world SQL tasks. Some parts of the tutorials are shown in Figure 7. Annotators were introduced to the full annotation workflow required for the creation of the BIRD-INTERACT benchmark. B.3 QUALIFICATION TEST Following the tutorial phase, candidates were required to complete qualification assignment consisting of 20 representative interactive text-to-SQL tasks. For each task, candidates were asked to: 1. Reproduce the environment and baseline single-turn text-to-SQL task. 2. Inject ambiguity into the task and annotate the corresponding unique clarification, ensuring that with clarification the original clear task could be recovered. 3. Create follow-up sub-task and annotate it with solution SQL and test scripts. 4. Validate that the solution SQLs passed all annotated test scripts in sequence across sub-tasks. 5. Document their approach and provide validation log. Only candidates who successfully completed the assignment with satisfactory quality were approved as annotators. This stringent qualification process ensured that all annotators met the high standards required for building robust and trustworthy benchmark. The overall success rate was approximately 90%, demonstrating the effectiveness of the tutorial materials and training program in preparing candidates for interactive text-to-SQL annotation. All annotators contributing to the final release of BIRD-INTERACT passed this qualification process."
        },
        {
            "title": "C BENCHMARK DESIGN PRINCIPLES",
            "content": "Our design philosophy for BIRD-INTERACT is guided by two core principles: incorporating realistic interaction challenges and ensuring robust, reproducible evaluation. Realistic Interaction Challenges. To mirror the complexity of real-world data analysis, we establish scenarios where interaction is indispensable for task completion. This is achieved through two mechanisms. (1) Ambiguity: We deliberately inject different types of ambiguityspanning user queries, knowledge bases, and database environmentssuch that tasks cannot be solved correctly without clarification. Resolving these ambiguities often requires multi-turn exchanges, forcing systems to decide when to query the user, consult the HKB, or explore the database. This design captures the iterative, source-dependent nature of ambiguity resolution. (2) Contextual Follow-ups: Every task includes subsequent, related query that requires the system to reason over the preceding conversation, the interaction history, and, critically, potentially changed database state. Reliable and Reproducible Evaluation. We ensure the reliability and reproducibility of evaluation from two key aspects. (1) Reference-based disambiguation: to avoid cases where certain ambiguities lack explicit annotations, the simulator is additionally provided with the reference SQL, allowing it to generate accurate clarifications when necessary. While in real-world scenarios, real users may only have vague initial goals without an answer when making request, this pragmatic design choice enhances evaluation reliability. (2) Simulator robustness and reproducibility: we employ two-stage function-driven design to safeguard against adversarial manipulation and ground-truth leakage."
        },
        {
            "title": "D COMPARISON WITH RELATED BENCHMARKS",
            "content": "D.1 TASK COMPARISON Table 4 compares BIRD-INTERACT with existing text-to-SQL and interactive benchmarks across multiple dimensions. We categorize related work into four groups: SQL Generation, Ambiguity 20 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Table 4: Data statistics of features in BIRD-INTERACT compared to the evaluation set of related benchmarks. # Avg Turns: Number of User-System interactions by unfolding the models interaction trajectory. # Toks./Output: Average number of tokens in the reference output; / indicates benchmarks without reference output. Dynamic User: Whether the benchmark supports real-time user interaction (vs. static offline datasets). Dynamic Env State: Whether the database or environment state can be modified during interaction. Amb. Sources: Sources of ambiguity in user queries or environments. LLM + Guard means LLM as user simulator with Guard mechanism to make actions more controllable. []: Results taken from publicly available Spider 2.0 Lite Gold SQL. Category Dataset # Tasks # Avg Turns # Toks. / Output Dynamic User Dynamic Env State Amb. Sources Ext. Knowledge SQL Generation Ambiguity Handling Static Conversation Interactive Benchmark Our Benchmark KaggleDBQA (Lee et al., 2021) WikiSQL (Zhong et al., 2017) Spider (Yu et al., 2018) Spider-2.0-SQL(Lei et al., 2025) Spider-2.0-DBT (Lei et al., 2025) BIRD-SQL (Li et al., 2023b) BIRD-Critic (Li et al., 2025d) BIRD-Mini-Dev (Li et al., 2023b) AMBROSIA (Saparina & Lapata, 2024) AmbiQT (Bhaskar et al., 2023) When Prompts Go Wrong (Larbi et al., 2025) InfoQuest (de Oliveira et al., 2025) CondAmbigQA (Li et al., 2025f) VQ-FocusAmbiguity (Chen et al., 2025a) SparC (Yu et al., 2019b) CoSQL (Yu et al., 2019a) CHASE (Guo et al., 2021) MT-Bench (Zheng et al., 2023) MINT (Wang et al., 2024) InterCode (Yang et al., 2023) τ -bench (Yao et al., 2025) WebShop (Yao et al., 2022) BIRD-INTERACT-LITE BIRD-INTERACT-FULL 272 15,878 2,147 547 78 1,534 1,100 1,500 1,277 3,000 300 1,000 200 5, 422 1300 2,494 160 586 2,208 165 500 300 600 1 1 1 1 1 1 1 1 1 1 1 3.76 1 1 1 1 1 3.12 1 7.08 1 7.46 7.83 24.28 15.59 30.18 412.37 / 50.01 109.66 63.56 88.36 31.72 55.71 / 44.94 1.54 34.58 39.34 43.71 37.58 64.97 40.35 / / LLM Offline Offline Offline LLM LLM User User Description User + Persona Query + Docs Visual 365.14 252.21 LLM + Guard LLM + Guard User + Env User + Env Table 5: Comparison of released databases across benchmarks. Benchmark # DBs # Col./DB KB Doc. License Cost BIRD-SQL (Li et al., 2023b) Spider (Yu et al., 2018) WikiSQL (Zhong et al., 2017) KaggleDBQA (Lee et al., 2021) SEDE (Hazoom et al., 2021) Spider 2.0 (Lei et al., 2025) BIRD-INTERACT-LITE BIRD-INTERACT-FULL 15 40 5230 8 1 632 18 54.2 27.1 6.3 23.4 212 743.5 126.9 91.4 CC BY-SA 4.0 CC BY-SA 4.0 BSD 3-Clause CC BY-SA 4.0 Apache License Free Free Free Free Free Restricted May incur cost CC BY-SA 4.0 CC BY-SA 4.0 Free Free Figure 8: Distribution of advanced SQL features in BIRD-INTERACT. Handling, Static Conversation, and Interactive Benchmarks. This taxonomy highlights the broader coverage and higher difficulty of BIRD-INTERACT. First, unlike most SQL generation benchmarks that evaluate single-turn queries or pre-collect static conversation history, BIRD-INTERACT integrates ambiguity handling, dynamic multi-turn interactions, and dynamic environments in unified framework. Our tasks require systems not only to generate SQL but also to actively engage in clarification and reasoning with both user and environment. Second, the # Avg Turns of BIRD-INTERACT is around 7.5 per task, significantly higher than most prior benchmarks, which typically unfold into one or few turns. Third, the # Toks./Output of BIRD-INTERACT is substantially larger (252365 tokens on average), indicating that our SQL queries are longer and structurally more complex. Fourth, unlike static conversational benchmarks with offline conversation transcripts, BIRD-INTERACT features Dynamic User during evaluation. Our two-stage function-driven user simulator ensures robustness by mapping clarification requests into symbolic actions before generating responses. This design reduces ground-truth leakage and adversarial manipulation, while preserving naturalness and diversity of interaction. Fifth, BIRD-INTERACT introduces multiple ambiguity sources. Whereas most prior datasets only consider ambiguity at the user query level, we additionally inject knowledge and environmental ambiguities. This requires systems to strategically alternate between user clarification and environment exploration to recover the true intent. Taken together, these characteristics establish BIRD-INTERACT as the first benchmark that jointly stresses SQL generation, ambiguity resolution, and dynamic interaction with both users and envi21 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions ronments. Compared to existing work, it sets higher bar for evaluating interactive text-to-SQL systems. D.2 DATABASE COMPARISON Table 5 compares the databases used in BIRD-INTERACT with those of other widely used text-to-SQL benchmarks. Compared to most prior benchmarks, our databases span diverse domains and contain more columns per database, resulting in more complex and richer schemas. All databases are paired with knowledge base documents. In terms of licensing, BIRD-INTERACT builds on the open-source LIVESQLBENCH (BIRD-Team, 2025) datasets released under CC BY-SA 4.0, ensuring unrestricted academic and industrial use. This licensing framework ensures unrestricted accessibility for both academic research and industrial applications. Spider 2.0 represents another high-quality benchmark with large data resources, but its reliance on data primarily sourced from BigQuery and Snowflake Marketplace introduces licensing complexities that may limit direct further academic adaptation and potentially incur usage costs for researchers."
        },
        {
            "title": "E EVALUATION METRICS",
            "content": "E.1 SUCCESS RATE (SR) The Success Rate (SR %) is our primary online evaluation metric, measuring whether each sub-task is solved correctly during interaction. Let denote the total number of tasks, where each task in BIRD-INTERACT consists of exactly two sub-tasks, denoted qi,1 and qi,2. Each sub-task qi,j is annotated with ground-truth SQL solution σ i,j and set of executable test cases Ti,j. predicted SQL σi,j is considered correct if it passes all test cases in Ti,j. The success rate for the j-th sub-task across all tasks is defined as: SRj = 1 (cid:88) i= I(cid:2)Ti,j(σi,j) = True(cid:3), (2) where I[] is the indicator function that equals 1 if the prediction is correct and 0 otherwise. In reporting, we provide SR separately for the two sub-tasks: (1) qi,1, the ambiguous priority sub-task, and (2) qi,2, the follow-up sub-task. To assess functional correctness, we rely on executable test scripts that validate predicted SQL against the annotated ground truth. Details of the test scripts are provided in Appendix F. E.2 NORMALIZED REWARD To capture the relative importance of different sub-tasks (e.g., success on the initial ambiguous subtask is critical for continuing the interaction) and to distinguish system behaviors such as first-attempt success versus post-debugging success, we propose Normalized Reward metric. It is calculated by the average reward across all tasks. This metric is reported in addition to the sub-task-level success rates described in Section 2. Formally, with total tasks, the normalized reward is calculated as = (cid:80) ri 100 = (cid:80) (cid:80) j{1,2} ri,j 100, where the ri, rij is the reward of the task and the sub-task of task i. In the c-Interact setting, to distinguish first-attempt and post-debugging solutions, the reward is defined by: ri,1 = ri,2 = 0.7 0.5 0 0.3 0.2 0 if 1st sub-task is solved without debugging if 1st sub-task is solved with debugging otherwise if 2nd sub-task is solved without debugging if 2nd sub-task is solved with debugging otherwise 22 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions In the a-Interact setting, since the interaction flow is not fixed, e.g. the debugging times, the reward only considers the pass or fail of each sub-task: ri = 1.0 0.7 0 if both sub-tasks are passed if only the 1st sub-task is passed otherwise"
        },
        {
            "title": "F TEST SCRIPTS",
            "content": "We check sub-task correctness using executable test scripts. For BI sub-tasks (analytical queries), we use default soft exact-match (EM) script that normalizes benign SQL differences (e.g., removing comments, redundant DISTINCT, or rounding) and compares execution results between the predicted SQL and the annotated solution SQL under task-specific conditions. For DM sub-tasks (data manipulation or state-changing operations), we use manually annotated, case-by-case verification scripts that assert task-specific postconditions of the database. F.1 BI QUERIES The default test script cleans predictions/solutions (e.g., remove comments, DISTINCT, ROUND wrappers) and then compares execution results between the predicted SQL and the annotated solution SQL via configurable comparator ex_base with conditions map (e.g., order:false to ignore row ordering if the task does not require ordering): def test_case_default(pred_sqls, sol_sqls, db_name, conn, conditions=None): = remove_comments(sol_sqls) \"\"\"Default test_case: pytest-style assertion.\"\"\" pred_sqls = remove_comments(pred_sqls) sol_sqls pred_sqls = remove_distinct(pred_sqls) pred_sqls = remove_round(pred_sqls) sol_sqls sol_sqls = remove_distinct(sol_sqls) = remove_round(sol_sqls) result = ex_base(pred_sqls, sol_sqls, db_name, conn, conditions) assert result == 1, f\"ex_base returned {result} but expected 1.\" return result F.2 DM QUERIES DM sub-tasks may involve DML/DDL, stored procedures, or functions and do not always return result set. We therefore use case-specific scripts that execute the predicted SQL and then assert task-specific postconditions. Depending on the sub-task, the test script may (i) check the return value of verification query (e.g., calling created function/view), (ii) inspect the presence/shape/content of created artifacts (tables, indexes, constraints), or (iii) compare targeted state properties (e.g., row counts, key invariants). For example, this is one test case for the user sub-task in Figure 1: def test_case(pred_sqls, sol_sqls, db_name, conn): execute_queries(pred_sqls, db_name, conn) verify_sql = \"SELECT * FROM rank_urgent_care()\" pred_query_result = execute_queries(verify_sql, db_name, conn) actual = pred_query_result[0] expected = [ (101, Ancient Scroll, Decimal(7.20)), (102, Bronze Vase, (103, Stone Tablet, Decimal(6.85)), Decimal(6.50)), ] assert len(actual) == len(expected) assert actual == expected return True 23 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions AMBIGUITY AND FOLLOW-UP ANNOTATION DETAILS G.1 USER QUERY AMBIGUITY ANNOTATION core step in constructing interactive scenarios is the deliberate introduction of ambiguity into originally unambiguous single-turn user queries. Our annotation process ensures that systems cannot succeed without active clarification, thereby reflecting the uncertainties inherent in real-world human database interactions (Saparina & Lapata, 2024; Dong et al., 2025; Min et al., 2020; Bhaskar et al., 2023). Figure 9 shows the distribution of annotated ambiguities across the dataset. Two basic ambiguity categories. We distinguish between two fundamental categories that guide annotation: Intent-level ambiguity arises directly from user language, where the request is vague, underspecified, or missing critical details (e.g., find elderly people without defining the age threshold). If not resolved, intent-level ambiguity can severely degrade user experience and lead to erroneous SQL. Clarifying such ambiguities is the primary requirement for an LLM to faithfully capture user intent. Implementation-level ambiguity occurs when the users high-level intent is clear, but the SQL execution admits multiple valid formulations, such as numeric precision, ranking direction, or null handling. While less disruptive to comprehension, resolving these cases improves SQL precision and alignment with user expectations. For each category, we provide annotators with structured taxonomy including type definitions, annotation conditions, and examples, ensuring systematic and consistent ambiguity injection, as outlined in Appendix G.4. Ambiguity and clarification sources. Each injected ambiguity is paired with unique clarification represented by key SQL snippet from the ground-truth SQL rather than natural language text. For instance, the ambiguous query find elderly people is linked to the clarification snippet WHERE age > 80. This design guarantees reproducibility: the user simulator can reliably ground clarifications in SQL semantics, while still generating diverse natural-language paraphrases during interaction. Quality control. To maintain benchmark reliability, annotators follow strict checklist: (1) Necessity of clarification: each ambiguous query must be unsolvable without clarification, ensuring genuine reliance on interaction. (2) Completeness after clarification: once clarification is provided, the information must suffice for an expert to reconstruct the exact solution SQL. This guarantees that injected ambiguities are both necessary and recoverable, enabling reproducible evaluation. G.2 KNOWLEDGE AND ENVIRONMENTAL AMBIGUITY ANNOTATION In addition to user query modifications, we also introduce ambiguities that arise from missing or noisy external resources. These require systems to reason dynamically with both knowledge bases and database environments. We annotate them in two categories: knowledge ambiguities and environmental ambiguities (Saparina & Lapata, 2024; Dong et al., 2025; Min et al., 2020; Huo et al., 2025; Bhaskar et al., 2023). Knowledge Ambiguities. We introduce incompleteness into the hierarchical knowledge base (HKB) to simulate the deployment conditions where documentation is often partial or fragmented. We distinguish two subtypes: One-shot knowledge ambiguity: individual knowledge entries are masked without involving dependent chains. For example, if the definition of CPI is omitted, the system cannot directly calculate indices that rely on it. These isolated gaps require the system to explicitly ask the user for missing facts. Knowledge chain breaking: intermediate nodes in multi-hop reasoning chains are masked, disrupting dependencies across concepts. Consider the chain \"urgent care\" \"AVS\" \"IF/CPI\" shown in Figure 2. By masking the intermediate node AVS, the inferential link is broken: the query becomes ambiguous, and the system must first request clarification from the user before proceeding to the knowledge IF/CPI. 24 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Database Inconsistencies. LIVESQLBENCH databases already contain noise, including string fields mixing numeric values with units, inconsistent column naming across related tables, and NULL values in critical fields. Moreover, their SQL tasks already involve this database noise, providing foundation for data quality challenges. We deliberately leverage these existing inconsistencies as evaluation scenarios. When constructing subsequent sub-tasks, we also intentionally involve these noisy columns to increase the complexity of multi-turn interactions. These require systems to handle data quality issues through appropriate querying strategies and robust SQL patterns. As in user query ambiguities, each ambiguity is also paired with ground-truth SQL fragment that acts as the clarification source. G.3 AMBIGUITY CHAIN We combine those individual ambiguities with different types into ambiguity chains that require Multi-Hop Ambiguity Resolution, which integrates three aspects: 1. Nested ambiguities. Clarifications themselves may require further explanation, requiring multi-stage resolution. Not all ambiguities are visible at the surface level of the query; some unfold only when earlier uncertainties are addressed. 2. Multiple clarification sources. Each ambiguity may require information from different sources. In particular, the system must decide whether to seek clarification from the user or to consult the environment (e.g., knowledge base, schema, or documentation). 3. Clarification flows. We define three canonical transition types that characterize how clarification flows across sources: User User: an initial user clarification still requires further follow-up inquiry to the user. User Environment: the users clarification points to auxiliary information that must be retrieved from the environment, e.g. KB. Environment User: the system first consults the environment, but the retrieved knowledge is incomplete or underspecified, necessitating return to the user for explanation. These transitions can compose into multi-hop clarification sequences such as User Environment User. For example, as shown in Figure 1, there are two ambiguities: (1) the vague query need urgent care is clarified as ranked by AVS (2) but because the KB entry for AVS is masked, the system must return to the user for further clarification. To implement such cases, (1) annotated clarification snippets are intentionally underspecified, and (2) some KB nodes in HKB are masked to simulate missing documentation. Together, these mechanisms ensure that successful resolution requires multi-stage reasoning and source selection. G.4 USER QUERY AMBIGUITY TAXONOMY We distinguish between two fundamental categories of user query ambiguity that guide annotation: Intent-Level Ambiguity Types. Intent-level ambiguity arises directly from user language, where the request is vague, underspecified, or missing critical details (e.g., find elderly people without defining the age threshold). If not resolved, intent-level ambiguity can severely degrade user experience and lead to erroneous SQL (Saparina & Lapata, 2024; Li et al., 2025f). We summarize six types of user query ambiguity in Table 6, according related works (Saparina & Lapata, 2024; Li et al., 2025f; Dong et al., 2025; Wang et al., 2020; Min et al., 2020; Bhaskar et al., 2023; Huo et al., 2024; Floratou et al., 2024; Huang et al., 2023; Ding et al., 2025) and guide the annotators to inject them into unambiguous user queries: (1) Lexical Ambiguity from tokens with multiple meanings, (2) Syntactic Ambiguity from multiple valid grammatical structures, (3) Semantic Ambiguity from vague phrasing (e.g., \"recent\"), (4) Schema Linking Ambiguity from unclear schema references, (5) Query Intent Ambiguity where user goals (e.g., \"top\") are underspecified, and (6) Knowledge Linking Ambiguity involving implicit references to external knowledge. The performance of different types is shown in Figure 14. 25 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Figure 9: Ambiguity types distribution. Table 6: Intent-Level User Query Ambiguity Taxonomy in BIRD-INTERACT Ambiguity Type Definition Example Lexical Ambiguity token has multiple meanings or senses within the query context. Show bills Bills could mean invoices, legislation, or billing records. Syntactic Ambiguity The sentence has multiple valid grammatical structures leading to different interpretations. Semantic Ambiguity The query is grammatically correct but semantically vague, lacking details necessary for precise interpretation. Get orders for customers from 2020 Are we filtering orders or customers by year? Recent transactions The time frame for recent is unspecified. Schema Ambiguity Linking Ambiguity in mapping query term to the correct schema element due to multiple plausible candidates. List users by status Status to account_status, could refer login_status, etc. Query Intent Ambiguity Uncertainty about the users intended operation or ranking criterion. Knowledge Linking Ambiguity referenced concept exists in the external knowledge base, but the querys link to the knowledge is implicit or unclear. Show the top customers Top may refer to revenue, number of orders, or frequency. Get Impact Score Impact Score refers to Artist Impact Score in the KB. Implementation-Level Ambiguity Types. Implementation-level ambiguity occurs when the users high-level intent is clear, but the SQL execution admits multiple valid formulations, such as numeric precision, ranking direction, or null handling. While less disruptive to comprehension than intentlevel ambiguity, resolving these cases improves SQL precision and alignment with user expectations. These ambiguities are annotated conditionally, i.e., only when the corresponding SQL operations are present in the ground-truth SQL. For each case, annotators identify the relevant SQL fragment and mark the corresponding clarification source. We summarize the following types: Decimal ambiguity. Annotated when the solution SQL applies rounding or numeric formatting. Example: ambiguous query show average score, clarified query show average score in two decimals, with the solution SQL using ROUND(AVG(score), 2). Join ambiguity. Annotated when the solution SQL requires non-default join semantics (e.g., LEFT JOIN, FULL OUTER JOIN). Example: ambiguous query list all customers and their orders, clarified query list all customers and their orders, even if they have no records, with the solution SQL using LEFT JOIN. Distinct ambiguity. Annotated when the SQL solution contains the DISTINCT keyword. Example: ambiguous query get all product names, clarified query get all different product names, with solution SQL SELECT DISTINCT product_name. Sort ambiguity. Annotated when the SQL solution applies an ORDER BY clause without LIMIT. Example: ambiguous query show recent purchases, clarified query show recent 26 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions purchases sorted by time, with solution SQL including ORDER BY purchase_time DESC. Null ambiguity. Annotated when the SQL solution contains null-handling operations (e.g., COALESCE, ISNULL). Example: ambiguous query count users by region, clarified query count users by region, treating null as 0, with solution SQL COUNT(COALESCE(region, 0)). Rank ambiguity. Annotated when ranking functions are applied in the solution SQL (e.g., ROW_NUMBER, DENSE_RANK). Example: ambiguous query show top customers with ranks of revenue, clarified query show top customers with ranks of revenue; if tied, assign the same rank, with SQL using DENSE_RANK(). Divide-by-zero ambiguity. Annotated when the SQL solution explicitly handles the case of dividing by zero. Example: ambiguous query show the ratio of passed to total exams, clarified query show the ratio of passed to total exams, treating cases with zero total as 0, with solution SQL using CASE WHEN total=0 THEN 0 ELSE passed/total END. These annotations ensure that implementation-level ambiguities are reproducible and systematically linked to concrete SQL constructs. By marking such cases only when relevant SQL operations are present, we preserve annotation consistency while enriching the benchmark with the challenges of SQL details in implementation. Table 7: Implementation-Level User Query Ambiguity Types in BIRD-INTERACT Ambiguity Type Annotation Condition Example Transformation Decimal Ambiguity ROUND function is used in solution SQL Join Ambiguity Non-default join (e.g., LEFT JOIN) is used in solution SQL \"Show average score in 2 decimal\" \"Show average score\" \"Show all customers and their orders even though they dont have records\" \"Show all customers and their orders\" Distinct Ambiguity DISTINCT keyword is used in solution SQL \"Get all different product names\" \"Get all product names\" Sort Ambiguity Null Ambiguity Rank Ambiguity ORDER BY is used without LIMIT in solution SQL \"Show recent purchases sorted by time\" \"Show recent purchases\" Solution SQL contains null handling operations (e.g., COALESCE, ISNULL) \"Count users by region, treat null as 0\" \"Count users by region\" ranking func- (e.g., ROW_NUMBER, RANK, Solution SQL uses tions DENSE_RANK) Divide-by-zero Ambiguity Solution SQL must handle division by zero explicitly \"Show top customers with ranks of revenue. If they are tied, give them the same rank number.\" \"Show top customers with ranks of revenue.\" \"Show the ratio of passed to total exams, treating cases with zero total as 0\" \"Show the ratio of passed to total exams\" G.5 FOLLOW-UP SUB-TASK TAXONOMY In addition to initial ambiguities, interactive scenarios require systems to handle diverse follow-up requests that extend or refine the analytical chain. We categorize follow-ups into six types  (Table 8)  according to related works (Yu et al., 2019b;a; Yin et al., 2023), covering constraint adjustments, topic pivots, attribute modifications, result-driven drill-downs, aggregation-based summarizations, and state-dependent follow-ups based on newly created objects. These follow-ups test whether evaluated systems can maintain context, adapt to evolving user needs and database, and produce coherent SQL across multiple turns. 27 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Table 8: Follow-up Sub-Task Taxonomy in BIRD-INTERACT Follow-up Type Description First Query Example Follow-up Example Constraint Change Tighten or relax filtering conditions. List employees hired in 2024. Only engineers. / Include 2023 as well. Topic Pivot Compare or switch entity values to explore alternatives. Sales of Product in 2023. What about Product B? Attribute Change Modify the requested attributes, metrics, or columns. Departments with >50 staff. Give their average salary. Result-based Aggregation State-Dependent Drill down, regroup, nest, or reformat based on the previous result set. List projects finished in 2023. For Apollo, show its budget. Request statistics, concatenations, counts, or Boolean checks (e.g., AVG, STRING_AGG, MAX FILTER, ARRAY_AGG+LIMIT, EXISTS). Final output is typically scalar, single row, or compact table. First query creates or modifies database object (e.g., table, view), thereby changing the database state, and the follow-up query operates on it. Show the top-10 artists by track count. Give me their names joined into single comma-separated string. table Create of employees with salary above 100k. From that table, list only engineers."
        },
        {
            "title": "H EXPERIMENT DETAILS",
            "content": "H.1 CHOICE OF POSTGRESQL AS THE EVALUATION DATABASE SYSTEM BIRD-INTERACT adopts PostgreSQL as the underlying database management system for evaluation. This choice is motivated by several key considerations: Enterprise Adoption and Feature Richness. PostgreSQL is among the most widely deployed opensource database systems in production environments, supporting advanced SQL features essential for complex analytics including window functions, CTEs, recursive queries, JSON processing, and user-defined functions. This enables evaluation on realistic, production-grade queries rather than basic patterns. Accessibility and Reproducibility. As an open-source system, PostgreSQL eliminates licensing costs and access barriers. Unlike proprietary cloud platforms (e.g., BigQuery, Snowflake) that may incur usage fees, PostgreSQL ensures any researcher can replicate our evaluation environment without financial constraints, enhancing long-term benchmark sustainability. Standards Compliance and Transferability. PostgreSQL maintains strong SQL standards adherence (SQL:2016) while providing well-documented extensions. This ensures evaluation results remain broadly applicable across database systems and that skills learned generalize beyond vendor-specific implementations. In summary, PostgreSQLs combination of real-world relevance, feature completeness, and unrestricted accessibility makes it optimal for evaluating interactive text-to-SQL systems under productionlike conditions. H.2 MODEL ALIAS The following aliases are used for the models in this work: Gemini-2.0-Flash: gemini-2-0-flash-001 DeepSeek-R1: deepseek-r1 GPT-4o: gpt-4o-2024-11-20 DeepSeek-V3: deepseek-chat O3-Mini: o3-mini-2025-01-31 Claude-Sonnet-3.7: claude-3-7-sonnet-20250219 28 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Qwen-3-Coder-480B: Qwen3-Coder-480B-A35B DeepSeek-Chat-V3.1: deepseek-chat-v3.1 Gemini-2.5-Pro: gemini-2-5-pro Claude-Sonnet-4: claude-sonnet-4-20250514 GPT-5: gptH.3 EXPERIMENT SETUP All experiments were conducted under deterministic decoding to ensure reproducibility. Specifically, we set temperature=0 and top_p=1 for all models. Each experiment was executed single time due to the high cost of commercial API calls and the deterministic nature of the outputs under these settings. For both c-Interact and a-Interact, the default user patience budget was set to 3, in addition to the required turns for ambiguity resolution, which equals the number of annotated ambiguities. In the Interaction Test-Time Scaling experiments, we considered patience values of 0, 3, 5, and 7 to evaluate robustness under varying interaction budgets. For a-Interact, the base budget was set to 6 to allow systems sufficient capacity to explore the environment and execute SQL queries before submitting. All model inferences were obtained directly from their official APIs or released checkpoints to ensure authenticity and consistency. For those models with reasoning capabilities, we set reasoning effort as default \"medium\". ACTION SPACE AND SELECTION PATTERNS IN a-INTERACT Table 9: Action space for the agent showing available actions, their environments, arguments, return values (as observation), and associated costs. Action Env. Arguments Return Value Cost execute DB get_schema DB get_all_column_meanings DB get_column_meaning DB get_all_external_knowledge_names DB get_knowledge_definition DB get_all_knowledge_definitions DB Query Result Database Schema All Columns Meanings sql - - table, column Column Meaning - knowledge - All Knowledge Names Knowledge Definition All Knowledge Definitions ask submit User question User sql User Clarification User Feedback 1 1 1 0.5 0.5 0.5 1 2 3 I.1 ACTION SPACE IN a-INTERACT Table 9 lists the nine actions an agent may invoke during the a-Interact evaluation. They naturally cluster into two families: Environment-only probes (cost 1). Seven low-cost calls let the agent inspect the database and hierarchical knowledge base (HKB) without engaging the user: execute: run candidate SQL statement and receive the result set; get_schema, get_all_column_meanings, get_column_meaning: expose structural and semantic metadata; get_all_external_knowledge_names, get_knowledge_definition, get_all_knowledge_definitions: retrieve business concepts from the HKB. Graduated costs (0.51) reflect the different levels of environmental resources consumed by these actions. Actions with smaller input and shorter output (get_column_meaning, etc.) are assigned lower cost (0.5), while broader retrievals that return substantially longer responses (get_all_column_meanings, get_schema, etc.) cost 1.0. 29 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Figure 10: System action distribution of systems under default setting (patience=3) on LITE set. P1 and P2 indicate the success rate for the first sub-task and the second sub-task. . Figure 11: System action distribution of systems under default setting (patience=3) on FULL set.. P1 and P2 indicate the success rate for the first sub-task and the second sub-task. . 30 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Figure 12: System action distribution of systems under default setting (patience=3) in heatmap on FULL set. . Figure 13: The interaction pattern of systems: action groups over turns under default setting (patience=3) on FULL set. . 31 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions User-mediated interactions (cost 2). When autonomous reasoning is insufficient, the agent can ask (cost 2): pose clarifying question to the user simulator; submit (cost 3): submit full SQL candidate to the user. The user will conduct the test-case evaluation and give the feedback to the agent. The higher penalties reflect the real-world expense of analyst involvement and encourage systems to reserve these calls for genuinely ambiguous scenarios or final validation. Overall, this action design balances expressive power with explicit cost signals, promoting strategic tool use, efficient information gathering, and minimal reliance on the user simulator. I.2 UNIVERSAL COST SCHEME FOR CUSTOM AGENTS We encourage users of our benchmark to develop their own agents with customized action spaces under the budget-constrained awareness testing evaluation. However, the action costs defined in our default setup may not directly apply to these new actions. To ensure fair and reproducible evaluation across agents with potentially different action spaces, we propose unified two-tier cost scheme within the a-Interact framework, which all customized agents are expected to follow when assigning costs to their actions. (1) Fixed-cost actions: If custom agent involves the actions of asking user, submitting SQL, or executing SQL, it should assign them the same costs as defined in our setup. User-side actions (ask=2, submit=3) are assigned globally fixed costs to reflect the intrinsic expense of human involvement, while execute is assigned fixed cost of 1.0 regardless of result size, since all agents interact with the same database interface and execution engine, ensuring consistent computational overhead and I/O behavior across implementations. (2) Token-aware actions: for custom environment actions that differ from those in our default setup (e.g., new action get_all_table_names), costs are determined dynamically based on the number of input and output tokens generated when calling the action, reflecting the relative amount of environmental resources consumed. According to our empirical statistics, we define token-aware rule applicable to all agents: if an environment action call incurs input tokens < 250 and output tokens < 1000, its cost should be set to 0.5; otherwise, it should be assigned cost of 1.0. This universal policy ensures fairness for agents using different action spaces. I.3 ACTION SELECTION PATTERNS AND THEIR IMPACT (FULL SET) Figure 11 and Figure 12 show how seven systems distribute their calls across the nine available actions  (Table 9)  on the FULL set. We summarize three observations: 1. Balanced strategies outperform extremes. The strongest performers, GPT-5 (29.2%) and Claude-Sonnet-4 (27.8%), adopt relatively balanced strategies. GPT-5 splits its budget almost evenly between environment probes (47%) and user involvement (ask+ submit: 52%). Claude-Sonnet-4 follows similar pattern, but with heavier emphasis on execute (29.9%) and lighter use of submit (20.0%). By contrast, O3-Mini expends an extreme 91% of its budget on user calls (36% ask, 55% submit) and allocates only 4% to execute, passing fewer than one-fifth of the first subtasks. On the other side, Qwen-3-Coder (48% execute) and DeepSeek-Chat (41% execute) are strongly execution-heavy and likewise underperform (P1 13.3% and 17.2%). This contrast suggests that successful agents must strike balance between exploring the environment and committing to user-facing actions, rather than over-investing in either extreme. 2. Submitting selectively helps, brute execution hurts. Across systems, the proportion of submit calls correlates positively with P1 (Pearson r0.41, Spearman ρ0.54), while the proportion of execute calls correlates negatively (Pearson 0.52, Spearman ρ 0.54). In practice, this means that repeatedly probing the database with tentative execute calls without consolidation tends to waste budget, whereas converging on grounded hypothesis and committing to submit improves success rates by getting the feedback from the user. For example, Claude-3.7-Sonnet and DeepSeek-Chat each keep submit usage below 17% and 11%, instead relying heavily on execute. At the other extreme, O3-Minis indiscriminate strategy of submitting more than half of all turns also 32 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Figure 14: Success Rate of LLMs on different ambiguity types over and a-Interact Modes. Figure 15: Success Rate of LLMs on linear and higher-order ambiguity over and a-Interact Modes. underperforms, confirming that it is not the absolute amount of submission that matters if ignoring the information from the user and environment. 3. Interaction patterns evolve over turns: explore first, then execute and submit. As shown in Figure 13, stronger systems (e.g., GPT-5, Claude-Sonnet-4) follow clear turn-by-turn strategy: in early turns they combine environment exploration with user clarifications to gather information, while in mid and later turns, they increase execute and submit calls to test and refine SQL. In contrast, weaker systems either submit too early (O3-Mini) or overuse execution without consolidation (Qwen3-Coder), leading to poorer performance. This demonstrates that performance depends not only on overall action mix but also on how actions are sequenced across interaction turns. Taken together, these results indicate that in the agentic c-Interact setting, performance depends less on sheer interaction times and more on how well system balances environment exploration with user interaction, commits to submissions at the right time, and avoids wasted budget."
        },
        {
            "title": "J PERFORMANCE ON DIFFERENT AMBIGUITY TYPES",
            "content": "Which knowledge missing type lead to more ambiguity? Linear or High-order? Figure 15 compares tasks where (1) the missing fact lies on simple, linear chain of the hierarchy with (2) those where the gap occurs within the chainwhat we term higher-order ambiguity. Linear cases correspond to one-shot knowledge gaps, while higher-order cases correspond to knowledge chain breaking in Section 3.2. In the scripted c-Interact setting, every model finds linear gaps easier: once 33 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Table 10: Success Rate and Final Normalized Reward of different models on BIRD-INTERACT-LITE. The success rate is cumulative; Reward* is the normalized reward. The values reported in c-Interact are after the debugging phase, and (+n) means the performance gained via debugging. Avg. Cost is the cost for one task on average in USD. Model Priority Question (Success Rate %) Overall DM BI Follow Ups (Success Rate %) DM BI Overall Reward* Avg. Cost c-Interact Text-to-SQL DeepSeek-V3 Qwen-3 DeepSeek-R1 Claude-Sonnet-3.7 Gemini-2.0-Flash GPT-4o O3-Mini 9.23 ( +1.54) 14.36 ( +2.56) 16.92 ( +3.08) 17.44 ( +3.59) 16.92 ( +3.59) 26.15 ( +7.18) 22.56 ( +1.54) 40.95 ( +6.67) 44.76 ( +2.86) 43.81 ( +6.67) 59.05 ( +1.90) 60.95 ( +7.62) 54.29 ( +6.67) 64.76 ( +3.81) 20.33 ( +3.33) 25.00 ( +2.67) 26.33 ( +4.33) 32.00 ( +3.00) 32.33 ( +5.00) 36.00 ( +7.00) 37.33 ( +2.33) 5.13 ( +1.54) 7.18 ( +0.51) 9.74 ( +2.05) 9.23 ( +2.05) 9.74 ( +1.03) 14.36 ( +1.03) 12.31 ( +0.00) 24.76 ( +1.90) 28.57 ( +4.76) 27.62 ( +3.81) 27.62 ( +7.62) 40.95 ( +3.81) 30.48 ( +1.90) 46.67 ( +0.95) 12.00 ( +1.67) 14.67 ( +2.00) 16.00 ( +2.67) 15.67 ( +4.00) 20.67 ( +2.00) 20.00 ( +1.33) 24.33 ( +0.33) a-Interact Text-to-SQL Gemini-2.0-Flash DeepSeek-R1 GPT-4o DeepSeek-V3 Qwen-3 O3-Mini Claude-Sonnet-3.7 8.21 6.67 12.31 11.79 7.18 14.87 22.05 44.76 47.62 43.81 44.76 49.52 45.71 56.19 21.00 21.00 23.33 23.33 22.00 25.67 34. 4.10 3.59 4.62 6.15 5.64 6.67 10.77 21.90 28.57 17.14 16.19 29.52 21.90 30.48 10.33 12.33 9.00 9.67 14.00 12.00 17.67 17.00 21.17 22.10 26.10 27.63 29.67 32.93 17.80 18.40 19.03 19.23 19.60 21.57 29.10 $ 0.01 $ 0.03 $ 0.08 $ 0.32 $ 0.04 $ 0.32 $ 0. 0.03 $ 0.09 $ 0.46 $ 0.06 $ 0.03 $ 0.08 $ 0.67 $ Figure 16: The performance under our proposed two-stage user simulator and baseline user simulator compared with human users on 100 sampled tasks. the prerequisite nodes are supplied, the remaining hop is almost mechanical. Insert break within the chain, however, and success drops sharply because the model must now infer which intermediate concept is still unknown before it can even formulate clarification. When we switch to the agentic a-Interact the story changes only for Claude-Sonnet-3.7, whose planning policy manages to erase the gap between the two categories; O3-Mini and Qwen-3 still stumble on higher-order cases. The trend suggests that the fundamental obstacle is not retrieval per se but the metacognitive step of localising the missing link in multi-step reasoning pathsomething only the most disciplined agent manages to do reliably. EXPERIMENTS ON BIRD-INTERACT-LITE Table 10 reports results on BIRD-INTERACT-LITE. We observe patterns consistent with those on the Full set: overall success rates and normalized rewards remain low, confirming the difficulty of interactive text-to-SQL even with simpler databases. Models that balance clarification with environment exploration, such as Claude-Sonnet-3.7, achieve higher SR and NR, while those relying too heavily on either execution or submission lag behind. Follow-up sub-tasks continue to pose greater challenge than priority queries, highlighting the difficulty of maintaining context across interactions. BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Figure 17: An example of an Abstract Syntax Tree (AST) for SQL query."
        },
        {
            "title": "L ERROR ANALYSIS",
            "content": "We conducted an error analysis by sampling 50 failed cases from our evaluation. We found that over 80% of the errors were caused by incomplete ambiguity resolution. In many cases, systems either asked too few clarification questions, asked none at all, or failed to detect the correct ambiguity and request the appropriate clarification. On average, each task in our benchmark contains around four ambiguities  (Table 1)  , but systems asked for clarification only about once per task (Figure 12). As result, most tasks were attempted with insufficient information, making it difficult to reach the correct solution. This highlights the current limitations of LLMs in humanAI collaborative ability. The remaining errors stem from common issues in text-to-SQL generation, such as SQL syntax mistakes, incorrect column selection, or misunderstanding of database constraints."
        },
        {
            "title": "M USER SIMULATOR DESIGN DETAILS",
            "content": "The main text describes our function-driven user simulator, which invokes the LOC() action to handle reasonable clarification questions that are not covered by pre-annotated ambiguities. This appendix details the Abstract Syntax Tree (AST)-based retrieval mechanism that allows the simulator to locate the relevant SQL fragment from the ground-truth (GT) query to answer such questions precisely. And the average cost for our function-driven user simulator is 0.03 USD per data. The primary challenge for the LOC() action is to find the specific part of the GT SQL that corresponds to the systems question without resorting to brittle keyword matching on the raw SQL string. An AST provides structured, hierarchical representation of the SQL query that is ideal for this task. Our retrieval process consists of three main steps: Parsing, Node Matching, and Contextual Snippet Extraction. 1. SQL Parsing into an AST. As first step, the ground-truth SQL query is processed by robust SQL parser (e.g., based on libraries like sqlglot) to generate an AST. As illustrated in Figure 17, this tree deconstructs the query into its fundamental syntactic components. Each node in the tree represents part of the query, such as clause (SELECT, FROM, WHERE), function (COUNT(), AVG()), an identifier (column or table names), an operator (=, >), or literal value (USA, 2023). This hierarchical structure makes every component of the query individually addressable. 2. Node Matching via Semantic Search of LLMs. With the AST generated, the next step is to identify the node(s) most relevant to the systems clarification question. To achieve this, we flatten the AST by traversing it and creating list of all its nodes. This approach is far more robust than simple keyword matching, as it can capture relationships like \"how many\" matching COUNT() or \"most recent\" matching an ORDER BY ... DESC clause. This AST-based method ensures that the LOC() function can reliably ground its responses in the GT SQL, providing accurate and contextually relevant information without leaking the entire query. EVALUATING THE FUNCTION-DRIVEN USER SIMULATOR To empirically validate the effectiveness of our proposed function-driven user simulator, we conduct comprehensive evaluation focused on its robustness and reliability. We first introduce new 35 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions benchmark, UserSim-Guard, specifically designed to challenge user simulators. We then present our experimental setup and report the results, comparing our approach against standard baseline. N.1 USERSIM-GUARD: BENCHMARK FOR SIMULATOR ROBUSTNESS To enable systematic evaluation of simulator performance, we constructed UserSim-Guard, manually curated dataset containing 1,989 challenging questions. Construction Methodology. The construction of UserSim-Guard was carried out by team of 7 trained annotators with expertise in SQL and natural language. To ensure data quality and diversity, we implemented rigorous annotation protocol. The dataset is structured around three categories of system clarification requests, designed to probe different aspects of simulators capabilities: AMB (Annotated Ambiguity): For this category, annotators were tasked with formulating natural language questions based on the pre-annotated ambiguities present in the BirdInteract-Lite benchmark. These questions directly test the simulators ability to correctly leverage the provided ambiguity annotations. LOC (Localizable Information): This category contains reasonable clarification questions that are not covered by the pre-annotated ambiguities. Annotators were instructed to carefully examine the ground-truth SQL query and identify potential points of confusion (e.g., specific column choices, formatting preferences, or sub-component logic) and craft questions accordingly. The answers to these questions can be located and inferred from the ground-truth SQL. UNA (Unanswerable): To test the simulators safety and adherence to its role, this category includes questions that are intentionally inappropriate or attempt to solicit privileged information. Annotators were prompted to formulate queries that directly ask for the ground-truth SQL, the database schema, or step-by-step guidance for solving the problem. robust simulator should refuse to answer such questions. Furthermore, to investigate the simulators sensitivity to different interaction styles, we instructed annotators to phrase each question in three distinct styles: Concise (terse and keyword-focused), Normal (standard conversational language), and Verbose (descriptive and context-rich). Quality Control. To ensure the highest data quality, we employed multi-stage quality control process. Each question-action pair in UserSim-Guard was annotated using double-blind, \"backto-back\" annotation scheme. Specifically, each data point was independently created by one annotator and then validated by second annotator. Any disagreements between the two annotators were resolved by third, senior annotator who made the final adjudication. This process minimizes individual bias and errors. We measured the inter-annotator agreement (IAA) using Fleiss Kappa, achieving score of 0.92, which indicates substantial agreement among our annotators and confirms the reliability of our labels. N.2 EXPERIMENTAL SETUP Models and Baselines. We evaluate our function-driven user simulator against baseline simulator that directly generates responses using single-pass LLM prompt. To ensure fair comparison, both our method and the baseline are implemented using two state-of-the-art large language models as backbones: Gemini-2.0-Flash and GPT-4o. Evaluation Framework. To provide an objective and comprehensive observation of different user simulator mechanisms, we designed robust evaluation framework using LLMs-as-Judge. This approach allows for nuanced assessment of response quality beyond simple string matching. To mitigate potential self-enhancement bias, we employed two powerful and independent models, Qwen-2.5-72B and Llama-3.1-70B, as evaluators. For each generated response from simulator, the LLM judges were asked to perform multiplechoice classification task. This format was chosen to mitigate bias of LLM-as-judge (Gu et al., 2024), reduce ambiguity, and create more differentiated assessments compared to open-ended feedback. The options were: BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions A. Perfect: The response correctly and accurately answers the question without revealing any inappropriate information. It is helpful and natural. B. Acceptable: The response is functionally correct and does not leak information, but it might be slightly unnatural, too brief, or could be phrased more helpfully. C. Incorrect: The response is factually wrong, fails to answer the question, leaks groundtruth information (especially for UNA questions), or is otherwise inappropriate. response is considered failure only if it is classified as C. For reporting purposes, we consider both and as correct. To ensure the reliability of our results, we adopt strict consistency-based evaluation: response is marked as correct only if both LLM judges independently classify it as either or B. We report the final Accuracy, which is the proportion of responses deemed correct under this consistency rule. N.3 RESULTS AND ANALYSIS Our analysis reveals significant reliability concerns with conventional user simulator designs, which are substantially mitigated by our function-driven approach. As shown in Figure 6, the contrast is most striking when handling UNA (Unanswerable) questions. Baseline user simulators consistently fail to implement necessary safeguards, often leaking groundtruth details or providing improper guidance. This leads to high failure rate of over 34% for both model backbones. In contrast, our proposed function-driven approach demonstrates substantially improved reliability. By first classifying the intent of the request and invoking the UNA() function, it correctly rejects inappropriate questions, with only 5.9% of its responses falling into problematic categories. This represents significant improvement in user simulator robustness. Table 11 presents more detailed breakdown of accuracy across all question categories. We observe that LLMs themselves already perform well on the first two categories (AMB and LOC), achieving over 90% accuracy even with the baseline approach. However, they struggle significantly with UNA (unanswerable) questions, where the baseline simulators fail in over 34% of cases for both backbone models. In contrast, our function-driven approach substantially mitigates this weakness, maintaining over 93% accuracy on UNA questions, which is great improvement that confirms the observations from Figure 6. This demonstrates that while LLMs can naturally handle straightforward clarification tasks, they require explicit structural constraints to avoid inappropriately answering questions that should be refused. Our two-stage design enforces such constraints by first identifying the question type before generating response, ensuring the simulators behavior remains predictable, controllable, and aligned with the goal of providing fair and realistic user feedback without leaking ground-truth information. Table 11: Accuracy (%) of user simulators on the UserSim-Guard benchmark. Our functiondriven approach consistently outperforms the baseline across all categories. Accuracy is reported based on the consistency of two independent LLM judges. Backbone Gemini-2.0-Flash Baseline Simulator Ours (Function-Driven) GPT-4o Baseline Ours (Function-Driven) AMB Acc. LOC Acc. UNA Acc. 93.57 94.76 90.72 98.81 95.52 96.75 96.30 98.88 66.93 93. 63.45 94."
        },
        {
            "title": "O PATHWAYS TO EFFECTIVE COMMUNICATION",
            "content": "Motivated by the Memory Grafting results, which highlight the importance of communication skills for interactive text-to-SQL systems, we proceed to deeper analysis. In this section, we investigate the specific communication patterns and dialogue strategies that lead to improved task performance. Through an in-depth analysis of high-quality interaction data, we identify recurring and highly effective pattern we term the \"funnel effect.\" This is characterized by series of progressively 37 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Figure 18: Case study of effective communication under c-Interact. deepening inquiries that begin with users relatively broad and ambiguous initial intent, then gradually narrow the scope and clarify key details, and ultimately converge into clear and executable analysis plan. We deconstruct this pattern into three primary phases. Initial Interaction Phase: Concept Clarification and Scoping. In the initial stage of high-quality dialogues, the Large Language Model (LLM) tends to pose questions aimed at clarifying core concepts. This allows it to quickly identify ambiguous areas within the users query and proactively initiate dialogue for disambiguation. Such questions are highly targeted and efficient, for example: How would you like to define the \"interference score\" for each telescope?, or Could you clarify what you mean by machines that are always breaking down? Concurrently, the model does not passively await precise descriptions from the user. Instead, it proactively offers specific options to guide the user toward more explicit definition, thereby preventing further vague statements from the user, for example: Should it be based on specific columns like atmospheric interference, RFI status, or combination of factors? Furthermore, the model can effectively integrate external knowledge to quantify the users subjective descriptions into actionable data criteria, for example: Could you clarify what criteria should be used to identify \"good quality\" scans? Should use the Premium Quality Scan definition from the external knowledge (SQS > 7.5, comprehensive coverage with Coverage 95% and Overlap 30%)? Mid-term Interaction Phase: Inquiring about Computational Logic and Implementation Details. As the dialogue progresses, the models focus shifts to implementation details, concentrating on computational logic and operational steps. Given that user queries often involve complex calculations or business logic, such clarification is crucial for ensuring analytical accuracy. This includes precise confirmation of formulas, weight allocation, and the mapping between query variables and specific data fields, for example: For the repair cost, should use the maintenance cost (MaintCost) or the replacement cost (ReplCost)...? The model also demonstrates forward-looking capability for error detection, anticipating and mitigating potential data processing errors through questioning, for example: notice that recvDay 38 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions and beginDay have different formats. Could you confirm how these dates are formatted so can correctly calculate the time difference between them? significant finding is the models ability to uncover analytical dimensions that the user may not have considered, effectively asking questions the user didnt know to ask. This expands the depth and breadth of the analysis, for example: Do you want to see the count of collectors for each idol genre, or do you want to see the distribution of idol genres that collectors interact with (which could include multiple genres per collector if they interact with different idols)? To ensure the accuracy of complex calculations, the model breaks them down into smaller, more easily verifiable steps and confirms each one with the user, for example: To calculate Achievement Density (AD), need membership duration in days. . . Final Interaction Phase: Formatting and Final Confirmation. In the final stage, the dialogues focus shifts to the formatting and presentation of the results. This typically involves final confirmation of the output fields, sorting rules, and numerical precision (such as the number of decimal places) to ensure the final deliverable fully aligns with the users expectations, for example: For the output format, would you like the results to be ordered in any specific way...? Also, should round the average BFR and standard deviation values to specific number of decimal places? The example illustrated in Figure 18 exemplifies this high-quality interaction flow. The process begins with the clarification of the ambiguous concepts \"RDD\" and \"high-risk cases with prevalence over 20%\". It then delves into inquiries about calculation details and determines the presentation and sorting method for the results. Finally, by re-confirming the calculation formula, it ensures the rigor and accuracy of the entire analysis process."
        },
        {
            "title": "P HUMAN EVALUATION OF DATASET QUALITY",
            "content": "To rigorously assess the quality and reliability of our BIRD-INTERACT benchmark, we conducted thorough human evaluation. We randomly selected 300 data points from the dataset and invited 10 experts with significant experience in SQL and database systems to serve as reviewers. Each data point, consisting of user question, ground-truth SQL query, and its ambiguity annotations, was evaluated against set of core quality metrics. The evaluation was performed using binary scoring system (1 for Accept, 0 for Reject) for each metric (Li et al., 2025c). Evaluation Metrics. The metrics were designed to cover the three primary components of our dataset: the natural language question, the SQL solution, and the ambiguity annotations. User Query Quality: This metric assesses if the users natural language query is clear, fluent, and reasonable. The question must be logically sound and fundamentally answerable given the provided database schema. question that is vague, unnatural, or impossible to answer based on the schema would be rejected. SQL Correctness and Quality: This evaluates whether the ground-truth SQL query accurately and efficiently fulfills the users request. The query must be both semantically correct (i.e., it logically answers the question) and syntactically valid. We also encouraged reviewers to reject queries that were unnecessarily complex or highly inefficient, ensuring high standard for the solutions. Ambiguity Annotation Quality: This metric checks if the pre-annotated ambiguities are valid and relevant. high-quality annotation must represent genuine point of confusion that text-to-SQL system might plausibly encounter (e.g., ambiguity in column selection, grouping logic, or filter conditions). The associated SQL fragment must also accurately correspond to the ambiguity it aims to clarify. Ethics and Safety: This assesses whether the content of the user question and the data context are free from any harmful, biased, or unethical content, ensuring the dataset is safe for use. Evaluation Results. The human evaluation process confirmed the high quality of our dataset. Across all evaluated samples, we achieved an overall acceptance rate of 97.3%, indicating strong 39 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Figure 19: System prompt under c-Interact. agreement from the experts on the datasets validity. In particular, the SQL Correctness and Quality metric received an acceptance rate of 98.7%, underscoring the technical reliability of our benchmark. The Ambiguity Annotation Quality was also highly rated at 95.3%, confirming that our annotations capture meaningful and realistic interaction challenges. These strong results validate that BIRDINTERACT is robust and high-quality resource for developing and evaluating interactive text-to-SQL systems."
        },
        {
            "title": "Q PROMPTS",
            "content": "Q.1 SYSTEM PROMPTS Figure 19 shows the system prompt used under the c-Interact (conversational) setting, and Figures 2022 show the system prompts used under the a-Interact (agentic) setting. 40 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Figure 20: System prompt under a-Interact (part 1). BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Figure 21: System prompt under a-Interact (part 2). 42 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Figure 22: System prompt under a-Interact (part 3). BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Figure 23: The prompt of baseline user simulator. Q.2 USER SIMULATOR PROMPTS Figure 23 shows the baseline user simulator, and Figure 24-25 show the our proposed two-stage function driven user simulator, containing parser and generator. 44 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Figure 24: Our proposed two-stage function-driven User Simulator: the prompt of User Simulator stage 1: LLM as Parser. 45 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Figure 25: Our proposed two-stage function-driven User Simulator: the prompt of User Simulator stage 2: LLM as Generator. 46 BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation via Lens of Dynamic Interactions Figure 26: LLM-as-judge prompt to evaluate the performance of user simulators."
        }
    ],
    "affiliations": [
        "Google Cloud",
        "The University of Hong Kong"
    ]
}