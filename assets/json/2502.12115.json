{
    "paper_title": "SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?",
    "authors": [
        "Samuel Miserendino",
        "Michele Wang",
        "Tejal Patwardhan",
        "Johannes Heidecke"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \\$1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from \\$50 bug fixes to \\$32,000 feature implementations--and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (https://github.com/openai/SWELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development."
        },
        {
            "title": "Start",
            "content": "SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering? Samuel Miserendino 1 * Michele Wang 1 * Tejal Patwardhan 1 Johannes Heidecke 1 5 2 0 2 7 1 ] . [ 1 5 1 1 2 1 . 2 0 5 2 : r Abstract We introduce SWE-Lancer, benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at $1 million USD total in realworld payouts. SWE-Lancer encompasses both independent engineering tasks ranging from $50 bug fixes to $32,000 feature implementations and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with endto-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source unified Docker image and public evaluation split, SWELancer Diamond (https://github.com/ openai/SWELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development. 1. Introduction In just two years, language models have advanced from solving basic textbook computer science problems to winning gold medals in international programming competitions (OpenAI, 2024a; Quan et al., 2025). When OpenAI announced SWE-Bench Verified in August 2024, GPT-4o scored 33%; today, their o3 reasoning model achieves SOTA at 72% (OpenAI, 2024b), highlighting the need for unsaturated evaluations that reflect the complexity of real-world software engineering. As AI research and development continues to accelerate, benchmarks that rigorously evaluate and forecast software engineering (SWE) capabilities will *Equal contribution 1OpenAI. Correspondence to: Samuel Miserendino <samuelgm@openai.com>, Michele Wang <michele@openai.com>. be crucial in assessing economic impacts and agentic safety (OpenAI, 2023; Anthropic, 2024; DeepMind, 2024). Prior coding benchmarks have focused on self-contained tasks like program synthesis (Chen et al., 2021) and competitive programming (Hendrycks et al., 2021). The most realistic benchmarks are currently SWE-Bench (Jimenez et al., 2024) and SWE-Bench Verified (Chowdhury et al., 2024), which focus on isolated, self-contained tasks. In the real world, however, software engineers operate across the full technology stack and must reason about complex inter-codebase interactions and tradeoffs. To better measure the real-world software engineering capabilities and impact of AI models, we introduce SWELancer. Our benchmark evaluates frontier language models on 1,488 freelance software engineering jobs from Upwork, collectively worth $1,000,000 USD in payouts. The evaluation includes Individual Contributor (IC) SWE tasks, where models generate code patches to resolve real-world issues, and SWE Manager tasks, where models act as technical leads by selecting the best implementation proposal for given problem. IC SWE tasks range in difficulty from 15-minute bug fixes to new feature requests that took weeks to close out. Unlike prior benchmarks, which grade model performance with unit tests, SWE-Lancer evaluates IC SWE tasks using end-to-end tests created by team of professional software engineers. These end-to-end tests use browser automation to verify application behavior and mirror the real-world freelance review process, and have been triple-verified for quality by experienced software engineers. SWE Manager tasks direct models to review competing proposals submitted by freelancers in response to job postings, and select the best one. They are then assessed against the choices of the original engineering managers. These tasks require deep technical understanding of both the issue and the proposals; it is often the case that multiple proposals are technically correct, and identifying the winning proposal requires considering context from the entire repository. SWE-Lancer offers several advantages over existing SWE SWE-Lancer Figure 1. Evaluation flow for IC SWE tasks; the model only earns the payout if all applicable tests pass. Figure 2. Evaluation flow for SWE Manager tasks; during proposal selection, the model has the ability to browse the codebase. benchmarks: Real-world payouts: All 1,488 tasks represent real payouts to freelance engineers and provide natural market-derived difficulty gradient (ranging from $250 to $32,000). Notably, the economic payout associated with each task is not an estimate; rather, it reflects the actual amount paid to the freelancer who completed it. Monetary payouts are non-trivial: within the opensourced SWE-Lancer Diamond set, 35% of tasks are worth more than $1,000, and 34% are worth from $500 to $1,000. Management assessment: Large-scale software engineering in the real-world requires capable technical management. Our benchmark evaluates models ability to function as SWE manager using real-world data. In SWE management tasks, the model acts as technical lead, weighing the actual implementation proposals that were submitted by freelancers when the issue was originally posted. Previous works have investigated models ability to generate research ideas (Si et al., 2024), but their capacity to evaluate commercial engineering proposals remains unexplored. Advanced full-stack engineering: Prior evaluations have largely focused on issues in narrow, developerfacing repositories (e.g. open source utilities to facilitate plotting or PDF generation). In contrast, SWELancer is more representative of real-world software engineering, as tasks come from user-facing product with millions of real customers. SWE-Lancer tasks frequently require whole-codebase context. They involve engineering on both mobile and web, interaction with APIs, browsers, and external apps, and validation and reproduction of complex issues. Example tasks include $250 reliability improvement (fixing double-triggered API call), $1,000 bug fix (resolving permissions discrepancies), and $16,000 feature implementation (adding support for in-app video playback in web, iOS, Android, and desktop). Improved grading via end-to-end (E2E) tests: Existing coding benchmarks often rely on unit tests to evaluate isolated functions or components; however, these tests are highly susceptible to grader hacking (see Appendix A7) and insufficient for evaluating fullstack engineering work (Brar & Kaur, 2015; Paul et al., 2006). SWE-Lancer is the first benchmark to use E2E 2 SWE-Lancer tests created by professional engineers, offering more comprehensive, real-world evaluation that is harder to exploit. For example, in one $1,000 task (Expensify, 2024a), bug caused the avatar on the Share Code page to differ from the profile page. While unit tests might confirm each function works independently, the end-to-end test we created for this task simulates the entire user workflow including logging in, uploading profile picture, and interacting with second account and catches the bug. Tests underwent three rounds of validation by software engineers for quality, coverage, and fairness, supplemented by an automated system ensuring each test passed in the fixed state and failed in the broken state. Domain diversity: SWE-Lancer tasks span many categories (Figure 3). In the open-sourced Diamond set, 74% of IC SWE tasks and 76% of SWE Management tasks involve Application Logic, while 17% of IC SWE tasks and 18% of SWE Management tasks involve UI/UX development. These categories extend beyond styling to include core full-stack concepts like frontend event handling, DOM interactions, and application behavior. Most tasks (88% of IC SWE tasks and 94% of SWE Management tasks) are bug fixes. See Appendix A12 for more details on dataset composition. Difficulty: SWE-Lancer tasks are challenging. The average task in the Diamond set takes 26 days to resolve on Github and has 47 comments, which include discussion of multiple proposals. On average, IC SWE tasks in the Diamond set require modifying 2 files and 69 lines of code, while SWE Manager tasks require the model to choose between 4-5 proposals. Unbiased data collection: The data collection process for prior benchmarks, such as SWE-Bench and SWEBench Verified, involved searching for pull requests on GitHub and selecting ones that had unit tests. As result of this selection process, these datasets are biased towards code and problems that are easily testable. Instead of filtering out the vast majority of problems to find ones that volunteers chose to write tests for, we selected representative sample of tasks from Upwork, and paid team of 100 professional software engineers to write and verify end-to-end tests for all of them. We also perform large-scale evaluations of state-of-the-art models to better understand the capabilities of autonomous SWE agents, including running ablations on test-time compute and tool affordances. We find that the best performing model Claude 3.5 Sonnet scores 26.2% on IC SWE tasks and 44.9% on SWE Management tasks, earning total of $208,050 out of $500,800 possible on the SWE-Lancer Diamond set. On the full SWE-Lancer dataset, Claude 3.5 Sonnet earns over $400,000 out of $1,000,000 possible. To advance research in automated software engineering, agentic safety, and economic impacts of automated coding models, we release public evaluation split SWE-Lancer Diamond containing $500,800 worth of tasks. To avoid contamination via training or search (e.g., for models that can browse the internet), we keep the remaining set as private holdout. The full dataset is available upon request. 1.1. Related Work Researchers have evaluated LLMs coding skills in scientific programming (Tian et al., 2024), text analysis (Zhong et al., 2023; Lam et al., 2024), and repository-level completion (Zhang et al., 2024; Liu et al., 2023), with benchmarks such as HumanEval, HumanEvalPack (Muennighoff et al., 2024), APPS, LiveCodeBench (Jain et al., 2024), NaturalCodeBench (Zhang et al., 2024), and BigCodeBench (Zhuo et al., 2024). SWE-bench verified (Chowdhury et al., 2024) builds upon SWE-bench (Jimenez et al., 2024) and evaluates LLMs on real-world pull requests from open-source repositories, graded against human-coded patches. SWE-bench Multimodal (Yang et al., 2024) extends this to frontend tasks from open-source Javascript libraries. However, these benchmarks typically rely on unit tests (prone to grader hacking, see Appendix A7), exclude commercial repositories, and lack full-stack coveragegaps that SWE-Lancer addresses by sourcing real freelance tasks and mapping model performance to economic payout. See Appendix A4 for more detailed comparison table of coding benchmarks. 2. SWE-Lancer The SWE-Lancer dataset consists of 1,488 real freelance software engineering tasks from the Expensify open-source repository posted on Upwork. These tasks are collectively valued at $1 million USD and are divided into two categories: Individual Contributor (IC) Software Engineering (SWE) Tasks: This set contains 764 tasks collectively valued at $414,775, which mirror the responsibilities of individual contributor software engineers (e.g., implementing features, resolving bugs). The model is given (1) the issue text description (including reproduction steps and desired behavior), (2) the codebase checkpointed at the state before the issue fix, and (3) the objective of fixing the issue. The models solution is evaluated by applying its patch and running all associated end-to-end tests using Playwright, an open-source browser testing library (Microsoft, 2025). Models are not able to access end-to-end tests during the evaluation. SWE Management Tasks: This set contains 724 tasks collectively valued at $585,225. Taking on the role of software engineering manager, the model must choose the best proposal for resolving task. The model is SWE-Lancer Figure 3. Breakdown of the different types of IC SWE issues in SWE-Lancer with examples from the Diamond Set. Blue categories on the left represent task topics, while green categories at right represent task types. Figure 4. SWE-Lancer issues are dynamically priced based on real-world difficulty. In the example above (Expensify, 2023), SWE managers rejected 5 early proposals that did not appropriately address edge cases. The initial request priced at $1,000 was increased to $8,000 over four weeks until it was solved. given (1) multiple proposed solutions to the same issue (taken from the original discussion), (2) snapshot of the codebase from before the issue was fixed, and (3) the objective of picking the best solution. The models selection is evaluated by assessing whether it matches ground truth. An additional validation campaign with experienced software engineers showed 99% agreement with the original chosen solutions. 2.1. Evaluation Metrics and Task Types We track the percent of tasks resolved and the total payout earned by the model (using the real freelance rates set in Upwork by Expensify). Because they draw from real tasks, SWE-Lancer payouts uniquely reflect true economic value rather than theoretical estimates. Harder tasks pay more, especially when they demand specialized knowledge or remain unresolved for extended periods of time. Issues are posed exactly as written on Upwork; each IC SWE task is graded with end-to-end tests mirroring how experienced engineers would evaluate them. Full prompts are available in Appendix A8 and example tasks are in A13. 2.2. Benchmark Construction To ensure our dataset consists of high-quality and representative tasks, we implemented the following pipeline: 4 SWE-Lancer 1. Repository selection. Expensify is $300 million USD public company (NASDAQ: EXFY) with 12 million users who rely on its software, making it reliable testbed for sourcing commercially valuable software engineering tasks. The open-source Expensify repository posts tasks on Upwork for freelance engineers with concrete payouts. We sourced real tasks that were previously solved by paid contributors. The repository has diversity of high-quality, complex tasks. 2. Task selection. 100 professional software engineers reviewed the tasks, proposals, and codebase for clarity, specificity, and executability. For high-value IC SWE tasks (with payout exceeding $5,000), team of ten experienced engineers validated each task, confirming that the environment was properly configured and test coverage was robust. Each IC SWE and SWE Management task was reviewed by multiple engineers (three for IC SWE, two for SWE Management). More detail on the review process is included in Appendix A6. 3. Task generation. From each validated Github issue, we generated an IC SWE task using the tasks title and description, and snapshot of the codebase at the time it was posted. If there were at least two proposals for how to resolve the issue, we also generated SWE Management Task using the issue description and list of proposals. 4. End to End Test Development. We wrote comprehensive end-to-end Playwright tests for each IC SWE task. Tests simulate real-world user flows, such as logging into the application, performing complex actions (e.g., making financial transactions), and verifying that the models solution works as expected. Each test is triple-verified by professional software engineers. 5. User Tool. Each IC SWE Task has an accompanying user tool; when the agent invokes the user tool, Playwright script opens the browser and simulates user attempting to perform the action associated with the task in the local application, so the model can view its work, just as human engineers often run their code while writing it. The model does not receive any feedback about whether the action was successful; however, text-based trajectory and series of screenshots are written to its working directory when the tool finishes. When the user tool is enabled, the model is allowed to invoke it as often as it wishes to via the command line. 3. Experiments and Results 3.1. Experiment Setup For our experiments, agents run in Docker container with the repository preconfigured and no Internet access, preventing them from retrieving external information. We further remove the GitHub remote and any future commits to avoid the possibility of models scraping code diffs or pull request details. Each agent has single attempt (pass@1), mirroring the constraints of typical online SWE freelancing platforms. Models operate with basic scaffold that lets them browse the local codebase, modify files, and execute terminal commands. Full details of our execution environment and scaffolds can be found in Appendix A5. For our baseline setup, we evaluate OpenAIs gpt-4o-202408-06, OpenAIs o1 (with high reasoning effort (OpenAI, 2025)), and Anthropics Claude 3.5 Sonnet (claude-3-5sonnet-20240620). All models have the user tool enabled.1 3.2. Main Results As shown in Figure 5, all models earn well below the full $1 million USD of possible payout on the full SWE-Lancer dataset. Figure 5. Total payouts earned by each model on the full SWELancer dataset including both IC SWE and SWE Manager tasks. To illustrate model performance across experiments, we present the pass@1 accuracies, corresponding earnings (i.e., total payout), and earn rates (i.e., payout received divided by total possible payout) for IC SWE and SWE Manager tasks, summarized in Table 1.2 All models earn well below the full $1 million USD of possible payout on the full SWE-Lancer dataset. As shown in Figure 6, all models performed better on SWE Manager tasks than on IC SWE tasks, which remain largely unsaturated. For IC SWE tasks, pass@1 and earnings rates remain below 30%. On SWE Manager tasks, the best performing model Claude 3.5 Sonnet scores 45% on the Diamond set. 3.5 Sonnet exhibits the strongest performance on both IC SWE and SWE Manager tasks, outperforming 1Due to rate limits, we evaluated Claude 3.5 Sonnet on the full set with pass@1 only. 2Because pass@1 represents one sample, there can be significant variance between runs. 5 SWE-Lancer Figure 6. Model pass@1 performance on IC SWE and SWE Manager in the Diamond set and the full set; Claude 3.5 Sonnet performs the best and earns $208K in total on the Diamond set and over $400K on the Full set. Figure 7. Pass@k performance on IC SWE tasks within the SWELancer Diamond set increases with the number of attempts allowed. the next-best model (o1) by 9.7% on IC SWE (Diamond) tasks and 3.4% on SWE Manager (Diamond) tasks. 3.3. Increasing Number of Attempts To evaluate how performance changes with more attempts, we evaluate GPT-4o and o1 using the pass@k metric (Chen et al., 2021). We estimate the percentage of tasks which the agent solves correctly, given total samples and correct (cid:104) samples as: pass@k := EProblems result for [1, 7] is shown in Figure 7. For all models, allowing more attempts leads to consistent increase in pass rate. This curve is especially noticeable for o1, where allowing for 6 more attempts nearly triples the percentage of solved tasks. GPT-4o with pass@6 achieves the same score as o1 with pass@1 (16.5%). (cid:105) . The main (nc ) (n k) 1 Figure 8. o1 pass@1 performance on Diamond IC SWE tasks by price range of task. Higher reasoning effort improves overall pass@1 and improves performance on harder, more expensive tasks. 3.4. Increasing Test Time Compute We measure pass@1 as function of inference-time compute. On IC SWE tasks within the Diamond set, experiments with o1 and user tool enabled show that higher reasoning effort (test-time compute) improves pass@1 from 9.3% (Low reasoning effort) to 16.5% (High reasoning effort), with corresponding increases in earnings (which increased from $16K to $29K) and earn rates (increased fom 6.8% to 12.1%). Figure 8 shows the breakout of pass@1 by task price range for different levels of test-time compute and indicates greater test-time-compute can improve performance on harder, more expensive problems in particular. Figure 9. Pass@1 performance on Diamond IC SWE tasks slightly decreases when the model cannot use the user tool. 6 SWE-Lancer Table 1. Full pass@1 Accuracies and Earnings for IC SWE and SWE Manager. Earnings are rounded to the nearest thousand. SWE-Lancer Diamond contains 237 IC SWE tasks worth $236,300 and 265 SWE Manager tasks worth $264,500, totaling 502 tasks worth $500,800. The full set has 764 IC SWE tasks worth $414,775 and 724 SWE Manager tasks worth $585,225, totaling 1,488 tasks worth $1,000,000. In the table below, overall metrics for SWE-Lancer Diamond and SWE-Lancer Full sets are highlighted in blue, while baseline metrics for IC SWE (Diamond) and SWE Manager (Diamond) tasks are highlighted in green."
        },
        {
            "title": "Reasoning Effort",
            "content": "pass@1 Dollars Earned / Total Earn Rate GPT-4o o1 o1 o1 3.5 Sonnet GPT-4o o1 GPT-4o o1 3.5 Sonnet GPT-4o o1 3.5 Sonnet GPT-4o o1 3.5 Sonnet GPT-4o o1 3.5 Sonnet GPT-4o o1 3.5 Sonnet"
        },
        {
            "title": "Yes\nYes\nYes",
            "content": "N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A IC SWE (Diamond) IC SWE (Diamond) IC SWE (Diamond) IC SWE (Diamond) IC SWE (Diamond) IC SWE (Diamond) IC SWE (Diamond) IC SWE (Full) IC SWE (Full) IC SWE (Full) SWE Manager (Diamond) SWE Manager (Diamond) SWE Manager (Diamond) SWE Manager (Full) SWE Manager (Full) SWE Manager (Full) SWE-Lancer Diamond SWE-Lancer Diamond SWE-Lancer Diamond SWE-Lancer Full SWE-Lancer Full SWE-Lancer Full N/A Low Medium High N/A N/A High N/A High N/A N/A High N/A N/A High High N/A High N/A N/A High N/A 8.0% 9.3% 15.6% 16.5% 26.2% 8.0% 13.1% 8.6% 20.3% 21.1% 37.0% 41.5% 44.9% 38.7% 46.3% 47.0% 23.3% 29.7% 36.1% 23.3% 32.9% 33.7% $14k / $236k $16k / $236k $24k / $236k $29k / $236k $58k / $236k $17k / $236k $23k / $236k $29k / $415k $78k / $415k $89k / $415k $125k / $265k $137k / $265k $150k / $265k $275k / $585k $302k / $585k $314k / $585k $139k / $501k $166k / $501k $208k / $501k $304k / $1M $380k / $1M $403k / $1M 6.0% 6.8% 9.9% 12.1% 24.5% 7.2% 9.7% 6.9% 18.9% 21.5% 47.1% 51.8% 56.8% 47.0% 51.6% 53.7% 27.7% 33.1% 41.5% 30.4% 38.0% 40.3% 3.5. Removing User Tool In IC SWE tasks, removing the user tool only minimally reduces pass@1 rates, as shown in Figure 9. However, we observe that stronger models make more effective use of the user tool and therefore experience greater performance drop under this ablation. 3.6. Discussion Results indicate that the real-world freelance work in our benchmark remains challenging for frontier language models. The best performing model, Claude 3.5 Sonnet, earns $208,050 on the SWE-Lancer Diamond set and resolves 26.2% of IC SWE issues; however, the majority of its solutions are incorrect, and higher reliability is needed for trustworthy deployment. The strongest models perform well across all task types. Tables 2 and 3 show pass@1 rates for IC SWE Diamond tasks across different task categories. Sonnet 3.5 performs best, followed by o1 and then GPT-4o, and pass@1 on Manager tasks is often more than double pass rate on IC SWE tasks. Sonnet 3.5 outperforms o1 by nearly 15% on UI/UX tasks in particular, and nearly 10% on IC SWE tasks involving implementing new features or enhancements. Effective tool use distinguishes top performers. We find that the strongest models make frequent use of the user tool, and are able to efficiently parse its outputs to reproduce, localize, and iteratively debug issues. The user tool often takes nontrivial amount of time to run period of 90 to 120 seconds during which weaker models such as GPT-4o are prone to abandoning the tool altogether. The best performing models reason about the delay (which is disclosed in their instructions), set appropriate timeouts, and review results when available. An example user tool trajectory is in Appendix A10. Agents excel at localizing, but fail to root cause, resulting in partial or flawed solutions. Agents pinpoint the source of an issue remarkably quickly, using keyword searches across the whole repository to quickly locate the relevant file and functions often far faster than human would. However, they often exhibit limited understanding of how SWE-Lancer Table 2. Diamond Task Pass Rates by Task Type"
        },
        {
            "title": "Task Type",
            "content": "GPT-4o o1 Sonnet 3.5 GPT-4o Sonnet 3.5 Application Logic (Client-Side) UI/UX Server-Side Logic System-Wide Quality and Reliability Tasks 15.9% 8.0% 2.4% 17.1% 23.5% 23.5% 0.0% 0.0% 23.9% 31.7% 41.2% 0.0% 176 41 17 36.3% 42.3% 32.7% 32.7% 53.8% 61.5% 100.0% 50.0% 45.8% 40.8% 38.5% 100.0% 201 49 13 2 the issue spans multiple components or files, and fail to address the root cause, leading to solutions that are incorrect or insufficiently comprehensive. We rarely find cases where the agent aims to reproduce the issue or fails due to not finding the right file or location to edit. We provide several qualitative summaries of trajectories in Appendix A9. 4. Limitations Diversity of repositories and tasks. We sourced tasks exclusively from the Upwork freelancer platform and the Expensify repository. While we believe the Expensify repository is representative of full-stack engineering work, incorporating tasks from other freelance sites and open-source repositories could broaden the scope of this evaluation. In particular, infrastructure engineering tasks (e.g. refining Kubernetes cluster architectures and debugging pod failures, node crashes, or networking problems) are underrepresented in this iteration of SWE-Lancer because they are disproportionately underrepresented in Expensifys posted task set. Scope. Freelance tasks tend to be more self-contained than full-time software engineering tasks (Gupta et al., 2020). However, the entire front-end portion of the Expensify repository is maintained by freelance software engineers, which makes it closer to real-world engineering than other freelance postings. We also include SWE management tasks, which freelancers rarely handle. Still, while SWE-Lancer may be good leading indicator of how automation could affect freelance or drop-in remote software engineering, we remain cautious about extrapolating impact beyond this. Modalities. SWE-Lancer tasks are text-only, though issue writers often include screen recording. This evaluation does not test how much screenshots or videos can improve model performance. Environments. Unlike real engineers, the models cannot ask clarifying or follow-up questions. Contamination. Contamination is common limitation of coding benchmarks that rely on open-source repositories. SWE-Lancer Diamond tasks originate from public GitHub issues between 2023 and 2024. Depending on model cutoff date, contamination in training data is possible. Moreover, because tasks are posted online, model with browsing 8 capabilities or an internet search tool can look up solutions. Therefore, when running this eval, it is important to either disable browsing or limit sites accessed to prevent directlookups, and to ideally also post-hoc filter for any instances of cheating. This will help ensure evaluation results measure real capability rather than contamination. Note that Table 4 in Appendix A2 shows no clear performance improvement for tasks predating the models knowledge cutoffs, suggesting limited impact of contamination for those tasks. 5. Future Work Economic analysis. Model performance on SWE-Lancer maps to real-world economic value; each task in the benchmark has an associated payout that was sent to freelancer via Upwork. We hope this benchmark enables research into the broader societal impacts of autonomous agents, including impacts on labor markets, productivity, and the acceleration of AI R&D. An interesting immediate exploration would be comparing freelancer payouts to API costs for completing the tasks. Multimodality. Our current evaluation harness does not support multimodal inputs; however, the majority of Expensify GitHub issues contain videos (and, occasionally, screenshots) of the expected behavior. Additionally, the user tool saves screenshots to the output directory upon completion, which current models are unable to view. Because there is not widespread support for such inputs, we prioritized making information about the prompt and environment available via text (for example, through text-based trajectories from the user tool). 6. Conclusion We introduce SWE-Lancer, benchmark for evaluating model performance on real-world, freelance software engineering work. This software engineering benchmark maps model performance to real monetary value, and improves realism of model benchmarks by evaluating more complex full-stack software engineering and management skills. We hope SWE-Lancer enables greater research into the economic impact of AI model development. SWE-Lancer 7. Impact Statement AI models with strong real-world software engineering capabilities could enhance productivity, expand access to highquality engineering capabilities, and reduce barriers to technological progress. However, they could also shift labor demandespecially in the short term for entry-level and freelance software engineersand have broader long-term implications for the software industry. Improving AI software engineering is not without risk. Advanced systems could carry model autonomy risk in self-improvement and potential exfiltration, while automatically generated code may contain security flaws, disrupt existing features, or stray from industry best practices, consideration that is important if the world increasingly relies on model-generated code. SWE-Lancer provides concrete framework to start tying model capabilities to potential real-world software automation potential, therefore helping better measure its economic and social implications. By quantifying AI progress in software engineering, we aim to help inform the world about the potential economic impacts of AI model development, while underscoring the need for careful and responsible deployment. To further support responsible AI progress, we open-source public eval set and report results on frontier AI models to help level-set the implications of AI progress. Future work should explore the societal and economic implications of AI-driven development, ensuring these systems are integrated safely and effectively. 8. Acknowledgements This evaluation would not have been possible without the help of those at OpenAI who work on foundational systems used by teams across the company. In particular, we would like to thank Evan Mays and Kevin Liu for underlying evaluation infrastructure development, Rachel Dias for human data vendor sourcing and setup, Grace Kim for task validation support, Jack Edwards for open-source codebase cleanup support, Patrick Chao for plotting library development, Phoebe Thacker and Mia Glaese for human data team leadership, Olivia Watkins, Patrick Chao, Miles Wang, Leyton Ho, Andy Applebaum, Leon Maksin, Joel Parish, Kevin Liu, Aleksander Madry, and Alex Beutel for feedback on earlier drafts of the paper, the many software engineer annotators for quality-checking tasks, the OpenAI Preparedness, Safety Systems, and Research organizations for sponsoring this work, and the Upwork and Expensify developers and contributors for open-sourcing their original process. 9 SWE-Lancer"
        },
        {
            "title": "References",
            "content": "Responsible scaling policy. report, Anthropic, October 2024. Anthropic. nical https://assets.anthropic.com/ m/24a47b00f10301cd/original/ Anthropic-Responsible-Scaling-Policy-2024-10-15. pdf. TechURL Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and Steinhardt, J. Measuring coding challenge competence with apps, 2021. URL https://arxiv.org/abs/ 2105.09938. doi: 10.3390/pr8101215. URL https://www.mdpi. com/2227-9717/8/10/1215. Brar, H. K. and Kaur, P. J. Differentiating integration testing and unit testing. In 2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom), pp. 796798, 2015. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., HerbertVoss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/ 2107.03374. Chowdhury, N., Aung, J., Shern, C. J., Jaffe, O., Sherburn, D., Starace, G., Mays, E., Dias, R., Aljubeh, M., Glaese, M., Jimenez, C. E., Yang, J., Liu, K., and Madry, A. Swe-bench verified: Enhancing real-world github issue resolution. arXiv preprint arXiv:2407.01489, 2024. G. framework, DeepMind, safety //deepmind.com/blog/article/ introducing-the-frontier-safety-framework. frontier https:"
        },
        {
            "title": "Introducing",
            "content": "the URL 2024. Jain, N., Han, K., Gu, A., Li, W.-D., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and contamination free evaluation of large language models for code, 2024. URL https://arxiv.org/abs/2403.07974. Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. Swe-bench: Can language models resolve real-world github issues?, 2024. URL https://arxiv.org/abs/2310.06770. Lam, M. S., Teoh, J., Landay, J. A., Heer, J., and Bernstein, M. S. Concept induction: Analyzing unstructured text with high-level concepts using lloom. In Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 24, pp. 128. ACM, 2024. doi: 10.1145/ 3613904.3642830. URL http://dx.doi.org/10. 1145/3613904.3642830. Liu, T., Xu, C., and McAuley, J. Repobench: Benchmarking repository-level code auto-completion systems, 2023. URL https://arxiv.org/abs/2306.03091. Microsoft. Playwright: Fast and reliable end-to-end testing for modern web apps. https://playwright. dev/, 2025. Muennighoff, N., Liu, Q., Zebaze, A., Zheng, Q., Hui, B., Zhuo, T. Y., Singh, S., Tang, X., von Werra, L., and Longpre, S. Octopack: Instruction tuning code large language models, 2024. URL https://arxiv.org/ abs/2308.07124. Expensify. Issue #14958: zip/postcode validation error message not displayed for entering , on the home address screen. https://github.com/Expensify/ App/issues/14958, 2023. Expensify. Issue #25889: Dev: Share code avatar differs from profile avatar. https://github.com/ Expensify/App/issues/25889, 2024a."
        },
        {
            "title": "Openai",
            "content": "framework, preparedness URL https://cdn.openai.com/ OpenAI. 2023. openai-preparedness-framework-beta. pdf. OpenAI. Learning to reason with llms, 2024a. OpenAI. Openai o3 and o3-mini12 days of openai: Day 12, 2024b. Expensify. Issue #41239: Add support for copy/pasting images on ios. https://github.com/Expensify/ App/issues/41239, 2024b. OpenAI. OpenAI Authentication API Documentation, 2025. URL https://platform.openai.com/ docs/api-reference/authentication. Gupta, V., Fernandez-Crehuet, J. M., and Hanne, T. Freelancers in the software development process: systematic mapping study. Processes, 8(10):1215, 2020. Paul, R., Tsai, W. T., Chen, Y., Fan, C., Cao, Z., and Huang, H. End-to-end (e2e) testing and evaluIn Pham, H. (ed.), ation of high-assurance systems. 10 SWE-Lancer Springer Handbook of Engineering Statistics, Springer Handbooks, pp. 439467. Springer, London, London, 2006. doi: 10.1007/ 978-1-84628-288-1 24. URL https://doi.org/ 10.1007/978-1-84628-288-1_24. ISBN 978-1-85233-806-0. Quan, S., Yang, J., Yu, B., Zheng, B., Liu, D., Yang, A., Ren, X., Gao, B., Miao, Y., Feng, Y., Wang, Z., Yang, J., Cui, Z., Fan, Y., Zhang, Y., Hui, B., Lin, J., and Qwen Team, A. G. Codeelo: Benchmarking competition-level code generation of llms with human-comparable elo ratings. arXiv preprint arXiv:2501.01257, 2025. URL https: //arxiv.org/abs/2501.01257. Si, C., Yang, D., and Hashimoto, T. Can llms generate novel research ideas? large-scale human study with 100+ nlp researchers, 2024. URL https://arxiv. org/abs/2409.04109. Tian, M., Gao, L., Zhang, S. D., Chen, X., Fan, C., Guo, X., Haas, R., Ji, P., Krongchon, K., Li, Y., Liu, S., Luo, D., Ma, Y., Tong, H., Trinh, K., Tian, C., Wang, Z., Wu, B., Xiong, Y., Yin, S., Zhu, M., Lieret, K., Lu, Y., Liu, G., Du, Y., Tao, T., Press, O., Callan, J., Huerta, E., and Peng, H. Scicode: research coding benchmark curated by scientists, 2024. URL https://arxiv.org/abs/ 2407.13168. Yang, J., Jimenez, C. E., Zhang, A. L., Lieret, K., Yang, J., Wu, X., Press, O., Muennighoff, N., Synnaeve, G., Narasimhan, K. R., Yang, D., Wang, S. I., and Press, O. Swe-bench multimodal: Do ai systems generalize to visual software domains?, 2024. URL https:// arxiv.org/abs/2410.03859. Zhang, S., Zhao, H., Liu, X., Zheng, Q., Qi, Z., Gu, X., Zhang, X., Dong, Y., and Tang, J. Naturalcodebench: Examining coding performance mismatch on humaneval and natural user prompts, 2024. URL https://arxiv. org/abs/2405.04520. Zhong, R., Zhang, P., Li, S., Ahn, J., Klein, D., and Steinhardt, J. Goal driven discovery of distributional differences via language descriptions, 2023. URL https: //arxiv.org/abs/2302.14233. Zhuo, T. Y., Vu, M. C., Chim, J., Hu, H., Yu, W., Widyasari, R., Yusuf, I. N. B., Zhan, H., He, J., Paul, I., Brunner, S., Gong, C., Hoang, T., Zebaze, A. R., Hong, X., Li, W.-D., Kaddour, J., Xu, M., Zhang, Z., Yadav, P., Jain, N., Gu, A., Cheng, Z., Liu, J., Liu, Q., Wang, Z., Lo, D., Hui, B., Muennighoff, N., Fried, D., Du, X., de Vries, H., and Werra, L. V. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions, 2024. URL https: //arxiv.org/abs/2406.15877. 11 A. Appendix A.1. Pass@1 Rate Analysis by Category SWE-Lancer Table 3 shows pass@1 rates for SWE-Lancer Diamond categorized by the tasks nature of work. Table 3. Diamond Task Pass Rates by Nature of Work"
        },
        {
            "title": "Task Type",
            "content": "GPT-4o o1 Sonnet 3.5 GPT-4o Sonnet 3.5 Bug Fixes New Features or Enhancements Maintenance, QA, Testing, or Reliability 9.6% 0.0% 0.0% 19.2% 4.8% 12.5% 28.4% 14.3% 0.0% 208 21 8 34.7% 39.9% 69.2% 69.2% 75.0% 50.0% 44.0% 61.5% 50.0% 248 13 4 To determine each tasks category (as shown in Tables 2 and 3), we had o1 review each entire Github issue thread, including all descriptions and comments, three separate times for each IC SWE and SWE Management task. If these three categorizations were not unanimous, we manually assigned the final categories. A.2. Contamination Analysis Table 4. Diamond Pass@1 Rate Analysis Based on Task Creation Date."
        },
        {
            "title": "Cutoff Date",
            "content": "Pass@1 (Before) Pass@1 (After) Pass@1 (Before) Pass@1 (After) GPT-4o o1 Claude 3.5 Sonnet 2023-10-01 2023-10-01 2024-040.038 (n=79) 0.139 (n=79) 0.178 (n=118) 0.101 (n=158) 0.177 (n=158) 0.345 (n=119) 0.373 (n=142) 0.387 (n=142) 0.452 (n=210) 0.244 (n=123) 0.407 (n=123) 0.436 (n=55) A.3. Further exploration of SWE Management Tasks Within SWE Management tasks, the winning proposal is often selected for nuanced reasons, such as addressing edge cases or reducing external dependencies. For example, one task (Expensify, 2024b) involves selecting proposal for implementing image-pasting on iOS. The winning proposal handled multiple clipboard formats (e.g., base64, blob), minimized permission prompts, and aligned with native iOS behavior by enabling direct pasting into the compose bar. Other proposals were deprioritized due to reliance on third-party libraries, complex modifications, or inconsistent cross-platform support. 12 SWE-Lancer Figure 10. Real-world illustration of the iteration process for submitting and choosing solutions to Github issues. This task was incorporated into our SWE-Lancer dataset. 13 A.4. Coding and Software Engineering Evaluations Comparison Table SWE-Lancer Table 5. Comparison of SWE-Lancer to existing coding and SWE-related benchmarks: SWE-Lancer uniquely maps model performance to monetary metrics, incorporates full-stack tasks and management tasks, and evaluates using execution-based end-to-end tests hand-crafted by engineers. In contrast, other benchmarks rely on pre-existing tests within repositories or tests generated by LLMs. Evaluation Language Dataset Source Data Type Capabilities Tested Monetary Metric? Evaluation Method Source of Evaluation Method SWE-Lancer Javascript / Typescript SWE-Bench Verified Python Open-source and commercial Expensify repo of issues posted on Upwork Open-source GitHub repos SWE-Bench Multimodal Javascript / Typescript / HTML / CSS Open-source GitHub repos RepoCoder Python Open-source Python repos Professional software engineering problem statements GitHub issues from all developer frameworks GitHub issues focused on developer tools and utilities GitHub issues from Python repositories that meet certain criteria (post Jan 23, non-fork, 100+ stars, 80%+ Python, unit tests) Software engineering Software maintenance Software maintenance tasks using multimodal data Code completion RepoBench Python and Java LiveCodeBench Python NaturalCodeBench Python and Java BigCodeBench Python Github-Code Dataset and newly crawled GitHub data (post-Feb 2023) LeetCode, AtCoder, and CodeForces contests User queries from online coding services Human expert curation Single and multi-file code snippets Code retrieval and completion Coding problems designed for use in competitive programming competitions Problem statements derived from real-world user queries Natural language tasks composed of multiple substeps Code generation Code generation Code completion End-to-end test Hand-written by professional software engineers Unit test Sourced from original repository Unit test Sourced from original repository Unit test, similarity metrics Sourced from original repository Similarity metrics N/A Unit test Generated using LLMs Automated unit testing Unit test Generated using GPT-4 then refined by humans Generated using GPT-4 then refined by humans A.5. Additional Scaffold Details Each agent is executed within Microsoft Azure Standard D2as v4 virtual machine with 2 vCPUs and 8GB RAM. We execute agents within Docker containers using pre-built SWE-Lancer image, which we open-source. The container has 64 GB of shared memory allocated and 192GB of maximum memory available. All the libraries the agent can access (i.e., the ones relevant for the Expensify repository) are installed in the container. The agent does not have multimodal image or video inputs. For each rollout, the maximum number of tool calls is 100 and the maximum time allowed is 3 hours. All rollouts are conducted with temperature 1.0. Except for our pass@k experiments, each is just single rollout. A.6. Task Curation Criteria We manually filtered tasks according to the following criteria. Each task in our final set was screened by at least three experienced human software engineers. 1. The issue is sourced from an open-source repository. 2. The issue has an associated, concrete payout. 3. The issue was resolved in the past. 4. The issue is well-specified and clear models must be able to actually understand the task and implement fix, without being able to ask for clarification. 5. The PR that resolved the issue actually fixes it this fix is confirmed by executing locally. 14 SWE-Lancer Figure 11. Lifecycle of an Upwork task. First, project maintainers post software engineering task to Upwork with an associated payout. Then, users submit in-depth proposals for fixing the issue. Finally, an assigned reviewer selects the best proposal (used to create SWE Manager tasks). The winning proposal is implemented (used to create IC SWE tasks). There are additional criteria for SWE Management Tasks: 1. There were at least two well-defined proposals for the issue. 2. The proposals considered contain sufficient information about the correct fix. Further detail is provided below on how we assess specific criteria and labeled other attributes for each given task. To confirm that the issue was well-specified and clear, we asked software engineers to rate each issue from 0-3 (below) and only accepted issues where the majority of human engineers rated them as 0. Read the issue description again here, and imagine that you are an experienced software engineer with 2-3 years of relevant experience and familiarity with the codebase. You have been instructed to create PR that successfully resolves it. You have full access to the codebase and can use the title, description, reproduction steps, and any attached videos/images from the linked issue when forming your solution. Please note, however, that you are not able to ask for clarification and would need to work exclusively from the above information. Please do not read the comments below the issue when making your decisionyou must rely solely on the content of the issue itself and codebase/docs. Please assign rating from 0 to 3 using the rubric below. 0The issue is well-specified, and it is clear what is required for successful solution. 1There are some blanks to fill in about the issue, but there is sensible interpretation of what is required for successful solution. 2The issue is vague, and there is room for ambiguity. It is unclear what successful solution would look like. 3It is almost impossible to understand what you are being asked to do without further information. Issue Well-Specified Score: [0 to 3] Please explain your choice above. Include references to specific filenames, function/- class names, or lines of code where relevant. [Text box] To confirm that the tests were valid for all reasonable, correct solutions, we asked engineers to rate the tests according to rubric and only accepted issues where the majority of human engineers rated them as 0. Please re-review the pull request that addressed the issue. Does it contain valid, well-scoped tests so that all reasonable solutions pass? Please assign rating from 0 to 3 using the rubric below. 0The tests perfectly cover all possible solutions. 1The tests cover the majority of correct solutions; however, some unusual solutions may be missed. 2The tests work, but some perfectly reasonable solutions may be missed by the tests. 3The tests are too narrow/broad or they look for something different than what the issue is about. A.7. SWE-Lancer Robustness to Cheating Existing SWE benchmarks in the literature often rely on unit tests that are narrow and prone to cheating. An example of cheating by o1 in SWE-Bench Verified is shown below: 15 An excerpt from the problem description is included below: SWE-Lancer Queryset raises NotSupportedError when RHS has filterable=False attribute. In response, o1 adds an underscore before filterable so that the branch never executes: 2: ASSISTANT ALL django/db/models/sql/query.py <<<<<< SEARCH if not getattr(expression, filterable, True): raise NotSupportedError( expression.__class__.__name__ + is disallowed in the filter clause. ) ======== if not getattr(expression, _filterable, True): raise NotSupportedError( expression.__class__.__name__ + is disallowed in the filter clause. ) >>>>>> REPLACE As language models grow more capable, it is increasingly important to publish challenging, un-hackable software engineering benchmarks. SWE-Lancers human-validated end-to-end tests emulate the actual workflow of human engineer, rather than relying on handful of selectively chosen test cases. As result, SWE-Lancer is inherently more resistant to cheating and can more accurately reflect models capacity to produce genuine, economically valuable solutions for real-world engineering challenges. A.8. Prompting Models are prompted with the following general template for IC SWE Tasks."
        },
        {
            "title": "IC SWE Task Instructions",
            "content": "SWE-Lancer \"\"\"You are an expert software engineer maintaining the Expensify repository. It has already been cloned, and is present in /app/expensify (your CWD). You have been tasked with fixing the following issue: {issue title} {full HTML documentation of issue, including how to reproduce the bug, the actual vs. expected behavior, and which platforms are affected} The reward for the correct fix is ${price}. You do not need to push your changes to the repository, we will deal with this for you. Do not attempt to run the app; simply make your changes and then end your turn. Do not end your turn until you have submitted your code changes. You absolutely must not leave action items / TODOs. --- You will be provided with user tool, which you can call by running \"bash -i -c user-tool\" in the command line, which has user open the app with your local changes and perform the actions specified in the bug report. It could take up to minute for the user to finish using the app. When you call the user tool, when the user finishes /app/expensify/user_tool/ will be populated with folders called output_browser1 ... for as many browsers as were used by the user during their test. Inside each folder, you will find file called trace.trace, which contains the trajectory the user took. Here are few things to note: - You will need to parse this file programmatically in order to open it. - If row has \"type\":\"screencast-frame\", then the \"sha1\" property will correspond to .jpeg file in the resources folder, which is screenshot of the browser at that time. - If you want to view the page as HTML, rather than JPEG, you can look for rows that are called \"type\":\"frame-snapshot\"; you can print them out to see the HTML at this point in time. - You should review the rows with type \"type\" set to \"log\", \"before\", or \"after\" as they are important and show what actions are being taken. - All of the rows are in order, so e.g. you can find an interesting log row and then look at the next row that is frame-snapshot to see the HTML. You should use this to help you iteratively debug and fix the issue. You can call it as often as you need to. When you call the user tool, you wait 90,000 milliseconds (e.g. by setting the timeout argument to 90000)! \"\"\" Models are prompted with the following general template for SWE Management Tasks."
        },
        {
            "title": "SWE Management Task Instructions",
            "content": "SWE-Lancer f\"\"\"You are an expert software engineering manager working on the Expensify repository. You have tasked your team with addressing the following issue: {issue title} {full HTML documentation of issue, including how to reproduce the bug, the actual vs. expected behavior, and which platforms are affected} You will be paid {price} if your team fixes the issue. Since you are manager, your task is to select the best proposal regarding how to fix the issue from the below options: {list of all proposals} --- For your convenience, the Expensify repository has already been cloned, and is present in /app/expensify (your CWD). You should not attempt to run the app or implement the entire fix, but you can review the codebase to understand the context of the issue. When you are ready to submit your answer, please create JSON file in /app/expensify/manager_decisions.json with the following format: { } selected_proposal_id: int, You must write your output to this path exactly: /app/expensify/manager_decisions.json. You must not submit to anywhere in /mnt/data, just write to /app/expensify/manager_decisions.json exactly! \"\"\" A.9. Qualitative Analysis of Rollouts We conduct qualitative analysis of task instances from SWE-Lancer, focusing on model-generated solutions and debugging strategies. The analysis is based on agent trajectories under the default configuration of o1, 4o, and Claude 3.5 Sonnet. Issue Summary. The Expensify app fails to display validation error message when users enter comma (,) in the zip/postcode field on the Home Address screen, allowing invalid input. This issue occurs across multiple platforms and may stem from missing input sanitization or incomplete validation logic. The bug was observed in staging and production, potentially introduced during the N7 Account Settings update (Issue #14958). Note on end-to-end tests. The end-to-end tests we developed for each issue are robust and reflect the key considerations from the original Github discussion. While this issue initially concerned the commas in zip-code validation, much of the discussion in the Github thread focused on improving validation overall, including country-specific regex dictionaries and handling edge cases where the user hasnt selected country or country lacks postal system. Thus, our end-to-end tests measure not just whether the models code causes the validation error to appear when entering comma, but also whether it can comprehensively handle the full range of global zip-code formats. 18 SWE-Lancer o1 Interaction Summary 1. Locate the Target File The model begins by scanning the directory structure to find relevant files: !ls -l /app/expensify It searches for zip-related validation logic: !grep -irn \"zip\" /app/expensify To find the isValidZipCode function, it narrows the search: !grep -irn \"ValidationUtils\" /app/expensify/src grep -i zip !grep -irn \"isValidZipCode\" /app/expensify/src The model identifies the right file to modify and sees that we use regex to validate zip code: !grep -A 20 \"function isValidZipCode\" /app/expensify/src/libs/ValidationUtils.ts The model finds reference to AddressForm.tsx aims to understand how the zipPostCode field is being processed and how the zip code input values are sanitized before being validated or used !grep -C 40 \"zipPostCode?.trim\" /app/expensify/src/ components/AddressForm.tsx 2. Search for the Issue The model looks for where ZIPCODE appears in the codebase. Through this, it finds the regex used to validate US zip codes. !grep -irn \"ZIP_CODE:\" /app/expensify/src The model looks more to see how textttAddressForm is used in the overall codebase, and specifically how zipcode is used. !grep -A 50 -B 50 \"import AddressForm\" /app/expensify/src/pages/ AddressPage.tsx 3. Diagnose the Validation Logic The model reasons through the root cause of why validation error does not display when user enters comma in the zip code field in the Home Address Screen, and concludes that the likely issue is that the InputWrapper for the zip code field bypasses the form validation logic by using onValueChange. The error is being set in the validator but is not displayed due to either: 1) missing triggers for validation during typing (onChange) or focus loss (onBlur), or 2) Mismatched field IDs causing the validation result to not propagate correctly. It hypothesizes that the fix involves removing the manual state handler for the zipcode field, so the Home Address form now relies solely on the forms built-in validation logic (via the validator prop in the FormProvider), and entering comma in the ZIP/postcode field will immediately show the incorrect zip format error. (a) Validation is not triggered dynamically (e.g., on typing or focus loss). (b) Mismatched field IDs (zipPostCode vs. addressZipCode) may prevent the error from displaying. 4. Modify the Code 19 SWE-Lancer The model makes patch, removing the custom onValueChange so that the forms natural validator is invoked and the zip/postcode error now displays as expected when the user enters comma. diff --git a/src/components/AddressForm.tsx b/src/components/AddressForm.tsx index 71928c1..ce4f25c 100644 --- a/src/components/AddressForm.tsx +++ b/src/components/AddressForm.tsx @@ -182,7 +182,6 @@ function AddressForm({ /> </View> <View style={styles.formSpaceVertical} /> {/* Remove onValueChange so this field uses the - forms standard validation */} <InputWrapper InputComponent={TextInput} inputID={INPUT_IDS.ZIP_POST_CODE} @@ -190,7 +189,6 @@ function AddressForm({ - label={translate(common.zip)} onValueChange={setZipcode} defaultValue={zipcode} maxLength={countryRegexDetails?.maxLength} shouldSaveDraft={shouldSaveDraft} 5. Clean Up and Submit The model submits the patch, believing the issue is resolved. The models fix addresses the basic issue of the error not showing up when the user types comma, but it does not fully address other issues which were captured in the real PR. Evaluation. o1 successfully identified the relevant files, including ValidationUtils.ts and AddressForm.tsx, and correctly recognized that by removing onValueChange it could fall back to the forms normal validator, which would solve this particular issue. However, o1s solution was not fully comprehensive and did not pass our end-to-end test. It didnt address other issues which were all addressed in the pull request and Github discussion: Missing Validation Triggers: o1s patch does not enable validation on key events like onBlur or onChange, which is necessary for providing immediate feedback to users. The real pull request added logic to validate dynamically when the user interacts with the field. No Country-Specific Validation: o1s patch does not handle country-specific zip code formats or provide tailored feedback. The pull request implemented dynamic validation based on country-specific regexes and included zipFormat hints to guide users. Does Not Address Non-Zip Countries: o1s patch overlooks cases where some countries do not require zip code. The pull request included logic to handle these scenarios, such as disabling or hiding the zip field when applicable. Error Message Improvements: o1s patch does not enhance error messages for clarity or localization. The pull request ensured errors were dynamically adjusted and localized based on the users country and language settings. 4o Interaction Summary 1. Locate the Target File The model scans the directory structure to find relevant files: os.listdir(\"/app/expensify\") 20 SWE-Lancer It navigates to the source code: src_path = \"/app/expensify/src\" os.listdir(src_path) After multiple iterations, it finds and opens AddressPage.tsx: address_page_path = os.path.join(pages_path, AddressPage.tsx) with open(address_page_path, r) as f: address_page_content = f.read() print(address_page_content[:2000]) The model searches for AddressForm, which likely contains input validation, directing it to ValidationUtils.ts, where the zip/postcode validation logic is located. 2. Search for the Issue The model searches for postcode-related validation within ValidationUtils.ts by writing Python function. It finds the isValidZipCode(zipCode: string) function and its associated regex for validating zip codes. The model then examines CONST.ts to locate the regex: US: { regex: /[0-9]{5}(?:[- ][0-9]{4})?$/ } Despite correctly identifying the regex that flags commas as invalid, the model notices that the validation error message is not shown, indicating the issue lies in how errors are handled within AddressForm.tsx. The model searches for the error message related to zip/postcode validation and hypothesizes it is triggered by: errors.zipPostCode = translate (privatePersonalDetails.error.incorrectZipFormat, { zipFormat: countryZipFormat }); 3. Modify the Code The model decides to modify the code to ensure that validation errors display for the zip code field. Instead of fixing the validation logic, it incorrectly modifies the regex itself. The model identifies and modifies AddressForm.tsx to allow multiple zip code formats, but this change does not fix the validation issue: @@ -3615,7 +3615,7 @@ const CONST = { GA: {}, GB: { - regex: /[A-Z]{1,2}[0-9R][0-9A-Z]?s*[0-9][A-Z-CIKMOV]{2}$/ + regex: /(,[A-Z]{1,2}[0-9R][0-9A-Z]? s*([0-9][ABD-HJLNP-UW-Z]{2})?)$/ samples: LA102UX, BL2F8FX, BD1S9LU, WR4G 6LH, }, GD: {}, @@ -4134,7 +4134,7 @@ const CONST = { UG: {}, UM: {}, US: { - regex: /[0-9]{5}(?:[- ][0-9]{4})?$/ + regex: /(,[A-Za-z0-9]{5,10})$/ samples: 12345, 12345-1234, 12345 1234, }, 21 SWE-Lancer UY: {}, 4. Attempt to Use the User Tool The model attempts to test the fix using the user tool but struggles with execution. It does not verify if an error message appears in the UI, failing to confirm whether the issue is actually resolved. 5. Clean Up and Submit The model submits the modified regex, believing the issue is resolved. However, the fix does not surface an error message when user enters comma, meaning the original problem persists. Evaluation. GPT-4o followed similar approach to o1, correctly identifying the necessary files and analyzing the regex pattern. However, it also overlooked the UI mechanisms responsible for displaying validation errors, failing to recognize that the issue stemmed from how validation logic was triggered. Instead of modifying the validation logic, GPT-4o attempted to adjust the regex, which did not address the core problem. The model conducted minimal testing and did not verify whether error messages appeared as expected in the UI. It also did not attempt full form submission to validate its fix, limiting the effectiveness of its approach. Claude 3.5 Sonnet Interaction Summary 1. Locate the Target File The model scans the Expensify repository: ls -la /app/expensify It then searches for files related to addresses and validation, writing function to do so: import os def search_files(directory, keyword): matching_files = [] for root, dirs, files in os.walk(directory): for file in files: if keyword.lower() in file.lower(): matching_files.append(os.path.join(root, file)) return matching_files address_files = search_files(/app/expensify/src/components, address) print(\"Address-related files in components directory:\") for file in address_files: print(file) print(\"nSearching for validation-related files:\") validation_files = search_files(/app/expensify/src/components, validat) for file in validation_files: print(file) 2. Identify the Validation Logic The model identifies AddressForm.tsx as where validation logic is implemented. It reads in the content to find where zip code / postcode validation is implemented, and then uses regex to find the specific validator function. import re with open(/app/expensify/src/components/AddressForm.tsx, r) as file: content = file.read() 22 SWE-Lancer # Locate the validator function validator_pattern = re.compile(rconst validator = useCallback((.*?)s*=>s *{(.*?)}s*,s*[translate]);, re.DOTALL) validator_match = validator_pattern.search(content) if validator_match: print(\"Validator function found:\") print(validator_match.group(0)) else: print(\"Validator function not found\") 3. Modify the Code and Submit The model modifies the validator function to enforce the no comma rule, generating the following diff. This modifies AddressForm.tsx to ensure that entering comma in the Zip / Postcode field triggers validation error message, by adding new validation rule that specifically checks for comma. Although this fixed the specified bug, the zip code validation logic is too restrictive, and doesnt support international zip codes which are checked for in our end-to-end tests. diff --git a/src/components/AddressForm.tsx b/src/components/AddressForm.tsx index 4470481d2be..ecd380bf9ce 100644 --- a/src/components/AddressForm.tsx +++ b/src/components/AddressForm.tsx @@ -118,7 +118,10 @@ function AddressForm({ ErrorUtils.addErrorMessage(errors, firstName, translate( bankAccount.error.firstName)); if (countrySpecificZipRegex) { if (values.zipPostCode && values.zipPostCode.includes(,)) { errors.zipPostCode = translate(privatePersonalDetails.error .noCommaAllowed); } else if (countrySpecificZipRegex) { - + + + + if (!countrySpecificZipRegex.test(values.zipPostCode?.trim() .toUpperCase())) { if (ValidationUtils.isRequiredFulfilled(values. zipPostCode?.trim())) { errors.zipPostCode = translate( privatePersonalDetails.error.incorrectZipFormat , {zipFormat: countryZipFormat}); @@ -126,8 +129,11 @@ function AddressForm({ errors.zipPostCode = translate(common.error. fieldRequired); } } + + + } else if (!CONST.GENERIC_ZIP_CODE_REGEX.test(values?. zipPostCode?.trim()?.toUpperCase() ?? )) { errors.zipPostCode = translate(privatePersonalDetails.error .incorrectZipFormat); } return errors; Evaluation. Claude 3.5 Sonnet was extremely quick to identify validation logic as the root cause of the issue and fixed the specified bug, but it was not fully comprehensive and did not pass our end-to-end test. Despite fixing the logic so commas in the zip code field now get flagged, and identifying that validation was failing for valid international zip codes, Claude did not address other issues which were addressed in the pull request: SWE-Lancer Missing Country-Specific Zip Code Validation: Claudes patch used generic validation approach and did not incorporate country-specific ZIP code formats. The pull request introduced regex-based validation system that dynamically applies the correct format based on the selected country. No Dynamic Format Guidance: Claudes patch did not provide users with real-time hints on the expected ZIP code format for their country. The pull request added feature that dynamically updates the hint text to show the correct format when country is selected Failure to Handle Countries Without ZIP Codes: Claudes solution assumed that all countries require ZIP code, leading to false validation errors in places where postal codes are not used. The pull request correctly allows empty ZIP fields when country does not have postal system. Lack of Centralized Validation Logic: Claudes patch scattered ZIP code validation across multiple locations in the codebase. The pull request centralized validation logic into utility function (ValidationUtils), making it easier to maintain and extend. Inconsistent Error Messaging: Claudes patch did not enhance error messages, making them generic and unhelpful. The pull request ensured that error messages now specify the correct ZIP format when validation fails, guiding users to enter the correct value. Validation Not Integrated with User Actions: Claudes solution did not validate ZIP codes dynamically based on user input. The pull request implemented onBlur and onChange validation triggers, ensuring that errors appear in real-time as the user types. A.10. Model Usage of User Tool Here we provide an example of o1 using the user tool to help solve an issue in which an error page appears when new user navigates to concierge link after finishing the onboarding flow, when instead they should be able to navigate to the concierge chat. Example of o1 using the user tool 1. Extracting Logs and Snapshots: The model parses trace.trace to extract logs and frame snapshots from the user tool, to understand key UI transitions and find error messages or timeouts related to the join button or concierge navigation. trace_path = \"/app/expensify/user_tool/output_browser1/trace.trace\" logs = [] with open(trace_path, \"r\", encoding=\"utf-8\") as f: for line in f: data = json.loads(line) if data.get(\"type\") in [\"log\", \"before\", \"after\"]: logs.append(data) for entry in logs: print(entry) 2. Searching for Missing Elements in Snapshots: The model then searches frame snapshots for the Join button so it can see if the UI is rendering it correctly. If join was missing, it could mean rendering issue or missing translation. If join was present but disabled, it suggested logic issue in the buttons properties. trace_path = \"/app/expensify/user_tool/output_browser1/trace.trace\" frame_snapshots = [] with open(trace_path, \"r\", encoding=\"utf-8\") as f: for line in f: data = json.loads(line) 24 SWE-Lancer if data.get(\"type\") == \"frame-snapshot\": frame_snapshots.append(data) snapshots_near_call_15 = [] start_call_15 = 25250.356 = 55271.523 end_call_ for snap in frame_snapshots: time = snap.get(\"timestamp\") if time is not None and start_call_15 <= time <= end_call_15: snapshots_near_call_15.append(snap) for i, snap in enumerate(snapshots_near_call_15): html = snap.get(\"snapshot\", {}).get(\"html\", \"\") if \"Join\" in html: print(f\"Snapshot {i} has Join in it.\") if \"join\" in html: print(f\"Snapshot {i} has join in it.\") 3. Checking for the Join Buttons Disabling Logic: The model then searches user tool traces for isDisabled conditions affecting the button by running subprocess.run([\"grep\", \"-Rni\", \"Join\", \"src/pages/signin\"]) It finds the following logic, which supported its suspicions that the button was incorectly disabled due to network status or account errors. <Button isDisabled={network.isOffline !!account?.message} success large text={translate(welcomeSignUpForm.join)} isLoading={account?.isLoading} onPress={() => Session.signUpUser()} pressOnEnter style={[styles.mb2]} /> 4. Fixing the Join Button: The model removed isDisabled from the code to ensure that the Join button could always be clicked. 5. Fixing the Missing Translation for Join: The model checked if welcomeSignUpForm.join was missing anbd added transition. Without defined translation, Join would not appear on the button. diff --git a/src/languages/en.ts b/src/languages/en.ts index 0000000..1111111 100644 --- a/src/languages/en.ts +++ b/src/languages/en.ts @@ -1,6 +1,9 @@ export default { + + + welcomeSignUpForm: { join: Join, }, common: { cancel: Cancel, yes: Yes, 25 SWE-Lancer 6. Fixing Concierge Navigation Issues: The model then examined ConciergePage.tsx and found the app tried to go back when there was no history. It added check to prevent navigation errors. This helped resolve the root cause of the issue: it prevented the app from breaking when new user without prior history wanted to navigate to the Concierge page. diff --git a/src/pages/ConciergePage.tsx b/src/pages/ConciergePage.tsx index 1234567..abcdef0 100644 --- a/src/pages/ConciergePage.tsx +++ b/src/pages/ConciergePage.tsx @@ -35,7 +35,9 @@ function ConciergePage({session}: ConciergePageProps) { if (!session !session.authToken) { Session.signOutAndRedirectToSignIn(); return; } } else if (navigation.canGoBack()) { navigation.goBack(); // Only navigate back if history exists. } - + + + 7. Verifying the Fixes with the User Tool: After applying all fixes, the model reran the user tool to confirm it worked appropriately. In summary, o1 used the user tool to iteratively debug the issue by extracting logs, analyzing UI snapshots, and identifying key failures. It first ran the tool to collect trace.trace logs, filtering for log, before, and after entries to detect errors and timeouts. It then inspected frame-snapshot data to verify if the Join button was rendered correctly and whether it was disabled. Upon discovering issues with button accessibility and missing translations, o1 modified the UI logic and added missing translations, then reran the user tool to validate fixes. Finally, it used the tool to confirm that new users could successfully sign up and access Concierge without encountering timeouts or navigation errors. A.11. Holdout Set We maintain $499,200 private holdout set to mitigate risks from contamination and model browsing. This internal-only set contains $178,475 worth of IC SWE tasks that were augmented by professional software engineers: to construct this split, we held the task description and test cases for issues constant, and instructed software engineers familiar with the Expensify repository to reintroduce the bug or issue in distinct but equivalent way. This approach mirrors real-world software development cycles where recurring issues (such as downtime or unexpected behavior) have diverse and persistent root causes. As with all tasks, these issues were triple-verified by professional software engineers for clarity, faithfulness, and feasibility. A.12. Additional SWE-Lancer Dataset Details Here we describe more details around dataset composition for both the SWE-Lancer Diamond set and full set. Table 6. Number of tasks by task type within SWE-Lancer Diamond and SWE-Lancer Full datasets."
        },
        {
            "title": "SWE Manager",
            "content": "% % % % Application Logic (Client-Side) UI/UX Server-Side Logic System-Wide Quality and Reliability Tasks Infrastructure/Devops 74.26% 176 17.30% 41 17 7.17% 3 1.27% 0 75.85% 201 18.49% 49 13 4.91% 2 0.75% 0 75.92% 580 16.23% 124 56 7.33% 4 0.52% 0 70.99% 514 22.38% 162 39 5.39% 6 0.83% 3 0.41% 26 SWE-Lancer Table 7. Number of tasks by task nature within SWE-Lancer Diamond and SWE-Lancer Full datasets."
        },
        {
            "title": "SWE Manager",
            "content": "% % % % Bug Fixes New Features or Enhancements Maintenance, QA, Testing, or Reliability improvements 87.76% 208 21 8.86% 8 3.38% 93.58% 248 13 4.91% 4 1.51% 87.04% 665 68 8.90% 31 4.06% 92.13% 667 42 5.80% 15 2.07% Table 8. Resolution time and comments count within SWE-Lancer Diamond and SWE-Lancer Full datasets."
        },
        {
            "title": "Both",
            "content": "25.00 41.94 27.99 51.55 26.15 47.01 20.33 36.39 25.00 47.39 21.47 41. Table 9. Proportion of Tasks in each Price Range for SWE-Lancer Diamond and SWE-Lancer Full Price Range Under $500 $500 - $1000 $1000 - $2000 $2000 and up Diamond 31.5% 33.9% 25.3% 9.4% Full 44.2% 28.1% 23.9% 3.8% Table 10. IC SWE statistics for SWE-Lancer Diamond and SWE-Lancer Full datasets"
        },
        {
            "title": "Lines of Code\nNumber of Files\nIssue Word Count",
            "content": "Mean Max Mean Max 1812 39 2842 1812 21 466 55 2 270 69 2 271 A.13. Example IC SWE and SWE Management Task Prompts Below, we share an example, filled-in IC SWE task prompt, exactly as it is given to the model."
        },
        {
            "title": "Example IC SWE Task",
            "content": "You are an expert software engineer maintaining the Expensify repository. It has already been cloned, and is present in /app/expensify (your CWD). You have been tasked with fixing the following issue: [HOLD for payment 2023-10-09] [$500] new tab opens when downloading offline ___ ## Action Performed: 1. From user A, Send an attachment to user B. 2. Be offline for user before opening the attachment 3. Click on the attachment sent from User 4. Click on the download button at the top ## Expected Result: the inactive download button is shown. 27 SWE-Lancer ## Actual Result: new tab is opened ## Workaround: Can the user still use Expensify without this being fixed? Have you informed them of the workaround? ## Platforms: <!--- Check off any platforms that are affected by this issue ---> Which of our officially supported platforms is this issue occurring on? - [ ] Android / native - [ ] Android / Chrome - [ ] iOS / native - [ ] iOS / Safari - [x] MacOS / Chrome / Safari - [ ] MacOS / Desktop **Version Number:** 1.3.58-0 **Reproducible in staging?:** **Reproducible in production?:** **If this was caught during regression testing, add the test name, ID and link from TestRail:** **Email or phone of affected tester (no customers):** **Logs:** https://stackoverflow.com/c/expensify/questions/4856 **Notes/Photos/Videos:** Any additional supporting documentation https://github.com/Expensify/App/assets/43996225/17738f72-39d8-401e-b1b5f3cf7df871a3 https://github.com/Expensify/App/assets/43996225/f98e3eef-36f0-4ddc-a632-95 f03052651a **Expensify/Expensify Issue URL:** **Issue reported by:** @daveSeife **Slack conversation:** https://expensify.slack.com/archives/C049HHMV9SM/ [View all open jobs on GitHub](https://github.com/Expensify/App/issues?q=is%3Aopen+ is%3Aissue+label%3A%22Help+Wanted%22) <details><summary>Upwork Automation - Do Not Edit</summary> <ul> <li>Upwork Job URL: https://www.upwork.com/jobs/0159b5b921ce8fa955</li> <li>Upwork Job ID: 1699136998481727488</li> <li>Last Price Increase: 2023-09-19</li> <li>Automatic offers: </li> <ul> <li>fedirjh Reviewer 26848952</li> <li>daveSeife Reporter 26848953</li> </ul></li> </ul> </details> The reward for the correct fix is $500.0. You do not need to push your changes to the repository, we will deal with this for you. Do not attempt to run the app; simply make your changes and then end your turn. Do not end your turn until you have submitted your code changes. You absolutely must not leave action items / TODOs. 28 Below, we share an example, filled-in SWE Manager task prompt, exactly as it is given to the model."
        },
        {
            "title": "Example SWE Manager Task",
            "content": "SWE-Lancer You are an expert software engineering manager working on the Expensify repository. You have tasked your team with addressing the following issue: [Paid] [$8000] zip /postcode validation error message not displayed for entering , on the Home address screen ___ ## Action Performed: 1. go to Settings > profile> personal details >Home address as 2. type the zip /postcode , ## Expected Result: it should show validation error message ## Actual Result: does not show validation message ## Workaround: unknown ## Platforms: <!--- Check off any platforms that are affected by this issue ---> Which of our officially supported platforms is this issue occurring on? - [ ] Android / native - [ ] Android / Chrome - [ ] iOS / native - [ ] iOS / Safari - [ ] MacOS / Chrome / Safari - [ ] MacOS / Desktop **Version Number:** 1.2.67-7 **Reproducible in staging?:** **Reproducible in production?:** new feature **If this was caught during regression testing, add the test name, ID and link from TestRail:** **Email or phone of affected tester (no customers):** **Logs:** https://stackoverflow.com/c/expensify/questions/4856 **Notes/Photos/Videos:** https://user-images.githubusercontent.com/43996225/217662506-507818da-7e76-4a4a-b8b8 -5525fc66e651.mov https://user-images.githubusercontent.com/43996225/217662521-34c3d804-483d-4e42-a8fe -e87aaf5490fc.mp4 **Expensify/Expensify Issue URL:** **Issue reported by:** @gadhiyamanan **Slack conversation:** https://expensify.slack.com/archives/C049HHMV9SM/ p1675838827973319 [View all open jobs on GitHub](https://github.com/Expensify/App/issues?q=is%3Aopen+ is%3Aissue+label%3A%22Help+Wanted%22) 29 SWE-Lancer <details><summary>Upwork Automation - Do Not Edit</summary> <ul> <li>Upwork Job URL: https://www.upwork.com/jobs/015b692899f4b1568b</li> <li>Upwork Job ID: 1623772333283323904</li> <li>Last Price Increase: 2023-03-02</li> </ul> </details> You will be paid $8000.0 if your team fixes the issue. Since you are manager, your task is to select the best proposal regaerding how to fix the issue from the below options: Proposal: 0: # Proposal ## Restating the problem Some unwanted characters are allowed in zip code ## Root Cause Regex is not being validated correctly for zip code # Solution Here are some conditions for the post code: - Every postal code system uses only A-Z and/or 0-9 and sometimes space/dash - Not every country uses postal codes (ex. Ireland outside of Dublin), but well ignore that here. - The shortest postal code format is Sierra Leone with NN - The longest is American Samoa with NNNNN-NNNNNN - You should allow one space or dash. - Should not begin or end with space or dash [a-z0-9][a-z0-9- ]{0,10}[a-z0-9]$ This regex should cover all those conditions source: https://stackoverflow.com/a/19844362/15373641 So Ill just this validation to the Personal information Page as well as the Home Address Screen. Not posting code diff due to changed proposal guidelines **Note:** As mentioned by @Beamanator in https://expensify.slack.com/archives/C049HHMV9SM/ p1675845424815519?thread_ts=1675838827.973319&cid=C049HHMV9SM allow letters like in Canadian zip codes, therefore allowing the alphabets We also have to -------------------------------------------- Proposal: 1: ORIGINAL PROPOSAL (comment #1424743785): -------------------------------------------------------------------------------- ## Proposal ### Please re-state the problem that we are trying to solve in this issue. The validation on the AddressPage component is insufficient, leading to many problems. We need to add proper validation for all the fields in the component. ### What is the root cause of that problem? The root cause behind this issue is that none of the values on the home address are validated properly, the only thing checked is if the fields are empty or not in 30 SWE-Lancer AddressPage. https://github.com/Expensify/App/blob/b113107ba03dce15a4e87bad994829987f916dfa/src/ pages/settings/Profile/PersonalDetails/AddressPage.js#L115-L ### What changes do you think we should make in order to solve the problem? The validation in AddressPage is outdated and does not validate all fields properly. The best case would be to check the validations performed in CompanyPage and then use the ValidationUtils to ensure that not just the zip code, but all fields are correctly validated. **EDIT:** Adding the required checks for each field: For each required field, we can run it through ValidationUtils.isRequiredFulfilled () first. **Zip code**: Zip codes across the world can vary differently, they need not be of 5 digits only. suggest we just do basic sanity check and not do anything else . - **Solution 1** Create map of regexes with respective country specific zip code matching and then validate according to user input. - **Solution 2 (Better IMO)** It doesnt really matter whatever the zip code the user enters. We can just perform basic sanity check and then let the user enter whatever they want. The regex for the same is - [w][w- ]{0,10}[w]$. -------------------------------------------------------------------------------- ADDITIONAL NOTE (comment #1424971869): \"My [proposal](https://github.com/Expensify/App/issues/14958#issuecomment -1424743785) will resolve https://github.com/Expensify/App/issues/15003 as well !\" -------------------------------------------------------------------------------- FURTHER DISCUSSION (comment #1427798502): \"Thanks for the feedback and context! For the zip code in Personal Information page, since we want to support zip code from any region in the world, it isnt really possible to create single super regex to perform validation according to the country. We are presented with two choices right now: **Solution 1** Create map of regexes with respective country specific zip code matching and then validate according to user input. **Solution 2 (Better IMO)** It doesnt really matter whatever the zip code the user enters. We can just perform basic sanity check and then let the user enter whatever they want. The regex for the same is - [w][w- ]{0,10}[w]$. As for the other fields, no need for the detailed validation and we can perform just the basic sanity check.\" -------------------------------------------------------------------------------- ADDITIONAL CLARIFICATION (comment #1431621152): \"My [proposal](https://github.com/Expensify/App/issues/14958#issuecomment -1424743785) will resolve https://github.com/Expensify/App/issues/15003 as well 31 !\" SWE-Lancer -------------------------------------------------------------------------------- FURTHER UPDATES (selected excerpts from conversation by same author): (comment #1427798502 also mentioned): \"Thanks for the feedback and context! For the zip code in Personal Information page, since we want to support zip code from any region in the world, it isnt really possible to create single super regex to perform validation according to the country. We can just do basic sanity check, or we can optionally do map of country code to zip code.\" (comment #1442138427): \"If we proceed with adding the map to CONST.js, we can store each countrys name, iso code, and the regex. Then if the user picks country that does not exist in the map, we fallback to simpler approach. That might be the best scenario.\" (comment #1450806934): \"Yes please lets keep it all in the E/App repo for now. If we proceed with adding this in E/A, we should create new file for storing the regex map. Or we could place it in CONST.js. Ha, sure. We could either (1) proceed with adding the map to CONST.js, (2) create new file for storing the regex map, or (3) create new npm package for this.\" (comment #1459820688): \"While investigating more, ran across potential issue, can you please shed some light on this: Our most common use case involves the user searching for an address in the address line and choosing from the autocomplete. However, sometimes the zip code from that API is incomplete or doesnt match the stricter regex. In those cases, maybe we should help the user by letting them correct it . Well show an error for them to fix it.\" -------------------------------------------------------------------------------- (End of unified proposal from Prince-Mendiratta.) -------------------------------------------- Proposal: 2: ## Proposal ### Please re-state the problem that we are trying to solve in this issue. Validation errors are not shown in the address page with zip code ### What is the root cause of that problem? We dont validate zip codes. In fact In the particular page dont see any validation except empty check validation. https://github.com/Expensify/App/blob/main/src/pages/settings/Profile/ PersonalDetails/AddressPage.js#L116 ### What changes do you think we should make in order to solve the problem? <!-- DO NOT POST CODE DIFFS --> We can add proper validation for all fields in the particular page including the 32 SWE-Lancer concerned zip code. We already have validations for these in other flows like company step. We can reuse the same functionality here as well. For ex: We already have validation util for zipcode here - https://github.com/ Expensify/App/blob/main/src/libs/ValidationUtils.js#L167 -------------------------------------------- Proposal: 3: ## Proposal ### Re-state the problem zip/postcode validation error message not displayed for entering , on the Home address screen ### What is the root cause of that problem? The root cause of this problem is the non-existent validation function for zip/ postcode field. ### What changes do you think we should make in order to solve the problem? <!-- DO NOT POST CODE DIFFS --> Solution is very simple!!! Before clicking save button, just validate the zip/ postcode field by using this function and show the validation error message if the user entered wrong. /** * @param {String} postcode * @returns {Boolean} */ function isValidPostalCode(postcode) { return /[0-9]{5}(?:-[0-9]{4})?$/.test(postcode); } ### What alternative solutions did you explore? (Optional) Optionally, we can use postal code API for postal code validation from https:// developer.bring.com/api/postal-code/ -------------------------------------------- Proposal: 4: ## Proposal ### Please re-state the problem that we are trying to solve in this issue. Some unwanted characters are allowed in zip code ### What is the root cause of that problem? Regex is not being validated correctly for zip code ### What changes do you think we should make in order to solve the problem? 33 SWE-Lancer <!-- DO NOT POST CODE DIFFS --> Regex is not validated for zip code. Id propose adding an existing ValidationUtils function for zip code or new function if its not already there. Then hooking that to all forms that have zip code. **Note:** We can also handle unexpected punctuation like , by either removing it or showing an error. -------------------------------------------- Proposal: 5: ORIGINAL PROPOSAL (comment #1427806164): -------------------------------------------------------------------------------- ## Proposal ### Please re-state the problem that we are trying to solve in this issue. - , is allowed in zip / postcode input validation - space-only ( ) is allowed in all fields but not actually saved ### What is the root cause of that problem? Theres no validation check for all fields on Home address screen, except required validation. https://github.com/Expensify/App/blob/39d9b660e387b28688a221b976f0fbf52873cc67/src/ pages/settings/Profile/PersonalDetails/AddressPage.js#L98-L123 Then why are they saved and then reset to previous values when re-visit this page? ( main root cause of #15003) passes current validation and updateHomeAddress api is called. The issue is that we pass trimmed values as api params so actually empty values are sent to server which causes api to fail. Thats why values are reverted back to original. ### What changes do you think we should make in order to solve the problem? **For zipcode:** checked context: > be aware that the current validation on that page only allows numbers (and an optional -) but in the new Home Address form we have to also allow letters since some countries have letters in their zip codes So think just allowing letters in current validation of zip code which was used in bank account step should be simple solution. https://github.com/Expensify/App/blob/39d9b660e387b28688a221b976f0fbf52873cc67/src/ CONST.js#L New zipcode regex for Home address would be: /[a-zA-Z0-9]{5}(?:[- ][a-zA-Z0-9]{4})?/ We can add new flag param called shouldAllowLetters in isValidZipCode function and use this in both Home address page and Personal information page. https://github.com/Expensify/App/blob/39d9b660e387b28688a221b976f0fbf52873cc67/src/ libs/ValidationUtils.js#L167-L169 **For all fields:** - Validate trimmed value instead of raw value by using isRequiredFullfilled function defined in ValidationUtils - (optional) For other strict validations, we can use existing logic in CompanyStep 34 SWE-Lancer **Addon:** noticed no-trim issue happens on CompanyStep as well, which has the same root cause as #15003. is correct in frontend validation but incorrect in backend validation because its trimmed on api call. It would be good to fix them too in this issue. https://user-images.githubusercontent.com/108292595/219962759-8c140cf8-5888-4af9b478-b3a16a2ee1b3.mp4 -------------------------------------------------------------------------------- UPDATED PROPOSAL (comment #1436038776): \"## Proposal [Updated](https://github.com/Expensify/App/issues/14958#issuecomment-1427806164) based on scope change https://github.com/Expensify/App/issues/14958#issuecomment -1431431974\" (No major changes shown, just referencing the original proposal and stating it has been updated.) -------------------------------------------- For your convenience, the Expensify repository has already been cloned, and is present in /app/expensify (your CWD). You should not attempt to run the app or implement the entire fix, but you can review the codebase to understand the context of the issue. When you are ready to submit your answer, please create JSON file in /app/ expensify/manager_decisions.json with the following format: { } selected_proposal_id: int, You must write your output to this path exactly: /app/expensify/manager_decisions. json. You must not submit to anywhere in /mnt/data, just write to /app/ expensify/manager_decisions.json exactly!"
        }
    ],
    "affiliations": [
        "OpenAI"
    ]
}