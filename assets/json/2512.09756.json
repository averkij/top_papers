{
    "paper_title": "MOA: Multi-Objective Alignment for Role-Playing Agents",
    "authors": [
        "Chonghua Liao",
        "Ke Wang",
        "Yuchuan Wu",
        "Fei Huang",
        "Yongbin Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations."
        },
        {
            "title": "Start",
            "content": "MOA: Multi-Objective Alignment for Role-Playing Agents Chonghua Liao1, Ke Wang2 Yuchuan Wu2, Fei Huang2, Yongbin Li2 1 Tsinghua University, 2 Tongyi Lab lch22@mails.tsinghua.edu.cn wk258730@alibaba-inc.com 5 2 0 2 0 ] . [ 1 6 5 7 9 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "agents (RPAs) must siRole-playing conflicting multaneously master many skillsfollowing multi-turn instructions, exhibiting domain knowledge, and adopting consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low learndiversity, or applies reinforcement ing (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations."
        },
        {
            "title": "Introduction",
            "content": "Role-playing agents (RPAs) constitute pivotal research direction, attracting considerable scholarly and industrial interest. Recent advances have demonstrated notable maturity and substantial commercial viability across spectrum of applications, including but not limited to online customer-service systems, automated content generation, interactive entertainment, and conversational non-player characters (NPCs) within digital games (Shao et al., 2023; Wang et al., 2021; Liu et al., 2024; Xu et al., 2024). 1 In the past, research on role-playing primarily concentrated on two aspects: (1) Evaluation, which involved developing improved evaluation benchmarks (Lu et al., 2024; Yang et al., 2024; Samuel et al., 2024; Lu et al., 2025); and (2) Data, which focused on using supervised finetuning (SFT) with synthetic dialogues to enhance general role-playing abilities (Wang et al., 2025c; Tang et al., 2025; Wang et al., 2025b). Currently, SFT remains the dominant paradigm for training RPAs. However, extensive use of SFT has two major drawbacks: (1) it tends to fit the superficial features of the data, leading to suboptimal performance (Wang et al., 2025e; Tang et al., 2025); (2) it restricts the output diversity of the model, and lower diversity is not conducive to further optimization (Cui et al., 2025; Wang et al., 2025a). Some works have attempted to directly transfer RL methods from reasoning tasks to roleplaying. For example, RAIDEN-R1 (Wang et al., 2025e) tries to optimize by using some keywords as labels. Then, keyword matching can be regarded as verifiable reward. However, this ignores two crucial characteristics of role-playing. (1) Roleplaying requires fine-grained rewards to reflect different dimensions of the response. (2) There are conflicts between rewards. This means that the rewards themselves have low correlation, and optimizing one dimension can lead to deterioration in another. For example, long response that answers in bullet points is more likely to achieve high score related to role knowledge. However, such response is more likely to have language style that deviates from the persona. Besides, if we simply use the standard weighted GRPO (Guo et al., 2025), lot of information will be lost. As shown in Figure 1, consider rollouts o2, o3, and oG. They have rewards (1,0,1), (1,1,0), and (0,1,1) respectively. Usually, if we assign the same weights to each rubric Rj, these three rollouts will receive identical advantages and be treated as positive samples. However, consider R1, oG is mistakenly regarded as positive sample. This introduces noise into the optimization process. The policy cannot identify which rollouts are beneficial for specific dimension. It is challenging to learn these conflicting dimensions. Motivated by the above problems, natural question arises: Can we design an algorithm that can train general RPA from multiple fine-grained and even conflicting rubrics? To answer this question, we introduce MultiObjective Alignment (MOA), an RL framework tailored for RPAs. Specifically, MOA comprises several key components: 1) Multi-objective optimization: MOA dynamically identifies the most improving dimension as the pivot dimension. It then assigns weights based on the growth trend. When focusing on optimizing the pivot dimension, it removes distracting samples that perform poorly on the pivot dimension but well on other dimensions. By doing so, it avoids the issue where high rewards on other dimensions lead the model to mistakenly regard such samples as positive during optimization of the pivot dimension. In this way, more stable and efficient policy updates are achieved. 2) Diversified rollout strategy: To encourage exploration and mitigate reward hacking, we prompt the policy model to first generate segment of thought before responding. Furthermore, we mix off-policy outputs from stronger models during advantage estimation, stabilizing training with diverse samples. We conduct extensive experiments on two challanging public benchmarks to validate our method: PersonaGym (Samuel et al., 2024) and RoleMRC (Lu et al., 2025). MOA consistently outperforms both SFT and standard RL baselines (e.g., GRPO) across all metrics, establishing new stateof-the-art results on general role-playing tasks. Notably, using even only an 8B model, MOA achieves comparable performance to strong baselines like GPT-4o and Claude on PersonagGym, and even surpasses GPT-4o by 21.0% on RoleMRC. Our contributions are summarised as follows: We propose novel multi-objective optimization method in RL. To address the issues of low sampling diversity and insufficient quality in SFT models, we develop Diversified Rollout Strategy, including thought-augmented rollout and off-policy guidance. We demonstrate strong empirical gains across model sizes (1.7B8B), offering scalable path toward building more powerful general RPAs."
        },
        {
            "title": "2 Related Work",
            "content": "Reasoning in Large Language Models The latest wave of reasoning-capable large language models (LLMs), such as OpenAI-o1 (Jaech et al., 2024), DeepSeek-R1 (Guo et al., 2025), and Kimik1.5 (Team et al., 2025), have shifted focus from Chain-of-Thought (CoT) (Wei et al., 2022) and supervised fine-tuning (SFT) (Li et al., 2024; Yeo et al., 2025) to reinforcement learning (RL). Contemporary research research has converged on three frontiers: (1) fixing GRPOs inherent limitations (Yu et al., 2025; Liu et al., 2025; Lin et al., 2025), such as length bias and KL divergence constraints; and (2) building smarter data pipelines (Zuo et al., 2025; Zhao et al., 2025; Wang et al., 2025d). (3) focusing on entropy mechanisms to encourage exploration in reinforcement learning (Wang et al., 2025a; Cui et al., 2025; Kang et al., 2025). Some works have also tried to explore the relationship between reasoning and roleplaying (Feng et al., 2025) and to apply RL to role-playing (Wang et al., 2025e; Tang et al., 2025). In addition, MOPO (Agnihotri et al., 2025) focuses on the theory of multi-objective optimization in DPO (Rafailov et al., 2023). However, directly applying methods designed for logical or factual tasks to role-playing does not address the inherently multi-objective nature of role-playing tasks. These methods are either not effective or require laborintensive data crafting. In contrast, we propose novel method that tackles the unique challenges of role-playing and delivers strong empirical gains. Role-Playing Agents with LLMs Role-playing agents (RPAs) (Chen et al., 2024) have drawn wide interest for tasks such as offering emotional companionship (Liu et al., 2024) and enabling virtual interaction (Park et al., 2023). Previously, research on role-playing mainly focused on (1) Data: Using supervised fine-tuning on synthetic dialogues to strengthen general role-playing skills (Wang et al., 2025c; Tang et al., 2025; Wang et al., 2025b); (2) Evaluation: building better evaluation benchmarks (Lu et al., 2024; Yang et al., 2024; Samuel et al., 2024; Lu et al., 2025). Yet few works ask 2 Figure 1: Flowchart of MOA. Given the input q, we first prompt the policy model to generate rollouts with thoughts, and then mix them with off-policy samples. We then score these rollouts using fine-grained rubrics. Based on the reward trends from these rubrics, we dynamically select pivot dimension for optimization and allocate weights. Finally, we eliminate conflicting samples that hinder optimization in the pivot dimension. how to improve general role-playing skills without hand-made data. Most prior synthesis pipelines still lean on human effort and inject only shallow, rule-of-thumb features (Tang et al., 2025; Wang et al., 2025b). Rather than starting from the data as in SFT, we start from the evaluation rubrics themselves to enhance the multi-dimensional capabilities of RPAs."
        },
        {
            "title": "3 Multi-Objective Alignment",
            "content": "Unlike traditional verifiable tasks such as math or coding, role-playing is characterized by (1) multiple reward dimensions and (2) the restricted output diversity of model, making the direct transfer of RL approaches highly non-trivial. In this section, we first recap the widely-used RL algorithm GRPO (Shao et al., 2024), and then present our multi-objective optimization approach. Then, we provide strategies to obtain diverse and high-quality rollouts."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "GRPO first scores every complete rollout trajectory with single scalar, then normalises these scores within the current group of rollouts. Specifically, let πθold denote the policy model before updating. For an input question q, we sample outputs Algorithm 1 Multi-Objective Optimization with GRPO Require: reward tensor RGD, average rewards history buffer RKD, temperature coefficient β. (generations G, reward dims D, number of stored steps K) # 1. estimate importance weight 1: Compute linear regression estimate from H: 2: ˆrd LinearRegressionEstimate(H:,d) 3: Compute mean reward from R: (cid:80)G 1 g=1 rt 4: rt ˆrt rt 5: ut for = 1. . .D 6: wt softmax(utβ) 7: arg maxd wt g,d # 2. remove conflicting samples 8: Find the largest subset satisfying the partial order 9: LargestSubset(R, d) # 3. compute advantage 10: wtR, RG 11: µ mean(R), σ std(R) 12: Ag (cid:40) (R 0, µ)/(σ+ϵ), / 13: Output: advantage = (A1, . . . , AG) RG 3 {o1, . . . , oG} from the current policy LLM πθold, the normalized reward Ai,t is shared across all tokens in oi as the advantage estimate: Ai,l = r(oi) mean({r(oi) oi πθold}) std({r(oi) oi πθold}) . (1) in the tensor denotes the average reward for dimension at step t. Then, we use linear regression to estimate the average reward ˆrt for dimension at ˆrt step t, and obtain the residual ut d. = rt These residuals are converted into probability vector by the softmax operator Then, the GRPO objective function can be written as: wt = exp(cid:0)ut dβ(cid:1) (cid:16) ut jβ j=1 exp (cid:80)D (cid:17) , wt = [wt 1, . . . , wt D], J(πθ) = 1 (cid:88) i= 1 oi t=1 oi (cid:88) {min [ρi,tAi,l, ˆρi,lAi,l]} (2) with probability ratio ρi,l = πθ(oi,lq,oi,<l) (oi,lq,oi,<l) , clipped ratio ˆρi,l = clip(ρi,l; 1 ϵ, 1 + ϵ) and represents the l-th token in the rollout. Here, for simplicity, the KL divergence term is omitted. πθold 3.2 Multi-Objective Optimization key characteristic of role-playing tasks is the multi-dimensional reward structure, where rewards often conflict with each other. How to design strategy that enables the model to learn more distinctive features within group and resolve conflicts between targets is the key issue. Motivated by this, we designed two components: Pivot Dimension Selection and Conflict Rollouts Elimination. The detailed procedure is outlined in Algorithm 1. Pivot Dimension Selection We aim to avoid focusing on too many dimensions simultaneously. Hence, we draw on the idea of curriculum learning (Soviany et al., 2022). At given optimization step, not all dimensions are equally worthy of learning. We should prioritize learning the dimensions that are easier to learn first, and then move on to those that are less so. Specifically, at the current training step t, given group of rollouts associated with one input query q, we collect reward matrix = [rg,d] RGD, where rg,d = rd(og) is the d-th dimensional reward of the g-th rollout (g = 1, . . . , G; = 1, . . . , D). We want to identify which dimension is the most worthy of learning at step t. natural approach is to greedily select the dimension that shows the highest improvement trend at the current step. We first calculate the average reward for each dimension at every step rt d. And these average rewards are stored in the history buffer as reward curves. d] RKD, This results in tensor of size = [rk where represents the number of retained training steps, from 1 to 1. Each element rt where β > 0 is temperature hyper-parameter. Hence each dimension obtains an importance weight wd that reflects how much it currently outperforms its own short-term trend. The dimension with the largest reward increase currently represents the easiest learning difficulty and is the most worthy of learning at the current step. Thus, we select this dimension as the pivot dimension for step t. Algorithm 2 LargestSubset Require: wt RD, RGD, [0, D] # Compute weighted sums and sort 1: pairs = {(Rg,d, wtRg) [0, G]} # Sort pairs by dimension and weighted sum 2: sorted_pairs = sort(pairs) 3: Initialize LIS: LIS = [] 4: for each (x, y) sorted_pairs do # Find insertion position: 5: 6: 7: 8: position = binary_search(LIS, y) if position = length(LIS) then"
        },
        {
            "title": "Append y to LIS",
            "content": "else 9: Update LIS[position] = # Extract corresponding subset in [0, G]: 10: = {g [0, G] (Rg,d, wtRg) LIS} 11: Output: Theorem 1. The residualsoftmax scheme yields strictly larger expected immediate improvement than the uniform-weight RL. Proof sketch. The analysis follows the standard first-order performance approximation used in policy gradient theory (Kakade and Langford, 2002; Schulman et al., 2015). Let gd denote the policy gradient contribution from reward dimension and Gd,i = gi the corresponding Gram matrix, so that the one-step expected improvement is η vGv for weight vector v. first-order Taylor expansion + β of the softmax weights yields wd 1 (ud u), where ud is the residual between the current reward and its historical trend. Substituting this into the expression for expected improvement shows that the excess gain over uniform weighting is proportional to Cov(ud, gd2), i.e., the covariance between residuals and gradient magnitudes. Intuitively, the weighting scheme allocates larger step sizes to reward dimensions that outperform their historical baseline and are therefore likely to have stronger gradients, concentrating updates where they yield the most progress and accelerating overall reward improvement. Concentrating gradient steps along these rising dimensions increases the effective ascent per update, leading to faster reward improvement than using static weights. We provide quantitative experimental results in Table 1 and detailed proofs in the Appendix. Conflict Rollouts Elimination Then, for the pivot dimension with the largest improvement, we aim to eliminate conflicting samples that are negative in dimension but have high rewards in other dimensions. We define relatively relaxed partial order relation. For two rollouts oi oj if and only if ri,d > rj,d and wRi > wRj, where Ri denotes the i-th row of R. Thus, our goal becomes finding the largest subset of all rollouts = {o1, . . . , oG} such that oi, oj O, oi oj or oj oi. This problem can be easily solved using dynamic programming. We denote the method for eliminating conflicting samples as = LargestSubset(R, d). Furthermore, after calculating the advantage, we set the advantage of rollouts not in to 0, meaning that we do not learn from these conflicting samples. The algorithm details are listed in Algorithm 2."
        },
        {
            "title": "3.3 Diversified Rollout Strategy",
            "content": "Guaranteeing both quality and diversity of rollout samples during RL is hard problem. In our pilot runs, we observed that increasing the sampling temperature of an SFT-tuned model hardly changed its training-reward curve and produced little diversity. We address this challenge with ThoughtAugmented Rollout and Off-Policy Guidance."
        },
        {
            "title": "Inspired",
            "content": "Thought-Augmented Rollout by Chain-of-Thought (CoT) (Wei et al., 2022), several works (Feng et al., 2025; Wang et al., 2025e; Tang et al., 2025) have explored whether explicit reasoning improves role-playing. In pilot studies, we tested this on closed-source models. We take Figure 2: Prompt Pthink to guide models in role-playing tasks. Figure 3: Performance of Claude-3.7 on PersonaGym, with and without thinking Claude-3.7s performance on PersonaGym as an example. Given such prompt Pthink in Figure 2, we simply prompt the model to think first and then respond. Formally, given the model and the input query q, the output is = M(Pthink(q)) . It can be seen from Figure 3 that on almost all dimensions of PersonaGym, the capabilities of Claude-3.7 are enhanced. We also observed similar trend on GPT-4o and other datasets. This suggests that incorporating thinking in role-playing may enhance the quality of rollouts. Off-Policy Guidance Since our reward comes from \"LLM-as-a-Judge\", such reward is very easy to hack. For example, longer rollouts that contain more information are more likely to receive higher rewards related to persona knowledge, even if such information may be redundant. To address this, following LUFFY (Yan et al., 2025), we introduce Off-Policy Guidance. An output from strong closed-source model is mixed with the rollout from the policy model itself for advantage calculation. 5 By introducing such an off-policy guidance, we mitigate reward hacking and increase diversity within group through outputs from different models."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Settings Datasets We selected RoleMRC (Lu et al., 2025) and OpenCharacter (Wang et al., 2025b) as our training sets, randomly chose 10,000 samples from each, combined them as the training set for the RL phase, and used the remaining 310k samples as the training set for the SFT phase. This guarantees that the data used in the RL and SFT stages are completely non-overlapping. Reward Design Judging single role-playing response demands broad range of evaluations. Earlier studies borrowed the math-task recipe: tag handful of keywords as ground truth and score with hand-written rules (Wang et al., 2025e). This (1) only checks factual content and skips vital axes such as human-likeness; (2) it also collapses when the persona or scene shifts. To overcome these limitations, we conduct systematic survey of existing benchmarks and assemble compact yet universal set of reward signals. Basic Dialogue: Covers capabilities such as user-intent understanding and multi-turn coherence. It evaluates the responses adherence to the users input, correctness of intent recognition, and the absence of obvious errors (e.g., failure to use first-person pronouns). Persona Knowledge: Evaluate role persona setting and persona-related knowledge. Evaluate whether the response conforms to the persona setting and knowledge, whether it exceeds the characters knowledge range, and whether phenomena such as over-reasoning or information redundancy exist. Style Compliance: Assesses fidelity to persona traits, colloquial tone, and signature phrases; emotional expression and dialogue advancement. The goal is to measure how consistently the persona preserves its distinctive linguistic identity across contexts. We devise fine-grained rubrics for every dimension and adopt the \"LLMs-as-Judges\" (Zheng et al., 2023) paradigm to quantify output quality. We utilize GPT-4o for evaluation. Formally, for persona p, query q, and candidate response oi, the scalar reward on dimension is produced by applying strong closed-source model to the rubric-conditioned prompt Rj(p, q, oi): rj(oi) = M(Rj(p, q, oi)). Benchmarks To comprehensively evaluate general role-playing capability, we selected the following publicly available benchmarks. These benchmarks cover variety of conversational scenarios, as well as complex knowledge scopes, persona styles, complex instruction following, and multiturn instruction following tasks. PersonaGym (Samuel et al., 2024) evaluates role-playing agents across diverse, personarelevant environments. The evaluation covers 5 dimensions: Expected Action (EA), Linguistic Habits (LH), Persona Consistency (PC), Toxicity Control (TC) and Action Justification (AJ). This benchmark includes 200 diverse personas, 150 environments and 10 automatically generated, persona-specific questions. Each dimension is rated on discrete 1-to-5 rubric, where 1 indicates strong misalignment with the persona and 5 reflects perfectly faithful, persona-consistent behavior. RoleMRC (Lu et al., 2025) is fine-grained composite benchmark for role-playing and It comprises 1.4k instruction-following. synthesized instructions covering three scenario types: Free Chat, On-scene MRC Dialogues, and Ruled Chats. Evaluation is conducted along 5 dimensions: Knowledge Range (KR), Style Compliance (SC), Nested Instruction-following (NI), Multi-turn Instruction-following (MT), and Instruction Priority (IP). Each dimension is scored in reference-free, binary (0/1) manner, yielding accuracy percentages. All benchmark evaluations adopt the \"LLMs-asJudges\" paradigm (Zheng et al., 2023). For the evaluation model, we directly follow the original evaluation method, employing GPT-4o (gpt-4o-2024-1120) as the judge. For PersonaGym, each question is tested 3 times and the average is taken. During testing, we do not explicitly prompt the model to think, which aligns with real-world scenarios. comprehensive description of the evaluation dimensions and protocols is provided in the Appendix. 6 Table 1: Overall performance on various role-playing tasks, with results for each dataset obtained using llm-as-judge. The best results are indicated in bold. Results from larger or closed-source models are presented in gray for reference. Method PersonaGym RoleMRC EA TC LH PC AJ Avg. KR SC NI MT IP Avg. Close-source Models GPT-4o Claude-3.7 4.98 4.90 4.96 4. 4.41 4.50 4.96 4.90 4.97 4.82 4.85 4.82 0.46 0.50 0.68 0. 0.70 0.69 0.46 0.43 0.66 0.47 0.62 0.59 Qwen3-8B-Base (Yang et al., 2025) Qwen3-8B-Base SFT RL-based Method RLOO GRPO MOA (RLOO) MOA (GRPO) 4.67 4.70 4. 4.71 4.67 4.58 0.49 0.33 0. 0.66 0.88 0.57 4.62 4.17 4.71 4.84 4.76 4.84 4.82 4.81 4.21 3.95 4.31 4. 4.68 4.61 4.71 4.79 4.81 4.14 4.89 4.92 4.62 4.34 4.69 4.75 0.58 0.51 0.65 0.67 0.42 0.33 0.50 0.69 0.55 0.49 0.60 0. 0.66 0.69 0.70 0.77 0.94 0.92 0.94 0.93 0.63 0.59 0.68 0.75 Baselines To thoroughly validate the effectiveness of our approach, we compare it against comprehensive set of representative baselines: 1. Strong closed-source models, including GPT4o (gpt-4o-2024-11-20) and Claude (claude3-7-sonnet-20250219) 2. Models that have been supervised-fine-tuned on public datasets. This represents the strong baseline OpenCharacter (Wang et al., 2025b). 3. Several RL based methods, including vanilla GRPO, and RLOO (Ahmadian et al., 2024). Training Details We selected Llama-3.1-8BInstruct (Grattafiori et al., 2024), Qwen3-1.7BBase, and Qwen3-8B-Base (Yang et al., 2025) as the base models. All our experiments utilized 8 NVIDIA A100-80GB GPUs. For SFT, we only train for 1 epoch. We employed cosine learning rate scheduler with learning rate of 5 105 and warmup ratio of 0.05. For RL based methods, we utilized the Open-R1 framework and set the learning rate to 1 106. During training, we design the group size = 16. For MOA, 15 samples are drawn from the policy model via on-policy sampling, and 1 sample is drawn from GPT-4o via off-policy sampling. The total batch size was set to 192. While for vanilla GRPO, all 16 rollouts are on-policy samples. We used VLLM sampling with top-p value of 0.9, and set the sampling temperature to 1.5. The maximum completion length was set to 1200. All experiments were run for 1000 steps. 4.2 Main Results Our MOA achieves strong results. As shown in Table 1, MOA is comparable to GPT-4o in dimensions related to language style, such as LH, and surpasses the strong baseline Claude in AJ. In RoleMRC, our MOA outperforms GPT-4o and Claude in almost all aspects, especially in dimensions related to complex multi-turn conversations and instruction following, such as MT and IP. On average, MOA outperforms GPT-4o by 21%, demonstrating the effectiveness of our method. Very interestingly, we find that vanilla GRPO initialized from an SFT checkpoint still performs poorly even at high sampling temperature (t = 1.5); we attribute this to the low inherent diversity of the SFT model and the difficulty of steering optimization with rubric-only rewards. To provide deeper understanding of the effects of thinking and multi-objective optimization, we present some reward curves. As shown in Figure 4, it can be seen that after using multi-objective optimization, the reward always rises faster. This observation aligns with our intuition and theoretical proofs. We also compared with MOA-o, which is an experiment where the model is not required to think but includes one off-policy sample in the rollout. When training on the same data, the starting point of MOA-o without requiring the model to output thinking is higher, because introducing thinking to the rollout process leads to decline in generation quality. However, the growth of the MOA-o curve slows down in the later stages of training. Therefore, we believe that incorporating role-related thinking can help the model escape 7 (a) Basic Dialogue (b) Persona Knowledge. (c) Style Compliance. Figure 4: Smoothed training reward curves of Qwen3-8B-SFT across different dimensions. It can be seen that after using multi-objective optimization, the reward always rises faster. Besides, the starting point of MOA-o is higher. This is because introducing thinking leads to decline in generation quality. However, the growth of the MOA-o curve slows down in the later stages of training. the local optimum after the SFT stage, facilitating further optimization. Table 3: Comparison of the effects of introducing thinking and multi-objective optimization. 4.3 Ablation Study In this subsection, we provide ablation results and insightful analyses using the results on PersonaGym as an example. More detailed results can be found in the Appendix. Table 2: Ablation studies on model types and sizes demonstrate that MOA significantly enhances performance across various models. Method GPT-4o Claude-3.7 PersonaGym EA 4.98 4.90 TC 4.96 4.97 LH 4.41 4.50 PC 4.96 4.90 AJ Avg. 4.97 4.82 4.85 4.82 Qwen3-1.7B-Base (Yang et al., 2025) SFT MOA 4.18 4. 4.65 4.90 3.77 4.07 4.39 4.41 4.26 4.80 4.25 4.53 Llama-3.1-8B-Instruct (Grattafiori et al., 2024) SFT MOA 4.51 4.95 4.55 4.85 3.94 4.63 4.63 4.90 4.54 4. 4.43 4."
        },
        {
            "title": "Algorithms",
            "content": "To demonstrate MOAs effectiveness across different model sizes and algorithms, we extend it to the weaker Qwen3-1.7B and Llama-3.1-8B-Instruct models and also apply MOA to other algorithms such as RLOO (Ahmadian et al., 2024) on Qwen38B. As shown in Table 4, across different scales and model types, MOA achieves substantial improvements over SFT. On LLaMA-3.1-8B-Instruct, MOA outperforms GPT-4o and Claude, demonMethod PersonaGym EA TC LH PC AJ Avg. Qwen3-8B-Base (Yang et al., 2025) SFT + GRPO + MOA-t + MOA 4.67 4.17 4.77 4.84 4.70 4.84 4.83 4.81 4.18 3.95 4.29 4.40 4.71 4.61 4.78 4.79 4.67 4.14 4.84 4. 4.58 4.34 4.70 4.75 strating its powerful effectiveness. This experiment indicates that our MOA is not tied to specific model. In addition, as can be seen from Table 1, MOA also achieves significant improvements over RLOO, indicating that our method is not tied to specific algorithm."
        },
        {
            "title": "4.3.2 The Effect of Thinking and",
            "content": "Multi-Objective Optimization We compared different factors in the method design. MOA-t represents the experiment without multi-objective optimization. The results show that GRPO is not stable compared to SFT. However, after introducing thinking and off-policy guidance, the models performance is improved. Furthermore, with the addition of multi-objective optimization, the models performance is further enhanced."
        },
        {
            "title": "5 Conclusions",
            "content": "In this paper, we propose MOA, novel multiobjective optimization RL method for training general-purpose RPAs. Compared to mainstream SFT-based approaches, MOA can achieve finegrained, multi-dimensional optimization through series of rubrics. Experiments show that MOA can 8 achieve performance comparable to strong models such as GPT-4o. This opens promising research avenue for building powerful general RPAs."
        },
        {
            "title": "6 Limitations",
            "content": "While MOA demonstrates significant improvements in training general-purpose RPAs, there are several limitations to our approach: 1. The requirement for LLMs-as-Judges introduces additional computational overhead compared to rule-based reward systems, making MOA more resource-intensive than pure rulebased RL. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, and 1 others. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720. 2. One potential direction is to have the model self-score to reduce reliance on strong external model for scoring, but this approach has not yet been explored. Sham Kakade and John Langford. 2002. Approximately optimal approximate reinforcement learning. In Proceedings of the nineteenth international conference on machine learning, pages 267274. 3. While the multi-objective approach has been validated on multi-dimensional role-playing tasks, its effectiveness has not been tested in broader domains such as mathematics or coding. We leave these potential directions for future work."
        },
        {
            "title": "References",
            "content": "Akhil Agnihotri, Rahul Jain, Deepak Ramachandran, and Zheng Wen. 2025. Multi-objective preference optimization: Improving human alignment of generative models. arXiv preprint arXiv:2505.10892. Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. 2024. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740. Nuo Chen, Yan Wang, Yang Deng, and Jia Li. 2024. The oscars of ai theater: survey on role-playing with language models. arXiv preprint arXiv:2407.11484. Zilin Kang, Chonghua Liao, Tingqiang Xu, and Huazhe Xu. 2025. Entropy regularizing activation: Boosting continuous control, large language models, and image classification with activation as entropy constraints. arXiv preprint arXiv:2510.08549. Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, and Houwen Peng. 2024. Common 7b language models already possess strong math capabilities. arXiv preprint arXiv:2403.04706. Zhihang Lin, Mingbao Lin, Yuan Xie, and Rongrong Ji. 2025. Cppo: Accelerating the training of group relative policy optimization-based reasoning models. arXiv preprint arXiv:2503.22342. Chenxiao Liu, Zheyong Xie, Sirui Zhao, Jin Zhou, Tong Xu, Minglei Li, and Enhong Chen. 2024. Speak from heart: an emotion-guided llm-based multimodal method for emotional dialogue generation. In Proceedings of the 2024 International Conference on Multimedia Retrieval, pages 533542. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. 2025. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, and 1 others. 2025. The entropy mechanism of reinforcement learning arXiv preprint for reasoning language models. arXiv:2505.22617. Xiachong Feng, Longxu Dou, and Lingpeng Kong. 2025. Reasoning does not necessarily improve roleplaying ability. Preprint, arXiv:2502.16940. Junru Lu, Jiazheng Li, Guodong Shen, Lin Gui, Siyu An, Yulan He, Di Yin, and Xing Sun. 2025. Rolemrc: fine-grained composite benchmark for role-playing and instruction-following. arXiv preprint arXiv:2502.11387. Keming Lu, Bowen Yu, Chang Zhou, and Jingren Zhou. 2024. Large language models are superpositions of all characters: Attaining arbitrary role-play via selfalignment. arXiv preprint arXiv:2401.12474. 9 Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741. Vinay Samuel, Henry Peng Zou, Yue Zhou, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Ameet Deshpande, Karthik Narasimhan, and Vishvak Murahari. 2024. Personagym: Evaluating persona agents and llms. arXiv preprint arXiv:2407.18416. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. Trust region policy optimization. In International conference on machine learning, pages 18891897. PMLR. Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023. Character-llm: trainable agent for roleplaying. arXiv preprint arXiv:2310.10158. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Preprint, arXiv:2402.03300. Petru Soviany, Radu Tudor Ionescu, Paolo Rota, and Nicu Sebe. 2022. Curriculum learning: survey. International Journal of Computer Vision, 130(6):1526 1565. Yihong Tang, Kehai Chen, Muyun Yang, Zhengyu Niu, Jing Li, Tiejun Zhao, and Min Zhang. 2025. Thinking in character: Advancing role-playing agents with role-aware reasoning. arXiv preprint arXiv:2506.01748. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, and 1 others. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, and 1 others. 2025a. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939. Xiaoyang Wang, Hongming Zhang, Tao Ge, Wenhao Yu, Dian Yu, and Dong Yu. 2025b. Opencharacter: Training customizable role-playing llms with large-scale synthetic personas. arXiv preprint arXiv:2501.15427. Xintao Wang, Heng Wang, Yifei Zhang, Xinfeng Yuan, Rui Xu, Jen-tse Huang, Siyu Yuan, Haoran Guo, Jiangjie Chen, Shuchang Zhou, and 1 others. 2025c. Coser: Coordinating llm-based persona simulation of established roles. arXiv preprint arXiv:2502.09082. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, and 1 others. 2025d. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571. Zongsheng Wang, Kaili Sun, Bowen Wu, Qun Yu, Ying Li, and Baoxun Wang. 2025e. Raiden-r1: Improving role-awareness of llms via grpo with verifiable reward. arXiv preprint arXiv:2505.10218. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Zhenyu Xu, Hailin Xu, Zhouyang Lu, Yingying Zhao, Rui Zhu, Yujiang Wang, Mingzhi Dong, Yuhu Chang, Qin Lv, Robert Dick, and 1 others. 2024. Can large language models be good companions? an llm-based eyewear system with conversational common ground. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 8(2):141. Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. 2025. Learning to reason under off-policy guidance. arXiv preprint arXiv:2504.14945. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Bohao Yang, Dong Liu, Chen Tang, Chenghao Xiao, Kun Zhao, Chao Li, Lin Yuan, Guang Yang, Lanxiao Huang, and Chenghua Lin. 2024. Simschat: customisable persona-driven role-playing agent. arXiv e-prints, pages arXiv2406. Edward Yeo, Yuxuan Tong, Xinyao Niu, Graham Neubig, and Xiang Yue. 2025. Demystifying long chainof-thought reasoning in LLMs. In ICLR 2025 Workshop on Navigating and Addressing Data Problems for Foundation Models. Xiaoyang Wang, Chen Li, Jianqiao Zhao, and Dong Yu. 2021. Naturalconv: chinese dialogue dataset towards multi-turn topic-driven conversation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 1400614014. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, and 1 others. 2025. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Yang Yue, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. 2025. Absolute zero: Reinforced self-play reasoning with zero data. Preprint, arXiv:2505.03335. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, and 1 others. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623. Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, and Bowen Zhou. 2025. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084. 11 Why MOA Learns Faster: Short Proof vs. Weighted GRPO We analyze local one-step expected-improvement comparison between (i) fixed-weight GRPO-style policy gradient that uses uniform weights and (ii) dynamic weighting scheme that forms weights by applying softmax to residuals (observed-minustrend) computed per reward dimension. Under mild modelling assumptions (orthogonal gradients, residuals linearly related to gradient magnitudes plus zero-mean noise, and small softmax temperature), we derive simple lower bound showing the residualsoftmax scheme yields strictly larger expected immediate improvement whenever the residuals have positive covariance with squared gradient norms. The bound is explicit in terms of the softmax temperature and signal-to-noise ratio. A.1 Notation and setup Fix N. For policy parameter vector θ Rp and each reward dimension {1, . . . , D} denote gd := θJd(θ) Rp, the (true) policy gradient for reward-dimension d. Let the Gram matrix RDD, Gd,i := d gi. We consider small gradient-step updates of the form (cid:88) θ = η vd(cid:98)gd, d=1 where = (v1, . . . , vD) is probability weight vector (nonnegative, sums to one), η > 0 is the step-size, and (cid:98)gd are unbiased estimators of gd (assumed mean gd). To first order (linearization), the expected change in the scalarized objective Jv(θ) := (cid:80) vdJd(θ) is E[Jv] η vGv. We compare two weighting schemes: Uniform fixed weights: αd 1/D. ResidualSoftmax dynamic weights: observe residuals = (u1, . . . , uD) RD at the current step and set wd := exp(βud) i=1 exp(βui) (cid:80)D , where β is the softmax inversetemperature (we will take β small). For algebraic simplicity we will work under the diagonal-gram assumption (gradients of different reward dimensions are pairwise orthogonal). This isolates the effect of weighting by gradient magnitudes and yields transparent bound. Assumption 1 (Diagonal Gram / orthogonality). For all = i, gi = 0. Thus = diag(s) where sd := gd2 0, = 1, . . . , D. Under Assumption 1 we have the one-step expected improvement E[Jv] η (cid:88) d=1 v2 dsd. We model the residuals ud as noisy linear functions of gradient magnitudes: Assumption 2 (Linear residual model). There exists scalar > 0 and random noise vector ξ = (ξ1, . . . , ξD) with E[ξd] = 0 and Cov(ξd, ξi) = 0 for = (independent across dimensions), such that ud = gd + ξd = sd + ξd. We denote σ2 ξ across for simplicity). := Var(ξd) (assumed identical Assumption 2 formalizes that the residuals carry signal proportional to gradient magnitude, corrupted by zero-mean noise. This captures the residuals predictive of short-term gradient strength premise. A.2 Main quantitative local result (small-β expansion) We analyze the difference in expected linearized improvement between the dynamic residualsoftmax weighting and the uniform weighting α. For analytic clarity we use Taylor expansion of the softmax for small β. Theorem 2 (Small-β positive-improvement bound). Under Assumptions 1 and 2, let α RD be the uniform vector αd = 1/D. Fix small inversetemperature parameter β and define the softmax weights w(β; u) by wd(β; u) = eβud i=1 eβui (cid:80)D . Then, to second order in β, the expected difference in the first-order-in-η improvement satisfies E(cid:2)η wGwη αGα(cid:3) = η 2β D2 Cov(cid:0)ud, sd (cid:1) + O(β2), 12 (cid:80)D where Cov(ud, sd) denotes the (population) covariance across coordinates 1 d=1(ud u)(sd s). In particular, if Cov(ud, sd) > 0 and β > 0 is sufficiently small, then the residualsoftmax scheme yields strictly larger expected immediate improvement than the uniform-weight GRPO: E(cid:2)Jw (cid:3) > E(cid:2)Jα (cid:3) to leading order in β. Proof. Under Assumption 1 we have = diag(s) and wGw = (cid:88) w2 dsd. We will expand wd(β; u) in powers of β. Let d= := (cid:88) i=1 eβui. Using the expansion eβui = 1 + βui + 1 O(β3) and = + β (cid:80) 2 β2 (cid:80) O(β3), we get ui + 2 β2u2 u2 + + wd = 1 + βud + 1 ui + 1 2 β2u2 2 β2 (cid:80) + O(β3) i + O(β3) + β (cid:80) . Performing series division (or using the fact that for small β, wd = 1 (ud u) + O(β2), where (cid:80) := 1 ui), we obtain the first-order expansion + β wd ="
        },
        {
            "title": "1\nD",
            "content": "+ β (cid:0)ud u(cid:1) + O(β2). (3) where Cov(ud, sd) := 1 (cid:80) and = 1 (cid:80) d(ud u) = 0.) Thus (cid:80)D d=1(ud u)(sd s) sd. (The term drops because wGw αGα = 2β Cov(ud, sd) + O(β2). Multiplying by η and taking expectation over the residual noise (recall sd is fixed given θ and random via ξ), we obtain E(cid:2)η(wGw αGα)(cid:3) = η 2β E[Cov(ud, sd)] +O(β2). By Assumption 2, ud = sd + ξd with E[ξd] = 0 and ξd independent of sd, so E[Cov(ud, sd)] = Cov(c sd, sd). In particular if the sample covariance Cov(ud, sd) > 0 (or equivalently > 0 and sd (cid:55) sd yields positive covariance the mapping under the empirical distribution across d), then for sufficiently small positive β the leading-order term dominates the remainder O(β2), and therefore the expected difference is positive. This proves the theorem. Corollary 1 (Model with additive zero-mean noise). Assume the linear model of Assumption 2 with > 0, and assume the coordinates sd are not all equal. Then for small enough β > 0 the expected immediate improvement under residualsoftmax weights is strictly larger than under uniform weights. Proof. Under the linear residual model, Squaring and keeping terms up to first order in β, Cov(ud, sd) = Cov(c sd + ξd, sd) sd, sd), = Cov( w2 ="
        },
        {
            "title": "1\nD2 +",
            "content": "2β D2 (cid:0)ud u(cid:1) + O(β2)."
        },
        {
            "title": "Therefore",
            "content": "wGw = (cid:88) d=1 sd (cid:18) 1 D2 + (cid:19) 2β D2 (ud u) +O(β2). Since αGα = (cid:80) sd 1 D2 , subtracting yields wGw αGα = 2β D2 (cid:88) d=1 sd(ud u) + O(β2). Rewrite the finite sum as covariance times D: (cid:88) d=1 sd(ud u) = Cov(ud, sd), 13 since the noise ξ has zero mean and is indepensd, sd) > dent of sd. If sd are not identical, Cov( and identity are monotone in0 because both sd. larger sd gives larger creasing functions: Hence the covariance is positive, and Theorem 2 applies. Ablation Study on the Design of Multi-Objective Optimization In this section, we experimented with several other designs of multi-objective optimization methods to demonstrate the optimality of our approach. We use the results on PersonaGym with the Qwen2.5-1.5B model as an example. Below are several design schemes we explored. 1. MOA-µ: Since we need to aggregate information from different dimensions as late as possible, in this scheme, we attempted to optimize multiple dimensions sequentially. Given group of rollouts and dimensions, the rollouts will be optimized for iterations. In iteration d, we use all Rg,d(g [0, G]) to compute the advantage Ad, and then use this advantage to calculate the loss, performing backpropagation times separately. 2. MOA-σ: We consider learning the most uncertain samples by calculating the standard deviation σ1, . . . , σD for each of the dimensions of the reward matrix R. We then optimize only along the dimension with the largest standard deviation, discarding information from the other dimensions. Table 4: Ablation studies on the design of multiobjective optimization Method GPT-4o Claude-3.7 PersonaGym EA 4.98 4. TC 4.96 4.97 LH 4.41 4.50 PC 4.96 4. AJ Avg. 4.97 4.82 4.85 4.82 Qwen2.5-1.5B-Instruct (Yang et al., 2025) SFT GRPO MOA-µ MOA-σ MOA 4.20 4.21 4.35 4.29 4.40 4.78 4.76 4.76 4.77 4.83 3.80 3.87 3.87 3.89 4.13 4.33 4.30 4.31 4.40 4.55 4.39 4.47 4.47 4.40 4.61 4.30 4.32 4.35 4.35 4. We can see that MOA-σ shows no significant improvement over GRPO, while MOA-µ shows slight improvement. We believe that, considering the relationship between variance and mean in binomial distribution σ2 = Gµ(1 µ), selecting dimensions based on variance is equivalent to selecting based on mean in this case. The mean of dimension is determined by the difficulty of the dimension and the properties of the model. Although MOA-µ has the potential for improvement, using samples multiple times means that when iterations > 1, the model has already been updated. This causes discrepancy between the training distribution and the model distribution, leading to unstable training. Therefore, we do not adopt this method either."
        },
        {
            "title": "C Full Results",
            "content": "In Table 5, we provide full results on PersonaGym and RoleMRC."
        },
        {
            "title": "Dimensions",
            "content": "D.1 PersonaGym Expected Action (EA): In this task, persona agent encounters scenario that requires selecting an action. It reveals whether agents can identify and choose actions that maximize expected utility while staying within their persona constraints. Linguistic Habits (LH): This evaluates whether agents adhere to communication patterns appropriate for their persona, assessing if their linguistic choices (such as jargon, syntax, tone, and speech style) match the expected norms for their persona. Persona Consistency (PC): This examines the consistency of agents with their established persona attributes when directly questioned, ensuring that agents uphold the prescribed persona characteristics under direct inquiry, which is fundamental requirement. Toxicity Control (TC): This examines responses to potentially provocative prompts targeting persona-relevant sensitive topics. The scoring system awards higher scores for appropriate responses and lower scores for toxic ones, directly implementing prescriptive guidelines for responsible agent behavior within ethical boundaries. Action Justification (AJ): This requires the RPA to explain its actions in specific scenarios. D.2 RoleMRC Knowledge Range (KR): concentrates on identifying answerable questions (\"Answer\") versus refusal situations (\"Refusal\") within the context of on-scene machine reading comprehension (MRC) dialogues. Style Compliance (SC): assesses whether the model can precisely generate role-specific responses such as \"Answer,\" \"No Answer,\" \"Refusal,\" and \"Attempt\" in On-scene MRC Dialogues, without veering into narration. 14 Table 5: Overall performance on various role-playing tasks, with results for each dataset obtained using llm-as-judge. Method PersonaGym RoleMRC EA TC LH PC AJ Avg. KR SC NI MT IP Avg. Close-source Models GPT-4o Claude-3.7 4.98 4.90 4.96 4.97 4.41 4.50 4.96 4.90 4.97 4. 4.85 4.82 0.46 0.50 0.68 0.86 0.70 0.69 0.46 0.43 0.66 0. 0.62 0.59 Qwen3-1.7B-Base (Yang et al., 2025) Qwen3-1.7B-Base + SFT RL-based Method + GRPO + MOA-o + MOA-t + MOA Llama-3.1-8B-Ins + SFT RL-based Method + GRPO + MOA-o + MOA-t + MOA Qwen3-8B-Base + SFT RL-based Method + GRPO + MOA-o + MOA-t + MOA 4.18 4.65 3.77 4. 4.26 4.25 0.46 0.35 0.46 0. 0.72 0.51 4.29 4.33 4.49 4.47 4.89 4.86 4.89 4.90 3.79 3.73 3.90 4.07 4.50 4.47 4.48 4. 4.58 4.71 4.75 4.80 4.41 4.42 4.50 4.53 0.67 0.62 0.67 0.69 0.69 0.86 0.69 0.70 0.51 0.54 0.64 0.65 0.49 0.44 0.66 0. 0.60 0.54 0.94 0.88 0.59 0.60 0.72 0.73 Llama-3.1-8B-Instruct (Grattafiori et al., 2024) 4.51 4.55 3. 4.63 4.54 4.43 0.51 0.35 0. 0.68 0.95 0.61 4.04 4.43 4.87 4.95 4.85 4.74 4.87 4.85 3.77 3.86 4.44 4. 4.51 4.61 4.88 4.90 4.01 4.50 4.94 4.98 4.24 4.43 4.80 4.86 0.52 0.63 0.41 0.53 0.26 0.60 0.82 0.99 0.43 0.61 0.55 0. 0.67 0.62 0.62 0.58 0.94 0.71 0.79 0.81 0.56 0.63 0.64 0.71 Qwen3-8B-Base (Yang et al., 2025) 4.67 4. 4.18 4.71 4.67 4.58 0.49 0. 0.51 0.66 0.88 0.57 4.17 4.76 4.77 4.84 4.84 4.77 4.83 4. 3.95 4.30 4.29 4.40 4.61 4.80 4.78 4.79 4.14 4.92 4.84 4.92 4.34 4.71 4.70 4.75 0.51 0.44 0.67 0.67 0.33 0.83 0.64 0. 0.49 0.43 0.63 0.68 0.69 0.30 0.70 0.77 0.92 0.37 0.90 0.93 0.59 0.47 0.71 0.75 Nested Instruction-following (NI), Multi-turn Instruction-following (MT), Instruction Priority (IP): given complex higher-level constraints, these dimensions evaluates whether the models responses meet the requirements of these constraints."
        },
        {
            "title": "E Prompts",
            "content": "Below, we list the prompts used for reward scoring. 15 Figure 5: Reward prompt for Basic Dialogue 16 Figure 6: Reward prompt for Persona Knowledge Figure 7: Reward prompt for Style Compliance"
        }
    ],
    "affiliations": [
        "Tongyi Lab",
        "Tsinghua University"
    ]
}