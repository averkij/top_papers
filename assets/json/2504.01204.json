{
    "paper_title": "Articulated Kinematics Distillation from Video Diffusion Models",
    "authors": [
        "Xuan Li",
        "Qianli Ma",
        "Tsung-Yi Lin",
        "Yongxin Chen",
        "Chenfanfu Jiang",
        "Ming-Yu Liu",
        "Donglai Xiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Articulated Kinematics Distillation (AKD), a framework for generating high-fidelity character animations by merging the strengths of skeleton-based animation and modern generative models. AKD uses a skeleton-based representation for rigged 3D assets, drastically reducing the Degrees of Freedom (DoFs) by focusing on joint-level control, which allows for efficient, consistent motion synthesis. Through Score Distillation Sampling (SDS) with pre-trained video diffusion models, AKD distills complex, articulated motions while maintaining structural integrity, overcoming challenges faced by 4D neural deformation fields in preserving shape consistency. This approach is naturally compatible with physics-based simulation, ensuring physically plausible interactions. Experiments show that AKD achieves superior 3D consistency and motion quality compared with existing works on text-to-4D generation. Project page: https://research.nvidia.com/labs/dir/akd/"
        },
        {
            "title": "Start",
            "content": "https://research.nvidia.com/labs/dir/akd/ Xuan Li1,2, Qianli Ma2 Tsung-Yi Lin2 Yongxin Chen2 Chenfanfu Jiang1 Ming-Yu Liu2 Donglai Xiang2 1 UCLA, 2 NVIDIA 5 2 0 2 1 ] . [ 1 4 0 2 1 0 . 4 0 5 2 : r Figure 1. By incorporating articulation into static assets, AKD synthesizes realistic motions distilled from large video diffusion models."
        },
        {
            "title": "Abstract",
            "content": "We present Articulated Kinematics Distillation (AKD), framework for generating high-fidelity character animations by merging the strengths of skeleton-based animation and modern generative models. AKD uses skeleton-based representation for rigged 3D assets, drastically reducing the Degrees of Freedom (DoFs) by focusing on joint-level control, which allows for efficient, consistent motion synthesis. Through Score Distillation Sampling (SDS) with pretrained video diffusion models, AKD distills complex, articulated motions while maintaining structural integrity, overcoming challenges faced by 4D neural deformation fields in preserving shape consistency. This approach is naturally compatible with physics-based simulation, ensuring physically plausible interactions. Experiments show that AKD achieves superior 3D consistency and motion quality compared with existing works on text-to-4D generation. 1. Introduction In traditional 3D graphics, skeleton-based character animation pipeline involves steps like shape modeling, rig- * Work done during an internship at NVIDIA. ging, motion capture, motion retargeting, and editing. As mature technology, such pipelines can achieve high realism and good controllability over the motion, but they typically require extensive manual work from digital artists, making the process time-consuming and thus hardly scalable. Recent advances in video generation models [6, 59] offer promising avenue for streamlining the animation authoring process: with text-to-video model, generating sequence of character animation only requires text prompt. However, existing video generation models still struggle to generate high-fidelity dynamics for real-world objects because of lack of 3D information. Common issues include failing to preserve the 3D structure consistency (e.g. number of limbs of character) during animation, or producing physically implausible articulated motion, such as foot-skating and ground penetration. Recent works on text-to-4D generation [1, 2] leverage these video generation models to distill the learned dynamic motion into consistent 4D sequences. These frameworks commonly rely on neural deformation fields which predict displacements at each location in pre-defined 3D volume to deform 3D shape. Animation is thus temporal sequence of such deformed shapes. While flexible, this approach introduces large number of Degrees of Freedom (DoFs), making optimization challenging and often resulting in suboptimal quality. Is it possible to have the best of both worlds, where generative models provide extensive knowledge of diverse motions from internet-scale data, while skeleton-based 3D animation allows low-DoF control, permanence of articulated structures, and even physical grounding via simulation? To answer this question, we introduce Articulated Kinematics Distillation (AKD), motion synthesis system that bridges traditional character animation pipelines and generative motion synthesis. Given rigged 3D asset, we distill articulated motion sequences from pre-trained video diffusion model using Score Distillation Sampling (SDS). The skeleton-based representation simplifies the distillation process by limiting the number of DoFs to that of few joints, in contrast to all query points in space-time as in the textto-4D works [2]. It also offers an effective regularization of the deformation space, enabling the distillation to concentrate on overall motion styles without worrying about maintaining reasonable local structures. More importantly, the skeleton-based formulation is naturally compatible with physics-based simulation, allowing the generated motion to be grounded by physics-based motion tracking to ensure physical plausibility. Experiments verify the effectiveness of our design: compared to previous text-to-4D methods, our framework produces results with better 3D shape consistency and more expressive motions. We summarize our contributions: We introduce novel text-driven motion synthesis framework for static 3D assets, combining articulated rigging systems and large video diffusion models. We demonstrate that incorporating non-uniform ground renderings enhances the video models adherence to basic physics between the character and the ground. Extensive experiments show that our generated motions exhibit higher quality than the state-of-art methods that can synthesize long-trajectory motions. Our generated motion can be used in physics-based motion tracking with differentiable physics to further boost its physical realism. 2. Related Work Deformable Gaussian Splatting In recent years, different types of 3D representations have been introduced to facilitate the reconstruction and generation of 3D/4D scenes, such as neural fields [32], iNGP [34], and 3D Gaussian Splatting [21]. Among these representations, 3D Gaussian Splatting is particularly suitable for representing dynamic scenes due to its explicit nature [52] compared to the NeRF based on neural implicit fields [35, 38], whose deformations are achieved by bending rendering rays [9, 37]. This advantage of 3DGS has sparked lot of works on 4D reconstruction and modeling from multi-view input, including general scenes [62], facial avatars [10, 54], and full-body avatars [15, 22, 27, 33, 43, 67], where 3D Gaussian kernels are bound to an articulated human model SMPL [28] through learned skinning weight. We adopt GS as our 3D shape representation, which naturally allows SDS gradients to smoothly propagate through the articulated deformation and rendering pipeline. Articulated Motion Reconstruction closely related topic to our work is the reconstruction of articulated motions of deformable objects in under-constrained settings, especially from monocular view. As special case, the reconstruction of human body poses [11, 20, 51] benefits strongly from the availability of dedicated deformable models such as SMPL [28] and the abundance of 2D/3D pose data [25, 31]. In contrast, the reconstruction of general objects, such as animals and humans in loose clothing, has remained more challenging due to the inherent 3D ambiguity from monocular view and lack of reliable priors for their articulation. One line of works following BANMO [23, 46, 5658] solve for static 3D template and articulated motion simultaneously from monocular video by leveraging various image measurements such as segmentation, optical flow, and DensePose. Several works from another line [16, 24, 49, 60, 61] train neural network that predicts the shape template of animals and their body deformation conditioned on single input image in weaklysupervised manner. Ponymation [45] learns motion VAE for horse motions from collection of horse videos. There are also works [17, 39, 47, 63] that directly train generaInstead of extracting tive models on articulated motions. articulation from particular input, our method focuses on generating novel skeleton motion utilizing video diffusion priors that have learned general visual knowledge. Generating 4D and Physics-Based Dynamics Generating 4D content is inherently challenging as it demands high consistency not only along the temporal axis to maintain motion, but also across multiple viewpoints to ensure spatial and structural accuracy in the generated content. Some works attempt to directly build priors models in the 4D domain, including diffusion models [19, 53] and reconstruction model [41], where the limited availability of 4D data pose challenge. Therefore, large body of works [1, 18, 26, 40, 44, 48, 64, 66] distill 4D motion from combination of generative models that operate in lower dimensions, including images, videos, and multi-view images (3D), which is difficult problem to due to spasity of supervision, noisy gradient from SDS, and high-degree of freedom in the optimization variables. Some works aim to improve the controllability of the 4D generation by introducing conditioning of trajectory [50] or sparse control points [50]. None of the works above allow the generated contents to be grounded in physics-based manner. The exceptions Figure 2. Pipeline. We novelly incorporate articulated skeletons into generative motion synthesis. With the low-dimensional parameterization of motions (a sequence of joint angles for articulated bones), the synthesis can focus more on motion modes instead of local-scale deformations. Given text prompt, we use text-to-3D method to generate 3D asset. The asset is deformed by the skeleton and differentiably rendered into videos. The SDS gradient is evaluated by pre-trained video diffusion transformer and backpropagated to joint angles. are PhysGaussian [52] and PhysDreamer [65], which models scene deformation by Material-Point-Method (MPM) simulator. Such formulation is more suitable for modeling the volumetric solid deformation and fluid dynamics, whereas our work focuses on articulated motion which can be simulated more efficiently by skeleton-based representation. 3. Background 3.1. Deformable Gaussian Splatting 3D Gaussian Splatting (3DGS) [21] utilizes set of 3D Gaussian kernels to represent 3D scene. These kernels can be rendered into images using customized differentiable rasterizer. Specifically, each kernel is defined by set of parameters, σp, xp, Σp, Cp, where σp denotes the opacity, xp and Σp represent the center and covariance matrix of the Gaussian ellipsoid, and Cp is the coefficient set of spherical harmonics that determines the view-dependent colors of the kernel. Given camera view, the color of pixel is rendered by α-blending all kernels along the ray direction: = (cid:88) ciαi i1 (cid:89) (1 αj), j=1 (1) where is the sorted index of the kernels in ascending order of z-depth, ci represents the kernels color under the given view, evaluated from spherical harmonics, and αi is the opacity σi weighted by the kernels 3D Gaussian distribution. Given set of images with known camera parameters, we can reconstruct the 3D scene using only RGB loss. Following PhysGaussian [52], given warping function ϕ : R3 R3 (defined in Sec. 4.1), we can deform the shape of each Gaussian kernel using the local linearization of ϕ at its center xp: ϕ(x) ϕ(xp) + ϕ(xp)(x xp), resulting in: xp = ϕ(xp), Σp = ϕ(xp)Σpϕ(xp)T , (2) where (xp, Σp) and ( xp, Σp) denote the Gaussian parameters before and after the deformation respectively. For simplicity, we can hold opacities and spherical harmonics constant. Our deformation system adheres to this convention to generate Gaussian kernels in motion. 3.2. Video Score Distillation Sampling The Score Distillation Sampling (SDS) method, proposed to distill 3D models from text-to-image priors [36], uses an approximated gradient of the diffusion loss [14]: LDiff(θ, z, y) = Et,ϵ (cid:104) w(t) ˆϵ ϵ2 2 (cid:105) , (3) where U(0, 1) is the diffusion time step, ϵ (0, I) is Gaussian noise, zt = 1 αtϵ is the noisy image at step t, is the rendered image, is the input text, and ˆϵ = ϵθ(zt; t, y) is the noise predicted by the diffusion model. Omitting the gradient through the noise-predicting module, the gradient of w.r.t. is given by: αtz + zLSDS(z, y) = Et,ϵ [w(t) (ˆϵ ϵ)] . (4) This pixel-level gradient is then backpropagated to the 3D model parameters via differentiable renderer, guiding the generation of 3D representation consistent with the input text. Videos, which are essentially 3D image tensors with an additional temporal axis, are handled similarly; the SDS gradient can be backpropagated to 4D representations. 4. Method We present Articulated Kinematics Distillation, textdriven articulated motion synthesis system for animating 3D assets, as illustrated in Fig. 2. We assume rigged 3D asset as input. For results in this paper, we use an off-theshelf text-to-3D generator, although assets from photogrammetry or created by artists should work similarly. We then convert the asset into dual mesh-3DGS representation: the mesh is used to compute appropriate skinning weights, and the 3DGS supports differentiable deformations and rendering during SDS optimization. We rig the asset by manually embedding an articulated skeleton, and compute Linear Blend Skinning (LBS) weights, which we transfer to nearby Gaussian kernels. To generate the motion, we optimize sequence of joint angles within the articulation tree, which pass through differentiable Forward Kinematics (FK) and 3DGS rasterization pipeline to render video sequence. The rendered video is then fed into pre-trained video diffusion model, which provides guidance for adjusting joint angles at each frame to produce text-aligned motion. In the following sections, we present the technical details of each component. 4.1. Rigging and Skinning for Gaussian Splatting The rigging system is widely used in the animation industry, where 3D character meshes are controlled via an embedded articulated skeleton. Artists manipulate joint angles between connected bones to create 3D animations. The transformations of the deformed bones are evaluated using Forward Kinematics [8] of the skeleton tree, and then mesh vertices are moved by interpolating transformations from nearby bones. The most common interpolation scheme is Linear Blend Skinning (LBS) [30], which defines the deformation function as: corresponding to pixel, the color is determined by the intersection point between the ray and the ground; if there is no intersection, the color is set to pre-assigned value. We also set the opacities of kernels located below the ground to zero to account for occlusion. Camera Trajectory. We let the camera smoothly follow the bounding box center of the deformed shape in each frame, as the video model we use often generates objectcentric videos. It is equivalent to moving the entire scene in the opposite direction while keeping the camera view fixed. 4.3. SDS for V-Prediction Diffusion Models The video model we use, CogVideoX-5B [59], is Diffusion Transformer (DiT) trained with v-prediction formulation [42]. The diffusion loss is defined as: LDiff(θ, z, y) = Et,ϵ (cid:104) w(t) ˆz2 2 (cid:105) , (6) where ˆz = αtzt vθ(zt; t, y) is the reconstruction based on the predicted velocity by the diffusion model vθ. Omitting the gradient through the velocity-predicting transformer, the SDS gradient is evaluated as: ϕ(x) = (cid:88) i=1 wi(Rix + Ti), (5) zLSDS(z, y) = Et,ϵ [w(t) (z ˆz)] . (7) where is the number of bones, (Ri, Ti) is the rigid transformation of bone i, and wi is the skinning weight, satisfying (cid:80) wi = 1. This deformation field is usually applied to mesh vertices, but it is defined on the entire 3D space, meaning that it can also deform Gaussian kernels following Eq. (2), where ϕ(x) = (cid:80)B i=1 wiRi. We transfer skinning weights to Gaussian kernels by barycentric interpolation of skinning weights at their nearest points on the template mesh. For the assets generated by text-to-3D methods, we use Pinocchio [4] to compute skinning weights for mesh vertices automatically. Please refer to our supplementary document for more details. 4.2. Rendering For each deformed frame of the 3DGS asset, we use the differentiable Gaussian Splatting rasterizer [21] to render it into an image. These rendered frames are then concatenated sequentially to form video tensor. Ground Rendering. Unlike previous text-to-4D frameworks [2, 66] which often render environments in nearly uniform colors, we incorporate checkerboard ground. We find that this setup can provide the distillation with clues about the interaction between assets and the ground. It helps reduce relative motions between contacting parts and the ground, and helps keep assets grounded. We render the checkerboard ground as background layer by ray casting and blend it with the rendering of the asset. For each ray Derivations are provided in the supplementary document. Modern video diffusion models are trained in latent space by encoding raw videos using Variational Autoencoders (VAE). Correspondingly, the SDS gradient is computed on the latent codes, and then needs to be backpropagated through the VAE encoder into the pixel space. This process can be extremely memory-intensive, especially when handling large number of frames. To reduce the memory footprint, we employ the gradient checkpointing techniques [7]. These techniques save memory by selectively storing intermediate activations and recomputing them during the backward pass, making it feasible to perform SDS optimization with large DiTs. 4.4. Optimization Given all the components, we now present the optimization process of our framework. Our approach uses 3-DoF compound spherical joints to connect bones, where each DoF represents the rotation angles around one of three linearly independent axes. The optimizable variables for each asset (cid:110) (cid:111)F 1 {Aj }B1 j=1 , Ti , where is the numconsist of Θ = ber of frames, Aj is the 3D angle vector for joint at frame i, and Ti denotes the 6-DoF rigid transformation of the root bone in the articulation tree at frame i. We use the following loss function for SDS optimization: i=0 = LSDS + λ1Lsmooth + λ2Lground. (8) Besides SDS loss, we incorporate two regularizers: smoothness penalty Lsmooth over time to encourage timeconsistent deformations, and ground penetration penalty Lground to enforce the assets to stay above the ground. Since our parameters have physical meanings, we can directly enforce smoothness on the control parameters, which is defined as the mean absolute error (MAE) of the Laplacian of the control parameters w.r.t. time, excluding the first and last frames: Lsmooth = MAE(tΘ), (tΘ)i = Θi1 2Θi + Θi+1. (9) This loss helps ensure that changes in the parameters across consecutive frames are gradual, annealing the noisy nature of the SDS loss. The ground penetration loss is defined as Lground = 1 (cid:88) vV max(vy, 0). (10) Here, we conceptualize bones as set of transformed cuboids, and represents the set of vertices of these cuboids. This loss penalizes any penetration of the asset below the ground plane by applying penalty proportional to the depth below ground for each vertex. We focus on motion synthesis for legged characters, including animals and humanoids, moving across the ground. In our experiments, we initialize the assets forward motion along x-axis by setting an initial displacement as = vi for some constant velocity v. The pace and trajectories of the assets can be further optimized during the distillation. 4.5. Physics-Based Motion Tracking The articulated skeletons embedded in the asset can be deployed in articulated rigid body simulators. To ground the synthesized motion in physics, we can further project the distilled motion trajectory to the nearest solution achievable in physics-based tracking in simulation environment. We accomplish this generation-to-simulation transition by searching for physical joint control sequence that minimizes the difference between simulated and synthesized bone trajectory. Specifically, we treat each bone as rigid cuboid and use compound joints to connect bones, which is consistent with the motion DoFs during motion distillation. We use semi-implicit articulated rigid body simulator to simulate the skeleton under gravity and ground collision. The simulation process can be abstracted as state transition S: [qk+1, qk+1] = S([qk, qk], t, τ k) (11) where qk is the concatenation of generalized coordinates of bones at the time step k, which represent 6-DoF rigid transqk is the generalized velocity, is the simuformations, lation time step size, and τ is active joint torques applied to the connected bones at joints. We use ProportionalDerivative (PD) controller to provide active joint torques, where the torque at joint around axis is given by: τ kl = ke(ˆθjl θjl) kd θjl (12) where ˆθjl is the control variable, and θjl is the current joint angle. This formulation tends to pull the joint angle θjl towards ˆθjl subject to damping. ke, kd are globally defined parameters that control joint elasticity and damping, respectively. The whole simulation process can be implemented using AutoDiff frameworks such as Warp [29]. Altogether, we use the following motion-tracking loss: Ltrack( ˆΘ, q0) = 1 1 1 (cid:88) i=1 qiN ˆqi1 + λ3 MAE(t ˆΘ). (13) Here, is the ratio between the frame time of the target motion { ˆqi} and the simulation time step t, and ˆΘ is the concatenation of control variable per simulation step. Like in the distillation, we add smoothness regularizer. We also optimize the initial generalized velocity q0 as well. The simulation starts from q0 = Proj( ˆq0), where ˆq0 is vertically shifted such that the lowest bone touches the ground. However, is typically in the hundreds for explicit simulators, resulting in thousands of simulation steps in total. This can cause severe gradient explosion issues during backpropagation. common technique to alleviate this issue is to apply gradient clipping before each optimizer step. However, the explosion may occur multiple times throughout backpropagation, which can make the final gradient useless. To address this, we employ fine-grained gradient clipping strategy, applying gradient clipping every few tens of backward steps. We achieve this by wrapping chunks of substeps into PyTorch layer and manually defining the backpropagation logic. 5. Implementation We use an off-the-shelf SDS-based text-to-3D system, TetSplatting [12], to generate static 3D mesh assets from texts. The text-to-3D module is not the focus of this work and can be replaced by any suitable framework. We set up skeleton inside the mesh manually in Blender [5] using its armature system. This manual setup typically takes only few minutes per instance. We convert the generated mesh into the Gaussian Splatting representation by applying 3DGS reconstruction [21] to images of the mesh from 200 random views, starting from mesh vertices as initial kernel centers. We adopt threestudio [13] for SDS optimization, and CogVideoX-5B [59] as the video diffusion model. By leveraging the gradient checkpointing mechanism of the VAE, all our experiments can be run on single NVIDIA A10040GB graphics card. We adopt forward kinematics and articulated rigid body simulation from Warp [29] and integrate it with PyTorch for data and gradient interchange. Figure 3. Qualitative comparisons with TC4D. The blurry artifacts generated by TC4D are highlighted. TC4D often fails to produce alternating leg movements (e.g., in the astronaut example), or shows limited local-scale motion (e.g., in the T-Rex example). Each asset undergoes 10,000 iterations of SDS optimization, requiring approximately 25 hours. 6. Experiment 6.1. Quantitative Comparisons We generate 29 static assets, including animals and humanoids, and compare our method with the state-of-theart open-sourced approach, TC4D [2], that can synthesize long-trajectory motions. TC4D moves the bounding box of the object along predefined path, while our method only initializes path and allows adjustments based on the guidance from the video model. Other text-to-4D frameworks, such as those in [1, 66], are limited to synthesizing localscale motions, which tend to have constrained magnitudes. We compare our method (without physics-based tracking for fairness) with TC4D using both automated video scores and user study. 6.1.1. Metrics Automated Metric We utilize VideoPhy [3] to automatically score videos. This model scores two types of metrics: Semantic Adherence (SA), which assesses the alignment between the text and the video content, and Physical Commonsense (PC), which evaluates whether the video adheres to basic physical laws. We observed that the evaluation of PC scores tends to concentrate on the local deformations of objects. For fair comparison with TC4D using this metric, we exclude ground rendering when using this metric. User Study We recruited 20 human evaluators to compare video clip pairs generated by AKD and TC4D using the same text prompts. We asked the evaluators to assess each comparison across several aspects: Motion Amount (MA), Physical Plausibility (PP), and Text Alignment (TA). Evaluators were instructed to choose the clip they felt performed better for each aspect. 6.1.2. Result PC SA 0.81 0.26 TC4D 0.40 0.34 Ours The automatic scores evaluated by VideoPhy are provided in the inset table where the means and standard deviations of each score are reported. Our method outperforms TC4D in terms of both metrics. The user study results show that our method was preferred in 51%, 53%, and 53% of the comparisons in terms of MA, PP, and TA, respectively. These results indicate that our method can synthesize motions in better quality. 0.31 0.15 0.39 0. 6.2. Qualitative Comparisons In Fig. 3, we visualize several comparisons between TC4D and ours. We refer readers to the supplemental video for complete results. Note that appearance is not our focus because the 3D assets can be generated by any text-to-3D method. We observe that TC4D rarely produces alternatFigure 4. Examples of our synthesized motions. ing leg movements, and areas where the legs converge often appear blurry. In contrast, our method, with its low-DoF parameterization, effectively maintains the geometric structure of each part within the assets during deformations. 6.3. Physics-based Motion Tracking The synthesized articulated motions may still float above the ground or slide relative to it, as video diffusion models cannot provide guidance for such fine-grained placements. However, the overall styles of motions are well captured. In Fig. 5, we present examples where the synthesized motions appear above the ground, while the physics-based tracking results adhere to gravity and have frictional contact with the ground, yet still track the target motion styles. For more results, we refer the reader to the supplemental video. 6.4. Synthesis Diversity Asset Diversity. In Fig. 4, we visualize more examples of motions by our method on different assets with varying articulated topologies. We refer readers to the supplemental video for animations. Motion Diversity. Our method supports the synthesis of different motion modes based on different text prompts. In Fig. 6, we illustrate the differences between walking and running behaviors. For quadrupeds, the limbs on each side alternate during walking, whereas they move in sync during running. 6.5. Ablation Studies Figure 5. We use physics-based motion tracking to project synthesized motions onto physics-grounded trajectories. Components in Training. We conducted ablation studies on several components in addition to SDS loss: ground rendering, ground penalty loss, and w.o. w.o. Lground ground 0.777 0.989 0.979 0.107 0.321 0.269 w.o. Lsmooth 0.984 0.294 SA 0.982 PC 0.294 Ostrich Tiger Figure 8. Ablation study on the base video diffusion model. Figure 9. Ablation on the text-to-3D module. We extract an asset from TC4D and achieve comparable appearance. motions. In Fig. 8, our method with VideoCrafter generates alternating leg motions but suffers from severe foot-skating issues. TC4D with CogVideoX still fails to produce alternating leg motions and exhibits blurry artifacts. 7. Conclusion In this paper, we present AKD, text-driven method for articulated motion synthesis powered by large video diffusion models. By distilling the motion knowledge from video generation model, our model offers an alternative to laborious motion authoring in traditional animation pipelines. The low-DoF, skeleton-based parameterization of motion allows the distillation process to focus more on overall articulated motion patterns rather than local-scale shape deformations, resolving the common issue of shape inconsistency as in previous work on 4D generation, and thus results in improved physical plausibility of the generated motion. We demonstrate that the generated skeleton motions can be transferred into simulation environments via physics-based motion tracking. We hope this work can inspire future research in generating labeled data for robotics applications. Our approach has several limitations that point to future work. The visual quality of our generated assets is still suboptimal; improving the visual quality can help reduce the gap between rendered outputs and the realistic video distributions encoded in video models. The diversity of motions produced by our method largely depends on the video models ability to synthesize desired motions; improved video priors in the future could enhance this diversity. Our method focuses on the articulated motion of objects, and may not be suitable for other types of deformation such as soft-body dynamics. Additionally, our pipeline assumes manually rigged skeleton. To scale up our method, future work can leverage automatic rigging techniques such as RigNet [55] to generalize to diverse character types, and can use more efficient sampling techniques for video models to accelerate convergence. Figure 6. Our method supports synthesizing different motions based on varying text descriptions. Figure 7. Ablation studies on ground rendering, ground penalty loss, and the smoothness loss. The artifacts are highlighted. smoothness loss, each targeting specific types of artifacts. The visualizations are presented in Fig. 7. The automated quantitative metrics evaluated using VideoPhy are reported in the inset table. Without ground rendering, the synthesized motion appears above the ground because the video model lacks information about the grounds location. The absence of the ground penalty loss results in penetration into the ground. The smoothness loss can encourage time consistency across frames; without it, dramatic and unnatural deformations may occur in certain frames. Both SA and PC scores decrease when one of these component is removed. Text-to-3D Module. In this paper, the text-to-3D module is not our focus. We use Tet-Splatting because of its efficiency. This module can be replaced by arbitrary text-to-3D frameworks. For example, we can extract assets from the static phase of TC4D. One example is shown in Fig. 9. Video Diffusion Model. The video model and the articulated representation both improve the quality of synthesized"
        },
        {
            "title": "References",
            "content": "[1] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David Lindell. 4d-fy: Text-to-4d generation using hybrid score disIn Proceedings of the IEEE/CVF Contillation sampling. ference on Computer Vision and Pattern Recognition, pages 79968006, 2024. 1, 2, 6 [2] Sherwin Bahmani, Xian Liu, Wang Yifan, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu, Jeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, et al. Tc4d: In European Trajectory-conditioned text-to-4d generation. Conference on Computer Vision, pages 5372. Springer, 2025. 1, 2, 4, 6 [3] Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, KaiWei Chang, and Aditya Grover. Videophy: Evaluating physical commonsense for video generation. arXiv preprint arXiv:2406.03520, 2024. 6 [4] Ilya Baran and Jovan Popovic. Automatic rigging and animation of 3d characters. ACM Transactions on graphics (TOG), 26(3):72es, 2007. 4, 11 [5] Blender Online Community. Blender - 3D modelling and rendering package. Blender Foundation, 2024. 5 [6] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7310 7320, 2024. 1 [7] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. arXiv Training deep nets with sublinear memory cost. preprint arXiv:1604.06174, 2016. 4 [8] Jacques Denavit and Richard Hartenberg. kinematic notation for lower-pair mechanisms based on matrices. 1955. 4 [9] Yutao Feng, Yintong Shang, Xuan Li, Tianjia Shao, Chenfanfu Jiang, and Yin Yang. Pie-nerf: Physics-based inIn Proceedings of the teractive elastodynamics with nerf. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 44504461, 2024. [10] Simon Giebenhain, Tobias Kirschstein, Martin Runz, Lourdes Agapito, and Matthias Nießner. Npga: Neural parametric gaussian avatars. arXiv preprint arXiv:2405.19331, 2024. 2 [11] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. Humans in 4d: Reconstructing and tracking humans with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1478314794, 2023. 2 [12] Chun Gu, Zeyu Yang, Zijie Pan, Xiatian Zhu, and Li Zhang. Tetrahedron splatting for 3d generation. In NeurIPS, 2024. 5 [13] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, ZiXin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. threestudio: unified framework for 3d content generation. https://github.com/threestudioproject/ threestudio, 2023. 5 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [15] Shoukang Hu, Tao Hu, and Ziwei Liu. Gauhuman: Articulated gaussian splatting from monocular human videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2041820431, 2024. 2, 11 [16] Tomas Jakab, Ruining Li, Shangzhe Wu, Christian RupFarm3d: Learning articuprecht, and Andrea Vedaldi. In 2024 Interlated 3d animals by distilling 2d diffusion. national Conference on 3D Vision (3DV), pages 852861. IEEE, 2024. 2 [17] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems, 36:2006720079, 2023. [18] Yanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, and Yao Yao. Consistent4d: Consistent 360 {deg} dynamic obarXiv preprint ject generation from monocular video. arXiv:2311.02848, 2023. 2 [19] Yanqin Jiang, Chaohui Yu, Chenjie Cao, Fan Wang, Weiming Hu, and Jin Gao. Animate3d: Animating any 3d arXiv preprint model with multi-view video diffusion. arXiv:2407.11398, 2024. 2 [20] Angjoo Kanazawa, Michael Black, David Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 71227131, 2018. 2 [21] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 3, 4, 5 [22] Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, and Anurag Ranjan. Hugs: Human gaussian splats. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 505515, 2024. 2, 11 [23] Jiahui Lei, Yufu Wang, Georgios Pavlakos, Lingjie Liu, and Kostas Daniilidis. Gart: Gaussian articulated template models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1987619887, 2024. [24] Zizhang Li, Dor Litvak, Ruining Li, Yunzhi Zhang, Tomas Jakab, Christian Rupprecht, Shangzhe Wu, Andrea Vedaldi, and Jiajun Wu. Learning the 3d fauna of the web. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 97529762, 2024. 2 [25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 2 [26] Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, and Karsten Kreis. Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85768588, 2024. 2 [27] Xinqi Liu, Chenming Wu, Jialun Liu, Xing Liu, Chen Zhao, Haocheng Feng, Errui Ding, and Jingdong Wang. Gva: Reconstructing vivid 3d gaussian avatars from monocular videos. CoRR, 2024. 2 [28] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. Smpl: skinned multiperson linear model. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 851866. 2023. [29] Miles Macklin. Warp: high-performance python framehttps : / / work for gpu simulation and graphics. github.com/nvidia/ warp, 2022. NVIDIA GPU Technology Conference (GTC). 5, 13 [30] Thalmann Magnenat, Richard Laperri`ere, and Daniel Thalmann. Joint-dependent local deformations for hand animation and object grasping. In Proceedings of Graphics Interface88, pages 2633. Canadian Inf. Process. Soc, 1988. 4 [31] Naureen Mahmood, Nima Ghorbani, Nikolaus Troje, Gerard Pons-Moll, and Michael Black. Amass: Archive In Proceedings of of motion capture as surface shapes. the IEEE/CVF international conference on computer vision, pages 54425451, 2019. 2 [32] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2 [33] Arthur Moreau, Jifei Song, Helisa Dhamo, Richard Shaw, Yiren Zhou, and Eduardo Perez-Pellitero. Human gaussian splatting: Real-time rendering of animatable avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 788798, 2024. 2, 11 [34] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4):115, 2022. 2 [35] Keunhong Park, Utkarsh Sinha, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Steven Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 58655874, 2021. [36] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben MildenIn The hall. Dreamfusion: Text-to-3d using 2d diffusion. Eleventh International Conference on Learning Representations, 2023. 3, 12 [37] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1031810327, 2021. 2 [38] Yi-Ling Qiao, Alexander Gao, Yiran Xu, Yue Feng, Jia-Bin Huang, and Ming Lin. Dynamic mesh-aware radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 385396, 2023. 2 [39] Sigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar, Amit Haim Bermano, and Daniel Cohen-Or. Single motion diffusion. In ICLR, 2024. 2 [40] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. Dreamgaussian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142, 2023. [41] Jiawei Ren, Kevin Xie, Ashkan Mirzaei, Hanxue Liang, Xiaohui Zeng, Karsten Kreis, Ziwei Liu, Antonio Torralba, Sanja Fidler, Seung Wook Kim, et al. L4gm: Large 4d gaussian reconstruction model. arXiv preprint arXiv:2406.10324, 2024. 2 [42] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. 4, 12 [43] Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, and Zeyu Wang. Splattingavatar: Realistic real-time human avatars with In Proceedings of the mesh-embedded gaussian splatting. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16061616, 2024. 2 [44] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dynamic scene generation. arXiv preprint arXiv:2301.11280, 2023. 2 [45] Keqiang Sun, Dor Litvak, Yunzhi Zhang, Hongsheng Li, Jiajun Wu, and Shangzhe Wu. Ponymation: Learning articulated 3d animal motions from unlabeled online videos. In European Conference on Computer Vision, pages 100119. Springer, 2025. 2 [46] Jeff Tan, Donglai Xiang, Shubham Tulsiani, Deva Ramanan, and Gengshan Yang. Dressrecon: Freeform 4d human reconstruction from monocular video. arXiv preprint arXiv:2409.20563, 2024. [47] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffuIn The Eleventh International Conference on sion model. Learning Representations, 2023. 2 [48] Lukas Uzolas, Elmar Eisemann, and Petr Kellnhofer. Motiondreamer: Exploring semantic video diffusion features for zero-shot 3d mesh animation, 2024. 2 [49] Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rupprecht, and Andrea Vedaldi. Magicpony: Learning arIn Proceedings of the ticulated 3d animals in the wild. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 87928802, 2023. 2 [50] Zijie Wu, Chaohui Yu, Yanqin Jiang, Chenjie Cao, Fan Wang, and Xiang Bai. Sc4d: Sparse-controlled video-to-4d generation and motion transfer. In European Conference on Computer Vision, pages 361379. Springer, 2025. 2 [51] Donglai Xiang, Hanbyul Joo, and Yaser Sheikh. Monocular total capture: Posing face, body, and hands in the wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1096510974, 2019. 2 [52] Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, and Chenfanfu Jiang. Physgaussian: Physicsintegrated 3d gaussians for generative dynamics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 43894398, 2024. 2, [53] Yiming Xie, Chun-Han Yao, Vikram Voleti, Huaizu Jiang, and Varun Jampani. Sv4d: Dynamic 3d content generation with multi-frame and multi-view consistency. arXiv preprint arXiv:2407.17470, 2024. 2 [54] Yuelang Xu, Benwang Chen, Zhe Li, Hongwen Zhang, Lizhen Wang, Zerong Zheng, and Yebin Liu. Gaussian head avatar: Ultra high-fidelity head avatar via dynamic gaussians. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2024. 2 [55] Zhan Xu, Yang Zhou, Evangelos Kalogerakis, Chris Landreth, and Karan Singh. Rignet: Neural rigging for articulated characters. arXiv preprint arXiv:2005.00559, 2020. 8 [56] Gengshan Yang, Minh Vo, Natalia Neverova, Deva Ramanan, Andrea Vedaldi, and Hanbyul Joo. Banmo: Building animatable 3d neural models from many casual videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 28632873, 2022. 2 [57] Gengshan Yang, Chaoyang Wang, Dinesh Reddy, and Deva Ramanan. Reconstructing animatable categories from In Proceedings of the IEEE/CVF Conference on videos. Computer Vision and Pattern Recognition, pages 16995 17005, 2023. [58] Gengshan Yang, Shuo Yang, John Zhang, Zachary Manchester, and Deva Ramanan. Ppr: Physically plausible reconstruction from monocular videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 39143924, 2023. 2 [59] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 4, 5, [60] Chun-Han Yao, Wei-Chih Hung, Yuanzhen Li, Michael Rubinstein, Ming-Hsuan Yang, and Varun Jampani. Lassie: Learning articulated shapes from sparse image ensemble via 3d part discovery. Advances in Neural Information Processing Systems, 35:1529615308, 2022. 2 [61] Chun-Han Yao, Wei-Chih Hung, Yuanzhen Li, Michael Rubinstein, Ming-Hsuan Yang, and Varun Jampani. Hi-lassie: High-fidelity articulated shape and skeleton discovery from In Proceedings of the IEEE/CVF sparse image ensemble. Conference on Computer Vision and Pattern Recognition, pages 48534862, 2023. 2 [62] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1944719456, 2024. 2 [63] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. Physdiff: Physics-guided human motion diffusion model. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1601016021, 2023. 2 [64] Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, and Yao Yao. Stag4d: Spatial-temporal anchored generative 4d gaussians. In European Conference on Computer Vision, pages 163179. Springer, 2025. 2 [65] Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, and William Freeman. Physdreamer: Physics-based interaction with 3d objects via video generation. In European Conference on Computer Vision, pages 388406. Springer, 2025. [66] Yufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Otmar Hilliges, and Shalini De Mello. unified approach for textIn Proceedings of and image-guided 4d scene generation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 73007309, 2024. 2, 4, 6 [67] Yang Zheng, Qingqing Zhao, Guandao Yang, Wang Yifan, Donglai Xiang, Florian Dubost, Dmitry Lagun, Thabo Beeler, Federico Tombari, Leonidas Guibas, et al. Physavatar: Learning the physics of dressed 3d avatars from visual observations. arXiv preprint arXiv:2404.04421, 2024."
        },
        {
            "title": "Appendix",
            "content": "A. Automatic Skinning Weight Computation Assigning skinning weights to Gaussian kernels is not straightforward task. For example, kernel close to bone actually should not be affected by it if the shortest segment from the kernel passes the outside region of the object. As concrete example, Gaussian kernels on one foot of human should not be influenced by the other foot. While using learnable weights could address this [15, 22, 33], it would undermine the degree-of-freedom (DoF) reduction achieved by the rigging system. To resolve ambiguities in weight assignment, we use reference mesh that aligns with the geometry of 3DGS to define weights on the mesh surface, and then transfer these weights to the Gaussian kernels. This mesh is typically an output form from text-to3D frameworks or can be generated using mesh extraction tools. For automatic weight computation on the mesh, we use the widely adopted auto-rigging system Pinocchio [4]. Pinocchio conceptualizes weight computation on simply connected mesh as heat diffusion process along the mesh surface, accounting for bone visibilities blocked by other surface parts. Specifically, for the bone b, the weight contribution vector wb on vertices is computed by solving the following Poisson equation: wb + Hbwb = Hbpb. (14) Here, is the cotangent surface Laplacian operator. pb = 1 only if the closest bone to the vertex is b. is diagojj = c/d2 nal matrix with entries only if bone is visible from vertex within the mesh, where is user-defined constant and dj is the distance from vertex to bone b. The visibility is determined by whether the segment from the vertex to its closest point on the bone is completely enclosed within the mesh. This equation can be interpreted as follows: the bone first transfers heat to its visible vertex, and Figure 10. Gallery of skeleton systems from our experiments. then the heat diffuses along the surface. The resulting heat distribution represents the weight field for that bone. We extend Pinocchio to support noisy mesh inputs that may have multiple components by manually setting the visibility of connected component to its nearest bone as true if all bones are invisible from that component. This can prevent the system matrix of Eq. (14) from being singular and allows the outlier components to be controlled by their nearest bones. By concatenating the weight contributions of each bone, we obtain the weight matrix RV B, where is the number of vertices. Each row of represents the skinning weight vector at given vertex. These weight vectors can be interpolated to arbitrary surface points on the mesh using barycentric interpolation. Since Gaussian kernels generated during reconstruction are typically located near the geometric surface, we identify weight vectors of Gaussian kernels with their nearest surface points on the mesh. large transformer. Taking the derivative of LDiff w.r.t. z, we get zLDiff =Et,ϵ =Et,ϵ (cid:20) (cid:20) 1 1 αt 1 1 αt (I (I ˆz ˆz zt (cid:21) ) (z ˆz) zt ˆz vθ vθ ) (z ˆz) (cid:21) . (16) Here we omit the constant 2 that arise from the derivative of the square function for notation simplicity. Following the SDS gradients for U-Net-based diffusions [36], where terms involving the gradients of U-Nets are omitted, we similarly omit ˆz vθ vθ : zLSDS =Et,ϵ (cid:20) 1 1 αt =Et,ϵ [z ˆz] . (cid:21) αt)2) (z ˆz) (1 ( (17) B. SDS Gradient for V-Prediction Diffusion C. Shadow Casting."
        },
        {
            "title": "Models",
            "content": "The video diffusion model in our pipeline, CogVideoX5B [59], is v-prediction diffusion model [42], where the model predicts the so-called velocity instead of noise. The diffusion loss for training is LDiff(θ, z, y) = Et,ϵ (cid:20) 1 1 αt ˆz2 2 (cid:21) , (15) where is the latent code of video, ˆz = 1 αtvθ(zt; t, y), zt = αtz + αtzt 1 αtϵ, and vθ is We can also cast shadows on the ground layer mentioned above to further indicate the spatial relationship between the object and the ground. When rendering ray intersects with the ground at point , the ground color is weighted by this heuristic shadow intensity: s(P ) = 1smax exp(βd(P )), where d(P ) is the vertical height from the ray-ground intersection point to the deformed asset, smax is the maximum level of shadowing to apply, and β is the decay coefficient as the object height above increases. This shadowing approximates distant, parallel light source positioned verFigure 11. Additional Motion Variety Experiments from standing to sitting, as well as precise human motion control such as cartwheeling and raising hands. Additionally, fine-grained motions, such as hand pose variations, are difficult to capture. We found that additional consistent textural descriptions in prompts are crucial for human motion synthesis. We will address these limitations in the revision. tically above the ground, complemented by diffusive ambient lighting. Incorporating shadows in SDS optimization does not yield substantial improvements in motion synthesis quality, but it can increase the immersiveness when humans evaluate the generated videos. We leave more thorough investigation of more advanced rendering techniques such as true global illumination to future work. D. Implementation Details Motion Synthesis The video SDS loss is evaluated with CFG = 100. The diffusion time is sampled uniformly from [tstart, tend], where tstart = 0.02 and tend decrease linearly from 0.98 to 0.5 over the first 5000 optimization itIn the training loss, we set λ1 = 2 105 and erations. λ2 = 107. total of 10,000 optimization iterations are performed. Motion Tracking We use Warp [29] to simulate skeletons as articulated rigid bodies while optimizing the tracking loss and applying gradient clipping with PyTorch. chunk of simulation substeps is wrapped as differentiable Pytorch layer using torch.autograd.Function. During the forward pass, data from the PyTorch scope is transferred to Warp, and Warp gradient tape is initialized and stored to capture the computational graph within the Warp scope during forward simulation. In the backward pass, gradients received from the PyTorch scope are transferred to the Warp scope, and the gradient tape backpropagates the gradients in reverse time. Once the required gradients for the layers inputs are computed, gradient clipping is applied to them before transferring them back to the PyTorch scope. To ensure that assets remain standing on their own, we introduce virtual penalty force to keep the root bones of their articulation trees aligned with their initial upward directions. During training, we set λ3 = 0.2 and perform 200 optimization iterations. E. Skeleton Gallery Here, we present gallery of skeleton systems utilized in our experiments, as shown in Fig. 10. All skeletons are manually crafted in Blender. Mesh representations of assets are initially imported into Blender, followed by the embedding of bones based on their biokinematic structure. F. Additional Motion Diversity Experiments. We find that whether our method can produce diverse motions largely depends on the ability of the video model to synthesize desired full-body motions. In Fig. 11, we show fox jumping, and girl dancing and punching. On the other hand, it is difficult for our method to generate animals sitting since CogVideoX struggles with transitions"
        }
    ],
    "affiliations": [
        "NVIDIA",
        "UCLA"
    ]
}