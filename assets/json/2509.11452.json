{
    "paper_title": "Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting",
    "authors": [
        "Yining Lu",
        "Zilong Wang",
        "Shiyang Li",
        "Xin Liu",
        "Changlong Yu",
        "Qingyu Yin",
        "Zhan Shi",
        "Zixuan Zhang",
        "Meng Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Prior works in multi-objective reinforcement learning typically use linear reward scalarization with fixed weights, which provably fail to capture non-convex Pareto fronts and thus yield suboptimal results. This limitation becomes especially critical in online preference alignment for large language models. Here, stochastic trajectories generated by parameterized policies create highly non-linear and non-convex mappings from parameters to objectives that no single static weighting scheme can find optimal trade-offs. We address this limitation by introducing dynamic reward weighting, which adaptively adjusts reward weights during the online reinforcement learning process. Unlike existing approaches that rely on fixed-weight interpolation, our dynamic weighting continuously balances and prioritizes objectives in training, facilitating effective exploration of Pareto fronts in objective space. We introduce two approaches of increasing sophistication and generalizability: (1) hypervolume-guided weight adaptation and (2) gradient-based weight optimization, offering a versatile toolkit for online multi-objective alignment. Our extensive experiments demonstrate their compatibility with commonly used online reinforcement learning algorithms (including GRPO, REINFORCE, and RLOO), effectiveness across multiple mathematical reasoning datasets, and applicability to different model families, consistently achieving Pareto dominant solutions with fewer training steps than fixed-weight linear scalarization baselines."
        },
        {
            "title": "Start",
            "content": "Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting LEARNING TO OPTIMIZE MULTI-OBJECTIVE ALIGNMENT THROUGH DYNAMIC REWARD WEIGHTING Yining Lu12 Zilong Wang2 Qingyu Yin2 Zhan Shi2 Zixuan Zhang2 Meng Jiang12 1University of Notre Dame 2Amazon Shiyang Li2 Xin Liu2 Changlong Yu2 5 2 0 S 4 1 ] . [ 1 2 5 4 1 1 . 9 0 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Prior works in multi-objective reinforcement learning typically use linear reward scalarization with fixed weights, which provably fail to capture non-convex Pareto fronts and thus yield suboptimal results. This limitation becomes especially critical in online preference alignment for large language models. Here, stochastic trajectories generated by parameterized policies create highly non-linear and nonconvex mappings from parameters to objectives that no single static weighting scheme can find optimal trade-offs. We address this limitation by introducing dynamic reward weighting, which adaptively adjusts reward weights during the online reinforcement learning process. Unlike existing approaches that rely on fixed-weight interpolation, our dynamic weighting continuously balances and prioritizes objectives in training, facilitating effective exploration of Pareto fronts in objective space. We introduce two approaches of increasing sophistication and generalizability: (1) hypervolume-guided weight adaptation and (2) gradient-based weight optimization, offering versatile toolkit for online multi-objective alignment. Our extensive experiments demonstrate their compatibility with commonly used online reinforcement learning algorithms (including GRPO, REINFORCE, and RLOO), effectiveness across multiple mathematical reasoning datasets, and applicability to different model families, consistently achieving Pareto dominant solutions with fewer training steps than fixed-weight linear scalarization baselines. Figure 1: Pareto fronts obtained by our gradient-based weight optimization compared to three baselines using fixed-weight reward interpolation. We train the Qwen3-8B model (Yang et al., 2025) on the Math500 dataset (Lightman et al., 2023) using GRPO (Shao et al., 2024). The three training configurations, accuracy-focused, balanced, and efficiency-focused, correspond to different weight distributions initialized to our optimization objectives: accuracy, conciseness, and clarity. We aim to train models that achieve strong problem-solving ability (higher accuracy) with computational efficiency (fewer tokens) while maintaining interpretable reasoning processes (better clarity). Gray dots indicate Pareto suboptimal checkpoints generated along training. Clearly, our dynamic reward weighting consistently builds superior Pareto fronts that dominate baselines across all objectives, demonstrating its effectiveness in multi-objective alignment. Work done during an internship at Amazon. 1 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting"
        },
        {
            "title": "INTRODUCTION",
            "content": "Online reinforcement learning (RL) has become the de facto approach in aligning large language models (LLMs) for complex reasoning tasks, such as mathematical problem solving (Shao et al., 2024; Zhang & Zuo, 2025), code generation (Chen et al., 2025; Yao et al., 2025), and logical reasoning (Xie et al., 2025; Liu et al., 2025). While this approach has demonstrated significant success in improving model performance, it predominantly focuses on optimizing accuracy while overlooking other essential objectives that are crucial for practical deployment. For instance, key factors such as response length and output clarity, which directly impact inference efficiency and user experience, are not included in the rewarding process. In this paper, we focus on multi-objective online RL for LLMs that simultaneously optimizes auxiliary objectives alongside traditional accuracy metrics for complex reasoning tasks. Existing multi-objective alignment studies rely on either fixed weights (Yao et al., 2025; Team et al., 2025) or heuristic rules for reward interpolation (Zhang & Zuo, 2025; Aggarwal & Welleck, 2025). However, these methods have three critical limitations: (1) Empirically, we find that different objectives vary in learning difficulty.1 As result, objectives that reach saturation quickly will plateau throughout the remaining training phases while continuing to receive equal gradient updates, leading to inefficient allocation of learning efforts. (2) Theoretically, static linear scalarization provably fails to cover non-convex regions of the Pareto front and thus yields suboptimal training results (Roijers et al., 2013; Hayes et al., 2022). (3) Heuristic rules for combining objectives lack flexibility and cannot generalize to new objectives or different tasks. One natural way to address these limitations is to dynamically adjust objective weights throughout training based on the learning progress of each objective. This training paradigm, which we term dynamic reward weighting, embodies core principle that has proven effective in other optimization domains (Liu & Vicente, 2024; Fan et al., 2024): Redirecting learning effort towards objectives with the greatest potential for improvement. In this work, we demonstrate that applying this principle to multi-objective alignment in LLMs could yield substantial improvements over existing static weighting schemes. As shown in Figure 1, our gradient-based method outperforms all three baselines with varying weight configurations. Specifically, we propose two progressively sophisticated approaches, spanning from strict to flexible training constraints, to suit different multi-objective alignment scenarios. (1) Hypervolume-guided weight adaptation (4): When user preferences for different objectives are given, this method encourages the policy to discover new non-dominated solutions at each training step. It rewards new checkpoints that demonstrate positive hypervolume contributions, thereby proactively pushing the Pareto front in the desired optimization direction. (2) Gradient-based weight optimization (5): When user preferences are not available, the method computes how learning each objective contributes to improving overall model performance through gradient analysis and dynamically reallocates weights accordingly. Extensive experiments demonstrate that our methods outperform static linear scalarization baselines, achieving superior Pareto fronts with fewer training steps across multiple online RL algorithms (GRPO, REINFORCE, and RLOO), datasets (Math500 and MATH), and model families (Qwen3 and Deepseek). These results highlight the importance of dynamic reward weighting for multi-objective RL.2 In summary, our contributions are threefold: We identify the importance of dynamic reward weighting in multi-objective LLM alignment and formalize this optimization challenge. We propose comprehensive toolkit spanning hypervolume and gradient-based methods to address scenarios with and without human preference priors. We demonstrate the effectiveness of our toolkit across multiple online RL algorithms, datasets, and models, showing improved training efficiency and Pareto optimal performance. 1We provide more experimental justification in Appendix A.2. 2Code to reproduce our results: github.com/yining610/dynamic-reward-weighting. 2 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting"
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Multi-objective RL seeks to balance multiple, sometimes conflicting objectives when optimizing policies (Fleming et al., 2005; Emmerich & Deutz, 2018; Hayes et al., 2022; Huang et al., 2025). This is crucial because optimizing under each criterion can lead to significantly different policies being learning (Radulescu et al., 2020). We divide the notable prior works on multi-objective RL for LLMs into two groups: methods designed for training steerable multi-objective policies (2.1) and those focused on general reasoning preference alignment (2.2)."
        },
        {
            "title": "2.1 STEERABLE MULTI-OBJECTIVE PREFERENCE FINETUNING",
            "content": "Most multi-objective RL works in the LLM era explore the problem from steerability perspective, aiming to train policies that can be steered to generate desirable outputs across continuum of userpreferred reward weights. Representative approaches include off-policy RL methods (Zhou et al., 2024; Guo et al., 2024; Xiong & Singh, 2025), post-hoc policy mixing (Rame et al., 2023; Wang et al., 2025), and conditional training that incorporates preference weights as input (Yang et al., 2024a;b; Wang et al., 2024; Zhong et al., 2024). However, these methods typically treat objectives in static manner, either by using fixed-weight linear scalarization for rewards or by merging trained policies with fixed weights. While these approaches enable inference-time flexibility for users to navigate trade-offs between objectives, they suffer from fundamental limitation: linear scalarization provably fails to capture non-convex regions of the Pareto front (Hayes et al., 2022; Roijers et al., 2013), resulting in suboptimal policies. From geometric perspective, linear scalarization is equivalent to sweeping hyperplane whose normal vector is the given weights over the objective space. According to the supporting hyperplane theorem (Boyd & Vandenberghe, 2004), for each weight vector, there exists non-zero supporting hyperplane that is tangent to the convex hull of the Pareto front at some point. Consequently, linear scalarization can only recover Pareto optimal solutions on the convex portions of the Pareto front but fails on those residing in non-convex (concave) regions (Wei & Niethammer, 2022). This limitation becomes particularly critical in online RL settings, where stochastic trajectories create highly non-linear and non-convex mappings from policy parameters to objectives. We therefore replace static weighting with dynamic reward weighting mechanism that continuously rebalances and reprioritizes objectives during training. This approach facilitates more thorough exploration of the objective space, enabling the final policy to approximate Pareto optimal solutions in regions that static linear methods cannot reach. 2.2 GENERAL MULTI-OBJECTIVE PREFERENCE FINETUNING more straightforward application of multi-objective RL is to train policy to acquire multiple capabilities in single training run (Mukherjee et al., 2024), which is the direction we aim to improve. Since the introduction of PPO (Ouyang et al., 2022) and GRPO (Shao et al., 2024), on-policy RL has become the de facto approach for LLM preference alignment. Wu et al. (2023) have applied RLHF to train LLMs that generate more relevant, factual, and informative text. Beyond accuracy, GRPO has been used to train LLMs for high-quality code generation (Yao et al., 2025) and to produce more concise responses in mathematical reasoning tasks (Zhang & Zuo, 2025; Team et al., 2025; Aggarwal & Welleck, 2025). Yet these methods still compute rewards with static linear scalarization or rely on human-defined interpolation rules, limiting their generalizability to new objectives. To the best of our knowledge, no unified toolkit currently exists for automatically guiding LLM training across arbitrary objective sets. The most relevant research direction is multi-gradient for multi-objective optimization Desideri (2012); Zhou et al. (2022); Liu & Vicente (2024); Gu et al. (2024); Xu et al. (2025); Li et al. (2025a). These methods compute gradients for each objective separately and then combine them using various aggregation strategies to guide parameter updates. However, existing approaches primarily focus on relatively simple supervised fine-tuning tasks, such as logistic regression, and study on classical two-objective Fonseca problems (Fonseca & Fleming, 1996). They are not directly applicable to LLM alignment. 3 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting"
        },
        {
            "title": "3.1 NOTATIONS",
            "content": "We now formulate common notations used throughout the paper. We consider multi-objective optimization problem with objectives, where each objective is associated with weight wi for = 1, 2, . . . , K. The weight vector is denoted as = (w1, w2, . . . , wK), and the corresponding reward vector is = (r1, r2, . . . , rK). Following standard practice in RL, we denote the policy parameterized by θ as πθ, and the objective function as J(θ). We use superscript to denote weighted quantities (e.g., Gw for weighted discounted return and rw for weighted reward) and to denote single-objective quantities (e.g., wi and Ji(θ))."
        },
        {
            "title": "3.2 PARETO FRONT AND HYPERVOLUME CONTRIBUTION",
            "content": "Definition 3.1 (Pareto Front). Let Rn be the feasible set and : RK, (x) = (f1(x), . . . , fK(x)), the vector of objectives to be maximized. Write if ui vi for every and = v. The Pareto set and Pareto front are formally defined as: Pareto set := = {x : (y) (x)} , Pareto front := (P ) = {f (x) } . Intuitively, the Pareto front consists of all objective vectors for which no objective can be improved without sacrificing at least one other objective. Points in the Pareto set are called Pareto optimal or non-dominated solutions. Definition 3.2 (Hypervolume Indicator). Let = {a(1), . . . , a(n)} RK be finite set of objective vectors, where each a(i) = (a(i) ). Let = (r1, . . . , rK) RK be reference point satisfying rj a(i) for all {1, . . . , n} and {1, . . . , K} (i.e., is dominated by every a(i)). The hypervolume of with respect to is: (cid:16) (cid:91) , where [a, r] = { RK rj xj aj, {1, . . . , K}} 1 , . . . , a(i) HV(A; r) = Λ [a, r] (cid:17) aA Λ denotes the K-dimensional Lebesgue measure. The hypervolume indicator is the K-dimensional volume of the region dominated by and bounded below by r; higher hypervolume indicates larger coverage of the objective space. Definition 3.3 (Hypervolume Contribution). For any point RK, its hypervolume contribution to set is defined as the change in hypervolume when is presented and therefore it is always non-negative: HV(a, A) = HV(A {a}) HV(cid:0)A {a}) 0."
        },
        {
            "title": "4 HYPERVOLUME-GUIDED WEIGHT ADAPTATION",
            "content": "A natural and straightforward way to improve multiobjective alignment is to encourage pushing the Pareto front at each training step. Therefore, we introduce hypervolume-guided weight adaptation, which leverages human-specified weights through two hierarchical components: (1) the standard reward vector r, and (2) meta-level reward rpareto(r, B) that encourages policies to achieve new Pareto optimality. Note that we do not directly modify the given human-specified weights w; instead, we use the meta-level signal to amplify rewards when training discovers new Pareto fronts. We design rpareto(r) as: Figure 2: Visualization of rpareto(r, B). rpareto(r, B) = 0.5 + 1.5 tanh(HV(r, B)), (1) which has the shape shown in Figure 2.3 represents the performance buffer storing validation performance of the current Pareto set. In practice, we compute HV(r, B) using the recursive 3We experimented with multiple other activation functions with varying range and found that Eq. 1 is the most stable one for our scenario. We provide more details in Appendix A.5. 4 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting dimension-sweep algorithm of Fonseca et al. (2006), which achieves computational complexity O(nk2 log n) for points in dimensions. The complete procedure is detailed in Algorithm 1."
        },
        {
            "title": "4.1 EXPERIMENT SETUP",
            "content": "We evaluate our proposed method across multiple online RL algorithms, including natural policy gradient methods REINFORCE (Sutton et al., 1999) and RLOO (Ahmadian et al., 2024), and heuristic policy gradient method GRPO (Shao et al., 2024). The main experiments are conducted on the Math500 dataset (Lightman et al., 2023), which is split into train, validation, and test sets, using the Qwen38B model (Yang et al., 2025). To provide more comprehensive evaluation of our method, we then extend our experiments to the MATH dataset (Hendrycks et al., 2021) and an additional model family, Deepseek-LLM-7B-Chat (DeepSeekAI et al., 2024) in 6. All evaluations are conducted on the test set. Algorithm 1: Hypervolume-Guided Weight Adaptation 1: Input: Training set Dtrain, validation set Dval, initial policy parameters θ0, human specified weights RK 0 2: Hyperparameters: batch size B, rollout size G, maximum training steps No hypervolume contribution yet Sample mini-batch Db Dtrain of size for all query Db do 3: Evaluate rθ0 = Evaluate(θ0, Dval) 4: Initialize Pareto set {rθ0 } 5: rpareto 1 6: for = 1 to do 7: 8: 9: 10: 11: 12: 13: 14: Sample answers {yi}G Compute reward vectors {ri}G Compute scalar rewards ri wri Adaptation: ri rpareto ri J(θt1) using ri end for Update θt1 via chosen RL objective function i=1 πθt1 ( q) i=1 for each yi 15: 16: rθt Evaluate(θt, Dval) rpareto 0.5 + 1.5 tanh(HV(rθt , B)) Update meta weight based on hypervolume contribution For math tasks, we design the following three objectives: (1) generating correct solutions (accuracy), (2) producing efficient solutions that minimize computational steps and tokens (conciseness), and (3) demonstrating clear reasoning with explicit, step-by-step thinking process (clarity). We denote the weight vector as = [waccuracy, wconciseness, wclarity]. Each objective is evaluated using heuristic rules to ensure the corresponding reward is verifiable (0 or 1). For easier analysis, in the following tables and figures, we present accuracy and clarity performance based on their reward scores, while conciseness performance is reported directly through response length. if HV(rθt ; B) > 0 then {rθt } 17: 18: 19: 20: end for Update Pareto set end if Following common practice in multi-objective RL (Rame et al., 2023; Li et al., 2025b), we use fixed-weight linear scalarization as our baseline. Specifically, we evaluate three carefully chosen weight configurations that represent different optimization priorities: 1) Accuracy-focused (w = [0.5, 0.25, 0.25]) prioritizing accuracy over conciseness and formatness; 2) Balanced (w = [0.334, 0.333, 0.333]) weighting all objectives equally; and 3) Efficiency-focused (w = [0.25, 0.375, 0.375]) emphasizing conciseness and clarity. More experiment setup details can be found in Appendix A.4. 4.2 RESULTS Method Training RL Accuracy-focused Balanced Efficiency-focused Baseline Hypervolume-guided Baseline Hypervolume-guided Baseline Hypervolume-guided Accuracy / Response Length / Clarity 0.832 / 701 / 0.962 0.850 / 619 / 0.970 0.832 / 687 / 0.935 0.825 / 683 / 0.955 0.837 / 650 / 0.970 GRPO REINFORCE 0.789 / 837 / 0.977 0.805 / 827 / 0.988 0.798 / 872 / 0.994 0.788 / 837 / 0.978 0.778 / 676 / 0.985 0.827 / 677 / 0.965 0.829 / 813 / 0.940 0.820 / 720 / 0.940 0.823 / 639 / 0.955 0.830 / 565 / 0.967 RLOO 0.840 / 731 / 0.967 0.790 / 618 / 1.000 0.843 / 700 / 0. Table 1: Pareto fronts obtained from hypervolume-guided weight adaptation compared to fixedweight baselines. and indicate optimization direction. Each value represents the average performance across the entire Pareto front under given training setup. Hypervolume-guided weighting outperforms baselines across most objectives, weight configurations, and RL algorithms, and in certain cases, demonstrates full dominance with superior results on all three objectives. We compare our hypervolume-guided weight adaptation against fixed-weight baselines in Table 1. Across all three online RL algorithms, there is consistently at least one weight configuration where 5 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting our method outperforms the baselines on all objectives. For example, under both accuracyand efficiency-focused settings, REINFORCE equipped with hypervolume-guided weighting achieves Pareto fronts that have better average scores across all three objectives. It is important to note that better average scores of Pareto fronts do not necessarily guarantee true Pareto optimality. Due to space constraints, we include the full Pareto front visualizations in Appendix A.6. These plots provide compelling visual evidence that REINFORCE with our hypervolume-guided weight adaptation achieves clearly superior Pareto fronts, dominating baseline methods under both accuracyand efficiency-focused settings. This visual analysis confirms and strengthens the quantitative results presented above. Figure 3: Meta-reward rpareto distributions, with vertical dashed lines indicating the average value. To better understand the performance differences between weight configurations, we analyze the meta-reward rpareto during REINFORCE training. As shown in Figure 3, the balanced weight setting yields higher meta-rewards that encourage more aggressive exploration, paradoxically, leading to suboptimal performance compared to two other configurations. This suggests that effective hypervolume-guided weighting requires careful calibration of meta-reward to avoid overly aggressive updates that can hinder convergence."
        },
        {
            "title": "5 GRADIENT-BASED WEIGHT OPTIMIZATION",
            "content": "We now consider more sophisticated yet flexible setting where reward weights are not fixed but dynamically adapted in real-time. In this scenario, human-specified preferences are not assumed. We formulate the online weights adaptation during training as an optimization problem and begin our study with the natural policy-gradient method REINFORCE: J(θ) = E(cid:2) log πθ(at st) Gw (2) (cid:3), which exhibits linearity to the per-objective gradients given by J(θ) = (cid:80)K We prove this linearity through simple nested summation Gw (cid:80) i=1 wiri such that J(θ) = (cid:80)K i=1 wi Ji(θ). t+l+1 = t. Then there exists constant t+l+1 = (cid:80)K = (cid:80) l=0 γl (cid:80)K i=1 wiGi l=0 γlrw (cid:80) i=1 wi t+l+1 = (cid:80)K l=0 γlri i=1 wiJi(θ) + C, which implies (cid:88) arg min wK J(θ(w)) = arg min wK i=1 wiJi(θ) Inspired by domain adaptation (Fan et al., 2024) and based on the above linearity assumption, we can derive the following reward weight update rule, where η is the learning rate and µ is the regularization factor (we provide full proof in Appendix A.1 and the whole procedure in Algorithm 2): w(t) = w(t) w(t) (cid:80) , where w(t) = w(t1) exp( η(t)I (t) µ ), (t) = Ji(θ(t)), (cid:88) k[K] Jk(θ(t)). (3) Remark. The (t) measures the total influence that learning objective has on the remaining 1 objectives (Ji, (cid:80) k=i Jk) plus the magnitude of its own gradient (Ji2 2). Intuitively, similar to prior studies in data influence (Koh & Liang, 2017) and domain adaptation (Fan et al., 2024), we upweight an objective when it has high learning potential on other objectives (high influence) and has not been learned enough yet (high gradient magnitude). 5.1 CONVERGENCE ANALYSIS natural concern in weight updating is whether the weights may collapse to zero or explode to the boundary value of one within single update step. To address this concern, we provide convergence analysis for Eq. 3. We first make some standard assumptions used across existing RL (Wang et al., 2017; Wu et al., 2020; Kumar et al., 2023) and multi-objective optimization studies (Zhou et al., 2022; Liu & Vicente, 2024; Xu et al., 2025). 6 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting Assumption 5.1 (Lipschitz Continuity). The policy πθ is differentiable with respect to θ and the output log πθ(a s) is L-Lipschitz and has bounded norm log πθ(a s) for any θ. Assumption 5.2 (Bounded Reward). The reward is bounded in the range [0, rmax]. Assumption 5.3 (Learning Rate). The learning rate η(t) is non-increasing and non-negative. Assumption 5.4 (Bounded Policy Gradient). The true gradient (Eq. 2) of the objective function is bounded Jk(θ) which is established by log πθ(a s) < (Assumption 5.1) and Gt = (cid:80) Lemma 5.1. Recall the weight update rule defined in Eq. 3, its true weight takes the closed form: t=1 τ (t)I (t) l=0 γlrt+l+1 (cid:80) 1γ (Assumption 5.2). l=0 γlrmax = rmax (cid:1) w(T ) = w(0) k[K] w(0) exp (cid:0) (cid:80)T exp (cid:0) (cid:80)T (cid:80) t=1 τ (t)I (t) (cid:1) , where τ (t) = η(t) µ Proof. The lemma holds when = 1: w(1) = (cid:80) w(0) k[K] w(0) exp (cid:0)τ (1)I (1) exp (cid:0)τ (1)I (1) (cid:1) (cid:1) . Suppose the lemma holds for all values up to 1, for step at , we have: w(T ) = = = w(T 1) w(T 1) exp(τ (T )I (T ) ) exp(τ (T )I (T ) i (cid:80) (cid:80) = ) (cid:80) (cid:1) t=1 τ (t)I (t) exp (cid:0) (cid:80)T 1 w(0) exp (cid:0) (cid:80)T 1 k[K] w(0) exp (cid:0) (cid:80)T 1 w(0) k[K] w(0) exp (cid:0) (cid:80)T 1 t=1 τ (t)I (t) (cid:1) t=1 τ (t)I (t) (cid:80) t=1 τ (t)I (t) (cid:1) exp(τ (T )I (T ) ) (cid:1) exp(τ (T )I (T ) ) w(0) w(0) w(0) w(0) exp (cid:0) (cid:80)T 1 exp (cid:0) (cid:80)T 1 exp (cid:0) (cid:80)T exp (cid:0) (cid:80)T t=1 τ (t)I (t) t=1 τ (t)I (t) (cid:1) t=1 τ (t)I (t) t=1 τ (t)I (t) (cid:1) (cid:80) (cid:80) (cid:1) exp(τ (T )I (T ) ) (cid:1) exp(τ (T )I (T ) ) Theorem 5.2. Suppose Assumptions 5.1-5.4 hold and that the step-size sequence of τ converges (cid:80) t=1 τ (t) ℓ < ϵ for every arbitrarily small positive number ϵ. Then, for every pair of objectives i, [K], their weight ratio is uniformly bounded: w(T ) = O(1), N. /w(T ) Proof. w(T ) w(T ) = w(0) w(0) w(0) w(0) w(0) w(0) = O(1) (cid:1) exp (cid:0) (cid:80)T exp (cid:0) (cid:80)T (cid:16) (cid:88) t=1 τ (t)I (t) t=1 τ (t)I (t) τ (t)2KC 2(cid:17) exp (cid:1) = t=1 exp(2KC 2ℓ) w(0) w(0) exp (cid:16) (cid:88) t=1 τ (t)(I (t) (cid:17) (t) ) , (Ii = Ji(θ), Jk(θ) KC 2) (cid:88) /w(T ) Remark. The ratio w(T ) measures the cumulative advantage that objective has gained over objective throughout training, determined by both their initial weights w(0) and the accumulated influence across iterations (t). Intuitively, an objective that is more influential on the gradient update will naturally gain higher weight relative to less influential objective. Keeping this ratio uniformly bounded has two merits. First, it offers numerical stability. bounded ratio prevents weight collapse or explosion from the exponential update, ensuring that the updated weights remain properly constrained and ultimately converge to valid multi-objective equilibrium. Second, it has practical guidance for hyperparameter selection. Initializing uniformly and employing learning rate scheduler in which η(t) converges, so that each step updates the weights within safe and predictable bounds. Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting"
        },
        {
            "title": "5.2 EXPERIMENT SETUP",
            "content": "initial objective weights w(0) RK 0 Algorithm 2: Gradient-Based Weight Optimization 1: Input: Training set Dtrain, initial policy parameters θ0, Sample mini-batch Db Dtrain of size for all query Db do 2: Hyperparameters: batch size B, rollout size G, learning rate η, regularization factor µ, maximum training steps As suggested by Theorem 5.2, we initialize the reward weights using balanced (uniform) setting and use polynomial learning rate scheduler whose learning rate sequence converges, instead of the constant scheduler used in 4. To maintain the linearity between the overall reward and the per-objective rewards, we do not add the token-level KL divergence penalty from the reference model in each reward. Furthermore, standard approaches for preference finetuning LLMs with REINFORCE, such as REINFORCE++ (Hu et al., 2025), typically employ clipping mechanisms that render the objective nondifferentiable and break the linearity assumptions made in Eq. 2. Therefore, in our experiments, we set both the clip range ϵ (Schulman et al., 2017) and dual clip constant (Ye et al., 2020) high (ϵ = = 100) to practically disable clipping.4 To improve computational efficiency, we compute gradients only from middle layers, as these layers typically encode the most abstract and generalizable knowledge patterns (Grosse et al., 2023). All other experiment settings follow 4.1. 3: for = 1 to do 4: 5: 6: 7: 8: 9: 10: 11: 12: end for Obtain per-objective gradients {Ji(θt1, Db)}K Compute influence signal (t) Update weights: Compute scalar rewards { w(t)ri }G Update θt1 via chosen RL objective function w(t) w(t1) exp(cid:0) η(t) I(t) w(t) w(t)(cid:14) (cid:80) Sample answers {yi}G Compute reward vectors {ri}G J(θt1) using scalarized rewards i=1 πθt1 ( q) i=1 See Eq. 3 i=1 for each yi 16: end for 13: 14: 15: w(t) i=1 (cid:1) µ 5.3 RESULTS Method Accuracy-focused Balanced Efficiency-focused Gradient-based(Ours) Training RL Accuracy / Response Length / Clarity GRPO 0.836 / 831 / 0.948 0.830 / 861 / 0.960 0.835 / 842 / 0.950 REINFORCE 0.764 / 1375 / 0.830 0.760 / 1336 / 0.850 0.755 / 1361 / 0.825 0.815 / 1100 / 0.905 0.820 / 847 / 0.937 0.830 / 824 / 0.940 RLOO 0.836 / 650 / 0.980 0.802 / 1202 / 0.868 0.820 / 701 / 0.980 Table 2: Pareto fronts obtained from gradient-based weight optimization compared to fixed-weight baselines. and indicate optimization direction. Each value represents the average performance across the entire Pareto front under given training setup. Clearly, the gradient-based weighting approach yields superior Pareto fronts compared to all baselines across both GRPO and REINFORCE training setups, and achieves improved performance on two key objectives (conciseness and clarity) under RLOO training. We compare our gradient-based weight optimization approach against baselines trained with different weight configurations in Table 2. Our method consistently outperforms all baselines in multi-objective alignment across different online RL algorithms. The corresponding Pareto fronts visualized in Figure 11 (Appendix A.6) further support these findings, demonstrating that our method generates superior Pareto fronts that dominate all baseline approaches under both GRPO and REINFORCE training. Learning differs for objectives. We analyze the evolution of objective weights during training to better understand the relative importance of each objective. As shown in Figure 4, the weight for conciseness rapidly converges to approximately 0.2, with the lost weight mostly shifted to accuracy. Consequently, the accuracy weight exhibits Figure 4: Reward weight evolution over training. 4We also experimented with standard clipping settings (ϵ = 0.2, = 3) and observed that disabling clipping did lead to improved training results. 8 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting continuous growth, which aligns with our intuition that accuracy is more challenging objective requiring sustained learning. In contrast, conciseness improvements can be achieved rapidly (see Figure 7 in Appendix A.2) and thus require less learning effort. The higher weights for accuracy and clarity compared to conciseness also support our preliminary findings that accuracy and clarity objectives are highly intertwined in the optimization process, effectively playing similar roles in model updates as evidenced by their consistently low KL-divergence trajectories shown in Figure 6, and driving LLM learning towards the same direction. Conversely, conciseness works orthogonally to these objectives, applying different influence on model training. We provide further analysis in Appendix A.2."
        },
        {
            "title": "6 GENERALIZABILITY AND CONVERGENCE RATE TESTS",
            "content": "Figure 5: First row: Pareto fronts of Qwen3-8B (Yang et al., 2025) model trained on MATH (Hendrycks et al., 2021) algebra problems using GRPO. Second row: Pareto fronts of Deepseek-7B (DeepSeek-AI et al., 2024) model trained on Math500 (Hendrycks et al., 2021) dataset using GRPO. To demonstrate the training efficiency of dynamic reward weighting, we compute the average number of training steps required to achieve the current Pareto fronts. As illustrated in Table 3, while the hypervolume-guided method that proactively pushes Pareto fronts has only marginal efficiency gains compared to baselines, the gradient-based method consistently has higher convergence rate, reducing the required steps by 6.1 on average across RL algorithms. Method Hypervolume-guided Gradient-based Online RL Baseline Ours () Baseline Ours () GRPO REINFORCE RLOO 66.5 63.8 61. 64.0 (-2.5) 65.5 (+1.7) 62.5 (+1.4) 71.5 68.5 68.7 62.6 (-8.9) 67.2 (-1.3) 60.5 (-8.2) Table 3: Average number of training steps required to reach Pareto fronts for Hypervolume-Guided and Optimization methods versus their respective baselines. We further extend our experiments to different datasets and model families to validate the generalizability of our methods (Figure 5). For fair comparison, we use the same constant learning rate for both methods and the baseline. Similar to our main experiment results, our two methods achieve superior trade-offs between accuracy, response length, and clarity compared to the baseline, with the gradient-based weighting showing the best overall performance."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We find that different objectives in multi-objective LLM alignment require varying learning efforts and therefore propose dynamic reward weighting. We introduce two dynamic reward weighting approaches of increasing sophistication: hypervolume-guided weight adaptation and gradient-based weight optimization. Our experiments across multiple online RL algorithms, datasets, and model families demonstrate that both methods consistently outperform fixed-weight linear scalarization baselines, achieving superior Pareto optimal solutions while improving training efficiency. 9 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting"
        },
        {
            "title": "REFERENCES",
            "content": "Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning, 2025. URL https://arxiv.org/abs/2503.04697. Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustun, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms, 2024. URL https://arxiv.org/abs/2402.14740. Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167175, 2003. ISSN 0167-6377. doi: https://doi.org/10.1016/S0167-6377(02)00231-6. URL https://www.sciencedirect. com/science/article/pii/S0167637702002316. S.P. Boyd and L. Vandenberghe. Convex Optimization. Number pt. 1 in Berichte uber verteilte ISBN 9780521833783. URL https:// messysteme. Cambridge University Press, 2004. books.google.com/books?id=mYm0bLd3fcoC. Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acereason-nemotron: Advancing math and code reasoning through reinforcement learning, 2025. URL https://arxiv.org/abs/2505.16400. DeepSeek-AI, :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. Deepseek llm: Scaling open-source language models with longtermism, 2024. URL https: //arxiv.org/abs/2401.02954. Jean-Antoine Desideri. Multiple-gradient descent algorithm (mgda) for multiobjective optimization. Comptes Rendus Mathematique, 350(5):313318, 2012. ISSN 1631-073X. doi: https://doi. org/10.1016/j.crma.2012.03.014. URL https://www.sciencedirect.com/science/ article/pii/S1631073X12000738. Michael TM Emmerich and Andre Deutz. tutorial on multiobjective optimization: fundamentals and evolutionary methods. Natural computing, 17:585609, 2018. URL https://link. springer.com/article/10.1007/s11047-018-9685-y. Simin Fan, Matteo Pagliardini, and Martin Jaggi. Doge: domain reweighting with generalization estimation. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. URL https://arxiv.org/abs/2310.15393. Peter J. Fleming, Robin C. Purshouse, and Robert J. Lygoe. Many-objective optimization: An engineering design perspective. In Carlos A. Coello Coello, Arturo Hernandez Aguirre, and Eckart Zitzler (eds.), Evolutionary Multi-Criterion Optimization, pp. 1432, Berlin, Heidelberg, 2005. Springer Berlin Heidelberg. ISBN 978-3-540-31880-4. URL https://link.springer. com/chapter/10.1007/978-3-540-31880-4_2. Carlos M. Fonseca and Peter J. Fleming. On the performance assessment and comparison of stochastic multiobjective optimizers. In Hans-Michael Voigt, Werner Ebeling, Ingo Rechenberg, and Hans-Paul Schwefel (eds.), Parallel Problem Solving from Nature PPSN IV, pp. 584 593, Berlin, Heidelberg, 1996. Springer Berlin Heidelberg. ISBN 978-3-540-70668-7. URL https://link.springer.com/chapter/10.1007/3-540-61723-X_1022. 10 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting C.M. Fonseca, L. Paquete, and M. Lopez-Ibanez. An improved dimension-sweep algorithm for the hypervolume indicator. In 2006 IEEE International Conference on Evolutionary Computation, pp. 11571163, 2006. doi: 10.1109/CEC.2006.1688440. URL https://ieeexplore. ieee.org/document/1688440. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzman, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vıtor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan SmothLearning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting ers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamile Lukoˇsiute, Karina Nguyen, Nicholas Joseph, Sam McCandlish, Jared Kaplan, and Samuel R. Bowman. Studying large language model generalization with influence functions, 2023. URL https://arxiv.org/ abs/2308.03296. Shangding Gu, Bilgehan Sel, Yuhao Ding, Lu Wang, Qingwei Lin, Alois Knoll, and Ming Jin. Safe and balanced: framework for constrained multi-objective reinforcement learning, 2024. URL https://arxiv.org/abs/2405.16390. Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Zexu Sun, Bowen Sun, Huimin Chen, Ruobing Xie, Jie Zhou, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Controllable preference optimization: Toward controllable multi-objective alignment. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 14371454, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.85. URL https: //aclanthology.org/2024.emnlp-main.85/. Conor Hayes, Roxana Radulescu, Eugenio Bargiacchi, Johan Kallstrom, Matthew Macfarlane, Mathieu Reymond, Timothy Verstraeten, Luisa Zintgraf, Richard Dazeley, Fredrik Heintz, 12 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting et al. practical guide to multi-objective reinforcement learning and planning. Autonomous Agents and Multi-Agent Systems, 36(1):26, 2022. URL https://link.springer.com/ article/10.1007/s10458-022-09552-y. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math the Neural Information dataset. Processing Systems Track on Datasets and Benchmarks, volume 1, 2021. URL https: //datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/ 2021/file/be83ab3ecd0db773eb2dc1b0a17836a1-Paper-round2.pdf. In J. Vanschoren and S. Yeung (eds.), Proceedings of Jian Hu, Xibin Wu, Wei Shen, Jason Klein Liu, Zilin Zhu, Weixun Wang, Songlin Jiang, Haoran Wang, Hao Chen, Bin Chen, Weikai Fang, Xianyu, Yu Cao, and Haotian Xu. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models, 2025. URL https: //arxiv.org/abs/2501.03262. Zenan Huang, Yihong Zhuang, Guoshan Lu, Zeyu Qin, Haokai Xu, Tianyu Zhao, Ru Peng, Jiaqi Hu, Zhanming Shen, Xiaomeng Hu, Xijun Gu, Peiyi Tu, Jiaxin Liu, Wenyu Chen, Yuzhuo Fu, Zhiting Fan, Yanmei Gu, Yuanyuan Wang, Zhengkai Yang, Jianguo Li, and Junbo Zhao. Reinforcement learning with rubric anchors, 2025. URL https://arxiv.org/abs/2508.12790. Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 18851894. PMLR, 0611 Aug 2017. URL https://proceedings.mlr.press/v70/koh17a. html. Harshat Kumar, Alec Koppel, and Alejandro Ribeiro. On the sample complexity of actorcritic method for reinforcement learning with function approximation. Mach. Learn., 112(7): 24332467, February 2023. doi: 10.1007/s10994-023-06303-2. URL https://doi.org/10.1007/s10994-023-06303-2. ISSN 0885-6125. Chengao Li, Hanyu Zhang, Yunkun Xu, Hongyan Xue, Xiang Ao, and Qing He. Gradient-adaptive policy optimization: Towards multi-objective alignment of large language models, 2025a. URL https://arxiv.org/abs/2507.01915. Zhuo Li, Guodong Du, Weiyang Guo, Yigeng Zhou, Xiucheng Li, Wenya Wang, Fangming Liu, Yequan Wang, Deheng Ye, Min Zhang, and Jing Li. Multi-objective large language model alignment with hierarchical experts, 2025b. URL https://arxiv.org/abs/2505.20925. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/2305.20050. Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models, 2025. URL https://arxiv.org/abs/2505.24864. Suyun Liu and Luis Nunes Vicente. The stochastic multi-gradient algorithm for multi-objective optimization and its application to supervised machine learning. Annals of Operations Research, 339(3):11191148, 2024. URL https://link.springer.com/article/10.1007/ s10479-021-04033-z. Mistral AI Team. Ministral-8b-instruct-2410, 2024. URL https://huggingface.co/ mistralai/Ministral-8B-Instruct-2410. Subhojyoti Mukherjee, Anusha Lalitha, Sailik Sengupta, Aniket Deshmukh, and Branislav Kveton. Multi-objective alignment of large language models through hypervolume maximization, 2024. URL https://arxiv.org/abs/2412.05469. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, 13 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155. Alexandre Rame, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa towards pareto-optimal Shukor, Laure Soulier, and Matthieu Cord. alignment by interpolating weights fine-tuned on diverse rewards. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 7109571134. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2023/ 2023. file/e12a3b98b67e8395f639fde4c2b03168-Paper-Conference.pdf. Rewarded soups: Diederik M. Roijers, Peter Vamplew, Shimon Whiteson, and Richard Dazeley. survey of multiISSN objective sequential decision-making. J. Artif. Int. Res., 48(1):67113, October 2013. 1076-9757. URL https://dl.acm.org/doi/abs/10.5555/2591248.2591251. Roxana Radulescu, Patrick Mannion, Yijie Zhang, Diederik M. Roijers, and Ann Nowe. utilitybased analysis of equilibria in multi-objective normal-form games. The Knowledge Engineering Review, 35:e32, 2020. doi: 10.1017/S0269888920000351. URL https://arxiv.org/ abs/2001.08177. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402. 03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, pp. 12791297. ACM, doi: 10.1145/3689031.3696075. URL http://dx.doi.org/10.1145/ March 2025. 3689031.3696075. Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In S. Solla, T. Leen, and K. Muller (eds.), Advances in Neural Information Processing Systems, volume 12. MIT Press, URL https://proceedings.neurips.cc/paper_files/paper/1999/ 1999. file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, and Zongyu Lin. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599. Kaiwen Wang, Rahul Kidambi, Ryan Sullivan, Alekh Agarwal, Christoph Dann, Andrea Michi, Marco Gelmi, Yunxuan Li, Raghav Gupta, Kumar Avinava Dubey, Alexandre Rame, Johan Ferret, Geoffrey Cideron, Le Hou, Hongkun Yu, Amr Ahmed, Aranyak Mehta, Leonard Hussenot, Olivier Bachem, and Edouard Leurent. Conditional language policy: general framework for steerable multi-objective finetuning. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 14 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting 21532186, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.118. URL https://aclanthology.org/2024. findings-emnlp.118/. Tianze Wang, Dongnan Gui, Yifan Hu, Shuhang Lin, and Linjun Zhang. Mpo: An efficient postprocessing framework for mixing diverse preference alignment, 2025. URL https://arxiv. org/abs/2502.18699. Yue Wang, Wei Chen, Yuting Liu, Zhi-Ming Ma, and Tie-Yan Liu. Finite sample analIn I. Guyon, U. Von ysis of the gtd policy evaluation algorithms in markov setting. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2017/ 2017. file/353de26971b93af88da102641069b440-Paper.pdf. Susan Wei and Marc Niethammer. The fairness-accuracy pareto front. Statistical Analysis and Data Mining: The ASA Data Science Journal, 15(3):287302, 2022. URL https: //onlinelibrary.wiley.com/doi/abs/10.1002/sam.11560. Yue Frank Wu, Weitong ZHANG, Pan Xu, and Quanquan Gu. finite-time analysis of two timescale actor-critic methods. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1761717628. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/ paper/2020/file/cc9b3c69b56df284846bf2432f1cba90-Paper.pdf. Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Fine-grained human feedIn A. Oh, T. Naumann, InInc., URL https://proceedings.neurips.cc/paper_files/paper/2023/ Noah Smith, Mari Ostendorf, and Hannaneh Hajishirzi. back gives better A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural formation Processing Systems, volume 36, pp. 5900859033. Curran Associates, 2023. file/b8c90b65739ae8417e61eadb521f63d5-Paper-Conference.pdf. language model rewards for training. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning, 2025. URL https://arxiv.org/abs/2502.14768. Nuoya Xiong and Aarti Singh. Projection optimization: general framework for multi-objective and multi-group rlhf, 2025. URL https://arxiv.org/abs/2502.15145. Mingjing Xu, Peizhong Ju, Jia Liu, and Haibo Yang. Psmgd: Periodic stochastic multi-gradient descent for fast multi-objective optimization. Proceedings of the AAAI Conference on Artificial Intelligence, 39(20):2177021778, Apr. 2025. doi: 10.1609/aaai.v39i20.35482. URL https: //ojs.aaai.org/index.php/AAAI/article/view/35482. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Kailai Yang, Zhiwei Liu, Qianqian Xie, Jimin Huang, Tianlin Zhang, and Sophia Ananiadou. Metaaligner: Towards generalizable multi-objective alignment of language models. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 3445334486. Curran Associates, Inc., 2024a. URL https://proceedings.neurips.cc/paper_files/paper/2024/ file/3d03800841fa1bb2f43ef1750aafcce4-Paper-Conference.pdf. 15 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu, and Jianshu Chen. Rewardsin-context: Multi-objective alignment of foundation models with dynamic preference adjustment. In International Conference on Machine Learning, pp. 5627656297. PMLR, 2024b. URL https://arxiv.org/abs/2402.10207. Feng Yao, Zilong Wang, Liyuan Liu, Junxia Cui, Li Zhong, Xiaohan Fu, Haohui Mai, Vish Krishnan, Jianfeng Gao, and Jingbo Shang. Training language models to generate quality code with program analysis feedback, 2025. URL https://arxiv.org/abs/2505.22704. Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang, Xipeng Wu, Qingwei Guo, Qiaobo Chen, Yinyuting Yin, Hao Zhang, Tengfei Shi, Liang Wang, Qiang Fu, Wei Yang, and Lanxiao Huang. Mastering complex control in moba games with deep reinforcement learning. Proceedings of the AAAI Conference on Artificial Intelligence, 34(04): 66726679, Apr. 2020. doi: 10.1609/aaai.v34i04.6144. URL https://ojs.aaai.org/ index.php/AAAI/article/view/6144. Jixiao Zhang and Chunsheng Zuo. Grpo-lead: difficulty-aware reinforcement learning approach for concise mathematical reasoning in language models, 2025. URL https://arxiv.org/ abs/2504.09696. Yifan Zhong, Chengdong Ma, Xiaoyuan Zhang, Ziran Yang, Haojun Chen, Qingfu Pareto alignment via preference In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. PaInformation ProcessURL Inc., 2024. Zhang, Siyuan Qi, and Yaodong Yang. adaptation for in Neural quet, ing Systems, volume 37, pp. 7552275558. Curran Associates, https://proceedings.neurips.cc/paper_files/paper/2024/file/ 89f39d0b3d49a47606a165eefba2778c-Paper-Conference.pdf. and C. Zhang (eds.), Advances J. Tomczak, Panacea: llms. Shiji Zhou, Wenpeng Zhang, Jiyan Jiang, Wenliang Zhong, Jinjie GU, and Wenwu Zhu. On the convergence of stochastic multi-objective gradient manipulation and beyond. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 3810338115. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2022/ 2022. file/f91bd64a3620aad8e70a27ad9cb3ca57-Paper-Conference.pdf. Zhanhui Zhou, Jie Liu, Jing Shao, Xiangyu Yue, Chao Yang, Wanli Ouyang, and Yu Qiao. Beyond one-preference-fits-all alignment: Multi-objective direct preference optimization. In LunWei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 1058610613, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.630. URL https: //aclanthology.org/2024.findings-acl.630/. 16 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 PROOF FOR GRADIENT-BASED WEIGHT OPTIMIZATION We first write optimizing reward weights RK along the training of the policy model θ as bilevel-optimization problem: arg min (cid:88) wRK i[K] Ji(θ(w)), s.t. θ(w) arg min θ (cid:88) i[K] wiJi(θ). (cid:80) The lower-level objective θ(w) is updated using the weights w, and the higher-level objective is found based on the loss function of the updated model. With greedy approximation of Ji(θ(t)(w)) (for simplicity, we denote θ(t)(w) = θ(t) hereafter), we search for the optiminw mal reward weights w(t) at step to minimize the average loss over objectives at step + 1: (cid:2)Ji(θ(t+1)) Ji(θ(t))(cid:3) (cid:88) (cid:88) (4) arg min wRK J(θ(t+1)) = arg min wRK Ji(θ(t+1)) = arg min wRK i[K] i[K] From the first-order Taylor approximation, we have Ji(θ(t+1)) = Ji(θ(t)) + Ji(θ(t)) (θ(t+1) θ(t)) + O(θ(t+1) θ(t)) (cid:104) η(t) (cid:88) w(t) (cid:105) Ji(θ(t)) + O(w), = Ji(θ(t)) + Ji(θ(t)) where O(θ(t+1)θ(t)) = O(w) as the high-order remainder from Tylor approximation. Therefore, we can write Eq. 4 as: arg min wRK J(θ(t+1)) = arg min wRK (cid:88) i[K] Ji(θ(t)) (cid:104) η(t) (cid:88) w(t) (cid:105) Jk(θ(t)) + O(w) k[K] i[K] = arg min wRK = arg min wRK = arg min wRK = arg min wRK η(t) (cid:88) Ji(θ(t)) (cid:104) (cid:88) w(t) Jk(θ(t)) (cid:105) + O(w) i[K] k[K] η(t) (cid:88) w(t) (cid:104) Ji(θ(t)) (cid:88) (cid:105) Jk(θ(t)) + O(w) i[K] η(t) (cid:88) i[K] k[K] (t) w(t) + O(w), where (t) Ji(θ(t)), (cid:88) k[K] Jk(θ(t)) η(t)w, (t) + O(w), where (t) = [I (t) 1 , (t) 2 , . . . , (t) ]. Follow common pratice in mirror descent (Beck & Teboulle, 2003), we estimate O(w) by introducing regularization term with factor µ via Bregman divergence Dh(ww(t1)) = h(w) h(w(t1)) h(w(t1)), w(t1), where we employ the entropy function h(w) = (cid:80) wi ln wi. So the above equation becomes: w(t) := arg min wRK η(t)w, (t) + µ (cid:16) h(w) h(w(t1)) h(w(t1)), w(t1) (5) (cid:17) η(t)w, (t) + µ (cid:16) h(w) h(w(t1)), (cid:17) . = arg min wRK We then take the derivative of Eq. 5: (cid:0) η(t)w, (t) + µ(cid:0)h(w) h(w(t1)), w(cid:1)(cid:1) = 0 η(t)I (t) + µ[ln + 1]i µ[ln w(t1) + 1]i = 0 ln w(t) = ln w(t1) + η(t)I (t) µ . Therefore, we can get the following update rule for reward weights with normalization: w(t) = w(t) w(t) (cid:80) , where w(t) = w(t1) exp( η(t)I (t) µ ), (t) = Ji(θ(t)), (cid:88) k[K] Jk(θ(t)). (6) 17 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting A.2 PRELIMINARY FINDINGS Figure 6: Pairwise KL divergence trajectories between models trained under different weight configurations. Rows represent the models own weight configurations and columns the configuration it is compared against; within each panel, the KL-divergence (y-axis) is plotted over training steps (x-axis). Accuracy and clarity objectives exhibit synergistic effects while conciseness drives learning orthogonally. To understand the synergistic or antagonistic effect between different objectives in model training, we analyze pairwise KL divergence trajectories between models trained with different weight configurations: accuracy-focused (0.5, 0.25, 0.25), efficiency-focused (0.25, 0.375, 0.375), accuracy-only (1, 0, 0), conciseness-only (0, 1, 0), and clarity-only (0, 0, 1). We conduct this study using the Qwen3-8B model trained on the Math500 dataset via GRPO. As shown in Figure 6, the KL divergence between the accuracyand clarity-only configurations remains consistently low throughout training, indicating these objectives exhibit strong synergistic effects and induce similar parameter updates. In contrast, models trained with conciseness-only weights diverge significantly from all accuracyand clarity-only(focused) models, as evidenced by steadily increasing KL divergence over time. This suggests that conciseness applies an orthogonal influence on model optimization, steering parameter updates in fundamentally different direction compared to accuracy and clarity. Objectives exhibit varying convergence rates. When training individual objectives, we observe that different objectives reach saturation at different training stages, as illustrated in Figure 7. 18 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting (a) Accuracy (b) Conciseness (c) Clarity Figure 7: Validation performance of individual objectives during training. Each objective reaches saturation at different training stages. For example, under the conciseness-only weight configuration, the model achieves optimal response brevity at approximately step 165 (b), whereas the clarity-only configuration reaches the best clarity after step 240 (c). A.3 LIMITATIONS While our proposed dynamic reward weighting methods have shown effectiveness on Qwen and DeepSeek models, key factor is that these models exhibit improvement spaces across all three objectives, enabling our approaches to further optimize training for them. In contrast, for models that inherently struggle to improve all objectives simultaneously, such as Ministral-8B-Instruct (Mistral AI Team, 2024) and Llama-3.1-8B-Instruct (Grattafiori et al., 2024) that we have tested under the same multi-objective settings, where accuracy consistently declines as responses become shorter, our method is less effective. We think this limitation is unlikely to be addressed via post-training tricks, as it reflects the requirement for models to possess inherent capabilities in each objective, which should ideally be established during pre-training. These observations motivate future works on (1) pre-training strategies that equip models with the capacity to learn and balance diverse objectives, and (2) methods to wisely decide which objectives to prioritize when inherent conflicts exist. A.4 HYPERVOLUME-GUIDED REWARD ADAPTATION DETAILS We compute three types of rewards: (1) accuracy reward through exact matching against the provided ground truth, (2) conciseness reward by comparing current response length to the global average response length from previous rollouts, and (3) clarity reward by checking whether responses contain explicit reasoning steps (e.g., first, second, third) according to our pre-defined rules. When training the REINFORCE algorithm with constant learning rate, we observed significant variation in convergence rates across runs using identical hyperparameters, particularly for the conciseness and clarity objectives. To ensure robust and fair comparisons, we repeated runs three times for all baselines and our proposed approach, reporting results from the run that achieved the fastest convergence in each case. This phenomenon was not observed in other algorithms or training settings, and we leave its investigation to future works. We also observed that zero reward would result in increased response length and degraded performance in both accuracy and formatting, which is likely due to the entropy penalty dominating the gradient updates. To mitigate this issue, we build minimum threshold of rpareto = 0.5 in Eq. 1 even when the current checkpoint is Pareto dominated (i.e., HV = 0). A.5 HYPERPARAMETERS We use verl (Sheng et al., 2025) for RL training. We obtain datasets and models from HuggingFace: Math500 (Lightman et al., 2023): https://huggingface.co/datasets/HuggingFaceH4/MATHMATH (Hendrycks et al., 2021): https://huggingface.co/datasets/DigitalLearningGmbH/MATHlighteval Qwen3: (Yang et al., 2025): https://huggingface.co/Qwen/Qwen3-8B Deepseek (DeepSeek-AI et al., 2024): https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat 19 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting (cid:111)"
        },
        {
            "title": "Name",
            "content": "rpareto (cid:110) 1 + Search Bounds 1 1 + exp(HV) , 2 1 + exp(HV) , 0.5 + 1.5tanh(HV), 0.5 + 1.5 tanh(3HV) Learning rate LR scheduler Batch size Mini-batch size Epoch Max response length Rollout size KL coefficient Clip range ϵ Dual clip constant Avg. training time GPU used 1e6 constant 64 32 90 2048 8 0.001 0.2 3 14 hrs 8 Nvidia H200 (143 GB) Table 4: Hyperparameters and other reproducibility information for hypervolume-based weight adaptation. Name Learning rate η LR scheduler Poly scheduler power Regularization factor µ Clip range ϵ Dual clip constant Search Bounds {1e4, 1e5, 1e6} {constant, cosine-annealing, exponential decay, polynomial} {1.01, 1.03} {1e4, 1e5, 1e6} {0.2, 100} {3, 100} Batch size Mini-batch size Epoch Max response length Rollout size Max gradient norm KL coefficient Target layer IDs Avg. training time GPU used 64 32 90 2048 8 1.0 0.001 [10, 11, 12, , 25] 24 hrs 8 Nvidia H200 (143 GB) Table 5: Hyperparameters and other reproducibility information for gradient-based weight optimization. We provide the hyperparameters used for GRPO training on Math500 in Table 4 and Table 5. 20 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting A.6 PARETO FRONTS VISUALIZATION Accuracy-focused Balanced Figure 8: Pareto fronts obtained from our hypervolume-guided weight adaptation against baselines under GRPO training. Efficiency-focused 21 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting Accuracy-focused Balanced Figure 9: Pareto fronts obtained from our hypervolume-guided weight adaptation against baselines under REINFORCE training. Efficiency-focused 22 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting Accuracy-focused Balanced Figure 10: Pareto fronts obtained from our hypervolume-guided weight adaptation against baselines under RLOO training. Efficiency-focused 23 Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting GRPO REINFORCE Figure 11: Pareto fronts obtained from our gradient-based weight optimization against baselines under different RL training algorithms. RLOO"
        }
    ],
    "affiliations": [
        "Amazon",
        "University of Notre Dame"
    ]
}