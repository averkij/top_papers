{
    "paper_title": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models",
    "authors": [
        "Haitao Hong",
        "Yuchen Yan",
        "Xingyu Wu",
        "Guiyang Hou",
        "Wenqi Zhang",
        "Weiming Lu",
        "Yongliang Shen",
        "Jun Xiao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated remarkable performance in reasoning tasks, where reinforcement learning (RL) serves as a key algorithm for enhancing their reasoning capabilities. Currently, there are two mainstream reward paradigms: model-based rewards and rule-based rewards. However, both approaches suffer from limitations: rule-based rewards lack robustness, while model-based rewards are vulnerable to reward hacking. To address these issues, we propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework that jointly optimizes both the policy model and the reward model. Cooper leverages the high precision of rule-based rewards when identifying correct responses, and dynamically constructs and selects positive-negative sample pairs for continued training the reward model. This design enhances robustness and mitigates the risk of reward hacking. To further support Cooper, we introduce a hybrid annotation strategy that efficiently and accurately generates training data for the reward model. We also propose a reference-based reward modeling paradigm, where the reward model takes a reference answer as input. Based on this design, we train a reward model named VerifyRM, which achieves higher accuracy on VerifyBench compared to other models of the same size. We conduct reinforcement learning using both VerifyRM and Cooper. Our experiments show that Cooper not only alleviates reward hacking but also improves end-to-end RL performance, for instance, achieving a 0.54% gain in average accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that dynamically updating reward model is an effective way to combat reward hacking, providing a reference for better integrating reward models into RL."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 3 1 6 5 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "COOPER: CO-OPTIMIZING POLICY AND REWARD MODELS IN REINFORCEMENT LEARNING FOR LARGE LANGUAGE MODELS Haitao Hong1,, Yuchen Yan1,, Xingyu Wu1, Guiyang Hou1, Wenqi Zhang1 Jun Xiao1 Weiming Lu1 Yongliang Shen1, 1Zhejiang University {haitaohong, yanyuchen, syl}@zju.edu.cn GitHub: https://github.com/zju-real/cooper (cid:128) Project: https://zju-real.github.io/cooper"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) have demonstrated remarkable performance in reasoning tasks, where reinforcement learning (RL) serves as key algorithm for enhancing their reasoning capabilities. Currently, there are two mainstream reward paradigms: model-based rewards and rule-based rewards. However, rule-based rewards lack robustness, both approaches suffer from limitations: while model-based rewards are vulnerable to reward hacking. To address these issues, we propose Cooper (Co-optimizing Policy Model and Reward Model), RL framework that jointly optimizes both the policy model and the reward model. Cooper leverages the high precision of rule-based rewards when identifying correct responses, and dynamically constructs and selects positivenegative sample pairs for continued training the reward model. This design enhances robustness and mitigates the risk of reward hacking. To further support Cooper, we introduce hybrid annotation strategy that efficiently and accurately generates training data for the reward model. We also propose reference-based reward modeling paradigm, where the reward model takes reference answer as input. Based on this design, we train reward model named VerifyRM, which achieves higher accuracy on VerifyBench compared to other models of the same size. We conduct reinforcement learning using both VerifyRM and Cooper. Our experiments show that Cooper not only alleviates reward hacking but also improves end-to-end RL performance, for instance, achieving 0.54% gain in average accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that dynamically updating reward model is an effective way to combat reward hacking, providing reference for better integrating reward models into RL."
        },
        {
            "title": "INTRODUCTION",
            "content": "He who teaches, who learns. Large language models (LLMs) have demonstrated remarkable capabilities in reasoning (Kumar et al., 2025; OpenAI et al., 2024), particularly in mathematical reasoning (Lewkowycz et al., 2022; Shao et al., 2024; Ying et al., 2024; Yang et al., 2024b), code reasoning (Roziere et al., 2023; Zhu et al., 2024; Hui et al., 2024), and commonsense reasoning (Wang et al., 2024), often achieving or even surpassing human-level performance. Recent advances demonstrate that reinforcement learning (RL) has become pivotal technique for enhancing these reasoning capabilities (Xu et al., Confucius * The first two authors have equal contributions. Corresponding author."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Model-based rewards are generally robust to variations in model outputs, but they are susceptible to being hacked by the policy model. In contrast, rule-based rewards are less prone to hacking but often lack robustness. We introduce Cooper, reinforcement learning framework that achieves both high robustness and resistance to reward hacking. In this figure, the black arrows indicate the rollout process, the blue arrows represent the reward assignment process, and the brown arrows denote the update process for the reward model. 2025). By generating multiple solution trajectories and optimizing the model to align with highquality responses, RL enables LLMs to achieve performance that often matches or exceeds human expertise (Havrilla et al., 2024). In RL algorithms, one critical factor affecting performance is the design of the reward function, as it determines the quality of evaluation for output sequences. In the early stages of RL for LLMs, human preference data was typically used to train reward model that assigns rewards based on the input question and model generations (Ouyang et al., 2022; Bai et al., 2022). This paradigm, known as reinforcement learning from human feedback (RLHF), has been widely adopted (Havrilla et al., 2024; Chen et al., 2025b). Since the introduction of reasoning models like OpenAIs o1 (Jaech et al., 2024), DeepSeek-R1 (Guo et al., 2025a) and Kimi-k1.5 (Team et al., 2025), RL for LLMs has shifted focus towards verifiable tasks, where rule-based reward functions are commonly employed to assign scores, thereby improving the reliability of the scoring system. However, both model-based and rule-based reward functions have inherent limitations. Model-based rewards, which rely on dynamic calculations based on the models parameters, are prone to reward hacking (Gao et al., 2023). Specifically, when fixed reward model is used, the model may exploit output patterns that deceive the reward function, thereby obtaining high scores regardless of the correctness of the output. This phenomenon can lead to catastrophic failures in the later stages of training. On the other hand, rule-based reward functions often rely on manually crafted rules to parse and verify the models output (Gandenberger & Kydlıˇcek, 2024; Gao et al., 2024). This method lacks robustness and is susceptible to misjudgment, which constrains the further optimization of the model (Christiano et al., 2017). In our preliminary experiments, we observed that rule-based reward functions exhibit high precision in identifying correct samples and high recall in detecting incorrect ones. That is, samples judged as correct by rule-based reward functions are usually indeed correct, whereas those judged as incorrect may still be correct in reality. This phenomenon arises because rule-based functions are typically handcrafted and lack robustness in answer extraction. As result, they often fail to accurately extract and match answers, especially when facing diverse output formats generated by different models. In this paper, we propose Cooper, novel reinforcement learning framework that enables synchronized co-optimization of the policy model and the reward model. Cooper introduces twostage training pipeline: (1) Policy model optimization: Policy model optimization follows the"
        },
        {
            "title": "Preprint",
            "content": "GRPO paradigm, involving sampling and scoring responses with reference-aware reward model, and performing policy updates based on within-group normalized advantages and KL regularization; (2) Reward model optimization: Reward model optimization continuously refines the reward model via contrastive learning, using positive samples identified by high-precision rule-based signals and negative samples generated by transforming correct responses into incorrect ones with an assistant LLM. To support cooper framework, we first focus on training an accurate and robust reference-based reward model. We construct large-scale dataset using responses generated from diverse LLMs across multiple high-quality math reasoning datasets. hybrid annotation strategy is applied using both rule-based verifier tools (e.g., Math-Verify) and LLM-based verifiers, allowing for automatic correctness labeling at scale. Using this labeled data, we train reward model VerifyRM that scores responses, with the query and reference answer as its input. We then integrate this reward model into the Cooper pipeline and validate its effectiveness through comprehensive experiments. Our results demonstrate that applying this co-optimization framework significantly improves the policy models reasoning ability. Our experiments show that models trained with Cooper outperform both rule-based and fixed-reward-model RL baselines across several challenging math reasoning benchmarks. The main contributions are summarized as follows: We introduce reward modeling dataset, which is labeled using hybrid annotation strategy that combines rule-based verification and LLM-as-a-judge verification, enabling efficient and reliable correctness supervision. The reward model trained on our constructed dataset achieves an accuracy of 89.42% on VerifyBench, surpassing existing reward models of the same scale. Based on the high precision of rule-based rewards in identifying correct answers, we propose Cooper, reinforcement learning framework that co-optimizes the policy model and the reward model simultaneously. This framework mitigates the problem of reward hacking commonly observed in reward-model-based RL and enhances overall training performance. Our work demonstrates that dynamically adjusting the parameters of the reward model during the RL training process can effectively mitigate the phenomenon of reward hacking, providing valuable insights for the research community on how to better utilize reward models in reinforcement learning."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Reinforcement Learning for Large Language Models. Reinforcement Learning (RL) has emerged as foundational method for aligning large language models (Christiano et al., 2017; Ziegler et al., 2020; Kaufmann et al., 2023). Early work such as InstructGPT (Ouyang et al., 2022) demonstrated that fine-tuning LLMs with reward model trained on human preference data can significantly improve response helpfulness and alignment. However, RLHF is often computationally expensive, costly, and reliant on large-scale human annotations. To mitigate these issues, recent methods have proposed simplifying the RLHF process. Direct Preference Optimization (DPO) (Rafailov et al., 2023) reformulates the RLHF objective as contrastive loss over preference pairs, eliminating the need for reward model training and sampling-based updates. Reinforcement Learning with AI Feedback (RLAIF) (Bai et al., 2022) proposes replacing human preference data with AI-generated feedback guided by predefined principles instead of humans, significantly lowering the annotation cost and improving scalability. Reward Models for Reinforcement Learning. Reward Models (RMs) has been widely used in reasoning tasks (Zhong et al., 2025) for reinforcement learning (Ouyang et al., 2022; Lambert et al., 2024b; Guo et al., 2025b) and verification-guided inference (Guo et al., 2025b; Li et al., 2023; Zhang et al., 2025; Yu et al., 2024). According to reward modeling mechanisms, Existing RMs broadly fall into three types: (1) discriminative reward models (Cai et al., 2024; Zang et al., 2025), typically implemented as classifiers over response sequences, assigning binary or fine-grained preference scores; (2) generative reward models (Liu et al., 2025; Alexandru et al., 2025; Hong"
        },
        {
            "title": "Preprint",
            "content": "et al., 2025), which generate textual feedback or critiques before producing scalar reward; and (3) implicit reward models (Lambert et al., 2024a), often optimized via DPO (Rafailov et al., 2023), where model likelihoods are interpreted as reward signals. Orthogonally, reward models can be categorized into outcome reward models (ORMs) (Liu et al., 2024a; Cobbe et al., 2021), which assign scalar feedback to final outputs, and process reward models (PRMs) (Setlur et al., 2025), which evaluate intermediate reasoning steps to provide denser and more interpretable supervision. Our VerifyLM belongs to the discriminative RM and ORM. Reinforcement Learning with Verifiable Rewards. As an alternative to learned reward models, Reinforcement Learning with Verifiable Rewards (RLVR) (Guo et al., 2025a; Lambert et al., 2024a; Yue et al., 2025) leverages rule-based verification functionssuch as exact answer matching or logical consistency checksto generate reward signals automatically. Notably, DeepSeek-R1 (Guo et al., 2025a) achieves strong reasoning performance through multi-stage pipeline combining supervised pretraining with Group Relative Policy Optimization (GRPO) (Shao et al., 2024). Cooper draws inspiration from the recent advancements in enhancing reasoning through RLVR, screening out the correct responses of the policy model via symbolic verification, and thereby constructing preference data to update the reward model."
        },
        {
            "title": "3 METHODS",
            "content": "Our methods consist of two main components. The first part proposes pipeline for constructing reference-based reward model VerifyRM, which includes data collection and annotation strategies, as well as the training procedure for the reward model. The second part presents Cooper, reinforcement learning algorithm that co-optimizes both the policy model and the reward model. In this framework, the RM trained in the first stage guides the policy models updates within Cooper while being updated itself concurrently. 3.1 TRAINING RECIPE OF VERIFYRM Most existing reward models score the input-output pairs of large language models (LLMs) directly (Zhong et al., 2025). However, in reasoning tasks, there typically exists reference answer. Yan et al. (2025) have demonstrated the importance of reference answers for model-based verification. Therefore, we propose method for constructing reference-based reward models to improve the accuracy of reward models in reasoning tasks. 3.1.1 DATA PREPARATION To train VerifyLM, the required data format consists of reasoning problem, its corresponding reference answer, model-generated completion, and label indicating whether the completion is correct. Problem-reference-completion triples collection. We collected 7 commonly used mathematical reasoning datasets, each containing math problems and their corresponding reference answers. Using 11 mainstream LLMs, we generated completions for these math problems, with each model providing one completion per problem. During sampling, we set the temperature to 0.7 and top to 0.95. In total, we collected 65K problem-reference-completion triples. Details of the datasets, LLMs, and their licenses are provided in the Appendix and B. Hybrid labeling for correctness. In prior works, researchers have relied heavily on manual annotation to determine the correctness of model completions (Chen et al., 2025a). We observe that current LLMs have already demonstrated strong capabilities in evaluating the correctness of completions against reference answers (e.g., Qwen3-4B achieves 94.17% accuracy on VerifyBench) (Yang et al., 2025). Motivated by this, we propose an automated hybrid labeling approach that combines rule-based verifier and an LLM-as-a-judge. Specifically, we use Math-verify (Gandenberger & Kydlıˇcek, 2024) as the rule-based verifier and Qwen3-4B (in non-thinking mode) (Yang et al., 2025) as the LLM-as-a-judge. We only retain samples for which both methods agree on the correctness label, resulting in dataset of 58.7K examples for training VerifyRM. Detailed statistics are provided in the Appendix D."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: An overview of the Cooper training framework. Each training step in Cooper consists of two stages: policy model optimization (blue area) and reward model optimization (green area). 3.1.2 REWARD MODEL TRAINING In this paper, following the approach of Cai et al. (2024), the reward model is formulated as text classifier. However, we incorporate the reference answer into the input of the reward model. The specific input template is provided in the appendix C.3. Inspired by Zang et al. (2025), we initialize our model from an already aligned LLM, replacing its original language modeling head with newly initialized score head. The model is then trained using binary cross-entropy loss. The objective function can be formally written as: L(θ) = E[{q,r,c,y}D]BCE(σ(Mθ(q, r, c)), y) (1) BCE(ˆy, y) = log ˆy (1 y) log(1 ˆy) (2) where denotes the question, denotes the reference answer, denotes the models completion, indicates the correctness label, and represents the training dataset. The sigmoid function σ is used to map the logits output by the model Mθ into the range [0, 1], which is then used to compute the binary cross-entropy loss. 3.2 REINFORCEMENT LEARNING WITH COOPER We propose Cooper, reinforcement learning framework that co-optimizes policy and reward models. Cooper enables simultaneous tuning of the policy model and reward model in single training step. We present Cooper in Figure 2 and Algorithm 1. Stage 1: Policy model optimization. Following the GRPO (Shao et al., 2024) paradigm, our policy model optimization proceeds as follows. For each training sample q, we sample set of responses {o1, o2, . . . , on} using the policyπθ. The reward model then evaluates each rollout, producing scores {r1, r2, . . . , rn}. We normalize these rewards across the group to compute advantage estimates {A1, A2, . . . , An}, which are subsequently used to update the policy via policy gradient. To regularize exploration and ensure training stability, KL divergence penalty is incorporated during reinforcement learning. However, unlike previous RL methods based on reward models, we incorporate reference answer, denoted as a, into the scoring process of the reward model. Consequently, the reward can be computed as: ri = Rφ(q, a, oi) The computation of the remaining variables and the optimization of the policy model follow the same methodology as proposed in Shao et al. (2024). (3)"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 Co-Optimizing Policy and Reward Models (Cooper) Input: initial policy model πθinit , reward models Rφinit , training data D; hyperparameters ε, β Output: π policy model πθ πθinit reward model Rφ Rφinit for iteration = 1 to do θ and φ reference model πref πθ Sample batch Db for each question and answer (q, a) Db do Generate rollouts: {o1, . . . , oG} using πθ Compute rewards by running rφ on each output oi: rj = σ(Rφ(q, oj, a)) Compute ˆAi,t for the t-th token of oi through group relative advantage estimation: ˆAi,t = ri mean ({r1, r2, , rG}) std ({r1, r2, , rG}) . Select positive response opos {o1, . . . , oG} using rule-based reward function: opos = {oi Rule(a, oi) = 1} Generate negative response oneg from an assistant LLM: oneg = (p, opos) end for Update the policy model πθ by maximizing the GRPO objective: Stage 1: Policy model optimization JGRPO(θ) = (cid:104) (Q), {oi}G i=1 πθold (O q) (cid:105) 1 G (cid:88) i=1 1 oi oi (cid:88) t=1 (cid:34) (cid:26) min πθ(oi,t q, oi,<t) πθold (oi,t q, oi,<t) ˆAi,t, clip (cid:18) πθ(oi,t q, oi,<t) πθold (oi,t q, oi,<t) (cid:19) , ε (cid:35) ˆAi,t β DKL [πθ πref] (cid:27) Update the reward model Rφ by minimizing the loss: Stage 2: Reward model optimization LRM = E[{q,a,opos,oneg}Db]logσ(Rφ(q, a, opos) Rφ(q, a, oneg)) end for return π θ and φ Stage 2: Reward model optimization. Cooper introduces new step into the existing GRPO pipeline: the optimization of the reward model. This is designed to ensure that the RMs parameters are continuously updated during the RL process, thereby reducing the risk of the policy model exploiting specific vulnerabilities in the RM (i.e., reward hacking) and ultimately maintaining the stability of training. Specifically, following the approach of Stiennon et al. (2020), we optimize the reward model using contrastive learning. Given question q, reference answer a, and pair of candidate responses opos and oneg to q, where opos is correct response and oneg is incorrect, the objective is to maximize the score difference assigned by the RM between opos and oneg. The optimization could be represented as: LRM = E[{q,a,opos,oneg}D]logσ(Rφ(q, a, opos) Rφ(q, a, oneg)) (4) To obtain set of positive and negative samples opos, oneg for given question and its reference answer a, we perform the following operations respectively: Positive sample selection. Based on our preliminary observations of rule-based rewards, we found that such rules tend to exhibit high precision but low recall in identifying correct responses. In other words, responses classified as correct by the rule are highly likely to be truly correct. Therefore, for single rollout that yields set of responses {o1, o2, ..., on}, we randomly select one response that is judged as correct by the rule and treat it as positive sample: opos = {oi Rule(a, oi) = 1} (5)"
        },
        {
            "title": "Preprint",
            "content": "Negative sample generation. We propose simple method for generating negative samples. Specifically, we utilize an assistant LLM to transform correct reasoning process into one that ultimately yields an incorrect answer, guided by carefully designed prompt (shown in Appendix C.2). To ensure the generated response is indeed incorrect, we incorporate verification mechanism. Leveraging the high precision of rule-based reward system, we pass the generated reasoning process through the rule-reward to verify its correctness. If it is not identified as incorrect by the rule-reward, the process is repeated until valid negative sample is obtained: We would like to mention that if no valid pair is constructed during our data construction process, we add loss mask to skip the optimization for that sample. oneg = (p, opos) (6)"
        },
        {
            "title": "4.1 PRELIMINARY EXPERIMENT",
            "content": "To validate our hypothesis that rule-based verifiers exhibit high precision despite low recall, we analyzed the verification patterns of Math-Verify (Gandenberger & Kydlıˇcek, 2024) and Qwen34B (Yang et al., 2025) on VerifyBench (Yan et al., 2025). Table 1 reveals clear asymmetry: MathVerify achieves 96% precision (345/360) when identifying correct responses but only 63% recall (345/549), while Qwen3-4B shows balanced performance with 90% precision and 99% recall. This reflects Math-Verifys conservative parsing, which only accepts responses with clearly extractable answers in expected formats, rejecting many correct solutions with non-standard presentations. Math-Verify VerifyBench (Math Reasoning) Pred = 1 Pred = 0 Pred = 1 Pred = 0 This finding directly motivates Coopers design. The near-perfect precision of rulebased verification when it succeeds provides highly reliable positive signals for training. By using Math-Verify to select positive examples for reward model updates, we leverage its precision while avoiding its recall limitations. Meanwhile, the reward model handles the broader distribution of responses during policy optimization. This complementary approach, combining rule-based precision for reward updates with model-based flexibility for policy scoring, forms the foundation of our cooptimization framework. 345 15 Table 1: Confusion matrices for rule-based (MathVerify) and model-based (Qwen3-4B) verifiers on VerifyBench. Label = 1 Label = 0 Qwen3-4B 6 491 204 534 543 58 4.2 EXPERIMENTS FOR VERIFYRM We trained VerifyRM following the methodology in Section 3.1, using Qwen2.5-Math1.5B-Instruct (Yang et al., 2024b) as the base model. Training was conducted for 3 epochs with learning rate of 2e-5 and batch size of 128. To ensure fair evaluation on VerifyBench, we excluded all overlapping queries from our training data. Table 2 compares VerifyRM against three catrule-based functions, egories of baselines: vanilla reward models, and reference-based verifiers. The results demonstrate clear performance stratification. Vanilla reward models without reference answers perform poorly (47-52% accuracy), confirming that standard preference-based rewards lack the precision needed for mathematical verification. Rulebased Math-Verify achieves 79.93%, validating its utility but also highlighting its brittleness. Method Rule-based function Math-Verify Vanilla reward model w/o reference FsfairX-LLaMA3-RM-v0.1 Skywork-Reward-V2-Llama-3.2-1B Skywork-Reward-V2-Llama-3.2-3B Skywork-Reward-V2-Llama-3.1-8B Llama-3.1-Tulu-3-8B-RM Reference-based verifier xVerify-0.5B-I xVerify-3B-Ia xVerify-8B-I xVerify-9B-C VerifyRM-1.5B (ours) VerifyBench-Math 79.93 49.53 47.23 52.63 52.06 51. 70.68 82.23 84.38 84.23 89.42 Table 2: Reward model accuracy on VerifyBench."
        },
        {
            "title": "Preprint",
            "content": "Base Model Reward GSM8K SVAMP MATH500 OB-EN Odyssey Average Qwen2.5-1.5B-Instruct Llama-3.2-1B-Instruct / Rule Model Cooper (ours) / Rule Model Cooper (ours) 74.10 76.44 30.78 77.02 50.39 56.56 36.32 57.14 84.60 87.26 72.04 87.65 71.33 72.24 59.35 73.45 54.63 57.55 29.70 58. 29.58 34.20 20.70 34.88 20.17 23.33 1.43 23.22 6.41 7.95 0.22 8.02 39.33 42.83 11.89 44.17 34.77 40.02 7.39 39.98 54.93 57.48 38.91 58. 38.50 42.19 24.80 42.69 Table 3: RL performance with different reward types across mathematical benchmarks. Among model-based verifiers, performance scales with model size, yet our VerifyRM-1.5B achieves the highest accuracy at 89.42%, outperforming even the 9B parameter xVerify model. This superior performance with fewer parameters validates two key design choices: incorporating reference answers provides crucial context for verification, and our hybrid annotation strategy creates higherquality training data than existing approaches. The strong performance of VerifyRM establishes the reliable reward signal necessary for Coopers co-optimization framework. 4.3 EXPERIMENTS FOR COOPER Setup. We implemented the Cooper algorithm based on the veRL (Sheng et al., 2024) framework. The experiments were conducted on the DeepMath (He et al., 2025) dataset. Due to resource constraints, we randomly sampled 10K examples from the original dataset for training. All experiments used Qwen2.5-1.5B-Instruct (Qwen et al., 2025) and Llama-3.2-1B-Instruct (Grattafiori et al., 2024) as the initial model. To avoid introducing additional knowledge, the assistant model in Cooper was also instantiated with the same model. In the GRPO algorithm, we set the global batch size to 512, the maximum prompt length to 1024, and the maximum response length to 3072. The learning rate was set to 1e-6, and the KL penalty coefficient was set to 0.001. For each prompt, we generated 16 rollouts during RL training. The models are trained with 10 epochs. Evaluation. We evaluate the model on five mathematical reasoning benchmarks: GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), MATH500 (Lightman et al., 2023), OlympiadBenchEN (OB-EN) (He et al., 2024), and Math Odyssey (Fang et al., 2024). Among them, GSM8K, MATH500, and SVAMP represent elementary-level mathematical problems, while OB-EN and Math Odyssey are competition-level tasks. During RL training, we periodically assess model performance. For all evaluations, we use temperature of 0.7 and top-p of 0.95, generating 8 samples per problem and computing the average accuracy to mitigate evaluation variance. Baselines. Since Cooper integrates the advantages of both rule-based reward functions and reward models, our baselines include: (1) model using Math-Verify as the reward function, and (2) model using VerifyRM-1.5B as the reward model without updating its parameters during training. 4.3.1 MAIN RESULTS Cooper achieves superior performance across diverse benchmarks. Table 3 demonstrates Coopers effectiveness: on Qwen2.5-1.5B-Instruct, Cooper achieves 58.02% average accuracy, outperforming rule-based rewards (57.48%) while dramatically surpassing the collapsed static reward model (38.91%). The improvements are consistent across both base models and particularly pronounced on challenging tasks like Math Odyssey (44.17% vs 42.83%), suggesting cooptimization becomes increasingly valuable for complex reasoning. Static reward models suffer catastrophic failure from reward hacking. The most striking finding is the severe degradation of static reward models: performance drops from 54.93% to 38.91% on Qwen2.5-1.5B-Instruct, 16% relative decrease. This collapse, consistent across both architectures, empirically validates that reward hacking is fundamental failure mode in RL for LLMs. Cooper not only prevents this catastrophic failure but achieves the highest performance,"
        },
        {
            "title": "Preprint",
            "content": "(a) Test set accuracy (%) across MATH500. (b) Train set reward during RL training. Figure 3: Training dynamics across RL training steps of Cooper. confirming that synchronized co-optimization successfully addresses the exploitation vulnerability inherent in fixed reward functions."
        },
        {
            "title": "5 ANALYSIS",
            "content": "To understand the mechanisms underlying Coopers effectiveness, we conduct comprehensive analysis examining three key aspects: the training dynamics that reveal how Cooper prevents reward hacking, the stability of the co-optimized reward model, and the impact of reward signal granularity on performance. 5.1 TRAINING DYNAMIC To understand how Cooper prevents reward hacking, we examine the training dynamics in Figures 3a and 3b. The test accuracy on MATH500 (Figure 3a) reveals critical divergence: while rulebased rewards and Cooper show steady improvement, the static reward model catastrophically collapses around step 120, dropping from 58% to below 52%. This collapse coincides with reward hacking visible in Figuree 3b, where the static models training rewards unnaturally spike to near 1.0, indicating the policy has discovered exploits in the fixed reward function. In contrast, Cooper maintains realistic reward levels around 0.5 throughout training while achieving the highest final accuracy (58.05%). This demonstrates that synchronized updates successfully prevent the policy from gaming the reward signal, as the policy evolves, the reward model adapts its decision boundaries, closing exploitation opportunities that would otherwise accumulate in static system. 5.2 STABILITY OF REWARD MODEL THROUGHOUT TRAINING potential concern with Cooper is whether continuous updates might destabilize the reward model. Figure 4 tracks VerifyRMs accuracy training, showing on VerifyBench throughout remarkable stability around 89.7% with fluctuations below 0.5%. This stability emerges from our careful update mechanism: by using highprecision rule-based signals for positive examples and systematic perturbations for negatives, each update reinforces correct decision boundaries rather than introducing noise. The consistent performance confirms that co-optimization can be implemented without the instability typically Figure 4: Accuracy of RM across training steps."
        },
        {
            "title": "Preprint",
            "content": "associated with moving target problems, validating that our contrastive learning approach maintains verification quality while adapting to new policy distributions."
        },
        {
            "title": "5.3 ABLATION ON CONTINUOUS VERSUS DISCRETE REWARDS",
            "content": "Reward GSM8K MATH500 Average To ensure Coopers improvements stem from the co-optimization framework rather than differences in reward signal granularity, we conduct an ablation study comparing continuous and discrete reward implementations. As shown in Table 4, when we binarize Coopers reward outputs to match the rule-based baselines format (1 for scores >0.5, 0 otherwise), Cooper still achieves 57.86% average accuracy, maintaining most of its advantage over both the baseline (54.93%) and rule-based rewards (57.48%). This result provides two key insights: first, the primary advantage of Cooper stems from preventing reward hacking through dynamic updates rather than from reward granularity; second, continuous rewards do provide additional benefits by enabling more nuanced credit assignment during policy optimization. These findings confirm that Coopers co-optimization framework addresses fundamental limitation in current RL approaches for LLMs, remaining effective across different reward signal designs. / Rule Cooper Cooper (discrete) Table 4: Ablation on continuous vs. rewards. 54.93 57.48 58.02 57.86 discrete 54.63 57.55 58.05 57. 74.10 76.44 77.02 76."
        },
        {
            "title": "6 DISCUSSION",
            "content": "Implications for reinforcement learning in LLMs. Cooper reveals that reward hacking is not hyperparameter issue but fundamental problem with static reward models. The 16% performance collapse with fixed rewards demonstrates that treating reward models as dynamic components is essential for stable RL. This principle extends beyond mathematical reasoning, any domain with partial verification capabilities could benefit from synchronized optimization. By shifting from an adversarial dynamic to co-evolutionary framework, Cooper suggests that much of RLs perceived instability may stem from reward exploitation rather than optimization challenges. The critical role of high-precision signals. Coopers success relies on an underappreciated property of rule-based verifiers: their asymmetric performance with high precision (96%) but low recall (63%). This pattern, common in structured domains, becomes strength when used to select positive training examples. By combining symbolic precision with neural flexibility, Cooper demonstrates that hybrid approaches may be essential for reliable AI systems. The key insight is transforming verification limitations into training advantages through careful system design. Limitations and future directions. Three main limitations constrain Coopers current implementation: (1) dependency on domain-specific verification tools limits generalization to tasks without clear correctness criteria; (2) computational overhead from dual optimization may affect scalability; (3) reliance on an assistant LLM for negative sample generation introduces external dependencies. Future work should explore self-supervised contrastive example generation, extend Cooper to process-based rewards for denser supervision, and develop theoretical frameworks for coevolutionary stability. Despite these limitations, Cooper establishes synchronized optimization as promising direction for addressing fundamental challenges in reinforcement learning for LLMs."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this paper, we introduce Cooper, reinforcement learning (RL) framework that co-trains the policy model and the reward model. Cooper combines the high precision of rule-based rewards with the robustness of model-based rewards, effectively mitigating the issue of reward hacking that often arises when using static reward model in RL. Compared to using either type of reward in isolation, Cooper achieves significantly better performance. In addition, we propose reference-answer-based reward model named VerifyRM. By leveraging hybrid annotation method that does not rely on manual labeling, VerifyRM outperforms existing models of the same scale on the VerifyBench"
        },
        {
            "title": "Preprint",
            "content": "benchmark. Our results demonstrate that dynamically updating the reward model during RL training is effective in countering reward hacking. Nevertheless, our work has room for improvement. One important direction is exploring how to update the reward model without depending on external LLMs. In future work, we aim to further pursue this line of research to develop more accurate and robust RL training paradigms."
        },
        {
            "title": "REFERENCES",
            "content": "Andrei Alexandru, Antonia Calvi, Henry Broomfield, Jackson Golden, Kyle Dai, et al. Atla selene mini: general purpose evaluation model. CoRR, abs/2501.17195, 2025. doi: 10.48550/ARXIV. 2501.17195. URL https://doi.org/10.48550/arXiv.2501.17195. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, et al. Training helpful arXiv preprint and harmless assistant with reinforcement learning from human feedback. arXiv:2204.05862, 2022. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, et al. Internlm2 technical report. CoRR, abs/2403.17297, 2024. doi: 10.48550/ARXIV.2403.17297. URL https://doi.org/ 10.48550/arXiv.2403.17297. Ding Chen, Qingchen Yu, Pengyuan Wang, Wentao Zhang, Bo Tang, et al. xverify: Efficient answer verifier for reasoning model evaluations. arXiv preprint arXiv:2504.10481, 2025a. Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, et al. An empirical study on eliciting and improving r1-like reasoning models. CoRR, abs/2503.04548, March 2025b. URL https://doi.org/10.48550/arXiv.2503.04548. Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 42994307, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ d5e2c0adad503c91f91df240d0cd4e49-Abstract.html. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, and Kai Zou. Mathodyssey: Benchmarking mathematical problem-solving skills in large language models using odyssey math data. arXiv preprint arXiv:2406.18321, 2024. Greg Gandenberger and Hynek Kydlıˇcek. Math-verify: robust mathematical expression evaluation system designed for assessing large language model outputs in mathematical tasks. https: //github.com/huggingface/Math-Verify, 2024. GitHub repository. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pp. 1083510866. PMLR, 2023. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, et al. The language model evaluation harness, 07 2024. URL https://zenodo.org/records/12608602. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, et al. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, et al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, et al. DeepseekarXiv preprint Incentivizing reasoning capability in llms via reinforcement learning. r1: arXiv:2501.12948, 2025a."
        },
        {
            "title": "Preprint",
            "content": "Jiaxin Guo, Zewen Chi, Li Dong, Qingxiu Dong, Xun Wu, et al. Reward reasoning model, 2025b. URL https://arxiv.org/abs/2505.14674. Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, et al. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning, 2025. URL https://arxiv.org/abs/ 2504.11456. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. Ilgee Hong, Changlong Yu, Liang Qiu, Weixiang Yan, Zhenghao Xu, et al. Think-rm: Enabling long-horizon reasoning in generative reward models, 2025. URL https://arxiv.org/ abs/2505.16265. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hullermeier. survey of reinforcement learning from human feedback. CoRR, abs/2312.14925, 2023. doi: 10.48550/ARXIV.2312. 14925. URL https://doi.org/10.48550/arXiv.2312.14925. Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, et al. Llm post-training: deep dive into reasoning large language models. arXiv preprint arXiv:2502.21321, 2025. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, et al. Tulu 3: Pushing frontiers in open language model post-training. CoRR, abs/2411.15124, 2024a. doi: 10. 48550/ARXIV.2411.15124. URL https://doi.org/10.48550/arXiv.2411.15124. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, et al. Rewardbench: Evaluating reward models for language modeling, 2024b. URL https:// arxiv.org/abs/2403.13787. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857, 2022. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, et al. Making language models better reasoners with step-aware verifier. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 53155333. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.291. URL https://doi.org/10.18653/v1/2023.acl-long.291. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, et al. Lets verify step by step, 2023. Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, et al. Skywork-reward: Bag of tricks for reward modeling in llms. CoRR, abs/2410.18451, 2024a. doi: 10.48550/ARXIV.2410.18451. URL https://doi.org/10.48550/arXiv.2410.18451."
        },
        {
            "title": "Preprint",
            "content": "Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu, et al. Are your llms capable of stable reasoning? arXiv preprint arXiv:2412.13147, 2024b. Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, et al. Inference-time scaling for generalist reward modeling. CoRR, abs/2504.02495, 2025. doi: 10.48550/ARXIV.2504.02495. URL https://doi.org/10.48550/arXiv.2504.02495. OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, and othrts. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2080 2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. naacl-main.168. URL https://aclanthology.org/2021.naacl-main.168. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, et al. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, et al. Rewarding progress: Scaling automated process verifiers for LLM reasoning. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=A6Y7AqlzLW. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, et al. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, et al. Learning to summarize In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin with human feedback. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 30083021. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/ paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, et al. Gemma 2: Improving open language models at practical size, 2024. URL https://arxiv.org/ abs/2408.00118. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, et al. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599. Weiqi Wang, Tianqing Fang, Chunyang Li, Haochen Shi, Wenxuan Ding, et al. Candle: iterative conceptualization and instantiation distillation from large language models for commonsense reasoning. arXiv preprint arXiv:2401.07286, 2024."
        },
        {
            "title": "Preprint",
            "content": "Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, et al. Towards large reasoning models: survey of reinforced reasoning with large language models. arXiv preprint arXiv:2501.09686, 2025. Yuchen Yan, Jin Jiang, Zhenbang Ren, Yijun Li, Xudong Cai, et al. Verifybench: Benchmarking reference-based reward systems for large language models, 2025. URL https://arxiv. org/abs/2505.15801. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024a. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024b. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, et al. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, et al. Internlm-math: Open math large language models toward verifiable reasoning. arXiv preprint arXiv:2402.06332, 2024. Fei Yu, Anningzhe Gao, and Benyou Wang. Ovm, outcome-supervised value models for planning in mathematical reasoning. In Kevin Duh, Helena Gomez-Adorno, and Steven Bethard (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pp. 858875. Association for Computational Linguistics, 2024. doi: 10. 18653/V1/2024.FINDINGS-NAACL.55. URL https://doi.org/10.18653/v1/2024. findings-naacl.55. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, et al. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? CoRR, abs/2504.13837, 2025. doi: 10.48550/ARXIV.2504.13837. URL https://doi.org/10.48550/arXiv. 2504.13837. Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, et al. Internlm-xcomposer2.5reward: simple yet effective multi-modal reward model. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pp. 6547 6563. Association for Computational Linguistics, 2025. URL https://aclanthology. org/2025.findings-acl.340/. Yi-Fan Zhang, Xingyu Lu, Xiao Hu, Chaoyou Fu, Bin Wen, et al. R1-reward: Training multimodal reward model through stable reinforcement learning, 2025. URL https://arxiv.org/ abs/2505.02835. Jialun Zhong, Wei Shen, Yanzeng Li, Songyang Gao, Hua Lu, et al. comprehensive survey of reward models: Taxonomy, applications, challenges, and future, 2025. URL https://arxiv. org/abs/2504.12328. Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, et al. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024. Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, et al. Fine-tuning language models from human preferences, 2020. URL https://arxiv.org/abs/1909. 08593."
        },
        {
            "title": "A DATA SOURCE",
            "content": "To construct high-quality dataset for training VerifyRM, we carefully curated mathematical problems from diverse sources that represent different difficulty levels and problem types. Table 5 presents the seven datasets used in our data collection pipeline, totaling 5,917 unique mathematical problems. Dataset License Sample Count MATH(Hendrycks et al., 2021) OlympiadBench(He et al., 2024) AIME 2024 AIME 2025 AMC23 LiveMathBench(Liu et al., 2024b) CC-BY-4.0 GSM8K(Cobbe et al., 2021) MIT Apache-2.0 MIT MIT / MIT 2000 1177 120 120 160 340 2000 Table 5: Number of samples used in constructing problem-reference-completion triples. Our dataset selection strategy ensures comprehensive coverage across mathematical domains. GSM8K and MATH provide elementary to undergraduate-level problems, while OlympiadBench, AIME and AMC23 contribute competition-level challenges. LiveMathBench adds recently created problems to avoid data contamination issues. Each dataset includes both the problem statement and verified reference answer, which serves as the ground truth for our hybrid annotation process. All data usage strictly complies with the licensing terms specified by the original sources."
        },
        {
            "title": "B LLM USAGE",
            "content": "To generate diverse model completions for our dataset, we employed 11 different large language models spanning various architectures and parameter scales. This diversity is crucial for training robust reward model that can generalize across different reasoning styles and output formats. Table 6 details the models used and their contribution to our final dataset of 65,087 problem-completion pairs. Model Sample Count Series ChatGLM Gemma ChatGLM3-6B(GLM et al., 2024) Gemma-2-2B-it(Team et al., 2024) Gemma-2-9B-it(Team et al., 2024) GLM-4 GLM-4-9B-Chat(GLM et al., 2024) InternLM 2.5 InternLM2.5-7B-Chat(Cai et al., 2024) Qwen2 Qwen2-1.5B-Instruct(Yang et al., 2024a) Qwen2-7B-Instruct(Yang et al., 2024a) LLaMA 3.1 LLaMA-3.1-8B-Instruct(Grattafiori et al., 2024) Qwen2.5 Qwen2.5-7B-Instruct(Qwen et al., 2025) Qwen2.5-14B-Instruct(Qwen et al., 2025) Qwen2.5-Math Qwen2.5-Math-1.5B-Instruct(Yang et al., 2024b) 5917 5917 5917 5917 5917 5917 5917 5917 5917 5917 Table 6: Number of samples generated by LLMs. Each generated one completion per problem. Each model generated responses using consistent sampling parameters (temperature=0.7, top p=0.95) to balance diversity with coherence. The model selection includes both general-purpose instruction-tuned models (e.g., LLaMA-3.1, Qwen2.5) and specialized mathematical reasoning models (e.g., Qwen2.5-Math). This mix ensures our reward model encounters both typical and specialized reasoning patterns during training, improving its robustness in practical applications."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Prompt template for LLM-as-a-judge used in hybrid annotation."
        },
        {
            "title": "C PROMPT TEMPLATES",
            "content": "Our system employs three carefully designed prompt templates for different components of the pipeline. Each template was iteratively refined to maximize performance while maintaining clarity and consistency. C.1 PROMPT TEMPLATE FOR LLM-AS-A-JUDGE For the hybrid annotation process, we utilize Qwen3-4B (Yang et al., 2025) as an LLM judge to assess completion correctness. Figure 5 shows our prompt template, which explicitly provides the problem, reference answer, and model completion. The prompt instructs the model to compare the final answers while being lenient about minor formatting differences, focusing on mathematical equivalence rather than syntactic matching. This design enables the LLM to handle diverse solution formats while maintaining accuracy in correctness judgments. C.2 PROMPT TEMPLATE FOR GENERATING NEGATIVE RESPONSE key innovation in Cooper is the dynamic generation of negative examples for reward model updates. Figure 6 presents our prompt for transforming correct solutions into plausible but incorrect ones. The prompt specifically instructs the assistant LLM to maintain the reasoning structure while introducing errors in calculations or logic. This approach ensures that negative examples resemble actual model errors rather than random corruptions, improving the reward models ability to distinguish subtle incorrectness patterns during contrastive learning. C.3 PROMPT TEMPLATE FOR VERIFYRM Figure 7 shows the input format for our reference-based reward model. Unlike traditional reward models that only consider the query and response, VerifyRM incorporates the reference answer as additional context. This three-part input structure (problem, reference, completion) enables more"
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Prompt template for generating negative response. Figure 7: Prompt template for VerifyRM showing the problem-reference-completion triple format. accurate verification by providing the expected solution approach and final answer, allowing the model to perform comparative analysis between the completion and reference."
        },
        {
            "title": "D DETAILS OF HYBRID ANNOTATION",
            "content": "Our hybrid annotation strategy combines Math-Verify (Gandenberger & Kydlıˇcek, 2024) and Qwen3-4B (Yang et al., 2025) to leverage their complementary strengths. Starting with 65,087 generated completions, we applied both methods independently and selected only samples where they agreed on the correctness label. Table 7 presents the detailed results. Qwen3-4B Predicted Correct Qwen3-4B Predicted Incorrect Math-Verify Predicted Correct Math-Verify Predicted Incorrect 32,119 (Correct) 5,883 Table 7: Hybrid annotation results. Bold entries indicate the 58,738 samples where both methods agree, which we selected for training VerifyRM. 466 26,619 (Incorrect) The high agreement rate (87.2%) validates our approach. The 466 disagreements where Math-Verify predicts correct but Qwen3-4B predicts incorrect likely represent formatting ambiguities, while the 5,883 opposite cases may include valid solutions with non-standard presentations. By selecting only consensus samples, we create high-confidence training set that inherits the precision of rule-based verification and the flexibility of model-based judgment, trading data quantity for quality to ensure robust reward model training."
        }
    ],
    "affiliations": [
        "Zhejiang University"
    ]
}