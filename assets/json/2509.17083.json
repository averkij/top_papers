{
    "paper_title": "HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis",
    "authors": [
        "Zipeng Wang",
        "Dan Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling real-time, high-quality novel view synthesis through explicit, optimizable 3D Gaussians. However, 3DGS suffers from significant memory overhead due to its reliance on per-Gaussian parameters to model view-dependent effects and anisotropic shapes. While recent works propose compressing 3DGS with neural fields, these methods struggle to capture high-frequency spatial variations in Gaussian properties, leading to degraded reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a novel scene representation that combines the strengths of explicit Gaussians and neural fields. HyRF decomposes the scene into (1) a compact set of explicit Gaussians storing only critical high-frequency parameters and (2) grid-based neural fields that predict remaining properties. To enhance representational capacity, we introduce a decoupled neural field architecture, separately modeling geometry (scale, opacity, rotation) and view-dependent color. Additionally, we propose a hybrid rendering scheme that composites Gaussian splatting with a neural field-predicted background, addressing limitations in distant scene representation. Experiments demonstrate that HyRF achieves state-of-the-art rendering quality while reducing model size by over 20 times compared to 3DGS and maintaining real-time performance. Our project page is available at https://wzpscott.github.io/hyrf/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 2 3 8 0 7 1 . 9 0 5 2 : r HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis Zipeng Wang zwang253@cse.ust.hk Dan Xu danxu@cse.ust.hk"
        },
        {
            "title": "Abstract",
            "content": "Recently, 3D Gaussian Splatting (3DGS) has emerged as powerful alternative to NeRF-based approaches, enabling real-time, high-quality novel view synthesis through explicit, optimizable 3D Gaussians. However, 3DGS suffers from significant memory overhead due to its reliance on per-Gaussian parameters to model view-dependent effects and anisotropic shapes. While recent works propose compressing 3DGS with neural fields, these methods struggle to capture high-frequency spatial variations in Gaussian properties, leading to degraded reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), novel scene representation that combines the strengths of explicit Gaussians and neural fields. HyRF decomposes the scene into (1) compact set of explicit Gaussians storing only critical high-frequency parameters and (2) grid-based neural fields that predict remaining properties. To enhance representational capacity, we introduce decoupled neural field architecture, separately modeling geometry (scale, opacity, rotation) and view-dependent color. Additionally, we propose hybrid rendering scheme that composites Gaussian splatting with neural field-predicted background, addressing limitations in distant scene representation. Experiments demonstrate that HyRF achieves state-of-the-art rendering quality while reducing model size by over 20 compared to 3DGS and maintaining real-time performance. Our project page is available at https://wzpscott.github.io/hyrf/."
        },
        {
            "title": "Introduction",
            "content": "Novel view synthesis is critical area in computer vision, with applications in scene manipulation [31, 44, 46, 32], autonomous driving [33, 42], virtual fly-throughs [43, 56, 24], and 3D generation models [19, 14, 12, 35]. Neural Radiance Fields (NeRF) [25] have emerged as leading technology, leveraging implicit scene representations through neural networks and volume rendering to generate novel views. While NeRF-based methods excel in producing high-quality renderings with compact model sizes, they are hindered by slow rendering speeds. In recent advancements, the 3D Gaussian Splatting (3DGS) [15] method has emerged as compelling alternative to NeRF-based approaches, enabling real-time rendering of high-resolution novel views. Unlike NeRF, which relies on continuous neural networks, 3DGS employs set of explicit, optimizable 3D Gaussians to represent scenes. This approach is able to bypass the computational overhead of volume rendering by leveraging an efficient differentiable point-based splatting process [57, 51], achieving real-time performance while enhancing rendering quality. However, 3DGS suffers from significant memory overhead due to its parameter-intensive representation of view-dependent colors and anisotropic shapes. Each 3D Gaussian requires 59 parameters, with 48 parameters dedicated to view-dependent color representation via spherical harmonics and 7 parameters encoding anisotropic scale and rotation. This stands in stark contrast to NeRF-based 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Figure 1: Mip-NeRF360 [3] struggles with inaccuracies in fine details and slow rendering speeds, while 3DGS [15] face challenges of large model sizes and blurry background. naive combination of neural fields and 3DGS leads to loss of high-frequency information. Our method overcomes these challenges through an innovative hybrid architecture. By synergistically combining neural fields, explicit Gaussians, and neural background map, we achieve competitive or superior performance in both visual quality and model compactness, while maintaining real-time rendering capabilities. methods, which efficiently model view-dependent effects through neural network conditioning with minimal parameter growth. natural approach to reducing 3DGS storage costs is to encode 3D Gaussian properties in grid-based neural fields [48, 41]. However, this method faces fundamental limitation: the fixed resolution of grid-based representations struggles to capture the high-frequency spatial variations in 3D Gaussian properties. This issue is particularly pronounced when modeling scenes with rapid opacity and scale changes at object boundaries or high-frequency view-dependent effects. As result, naively fitting 3D Gaussians to neural fields often fails to reconstruct fine details, such as thin geometric structures and high-frequency color variations. In this paper, we present Hybrid Radiance Fields (HyRF), novel scene representation that effectively addresses the frequency limitations of neural Gaussian approaches while maintaining low memory overhead. Our key insight is to decompose the representation into two complementary components: grid-based neural fields that capture low-frequency variations, and sparse set of explicit compact Gaussians that preserve high-frequency details. Our neural component employs decoupled architecture with two specialized neural fields: geometry network dedicated to modeling geometric Gaussian properties (scale, opacity, and rotation), and separate appearance network for view-dependent color prediction. This explicit disentanglement of geometric and photometric learning objectives significantly enhances representational capacity of neural fields while maintaining parameter efficiency. Meanwhile, our explicit Gaussian component stores only essential properties, i.e., 3D positions, isotropic scales, opacity values, and diffuse colors, in order to minimize memory overhead while preserving critical scene details. To achieve both efficiency and rendering quality, we propose hybrid rendering pipeline that operates in three stages. First, our visibility pre-culling module eliminates Gaussians outside the current view frustum, significantly reducing computational overhead of querying neural fields. Next, we process the remaining visible Gaussians by querying their positions through our neural field to predict neural Gaussian properties, which are then combined with the stored explicit parameters to recover high-frequency details. To address the insufficient background modeling of Gaussian representations, we implement learnable solution where the neural field generates background map projected onto background sphere. This background map is composited with the foreground Gaussian rendering through alpha blending, therefore achieves high visual quality for both foreground and remote background objects. In summary, our key contributions include: (i) novel integration of neural fields with explicit compact Gaussians, preserving high-frequency details while minimizing memory overhead. (ii) 2 dual-field architecture that improves the modeling of Gaussian properties by disentangling geometry and view-dependent effects. (iii) hybrid rendering strategy that reduces computational overhead and improves rendering quality for backgrounds. (iv) Extensive experiments demonstrate that our method achieves superior rendering quality, reduces model size by 20 compared to 3DGS [15], and maintains real-time performance."
        },
        {
            "title": "2 Related Work",
            "content": "Neural Radiance Fields. Neural Radiance Fields (NeRF) [25] revolutionized novel view synthesis by modeling scenes as volumetric radiance fields, where each point in space is associated with radiance and density values through multi-layer perceptron (MLP). The state-of-the-art MLP-based method, Mip-NeRF360 [3], has achieved significant improvements in anti-aliasing and handling unbounded scenes. However, MLP-based radiance fields suffer from slow training and rendering speeds due to the extensive querying required for volume rendering. To address these inefficiencies, recent approaches have integrated NeRF with structured arrays of learnable features [21, 52, 36, 9, 40]. For instance, TensoRF [4] employs tensor decomposition to represent scenes using compact low-rank tensor components, while Instant-NGP [27] combines multi-resolution hash table with fully-fused MLP [26], significantly accelerating rendering. Despite these advancements, grid-based methods still face challenges in achieving real-time rendering and matching the quality of MLP-based approaches, often due to limited grid resolution or hash collisions. Explicit Radiance Fields. Another line of research [1, 51, 47] explores replacing implicit neural fields with explicit, point-based scene representations, which can be rendered more efficiently using rasterization techniques. Notably, 3D Gaussian Splatting (3DGS) [15] introduced scene representation based on 3D Gaussians, synthesizing novel views through point-based alpha blending [57]. This approach achieves state-of-the-art rendering quality and real-time performance. However, the size of models using 3D Gaussian representations is always considerably larger than NeRF-based methods. Compressed 3D Gaussian Splatting. While 3D Gaussian Splatting (3DGS) achieves superior rendering performance compared to NeRF-based methods, its significantly larger model size has motivated research into compact representations that preserve its performance advantages. Existing approaches fall into two main categories: (1) parameter compression techniques using vector quantization [17, 28], and (2) hybrid neural-3DGS architectures [29, 17, 6, 41] that uses neural components to predict 3D Gaussian properties instead of explicitly storing them. Closely related to our work, Scaffold-GS [23] employs anchor points with neural features to predict local Gaussian properties, achieving superior compactness while maintaining rendering quality. Our approach differs fundamentally by predicting all Gaussian properties globally through grid-based neural fields, while augmenting high-frequency details with explicit residual Gaussians. This architecture enables both superior compression ratios and enhanced view quality. Furthermore, our method remains compatible with vector quantization techniques, achieving additional efficiency gains since our explicit Gaussians contain far fewer parameters than conventional 3DGS representations. Recently, LocoGS [39] explores similar idea by storing Gaussian properties in neural fields. In contrast, our method stores explicit residuals for Gaussian shapes and introduces decoupled neural fields, leading to improved representation of high-frequency scene components."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Preliminary: 3DGS In 3DGS, scene is depicted through collection of optimizable 3D Gaussians. Each Gaussian is defined by its 3D coordinates p, opacity α, rotation r, scaling factor s, and color c. The opacity α is defined as scalar value ranging from 0 to 1. The size of the Gaussian in 3D is indicated by scale s. Rotation is expressed as quaternion r. The color uses set of spherical harmonics to account for view-dependent effects, which is then converted into an RGB color before rasterization. 3DGS uses 3D points obtained from Structure-from-Motion libraries like COLMAP [37, 38] as initial 3D Gaussians and adaptively densifies them based on the accumulated gradients. During rendering, the 3D Gaussians are ordered by depth, projected onto 2D image planes, and combined using the 3 Figure 2: Framework overview. Our method represents the scene using grid-based neural fields and set of compact explicit Gaussians storing only 3D position, 3D diffuse color, isotropic scale, and opacity. We encode the point position into high-dimensional feature using the neural field and decode it into Gaussian properties with tiny MLP. These Gaussian properties are then aggregated with the explicit Gaussians and integrated into the 3DGS rasterizer. following point-based alpha-blending method. = (cid:88) iN i1 (cid:89) ciαi (1 αj), j=1 (1) where is the final predicted pixel color, is the set of sorted Gaussians projected onto the pixel. 3.2 Hybrid Radiance Fields Our method represents scene using 1) explicit set of 3D Gaussians each holds only 8 parameters, including positions pe R3, diffuse color ce R3, isotropic scale se and opacity αe R. and 2) compact grid-based neural field. We choose the multi-resolution hash encoding [27] as our neural field for its efficiency and strong performance. An overview is illustrated in Fig. 2. Decoupled neural fields: Empirical results demonstrate that predicting all Gaussian properties through single neural field fails to achieve satisfactory performance. We attribute this limitation to the weak correlation between Gaussian geometry and appearance attributes, which makes them hard to be learned jointly within single neural field. To address this issue, we propose decoupled neural field architecture, which predicts geometry properties (scale, opacity and rotation) and appearance property (view-dependent color) with two separate neural fields Θgeo and Θrad. Given the position of 3D point pi, we first employ scene contraction technique similar to that in MipNeRF360 [3] to constrain the input coordinates. We first normalize the coordinates using the axis-aligned bounding box (AABB) B0 of the scene, which we defined as the minimum and maximum camera positions. Next, we contract the normalized points to the range (0, 1) using the following formula: contract(pi) = (cid:40) 0.25 pi + 1 0.25 (2 1 pi )( pi pi ) + if pi 1 otherwise. (2) Note that we contract the points to (0, 1) instead of (2, 2) to meet the input requirements for the multi-resolution hash [27]. Then we use the decoupled neural fields to encode it into two high-dimensional features: fi rad = enc(pi; Θrad), fi geo = enc(pi; Θgeo), (3) where fi rad and fi geo are the encoded features. 4 Figure 3: (a) Visibility Pre-Culling. We first determine whether each Gaussian lies within the current view frustum before applying neural field decoding. (b) Hybrid Rendering Pipeline. For each camera ray, we: (1) compute its intersection point pswith background sphere, (2) sample the radiance field at ps, and (3) composite the foreground and background colors using alpha blending. The encoded features are then decoded into 3D Gaussian properties using two MLP-based decoders. For view-independent properties of opacity α, scale and rotation r, we directly decoded them as: (αn, sn, rn) = dec(fi enc, Φgeo) (4) To account for the view-dependent effects of Gaussian colors, we incorporate view direction component to the MLP input using positional encoding techniques similar to NeRF-based methods [27, 3]. The view direction encoding is calculated as: where PE() is positional encoding technique [25]. The view-dependent color is decoded as: fi dir = PE( pi pcam pi pcam2 ), cn = dec(fi enc fi dir, Φc), (5) (6) where denotes tensor concatenation. Note the derived neural Gaussian properties (αn, rn, sn, cn) here are raw outputs from MLP without activations. Aggregation with explicit Gaussians: Grid-based neural fields often overlook high-frequency scene components such as intrinsic structures. We address this problem by aggregating the predicted properties from neural fields with explicit properties stored in each Gaussian. Similar to 3DGS, we apply the sigmoid function to activate opacity and color, and use normalization function for rotation: α = σ(αn + αe), = σ(cn + ce), = Normalize(rn), = σ(sn + se) (7) where σ denotes the sigmoid function, and Normalize() denotes L2 normalization of the quaternion. The aggregated Gaussian properties (α, r, s, c) are then fed to the 3DGS rasterizer. 3.3 Hybrid Rendering Visibility pre-culling: To reduce the computational overhead of querying the neural fields, we eliminate points that will not be projected onto the image plane before deriving their properties using the neural fields. An illustration of the visibility pre-culling process is provided in Fig. 3(a). Specifically, given point pi and camera viewpoint, we calculate the camera-space coordinates of the point pi using the cameras rotation matrix R33 and translation vector R3 as follows: We retain point only if it is projected within the image frame, determined by the condition: (xi 1 + tol) (yi 1 + tol), pi = Rpi + t. (8) (9) where xi and yi are the first and second elements of pi, respectively. We incorporate tolerance band tol in the culling process to preserve Gaussians that are partially projected outside but still intersect 5 with the image plane. Additionally, we discard Gaussians positioned too close to the image plane, as they may introduce optimization instability. Background rendering: 3DGS often struggle to effectively densify and optimize extremely distant objects, frequently resulting in blurry backgrounds. To address this issue, we propose hybrid rendering technique that leverages the radiance field Θrad to predict the background color. An illustration of the background rendering process is provided in Fig. 3(b). Unlike [18], which predicts the background as points at infinity, we construct background sphere with large radius r. For each ray projected from given camera viewpoint, we compute the intersection point ps between the ray and the sphere. We then use the radiance field and decoder to predict the color at point ps. The background color Cbg combines the background point color with remaining visibility after accumulating the foreground Gaussians: (cid:89) Cbg = (1 αi)cs. i=1 (10) Finally, the pixel color is obtained by combining the foreground and background colors: = Cfg + Cbg, (11) where Cfg is given by Eq. 1. In the rendering stage, we predict Cbg only for pixels with an accumulated transmittance = (cid:81)N i=1(1 αi) lower than threshold τT , thereby increasing rendering speed. 3.4 Optimization Our method is optimized using the same L1 loss and SSIM loss [45] as the original 3DGS: = (1 λ)L1 + λLssim, (12) where λ is the weight for SSIM loss. Similar the original 3DGS, we periodically reset the explicit opacity to small value during densification and prune Gaussians with low opacity."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Dataset: We conduct experiments on three standard real-world datasets: MipNeRF360 [3], Tanks & Temples [16], and Deep Blending [13], which together encompass total of 13 scenes. Additionally, we utilize the NeRF Synthetic dataset [25], featuring 8 object-centered scenes. Furthermore, we examine two large-scale urban datasets captured by drones: Mill19 [43] and Urbanscene3D [20], which collectively include 4 scenes. In total, our experiments span 25 scenes across various datasets. Baselines: For the MipNeRF360 [3], Tanks & Temples [16], and Deep Blending [13] datasets, we compare our method with the MLP-based NeRF method MipNeRF360 [3], two popular gridbased NeRF methodsPlenoxels [8] and Instant-NGP [27]as well as the original 3DGS [15] and its advanced derivative, Scaffold-GS [23]. For the NeRF Synthetic dataset [25], we compare our method with MipNeRF [2], Instant-NGP [27], 3DGS [15], and Scaffold-GS [23]. For the urban-scale datasets [43, 20], we evaluate our method with two prominent NeRF-based techniques: MegaNeRF [43] and SwitchNeRF [56], in addition to 3DGS [15] and Scaffold-GS [23]. To demonstrate the compactness of our method, we also compare compressed version of our approach with five recent 3DGS compression methods [22, 17, 28, 6, 11, 48]. Implementation: Our method is built on top of the original 3DGS implementation. For the neural fields, we adopt multi-resolution hash encodings [27] with 16 levels, where each hash entry stores feature of size 2. The maximum hash size per level for the radiance field is set to 217 for synthetic scenes, 218 for standard scenes, and 221 for large scenes. The hash size for the geometry field is half that of the radiance field. For the decoder, we use fully-fused MLP [26] with 2 hidden layers, each containing 64 neurons. For background rendering, we set the transmittance threshold τT to 0.2 and = 100 for all scenes. All other hyperparameters remain consistent with the original 3DGS. All experiments are conducted on one NVIDIA 3090 GPU. Evaluation metrics: We evaluate rendering quality of novel view synthesis using PSNR, SSIM [45], and LPIPS [55]. We also report the rendering frame rate (FPS) and model size in MB. Figure 4: Qualitative comparisons of our method against previous approaches on standard real-world datasets [3, 16, 13]. The selected scenes include the bicycle and counter scenes from the MipNeRF360 dataset [2], the playroom scene from the DeepBlending dataset [13], and the truck scene from the Tanks & Temples dataset [16]. Arrows and insets are used to highlight key differences. Table 1: Quantitative evaluation of our method compared to previous works on the MipNeRF360 [3], Tanks & Temples [16], and Deep Blending [13] datasets. We consistently achieve the best rendering quality, with model sizes comparable to NeRF-based methods and rendering speeds similar to 3DGSbased methods. The best results are indicated in bold, while the second-best results are underlined. Dataset Tanks&Temples [16] PSNR SSIM LPIPS FPS Size(MB) PSNR SSIM LPIPS FPS Size(MB) PSNR SSIM LPIPS FPS Size(MB) Deep Blending [13] Mip-NeRF360 [3] Plenoxels [8] Instant-NGP [27] M-NeRF360 [3] 3DGS [15] Scaffold-GS [23] 23.08 25.59 27.69 27.21 27.39 0.626 0.699 0.792 0.815 0.806 Ours 27.78 0. 0.463 0.331 0.237 0.214 0.252 0.211 6.79 9.43 0.06 117 86 102 2150 48 8.6 734 244 21.08 21.92 22.22 23.14 23.96 24.02 0.719 0.745 0.759 0.841 0.853 0.844 0.379 0.305 0.257 0.183 0.177 0. 13.0 14.4 0.14 130 94 106 2355 48 8.6 411 86.5 39 23.06 24.96 29.40 29.41 30.21 0.795 0.817 0.901 0.903 0. 30.37 0.910 0.510 0.390 0.245 0.243 0.254 0.241 11.2 2.79 0.09 112 120 2764 48 8.6 676 66 34 4.2 Results and Evaluation Standard real-world scenes: Table 1 presents the quantitative results evaluated on real-world scenes. Our method achieves state-of-the-art rendering quality while maintaining compact model size and real-time rendering speed. Compared to 3DGS [15], our method delivers superior rendering quality while reducing the model size by over 12 times and maintaining comparable rendering speed. When compared to Scaffold-GS [23], our method shows significant improvements in rendering quality, with model sizes 1.5 to 5 times smaller and faster rendering speeds. Qualitative comparisons between our method and previous approaches are illustrated in Fig. 4. Our method excels in capturing fine details, as demonstrated in the bicycle, counter, and playroom scenes, while also achieving better background modeling, as seen in the truck scenes. Object-centered synthetic scenes: Table. 2 presents the qualitative results on the NeRF Synthetic [25] dataset. Our method achieves the best results among all the comparison methods, with size slightly larger than Instant-NGP [27] and over 4 times smaller than 3DGS. Table 2: Comparison on the NeRF Synthetic dataset [25]. Size(MB) PSNR MipNeRF [2] Instant-NGP [49] 3DGS [15] Scaffold-GS [23] Ours 32.63 33.18 33.32 33.68 33. 2.4 12 53 23 13 Large-scale real-world scenes: Table. 3 presents the qualitative results for two urban-scale datasets [43, 20]. Our approach achieves superior rendering quality with more compact model size 7 Table 3: Quantitative evaluation of our method compared to previous works on two urban-scale datasets: Mill19 [43] and Urbanscene3D [20] dataset. Our method achieves the best rendering quality among all compared methods, being 4 to 7 times smaller than 3DGS-based methods and over 7000 times faster than NeRF-based methods. Dataset PSNR SSIM Mill19 [43] LPIPS MegaNeRF [43] SwitchNeRF [56] 3DGS [15] Scaffold-GS [23] Ours 22.50 22.93 22.41 22. 23.52 0.55 0.571 0.695 0.658 0.709 0.510 0.485 0.348 0.339 0.319 FPS <0.01 <0.01 81 75 Size(MB) PSNR 32 17 1566 560 215 23.84 24.54 21.41 20. 24.68 LPIPS Urbanscene3D [20] FPS <0.01 <0.01 84 34 0.440 0.418 0.287 0.295 SSIM 0.699 0.725 0.763 0. 0.791 0.272 77 Size(MB) 32 17 935 435 Table 4: Quantitative evaluation of our method compared to previous 3DGS compression work on the MipNeRF-360 dataset [3]. Table 5: Ablation studies of the key components of our method on the Tanks & Temples dataset [16]. PSNR SSIM LPIPS Size(MB) PSNR SSIM LPIPS FPS Size(MB) Niedermayr et al. [30] Lee et al. [17] Girish et al. [11] Papantonakis et al. [34] Chen et al. [6] Ours 26.98 27.08 27.15 27.1 27.59 27.66 0.801 0.798 0.808 0.809 0.808 0.814 0.238 0.247 0.228 0.226 0. 0.210 28.84 48.80 68.10 25.40 22.50 18.04 Full model w/o Decouple. w/o Explicit w/o Neural w/o Background w/o Pre-culling 24. 23.78 23.45 22.22 23.43 24.06 0.847 0.840 0.829 0.797 0.838 0.847 0.175 0.187 0.196 0.266 0.19 0.175 101 121 127 112 27 41 37 27 14 41 41 compared to 3DGS. Notably, the gap of rendering speed between our method and 3DGS narrows as the number of rendered points increases. In contrast, Scaffold-GS experiences significant decline in speed as the number of Gaussians grows. qualitative comparison is can be found in the supplementary materials, where our method demonstrates better ability to capture fine details and handle lighting variations, where 3DGS and Scaffold-GS suffers from blurs and artifacts. Model compression: Though our method does not inherently include post-processing compression techniques, it remains compatible with most existing 3DGS compression approaches [22, 17]. Our representation achieves better performance by storing significantly fewer explicit Gaussian parameters. To evaluate our methods compactness, we apply post-processing techniques similar to [17], including: (1) storing point positions as half-precision tensors, (2) applying residual vector quantization (R-VQ) and Huffman encoding to explicit Gaussian properties, and (3) employing Huffman encoding with 8-bit min-max quantization for the hash table (see supplementary materials for details). As shown in Table 4, our compressed results outperform five state-of-the-art 3DGS compression methods in both model size and rendering quality. Notably, while conventional 3DGS compression methods typically sacrifice rendering quality for storage efficiency, our approach maintains superior visual fidelity even after aggressive compression. 4.3 Model Analysis Decoupled neural fields: We conduct comparative analysis between our decoupled neural fields approach and single neural field that predicts all Gaussian parameters simultaneously. To maintain experimental fairness, we configure the maximum hash size of the single neural field to be 218, which leads to slightly larger parameter count as our decoupled architecture. As demonstrated in Table 5, the single neural field exhibits consistent degradation across all image quality metrics. This limitation arises from the inherent challenge of using single network to concurrently represent both geometric and appearance properties of 3D Gaussians, resulting in compromised rendering fidelity and inaccurate geometry such as gaps and holes, as visually confirmed in Fig. 5. Hybrid rendering: We evaluate our model using two rendering approaches: (1) our proposed hybrid rendering pipeline and (2) conventional 3DGS rasterization. Quantitative results in Table 5 show that disabling background rendering results in significantly degraded visual quality, despite offering only marginal improvements in rendering speed. This finding supports our hypothesis that standard 3DGS approaches struggle to properly densify and optimize distant objects. As shown in Fig. 6, our qualitative analysis further reveals that background rendering plays crucial role in maintaining high-frequency details for distant scene elements, with particularly notable of fine cloud structures. Neural Gaussians: Our method leverages neural fields to predict the anisotropic shape and viewdependent color of 3D Gaussians. Without these neural components, our framework falls back to 8 Figure 5: Ablation of decoupled neural fields. Using single neural field to predict Gaussian properties causes gaps and holes. Figure 6: Ablation of background rendering. The learnable background map improves the quality of distant objects (see the clouds and sky). Figure 7: Detailed ablation studies of each of the explicit Gaussian properties. Figure 8: Comparison of PSNR and model size changes during the training phase. isotropic Gaussians with diffuse shading which has limited representation capacity, leading to noticeable degradation in novel view synthesis quality, as demonstrated in Tab. 5. Visibility pre-culling: As demonstrated in Table 5, our frustum pre-culling strategy achieves 3.9 rendering speed improvement while maintaining equivalent visual quality for real-world 360 scenes, which represent our primary target scenario. Training time: We analyze the training time of our method and compare it with other baseline methods in Fig. 8. Our method achieves significantly faster convergence, while maintaining substantially smaller model size compared to baselines. Table 6: Detailed ablation studies of each of the explicit Gaussian properties. Explicit Gaussians: In Table 5, we evaluate the impact of removing all explicit Gaussian properties except positions, which are retained as they are required for neural field queries. We analyze the contribution of each explicit Gaussian componentcolor, scale, and opacitythrough systematic ablation. Visual comparisons on the Deep Blending dataset [13] are presented in Fig. 7. Removing explicit color components causes noticeable quality deterioration, as the neural network struggles to model illumination variations and may produce unnatural colors due to hash collisions. The absence of explicit scale significantly impairs reconstruction of thin structures like edges and corners. We also observes removing of explicit scale often leads to instability in training. Finally, removing explicit opacity results in floaters, which also degrades output quality. w/o color. w/o scale w/o opacity 0.896 0.865 0.902 29.18 28.74 30.21 0.251 0.282 0. Full model LPIPS PSNR SSIM 0.910 0. 30."
        },
        {
            "title": "5 Conclusion",
            "content": "We have presented Hybrid Radiance Fields (HyRF), novel approach that bridges the gap between the rendering efficiency of 3D Gaussian Splatting and the compact representation of neural fields. Our work addresses the fundamental limitations of current novel view synthesis methods by introducing hybrid explicit-implicit representation that preserves high-frequency details, decoupled neural field architecture that separately optimizes geometric and appearance properties, and hybrid rendering pipeline that effectively combines the strengths of both representations. Our approach resolves the memory bottleneck of explicit Gaussian representations without sacrificing their rendering quality or 9 speed advantages. As novel view synthesis continues to play crucial role in diverse applications from virtual production to autonomous systems, we believe our contributions represent significant step toward practical, high-quality real-time neural rendering. Limitations: As in the original 3DGS, our present method does not address the aliasing issue [53] and sometimes produces inaccurate surface reconstruction. Moreover, the neural field components in HyRF currently benefit from high-end GPUs for high rendering speed. Achieving comparable efficiency on web platforms or integrated graphics remains an open challenge for the community."
        },
        {
            "title": "References",
            "content": "[1] Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, and Victor Lempitsky. Neural point-based graphics. In ECCV, 2020. [2] Jonathan Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul Srinivasan. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. In CVPR, 2021. [3] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In CVPR, 2022. [4] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In ECCV, 2022. [5] Youyu Chen, Junjun Jiang, Kui Jiang, Xiao Tang, Zhihao Li, Xianming Liu, and Yinyu Nie. Dashgaussian: Optimizing 3d gaussian splatting in 200 seconds. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1114611155, 2025. [6] Yihang Chen, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, and Jianfei Cai. Hac: Hash-grid assisted context for 3d gaussian splatting compression. In ECCV, 2025. [7] Guangchi Fang and Bing Wang. Mini-splatting2: Building 360 scenes within minutes via aggressive gaussian densification. arXiv preprint arXiv:2411.12788, 2024. [8] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In CVPR, 2022. [9] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In CVPR, 2023. [10] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The international journal of robotics research, 32(11):12311237, 2013. [11] Sharath Girish, Kamal Gupta, and Abhinav Shrivastava. Eagles: Efficient accelerated 3d gaussians with lightweight encodings. arXiv preprint arXiv:2312.04564, 2023. [12] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing 3d scenes with instructions. In CVPR, 2023. [13] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep blending for free-viewpoint image-based rendering. TOG, 2018. [14] Ajay Jain, Ben Mildenhall, Jonathan Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. In CVPR, 2022. [15] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ToG, 2023. [16] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. TOG, 2017. [17] Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, and Eunbyung Park. Compact 3d gaussian representation for radiance field. In CVPR, 2024. 10 [18] Wanzhang Li, Fukun Yin, Wen Liu, Yiying Yang, Xin Chen, Biao Jiang, Gang Yu, and Jiayuan Fan. Unbounded-gs: Extending 3d gaussian splatting with hybrid representation for unbounded large-scale scene reconstruction. IEEE Robotics and Automation Letters, 2024. [19] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In CVPR, 2023. [20] Liqiang Lin, Yilin Liu, Yue Hu, Xingguang Yan, Ke Xie, and Hui Huang. Capturing, reconstructing, and simulating: the urbanscene3d dataset. In ECCV, 2022. [21] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. NeurIPS, 2020. [22] Xiangrui Liu, Xinju Wu, Pingping Zhang, Shiqi Wang, Zhu Li, and Sam Kwong. Compgs: arXiv preprint Efficient 3d scene representation via compressed gaussian splatting. arXiv:2404.09458, 2024. [23] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffoldgs: Structured 3d gaussians for view-adaptive rendering. In CVPR, 2024. [24] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In CVPR, 2021. [25] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. ECCV, 2021. [26] Thomas Müller. tiny-cuda-nn, 2021. [27] Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. TOG, 2022. [28] KL Navaneet, Kossar Pourahmadi Meibodi, Soroush Abbasi Koohpayegani, and Hamed Pirsiavash. Compact3d: Compressing gaussian splat radiance field models with vector quantization. arXiv preprint arXiv:2311.18159, 2023. [29] KL Navaneet, Kossar Pourahmadi Meibodi, Soroush Abbasi Koohpayegani, and Hamed Pirsiavash. Compgs: Smaller and faster gaussian splatting with vector quantization. In ECCV, 2024. [30] Simon Niedermayr, Josef Stumpfegger, and Rüdiger Westermann. Compressed 3d gaussian splatting for accelerated novel view synthesis. In CVPR, 2024. [31] Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature fields. In CVPR, 2021. [32] Takashi Otonari, Satoshi Ikehata, and Kiyoharu Aizawa. Entity-nerf: Detecting and removing moving entities in urban scenes. In CVPR, 2024. [33] Jingyi Pan, Zipeng Wang, and Lin Wang. Co-occ: Coupling explicit feature fusion with volume rendering regularization for multi-modal 3d semantic occupancy prediction. RAL, 2024. [34] Panagiotis Papantonakis, Georgios Kopanas, Bernhard Kerbl, Alexandre Lanvin, and George Drettakis. Reducing the memory footprint of 3d gaussian splatting. Proceedings of the ACM on Computer Graphics and Interactive Techniques, 7(1):117, 2024. [35] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [36] Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srinivasan, Ben Mildenhall, Andreas Geiger, Jon Barron, and Peter Hedman. Merf: Memory-efficient radiance fields for real-time view synthesis in unbounded scenes. TOG, 2023. 11 [37] Johannes Lutz Schönberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, 2016. [38] Johannes Lutz Schönberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. In ECCV, 2016. [39] Seungjoo Shin, Jaesik Park, and Sunghyun Cho. Locality-aware gaussian compression for fast and high-quality rendering. International Conference on Learning Representations, 2025. [40] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In CVPR, 2022. [41] Xiangyu Sun, Joo Chan Lee, Daniel Rho, Jong Hwan Ko, Usman Ali, and Eunbyung Park. F-3dgs: Factorized coordinates and representations for 3d gaussian splatting. arXiv preprint arXiv:2405.17083, 2024. [42] Adam Tonderski, Carl Lindström, Georg Hess, William Ljungbergh, Lennart Svensson, and Christoffer Petersson. Neurad: Neural rendering for autonomous driving. In CVPR, 2024. [43] Haithem Turki, Deva Ramanan, and Mahadev Satyanarayanan. Mega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs. In CVPR, 2022. [44] Yuxin Wang, Wayne Wu, and Dan Xu. Learning unified decompositional and compositional nerf for editable novel view synthesis. In CVPR, 2023. [45] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 2004. [46] Silvan Weder, Guillermo Garcia-Hernando, Aron Monszpart, Marc Pollefeys, Gabriel Brostow, Michael Firman, and Sara Vicente. Removing objects from neural radiance fields. In CVPR, 2023. [47] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. Synsin: End-to-end view synthesis from single image. In CVPR, 2020. [48] Minye Wu and Tinne Tuytelaars. Implicit gaussian splatting with efficient multi-level tri-plane representation. arXiv preprint arXiv:2408.10041, 2024. [49] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In CVPR, 2022. [50] Ziyi Yang, Xinyu Gao, Yang-Tian Sun, Yihua Huang, Xiaoyang Lyu, Wen Zhou, Shaohui Jiao, Xiaojuan Qi, and Xiaogang Jin. Spec-gaussian: Anisotropic view-dependent appearance for 3d gaussian splatting. Advances in Neural Information Processing Systems, 37:6119261216, 2024. [51] Wang Yifan, Felice Serena, Shihao Wu, Cengiz Öztireli, and Olga Sorkine-Hornung. Differentiable surface splatting for point-based geometry processing. TOG, 2019. [52] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for real-time rendering of neural radiance fields. In CVPR, 2021. [53] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. In CVPR, 2024. [54] Zehao Yu, Torsten Sattler, and Andreas Geiger. Gaussian opacity fields: Efficient adaptive surface reconstruction in unbounded scenes. ACM Transactions on Graphics (ToG), 43(6):113, 2024. [55] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [56] MI Zhenxing and Dan Xu. Switch-nerf: Learning scene decomposition with mixture of experts for large-scale neural radiance fields. In ICLR, 2022. [57] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Ewa splatting. TVCG, 2002."
        },
        {
            "title": "A Technical Appendices and Supplementary Material",
            "content": "A.1 Scene Contraction We employ scene contraction technique similar to that in MipNeRF360 [3] to constrain the input coordinates of the multi-resolution hash to the range (0, 1). First, we normalize the coordinates using the axis-aligned bounding box (AABB) B0 of the scene. For NeRF synthetic dataset, we set the minimum and maximum and the AABB to be -1.3 and 1.3. For standard dataset, we define the AABB using the minimum and maximum camera positions. For large-scale datasets, we use the points between the 1st and 99th percentiles of the initial point clouds to establish the AABB. The normalized point is derived as follows: Next, we contract the normalized points to the range (0, 1) using the following formula: = B0 . contact(p) = (cid:40) 0.25 + 1 0.25 (2 1 )( p ) + 1 if 1 otherwise, (13) (14) where contact() is the scene contraction function. Note that we contract the points to (0, 1) instead of (2, 2) to meet the input requirements for the multi-resolution hash [27]. A.2 Derivation of Ray-Sphere Intersection In this section, we provide the detailed derivation of the ray-sphere intersection, which is used in the hybrid rendering module to compute background points. Given ray r(t) = + td and sphere centered at the origin with radius r, we substitute the ray equation into the sphere equation: (o + td) (o + td) = r2, (15) which expands to: + 2t(o d) + t2(d d) = r2. (16) Let = d, = 2(o d), and = r2. The equation then simplifies to quadratic in t: The solutions to this quadratic equation are given by: At2 + Bt + = 0. t = B2 4AC 2A . (17) (18) Since the ray originates inside the sphere, the equation always yields two real solutions. We select the positive solution, as it corresponds to the intersection point in the forward direction of the ray. A.3 Ablation for View-dependent Appearance Modeling We provide an additional ablation study that compares two approaches (SH Coefficients for \"high rank per Gaussian spherical harmonics parameters\" and Hybrid for \"MLP and integration of neural field and explicit Gaussian\") for view-dependent appearance modeling, as shown in Table. 7. Our hybrid approach not only achieves significant reduction in model size, but also achieves slightly better visual quality compared with using SH coefficients. This comparison demonstrates that our hybrid approach provides compact and more powerful way in modeling view-dependent appearance. Table 7: Ablation study of SH and MLP based appearance modeling. PNSR SSIM LPIPS Size (MB) SH Coefficients Hybrid (Ours) 30.12 30.37 0.908 0.910 0.243 0.241 267 34 A.4 Evaluation in Street Scenes To evaluate HyRFs performance in street scenes, we conducted experiments on the KITTI [10] dataset (2011_09_26_drive_0002 sequence), as shown in Table. 10. Our method achieves similar visual quality compared with 3DGS while being over 10 times smaller in model size. After adding the background rendering technique, our complete method shows consistent quality improvements, particularly for distant objects and sky regions. Table 8: Evaluation in street scenes on the KITTI [10] dataset . PNSR SSIM LPIPS Size (MB) 3DGS HyRF (w/o background) HyRF (Full) 19.37 19.42 19.56 0.665 0.660 0.667 0.272 0.273 0.273 472 36.7 36.4 A.5 Number of Explicit Gaussians The significant memory savings of HyRF come from both decreased per-Gaussian storage and reduced number of Gaussians. As stated in the paper, HyRF only stores 8 parameters per-Gaussian, in contrast to 59 parameters as in 3DGS. Moreover, HyRF naturally converges to fewer Gaussians while maintaining quality. As shown in Table. 11, HyRF achieves 24-45% reduction in the number of explicit Gaussians compared to 3DGS on three dataset (MipNeRF360, Tanks&Temples and DeepBlending), without additional pruning techniques. We hypothesize this reduction of number of Gaussians stems from two key factors: (1) Faster convergence during training, reducing the need for aggressive densification, and (2) The neural fields ability to represent view-dependent effects without requiring excessive Gaussians. Table 9: Comparison of number of explicit Gaussians. MipNeRF360 Tanks&Temples DeepBlending 3DGS HyRF 3.31M 2.52M 1.84M 1.01M 2.81M 1.74M A.6 Additional Comparison with Recent 3DGS-based Methods We conduct additional comparison experiments with several recent 3DGS-based methods, namely GOF [54], Spec-GS [50], Mini-Splatting2 [7] and DashGaussian [5] on the DeepBlending [13] dataset. To provide more comprehensive evaluation, we have expanded the comparison table to include rendering speed (FPS), training time (Time), peak GPU memory usage (Memory), and model storage size (Size) across state-of-the-art methods. A.7 Additional Comparison on Specular Scenes we have conducted additional quantitative comparisons using the anisotropic synthetic dataset from Spec-GS [50], which features 8 object-centered scenes with strong specular highlights. Compared with 3DGS, HyRF achieves significantly better rendering quality (1.58 dB PSNR) while using 82% less memory. The improved performance highlights the benefits of using MLPs over SH coefficients for modeling high-frequency view-dependent effects. Table 10: Comparison with recent 3DGS-based methods. PSNR SSIM LPIPS FPS Time (min) Memory (GB) Size (MB) 3DGS GOF Spec-GS MiniSplatting2 DashGaussian 29.41 30.42 30.57 30.08 30.02 0.903 0.914 0.912 0.912 0.907 HyRF 30.37 0.910 0.243 0.237 0.234 0.240 0.248 0.241 112 96 107 136 132 14 14.4 20.3 17.8 2.75 2.62 12.5 5.54 6.62 5.79 3.65 4.32 1.83 676 721 765 155 34 Table 11: Comparison on the Spec-GS dataset. PSNR SSIM LPIPS Size (MB) 3DGS HyRF (Ours) 33.83 35. 0.966 0.970 0.062 0.053 47 8.2 A.8 Additional Qualitative Comparisons In Fig. 9, we show the Additional qualitative comparisons of our method against previous approaches on standard real-world datasets. Figure 9: Additional qualitative comparisons of our method against previous approaches on standard real-world datasets. A.9 Per-scene Metrics Table. 12-15 present per-scene metrics for MipNeRF360 [3], Tanks & Temples [16] and Deep Blending [13] datasets. Table. 16 and 17 provide per-scene metrics for the per-scene metrics for NeRF Synthetic dataset [25]. Finally, Table. 18 lists per-scene metrics for Mill19 [43] and Urbanscene3D [20] datasets. 15 Table 12: PSNR scores for scenes in Mip-NeRF360 [3], Tanks & Temples [16] and Deep Blending [13] datasets. dataset scene bicycle flowers garden stump treehill room counter kitchen bonsai mean Mip-NeRF360 [3] Tanks&Temples [16] train mean truck Deep Blending [13] drjohnson playroom mean Plenoxels [8] Instant-NGP [27] M-NeRF360 [3] 3DGS [15] Scaffold-GS [23] Ours 21.91 22.17 24.37 25.25 24.50 25.45 20.09 20.65 21.73 21.52 21.38 21. 23.49 25.06 26.98 27.41 27.17 27.54 20.66 23.46 26.40 26.55 26.27 26.19 22.24 22.37 22.87 22.49 22.44 22. 27.59 29.69 31.63 30.63 31.93 31.98 23.62 26.69 29.55 28.70 29.34 29.4 23.42 29.47 32.23 30.32 31.30 32. 24.66 30.68 33.46 31.98 32.70 33.04 23.08 25.59 27.69 27.21 27.39 23.22 23.38 24.91 25.19 25.77 18.92 20.45 19.52 21.10 22.15 21.08 21.92 22.22 23.14 23. 27.78 25.92 22.12 24.02 23.14 28.25 29.14 28.77 29.80 29. 22.98 21.66 29.65 30.04 30.62 31.02 23.06 24.96 29.40 29.41 30.21 30.37 Table 13: SSIM scores for scenes in Mip-NeRF360 [3], Tanks & Temples [16] and Deep Blending [13] datasets. dataset scene bicycle flowers garden Mip-NeRF360 [3] treehill stump room counter Plenoxels [8] Instant-NGP [27] M-NeRF360 [3] 3DGS [15] Scaffold-GS [23] Ours 0.496 0.512 0.685 0.771 0.705 0.762 0.431 0.486 0.584 0.605 0.607 0. 0.606 0.701 0.809 0.868 0.842 0.854 0.523 0.594 0.745 0.775 0.784 0.756 0.509 0.542 0.631 0.638 0.620 0. 0.842 0.871 0.910 0.914 0.925 0.930 0.759 0.817 0.892 0.905 0.914 0.915 Tanks&Temples [16] Deep Blending [13] kitchen bonsai mean 0.648 0.858 0.917 0.922 0.928 0.927 0.814 0.906 0.938 0.938 0.946 0.950 0.626 0.699 0.792 0.815 0. 0.816 truck 0.774 0.800 0.857 0.879 0.883 0.883 train 0.663 0.689 0.660 0.802 0. 0.806 mean 0.719 0.745 0.759 0.841 0.853 0.844 drjohnson playroom mean 0.787 0.854 0.901 0.899 0.901 0.904 0.802 0.779 0.900 0.906 0.904 0.916 0.795 0.817 0.901 0.903 0.906 0. Table 14: LPIPS scores for scenes in Mip-NeRF360 [3], Tanks & Temples [16] and Deep Blending [13] datasets. dataset scene bicycle flowers garden stump Mip-NeRF360 [3] treehill room counter kitchen bonsai mean Tanks&Temples [16] train mean truck Deep Blending [13] drjohnson playroom mean Plenoxels [8] Instant-NGP [27] M-NeRF360 [3] 3DGS [15] Scaffold-GS [23] Ours 0.506 0.446 0.301 0.205 0.306 0. 0.521 0.441 0.344 0.336 0.362 0.301 0.3864 0.257 0.170 0.103 0.146 0.144 0.503 0.421 0.261 0.210 0.284 0. 0.540 0.450 0.339 0.317 0.346 0.328 0.4186 0.261 0.211 0.220 0.202 0.189 0.441 0.306 0.204 0.204 0.191 0. 0.447 0.195 0.127 0.129 0.126 0.124 0.398 0.205 0.176 0.205 0.185 0.167 0.463 0.331 0.237 0.214 0.252 0.335 0.249 0.159 0.148 0. 0.422 0.360 0.354 0.218 0.206 0.379 0.305 0.257 0.183 0.177 0.211 0.140 0.212 0. 0.521 0.352 0.237 0.244 0.250 0.242 0.499 0.428 0.252 0.241 0.258 0.239 0.510 0.390 0.245 0.243 0.254 0. Table 15: Model size (MB) for scenes in Mip-NeRF360 [3], Tanks & Temples [16] and Deep Blending [13] datasets. dataset scene bicycle flowers garden stump treehill room counter kitchen bonsai mean Mip-NeRF360 [3] Tanks&Temples [16] train mean truck Deep Blending [13] drjohnson playroom mean 3DGS [15] Scaffold-GS [23] Ours 1291 68 1045 217 55 1268 271 54 1034 51 872 209 61 327 133 41 261 39 414 173 38 281 258 38 634 49 578 107 41 240 66 37 411 39 715 69 48 515 63 36 676 34 Table 16: PSNR scores for scenes in Synthetic NeRF dataset [25]. Table 17: Model size for scenes in Synthetic NeRF dataset [25]. scene 3DGS [15] Scaffold-GS [23] Mic 35.36 37.25 Chair 35.83 35.28 30.80 31.17 Ours 35. 35.51 31.76 Ship Materials Lego Drums Ficus Hotdog mean scene Mic Chair Ship Materials Lego Drums Ficus Hotdog mean 30.00 30.65 30.13 35.78 35. 36.30 26.15 26.44 26.44 34.87 35.21 35.60 37.72 37. 38.18 33.32 33.68 33.72 3DGS [15] Scaffold-GS [23] 50 12 116 63 16 35 18 78 13 93 35 59 11 44 Ours 11.2 10.9 12.2 13.6 12. 13.2 14.7 12.4 53 23 13 Table 18: Per-scene metrics on Mill19 [43] dataset. Scene Rubble Building Sci-art Residence PSNR SSIM LPIPS Size PSNR SSIM LPIPS Size PSNR SSIM LPIPS Size PSNR SSIM LPIPS Size 3DGS [15] Scaffold-GS [23] Ours 24.21 22. 25.3 0.695 0.662 0.709 0.357 0.342 0.331 1566 183 20.6 19.97 21.75 0.677 0.655 0.710 0.340 0. 0.305 1424 599 194 21.84 18.9 26.07 0.801 0. 0.830 0.279 0.286 0.247 596 303 123 20.97 19. 23.28 0.726 0.695 0.751 0.295 0.303 0.295 1273"
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology"
    ]
}