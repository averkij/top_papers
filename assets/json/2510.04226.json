{
    "paper_title": "Epistemic Diversity and Knowledge Collapse in Large Language Models",
    "authors": [
        "Dustin Wright",
        "Sarah Masud",
        "Jared Moore",
        "Srishti Yadav",
        "Maria Antoniak",
        "Chan Young Park",
        "Isabelle Augenstein"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) tend to generate lexically, semantically, and stylistically homogenous texts. This poses a risk of knowledge collapse, where homogenous LLMs mediate a shrinking in the range of accessible information over time. Existing works on homogenization are limited by a focus on closed-ended multiple-choice setups or fuzzy semantic features, and do not look at trends across time and cultural contexts. To overcome this, we present a new methodology to measure epistemic diversity, i.e., variation in real-world claims in LLM outputs, which we use to perform a broad empirical study of LLM knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200 prompt variations sourced from real user chats. For the topics in our study, we show that while newer models tend to generate more diverse claims, nearly all models are less epistemically diverse than a basic web search. We find that model size has a negative impact on epistemic diversity, while retrieval-augmented generation (RAG) has a positive impact, though the improvement from RAG varies by the cultural context. Finally, compared to a traditional knowledge source (Wikipedia), we find that country-specific claims reflect the English language more than the local one, highlighting a gap in epistemic representation"
        },
        {
            "title": "Start",
            "content": ""
        },
        {
            "title": "Dustin Wright",
            "content": "Sarah Masud Peter Ebert Christiansen"
        },
        {
            "title": "Jared Moore\nChan Young Park\nZ",
            "content": "Srishti Yadav ] Maria Antoniak"
        },
        {
            "title": "Isabelle Augenstein",
            "content": "^"
        },
        {
            "title": "Stanford University",
            "content": "]"
        },
        {
            "title": "Microsoft Research",
            "content": "^ Correspondence: dw@di.ku.dk 5 2 0 2 5 ] . [ 1 6 2 2 4 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) tend to generate lexically, semantically, and stylistically homogenous texts. This poses risk of knowledge collapse, where homogenous LLMs mediate shrinking in the range of accessible information over time. Existing works on homogenization are limited by focus on closed-ended multiple-choice setups or fuzzy semantic features, and do not look at trends across time and cultural contexts. To overcome this, we present new methodology to measure epistemic diversity, i.e., variation in real-world claims in LLM outputs, which we use to perform broad empirical study of LLM knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200 prompt variations sourced from real user chats. For the topics in our study, we show that while newer models tend to generate more diverse claims, nearly all models are less epistemically diverse than basic web search. We find that model size has negative impact on epistemic diversity, while retrieval-augmented generation (RAG) has positive impact, though the improvement from RAG varies by the cultural context. Finally, compared to traditional knowledge source (Wikipedia), we find that country-specific claims reflect the English language more than the local one, highlighting gap in epistemic representation."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) are being adopted for knowledge-intensive tasks such as summarization (Wright et al., 2025), writing assistance (Sun et al., 2025), and research (Si et al., 2025). Search interfaces are now prioritizing AI Overviews to answer queries. It is speculated that people will Figure 1: In this work, we measure epistemic diversity via variability in claims about the world for characterizing knowledge collapse in LLMs. soon access most information through an LLM intermediary (Peterson, 2025). At the same time, recent studies have noted that LLM outputs are homogeneous (Sourati et al., 2025b). For example, LLMs reflect only narrow range of writing and reasoning styles, use limited vocabulary (Sourati et al., 2025a) and convey only certain semantics (Lee et al., 2025). This phenomenon, where models regress to central tendency, also affects knowledge i.e., the pieces of information that models tend to generate (Dutta and Chakraborty, 2023). This could in turn limit the information available to the general public and subsequently dwindle our collective knowledge, phenomenon broadly defined as knowledge collapse  (Fig. 1)  .2 Recent work (Peterson, 2025) has begun to theorize this, but offers limited empirical data. To better characterize and understand this problem, we investigate, for the first time, whether LLMs themselves exhibit knowledge collapse. To do so, we perform an empirical study measuring epistemic diversity, defined as the diver1Code and data: https://github.com/dwright37/ 2For an in depth definition and discussion, see Peterson llm-knowledge (2025). sity of claims about the world in given corpus (e.g., set of LLM responses). To measure this, we develop new methodology intended to reveal the diversity of claims occurring in free-text LLM outputs. This involves (1) sampling outputs from LLMs using set of 200 natural writing assistance prompts collected from Röttger et al. (2025), (2) partitioning the LLM responses into unique classes of semantically equivalent claims (Farquhar et al., 2024), and (3) quantifying the diversity of the sample of claims with Hill-Shannon diversity, widely used metric for measuring species diversity in ecology (Roswell et al., 2021). From among the Llama, Gemma, Qwen, and OpenAI model families, we study 27 LLMs spanning multiple versions, sizes, and release dates, querying them for 155 topics. For the selected topics, while we find that epistemic diversity has increased since 2023 for three of four model families, it remains low for all models compared to rudimentary web search. Similar to contemporary work (Zhang et al., 2025), we find that model size has statistically significant negative impact on epistemic diversity smaller models generate more diverse knowledge than larger ones. In contrast, retrieval-augmented generation (RAG) has statistically significant positive impact, highlighting the importance of RAG in preventing future knowledge collapse. For country-specific topics, we find that RAG has an uneven effect; certain countries (e.g., the USA) see more benefit due to greater diversity in their RAG sources. Finally, compared to traditional knowledge source (Wikipedia) in both English and local languages for country-specific topics, we find that the claims generated in our study reflect English language knowledge more than local language knowledge, highlighting gap in epistemic representation."
        },
        {
            "title": "2 Background",
            "content": "LLMs are increasingly being used for knowledgecentric tasks (Yang et al., 2024), and can influence peoples behavior (Anderson et al., 2024; Jakesch et al., 2023; Bai et al., 2025a). Hence, lack of diversity in LLMs outputs may reduce the diversity in our collective knowledge. Our work studies this risk through the lenses of LLM homogenization and knowledge collapse. sity includes lexical and stylistic (Sourati et al., 2025a), semantic (Lee et al., 2025), creative (Xu et al., 2025), and perspective diversity (Wright et al., 2024; Abdurahman et al., 2024; Zhang et al., 2025; Durmus et al., 2023; Röttger et al., 2024; Moore et al., 2024). Our work is most similar to perspective diversity, which has shown that LLMgenerated views, opinions, and beliefs tend to reflect only small subset of the world (Durmus et al., 2023; Atari et al.; Abdurahman et al., 2024; Alvero et al., 2024). Perspective diversity is usually measured with multiple-choice survey responses (Durmus et al., 2023) or free-text responses partitioned into different classes (Zhang et al., 2025). In our work we measure the epistemic diversity of models by partitioning the LLM output space into clusters of claims. This allows us to avoid shallow and/or fuzzy features such as semantic and lexical similarity, and improves upon unreliable multiple-choice setups (Röttger et al., 2024). Knowledge Collapse LLMs can exacerbate their own biases, leading to model collapse (Shumailov et al., 2024). growing concern among scholars is that such homogenization, combined with increased adoption of LLMs, will lead to epistemic problems at societal level (Zheng and Lee, 2023; Messeri and Crockett, 2024; Peterson, 2025; Wagner and Jiang, 2025; Qiu et al., 2025). Peterson (2025) defines knowledge collapse as LLMs facilitating dwindling of knowledge into an increasingly narrow set of ideas. Knowledge collapse may affect existing knowledge sources such as Wikipedia (Wagner and Jiang, 2025), erase minoritized knowledge (Zheng and Lee, 2023), pollute scientific discoveries (Messeri and Crockett, 2024), hamper political discourse (Coeckelbergh, 2025), and limit ideation in writing (Anderson et al., 2024). These concerns echo other contemporary epistemic issues such as popularity bias (Ciampaglia et al., 2018), and filter bubbles (Nguyen et al., 2014) occurring with the use of recommender systems. To assess the risk of knowledge collapse, our work offers new methodology for measuring epistemic diversity and an empirical study of this phenomenon in LLMs."
        },
        {
            "title": "3 Problem Setup and Notation",
            "content": "LLM Homogenization An increasing body of work shows that LLMs suffer from lack of diversity along many dimensions; for recent survey, see Sourati et al. 2025b. This lack of diverWe measure the epistemic diversity of LLMs as the variation in the claims that the models make across different topics and prompts. To do so, we develop methodology that is naturalistic and theoretically motivated. It is naturalistic in that it uses natural prompts to sample open-ended responses (Wright et al., 2024; Moore et al., 2024; Röttger et al., 2025). It is theoretically motivated in that it adopts statistically grounded measure of diversity used widely in ecology for measuring species diversity (Peterson, 2025; Roswell et al., 2021). We measure epistemic diversity after transforming open-ended LLM responses into distributions over meaning classes (Farquhar et al., 2024). To describe our approach  (Fig. 1)  , we adopt the following notation. First, corpus of text Pmt is elicited about topic from model m. Pmt contains free text which can be decomposed into list of claims Cmt, which can be further partitioned into set of unique meaning classes Xmt. unique meaning class is cluster where all claims within given class mutually entail each other and do not mutually entail claims in other classes. Then, xi Xmt is defined as the empirical frequency of meaning class calculated from Cmt, and pi is the probability of i, calculated as the relative frequency of in the sample, i.e., xi . Why not semantic similarity? An alternative version of our setup would be to partition claims based solely on semantic similarity, as previous work has done (Lee et al., 2025; Wright et al., 2024). However, sentence embeddings (Reimers and Gurevych, 2019) can only measure equivalence based on functionally similar phrases. For example, the phrases Claude Shannon is the father of information theory and Claude Shannon is not the father of information theory have extremely high semantic similarity (0.94)3, even though they present opposing claims. Similarly, Claude Shannon is the father of quantum theory is an entirely different claim from Claude Shannon is the father of information theory, but these two also have high semantic similarity (0.805). This setup would not reflect our definition of meaning classes."
        },
        {
            "title": "4 Data Collection",
            "content": "We first construct Xmt for particular model and topic (for details about the specific models and topics we study see 6). Acquiring Xmt involves three step process similar to Wright et al. (2024) and Zhang et al. (2025) but focused on claims: 1. Generate: Acquire corpus Pmt of free-text LLM responses to natural input prompts. 2. Decompose: Decompose Pmt into list of atomic claims Cmt. 3. Cluster: Group the list of atomic claims into meaning classes Xmt. 4.1 Generation and Decomposition We use the open-ended writing assistance prompt templates collected in Röttger et al. (2025). The original prompt templates are sourced from WildChat (Zhao et al., 2024), collection of natural conversations between users and ChatGPT. From the original 1,000 writing assistance prompts, we manually select subset of 479 prompts that primarily focus on information seeking and informational writing. We manually filter out NSFW templates, creative writing templates such as write 50s soviet style song about t, outlines such as write an index for book on t, or those explicitly asking for references. We then randomly sample 200 of these prompt templates for generation. After generating responses (Pmt), we decompose each response into list of individual claims Cmt. We do so using strong open-weight LLM,4 prompting the model to decompose nonoverlapping chunks of three sentences at time. This allows us balance claim recall while with ensuring that each input chunk is decontextualized. Evaluation To evaluate the quality of decomposition, we develop three initial decomposition prompts (P1-P3) and have two independent annotators label (1) the quality of individual decomposed claims on Likert scale from 1-5, and (2) how many claims in the original input chunk are missing from the list of decomposed claims. (On our scale, 1 means that the decomposed claim is not inferable from the original text, and 5 means that the claim is fully inferable and decontextualized.) The full annotation instructions appear in the Appendix A.1. We label 100 input chunks for quality (1) and 108 instances for missing (2), all anonymized for the prompt type (P1-P3). Between the two annotators, we achieve Kendall-Tau (τ ) correlation (Kendall, 1938) of 0.53 for (1) and 0.39 for (2), indicating moderate to strong agreement.5 We break ties by having third annotator label those instances where the original annotators disagreed. We then set up an LLM-as-a-judge using GEval (Liu et al., 2023), which achieves 0.6 Pearson 4Huggingface ID: meta-llama/Llama-3.1-70B-Instruct 5We select τ because it is suited for ordinal data with 3Huggingface ID: all-MiniLM-L6-v2 ranking ties (Kendall, 1945). P1 P2 P3 Algorithm 1: Clustering algorithm Quality Missing Claims 4.58 0.06 4.67 0.24 4.69 0.25 Table 1: Performance of different prompts for claim decomposition using LLM-as-a-judge ( 4.1). Quality measures how well the decomposed claims align with the original chunk on scale from 1 to 5. Missing Claims measures the average number of claims missed during the decomposition. We use P3 for our final decomposition due to its higher quality and minimal missing claims. correlation with the human labels for decomposition quality and 0.68 for the number of missing claims. We use ths to automatically label 6k instances across three decomposition prompt variants (P1-P3; see Appendix A.2 for prompt text). The results are given in Table 1. We use P3 for the final decomposition in order to prioritize quality."
        },
        {
            "title": "4.2 Clustering",
            "content": "Algorithm 1 outlines our approach for clustering the decomposed claims Cmt into meaning classes Xmt Following Farquhar et al. (2024), we bootstrap clusters based on mutual entailment using strong pretrained model for natural language inference.6 This is done by assuming that when given claim mutually entails at least one claim in an existing cluster, it also entails all other claims in the cluster. To reduce measuring mutual entailment across n2 pairs for set of claims, we only check entailment between the most similar claims (i.e., comparisons), where similarity is measured using strong S-BERT model.7 To mitigate potential drift in cluster cohesion (i.e., large clusters containing multiple meaning classes), we perform post-processing step to break up large clusters using DBSCAN (Ester et al., 1996), related clustering approach (Peterson, 2025; Wright et al., 2024). input :Decomposed corpus with claims cj; Max retrieval length # Previous claims [C[0]]; # Claim cluster [0]; forall cj C[1 :] do # Sort by cos similarity candidates most_similar(cj, A)[:N ]; scores = []; forall ck candidates do if entails(ck, cj) & entails(cj, ck) then scores.append(entail_prob(ck, cj) * entail_prob(cj, ck)); if len(scores) == 0 then max(L) + 1; else cluster(max(scores)); L.append(i); A.append(cj); # Count the number of elements in each unique cluster count(L); return X, L; missing (2), achieving Kendall-Tau correlation of 0.38 for (1) and 87% agreement accuracy for (2). We again break ties using third annotator and set up an LLM-as-a-judge using G-Eval. We develop prompts that achieve 0.68 Pearson with the human labels for measuring cluster cohesion and 83.5 weighted F1 score for measuring binary missing claims. 15,000 samples of 3-5 sentences are then automatically labeled from clusters acquired using Algorithm 1, where semantic similarity for task (2) is measured using S-BERT. We acquire cluster cohesion score of 4.08 and missing rate of 13.79 (out of 100), indicating that the cluster cohesion is generally of high quality while missing only relatively small proportion of singletons."
        },
        {
            "title": "5 Measuring Epistemic Diversity",
            "content": "Evaluation Two independent annotators label: (1) the degree of cohesion among the clustered claims (Likert score between 1 and 5), and (2) whether semantically similar, non-clustered claims are grouped with given cluster (binary yes/no). See Appendix A.1 for full instructions. We label 100 instances of cohesion (1) and 360 instances of 6Huggingface ID: microsoft/deberta-large-mnli 7Huggingface ID: all-MiniLM-L6-v2 After obtaining Xmt, we must quantify cluster diversity to rank different models. We could simply count the number of clusters for each model, but that would discount the distribution of the clusters (e.g., for one model, there might be long tail of clusters with only one claim in each cluster, which should rank lower compared to model with fewer clusters that each contain many claims). Conversely, we could turn these counts into probabiliFigure 2: Histograms of the top ten clusters for four topics after generating text, decomposing, and clustering decomposed claims across all models in our study. The frequency of claims in each cluster, xi, is represented by the colored bars. By the 10th cluster, xi is halved for all four topics, indicating large decay rate for xi. The top clusters for each topic convey broad and general information for each topic. ties, obtaining categorical distribution over clusters, and then calculate entropy (Peterson, 2025), but then the change in total number of clusters across topics would not scale properly (e.g., compared to another setting that has half as many clusters, the diversity metric would only lower by maximum of one point). We therefore use Hill diversity (Hill, 1973; Jost, 2006) as our diversity index. Hill diversity offers general way to measure the relative abundance of different categories of items in sample, and satisfies the replication principle where relative changes in categories and abundances between samples result in proportionate changes to the score (Roswell et al., 2021). Using the notation from 3, we calculate the general Hill diversity as D(Xmt) = 1/l )l , Xmt(cid:88) pi( 1 pi where low values of the free parameter will provide more weight to more frequent claims in the sample, while higher values give more emphasis to rare classes. To balance the emphasis between common and rare claims, we choose = 0, a.k.a the Hill-Shannon diversity (HSD), which resolves to DS(Xmt) = exp{ pi ln pi}, (1) Xmt (cid:88) Note that the way one samples Cmt has large impact on DS. This can occur despite equal-effort sampling, e.g., prompting all models with the same number of prompts, because the long tail of claims may vastly differ between settings. To illustrate: consider two models, one with low diversity (a) and one with high diversity (b), from which we sample an equal number of model responses. Assume both models also produce roughly the same number of claims, n, after decomposition; is less diverse, so samples may be sufficient to fully characterize Xat, and sampling more claims is unlikely to introduce new meaning classes or have substantial impact on DS(Xat). On the other hand, the introduction of new claims from the more diverse model may introduce new meaning classes to Xbt which take longer to uncover due to their rarity, thus increasing DS(Xbt). Therefore, by sampling claims for both models, we have potentially overestimated the diversity of relative to b. To account for this, we follow best practices by using combination of coverage estimation and rarefaction as outlined in Chao and Jost (2012) and Roswell et al. (2021). Coverage allows us to estimate how well our sample captures the true diversity of given setting, i.e., how completely we have performed our sampling. It is calculated using the estimator from Chao and Jost (2012): i.e., raised to the entropy in nats. (Xmt) = 1 (cid:20) f1 (n 1)f1 (n 1)f1 + 2f2 (cid:21) , (2) where is the number of claims, f1 is the number of singleton classes in Xmt (i.e., the number of meaning classes where xi = 1), and f2 is the number of doubleton classes (i.e., the number of meaning classes where xi = 2). When we wish to compare the diversity of models on topic t, we use Equation 2 to calculate the coverage of all models and rarefy each set of claims (i.e., downsample) to the minimum coverage achieved across models."
        },
        {
            "title": "6 Empirical Results",
            "content": "To what extent are LLMs exhibiting knowledge collapse? To answer, we pose several subquestions focused on how epistemic diversity has changed over time (RQ1), how epistemic diversity is impacted by generation setting (RQ2), how size and model family impact epistemic diversity (RQ3), and how cultural context impacts both diversity and representation (RQ4)."
        },
        {
            "title": "6.1 Models, Topics, and Settings",
            "content": "Models We select 27 LLMs across 4 model families. This includes Llama (versions 2, 3.1, 3.2, and 3.3), Gemma (versions 1, 1.1, 2, and 3), Qwen (versions 1.5, 2.5, and 3), and OpenAI (GPT 3.5 Turbo, 4o, and 5). Within each open-weight model family, we also select small, medium, and large models across versions, thus including multiple release dates (between 2023-2025). Table 4 provides the details of all models. Settings We test each model with two different settings: (1) parametric memory with instructionfine-tuning only (IFT) and (2) retrieval augmented generation (RAG). In addition, to compare the epistemic diversity of LLMs with that of traditional web search, we include as baseline 20 retrieved web pages from Google search for each topic.8 Furthermore, we use the search data as our RAG database by splitting each web page into paragraphs. For retrieval, we match prompts to the top paragraphs using S-BERT embeddings, including up to 1000 tokens of context from these retrieved paragraphs. Since many prompts match with similar top paragraphs in the search results we additionally shuffle the ranks of all paragraphs with cosine similarity to the query above 0.35 to encourage different prompts to use different con8Search performed on Aug 10, 2025, in the US region. We filter out content from social media, PDFs, and any page with fewer than 1000 characters. texts.9 This way, we can observe how varying context impacts output diversity in the RAG setting. For generation, we use top sampling (p = 0.9, temperature = 1.0) and generate maximum of 2,100 tokens per prompt ( the median number of tokens on the web pages in our search results). For rarefaction, we calculate the coverage of each sample for each setting on each topic using Equation 2, and rarefy all samples to the minimum.10 Due to the low coverage achieved by the search baseline, we only rarefy search if the coverage is above the minimum achieved by an LLM. Therefore, all results with search should be considered as lower-bound baseline; the true diversity of search relative to the LLMs tested will be higher. Topics As our study is broad (27 models, two settings, and 200 prompt templates per model per topic), we constrain the number of selected topics for computational feasibility. We use sample of 30 general topics from IssueBench (Röttger et al., 2025) and 125 additional hand-curated topics. These include 10-13 country-specific topics about important figures and historical events to help measure the impact of cultural context on diversity. To ensure that these are well documented and known topics, we only select topics where their associated English Wikipedia page has content rating of at least C.11 Following IssueBench, these include mix of both controversial and non-controversial topics, e.g., general topics for which there may be contentious views, like nuclear weapons and pornography, and events which may be subject to diverse and varied interpretations. All events occurred before the earliest knowledge cutoff of any model in our study. In total, we have 155 topics (see Appendix A.3 for the list), yielding 1.7M responses prior to decomposition and 69.5M claims after decomposition. While the topics were handpicked based on the above criteria, the selection process could nevertheless impact the generalizability of our results. However, if we can identify knowledge collapse even for this limited set of topics, this would already be cause for concern. 9We calculated this as the mean cosine similarity across all paragraphs and prompts in the data. 10We found that by sampling 200 responses per LLM per topic, the coverage tends to vary widely between models (20%- 80%), while for search it tends to be < 20%. 11https://en.wikipedia.org/wiki/Wikipedia: Content_assessment Figure 3: Information diversity vs. model release date. Each point is single model, with lines connecting models of approximately the same size across released versions. Error bars are 95% boostrapped confidence intervals based on the HSD of each topic (N=155). Absolute diversity is low for all models compared to very modest search baseline (top 20 Google search results for each topic). However, for all families except Qwen and most sizes, we see trend of improved diversity. Clustering For our clustering algorithm we have one parameter to select: the number of claims to retrieve for comparison. To choose this, we first ran sample of 50,000 claims from 10 topics (5,000 each) through Algorithm 1, setting = 10. From the list of top 10 most similar claims, we measure mutual entailment with the retrieving claim, and count the number of times that the kth most similar claim (k [1, 10]) is mutually entailed. We found that when claim is matched, 98.4% of the time it is within the first 6 most similar claims, and therefore select = 6 for our experiments. We give examples of the top 10 clusters for four topics (democracy, free speech, Berlin Wall, and Kpop) in Fig. 2. Clusters are aggregated across every claim in the dataset, i.e., all models and settings. The frequency of claims drops rapidly; the 10th cluster is half as frequent as the first in all examples shown. Additionally, high-frequency clusters unsurprisingly provide broad, common information about each topic (e.g., the definitions of democracy and free speech, and the highest-level descriptive information about the Berlin Wall and K-pop)."
        },
        {
            "title": "6.2 Epistemic Diversity Across Time",
            "content": "First, we focus on RQ1: How has epistemic diversity in LLMs changed over time? We measure epistemic diversity across the 155 topics and 27 models mentioned previously, and plot HillShannon diversity (HSD) vs. model release date in Fig. 3. We plot traditional search diversity as baseline and separately plot IFT and RAG to see the impact of generation setting. We connect models over time based on their relative size and version. In terms of relative diversity for the topics we studied, evidence suggests that models are improving. This is particularly pronounced for the more recent models released after March 2025 (Gemma 3 and GPT-5), which show sharp increases in diversity. These upward trends occur primarily for Llama, Gemma, and OpenAI models; Qwen models are generally stagnant. Other exceptions include the larger (70B) Llama models, which trend downwards, and the larger (32-70B) Qwen models. Finally, we observe that generation setting (IFT or RAG) and model size appear to have significant impact on diversity; we explore these two factors more rigorously in the following sections. That being said, when looking at epistemic diversity compared to the search baseline, LLM outputs have low diversity. One can generally expect to encounter more diverse information about these topics by reading through the top-20 web pages rather than prompting an LLM in different ways. The search baseline is, in fact, quite weak baseline, as we use only the top 20 Google search results for each topic, compared to sampling from each language model with 200 unique prompts per topic. Further, as mentioned in 6.1, the search baseline is under-estimated relative to the LLM results. Therefore, while it is encouraging that LLMs Predictor (cid:98)β p-value Intercept (IFT) 1054.77(242.65) 1e3*** RAG Search +739.186(34.06) +1311.14(1284.00) < 0.05* 1e3*** Table 2: Estimated coefficients ( ˆβ) and p-values of linear mixed effects model with the setting (IFT, RAG, Search) as fixed effects and the model as random effects. The dependent variable is HSD. Both RAG and the search baseline lead to statistically significantly more diverse responses than IFT. Predictor (cid:98)β p-value Intercept (Med.) 1365.95(679.47) 1e3*** Small Large +277.415(51.88) 1e3*** -142.698(51.38) 1e3*** Table 3: Estimated coefficients ( ˆβ) and p-values of linear mixed effects model with the model size (Small, Medium, and Large) as fixed effects and the setting (IFT and RAG) as random effect. The dependent variable is HSD. We find an inverse relationship between model size and diversity. have become more epistemically diverse over time, their diversity relative to search baselines is often low, which may risk knowledge collapse."
        },
        {
            "title": "6.3 Epistemic Diversity Across Settings",
            "content": "Our next focus is on RQ2: What is the impact of generation setting on epistemic diversity? We look at epistemic diversity across three settings: parametric knowledge in instruction finetuned models (IFT), the same models with retrievalaugmented generation (RAG), and the traditional search engine baseline. From Fig. 3, we see that, on our topics, RAG appears to have strong positive impact on diversity, and search tends to be better than both IFT and RAG. To test if this is statistically significant, we use linear mixed effects regression model with HSD as the dependent variable, generation setting as categorical fixed effect, and the model as random effect in order to control for the baseline diversity of each model. From Table 2, we see that both RAG and the traditional search baseline yield significantly more diverse outputs than relying on parametric memory with IFT, at least for our topics. This highlights the potential of RAG in ensuring that models are epistemically diverse going forward. However, Figure 4: Heatmap of the Jensen-Shannon divergence (JSD) across models, based on the empirical probability distributions over clusters (pi) for each topic. higher JSD means that the distributions generated by the two models are more different. Open-weight models tend to be more similar to each other than to GPT. All LLMs are more different from the search baseline than to each other, indicating marked difference in the distribution of information in the search baseline from the LLMs. this may depend on RAG databases remaining human written. If traditional search platforms and RAG knowledge bases become dominated by LLMgenerated content, the diversity benefits of RAG could be erased, risking knowledge collapse. Therefore, given the benefits that RAG can endow to epistemic diversity, we recommend RAG databases remain diverse and prevent contamination from an overabundance of LLM generated text. Practitioners should also take extra care to prevent contamination of RAG sources from an overabundance of LLM-generated content."
        },
        {
            "title": "6.4 Epistemic Diversity Across Models",
            "content": "We now look at RQ3: How does model selection impact epistemic diversity? There are two aspects to consider here: the impact of model size and the similarity between model families. Firstly, from Fig. 3, we see that model size appears to have an unintuitive negative impact on epistemic diversity. To determine if the effect is statistically significant, we use linear mixed effects model with the HSD as dependent variable, the model size binned into three categories (Small, Medium, and Large; see Table 4 for how these are determined) as the independent variable, and generFigure 5: Average diversity per country across all models with bootstrapped 95% confidence intervals. Bars are sorted according to the difference between RAG and IFT diversity. Countries tend to have similar diversity to each other with instruction fine-tuning only. However, RAG appears to have an uneven impact on different countries, where the US and general topics see the most benefit ation setting (IFT or RAG) as random effect. The results in Table 3, highlight that the observed effect w.r.t. model size is statistically significant. This echoes contemporary work which has shown that larger models tend to memorize more (Morris et al., 2025), can show greater bias (Bai et al., 2025b), and can be less diverse (Zhang et al., 2025) than smaller models. Therefore, in settings where epistemic diversity is important (e.g., in viewpoint/opinion generation), smaller models may be better choice. But how do different models compare? We measure model similarity by measuring the JensenShannon divergence (JSD) between the distributions of meaning classes pi generated by the most diverse model from each family and average this across topics  (Fig. 4)  . We observe that the overall divergence is high across models for our topics. This suggests that prompting multiple models may improve epistemic diversity. Additionally, we observe that the open-weight models are more similar to each other than to GPT5 or the search baseline. Finally, we see that the greatest divergence across models is with the search baseline, suggesting that traditional search and LLMs produce largely different distributions of claims, at least for our topics."
        },
        {
            "title": "6.5 Epistemic Diversity Across Countries",
            "content": "Figure 6: Comparison of the diversity of claims generated by the models matched to English Wikipedia and local language Wikipedia. To capture minimal representativeness (Peterson, 2025), we match claims from Wikipedia in English and the local language, respectively, to claims generated by our models in the IFT setting, and restrict the measurement of HSD to only matched claims. Bars are sorted according to the difference between English and local language representativeness. vs. country as bar chart in Fig. 5. The country of each topic is selected either if (1) historical event occurred in particular country or (2) public figure is primarily associated with certain country. We plot separate bar charts for the IFT and RAG settings. For IFT only, the cultural context does not have strong impact on HSD, as most countries are within the same 95% confidence intervals, with the exception of Ethiopia, Saudi Arabia, and Brazil, which are higher. However, each country is impacted differently by RAG. In particular, the USA, India, Russia, France, and China, as well as general concepts, see more benefit from RAG. This is likely because the search we use to acquire documents for RAG is performed in the US region, which may under-represent topic-specific information from certain countries. To check this, we measure Pearsons correlation between the epistemic diversity of search and the difference between RAG diversity and IFT diversity, averaging by country. We find that these two are strongly correlated at 0.73 (p < 0.01, N=13), meaning that when the diversity of the RAG source for particular countrys set of topics increases, the improvement in diversity from IFT to RAG is likewise greater. Finally, we consider RQ4: Whose knowledge is represented by LLMs? through two subquestions. First, how does the cultural context of topic impact epistemic diversity? To answer, we plot HSD Second, to what extent are English and local language knowledge represented by the claims generated by LLMs? To answer this, we match LLM claims to Wikipedia claims in both English and the countrys local language using InfoGap (Samir et al. 2024, see Appendix A.5 for details). We restrict this experiment to 9 countries in our study where the local language Wikipedias are in the top 20 in terms of active users, with the exception of Saudi Arabia for which the local language pages had minimal content. In order to quantify the extent to which Wikipedia claims appear in model outputs, we measure the HSD of each model after filtering out claims that are not matched to either the English or local language Wikipedia. This is equivalent to minimal representativeness outlined by Peterson (2025), which defines that, for set of items that are relevant to particular task (in this case, claims which appear on either English or local language Wikipedia pages), any item should have at least some chance of appearing in the LLM output. Results are plotted as bar chart in Fig. 6, aggregated across models and topics for each country. For the selected topics, we observe statistically significant knowledge gap between English and local languages for 5 out of 8 countries, while for the remaining 3, the gap is not statistically meaningful. In no case is local representation statistically significantly greater than English representation. Additionally, representation for the USA specific topics are statistically significantly greater than every other country. This suggests that current English LLMs may fail to present knowledge which has not been presented in English for certain countries, potentiating risk in terms of knowledge erasure."
        },
        {
            "title": "7 Discussion and Conclusion",
            "content": "Our work is the first broad study of knowledge collapse in LLMs, where we propose new methodology for measuring epistemic diversity. We use this to conduct an empirical study of 27 LLMs across 155 topics, with 200 prompt variations. Limitations In terms of topical coverage, we use combination of randomly selected and hand curated topics. We attempted to select topics that broadly cover general concepts, historical events, and important figures across 12 countries, including both controversial and non-controversial topics. However, manual selection of topics may inevitably impact the generalizability of our results. Additionally, for some countries in the study, selection of topics is performed from an outsider perspective. An improvement to our setup could be to cover more topics by identifying what people typically search for using LLMs and randomly sampling from that pool of queries. Other notions of popularity can also be examined in terms of Wikipedia page views or trending search keywords. Regarding our decomposition and clustering algorithms, while we find through manual and automated evaluation that the quality tends to be high, performance is not perfect. Therefore, there is some irreducible noise in the results. Next, we choose RAG setup which is intended to simulate real-world RAG scenario, while actual RAG implementations will inevitably vary. It would therefore be useful to examine the utility of RAG in more systematic way, benchmarking several implementations. Finally, while epistemic diversity is important, it should also be contextualized with other important aspects of knowledge such as factuality and relevance. Conclusions Our results show that overall epistemic diversity is low in our pool of topics when compared to baseline traditional search. That being said, for most model families and sizes tested, we observe an encouraging trend of knowledge expansion, indicating that so far, LLMs do not appear to be locked in to their narrow epistemic frames. Our results also highlight that RAG and the use of smaller models can help stave off knowledge collapse going forward. Going deeper, we find that the choice of RAG database may be important for improving epistemic representation across cultural contexts, as not all country-specific topics see equal gains. Here, practitioners need to be cautious about expanding their RAG sources with LLM-generated content. Finally, compared to traditional knowledge source (Wikipedia) in both English and local languages for country-specific topics, we find that the claims generated in our study reflect English language knowledge more than local language knowledge, highlighting the need to investigate how local knowledge can be incorporated into LLM outputs. The general methodology presented in this paper can be used in the future to study epistemic diversity for any arbitrary set of topics, downstream tasks, and real-world use cases with open-ended plain-text LLM outputs. This allows researchers to answer research questions about which, whose, and how much knowledge LLMs are representing"
        },
        {
            "title": "Acknowledgements",
            "content": "DW is supported by Danish Data Science Academy postdoctoral fellowship (grant: 20231425). JM is supported by the Stanford Interdisciplinary Graduate Fellowship, the Stanford Center for Affective Science Graduate Fellowship, and the Future of Life Institute Vitalik Buterin PhD Fellowship. SY is supported in part by the Pioneer Centre for AI, DNRF grant number P1. This work is also partially funded by DFF Sapere Aude research leader grant under grant agreement No 0171-00034B."
        },
        {
            "title": "References",
            "content": "Suhaib Abdurahman, Mohammad Atari, Farzan Karimi-Malekabadi, Mona Xue, Jackson Trager, Peter Park, Preni Golazizian, Ali Omrani, and Morteza Dehghani. 2024. Perils and opportunities in using large language models in psychological research. PNAS nexus. A. J. Alvero, Jinsook Lee, Alejandra Regla-Vargas, René F. Kizilcec, Thorsten Joachims, and Anthony Lising Antonio. 2024. Large language models, social demography, and hegemony: comparing authorship in human and synthetic text. J. Big Data. Barrett R. Anderson, Jash Hemant Shah, and Max Kreminski. 2024. Homogenization Effects of Large Language Models on Human Creative Ideation. In Proceedings of the 16th Conference on Creativity & Cognition. ACM. Mohammad Atari, Mona Xue, Peter Park, Damián Blasi, and Joseph Henrich. Which humans? Hui Bai, Jan Voelkel, Shane Muldowney, Johannes Eichstaedt, and Robb Willer. 2025a. LLM-generated messages can persuade humans on policy issues. Nature Communications. Xuechunzi Bai, Angelina Wang, Ilia Sucholutsky, and Thomas Griffiths. 2025b. Explicitly unbiased large language models still form biased associations. Proceedings of the National Academy of Sciences. Anne Chao and Lou Jost. 2012. Coverage-based rarefaction and extrapolation: standardizing samples by completeness rather than size. Ecology. Giovanni Luca Ciampaglia, Azadeh Nematzadeh, Filippo Menczer, and Alessandro Flammini. 2018. How algorithmic popularity bias hinders or promotes quality. Nature Scientific Reports. Mark Coeckelbergh. 2025. LLMs, Truth, and Democracy: An Overview of Risks. Science and Engineering Ethics. Esin Durmus, Karina Nyugen, Thomas I. Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, Liane Lovitt, Sam McCandlish, Orowa Sikder, Alex Tamkin, Janel Thamkul, Jared Kaplan, Jack Clark, and Deep Ganguli. 2023. Towards Measuring the Representation of Subjective Global Opinions in Language Models. arXiv preprint arXiv:2306.16388. Subhabrata Dutta and Tanmoy Chakraborty. 2023. Thus spake chatgpt. Communications of the ACM. Martin Ester, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu. 1996. Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD). AAAI Press. Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. 2024. Detecting hallucinations in large language models using semantic entropy. Nature. Mark Hill. 1973. Diversity and Evenness: Unifying Notation and its Consequences. Ecology. Maurice Jakesch, Advait Bhat, Daniel Buschek, Lior Zalmanson, and Mor Naaman. 2023. CoWriting with Opinionated Language Models Affects Users Views. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. ACM. Lou Jost. 2006. Entropy and diversity. Oikos. Maurice Kendall. 1938. New Measure of Rank Correlation. Biometrika. Maurice Kendall. 1945. The treatment of ties in ranking problems. Biometrika. Jinsook Lee, A. J. Alvero, Thorsten Joachims, and René F. Kizilcec. 2025. Poor Alignment and Steerability of Large Language Models: Evidence from College Admission Essays. arXiv preprint arXiv:2503.20062. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. GEval: NLG evaluation using GPT-4 with better human alignment. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics. Lisa Messeri and Molly Crockett. 2024. Artificial intelligence and illusions of understanding in scientific research. Nature. Jared Moore, Tanvi Deshpande, and Diyi Yang. 2024. Are Large Language Models Consistent In Findings of over Value-laden Questions? the Association for Computational Linguistics: EMNLP. Association for Computational Linguistics. John X. Morris, Chawin Sitawarin, Chuan Guo, Narine Kokhlikyan, G. Edward Suh, Alexander M. Rush, Kamalika Chaudhuri, and Saeed How much do lanMahloujifar. 2025. arXiv preprint guage models memorize? arXiv:2505.24832. Tien T. Nguyen, Pik-Mai Hui, F. Maxwell Harper, Loren G. Terveen, and Joseph A. Konstan. 2014. Exploring the Filter Bubble: the Effect of Using Recommender Systems on Content Diversity. In 23rd International World Wide Web Conference, WWW. ACM. Andrew J. Peterson. 2025. AI and the problem of knowledge collapse. AI & Society. Tianyi Alex Qiu, Zhonghao He, Tejasveer Chugh, and Max Kleiman-Weiner. 2025. The Lock-in Hypothesis: Stagnation by Algorithm. arXiv preprint arXiv:2506.06166. Nils Reimers and Iryna Gurevych. 2019. Sentencebert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics. Michael Roswell, Jonathan Dushoff, and Rachael Winfree. 2021. conceptual guide to measuring species diversity. Oikos. Paul Röttger, Musashi Hinck, Valentin Hofmann, Kobi Hackenburg, Valentina Pyatkin, Faeze Brahman, and Dirk Hovy. 2025. IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in LLM Writing Assistance. arXiv preprint arXiv:2502.08395. Paul Röttger, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Kirk, Hinrich Schütze, and Dirk Hovy. 2024. Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics. Farhan Samir, Chan Young Park, Anjalie Field, Vered Shwartz, and Yulia Tsvetkov. 2024. Locating Information Gaps and Narrative Inconsistencies Across Languages: Case Study of LGBT People Portrayals on Wikipedia. In Empirical Methods in Natural Language Processing, EMNLP. Association for Computational Linguistics. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross J. Anderson, and Yarin Gal. 2024. AI models collapse when trained on recursively generated data. Nature. Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. 2025. Can LLMs Generate Novel Research Ideas? Large-Scale Human Study with 100+ In The Thirteenth InternaNLP Researchers. tional Conference on Learning Representations, ICLR 2025. Zhivar Sourati, Farzan Karimi-Malekabadi, Meltem Ozcan, Colin McDaniel, Alireza S. Ziabari, Jackson Trager, Ala N. Tak, Meng Chen, Fred Morstatter, and Morteza Dehghani. 2025a. The Shrinking Landscape of Linguistic Diversity in the Age of Large Language Models. arXiv preprint arXiv:2502.11266. Zhivar Sourati, Alireza Ziabari, and Morteza Dehghani. 2025b. The Homogenizing Effect of Large Language Models on Human Expression and Thought. arXiv preprint arXiv:2508.01491. Luning Sun, Yuzhuo Yuan, Yuan Yao, Yanyan Li, Hao Zhang, Xing Xie, Xiting Wang, Fang Luo, and David Stillwell. 2025. Large Language Models show both individual and collective creativity comparable to humans. Thinking Skills and Creativity. Christian Wagner and Ling Jiang. 2025. Death by AI: Will large language models diminish Wikipedia? Journal of the Association for Information Science and Technology. Dustin Wright, Arnav Arora, Nadav Borenstein, Srishti Yadav, Serge J. Belongie, and Isabelle Augenstein. 2024. LLM Tropes: Revealing FineGrained Values and Opinions in Large Language Models. In Findings of the Association for Computational Linguistics: EMNLP. Association for Computational Linguistics. Dustin Wright, Zain Muhammad Mujahid, Lu Wang, Isabelle Augenstein, and David Jurgens. 2025. Unstructured Evidence Attribution for Long Context Query Focused SummaIn Proceedings of the 2025 Conferrization. ence on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics. Weijia Xu, Nebojsa Jojic, Sudha Rao, Chris Brockett, and Bill Dolan. 2025. Echoes in AI: Quantifying Lack of Plot Diversity in LLM Outputs. arXiv preprint arXiv:2501.00273. Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Shaochen Zhong, Bing Yin, and Xia Ben Hu. 2024. Harnessing the Power of LLMs in Practice: Survey on ChatGPT and Beyond. ACM Transactions on Knowledge Discovery from Data. Yiming Zhang, Harshita Diddee, Susan Holm, Hanchen Liu, Xinyue Liu, Vinay Samuel, Barry Wang, and Daphne Ippolito. 2025. NoveltyBench: Evaluating Language Models for Humanlike Diversity. In Conference on Language Modeling (COLM). Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. 2024. WildChat: 1M ChatGPT Interaction Logs in the Wild. In The Twelfth International Conference on Learning Representations, ICLR 2024. Elise Li Zheng and Sandra Soo-Jin Lee. 2023. The Epistemological Danger of Large Language Models. The American Journal of Bioethics."
        },
        {
            "title": "A Replication Details",
            "content": "A.1 Evaluation Annotation Instructions Decomposition Quality You will be given short piece of text (around 3 sentences) about provided topic, and list of atomic claims. Your task is to annotate to what degree each claim is represented in the original short piece of text. Each claim will be rated on scale from 0 to 5, where each value has the following meaning: 0 - EMPTY (skip) 1 - The claim is totally irrelevant to the original piece of text OR does not explicitly talk about the provided TOPIC 2 - The claim is incomplete or somewhat included in the original piece of text and missing clearly important context 3 - The claim is included in the original piece of text but missing some potentially important context. This includes claims which could be inferred from the original context but arent explicitly stated (e.g., \"The lifecycle of plastic includes production.\" being inferred from \"It addresses the entire lifecycle of plastic, from production and consumption to disposal and recycling.\") 4 - The claim is included in the original piece of text and is missing only unimportant context (the most important information is represented) 5 - The claim is included in the original piece of text and no context is missing Decomposition Missing Claims In addition, after each group of claims you will be asked to enumerate any missing claims which are relevant to the topic. Please compare the text to the claims, and identify any relevant claims that were not listed. You just need to provide the NUMBER of claims that you estimate are missing (you do not need to write the claims down, just the number). Clustering Cohesion You will be shown groups of text. Your job is to rate each group of text for the degree to which each piece of text in the group conveys at least one piece of information in common. Each group will have maximum of 10 pieces of text and minimum of 3 pieces of text. Rate each group as follows: 1 - None of the sentences have any information in common 2 - Some of the sentences have information in common but most sentences convey something different 3 - About half of the sentences convey one thing in common 4 - Most of the sentences convey at least one piece of information in common 5 - All of the sentences convey at least one piece of information in common Clustering Missing Sentences You will then be shown 10 additional pieces of text. Your task will be to determine whether or not these pieces of text belong in the group. You will rate each piece of text as follows: 0 - The sentence does not convey any information in common with the group 1 - The sentence conveys the same common information shared by the group; the sentence may convey some more specific or extra information, but the core information of the group is conveyed by the sentence as well A.2 Prompts Here we show decomposition prompts P1  (Fig. 7)  and P2  (Fig. 8)  used in our evaluation. Decomposition prompt P3, which we use in the final experiments, is the same as P2 but includes three incontext examples. All three prompts can be found in our linked code repository.12 Please breakdown the following paragraph into list of independent facts. Please return the facts as list. Please do not number the list. Please simply separate each fact by new line character. {content} Figure 7: Decomposition prompt P1. A.3 Topics USA Donald Trump; Claude Shannon; Barack Obama; Frederick Douglass; Bob Dylan; Iran hostage crisis; 9/11; American civil war; Roe v. Wade; Independence Day (United States); Argentina Javier Milei; Juan Domingo Perón; Cristina Fernández de Kirchner; Jorge Rafael Videla; Diego Maradona; Falklands War; Infamous 12https://github.com/dwright37/llm-knowledge am going to show you piece of TEXT that talks about the following ISSUE: {issue}. Please read every single sentence in the TEXT and then decompose each sentence into individual, atomic STATEMENTS. Please make the STATEMENTS totally faithful to the original TEXT. Please only include STATEMENTS about issue. If there are no sentences in the TEXT which include statements about issue, please just say EMPTY. Please ignore sentences which talk about the TEXT itself, for example \"In sum, we have talk about {issue}.\" Please only include STATEMENTS which are fully supported by the TEXT. Please dont hallucinate information. Please do not repeat STATEMENTS. Please return the STATEMENTS as list. Please do not number the list. Please simply separate each STATEMENT by new line character. Please double check that each of the STATEMENTS in the list is entailed and factually accurate with the text. If the TEXT is presented as list of bullet points, please make sure to decontextualize the bullet points (dont just list them). Here is the text: {content} Figure 8: Decomposition prompt P2. Decade (Argentina); May Revolution; 19982002 Argentine great depression; Cadet scandal in Argentina; Ethiopia Kenenisa Bekele; Sophia Bekele; Meles Zenawi; Mengistu Haile Mariam; Kebede Michael; History of Ethiopia; Ethiopian Civil War; Tigray War; History of the Federal Democratic Republic of Ethiopia; COVID-19 pandemic in Ethiopia; Korea Syngman Rhee; Yuna Kim; Kim Soo-hyun; Sejong the Great; Ban Ki-moon; Gwangju uprising; Seollal; South Korean March First Movement; K-pop; Sinking of MV Sewol; Russia Vladimir Putin; Joseph Stalin; Fyodor Dostoevsky; Alexei Navalny; Garry Kasparov; October Revolution; Soviet Invasion of Poland; Sputnik 1; 1993 Russian constitutional crisis; Wagner Group rebellion; France Charles de Gaulle; Marine le Pen; Albert Camus; Michel Foucault; Napoleon; Dreyfus affair; the French Revolution; Charlie Hebdo shooting; Arenc affair; Yellow vests protests in France; India Jallianwala Bagh massacre; Partition of India; 2002 Gujarat riots; Indian Rebellion of 1857; Narendra Modi; Jalalud-din Muhammad Akbar; B. R. Ambedkar; Indira Gandhi; Vallabhbhai Patel; Non-cooperation movement (19191922); 2008 Mumbai attacks; Saudi Arabia Assassination of Jamal Khashoggi; Al-Yamamah arms deal; Khobar Towers bombing; Proclamation of the Kingdom of Saudi Arabia; Riyadh International Book Fair; Ibn Saud; Hatoon al-Fassi; Manal al-Sharif; Ayatollah Sheikh Nimr Baqir al-Nimr; South Africa 2021 South African unrest; 1999 Tempe military base shooting; Soweto uprising; Miriam Makeba; Nadine Gordimer; Nelson Mandela; Evelyn Mase; Christiaan Barnard; Crizelda Brits; Rand Rebellion; South African Border War Brazil Paraguayan War; 1937 Brazilian coup détat; Revolution of the Ganhadores; Brazilian Carnival; Dilma Rousseff; Pedro II of Brazil; José Paranhos, Viscount of Rio Branco; Indigenous peoples in Brazil; Carmen Miranda; Carlos Chagas; Mensalão scandal; China 1989 Tiananmen Square protests and massacre; 20192020 Hong Kong protests; Annexation of Tibet; Qingming Festival; 2010 Yushu earthquake; Jinan incident; Du Fu; Xi Jinping; Chinese Communist Party; Ai Weiwei; Soong Ching-ling; Germany Berlin Wall; Nuremberg trials; Night of the Long Knives; Clara Josephine Schumann; Wilhelm Richard Wagner; Frederick the Great; Karl Marx; Eschede train disaster; Frauke Petry; Ernst Nolte; General nuclear weapons; slavery; pornography; marriage; white supremacy; international relations; prisons; domestic violence; patriotism; same-sex marriage; free speech; political corruption; universal basic income; global hunger; plastic waste; political correctness; fascism; racism; colonialism; the impact of climate change; democracy; feminism; human rights; genocide; war; censorship; artificial intelligence; renewable energy; capitalism; autonomous vehicles; misinformation; affirmative action; A.4 Model Details Table 4 provides the details of the 4 model families employed in this study, along with demarcation of small, medium, and large sets. A."
        },
        {
            "title": "InfoGap Details",
            "content": "We use GPT 5 to decompose English and local language Wikipedias to individual claims, retrieve candidate matches to LLM generated claims using multilingual sentence embedding model,13 and predict if the retrieved multilingual claims match to the LLM generated English claims using GPT 5 (Samir et al., 2024). We exclude claims generated using RAG to avoid Wikipedia text being included in the generation context. A.6 Generation Details We retrieve 20 Google search results for each topic. Over the 155 topics, these 3,200 pages have an average of 12,836 and median of 2,132 tokens per page. For fair comparison, we allow each model to generate up to 2,100 tokens for each of the 200 prompt variations. For RAG, we include up to 1,000 additional tokens for context. 13HuggingFace: sentence-transformers/LaBSE Family Ver. Size Release Date Endpoint 1.5 1.5 1.5 2.5 2.5 2.5 3 3 3 1 1.1 2 2 2 3 3 3 2 2 2 3.1 3.1 3.2 3. 7B (S) 14B (M) 72B (L) 7B (S) 14B (M) 72B (L) 8B (S) 14B (M) 32B (L) 2B (S) 7B (M) 2B (S) 9B (M) 27B (L) 1B (S) 12B (M) 27B (L) 7B (S) 13B (M) 70B (L) 8B (S) 70B (L) 3B (S) 70B (L) Qwen"
        },
        {
            "title": "Llama",
            "content": "February 2024 Qwen/Qwen1.5-7B-Chat Qwen/Qwen1.5-14B-Chat Qwen/Qwen1.5-72B-Chat September 2024 Qwen/Qwen2.5-7B-Instruct Qwen/Qwen2.5-14B-Instruct Qwen/Qwen2.5-72B-Instruct Qwen/Qwen3-8B Qwen/Qwen3-14B Qwen/Qwen3-32B April 2025 February 2024 April 2024 July 2024 June 2024 March google/gemma-2b-it google/gemma-1.1-7b-it google/gemma-2-2b-it google/gemma-2-9b-it google/gemma-2-27b-it google/gemma-3-1b-it google/gemma-3-12b-it google/gemma-3-27b-it July 2023 meta-llama/Llama-2-7b-chat-hf meta-llama/Llama-2-13b-chat-hf meta-llama/Llama-2-70b-chat-hf meta-llama/Llama-3.1-8B-Instruct meta-llama/Llama-3.1-70B-Instruct September 2024 meta-llama/Llama-3.2-3B-Instruct December 2024 meta-llama/Llama-3.3-70B-Instruct July 2024 OpenAI**"
        },
        {
            "title": "3.5 Undisclosed November 2022\n4\n5",
            "content": "Undisclosed August 2024 Undisclosed August 2025 gpt-3.5-turbo-0125 gpt-4o-2024-08-06 gpt-5-2025-08-07 Table 4: List of different model versions (Ver.), sizes, year of release, along with model endpoints used either from HuggingFace or the respective API endpoint. ** is for closed-weight models. In terms of model size: Small 8B, 9 Medium< 27B and Large 27B. As Gemma models tended to be smaller we include one 7B model (Gemma 1.1 7B) in the Medium category for Fig. 3."
        }
    ],
    "affiliations": [
        "GitHub",
        "University of Copenhagen"
    ]
}