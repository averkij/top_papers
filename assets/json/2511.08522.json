{
    "paper_title": "AlphaResearch: Accelerating New Algorithm Discovery with Language Models",
    "authors": [
        "Zhaojian Yu",
        "Kaiyue Feng",
        "Yilun Zhao",
        "Shilin He",
        "Xiao-Ping Zhang",
        "Arman Cohan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models have made significant progress in complex but easy-to-verify problems, yet they still struggle with discovering the unknown. In this paper, we present \\textbf{AlphaResearch}, an autonomous research agent designed to discover new algorithms on open-ended problems. To synergize the feasibility and innovation of the discovery process, we construct a novel dual research environment by combining the execution-based verify and simulated real-world peer review environment. AlphaResearch discovers new algorithm by iteratively running the following steps: (1) propose new ideas (2) verify the ideas in the dual research environment (3) optimize the research proposals for better performance. To promote a transparent evaluation process, we construct \\textbf{AlphaResearchComp}, a new evaluation benchmark that includes an eight open-ended algorithmic problems competition, with each problem carefully curated and verified through executable pipelines, objective metrics, and reproducibility checks. AlphaResearch gets a 2/8 win rate in head-to-head comparison with human researchers, demonstrate the possibility of accelerating algorithm discovery with LLMs. Notably, the algorithm discovered by AlphaResearch on the \\emph{``packing circles''} problem achieves the best-of-known performance, surpassing the results of human researchers and strong baselines from recent work (e.g., AlphaEvolve). Additionally, we conduct a comprehensive analysis of the remaining challenges of the 6/8 failure cases, providing valuable insights for future research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 2 2 5 8 0 . 1 1 5 2 : r AlphaResearch: Accelerating New Algorithm Discovery with Language Models"
        },
        {
            "title": "DISCOVERY WITH LANGUAGE MODELS",
            "content": ": ACCELERATING NEW ALGORITHM Zhaojian Yu1, Kaiyue Feng2, Yilun Zhao3, Shilin He4, Xiao-Ping Zhang1, Arman Cohan3 1Tsinghua University 2New York University 3Yale University 4ByteDance"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models have made significant progress in complex but easy-toIn this verify problems, yet they still struggle with discovering the unknown. paper, we present AlphaResearch, an autonomous research agent designed to discover new algorithms on open-ended problems. To synergize the feasibility and innovation of the discovery process, we construct novel dual research environment by combining the execution-based verify and simulated real-world peer review environment. AlphaResearch discovers new algorithm by iteratively running the following steps: (1) propose new ideas (2) verify the ideas in the dual research environment (3) optimize the research proposals for better performance. To promote transparent evaluation process, we construct AlphaResearchComp, new evaluation benchmark that includes an eight open-ended algorithmic problems competition, with each problem carefully curated and verified through executable pipelines, objective metrics, and reproducibility checks. AlphaResearch gets 2/8 win rate in head-to-head comparison with human researchers, demonstrate the possibility of accelerating algorithm discovery with LLMs. Notably, the algorithm discovered by AlphaResearch on the packing circles problem achieves the best-of-known performance, surpassing the results of human researchers and strong baselines from recent work (e.g., AlphaEvolve). Additionally, we conduct comprehensive analysis of the remaining challenges of the 6/8 failure cases, providing valuable insights for future research.1 Figure 1: Comparison of OpenEvolve (with program-based reward), ShinkaEvolve (with programbased reward) and AlphaResearch (with program-based and peer-review reward). We run three agents on Packing Circles (n=26) problems. AlphaResearch achieves better performance than others. Equal contribution. 1Code, data and models are available at https://github.com/answers111/alpha-research. 1 AlphaResearch: Accelerating New Algorithm Discovery with Language Models Figure 2: The launch of AlphaResearch contains two steps. (1) Train reward models with realworld peer-reviewed records. (2) Prepare initial research proposals, initial programs and evalution program. AlphaResearch will refine the research proposals and programs autonomously."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent progress has shown that frontier LLMs like GPT-5 (OpenAI, 2025) and Gemini 2.5 (Comanici et al., 2025) could achieve expert-level performance in complex tasks such as mathematics (Trinh et al., 2024; Lin et al., 2025) and programming (Jimenez et al., 2024; Jain et al., 2025). While LLMs excel at processing and reasoning on problems that are within the boundary of existing human knowledge (Wang et al., 2024b; Phan et al., 2025), their capacity for independent discovery that pushes the boundaries of human knowledge still remains question of paramount importance (Novikov et al., 2025). Can these models create advanced knowledge or algorithms that surpass human researchers? Previous studies demonstrate that LLMs can generate novel ideas at human expert level (Si et al., 2024; Wang et al., 2024a). However, the outcome evaluation of LLM-generated research ideas still struggles with biased verification methods (Ye et al., 2024) that constrain the exploration of novel machine knowledge, such as LLM-as-a-judge (Lu et al., 2024), where misaligned LLMs are used to evaluate fresh ideas and inevitably favor solutions within existing knowledge boundaries. Furthermore, the ideationexecution gap (Si et al., 2025) between generating and executing on new ideas also hinders models from producing advanced research outcomes. Moreover, prior attempts at autonomous algorithm discovery face fundamental tension. Execution-based verification systems like AlphaEvolve Novikov et al. (2025) can rigorously validate whether code runs and meets constraints, but this verification alone might not be completely sufficient for discovery. For example, these systems could converge on technically correct but scientifically uninteresting or less impactful solutionscode that executes successfully yet offers no advancement over existing methods. Conversely, idea-generation systems evaluated purely by LLM judges can propose innovative concepts that prove computationally infeasible or violate problem constraints when implemented. The absence of real-world research environment rewards in execution-based agents and execution-based reward in idea-generation systems renders the discovery of new knowledge and algorithms challenging for current autonomous research agents (Tian et al., 2024). and algorithms challenging for current autonomous research agents (Tian et al., 2024). To combine the feasibility and innovation of the algorithm discovery process, we introduce AlphaResearch, an autonomous research agent that could discover new advanced algorithms with suite of research skills including idea generation and code implementation that could interact with the environment. To synergize these research skills during the discovery process, we construct 2 AlphaResearch: Accelerating New Algorithm Discovery with Language Models novel dual research-based environment (Tian et al., 2024), where novel insights are forged by the simulated real-world peer-reviewed environment and execution-based verification. We use this dual environments to accelerate the discovery process because many research ideas can be evaluated before even implementing and executing on the idea. based on factors such as novelty, literature and the knowledge used. Specifically, we (1) train reward model AlphaResearch-RM-7B with realworld peer-reviewed records, addressing the limitation of prior coding-only approaches that lack real-world research feedback, and use it to score the fresh ideas generated by LLMs; (2) construct an automatic program-based verifiable environment that executes these ideas with an interpreter. This dual environment facilitates rigorous algorithm discovery process for autonomous research agents. As illustrated in Figure 2, AlphaResearch discovers new algorithms by iteratively running the following steps: (i) proposing new research ideas, (ii) verify the ideas in the dual researchbased environment, and (iii) optimizing the proposals for higher reward from the environment. The synergy between an iterative real-world peer review environment and program-based verification empowers AlphaResearch to continuously accept novel research ideas and verify them via program execution. Once the generated optimal program surpasses current human-best achievements, these validated novel ideas could form feasible algorithms, thereby pushing the boundaries of human research forward. Comparable systems such as AlphaEvolve (Novikov et al., 2025) have not publicly disclosed all the test problems so far. To compare our AlphaResearch with human researchers for novel algorithm discovery, we construct AlphaResearchComp, simulated discovery Competition between research agents and human researchers, by collecting 8 open-ended research problems and their bestof-human records (shown in Appendix D). Our results demonstrate that AlphaResearch surpasses human researchers on two problems but fails on the other six. The novel algorithms discovered by AlphaResearch not only surpass best-of-human performance but also significantly outperform the state-of-the-art results achieved by AlphaEvolve. Specifically, AlphaResearch optimize the result of Packing Circles (n=32) problem to 2.939, where the goal is to pack disjoint circles inside unit square so as to maximize the sum of their radii. This result surpasses both the best humandesigned solutions and the previous state-of-the-art performance obtained by AlphaEvolve. These entirely novel ideas and algorithms constitute the most advanced solutions currently present in the human knowledge base, demonstrating the feasibility of employing LLMs to advance the frontiers of human knowledge. The six failure modes in AlphaResearchComp demonstrate the challenges for the autonomous algorithm discovery with research agents. We analyze the benefits and remaining challenges of autonomous research agents for knowledge discovery, providing valuable insights for future work."
        },
        {
            "title": "2.1 OVERVIEW",
            "content": "AlphaResearch discovers novel algorithms by continuously optimizing the research outcome from reward model that synergizes rigorous program verification and simulated real-world peer review environment. As shown in Figure 2, given initial idea i0 and program p0, AlphaResearch runs the program p0 with execution, producing r0, which represents the initial overall rating. The triplet (i0, p0, r0) will be fed to AlphaResearch for subsequent processing, including newer idea generation, idea verification (rejected by AlphaResearch-RM), and program-based execution. When reaching point where execution output rn surpasses the previous rating, AlphaResearch will save the triplet (ibest, pbest, rbest) as the best record. We repeat the process until rbest surpasses the best-of-human score, or the maximum round is reached. The resulting trajectory is denoted as τ = i0p0r0...in1pn1rn1inpnrn, where is the total rounds."
        },
        {
            "title": "2.2 ACTIONS",
            "content": "New Idea Generation. For each step k, AlphaResearch start with generating new idea ik based on sampled previous step (it, pt, rt) from previous trajectory τk1 = i0p0r0...ik1pk1rk1. This process can be denoted as: ik PA(it pt rt) (1) 3 AlphaResearch: Accelerating New Algorithm Discovery with Language Models (it, pt, rt) P(τk1) ik PA(it pt rt) if RM(ik) < threshold then Algorithm 1 AlphaResearch Require: initial idea i0, initial program p0, initial result r0, model A, evaluation program E(), maximum iteration rounds n, 1: τ0 (i0, p0, r0), rbest = 0 2: for = 1 to do do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end for 15: return (ibest, pbest, rbest) end if pk PA(pt ik) rk E(pk) if rk > rbest then States Sampling New Idea Generation (Eq. 1) Program Generation (Eq. 2) Program-based Execution end if τk τk1 ik pk rk (ibest, pbest, rbest) = (ik, pk, rk) Reward Model for New Idea Trajectory Update (Eq. 3) Initialization continue where means concatenation, is the sampled step from trajectory τi1. We use reward model to filter out high-quality ideas overall. If RM(in) outputs negative score, we reject the idea and skip subsequent actions in this round. Program-based Verification. After obtaining the fresh idea, AlphaResearch generates new program pk based on the previous implementation pt and new idea ik next: pk PA(pt ik) (2) and yield the evaluation result rk by verifying pk with code executor rk E(pk). Then, we update the trajectory τk with the newly generated idea ik, program pk and result rk: We repeat the above interaction process until reaches the maximum rounds and get the best result (ibest, pbest, rbest) as final output. τk τk1 ik pk rk (3)"
        },
        {
            "title": "2.3.1 REWARD FROM REAL-WORLD RESEARCH RECORDS",
            "content": "Existing autonomous idea generation process suffers from trade-off where highly novel research ideas may lack feasibility (Guo et al., 2025). To address this gap and ensure the feasibility of idea candidates, we train reward model with ideas from real-world peer-review information to simulate the real-world peer-review environment. Dataset For Reward Model. To train our reward model (RM) to identify good ideas, we collect all ICLR peer review records from 2017 to 2024 as our training set. We sample subset of ICLR 2025 records as test set, where the dates of train and test are disjoint, which prevents knowledge contamination between the train and test split. We select Qwen2.5-7B-Instruct as our base model, whose release date (Sept, 2024) is earlier than the ICLR 2025 author-reviewer rebuttal period (Oct, 2024). For each record in the training dataset, we extract the abstract part as RM input and wrap the average peer-review overall ratings with boxed{} as RM output. We fine-tune Qwen2.5-7BInstruct with the RM pairs, yielding the AlphaResearch-RM-7B model. Can LLMs Identify Good Ideas? To simplify the RM evaluation, we binarize the RM output score according to the ICLR 2025 Reviewer Guide: records with an average overall rating > 5.5 are treated as positive, while those with ratings 5.5 are treated as negative. We compute the binary classification accuracy and evaluate three models (GPT-5, Qwen2.5-Coder-Instruct, and AlphaResearch-RM-7B) on the AlphaResearch-RM test set. To benchmark human performance, three of the authors independently assessed 100 randomly sampled examples, and we report their 4 AlphaResearch: Accelerating New Algorithm Discovery with Language Models Table 1: Dataset for reward model training. We use the end of author-reviewer rebuttal period as the latest knowledge date. Table 2: Evaluation results of RM. We use the more recent date between the model release date and the dataset cutoff as the latest date."
        },
        {
            "title": "Records\nRange\nNum\nStart Date\nEnd Date",
            "content": "ICLR 20172024 24,445 2016-11 2023-12 ICLR 2025 100 2024-10 2024-"
        },
        {
            "title": "Acc",
            "content": "Random (theoretical) Human Annotator - - GPT-5 (medium) 2025-08 Qwen2.5-7B-Instruct 2024-09 AlphaResearch-RM-7B 2024-09 50.0% 65.0% 53.0% 37.0% 72.0% Table 3: Problem overview in AlphaResearchComp. Detailed definitions, baseline values, and references for each problem are provided in the Appendix D."
        },
        {
            "title": "Human Researcher",
            "content": "packing circles (n=26) packing circles (n=32) minimizing max-min distance raio (d=2, n=16) third autocorrelation inequality spherical code (n=30) autoconvolution peak minimization (upper bound) littlewood polynomials (n=512) MSTD (n=30) 2.634 2.936 12.89 1.4581 David Cantrell (2011) Eckard Specht (2012) David Cantrell (2009) Carlos Vinuesa (2009)"
        },
        {
            "title": "0.67365 Hardin & Sloane (1996, 2002)",
            "content": "0.755 32 1.04 MatolcsiVinuesa (2010) RudinShapiro (1959/1952) Hegarty (2006/2007) average scores. Table 2 presents the evaluation results that eliminate the knowledge contamination, highlighting the following observations: (1) Both GPT-5 and Qwen2.5-7B-Instruct have lower than 50% accuracy when identifying the good ideas from ICLR 2025 records. (2) After fine-tuned with ideas from previous ICLR peer-review information, AlphaResearch-RM-7B demonstrates 72% classification accuracy on unseen ICLR 2025 ideas, significantly outperforming baseline models and human experts. Based on these observations, we use the fine-tuned AlphaResearch-RM-7B as the final RM to simulate real-world peer-review environment and filter out good ideas generated by AlphaResearch."
        },
        {
            "title": "2.3.2 REWARD FROM PROGRAM-BASED EXECUTION",
            "content": "Inspired by AlphaEvolve (Novikov et al., 2025), we construct automatic evaluation process with code executor where each new program pk generated by AlphaResearch will be captured and evaluated. The evaluation program E() includes two modules: (i) Verification module that validates whether pk conforms to the problem constraints. (ii) Measurement module that output the score rk of program performance. The program output rk will be injected to the idea generation prompt (if sampled), thereby participating in the optimization process for fresh ideas. These programs and results are stored in candidates pool, where the primary goal is to optimally resurface previously explored ideas in future generations. The verifiable reward by code executor significantly simplify the action spaces of AlphaResearch, thereby enhancing the efficiency of the discovery process."
        },
        {
            "title": "3 ALPHARESEARCHCOMP",
            "content": "Problems collection. AlphaEvolve has not publicly disclosed all the test problems so far. To provide more transparent evaluation, we curate and open source set of 8 frontier program-based research tasks spanning geometry, number theory, harmonic analysis, and combinatorial optimization. These problems were selected based on the following principles: Well-defined Objectives. Each task has precise mathematical formulation with an objective function that admits rigorous automatic evaluation. 5 AlphaResearch: Accelerating New Algorithm Discovery with Language Models Table 4: Results on AlphaResearchComp. inidicates that higher score is better and for lower."
        },
        {
            "title": "AlphaResearch\nbest",
            "content": "init Excel@best packing circles (n=26) packing circles (n=32) minimizing max-min distance ratio third autocorrelation inequality spherical code (d=3, n=30) autoconvolution peak minimization littlewood polynomials (n=512) MSTD (n=30) 2.634 2.936 12.89 1.458 0.6736 0.755 32 1.04 0 0 15.55 35.746 0.5130 1.512 32 1.04 2.636 2.939 12.92 1.546 0.6735 0.756 32 1. 0.32% 0.10% -0.23% -6.03% -0.01% -0.13% 0 0 Known Human-best Baselines. For every problem, we provide the best-known human result from the literature. These represent conjectured best-known values rather than proven optima, ensuring ample room for further improvement. Table 3 presents the overview of the curated problems. They are either refined from prior work (e.g., AlphaEvolve) or collected from online repositories and domain experts. Each problems baseline is supported by verifiable resources in the corresponding field. This design enables AlphaResearch to demonstrate both the reproducibility of established mathematical results and the potential for discovery beyond current human-best achievements. Initialization Strategy. After obtaining the research problems of AlphaResearchComp, we construct diverse initial states for each problem with the following strategies: (1) For the Packing Circles (n=26) and Packing Circles (n=32) problems, we initialize them with null programs (r0 = 0) to simulate researches starting from scratch.(2) For the Littlewood Polynomials and MSTD (n=30) problems, we directly adopt the best-known solutions (r0 = rhuman) from human researchers to emulate improvements upon established methods. (3) For the remaining problems, we employ moderate initialization strategy (0 < r0 < rhuman) to ensure sufficient room for the research agent to explore. This initialization strategy simulates variety of real-world scenarios for the research agent, thereby facilitating thorough evaluation process. Metrics. For benchmarks like code generation with good verification techniques (e.g., unit tests), pass@k (Chen et al., 2021) is metric denoting that at least one out of i.i.d. task trials is successful, which captures the ability of LLMs to solve easy-to-verified problems. For open-ended real-world algorithm discovery tasks, we propose new metric - excel@best (excel at best), defined as the percentage excess on baseline (best of human level) results: excel@best ="
        },
        {
            "title": "E\nProblems",
            "content": "(cid:20) rbest rhuman Id rhuman (cid:21) (4) where rhuman indicates the results of human best level. Id indicates the optimization direction where Id = 1 represents that higher score is better and Id = 1 represents lower."
        },
        {
            "title": "4.1 SETUP",
            "content": "We select o4-mini, strong but cost-efficient LLM as our research agent and run AlphaResearch on each problem to get the best algorithm. We perform supervised finetuning on Qwen-2.5-7BInstruct (Yang et al., 2025) with the collected ICLR papers, yielding AlphaResearch-RM-7B. We do not compute loss on paper information, only on the predicted rating scores within the context. For fine-tuning, we adopt learning rate of 1e-5 with linear warm-up for 100 steps. All models are trained for 2 epochs with batch size of 128, using bfloat16 precision under FSDP. All other unspecified hyperparameters follow the default settings of the Hugging Face Trainer 2. 2https://huggingface.co/docs/transformers/main_classes/trainer 6 AlphaResearch: Accelerating New Algorithm Discovery with Language Models Figure 3: Execution-based reward of AlphaResearch on packing circles (n=26) problem (left) and third autocorrelation inequality problem (right)."
        },
        {
            "title": "4.2 RESULTS",
            "content": "LLMs could sometimes discover new algorithms themselves. Table 4 presents the results of AlphaResearchComp on 8 algorithms discovery problems. AlphaResearch achieved 2/8 win rate (excel@best > 0) against human researchers, with the algorithm discovered one notable success: by AlphaResearch for Packing Circles problem reaches the best-of-known performance (2.636 for n=26, 2.939 for n=32), outperforming human researchers (2.634 for n=26, 2.936 for n=32) and AlphaEvolve (2.635 for n=26, 2.937 for n=32). LLMs can refine their research ideas autonomously. AlphaResearch discovers advanced algorithm by iteratively propose and verify new research ideas. As shown in Table 4, 6/8 problems demonstrate consistent improvement throughout the discovery process. Figure 3 presents two examples of the reward trend in AlphaResearch, where the execution-based reward initially grows rapidly, then slowly plateaus for optimal performance seeking. This improvement trend emphasizes the autonomous discovery ability of research agents. Figure 4: The idea comparison between execution-only research agent and AlphaResearch where AlphaResearch-RM-7B are used. The discovery of superhuman algorithms remains challenging for LLMs. Despite exhibiting continuous reward growth, AlphaResearchs performance still underperforms human researchers in 6 out of 8 problems. We initialize AlphaResearch with the best-of-known solution from human researchers on Littlewood polynomials and MSTD(n=30) problems, where AlphaResearch didnt show an increase in execution-based rewards. This indicates that current LLMs still struggle to consistently find better algorithms than human researchers."
        },
        {
            "title": "4.3 ABLATIONS AND ANALYSIS",
            "content": "AlphaResearch vs. OpenEvolve. As shown in Figure 1, we compare AlphaResearch, OpenEvolve (Sharma, 2025) and ShinkaEvolve (Lange et al., 2025) on packing circles (n=26) problem at the first 500 steps for simplicity. AlphaResearch achieves better performance than OpenEvolve and slightly surpasses ShinkaEvolve, which demonstrates its advantages on accelerating algorithm discovery. 7 AlphaResearch: Accelerating New Algorithm Discovery with Language Models Figure 5: Reward overview during the discovery process. Each action in AlphaResearch will obtain 3 kinds of reward: (1) idea scrapping due to lower RM score than threshold, (2) idea execution successes, and (3) idea execution fails. AlphaResearch vs. Execution-based agent that propose fresh ideas without idea verification. To compare AlphaResearch with execution-only agent that propose fresh ideas without idea verification, we utilize AlphaResearch-RM-7B to evaluate the novelty of ideas generated by the executiononly agent and ideas produced by AlphaResearch. As illustrated in Figure 4, the ideas generated by AlphaResearch generally achieves higher scores than execution-only research agent. This illustrates that AlphaResearch tends to generates better ideas to get higher peer review reward, thus facilitating more effective research optimization process. Analysis of the discovery process. We analyze the reward distribution in AlphaResearch discovery process. As shown in Figure 5, approximately 30%40% of newly proposed ideas fall below the RM threshold and are thus discarded. The remaining ideas are executed, with the success rate of execution largely depending on the inherent characteristics of the problems. For exmample, the execution success rate on Packing Circles problem is 28.9%, whereas it reaches 51.7% on the Third Autocorrelation Inequality problem. Figure 3 illustrates the execution-based rewards for these two examples in AlphaResearch. Despite the substantial variations in execution success rates, the execution-based rewards in both cases exhibit consistent increasing trend. These findings demonstrate the interactions between LLM-based autonomous research agent and real-world environments. Figure 6: The impact of real-world peer review environment on execution results. AlphaResearchRM-7B filters 151 bad ideas where 108 ideas fail to executed and 43 successes. The impact of real-world peer-review environment. To assess the effectiveness of reward from simulated real-world peer-view environment, we ablate AlphaResearch-RM-7B at the first 400 iterations on Packing Circles problem. Figure 6 presents the execution results of w/ and w/o AlphaReasearch-RM-7B during the discovery process. Compared to the baseline without RM, AlphaResearch-RM-7B successfully filtered 151 ideas below the threshold. This process yielded 108 correct rejections of execution failures while making 43 erroneous rejections of viable ideas. AlphaResearch attained an accuracy of 71.5% (108/151), result that aligns closely with its perfor8 AlphaResearch: Accelerating New Algorithm Discovery with Language Models Figure 7: We show an example of formatted task of AlphaResearch. mance on the AlphaResearch-RM test set, as shown in Table 2. This outcome effectively demonstrates the models generalization capabilities and the efficacy of incorporating feedback from simulated real-world peer-review environment."
        },
        {
            "title": "4.4 CASE STUDY",
            "content": "We select the successful example from AlphaResearch to better understand the discovery process. Well consider the problem Packing Circles where the goal is to pack disjoint circles inside unit square so as to maximize the sum of their radii, shown in Figure 7. We first initialize AlphaResearch with original research proposal and an related program that return the list of circles (x, y, r) as output, as shown in Appendix D.4. The verification program first employs verify_circles function to check if the outputs of initial program meet the problem constraints (e.g., all circles are inside unit square) and evaluate function to output the sum of their radii. The metadata including: (1) research ideas, (2) programs, (3) and execution results are subsequently preserved as candidates which represents the end of one step. At the next step, AlphaResearch will sample from the candidate pool and generate new idea to improve the research proposals from the sampled metadata. After generating the new research ideas, AlphaResearch will further generate patch to modify the existing program if the idea obtain postive score from AlphaResearch-RM. The new program is then evaluated by the same verification program, thereby generating new metadata. We select the best program and idea as the final solution of AlphaResearch in this iterative process. s"
        },
        {
            "title": "5 RELATED WORK",
            "content": "LLMs for New Idea Generation and Verification. Several recent works explored methods to improve research idea generation, such as iterative novelty refinement (Wang et al., 2024a; Baek et al., 2024). These works focus on improving the research idea over vanilla prompting but critically 9 AlphaResearch: Accelerating New Algorithm Discovery with Language Models miss an effective verification method. To promote more reliable AI genrated research ideas, many studies have proposed solutions from different perspectives, such as comparisons with any human expert (Si et al., 2024), using LLMs for executing experiments by generating code with humancurated research problems (Huang et al., 2024; Tian et al., 2024) and executing LLM-generated research ideas with LLM-generated programs (Li et al., 2024; Lu et al., 2024; Aygün et al., 2025). These works either use automatic program evaluation or misaligned LLM evaluator method, which presents challenge for their scalability to real-world advanced algorithm discovery. Our AlphaResearch presents more feasible direction by combining program execution with RM training from real-world peer-reviewed research records. LLMs for Code Generation. In autonomous research agents, code generation serves as fundamental step. Previous models (Guo et al., 2024; Yu et al., 2023; Hui et al., 2024) and benchmarks (Chen et al., 2021; Yu et al., 2025) for code generation are in longstanding pursuit of synthesizing code from natural language descriptions. SWE-Bench (Jimenez et al., 2024) introduces the problems in real-world software development. Many studies on SWE-Bench have greatly contributed to the emergence of coding agents like SWE-Agent (Yang et al., 2024) and OpenHands (Wang et al., 2025). These agent framework greatly facilitate the training of agentic LLMs like Kimi-K2 (Team et al., 2025) and GLM-4.5 (Zeng et al., 2025). The surge of these models on SWE-Bench underscores critical need to reassess the future directions of coding agent research. Our AlphaResearchComp benchmark shows that testing LLMs on open-ended research for algorithm discovery is promising direction to adapt language models to real-world tasks."
        },
        {
            "title": "6 DISCUSSION",
            "content": "Limitations and Future Directions. Although AlphaResearch successfully discovers novel algorithms, we hope to expand its coverage to more realistic applications like accelerate tensor computations. Second, our experiments aim to establish the simplest and most straight-forward approaches for algorithm discovery. Future research should pay more attention to augment the research agents with useful external tools and the application to more complex problems. Lastly, the training of RM in AlphaResearch is based on small models (e.g., Qwen-2.5-7B-Instruct) and 24,445 ICLR peer review records. Enhancing the reward model parameter and dataset size are two important directions, which is left for future research. Conclusion. We present AlphaResearch, an autonomous research agent that synergistically combines new idea generation with program-based verification for novel algorithm discovery. Our approach demonstrates the potential of employing LLM to discover unexplored research area, enabling language models to effectively tackle complex open-ended tasks. We construct AlphaResearchComp, including 8 open-ended algorithmic problems, where AlphaResearch outperforms human researchers in 2/8 algorithmic problems but lags behind in the remaining 6 problems. Our systematic analysis of the benefits and remaining challenges of autonomous algorithm discovery provides valuable insights for future research, contributing to the development of more advanced and capable research agents."
        },
        {
            "title": "REFERENCES",
            "content": "Eser Aygün, Anastasiya Belyaeva, Gheorghe Comanici, Marc Coram, Hao Cui, Jake Garrison, Renee Johnston Anton Kast, Cory McLean, Peter Norgaard, Zahra Shamsi, et al. An ai system to help scientists write expert-level empirical software. arXiv preprint arXiv:2509.06503, 2025. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models. ArXiv, abs/2404.07738, 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the 10 AlphaResearch: Accelerating New Algorithm Discovery with Language Models frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming the rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. Sikun Guo, Amir Hassan Shariatmadari, Guangzhi Xiong, Albert Huang, Myles Kim, Corey Williams, Stefan Bekiranov, and Aidong Zhang. Ideabench: Benchmarking large language models for research idea generation. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2, pp. 58885899, 2025. Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation. In ICML, 2024. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id= chfJJYC3iL. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=VTF8yNQM66. Robert Tjarko Lange, Yuki Imajuku, and Edoardo Cetin. Shinkaevolve: Towards open-ended and sample-efficient program evolution. arXiv preprint arXiv:2509.19349, 2025. Ruochen Li, Teerth Patel, Qingyun Wang, and Xinya Du. MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents. ArXiv, abs/2408.14033, 2024. Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, Hongzhou Lin, Kaiyu Yang, Jia LI, Mengzhou Xia, Danqi Chen, Sanjeev Arora, and Chi Jin. Goedel-prover: frontier model for open-source automated theorem proving. In Second Conference on Language Modeling, 2025. URL https: //openreview.net/forum?id=x2y9i2HDjD. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292, 2024. Alexander Novikov, Ngân Vu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al. Alphaevolve: coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025. OpenAI. Gpt-5. 2025. URL https://openai.com/index/introducing-gpt-5/. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. URL https://arxiv.org/abs/2501.14249. Asankhaya Sharma. Openevolve: an open-source evolutionary coding agent, 2025. URL https: //github.com/codelion/openevolve. Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can llms generate novel research ideas. 2024. Chenglei Si, Tatsunori Hashimoto, and Diyi Yang. The ideation-execution gap: Execution outcomes of llm-generated versus human research ideas. arXiv preprint arXiv:2506.20803, 2025. 11 AlphaResearch: Accelerating New Algorithm Discovery with Language Models Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Min Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, E. A. Huerta, and Hao Peng. SciCode: Research Coding Benchmark Curated by Scientists. ArXiv, abs/2407.13168, 2024. Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476482, 2024. Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. Scimon: Scientific inspiration machines optimized for novelty. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 279299, 2024a. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. Openhands: An open platform for AI software developers as generalist agents. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=OJd3ayDDoF. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. MMLU-pro: more robust and challenging multitask language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024b. URL https://openreview. net/forum?id=y10DM6R2r3. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024. Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, et al. Justice or prejudice? quantifying biases in llm-as-ajudge. arXiv preprint arXiv:2410.02736, 2024. Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, and Qiufeng Yin. Wavecoder: Widespread and versatile enhancement for code large language models by instruction tuning. arXiv preprint arXiv:2312.14187, 2023. Zhaojian Yu, Yilun Zhao, Arman Cohan, and Xiao-Ping Zhang. HumanEval pro and MBPP pro: Evaluating large language models on self-invoking code generation task. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 1325313279, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025. findings-acl.686. URL https://aclanthology.org/2025.findings-acl.686/. Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025. 12 AlphaResearch: Accelerating New Algorithm Discovery with Language Models"
        },
        {
            "title": "2.2 Actions",
            "content": ". . . ."
        },
        {
            "title": "2.3 Environment .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4.2 Results .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4.3 Ablations and Analysis .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4.4 Case Study .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C The Use of Large Language Models",
            "content": "D Curated Problems and Human-Best Values D.1 Spherical Code (S2, = 30). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Littlewood Polynomials. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Sum vs. Difference Sets (MSTD). . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Packing Circle in Square (variable radii). . . . . . . . . . . . . . . . . . . . . . . D.5 Third Autocorrelation Inequality. . . . . . . . . . . . . . . . . . . . . . . . . . . . D.6 Third-Order Autocorrelation Inequality (C3 Upper Bound) . . . . . . . . . . . . . 2 3 3 4 4 5 5 6 7 7 9 9 10 15 16 17 18 20 23 28 28 13 AlphaResearch: Accelerating New Algorithm Discovery with Language Models"
        },
        {
            "title": "A EXAMPLES",
            "content": "We show an example of the constructions discovered by AlphaResearch on problem Packing Circles."
        },
        {
            "title": "AlphaEvolve",
            "content": "1 packing_circles_alphaevolve = np.array([[0.09076163, 0.40381803, 0.090761620923837], [0.07310993, 0.92689178, 0.07310821268917801], [0.08745017, 0.22570576, 0.087381421261857], [0.24855246, 0.30880277, 0.093428060657193], [0.4079865, 0.06300614, 0.063006133699386], [0.47646318, 0.90136179, 0.09863820013617901], [0.89604966, 0.10309934, 0.10309932969006601], [0.9066386, 0.68096117, 0.09336139066386], [0.08962002, 0.76509474, 0.0895289910471], [0.06973669, 0.06965159, 0.06965158303484101], [0.40979823, 0.21756451, 0.09156283084371601], [0.25742466, 0.88393887, 0.11606111839388701], [0.09064689, 0.58506214, 0.090482500951749], [0.90294698, 0.30231577, 0.09623644037635501], [0.57265603, 0.10585396, 0.105853949414604], [0.74007588, 0.40129314, 0.09435083056491601], [0.57539962, 0.71183255, 0.115160168483982], [0.7367635, 0.21592191, 0.09104997089500201], [0.41096972, 0.40263617, 0.093512520648747], [0.88664452, 0.88667032, 0.113317128668286], [0.57582722, 0.49961748, 0.09705531029446801], [0.24962585, 0.49417195, 0.09194421080557799], [0.90546338, 0.49309632, 0.094507120549287], [0.67381348, 0.90149423, 0.09850576014942301], [0.24310147, 0.1077195, 0.10771948922805], [0.40815297, 0.5886157, 0.09248833075116601], [0.24737889, 0.6771266, 0.090994980900501], [0.75801377, 0.7532924, 0.07192969280703], [0.73526642, 0.06243992, 0.062439303756069], [0.57415412, 0.30715219, 0.095403150459684], [0.39239379, 0.75259664, 0.07223814277618501], [0.7439361, 0.58879735, 0.093166630683336]])"
        },
        {
            "title": "AlphaResearch",
            "content": "1 packing_circles_alpharesearch = np.array([[(0.1115677319034151, 0.11156773191787371, 0.11156438489140026), (0.09380224787136374, 0.3161654253705352, 0.09379943380606216), (0.09485964915877172, 0.5048217088596118, 0.09485680337610973), (0.09657322554702913, 0.6962443020287629, 0.09657032835808858), (0.10365512530384222, 0.8963448746980195, 0.10365201565567386), (0.3334956594919712, 0.09664441783072292, 0.0966415184920332), (0.26448615440016093, 0.9376113341122044, 0.06238679422590162), (0.5287192731314015, 0.09859146596680078, 0.09858850822808951), (0.591325020569507, 0.9366833118077788, 0.0633147886877468), (0.7427106948954978, 0.11611889563206494, 0.11611541209023483), (0.7566639864477509, 0.8920585771994192, 0.1079381845606288), (0.9269317750270191, 0.07306822497789416, 0.07306603293080358), (0.9105741716090636, 0.23473376300222965, 0.08942314561430993), (0.9094700615258342, 0.41468336419923396, 0.09052722258939731), (0.9124275486288124, 0.7738960294683863, 0.08756982419268892), (0.9302276007184027, 0.9302276007259072, 0.06977030612132157), (0.5931627035790205, 0.4107363306659128, 0.09216300786888813), (0.5896628759126524, 0.5965222415947758, 0.09365298106148348), (0.26303074890883915, 0.783747668079202, 0.09148238826692158), (0.42710033854875884, 0.28662965969327264, 0.1151473780101257), (0.7511102582575875, 0.5051558281448295, 0.09185177348783963), (0.4273023330525072, 0.8937703360976411, 0.10622647700018645), (0.24372345356089029, 0.24143034678815986, 0.07371479291303436), (0.4260882762526937, 0.6918664604322906, 0.09567746779211372), (0.2572363869779693, 0.4085253312744954, 0.09392364829884896), (0.9094294608754079, 0.5957810763279916, 0.0905678220228201), (0.42560864125756626, 0.49898110459434486, 0.09720528992590773), (0.7533817110763772, 0.32263902019589896, 0.09067643144615074), (0.5903729314333418, 0.7817733747765757, 0.09159665425215473), (0.7515568081174837, 0.6905957415401818, 0.09358581053778628), (0.2605636694821685, 0.5973506902903994, 0.09492800518715086), (0.6095540558280068, 0.24805951545091487, 0.07133567304015336)]]) 14 AlphaResearch: Accelerating New Algorithm Discovery with Language Models Figure 8: New construction of AlphaResearch (right) improving the best known AlphaEvolve (right) bounds on packing circles to maximize their sum of radii. Left: 32 circles in unit square with sum of radii 2.9379. Right: 32 circles in unit square with sum of radii 2."
        },
        {
            "title": "Prompt for New Program Generation",
            "content": "You are an expert software developer tasked with iteratively improving codebase. Your job is to analyze the current program and suggest improvements based on the current proposal and feedback from previous round. Focus on making targeted changes that will increase the programs performance metrics. # Previous Proposal: {previous proposal} # Previous Program: {previous program} # Previous Performance Metrics: {previous result} # Current Proposal {proposal} # Task Suggest improvements to the program that will lead to better performance on the specified metrics. You MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes: 1 <<<<<<< SEARCH 2 3 # Original code to find and replace (must match exactly) 4 5 ======= 6 7 # New replacement code 8 9 <<<<<<< REPLACE Example of valid diff format: 1 <<<<<<< SEARCH 2 for in range(m): 3 for in range(p): AlphaResearch: Accelerating New Algorithm Discovery with Language Models 4 5 for in range(n): C[i, j] += A[i, k] * B[k, j] 6 7 ======= 8 9 # Reorder loops for better memory access pattern 10 11 for in range(m): for in range(n): 13 14 for in range(p): C[i, j] += A[i, k] * B[k, j] 15 16 >>>>>>> REPLACE You can suggest multiple changes. Each SEARCH section must exactly match code in the current program. Be thoughtful about your changes and explain your reasoning thoroughly. IMPORTANT: Do not rewrite the entire program - focus on targeted improvements."
        },
        {
            "title": "Prompt for New Idea Generation",
            "content": "You are research advisor tasked with evolving and improving research proposals. Your goal is to generate new research proposal that builds upon the current proposal while addressing its limitations and incorporating insights from successful approaches. Based on the following information, generate an improved research proposal: Focus on: 1. Identifying weaknesses in the current approach based on performance metrics 2. Proposing novel improvements that could enhance performance 3. Learning from successful inspirations while maintaining originality 4. Ensuring the new proposal is implementable - Current Proposal: {proposal} - Current Program: {program} - Current Metrics: {results} Please generate new research proposal that: 1. Addresses the limitations shown in the current metrics 2. Incorporates insights from successful approaches 3. Proposes specific technical improvements 4. Maintains clarity and technical rigor Return the proposal as clear, concise research abstract. Prompt for AlphaResearch-RM-7B You are an expert reviewer tasked with evaluating the quality of research proposal. Your goal is to assign score between 1 and 10 based on the proposals clarity, novelty, technical rigor, and potential impact. Here are the criteria: 1. Read the following proposal carefully and provide score from 1 to 10. 2. Score 6 means slightly higher than the borderline, 5 is slightly lower than the borderline. Write the score in the boxed{}. {proposal}"
        },
        {
            "title": "C THE USE OF LARGE LANGUAGE MODELS",
            "content": "During the preparation of this manuscript, we utilized large language models (LLMs) for grammar checking and writing suggestions to enhance the readability and clarity of the content. 16 AlphaResearch: Accelerating New Algorithm Discovery with Language Models CURATED PROBLEMS AND HUMAN-BEST VALUES We summarize the ten problems used in the ALPHARESEARCH benchmark. For each item we state the objective, the current human-best value at the benchmarks default parameters, and whether this value is proved optimal or only best-known. AlphaResearch: Accelerating New Algorithm Discovery with Language Models D.1 SPHERICAL CODE (S2, = 30). Problem Description: Place = 30 points on the unit sphere in R3 to maximize the minimal pairwise angle θmin. Human Best: θmin 0.673651 radians ( 38.5971)."
        },
        {
            "title": "Initial Proposal",
            "content": "Problem definition. Choose = 30 points on the unit sphere S2 to maximize the minimum pairwise angle θmin = min i<j arccos(cid:0)pi, pj(cid:1). Constraints. Points are unit vectors (rows normalized). Metric is θmin in radians. Optimization goal. Maximize θmin. The evaluator returns {score, θmin, N, dimension}, with score = θmin. Best-known reference (for = 30 on S2): cos(θ) 0.7815518750949873 θ 0.6736467551690225 rad. Reference table: Henry Cohns spherical codes data (https://cohn.mit.edu/ spherical-codes). Best-known results (human). On S2 (3D), small optima coincide with symmetric polyhedra (e.g., tetrahedron, octahedron, icosahedron). For larger , best codes come from numerical optimization; exact optimality is only known in limited cases. Algorithmic goal. Construct codes with larger θmin. The baseline seeds with symmetric configurations and uses farthest-point maxmin. Stronger methods include: Energy minimization, Projected gradient / coordinate descent, Stochastic maxmin refinement. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17 18 19 20 21 22 23 24 25 Initial Program: import numpy as np def _normalize_rows(P): nrm = np.linalg.norm(P, axis=1, keepdims=True) nrm = np.maximum(nrm, 1e-12) return / nrm def seed_platonic(n): \"\"\"Return good symmetric seed on S^2 for some n; else None.\"\"\" if == 2: # antipodal return np.array([[0,0,1],[0,0,-1]], dtype=float) if == 3: # equilateral on equator ang = 2*np.pi/3 return np.array([[1,0,0],[np.cos(ang),np.sin(ang),0],[np.cos(2*ang),np.sin(2*ang),0]], dtype=float) if == 4: # tetrahedron return _normalize_rows(np.array([[1,1,1],[1,-1,-1],[-1,1,-1],[-1,-1,1]], dtype=float)) if == 6: # octahedron return np.array([[1,0,0],[-1,0,0],[0,1,0],[0,-1,0],[0,0,1],[0,0,-1]], dtype=float) if == 8: # cube vertices = np.array([[sx,sy,sz] for sx in (-1,1) for sy in (-1,1) for sz in (-1,1)], dtype= float) return _normalize_rows(V) if == 12: # icosahedron (one realization) phi = (1+np.sqrt(5))/2 18 AlphaResearch: Accelerating New Algorithm Discovery with Language Models 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 = [] for in (-1,1): += [[0, s, phi],[0, s, -phi],[ s, phi,0],[ s, -phi,0],[ phi,0, s],[-phi,0, s]] = np.array(V, dtype=float) return _normalize_rows(V) return None def farthest_point_greedy(n, seed=None, rng=np.random.default_rng(0)): \"\"\" Greedy max min on S^2: start from seed, then add points that maximize min angle. \"\"\" def random_unit(k): = rng.normal(size=(k,3)); return _normalize_rows(X) if seed is None: = random_unit(1) # start with one random point else: = _normalize_rows(seed) while len(P) < n: # candidates per iteration (tune as needed) # generate candidates and pick the one with largest min angle to current set = random_unit(2000) # cosines to existing points cos = @ P.T # min angle to set -> maximize this min_ang = np.arccos(np.clip(np.max(cos, axis=1), -1.0, 1.0)) idx = np.argmax(min_ang) = np.vstack([P, C[idx:idx+1]]) return def main(): = 30 seed = seed_platonic(n) pts = farthest_point_greedy(n, seed=seed, rng=np.random.default_rng(42)) print(f\"n={n}, points={len(pts)}\") return pts if __name__ == \"__main__\": points = main() np.save(\"points.npy\", points) # Ensure compatibility with evaluators that expect global variable try: points # type: ignore[name-defined] except NameError: points = main() 19 AlphaResearch: Accelerating New Algorithm Discovery with Language Models D.2 LITTLEWOOD POLYNOMIALS. Problem Description For coefficients ck {1} and Pn(t) = (cid:80)n1 suptR Pn(t). k=0 ckeikt, minimize Pn = Human Best: the RudinShapiro construction gives Pn 2n. At the benchmark setting = 512, this yields P512 32 (so the larger-is-better score 1/Pn is 1/32 = 0.03125). Sharper constants are known for special families, but 2n remains clean baseline."
        },
        {
            "title": "Initial Proposal",
            "content": "Choose coefficients ck {1} for (z) = n1 (cid:88) k=0 ckzk, = 1, so as to minimize the supremum norm = max z=1 (z). Constraints. Coefficients ck are restricted to 1. The metric is estimated by FFT sampling on an equally spaced grid (denser grid tighter upper bound). Optimization Goal. The evaluator returns: score = 1 1.0, , if valid, otherwise. Notes on Bounds. For the RudinShapiro construction of length n, classical identity gives 2n. For the benchmark default = 512, this yields 1024 = 32, so Initial Program: import numpy as np score = 1 32 = 0.03125. def rudin_shapiro(n: int): \"\"\" First signs of the Rudin-Shapiro sequence. \"\"\" = np.ones(n, dtype=int) for in range(n): x, cnt, prev = k, 0, 0 while x: = & 1 if & prev: cnt ^= prev = >>= 1 # saw 11 a[k] = 1 if cnt == 0 else -1 return def random_littlewood(n: int, seed=0): rng = np.random.default_rng(seed) return rng.choice([-1, 1], size=n).astype(int) def main(): 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 AlphaResearch: Accelerating New Algorithm Discovery with Language Models 24 25 26 27 28 29 30 31 32 33 34 35 36 = 512 = rudin_shapiro(n) print(f\"n={n}, coeffs={len(c)}\") return if __name__ == \"__main__\": coeffs = main() # Ensure compatibility with evaluators that expect global variable try: coeffs # type: ignore[name-defined] except NameError: coeffs = main() 21 AlphaResearch: Accelerating New Algorithm Discovery with Language Models D.3 SUM VS. DIFFERENCE SETS (MSTD). Problem Description For finite set Z, maximize A+A/AA. Human Best: MSTD sets exist; the smallest possible size is = 8 (classification up to affine equivalence is known). For larger A, extremal ratios remain open; our benchmark instance reports representative value ( 1.04 for = 30)."
        },
        {
            "title": "Initial Proposal",
            "content": "Objective. Classical MSTD (enforced): Given {0, 1, . . . , 1} represented by 0/1 indicator array of length , maximize the ratio = + A . Score: score = (higher is better). Comparisons should be made under the same . Default setup. = 30. Evaluator enforces = (classical setting). If pair (A, B) is provided, is ignored and is used. Known best for = 30 (baseline). Conways MSTD set = {0, 2, 3, 4, 7, 11, 12, 14} yields 1.04. This is the baseline included in initial_program.py. Better ratios may exist for = 30; pushing upwards is the optimization goal. Notes. > 1 is rare and indicates sum-dominance. The ratio depends strongly on ; do not compare ratios across different without normalization scheme. If cross-N comparison is necessary, consider reporting both and , or use log as an auxiliary measure. Initial Program: import numpy as np def main(): = 30 # Conway MSTD set example; we take A=B for classical MSTD = [0, 2, 3, 4, 7, 11, 12, 14] = A[:] A_ind = np.zeros(N, dtype=int); A_ind[A] = 1 B_ind = np.zeros(N, dtype=int); B_ind[B] = 1 return A_ind, B_ind # Ensure globals for evaluator try: A_indicators; B_indicators # type: ignore[name-defined] except NameError: A_indicators, B_indicators = main() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 22 AlphaResearch: Accelerating New Algorithm Discovery with Language Models D.4 PACKING CIRCLE IN SQUARE (VARIABLE RADII). Problem Description In the unit square, place disjoint circles (radii free) to maximize the sum of radii (cid:80) ri. Best-known: for = 26, (cid:80) ri = 2.634 (Cantrell, 2011); for = 32, (cid:80) ri = 2.936 (Specht, 2012)."
        },
        {
            "title": "Initial Proposal",
            "content": "Problem definition. Given an integer n, place disjoint circles in the unit square [0, 1]2 to maximize the total sum of radii. Objective and metric. Score: score = (cid:80)n Validity: circles must be pairwise disjoint and fully contained in the unit square. (larger is better). i=1 ri Notes on records. This variable-radius sum of radii objective is not the classical equal-radius packing; authoritative SOTA tables are not standardized. Values reported in code or experiments should be treated as benchmarks rather than literature SOTA. Goal. Create algorithms that increase the total sum of radii for {26, 32} under the above validity constraints. Initial Program: import math import random from concurrent.futures import ThreadPoolExecutor def pack_circles(n, square_size=1.0): \"\"\" Pack disjoint circles in unit square using uniform tiling approach. Returns the sum of radii and list of circles (x, y, r). \"\"\" def max_circle_radius(x, y, circles, square_size=1.0, skip_idx=None): \"\"\" Compute the maximum radius for circle centered at (x, y) that: - Stays within the unit square [0, square_size] times [0, square_size]. - Does not overlap with existing circles. skip_idx: if provided, index in circles[] to ignore (self). \"\"\" # Distance to nearest boundary of the unit square r_max = min(x, y, square_size - x, square_size - y) # Check distance to existing circles, exit early if r_max rightarrow 0 # early exit if r_max is tiny, and avoid needless sqrt for idx, (cx, cy, cr) in enumerate(circles): if skip_idx == idx: continue if r_max <= 1e-8: break dx = - cx dy = - cy sep = r_max + cr if dx*dx + dy*dy < sep*sep: # only compute sqrt when we know we can shrink dist = math.sqrt(dx*dx + dy*dy) r_max = min(r_max, dist - cr) return max(r_max, 0.0) def uniform_tiling_circles(n, square_size=1.0): \"\"\" Uniformly tile the square with circles using optimal grid placement. \"\"\" if <= 0: return [] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 23 AlphaResearch: Accelerating New Algorithm Discovery with Language Models 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 circles = [] # Calculate optimal grid dimensions # For circles, find the best grid layout (rows cols) best_layout = None best_total_radius = 0 # Try different grid configurations for rows in range(1, min(n + 1, 20)): cols = math.ceil(n / rows) if cols > 20: # Limit grid size continue # Calculate spacing spacing_x = square_size / (cols + 1) spacing_y = square_size / (rows + 1) # Use the smaller spacing to ensure circles fit min_spacing = min(spacing_x, spacing_y) # Calculate maximum radius for this layout max_radius = min_spacing / 2 # Ensure radius doesnt exceed boundaries max_radius = min(max_radius, spacing_x / 2 - 1e-6, spacing_y / 2 - 1e-6) if max_radius <= 0: continue # Place circles in uniform grid temp_circles = [] count = 0 for row in range(rows): for col in range(cols): if count >= n: break = spacing_x * (col + 1) = spacing_y * (row + 1) # Ensure circle stays within bounds if (x - max_radius >= 0 and + max_radius <= square_size and - max_radius >= 0 and + max_radius <= square_size): temp_circles.append((x, y, max_radius)) count += 1 if count >= n: break # Calculate total radius for this layout total_radius = len(temp_circles) * max_radius if total_radius > best_total_radius and len(temp_circles) == n: best_total_radius = total_radius best_layout = temp_circles # If we found valid layout, return it if best_layout: return best_layout # Fallback: use hexagonal packing for better density return hexagonal_packing(n, square_size) def hexagonal_packing(n, square_size=1.0): \"\"\" Use hexagonal close packing for better space utilization. \"\"\" circles = [] # Estimate number of rows and columns for hexagonal packing # Hexagonal packing has rows offset by sqrt(3)/2 * diameter rows = int(math.sqrt(n * 2 / math.sqrt(3))) + 2 count = 0 row = 0 24 AlphaResearch: Accelerating New Algorithm Discovery with Language Models while count < and row < rows: # Calculate position for this row = (row + 0.5) * (square_size / (rows + 1)) # Number of circles in this row if row % 2 == 0: cols = int(math.sqrt(n)) + 1 else: cols = int(math.sqrt(n)) spacing_x = square_size / (cols + 1) for col in range(cols): if count >= n: break if row % 2 == 0: = spacing_x * (col + 1) else: = spacing_x * (col + 1) + spacing_x / 2 # Calculate maximum radius for this position = max_circle_radius(x, y, circles, square_size) if > 0: circles.append((x, y, r)) count += 1 row += 1 return circles def optimize_placement(n, square_size=1.0): \"\"\" Optimize circle placement using uniform tiling with radius maximization. \"\"\" circles = [] # First, try hexagonal packing for high initial density hex_circles = hexagonal_packing(n, square_size) if len(hex_circles) == n: # Ensure maximum radii for hex layout with stronger refinement hex_refined = refine_circles(hex_circles, square_size, iterations=20) return hex_refined # Fallback to uniform grid placement grid_circles = uniform_tiling_circles(n, square_size) if len(grid_circles) == n: return grid_circles # If uniform tiling didnt work perfectly, use adaptive approach # Calculate optimal radius based on density area_per_circle = (square_size * square_size) / estimated_radius = math.sqrt(area_per_circle / math.pi) * 0.9 # Conservative estimate # Create grid with optimal spacing spacing = estimated_radius * 2.1 # Include gap cols = int(square_size / spacing) rows = int(square_size / spacing) actual_spacing_x = square_size / (cols + 1) actual_spacing_y = square_size / (rows + 1) count = 0 for row in range(rows): for col in range(cols): if count >= n: break = actual_spacing_x * (col + 1) = actual_spacing_y * (row + 1) # Calculate maximum possible radius = max_circle_radius(x, y, circles, square_size) if > 0: circles.append((x, y, r)) count += 1 if count >= n: 25 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 AlphaResearch: Accelerating New Algorithm Discovery with Language Models 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 break # If we still need more circles, use remaining space remaining = - len(circles) if remaining > 0: # Place remaining circles in remaining spaces for in range(remaining): # Try different positions systematically best_r = 0 best_pos = (0.5, 0.5) # Fine grid search (increased resolution) grid_points = 100 for gx in range(1, grid_points): for gy in range(1, grid_points): = gx / grid_points = gy / grid_points = max_circle_radius(x, y, circles, square_size) if > best_r: best_r = best_pos = (x, y) if best_r > 0: circles.append((best_pos[0], best_pos[1], best_r)) return circles def refine_circles(circles, square_size, iterations=80, perturb_interval=3): \"\"\" Iteratively grow each circle to its maximum radius under non-overlap constraints. Includes randomized update order, periodic micro-perturbation to escape local minima, and final local-center-perturbation pass for densification. \"\"\" for it in range(iterations): # randomize update order to avoid sweep-order bias indices = list(range(len(circles))) random.shuffle(indices) for in indices: x, y, _ = circles[i] # Compute maximal feasible radius here, skipping self = max_circle_radius(x, y, circles, square_size, skip_idx=i) circles[i] = (x, y, r) # Periodic micro-perturbation: jiggle few circles if it % perturb_interval == 0 and len(circles) > 0: subset = random.sample(indices, min(5, len(circles))) for in subset: x0, y0, r0 = circles[j] dx = random.uniform(-0.03, 0.03) dy = random.uniform(-0.03, 0.03) nx = min(max(x0 + dx, 0), square_size) ny = min(max(y0 + dy, 0), square_size) # Compute maximal radius skipping self nr = max_circle_radius(nx, ny, circles, square_size, skip_idx=j) if nr > r0: circles[j] = (nx, ny, nr) # Full local center-perturbation phase for final densification for in range(len(circles)): x, y, = circles[i] best_x, best_y, best_r = x, y, delta = 0.1 for _ in range(20): dx = random.uniform(-delta, delta) dy = random.uniform(-delta, delta) nx = min(max(x + dx, 0), square_size) ny = min(max(y + dy, 0), square_size) # Compute maximal radius skipping self nr = max_circle_radius(nx, ny, circles, square_size, skip_idx=i) if nr > best_r: best_x, best_y, best_r = nx, ny, nr else: delta *= 0.9 circles[i] = (best_x, best_y, best_r) # Physics-inspired soft relaxation to escape persistent overlaps for in range(len(circles)): x, y, = circles[i] fx, fy = 0.0, 0.0 for j, (xj, yj, rj) in enumerate(circles): if == j: continue AlphaResearch: Accelerating New Algorithm Discovery with Language Models 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 dx = - xj dy = - yj = (dx*dx + dy*dy) ** 0.5 overlap = (r + rj) - if overlap > 0 and > 1e-8: fx += dx / * overlap fy += dy / * overlap # Nudge the center by 10% of the computed net \"repulsive\" force nx = min(max(x + 0.1 * fx, 0), square_size) ny = min(max(y + 0.1 * fy, 0), square_size) nr = max_circle_radius(nx, ny, circles, square_size, skip_idx=i) circles[i] = (nx, ny, nr) return circles def multi_start_optimize(n, square_size, starts=None): \"\"\" Parallel multi-start global rightarrow local optimization using ThreadPoolExecutor. Number of starts adapts to problem size: max(100, 10*n). \"\"\" if starts is None: if <= 50: starts = max(200, * 20) else: starts = max(100, * 10) # precompute hexagonal packing baseline hex_circ = hexagonal_packing(n, square_size) hex_sum = sum(r for _, _, in hex_circ) best_conf = None best_sum = 0. # single trial: seed rightarrow refine rightarrow score def single_run(_): conf0 = optimize_placement(n, square_size) conf1 = refine_circles(conf0, square_size, iterations=40) s1 = sum(r for _, _, in conf1) return s1, conf1 # dispatch trials in parallel with ThreadPoolExecutor() as executor: for score, conf in executor.map(single_run, range(starts)): if score > best_sum: best_sum, best_conf = score, conf.copy() # early exit if near the hex-baseline if best_sum >= hex_sum * 0.995: break return best_conf # Use multi-start global rightarrow local optimization (adaptive number of starts) circles = multi_start_optimize(n, square_size) # Quick 2-cluster remove-and-reinsert densification (extended iterations) for _ in range(8): # remove the two smallest circles to create larger gap smallest = sorted(range(len(circles)), key=lambda i: circles[i][2])[:2] removed = [circles[i] for in smallest] # pop in reverse order to keep indices valid for in sorted(smallest, reverse=True): circles.pop(i) # refine the remaining configuration briefly circles = refine_circles(circles, square_size, iterations=8) # reinsert each removed circle with more sampling for x_old, y_old, _ in removed: best_r, best_pos = 0.0, (x_old, y_old) for _ in range(500): = random.uniform(0, square_size) = random.uniform(0, square_size) = max_circle_radius(x, y, circles, square_size) if > best_r: best_r, best_pos = r, (x, y) circles.append((best_pos[0], best_pos[1], best_r)) # final local polish after reinsertion circles = refine_circles(circles, square_size, iterations=5) # end 2-cluster remove-and-reinsert densification # Calculate total radius total_radius = sum(circle[2] for circle in circles) return total_radius, circles 27 AlphaResearch: Accelerating New Algorithm Discovery with Language Models D.5 THIRD AUTOCORRELATION INEQUALITY. Problem Description Let C3 be the largest constant such that maxt1/2 (f )(t) C3 for all (signed) . (cid:0) (cid:82) 1/4 1/4 (cid:1)2 Best-known: classical 1.4581 upper bound. D.6 THIRD-ORDER AUTOCORRELATION INEQUALITY (C3 UPPER BOUND)"
        },
        {
            "title": "Initial Proposal",
            "content": "Problem. For piecewise-constant nonnegative functions on fixed support with unit mass, we evaluate an upper bound Cupper_bound derived from the maximum of the autoconvolution (normalized by squared L1 mass). The benchmark score is score ="
        },
        {
            "title": "1\nCupper_bound",
            "content": ", so that larger score indicates smaller upper bound and hence better result. Evaluator. The evaluator calls find_better_c3_upper_bound() from the target program to obtain step heights, computes the normalized autoconvolution maximum, and returns 1/Cupper_bound. Baseline algorithm. simple genetic algorithm over height sequences serves as the baseline search method. The algorithm includes: Tournament selection, One-point crossover, Gaussian mutation. Initial Program: import numpy as np import scipy.integrate def calculate_c3_upper_bound(height_sequence): = len(height_sequence) delta_x = 1 / (2 * N) def f(x): if -0.25 <= <= 0.25: index = int((x - (-0.25)) / delta_x) if index == N: index -= return height_sequence[index] else: return 0.0 integral_f = np.sum(height_sequence) * delta_x integral_sq = integral_f**2 if integral_sq < 1e-18: return 0. t_points = np.linspace(-0.5, 0.5, 2 * + 1) max_conv_val = 0.0 for t_val in t_points: lower_bound = max(-0.25, t_val - 0.25) upper_bound = min(0.25, t_val + 0.25) if upper_bound <= lower_bound: convolution_val = 0.0 else: def integrand(x): return f(x) * f(t_val - x) convolution_val, _ = scipy.integrate.quad(integrand, lower_bound, upper_bound, limit=100) 28 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 AlphaResearch: Accelerating New Algorithm Discovery with Language Models if abs(convolution_val) > max_conv_val: max_conv_val = abs(convolution_val) return max_conv_val / integral_sq def genetic_algorithm(population_size, num_intervals, generations, mutation_rate, crossover_rate): population = np.random.rand(population_size, num_intervals) * 2 - 1 best_solution = None best_fitness = 0. for gen in range(generations): fitness_scores = np.array([calculate_c3_upper_bound(individual) for individual in population]) current_best_idx = np.argmax(fitness_scores) if fitness_scores[current_best_idx] > best_fitness: best_fitness = fitness_scores[current_best_idx] best_solution = population[current_best_idx].copy() # print(f\"Generation {gen}: New best fitness = {best_fitness}\") new_population = np.zeros_like(population) for in range(population_size): competitors_indices = np.random.choice(population_size, 2, replace=False) winner_idx = competitors_indices[np.argmax(fitness_scores[competitors_indices])] new_population[i] = population[winner_idx].copy() for in range(0, population_size, 2): if np.random.rand() < crossover_rate: parent1 = new_population[i] parent2 = new_population[i+1] crossover_point = np.random.randint(1, num_intervals - 1) new_population[i] = np.concatenate((parent1[:crossover_point], parent2[ crossover_point:])) new_population[i+1] = np.concatenate((parent2[:crossover_point], parent1[ crossover_point:])) for in range(population_size): if np.random.rand() < mutation_rate: mutation_point = np.random.randint(num_intervals) new_population[i, mutation_point] += np.random.normal(0, 0.1) new_population[i, mutation_point] = np.clip(new_population[i, mutation_point], -2, 2) population = new_population return best_solution def find_better_c3_upper_bound(): NUM_INTERVALS = 4 POPULATION_SIZE = 2 GENERATIONS = 10 MUTATION_RATE = 0.1 CROSSOVER_RATE = 0.8 height_sequence_3 = genetic_algorithm(POPULATION_SIZE, NUM_INTERVALS, GENERATIONS, MUTATION_RATE, CROSSOVER_RATE) return height_sequence_ 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 98"
        }
    ],
    "affiliations": [
        "ByteDance",
        "New York University",
        "Tsinghua University",
        "Yale University"
    ]
}