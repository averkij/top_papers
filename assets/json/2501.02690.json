{
    "paper_title": "GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking",
    "authors": [
        "Weikang Bian",
        "Zhaoyang Huang",
        "Xiaoyu Shi",
        "Yijin Li",
        "Fu-Yun Wang",
        "Hongsheng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "4D video control is essential in video generation as it enables the use of sophisticated lens techniques, such as multi-camera shooting and dolly zoom, which are currently unsupported by existing methods. Training a video Diffusion Transformer (DiT) directly to control 4D content requires expensive multi-view videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that optimizes a 4D representation and renders videos according to different 4D elements, such as camera pose and object motion editing, we bring pseudo 4D Gaussian fields to video generation. Specifically, we propose a novel framework that constructs a pseudo 4D Gaussian field with dense 3D point tracking and renders the Gaussian field for all video frames. Then we finetune a pretrained DiT to generate videos following the guidance of the rendered video, dubbed as GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense 3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art sparse 3D point tracking method, in accuracy and accelerates the inference speed by two orders of magnitude. During the inference stage, GS-DiT can generate videos with the same dynamic content while adhering to different camera parameters, addressing a significant limitation of current video generation models. GS-DiT demonstrates strong generalization capabilities and extends the 4D controllability of Gaussian splatting to video generation beyond just camera poses. It supports advanced cinematic effects through the manipulation of the Gaussian field and camera intrinsics, making it a powerful tool for creative video production. Demos are available at https://wkbian.github.io/Projects/GS-DiT/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 0 9 6 2 0 . 1 0 5 2 : r GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking Weikang Bian1,2 Yijin Li3 Zhaoyang Huang3 Fu-Yun Wang1 1Multimedia Laboratory, The Chinese University of Hong Kong 2Centre for Perceptual and Interactive Intelligence 3Avolution AI Xiaoyu Shi1 Hongsheng Li1,2 Figure 1. GS-DiT generates multi-camera shooting videos by bringing pseudo 4D Gaussian fields to video diffusion transformers."
        },
        {
            "title": "Abstract",
            "content": "4D video control is essential in video generation as it enables the use of sophisticated lens techniques, such as multicamera shooting and dolly zoom, which are currently unsupported by existing methods. Training video Diffusion Transformer (DiT) directly to control 4D content requires Inspired by Monocular Dyexpensive multi-view videos. namic novel View Synthesis (MDVS) that optimizes 4D representation and renders videos according to different 4D elements, such as camera pose and object motion editing, we bring pseudo 4D Gaussian fields to video generation. Specifically, we propose novel framework that constructs pseudo 4D Gaussian field with dense 3D point tracking and renders the Gaussian field for all video frames. Then we finetune pretrained DiT to generate videos following the guidance of the rendered video, dubbed as GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense 3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art sparse 3D point tracking method, in accuracy and accelerates the inference speed by two orders of magnitude. During the inference stage, GS-DiT can generate videos with the same dynamic content while adhering to different camera parameters, addressing significant limitation of current video generation models. GS-DiT demonstrates strong generalization capabilities and extends the 4D controllability of Gaussian splatting to video generation beyond just camera poses. It supports advanced cinematic effects through the manipulation of the Gaussian field and camera intrinsics, making it powerful tool for creative video production. Demos are available at https://wkbian.github.io/Projects/GSDiT/. 1. Introduction Video generation [8, 14, 19, 21, 25, 42, 55] has made rapid progress in recent years. Since the demonstration of Sora [55], the video generated by Diffusion Transformers (DiT) is approaching cinematic quality by scaling up the parameters and the number of training videos. However, in order to produce effective lens language, such as multicamera shooting, dolly zoom, and object motion editing in videos, video creators often need to have more precise 4D control over the video content. For example, multi-camera shooting requires the presentation of the same content from different perspectives and dolly zoom requires adjusting the intrinsic and extrinsic parameters of the camera at the same time. Yet, current video generation methods are unable to support such 4D video control. Generative Camera Dolly (GCD) [46] demonstrates the possibility of utilizing the generative model for multicamera shooting. After preparing multi-camera shooting videos as training data, it switches videos captured by different camera trajectories and the same dynamic contents as input condition video and output supervision. However, capturing large amount of synchronized multi-trajectory videos in open-door real scenes is too expensive, e.g., car driving videos, so GCD chooses to collect training data in simulator, which exhibits poor generalization performance to the real world and prohibits it from being trained on web videos. This raises the question: can we directly learn multi-camera shooting video generation from normal monocular videos? One straightforward solution is to optimize 4D Gaussian field [3, 50, 52] for each monocular video and then reprogram the camera trajectories to render novel view videos for training. However, the optimization process is highly time-consuming, requiring at least one hour for 70-frame video, which makes the preparation of training data impractical. Furthermore, due to the ill-posed nature of 4D video optimization, the rendered videos suffer from significant artifacts. Fine-tuning video generation models with these videos will inevitably degrade the video quality. Fortunately, we observe that the videos rendered from the Gaussian field although contain artifacts, still provide strong cues for video generation. Drawing inspiration from recent works on classical monocular dynamic video synthesis (MDVS) [16], we build the pseudo 4D Gaussian field and render the novel view videos to guide the video generation. Specifically, we first propose an efficient dense 3D point-tracking (D3D-PT) method to facilitate the pseudo 4D Gaussian field construction. Given an input video, we select reference frame and estimate its dense 3D point trajectories across the subsequent frames. The pseudo 4D Gaussian field is then directly constructed from 3D point trajectories without any optimization. The Gaussian primitives [30] of the field have constant color values derived from the pixel values, replacing the view-dependent spherical harmonic representation. Other parameters of the Gaussian primitives, such as opacity, are all held constant. We then generate the training data by rendering the Gaussian field using the original camera poses from the input video. pretrained Video DiT is finetuned with the rendered results as input to generate the original input video from the rendered video, dubbed as GS-DiT. Since it is difficult to annotate the ground truth of 3D point tracking in the real world, point trackers are trained on synthetic data, and the depth distribution in the real world may be quite different from that in synthetic data. Our D3D-PT is loosely coupled to the depth distribution so that can be flexibly adapted to different scenarios. In the inference stage, given monocular dynamic video, we also build pseudo 4D Gaussian field and render it according to the newly assigned camera parameters. Then, our finetuned DiT generates the final video following the guidance of the rendered video. Besides the camera pose control, we can also adjust the camera intrinsics and edit the Gaussian field. Such 4D control of video generation supports video creators. Our proposed framework does not require multi-view videos for training. GS-DiT can therefore be trained on any monocular dynamic videos and learn various motion dynamics. Compared to GCD, GS-DiT presents strong generalization capacity and can be consistently improved when the base DiT model is scaled up. GS-DiT can also support more 4D controllability than GCD, such as camera intrinsic control and object motion editing. The main contributions of this paper are summarized as follows: We propose novel framework that advances video generation with pseudo 4D Gaussian fields through efficient dense 3D point tracking. We propose an efficient dense 3D point tracking (D3DPT) method that outperforms SpatialTracker in accuracy, and accelerates dense 3D point tracking by two orders of magnitude. D3D-PT facilitates the training of video generation from rendered pseudo 4D Gaussian field. We propose GS-DiT that learns to generate videos following the guidance of rendered videos. GS-DiT can generate multi-camera shooting videos and provides 4D video control such as camera intrinsic and object motion editing. 2. Related Work Point tracking. PIPs [20] and TAP-Net [11] first address the point-tracking problem that estimates point trajectories in videos for the query pixels in the starting video frame. In contrast to optical flow that tackles dense pixel correspondence between pair of video frames, they are interested in independent pixel correspondence throughout multiple video frames. CoTracker [27] and Context-PIPs [5] realize that even though focusing on long-term temporal pixel correspondence, spatial context information is still vital and improves point tracking accuracy with more spatial information. DOT [32] estimates dense point tracking by incorporating pretrained point tracker to provide initialization and refine the dense prediction with RAFT [43]. SpatialTracker [53] is the first method that extends the 2D point tracking to 3D via depth estimator. In our GS-DiT framework, we need dense 3D point tracker to build the pseudo4D Gaussian field but existing point trackers cannot complete this task. We propose the first dense 3D point tracking method, surpassing SpatialTracker by accuracy and accelerating the speed by two orders. Video diffusion models. Diffusion models [10, 24, 28, 33, 34, 48] emerge as prominent approach for generative modeling, excelling in synthesizing diverse and highquality samples. Although early diffusion models are primarily validated on image generation tasks [10, 23], their application is soon extended to video generation, revealing significant potential. Initial video diffusion models [7, 14, 21, 25, 26, 42] are often based on off-the-shelf image diffusion models [40], augmented with temporal layers to capture the relationships among frames. Since Sora [8], video generation models have begun to shift to the transformer architectures, such as CogVideoX series [55]. Video Diffusion with Transformers (DiT) [36] utilizes temporal causal VAE [56] and 3D attention for better temporal consistency. Recent works show that image and video diffusion models can be re-purposed for other dense video translation tasks including monocular depth estimation [17, 29, 41], 3D generation and reconstruction [6, 9, 38, 47], and amodal segmentation [35]. Text-to-4D, image-to-4D, and Video-to4D [45] papers have also attracted researchers attention but the scenarios are highly limited, e.g., single objects or animals. In addition to the generation quality, the generation controllability, such as the camera pose [22, 51], is also crucial. GCD [46] firstly grants the video generation model to synthesize novel view video according to the assigned camera trajectory so that the director can recreate the footage he wants for given video. However, GCD needs to be trained on video data rendered from simulator due to its training paradigm, which highly limits its generalization performance. We bring the 4D Gaussian splatting [52] to video diffusion transformer so that we can migrate the controllability of 4D Gaussian splatting, such as camera intrinsic and extrinsic and object editing, to video generation. 3. Method Achieving precise control over 4D content in video generation is essential for integrating this technology into film production. To better accommodate video generation in cinematic studios, we tackle the 4D video control problem. Inspired by classical Monocular Dynamic Video Synthesis (MDVS) methods [15, 16, 50] that render novel view videos by obtaining 4D representation, we propose novel framework that learns 4D video control with the help of pseudo 4D Gaussian fields. In contrast to GCD [46] which learns multi-camera shooting video generation from multiview synchronized videos, our proposed framework directly learns from conventional monocular videos and can be naturally scaled up along with the base video Diffusion Transformers (DiT). In this section, we first introduce the efficient dense 3D point tracking (D3D-PT), which is the cornerstone of the pseudo-4D Gaussian fields in training, and then demonstrate the 4D Gaussian field construction. Finally, we elaborate on how to fine-tune pre-trained DiT to generate videos with 4D control. 3.1. Dense 3D Point Tracking Given the video frames RT HW and the corresponding metric depth maps RT HW , the goal of dense 3D point tracking (D3D-PT) is to estimate the 3D trajectories and visibilities RT HW of all query pixels on the first frame x0 RHW 2 throughout the video. The 3D point trajectories is parameterized as their 2D locations and depth on the images RT HW 2, RT HW . Taking both spatial and temporal video information is necessary but directly learning dense 3D point tracking by encoding the whole video is too expensive. Inspired by DOT [32], we propose to estimate dense 3D point tracking in two stages: initialize the 3D point tracks in the first stage by encoding sparse but longer temporal information and then iteratively refine them in the second stage by encoding dense pair-wise information. Initialization. Following DOT [32], we estimate sparse 2D point-tracking with CoTracker [27] and obtain the dense point tracking xt {x0, . . . , xT 1} through interpolation. With such 2D point tracking initialization, we further initialize the point depths in all frames {d0, . . . , dT 1}. Intuitively, the point depth in the first frame is assigned according to its depth map. As the point depth variaFigure 2. An overview of GS-DiT. In the training stage, we build pseudo 4D Gaussian field from an input video via dense 3D point tracking. Our GS-DiT learns to generate the original video guided by the video rendered from the pseudo 4D Gaussian field. In the inference stage, we can build the pseudo 4D Gaussian via dense 3D point tracking, directly lifting the depth map, or optimizing 4D Gaussian field. Editing and rendering the Gaussian field with scheduled camera intrinsic and extrinsic bring various cinematic effects. tions are derived from the first frame, we also initialize the point depth in the other frames by the depth of the first frame dt = d0 = D0(x0). All of the initial point visibilities vt {v0, . . . , vT 1} are set to 1, indicating that the points are visible. The initialized variables {x0, . . . , xT 1}, {d0, . . . , dT 1}, {v0, . . . , vT 1} will be iteratively refined in the next stage. Dense 3D Point Tracking Refinement. We sequentially refine the coarse initialization results for each target video frame. Given the reference frame I0 and target frame It, we refine the point tracks xt and visibilities vt with RAFT [43] network. Specifically, after computing the correlation pyramid from the image features of I0 and It, we iterativelly predict the residual point tracks xt,i and visibilities vt,i at the i-th iteration according to the cropped correlation information and update them xt,i = xt,i1 + xt,i, vt,i = vt,i1+vt,i. Please refer to the supplementary for more details. But how do we refine the point depth? Adopting RAFT-3D [44] is not feasible because we do not accept the rigid body assumptions in conventional dynamic videos. Another straightforward solution is to expand the RGB images to RGB-D images by concatenating the depth map to the RGB images and refining the point depth concurrently. However, we rely on monocular depth prediction methods in the inference stage. The distribution of predicted depth may mismatch the depth distribution used during training and such tight-coupled design presents poor generalization performance, which is critical in the following video generation training. To improve the generalization performance of 3D point tracking, we propose loosely coupled point depth refinement module. Given the depth map of the target frame Dt, we build 4-layer depth pyramid ˆDt by downsampling the depth map 4 times. In addition to the depth information of the current frame, we iteratively refine the point depth with the 2D point tracking features. At the i-th iteration, given the point tracking xt,i and its feature mt,i, inp, net (see supplementray), we crop depth information ˆDt(xt,i) RHW 994 centered at xt,i, encode them through motion encoder otionEnc: md t,i = otionEnc(mt,i, dt,i1, ˆDt(xt,i)), (1) and refine the point depth dt,i = dt,i1 + dt,i with recurrent network ConvGRU and lightweight convolutional network DepthHead: net = ConvGRU (net, inp, md t,i), dt,i = DepthHead(net). (2) In this way, our network refines the point depth prediction by reading depth information from ˆDt(xt,i) and generalizes well when the input depth distribution differs from the training depth. Our 3D point tracking accuracy surpasses SpatialTracker and accelerates the speed by two orders of magnitude, which builds the pseudo 4D Gaussian fields for our training videos. 3.2. Pseudo 4D Gaussian Field Inspired by typical Monocular Dynamic View Synthesis (MDVS) that renders novel view images via 4D Gaussian field, we tackle the 4D video control problem with the help of 4D Gaussian fields. Optimizing 4D Gaussian field with geometric constraints is time-consuming, around 1 hour per video, and such method is always limited in object-centric scenarios due to the canonical space assumption. Our goal is to control the camera intrinsic, extrinsic, and object motion in open-world videos. The 4D video control can support various lens languages, such as multi-camera shooting and dolly zoom. Therefore, we need to build the Gaussian field efficiently and flexibly. We observe that the 4D motion field can be simply represented by the movements of 3D points along the time, which motivates us to construct pseudo 4D Gaussian field by the proposed efficient dense 3D point tracking. Specifically, 4D Gaussian field consists of = points have the following parameters: RN 3, α RN 3, RT 3, RN 3 describing the scale, opacity, position, and color of Gaussians. We create Gaussian for each pixel in the source frame. The scale and opacity are set as constants. We estimate the dense 3D trajectories starting from the source video frame throughout all video frames and assign them to Gaussian positions pt RN 3 = (xt, dt) and 3D Gaussian field at timestep can be directly extracted. The color of Gaussians is naturally assigned the RGB value of the corresponding pixel. Given Gaussian field at timestep and transformed to the camera coordinate through camera pose, we render the image ˆI following the standard Gaussian splatting [30]: ˆI = 1 (cid:88) i=0 ciαi i1 (cid:89) j= (1 αj) . (3) We call this 4D Gaussian field pseudo because we do not optimize the parameters and thus the rendered images and videos inevitably contain artifacts. Fortunately, we do not directly regard such rendered videos as our output but exploit them as guidance for video generation. We provide an overview of GS-DiT in Fig. 2. During training, the videos are rendered according to the same camera trajectories as the original training video, and our proposed GSDiT learns to generate the original video from the rendered video. The rendered video used as conditional signals provides temporal information with multi-view consistency but presents serious artifacts. GS-DiT is encouraged to restore the artifacts by connecting the temporal guidance information and strong video prior. In the inference stage, we can also construct pseudo 4D Gaussian field and render the videos with novel camera trajectories to guide the video generation. Besides dense 3D point tracking, the pseudo 4D Gaussian field can be constructed by other methods, e.g., directly lifting per-frame depth to the 3D space or optimizing real 4D Gaussian field in canonical space. In this paper, we use the lift depth map as major example because it is the most efficient way. With the pseudo 4D Gaussian field design, our GS-DiT can generate various visual effects. For example, based on the Figure 3. The neural network architecture of GS-DiT. GSDiT generates video conditioned on the video rendered from our pseudo 4D Gaussian field. same Gaussian field, rendering videos with different camera trajectories generates multi-camera shooting videos, and simultaneously adjusting the camera intrinsic and extrinsic produces dolly zoom effects. 3.3. Video Generation with GS-DiT We obtain our GS-DiT that controls 4D content in videos by finetuning the pre-trained 4D CogVideoX [55], an opensource DiT architecture video generation model. We first brief CogVideoX and then elaborate on how to finetune the pretrained CogVideoX as our GS-DiT. CogVideoX. CogVideoX contains three parts: T5 text encoder [39], 3D causal VAE that compresses 4 RGB video frames into 1 latent feature frame of 16 channels, and full attention transformer backbone to encode all information. CogVideoX generates video consisting of 49 frames from text. The training video is first compressed into 13 latent feature frames by the 3D causal VAE, where three dummy frames are padded in front, and diffusion training is conducted in the latent space with full attention transformer. GS-DiT is designed to generate video from guidance video. The guidance video is rendered from the pseudo 4D Gaussian field with given camera trajectory, so the camera trajectory information is implicitly represented by the rendered video. In this way, GS-DiT can generate multicamera shooting videos. Specifically, we render 13 video frames as the guidance. To inject the guidance video information into DiT, we pad 3 dummy frames in front of each frame and individually encode the 4-frame group into 16channel latent feature frame with the 3D VAE. Such conditional latent features are concatenated to the corresponding noise features, which constitute 32-channel features. The patch embedding layer used in the attention is expanded to accept the 32-channel features and we initialize the additional convolution parameters by 0. Following CogVideoX, we finetune our model θ with the standard DDPM [24] formulation: L(θ) := Et,I,ϵ (cid:16) (cid:13) (cid:13) (cid:13)ϵ ϵθ αtI + 1 αtϵ, ˆI, (cid:17)(cid:13) 2 (cid:13) (cid:13) . (4) We slightly abuse the annotation and α, which indicates the denoising timestamp and coefficients in the standard Methods TAP-Net [11] PIPs [20] OmniMotion [49] TAPIR [12] BootsTAPIR [13] CoTracker [27] DOT [32] SpatialTracker [53] Ours + ZoeDepth AJ < δx Kinetics avg OA AJ < δx DAVIS avg OA RGB-Stacking AJ < δx avg 38.5 31.7 - 49.6 54.6 48.7 48.4 50.1 51.9 54.4 53.7 - 64.2 68.4 64.3 63.8 65.9 64. 80.6 72.9 - 85.0 86.5 86.5 85.2 86.9 86.1 33.0 42.2 46.4 56.2 61.4 60.6 60.1 61.1 61.3 48.6 64.8 62.7 70.0 73.6 75.4 74.5 76.3 74. 78.8 77.7 85.3 86.5 88.7 89.3 89.0 89.5 89.4 54.6 15.7 69.5 54.2 - 63.1 77.1 63.5 77.0 68.3 28.4 82.5 69.8 - 77.0 87.7 77.6 86. Table 1. Comparison of 2D point tracking on TAPVid. OA AJ < δx Average avg OA 87.7 77.1 90.3 84.4 - 87.8 93. 88.2 92.8 42.0 29.9 - 53.3 - 57.4 61.9 58.2 63.4 57.1 50.0 - 68.0 - 72.2 75.3 73.3 75.2 82.4 75.9 - 85.3 - 87.8 89. 88.2 89.4 Methods 3D-AJ Aria APD OA 3D-AJ APD OA 3D-AJ APD OA 3D-AJ APD OA DriveTrack PStudio Average BootsTAPIR [13] + ZoeDepth [4] SpatialTracker [53] Ours + ZoeDepth[4] Ours + Depth Anything V2 [54] Ours + UniDepth V2 [37] 8.6 9.2 10.0 14.5 15. 14.5 15.1 16.1 21.9 22.2 86.9 89.9 90.0 90.0 90.0 5.1 5.8 7.2 8.6 11. 8.7 10.2 12.0 13.8 18.1 83.5 82.0 81.1 81.1 81.1 10.2 9.8 9.8 11.3 6. 17.7 17.7 17.3 19.4 12.2 82.0 78.4 80.5 80.5 80.5 8.0 8.3 9.0 11.4 11. 13.6 14.3 15.1 18.3 17.5 84.1 83.4 83.9 83.9 83.9 Table 2. Comparison of 3D point tracking on TAPVid-3D minival split. DDPM here. ϵ (0, 1) is the standard Gaussian noise and ϵθ is the prediction of our GS-DiT. 4. Experiments We first evaluate our proposed dense 3D point tracking (D3D-PT) and then evaluate our 4D video control framework GS-DiT. Finally, we show the controllability of GSDiT with dolly zoom and object motion editing. We strongly suggest readers refer to the supplemented video. 4.1. Dense 3D Point Tracking As there are no available dense 3D point tracking benchmarks, we evaluate our dense 3D point tracking method on the sparse 3D point tracking benchmark TAPVid-3D [31] and sparse 2D point tracking benchmark TAPVid [11]. To our best knowledge, we are the first that address the dense 3D point tracking problem. D3D-PT achieves state-of-theart performance even compared with the sparse 3D point tracking methods and sparse 2D point tracking methods. Nonetheless, D3D-PT presents extreme efficiency that accelerates 3D point tracking of dense pixels by 90 times. Implementation Details We train our D3D-PT model on 8 NVIDIA A100-SXM4-80GB GPUs for 500,000 iterations. Our training data is generated with the Kubric [18] simulator. We generate 11,000 24-frame RGB-D sequences in total at 512 512 resolution with corresponding dense 3D point tracking ground truth. We set the batch size to 64 and the learning rate to 1 104. Experimental Setup We follow TAPVid and TAPVid-3D to evaluate sparse 2D and 3D point tracking. TAPVid consists of three sub-datasets Kinetics, DAVIS, and RGBStacking. Kinetics contains 1144 250-frame videos, covering various human-related actions. DAVIS contains 30 real videos ranging in length from 34 to 104. RGB-Stacking contains 50 synthetic 250-frame robotic stacking videos. We adopt the average Jaccard (AJ), < δx avg, and the occlusion accuracy (OA) evaluation metrics for sparse 2D point tracking. < δx avg evaluates the average fraction of visible points that are within pixel thresholds δ {1, 2, 4, 8, 16} of their ground truth. TAPVid-3D includes three tracks: Aria, DriveTrack, and PStudio. We use the minival split, where each track has 50 videos. Aria contains 300-frame real videos recorded in indoor scenes. DriveTrack contains outdoor driving videos with lengths from 25 to 300. PStudio contains 150-frame indoor real videos focusing on human motions. We also adopt 3D metrics to measure the quality of the predicted 3D point trajectories (APD), point visibility (OA), and both simultaneously (3D-AJ [31]). Comparison on 2D Point Tracking We compare our D3DPT with representative sparse 2D point tracking methods. The most related works are DOT and SpatialTracker. As shown in Tab. 1, even though our method is designed for dense 3D point tracking, it can also achieve state-of-theart performance on 2D sparse point tracking benchmarks. BootsTAPIR ranks 1st on Kinetics and DAVIS benchmarks because it is carefully improved with the bootstrap technique based on TAPIR. Our method, which is not improved by bootstrap, ranks 2nd on the benchmarks and significantly outperforms BootsTAPIRs base model TAPIR. Our method outperforms DOT, the dense 2D point tracking method, by 7.2% on the AJ of Kinetics. Our method also consistently outperforms the most recent sparse 3D point tracking method, SpatialTracker, in AJ on all of the benchmarks. Comparison on 3D Point Tracking We compare our D3DPT with BootsTAPIR + ZoeDepth and SpatialTracker in Tab. 2. Notice that our method predicts dense 3D point tracking while only the sparse points that are annotated with ground truth are evaluated. The TAPVid-3D benchmark ofMethods MonST3R GCD Ours PSNR 14.12 15.04 19.18 DAVIS SSIM 0.59 0.41 0.60 LPIPS PSNR 0.31 0.48 0.23 15.32 11.96 17.92 Sora SSIM 0.59 0.32 0.60 LPIPS PSNR Pixabay SSIM LPIPS 0.30 0.52 0. 19.78 13.71 22.66 0.74 0.42 0.73 0.22 0.50 0.15 Table 3. Comparison of the multi-shooting video generation quality. Figure 4. Qualitative comparison of synchronized video generation with camera control. (a)(b) are the first and last frame of the input video. (c) is the camera trajectory visualization. (d)(e)(f) are the last frame of the video generated by our GS-DiT, GCD, and MonST3R. ficially provides the results of BootsTAPIR + ZoeDepth, which combines the state-of-the-art sparse 2D point tracker BootsTAPIR and metric depth estimator ZoeDepth. SpatialTracker is pioneering method specifically designed for sparse 3D point tracking. For fair comparison, we use ZoeDepth as our depth estimator (Ours + ZoeDepth). Our method significantly outperforms both methods on Aria and DriveTrack in terms of 3D-AJ and APD. Comparison on inference time We compare the inference time on the DAVIS dataset, which contains 90 480P videos with an average length of 69 (from 24 to 104). We estimate dense 3D point tracking of the entire video on DAVIS starting from the first frame. We report the average time required to estimate single frame. The baseline method SpatialTracker cannot directly estimate dense 3D point tracking, so we split all query points into multiple chunks according to the default 50 50 sparse grid and estimate the total inference time to process all pixels. SpatialTracker takes 89.8 seconds when processing frame. Such an expensive time consumption prohibits it from processing large-scale video data. Our method reduces the time overhead by two orders of magnitude. Moreover, as shown in Tab. 2, our method also outperforms SpatialTracker in accuracy. We, therefore, utilize our D3D-PT to construct the pseudo 4D Gaussian field in the following GS-DiT training. 4.2. 4D Video Control Multi-camera shooting video generation requires camera extrinsic control. We quantitatively and qualitatively evaluate our GS-DiT in multi-camera shooting video generation. We also show that we can control the camera intrinsic and object motion in videos with GS-DiT. Figure 5. Video generation with 4D control. (a) presents the dolly zoom effects and (b) rotates the body of the fan. Implementation Details We train our GS-DiT model on the WebVid-10M [2] dataset. We randomly select 400K video clips with length of 49 and stride 2 as the training data. We estimate the dense 3D point tracking with our D3D-PT and build pseudo 4D Gaussian field for each video. Then we render the guidance videos from the pseudo 4D Gaussians to obtain the training set. We train our GSDiT for 100,000 steps on 8 NVIDIA A100-SXM4-80GB GPUs. We set batch size as 8, learning rate as 3 105, and the resolution as 320 512. Experimental Setup Our GS-DiT bridges the 4D Gaussian splatting and the video generation. We select GCD [46] and MonST3R [57] to compare the video quality and camera pose control. GCD is the most relevant work that generates novel view videos while keeping the contents of the original video. Its generalization performance is poor because the training data is generated by simulator. MonST3R is an efficient 4D video reconstruction method. We build the 4D Gaussian field with MonST3R for the input video and render images through the test camera poses for evaluation. Quantitative Comparison on Multi-camera Shooting Video Generation. We evaluate the video quality and camera controllability of video generation models on openworld dynamic videos. An ideal benchmark should be established by capturing the same dynamic scenes with multiple synchronized cameras but it is too expensive in the real open world. Instead, given monocular dynamic video, we design special Arcball Rotation camera trajectory: starting from the identity pose, rotating the camera to 30 in the middle frame, and then rotating back to the identity pose in the last frame. The rotation is orbit movement around the (0, 0, 2) point. For this kind of rotational camera motion, we set eight motion directions: left, right, up, down, upper left, lower left, upper right, and lower right. Given video and set of camera trajectories for evaluation, the first and last frame of the generated video should be identical to the original video according to the camera control, but the content in the middle segment has corresponding viewpoint changes. We evaluate the video generation controllability and quality by computing the visual alignment of the last frame between the original video and the generated video. Therefore, we build three datasets: DAVIS, Sora, and Pixabay, for evaluation. DAVIS contains the same 30 real videos as those selected in the TAPViD benchmark. Sora contains 15 generated videos collected from its technical report. Pixabay contains 40 real videos collected from Pixabays website. We use the standard SSIM, PSNR, and LPIPS metrics for evaluation. For each method, we generate 240, 120, and 320 videos respectively on DAVIS, Sora, and Pixabay for evaluation. As shown in Tab. 3, our method surpasses GCD and MonST3R all-sided. Actually, GCD is almost collapsed due to its poor generalization performance. Qualitative Comparison on Camera Pose Control We qualitatively compare our method with GCD and MonST3R in Fig. 4. There are six videos drawn from DAVIS and Sora datasets covering animals, humans, and vehicles. We design different camera trajectories as queries. For each method, we re-generate the input video according to the query camera trajectories as the visualization. The videos generated by our GS-DiT accurately respond to the query camera trajectory, keep pleasing visual quality, and synchronize the object motion of the original video. In contrast, GCD roughly aligns the camera trajectory and presents poor visual quality, which presents poor generalization performance. Due to the collapse of the visual quality, we can not tell whether the generated video is synchronized to the original video. In the 2nd, 3rd, 4th, and 6th row, MonST3R presents good camera pose controllability but the visual quality is corrupted because some areas are unobserved. Camera Intrinsic and Object Motion Control Besides generating videos with camera extrinsic control, we can also control the camera intrinsic and edit the 4D Gaussian field when rendering the video. As shown in Fig. 5, we apply the dolly zoom effects to the camel video (a) by simultaneously controlling the camera intrinsic and extrinsic. We also edit the pseudo Gaussian field derived from the video containing rotating fan (b). We apply body rotation to the fan and generate the final video with GS-DiT. The generated video obtains the original rotation movement of the input video and also presents the body rotation assigned by our control. There are no floater artifacts after the object editing and the final video is natural. 5. Conclusion We have proposed framework for 4D video control. By bringing pseudo Gaussian fields to video generation, we have endowed our GS-DiT with the 4D controllability from Gaussian fields, such as camera intrinsic and extrinsic editing and object motion editing. Since our 4D representation is not optimized, we look forward to generating real 4D video with our GS-DiT in the future."
        },
        {
            "title": "References",
            "content": "[1] AF Agarap. Deep learning using rectified linear units (relu). arXiv preprint arXiv:1803.08375, 2018. 12 [2] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for In IEEE International Conference on end-to-end retrieval. Computer Vision, 2021. 8 [3] Aayush Bansal, Minh Vo, Yaser Sheikh, Deva Ramanan, and Srinivasa Narasimhan. 4d visualization of dynamic events In Proceedings of from unconstrained multi-view videos. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 53665375, 2020. 2 [4] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Muller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023. 6 [5] Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yitong Dong, Yijin Li, and Hongsheng Li. Context-pips: Persistent independent particles demands spatial context features. In Advances in Neural Information Processing Systems, pages 5528555298, 2023. [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 [7] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 3 [8] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 2, 3 [9] Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, and Huaping Liu. V3d: Video diffusion models are effective 3d generators. arXiv preprint arXiv:2403.06738, 2024. 3 [10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 3 [11] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adria Recasens, Lucas Smaira, Yusuf Aytar, Joao Carreira, Andrew Zisserman, and Yi Yang. TAP-vid: benchmark for tracking any point in video. Advances in Neural Information Processing Systems, 35:1361013626, 2022. 3, 6 [12] Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, and Andrew Zisserman. Tapir: Tracking any point with per-frame initialization and temporal refinement. ICCV, 2023. [13] Carl Doersch, Pauline Luc, Yi Yang, Dilara Gokay, Skanda Koppula, Ankush Gupta, Joseph Heyward, Ignacio Rocco, Ross Goroshin, Joao Carreira, and Andrew Zisserman. BootsTAP: Bootstrapped training for tracking-any-point. Asian Conference on Computer Vision, 2024. 6 [14] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 73467356, 2023. 2, 3 [15] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthesis from dynamic monocular video. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 57125721, 2021. 3 [16] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, and Angjoo Kanazawa. Monocular dynamic view synthesis: reality check. Advances in Neural Information Processing Systems, 35:3376833780, 2022. 2, 3 [17] Gonzalo Martin Garcia, Karim Abou Zeid, Christian Schmidt, Daan de Geus, Alexander Hermans, and Bastian Leibe. Fine-tuning image-conditional diffusion models is easier than you think. arXiv preprint arXiv:2409.11355, 2024. [18] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh- (Derek) Liu, Henning Meyer, Yishu Miao, Derek Ti Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: scalable dataset generator. 2022. 6 [19] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-toInternaimage diffusion models without specific tuning. tional Conference on Learning Representations, 2024. 2 [20] Adam Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. Particle video revisited: Tracking through occlusions using point trajectories. In ECCV, 2022. 3, 6 [21] William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood. Flexible diffusion modeling of long videos. Advances in Neural Information Processing Systems, 35:2795327965, 2022. 2, 3 [22] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 3 [23] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 3 [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3, 5 [25] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Imagen Poole, Mohammad Norouzi, David Fleet, et al. video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 2, 3 [26] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 3 [27] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. In Proc. ECCV, 2024. 3, 6 [28] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. 3 [29] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth In Proceedings of the IEEE/CVF Conference estimation. on Computer Vision and Pattern Recognition, pages 9492 9502, 2024. 3 [30] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, [31] Skanda Koppula, Ignacio Rocco, Yi Yang, Joe Heyward, Joao Carreira, Andrew Zisserman, Gabriel Brostow, and Carl Doersch. TAPVid-3D: benchmark for tracking any point in 3D. Advances in Neural Information Processing Systems, 2024. 6 [32] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. Dense optical tracking: Connecting the dots. In CVPR, 2024. 3, 6, 12 [33] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3 [34] Alexander Quinn Nichol and Prafulla Dhariwal. Improved In International denoising diffusion probabilistic models. conference on machine learning, pages 81628171. PMLR, 2021. 3 [35] Ege Ozguroglu, Ruoshi Liu, Dıdac Surıs, Dian Chen, Achal pix2gestalt: Dave, Pavel Tokmakov, and Carl Vondrick. In 2024 Amodal segmentation by synthesizing wholes. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 39313940. IEEE Computer Society, 2024. 3 [36] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [37] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. UniDepth: Universal monocular metric depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 6 [38] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 3 [39] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 5 [40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3 [41] Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang, Yujun Shen, Matteo Poggi, and Yiyi Liao. Learning temporally consistent video depth from video diffusion priors. arXiv preprint arXiv:2406.01493, 2024. 3 [42] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion In ACM SIGGRAPH 2024 Conference Papers, modeling. pages 111, 2024. 2, [43] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In European conference on computer vision, pages 402419. Springer, 2020. 3, 4 [44] Zachary Teed and Jia Deng. Raft-3d: Scene flow using rigidmotion embeddings. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83758384, 2021. 4, 12 [45] Basile Van Hoorick, Purva Tendulkar, Dıdac Surıs, Dennis Park, Simon Stent, and Carl Vondrick. Revealing occlusions with 4d neural fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 30113021, 2022. 3 [46] Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sargent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi Zheng, and Carl Vondrick. Generative camera dolly: Extreme monocular dynamic novel view synthesis. 2024. 2, 3, 8 [47] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision, pages 439457. Springer, 2025. 3 [48] Fu-Yun Wang, Ling Yang, Zhaoyang Huang, Mengdi Wang, and Hongsheng Li. Rectified diffusion: Straightness is not your need in rectified flow. arXiv preprint arXiv:2410.07303, 2024. [49] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski, and Noah Snavely. Tracking everything everywhere all at once. In International Conference on Computer Vision, 2023. 6 [50] Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from single video. arXiv preprint arXiv:2407.13764, 2024. 2, 3 [51] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 3 [52] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2031020320, 2024. 2, 3 [53] Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker: In Proceedings of Tracking any 2d pixels in 3d space. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3, [54] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv:2406.09414, 2024. 6 [55] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3, 5 [56] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 3 [57] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and MingHsuan Yang. Monst3r: simple approach for estimatarXiv preprint ing geometry in the presence of motion. arxiv:2410.03825, 2024."
        },
        {
            "title": "Appendix",
            "content": "A. More Details about D3D-PT RAFT [44] is designed for optical flow estimation, i.e., re- : IHW 2 gressing per-pixel displacement field ft RHW 2 that maps each source pixel to the coordinate xt in the target video frame t. Suppose the coordinate of the pixels in the source image is x0, the target coordinate can be derived from xt = x0 + ft. DOT [32] adopt RAFT to refine the dense point tracking. Drawing inspiration from RAFT and DOT, we design the network architecture of our D3D-PT by iteratively refining the dense 3D point tracking, including the 2D point tracking xt, the visibility vt, and the depth dt. We refine the 2D point tracking xt and the visibility vt following RAFT, and refine the depth dt with recurrent decoder that is loosely coupled to the 2D point tracking decoder. RAFT encodes image features with shallow CNNs, computes correlation volumes for all pairs of pixel features, builds 4-layer correlation pyramids by average pooling, and iteratively refines the correspondence estimation with recurrent decoder according to the correlation pyramids and the image features. Specifically, given pair of RGB images, RAFT encodes them with siamese network as feat0 RHW and featt RHW corresponding to the source image and the target image. H, W, denote the height, width, and channels of the encoded feature map. The source image is additionally encoded with CNN to provide the context information inp RHW C/2. With the correlation pyramid Corr built from feat0 and featt, RAFT encodes the motion feature mt,i from the flow and visibility estimated at the last iteration ft,i1, vt,i1, and cropped correlation information Corr(xt,i1): mt,i = otionEncf low(ft,i1, Corr(xt,i1), vt,i1). (5) We show the structure of the motion encoder in Fig. A2. There is ReLU [1] activation function between the convolution layers. The motion features mt,i will be fed to the ConvGRUf low to estimate the flow update: netrgb,i = ConvGRUf low(mt,i, netrgb,i1, inp), ft,i = lowHead(netrgb,i), vt,i = isHead(netrgb,i). (6) otionEncf low, ConvGRUf low, and lowHead are standard blocks used in DOT. netrgb is an iterativelly updated hidden state. ConvGRUf low is the recurrent decoder used by RAFT. lowHead, isHead, and DepthHead used to regress residual flow, visibility, and depth share similar structures. We show them in Fig. A3, Fig. A4, and Methods AJ < δx DAVIS avg RGB-RAFT (DOT) [32] RGBD-RAFT D3D-PT (Ours) 60.1 55.7 63.4 74.5 71.6 75.2 OA 89.0 86.7 89.4 Table A1. Ablation Study on DAVIS. Fig. A5. Besides updating flow estimation, the motion feature mt,i and the hidden feature netrgb,i1 will also be used in the depth refinement as elaborated in the main paper. We assign netrgb,i1 to net used in the main paper. B. Ablation Study Dense 3D Point Tracking. We conduct an ablation study on DAVIS to show the superiority of our loosely coupled dense 3D point tracking design. As shown in Tab. A1, DOT [32] can be regarded as the baseline that takes the original RAFT to estimate the dense 2D point tracking. Directly extending the RAFT to accept RGB-D images as inputs (RGBD-RAFT) degrades the point tracking accuracy seriously because the depth distribution in training is different from the distribution of depth estimated in the inference stage. On the contrary, our loosely coupled 3D point tracking design D3D-PT improves the tracking accuracy on the DAVIS. GS-DiT GS-DiT generates video conditioned on the input video, which is rendered from the pseudo 4D Gaussian field. The generated video is expected to fix the artifacts, such as the blurs and the incomplete areas, derived from the imperfect 4D Gaussians. Such process is similar to video inpainting, so we set simple baseline that trains DiT-based video inpainting model to reveal the essence of building the pseudo 4D Gaussian field for training. We corrupted the videos with two simple masks: evenly distributed dispersed masks (Inpainting-A) and fixed-size grid masks at random locations (Inpainting-B). We train all models with 5000 iterations at 320 512 resolution. We expect that the masked region occupies 40% of the images, so we set the dispersed occlusion ratio as 40% in Inpainting-A and the grid mask with the size of 256 256 in Inpainting-B. We remove part of the information to obtain the corrupted video according to the generated random mask as the condition video. As shown in Fig. A1, our GS-DiT obtains clear details and infers reasonable unobserved regions. In contrast, both inpainting models (Inpainting-A and Inpainting-B) fail to infer the incomplete regions and present blurry video frames. Moreover, the blurry effects is severe in the video frame generated by Inpainting-A. This comparison shows that building the pseudo 4D Gaussian field with our D3DPT is the cornerstone of the GS-DiT. Figure A1. Comparison with video inpainting. (a) is the input frame. (b) is the video frame generated by our GS-DiT. (c) and (d) are the video frames generated by Inpainting-A and Inpainting-B. Figure A2. The structure of otionEncf low. Figure A4. The structure of isHead. Figure A3. The structure of lowHead. Figure A5. The structure of DepthHead."
        }
    ],
    "affiliations": [
        "Avolution AI",
        "Centre for Perceptual and Interactive Intelligence",
        "Multimedia Laboratory, The Chinese University of Hong Kong"
    ]
}