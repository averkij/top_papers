{
    "paper_title": "MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive Multimodal Retrieval",
    "authors": [
        "Siyue Zhang",
        "Yuan Gao",
        "Xiao Zhou",
        "Yilun Zhao",
        "Tingyu Song",
        "Arman Cohan",
        "Anh Tuan Luu",
        "Chen Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce MRMR, the first expert-level multidisciplinary multimodal retrieval benchmark requiring intensive reasoning. MRMR contains 1,502 queries spanning 23 domains, with positive documents carefully verified by human experts. Compared to prior benchmarks, MRMR introduces three key advancements. First, it challenges retrieval systems across diverse areas of expertise, enabling fine-grained model comparison across domains. Second, queries are reasoning-intensive, with images requiring deeper interpretation such as diagnosing microscopic slides. We further introduce Contradiction Retrieval, a novel task requiring models to identify conflicting concepts. Finally, queries and documents are constructed as image-text interleaved sequences. Unlike earlier benchmarks restricted to single images or unimodal documents, MRMR offers a realistic setting with multi-image queries and mixed-modality corpus documents. We conduct an extensive evaluation of 4 categories of multimodal retrieval systems and 14 frontier models on MRMR. The text embedding model Qwen3-Embedding with LLM-generated image captions achieves the highest performance, highlighting substantial room for improving multimodal retrieval models. Although latest multimodal models such as Ops-MM-Embedding perform competitively on expert-domain queries, they fall short on reasoning-intensive tasks. We believe that MRMR paves the way for advancing multimodal retrieval in more realistic and challenging scenarios."
        },
        {
            "title": "Start",
            "content": "MRMR: REALISTIC AND EXPERT-LEVEL MULTIDISCIPLINARY BENCHMARK FOR REASONINGINTENSIVE MULTIMODAL RETRIEVAL Siyue ZhangN Yuan GaoJ Xiao ZhouJ Yilun Zhao Tingyu Song Arman Cohan Anh Tuan Luu Chen ZhaoS Nanyang Technological University Shanghai Jiao Tong University AUniversity of the Chinese Academy of Sciences CCenter for Data Science, New York University Yale University SNYU Shanghai 5 2 0 2 0 1 ] . [ 1 0 1 5 9 0 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce MRMR, the first expert-level multidisciplinary multimodal retrieval benchmark requiring intensive reasoning. MRMR contains 1,502 queries spanning 23 domains, with positive documents carefully verified by human experts. Compared to prior benchmarks, MRMR introduces three key advancements. First, it challenges retrieval systems across diverse areas of expertise, enabling fine-grained model comparison across domains. Second, queries are reasoningintensive, with images requiring deeper interpretation such as diagnosing microscopic slides. We further introduce Contradiction Retrieval, novel task requiring models to identify conflicting concepts. Finally, queries and documents are constructed as imagetext interleaved sequences. Unlike earlier benchmarks restricted to single images or unimodal documents, MRMR offers realistic setting with multi-image queries and mixed-modality corpus documents. We conduct an extensive evaluation of 4 categories of multimodal retrieval systems and 14 frontier models on MRMR. The text embedding model Qwen3-Embedding with LLM-generated image captions achieves the highest performance, highlighting substantial room for improving multimodal retrieval models. Although latest multimodal models such as Ops-MM-Embedding perform competitively on expert-domain queries, they fall short on reasoning-intensive tasks. We believe that MRMR paves the way for advancing multimodal retrieval in more realistic and challenging scenarios."
        },
        {
            "title": "INTRODUCTION",
            "content": "LLM-based agents, such as DeepResearch (OpenAI, 2024; Qiao et al., 2025), have been widely applied in domains including science, engineering, medicine, and finance (Zhao et al., 2025; Tang et al., 2024; Barry et al., 2025; Phan et al., 2025). These systems move beyond the intrinsic knowledge of LLMs by actively retrieving and integrating external information, making strong and robust retrieval component essential (Chen et al., 2025). In practice, many expert-domain applications rely on multimodal information, underscoring the need for retrieval methods that can handle queries and documents spanning both visual and textual modalities, or even interleaved imagetext content (Zhang et al., 2021; Liu et al., 2021; 2023). For instance, given medical image, the agent system should retrieve similar cases or guidelines to support clinical decisions. While existing multimodal retrieval benchmarks have made progress, they are insufficient to capture the complexity of agentic scenarios. We identify three key limitations: (1) Multidisciplinary expert domains: most multimodal benchmarks are built on Wikipedia text and images, focusing on generaldomain knowledge (Hu et al., 2023; Chen et al., 2023; Zhang et al., 2025b). However, state-of-theart LLMs already demonstrate strong capabilities in handling such knowledge (Team et al., 2025), making it essential to develop benchmarks for high-stakes expert domains such as medicine, science, Equal contribution. 1 Figure 1: Overview of the MRMR benchmark. MRMR includes 1,502 expert-annotated examples, covering 23 domains across 6 disciplines. It is specifically designed to assess multimodal retrieval models in expert-level, reasoning-intensive tasks. Notably, we originally introduce the Contradiction Retrieval task in the multimodal setting, which requires retrieving documents that conflict with the user query and features deeper logical reasoning. and engineering. (2) Reasoning intensity: existing benchmarks primarily target semantic matching and information-seeking tasks, whereas real-world queries often involve expert-domain images and require deeper understanding and logical reasoning over them. (3) Image-text interleaving: prior benchmarks mostly support single-image queries with supplementary text, yet real-world queries and documents typically consist of interleaved text and multiple images (Zhang et al., 2025b). To address these gaps, we introduce MRMR, comprehensive benchmark measuring retrieval models in expert-level Multidisciplinary and Reasoning-intensive Multimodal Retrieval. Figure 1 presents an overview of our benchmark. MRMR consists of 1,502 expert-annotated examples, categorized into three types of retrieval tasks: (1) Knowledge for retrieving web pages related to queries involving multiple expert-domain images; (2) Theorem for retrieving theorems involved in solving multimodal math problems; and (3) Contradiction for retrieving contradictory statements or rules given case description. Specifically, we derive complex multidisciplinary queries from established Visual Question Answering (VQA) benchmarks (Yue et al., 2024; 2025) and assign expert annotators to collect positive documents from Internet. To build sizable corpus, we additionally include negative documents from knowledge-intensive collections (Wang et al., 2024a; Su et al., 2025). To further elevate the reasoning challenge, we originally introduce Contradiction Retrieval, which requires models not only to detect semantic relevance but also to perform logical reasoning to identify conflicting concepts. To foster deeper integration of visual and textual content, we represent both queries and documents in an interleaved multimodal format. We conduct an extensive evaluation on MRMR across four main categories of multimodal retrieval paradigms and 14 representative models. The results reveal that current multimodal retrieval systems consistently underperform text-only retrievers with image captioning on knowledgeand reasoningintensive multimodal queries. The highest score of 52.1 nDCG@10 is achieved by the text embedding model Qwen3-Embedding (Zhang et al., 2025d) combined with LLM-based image captioning. The best-performing multimodal model, Ops-MM-Embedding (OpenSearch-AI, 2025), trails by 6.7 points, mainly due to its limited reasoning capabilities rather than domain expertise. Its performance drops from 67.4 on Knowledge tasks to 30.1 and 36.6 on Theorem and Contradiction tasks, even though the corpora for these two tasks are much smaller than that of Knowledge. More importantly, the multidisciplinary setup in MRMR reveals substantial performance differences across models and domains. For instance, Ops-MM-Embedding surpasses the second-best model, MM-Embed (Lin et al., 2025), in the Art discipline, whereas their performances are comparable in the Medicine discipline. We hope our benchmark and findings will help progress in multimodal retrieval. 2 Table 1: Comparison of multimodal retrieval benchmarks and MRMR. In the Modality column, I indicates retrieving image documents using text query. The #Domain column reports the number of domains; Open denotes datasets built from Wikidata with general domains. The Expert, Reason, and Interleaved columns indicate whether expert knowledge is required, whether intensive reasoning is involved, and whether data are in the interleaved image-text format. Benchmarks Modality Retrieval Type #Domain NIGHTS SciMMIR IT EDIS Wiki-SS IT WebQA IT ViDoRe IT MMDocIR IT FashionIQ IT CIRR IT CIRCO IT IT InfoSeek IT IT OVEN wikiHow-TIIR IT IT Visual Similarity Image Caption Image Caption Document QA Document QA Document QA Document QA Composed Image Composed Image Composed Image VQA VQA VQA MRMR IT IT VQA Open 11 Open Open Open 10 10 1 Open Open Open Open Open"
        },
        {
            "title": "2 RELATED WORK",
            "content": "#Query Expert? Reason? 20K 530K 3,241 3,610 2,511 3,810 1,658 12,238 4,148 1,020 1.35M 18,341 7,654 Interleaved? 1,502 Benchmarking multimodal retrieval. As illustrated in Table 1, existing multimodal retrieval datasets mainly focus on semantic matching or information-seeking tasks. Early semantic matching benchmarks are built from paired imagetext data, where the text is semantically aligned with the image (Liu et al., 2023; Wu et al., 2024; Xiao et al., 2025; Jiang et al., 2025c), and the task is to retrieve the corresponding modality. Composed Image Retrieval (CIR) emerges as challenging task that allows users to search for target images using multimodal query, comprising reference image and modification text specifying the users desired changes to the reference image (Zhang et al., 2021; Liu et al., 2021; Baldrati et al., 2023; Zhang et al., 2024). Information-seeking benchmarks either retrieve supporting evidence for visual questions (Hu et al., 2023; Chen et al., 2023) or retrieve multimodal documents for textual queries (Ma et al., 2024; Macé et al., 2025; Dong et al., 2025). As all prior studies focus on single-image inputs, TIIR (Zhang et al., 2025b) proposes more realistic setup in which the query and document consist of interleaved textimage sequences supporting multiple images. However, it is limited to searching general-domain wikiHow tutorials. To further advance multimodal retrieval, we construct MRMR, the first benchmark comprising complex multidisciplinary queries that require in-depth reasoning in the interleaved textimage format. Multimodal retrieval models and multimodal retrieval augmented generation. State-of-theart multimodal retrieval models commonly rely on large pre-trained encoders such as CLIP (Radford et al., 2021) and BLIP (Li et al., 2023), which map images and texts into shared embedding space. Their outputs are often combined using fusion strategies (e.g., score fusion) to integrate information across modalities (Wei et al., 2024). More recent works adapt multimodal large language models (MLLMs) for universal multimodal embeddings by fine-tuning them on diverse retrieval tasks (Jiang et al., 2025b; Zhang et al., 2025c; Jiang et al., 2025c; Lin et al., 2025). In these approaches, multimodal queries are processed through the MLLM, and the hidden states from the final transformer layer, typically the last token representation, are used as the dense embedding for retrieval. In this work, we benchmark diverse set of multimodal retrieval approaches, including text retrievers with image captioning, text and image two-stream models with vector fusion, and multimodal retrievers. Additionally, thanks to advances in both retriever and generative models, multimodal retrieval-augmented generation (MM-RAG) has emerged as key application (Hu et al., 2025; Jiang et al., 2025a; Wu et al., 2025b; Zhan et al., 2025; Wasserman et al., 2025). While various MM-RAG benchmarks have been introduced, most focus on evaluating response generation and lack evidence-level relevance annotations, making it impractical to assess retrieval performance and its contribution within MM-RAG (Chen et al., 2025). 3 Table 2: Data statistics of MRMR. For each dataset, we show the number of queries (Q) and documents (D), the average number of positive documents (D+) per example, the average number of text tokens of queries and documents (measured by the GPT-2 tokenizer (Radford et al., 2019), not including task instruction text), the average number of images in queries and documents, and sources of queries and documents. Knowledge datasets share common retrieval corpus, while Theorem datasets share another. Examples for each dataset can be found in Appendix G. Total Number Avg. #Text Avg. #Images Source Ex. Dataset D+ Q D Art Medicine Science Humanities Math Physics Engineering Business Negation Vehicle Design Traffic Case 157 167 137 94 72 107 236 164 200 88 80 26,223 26,223 26,223 26, 14,257 14,257 14,257 14,257 4 700 796 1.8 2.2 1.8 1.9 2.6 2.1 2.1 2.2 1.0 1. 1.8 15.4 32.0 32.1 54.5 62.1 55.6 50.5 63.8 Knowledge 421.6 421.6 421.6 421.6 1.1 1.1 1.2 1. Theorem 364.3 364.3 364.3 364.3 1.0 1.0 1.1 1.0 Contradiction 0.0 152.5 12.8 107. 19.5 123.3 1.0 1.0 1.0 0.72 0.72 0.72 0.72 0.001 0.001 0.001 0. 0.00 0.04 0.58 MMMU-Pro knowledge question PIN-14M, Web pages MMMU-Pro calculation question BRIGHT, Web pages COCO DesignQA Synthetic Synthetic Design Rules Driving Handbook Fig. 9 Fig. 10 Fig. 11 Fig. 12 Fig.13 Fig.14 Fig.15 Fig.16 Fig.17 Fig. Fig.19 Reasoning-intensive retrieval. Beyond keywordand semantic-based information retrieval, BRIGHT (Su et al., 2025) has introduced the first benchmark in the text domain that requires intensive reasoning to identify relevant documents. For example, given new math or physics problem, the retrieval system is expected to provide previously solved problems using the same theorems or relevant theorem statements. To tackle this challenge, recent methods train the text retrievers using synthetic datasets containing complex queries and hard negatives (Weller et al., 2025; Das et al., 2025; Zhang et al., 2025a; Long et al., 2025; Shao et al., 2025; FlagEmbedding, 2025). Our work extends reasoning-intensive retrieval into the multimodal domain. MRMR is constructed by sourcing expert-level queries from the multimodal understanding and reasoning benchmark MMMU (Yue et al., 2024), collecting image-text interleaved documents from web pages, and obtaining relevance annotations from human experts."
        },
        {
            "title": "3 MRMR BENCHMARK",
            "content": "3.1 TASK FORMULATION We define the task of multimodal retrieval as follows. Let = {q1, . . . , qn} be the set of queries and = {d1, . . . , dm} the document corpus. Each query and document is represented as sequence of segments (x1, . . . , xk), where each segment can be either text or an image. For query q, document can be either positive document d+ (relevant) or negative document (non-relevant). In reasoning-intensive retrieval, document is considered relevant if it provides principles or theorems that support the reasoning chain required to answer query (Su et al., 2025). Unlike prior studies (Xiao et al., 2025; Dong et al., 2025), we do not constrain the corpus to uniform data types, reflecting more realistic retrieval scenarios. To evaluate diverse reasoning capabilities, we design three types of retrieval tasks in MRMR: Knowledge. It emphasizes reasoning over broad expert domain knowledge. For multimodal query, document is relevant if expert annotators confirm that it contributes to reasoning about the query by providing critical concepts or theoretical foundations. Theorem. It targets the theorem-based reasoning over calculation problems. For multimodal calculation query, document is relevant if it conveys the same underlying theorem or formula needed to solve the problem. 4 Contradiction. It requires logical reasoning to detect conflicting or inconsistent concepts. For multimodal case description query, document is relevant if it provides the rule or requirement that the query violates."
        },
        {
            "title": "3.2 KNOWLEDGE: RETRIEVING WEB PAGES THAT HELP ANSWER QUESTIONS",
            "content": "MMMU (Yue et al., 2024) is one of the most widely used benchmark for evaluating multi-discipline multimodal understanding in MLLMs. Its robust version, MMMU-Pro (Yue et al., 2025), excludes questions solvable by text-only models, expands the candidate options, and provides verified correct answers. We repurpose the knowledgeand reasoning-intensive questions in MMMU-Pro as queries and construct corpus of imagetext interleaved documents. The positive documents D+ are scraped from relevant websites referenced by the GPT-Search1 model (OpenAI, 2024) and verified by human experts; while negative documents are augmented by sampling from the multimodal collection PIN-14M (Wang et al., 2024a) (see Figure 2). Selecting questions. We prompt GPT-52 to categorize MMMU-Pro questions into two groups, i.e., knowledge-based and calculation questions. We adopt calculation questions for the Theorem subset in Section 3.3. For knowledge questions, we then instruct GPT-5 to filter out questions that require only superficial reasoning over text and images, without reliance on external domain expertise. For the remaining questions, we generate detailed descriptions for each associated image using GPT-5, which we include as part of the input context for subsequent steps. Constructing positive and hard negative documents. Unlike keywordor semantic-based multimodal retrieval benchmarks, collecting positive documents for our queries is more time-consuming because it requires identifying and validating multimodal sources that support the querys answer. To address this, we design semi-automated pipeline with human expert annotators. Specifically, for each query, given the GPT-5-generated image descriptions and ground-truth answer, we prompt GPT-Search to reason over the question and generate an explanation for the correct answer with reference web links pointing to diverse materials such as Wikipedia, books, academic papers, and blogs. To preserve the completeness of multimodal content, we capture these webpages as PDFs, apply MonkeyOCR (Li et al., 2025) to extract interleaved text and images, and split the content into chunks while preserving image references. Resulting documents are then screened by GPT-5 and validated by human experts about whether they support the correct answer. Documents with GPThuman agreement on relevance are retained as positives, those agreed irrelevant as hard negatives, while ambiguous cases (3060% across domains) are discarded. In cases where GPT-Search fails to retrieve relevant documents (38.2% of questions), expert annotators are instructed to search the web and create one supporting document, optionally including image links within the text. Due to the complexity of the questions, the number of positive documents per query is typically fewer than four. We annotate data anonymously through the Turkle platform (HLT-COE@JHU, 2025), with detailed guidelines provided in Appendix B. Constructing additional negative documents. After the previous step, we obtain 993 cleaned and annotated documents for 555 queries. To construct sizable retrieval corpus comparable to (Xiao et al., 2025; Su et al., 2025), we supplement these with negative documents sampled from the large-scale multimodal collection PIN-14M (Wang et al., 2024a), which contains knowledgeintensive resources such as medical articles from PubMed Central (PMC)3 and web content from OBELICS (Laurençon et al., 2023). Given the wide topic coverage and large number of documents in PIN-14M, we assume low probability of false negatives for our sampled documents. We validate this assumption through manual error analysis in Section 5.1. In total, we curate corpus of 26,223 documents, including text only, image only, and text-image interleaved.4 1GPT-Search refers to the version gpt-4o-search-preview-2025-03-11 throughout this work. 2GPT-5 refers to the version gpt-5-2025-08-07 throughout this work. 3https://www.ncbi.nlm.nih.gov/pmc/ 4The corpus could be further expanded by sampling additional expert-domain documents, which naturally increases retrieval difficulty and the probability of false negatives. We leave it as future work. 5 Figure 2: An overview of the data construction workflow for MRMR (Knowledge). We select and convert knowledgeand reasoning-intensive questions from MMMU-Pro (Yue et al., 2025) into retrieval queries. Web pages such as Wikipedia, blogs, and papers referenced by the GPT-Search model during reasoning are processed into documents through screen capturing, OCR (Li et al., 2025), and chunking. The relevance of resulting documents is first evaluated by GPT and then verified by expert annotators. Lastly, we source negative documents from the knowledge-intensive multimodal collection PIN-14M (Wang et al., 2024a) to construct sizable corpus. 3.3 THEOREM: RETRIEVING RELEVANT THEOREMS THAT SOLVE PROBLEMS As introduced by Su et al. (2025), retrieving relevant theorem statements can assist users in solving new math or physics problems. We extend this formulation to the multimodal domain by leveraging challenging calculation problems from MMMU-Pro. In this setting, the query is image-centric calculation problem, and the corpus consists of theorem descriptions across domains such as mathematics, physics, engineering, and business. document is regarded as positive if it describes theorem applicable to solving the query problem. Selecting questions. From the calculation questions in MMMU-Pro, we first use GPT-5 to exclude questions that explicitly state the required theorem in the text. The remaining questions are then organized into four major domains: Math, Physics, Engineering, and Business. The Engineering domain further includes areas such as Mechanical Engineering, Computer Science, and Electronics, while the Business domain covers Finance, Economics, Marketing, and related fields. Then, we prompt GPT-5 to reason through each multimodal question, produce final answer, and summarize the key theorems used in the solution. We exclude questions for which GPT-5 produces incorrect answers, with final set of 579 questions. Constructing positive and negative documents. We adopt the theorem statements from BRIGHT (Su et al., 2025) as the primary retrieval corpus (13.8k documents), reflecting the realistic setting where most theorems are expressed in text. For each question, the summarized key theorems are used as queries to retrieve the top-10 candidate statements from the corpus with the Qwen3Embedding model (Zhang et al., 2025d). Among these candidates, GPT-5 identifies the most relevant theorem statements, which are retained as positive documents, while the rest serve as negatives. Constructing additional positive documents. Not all theorems have relevant counterparts in BRIGHT. To address this, we scrape additional theorem statements, optionally accompanied by illustrative images, from webpages such as Wikipedia, following the OCR pipeline described in Section 3.2. GPT-5 then rewrites these theorems to match the format of BRIGHT statements. Finally, we deduplicate the scraped documents to ensure consistent and complete retrieval corpus. Consequently, 63.6% of the positive documents are sourced from webpages, with the remainder drawn from the BRIGHT corpus. More details are presented in Appendix C. 3.4 CONTRADICTION: RETRIEVING CONTRADICTORY RULES AND REQUIREMENTS Most existing datasets emphasize retrieving positively supporting evidence for query (Xiao et al., 2025; Chen et al., 2023; Dong et al., 2025). However, retrieving contradictory information could be of great importance especially in expert domains. For example, user may provide case description 6 and seek evidence of violation of laws, policies, or guidelines, as shown in Figure 19. In this setting, the query is case description (e.g., traffic or vehicle design cases), while the corpus comprises mandated rules (e.g., driving theory handbooks or design requirements). document is considered positive if it contains the statement or rule contradicting the query case. Unlike traditional retrieval tasks, this new formulation requires not only semantic matching between query and document but also deep logical reasoning to identify conflicting concepts. Negation. To study contradiction retrieval, we first design synthetic task inspired by the negation benchmark NegBench (Alhamoud et al., 2025). Given an image from COCO (Lin et al., 2014) with ground truth object annotations, we synthesize four candidate text descriptions: three accurately reflecting the objects and one containing contradiction, either by asserting the existence of nonexistent object or the absence of an existent one. Multimodal retrievers are evaluated on their ability to pinpoint the text description with contradiction relative to the given image. For example, in Figure 17, the query image shows keyboard on the table, while the positive document explicitly states that none is present, revealing contradiction. More details are provided in Appendix D.1. Vehicle Design. To evaluate contradiction retrieval in engineering documents, we construct vehicle design task by leveraging the Formula SAE Rulebook and design cases from the DesignQA dataset (Doris et al., 2025). In industrial product design, designers must review hundreds of pages of requirement documents to ensure their designs comply with specifications. To assist designers, retrieval systems are expected to identify the specific sections that design case fails to satisfy. For example, in Figure 18, the vehicles wheelbase in the design is shorter than the required minimum, indicating contradiction. During data preparation, we introduce variations to the design cases and chunk the lengthy design document, as detailed in Appendix D.2. Traffic Case. Retrieval systems have been applied to legal documents to assist legal professionals in preparing arguments and citations (Feng et al., 2024). To evaluate this capability in multimodality, we construct traffic case task to assess whether retrievers can identify which driving rules are violated in traffic cases. We build the corpus by chunking official driving handbooks (Singapore Police Force, 2017) into sections. Meanwhile, we build the query set by selecting dozens of driving rules, each linked to several annotated violation cases. We augment these violation cases by replacing key textual elements with AI-generated images using Qwen-Image (Wu et al., 2025a). For example, as shown in Figure 19, car is driving only 3 meters behind the vehicle ahead significantly less than the required safe distance. Further details are provided in Appendix D.3."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP We evaluate 4 types of multimodal retrieval setups with 14 frontier models, as follows: (1) Text models with image caption (T2T): We assess text retrievers, namely BGE-M3 (Chen et al., 2024), NE-Embed-V2 (Lee et al., 2025), and Qwen3-Embedding-8B (Zhang et al., 2025d), by pairing with MLLM-generated image captions (see Appendix E.1 for details). (2) Text and image two-stream models with vector fusion (IT2IT): We evaluate CLIP-style two-stream models, including EVACLIP (Sun et al., 2023), SigLIP (Zhai et al., 2023), OpenCLIP (Cherti et al., 2023), and JinaCLIP (Koukounas et al., 2024), by simple vector-fusion strategy. Given an input sequence, we concatenate all text chunks for one text embedding t, while all images are concatenated vertically for another image embedding i. Following MTEB (Xiao et al., 2025), the final score is computed using the fused embedding = + i. (3) Multimodal models with merged image (IT2IT): We evaluate multimodal retrievers including VISTA (Zhou et al., 2024), E5-V (Jiang et al., 2025b), MM-Embed (Lin et al., 2025), VLM2Vec (Jiang et al., 2025c), Ops-MM-Embedding (OpenSearch-AI, 2025) and GME-Qwen2-VL (Zhang et al., 2025c). Since these models support only single-image input, multiple images are concatenated in the same way as for two-stream models. (4) Multimodal models with document as image (T2I): We also include the document retrieval paradigm that receives text-only query and encode entire multimodal documents as screenshot images, such as ColPali (Faysse et al., 2025). Because these models are trained for text queries, query images are replaced with LLM-generated captions, similar to the text retriever setup. Besides, we note that native imagetext interleaved model, TIIR (Zhang et al., 2025b), has been introduced and is expected to 7 Table 3: The performance of retrieval models on MRMR. We report nDCG@10 for all subtasks except Negation, for which we use Hit@1: Art, Medicine (Med.), Science (Sci.), Humanities (Hum.), Math, Physics (Phy.), Engineering (Eng.), Business (Bus.), Negation (Neg.), Design, and Traffic. Avg. denotes the average score across 11 subtasks. The best score on each subtask is highlighted in bold, and the second best is underlined. Model Art Med. Sci. Hum. Math Phy. Eng. Bus. Neg. Design Traffic Knowledge Theorem Contradiction Avg. Text Models with Image Caption (T2T) BGE-M3 NV-Embed-v2 Qwen3-Embedding 48.6 70.7 71.9 30.0 46.8 53.2 42.4 65.7 72.5 45.6 66.6 74. 13.5 26.2 35.9 15.7 27.3 48.1 18.3 29.0 39.6 26.6 36.9 43.7 16.0 12.5 12.0 Text and Image Two-Stream Models with Vector Fusion (IT2IT) EVA-CLIP SigLIP OpenCLIP JinaCLIP VISTA E5-V MM-Embed VLM2Vec GME-Qwen2-VL Ops-MM-Embedding 10.2 26.7 56.0 21.4 21.3 25.1 65.6 53.5 54.3 79.3 13.5 14.7 17.9 16.8 26.1 26.7 33.2 27. 12.9 12.3 22.0 10.7 6.2 6.2 5.7 8.3 10.5 5.5 5.0 5.9 9.3 4.1 7.0 8.4 11.7 7.5 9.7 10.4 Multimodal Models with Merged Image (IT2IT) 27.8 11.7 53.0 22.4 40.1 52.5 32.6 16.6 63.5 36.7 46.8 70.0 17.0 10.8 62.8 24.0 45.6 67.8 14.2 1.1 21.6 1.1 3.0 23.7 14.3 1.5 26.3 1.3 3.6 34.2 19.5 4.1 24.4 2.4 9.3 27. 14.2 2.0 31.7 2.5 4.6 35.3 Multimodal Models with Document as Image (T2I) GME-Qwen2-VL Ops-MM-Embedding ColPali 54.0 67.7 36.1 40.7 48.8 29.9 59.0 67.7 42. 50.1 63.9 29.2 15.7 24.4 5.7 22.7 29.3 14.8 20.5 25.7 12.0 32.5 33.7 24.6 8.5 13.5 13.0 10. 20.0 11.5 7.0 11.5 15.0 8.0 14.5 10.5 28.5 25.9 42.1 67.8 4.4 4.9 8.1 16.5 20.2 3.7 23.8 5.6 26.3 55.9 56.1 59.8 19. 17.4 42.2 54.2 5.4 9.6 12.4 9.7 9.4 2.1 34.9 18.3 29.6 45.8 40.1 46.3 18.2 27.3 42.4 52.1 10.8 12.0 17.3 13. 19.1 8.2 37.7 16.3 25.3 45.4 36.9 43.4 23.7 best fit the interleaved format of MRMR; however, it is not publicly available. We provide details of each model in Appendix E.1. Following prior work (Xiao et al., 2025; Su et al., 2025), we use nDCG@10 as the main evaluation metric except Negation. Since each query in Negation has exactly one gold document among four candidates, we adopt Hit@1 as the main metric for this task. 4.2 MAIN RESULTS Multimodal retrieval systems lag behind text retrieval-based approaches on knowledgeand reasoning-intensive images. As shown in Table 3, the text retriever Qwen3-Embedding combined with LLM-based image captioning achieves the highest performance (52.1 nDCG@10). Although captions may omit certain visual details, they provide rich contextual descriptions and additional knowledge that substantially benefit retrieval. In contrast, multimodal systems struggle with the expert-level query images in MRMR, which often require deep reasoning, such as diagnosing microscopic tissue sections (Figure 1). CLIP-style two-stream models are particularly limited, as their training emphasizes alignment of superficial textimage semantics and model sizes are relatively small. The most recent MLLM-based embedding models, such as Ops-MM-Embedding, show promising results under both interleaved textimage and document-as-image paradigms, indicating the effectiveness of unified training on diverse retrieval tasks. Multimodal retrieval systems perform particularly poorly on reasoning-intensive tasks. While Ops-MM-Embedding achieves solid 67.4 nDCG@10 on Knowledge subtasks, its performance drops sharply to 30.1 and 36.6 on Theorem and Contradiction, respectively. Models such as E5-V and VLM2Vec perform even worse, essentially failing on these tasks. This gap highlights the difficulty of extracting abstract concepts from practical problems, for example linking an imagebased physics question to the relevant theorem in Figure 1. Notably, Hit@1 scores for all models on the synthetic Contradiction task Negation remain below 25%equivalent to random guessing given four candidates per query. As illustrated in the Negation example Figure 17, humans can readily detect conflicting concepts embedded within supporting evidence, yet retrieval models struggle even for strong text embedding models. Although the candidate corpora for the Design and Traffic sub8 tasks are much smaller than those of standard knowledge bases (Su et al., 2025; Dong et al., 2025), models still struggle to identify the underlying contradictions. Nevertheless, surface-level semantic matching remains useful in these settings, as it allows models to locate relevant documents without fully resolving the conflicting concepts (e.g., query concerning driving speed matched with document specifying the speed limit). These findings suggest that current retrieval models possess strong capabilities in semantic matching and information seeking, but remain fundamentally limited in their reasoning ability. Substantial differences in performance are evident across models and domains. Across all four multimodal retrieval settings, we observe wide performance difference between models. For instance, among multimodal models with merged image, the weakest model, E5-V, achieves only 8.2 nDCG@10, whereas Ops-MM-Embedding reaches 45.4 nDCG@10, revealing substantial methodological differences. As MRMR is the first multidisciplinary multimodal retrieval benchmark, it enables fine-grained domain-level evaluation. For example, as shown in the breakdown performance Table 7, MM-Embed performs competitively with Ops-MM-Embedding in medical domains such as Clinical Medicine and Diagnostics, yet lags behind in art-related tasks. We also observe pronounced variation in retrieval difficulty across domains. In the Art subtasks, systems can often succeed by matching query images to visually identical or similar artworks, which narrows the search space. However, in medical imaging, such overlap is rare, and models are required to identify underlying pathological and radiological features rather than relying on superficial visual similarity."
        },
        {
            "title": "5 ANALYSIS",
            "content": "5.1 QUALITATIVE ANALYSIS To examine model limitations, we conduct 20 error case studies, each using the top-5 documents retrieved by Ops-MM-Embedding. We have observed two major failure patterns. (1) Visual bias over contextual relevance: in the Agriculture case (Appendix Figure 7), the model ranks negative document higher because it contains nematode SEM image resembling the earthworm image in the query, even though the positive document provides detailed discussion of the key topic Fauna. Similar errors occur in Medicine, where visually similar eye images from different diseases mislead the model. (2) Failure of higher-level deduction: in the Traffic case (Appendix Figure 8), the model assigns higher score to negative document than to positive one because both depict cars, tunnels, and lane markings. However, it fails to infer that the car is crossing the line, which contradicts the positive documents instruction to Stay in lane. Although multimodal retrievers exhibit these shortcomings and lag behind text-only retrievers with image captions, we believe they remain essential because many real-world queries inherently span across modalities. Fundamentally, textual descriptions alone cannot fully capture the nuanced information in images, especially when MLLMs lack the required visual knowledge. 5.2 TEST-TIME SCALING IN RETRIEVAL Query expansion is widely used technique, recently framed as test-time scaling in retrieval (Shao et al., 2025). Prior work (Su et al., 2025) demonstrates that incorporating explicit reasoning substantially improves performance on reasoning-intensive text retrieval tasks. Motivated by this, we have conducted comparative experiments to evaluate the effectiveness for multimodal retrieval. Specifically, we prompt MLLMs, including Qwen2-VL-2B-Instruct (Wang et al., 2024b) and Qwen2.5VL-72B-Instruct (Bai et al., 2025), to generate reasoning traces, including question summarization and chain-of-thought reasoning, following (Su et al., 2025). As shown in Table 8, replacing the original queries with MLLM-generated reasoning traces leads to substantial performance improvements: +16.5 for Qwen2-VL-2B and +26.5 for Qwen2.5-VL-72B. The improvements are particularly pronounced on Knowledge tasks, whereas Theorem tasks benefit to lesser extent. Meanwhile, we observe that, without constraining output length, the larger model Qwen2.5-VL-72B produces on average 20% and 60% more tokens than Qwen2-VL-2B in Knowledge and Theorem respectively, trading higher inference cost for larger performance gains."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduce MRMR, realistic, multidisciplinary, reasoning-intensive multimodal retrieval benchmark. We leverage knowledgeand reasoning-intensive questions from MMMU-Pro and build In addition, we sizable multimodal corpus with positive documents verified by human experts. introduce Contradiction Retrieval for evaluating models logical reasoning capabilities to identify conflicts. Comprehensive evaluation shows that multimodal retrieval systems lag behind their textretrieval counterparts, indicating substantial room for improvement. Although state-of-the-art multimodal models excel in Knowledge domains, they drop nearly 30 points on reasoning-intensive tasks. We hope MRMR facilitates identifying model limitations and advancing multimodal retrieval."
        },
        {
            "title": "CODE OF ETHICS AND ETHICS STATEMENT",
            "content": "All data used in constructing MRMR are sourced from publicly available materials and are employed solely for academic research, not commercial use. We have carefully ensured that the dataset contains no private information or harmful content, such as discriminatory, violent, or unethical material. Our goal is to support socially beneficial research, and MRMR is released for unrestricted academic use. All experiments and data adhere to high scientific standards, ensuring accuracy, transparency, and reproducibility. For test-time scaling, we primarily focus on text expansion rather than image resizing and process as the text expansion has shown more significant impacts."
        },
        {
            "title": "REPRODUCIBILITY",
            "content": "Our datasets and annotation process are introduced in Section 3, and the experimental settings are described in Section 4. Specific implementation details can be found in Appendix E.1. To facilitate the reproduction of our experiments, the data is provided at https://huggingface.co/ datasets/MRMRbenchmark."
        },
        {
            "title": "REFERENCES",
            "content": "Kumail Alhamoud, Shaden Alshammari, Yonglong Tian, Guohao Li, Philip Torr, Yoon Kim, In Proceedand Marzyeh Ghassemi. Vision-language models do not understand negation. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), URL https://openaccess.thecvf.com/content/CVPR2025/papers/ 2025. Alhamoud_Vision-Language_Models_Do_Not_Understand_Negation_CVPR_ 2025_paper.pdf. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, and Alberto Del Bimbo. Zero-shot comIn Proceedings of the IEEE/CVF International posed image retrieval with textual inversion. Conference on Computer Vision (ICCV), 2023. URL https://openaccess.thecvf. com/content/ICCV2023/papers/Baldrati_Zero-Shot_Composed_Image_ Retrieval_with_Textual_Inversion_ICCV_2023_paper.pdf. Mariam Barry, Gaetan Caillaut, Pierre Halftermeyer, Raheel Qader, Mehdi Mouayad, Fabrice Le Deit, Dimitri Cariolaro, and Joseph Gesnouin. GraphRAG: Leveraging graph-based efIn Proceedings ficiency to minimize hallucinations in LLM-driven RAG for finance data. of the Workshop on Generative AI and Knowledge Graphs (GenAIK), 2025. URL https: //aclanthology.org/2025.genaik-1.6/. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2024. URL https://huggingface.co/BAAI/bge-m3. Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang. Can pre-trained vision and language models answer visual information-seeking questions? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://aclanthology.org/2023.emnlp-main.925/. Zijian Chen, Xueguang Ma, Shengyao Zhuang, Ping Nie, Kai Zou, Andrew Liu, Joshua Green, Kshama Patel, Ruoxi Meng, Mingyi Su, Sahel Sharifymoghaddam, Yanxi Li, Haoran Hong, Xinyu Shi, Xuye Liu, Nandan Thakur, Crystina Zhang, Luyu Gao, Wenhu Chen, and Jimmy Lin. Browsecomp-plus: more fair and transparent evaluation benchmark of deep-research agent, 2025. URL https://arxiv.org/abs/2508.06600. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. URL https://github.com/mlfoundations/ open_clip. Chroma. Chromadb: An open-source vector embedding database, 2025. URL https:// github.com/chroma-core/chroma. Apache 2.0 license. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Debrup Das, Sam Nuallain, and Razieh Rahimi. Rader: Reasoning-aware dense retrieval models, 2025. URL https://arxiv.org/abs/2505.18405. Kuicai Dong, Yujing Chang, Xin Deik Goh, Dexun Li, Ruiming Tang, and Yong Liu. Mmdocir: Benchmarking multi-modal retrieval for long documents, 2025. URL https://arxiv.org/ abs/2501.08828. Anna Doris, Daniele Grandi, Ryan Tomich, Md Ferdous Alam, Mohammadmehdi Ataei, Hyunmin Cheong, and Faez Ahmed. Designqa: multimodal benchmark for evaluating large language models understanding of engineering documentation. Journal of Computing and Information Science in Engineering, 2025. Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, and Pierre Colombo. Colpali: Efficient document retrieval with vision language models. In International Conference on Learning Representations (ICLR), 2025. URL https://openreview.net/ forum?id=ogjBpZ8uSi. Yi Feng, Chuanyi Li, and Vincent Ng. Legal case retrieval: survey of the state of the art. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2024. URL https://aclanthology.org/2024.acl-long.350/. FlagEmbedding. Bge-reasoner: Towards end-to-end reasoning-intensive information retrieval. https://github.com/FlagOpen/FlagEmbedding/tree/master/research/ BGE_Reasoner, 2025. Accessed: 2025-09-12. Lixue Gong, Xiaoxia Hou, Fanshi Li, Liang Li, Xiaochen Lian, Fei Liu, Liyang Liu, Wei Liu, Wei Lu, Yichun Shi, Shiqi Sun, Yu Tian, Zhi Tian, Peng Wang, Xun Wang, Ye Wang, Guofeng Wu, Jie Wu, Xin Xia, Xuefeng Xiao, Linjie Yang, Zhonghua Zhai, Xinyu Zhang, Qi Zhang, Yuwei Zhang, Shijia Zhao, Jianchao Yang, and Weilin Huang. Seedream 2.0: native chinese-english bilingual image generation foundation model, 2025. URL https://arxiv.org/abs/2503.07703. HLT-COE@JHU. Turkle: An open-source clone of amazon mechanical turk. https://github. com/hltcoe/turkle, 2025. Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina Toutanova, and Ming-Wei Chang. Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. URL https://arxiv.org/abs/2302.11154. Wenbo Hu, Jia-Chen Gu, Zi-Yi Dou, Mohsen Fayyaz, Pan Lu, Kai-Wei Chang, and Nanyun Peng. Mrag-bench: Vision-centric evaluation for retrieval-augmented multimodal models. Proceedings of The International Conference on Learning Representations (ICLR), 2025. URL https:// openreview.net/forum?id=Usklli4gMc. Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Chaoyou Fu, Guanglu Song, Peng Gao, Yu Liu, Chunyuan Li, and Hongsheng Li. Mmsearch: In International Benchmarking the potential of large models as multi-modal search engines. Conference on Learning Representations (ICLR), 2025a. URL https://arxiv.org/abs/ 2409.12959. Ting Jiang, Shaohan Huang, Minghui Song, Zihan Zhang, Haizhen Huang, Liang Wang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, deqing wang, and Fuzhen Zhuang. E5-v: Universal embeddings with multimodal large language models, 2025b. URL https://openreview.net/ forum?id=rD6LQagatR. Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. Vlm2vec: Training vision-language models for massive multimodal embedding tasks. Proceedings of The International Conference on Learning Representations (ICLR), 2025c. URL https: //openreview.net/forum?id=TE0KOzWYAF. Andreas Koukounas, Georgios Mastrapas, Bo Wang, Mohammad Kalim Akram, Sedigheh Eslami, Michael Günther, Isabelle Mohr, Saba Sturua, Scott Martens, Nan Wang, and Han Xiao. jina-clipv2: Multilingual multimodal embeddings for text and images, 2024. URL https://arxiv. org/abs/2412.08802. Hugo Laurençon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. OBELICS: An open web-scale filtered dataset of interleaved image-text documents. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=SKN2hflBIZ. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models, 2025. URL https://arxiv.org/abs/2405.17428. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the 40th International Conference on Machine Learning, 2023. Zhang Li, Yuliang Liu, Qiang Liu, Zhiyin Ma, Ziyang Zhang, Shuo Zhang, Zidun Guo, Jiarui Zhang, Xinyu Wang, and Xiang Bai. Monkeyocr: Document parsing with structure-recognition-relation triplet paradigm, 2025. URL https://arxiv.org/abs/2506.05218. Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, and Wei Ping. Mm-embed: Universal multimodal retrieval with multimodal llms. In International Conference on Learning Representations (ICLR), 2025. URL https://openreview.net/forum? id=i45NQb2iKO. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, PiIn Eurootr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. pean conference on computer vision, 2014. URL https://cocodataset.org/images/ coco-paper.png. Siqi Liu, Weixi Feng, Tsu jui Fu, Wenhu Chen, and William Yang Wang. EDIS: Entity-driven image search over multimodal web content. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://arxiv.org/abs/2305. 13631. Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and Stephen Gould. trieval on real-life images with pre-trained vision-and-language models. ings of Image reIn Proceedthe IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 12 https://openaccess.thecvf.com/content/ICCV2021/papers/ URL Liu_Image_Retrieval_on_Real-Life_Images_With_Pre-Trained_ Vision-and-Language_Models_ICCV_2021_paper.pdf. Meixiu Long, Duolin Sun, Dan Yang, Junjie Wang, Yue Shen, Jian Wang, Peng Wei, Jinjie Gu, and Jiahai Wang. Diver: multi-stage approach for reasoning-intensive information retrieval, 2025. URL https://arxiv.org/abs/2508.07995. Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. Unifying multimodal retrieval via document screenshot embedding. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024. URL https://aclanthology.org/ 2024.emnlp-main.373/. Quentin Macé, António Loison, and Manuel Faysse. Vidore benchmark v2: Raising the bar for visual retrieval, 2025. URL https://arxiv.org/abs/2505.17166. MediaWiki. Api:search mediawiki,, 2024. URL https://www.mediawiki.org/w/ [Online; accessed 25-Septemberindex.php?title=API:Search&oldid=6905053. 2025]. Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: Massive text embedding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023. eacl-main.148/. OpenAI. chatgpt introducing-chatgpt-search/, 2024. Accessed: 2025-09-17. Introducing search. https://openai.com/index/ OpenSearch-AI. Opensearch-ai/ops-mm-embedding-v1-7b, 2025. URL https: //huggingface.co/OpenSearch-AI/Ops-MM-embedding-v1-7B. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:28252830, 2011. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, et al. Humanitys last exam, 2025. URL https://arxiv.org/abs/2501.14249. Zile Qiao, Guoxin Chen, Xuanzhong Chen, Donglei Yu, Wenbiao Yin, Xinyu Wang, Zhen Zhang, Baixuan Li, Huifeng Yin, Kuan Li, Rui Min, Minpeng Liao, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Webresearcher: Unleashing unbounded reasoning capability in long-horizon agents, 2025. URL https://arxiv.org/abs/2509.13309. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. URL https://d4mucfpksywv. cloudfront.net/better-language-models/language-models.pdf. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamila Mishkin, Jack Clark, Gretchen Krueger, and Ilya In ProSutskever. Learning transferable visual models from natural language supervision. ceedings of the 38th International Conference on Machine Learning (ICML), 2021. URL https://proceedings.mlr.press/v139/radford21a/radford21a.pdf. Gerard Salton and Christopher Buckley. Term-weighting approaches in automatic text retrieval. Information Processing & Management, 1988. URL https://www.sciencedirect.com/ science/article/pii/0306457388900210. Rulin Shao, Rui Qiao, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Wen tau Yih, Pang Wei Koh, and Luke Zettlemoyer. Reasonir: Training retrievers for reasoning tasks. Proceedings of Conference on Language Modeling, 2025. URL https://arxiv.org/abs/2504.20595. Singapore Police Force. Basic theory of driving, 2017. URL https://www.police.gov.sg/ /media/spf/files/tp/online%20learning%20portal/bt%20eng%209th% 20edition%20130717.pdf. Accessed: 2025-09-21. Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary S. Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan O. Arik, Danqi Chen, and Tao Yu. Bright: realistic and challenging benchmark for reasoning-intensive retrieval. In International Conference on Learning Representations (ICLR), 2025. URL https: //openreview.net/forum?id=ykuc5q381b. Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale, 2023. URL https://arxiv.org/abs/2303.15389. Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark Gerstein. MedAgents: Large language models as collaborators for zero-shot medical reasoning. In Findings of the Association for Computational Linguistics: ACL 2024, 2024. URL https://aclanthology.org/2024.findings-acl.33/. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Junjie Wang, Yuxiang Zhang, Minghao Liu, Yin Zhang, Yatai Ji, Weihao Xuan, Nie Lin, Kang Zhu, Zhiqiang Lin, Yiming Ren, Chunyang Jiang, Yiyao Yu, Zekun Wang, Tiezhen Wang, Wenhao Huang, Jie Fu, Qunshu Lin, Yujiu Yang, Ge Zhang, Ruibin Yuan, Bei Chen, and Wenhu Chen. PIN: knowledge-intensive dataset for paired and interleaved multimodal documents. 2024a. URL https://huggingface.co/datasets/m-a-p/PIN-14M. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024b. URL https://arxiv.org/abs/2409. 12191. Navve Wasserman, Roi Pony, Oshri Naparstek, Adi Raz Goldfarb, Eli Schwartz, Udi Barzelay, and Leonid Karlinsky. Real-mm-rag: real-world multi-modal retrieval benchmark, 2025. URL https://arxiv.org/abs/2502.12342. Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. Uniir: Training and benchmarking universal multimodal information retrievers. In The European Conference on Computer Vision (ECCV), 2024. URL https://www.ecva.net/papers/ eccv_2024/papers_ECCV/papers/11927.pdf. Orion Weller, Kathryn Ricci, Eugene Yang, Andrew Yates, Dawn Lawrie, and Benjamin Van Durme. Rank1: Test-time compute for reranking in information retrieval, 2025. URL https: //arxiv.org/abs/2502.18418. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025a. URL https://arxiv.org/abs/ 2508.02324. Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, and Ziwei Liu. Mmsearchr1: Incentivizing lmms to search, 2025b. URL https://arxiv.org/abs/2506.20670. Siwei Wu, Yizhi Li, Kang Zhu, Ge Zhang, Yiming Liang, Kaijing Ma, Chenghao Xiao, Haoran Zhang, Bohao Yang, Wenhu Chen, Wenhao Huang, Noura Al Moubayed, Jie Fu, and Chenghua Lin. SciMMIR: Benchmarking scientific multi-modal information retrieval. In Findings of the Association for Computational Linguistics: ACL 2024, 2024. URL https://aclanthology. org/2024.findings-acl.746/. 14 Chenghao Xiao, Isaac Chung, Imene Kerboua, Jamie Stirling, Xin Zhang, Márton Kardos, Roman Solomatin, Noura Al Moubayed, Kenneth Enevoldsen, and Niklas Muennighoff. Mieb: Massive image embedding benchmark, 2025. URL https://arxiv.org/abs/2504.10471. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. MMMU-pro: more robust multi-discipline multimodal understanding benchmark. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2025. URL https://aclanthology.org/2025.acl-long.736/. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, 2023. URL https://arxiv.org/pdf/2303.15343. Zaifu Zhan, Jun Wang, Shuang Zhou, Jiawen Deng, and Rui Zhang. Mmrag: Multi-mode retrievalaugmented generation with large language models for biomedical in-context learning, 2025. URL https://arxiv.org/abs/2502.15954. Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, and Ming-Wei Chang. Magiclens: Self-supervised image retrieval with open-ended instructions. In The Fortyfirst International Conference on Machine Learning (ICML), 2024. URL https://arxiv. org/abs/2403.19651. Siyue Zhang, Yilun Zhao, Liyuan Geng, Arman Cohan, Anh Tuan Luu, and Chen Zhao. Diffusion vs. autoregressive language models: text embedding perspective, 2025a. URL https:// arxiv.org/abs/2505.15045. Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. Fashion iq: new dataset towards retrieving images by In The IEEE / CVF Computer Vision and Pattern Recognition natural language feedback. URL https://openaccess.thecvf.com/content/ Conference (CVPR), 2021. CVPR2021/papers/Wu_Fashion_IQ_A_New_Dataset_Towards_Retrieving_ Images_by_Natural_CVPR_2021_paper.pdf. Xin Zhang, Ziqi Dai, Yongqi Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Jun Yu, Wenjie Li, and Min Zhang. Towards text-image interleaved retrieval. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2025b. URL https://aclanthology.org/2025.acl-long.214/. Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. Gme: Improving universal multimodal retrieval by multimodal llms. In The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR), 2025c. URL https://openaccess.thecvf.com/content/CVPR2025/ papers/Zhang_Bridging_Modalities_Improving_Universal_Multimodal_ Retrieval_by_Multimodal_Large_Language_CVPR_2025_paper.pdf. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025d. Xuejiao Zhao, Siyan Liu, Su-Yin Yang, and Chunyan Miao. Medrag: Enhancing retrievalaugmented generation with knowledge graph-elicited reasoning for healthcare copilot, 2025. URL https://arxiv.org/abs/2502.04413. 15 Junjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, and Yongping Xiong. VISTA: Visualized text embedding for universal multi-modal retrieval. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2024. URL https: //aclanthology.org/2024.acl-long.175/. 18 18 18 18 18 19 19 19 19 19 20 20 20 20 22 22 22 23 24"
        },
        {
            "title": "Appendix Contents",
            "content": "A The Use of Large Language Models (LLMs) Dataset Construction: Knowledge B.1 Annotator Biography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Annotation Guideline and Interface . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Data Annotation Payment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Dataset Construction: Theorem C.1 Theorem Database Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Wikipedia Content Processing Pipeline . . . . . . . . . . . . . . . . . . . . . . . . C.3 Deduplication Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Quality Control and Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . Dataset Construction: Contradiction . . . D.1 Negation . D.2 Vehicle Design . . D.3 Traffic Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Experiment Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.1 Models and Instructions . E.2 Implementations and Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Detailed Results . . . . . Analysis Details F.1 Qualitative Analysis . F.2 Test-Time Scaling in Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Data Examples 17 THE USE OF LARGE LANGUAGE MODELS (LLMS) In this work, large language models (LLMs) are employed solely as tools for data generation, as described in the main paper. Importantly, no parts of the manuscript are generated by LLMs. Hence, there are no concerns of plagiarism or scientific misconduct related to text generation. DATASET CONSTRUCTION: KNOWLEDGE B.1 ANNOTATOR BIOGRAPHY The detailed biographies of the annotators involved in MRMR construction are presented in Table 4. All annotators are from universities ranked in the Top 500 of the 2025 QS Global Rankings3 and are fluent in English. Annotators assess documentquery relevance by judging whether document facilitates answering the query. To ensure quality, independent validators conduct an additional round of verification. Table 4: Biographies of 24 annotators involved in MRMR construction (Author biographies are hidden to protect identity confidentiality). ID Year Major Assigned Subject(s) Author? Validator? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 3rd year Undergraduate Biological Engineering Biological Engineering 1st year Master Biomedical Engineering 1st year Master Biomedical Engineering 2nd year Master Biomedical Engineering 1st year Master Chemistry 1st year PhD Chemistry 2nd year Master 3rd year PhD Medicine 3rd year Undergraduate Clinical Medicine 3rd year Undergraduate Medicine 2nd year Master 2nd year Master 3rd year Undergraduate 4th year Undergraduate 1st year Master 1st year Master 1st year PhD 1st year Master 2nd year PhD Clinical Medicine Clinical Medicine Pharmacology Pharmacology Music Clinical Medicine Sociology Bioinformatics Agricultural and Biosystems Engineering Literature 4th year Undergraduate 3rd year Undergraduate Geography and Biology Biology Biology, Pharmacy Biology, Pharmacy Biology, Pharmacy Chemistry Chemistry Basic Medicine Clinical Medicine, Diagnostics Basic Medicine Clinical Medicine, Diagnostics Clinical Medicine, Diagnostics Pharmacy Pharmacy Music Clinical Medicine Sociology, Psychology Biology Agriculture History, Literature Geography Environmental Studies 4th year PhD Computer Science 4th year Undergraduate Computer Science 3rd year Undergraduate Electronic Engineering - - - B.2 ANNOTATION GUIDELINE AND INTERFACE To facilitate data annotation, we develop the following interface based on Turkle (HLT-COE@JHU, 2025), an open-source clone of Amazons Mechanical Turk. The annotation guideline and interface is detailed in Figure 3, Figure 4, and Figure 5. B.3 DATA ANNOTATION PAYMENT The annotation and validation process for MRMR spanned three months. Each annotator was assigned approximately 50 questions aligned with their academic major. After annotation, validators independently assessed the quality of the labels. We provided base rate of 7 USD per hour, with quality adjustment of about 10%. On average, annotating single question required 10 minutes, while validation took 4 minutes. This compensation scheme ensured that annotators received wages competitive with the average teaching assistant salary at their universities. To maintain manageable workload and reduce pressure, we recommended maximum of 10 questions per day. Back to Appendix Table of Contents Figure 3: Annotation Interface - Step 1: Question Understanding. Annotators are first shown the question, associated images, candidate options, the correct answer, and an AI-generated explanation. The explanation is provided to aid understanding, though annotators are informed it may be incorrect. In this step, they judge whether the given answer is correct based on their own knowledge. DATASET CONSTRUCTION: THEOREM C.1 THEOREM DATABASE CONSTRUCTION The BRIGHT theorem corpus was embedded using Qwen3-Embedding (Zhang et al., 2025d) and indexed in ChromaDB, which supports efficient semantic search via HNSW (Chroma, 2025). Each entry retains unique theorem_id and the original text, enabling fast, semantics-aware retrieval with full traceability to the source. C.2 WIKIPEDIA CONTENT PROCESSING PIPELINE We retrieved Wikipedia content by querying the MediaWiki Search API (MediaWiki, 2024) using theorem names as search keys. For supplementary sources in PDF format, we employed MonkeyOCR (Li et al., 2025) to convert scanned documents into Markdown. The resulting text was then processed through structured extraction prompt (Figure 6) using GPT-5 to perform final cleaning, normalization, and precise theorem statement extraction. C.3 DEDUPLICATION METHODOLOGY All theorems extracted from Wikipedia were deduplicated prior to inclusion in the corpus. Deduplication was performed in two stages: first by theorem name, and then by semantic content using TF-IDFbased cosine similarity (Salton & Buckley, 1988). Specifically, we employed TfidfVectorizer to compute TF-IDF vectors for all theorem statements (Pedregosa et al., 2011), followed by pairwise cosine similarity. Entries with near-identical content (cosine similarity 0.85) were collapsed into single representative instance. C.4 QUALITY CONTROL AND VALIDATION We ensured corpus quality through automated deduplication as mentioned in Section C.3, manual spot-checking of 20% of newly added Wikipedia content by domain experts, and robust error handling for failed downloads or OCR issues. Back to Appendix Table of Contents Figure 4: Annotation Interface Step 2: Candidate Document Evaluation. After understanding the question, annotators are instructed to review candidate documents individually and judge whether each can facilitate correctly answering the question. Documents are shown in image format, with up to eight candidates presented. Document relevance definition has been explained to annotators before the annotation process. DATASET CONSTRUCTION: CONTRADICTION D.1 NEGATION First, we randomly select 200 samples from the COCO (Lin et al., 2014) dataset, each containing at least three positive objectives. For each entry, we construct description using the template, The image includes a, b, c, but no d. In the positive description, we randomly select three positive objectives to replace a, b, and c, and select one negative objective to replace d. For the negative description, we generate two variations: one where all four objectives (a, b, c, d) are selected from the positive objectives, and another where one of a, b, or is replaced by randomly selected negative objective. The image from each sample is used as the query, and the three positive descriptions and one negative description are used as the corpus. Finally, we manually review the 200 queries and corresponding gold documents to ensure that the contradictory descriptions are identifiable by humans, and revise any ambiguous queries for clarity. D.2 VEHICLE DESIGN On one hand, to construct the queries, we use design cases from the DesignQA dataset (Doris et al., 2025) and augment them through appropriate modifications, such as altering numerical values and introducing variations in image elements. On the other hand, to construct the corpus, we apply MonkeyOCR (Li et al., 2025) to extract and segment the Formula SAE Rulebook into 700 files, organized by rule ID. Finally, we review all the queries to ensure they represent incorrect designs. D.3 TRAFFIC CASE First, we select set of traffic rules and, based on these rules, create traffic violation cases by crafting relevant stories. These stories are then used as prompts to generate 12 images per story using GPT5. Afterwards, we manually review all the generated images and use Doubao (Gong et al., 2025) 20 Back to Appendix Table of Contents Figure 5: Annotation Interface Step 3: Create Relevant Document. If none of the candidate documents are deemed relevant, annotators are required to search for suitable web page and provide the gold evidence content. They are encouraged to include images from the source, and the final document is written in an interleaved imagetext format. You are given markdown document. Your task is to extract the specific theorem, formula, equation, algorithm, or concept named {theorem_name} from this document. Instructions: 1. Carefully locate the section that describes the theorem {theorem_name}. 2. Extract the complete definition, explanation, and any associated formulas or equations. 3. Remove all reference citations. 4. If there are referenced images in the content, preserve the image references exactly as they appear. 5. Your response MUST follow the following LaTeX-style format: begin{definition}[{theorem_name}] Complete definition and explanation, preserving mathematical notation. Include examples if present. end{definition} Here is the document content: {markdown_content} Figure 6: GPT-5 prompt for cleaning the theorem content. to refine and enhance them for better clarity and relevance. Additionally, we leverage Doubao to generate specific objectives from the queries in order to construct imagetext interleaved queries. For the corpus, we use MonkeyOCR to split Basic Theory of Driving and Final Theory of Driving Back to Appendix Table of Contents (Singapore Police Force, 2017), two official driving handbooks in Singapore, into separate files, which are then organized and used as the corpus. Finally, we conduct manual review of all the queries, ensuring that any additional corpus IDs caused by excessive image details are properly incorporated into the queries."
        },
        {
            "title": "E EXPERIMENT DETAILS",
            "content": "E.1 MODELS AND INSTRUCTIONS Table 5: Details of the multimodal retriever models evaluated in MRMR. Model Size Version BGE-M3 (Chen et al., 2024) NE-Embed-V2 (Lee et al., 2025) Qwen3-Embedding (Zhang et al., 2025d) 600M BAAI/bge-m3 8B 8B nvidia/NV-Embed-v2 Qwen/Qwen3-Embedding-8B EVA-CLIP (Sun et al., 2023) SigLIP (Zhai et al., 2023) JinaCLIP (Koukounas et al., 2024) OpenCLIP (Cherti et al., 2023) 400M QuanSun/EVA02-CLIP-L-14 650M google/siglip-large-patch16-256 860M jinaai/jina-clip-v2 1.4B laion/CLIP-ViT-g-14-laion2B-s34B-b88K VISTA (Zhou et al., 2024) VLM2Vec (Jiang et al., 2025c) GME-Qwen2-VL (Zhang et al., 2025c) Ops-MM-Embedding (OpenSearch-AI, 2025) E5-V (Jiang et al., 2025b) MM-Embed (Lin et al., 2025) 200M BAAI/bge-visualized-m3 4B 7B 7B 8B 8B TIGER-Lab/VLM2Vec-Full Alibaba-NLP/gme-Qwen2-VL-7B-Instruct OpenSearch-AI/Ops-MM-embedding-v1-7B royokong/e5-v nvidia/MM-Embed ColPali (Faysse et al., 2025) 3B vidore/colpali-v1.3 Following TIIR, we evaluate text retrievers on multimodal retrieval tasks by replacing images with captions generated by an LLM. To simulate real-time inference, we apply the standardized prompt Describe the image\" and use Qwen2-VL-2B-Instruct to produce the captions. Table 6: Instruction prompts used during model evaluation in MRMR. Task Modality Prompt Knowledge Theorem Negation Multimodal Text Multimodal Text Retrieve relevant documents that help answer the question. Retrieve relevant theorems that are involved in solving the problem. Multimodal Given an image, retrieve descriptions that have contradictory information with the image. Text Given an image caption, retrieve descriptions that have contradictory information with the image caption. Vehicle Design Multimodal Given vehicle design, retrieve the design requirements that it violates. Text Given vehicle design description, retrieve the design requirements that it violates. Traffic Case Multimodal Given traffic case, retrieve the driving rule documents that it violates. Text Given traffic case description, retrieve the driving rule documents that it violates. E.2 IMPLEMENTATIONS AND MACHINES The MRMR dataset is constructed following the conventions of MTEB (Muennighoff et al., 2023), including data format and evaluation pipeline, with modifications to support mixed-modality inputs during evaluation. All experiments are conducted on NVIDIA A100, A6000, or H100 GPUs. The runtime of full evaluation depends on the model, but with the limited corpus size for efficiency, one complete run can be completed within 4 hours on single A100 GPU for open-source dense models. To further accelerate dense model evaluation, we employ FlashAttention (Dao et al., 2022). 22 Back to Appendix Table of Contents E.3 DETAILED RESULTS Table 7: Detailed performance of retrieval models on MRMR (Knowledge). Knowledge Model Music Design Theo. Art Hist. Soci. Psy. Lit. Pharm. Diag. Clinic. Basic. Agri. Geo. Chem. Bio. BGE-M3 NV-Embed-v2 Qwen3-Embedding EVA-CLIP SigLIP OpenCLIP JinaCLIP VISTA E5-V MM-Embed VLM2Vec GME-Qwen2-VL Ops-MM-Embedding GME-Qwen2-VL Ops-MM-Embedding ColPali 43.4 63.8 62.8 30.5 25.0 20.9 18.5 39.3 13.0 51.6 34.4 55.1 58.5 58.2 60.6 25.1 44.0 61.8 62. 1.5 25.6 50.7 11.0 3.5 23.4 60.8 44.0 40.4 75.6 46.5 59.0 27.7 Text Models with Image Caption 49.4 70.1 74.8 57.2 86.8 87. 47.7 70.6 76.1 39.5 64.3 74.0 52.2 59.7 69.3 15.8 95.8 97.8 58.5 78.0 83.1 11.2 19.8 34. Text and Image Two-Stream Models with Vector Fusion 3.5 26.2 62.9 23.0 17.2 17.6 68.3 49.6 57.1 84.2 53.6 68.4 46.4 7.5 30.0 86.4 33.1 16.7 16.7 35.8 14. 5.5 1.4 10.2 0.0 16.3 14.7 15.1 17.1 0.0 22.7 22.7 0.0 22.7 13.8 11.1 17.8 Multimodal Models with Merged Image 27.5 46.1 80.5 84.8 64.8 96. 12.3 15.6 57.5 36.4 39.2 71.4 13.9 4.3 69.4 12.3 50.6 71.1 28.0 10.8 59.5 19.3 51.1 59.7 0.0 7.7 94.1 19.2 32.9 73.7 48.9 12.5 63.8 17.4 57.2 76.1 Multimodal Models with Document as Image 58.4 82.4 43.7 52.5 68.3 31.7 48.5 63.0 19.4 48.2 58.6 38.5 52.1 68.3 0.0 72.9 74.3 64. 10.3 9.7 10.6 6.1 18.2 7.1 35.1 13.6 20.6 30.9 16.8 31.2 10.6 28.2 46.0 47.0 10.0 15.6 20.8 21.7 23.9 13.5 50.9 23.7 32.1 50. 31.7 39.3 23.0 36.2 59.0 64.0 16.4 19.6 25.8 21.1 31.2 13.7 68.9 33.1 62.2 64.5 40.2 65.9 60.1 38.7 65.3 69. 41.6 30.2 34.1 35.1 33.6 18.3 60.9 39.0 38.9 58.7 49.7 57.2 36.7 48.6 63.3 76.5 15.4 18.3 45.8 24.7 22.0 13.1 76.0 40.7 48.4 78. 69.0 69.3 32.6 37.6 70.0 74.0 20.4 26.7 23.9 30.4 36.9 23.3 62.1 37.5 63.6 80.4 53.8 76.1 67.6 48.3 63.6 72. 18.5 27.3 34.3 15.4 33.1 10.0 60.7 30.8 39.6 69.0 45.4 71.9 56.3 Avg. 41.0 64.9 70.4 14.8 20.2 31.9 18. 24.3 15.6 63.8 33.5 47.1 68.7 49.8 63.4 36.5 23 Back to Appendix Table of Contents"
        },
        {
            "title": "F ANALYSIS DETAILS",
            "content": "F.1 QUALITATIVE ANALYSIS Figure 7: Error case example in Agriculture where the multimodal embedding model Ops-MMEmbedding prioritizes the negative document in the left over the positive document in the right. 24 Back to Appendix Table of Contents Figure 8: Error case example in Traffic where the multimodal embedding model Ops-MMEmbedding prioritizes the negative document in the left over the positive document in the right. Back to Appendix Table of Contents F.2 TEST-TIME SCALING IN RETRIEVAL Table 8: nDCG@10 scores of the multimodal retriever GME-Qwen2-VL on MRMR Knowledge and Theorem tasks, comparing the original queries with query expansions generated by Qwen2-VL2B-Instruct and Qwen2.5-VL-72B-Instruct. The average query length (Q #Text) before and after expansion is reported as the number of tokens measured by the GPT-2 tokenizer. Knowledge Theorem Avg. Model #Text Art Med. Sci. Hum. #Text Math Phy. Eng. Bus. Original Qwen2-VL-2B Qwen2.5-VL-72B 31.4 699.6 843. 54.3 64.9 76.9 40.1 49.6 61.8 46.8 64.6 77.0 45.6 48.9 72.2 56.6 809.9 1302.7 3.0 24.0 29. 3.6 30.9 34.7 9.3 25.0 29.7 4.6 31.3 37.1 25.9 42.4 52."
        },
        {
            "title": "G DATA EXAMPLES",
            "content": "Figure 9: Music example. 26 Back to Appendix Table of Contents Figure 10: Clinic Medicine example. 27 Back to Appendix Table of Contents Figure 11: Biology example. 28 Back to Appendix Table of Contents Figure 12: Psychology example. 29 Back to Appendix Table of Contents Figure 13: Math example. 30 Back to Appendix Table of Contents Figure 14: Physics example. 31 Back to Appendix Table of Contents Figure 15: Engineering example. 32 Back to Appendix Table of Contents Figure 16: Business example. 33 Back to Appendix Table of Contents Figure 17: Negation example. Figure 18: Vehicle Design example. 34 Back to Appendix Table of Contents Figure 19: Traffic Case example. Back to Appendix Table of Contents"
        }
    ],
    "affiliations": [
        "Center for Data Science, New York University",
        "NYU Shanghai",
        "Nanyang Technological University",
        "Shanghai Jiao Tong University",
        "University of the Chinese Academy of Sciences",
        "Yale University"
    ]
}