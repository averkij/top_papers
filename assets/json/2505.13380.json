{
    "paper_title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition",
    "authors": [
        "Nam V. Nguyen",
        "Huy Nguyen",
        "Quang Pham",
        "Van Nguyen",
        "Savitha Ramasamy",
        "Nhat Ho"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the network's depth or width. However, we argue that effective SMoE training remains challenging because of the suboptimal routing process where experts that perform computation do not directly contribute to the routing process. In this work, we propose competition, a novel mechanism to route tokens to experts with the highest neural response. Theoretically, we show that the competition mechanism enjoys a better sample efficiency than the traditional softmax routing. Furthermore, we develop CompeteSMoE, a simple yet effective algorithm to train large language models by deploying a router to learn the competition policy, thus enjoying strong performances at a low training overhead. Our extensive empirical evaluations on both the visual instruction tuning and language pre-training tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE compared to state-of-the-art SMoE strategies. We have made the implementation available at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an improved version of the previous study at arXiv:2402.02526"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 0 8 3 3 1 . 5 0 5 2 : r CompeteSMoE Statistically Guaranteed Mixture of Experts Training via Competition Nam V. Nguyen Van Nguyen Huy Nguyen Savitha Ramasamy Quang Pham Nhat Ho FPT Software AI Center The University of Texas at Austin Independent Researcher Institute for Infocomm Research, ASTAR Correspondence to: quangg2012@gmail.com May 20, 2025 Figure 1: The evolution of zero-shot performance averaged over nine visual instruction tuning tasks throughout training of various SMoE algorithms using 5.1B parameters backbone."
        },
        {
            "title": "Abstract",
            "content": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the networks depth or width. However, we argue that effective SMoE training remains challenging because of the suboptimal routing process where experts that perform computation do not directly contribute to the routing process. In this work, we propose competition, novel mechanism to route tokens to experts with the highest neural response. Theoretically, we show that the competition mechanism enjoys better sample efficiency than the traditional softmax routing. Furthermore, we develop CompeteSMoE, simple yet effective algorithm to train large language models by deploying router to learn the competition policy, thus enjoying strong performances at low training overhead. Our extensive empirical evaluations on both the visual instruction tuning and language pre-training tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE compared to state-of-the-art SMoE strategies. We have made the implementation available at: (cid:135) CompeteSMoE. This work is an improved version of the previous study at https://arxiv.org/abs/2402.02526."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have emerged as promising architecture for artificial general intelligence. In recent years, LLMs have shown remarkable success in solving many cognitive tasks, ranging 1 from language, vision understanding [5, 32, 23, 83, 4, 47, 46], to code generation [93], reinforcement learning [13] and life sciences [82]. Since the release of the original Transformer model [91], extensive efforts have been devoted to scaling the model complexity to take advantage of massive datasets and advanced computing hardware [79, 8, 24]. To go beyond simply increasing the depth and width of the network, Sparse Mixture-of-experts (SMoE) [26] has emerged as an appealing solution for scaling LLMs. By modularizing the network and activating only subsets of experts per input, SMoE offers constant computational costs when increasing the model complexity, often resulting in improved performance. Despite the initial success, practical SMoE training has been known to be notoriously challenging in both engineering and algorithmic aspects. Thus, despite the rapid development of advanced SMoE research in theory and algorithm [43, 81, 11], limited progress has been made in leading industrial models such as DeepSeek [20, 21] or Phi-MoE [1] as they still implement variants of the vanilla routing mechanism since the original Switch Transformer [26]. We argue that this discrepancy exists because many state-of-the-art strategies often rely on intuitive conceptualizations, which can only offer greedy solutions that work training in the limited training data and small model regimes. Furthermore, many of existing works [42, 22, 60, 18] still follow the in-domain evaluation and ignores the zero-shot generalization capabilities of pre-train language models, which are their main use cases. This work makes step towards statistically guaranteed SMoE training strategy that can yield improvements on wide range of training settings in large-scale models. To this end, we investigate the core mechanism of routing tokens to experts in SMoE, arguing that it could be suboptimal because the experts performing the calculation do not directly contribute to the routing process. This limitation has motivated us to develop radical routing strategy to distribute tokens to experts more effectively than using the traditional router. To this end, motivated by the Winner-take-all (WTA) principle [30] originated in biology [80, 3, 25], we propose the competition mechanism for SMoE training. The core mechanism of competition is activating all experts and defining winning criterion so that tokens are only sent to the winning experts. Thus, competition addresses the fundamental limitation of traditional routing schemes by involving experts into the routing process, which we rigorously show to achieve better sample efficiency or convergence rate than the traditional softmax routing. Furthermore, we go beyond statistical analysis by developing the CompeteSMoE algorithm that implements the competition mechanism into large-scale models at modest overhead. Specifically, CompeteSMoE improved the zero-shot performance across 15 common benchmarks in both vision-language finetuning (Figure 1) and language pre-training settings. In summary, our work makes the following contributions. First, we propose novel competition mechanism for training SMoE, which enjoys better convergence rate than softmax routing. Second, we develop CompeteSMoE, scalable and effective training strategy for SMoE training via competition. Lastly, we conduct extensive experiments to explore the behaviours of CompeteSMoE, including its performance, scalability, convergence property, and routing efficacy."
        },
        {
            "title": "2 CompeteSMoE",
            "content": "We first recap the foundation of MoE in transformers in Section 2.1. Then, we introduce the competition mechanism in Section 2.2, discuss the scheduled router training in Section 2.3, and 2 (a) The router learns the competition policy. (b) Normal routing using the router. Figure 2: An illustrative of the interleaved learning phases in CompeteSMoE: (a) activating all experts for the router to learn the competition policy; and (b) normal routing using the router. detail the CompeteSMoE algorithm in Section 2.4."
        },
        {
            "title": "2.1 Background",
            "content": "The traditional SMoE layer [85] consists of router R(, Wr) parameterized by Wr and experts parameterized by Wei, [N ], respectively. The router takes the input token as {g(, Wei)}N i=1 input and produces an affinity score vector on experts as sR = σ(TopK(xWr)), where σ is scoring function, often implemented as softmax or sigmoid function. The TopK function keeps the largest elements in vector and sets the other elements to negative infinity (). With this notation, the SMoE layer takes an input token and calculate the final output by aggregating the outputs of each expert weighted by their affinity scores as: ˆy = (cid:88) i=1 si g(x; Wei) (1) In practice, it is common for to be smaller than , i.e. < , to improve the model efficiency. For completeness, we provide list of all notations and their meanings in Table 5, Appendix A."
        },
        {
            "title": "2.2 Routing via Competition",
            "content": "We now introduce the competition mechanism as an effective routing strategy to facilitate SMoE training. The key idea of competition is allowing all experts to calculate their outputs, and selection is performed via the winner-take-all mechanism. Thus, experts will compete with one another and the best ones are selected to calculate the final output. To implement the competition, we propose to use the experts neural response as its affinity score, i.e. si = E[κ(g(x, Wei))], where κ() is an activation function over the experts neural responses. In the experiments, we will implement κ as the softplus function, unless otherwise stated. However, our competition mechanism and the theoretical analysis thereafter are general and do not make strong assumptions about κ. We will provide the results of other choices of κ in Appendix D. With this notation, training of SMoE with competition is formulated via the following steps: 3 1. Compute the output of all experts for given input as g(x, Wei), 2. Compute the affinity score of each expert: si = E[log(1 + eg(x,Wei ))], [N ]. [N ]. 3. Select the Top-K experts based on the highest neural response and compute the normalized is similar to the traditional affinity scores: ˆsi = TopK0(si, K), si . Here, TopK0 j=1 ˆsj but sets the values outside the highest values to be 0 instead of . = ˆsi (cid:80)N TopK 4. Compute the final output as weighted sum of the selected experts: ˆy = (cid:80)N i=1 si g(z, Wei). Competition starkly contrasts with the standard SMoE implementation discussed in Section 2.1 where the affinity score is calculated as the dot product between the input and the experts embeddings, i.e., columns of Wr, and only the few selected experts actually perform their calculation. Although using expert embedding is more efficient, it results in suboptimal routing policies because the embedding is detached from the experts forward calculation. In contrast, competition proposes that experts who respond the strongest to an input are selected to process that input, while suppressing the other experts. We will rigorously show the theoretical guarantees of routing via competition in Section 3."
        },
        {
            "title": "2.3 Scheduled Training of the Router",
            "content": "One major drawback of competition-based expert selection is the high computational overhead of activating all experts, which limits its viability to large-scale models with billions of parameters. To make competition applicable to LLM training, we propose scheduled training mechanism that trains router to learn the competition policy. Thus, well-trained router is expected to pick experts that would win competition without performing the full competition procedure. Furthermore, using routers is also efficient during inference since it enjoys the same complexity as the original SMoE. To this end, we employ learnable router R(; Wr) trained to jointly minimize the task loss and approximate the competition policy. Although distilling the competition policy to router network presents promising solution for large models, this router should learn the competition policy at minimal computational overhead. Thus, in the following, we present the router loss for effective training and discuss the router schedulers to ensure that training remains efficient."
        },
        {
            "title": "2.3.1 Router Loss",
            "content": "The router is trained to learn the competition policy and use it to minimize the task loss. We propose to learn the competition policy by minimizing distillation loss, LD, which characterizes the discrepancy between the competition and router policies. For ease of notation, we use IC [N ] to denote the indices of the experts who won the competition. Then, the distillation loss LD can be computed by minimizing the mean squared errors (MSE) between the competition and router policies, via their affinity scores as: LD(sR, sC) = MSE(sR, sC) + α (cid:88) jIC (sj sj R)2, (2) where α R+ is hyperparameter to encourage the router to pay more attention to winning experts from competition. Diversity Loss One of our main experimental settings is using sparse upcycling [41] to bypass the expensive pre-training cost, which allows us to test SMoE algorithms on larger models with low budget. However, sparse upcycling duplicates the experts and make them have similar outputs, which results in no competition in the early stages of training and limited training efficacy. To mitigate this issue, we introduce the Diversity Loss, Ldiv, to promote diverse representations of the winning experts. Formally, given the output matrix RKD representing the outputs of winning experts for an input x, the diversity loss is computed as the mean of the off-diagonal elements in the correlation matrix constructed from O: Ldiv(O) ="
        },
        {
            "title": "1\nK(K − 1)",
            "content": "K (cid:88) (cid:88) i=1 j=1 j=i Ci,j, where = O2 . (3) We apply the Diversity Loss only within the competition mechanism and emphasize the winning experts as defined in Eq. 2.2, rather than those selected by the router R(; Wr). By penalizing winning experts when they produce similar outputs, Ldiv promotes more effective competition outcome when using the sparse upcycling strategy."
        },
        {
            "title": "2.3.2 Router Training Schedule",
            "content": "Schedulers are essential to ensure that the routers can effectively learn good routing policy while maintaining limited computational overhead. In the worst case, when all layers of deep network perform competition simultaneously, this SMoE becomes dense and could crash the training process. Thus, we need to carefully design schedule to manage the competition frequency across layers. To this end, we employ two schedulers; one is applied per layer independently, while the other monitors the total competition frequency of all layers. For layer in deep network, we first employ scheduler λl(t) to determine whether competition should be activated at time step for this layer. We simply implement λl(t) by sampling from Bernoulli distribution with probability ω, which is fixed for all layers. Furthermore, we also employ global scheduler to monitor the competition frequency across layers. Specifically, we only allow the total number of layers performing competition at any time step to be Amax. Any layers exceeding this threshold are deferred to perform competition in the next step. Appendix will provide detailed formulation of the global scheduler."
        },
        {
            "title": "2.4 The CompeteSMoE Algorithm",
            "content": "We are now ready to describe the CompeteSMoE algorithm to enhance SMoE training of large-scale models. Before training, we use the schedulers to generate all time steps for which the competition , where Λ(l, t) = 1 indicating that mechanism is activated at each layer and store them in {Λ(l)}L the llayer will perform competition at time t. Note that this step is performed offline, only one time before training starts. Then, according to the schedule Λ(l, t), the training dynamic involves: (i) training the activated experts to minimize the task loss, LNLL, and (ii) training the activated router to minimize the task and router losses. We provide an illustration of CompeteSMoE training l=1 5 in Figure 2. Formally, the training step at time is computed as:"
        },
        {
            "title": "W l",
            "content": "e e ξt"
        },
        {
            "title": "W l",
            "content": "r r ξt e r LNLL(ˆy, y), [L] (cid:2)γ LD(sR, sC) + β Ldiv(C) + LNLL(ˆy, y)(cid:3), if Λ(l, t) = 1 (4) (5) where LNLL is the negative log-likelihood (task loss) between the predicted output ˆy and the groundtruth y, LD is the distillation loss defined in equation (2), Ldiv(C) is the diversity loss defined in equation (3) , ξt is the step size. We also wish to emphasize that CompeteSMoE only uses the routers during inference, thus enjoying the same serving cost as the traditional SMoE. We now discuss general guideline to set the hyper-parameters introduced by CompeteSMoE. We recommend the balancing hyper-parameters α, β, γ to be small values such as 0.01 or 0.005. The Bernoulli parameter ω should also be small (e.g. 0.07) so that competition is not activated too often. The global scheduler thresholds should be set based on the specific backbone architecture and training infrastructure to ensure stability. We found Amax = 9 for vision-language models and Amax = 3 for language model pre-training to maximize the memory usage of our hardware. Lastly, we emphasize that the value ranges of these hyper-parameters can be derived by their definition, which greatly reduces the effort for hyper-parameter searching. As long as they follow this guideline, the final performance should be robust to the exact configuration as we will illustrate in Appendix H."
        },
        {
            "title": "3 Statistical Guarantee of the Competition Mechanism",
            "content": "In this section, we perform convergence analysis of Gaussian MoE models equipped with the competition mechanism. Our primary objective is to theoretically justify the effectiveness of the competition mechanism by investigating its sample efficiency in terms of expert estimation. Problem setting. Let (X1, Y1), (X2, Y2), . . . , (Xn, Yn) be i.i.d samples drawn from bounded subsets Rd1 and according to the following conditional density function: pG(Y X) := (cid:88) i=1 exp(log(1 + exp(g(X, ei)))) j=1 exp(log(1 + exp(g(X, (cid:80)N ej )))) (Y g(X, ei), ν ). (6) e1, ν 1 ), (W Here, is the number of ground-truth experts denoted by g(X, ei), while (µ, ν) stands for the Gaussian density with mean µ and variance ν. In addition, we also define := (cid:80)N ) as ,ν mixing measure with ground-truth parameters (W ), where δ denotes the Dirac measure. For the ) are distinct parameters belongsake of theory, we assume that (W e2, ν ing to compact space Θ Rd2 R+ for some d2 N. Next, we assume that the expert function g(X, We) is non-zero and differentiable with respect to its parameter We for almost surely X. Furthermore, for any parameter We Rd2, if there exists α(u) for 1 u, d2 such that (cid:80)d2 (X, We) , α(uv) 3 2g (X, We)+(cid:80)d2 u,v=1 α(uv) (v) 0 for almost surely X, then we must have α(u) 3 = 0 for all 1 u, d2. For example, it can be verified that feed-forward networks (FFNs) of the form g(X, (We,2, We,1, b)) = e,1X + b) we used in Section 2.2 satisfy this algebraic independence condition. On We,2Softplus(W 1 , α(uv) (X, We)+(cid:80)d2 ei, ν 2 ), . . . , (W 2 = α(uv) 1 = α(uv) u,v=1 α(uv) u=1 α(u) i=1 δ(W ei W (u) W (u) eN , ν (v) (u) 2 1 3 (X, We) = 6 the other hand, since linear experts g(X, (a, b)) = aX + does not meet this condition, we will conduct separate convergence analysis for them in Appendix K. Maximum likelihood estimation. Since the number of ground-truth experts is typically unknown in practice, we fit the model equation (6) with mixture of > experts. Then, we estimate the unknown parameters (W ), for 1 , via estimating the ground-truth mixing measure using the maximum likelihood method as follows: ei, ν (cid:98)Gn arg max GGN (Θ) 1 (cid:88) i=1 log(pG(YiXi)), (7) where we define GN (Θ) := {G = (cid:80)N Proposition 3.1. With the MLE defined in equation (7), the convergence rate of the density estimation i=1 δ(Wei ,νi) : 1 N, (Wei, νi) Θ}. (cid:98)Gn (Y X) to the ground-truth density pG(Y X) is given by: (X), pG(X))] = OP ((cid:112)log(n)/n), EX [V (p (cid:98)Gn (cid:82) p1 p2dm as the Total Variation distance between two probability Above, we denote (p1, p2) := 1 2 density functions p1, p2 dominated by the Lebesgue measure m. The proof of Proposition 3.1 can be found in Appendix L.3. The above result indicates that the density estimation at parametric rate of order (cid:101)OP (n1/2). Thus, if we can construct some loss function between two mixing measures (cid:98)Gn and G, denoted (X), pG(X))] L( (cid:98)Gn, G), then we will obtain parameter by L( (cid:98)Gn, G), such that EX [V (p and expert estimation rates via the bound L( (cid:98)Gn, G) = OP ((cid:112)log(n)/n). For that purpose, let us introduce the concept of Voronoi loss proposed in Manole et al. [54]. converges to its true counterpart pG (cid:98)Gn (cid:98)Gn Voronoi loss. For an arbitrary mixing measure G, we distribute its atoms to the following Voronoi cells generated by the support points of the ground-truth mixing measure G: Cj Cj(G) := {i [N ] : θi θ θi θ ℓ , ℓ = j}, (8) where we denote θi := (Wei, νi) and θ of each Voronoi cell Cj indicates the number of fitted atoms for the ground-truth atom θ build loss function based on these Voronoi cells as follows: ) for all [N ] and [N ]. Here, the cardinality . Then, we := (W ej , ν L1(G, G) := (cid:88) j=1 (cid:12) (cid:12) (cid:12) (cid:88) iCj (cid:12) exp(ci) exp(c (cid:12) (cid:12) + ) (cid:88) + (cid:88) (cid:88) (cid:104) exp(ci) Wei ej + νi ν (cid:105) j[N ]:Cj =1 (cid:88) exp(ci) iCj (cid:104) Wei ej 2 + νi ν 2(cid:105) . (9) j[N ]:Cj > iCj Given the above Voronoi loss, we are ready to capture the convergence rates of parameter estimation and expert estimation in Theorem 3.2 whose proof can be found in Appendix L.1. Theorem 3.2. The following lower bound holds for any mixing measure GN (Θ): This lower bound and the result of Theorem 3.1 imply that L1( (cid:98)Gn, G) = OP ((cid:112)log(n)/n). EX [V (pG(X), pG(X))] L1(G, G). (10) 7 few remarks regarding Theorem 3.2 are in order. (i) Expert estimation rates. From the above results and the formulation of the Voronoi loss L1, it , i.e., for [N ] : Cj = 1, follows that the rates for estimating exact-specified parameters are of parametric order (cid:101)OP (n1/2). Meanwhile, those for over-specified parameters , i.e., for [N ] : Cj > 1, are slightly slower, of order (cid:101)OP (n1/4). Since the expert function g(X, We) is ej for Lipschitz continuous w.r.t its parameter We, we have g(X, (cid:99)W almost surely X. As result, the estimation rates for exact-specified and over-specified experts ej ) are also of orders (cid:101)OP (n1/2) and (cid:101)OP (n1/4), respectively. Furthermore, we show in g(X, Appendix that experts of linear form g(X, (a, b)) = aX + also admit these estimation rates. ei) g(X, ej ) (cid:99)W ei ej , ν ej , ν (ii) Sample efficiency of the competition mechanism. Therefore, we need at most O(ϵ4) data points to approximate these experts with given error ϵ > 0. On the other hand, when not using the competition mechanism [61], the convergence rates of expert estimation become significantly slow and decrease when the number of fitted experts increases. For instance, if an expert g(X, ej ) is fitted by three experts, i.e., Cj = 3, then its estimation rate is of order (cid:101)OP (n1/12). Thus, we need much more data points, specifically O(ϵ12), to approximate this expert. Consequently, we conclude that the competition mechanism helps improve the sample efficiency in terms of expert estimation."
        },
        {
            "title": "4.1 Sparse Mixture of Experts",
            "content": "Mixture of Experts (MoE) is fundamental model in machine learning [35, 37] and an instance of the conditional computation framework where different experts are responsible for different regions of the input space [96, 6, 55, 63, 68]. Extensive efforts have been devoted to establishing theoretical foundation for MoE, including the universal approximation properties [74, 65, 64, 72, 66, 69], model selection criterion [39, 58, 73, 71, 70], convergence rate for density estimations [57, 75, 76] and the problem of parameter estimation [33, 61, 62, 59]. SMoE, the sparse variant of MoE, is more commonly applied to scale large language models [26]. It is often the architecture of choice in many leading industrial models such as Mixtral [36] and DeepSeek [17, 20, 21]. Within the research community, developing novel routing strategies has been major focus. Notable strategies include letting experts select tokens [101], improving the expert selection process [44, 26, 102, 11, 19, 10, 22], or global expert assignment scheme[45, 14]. Despite the promising progress, many such strategies often do not scale well to LLMs with billions of parameters or the language pre-training setting. In contrast, our work goes beyond both the pure theoretical or analytical studies by developing theoretically-grounded algorithm for effective training of large-scale LLM models. Orthogonal to the aforementioned papers, GShard [44] developed an efficient framework to automatically sharding massive SMoE models across many devices. Lastly, sparse upcycling [41] duplicated pre-trained models to build an MoE, which bypasses the expensive costs of training from scratch."
        },
        {
            "title": "4.2 Competitive Learning",
            "content": "Competitive learning refers to framework where computational units compete with one another for the right to response to an input [56]. Its development is closely related to the biological brain where only certain cells respond strongly to particular pattern and send suppressive signals to the remaining cells [3, 89, 25]. Early investigations of competitive learning showed encouraging results in various learning strategies such as action selection [27], self-organizing maps [92, 40], feature discovery [84], and spiking networks [77]. Recently, the competition mechanism also motivates the development of various advanced machine learning methods such as maxout networks [28], compete to compute [88], and independent mechanisms [2, 29]. Our study establishes framework to apply competition to SMoE training and develops an algorithm to train large scale SMoE with improved performances at low training overhead."
        },
        {
            "title": "5.1 Experimental Settings",
            "content": "Training tasks. We consider two tasks: (i) visual instruction tuning (VIT); and (ii) language pre-training. For VIT, we adopt the LibMoE [67] framework, which uses sparse upcycling [41] to transform an existing dense checkpoint into MoE. Training comprises three phases, where the first two focuses on initializing dense vision-language connector, and the last phase employs sparse upcycling to compare different MoE algorithms on the LLaVA 1.5 Instruction Tuning dataset [50]. For the language pre-training task, we follow the MoEUT framework [16] and pretrain on subset of the SlimPajama dataset [87]. Hyper-Parameters. For the VIT setting, we use Phi3.5 mini [1] as the LLM and SigLiP [98] as the vision encoder, totaling 5.1B parameters. Additionally, we sparse upcycled the dense models into four experts, and activated two per token. When training SMoE, all methods use the balancing loss [26] and z-loss [26], and are trained on approximately 109 tokens of the LLaVA 1.5 dataset. For the language pretraining task, we construct decoder-only transformer with 151M parameters (16 layers, four attention heads per layer, and hidden dimension of 512), each SMoE layer consists of 64 experts and eight of which are activated per token (K = 8). We train this model with balancing loss [16] on 7 109 tokens from the SlimPajama dataset. All experiments are conducted on cluster of 4xH100. Due to the expensive costs of the experiments, we only conducted one run using the same random seeds for all methods. We provide full description of the training setting in Appendix I. Evaluation Benchmarks. All models are evaluated under the zero-shot settings using the wellestablished benchmarks from the community. For the VIT task, we consider the following benchmarks: AI2D [38], TextVQA [86], GQA [34], HallusionBench [31], MathVista (test-mini split) [53], MMBench (English subset) [51], MME RealWorld Lite [100], MMMU Validation [95], MMStar [9], POPE [49], and OCRBench [52]. For benchmarks requiring GPT-based evaluation, such as MathVista and HallusionBench, we use GPT-4o,version 2024-08-06. These benchmarks are selected to cover wide range of capabilities of the model, from perception, reasoning, to assessing hallucination. For the language pre-training task, we consider the LAMBADA [78], BLiMP [94], Childrens Book Test (CBT) [99], HellaSwag [97], PIQA [7], ARC-Easy [15], and ARC-Challenge [15] benchmarks, which are common for the models at our scale. 9 Baseline. We compare CompeteSMoE against suite of state-of-the-art SMoE algorithms. First, SMoE [26], the original SMoE and still stands strong in todays leading models. Then, we consider activation-based SMoE such as XMoE [12], Perturbed Cosine Router (PCosine) [60], and MoEUT [16], which incorporate cosine similarity or sigmoid activation to improve routing efficiency. Furthermore, inspired by the DeepSeek V2 architecture [20], we also considered the SharedExpert V2 (SharedE-V2) baseline, which enhances SMoE with one shared expert. Similarly, for the language pretraining task, we also implement the SharedE-V3 baseline, which follows the DeepSeek V3 architecture [21]. SharedE-V3 replaces the softmax routing in SharedE-V2 with the normalized sigmoid. We implement these two baselines according to the public DeepSeek repository1."
        },
        {
            "title": "5.2 Main Results",
            "content": "Table 1: Performance comparison of various SMoE strategies on the VIT setting with 5B parameters model. Bolded numbers indicate the best result, underlined numbers are second-best. indicates that lower values are better, and indicates that higher values are better. Method AI2D SMoE [26] XMoE [11] PCosine [60] MoEUT [16] SharedE-V2 [20] 65.90 65.19 65.45 65.09 64.93 Text VQA 41.23 41.14 41.68 41.37 41.53 CompeteSMoE 66.22 41.92 GQA 60.96 60.63 61.38 61.48 61. 61.25 MM Bench 70.88 71.31 71.56 71.39 71.05 Hallusion 39.64 41.22 40.27 41.01 41.20 Math Vista 31.40 31.50 30.80 31.90 31.20 42.22 42.89 42.56 41.78 42.56 72.59 41.22 31.70 42. MMMU MMStar POPE OCR MME RWL Avg. Acc Avg. Rank 40.52 42.60 41.87 42.10 41. 42.25 86.56 86.12 86.90 86.52 86.08 32.10 31.30 30.80 32.20 32.40 31.89 32.51 32.05 30.95 32.36 49.39 49.67 49.57 49.62 49.63 4.55 3.50 3.42 3.64 4. 86.91 33.20 32.52 50.16 1.77 Table 2: Performance comparison of various SMoE strategies on the language pre-training setting. Bolded numbers indicate the best results, underlined numbers are second best. indicates that lower values are better, indicates that higher values are better."
        },
        {
            "title": "MoE",
            "content": "PPL LAMBADA BLiMP CBT HellaSwag PIQA ARC-E ARC-C SMoE [26] XMoE [11] PCosine [60] MoEUT [16] SharedE-V2 [20] SharedE-V3 [21]"
        },
        {
            "title": "CompeteSMoE",
            "content": "13.72 14.05 14.39 13.68 13.71 13.72 25.49 24.55 25.43 25.78 24.60 25.78 76.03 76.02 76.10 77.24 75.68 76.82 13.66 26.45 77. 75.40 75.45 74.21 75.22 75.29 75.58 75.51 29.00 28.62 28.66 29.05 29.18 29.30 29.10 59.09 58.05 57.07 59.03 58.71 58.49 58. 32.94 33.28 31.97 33.45 32.52 33.40 20.94 20.43 20.17 20.94 20.77 21.97 33.74 22.40 46.17 1. Avg. Acc Avg. Rank 45.56 45.20 44.80 45.82 45.25 45.91 3.94 5.63 6.25 2.94 4.63 2."
        },
        {
            "title": "5.2.1 Performance Comparison",
            "content": "We report the results of the considered algorithms under the VIT and language pre-training in Table 1 and Table 2, respectively. Overall, we observe that CompeteSMoE offers significant improvements over many benchmarks in both experiments. In addition, CompeteSMoE demonstrated the best performance in many of the challenging and important capabilities such as real-world visual perception and reasoning (MME RWL), reducing visual hallucination (Hallusion and POPE), OCR (OCRBench) and text-only reasoning (ARC-E and ARC-C). Furthermore, we report the evolution of the zero-shot 1https://github.com/deepseek-ai/DeepSeek-V3 10 performances on the VIT tasks during training in Figure 1. The results showed that CompeteSMoE consistently achieved better results than the baselines throughout training, corroborating our theoretical results that the competition mechanism enjoys better sample efficiency."
        },
        {
            "title": "5.2.2 Expert Routing Behavior Analysis",
            "content": "Table 3: Performance of SMoE and CompeteSMoE when changing top-1 expert to top-(K+1). Numbers in parentheses indicates the changes compared to the original routing results in Table 1."
        },
        {
            "title": "SMoE",
            "content": "Text VQA MMBench MMMU MMStar 42.94 (+2.42) 41.09 (-0.14) 43.22 (+1.00) 71.39 (+0.52) POPE 86.40 (-0.16) 31.50 (-0.60) OCR Bench Avg. Change CompeteSMoE 41.48 (-0.45) 71.22 (-1.37) 41.67 (-0.33) 40.55 (-1.70) 86.10 (-0.81) 31.70 (-1.50) 0.51 -1.03 (a) Evaluating the Effectiveness of Expert Routing. We investigate the experts selections quality of different policies. To this end, during inference, we replace the expert with the highest affinity score with the expert with the + 1 highest score, which is equivalent to shifting the selected experts down by one rank. Table 3 reports the results of this experiment in the VIT setting. The results show that the SMoE routing policy is clearly suboptimal since selecting worse expert led to improvements on several benchmarks. On the other hand, CompeteSMoE performances drop in all cases when we deliberately deviate from the router that learned the competition policy. This result shows that CompeteSMoE facilitated more effective routing policy compared to the traditional SMoE. Figure 3: Comparison of expert change rates at different training stages. Lower values are better. In Figure 1, we showed that CompeteSMoE (b) Stability of Expert Routing During Training. achieved better convergent rates on zero-shot benchmarks than many baselines. We now investigate the convergence rate of the router, showing that CompeteSMoE can quickly find good routing policy on zero-shot evaluation benchmarks. To this end, we introduce Expert Change Rate (ECR) to measure the convergence rate of routers. Specifically, given dataset D, we record the expert assignments in all layers for each token in using two model checkpoints at time steps and . Then, the ECR of from to is the number of mismatched assignments normalized by all assignments. If the router has converged, then we expect ECR to be low. Otherwise, high ECR values indicate that the routers policy is changing and unstable. Figure 3 reports the ECR throughout training on four VIT zero-shot benchmarks. We can clearly see that CompeteSMoE has lower ECR in all cases, suggesting that its routers have faster convergence rate than SMoE. Together with the better performances as reported in Figure 1 and Table 1, these experiments corroborate with our theoretical results in Section 3 and showed that CompeteSMoE not only achieved better sample efficiency, but also the final performance on zero-shot benchmarks."
        },
        {
            "title": "5.3 Complexity Analysis",
            "content": "Table 4: Computation complexities of various SMoE algorithms. Method Training time Throughput Training Inference SMoE XMoE MoEUT PCosine SharedE-V2 CompeteSMoE 12h39m 13h37m 12h59m 13h37m 12h21m 13h01m 14.59 13.57 14.23 13.57 14.95 14.18 9.87 8.97 9.61 8.59 9.66 9.88 We compare the computational complexities of various methods in Table 4. We report the wall-clock training time, training throughput, and inference throughput in the VIT setting of the 5.1B model. The results show that CompeteSMoEs training complexity is quite comparable to the standard SMoE, which is only about 3% faster. During inference, CompeteSMoE only uses the simple router, which is exactly the same as SMoE, and is more efficient than cosine similarity-based strategies such as XMoE and PCosine because they introduce additional parameters to the router. In summary, this result shows that CompeteSMoE can effectively leverage competition to improve training with modest training overhead."
        },
        {
            "title": "6 Conclusion",
            "content": "This work proposes competition, novel strategy to route tokens to experts, and rigorously show that it enjoys better sample efficiency than softmax routing. Building upon this foundation, we develop CompeteSMoE, an effective algorithm to train large-scale SMoE models with competition at low computational overhead. Extensive experiments on the visual instruction tuning and language pre-training tasks demonstrate that CompeteSMoE enjoys both faster convergence rate and final performance on many common zero-shot benchmarks at minimal overhead. Despite achieving encouraging results, CompeteSMoE introduces several hyper-parameters, which may increase the cost for hyper-parameter search. In Section 2.4, we provided guideline for hyperparameter configuration to alleviate this issue. Algorithmically, CompeteSMoE applies competition on each SMoE layer independently and does not take into account the interactions among experts at different layers. An ideal solution is to perform graph traversal algorithm through the network depth to determine an optimal expert selection at all layers simultaneously. However, this idea goes beyond the scope of this work, and we will leave it for future studies. 12 Supplement to CompeteSMoE Statistically Guaranteed Mixture of Experts Training via Competition This document provides the suppplementary materials for the paper CompeteSMoE Statistically Guaranteed Mixture of Experts Training via Competition, and is organized as follows."
        },
        {
            "title": "B Broader Impact",
            "content": "C Adaptive Layer-wise Competition Control"
        },
        {
            "title": "H Ablation Study",
            "content": "I Hyperparameter Setting I.1 Vision Language Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I.2 Language Model Pretrain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I.3 Hyperparameter Settings for CompeteSMoE . . . . . . . . . . . . . . . . . . . . . . . Training Curves on Vision-Language Benchmarks"
        },
        {
            "title": "L Proof of Theoretical Results",
            "content": "L.1 Proof of Theorem 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L.2 Proof of Theorem K.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L.3 Proof of Proposition 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L.3.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L.3.2 Main Proof . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 15 15 16 17 18 19 21 21 22 22 23 24 24 33 41"
        },
        {
            "title": "A Summary of Main Notations",
            "content": "Table 5: Summary of Main Notations. Symbol Description R, Wr g, We s, sR, sC TopK TopK0 [M ] ˆy, l κ σ E[] IC α γ β ω Amax λ(t) Λ(l) LNLL LD Ldiv ξt Qprev an = O(bn) or an bn an = OP (bn) an = (cid:101)OP (bn) w(u), wu wz z! (µ, ν) δ Θ d1 d2 (cid:98)Gn , 1 h(p1, p2) (p1, p2) Router network (function) and its parameter Expert network (function), and its parameter Input Affinity scores, affinity scores from the router, affinity scores from competition Function retaining the largest vector elements and setting others to Function retaining the largest vector elements and setting others to 0 Number of experts activated per input The total number of experts Set of {1, 2, ..., } for any positive integer Predicted output, ground truth Current t-th iteration Total number of training steps The l-th SMoE layer Total number of SMoE layers in the model Activation function Scoring function Mean of vector elements Base of the exponential function Indices of experts who won in the competition mechanism Hyper-parameter prioritizing winning experts in distillation loss Hyper-parameter for distillation loss Hyper-parameter for diversity loss Bernoulli probability for scheduling competition in each layer Maximum number of layers that can perform competition on single time step scheduler determining whether to perform competition at the t-th step vector storing the results of the scheduler λ(t) at all time steps of the l-th layer Negative log-likelihood function (task loss) Distillation loss Diversity loss Step size benchmark dataset for evaluation Cumulative competition activations over layers 1 to 1 If an Cbn for all N, where > 0 is some universal constant ϵ > 0, > 0 : P(An/bn > ) < ϵ for all sufficiently large an = OP (bn logc(bn)), for some > 0. The u-th entry of vector Rd wz = wz1 := w1 + w2 + . . . + wd, for any vector Rd z! := z1!z2! . . . zd!, for any vector Nd The number of ground-truth experts Univariate Gaussian density with mean µ and variance ν Ground-truth mixing measure Dirac measure Lebesgue measure Parameter space Dimension of input space Dimension of expert parameter space Maximum likelihood estimator for ℓ2-norm and ℓ1-norm value Cardinality of any set Hellinger distance h(p1, p2) := Total Variation distance (p1, p2) := 1 p1 (cid:82) p1 p2dm for any densities p1, p2 , for any vector Rd and Nd for any densities p1, p2 2 . . . wzd p2)2dm 1 wz (cid:16) 1 2 (cid:17)1/2 (cid:82) ( We summarize the main notations used in the main paper in Table 5, including those introduced later in the supplementary material."
        },
        {
            "title": "B Broader Impact",
            "content": "Although our work mostly contributes to the machine learning literature, it also drew inspiration from biology and neuroscience. Specifically, the competition mechanism is rooted in biology, has been studied in neuroscience, and has motivated few machine learning algorithms. Our work contributed theoretically grounded algorithm to train large-scale SMoE models, which could potentially push the frontier of the next LLM generation. Lastly, working with large models requires rather costly resources. We took serious precautions during the development of this work, including providing guideline for hyper-parameter selection, and conducting single experiment using the same random seed to ensure the results are reliable at low cost. Adaptive Layer-wise Competition Control While scheduled training reduces computational overhead, excessive simultaneous competition activations across multiple SMoE layers can destabilize the training process. To address this, we propose dynamic mechanism that regulates the number of active competition layers at each training step, enhancing training efficiency. This is achieved by enforcing global constraint on the maximum number of simultaneously active layers. For given layer l, we compute the cumulative competition activations from all preceding layers (i.e., layers 1 through 1) as: Qprev = l1 (cid:88) i=1 Λ(i), (11) where Λ(i) RT denotes the activation state vector of layer over training steps, and Qprev RT represents the cumulative competition activations up to layer 1. predefined threshold Amax governs the total number of active layers permitted per training step. If activating layer at step exceeds this threshold i.e., if Qprev(t) + Λ(l, t) > Amax with Λ(l, t) = 1 we redistribute the activation to an alternative step = satisfying: Qprev(t) + 1 Amax, {1, . . . , }, Λ(l, t) = 0. (12) Upon identifying t, we update the activation schedule by setting Λ(l, t) = 1 and Λ(l, t) = 0. Empirical results indicate that only 0% to 7% of layers are active at any step, ensuring the availability of suitable satisfying Eq. 12. In summary, this approach dynamically balances competition activations across layers, substantially reducing computational overhead while maintaining training stability for CompeteSMoE. 15 Effectiveness of Activation Functions in the Competition Mechanism Figure 4: Performance comparison of different activation functions used within the Competition Mechanism over 9 benchmarks. In this section, we investigate the impact of different activation functions on the effectiveness of the Competition Mechanism. Specifically, we analyze their role in computing affinity scores, as originally defined in Eq. 2.2. To support broader class of activation-based diversity functions, we generalize this formulation by redefining the affinity score as: si = E[κ(g(x, Wei))], [N ], (13) This formulation enables the competition mechanism to flexibly incorporate different activation profiles for expert selection. As shown in Figure 4, we compare the performance of several widely used activation functions within the Competition Mechanism, including Softplus, SiLU, Sigmoid, ReLU, and Softmax. Among these, Softplus consistently achieves the highest overall accuracy and ranking. We attribute this to its smooth and well-behaved response across the input domain. Specifically, Softplus softly suppresses negative values while preserving the magnitude of positive values, enabling it to retain useful signal across the entire activation range. This property not only preserves important representational information but also ensures continuous gradient flow, contributing to more stable optimization. In contrast, Sigmoid also suppresses negative values but squashes the entire input range into [0, 1], which can result in significant information loss and vanishing gradients for large magnitude inputs. ReLU, while preserving the magnitude of positive inputs, entirely discards negative values, potentially eliminating informative cues encoded in negative activations. We additionally experimented with an alternative affinity scoring formulation using the exponential function, i.e., E[eg(x,Wei )]. However, this led to uncontrolled growth in the output magnitudes, resulting in numerical instability and the emergence of NaN values during training. In contrast, Softplus provides controlled approximation of the exponential while avoiding such instability, 16 making it more suitable for robust training under the Competition Mechanism. In summary, activation functions that gently suppress negative activations while maintaining linear or near linear behavior for positive inputs such as Softplus are better aligned with the requirements of the Competition Mechanism. Their balanced characteristics lead to more stable expert affinity computation and improved overall performance. Evaluation of Mean and Norm Strategies for Competition Mechanism We conduct an empirical investigation to compare the mean-based strategy, as defined in Eq. 2.2, with norm-based formulation. Specifically, we compute the affinity score of expert using the L2 norm of its output vector: si = g(x, Wei), [N ], (14) As shown in Figure 4, the CompeteSMoE-Norm variant using Equation 14 yields higher performance compared to the SMoE standard. However, when we switch to the CompeteSMoE-Softplus configuration that employs mean based strategy, substantial improvement is observed in both average accuracy and ranking. In conclusion, the mean-based strategy proves to be the most effective setting for expert output aggregation within the Competition Mechanism."
        },
        {
            "title": "F Evaluation of Distillation Loss Effectiveness",
            "content": "Table 6: Performance comparison between LD and LDwo-reg across 9 benchmark datasets. Loss Function Avg. Acc Avg. Rank LDwo-reg LD 52.92 53. 1.78 1.22 Figure 5: Learning performance of LD and measured by the Level Learning metric at LDwo-reg every 20% of training steps on the MMBench-EN benchmark. In Section 3, we established the theoretical foundation for the competition mechanism and demonstrated its empirical effectiveness in Table 1. key challenge in optimizing the router network is accurately modeling the distribution of competitive routing decisions. We carefully investigated two objective functions: the distillation loss LD (see details in Eq. 2) and variant distillation without the regularization term, which emphasizes penalizing experts who won the loss LDwo-reg 17 competition. We define LDwo-reg as follows: LDwo-reg(sR, sC) = MSE(sR, sC) (15) Figure 5 illustrates the progression of the Level Learning (LL) metric, which measures the number of Top-K experts selected by the router network that align with the Top-K experts from the competition mechanism. high LL value indicates that the router network effectively learns from the competition mechanism, whereas low value suggests poor learning performance. Notably, LD consistently enables faster and more stable convergence compared to LDwo-reg . In particular, during the initial 60% of training (up to 9,600 steps), LD maintains clear advantage, effectively mitigating the early performance drop observed with LDwo-reg . Moreover, LD achieves peak LL score of 1210 by 12,000 peak of 1190, and exhibits more stable learning dynamics in later stages. steps, surpassing the LDwo-reg Additionally, quantitative results in Table 6 further confirm this trend, with LD yielding higher average accuracy (53.21% vs. 52.92%) and lower average rank (1.22 vs. 1.78) across nine benchmarks. These findings underscore the effectiveness of LD in guiding the router network to better approximate the competition mechanism. Furthermore, they suggest its potential as preferred optimization objective in competitive MoE architectures."
        },
        {
            "title": "G Further Analysis of Router Behavior",
            "content": "In this section, we further analyst about router behavior in SMoE and CompeteSMoE. Figure 6: Entropy analysis of expert selection frequency across perception and reasoning tasks. Lower entropy indicates higher specialization in expert routing. (a) Experts distribution on Reasoning and Perception. As illustrated in Figure 6, we analyze the entropy of expert distribution across layers for SMoE and CompeteSMoE algorithms, evaluated on three benchmarks: MME Real-World Perception and OCR Bench for perception capacity, and MME Real-World Reasoning and MathVista for reasoning capacity. On perception tasks, CompeteSMoE exhibits higher entropy in the early layers, indicating exploratory behavior, but significantly reduces entropy in the middle and final layers. In contrast, on MathVista benchmark requiring higher-level reasoning CompeteSMoE maintains low entropy in the early and intermediate layers, approaching entropy levels similar to SMoE in the final layers. Both models demonstrate increasing entropy toward the final layers, suggesting more balanced expert allocation as the network deepens, consistent with typical Transformer-based architectures where later layers aggregate information from multiple upstream experts. Regarding the representation collapse issue, both SMoE and CompeteSMoE achieve high degree of balance in expert distribution, with entropy scores exceeding 1.99 (compared to the maximum entropy of 2 for four experts). 18 Figure 7: Layer-wise entropy of expert weight distributions for CompeteSMoE and SMoE across three tasks: Real-World Perception, Real-World Reasoning, and Mathematical Reasoning. (b) Effective Expert Aggregation via Weight Distribution. As shown in Figure 7, we analyze the entropy of expert weight distributions across layers and tasks, which reflects how expert contributions are aggregated. Lower entropy typically suggests more confident expert selection. Both SMoE and CompeteSMoE exhibit decreasing entropy across layers, implying increased decisiveness in expert routing at deeper layers. While SMoE generally maintains lower entropy, especially on MathVista, it tends to concentrate weights heavily on small subset of experts. In contrast, CompeteSMoE distributes weights more evenly among the selected experts. This balanced aggregation allows CompeteSMoE to better leverage complementary knowledge from multiple experts. Finally, we observe slight difference between the two models, with both showing trend toward more confident weight distributions in the final layers."
        },
        {
            "title": "H Ablation Study",
            "content": "We conducted an ablation study on 5.1B parameter VLM, evaluating performance across various configurations. The best performance was observed with the large-scale model. Table 7: Ablation of Competition Mechanism (CM) and Diversity Loss (DL) on 9 benchmarks. CM DL Avg. Acc Avg. Rank 53.21 52.71 52.90 52.47 1.45 2.91 2.27 3.36 Table 8: Ablation study on the activation frequency of the Competition Mechanism (CM) during training. ω Avg. Acc Avg. Rank 3% 5% 7% 9% 52.81 52.92 53.21 52.82 2.72 2.61 1.83 2.83 Table 9: Effect of coefficient α in Distillation loss. α Avg. Acc Avg. Rank 0.0 0.1 0.2 0.3 52.92 53.21 52.98 52. 2.83 1.78 2.56 2.83 19 Effect of Component-wise Design on Model Performance. To better understand the individual contributions of the two core components, the Competition Mechanism (CM) and Diversity Loss (DL), we conduct component-wise ablation study, as shown in Table 7, across nine benchmark datasets. Overall, both components independently outperform the SMoE baseline. In detail, removing DL leads to drop of 0.49% in average accuracy and an increase of 1.45 in average rank. In contrast, removing CM results in smaller accuracy drop of 0.30% and an increase of 0.81 in average rank. These results indicate that the Competition Mechanism contributes more significantly to model performance than Diversity Loss when evaluated in isolation. Notably, combining both components yields the best overall performance, suggesting that CM and DL are complementary. Specifically, DL encourages output diversity among won experts, which in turn enables CM to compute more informative and discriminative affinity scores for expert routing. Effect of the Distillation Loss Coefficient α. We investigate the impact of the Distillation Loss coefficient α, which balances the main objective with an auxiliary regularization term. As shown in Figure 9, setting α = 0.1 achieves the best performance, indicating that moderate regularization strength provides useful inductive bias. Increasing α further leads to performance degradation, suggesting that excessive influence from the auxiliary loss may conflict with the primary learning signal. Analysis of Competition Mechanism Activation Frequency. We further investigate how often the Competition Mechanism (CM) should be activated during training. Table 8 reports model performance when CM is applied at different ω of training steps. We observe that using small ω (e.g., 3%) leads to suboptimal results, likely due to insufficient competitive pressure. As ω increases, performance improves, with the best accuracy (53.21%) and rank (1.83) achieved at 7%. Notably, increasing ω beyond this point (e.g., 9%) offers no further gain and may introduce instability, suggesting saturation effect. These results highlight that moderate activation schedule (e.g., ω in the range of 57%) is sufficient to leverage the benefits of competition while maintaining training stability."
        },
        {
            "title": "I Hyperparameter Setting",
            "content": "I.1 Vision Language Model Table 10: Hyperparameter configurations for three training stages of Phi-3.5 Mini: Pre-Training (PT), Pre-FineTuning (PFT), and Visual Instruction Tuning (VIT)."
        },
        {
            "title": "Learning rate",
            "content": "PT 1e-"
        },
        {
            "title": "PFT",
            "content": "2e-"
        },
        {
            "title": "VIT",
            "content": "4e-"
        },
        {
            "title": "Batch size per GPU",
            "content": "64"
        },
        {
            "title": "GPUs",
            "content": "4H100 4H100 4H"
        },
        {
            "title": "ZeRO optimization",
            "content": "ZeRO-2 ZeRO-2 ZeRO-"
        },
        {
            "title": "Balance loss coefficient",
            "content": "Z-loss coefficient No 0.0 0.0 No 0. 0."
        },
        {
            "title": "Maximum tokens",
            "content": ""
        },
        {
            "title": "Yes",
            "content": "0.01 0.001 2048 Table 11: Model configuration for the Visual Instruction Tuning (VIT) stage of Phi-3.5 Mini, incorporating MoE architecture with 4 experts and top-2 expert selection. #params"
        },
        {
            "title": "NE K",
            "content": "5.1B Phi-3.5 Mini Instruct SigLIP-SO400M-Patch14-224 4 2 As shown in Table 10, we present the hyperparameter settings for three training stages, following prior work [67, 48]. Additionally, Table 11 details the configurations of the pretrained language model and vision encoder. When training MoE blocks during the VIT stage, only the router network is initialized from scratch. So the weights of the router network are sampled from normal distribution with mean of 0 and standard deviation of 0.02, referenced from the initialization method in the public GPT-2 repository2, using fixed random seed of 42 to ensure reproducibility and fairness in validation across algorithms. Overall, we train all MoE algorithms on large-scale 5.1B-parameter model with identical settings to ensure fair comparison. 2https://github.com/openai/gpt-2 21 I.2 Language Model Pretrain Table 12: Model architecture configuration for pretraining the MoE language model. Abbreviations: #params (total trainable parameters), nlayers (transformer layers), dexpert (expert dimension), (hidden size), dhead (attention head dimension), NE (number of experts), (top-K experts per token), Nwarmup (warmup steps for learning rate). #params nlayers 151M dexpert dhead NE 512 82 16 64 8 Table 13: Hyperparameter configuration used for language model pretraining."
        },
        {
            "title": "Schedule",
            "content": "Batch size / GPU"
        },
        {
            "title": "Optimizer",
            "content": "Balance coeff. Z-loss coeff. 0."
        },
        {
            "title": "Cosine",
            "content": "64 4 H100 AdamW 0.01 0.00 For pretraining the language model, we leverage the MoEUT [16] framework to train Mixtureof-Experts (MoE)-based architectures. Unlike approaches that employ parameter sharing across layers, we preserve the original Transformer architecture without such sharing. Furthermore, SMoE layers are integrated exclusively into the MLP blocks, leaving the attention modules unmodified, as detailed in Table 12. For hyper-parameters, we adopt the coefficient settings specified in the MoEUT framework. Model weights are initialized according to the MoEUT framework, with fixed random seed of 42 to ensure reproducibility. Consistent with the MoEUT framework, we exclude the Z-loss term by setting its coefficient to 0, as reported in Table 13. I.3 Hyperparameter Settings for CompeteSMoE Table 14: Hyperparameter Configuration for CompeteSMoE on Large-Scale Model. warm up ω γ α β"
        },
        {
            "title": "Amax",
            "content": "0.05 0.07 0.01 0.1 0.005 In Table 14, we present the training configuration for CompeteSMoE on large-scale model. First, we conduct warm-up training on the SMoE for 5% of the total training steps to stabilize the router network and experts before initiating the competition mechanism. Second, we set Amax = 9, which ensures stable training by preventing excessive simultaneous activation of competitive layers. This hyperparameter should be adjusted based on the number of training steps and the number of SMoE layers. Additionally, the hyper-parameters ω and α are analyzed in Appendix H. We fix β at small value of 0.005, set γ to slightly larger value of 0.01, and use the balanced loss across all training steps. 22 Training Curves on Vision-Language Benchmarks In Figure 8, we include additional training performance curves for 9 benchmarks, supplementing the results presented in Figure 1. Figure 8: Training curves of CompeteSMoE compared to five advanced MoE algorithms on visionlanguage benchmarks."
        },
        {
            "title": "K Additional Theoretical Results",
            "content": "In this appendix, we analyze the convergence behavior of Gaussian mixture of linear experts equipped with the competition mechanism. In particular, we consider experts of the linear form g(X, (a, b)) := aX + b, where Rd and R. Then, the conditional density function pG(Y X) in equation equation (6) becomes pG(Y X) := (cid:88) i=1 exp(log(1 + exp((a j=1 exp(log(1 + exp((a )X + i ))) )X + (cid:80)N ))) (Y (a )X + , ν ). (16) 23 Our ultimate goal is to compare the sample efficiency of this model to that without the competition mechanism [61] in terms of expert estimation. For that purpose, we use Voronoi loss tailored to the setting of linear experts, which is given by L2(G, G) : = + + (cid:88) j=1 (cid:12) (cid:12) (cid:12) (cid:88) iCj (cid:88) (cid:12) exp(ci) exp(c (cid:12) ) (cid:12) (cid:88) (cid:104) exp(ci) ai + bi j + νi ν (cid:105) j[N ]:Cj =1 (cid:88) iCj (cid:88) j[N ]:Cj =1 iCj (cid:104) exp(ci) ai 2 + bi 2 + νi ν 2(cid:105) . (17) Equipped with the above Voronoi loss, we establish the convergence rate of parameter and expert estimations in the Gaussian mixture of linear experts with the competition in Theorem K.1. Theorem K.1. The following lower bound holds for any mixing measure GN (Θ): EX [V (pG(X), pG(X))] L2(G, G). (18) This lower bound indicates that L2( (cid:98)Gn, G) = OP ((cid:112)log(n)/n). The proof of Theorem K.1 can be found in Appendix L.2. few remarks regarding the results of this theorem are in order. (i) Parameter estimation rates. The bound of the Voronoi loss L2( (cid:98)Gn, G) in Theorem K.1 reveals that the estimation rates for exact-specified parameters , i.e., for [N ] : Cj = 1, are of parametric order (cid:101)OP (n1/2), whereas those for their over-specified counterparts, i.e., for [N ] : Cj > 1, are slightly slower, of order (cid:101)OP (n1/4). , ν , (ii) Expert estimation rates. Note that the input space is bounded, then we have (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)((cid:98)an (cid:12) )X + j )X for almost surely X. Consequently, the estimation rates for exact-specified and over-specified experts (a are also of orders (cid:101)OP (n1/2) and (cid:101)OP (n1/4), respectively. )X + (cid:98)bn + (cid:98)bn b , (a (cid:98)an (iii) Sample efficiency of the competition mechanism. Thus, we need polynomially many data points O(ϵ4) to estimate these linear experts with given error ϵ > 0. By contrast, when not using the competition mechanism [61], the linear expert estimation rates are substantially slowed down since they hinge on the solvability of some complex system of polynomial equations and are is decelerated as the number of fitted experts grows. For example, if linear expert (a fitted by two experts (or three experts), that is, Cj = 2 (or Cj = 3), then the rate for estimating this linear expert is of order (cid:101)OP (n1/8) (or (cid:101)OP (n1/12)). Therefore, we need O(ϵ8) (or O(ϵ12)), to estimate this expert. For that reason, we claim that the Gaussian MoE becomes more sample-efficient when equipped with the competition mechanism. )X + j"
        },
        {
            "title": "L Proof of Theoretical Results",
            "content": "L.1 Proof of Theorem 3.2 In this proof, we aim to demonstrate that the following lower bound holds for any GN (Θ): EX [V (pG(X), pG(X))] L1(G, G). (19) 24 For that purpose, we first establish the local part of the above bound, that is, lim ε0 inf GGN (Θ):L1(G,G)ε EX [V (pG(X), pG(X))] L1(G, G) > 0. This local part implies that there exists positive constant ε that satisfies inf GGN (Θ):L1(G,G)ε EX [V (pG(X), pG(X))] L1(G, G) > 0. Then, it is sufficient to derive the following global part of the bound in equation (19): inf GGN (Θ):L1(G,G)>ε EX [V (pG(X), pG(X))] L1(G, G) > 0. (20) (21) Local part: In this part, we will establish the local part in equation equation (20) using the proof by contradiction method. Suppose that the local part is not true, then we can find sequence of mixing measures (Gn) given by Gn := (cid:80)N i=1 exp(cn )δ(W ei ) GN (Θ) such that L1(Gn, G) 0 and ,νn EX [V (pGn(X), pG(X))]/L1(Gn, G) 0, as . As we use asymptotic arguments in this proof, we may assume without loss of generality (WLOG) that the Voronoi cells Cn := Cj(Gn) is independent of the sample size n. Then, the Voronoi loss of interest turns into L1(Gn, G) := (cid:88) j=1 (cid:12) (cid:12) (cid:12) (cid:88) iCj exp(cn ) exp(c ) (cid:88) (cid:88) + j[N ]:Cj =1 iCj (cid:12) (cid:12) (cid:12) + (cid:88) (cid:88) exp(cn ) (cid:104) ei ej + νn (cid:105) ν j[N ]:Cj =1 (cid:104) exp(cn ) iCj ei ej 2 + νn ν 2(cid:105) . (22) Since L1(Gn, G) 0 as , we have (W ei, νn ) (W ej , ν ) for all [N ] and Cj. Subsequently, we divide the rest of this proof into three main steps. Step 1: Taylor expansion. (cid:105) j=1 exp(log(1 + exp(g(x, ej )))) In this step, we aim to decompose the term Tn(Y X) := [pGn(Y X) pG(Y X)] can be decomposed as (cid:104) (cid:80)N Tn(Y X) = (cid:88) (cid:88) j=1 iCj (cid:104) exp(cn ) exp(log(1 + exp(g(X, ei))))f (Y g(X, ei), νn ) exp(log(1 + exp(g(X, ej ))))f (Y g(X, (cid:105) ej ), ν ) + (cid:88) (cid:88) (cid:104) exp(cn ) exp(log(1 + exp(g(X, ei)))) exp(log(1 + exp(g(X, ej ))))]pGn(Y X) j=1 iCj (cid:88) (cid:104) (cid:88) j=1 iCj exp(cn ) exp(c ) (cid:105) exp(log(1 + exp(g(X, ej ))))[f (Y g(X, ej ), ν ) pGn(Y X)] := Tn,1(Y X) Tn,2(Y X) + Tn,3(Y X). 25 Next, we continue to decompose the term Tn,1(Y X) as Tn,1(Y X) = (cid:88) (cid:88) (cid:104) exp(cn ) j[N ]:Cj = iCj exp(log(1 + exp(g(x, ei))))f (Y g(X, ei), νn ) exp(log(1 + exp(g(x, ej ))))f (Y g(X, ej ), ν ) (cid:105) + (cid:88) (cid:88) (cid:104) exp(cn ) j[N ]:Cj >1 iCj exp(log(1 + exp(g(x, ei))))f (Y g(X, ei), νn ) exp(log(1 + exp(g(x, ej ))))f (Y g(X, (cid:105) ej ), ν ) := Tn,1,1(Y X) + Tn,1,2(Y X). Let us denote Fρ(Y X; We, ν) := exp(log(1 + exp(g(X, We)))) ρf first-order Taylor expansion to the function F0(Y X; We, ν) around the point (W the term Tn,1,1(Y X) as gρ (Y g(X, We), ν). By applying the ), we rewrite ej , ν Tn,1,1(Y X) = (cid:88) 2 (cid:88) j[N ]:Cj =1 ρ= (j) n,1,1,ρ(X)Fρ(Y ; X, ej , ν ) + Rn,1,1(Y X), where Rn,1,1(Y X) is the Taylor remainder such that Rn,1,1(Y X)/L1(Gn, G) 0 as , and (j) n,1,1,0(X) := (j) n,1,1,1(X) := (j) n,1,1,2(X) := (cid:88) iCj (cid:88) iCj (cid:88) iCj exp(cn ) exp(cn ) d2(cid:88) u=1 d2(cid:88) u= (W eij )(u) W (u) (X, ej ) 1 1 + exp(g(X, ej )) , (W eij )(u) W (u) (X, ej ), 1 exp(cn )(νn ij), in which eij := ν Meanwhile, by means of the second-order Taylor expansion, the term Tn,1,2(Y X) can be ei ej ij := νn and νn . represented as Tn,1,2(Y X) = (cid:88) 4 (cid:88) j[N ]:Cj >1 ρ=0 (j) n,1,2,ρ(X)Fρ(Y ; X, ej , ν ) + Rn,1,2(Y X), 26 where Rn,1,2(Y X) is the Taylor remainder such that Rn,1,2(Y X)/L1(Gn, G) 0 as , and (cid:34) d2(cid:88) exp(cn ) u= (W eij )(u) W (u) (X, ej ) 1 + exp(g(X, ej )) (W exp(cn ) eij )(v) eij )(u)(W 1 + 1{u=v} (cid:34) d2(cid:88) (W u=1 (X, ej ) eij )(u) W (u) (cid:32) 2 2g (v) (u) (X, ej ) + W (u) 1 + exp(g(X, (X, ej ) W (v) ej )) (X, ej ) (cid:35) , (W eij )(u)(W 1 + 1{u=v} eij )(v) (X, ej ) W (v) (u) 1 + exp(g(X, (X, ej ) ej )) + 2g (v) (u) (cid:33)(cid:35) , (X, ej ) (cid:34) exp(cn ) 1 2 (νn ij) + d2(cid:88) u,v=1 (W eij )(u)(W 1 + 1{u=v} eij )(v) W (u) (X, ej ) W (v) (X, ej ) iCj d2(cid:88) + (W eij )(u)(νn ij) W (u) (X, ej ) 1 + exp(g(X, ej )) 1 (cid:35) , d2(cid:88) exp(cn ) 1 2 (W eij )(u)(νn ij) W (u) (X, ej ), exp(cn ) (νn ij)2. u=1 1 4 (j) n,1,2,0(X) := + (cid:88) iCj d2(cid:88) u,v=1 (j) n,1,2,1(X) := + (cid:88) iCj d2(cid:88) u,v=1 (j) n,1,2,2(X) := (cid:88) u=1 (j) n,1,2,3(X) := (j) n,1,2,4(X) := (cid:88) iCj (cid:88) iCj Next, we decompose the term Tn,2(Y X) as Tn,2(Y X) (cid:88) := (cid:88) (cid:104) exp(cn ) exp(log(1 + exp(g(X, ei)))) exp(log(1 + exp(g(X, ej ))))]pGn(Y X) j[N ]:Cj =1 (cid:88) iCj (cid:88) exp(cn ) (cid:104) + exp(log(1 + exp(g(X, ei)))) exp(log(1 + exp(g(X, ej ))))]pGn(Y X) j[N ]:Cj >1 iCj := Tn,2,1(Y X) + Tn,2,2(Y X). Note that we can rewrite the term Tn,1,2(Y X) using the first-order Taylor expansion to the function as exp(log(1 + exp(g(W ei)))) around the point ej Tn,2,1(Y X) = (cid:88) (cid:88) exp(cn ) j[N ]:Cj =1 iCj d2(cid:88) u=1 (W eij )(u) W (u) (X, ej ) 1 + exp(g(X, ej )) Hn(Y X; ej ) +Rn,2,1(Y X), where we denote Hn(Y X; We) = exp(log(1 + exp(g(X, We))))pGn(Y X) and Rn,2,1(Y X) is the Taylor remainder such that Rn,2,1(Y X)/L1(Gn, G) 0 as . 27 On the other hand, by means of the second-order Taylor expansion, we have Tn,2,2(Y X) = (cid:88) (cid:88) exp(cn ) (cid:34) d2(cid:88) (W eij )(u) j[N ]:Cj >1 iCj u=1 W (u) (X, ej ) 1 + exp(g(X, ej )) d2(cid:88) + u,v=1 (W eij )(u)(W 1 + 1{u=v} eij )(v) 2g (v) (u) (X, ej ) + W (u) 1 + exp(g(X, (X, ej ) W (v) ej )) (X, ej ) (cid:35) Hn(Y X; ej ) + Rn,2,2(Y X), where Rn,2,1(Y X) is the Taylor remainder such that Rn,2,2(Y X)/L1(Gn, G) 0 as . From the above equation, [Tn,1,1(Y X)Rn,1,1(Y X)], [Tn,1,2(Y X)Rn,1,2(Y X)], [Tn,2,1(Y X) Rn,2,1(Y X)], [Tn,2,2(Y X) Rn,2,2(Y X)] and [Tn,3(Y X)] can be seen as combination of elements of the set := (cid:83)N ρ=0 Sρ,j, where we define (cid:83)5 j= (cid:40) S0,j := W (u) W (u) (X, ej ) 1 + exp(g(X, ej ) W (v) (X, ei)) (X, ej ) F0(Y X; ej , ν ), 2g (v) (u) 1 + exp(g(X, (X, ej ) ej )) F0(Y X; ej , ν ), F0(Y X; ej , ν ), F0(Y X; ej , ν ) : 1 u, d2 (cid:41) , 1 + exp(g(X, ej )) (X, ej ) 1 + exp(g(X, W (u) ei)) F1(Y X; ej , ν ), 2g (v) (u) (X, ej )F1(Y X; ej , ν W (u) (X, ej ) W (v) (X, ej ) 1 + exp(g(X, ej )) (cid:41) , ) : 1 u, d2 S1,j := (cid:40) (cid:40) S2,j := F2(Y X; ej , ν ), W (u) (X, ej ) 1 + exp(g(X, ei)) F2(Y X; ej , ν ), W (u) (X, ej ) W (v) (X, ej )F2(Y X; ej , ν ) : 1 u, d2 (X, ei)F3(Y X; ej , ν ) : 1 (cid:41) , S3,j := W (u) (cid:40) (cid:40) S4,j := F4(Y X; ej , ν ) (cid:41) , F1(Y X; ej , ν ) (cid:41) , (cid:40) S5,j := W (u) W (u) (X, ej ) 1 + exp(g(X, ej ) W (v) (X, ei)) (X, ej ) 1 + exp(g(X, ej )) Hn(Y X; ej , ν ), 2g (v) (u) 1 + exp(g(X, (X, ej ) ej )) Hn(Y X; ej , ν ), Hn(Y X; ej , ν ), Hn(Y X; ej , ν ) : 1 u, d2 (cid:41) . In this step, we will show that at least one among Step 2: Non-vanishing coefficients. the coefficients in the representations of [Tn,1,1(Y X) Rn,1,1(Y X)]/L1(Gn, G), [Tn,1,2(Y X) Rn,1,2(Y X)]/L1(Gn, G), [Tn,2,1(Y X)Rn,2,1(Y X)]/L1(Gn, G), [Tn,2,2(Y X)Rn,2,2(Y X)]/L1(Gn, G) and [Tn,3(Y X)]/L1(Gn, G) does not approach zero when goes to infinity. Assume by contrary that all of them vanish as . Then, by considering the coefficients of the term F0(Y X; ej , ν ) for [N ], we have"
        },
        {
            "title": "1\nL1(Gn, G∗)",
            "content": "(cid:88) j=1 (cid:12) (cid:12) (cid:12) (cid:88) iCj exp(cn ) exp(c ) (cid:12) (cid:12) (cid:12) 0. )) F0(Y X; ej , ν ) for [N ] : Cj = 1, we have W (X,W ej 1+exp(g(X,W ei (u) )"
        },
        {
            "title": "1\nL1(Gn, G∗)",
            "content": "(cid:88) (cid:88) j[N ]:Cj =1 iCj exp(cn )W eij 1 0. Due to the equivalence between the ℓ1-norm and the ℓ2-norm, we obtain"
        },
        {
            "title": "1\nL1(Gn, G∗)",
            "content": "(cid:88) (cid:88) j[N ]:Cj =1 iCj exp(cn )W eij 0. F2(Y X; ej , ν ) for [N ] : Cj = 1, we have"
        },
        {
            "title": "1\nL1(Gn, G∗)",
            "content": "(cid:88) (cid:88) j[N ]:Cj =1 iCj exp(cn )νn ij 0. W (u) g ) (X,W ej (u) 1+exp(g(X,W ej )) (X,W ej ) F0(Y X; ej , ν ) for [N ] : Cj > 1, we have"
        },
        {
            "title": "1\nL1(Gn, G∗)",
            "content": "(cid:88) (cid:88) j[N ]:Cj >1 iCj exp(cn )W eij 2 0. F4(Y X; ej , ν ) for [N ] : Cj > 1, we have"
        },
        {
            "title": "1\nL1(Gn, G∗)",
            "content": "(cid:88) (cid:88) j[N ]:Cj =1 iCj exp(cn )νn ij2 0. By taking the sum of the above limits, we obtain 1 = L1(Gn,G) L1(Gn,G) 0 as , which is contradiction. Thus, not all the coefficients in the representations of [Tn,1,1(Y X) Rn,1,1(Y X)]/L1(Gn, G), [Tn,1,2(Y X) Rn,1,2(Y X)]/L1(Gn, G), [Tn,2,1(Y X) Rn,2,1(Y X)]/L1(Gn, G), [Tn,2,2(Y X) Rn,2,2(Y X)]/L1(Gn, G) and [Tn,3(Y X)]/L1(Gn, G) converge to zero as . Stage 3 - Fatous argument: In this stage, we use the Fatous lemma to show contradiction to the result of Step 2. For that purpose, let us denote mn as the maximum of the absolute values of the coefficients in the representations of [Tn,1,1(Y X) Rn,1,1(Y X)]/L1(Gn, G), [Tn,1,2(Y X) Rn,1,2(Y X)]/L1(Gn, G), [Tn,2,1(Y X) Rn,2,1(Y X)]/L1(Gn, G), [Tn,2,2(Y X) 29 Rn,2,2(Y X)]/L1(Gn, G) and [Tn,3(Y X)]/L1(Gn, G). It follows from the result of Step 2 that 1/mn as . In addition, we also denote (cid:80) iCj eij )(u) exp(cn )(W mnL1(Gn, G) eij )(u)(W eij )(v) )(W mnL1(Gn, G) )(W mnL1(Gn, G) exp(cn eij )(u)(νn ij) α(u) 1,j , α(uv) 2,j , γ(u) , (cid:80) iCj exp(cn (cid:80) iCj (cid:80) (cid:80) (cid:80) exp(cn )(νn ij) iCj mnL1(Gn, G) exp(cn )(νn iCj mnL1(Gn, G) exp(cn ij)2 iCj ) exp(c ) mnL1(Gn, G) β1,j, β2,j, ξj, as for any [N ] and u, [d2] with note that at least one among α(u) γ(u) and ξj is non-zero. 1,j , β1,j, α(uv) 2,j , β2,j, By applying the Fatous lemma, we have 0 = lim EX [V (pG(X), pG(X))] mnL1(Gn, G) = (cid:90) 1 lim inf pGn(Y X) pG(Y X) mnL1(Gn, G) d(X, ), which implies that [pGn(Y X) pG(Y X)]/[mnL1(Gn, G)] 0 as for almost surely (X, ). Since the term (cid:80)N ei)))) is bounded, we also have Tn(Y X)/[mnL1(Gn, G)] 0 as . Then, it follows that j=1 exp(log(1+exp(g(x, 0 = lim Tn,1,1(Y X) + Tn,1,2(Y X) mnL1(Gn, G) lim Tn,2,1(Y X) + Tn,2,2(Y X) mnL1(Gn, G) + lim 30 Tn,3(Y X) mnL1(Gn, G) , (23) for almost surely (X, ) Y, where we have (cid:34) d2(cid:88) (cid:88) lim Tn,1,1(Y X) mnL1(Gn, G) := j[N ]:Cj =1 u=1 α(u) 1,j W (u) (X, ej ) 1 + exp(g(X, ej )) F0,j(Y X) + d2(cid:88) u=1 α(u) 1,j W (u) (X, ej )F1,j(Y X) + (cid:35) β1,jF2,j(Y X) , 1 2 lim Tn,1,2(Y X) mnL1(Gn, G) := (cid:88) (cid:34)(cid:32) d2(cid:88) u=1 α(u) 1,j W (u) (X, ej ) 1 + exp(g(X, ej )) j[N ]:Cj >1 2g (v) (X, (u) d2(cid:88) u,v=1 α(uv) 2,j 1 + 1{u=v} ej ) + W (u) 1 + exp(g(X, (X, ej ) W (v) ej )) (X, ej ) (cid:33) F0,j(Y X) (cid:32) d2(cid:88) u=1 α(u) 1,j W (u) (X, ej ) + d2(cid:88) u,v=1 α(uv) 2,j 1 + 1{u=v} (cid:32) 2 (X, ej ) W (v) (u) 1 + exp(g(X, (X, ej ) ej )) (u) d2(cid:88) 2g (v) α(uv) 2,j 1 + 1{u=v} u,v=1 (cid:33)(cid:33) (X, ej ) F1,j(Y X) + (cid:32) 1 β1,j + γ(u) 1 2 d2(cid:88) u=1 (cid:33) W (u) (X, ej ) 1 + exp(g(X, ej )) W (u) (X, ej ) W (v) (X, ej ) F2,j(Y X) d2(cid:88) u=1 1 γ(u) W (u) (X, ej )F3,j(Y X) + (cid:35) β2,jF4,j(Y X) , 1 4 + + + + + and and lim Tn,2,1(Y X) mnL1(Gn, G) := (cid:88) d2(cid:88) α(u) 1,j j[N ]:Cj =1 u=1 W (u) (X, ej ) 1 + exp(g(X, ej )) Hj(Y X), lim Tn,2,2(Y X) mnL1(Gn, G) := (cid:88) (cid:34) d2(cid:88) u=1 α(u) 1,j W (u) (X, ej ) 1 + exp(g(X, ej )) j[N ]:Cj >1 2g (v) (X, (u) d2(cid:88) + u,v=1 α(uv) 2,j 1 + 1{u=v} ej ) + W (u) 1 + exp(g(X, (X, ej ) W (v) ej )) (X, ej ) (cid:35) Hj(Y X), lim Tn,3(Y X) mnL1(Gn, G) (cid:88) := ξj[F0,j(Y X) Hj(Y X)]. j=1 It is worth noting that for almost every X, the set (cid:40) (cid:41) Fρ,j(Y X), Hj(Y X) : 0 ρ 4, [N ] 31 is linearly independent w.r.t . Therefore, it follows that the coefficients of those terms in the limit in equation equation (23) become zero. For [N ] such that Cj = 1, by considering the coefficients of F0,j(Y X), we have ξj + (cid:80)d2 u=1 α(u) 1,j W (X,W ej 1+exp(g(X,W ej (u) ) )) = 0 for almost surely X. Since the expert function is strongly identifiable, we deduce ξj = α(u) 1,j = 0 for all [d2]; F2,j(Y X), we have β1,j = 0. For [N ] such that Cj > 1, by considering the coefficients of F0,j(Y X), we have ξj + d2(cid:88) u=1 α(u) 1,j W (u) (X, ej ) 1 + exp(g(X, ej )) d2(cid:88) + u,v=1 α(uv) 2,j 1 + 1{u=v} 2g (v) W (u) (X, ej ) + W (u) 1 + exp(g(X, (X, ej ) W (v) ej )) (X, ej ) = 0 for almost surely X. Since the expert function is strongly identifiable, we deduce ξj = α(u) α(uv) 2,j = 0 for all u, [d2]; 1,j = F3,j(Y X), we have (cid:80)d2 u=1 function is strongly identifiable, we deduce γ(u) W (u) (X, 2 γ(u) 1 = 0 for all [d2]; ej ) = 0 for almost surely X. Since the expert F4,j(Y X), we have β2,j = 0. Putting the above results together, we have (ii) ξj = α(u) = 0 for all [N ] and u, [d2]. This contradicts to the fact that at least one among them is non-zero. Consequently, we achieve the local part in equation (20). Global part: Now, it suffices to demonstrate that 1,j = β1,j = α(uv) 2,j = β2,j = γ(u) inf GGN (Θ):L1(G,G)>ε EX [V (pG(X), pG(X))] L1(G, G) > 0, for some positive constant ε. Given the above result, it is sufficient to derive the global part in equation (21), that is, inf GGN (Θ):L1(G,G)>ε EX [V (pG(X), pG(X))]/L1(G, G) > 0. Assume by contrary that the global part does not hold true, then we can find sequence (cid:101)Gn GN (Θ) such that L1( (cid:101)Gn, G) > ε and EX [V (p (X), pG(X))] 0 as . Since Θ is compact set, we are able to replace (cid:101)Gn with its subsequence which converges to some mixing measure (cid:101)G GN (Θ). Recall that L1( (cid:101)Gn, G) > ε, then we also get that L1( (cid:101)G, G) > ε. (cid:101)Gn 32 On the other hand, by means of the Fatous lemma, we have 0 = lim EX [2V (p (cid:101)Gn (X), pG(X))] (cid:90) lim inf p (cid:101)Gn (Y X) pG(Y X)d(X, ), which follows that or equivalently L1( (cid:101)G, G) = 0. This contradicts to the fact that L1( (cid:101)G, G) > ε > 0. Hence, we reach the conclusion in equation (21), and the proof is completed. (cid:101)G(Y X)pG(Y X) = 0 for almost surely (X, ). Thus, we achieve that (cid:101)G G, L.2 Proof of Theorem K.1 As in Appendix L.1, we also start with establishing the local part lim ε0 inf GGN (Θ):L2(G,G)ε EX [V (pG(X), pG(X))] L2(G, G) > 0. (24) Assume by contrary that the local part is not true, then we can find sequence of mixing measures (Gn) given by Gn := (cid:80)N ) GN (Θ) such that L2(Gn, G) 0 and i=1 exp(cn )δ(an ,νn ,bn EX [V (pGn(X), pG(X))]/L2(Gn, G) 0, as . Recall that the Voronoi loss L2(Gn, G) is given by L2(Gn, G) := (cid:88) j=1 (cid:12) (cid:12) (cid:12) (cid:88) iCj exp(cn ) exp(c ) (cid:88) (cid:88) + j[N ]:Cj =1 iCj (cid:12) (cid:12) (cid:12) + (cid:88) (cid:88) exp(cn ) (cid:104) ei ej + νn (cid:105) ν j[N ]:Cj =1 (cid:104) exp(cn ) iCj ei ej 2 + νn ν 2(cid:105) . (25) Since L2(Gn, G) 0 as , we obtain (an ) (a Next, we divide the rest of this proof into three main steps. Step 1: Taylor expansion. )X + j=1 exp(log(1 + exp((a (cid:105) ))) (cid:104) (cid:80)N , νn , bn In this step, we aim to decompose the term Tn(Y X) := [pGn(Y X) pG(Y X)] can be decomposed as , j , ν ) for all [N ] and Cj. Tn(Y X) = (cid:88) (cid:88) j= iCj (cid:104) exp(cn ) exp(log(1 + exp((an )X + bn )))f (Y (an )X + bn , νn ) exp(log(1 + exp((a )X + )))f (Y (a )X + (cid:105) , ν ) + (cid:88) (cid:88) (cid:104) exp(cn ) exp(log(1 + exp((an )X + bn ))) exp(log(1 + exp((a )X + )))]pGn(Y X) j= iCj (cid:88) (cid:104) (cid:88) j=1 iCj exp(cn ) exp(c ) (cid:105) exp(log(1 + exp((a )X + )))[f (Y (a )X + j , ν ) pGn(Y X)] := Tn,1(Y X) Tn,2(Y X) + Tn,3(Y X). 33 Next, we continue to decompose the term Tn,1(Y X) as Tn,1(Y X) = (cid:88) (cid:88) (cid:104) exp(cn ) j[N ]:Cj =1 iCj exp(log(1 + exp((an )X + bn )))f (Y (an )X + bn , νn ) exp(log(1 + exp((a )X + )))f (Y (a )X + , ν ) (cid:105) + (cid:88) (cid:88) (cid:104) exp(cn ) j[N ]:Cj >1 iCj exp(log(1 + exp((an )X + bn )))f (Y (an )X + bn , νn ) exp(log(1 + exp((a )X + )))f (Y (a )X + , ν ) (cid:105) := Tn,1,1(Y X) + Tn,1,2(Y X). Let us denote Fρ(Y X; a, b, ν) := exp(log(1 + exp(aX + b))) ρf first-order Taylor expansion to the function F0(Y X; a, b, ν) around the point (a the term Tn,1,1(Y X) as gρ (Y aX + b, ν). By applying the ), we rewrite , , ν Tn,1,1(Y X) = (cid:88) 2 (cid:88) j[N ]:Cj =1 ρ=0 (j) n,1,1,ρ(X)Fρ(Y ; X, , , ν ) + Rn,1,1(Y X), where Rn,1,1(Y X) is the Taylor remainder such that Rn,1,1(Y X)/L2(Gn, G) 0 as , and (j) n,1,1,0(X) := (j) n,1,1,1(X) := (j) n,1,1,2(X) := (cid:88) iCj (cid:88) iCj (cid:88) iCj u=1(an 1 + exp((a ij)(u)X (u) + (bn ij) )X ) (cid:35) ij)(u)X (u) + (bn ij) (an , , (cid:80)d exp(cn ) (cid:34) (cid:88) exp(cn ) u=1 1 2 exp(cn )(νn ij), in which an ij := an , bn ij := bn b and νn ij := νn ν . Meanwhile, by means of the second-order Taylor expansion, the term Tn,1,2(Y X) can be represented as Tn,1,2(Y X) = (cid:88) 4 (cid:88) j[N ]:Cj >1 ρ= (j) n,1,2,ρ(X)Fρ(Y ; X, , , ν ) + Rn,1,2(Y X), 34 where Rn,1,2(Y X) is the Taylor remainder such that Rn,1,2(Y X)/L2(Gn, G) 0 as , and (j) n,1,2,0(X) := (j) n,1,2,1(X) := (cid:88) iCj (cid:88) iCj (cid:34) (cid:80)d exp(cn ) u=1(an 1 + exp((a ij)(u)X (u) + (bn ij) )X ) + (cid:80)d u=1(an ij)(u)(bn 1 + exp((a + (cid:34) (cid:88) exp(cn ) (an ij)(u)X (u) + (bn ij) + u=1 + (bn ij)2 + 2 (cid:80)d u=1(an 1 + exp((a (j) n,1,2,2(X) := (cid:34) exp(cn ) 1 (cid:88) iCj (νn ij) + (cid:88) u,v= (an ij)(u)(an 1 + 1{u=v} (cid:88) (an ij)(u)(bn ij)X (u) + + 1 2 (cid:80)d u=1(an ij)(u)(νn 1 + exp((a (cid:80)d u,v=1 (an ij )(u)(an 1+1{u=v} ij )(v) (u)X (v) 1 + exp((a )X ) (cid:35) ij)2 ij)X (u) + 1 2 (bn )X ) (an 2 (cid:80)d u,v=1 1 + exp((a ij)X (u) ij)(u)(bn )X ) ij)(v)X (u)X (v) , , ij )(u)(an 1+1{u=v} ij )(v) (u)X (v) )X ) (cid:35) + 1 2 (bn ij) ij)(νn ij) (cid:35) , ij)X (u) + (bn )X ) (cid:35) , ij)(νn ij) 1 (an ij)(u)(νn ij)X (u) + 1 2 (bn u= (j) n,1,2,3(X) := (j) n,1,2,4(X) := (cid:88) iCj (cid:88) iCj exp(cn ) exp(cn ) (cid:34) (cid:88) u=1 1 4 (νn ij)2. Next, we decompose the term Tn,2(Y X) as Tn,2(Y X) (cid:88) := (cid:88) (cid:104) exp(cn ) exp(log(1 + exp((an )X + bn ))) exp(log(1 + exp((a )X + )))]pGn(Y X) j[N ]:Cj =1 (cid:88) iCj (cid:88) exp(cn ) (cid:104) + exp(log(1 + exp((an )X + bn ))) exp(log(1 + exp((a )X + )))]pGn(Y X) j[N ]:Cj >1 iCj := Tn,2,1(Y X) + Tn,2,2(Y X). Note that we can rewrite the term Tn,1,2(Y X) using the first-order Taylor expansion to the function exp(log(1 + exp((an ))) around the point (a )X + bn ) as , Tn,2,1(Y X) = (cid:88) (cid:88) exp(cn ) j[N ]:Cj =1 iCj (cid:80)d u=1(an 1 + exp((a ij)(u)X (u) + (bn ij) )X ) Hn(Y X; , ) +Rn,2,1(Y X), where we denote Hn(Y X; a, b) = exp(log(1 + exp(aX + b)))pGn(Y X) and Rn,2,1(Y X) is the Taylor remainder such that Rn,2,1(Y X)/L2(Gn, G) 0 as . 35 On the other hand, by means of the second-order Taylor expansion, we have Tn,2,2(Y X) = (cid:88) (an j[N ]:Cj >1 ij )(u)(an ij )(v) 1+1{u=v} (cid:80)d u,v=1 + 1 + exp((a )X ) exp(cn ) (cid:88) iCj (u)X (v) (cid:34) (cid:80)d u=1(an 1 + exp((a ij)(u)X (u) + (bn ij) )X ) + (cid:80)d u=1(an ij)(u)(bn 1 + exp((a ij)X (u) + 1 2 (bn )X ) ij)2 (cid:35) Hn(Y X; ej ) where Rn,2,1(Y X) is the Taylor remainder such that Rn,2,2(Y X)/L2(Gn, G) 0 as . From the above equation, [Tn,1,1(Y X)Rn,1,1(Y X)], [Tn,1,2(Y X)Rn,1,2(Y X)], [Tn,2,1(Y X) Rn,2,1(Y X)], [Tn,2,2(Y X) Rn,2,2(Y X)] and [Tn,3(Y X)] can be seen as combination of elements of the set := (cid:83)N ρ=0 Sρ,j, where we define (cid:83)5 j=1 + Rn,2,2(Y X), S0,j := (cid:40) (cid:40) (u) 1 + exp((a )X ) F0,j(Y X), (u)X (v) 1 + exp((a )X ) F0,j(Y X), 1 )X 1 + exp((a ) F0,j(Y X), F0,j(Y X) : 1 u, (cid:41) , S1,j := F1,j(Y X), (u)F1,j(Y X), (u) 1 + exp((a )X ) F1,j(Y X), (u)X (v) 1 + exp((a )X ) F1,j(Y X), 1 )X 1 + exp((a ) F1,j(Y X) : 1 u, (cid:41) , (cid:40) S2,j := F2,j(Y X), (u)F2,j(Y X), (u)X (v)F2,j(Y X), (u) 1 + exp((a )X ) F2,j(Y X), 1 )X 1 + exp((a ) F2,j(Y X) : 1 u, (cid:41) , (cid:40) S3,j := F3,j(Y X), (u)F3,j(Y X) : 1 (cid:41) , (cid:40) S4,j := F4,j(Y X) (cid:41) , (cid:40) S5,j := (u) 1 + exp((a )X ) Hn,j(Y X), (u)X (v) 1 + exp((a )X ) Hn,j(Y X), 1 )X 1 + exp((a ) Hn,j(Y X), Hn,j(Y X) : 1 u, (cid:41) . In this step, we will show that at least one among Step 2: Non-vanishing coefficients. the coefficients in the representations of [Tn,1,1(Y X) Rn,1,1(Y X)]/L2(Gn, G), [Tn,1,2(Y X) Rn,1,2(Y X)]/L2(Gn, G), [Tn,2,1(Y X)Rn,2,1(Y X)]/L2(Gn, G), [Tn,2,2(Y X)Rn,2,2(Y X)]/L2(Gn, G) and [Tn,3(Y X)]/L2(Gn, G) does not approach zero when goes to infinity. Assume by contrary that all of them vanish as . Then, by considering the coefficients of the term 36 F0,j(Y X) for [N ], we have"
        },
        {
            "title": "1\nL2(Gn, G∗)",
            "content": "(cid:88) j=1 (cid:12) (cid:12) (cid:12) (cid:88) iCj exp(cn ) exp(c ) (cid:12) (cid:12) (cid:12) 0. (u) 1+exp((a )Xb ) F0,j(Y X) for [N ] : Cj = 1, we have"
        },
        {
            "title": "1\nL2(Gn, G∗)",
            "content": "(cid:88) (cid:88) j[N ]:Cj =1 iCj exp(cn )an ij 0. 1 )Xb 1+exp((a ) F0,j(Y X) for [N ] : Cj = 1, we have"
        },
        {
            "title": "1\nL2(Gn, G∗)",
            "content": "(cid:88) (cid:88) j[N ]:Cj =1 iCj F2,j(Y X) for [N ] : Cj = 1, we have"
        },
        {
            "title": "1\nL2(Gn, G∗)",
            "content": "(cid:88) (cid:88) j[N ]:Cj =1 iCj exp(cn )bn ij 0. exp(cn )νn ij 0. (u)X (v) 1+exp((a )Xb )) F0,j(Y X) for [N ] : Cj > 1, we have"
        },
        {
            "title": "1\nL2(Gn, G∗)",
            "content": "(cid:88) (cid:88) j[N ]:Cj >1 iCj exp(cn )an 2 0. 1 )Xb 1+exp((a )) F1,j(Y X) for [N ] : Cj > 1, we have"
        },
        {
            "title": "1\nL2(Gn, G∗)",
            "content": "(cid:88) (cid:88) j[N ]:Cj >1 iCj F4,j(Y X) for [N ] : Cj > 1, we have"
        },
        {
            "title": "1\nL2(Gn, G∗)",
            "content": "(cid:88) (cid:88) j[N ]:Cj =1 iCj exp(cn )bn 2 0. exp(cn )νn ij2 0. By taking the sum of the above limits, we obtain 1 = L2(Gn,G) L2(Gn,G) 0 as , which is contradiction. Thus, not all the coefficients in the representations of [Tn,1,1(Y X) Rn,1,1(Y X)]/L2(Gn, G), [Tn,1,2(Y X) Rn,1,2(Y X)]/L2(Gn, G), [Tn,2,1(Y X) Rn,2,1(Y X)]/L2(Gn, G), [Tn,2,2(Y X) Rn,2,2(Y X)]/L2(Gn, G) and [Tn,3(Y X)]/L2(Gn, G) converge to zero as . Stage 3 - Fatous argument: In this stage, we use the Fatous lemma to show contradiction to the result of Step 2. For that purpose, let us denote mn as the maximum of the absolute values of the coefficients in the representations of [Tn,1,1(Y X) Rn,1,1(Y X)]/L2(Gn, G), 37 [Tn,1,2(Y X) Rn,1,2(Y X)]/L2(Gn, G), [Tn,2,1(Y X) Rn,2,1(Y X)]/L2(Gn, G), [Tn,2,2(Y X) Rn,2,2(Y X)]/L2(Gn, G) and [Tn,3(Y X)]/L2(Gn, G). It follows from the result of Step 2 that 1/mn as . In addition, we also denote (cid:80) iCj ij)(u) exp(cn )(an mnL2(Gn, G) ij)(u)(an ij)(v) (cid:80) iCj exp(cn )(an mnL2(Gn, G) α(u) 1,j , α(uv) 2,j , (cid:80) iCj (cid:80) exp(cn exp(cn )(bn ij) iCj mnL2(Gn, G) )(an mnL2(Gn, G) )(bn ij)(u)(νn ij) ij)(νn ij) exp(cn mnL2(Gn, G) ϕ(u) 1,j , γ(u) 1,j , γ3,j, (cid:80) iCj β1,j, β2,j, ϕ2,j, (cid:80) (cid:80) (cid:80) (cid:80) (cid:80) exp(cn )(νn ij) iCj mnL2(Gn, G) exp(cn ij) ij)2 )(νn iCj mnL2(Gn, G) )(bn exp(cn iCj mnL2(Gn, G) exp(cn )(an mnL2(Gn, G) ) exp(c exp(cn ) iCj iCj mnL2(Gn, G) ξj, ij)(u)(bn ij) γ(u) 2,j , as for any [N ] and u, [d2] with note that at least one among α(u) ϕ1,j, ϕ2,j, γ(u) 1,j , γ3,j and ξj is non-zero. , γ(u) 2,j 1,j , β1,j, α(uv) 2,j , β2,j, By applying the Fatous lemma, we have 0 = lim EX [V (pG(X), pG(X))] mnL2(Gn, G) = (cid:90) 1 2 lim inf pGn(Y X) pG(Y X) mnL2(Gn, G) d(X, ), which implies that [pGn(Y X) pG(Y X)]/[mnL2(Gn, G)] 0 as for almost surely (X, ). Since the term (cid:80)N 0 as . Then, it follows that ))) is bounded, we also have Tn(Y X)/[mnL2(Gn, G)] j=1 exp(log(1+exp((a )X+b 0 = lim Tn,1,1(Y X) + Tn,1,2(Y X) mnL2(Gn, G) lim Tn,2,1(Y X) + Tn,2,2(Y X) mnL2(Gn, G) + lim 38 Tn,3(Y X) mnL2(Gn, G) , (26) for almost surely (X, ) Y, where we have lim Tn,1,1(Y X) mnL2(Gn, G) := (cid:88) j[N ]:Cj =1 (cid:34) (cid:80)d u=1 α(u) 1 + exp((a 1,j (u) + ϕ1,j )X ) F0,j(Y X) (cid:35) (cid:16) (cid:88) + u=1 α(u) 1,j (u) + ϕ1,j (cid:17) lim Tn,1,2(Y X) mnL2(Gn, G) := (cid:88) j[N ]:Cj >1 β1,jF2,j(Y X) , F1,j(Y X) + 1 2 u=1 α(u) 1 + exp((a (cid:34)(cid:32) (cid:80)d 1,j (u) + ϕ1,j )X ) (cid:80)d u,v=1 α(uv) 2,j 1+1{u=v} (u)X (v) 1 + exp((a )X ) + (cid:80)d u=1 γ(u) 1 + exp((a 2,j (u) + 2 ϕ2,j )X ) (cid:33) F0,j(Y X) (cid:32) (cid:88) α(u) 1,j (u) + ϕ1,j + 2 (cid:80)d u,v=1 α(uv) 2,j 1+1{u=v} (u)X (v) 1 + exp((a u=1 ϕ2,j + 2 (cid:80)d 1 + exp((a (cid:88) γ(u) 2,j (u) + (cid:33) u=1 γ(u) 2,j (u) )X ) (cid:80)d u=1 γ(u) 1 + exp((a 1 2 F1,j(Y X) + )X ) (cid:32) 1 2 β1,j + (cid:33) 1,j (u) + γ3,j )X ) F2,j(Y X) γ(u) 1,j (u) + (cid:17) γ3,j 1 2 F3,j(Y X) + 1 4 β2,jF4,j(Y X) (cid:35) , u=1 (cid:16) (cid:88) u=1 1 (cid:88) u,v=1 α(uv) 2,j (u)X (v) 1 + 1{u=v} + 1 2 ϕ2,j lim Tn,2,1(Y X) mnL2(Gn, G) := (cid:88) j[N ]:Cj =1 lim Tn,2,2(Y X) mnL2(Gn, G) := (cid:88) j[N ]:Cj >1 (cid:80)d u=1 α(u) 1 + exp((a 1,j (u) + ϕ1,j )X ) Hj(Y X), (cid:34) (cid:80)d u=1 α(u) 1 + exp((a 1,j (u) + ϕ1,j )X ) (cid:80)d u,v=1 α(uv) 2,j 1+1{u=v} (u)X (v) 1 + exp((a )X ) + + (cid:80)d u=1 γ(u) 1 + exp((a 2,j (u) + 1 2 ϕ2,j )X ) (cid:35) Hj(Y X), + + + + + and and lim Tn,3(Y X) mnL2(Gn, G) (cid:88) := ξj[F0,j(Y X) Hj(Y X)]. j=1 It is worth noting that for almost every X, the set (cid:40) (cid:41) Fρ,j(Y X), Hj(Y X) : 0 ρ 4, [N ] is linearly independent w.r.t . Therefore, it follows that the coefficients of those terms in the limit in equation equation (26) become zero. For [N ] such that Cj = 1, by considering the coefficients of F1,j(Y X), we have (cid:80)d ϕ1,j = 0 for all [d]; u=1 α(u) 1,j (u) + ϕ1,j = 0 for almost surely X, indicating that α(u) 1,j = F0,j(Y X), we have ξj + (cid:80)d u=1 α(u) 1,j (u) 1+exp((a )Xb ) + ϕ1,j 1+exp((a )Xb ) = 0 for almost surely X. Since α(u) 1,j = ϕ1,j = 0 for all [d], we also get ξj = 0. F2,j(Y X), we have β1,j = 0. For [N ] such that Cj > 1, by considering the coefficients of F1,j(Y X), we have (cid:88) u=1 α(u) 1,j (u) + ϕ1,j + 2 (cid:80)d u,v=1 α(uv) 2,j 1+1{u=v} (u)X (v) 1 + exp((a )X ) + ϕ2,j + 2 (cid:80)d 1 + exp((a u=1 γ(u) 2,j (u) )X ) = 0, for almost surely X. Since the set (cid:40) 1, (u), 1 )X 1 + exp((a ) (u)X (v) , (u) 1 + exp((a , )X ) (cid:41) : u, [d] 1 + exp((a )X ) is linearly independent w.r.t X, we deduce α(u) u, [d]. 1,j = ϕ1,j = α(uv) 2,j = ϕ2,j = γ(u) 2,j = 0 for all F0,j(Y X), we have ξj + (cid:80)d u=1 α(u) 1 + exp((a 1,j (u) + ϕ1,j )X ) (cid:80)d u,v=1 + α(uv) 2,j 1+1{u=v} (u)X (v) 1 + exp((a )X ) + (cid:80)d u=1 γ(u) 1 + exp((a 2,j (u) + 2 ϕ2,j )X ) = 0, for almost surely X. Since α(u) ξj = 0. 1,j = ϕ1,j = α(uv) 2,j = ϕ2,j = γ(u) 2,j = 0 for all u, [d], we get F3,j(Y X), we have (cid:80)d γ3,j = 0 for all [d]; u=1 F2,j(Y X), we have 1 2 γ(u) 1,j (u) + 2 γ3,j = 0 for almost surely X, indicating that γ(u) 1,j = 1 2 β1,j + (cid:88) u,v= α(uv) 2,j (u)X (v) 1 + 1{u=v} + 1 2 ϕ2,j + (cid:88) u= γ(u) 2,j (u) + 1 2 40 (cid:80)d u=1 γ(u) 1 + exp((a 1,j (u) + γ3,j )X ) = 0, for almost surely X. Since α(uv) β1,j = 0. 2,j = ϕ2,j = γ(u) 2,j = γ(u) 1,j = γ3,j = 0 for all u, [d], we also get F4,j(Y X), we have β2,j = 0. Putting the above results together, we have ξj = α(u) 1,j = γ(u) 2,j = γ3,j = 0 for all [N ] and u, [d]. This contradicts the fact that at least one among them is different from zero. Consequently, we achieve the local part in equation (24). 1,j = ϕ1,j = β1,j = α(uv) 2,j = ϕ2,j = β2,j = γ(u) L.3 Proof of Proposition 3.1 In this proof, we first present some fundamental results on the density estimation problem for M-estimators in [90] in Appendix L.3.1, and then provide the main proof in Appendix L.3.2. L.3.1 Preliminaries To streamline our discussion, let us introduce some necessary concepts from the empirical process theory. In particular, let Pk(Θ) be the set of all conditional densities with respect to mixing measures in GN (Θ), i.e. Additionally, we also consider two following variants of the set PN (Θ): PN (Θ) := {pG(Y X) : GN (Θ)}. k(Θ) := {p(G+G)/2(Y X) : GN (Θ)}, 1/2 (Θ) := {p1/2 (G+G)/2(Y X) : GN (Θ)}. Next, we define for each δ > 0 Hellinger ball centered around the true conditional density pG(Y X) and intersect with the set 1/2 (Θ) as below 1/2 (Θ, δ) := {p1/2(Y X) 1/2 (Θ) : h(pG, pG) δ}. Moreover, the size of this Hellinger ball is quantified by the following term: JB(δ, 1/2 (Θ, δ)) := (cid:90) δ δ2/213 1/2 (t, 1/2 (Θ, t), 2)dt δ, (27) 1/2 (Θ, t), 2) stands for the bracketing entropy of where HB(t, and δ := max{t, δ}. Now, we are ready to recall the results in [90]. 1/2 (Θ, t) under the L2-norm, Lemma L.1 (Theorem 7.4,[90]). Take Ψ(δ) JB(δ, increasing function of δ. Then, for universal constant and nδ2 1/2 (Θ, δ)) such that Ψ(δ)/δ2 is nonn cΨ(δn), we achieve that (cid:16) EX [h(p (cid:98)Gn (X), pG(X))] > δ (cid:17) exp(nδ2/c2), for any δ δn. Proof of Lemma L.1 is available in [90]. Apart from this result, we also need to introduce the upper bounds of the covering number (ε, PN (Θ), ) and the bracketing entropy HB(ε, PN (Θ), 2) as follows: Lemma L.2. Suppose that Θ is bounded set, then we have for any ε (0, 1/2) that (a) log (ε, PN (Θ), ) log(1/ε); (b) HB(ε, PN (Θ), 2) log(1/ε). Proof of Lemma L.2. Part (a). Recall that Θ is compact set, then there exists an ε-cover, which we denote as Θε. Moreover, it can be verified that Θε O(ε(d2+1)N ). Next, for each mixing measure = (cid:80)N , where (W ei, νi) Θε is the closest point to (Wei, νi) in this set for any [N ]. Subsequently, we demonstrate that the set i=1 δ(Wei ,νi) GN (Θ), we consider another one = (cid:80)N i=1 δ(W ei ,νi) (cid:110) := pG(Y X) : (W ei, νi) Θε, [N ] (cid:111) is an ε-cover of the metric space (PN (Θ), ). In other words, we need to show that for any pG(Y X) PN (Θ), there exists some density pG(Y X) such that pG pG ε. Next, we decompose the term Tn(Y X) := j=1 exp(log(1 + exp(g(X, ej )))) [pG(Y X) (cid:104) (cid:80)N (cid:105) pG(Y X)] as Tn(Y X) = (cid:88) i=1 (cid:104) exp(log(1 + exp(g(X, Wei)))) (cid:105) (Y g(X, Wei), νi) (Y g(X, ei), νi) (cid:88) (cid:104) + i=1 (cid:105) exp(log(1 + exp(g(X, Wei)))) exp(log(1 + exp(g(X, ej )))) (cid:104) (Y g(X, ei), νi) pG(Y X) (cid:105) . As Θ and are bounded, we may assume that exp(log(1+exp(g(X, Wei)))) B1 and (Y g(X, ei), νi) pG(Y X) B2 for some positive constants B1, B2. Thus, we obtain that Tn(Y X) (cid:88) i= B1 (cid:104) (cid:105) Wei ei + νi νi + (cid:88) i= B2 Wei ei ε. Additionally, since the term (cid:80)K for almost surely (X, ), or equivalently, j=1 exp(g(X, ej )) is bounded, we obtain pG(Y X) pG(Y X) ε pG pG = sup (X,Y )X pG(Y X) pG(Y X) ε. This result indicates that is an ε-cover of the metric space (PN (Θ), ). Therefore, we get or equivalently, (ε, PN (Θ), ) Θε O(ε(d2+1)N ), log (ε, PN (Θ), ) Θε log(1/ε). 42 Part (b). Firstly, we will derive an upper bound for the Gaussian experts (Y g(X, We), ν). Since Θ is compact set, we have g(X, We) M1 and M2 ν M3 for any and (We, ν) Θ. Then, it follows that (Y g(X, We), ν) B(Y X), where B(Y X) := 1 2πM2 1 2πM exp(Y 2/(8M 2 3 )), for 2M1 , for < 2M1, for any . Next, let η ε be some positive constant that we choose later, then we denote {π1, π2, . . . , πN } as an η-cover over PN (Θ). Based on this cover, we build the following brackets Li(Y X) := max{πi(Y X) η, 0} and Ui(Y X) := max{πi(Y X) + η, B(Y X)}, for any [N ]. We can validate that PN (Y X) (cid:83)N i=1[Li(Y X), Ui(Y X)] and Ui(X, ) Li(X, ) min{2η, B(Y X)}. As result, we have Ui Li2 = (cid:16) (cid:90) [Ui(Y X) Li(Y X)]2d(X, ) (cid:17)1/2 2η."
        },
        {
            "title": "The above result implies that",
            "content": "HB(2η, PN (Θ), 2) log (η, PN (Θ), ) log(1/η). Then, by setting η = ε/2, we arrive at HB(ε, PN (Θ), 1) log(1/ε). Hence, the proof is completed. L.3.2 Main Proof"
        },
        {
            "title": "Since P",
            "content": "1/2 (Θ, t) 1/2 (Θ) for any > 0, we have 1/2 1/2 (Θ), 2) = HB(t/ (Θ, t), 2) HB(t, HB(t, 2, (Θ), h), (28) where the last equality is due to the relationship between the Hellinger distance and the L2-norm. Note that for any two mixing measure and G, Lemma 4.2 in [90] indicates that pG + h2(cid:16) 1 2 1 2 2, (Θ), h) HB(t, Fk1,k2(Θ), h). This result together with equation equah2(pG, pG), pG + pG, pG 1 2 1 2 1 2 (cid:17) which yields HB(t/ tion (28) implies that From equation (27) and part (b) of Lemma L.2, we have that HB(t, 1/2 (Θ, t), 2) HB(t, PN (Θ), h). JB(δ, 1/2 (Θ, δ)) = (cid:90) δ δ2/ (cid:90) δ δ2/213 (cid:90) δ δ2/213 1/2 (t, 1/2 (Θ, t), 2)dt δ 1/2 (t, 1/2 (Θ, t), h)dt δ log(1/t)dt δ. Next, let Ψ(δ) = δ(cid:112)log(1/δ), then it can be verified that Ψ(δ)/δ2 is non-increasing function of δ. Furthermore, the above result indicates that Ψ(δ) JB(δ, (cid:101)F 1/2 (Θ, δ), 2). By considering k1,k2 the sequence (δn) defined as δn := (cid:112)log(n)/n, we have cΨ(δn) for some universal constant > 0. It follows from Lemma L.1 that (cid:16) nδ2 (cid:17) (X), pG(X))] > C(cid:112)log(n)/n exp(c log(n)), EX [h(p (cid:98)Gn for some universal constant > 0 depending only on Θ. Since the Total Variation distance is upper bounded by the Hellinger distance, we deduce (cid:16) EX [V (p (cid:98)Gn (X), pG(X))] > C(cid:112)log(n)/n (cid:17) exp(c log(n)), or equivalently, EX [V (p (cid:98)Gn (X), pG(X))] = OP ((cid:112)log(n)/n). Hence, the proof is completed."
        },
        {
            "title": "References",
            "content": "[1] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. H. Awadalla, N. Bach, A. Bahree, A. Bakhtiari, H. S. Behl, A. Benhaim, M. Bilenko, J. Bjorck, S. Bubeck, M. Cai, C. C. T. Mendes, W. Chen, V. Chaudhary, P. Chopra, A. D. Giorno, G. de Rosa, M. Dixon, R. Eldan, D. Iter, A. Goswami, S. Gunasekar, E. Haider, J. Hao, R. J. Hewett, J. Huynh, M. Javaheripi, X. Jin, P. Kauffmann, N. Karampatziakis, D. Kim, Y. J. Kim, M. Khademi, L. Kurilenko, J. R. Lee, Y. T. Lee, Y. Li, C. Liang, W. Liu, E. Lin, Z. Lin, P. Madan, A. Mitra, H. Modi, A. Nguyen, B. Norick, B. Patra, D. Perez-Becker, T. Portet, R. Pryzant, H. Qin, M. Radmilac, C. Rosset, S. Roy, O. Saarikivi, A. Saied, A. Salim, M. Santacroce, S. Shah, N. Shang, H. Sharma, X. Song, O. Ruwase, P. Vaddamanu, X. Wang, R. Ward, G. Wang, P. Witte, M. Wyatt, C. Xu, J. Xu, S. Yadav, F. Yang, Z. Yang, D. Yu, C.-Y. Zhang, C. Zhang, J. Zhang, L. L. Zhang, Y. Zhang, Y. Zhang, and X. Zhou. Phi-3 technical report: highly capable language model locally on your phone. ArXiv, abs/2404.14219, 2024. [2] A. G. ALIAS PARTH GOYAL, A. Didolkar, N. R. Ke, C. Blundell, P. Beaudoin, N. Heess, M. C. Mozer, and Y. Bengio. Neural production systems. Advances in Neural Information Processing Systems, 34:2567325687, 2021. [3] P. Andersen, G. N. Gross, T. Lomo, and O. Sveen. Participation of inhibitory and excitatory interneurones in the control of hippocampal cortical output. In UCLA forum in medical sciences, volume 11, pages 415465, 1969. [4] H. Bao, L. Dong, S. Piao, and F. Wei. BEiT: BERT Pre-Training of Image Transformers. In International Conference on Learning Representations, 2022. [5] H. Bao, W. Wang, L. Dong, Q. Liu, O. K. Mohammed, K. Aggarwal, S. Som, S. Piao, and F. Wei. VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022. 44 [6] Y. Bengio. Deep Learning of Representations: Looking Forward. In A.-H. Dediu, C. MartínVide, R. Mitkov, and B. Truthe, editors, Statistical Language and Speech Processing, pages 137, Berlin, Heidelberg, 2013. Springer Berlin Heidelberg. [7] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. Piqa: Reasoning about physical commonsense in natural language, 2019. [8] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [9] L. Chen, J. Li, X. wen Dong, P. Zhang, Y. Zang, Z. Chen, H. Duan, J. Wang, Y. Qiao, D. Lin, and F. Zhao. Are we on the right way for evaluating large vision-language models? ArXiv, abs/2403.20330, 2024. [10] T. Chen, Z. Zhang, A. K. JAISWAL, S. Liu, and Z. Wang. Sparse moe as the new dropout: Scaling dense and self-slimmable transformers. In The Eleventh International Conference on Learning Representations, 2023. [11] Z. Chi, L. Dong, S. Huang, D. Dai, S. Ma, B. Patra, S. Singhal, P. Bajaj, X. Song, X.-L. Mao, H. Huang, and F. Wei. On the Representation Collapse of Sparse Mixture of Experts. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022. [12] Z. Chi, L. Dong, S. Huang, D. Dai, S. Ma, B. Patra, S. Singhal, P. Bajaj, X. Song, X.-L. Mao, H. Huang, and F. Wei. On the representation collapse of sparse mixture of experts, 2022. [13] Y. Chow, A. Tulepbergenov, O. Nachum, D. Gupta, M. Ryu, M. Ghavamzadeh, and C. Boutilier. Mixture-of-Expert Approach to RL-based Dialogue Management. In The Eleventh International Conference on Learning Representations, 2023. [14] A. Clark, D. De Las Casas, A. Guy, A. Mensch, M. Paganini, J. Hoffmann, B. Damoc, B. Hechtman, T. Cai, S. Borgeaud, G. B. Van Den Driessche, E. Rutherford, T. Hennigan, M. J. Johnson, A. Cassirer, C. Jones, E. Buchatskaya, D. Budden, L. Sifre, S. Osindero, O. Vinyals, M. Ranzato, J. Rae, E. Elsen, K. Kavukcuoglu, and K. Simonyan. Unified Scaling Laws for Routed Language Models. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 40574086. PMLR, July 2022. [15] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge. Preprint arXiv:1803.05457, 2018. [16] R. Csordás, K. Irie, J. Schmidhuber, C. Potts, and C. D. Manning. Moeut: Mixture-of-experts universal transformers, 2024. [17] D. Dai, C. Deng, C. Zhao, R. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y. Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066, 2024. 45 [18] D. Dai, L. Dong, S. Ma, B. Zheng, Z. Sui, B. Chang, and F. Wei. Stablemoe: Stable routing strategy for mixture of experts. arXiv preprint arXiv:2204.08396, 2022. [19] D. Dai, L. Dong, S. Ma, B. Zheng, Z. Sui, B. Chang, and F. Wei. StableMoE: Stable Routing Strategy for Mixture of Experts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 70857095, Dublin, Ireland, May 2022. Association for Computational Linguistics. [20] DeepSeek-AI, A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C. Dengr, C. Ruan, D. Dai, D. Guo, D. Yang, D. Chen, D. Ji, E. Li, F. Lin, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Xu, H. Yang, H. Zhang, H. Ding, H. Xin, H. Gao, H. Li, H. Qu, J. L. Cai, J. Liang, J. Guo, J. Ni, J. Li, J. Chen, J. Yuan, J. Qiu, J. Song, K. Dong, K. Gao, K. Guan, L. Wang, L. Zhang, L. Xu, L. Xia, L. Zhao, L. Zhang, M. Li, M. Wang, M. Zhang, M. Zhang, M. Tang, M. Li, N. Tian, P. Huang, P. Wang, P. Zhang, Q. Zhu, Q. Chen, Q. Du, R. J. Chen, R. L. Jin, R. Ge, R. Pan, R. Xu, R. Chen, S. S. Li, S. Lu, S. Zhou, S. Chen, S. Wu, S. Ye, S. Ma, S. Wang, S. Zhou, S. Yu, S. Zhou, S. Zheng, T. Wang, T. Pei, T. Yuan, T. Sun, W. L. Xiao, W. Zeng, W. An, W. Liu, W. Liang, W. Gao, W. Zhang, X. Q. Li, X. Jin, X. Wang, X. Bi, X. Liu, X. Wang, X. Shen, X. Chen, X. Chen, X. Nie, X. Sun, X. Wang, X. Liu, X. Xie, X. Yu, X. Song, X. Zhou, X. Yang, X. Lu, X. Su, Y. Wu, Y. K. Li, Y. X. Wei, Y. X. Zhu, Y. Xu, Y. Huang, Y. Li, Y. Zhao, Y. Sun, Y. Li, Y. Wang, Y. Zheng, Y. Zhang, Y. Xiong, Y. Zhao, Y. He, Y. Tang, Y. Piao, Y. Dong, Y. Tan, Y. Liu, Y. Wang, Y. Guo, Y. Zhu, Y. Wang, Y. Zou, Y. Zha, Y. Ma, Y. Yan, Y. You, Y. Liu, Z. Z. Ren, Z. Ren, Z. Sha, Z. Fu, Z. Huang, Z. Zhang, Z. Xie, Z. Hao, Z. Shao, Z. Wen, Z. Xu, Z. Zhang, Z. Li, Z. Wang, Z. Gu, Z. Li, and Z. Xie. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model, 2024. [21] DeepSeek-AI, A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Guo, D. Yang, D. Chen, D. Ji, E. Li, F. Lin, F. Dai, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Zhang, H. Ding, H. Xin, H. Gao, H. Li, H. Qu, J. L. Cai, J. Liang, J. Guo, J. Ni, J. Li, J. Wang, J. Chen, J. Chen, J. Yuan, J. Qiu, J. Li, J. Song, K. Dong, K. Hu, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Xu, L. Xia, L. Zhao, L. Wang, L. Zhang, M. Li, M. Wang, M. Zhang, M. Zhang, M. Tang, M. Li, N. Tian, P. Huang, P. Wang, P. Zhang, Q. Wang, Q. Zhu, Q. Chen, Q. Du, R. J. Chen, R. L. Jin, R. Ge, R. Zhang, R. Pan, R. Wang, R. Xu, R. Zhang, R. Chen, S. S. Li, S. Lu, S. Zhou, S. Chen, S. Wu, S. Ye, S. Ye, S. Ma, S. Wang, S. Zhou, S. Yu, S. Zhou, S. Pan, T. Wang, T. Yun, T. Pei, T. Sun, W. L. Xiao, W. Zeng, W. Zhao, W. An, W. Liu, W. Liang, W. Gao, W. Yu, W. Zhang, X. Q. Li, X. Jin, X. Wang, X. Bi, X. Liu, X. Wang, X. Shen, X. Chen, X. Zhang, X. Chen, X. Nie, X. Sun, X. Wang, X. Cheng, X. Liu, X. Xie, X. Liu, X. Yu, X. Song, X. Shan, X. Zhou, X. Yang, X. Li, X. Su, X. Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Y. Zhang, Y. Xu, Y. Xu, Y. Huang, Y. Li, Y. Zhao, Y. Sun, Y. Li, Y. Wang, Y. Yu, Y. Zheng, Y. Zhang, Y. Shi, Y. Xiong, Y. He, Y. Tang, Y. Piao, Y. Wang, Y. Tan, Y. Ma, Y. Liu, Y. Guo, Y. Wu, Y. Ou, Y. Zhu, Y. Wang, Y. Gong, Y. Zou, Y. He, Y. Zha, Y. Xiong, Y. Ma, Y. Yan, Y. Luo, Y. You, Y. Liu, Y. Zhou, Z. F. Wu, Z. Z. Ren, Z. Ren, Z. Sha, Z. Fu, Z. Xu, Z. Huang, Z. Zhang, Z. Xie, Z. Zhang, Z. Hao, Z. Gou, Z. Ma, Z. Yan, Z. Shao, Z. Xu, Z. Wu, Z. Zhang, Z. Li, Z. Gu, Z. Zhu, Z. Liu, Z. Li, Z. Xie, Z. Song, Z. Gao, and Z. Pan. Deepseek-v3 technical report, 2025. [22] G. Do, K. Le, Q. Pham, T. Nguyen, T.-N. Doan, B. T. Nguyen, C. Liu, S. Ramasamy, X. Li, 46 and S. Hoi. Hyperrouter: Towards efficient training and inference of sparse mixture of experts. arXiv preprint arXiv:2312.07035, 2023. [23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations, 2021. [24] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, B. Zoph, L. Fedus, M. P. Bosma, Z. Zhou, T. Wang, E. Wang, K. Webster, M. Pellat, K. Robinson, K. Meier-Hellstern, T. Duke, L. Dixon, K. Zhang, Q. Le, Y. Wu, Z. Chen, and C. Cui. GLaM: Efficient Scaling of Language Models with Mixture-of-Experts. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 55475569. PMLR, July 2022. [25] J. C. Eccles. The cerebellum as neuronal machine. Springer Science & Business Media, 2013. [26] W. Fedus, B. Zoph, and N. Shazeer. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. Journal of Machine Learning Research, 23(120):139, 2022. [27] J. A. Feldman and D. H. Ballard. Connectionist models and their properties. Cognitive science, 6(3):205254, 1982. [28] I. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks. In International conference on machine learning, pages 13191327. PMLR, 2013. [29] A. Goyal, A. Lamb, J. Hoffmann, S. Sodhani, S. Levine, Y. Bengio, and B. Schölkopf. Recurrent Independent Mechanisms. In International Conference on Learning Representations, 2021. [30] S. Grossberg and S. Grossberg. Contour enhancement, short term memory, and constancies in reverberating neural networks. Studies of Mind and Brain: Neural Principles of Learning, Perception, Development, Cognition, and Motor Control, pages 332378, 1982. [31] T. Guan, F. Liu, X. Wu, R. Xian, Z. Li, X. Liu, X. Wang, L. Chen, F. Huang, Y. Yacoob, D. Manocha, and T. Zhou. Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. 2023. [32] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang. Conformer: Convolution-augmented Transformer for Speech Recognition. In Proc. Interspeech 2020, pages 50365040, 2020. [33] N. Ho, C.-Y. Yang, and M. I. Jordan. Convergence Rates for Gaussian Mixtures of Experts. Journal of Machine Learning Research, 23(323):181, 2022. [34] D. A. Hudson. Gqa : new dataset for real-world visual reasoning and compositional question answering supplementary material. 2019. [35] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. Publisher: MIT Press. 47 [36] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. de las Casas, E. B. Hanna, F. Bressand, G. Lengyel, G. Bour, G. Lample, L. R. Lavaud, L. Saulnier, M.-A. Lachaux, P. Stock, S. Subramanian, S. Yang, S. Antoniak, T. L. Scao, T. Gervet, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mixtral of experts, 2024. [37] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural computation, 6(2):181214, 1994. Publisher: MIT Press. [38] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi. diagram is worth dozen images. ArXiv, abs/1603.07396, 2016. [39] A. Khalili. New estimation and feature selection methods in mixture-of-experts models. Canadian Journal of Statistics, 38(4):519539, 2010. [40] T. Kohonen. Self-organized formation of topologically correct feature maps. Biological cybernetics, 43(1):5969, 1982. [41] A. Komatsuzaki, J. Puigcerver, J. Lee-Thorp, C. R. Ruiz, B. Mustafa, J. Ainslie, Y. Tay, M. Dehghani, and N. Houlsby. Sparse upcycling: Training mixture-of-experts from dense checkpoints, 2023. [42] M. Le, A. Nguyen, H. Nguyen, T. Nguyen, T. Pham, L. V. Ngo, and N. Ho. Mixture of experts meets prompt-based continual learning, 2025. [43] J. Lee-Thorp and J. Ainslie. Sparse Mixers: Combining MoE and Mixing to build more efficient BERT. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5875, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. [44] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen. GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding. In International Conference on Learning Representations, 2021. [45] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and L. Zettlemoyer. BASE Layers: Simplifying Training of Large, Sparse Models. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 62656274. PMLR, July 2021. [46] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. [47] J. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 1288812900. PMLR, 2022. [48] J. Li, X. Wang, S. Zhu, C.-W. Kuo, L. Xu, F. Chen, J. Jain, H. Shi, and L. Wen. Cumo: Scaling multimodal llm with co-upcycled mixture-of-experts, 2024. [49] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J. rong Wen. Evaluating object hallucination in large vision-language models. In Conference on Empirical Methods in Natural Language Processing, 2023. 48 [50] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [51] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, K. Chen, and D. Lin. Mmbench: Is your multi-modal model an all-around player? ArXiv, abs/2307.06281, 2023. [52] Y. Liu, Z. Li, M. Huang, B. Yang, W. Yu, C. Li, X.-C. Yin, C.-L. Liu, L. Jin, and X. Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12), Dec. 2024. [53] P. Lu, H. Bansal, T. Xia, J. Liu, C. yue Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations, 2023. [54] T. Manole and N. Ho. Refined convergence rates for maximum likelihood estimation under finite mixture models. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 1497915006. PMLR, 1723 Jul 2022. [55] S. Masoudnia and R. Ebrahimpour. Mixture of experts: literature survey. Artificial Intelligence Review, 42(2):275293, 2014. [56] J. L. McClelland, D. E. Rumelhart, P. R. Group, et al. Parallel distributed processing, volume 2: Explorations in the microstructure of cognition: Psychological and biological models, volume 2. MIT press, 1987. [57] E. F. Mendes and W. Jiang. On convergence rates of mixtures of polynomial experts. Neural computation, 24(11):30253051, 2012. Publisher: MIT Press. [58] L. Montuelle and E. Le Pennec. Mixture of Gaussian regressions model with logistic weights, penalized maximum likelihood approach. Electronic Journal of Statistics, 8(1):16611695, 2014. Publisher: The Institute of Mathematical Statistics and the Bernoulli Society. [59] H. Nguyen, P. Akbarian, and N. Ho. Is temperature sample efficient for softmax Gaussian mixture of experts? In Proceedings of the ICML, 2024. [60] H. Nguyen, P. Akbarian, T. Pham, T. Nguyen, S. Zhang, and N. Ho. Statistical advantages of perturbing cosine router in mixture of experts. In International Conference on Learning Representations, 2025. [61] H. Nguyen, T. Nguyen, and N. Ho. Demystifying softmax gating in Gaussian mixture of experts. In Advances in Neural Information Processing Systems, 2023. [62] H. Nguyen, T. Nguyen, K. Nguyen, and N. Ho. Towards convergence rates for parameter estimation in Gaussian-gated mixture of experts. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, 2024. 49 [63] H. D. Nguyen and F. Chamroukhi. Practical and theoretical aspects of mixture-of-experts modeling: An overview. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8(4):e1246, 2018. Publisher: Wiley Online Library. [64] H. D. Nguyen, F. Chamroukhi, and F. Forbes. Approximation results regarding the multipleoutput Gaussian gated mixture of linear experts model. Neurocomputing, 366:208214, 2019. [65] H. D. Nguyen, L. R. Lloyd-Jones, and G. J. McLachlan. universal approximation theorem for mixture-of-experts models. Neural computation, 28(12):25852593, 2016. Publisher: MIT Press. [66] H. D. Nguyen, T. Nguyen, F. Chamroukhi, and G. J. McLachlan. Approximations of conditional probability density functions in Lebesgue spaces via mixture of experts models. Journal of Statistical Distributions and Applications, 8(1):13, Aug. 2021. [67] N. V. Nguyen, T. T. Doan, L. Tran, V. Nguyen, and Q. Pham. Libmoe: library for comprehensive benchmarking mixture of experts in large language models, 2024. [68] T. Nguyen. Model Selection and Approximation in High-dimensional Mixtures of Experts Models: from Theory to Practice. PhD Thesis, Normandie Université, Dec. 2021. [69] T. Nguyen, F. Chamroukhi, H. D. Nguyen, and G. J. McLachlan. Approximation of probability density functions via location-scale finite mixtures in Lebesgue spaces. Communications in Statistics - Theory and Methods, 52(14):50485059, 2023. [70] T. Nguyen, D. N. Nguyen, H. D. Nguyen, and F. Chamroukhi. non-asymptotic risk bound for model selection in high-dimensional mixture of experts via joint rank and variable selection. In Australasian Joint Conference on Artificial Intelligence. Springer, 2023. [71] T. Nguyen, H. D. Nguyen, F. Chamroukhi, and F. Forbes. non-asymptotic approach for model selection via penalization in high-dimensional mixture of experts models. Electronic Journal of Statistics, 16(2):4742 4822, 2022. [72] T. Nguyen, H. D. Nguyen, F. Chamroukhi, and G. J. McLachlan. Approximation by finite mixtures of continuous density functions that vanish at infinity. Cogent Mathematics & Statistics, 7(1):1750861, Jan. 2020. Publisher: Cogent OA. [73] T. Nguyen, H. D. Nguyen, F. Chamroukhi, and G. J. McLachlan. An l1-oracle inequality for the Lasso in mixture-of-experts regression models. arXiv:2009.10622, Jan. 2021. [74] A. Norets. Approximation of conditional densities by smooth mixtures of regressions. The Annals of Statistics, 38(3):1733 1766, 2010. Publisher: Institute of Mathematical Statistics. [75] A. Norets and J. Pelenis. Adaptive Bayesian estimation of conditional discrete-continuous distributions with an application to stock market trading activity. Journal of Econometrics, 2021. [76] A. Norets and J. Pelenis. Adaptive Bayesian Estimation of Discrete-Continuous Distributions Under Smoothness and Sparsity. Econometrica, 90(3):13551377, 2022. 50 [77] M. Oster and S.-C. Liu. Spiking inputs to winner-take-all network. Advances in Neural Information Processing Systems, 18, 2005. [78] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and R. Fernández. The lambada dataset: Word prediction requiring broad discourse context, 2016. [79] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [80] M. Riesenhuber and T. Poggio. Hierarchical models of object recognition in cortex. Nature neuroscience, 2(11):10191025, 1999. [81] C. Riquelme, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton, A. Susano Pinto, D. Keysers, and N. Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34:85838595, 2021. [82] A. Rives, J. Meier, T. Sercu, S. Goyal, Z. Lin, J. Liu, D. Guo, M. Ott, C. L. Zitnick, J. Ma, and R. Fergus. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118, 2021. [83] C. R. Ruiz, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton, A. S. Pinto, D. Keysers, and N. Houlsby. Scaling Vision with Sparse Mixture of Experts. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, 2021. [84] D. E. Rumelhart and D. Zipser. Feature discovery by competitive learning. Cognitive science, 9(1):75112, 1985. [85] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously In International Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. Conference on Learning Representations, 2017. [86] A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards vqa models that can read. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 83098318, 2019. [87] D. Soboleva, F. Al-Khateeb, R. Myers, J. R. Steeves, J. Hestness, and N. Dey. SlimPajama: 627B token cleaned and deduplicated version of RedPajama, June 2023. [88] R. K. Srivastava, J. Masci, S. Kazerounian, F. Gomez, and J. Schmidhuber. Compete to compute. Advances in neural information processing systems, 26, 2013. [89] C. Stefanis. Interneuronal mechanisms in the cortex. In UCLA forum in medical sciences, volume 11, pages 497526, 1969. [90] S. van de Geer. Empirical Processes in M-estimation. Cambridge University Press, 2000. [91] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is All you Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. 51 [92] C. Von der Malsburg. Self-organization of orientation sensitive cells in the striate cortex. Kybernetik, 14(2):85100, 1973. [93] Y. Wang, W. Wang, S. Joty, and S. C. Hoi. Codet5: Identifier-aware unified pre-trained encoderdecoder models for code understanding and generation. arXiv preprint arXiv:2109.00859, 2021. [94] A. Warstadt, A. Parrish, H. Liu, A. Mohananey, W. Peng, S.-F. Wang, and S. R. Bowman. Blimp: The benchmark of linguistic minimal pairs for english, 2023. [95] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. ArXiv, abs/2311.16502, 2023. [96] S. E. Yuksel, J. N. Wilson, and P. D. Gader. Twenty Years of Mixture of Experts. IEEE Transactions on Neural Networks and Learning Systems, 23(8):11771193, 2012. [97] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can machine really finish your sentence?, 2019. [98] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pretraining. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. [99] M. Zhang, X. Yang, X. Zhang, T. Labrum, J. C. Chiu, S. M. Eack, F. Fang, W. Y. Wang, and Z. Z. Chen. Cbt-bench: Evaluating large language models on assisting cognitive behavior therapy, 2025. [100] Y.-F. Zhang, H. Zhang, H. Tian, C. Fu, S. Zhang, J. Wu, F. Li, K. Wang, Q. Wen, Z. Zhang, L. Wang, R. Jin, and T. Tan. Mme-realworld: Could your multimodal llm challenge highresolution real-world scenarios that are difficult for humans?, 2025. [101] Y. Zhou, T. Lei, H. Liu, N. Du, Y. Huang, V. Zhao, A. M. Dai, z. Chen, Q. V. Le, and J. Laudon. Mixture-of-Experts with Expert Choice Routing. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 71037114. Curran Associates, Inc., 2022. [102] S. Zuo, X. Liu, J. Jiao, Y. J. Kim, H. Hassan, R. Zhang, J. Gao, and T. Zhao. Taming Sparsely Activated Transformer with Stochastic Experts. In International Conference on Learning Representations, 2022."
        }
    ],
    "affiliations": [
        "FPT Software AI Center",
        "Independent Researcher",
        "Institute for Infocomm Research, ASTAR",
        "The University of Texas at Austin"
    ]
}