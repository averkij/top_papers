{
    "paper_title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
    "authors": [
        "Chenghao Fan",
        "Wen Heng",
        "Bo Li",
        "Sichen Liu",
        "Yuxuan Song",
        "Jing Su",
        "Xiaoye Qu",
        "Kai Shen",
        "Wei Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages."
        },
        {
            "title": "Start",
            "content": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model Chenghao Fan1,2, Wen Heng2, Bo Li2, Sichen Liu1, Yuxuan Song2, Jing Su2, Xiaoye Qu1, Kai Shen2, Wei Wei1 1Huazhong University of Science and Technology, 2ByteDance Seed"
        },
        {
            "title": "Abstract",
            "content": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in controlled study and introduce Stable-DiffCoder, block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate block diffusion continual pretraining (CPT) stage enhanced by tailored warmup and blockwise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than wide range of 8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages. Date: January 23, 2026 Correspondence: Wei Wei at weiw@hust.edu.cn Project Page: https://github.com/ByteDance-Seed/Stable-DiffCoder Model Page: https://huggingface.co/collections/ByteDance-Seed/stable-diffcoder 6 2 0 2 2 2 ] . [ 1 2 9 8 5 1 . 1 0 6 2 : r Figure 1 Benchmark performance of Stable-DiffCoder-8B-Instruct."
        },
        {
            "title": "Introduction",
            "content": "Autoregressive (AR) language models have achieved strong results in natural language and code by modeling sequences left-to-right [13, 21, 22, 31, 49, 61, 67], but their strictly sequential decoding underutilizes the inherently non-autoregressive nature of code, where developers routinely infill missing spans, iterate from scratch toward the final version, revise earlier segments using later context, and generate independent blocks in parallel [23, 29, 58]. To better align with this paradigm, diffusion-based language models (DLLMs) [4, 43] provide complementary view by treating generation as iterative denoising: they corrupt and reconstruct sequences under random masking, enabling non-sequential, block-wise decoding and implicitly augmenting each clean example with many arbitrary corruption patterns, which is especially appealing for rare high-quality or long-tail code samples [14, 41]. Recently, mask-based DLLMs that instantiate this corruption-and-denoising view have gained attention for their potential in fast parallel generation and high inference speed ceilings [30, 38, 40, 51, 57, 60]. In this work, we therefore focus on such mask-based DLLMs and use training modes to refer to the different corruption or masking patterns sampled for each example. Despite this focus on efficiency, their impact on modeling quality remains unclear, leaving open whether diffusion can systematically improve model capability. ln principle, this stochastic training should help models absorb scarce supervision and generalize beyond specific surface forms, yet existing diffusion-based code models still lag behind strong AR baselines in overall accuracy [25, 51, 62], and prior work often changes data, architecture, and training pipeline simultaneously [56, 59], leaving open central question: under fixed data and compute budget, can the additional training modes introduced by diffusion actually improve model capability? In this work, we systematically study how to enable diffusion models to learn new knowledge more efficiently, by conducting controlled experiments on 2.5B model size. Building on the observations, we propose Stable-DiffCoder, diffusion language model that reuses the autoregressive Seed-Coder [49] pipeline and data but achieves stronger performance. Starting from the AR Seed-Coder checkpoint before annealing, we perform continual pretraining on 1.3T tokens using small-block diffusion (block size 4), preserving representation plasticity while introducing multi-token prediction patterns. To make the training process more suitable for DLLM, we design (i) the tailored warmup training process, (ii) block-wise clipped noise schedule that guarantees non-trivial supervision within each block. Finally, we carry out comprehensive evaluation on broad suite of code benchmarks. Across both base and instruction-tuned settings, Stable-DiffCoder almost uniformly surpasses its AR counterpart Seed-Coder of the same size, and establishes new state-of-the-art results among 8B-scale diffusion code models on many metrics. These results suggest that, when equipped with an appropriate curriculum and training design, diffusion language models can not only match but exceed autoregressive models in code understanding and generation quality, even before factoring in any potential inference-time speed advantages."
        },
        {
            "title": "2.1 Autoregressive Language Models",
            "content": "Transformer-based [54] AR models have been the dominant paradigm in language modeling, powering many state-of-the-art code and text generation systems [13, 21, 22, 31, 49, 61, 67]. Let be finite vocabulary and x1:T = (x1, . . . , xT ) token sequence. In an AR language model, it is assumed that each token is conditioned on all preceding tokens, so that the joint distribution is factorized in left-to-right manner as pθ(x1:N ) = pθ(x1) (cid:89) i=2 pθ(xi x<i) (1) where xt is the t-th token in the sequence. We typically parameterize pθ with Transformer, and train by maximizing token-level cross-entropy. Concretely, given data pdata, the training loss is: LAR(θ) = Expdata (cid:104) log pθ(x1) + (cid:105) log pθ(xi x<i) (cid:88) i=2 (2) This is the standard next-token prediction objective widely used in mainstream AR models. Notable AR transformers like GPT-series [6, 31, 61] and code-specific LLMs [22, 49] have achieved high accuracy on 2 programming benchmarks, but their token-by-token generation can be slow and lacks mechanism for global planning or simultaneous reasoning over multiple parts of the sequence. This limitation has motivated exploration of non-AR generation alternatives."
        },
        {
            "title": "2.2 Masked Diffusion Language Models",
            "content": "Masked diffusion language models (DLLMs) have recently emerged as promising non-AR alternative for text and code generation [5, 10, 18, 28, 43, 51, 62, 65]. Inspired by continuous diffusion models in vision, discrete diffusion LMs optimize variational evidence lower bound by corrupting observed sequences (e.g., by randomly masking tokens) and training denoiser to reconstruct the clean data [4, 36, 39]. This framework provides principled likelihood-based foundation for parallel or partially parallel text generation, which earlier heuristic non-AR methods often lacked [26, 33, 58]. , where x0 Instead of factorizing the distribution autoregressively, DLLMs define sequence of latent variables x0 1:N , .. denotes fully corrupted state. In DLLMs, the corruption ., xT is instantiated via dedicated [MASK] token. The forward noising process progressively replaces tokens with [MASK], while the reverse denoising process iteratively reconstructs the clean text step by step. denotes the clean text and xT 1:N , x1 1:N 1:N 1:N pθ(x1:N ) = pθ(xT 1:N ) (cid:89) t= pθ(xt1 1:N xt 1:N ) (3) where pθ(xT network. 1:N ) is typically uniform prior over noise states, and pθ(xt 1:T xt 1:T ) is parameterized by neural In particular, for DLLMs, prior theoretical analyses [44, 47, 50] have shown that this variational denoising objective can be equivalently reduced to weighted cross-entropy loss, which enables efficient training in practice: LDLLM(θ) = Ex0pdata, tU (0,1), xtq(xtx0) (cid:104) w(t) (cid:88) i=1 1[xt = MASK] log pθ(x0 xt 1:N ) (cid:105) (4) where q(xt x0) denotes the corruption process, 1[xt = [MASK]] is the indicator function specifying whether the i-th token is masked at timestep t, and w(t) is weighting coefficient that depends on the corruption level. For example, under linear noise schedule one may set w(t) = 1 . This formulation allows DLLMs to leverage wide range of advanced model architectures, requiring only minor modifications to the original implementation in order to support diffusion language models training. Recently, several works combine AR structure with diffusion to form block diffusion variants of DLLMs [1, 5, 10, 33, 34, 53, 56, 57]. Instead of corrupting the whole sequence with random masks, these models only diffuse contiguous block of tokens at each step while keeping the surrounding context clean, so that the denoiser learns to generate span conditioned on mostly uncorrupted prefix. Beyond offering higher ceiling for parallel decoding speed [18, 28, 38, 51, 56], diffusion-style training also enables repeated reuse of the same underlying examples under diverse corruption trajectories, which can extract more information from rare high-quality and long-tail samples and potentially improve overall model capability [14, 41]. Our work aims to probe this question in the code domain under controlled conditions: given the same data, architecture, and training pipeline as strong AR baseline, we introduce block diffusion stage and investigate whether the additional training modes provided by diffusion can translate into tangible gains in code understanding and generation accuracy, rather than merely offering different decoding mechanism."
        },
        {
            "title": "3.1 Efficient Knowledge Compression and Training-Inference Alignment in DLLMs\nMotivation. Although random masking can dramatically improve data reusability for diffusion language models\n[14, 41] by repeatedly revisiting the same original examples with different mask patterns applied, it imposes a\nsubstantial computational cost. Moreover, some mask patterns only demonstrate negligible training utility, as",
            "content": "3 shown in recent work [26]. In particular, many masked tokens are trained in contexts where the correct answer is only weakly constrained, and the resulting gradients are dominated by noisy token co-occurrence rather than sharp reasoning signal. Moreover, even when the model compresses knowledge efficiently under some training contexts, this does not automatically translate into good performance under the inference contexts, where mismatch between the training and inference paradigms has been placed. We next formalize this effect and use it to motivate practical training curricula."
        },
        {
            "title": "3.1.1 Token Reasoning Knowledge under Random Masking",
            "content": "In the RADD [44] formulation, the concrete score at time for coordinate takes the below form: s(xt, t)(i,ˆxi) = α(t) p0 (cid:0)ˆxi xUM (cid:1), (5) denotes all unmasked tokens at time t, and α(t) depends only on and the forward kernel. The where xUM time dependence is thus factored out, and what remains is the clean-data conditional p0(ˆxi c) for various contexts induced by random masks. However, not every masked context can lead to faithful reasoning unmask order towards the final answer. Consider clean sequence: If we heavily mask the middle and final parts to obtain: = 1, = 2, + = 3; = 3, = 4, + = 7. = 1, = 2, [MASK1] ...[MASK2] + = [MASKn], (6) (7) the last [MASKn] cannot by itself teach the rule that + equals the sum of the preceding and b, because the model never sees clean evidence for the pair = 3, = 4. From this particular corrupted view, the model mainly learns that some number tends to appear after + =, and that certain pairs like = 3, = 4 and 7 co-occur. From the clean evidence the model has access to, the correct answer should be 3, yet the masked training signal biases the model toward predicting 7, creating contradictory and misleading supervision signal. By contrast, in sample like below: = 1, = 2, + = 3 masked as = 1, = 2, + = [MASK]. (8) the context strongly constrains the answer. In this way, across many such examples, the model can discover stable mapping from evidence to output, which corresponds to an arithmetic reasoning rule rather than mere memorization. Definition 3.1 (Token Reasoning Knowledge) Let be clean training sequence sampled from the real data distribution p0, and denote context extracted from that is used to predict single token xi (similarly = xUM for multiple tokens). For autoregressive training, cAR = x<i, and for diffusion with random masks, ci . Given fixed context c, the clean-data conditional p0( c) induces candidate set: C(c) = (cid:8)v : p0(v c) ε(cid:9), K(c) = C(c), ε > 0. (9) We define the token reasoning knowledge contained in training example with context as the conditional distribution of the next token restricted to its candidate set, denoted compactly by p0 c) such The models objective is to recover this token reasoning knowledge by learning conditionals pθ(x0 (cid:0)C(c) c(cid:1). Concretely, training provides randomly sampled pairs (c, x0 that, pθ ) obtained from (x, i) under p0, and the model adjusts pθ( c) based on the empirical distribution of these pairs. The difficulty of learning this knowledge is analogous to multi-class classification problem [12, 27]: (1) the size of the candidate set K(c); (2) how often each contextlabel pair (c, x0 ) appears in the training process. (cid:0)C(c) c(cid:1) p0 (cid:0)C(c) c(cid:1) With respect to K(c), we can distinguish three qualitative regimes: Reasoning regime. K(c) is small and the ground-truth token x0 has large probability under p0( c). is almost deterministic, so repeated samples with similar provide highly aligned The mapping (cid:55) x0 gradients and quickly reinforce stable reasoning rule. 4 is one of many plausible candidates with Correlation regime. K(c) is moderate or large, and x0 comparable probability. The model still learns that x0 correlates with patterns in c, but samples with similar often yield different targets, so gradients partially cancel and updates primarily fit noisy co-occurrence rather than sharp rule. Noise regime. K(c) is very large (on the order of V) and p0( c) is nearly flat. The context contains almost no information about x0 , and the model can only memorize idiosyncratic pairs (c, x0 ). Because p0( c) is defined with respect to the real data distribution, extremely unnatural or noisy contexts have negligible probability and can be ignored in expectation, as clarified in previous works [15, 64]. As more relevant evidence is revealed in (for example, longer and cleaner context), the conditional entropy H(Xi c) cannot increase, and the candidate set size K(c) typically shrinks. For autoregressive architectures and block diffusion with small block sizes, the context usually contains long, contiguous, and clean left-side evidence, which also matches the inherently autoregressive structure of natural language [48]. As result, sampled contexts tend to have relatively small K(c) and allow token reasoning knowledge to be compressed efficiently. In contrast, fully bidirectional or large-block diffusion readily produces contexts similar to Eq. 7 that fall into the correlation or noise regimes, dramatically reducing the efficiency of knowledge compression. ). To reliably learn the token reasoning knowledge associated with context c, the Contextlabel Pair (c, x0 model needs sufficient number of informative contextlabel pairs (c, x0 ). DLLMs based on random masking ) that the generate wide variety of contexts c, which greatly increases the number of contextlabel pairs (c, x0 model must learn [55]. If the total training data is fixed, this diversification reduces the number of effective pairs corresponding to any specific piece of knowledge, lowering its learning efficiency and forcing the model to relearn the same underlying knowledge many more times than an AR model would require [42, 52, 55]. Although such random contexts can be viewed as form of data augmentation, our analysis of the size of K(c) shows that many of these contexts fall into the correlation or noise regimes and therefore cannot be mapped to clear reasoning rule. In some cases, such as the pattern in Eq. 7, the masked context even encourages learning wrong association: the model is asked to predict target under incomplete or misleading evidence. Such problematic contexts appear frequently when clean evidence is heavily disrupted, for example in purely bidirectional or large-block diffusion settings. Moreover, the knowledge compressed during training is only useful at inference if it is associated with contexts that actually occur at test time. Let Ctrain denote the set (or distribution) of contexts sampled during training, and let Cinfer denote the contexts encountered along inference trajectories. Good performance requires Ctrain and Cinfer to be as close as possible. For instance, when training with block diffusion of block size and using left-to-right block-wise decoding of size at inference, the inference contexts closely match the training contexts, which makes the learned token reasoning knowledge directly applicable at test time. In summary, to efficiently learn new knowledge while ensuring that the data augmentation induced by DLLM masking remains effective, two conditions should be satisfied: (1) the model should be exposed to clean and reliable reasoning evidence so that clear reasoning rules can be learned, and (2) the number of distinct sampled contexts should not grow excessively, and their form should align as closely as possible with the contexts encountered during inference. Based on these principles, we next design experiments to explore more suitable training pipelines. 3.1.2 Curriculum Design and Empirical Results at 2.5B Scale Since initializing from an AR checkpoint has become standard and effective practice, we also start from 2.5B AR model trained on general-domain data, and use code data as new CPT data. Starting from this AR checkpoint, we consider the following curricula for learning new knowledge under the same compute budget, and evaluate them using block-wise decoding in the style of LLaDA [43]: (1) AR BiDLLM: continue pure AR training on the new data, then perform CPT to bidirectional DLLM; 5 Figure 2 Training dynamics under different block sizes. We compare ARDLLM and AR against BiDLLM baseline for block sizes 32, 2, and 1 (top to bottom). Solid lines indicate training up to 100k steps, while dotted lines denote continued training after switching to BiDLLM. The horizontal dashed line marks the fixed baseline reference (0-step AR). Figure 3 As shown by the black solid line, we initialize from the pre-annealing checkpoint of Seed-Coder and perform CPT with small-block DLLM to obtain StableDiffCoder-Base, aiming to study efficient knowledge acquisition in diffusion-based models. The dashed line denotes an alternative pipeline for larger blocks, where new knowledge is first compressed using an AR model or small-block DLLM before being transferred to the large-block diffusion setting. (2) ARDLLM BiDLLM: continue training with causal-structured DLLM (AR-style diffusion), then CPT to bidirectional DLLM; (3) BiDLLM: directly CPT the AR checkpoint into bidirectional DLLM and train new knowledge in that regime. As shown in Fig. 2, we report the average performance across multiple code benchmarks. Under the same compute budget and after continued pre-training (CPT), the overall performance follows the ranking (1) > (2) > (3) under the same compute. Scheme (1) preserves purely AR training structure before CPT. Under small-block decoding (block size 1 or 2), its Cinfer are almost identical to its Ctrain and satisfy the two principles from the previous section: the model sees clean reasoning evidence, and the number of distinct contexts remains controlled and well aligned with small-block inference. Consequently, before CPT to BiDLLM, scheme (1) consistently achieves the best performance for small blocks. For large-block decoding (block size 32), however, scheme (1) underperforms scheme (3), because the training context distribution Ctrain of pure AR does not cover the bidirectional patterns required by large-block decoding. Formally, with causal attention, tokens later in the block cannot influence earlier ones, so when decoding an entire block at once, the generation process becomes less stable. After CPT into bidirectional DLLM, scheme (1) performs well across all block sizes. This suggests that the 6 AR phase has already compressed the new token reasoning knowledge effectively in well-aligned context family for small blocks, and the subsequent BiDLLM CPT primarily adapts the model to the larger-block context distribution. It is worth noting that CPT into fully bidirectional model introduces some degradation for block-1 decoding: both pure AR and ARBiDLLM lose similar amount of performance between 0 and 100k CPT steps at block size 1. However, the results also show that scheme (1) achieves higher learning efficiency during the AR phase, so even after the CPT-induced drop, it still attains better final performance. Interestingly, scheme (2) also achieves strong performance after CPT. In particular, after converting to BiDLLM, it outperforms scheme (3) under block-32 decoding, despite having seen similar amount (or even less) clean evidence during training. One explanation is that the training context distribution Ctrain for AR-style diffusion in scheme (2) more closely matches the promptresponse pattern commonly used at inference, where most reasoning evidence lies on the left. Thus scheme (2) enjoys better traininginference alignment than scheme (3), and this advantage persists even when both are eventually converted to fully bidirectional DLLM. Finally, based on the above theoretical analysis and empirical results, we recommend the following training procedure: 1. For new knowledge, first use AR training to efficiently compress it; 2. Then perform CPT with small-block diffusion objective, leveraging its data-augmentation properties to further improve model quality; 3. if one wishes to explore larger block diffusion, additional CPT can be applied starting from the model obtained in step (2). In our final system, we adopt steps (1) and (2) to train Stable-DiffCoder, as illustrated by the solid path leading to the DLLM Base Model in Fig. 3."
        },
        {
            "title": "3.2 Warmup for Stable DLLM Continued Pretraining",
            "content": "Previous work [16] has reported that CPT of mask diffusion language models (DLLMs) is highly sensitive to the learning rate. In practice, the stability of the loss and gradient norm is also affected by architecture (e.g., attention pattern, logit parametrization), numerical precision, and infrastructure details. This motivates more robust warmup procedure for DLLM CPT. To reduce the architectural and objective gap between AR and DLLM, we reuse the AR token head and logit-shift parametrization, and only change the attention pattern from causal lower-triangular mask to bidirectional, full attention mask. Even under this minimal change, naive ARDLLM CPT shows high initial loss and large gradient-norm spikes. We attribute the instability mainly to three factors: (i) the change of attention mask, which induces structural distribution shift in internal representations; (ii) the higher task difficulty when the corruption process masks large fraction of tokens, compared to AR next-token prediction; (iii) the ELBO-motivated loss weight w(t) in the DLLM objective (Eq. 4), where w(t) can be large at low masking ratios (e.g., under linear noise schedule, masking 10% of tokens yields w(t) 10). This effectively acts as loss scaling that amplifies gradient norms, making training less stable. Rather than annealing the attention mask [16] (which is inconvenient for highly optimized kernels such as FlashAttention that assume fixed mask), we warm up only the corruption process. In standard DLLM, the corruption level is sampled uniformly from [0, 1], and the masking ratio ranges from 0 to 1. During warmup, we cap the maximum corruption level: U(0, umax), and linearly increase qmax from small value qinit (e.g., 103) to 1 over Swarmup steps: umax(s) = uinit + (1 uinit) Swarmup , = 0, . . . , Swarmup. 7 BiDLLM Without Warmup BiDLLM With Warmup BlockDLLM With Warmup (no-shift) Figure 4 Comparison of training stability before and after applying warmup. The left figure shows the behavior without warmup, and the right figure shows the behavior with warmup. When warmup is used, both the training loss and gradient norm become significantly more stable, and they quickly decrease to level comparable to that of the AR continual pretraining stage. BiDLLM refers to purely bidirectional masked diffusion model, BlockDLLM to block-masked diffusion model, and no-shift indicates that token logit shifting is disabled, which is the configuration adopted in our final model. This implements curriculum from easy (low mask ratio, almost AR-like) reconstruction to the full DLLM regime. To further suppress gradient spikes, we drop w(t) in the warmup phase and optimize Lwarmup DLLM (θ) = Ex0pdata, tU (0,umax), xtq(xtx0) (cid:104) (cid:88) i=1 1[xt = MASK] log pθ(x0 xt (cid:105) 1:N ) . (10) After warmup, we revert to the original loss in Eq. (4) with U(0, 1). This warmup produces much smoother CPT trajectory: the gradient norm spike at the ARDLLM boundary is strongly reduced, and the loss follows characteristic -shaped curve (first decreasing on easy tasks, then increasing as the mask ratio grows, and finally decreasing again). The peak loss during warmup is significantly lower than the naive ARDLLM baseline, and the final loss is comparable to ARAR CPT. We observe similarly stable behavior when we remove the logit shift and train block-diffusion DLLM with the same warmup (see Fig. 4)."
        },
        {
            "title": "3.3 Block-wise Clipped Noise Scheduling for Masked Block Diffusion",
            "content": "In masked block diffusion training, at each step we corrupt only single contiguous block of Motivation. tokens rather than the full sequence. If one reuses global continuous-time schedule [0, 1] (cid:55) u(t) (with the corruption/mask rate) that was designed for whole-sequence diffusion, significant portion of training steps produce weak or even zero learning signal when the block length is small. Concretely, when the corruption is applied only inside block of size B, the expected number of corrupted tokens at time is E[m t] = u(t) and the probability of observing no corrupted token is Pr[m = 0 t] = (1 u(t))B.. Under standard global linear schedule u(t) = 1 with Unif[0, 1], the fraction of steps with = 0 equals Et[(1 t)B] = (cid:90) 1 0 (1 t)B dt = 1 + , (11) which is non-negligible for typical block sizes (e.g., 1/3 for B=2, 1/5 for B=4, 1/9 for B=8). 8 Instead of redesigning the global schedule, we adopt simple block-aware sampling rule. At each training step, after sampling t, we define block-specific mask rate by clipping ublk(t) = min(cid:0)1, max(u(t), 1/B)(cid:1), so that qblk(t) [1/B, 1] for all t. This guarantees that the expected number of masked tokens in the block satisfies E[m t] = ublk(t) 1. In addition, it prevents the loss weight w(t) = 1 for those tokens from becoming excessively large, thereby promoting more stable training. ublk(t) Given ublk(t), we independently mask each token in the chosen block with probability ublk(t), while tokens outside remain clean and provide context. To further ensure that every block contributes to the loss, we apply the following fallback rule: if, after sampling, no token in is masked (i.e., = 0), we uniformly sample one position in and force it to be masked. In this way, every training step contains at least one supervised token inside the block, while still preserving the overall shape of the original schedule u(t)."
        },
        {
            "title": "4 Experiments",
            "content": "We conduct extensive experiments to compare the base model and its instruction-tuned (instruction) variant across diverse set of code-centric benchmarks."
        },
        {
            "title": "4.1 Experiments Setting",
            "content": "To preserve representation plasticity and enable structural adaptation to the target data distribution, we perform continuous pretraining from Seed-Coder pre-annealing checkpoint, using context length of 8192 with packed sequences. We reuse the Seed-Coder training pipeline and compress its multi-stage continuous pretraining data to total of 1.3T tokens via subsampling. The supervised fine-tuning (SFT) stage fully reuses the original Seed-Coder SFT dataset. We adopt block diffusion with block size of 4 for this continuous pretraining stage. Since the no-logit-shift formulation is more consistent with the absorbing diffusion paradigm, where each masked position predicts itself and the input and prediction targets are aligned at both the token and sentence levels, we follow no-logit-shift design similar to that used in LLADA and SDAR. To prevent repeated compilation of flex attention, we make the attention between packed samples mutually visible, so that the same attention mask and operator can be reused in every forward pass. To maintain training efficiency, we adopt the same packing strategy during SFT. However, after each sample we randomly append 14 <eos> tokens, enabling the model to preserve the ability to generate variable-length outputs within each packed block. In our experiments, we compare our method against broad set of strong AR models and DLLM that have demonstrated competitive performance on code generation tasks. The AR baselines include StarCoder2 [37], DeepSeek-Coder [20], CodeQwen1.5 [45], OpenCoder [21], Qwen2.5Coder [22], Seed-Coder [49], CodeLlama [46], and Llama3.1 [35]. The DLLM baselines include LLaDA [43], Dream [62], DiffuCoder [17], Dream-Coder [59], LLaDA-MoE [66], Fast-dLLMv2 [56], SDAR [10], Seed-Diffusion-Preview, LLaDA2.0 [5], SDLM [34], WeDLM [30], Mercury Coder [28], and Gemini Diffusion [18]."
        },
        {
            "title": "4.2 Benchmarks",
            "content": "We evaluate the base and instruction models on diverse suite of coding benchmarks spanning function-level code generation, execution-centric code reasoning, multilingual generalization, and instruction-following code editing: HumanEval / HumanEval+: HumanEval [9] contains 164 Python function-completion tasks evaluated by unit tests. HumanEval+ from EvalPlus [32] substantially expands the test suites (about 80) for stricter functional correctness. We use EvalPlus to report results on both HumanEval and HumanEval+. MBPP / MBPP+: MBPP [3] includes 974 crowd-sourced Python programming problems with tests. We adopt the human-verified EvalPlus subset (399 well-formed tasks) and additionally evaluate on MBPP+, which augments the tests (about 35) for more reliable pass@1 estimation [32]. 9 Model Type Size HumanEval HE HE+ MBPP MBPP+ MBPP 8B Models StarCoder2-7B DeepSeek-Coder-6.7B-Base CodeQwen1.5-7B OpenCoder-8B-Base Qwen2.5-Coder-7B Seed-Coder-8B-Base LLaDA-8B-Base Dream-7B-Base DiffuCoder-7B-Base Dream-Coder-7B-Base LLaDA-MoE-7B-A1B-Base WeDLM-8B-Base Stable-DiffCoder-8B-Base AR AR AR AR AR AR DLLM DLLM DLLM DLLM DLLM DLLM DLLM 7B 35.4 6.7B 47.6 7B 51.8 8B 66.5 7B 72.0 8B 77.4 8B 35.4 7B 56.7 7B 67.1 7B 66.5 1B/7B 45.7 8B 75.0 8B 79.3 13B+ Models StarCoder2-15B CodeLlama-70B-Base Llama-3.1-70B-Base DeepSeek-Coder-33B-Base DeepSeek-Coder-V2-Lite-Base Qwen2.5-Coder-14B AR AR AR AR AR AR 15B 46.3 70B 52.4 70B 54.9 33B 54.9 2.4B/16B 40.9 14B 83.5 29.9 39.6 45.7 63.4 67.1 68.3 30.5 50.0 60.4 60.4 - 68. 73.8 37.8 50.6 51.2 47.6 34.1 75.6 54.4 70.2 72.2 79.9 79.4 82.0 50.1 68.7 74.2 75.9 52.4 67.0 83.6 66.2 71.0 81.4 74.2 71. 83.6 45.6 56.6 60.2 70.4 68.3 69.0 42.1 57.4 60.9 61.6 - - 67.7 53.1 65.6 67.2 60.7 59.4 69.8 Table 1 Performance of various base models on HumanEval(+) and MBPP(+). CRUXEval: Small token-level changes can drastically alter program behavior, making code reasoning sensitive and precise. CRUXEval [19] provides 800 Python functions with I/O examples and evaluates two tasks: CRUXEval-I (predict inputs given outputs) and CRUXEval-O (predict outputs given inputs). MultiPL-E: Beyond Python, we evaluated base models on code generation across multiple programming languages using MultiPL-E [7], which extends HumanEval to 18 languages. Following Qwen2.5-Coder, we report results on eight representative mainstream languages to assess cross-language coding performance and language-specific effects. MHPP: To better separate strong models beyond saturated benchmarks (e.g., HumanEval/MBPP), we include MHPP [11], which targets harder Python problems with more complex specifications and reasoning requirements. BigCodeBench: BigCodeBench [68] focuses on challenging, realistic programming by requiring tool-like function calls from 139 libraries across 7 domains, covering 1,140 Python tasks with rich context. Each task has multiple tests (5.6 on average) with high branch coverage, and we report results on both the full set and the Hard split of BigCodeBench-Completion. LiveCodeBench: To mitigate contamination and overfitting to static benchmarks, LiveCodeBench [24] continuously collects time-stamped problems from competitive programming platforms (e.g., LeetCode, AtCoder, Codeforces), enabling evaluation within recent, user-specified time windows. We follow the v5 evaluation benchmark used by Seed-Coder for comparison. MBXP: MBXP [2] translates MBPP problems and unit tests into 10+ programming languages, enabling execution-based multilingual evaluation. We report results across 13 widely used languages. NaturalCodeBench: NaturalCodeBench [63] contains 402 high-quality problems curated from real user queries (Python and Java) across six practical domains (e.g., software engineering, data science, system administration), with more diverse and complex inputs than classic algorithmic benchmarks. 10 Size Python C++ Java PHP 8B Models TS C# Bash JS Average Model StarCoder2-7B DeepSeek-Coder-6.7B-Base CodeQwen1.5-7B OpenCoder-8B-Base Qwen2.5-Coder-7B Seed-Coder-8B-Base Stable-DiffCoder-8B-Base 7B 6.7B 7B 8B 7B 8B 8B 35.4 49.4 51.8 66.5 72.0 77. 80.5 40.4 50.3 52.2 63.4 62.1 69.6 69.4 38.0 43.0 42.4 63.9 53.2 72.8 74.1 StarCoder2-15B CodeLlama-70B-Base Llama-3.1-70B-Base DeepSeek-Coder-33B-Base DeepSeek-Coder-V2-Lite-Base Qwen2.5-Coder-14B 15B 70B 70B 33B 2.4B/16B 14B 13B+ Models 46.3 52.4 54.9 56.1 40.9 83.5 47.2 49.7 41.0 58.4 45.9 69. 46.2 44.7 41.1 51.9 34.8 46.8 30.4 38.5 46.6 61.5 59.0 63.9 74.4 39.1 46.6 48.4 44.1 47.2 64. 34.0 49.7 52.2 68.6 64.2 77.4 74.8 42.1 57.2 57.9 52.8 48.4 69.2 46.2 50.0 55.7 54.3 60.8 53. 70.3 53.2 46.7 44.2 51.3 41.7 63.3 13.9 28.5 36.7 44.3 38.6 48.1 53.2 15.8 31.6 29.1 32.3 19. 39.9 36.0 48.4 49.7 65.8 60.3 77.6 73.1 43.5 56.5 55.3 55.3 44.7 61.5 34.3 44.7 48.4 61.0 58.8 67. 71.2 41.7 48.2 46.5 50.3 40.4 62.3 Table 2 Performance of base models on MultiPL-E. Model Size CRUXEval Input-CoT Output-CoT StarCoder2-7B DeepSeek-Coder-6.7B-Base OpenCoder-8B-Base Qwen2.5-Coder-7B Seed-Coder-8B-Base Stable-DiffCoder-8B-Base 8B Models 7B 6.7B 8B 7B 8B 8B 13B+ Models StarCoder2-15B CodeLlama-34B-Base DeepSeek-Coder-33B-Base DeepSeek-Coder-V2-Lite-Base Qwen2.5-Coder-14B 15B 34B 33B 2.4B/16B 14B 39.5 39.0 43.3 56.5 52.0 53.8 46.1 49.4 50.6 53.4 60.6 35.1 41.0 43.9 56.0 54. 60.0 47.6 43.9 48.8 46.1 66.4 Table 3 Performance of base models CRUXEval. Aider: We use Aiders 1 code editing benchmark to assess instruction-following edits on existing codebases, based on 133 Exercism exercises, requiring models to produce changes that can be applied automatically and pass tests. CanItEdit: CanItEdit [8] evaluates instructional code editing with 105 hand-crafted problems covering both detailed and underspecified (lazy) instructions, testing robustness across diverse edit scenarios."
        },
        {
            "title": "4.3 Evaluation of Base Models",
            "content": "4.3.1 Code Generation Humaneval and MBPP As shown in Table 1, we evaluate Stable-DiffCoder-8B-Base using the EvalPlus chat-style prompting template. Among diffusion language models (DLLMs) of comparable scale (approximately 8B parameters), our base model achieves the best overall performance on both HumanEval(+) and MBPP(+). When compared with similarly sized autoregressive (AR) baselines, Stable-DiffCoder consistently outperforms 1https://aider.chat/docs/leaderboards/edit.html 11 Model Type Size HumanEval HE HE+ MBPP MBPP+ MBPP 8B Models CodeLlama-7B-Instruct DeepSeek-Coder-6.7B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat Llama-3.1-8B-Instruct OpenCoder-8B-Instruct Qwen2.5-Coder-7B-Instruct Qwen3-8B Seed-Coder-8B-Instruct LLaDA-8B-Instruct Dream-7B-Instruct LLaDA-MoE-7B-Instruct Fast-dLLMv2 DiffuCoder-7B-Instruct Dream-Coder-7B-Instruct SDAR-8B-Chat WeDLM-8B-Chat Stable-DiffCoder-8B-Instruct AR AR AR AR AR AR AR AR AR DLLM DLLM DLLM DLLM DLLM DLLM DLLM DLLM DLLM 7B 40.9 6.7B 74.4 7B 83.5 9B 82.3 8B 68.3 8B 83.5 7B 88.4 8B 84.8 8B 84.8 8B 49.4 7B 63.4 1B/7B 61.6 7B 43.9 7B 72.0 7B 82.9 8B 78.7 8B 80.5 8B 86.6 13B+ Models and External API StarCoder2-15B-Instruct Codestral-22B CodeLlama-70B-Instruct DeepSeek-Coder-33B-Instruct DeepSeek-Coder-V2-Lite-Istruct DeepSeek-Coder-V2-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Coder-32B-Instruct SDAR-30B-Chat LLaDA2.0-mini LLaDA2.0-flash SDLM-32B Seed-Diffusion-Preview(0705) Mercury Coder Gemini Diffusion 15B 67.7 AR 22B 81.1 AR 70B 72.0 AR 33B 81.1 AR 2.4B/16B 81.1 AR 21B/236B 85.4 AR 14B 89.6 AR 32B 92.7 AR 3B/30B 87.2 DLLM 1B/16B 86.6 DLLM DLLM 6B/100B 94.5 32B 81.1 DLLM 82.8 DLLM 90.0 DLLM 89.6 DLLM - - - 33.5 71.3 78.7 74.4 59.8 78. 84.1 80.5 78.7 - - - 40.2 65.2 - - 73.8 82.3 60.4 73.2 65.9 75.0 75.6 82.3 87.2 87.2 - 79.9 87.8 73.8 - - - 54.0 74.9 77.7 82.0 70.1 79.1 83.5 77.0 85.2 41.0 68.3 70.0 50.0 75.1 79.6 72.0 70.5 85. 78.0 78.2 77.8 80.4 82.8 89.4 86.2 90.2 71.6 81.5 88.3 80.9 79.4 77.1 76.0 44.4 65.6 67.2 69.0 59.0 69.0 71.7 67.2 71.2 - - - 41.3 61.9 - - - 72.8 65.1 62.2 64.6 70.1 70.4 75.1 72.8 75.1 - 74.1 79.6 57.1 - - - Table 4 Performance of various base models on HumanEval(+) and MBPP(+). them on HumanEval and HumanEval+, as well as on MBPP, while being only slightly inferior on MBPP+. Notably, Stable-DiffCoder exhibits substantial improvement over our AR baseline, Seed-Coder-8B-Base, across all evaluated HumanEval and MBPP settings. MBXP Beyond Python-centric benchmarks, we further evaluate multilingual code generation on MultiPL-E  (Table 2)  . Stable-DiffCoder yields particularly large gains in languages such as C# and PHP. Since these languages are sparsely represented in our training corpus, we hypothesize that diffusion-style stochastic sampling can effectively amplify learning signals from low-resource code by exposing the model to multiple corrupted-and-denoised views of the same underlying example, thereby improving generalization in data-scarce languages. 4.3.2 Code Reasoning CRUXEval We additionally assess code reasoning on CRUXEval  (Table 3)  , which consists of Python-only problems. We observe that Stable-DiffCoder outperforms Seed-Coder-Base on both reasoning inputs (InputCoT) and reasoning outputs (Output-CoT). This suggests that incorporating moderate degree of random masking objectives can effectively enhance the models reasoning capability. Moreover, the inputs and outputs in CRUXEval are inherently structured rather than strictly following left-to-right causal logic. As result, DLLMs benefit from any-order modeling, which enables them to more comprehensively capture the relationships among these structured components."
        },
        {
            "title": "4.4 Evaluation of Instruction Models",
            "content": "Model Size MHPP pass@1 BigCodeBench Full Hard LiveCodeBench pass@1 CodeLlama-7B-Instruct DeepSeek-Coder-6.7B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat Llama-3.1-8B-Instruct OpenCoder-8B-Instruct Qwen2.5-Coder-7B-Instruct Qwen3-8B Seed-Coder-8B-Instruct Stable-DiffCoder-8B-Instruct 8B Models 7B 6.7B 7B 9B 8B 8B 7B 8B 8B 8B 6.7 20.0 17.6 26.7 17.1 30.5 26.7 32.8 36.2 42.4 25.7 43.8 43.6 49.0 40.5 50.9 48.8 51.7 53.3 54. 13B+ Models and External API StarCoder2-15B-Instruct Codestral-22B CodeLlama-70B-Instruct DeepSeek-Coder-33B-Instruct DeepSeek-Coder-V2-Lite-Instruct DeepSeek-Coder-V2-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Coder-32B-Instruct LLaDA2.0-flash Seed-Diffusion-Preview(0705) Mercury Coder Gemini Diffusion 15B 22B 70B 33B 2.4B/16B 21B/236B 14B 32B 6B/100B - - - 19.0 25.2 19.5 32.9 30.5 31.9 36.7 42.4 - - - - 45.1 52.5 49.6 51.1 47.6 59.7 52. 52.3 41.6 53.2 45.5 45.4 4.1 15.5 15.5 17.6 13.5 18.9 20.3 23.0 26.4 31.8 14.9 24.3 15.5 20.9 18.2 33.1 16.2 20.9 - - - - 3.6 9.6 3.0 17.5 11.5 17.1 17.3 23. 24.7 23.5 5.3 20.5 14.5 14.5 14.2 28.9 19.3 30.7 - - - - Table 5 Performance of instruct models on MHPP, BigCodeBench-Completion and LiveCodeBench (v5). Model Size Python Java C++ C# TS JS PHP Go Kotlin Perl Ruby Scala Swift Average CodeLlama-7B-Instruct DeepSeek-Coder-6.7B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat Llama-3.1-8B-Instruct OpenCoder-8B-Instruct Qwen2.5-Coder-7B-Instruct Qwen3-8B Seed-Coder-8B-Instruct Stable-DiffCoder-8B-instruct 7B 6.7B 7B 9B 8B 8B 7B 8B 8B 8B StarCoder2-15B-Instruct Codestral-22B CodeLlama-70B-Instruct DeepSeek-Coder-33B-Instruct DeepSeek-Coder-V2-Lite-Instruct DeepSeek-Coder-V2-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Coder-32B-Instruct Seed-Diffusion-Preview(0705) 15B 22B 70B 33B 2.4B/16B 21B/236B 14B 32B - 8B Models 50.0 55.9 64.4 70.3 56.6 71.0 71.5 68.9 74.2 71. 42.3 64.8 66.7 74.1 59.1 67.6 72.2 73.0 72.8 73.0 32.9 30.9 66.8 79.1 59.1 71.3 74.1 72.8 77.0 77.8 45.5 64.7 67.5 73.3 59.1 61.4 74.1 73.8 78.8 76. 38.8 52.2 66.6 73.4 59.8 68.1 70.5 69.0 72.7 75.3 13B+ Models and External API 25.1 73.6 66.6 71.8 73.3 78.2 77.5 80.4 67.7 25.9 77.3 68.6 76.8 75.3 77.6 84. 86.3 72.6 21.7 70.1 69.2 69.9 72.4 72.6 80.1 73.5 70.3 20.7 71.7 47.8 72.4 72.4 74.8 77.6 78.3 73.0 59.8 68.5 62.5 69.8 73. 80.5 77.7 79.3 76.6 36.6 25.8 67.3 76.4 62.5 68.1 74.2 72.3 74.7 73.8 53.5 74.9 70.5 75.1 75.1 75.8 79.7 87.6 74.7 48.8 93.8 55.1 90.9 85.7 94.4 96.0 92.9 95. 98.7 90.4 97.1 77.7 96.4 95.1 89.1 97.1 96.4 92.9 54.0 74.9 77.7 82.0 70.1 79.1 83.5 77.0 85.2 85.7 78.0 78.2 77.8 80.4 82.8 89.4 86. 90.2 79.4 47.2 59.6 60.9 64.4 52.2 66.4 65.5 62.0 73.4 72.5 46.7 71.0 57.2 70.1 69.9 74.5 75.3 75.6 71.2 50.1 3.3 61.1 60.9 42.6 56.1 64.4 64. 72.5 70.5 31.9 66.6 51.1 66.6 61.6 70.7 76.2 74.7 71.2 36.9 65.9 65.9 67.3 55.9 70.5 75.5 69.0 78.0 77.1 56.1 74.2 67.0 75.1 74.5 80.2 79. 83.4 72.5 40.2 54.8 60.0 63.5 44.5 63.1 64.2 63.1 70.3 71.4 43.2 64.4 51.3 64.6 63.5 67.9 73.1 63.3 67.0 33.2 47.4 54.7 57.3 31.8 56. 62.0 42.2 54.2 54.2 42.0 50.1 48.7 54.3 55.0 59.0 67.2 66.7 54.2 42.8 53.4 64.2 71.8 56.8 68.8 72.9 69.3 75.3 75.3 45.8 72.1 62.8 72.6 72.6 76.2 79. 79.7 72.6 Table 6 Performance of instruct models on MBXP. 13 NCB (zh) NCB (en) Python Java Total Python Java Total 8B Models Model CodeLlama-7B-Instruct DeepSeek-Coder-6.7B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat Llama-3.1-8B-Instruct OpenCoder-8B-Instruct Qwen2.5-Coder-7B-Instruct Qwen3-8B Seed-Coder-8B-Instruct Stable-DiffCoder-8B-Instruct Size 7B 6.7B 7B 9B 8B 8B 7B 8B 8B 8B 18.6 38.6 30.0 41.4 27.1 40.0 34.3 37.1 55.7 51.4 8.6 31.4 28.6 45.7 24.3 30.0 37.1 32.9 45.7 45. 13.6 35.0 29.3 43.6 25.7 35.0 35.7 35.0 50.7 48.6 13B+ Models and External API StarCoder2-15B-Instruct Codestral-22B CodeLlama-70B-Instruct DeepSeek-Coder-33B-Instruct DeepSeek-Coder-V2-Lite-Instruct Qwen2.5-Coder-14B-Instruct Seed-Diffusion-Preview(0705) 15B 22B 70B 33B 2.4B/16B 14B - 44.3 40.0 35.1 44.3 41. 48.6 52.9 30.0 44.3 32.1 38.9 47.1 48.6 38.6 37.2 42.2 33.6 41.6 44.3 48.6 45.8 Total 14.6 33.9 25.7 42.5 24.3 32.5 35.4 35.7 49.6 48.6 39.0 42.9 32.6 43.0 41.8 46.4 43.9 17.1 32.9 30.0 38.6 22.9 35.7 34.3 34.3 50.0 50. 38.6 41.4 32.8 44.3 41.4 42.9 45.7 14.3 32.9 27.1 44.3 22.9 24.3 35.7 38.6 47.1 47.1 42.9 45.7 30.5 44.3 37. 45.7 38.6 15.7 32.9 28.6 41.5 22.9 30.0 35.0 36.5 48.6 48.6 40.8 43.6 31.7 44.3 39.3 44.3 42. Table 7 Performance of instruct models on NaturalCodeBench. Model Size Input-CoT Output-CoT 8B Models CodeLlama-7B-Instruct DeepSeek-Coder-6.7B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat Llama-3.1-8B-Instruct OpenCoder-8B-Instruct Qwen2.5-Coder-7B-Instruct Qwen3-8B Seed-Coder-8B-Instruct Stable-DiffCoder-8B-Instruct 7B 6.7B 7B 9B 8B 8B 7B 8B 8B 8B 13B+ Models StarCoder2-15B-Instruct Codestral-22B CodeLlama-70B-Instruct DeepSeek-Coder-33B-Instruct DeepSeek-Coder-V2-Lite-Instruct Qwen2.5-Coder-14B-Instruct 15B 22B 70B 33B 2.4B/16B 14B 36.1 42.6 44.0 47.5 35.6 39.9 65.8 73.8 63.3 62. 45.5 61.3 56.5 47.3 53.0 69.5 36.2 45.1 38.8 55.6 37.8 43.0 65.9 76.9 67.1 69.0 60.9 63.5 57.8 50.6 52.9 79. Table 8 Performance of instruct models on CRUXEval. 4.4.1 Code Generation Humaneval and MBPP. On standard function-level code generation benchmarks, Stable-DiffCoder-8B-Instruct achieves strong gains over the autoregressive baseline. As shown in Table 4, Stable-DiffCoder-8B-Instruct significantly improves upon Seed-Coder-8B-Instruct on both HumanEval(+) and MBPP(+), and on MBPP it outperforms all other instruction models while clearly surpassing all 8B-scale diffusion models. MHPP. On the more challenging MHPP benchmark, which only reveals scores through an official submission interface to mitigate data contamination and ensure fair comparison, Stable-DiffCoder-8B-Instruct attains the best performance among all compared models and reaches the level of Qwen2.5-Coder-32B-Instruct  (Table 5)  . 14 Model Size Aider tries=2 CanItEdit pass@1 8B Models CodeLlama-7B-Instruct DeepSeek-Coder-6.7B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat Llama-3.1-8B-Instruct OpenCoder-8B-Instruct Qwen2.5-Coder-7B-Instruct Qwen3-8B Seed-Coder-8B-Instruct Stable-DiffCoder-8B-Instruct 7B 6.7B 7B 9B 8B 8B 7B 8B 8B 8B 13B+ Models and External API StarCoder2-15B-Instruct Codestral-22B CodeLlama-70B-Instruct DeepSeek-Coder-33B-Instruct DeepSeek-Coder-V2-Lite-Instruct Qwen2.5-Coder-14B-Instruct Seed-Diffusion-Preview(0705) 15B 22B 70B 33B 2.4B/16B 14B - 1.5 44.4 38.3 54.1 33.1 30.8 57.9 55.6 57.1 54.9 38.2 51.1 15.0 54.5 52. 69.2 44.4 25.7 36.9 34.8 50.5 39.5 39.0 49.5 45.7 50.5 60.0 31.4 52.4 40.5 46.2 45.2 52.9 54.3 Table 9 Performance of instruct models on Aider (whole format) and CanItEdit. BigCodeBench. For more realistic programming tasks, BigCodeBench measures the ability to solve challenging, real-world coding problems with rich context and tool-like function calls. As shown in Table 5, Stable-DiffCoder8B-Instruct delivers substantial improvements over Seed-Coder-8B-Instruct and, among all models, is only surpassed by the much larger DeepSeek-Coder-V2-Instruct (21B/236B), demonstrating the strong practical code generation capabilities of Stable-DiffCoder-8B-Instruct. LiveCodeBench. To ensure fair comparison under identical data and training settings, we adopt exactly the same evaluation configuration as Seed-Coder-8B-Instruct on the LiveCodeBench (v5, 2024.10-2025.02). Here Stable-DiffCoder-8B-Instruct (23.5%) is slightly behind Seed-Coder-8B-Instruct (24.7%), but matches the performance of Qwen3-8B and remains stronger than other 8B-scale models  (Table 5)  . MBXP. We further evaluate multilingual code generation with MBXP. As summarized in Table 6, under the 10+ programming language setting Stable-DiffCoder-8B-Instruct achieves an overall average comparable to Seed-Coder-8B-Instruct, while attaining the highest scores in most languages among 8B instruct models. Due to the need to extensively supplement the scarce data such as C# and PHP that are lacking in pretraining during SFT, the advantage in multilingual coding capabilities has been reduced. NaturalCodeBench. On NaturalCodeBench, which targets practical software engineering problems in Python and Java, Stable-DiffCoder-8B-Instruct is slightly weaker than Seed-Coder-8B-Instruct on Chinese Python queries but is otherwise on par overall  (Table 7)  . Both models, however, are clearly ahead of other 8B models and even competitive with many 13B+ and external API systems. 4.4.2 Code Reasoning CRUXEval. For execution-centric code reasoning, we evaluate on CRUXEval. Consistent with the trends observed for base models, Table 8 shows that Stable-DiffCoder-8B-Instruct achieves stronger performance than Seed-Coder-8B-Instruct on the Output-CoT setting and slightly better average performance across Input-CoT and Output-CoT. Nonetheless, Qwen3-8B remains ahead on CRUXEval, indicating that there is still considerable headroom for specialized small-code models on fine-grained reasoning tasks."
        },
        {
            "title": "4.4.3 Code editing",
            "content": "CanItEdit. On CanItEdit, Stable-DiffCoder-8B-Instruct substantially outperforms all other models  (Table 9)  . We hypothesize that this gain benefits from the denoising nature of DLLMs: random masking and reconstruction inherently train the model on editand infill-like patterns, enabling it to better exploit editing supervision and extract more editing-related knowledge from the same data. Aider. On Aider, which evaluates multi-turn editing and reasoning over entire codebases, Stable-DiffCoder8B-Instruct remains slightly weaker than Seed-Coder-8B-Instruct under the tries=2 setting  (Table 9)  . This task requires concatenating outputs across turns, often yielding very long contexts that exceed the 8192-token window used during training, and we observe mild performance drop in this regime. Even so, StableDiffCoder-8B-Instruct reaches performance comparable to Qwen3-8B and surpasses larger models such as DeepSeek-Coder-33B-Instruct, indicating that it still offers strong practical code editing abilities despite this limitation."
        },
        {
            "title": "5 Conclusion, Limitation, and Future Work",
            "content": "In this report, we present Stable-DiffCoder, which serves as practical best-practice attempt demonstrating that the training paradigm of diffusion language models can provide effective data augmentation and lead to improved model performance. By analyzing how to efficiently enhance knowledge learning in DLLMs, designing well-aligned training pipeline, and incorporating techniques that stabilize and optimize the learning process, we show thatwhile keeping the architecture and data identical to Seed-Coderthe diffusion-based training procedure yields consistently better results. These findings further indicate that the sampling process of text diffusion models can function as principled and effective form of data augmentation for model training. On comprehensive code evaluation benchmarks, Stable-DiffCoder achieves state-of-the-art results among8B AR-based or diffusion-based code models. Since Stable-DiffCoder is primarily focused on the code domain and lacks large-scale training data from other areas, its performance on mathematical reasoning and general-purpose text tasks may be relatively limited. Whether text diffusion sampling can provide even greater benefits in broader domains remains an open question, requiring future model iterations and deeper empirical exploration."
        },
        {
            "title": "6 Contributions",
            "content": "Project Lead Chenghao Fan1,2 Core Contributor Chenghao Fan1,2 Contributor Wen Heng2, Bo Li2, Sichen Liu1, Yuxuan Song2 Jing Su2, Supervision Wen Heng2, Xiaoye Qu1, Kai Shen2, Wei Wei1, (Sorted Alphabetically) Affiliation 1School of Computer Science & Technology, Huazhong University of Science and Technology 2ByteDance Seed"
        },
        {
            "title": "7 Acknowledgments",
            "content": "We gratefully acknowledge and thank every Seed-LLM team member not explicitly mentioned above. We also acknowledge and thank every Seed team member for the valuable support. We thank Shulin Xin, Qi Liu, Yirong Chen, Zhexi Zhang, Ziwen Xu, Shen Nie, Hongrui Zhan, Shen Zheng for insightful technical discussions."
        },
        {
            "title": "References",
            "content": "[1] Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025. [2] Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, Sujan Kumar Gonugondla, Hantian Ding, Varun Kumar, Nathan Fulton, Arash Farahani, Siddhartha Jain, Robert Giaquinto, Haifeng Qian, Murali Krishna Ramanathan, Ramesh Nallapati, Baishakhi Ray, Parminder Bhatia, Sudipta Sengupta, Dan Roth, and Bing Xiang. Multi-lingual evaluation of code generation models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=Bo7eeXm6An8. [3] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. ArXiv preprint, abs/2108.07732, 2021. URL https://arxiv.org/abs/2108.07732. [4] Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces, 2023. URL https://arxiv.org/abs/2107.03006. [5] Tiwei Bie, Maosong Cao, Kun Chen, Lun Du, Mingliang Gong, Zhuochen Gong, Yanmei Gu, Jiaqi Hu, Zenan Huang, Zhenzhong Lan, et al. Llada2. 0: Scaling up diffusion language models to 100b. arXiv preprint arXiv:2512.15745, 2025. [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [7] Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda. MultiPL-E: scalable and polyglot approach to benchmarking neural code generation. IEEE Transactions on Software Engineering, 49(7):36753691, 2023. doi: 10.1109/TSE.2023.3267446. URL https://www.computer. org/csdl/journal/ts/2023/07/10103177/1MpWUtj7Rwk. [8] Federico Cassano, Luisa Li, Akul Sethi, Noah Shinn, Abby Brennan-Jones, Jacob Ginesin, Edward Berman, George Chakhnashvili, Anton Lozhkov, Carolyn Jane Anderson, and Arjun Guha. Can it edit? evaluating the ability of large language models to follow code editing instructions. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=D06yk3DBas. [9] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374. [10] Shuang Cheng, Yihan Bian, Dawei Liu, Linfeng Zhang, Qian Yao, Zhongbo Tian, Wenhai Wang, Qipeng Guo, Kai Chen, Biqing Qi, et al. Sdar: synergistic diffusion-autoregression paradigm for scalable sequence generation. arXiv preprint arXiv:2510.06303, 2025. [11] Jianbo Dai, Jianqiao Lu, Yunlong Feng, Dong Huang, Guangtao Zeng, Rongju Ruan, Ming Cheng, Haochen Tan, and Zhijiang Guo. MHPP: Exploring the capabilities and limitations of language models beyond basic code generation, 2024. URL https://arxiv.org/abs/2405.11430. [12] Arpan Dasgupta, Preeti Lamba, Ankita Kushwaha, Kiran Ravish, Siddhant Katyan, Shrutimoy Das, and Pawan Kumar. Review of extreme multilabel classification, 2025. URL https://arxiv.org/abs/2302.05971. [13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. 18 [14] Zitian Gao, Haoming Luo, Lynx Chen, Jason Klein Liu, Ran Tao, Joey Zhou, and Bryan Dai. What makes diffusion language models super data learners? arXiv preprint arXiv:2510.04071, 2025. [15] Aritra Ghosh, Naresh Manwani, and P. Shanti Sastry. Making risk minimization tolerant to label noise. ArXiv, abs/1403.3610, 2014. URL https://api.semanticscholar.org/CorpusID:8774197. [16] Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, et al. Scaling diffusion language models via adaptation from autoregressive models. arXiv preprint arXiv:2410.17891, 2024. [17] Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, and Yizhe Zhang. Diffucoder: Understanding and improving masked diffusion models for code generation. arXiv preprint arXiv:2506.20639, 2025. [18] Google DeepMind. Gemini diffusion. https://deepmind.google/models/gemini-diffusion, 2025. [19] Alex Gu, Baptiste Rozière, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida I. Wang. CRUXEval: benchmark for code reasoning, understanding and execution, 2024. URL https://arxiv.org/abs/2401.03065. [20] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. DeepSeek-Coder: When the large language model meets programming the rise of code intelligence, 2024. URL https://arxiv.org/abs/2401.14196. [21] Siming Huang, Tianhao Cheng, Jason Klein Liu, Weidi Xu, Jiaran Hao, Liuyihan Song, Yang Xu, Jian Yang, Jiaheng Liu, Chenchen Zhang, et al. Opencoder: The open cookbook for top-tier code large language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3316733193, 2025. [22] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [23] Daniel Israel, Tian Jin, Ellie Cheng, Guy Van den Broeck, Aditya Grover, Suvinay Subramanian, and Michael Carbin. Planned diffusion. arXiv preprint arXiv:2510.18087, 2025. [24] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=chfJJYC3iL. [25] Wonjun Kang, Kevin Galim, Seunghyuk Oh, Minjae Lee, Yuchen Zeng, Shuibai Zhang, Coleman Hooper, Yuezhou Hu, Hyung Il Koo, Nam Ik Cho, et al. Parallelbench: Understanding the trade-offs of parallel decoding in diffusion llms. arXiv preprint arXiv:2510.04767, 2025. [26] Jaeyeon Kim, Kulin Shah, Vasilis Kontonis, Sham Kakade, and Sitan Chen. Train for the worst, plan for the best: Understanding token ordering in masked diffusions. arXiv preprint arXiv:2502.06768, 2025. [27] Frederik Kunstner, Robin Yadav, Alan Milligan, Mark Schmidt, and Alberto Bietti. Heavy-tailed class imbalance and why adam outperforms gradient descent on language models, 2024. URL https://arxiv.org/abs/2402. 19449. [28] Inception Labs. Introducing mercury: new paradigm for fast language diffusion, 2025. URL https://www. inceptionlabs.ai/introducing-mercury. Accessed: 2025-04-05. [29] Chengze Li, Yitong Zhang, Jia Li, Liyi Cai, and Ge Li. Beyond autoregression: An empirical study of diffusion large language models for code generation. arXiv preprint arXiv:2509.11252, 2025. [30] Aiwei Liu, Minghua He, Shaoxun Zeng, Sijun Zhang, Linhao Zhang, Chuhan Wu, Wei Jia, Yuan Liu, Xiao Zhou, and Jie Zhou. Wedlm: Reconciling diffusion language models with standard causal attention for fast inference, 2025. URL https://arxiv.org/abs/2512.22737. [31] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [32] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36:2155821572, 2023. 19 [33] Jingyu Liu, Xin Dong, Zhifan Ye, Rishabh Mehta, Yonggan Fu, Vartika Singh, Jan Kautz, Ce Zhang, and Pavlo Molchanov. Tidar: Think in diffusion, talk in autoregression. arXiv preprint arXiv:2511.08923, 2025. [34] Yangzhou Liu, Yue Cao, Hao Li, Gen Luo, Zhe Chen, Weiyun Wang, Xiaobo Liang, Biqing Qi, Lijun Wu, Changyao Tian, et al. Sequential diffusion language models. arXiv preprint arXiv:2509.24007, 2025. [35] Llama Team. Introducing llama 3.1: Our most capable models to date. https://ai.meta.com/blog/ meta-llama-3-1/, July 2024. [36] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. In Forty-first International Conference on Machine Learning, 2024. [37] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. StarCoder 2 and The Stack v2: The next generation, 2024. URL https://arxiv.org/abs/2402.19173. [38] Yuxin Ma, Lun Du, Lanning Wei, Kun Chen, Qian Xu, Kangyu Wang, Guofeng Feng, Guoshan Lu, Lin Liu, Xiaojing Qi, et al. dinfer: An efficient inference framework for diffusion language models. arXiv preprint arXiv:2510.08666, 2025. [39] Chenlin Meng, Kristy Choi, Jiaming Song, and Stefano Ermon. Concrete score matching: Generalized score matching for discrete data. Advances in Neural Information Processing Systems, 35:3453234545, 2022. [40] Amin Karimi Monsefi, Nikhil Bhendawade, Manuel Rafael Ciosici, Dominic Culver, Yizhe Zhang, and Irina Belousova. Fs-dfm: Fast and accurate long text generation with few-step diffusion language models. arXiv preprint arXiv:2509.20624, 2025. [41] Jinjie Ni, Qian Liu, Longxu Dou, Chao Du, Zili Wang, Hang Yan, Tianyu Pang, and Michael Qizhe Shieh. Diffusion language models are super data learners. arXiv preprint arXiv:2511.03276, 2025. [42] Jinjie Ni, Qian Liu, Chao Du, Longxu Dou, Hang Yan, Zili Wang, Tianyu Pang, and Michael Qizhe Shieh. Training optimal large diffusion language models. arXiv preprint arXiv:2510.03280, 2025. [43] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models, 2025. URL https://arxiv.org/abs/2502.09992. [44] Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data, 2025. URL https: //arxiv.org/abs/2406.03736. [45] Qwen Team. Code with CodeQwen1.5, April 2024. URL https://qwenlm.github.io/blog/codeqwen1.5/. [46] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code Llama: Open foundation models for code, 2024. URL https://arxiv.org/abs/2308.12950. [47] Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models, 2024. URL https://arxiv.org/abs/2406.07524. [48] Dale Schuurmans, Hanjun Dai, and Francesco Zanini. Autoregressive large language models are computationally universal. arXiv preprint arXiv:2410.03170, 2024. [49] ByteDance Seed, Yuyu Zhang, Jing Su, Yifan Sun, Chenguang Xi, Xia Xiao, Shen Zheng, Anxiang Zhang, Kaibo Liu, Daoguang Zan, et al. Seed-coder: Let the code model curate data for itself. arXiv preprint arXiv:2506.03524, 2025. [50] Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37:103131103167, 2024. [51] Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, et al. Seed diffusion: large-scale diffusion language model with high-speed inference. arXiv preprint arXiv:2508.02193, 2025. [52] Haocheng Sun, Cynthia Xin Wen, and Edward Hong Wang. Why mask diffusion does not work. arXiv preprint arXiv:2510.03289, 2025. [53] Yuchuan Tian, Yuchen Liang, Jiacheng Sun, Shuo Zhang, Guangwen Yang, Yingte Shu, Sibo Fang, Tianyu Guo, Kai Han, Chao Xu, et al. From next-token to next-block: principled adaptation path for diffusion llms. arXiv preprint arXiv:2512.06776, 2025. [54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [55] Dimitri von Rütte, Janis Fluri, Omead Pooladzandi, Bernhard Schölkopf, Thomas Hofmann, and Antonio Orvieto. Scaling behavior of discrete diffusion language models. arXiv preprint arXiv:2512.10858, 2025. [56] Chengyue Wu, Hao Zhang, Shuchen Xue, Shizhe Diao, Yonggan Fu, Zhijian Liu, Pavlo Molchanov, Ping Luo, Song Han, and Enze Xie. Fast-dllm v2: Efficient block-diffusion llm. arXiv preprint arXiv:2509.26328, 2025. [57] Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding, 2025. URL https://arxiv.org/abs/2505.22618. [58] Zirui Wu, Lin Zheng, Zhihui Xie, Jiacheng Ye, Jiahui Gao, Yansong Feng, Zhenguo Li, Victoria W., Guorui Zhou, and Lingpeng Kong. Dreamon: Diffusion language models for code infilling beyond fixed-size canvas, 2025. URL https://hkunlp.github.io/blog/2025/dreamon. [59] Zhihui Xie, Jiacheng Ye, Lin Zheng, Jiahui Gao, Jingwei Dong, Zirui Wu, Xueliang Zhao, Shansan Gong, Xin Jiang, Zhenguo Li, et al. Dream-coder 7b: An open diffusion language model for code. arXiv preprint arXiv:2509.01142, 2025. [60] Chenkai Xu, Yijie Jin, Jiajun Li, Yi Tu, Guoping Long, Dandan Tu, Tianqi Hou, Junchi Yan, and Zhijie Deng. Lopa: Scaling dllm inference via lookahead parallel decoding. arXiv preprint arXiv:2512.16229, 2025. [61] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [62] Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025. [63] Shudan Zhang, Hanlin Zhao, Xiao Liu, Qinkai Zheng, Zehan Qi, Xiaotao Gu, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. NaturalCodeBench: Examining coding performance mismatch on humaneval and natural user prompts, 2024. URL https://arxiv.org/abs/2405.04520. [64] Xiong Zhou, Xianming Liu, Deming Zhai, Junjun Jiang, and Xiangyang Ji. Asymmetric loss functions for noisetolerant learning: Theory and applications. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45: 80948109, 2023. URL https://api.semanticscholar.org/CorpusID:256687692. [65] Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Llada 1.5: Variance-reduced preference optimization for large language diffusion models, 2025. URL https://arxiv.org/abs/2505.19223. [66] Fengqi Zhu, Zebin You, Yipeng Xing, Zenan Huang, Lin Liu, Yihong Zhuang, Guoshan Lu, Kangyu Wang, Xudong Wang, Lanning Wei, et al. Llada-moe: sparse moe diffusion language model. arXiv preprint arXiv:2509.24389, 2025. [67] Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024. [68] Terry Yue Zhuo, Vu Minh Chien, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen GONG, James Hoang, Armel Randy 21 Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, and Leandro Von Werra. BigCodeBench: Benchmarking code generation with diverse function calls and complex instructions. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=YrycTjllL0."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Huazhong University of Science and Technology"
    ]
}