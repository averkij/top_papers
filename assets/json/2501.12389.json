{
    "paper_title": "Taming Teacher Forcing for Masked Autoregressive Video Generation",
    "authors": [
        "Deyu Zhou",
        "Quan Sun",
        "Yuang Peng",
        "Kun Yan",
        "Runpei Dong",
        "Duomin Wang",
        "Zheng Ge",
        "Nan Duan",
        "Xiangyu Zhang",
        "Lionel M. Ni",
        "Heung-Yeung Shum"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce MAGI, a hybrid video generation framework that combines masked modeling for intra-frame generation with causal modeling for next-frame generation. Our key innovation, Complete Teacher Forcing (CTF), conditions masked frames on complete observation frames rather than masked ones (namely Masked Teacher Forcing, MTF), enabling a smooth transition from token-level (patch-level) to frame-level autoregressive generation. CTF significantly outperforms MTF, achieving a +23% improvement in FVD scores on first-frame conditioned video prediction. To address issues like exposure bias, we employ targeted training strategies, setting a new benchmark in autoregressive video generation. Experiments show that MAGI can generate long, coherent video sequences exceeding 100 frames, even when trained on as few as 16 frames, highlighting its potential for scalable, high-quality video generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 9 8 3 2 1 . 1 0 5 2 : r a"
        },
        {
            "title": "Taming Teacher Forcing for Masked Autoregressive Video Generation",
            "content": "Deyu Zhou1 Quan Sun2 Yuang Peng2, 4 Kun Yan2 Runpei Dong3 Duomin Wang2 Zheng Ge2 Nan Duan2 Xiangyu Zhang2 Lionel M. Ni1 Heung-Yeung Shum5 1HKUST(GZ) 2StepFun 3UIUC 4THU 5HKUST MAGI-VIDEO-GENERATION.GITHUB.IO"
        },
        {
            "title": "Abstract",
            "content": "We introduce MAGI, hybrid video generation framework that combines masked modeling for intra-frame generation with causal modeling for next-frame generation. Our key innovation, Complete Teacher Forcing (CTF), conditions masked frames on complete observation frames rather than masked ones (namely Masked Teacher Forcing, MTF), enabling smooth transition from token-level (patch-level) to frame-level autoregressive generation. CTF significantly outperforms MTF, achieving +23% improvement in FVD scores on first-frame conditioned video prediction. To address issues like exposure bias, we employ targeted training strategies, setting new benchmark in autoregressive video generation. Experiments show that MAGI can generate long, coherent video sequences exceeding 100 frames, even when trained on as few as 16 frames, highlighting its potential for scalable, high-quality video generation. 1. Introduction Generating order matters in autoregressive image generation. While existing approaches mostly reply on simple raster-scan order, recent studies [8, 19] have demonstrated superior results through alternative generation strategies. These include randomized spatial order with bidirectional attention [4, 19, 46] and generation along increased scales [5, 8]. Despite its fundamental importance, in the realm of autoregressive video generation, however, the discussion of generation order has been largely overlooked. Existing autoregressive video generation methods can be categorized into two groups according to the prediction granularity, as shown in Tab. 1. The first group, exemplified by MAGViT [46], adopts masked modeling approach* where visual tokens are generated in descending order according to their logit probabilities. While this approach uses bi-directional attention across frames, it presents two signif- *We follow the definition in MAR [19], which considers masked image generation as form of auto-regressive generation with certain order. icant limitations: substantial computational overhead during inference since it is unable to utilize KV Cache [9, 20], and neglect temporal causality between consecutive frames. These oversights are particularly noteworthy given the remarkable success of causal modeling in large language models. Some other methods in this group [2, 6] have explored frame-level prediction with casual temporal attention. However, these methods are limited due to incomplete history observation, which causes an intrinsic traininginference gap. The second group comprises fully autoregressive approaches operating on visual patches [17, 42, 44, 45], exemplified by recent works like VideoPoet [17] and Emu3 [42]. Despite their successful temporal modeling, these methods have not incorporated contemporary advances from image generation research in intra-frame generation. Instead, they continue to rely on raster-scan order - an approach demonstrated sub-optimal in image generation [19]. In this paper, we introduce Masked Autoregressive video GeneratIon (MAGI), hybrid framework that synergizes the strengths of both video generation paradigms by combining masked modeling for intra-frame generation with causal modeling for inter-frame dependencies. We start with naive implementation. This initial implementation explores straightforward approach where tokens in each next frame are generated conditioned on previous masked frames (Figure 1(a)). While this can be viewed as natural extension of MaskGIT [4] and Muse [5] to autoregressive video generation, our analysis reveals fundamental limitation in its implementation of teacher forcing (TF) [43] - crucial training technique in autoregressive models for avoiding exposure bias and improving scalability. To be specific, TF replaces predicted tokens with ground-truth tokens during training. However, our initial approach, which we term as Masked Teacher Forcing (MTF), deviates from this principle by substituting ground-truth tokens with mask tokens. Despite similar approaches being explored in interactive generative models (e.g., Genie [2]), we identify critical training-inference discrepancy: during training, next-frame generation is conditioned on mixture of visible 1 Table 1. Comparison of autoregressive paradigms across different methods. We compare MAGI with various methods based on temporal attention mechanisms (how each frame attends to others), support for KV Cache [20] and variable context lengths, observation completeness (the completeness of the context frames used for conditioning), and the prediction granularity (patch or frame). : MAGVIT predicts masked tokens of multiple frames simultaneously. Genie [2] and MAGI predict multiple tokens of single frame simultaneously. Prediction Granularity Method Temporal Attention KV Cache Var. Context Observation Patch Frame VideoGPT [45], Phenaki [39], Omni [41] Causal MAGVIT [46], GameNGen [36] Diffusion Forcing [6] Genie [2] MAGI (with CTF) Bidirectional Causal Causal Causal Complete Complete Noisy Masked Complete and masked tokens, while during inference, it relies solely on complete visual tokens from previous frames. Our experiments demonstrate that this inconsistency compromises generation quality, especially motion coherency. To address these limitations, we propose Complete Teacher Forcing (CTF), an advanced paradigm that maintains training-test consistency by conditioning next-frame generation on complete visual observations from history frames during training. This is achieved through novel mechanism that prepends complete video tokens to the masked input sequence and employs carefully designed attention mask. Empirical results show that CTF better captures motion, outperforming MTF by 23% in terms of FVD scores on first-frame conditioned video prediction. In addition, we tackle the persistent challenge of exposure bias [28] in autoregressive video generation through two complementary strategies: dynamic interval training and noise injection [36]. Dynamic interval training refers to randomly sampling frames with varying intervals during training, which introduces diversity into the data distribution and helps the model to better generalize to different temporal frequencies. Dynamic noise injection, on the other hand, involves adding random noise to observation frames during training, which helps improve the models robustness by simulating the possible errors may occur in inference. Combined with these two techniques, MAGI establishes robust baseline for autoregressive video generation. Through extensive evaluations, we demonstrate that MAGI achieves superior video generation quality and length scalability. For example, MAGI can generate coherent video sequences that exceed 100 frames, even when trained on sequences as short as 16 frames. This underscores the potential of our approach for scalable video generation. 2. Background and Problem Statement 2.1. Autoregressive Video Generation Given video RN HW 3, the sequence of frames with height and width can be denoted as = {fi RHW 3i = 1, . . . , }. Each frame can be further divided into patches, represented as {x11, x12, . . . , x1m, . . . , xn1, xn2, . . . , xnm}. Autoregressive video generation models primarily differ in the granularity of causality (patch-level vs. frame-level) and the training paradigm (teacher forcing), discussed below. Patch-level Methods Patch-level autoregressive models [42, 45] operate by modeling the video on patch-bypatch basis. The likelihood of generating entire video with model parameterized by θ is decomposed into the product of posterior of each patch conditioned all preceding patches: p(V ) = (cid:89) t=1 p(xt x1, x2, . . . , xt1; θ), (1) where pt represents patch in predefined ordering. The model generates each patch sequentially by conditioning on all previously generated patches, capturing both spatial and temporal dependencies. Frame-level Methods Frame-level autoregressive models [2, 36] model the video at the granularity of entire frames. Similarly, the likelihood of video modeling is: p(V ) = (cid:89) t=1 p(ft f1, f2, . . . , ft1; θ), (2) where ft is the entire frame generated conditioned on all history frames. In this fashion, the patches in each frame are generated in parallel. Teacher Forcing Teacher forcing [43] is widely adopted training strategy in autoregressive models, where the model conditions on the true previous frame or patch rather than its own predictions [10, 43]. This method is commonly used in patch-level autoregressive video models [17, 41, 44, 45] as well as in language models [24, 25], where the model is supervised through next token or patch prediction, effectively shifting one token or patch at time during training. 2.2. Can we achieve frame-level teacher forcing? While teacher forcing is effective in autoregressive language models [24, 25], its application to frame-level video Few works adopt generation is largely unexplored. straightforward shift one frame paradigm for frame-level video models. 2 Figure 1. Conceptual comparison of masked teacher forcing and our proposed complete teacher forcing mechanisms in autoregressive video generation. The blocks here illustrate individual frames. These two mechanisms differ in how history observation frames are used to condition next frame generation during training. (a): The observation frames are masked frames [2, 12] that are partially observable. (b): Every frame is generated conditioned on fully observable history frames. Instead, existing methods often rely on masked frame prediction, where masked frames are conditioned on other frames. For example, Genie [2] extends MaskGIT [4] to video generation by conditioning masked frames on other masked teacher framesa strategy we refer to as Masked Teacher Forcing (MTF). Similarly, Diffusion Forcing [6] introduces temporal causal attention with novel noise strategy but conditions each frame on noisy teacher frames, further diverging from traditional teacher forcing. GameNGen [36] employs bidirectional diffusion models with binary mask tokens to represent masked and conditional frames; however, it relies on fixed-length conditional frames, limiting autoregressive flexibility and preventing the use of efficient mechanisms like KV Cacheone notable advantage of traditional autoregressive methods. Generating videos autoregressively at the patch level is also suboptimal because inter-frames lack clear raster-scan spatial causality. In contrast, videos exhibit strong temporal causality, suggesting that predicting videos frame by frame, while generating the tokens within each frame in parallel, is more effective. To address these issues, MAGI fully leverages teacher forcing for frame-level autoregressive video generation, enabling more flexible, efficient, and scalable video generation. 3. Approach Existing frame-level autoregressive video generation models, such as GameNGen [36], typically use binary masks to distinguish between frames used for prediction and those used for conditioning. These models are trained to predict future frames conditioned on fixed-length sequence of preceding frames. This fixed conditioning contrasts with 3 the greater flexibility of autoregressive language models, In this which condition on prefixes of variable length. work, we leverage MAR [19], state-of-the-art masked image generation framework, and investigate crucial design choices for masked autoregressive video generation. Our focus includes exploring different autoregressive paradigms and methods for mitigating exposure bias and error accumulation during autoregressive inference. Masked Teacher Forcing vs. Complete Teacher Forcing. natural extension of masked image generation [4, 19] to autoregressive video prediction is to modify the attention mechanism [2], incorporating causal temporal attention during the training phase for masked frames. We refer to this approach as Masked Teacher Forcing (MTF), as illustrated on the right of Fig. 1. However, this method introduces significant training-inference gap: during training, the model attends to high mask ratio, while at inference time, it must predict future frames conditioned on previously generated unmasked ones. This mismatch between training and inference undermines the models ability to generate realistic videos, ultimately limiting its performance. The formal formulation of MTF is as follows: p(f m 1 , 2 , . . . , j1; θ), {1, 2, . . . , n}. (3) 1 , 2 , . . . , where represents the j-th masked frame, while the frames j1 are the previously predicted masked frames. In this formulation, during training, each masked frame attends to previously masked frames and itself. However, the approach introduces significant gap between the training and the inference phase, because the model must predict masked frames conditioned on previously unmasked generated ones at the inference stage. Figure 2. Overview of MAGI video generation framework. MAGI receives observation frames and corresponding masked frames as inputs, enabling autoregressive video generation with Complete Teacher Forcing (Sec. 3). To address this issue and more effectively bridge the gap between training and inference, we propose novel autoregressive paradigm, termed Complete Teacher Forcing (CTF), as shown on the left of Fig. 1. Unlike MTF, CTF conditions on unmasked observed frames during training, predicting each masked frame. Formally, let the observation frames be denoted as {f1, f2, . . . , fn}, where is the number of frames. The autoregressive formulation for predicting the j-th masked frame, , is conditioned on both the previous unmasked observation frames and itself, i.e., p(f f1, f2, . . . , fj1; θ), {1, 2, . . . , n}, (4) where p(f ; θ) is the autoregressive model. Note that is conditioned only on itself, which resembles image 1 generation. For all > 1, attends to both the previj ous unmasked observation frames and itself, making this approach consistent between training and inference. Addressing Exposure Bias & Error Accumulation Although CTF reduces the gap between training and inference, it still faces challenges related to exposure bias [50] and error accumulation [18]. Specifically, the model may struggle to generate realistic video sequences during inference when relying on self-predicted frames. To mitigate these issues, we introduce two strategies: Dynamic interval training. Video clips are sampled with random frame intervals, which forces the model to learn longer temporal dependencies and larger motion ranges, thus improving prediction stability for long videos. However, we find that vanilla usage of this strategy leads to videos generated with uncontrollable motion range during inference. For example, prediction with random motion range leads to unsatisfactory video generation results on UCF-101 that requires generation in 25 FPS. To support controllable generation to handle varying userspecified frame intervals, we introduce learnable interval Figure 3. Temporal attention mask in our CTF during training. The attention within observation frames causal, while the attention within masked frames are atrous. In this fashion, each masked frame attends to itself and unmasked history observation frames. During inference, standard causal mask is employed, where each frame attends only to previously generated frames. embeddings which encodes different intervals into specific embeddings like positional encoding [38]. Specifically, the x-th interval embedding will be added to hidden states when sampling with an interval which enables the models awareness of the desired generation interval. Dynamic noise injection. As pointed out by Valevski et al. [36], there is domain shift caused by teacher forcing and auto-regressive modeling, and thus data corruption using an injected noise during training is useful. Inspired by this practice, we also adopt dynamic noise injection strategy. In addition, similar to interval embeddings, learnable noise level embedding is concatenated with hidden states as the model inputs. Both techniques are crucial for improving the robustness of CTF by ensuring better generalization and stability during inference. 4. Architecture We introduce MAGI, as shown in Fig. 2, novel family of masked autoregressive video generation models, which incorporates the techniques outlined above. We implement MAGI with Transformer architecture, detailed as follows. Transformer Decoder & Temporal Attention MAGI employs stack of spatial-temporal Transformer blocks consisting of interleaved 2D spatial attention layers and 1D temporal attention layers. As shown in Fig. 2, we prepend the complete observation frames with masked frames as the input of our Transformer. For CTF, we design special temporal attention mask, as shown in Fig. 3. In the temporal layer, each frame only attends to itself and its preceding observation frames. The observation frames only attend to previous frames. Thus, the attention paradigm of CTF is consistent between training and inference. 4 Diffusion Head [19] Atop the Transformer decoder, we stack multi-layer perceptron (MLP) layers as the diffusion head, following MAR [19]. This component predicts masked tokens through denoising diffusion procedure [14], enhancing the models capacity for autoregressive generation. Learnable Positional Embeddings To distinguish between masked and unmasked frames, we introduce two distinct learnable positional embeddings: one for the masked frames and another for the observation frames. Additionally, we adopt learnable positional embedding for the diffusion head, as proposed in MAR [19]. Spatial and Temporal Positional Embeddings We utilize sinusoidal positional embeddings [38] for both 2D spatial and 1D temporal encodings, ensuring that the model effectively captures spatiotemporal relationships across frames. Frame Interval Embeddings As part of our dynamic interval training strategy, we introduce learnable interval embedding, with vocabulary length of 25, which covers frame intervals ranging from 1 to 25. This embedding allows the model to capture long-range temporal dependencies across varying frame intervals. Noise Level Embeddings To support dynamic noise injection, we incorporate random Gaussian noise into the observation frames during training, following GameNGen [36]. We use noise level with range of [1, 5], which is encoded through learnable vocabulary embedding, with dimension the same as the Transformers hidden dimension. This enables the model to adapt to different noise levels during the denoising process. 5. Experiments 5.1. Experimental Setup Datasets For the five-frame conditional video prediction tasks, we use the Kinetics-600 dataset [3], which consists of 480,000 videos spanning 600 action categories. For unconditional and first-frame conditional video generation, we utilize the UCF-101 dataset [31], containing over 13,000 clips across 101 human action classes and with totally 27 hours of recording length. Implementation Details We set the learning rate to 2 104 for Kinetics-600 and 1 104 for UCF-101. Our largest model is trained for 150 epochs on Kinetics-600 and 1,400 epochs on UCF-101, with batch sizes of 256 and 128, respectively. Other hyperparameters follow the settings in MAR [19]. For Kinetics-600, we use the 3D-VAE from OmniTokenizer [41]. For UCF-101, we use the 2D-VAE from Stable Diffusion 1.4 [27] and the 3D-VAE from Cosmos [23]. During training, we sample 17 frames for the 3D-VAE and 16 frames for the 2D-VAE, respectively. All models are trained at resolution of 256 256. Inference and Evaluation For inference, we follow the strategy outlined in MAR [19], using 64 iterative steps for masked frame prediction per frame. The inference procedure is significantly accelerated using KV Cache. For evaluation on Kinetics-600, we generate 50,000 videos using context frames randomly sampled from the test set and compute the Frechet Video Distance (FVD) [35] against the ground-truth videos, both resized to 64 64 resolution. For UCF-101, we follow the evaluation protocol of previous works [21, 41]. We randomly sample 2,048 videos and compute the FVD against 2,048 randomly selected groundtruth videos from the dataset. 5.2. Teacher Forcing Matters in Autoregressive"
        },
        {
            "title": "Video Generation",
            "content": "CTF Outperforms MTF Our CTF significantly outperforms MTF by nearly 23% on first-frame conditioned video prediction. As shown in Fig. 5b, although MTF achieves slightly better frame-wise FID, it results in substantially worse overall FVD compared to CTF. Per-frame analysis (Fig. 4c-d) reveals that CTF better models temporal motion, while MTF generates high-quality static images lacking temporal coherence. Why Does CTF Achieve Superior FVD? We argue that although MTFs training on highly masked observations (using an optimal mask ratio of 70%100%) is beneficial for generating visually similar frames and achieving lower FID scores, it imposes significant limitation. Because MTF is trained to predict frames with minimal past information, it cannot effectively utilize full observations during inference. In contrast, CTF, trained on full observations, captures motion more accurately, leading to superior FVD scores. 5.3. Addressing Exposure Bias Training Strategies Matters An ablation study (Fig. 5c) demonstrates the importance of our proposed training strategies: Removing either or both dynamic interval training and dynamic noise injection significantly degrades CTFs FVD and FID, underscoring the exposure bias in autoregressive prediction. Combining both yields the best performance, demonstrating their synergistic effect. Does MTF Also Benefit from These Training Strategies? These training strategies prove beneficial not only for CTF but also for MTF. While we focus our quantitative analysis on CTF and omit the full results for MTF due to space limitations, similar trends of performance improvement were observed. Notably, even when MTF benefits from these strategies, CTF consistently achieves superior results (see Fig. 5b), demonstrating its inherent advantages. Figure 4. Case Study of Proposed Training Techniques: This figure evaluates the impact of dynamic interval training and dynamic noise injection on CTF and MTF by: 1) visualizing CTF with and without these strategies; and 2) comparing CTF and MTF when both use them. All methods perform first-frame conditional video prediction on UCF-101 [31]. The results demonstrate the efficacy of the proposed training strategies and the superior motion and temporal coherence of CTF. 5.4. Benchmarking with Previous Methods Video Prediction We evaluate our model trained on Kinetics-600 [3] against existing non-autoregressive (NAR) and autoregressive (AR) methods (Tab. 2). Our method, MAGI, achieves an FVD score of 11.5, establishing new state-of-the-art among AR models and significantly outperforming the patch-level AR method Omni [41]. Notably, our MAGI achieves -21.4 significantly lower FVD than Omni whose FVD is 32.9. This result demonstrates the effectiveness of frame-level autoregressive modeling with Complete Teacher Forcing. Examples of generated videos are provided in Appendix Fig. 7. Unconditional Video Generation For unconditional video generation on UCF-101 [31] (Tab. 3), our method achieves state-of-the-art results among AR models, outperforming Latte [21], DiT-based NAR model, by approximately 50 FVD points using the same VAE. Furthermore, using stronger VAE (Cosmos [23]), our method becomes competitive with state-of-the-art NAR methods. 5.5. Further Analysis Tab. 1 outlines the core differences between MAGI and other methods. MAGI achieves smooth transition from patch-level to frame-level autoregressive generation while retaining key advantages, analyzed below. 6 (a) Speed comparison. (b) MTF v.s. CTF. (c) Ablation on first-frame conditional generation. Figure 5. a) Speed of generation process with varying numbers of frames. MAGI achieves significant speedup by utilizing KV Cache. b) Masked Teacher Forcing (MTF) v.s. Complete Teacher Forcing (CTF). Both methods utilize the proposed training strategies dynamic interval training and dynamic noise injection. We report FID scores for individual frames and FVD scores for all frames on UCF101 [31]. CTF achieves significantly better FVD scores but slightly worse FID scores compared to MTF. This result demonstrates that CTF better captures motion with temporal coherence, even when the quality of individual frames is lower. c) The results of ablation study on first-frame conditional video predcition of UCF-101 [31]. The star indicates the FVD scores of each method with the same color. Table 2. Video Prediction on Kinetics-600 [3]. The results are evaluated on the testset on Kinetics-600. We report the FVD64, 50K, obtained from 50K samples in the resolution of 64 64. NAR: Non-autoregressive methods. AR: Autoregressive methods. Table 3. Unconditional video generation on UCF-101 [31]. We report the FVD256, 2048 which is the FVD obtained from 2048 samples in the resolution of 256 256. Type Method VAE FVD256, 2048 Type Method VAE FVD64, 50K NAR Video Diffusion [15] NAR NAR NAR AR AR AR RIN [16] MAGVIT [46] MAGVIT-v2 [47] ViVQVAE [40] Phenaki [39] Omni [41] - - MAGVIT [46] MAGVIT-v2 [47] VQVAE [37] VQVAE [37] Omni [41] AR MAGI Omni [41] 16.2 10.8 9.9 4.3 64.3 36.4 32.9 11.5 Generation Order Matters Our experiments (Fig. 5b and 5c) demonstrate that single-frame FID increases as the number of AR steps grows, regardless of the training paradigms and strategies employed. This finding underscores the importance of generation order in autoregressive video generation. Since frame-level generation requires fewer AR steps than patch-level generation, it can better preserve image quality over long sequences by mitigating error accumulation. Moreover, fewer autoregressive steps reduce the propagation of errors that typically accumulate in sequential generation processes, leading to more coherent and stable video outputs. This efficiency not only enhances visual fidelity but also contributes to faster inference times, which are critical for practical applications. KV Cache key practical advantage of MAGI is its efficient inference. MAGIs frame-level KV Cache enables approximately linear scaling of inference time with the number of generated frames (Fig. 5a), significant advantage over the parallel computation required by NAR methods. MoCoGAN [34] NAR NAR MoCoGAN-HD [33] NAR NAR NAR NAR NAR NAR NAR AR DIGAN [48] StyleGAN-V [30] PVDM [49] MoStGAN-V [29] Latte [21] DiM [22] Matten [32] VideoGPT [45] AR AR MAGI MAGI - - - - PVDM [49] - SD1.4 [27] SD1.4 [27] SD1.4 [27] 3D VQVAE [37] SD1.4 [27] Cosmos [23] 2886.9 1729.6 1630.2 1431.0 1141.9 1380.3 477.9 358.8 210.6 2880.6 420.6 297.8 Practical Advantages of Autoregressive Generation To explore this, we train MAGI on UCF-101 [31] using randomly sampled 16-frame clips and evaluate its long-term prediction capability in first-frame conditional generation setting. Fig. 6 demonstrates the promising predictive performance of MAGI, achieving reasonable results for sequences up to 100 frames in specific scenarios, particularly those with relatively static backgrounds and simple object motions. We observe performance degradation for videos with non-periodic motions (e.g., diving), which we attribute, in part, to the simplicity of the UCF-101 dataset and the challenging extrapolation required by the first-frame conditional setting. Specifically, for non-periodic actions like diving, the model lacks cues to predict subsequent actions once the primary action is complete, rendering metrics like FVD inappropriate. Therefore, we focus on demonstrating MAGIs long-range prediction capability in these specific scenarios to showcase its potential for capturing long-range temporal dependencies and motivate further investigation. Figure 6. Long-term Video Prediction. MAGI predicts over 100 frames from single input frame, maintaining reasonable motion even when trained on only 16 frames. 6. Related Work Autoregressive Visual Generation Autoregressive language modeling [24, 25] has propelled the development of visual content generation using discrete visual tokens [37]. In this framework, pre-trained visual tokenizers like VQVAE [37] map visual patches into discrete latent space, allowing visual generation to be approached similarly to language modeling. Early works such as DALL-E [26] focus on text-to-image generation by learning joint distribution between text and discrete image representations using an autoregressive cross-entropy loss. Concurrently, VideoGPT [45] extends this idea to video generation, employing discrete tokens for autoregressive video prediction. VideoPoet [17] further advances this approach by integrating causal video tokenizer, MAGVIT-v2 [47], for multimodal video generation. OmniTokenizer [41] proposes unified tokenizer for both discrete and continuous representations, enabling patch-level autoregressive video generation. In contrast, our work focuses on frame-level causality rather than patch-level, avoiding the limitations of rasterscan order and outperforming patch-level baselines. Masked and Diffusion Video Generation Diffusion models have recently gained prominence in visual generation tasks [7, 11, 13, 14, 27], extending effectively to video generation. Video diffusion models [1, 15] employ bidirectional attention and binary mask embeddings to enable frame-level autoregressive prediction. Notable works such as GameNGen [36] use bidirectional diffusion models for real-time game generation. However, due to their bidirectional nature, these models cannot leverage KV Cache for extended video generation, limiting their scalability. Several masked video generators, such as Genie [2], extend MaskGIT [4] into causal-attention-based architecture for video generation. Despite their advantages, these methods suffer from the training-inference gap inherent in masked autoregressive modeling, which negatively impacts generation quality. In contrast, our approach fully leverages KV Cache during inference, facilitated by our training paradigm that bridges the training-inference gap through novel Complete Teacher Forcing paradigm. Addressing Exposure Bias for Autoregressive Video Generation Autoregressive models often suffer from exposure bias [50] and error accumulation [18], which degrade performance over long sequences. To mitigate these issues, we adopt noise injection from GameNGen [36]where Gaussian noise is added to observation frame latents during training. Additionally, we introduce dynamic interval training, exposing the model to frames with varying temporal intervals to enhance generalization. As shown in Fig. 4, both strategies individually improve robustness, and their combination yields even greater performance gains in autoregressive video generation. 7. Conclusion We present MAGI, hybrid framework that combines masked and causal modeling to achieve efficient and scalable video generation. We identify that the teacher forcing paradigm makes significant differenceour Complete Teacher Forcing (CTF) approach effectively bridges the training-inference gap inherent in Masked Teacher Forcing (MTF). Additionally, we introduce essential training strategies to alleviate exposure bias. Through comprehensive experiments, we validate the effectiveness of each component. Our final model achieves state-of-the-art performance among autoregressive video generation methods. We also demonstrate the generation of long, coherent video sequences exceeding 100 frames from training sequences as short as 16 frames, highlighting MAGIs potential as scalable autoregressive video generation model."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank Tianhong Li for his insightful suggestions, invaluable help, and exceptional support. We would also like to thank Chenfei Wu, Haoyang Huang, Guoqing Ma, Hongyu Zhou, Liangyu Chen, Chunrui Han, Yimin Jiang and Yu Deng for their constructive discussions and advice, which greatly improved this work."
        },
        {
            "title": "References",
            "content": "[1] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 8 [2] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. 1, 2, 3, 8 [3] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. short note about kinetics600. arXiv preprint arXiv:1808.01340, 2018. 5, 6, 7 [4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. 1, 3, 8 [5] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, and Dilip Krishnan. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. 1 [6] Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. arXiv preprint arXiv:2407.01392, 2024. 1, 2, 3 [7] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. [8] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. 1 [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive KV cache compression for llms. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 1 [10] Anirudh Goyal, Alex Lamb, Ying Zhang, Saizheng Zhang, Aaron C. Courville, and Yoshua Bengio. Professor forcIn ing: new algorithm for training recurrent networks. Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 4601 4609, 2016. 2 [11] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 8 [12] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross B. Girshick. Masked autoencoders In IEEE/CVF Conference on are scalable vision learners. Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1597915988. IEEE, 2022. [13] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity arXiv preprint video generation with arbitrary lengths. arXiv:2211.13221, 2(3):4, 2022. 8 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 5, 8 [15] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 7, 8 [16] Allan Jabri, David Fleet, and Ting Chen. Scalable adaparXiv preprint tive computation for iterative generation. arXiv:2212.11972, 2022. 7 [17] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. 1, 2, 8 [18] Michael Laskey, Jonathan Lee, Roy Fox, Anca Dragan, and Ken Goldberg. Dart: Noise injection for robust imitation learning. In Conference on robot learning, pages 143156. PMLR, 2017. 4, [19] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 1, 3, 5, 11 [20] Shi Luohe, Hongyi Zhang, Yao Yao, Zuchao Li, and hai zhao. Keep the cost down: review on methods to optiIn First Conference mize LLMs KV-cache consumption. on Language Modeling, 2024. 1, 2 [21] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 5, 6, 7 [22] Shentong Mo and Yapeng Tian. Scaling diffusion mamba with bidirectional ssms for efficient image and video generation. arXiv preprint arXiv:2405.15881, 2024. 7 [23] NVIDIA. Cosmos tokenizer. https://github.com/ NVIDIA/Cosmos-Tokenizer, 2024. Accessed: 202411-15. 5, 6, 7 [24] Alec Radford. Improving language understanding by generative pre-training. 2018. 2, 8 9 Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In International Conference on Learning Representations, 2022. 2, 7 [40] Jacob Walker, Ali Razavi, and Aaron van den Oord. Predicting video with vqvae. arXiv preprint arXiv:2103.01950, 2021. 7 [41] Junke Wang, Yi Jiang, Zehuan Yuan, Binyue Peng, Zuxuan Wu, and Yu-Gang Jiang. Omnitokenizer: joint image-video tokenizer for visual generation. arXiv preprint arXiv:2406.09399, 2024. 2, 5, 6, 7, 8 [42] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 1, [43] Ronald J. Williams and David Zipser. learning algorithm for continually running fully recurrent neural networks. Neural Computation, 1(2):270280, 1989. 1, 2 [44] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. Nuwa: Visual synthesis pretraining for neural visual world creation. In European conference on computer vision, pages 720736. Springer, 2022. 1, 2 [45] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. 1, 2, 7, 8 [46] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, MingIrfan Essa, et al. Magvit: Hsuan Yang, Yuan Hao, In Proceedings of Masked generative video transformer. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1045910469, 2023. 1, 2, 7, 11 [47] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 7, 8 [48] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. arXiv preprint arXiv:2202.10571, 2022. [49] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1845618466, 2023. 7 [50] Wen Zhang, Yang Feng, Fandong Meng, Di You, and Bridging the gap between training and inarXiv preprint Qun Liu. ference for neural machine translation. arXiv:1906.02448, 2019. 4, 8 [25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 2, 8 [26] A. Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. ArXiv, abs/2102.12092, 2021. 8 [27] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 5, 7, [28] Florian Schmidt. Generalization in generation: closer look at exposure bias. In Proceedings of the 3rd Workshop on Neural Generation and Translation@EMNLP-IJCNLP 2019, Hong Kong, November 4, 2019, pages 157167. Association for Computational Linguistics, 2019. 2 [29] Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Mostgan-v: Video generation with temporal motion styles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 56525661, 2023. 7 [30] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: continuous video generator with the price, image quality and perks of stylegan2. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 36263636, 2022. 7 [31] Soomro. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 5, 6, 7 [32] Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Dim: Diffusion mamba for efficient high-resolution image synthesis. arXiv preprint arXiv:2405.14224, 2024. [33] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris Metaxas, and Sergey Tulyakov. good image generator is what you need for high-resolution video synthesis. arXiv preprint arXiv:2104.15069, 2021. 7 [34] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 15261535, 2018. 7 [35] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 5 [36] Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. 2, 3, 4, 5, 8 [37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 7, 8 [38] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 4, [39] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi 10 Patch Size in Input Layer: 2 (consistent across all experiments) Exponential Moving Average (EMA): Applied with decay rate of 0.9999. All reported results are generated using the EMA model. Training Precision: BF16, which is crucial for model convergence. A.4. Inference Procedure During inference, the following settings are applied: Default Masked Prediction Steps: 64 Denoising Steps for Diffusion Head: 100 It is important to note that: During training, each spatial layer employs fully bidirectional attention without any attention mask, while each temporal layer utilizes causal attention with designed attention mask. At inference time, the attention mask is unnecessary as the model attends only to previously generated frames. When inferring longer sequences, we interpolate the temporal position embeddings using linear interpolation. In our proposed CTF paradigm, the sequence length of the model input is doubled solely during training. The sequence length remains unchanged during inference. B. Future Work Future research directions include: Text-to-Video Generation: Extend our approach to textto-video tasks and explore the scalability of both model and data sizes. Interactive Game Generation: Apply our model to realtime interactive game generation within dynamic environments, presenting promising new direction. A. Additional Details A.1. Network Architecture Our spatial-temporal transformer comprises 20 spatialtemporal blocks. Each spatial-temporal block consists of spatial layer followed by temporal layer. The specific configurations are as follows: Hidden States Dimension: 1280 MLP Dimension in Attention Layers: 5120 Number of Attention Heads: 16 Diffusion Head: Comprises 3 blocks with dimension of 1024, implemented identically to the diffusion head in MAR [19]. Overall, our model consists of approximately 850 million parameters, which is on the same scale as the baseline models, ensuring fair comparison. A.2. Data Sampling A.2.1. Training During training, we uniformly sample video from the training set and subsequently extract clip based on randomly selected frame interval. Specifically, with dynamic interval training, the frame interval is uniformly sampled from 1 to 25. A.2.2. FVD Evaluation We use different sampling strategies for the UCF-101 and Kinetics-600 (K600) datasets for the real distribution: UCF-101: We randomly sample 2,048 videos and extract single clip from each using fixed frame interval of 3. K600: We randomly sample 50,000 videos and extract single clip from each with frame interval of 1. We also generate equal number of clips for FVD computation. It is important to note that MAGVIT-1 [46] utilizes real distribution of 300,000 videos by applying 6 random spatial and temporal crops per video in the evaluation on K600. To maintain simplicity and ensure reproducibility, we limit our evaluation to 50,000 videos for the real distribution. Consequently, our Frechet Video Distance (FVD) score does not benefit from the larger sample size used in MAGVIT-1. A.3. Training Procedure The training process involves the following configurations: Warmup Steps: 10,000 Learning Rate: 2 104 Weight Decay: 0.02 Input Frames: 17 frames when using 3D-VAE 16 frames when using 2D-VAE Batch Size: 256 for K600 and 128 for UCF-101. Training Epoch: 150 for K600 and 1400 for UCF-101. 11 Figure 7. Case Study: Video Prediction on Kinetics-600. MAGI generates high-quality future frames conditioned on past frames."
        }
    ],
    "affiliations": [
        "HKUST",
        "HKUST(GZ)",
        "StepFun",
        "THU",
        "UIUC"
    ]
}