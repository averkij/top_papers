{
    "paper_title": "Neural Metamorphosis",
    "authors": [
        "Xingyi Yang",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper introduces a new learning paradigm termed Neural Metamorphosis (NeuMeta), which aims to build self-morphable neural networks. Contrary to crafting separate models for different architectures or sizes, NeuMeta directly learns the continuous weight manifold of neural networks. Once trained, we can sample weights for any-sized network directly from the manifold, even for previously unseen configurations, without retraining. To achieve this ambitious goal, NeuMeta trains neural implicit functions as hypernetworks. They accept coordinates within the model space as input, and generate corresponding weight values on the manifold. In other words, the implicit function is learned in a way, that the predicted weights is well-performed across various models sizes. In training those models, we notice that, the final performance closely relates on smoothness of the learned manifold. In pursuit of enhancing this smoothness, we employ two strategies. First, we permute weight matrices to achieve intra-model smoothness, by solving the Shortest Hamiltonian Path problem. Besides, we add a noise on the input coordinates when training the implicit function, ensuring models with various sizes shows consistent outputs. As such, NeuMeta shows promising results in synthesizing parameters for various network configurations. Our extensive tests in image classification, semantic segmentation, and image generation reveal that NeuMeta sustains full-size performance even at a 75% compression rate."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 1 ] . [ 1 8 7 8 1 1 . 0 1 4 2 : r a"
        },
        {
            "title": "Xingyi Yang",
            "content": "and Xinchao Wang National University of Singapore xyang@u.nus.edu, xinchao@nus.edu.sg https://adamdad.github.io/neumeta/ Abstract. This paper introduces new learning paradigm termed Neural Metamorphosis (NeuMeta), which aims to build self-morphable neural networks. Contrary to crafting separate models for different architectures or sizes, NeuMeta directly learns the continuous weight manifold of neural networks. Once trained, we can sample weights for any-sized network directly from the manifold, even for previously unseen configurations, without retraining. To achieve this ambitious goal, NeuMeta trains neural implicit functions as hypernetworks. They accept coordinates within the model space as input, and generate corresponding weight values on the manifold. In other words, the implicit function is learned in way, that the predicted weights is well-performed across various models sizes. In training those models, we notice that, the final performance closely relates on smoothness of the learned manifold. In pursuit of enhancing this smoothness, we employ two strategies. First, we permute weight matrices to achieve intra-model smoothness, by solving the Shortest Hamiltonian Path problem. Besides, we add noise on the input coordinates when training the implicit function, ensuring models with various sizes shows consistent outputs. As such, NeuMeta shows promising results in synthesizing parameters for various network configurations. Our extensive tests in image classification, semantic segmentation, and image generation reveal that NeuMeta sustains full-size performance even at 75% compression rate. Keywords: Weight Manifold Morphable Neural Network Implicit Neural Representation"
        },
        {
            "title": "Introduction",
            "content": "The world of neural networks is mostly dominated by the rigid principle: Once trained, they function as static monoliths with immutable structures and parameters. Despite the growing intricacy and sophistication of these architectures over the decades, this foundational approach has remained largely unchanged. This inherent rigidity presents challenges, especially when deployed in new scenarios unforeseen during the networks initial design. Each unique scenario calls for new model of distinct configuration, involving repeated design, training, and storage processes. Such an approach is not only resource-intensive, but also limits the models prompt adaptability in rapidly changing environments. Corresponding author. 2 X. Yang et al. In our study, we embark on an ambitious quest to design neural networks that can, once trained, be continuously morphed for various hardware configurations. Particularly, our goal is to move beyond the confines of fixed and pre-trained architectures, and create networks that readily generalize to unforeseen sizes and configurations during the training phase. Indeed, this problem has been considered in slightly different setup, , employing strategies like flexible models [2,8,57] and network pruning techniques [11,12, 32]. The former ones are designed to self-adapt to various subnetwork configurations, whereas the latter ones aim to eliminate redundant connections, achieving models that are streamlined yet robust. Nevertheless, these solutions have their own challenges: flexible models are confined to the their training configurations, and pruning methods compromise performance and often require further retraining. Most importantly, they are still building numerous rigid models, without the ability to be continuously morphed. To this end, we present new learning paradigm, termed Neural Metamorphosis (NeuMeta). That is, interpreting neural netinstead of works not as discrete entities, we see them as points sampled from continuous and high-dimensional weight manifold. This shift allows us to learn the manifold as whole, rather than handling isolated points. As such, NeuMeta, as its name implies, can Fig. 1: Pipeline of Neural Metamorphosis. smoothly morphs one network to another, with similar functionality but different architecture, such as width and depth. Once done, we can generate the weights for arbitrary-sized models, by sampling directly from this manifold. At the heart of our paradigm is the use of Implicit Neural Representation (INR) as hypernetworks to fit the manifold. Intuitively, the INR acts as an indexing function of weights: upon receiving the network configuration and weight coordinates as inputs, it produce the corresponding weight values. In the training phase, the INR is assigned two goals: it approximates the weights of the pretrained network, while simultaneously minimizing the task loss across variety of randomly sampled network configurations. During the testing phase, the INR receives the weight coordinates for the desired network configuration, outputting values to parameterize the target network. This implementation is, by nature, different from existing methods that builds continuous neural networks that rely on integral operations and explicit continuous weight function [29, 50]. Nonetheless, our ambitious effort comes with great challenges, causing the simplistic solutions to fail. primary difficulty arises from the inherently nonsmooth properties of networks weights, which hinders the fitting of the INR. To overcome this, we put forth two strategic solutions. The first involves the permutation of weight matrices to enhance the intra-network smoothness. NeuMeta 3 Recognizing the flaws of prior attempts, we formulate it as multi-objective Shortest Hamiltonian Path problem (mSHP). By solving this problem within individual network cliques, we enhance the smoothness of the networks weights. The second strategy, aimed at cross-network smoothness, involves introducing random noise into the input. During the INR training, we maintain output consistency of the main model, irrespective of this input noise. During testing, the expected output value around this sampled coordinates is employed as the predicted weight value. This strategy moves away from rigid grid, allowing for greater flexibility in generating new networks. Together, these strategies simplify the process of modeling the weight manifold, thereby enhancing the robustness of our approach. We evaluated NeuMeta on various tasks including image classification, semantic segmentation, and generation. Our findings reveal that NeuMeta not only matches the performance of original model but also excels across different model sizes. Impressively, it maintains full-size model performance even with 75% compression rate. Remarkably, NeuMeta extrapolates unseen weights. In other words, it can generates parameters for network sizes outside its training range, accommodating both larger and smaller models. This papers contributions are summarized as follows: We introduce Neural Metamorphosis, new learning paradim that leverage INRs to learn neural networks continuous weight manifold. Once trained, this INR can generate weights for various networks without retraining. We introduce dual strategies to improve both intra-network and cross-network smoothness of the weight manifold. This smoothness is key to generate highperforming networks. The proposed method undergoes thorough evaluations across multiple domains, including image classification, segmentation and generation, underscoring their versatility and robustness in varied computational setup."
        },
        {
            "title": "2 Related Work",
            "content": "Efficient Deep Neural Networks. In recourse-limited applications, the efficiency of neural networks becomes critical concern. Researcher have explored structure pruning methods [17,35] that trim non-essential neurons to reduce computation. Another development is the flexible neural networks [2,3,15,22,56,57], which offer modifiable architectures. These networks are trained on various subnetwork setups, allowing for dynamic resizing. In our context, we present new type of flexible model that directly learns the continuous weight manifold. This allows it to generalize to configurations that havent been trained on, an infeasible quest for existing approaches. Continuous Deep Learning. Beyond traditional neural networks that discretize weights and outputs, continuous deep learning models represent these elements as continuous functions [4, 50]. The concept extends to neural networks with infinite hidden width, modeled as Gaussian processes [39]. Further 4 X. Yang et al. Method Continuous HyperNet Resizable Checkpoint-Free Generalize to Unseen Structure Prune [37] Network Transform [5, 53, 55] Flexiable NN [2, 57] Continuous NN [46, 50] Weight Generator [24] NeuMeta (Ours) Table 1: Comparing methods for building neural networks but can be resized. Structural pruning () only reduces network size. Network Transform manipulates weights to construct functionally identical version of the source network. Flexible Models are training-dependent and fail with unseen networks. Existing Continuous NNs are only valid for specific operators. Weight generators need extensive training checkpoints. Our NeuMeta uniquely learns continuous weight manifold with INR hypernet. As such we can generalize to any neural operator and unseen configurations beyond training. endeavors replace matrix multiplication with integral operations [46, 50]. Neural Ordinary Differential Equations (Neural ODEs) [4] build models by defining dynamical system with differential equations, fostering continuous transformation of data. Our method diverges from those above, using an INR to create continuous weight manifold. This allows for continous weight sampling and introduces new type of continuous network. Knowledge Transfer. Pre-trained neural networks have become cornerstone for advancing deep learning, enabling rapid progress in downstream tasks due to their transferable learned features [5, 53]. Techniques like network transform [5, 53, 55] and knowledge distillation [21, 54, 58] adapt these features to fit new architectures and more compact networks. Our approach also transfer knowledge, but instead of doing one-to-one transfer, we derive continuous manifold of weights from trained neural network. This enables one-to-many knowledge transfer, which can create multiple networks of various sizes. HyperNetworks. HyperNetworks optimize neural architectures by generating weights via auxiliary networks [16]. To accommodate weights for new models or architectures, they are trained on vast range of checkpoints, learning weight distribution [24, 41, 47], facilitating multitask learning [38, 43], continual learning [40], fewshot learning [48] and process implicit function [6]. Unlike typical hypernetworks producing fix-sized weights, our method uses an INR as hypernetwork that learns to predict variable-sized weights, offering dynamic, on-demand weight adaption. Similarly, [1] uses INR as hypernet, but their approach is confined to fixed-size weight prediction. [43] predicts connections between layers without accounting for various weight sizes. We provide comparative analysis of the aforementioned methods in Table 1."
        },
        {
            "title": "Implicit Representation on Weight Manifold",
            "content": "In this section, we introduce the problem setup of NeuMeta and present our solution. In short, our idea is to create neural implicit function to predict weight for many different neural networks. We achieve this goal by posing the principle of smoothness in this implicit function. NeuMeta"
        },
        {
            "title": "3.1 Problem Definition",
            "content": "Lets imagine the world of neural networks as big space called F. In this space, every neural network model fi is associated with set of weight Wi = {w(i,j)}. Such model fi is uniquely identified by its configuration I, such as width (channel number) and depth (layer number). Furthermore, each weight element within the network is indexed by , indicating its specific location, including aspects like the layer index and channel index. The combination of configurations and indices, forms the model space, uniquely indexing each weight. We say the all weights values that makes up good model on dataset lies on weight manifold W. We also assume we have access to pretrained model (; Woriginal). Our goal is to learn the weight manifold W. Definition 1. (Neural Metamorphosis) Given labeled dataset and pretrained model (; Woriginal), we aim to develop function : that maps any points in the model space to its optimal weight manifold. This is achieved by minimizing the expected loss across full . E(i,j)IJ (cid:2)Ltask(fi(W ); D)(cid:3), min s.t.W = {w (i,j)}, (i,j) = (i, j), (1) where Ltask denotes the task-specific loss function. In other words, give us the best set of weights for any model setup in , rather than fitting single or set of neural networks [2,57]. In context neural network, our , which inputs coordinates and outputs values, is known as implicit neural representation (INR) [49]. Consequently, we choose INR as our , offering scalable and continuous method learn this mapping. Connecting to Continuous NN. NeuMeta can be viewed as method to build Continuous NNs, by representing weight values as samples from continuous weight manifold. Here, we would like to see how it differs from existing methods. For example, in continuous-width NNs [46, 50], linear operations are typically defined by the Riemann integral over inputs and weights: (x) = = (jxjW (j)) (cid:88) (cid:90) 1 0 x(j)W (j)δj, (2) where and represent discrete input and weight vectors. is the continuousvalued channel index. (j) is continuous weight function, and is the width of the sub-interval between sampled points for integral. Our method offers three key advantages over this traditional approach: Integral-Free. NeuMeta requires no integral. Learned Continuous Sampling. Our method jointly learns the continuous weight function and the sampling interval wj = jW (j), rather that learning (j) along. This enables us to generate continuous-width NN on-fly, feat unachievable with discrete learned sampling [50]. INR Parameterization. INR offers generalized form to model the continuous function1. 1 Prior designs using kernel [50] or piece-wise functions [46] can be considered special cases of INR, as detailed in our supplementary material. 6 X. Yang et al. Fig. 2: Diagram of NeuMeta and our content organization. Challenge and Solution. Our effort, while ambitious, presents distinct challenges. First, an INR design for neural network weight is largely unexplored. Second, it is essential to train on limited samples from the weight manifold and then generalize to unseen ones. Our solution, as depicted in Figure 2, includes an INR-based architecture in Section 3.2 and strategy for learning on smooth weight manifold in Sec 3.3. The training process is discussed in Sec 3.4 ."
        },
        {
            "title": "3.2 Network Architecture",
            "content": "At the core of NeuMeta, we employ an INR model, (; θ) : Rk Rd, to parameterize the weight manifold. This function, based on multi-layer perceptron (MLP), transforms model space into weight values. In our implementation, we set the parameter = 6. For convolutional networks, the dimension = K, the maximum kernel size, whereas for non-convolutional setups, = 1. Considering generalized network with layers, each layer with an inputoutput channel size of (Cin, Cout). Each weight element, w(i,j), is associated with unique index within the network. This index is represented as coordinate pair (i, j), with = (L, Cin, Cout) denoting the network structure and = (l, cin, cout) indicating the its specific layer, input, and output channel number. To ensure the same coordinate system is applicable to all (i, j), these raw coordinates undergo normalization, typically rescaling them with constant = (cid:20) , cin Cin , cout Cout , , Cin , Cout (cid:21) , (3) Similar to prior technique [36], the normalized coordinates undergo transformation through sinusoidal position embedding, to extract its Fourier features. γPE(v) = (cid:104) sin(20πv), cos(20πv), . . . , sin(2L1πv), cos(2L1πv) (cid:105) These encoded Fourier features, γPE(v), serve as inputs to the MLP, yielding the weights: w(i,j) = (γPE(v); θ) = . (4) MLP(γPE(v); θ) Cin In equation (4), the output of the MLP is scaled by the number of input channels Cin, ensuring that the networks output maintains scale invariance relative to the size of the input channel [14, 19]. To handle lots of parameters with INR, we adopting block-based approach [44, 52]. Instead of single large INR, weights are divided into grid, with each segment controlled by separate MLP network. The full architecture will be mentioned in the supplementary material. In our framework, the weights for standard neural network operations are defined as follows: NeuMeta 7 Linear Operation. For linear operations, we obtain the scalar weight as the element-wise average of w(i,j). Convolution Operation. For convolution layers, weights w(i,j) are reshaped into k. If the kernel size is smaller than the K, only the central elements are utilized. Batch Normalization Layer. For batch normalization, we use re-parameterization strategy [9], integrating BN weights into adjacent linear or convolution layers. This method integrates BN operations into unified framework."
        },
        {
            "title": "3.3 Maintaining Manifold Smoothness",
            "content": "A critical design within our paradigm is ensuring the weight manifold remains smooth. We, in this section, discuss why this smoothness is crucial for the models performance, and outline our strategy for achieving this local smoothness. Intra-Model Smoothness. Modern neural networks heavily rely on their ability to model smooth signals [42] to ensure convergence. Yet, empirical evidence suggests that the weight matrices are typically non-smooth. To enable our INR to reconstruct weights, we must find strategies that promote smoothness. To address this challenge, previous studies have explored the concept of weight permutation [1, 50]. It is often likened to the Traveling Salesman Problem (TSP) [28]. However, such an approach, while seemingly straightforward, overlooks the crucial inter-dependencies within and between weight matrices. Lets consider weight matrix RCoutCinand measure its smoothness using total variation, denoted as (W). It is defined as the sum of variations along both channels: (W) = Vin(W) + Vout(W). In fact, applying the TSP formulation presents 3 problems: (P1) Loop VS Non-Loop: Unlike TSP, which necessitates returning to the starting point, ensuring 2D weight matrix smoothness doesnt require looping back. Instead, it is better to be considered as Shortest Hamiltonian Path (SHP) [13] problem, allowing for an arbitrary starting channel. (P2) Breaking Smoothness for the Connecting Layer: Unlike isolated weight matrices, neural networks consist of connected layers, creating complex inter-layer relationships. This is illustrated in Figure 3, where permutations in one layer necessitate corresponding reversals in adjacent layers to maintain the networks functional equivalence. For example, with an activation function σ() and valid permutation pair and 1 (where 1 = I), the following equation holds: WiP σ(P 1Wi1X) = Wiσ(Wi1X) (5) As result, 1 may affect the adjacent layers, with increased TV for WiP . (P3) Breaking Smoothness for the Other Dimension: permutation enhancing smoothness in output channel, might introduce non-smooth patterns in the input channel, thus reducing the overall smoothness. Luckily, we find that the computation of the TV measurement renders (P3) infeasible, implying our focus should be directed towards (P1) and (P2). 8 X. Yang et al. Fig. 3: Intra-model smoothness via permutation equivalence. Our approach involves permuting weights to minimize total variance within each neural clique graph, thereby enhancing global smoothness. Proposition 1. (Axis Alignment) 2 Let be given matrix and be permutation. The application of permutation in one dimension of does not influence the total variation in the orthogonal dimension. (WP ) = Vin(WP ) + Vout(W) (P W) = Vin(W) + Vout(P W) (6) (7) Hence, to tackle global smoothness, we address challenges P1 and P2. We consider neural network as dependency graph = (V, E) [11], where each node vi represents an operation with weight Wi and each edge eij indicates inter-connectivity between vi and vj. Each graph clique = (VC, EC) is full-connected, representing group of operation is connected. As results, each corresponds to unique permutation matrix . Our objective is to determine all in way that minimizes the total variation across the whole network. Luckily, based on the Proposition 1, this complex optimization can be broken down into multiple independent optimizations, each on clique. We define this as multi-objective Shortest Hamiltonian Path (mSHP ) problem: arg min (cid:88) (cid:16) (cid:17) Vout(P Wi) + Vin(WjP 1) (8) eij EC To address each mSHP problem, we transform it into TSP problem by adding dummy node. This new node has edges with zero-distance to all others in the clique. We then solve TSP using 2.5-opt local search [51]. The resulting permutation is applied to all weight matrices within the clique. This promotes the weight smoothness and preserves the functionality of the network. Since each individual mSHP problem is only correlated to one clip graph, we can solve the optimal in relative small scale, very efficently. In fact, with 20 cliques per network, the total computation time is < 4 sec. Cross-Model Smoothness. Another crucial challenge is to perverse the generalization behavior of the INR with different network configurations, which means, small perturbation in the configuration, will eventually not affect the main models performance. We address this by adding coordinate variation in the INRs learning process. 2 Proof in the supplementary material. NeuMeta During training, rather than using fixed coordinates and model sizes as in Equation 4, we introduce slight variations to the input coordinates. Specifically, we add small perturbation ϵ to the input coordinate (i, j) = (i, j) + ϵ, where ϵ is drawn uniformly from U(a, a). This strategy aims to minimize the expected loss EϵU(a,a)[L]. For model evaluation, we sampling weight from small neighborhood, as illustrated in Figure 4. We compute this by averaging the weights obtained from multiple input, each perturbed by different ϵ U(a, a): Fig. 4: Cross-model smoothness via coordinate perturbation. Unlike the predict weights in discrete grid (Left), our INR predicts weight as the expectation within small neighborhood (Right). w(i,j) = EϵU(a,a)[w(i,j)] 1 (cid:88) (γPE(v); θ) (9) This is implemented by inferring the INR = 50 times with varied sampled inputs and then computing the average of these weights to parameterize the main network. This approach is designed to enhance the stability and reliability of the INR under different configurations."
        },
        {
            "title": "3.4 Training and Optimization",
            "content": "Our approach optimizes the INR, denoted as (; θ), to accurately predict weights for the main network of different configurations. It pursues two primary goals: approximating the weights of the pretrained network (; Woriginal), and minimizing task-specific loss across range of randomly sampled networks. As such, the optimization leverages composite loss function, divided into three distinct components: task-specific loss, reconstruction loss, and regularization loss. Task-specific Loss. Denoted as Ltask(y, ˆy(W)), this measures the difference between actual labels and predictions ˆy, based on weights from the INR. Reconstruction Loss. This element, expressed as Lrecon = Woriginal2 2W Woriginal2, assesses how close the INR-derived weights to the ideal weights Woriginal, weighted by the magnitude Woriginal2 2. Regularization Loss. Symbolized as Lreg = W2. This introduces L2 norm regularization on the predicted weights, to prevent overfitting by controlling the complexity of the derived model [27, 34]."
        },
        {
            "title": "We minimize the composite objective by sampling different points on the",
            "content": "model space min θ Ei,j,ϵ[L] = min θ Ei,j,ϵ[Ltask + λ1Lrecon + λ2Lreg] (10) This loss function ensuring not only proficiency in the primary task through precise weight, but also bolstering model robustness via regularization. During training, we iteratively evaluates various combinations (i, j), striving to minimize 10 X. Yang et al. the expected loss. The loss function is backpropagated from the main network to the INR as follows: θL = Ltask W θ + λ1 Lrecon θ + λ2 Lreg θ (11) This equation represents the gradient of the loss with respect to θ."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we present our experimental analysis and various applications of NeuMeta, spanning classification, semantic segmentation, and image generation. To substantiate our design choices, we conduct different ablation studies. Additionally, we delve into exploring the properties of the learned weight manifold."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Datasets and Evaluation. We evaluate the proposed method on 3 tasks across 6 different visual datasets. For image classification, we select 4 dataset: MNIST [31], CIFAR10, CIFAR100 [26] and ImageNet [7]. Training includes horizontal flip augmentation, and we report top-1 accuracy. We also report the training time and the final size of stored parameters to evaluate our savings. In semantic segmentation, we utilize PASCAL VOC2012 [10], standard dataset for object segmentation tasks. We utilize its augmented training set [18], incorporating preprocessing techniques like random resize crop, horizontal flip, color jitter, and Gaussian blur. Performance is quantified using mean Intersectionover-Union (mIOU) and F1 score, averaged across 21 classes. For image generation, we employ MNIST and CelebA [33]. vanilla variational auto-encoder (VAE) fits the training data, with evaluation based on reconstruction MSE and negative log-likelihood (NLL). Implementation Details. Our INR utilizes MLPs with ReLU activation, comprising five layers with residual connections and 256 neurons each. We employ block-based INR approach, where each parameter type, such as weights and biases, is represented by separate MLP. The positional embedding frequency is set to 16. Optimization is done using Adam [23] with 1e-3 initial learning rate and cosine decay, alongside 0.995 exponential moving average. During training, we maintain the balance between different objectives with λ1 = 1 and λ2 = 1e4. In each training batch, we sample one network configuration, update random subset of layers in the main network, computing gradients for the INR, to speed up training. The configuration pool, created by varying the channel number of the original network, utilizes compression rate γ = 1 sampled channel number . We randomly sample network width with compress rate γ [0, 0.5] for training. For example, 128-channel layer will have its width sampled from 64 128. full channel number For classification tasks, we apply LeNet [30] on MNIST, ResNet20 [20] on CIFAR10 and CIFAR100, and ResNet18 and ResNet50 on ImageNet, using batch sizes of 128 (MNIST, CIFAR) and 512 (ImageNet). We the INR train for 200 NeuMeta 11 epochs. For segmentation task, we use the U-Net [45] with the ResNet18 backbone. The details are mentioned in the supplementary material. Baselines. Our NeuMeta model is benchmarked against three family of methods: Structure Pruning, Flexible Models, and Continuous-Width Models. Individually Trained Model. Each models is trained separately. Structure Pruning. We evaluate against pruning methods that eliminate channels based on different criteria. This includes Weight-based pruning (removing neurons with low ℓ1/ℓ2-norm), Taylor-based method (using gradients related to output), Hessian-based pruning (using Hessian trace for sensitivity), and Random pruning (random channel removal). Flexible Model. We compare with the Slimmable network [57], which trains subnetworks of various sizes within the main model for dynamic testtime resizing. We train the model on {0%, 25%, 50%} compressed ratio, and also test on 75% compressed setup. Continuous-Width NN. Comparison is made with the Integral Neural Network [50], which uses kernel representation for continuous weight modeling. For comparison, we focus on the uniform sampling method, as its learned sampling technique does not support resizing. We train the model on [0%, 50%] compress rate range, but also test on other values, like 75%. We ensure that all compression is applied uniformly for all methods, guaranteeing that all models compared have the exactly same cost for inference."
        },
        {
            "title": "4.2 Enhancing Efficiency by Morphing the Networks",
            "content": "Image Classification. As depicted in Figure 5, in the realm of image classification, NeuMeta consistently surpasses existing pruning-based methods in accuracy across MNIST, CIFAR10, CIFAR100, and ImageNet datasets at various compression ratios. It is worth-noting that, pruning-based methods show marked accuracy decrease, approximately 5% on ImageNet and 6% on CIFAR100, when the compression ratio exceeds 20%. Conversely, NeuMeta retains stable performance up to 40% compression. However, minor performance reduction is noted in our full-sized model, highlighting limitation in the INRs ability to accurately recreate the complex pattern of network weights. Table 2 compares NeuMeta with Slimable NN and INN, including Oracle results of independently trained models for reference. We stick to the same model size for all method, to ensure the comparision is fair. Remarkably, NeuMeta often surpasses even these oracle models on large compress rate. This success is attributed to the preserved smoothness across networks of varying sizes, which inadvertently enhances smaller networks. Our approach outperforms both Slimable NN and the kernel representation in INN. Notably, at an untrained compression ratio of 75%, other methods significantly underperform. Furthermore, when evaluating total training time and parameter storage requirements, our approach demonstrates improved efficiency. Unlike the exhaustive individual model training and storage approach, other methods achieve some 12 X. Yang et al. (a) LeNet on MNIST (b) R20 on CIFAR10 (c) R20 on CIFAR100 (d) R18 on ImageNet Fig. 5: Accuracy comparison of NeuMeta versus different structure pruning methods on MNIST, CIFAR10, CIFAR100 and ImageNet. Our method consistently outperforms pruning-based methods. R18 and R20 are short for ResNet18 and ResNet20. Method γ = 0% γ = 25% γ = 50% γ = 75% Total Train Cost Acc Acc Acc Acc (GPU hours) ResNet20 on CIFAR Individual 92.60 Slimable [57] 90.44 91.33 INN [50] 91.76 Ours 90.65 90.44 90.50 91.32 89.57 88.41 89.24 90.56 87.04 18.56 71.70 89.56 5.3 1.6 1.8 1.3 Method γ = 0% γ = 25% γ = 50% γ = 75% Total Train Cost Acc Acc Acc Acc (GPU hours) ResNet20 on CIFAR100 68.83 Individual Slimable [57] 64.44 65.86 INN [50] 66.07 Ours 66.37 64.01 65.53 66.23 64.87 63.38 63.35 65.36 61.37 1.59 27.60 62.62 5.5 1.5 1.9 1. Stored Params 0.67M 0.35M 0.27M 0.20M Stored Params 0.70M 0.37M 0.28M 0.20M Table 2: Accuracy comparison of ResNet20 on CIFAR10 and CIFAR100 at different compression ratios. The 75% compression ratio wasnt applied in training. level of savings. However, Slimable NNs separate storage for BN parameters still renders it less efficient. Our method achieves the least storage size by storing few MLPs instead of the original parameters, thus reducing the overall parameter count even below that of single model. Semantic Segmentation. For semantic segmentation on the PASCAL VOC2012 dataset, NeuMeta demonstrates superior performance in Table 8. It surpasses the Slimmable network that requires hard parameter sharing, especially at an untrained 75% compression rate. On this setup, we show significant improvement of 20 mIOU. However, for complex tasks like segmentation, slight performance drop is observed at smaller compression rate. It is attributed to the INRs limited representation ability. More results is provided in the supplementary. Image Generation. We implement NeuMeta to generate images on MNIST and CelebA, using VAE. Since Slimable NN and INN havent been previously adapted for VAE before, we only compare with the pruning method, in Figure 7. Our approach demonstrated superior performance in terms of lower negative log-likelihood (NLL) across various compression ratios. For example, we visualize the generated results of when compressed by 25% for MNIST and 50% for CelebA in Figure 6. Compared with the ℓ1-based pruning, our method significantly improved reconstruction MSE from 53.7632.58 for MNIST and from 620.87128.60 for CelebA. Correspondingly, the NLL was reduced by 61.33 for MNIST and 492.26 for CelebA. NeuMeta 13 (a) Results on MNIST (b) Results on CelebA Fig. 6: VAE Visualizations on MNIST and CelebA Datasets on the same compress rate. Lower NLL and MSE indicates better performance. (a) MNIST Compress Rate vs. NLL (b) CelebA Compress Rate vs. NLL Fig. 7: Comparative analysis of compress rate and NLL on different datasets. Lower NLL indicates better performance. Method 25% 50% 75% mIOU F1 mIOU F1 mIOU F1 Individual 84.70 90.63 83.14 89.59 82.79 89.36 Slimmable [57] 81.09 88.14 80.92 88.03 61.19 72. Ours 81.94 88.75 81.93 88.74 81.94 88.75 Fig. 8: Comparison of different methods across compressed ratio for U-Net. The 75% compression ratio wasnt seen in training."
        },
        {
            "title": "4.3 Exploring the Properties for NeuMeta",
            "content": "As we represent the weights as smooth manifold, we investigate its effects on network. Specifically, we compare NeuMeta induced networks, with individually trained models and models distilled [21] from full-sized versions. NeuMeta promote feature similarity. We analyzed the last layer features of ResNet20 trained on CIFAR10, particularly from layer3.2, using linear central kernel alignment (CKA) [25] score between each resized and the full-sized model. The result is shown in Figure 9 (Top). It reveals higher feature map correlations across models compared to other methods, indicating that NeuMeta encourages similar network representations across different sizes. NeuMeta as Implicit Knowledge Distillation. We also report the the pairwise output KL divergence in Figure 9 (Bottom), key metric in knowledge distillation [21]. Individually trained models show higher divergence, whereas both KD and NeuMeta result in reduced divergence. These results imply that NeuMeta not only aligns internal representations but also ensures consistent network outputs, as an implicit form of distillation."
        },
        {
            "title": "4.4 Ablation Study",
            "content": "Weight Permutation. To validate the effectiveness of our permutation strategy, we analyzed its impact on CIFAR10 accuracy. The comparison of Exp 2 and 4 in Table 11 demonstrates significant 11.51 accuracy increase due to our permutation strategy. Detailed comparisons of our mSHP-based method with the TSP solution from [50] are presented in the supplementary material. It shows 14 X. Yang et al. Fig. 10: Ablation study with or without manifold sampling. No. Weight Permutation λ1 λ2 Accuracy 1 2 3 4 5 0 0 1e-4 1 1e-4 1 1 1e-4 10 1e-4 100 1e-4 73.56 80.33 64.37 91.84 91.73 91.47 Fig. 11: Ablation study for weight permutation and objective hyperprameters on CIFAR10 ResNet20. Fig. 9: Similarity Analysis Between Models. (Top) the CKA comparison between the full model and various other models of different sizes. (Bottom) heatmap of the output KL divergence for each pair of models. that our mSHP-based solution achieved lower weight total variation score, indicating superior with-in model smoothness. Objective. We verify different terms in our training objective in Eq 10. From Exp 1, 4-6 in Table 11, we find the optimal reconstruction weight λ1 = 1 yields the best performance. Comparing Exp 3 and 4, we observe performance boost with weight penalty term at λ2 = 1e 4. Manifold Sampling. Figure 10 evaluates our manifold sampling method with ResNet20 on CIFAR10. Sampling from the weight manifold neighborhood consistently improves performance, especially in untrained model sizes."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper presents Neural Metamorphosis (NeuMeta), novel paradigm that builds self-morphable neural networks. Through the training of neural implicit functions to fit the continuous weight manifold, NeuMeta can dynamically generate tailored network weights, adaptable across variety of sizes and configurations. core focus of our approach is to maintain the smoothness of weight manifold, enhancing the models fitting ability and adaptability to novel setups. Experiments on image classification, generation and segmentation indicate that, our method maintain robust performance, even under large compression rate."
        },
        {
            "title": "Acknowledgement",
            "content": "This project is supported by the National Research Foundation, Singapore, under its AI Singapore Programme (AISG Award No: AISG2-RP-2021-023). NeuMeta"
        },
        {
            "title": "References",
            "content": "1. Ashkenazi, M., Rimon, Z., Vainshtein, R., Levi, S., Richardson, E., Mintz, P., Treister, E.: Nern: Learning neural representations for neural networks. In: The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net (2023), https://openreview.net/ pdf?id=9gfir3fSy3J 4, 7 2. Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once for all: Train one network and specialize it for efficient deployment. In: International Conference on Learning Representations (2020), https://arxiv.org/pdf/1908.09791.pdf 2, 3, 4, 5 3. Chavan, A., Shen, Z., Liu, Z., Liu, Z., Cheng, K.T., Xing, E.P.: Vision transformer slimming: Multi-dimension searching in continuous optimization space. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 49314941 (2022) 3 4. Chen, R.T., Rubanova, Y., Bettencourt, J., Duvenaud, D.K.: Neural ordinary differential equations. Advances in neural information processing systems 31 (2018) 3, 4 5. Chen, T., Goodfellow, I.J., Shlens, J.: Net2net: Accelerating learning via knowledge transfer. In: Bengio, Y., LeCun, Y. (eds.) 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings (2016), http://arxiv.org/abs/1511.05641 4 6. De Luigi, L., Cardace, A., Spezialetti, R., Zama Ramirez, P., Salti, S., Di Stefano, L.: Deep learning on implicit neural representations of shapes. In: International Conference on Learning Representations (ICLR) (2023) 4 7. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: largescale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp. 248255. Ieee (2009) 8. Devvrit, F., Kudugunta, S., Kusupati, A., Dettmers, T., Chen, K., Dhillon, I., Tsvetkov, Y., Hajishirzi, H., Kakade, S., Farhadi, A., Jain, P.: Matformer: Nested transformer for elastic inference. In: Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@NeurIPS 2023) (2023), https://openreview.net/forum?id=93BaEweoRg 2 9. Ding, X., Zhang, X., Ma, N., Han, J., Ding, G., Sun, J.: Repvgg: Making vgg-style convnets great again. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1373313742 (2021) 7 10. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The pascal visual object classes (voc) challenge. International Journal of Computer Vision 88(2), 303338 (Jun 2010) 10 11. Fang, G., Ma, X., Song, M., Mi, M.B., Wang, X.: Depgraph: Towards any structural pruning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1609116101 (2023) 2, 8 12. Frankle, J., Carbin, M.: The lottery ticket hypothesis: Finding sparse, trainable neural networks. In: 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net (2019), https://openreview.net/forum?id=rJl-b3RcF7 2 13. Garey, M.R., Johnson, D.S.: Computers and Intractability: Guide to the Theory of NP-Completeness. W. H. Freeman (1979) 7 14. Glorot, X., Bengio, Y.: Understanding the difficulty of training deep feedforward neural networks. In: Proceedings of the thirteenth international conference on ar16 X. Yang et al. tificial intelligence and statistics. pp. 249256. JMLR Workshop and Conference Proceedings (2010) 6 15. Grimaldi, M., Mocerino, L., Cipolletta, A., Calimera, A.: Dynamic convnets on tiny devices via nested sparsity. IEEE Internet of Things Journal 10(6), 5073 5082 (2022) 16. Ha, D., Dai, A., Le, Q.: Hypernetworks (2016) 4 17. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. In: Bengio, Y., LeCun, Y. (eds.) 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings (2016), http: //arxiv.org/abs/1510.00149 3 18. Hariharan, B., Arbeláez, P., Bourdev, L., Maji, S., Malik, J.: Semantic contours from inverse detectors. In: 2011 international conference on computer vision. pp. 991998. IEEE (2011) 10 19. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification. In: Proceedings of the IEEE international conference on computer vision. pp. 10261034 (2015) 6 20. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770778 (2016) 10 21. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531 (2015) 4, 22. Hou, L., Huang, Z., Shang, L., Jiang, X., Chen, X., Liu, Q.: Dynabert: Dynamic bert with adaptive width and depth. Advances in Neural Information Processing Systems 33, 97829793 (2020) 3 23. Kingma, D., Ba, J.: Adam: method for stochastic optimization. In: International Conference on Learning Representations (ICLR). San Diega, CA, USA (2015) 10 24. Knyazev, B., Drozdzal, M., Taylor, G.W., Romero Soriano, A.: Parameter prediction for unseen deep architectures. Advances in Neural Information Processing Systems 34, 2943329448 (2021) 4 25. Kornblith, S., Norouzi, M., Lee, H., Hinton, G.: Similarity of neural network representations revisited. In: International conference on machine learning. pp. 3519 3529. PMLR (2019) 13 26. Krizhevsky, A.: Learning multiple layers of features from tiny images. Tech. rep. (2009) 10 27. Krogh, A., Hertz, J.: simple weight decay can improve generalization. Advances in neural information processing systems 4 (1991) 9 28. Lawler, E.L., Lenstra, J.K., Kan, A.R., Shmoys, D.B.: The traveling salesman problem: guided tour of combinatorial optimization. The Journal of the Operational Research Society 37(5), 535 (1986) 7 29. Le Roux, N., Bengio, Y.: Continuous neural networks. In: Artificial Intelligence and Statistics. pp. 404411. PMLR (2007) 2 30. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. In: Proceedings of the IEEE. vol. 86, pp. 22782324 (1998), http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.7665 10 31. LeCun, Y., Cortes, C.: MNIST handwritten digit database (2010), http://yann. lecun.com/exdb/mnist/ 32. Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P.: Pruning filters for efficient convnets. In: 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net (2017), https://openreview.net/forum?id=rJqFGTslg 2 NeuMeta 17 33. Liu, Z., Luo, P., Wang, X., Tang, X.: Deep learning face attributes in the wild. In: Proceedings of International Conference on Computer Vision (ICCV) (December 2015) 10 34. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017) 35. Ma, X., Fang, G., Wang, X.: Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems 36, 2170221720 (2023) 3 36. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM 65(1), 99106 (2021) 6 37. Molchanov, P., Mallya, A., Tyree, S., Frosio, I., Kautz, J.: Importance estimation for neural network pruning. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1126411272 (2019) 4 38. Navon, A., Shamsian, A., Chechik, G., Fetaya, E.: Learning the pareto front with hypernetworks. In: International Conference on Learning Representations (2021), https://openreview.net/forum?id=NjF772F4ZZR 4 39. Neal, R.M.: Bayesian learning for neural networks, vol. 118. Springer Science & Business Media (2012) 40. von Oswald, J., Henning, C., Grewe, B.F., Sacramento, J.: Continual learning with hypernetworks. In: International Conference on Learning Representations (2020), https://openreview.net/forum?id=SJgwNerKvB 4 41. Peebles, W., Radosavovic, I., Brooks, T., Efros, A.A., Malik, J.: Learning to learn with generative models of neural network checkpoints. arXiv preprint arXiv:2209.12892 (2022) 4 42. Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F., Bengio, Y., Courville, A.: On the spectral bias of neural networks. In: International Conference on Machine Learning. pp. 53015310. PMLR (2019) 7 43. Raychaudhuri, D.S., Suh, Y., Schulter, S., Yu, X., Faraki, M., Roy-Chowdhury, A.K., Chandraker, M.: Controllable dynamic multi-task architectures. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1095510964 (2022) 4 44. Reiser, C., Peng, S., Liao, Y., Geiger, A.: Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1433514345 (2021) 6 45. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. pp. 234241. Springer (2015) 46. Roux, N.L., Bengio, Y.: Continuous neural networks. In: Meila, M., Shen, X. (eds.) Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics, AISTATS 2007, San Juan, Puerto Rico, March 21-24, 2007. JMLR Proceedings, vol. 2, pp. 404411. JMLR.org (2007), http://proceedings.mlr.press/ v2/leroux07a.html 4, 5 47. Schürholt, K., Knyazev, B., Giró-i Nieto, X., Borth, D.: Hyper-representations as generative models: Sampling unseen neural network weights. Advances in Neural Information Processing Systems 35, 2790627920 (2022) 4 48. Sendera, M., Przewięźlikowski, M., Karanowski, K., Zięba, M., Tabor, J., Spurek, P.: Hypershot: Few-shot learning by kernel hypernetworks. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 24692478 (2023) 4 18 X. Yang et al. 49. Sitzmann, V., Martel, J., Bergman, A., Lindell, D., Wetzstein, G.: Implicit neural representations with periodic activation functions. Advances in neural information processing systems 33, 74627473 (2020) 50. Solodskikh, K., Kurbanov, A., Aydarkhanov, R., Zhelavskaya, I., Parfenov, Y., Song, D., Lefkimmiatis, S.: Integral neural networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1611316122 (June 2023) 2, 3, 4, 5, 7, 11, 12, 13 51. Stattenberger, G., Dankesreiter, M., Baumgartner, F., Schneider, J.J.: On the neighborhood structure of the traveling salesman problem generated by local search moves. Journal of Statistical Physics 129, 623648 (2007) 8 52. Tancik, M., Casser, V., Yan, X., Pradhan, S., Mildenhall, B., Srinivasan, P.P., Barron, J.T., Kretzschmar, H.: Block-nerf: Scalable large scene neural view synthesis. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 82488258 (2022) 6 53. Wei, T., Wang, C., Rui, Y., Chen, C.W.: Network morphism. In: International conference on machine learning. pp. 564572. PMLR (2016) 4 54. Yang, X., Ye, J., Wang, X.: Factorizing knowledge in neural networks. In: European Conference on Computer Vision. pp. 7391. Springer (2022) 4 55. Yang, X., Zhou, D., Liu, S., Ye, J., Wang, X.: Deep model reassembly. Advances in neural information processing systems 35, 2573925753 (2022) 4 56. Yu, J., Huang, T.S.: Universally slimmable networks and improved training techniques. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 18031811 (2019) 3 57. Yu, J., Yang, L., Xu, N., Yang, J., Huang, T.S.: Slimmable neural networks. In: 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net (2019), https://openreview. net/forum?id=H1gMCsAqY7 2, 3, 4, 5, 11, 12, 13 58. Yuan, L., Tay, F.E., Li, G., Wang, T., Feng, J.: Revisiting knowledge distillation via label smoothing regularization. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 39033911 (2020)"
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}