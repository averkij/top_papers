{
    "paper_title": "Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in $\\{\\pm 1, \\pm i\\}$",
    "authors": [
        "Feiyu Wang",
        "Xinyu Tan",
        "Bokai Huang",
        "Yihao Zhang",
        "Guoan Wang",
        "Peizhuang Cong",
        "Tong Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have revolutionized artificial intelligence, yet their massive memory and computational demands necessitate aggressive quantization, increasingly pushing representations toward the theoretical limit of a single bit. While complex-valued LLMs, such as iFairy, offer a superior chance for low-bit representation compared to real-valued counterparts, they require training from scratch, preventing the utilization of the vast ecosystem of pre-trained real-valued foundation models. Here we present Fairy2i, a universal framework that transforms pre-trained real-valued layers into an equivalent widely-linear complex form, enabling extremely low-bit quantization while reusing existing checkpoints. By proving a lossless mathematical equivalence between real and widely-linear maps, we convert standard Transformers into the complex domain and employ a phase-aware quantization scheme with a highly efficient codebook of fourth roots of unity. Furthermore, we introduce a recursive residual quantization mechanism that iteratively minimizes quantization error, allowing inference to proceed via efficient multiplication-free accumulation. We demonstrate that Fairy2i restores the performance of LLaMA-2 7B at an effective 2-bit precision to levels nearly comparable with full-precision baselines, significantly outperforming state-of-the-art real-valued binary and ternary quantization methods. This work bridges the gap between the representational efficiency of complex-valued arithmetic and the practical utility of pre-trained models, paving a new way for efficient inference on commodity hardware."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 2 1 0 9 2 0 . 2 1 5 2 : r FAIRY2I: Training Complex LLMs from Real LLMs with All Parameters in {1, ğ‘–} Feiyu Wang, Xinyu Tan, Bokai Huang, Yihao Zhang, Guoan Wang, Peizhuang Cong, Tong Yang Peking University Abstract Large language models (LLMs) have revolutionized artificial intelligence, yet their massive memory and computational demands necessitate aggressive quantization, increasingly pushing representations toward the theoretical limit of single bit. While complex-valued LLMs, such as iFairy, offer superior chance for low-bit representation compared to real-valued counterparts, they require training from scratch, preventing the utilization of the vast ecosystem of pre-trained real-valued foundation models. Here we present FAIRY2I, universal framework that transforms pre-trained real-valued layers into an equivalent widely-linear complex form, enabling extremely low-bit quantization while reusing existing checkpoints. By proving lossless mathematical equivalence between real and widely-linear maps, we convert standard Transformers into the complex domain and employ phase-aware quantization scheme with highly efficient codebook of fourth roots of unity ({1, ğ‘–}). Furthermore, we introduce recursive residual quantization mechanism that iteratively minimizes quantization error, allowing inference to proceed via efficient multiplication-free accumulation. We demonstrate that FAIRY2I restores the performance of LLaMA-2 7B at an effective 2-bit precision to levels nearly comparable with full-precision baselines, significantly outperforming state-of-the-art real-valued binary and ternary quantization methods. This work bridges the gap between the representational efficiency of complex-valued arithmetic and the practical utility of pre-trained models, paving new way for efficient inference on commodity hardware."
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) keep growing in width, depth, and sequence length [1, 2, 3, 4, 5, 6], pushing memory bandwidth and compute budgets to their limits in both training and deployment [7, 8, 9]. Quantization seeks to address these bottlenecks by storing weights and activations in few bits and replacing many floating-point operations with much cheaper integer arithmetic [10]. There exist two mainstream workflows for quantization in the literature. Post-training quantization (PTQ) [11, 12, 13, 14, 15, 16] is attractive for its simplicity and zero retraining cost. However, at extremely low bit widths (e.g., 12 bits), it typically suffers from severe accuracy drop. This failure often stems from compounding factors, including that calibration data are limited, per-tensor or per-group scaling is insufficient to realign codebooks, outlier channels dominate clipping ranges, etc. Quantization-aware training (QAT) [17, 18, 19, 20] inserts the quantizer into the training loop often with straight-through estimators [21], allowing the model to adapt to the quantized pattern. While current QAT methods outperform the PTQ methods under extremely low bit budgets, training model from scratch with QAT is often prohibitively time-consuming and unstable since QAT methods still operate with tiny codebooks, offering insufficient freedom to match layer statistics. Corresponding author: yangtong@pku.edu.cn 1 Recently, the iFairy framework [22] demonstrates the efficacy of transitioning from real-valued to complexvalued transformer architecture within the extremely low-bit quantization regime. By concurrently encoding magnitude and phase, complex weights introduce an additional degree of freedom that enhances modeling flexibility and representational capacity, thereby providing theoretical basis for the superior performance of quantized complex-valued models. Crucially, iFairy adopts the fourth roots of unity on the unit circle ({1, ğ‘–}) as compact codebook. This approach can fully exploit the 2 bit representation budget and hence achieve better alignment with weight statistics compared to real-valued counterparts, such as BitNet [23, 24, 25, 26], BitCPM4 [27], and ParetoQ [20], which typically rely on ternary set {+1, 0, 1}, resulting in the underutilization of the available 2-bit encoding space. Consequently, complex-valued Transformer models exhibit significantly improved accuracy retention at extremely low bit precision compared to state-of-the-art real-valued QAT approaches. Despite these benefits, iFairy models are trained from scratch with the QAT method. Reproducing such end-to-end training on larger models demands substantial time and compute, which hinders adoption when strong real-valued checkpoints already exist. In practice, many users require path that reuses existing real-valued checkpoints and still obtains complex-domain benefits at an extremely low bit width. To address this problem, we propose FAIRY2I, universal framework for extremely-low bit QAT which converts real-valued model into complex-valued computation pattern and quantizes the converted model with efficient quantization paradigms in the complex domain. Specifically, we keep the pretrained real model, re-parameterize each real-valued linear layer in an equivalent widely-linear complex form, then continue training at extremely low bits in the QAT method with simple phase-aware quantizer (e.g., PhaseQuant [22]), and further quantize the residual error to reduce the remaining gap at tiny extra bit cost. The whole framework consists of three components, including widely-linear representation, phase-aware complex quantization, and recursive residual error quantization. In widely-linear representation [28], we state that any real linear map on an even-dimensional space can be written exactly as the sum of complex-linear part and conjugate-linear part. We apply this to every linear layer of real-valued model, generally including projection, projection, projection, and projection in the self-attention block, as well as Up projection, Gate projection, and Down projection in the feed-forward network, taking LLaMA-alike architecture as an example. For attention, using the real part of the Hermitian inner product of queries and keys preserves the original scores. It can be proven that the forward computation is unchanged after the above transformation. On top of the widely-linear form, phase-aware complex quantization uses very small complex codebook {1, ğ‘–} with shared scaling factors across 1 and ğ‘– per group, respectively. We then train the transformed model with the QAT method while keeping full-precision master weights, so we can reuse existing realvalued checkpoints. To further reduce quantization error, we propose recursive residual error quantization, which also quantizes the residual error using the same phase-aware mechanism, recursively. The deployed weight is the sum of the extremely low bit representation of the original weight and its several successive quantization errors. This residual quantization trades tiny extra code (one more 12 bits per weight, plus small per-group metadata) for noticeable reduction in error. We apply FAIRY2I in off-the-shelf mainstream open source LLMs, such as the LLaMA series. The evaluation results validate the efficiency of the FAIRY2I framework. Our contributions can be summarized as follows: We propose universal method to transform any real layer into an equivalent widely-linear complex form, keeping the pre-quantization model behavior unchanged. We propose to continue training at extremely low bits with phase-aware quantizer, reusing real-valued checkpoints. We propose to further reduce quantization error by quantizing the residual with the same mechanism, yielding compact sum of few low-bit terms, which can significantly reduce the quantization error. 2 The rest of the paper is organized as follows. Section 2 reviews related work on quantization and complexvalued neural networks. Section 3 formalizes the complex-valued reparameterization, phase-aware quantizer, and the recursive residual error quantization and introduces the FAIRY2I framework comprehensively. Section 4 presents empirical results on standard backbones. Section 5 concludes this paper and discusses future work."
        },
        {
            "title": "2.1 Quantization for Large Language Models",
            "content": "Model quantization has become de facto standard for compressing Large Language Models (LLMs). Existing methods can be broadly categorized into Post-Training Quantization (PTQ) [11] and Quantization-Aware Training (QAT). Post-training quantization (PTQ). PTQ methods aim to compress models without extensive retraining, often using small calibration dataset. Early works like AWQ [12] and GPTQ [11] focused on preserving the weights output activation distribution, achieving impressive results at 3-4 bits. More recent approaches like OmniQuant [29], QuIP [14], QuIP# [15] attempt to push the limit to 2 bits. However, PTQ methods typically suffer from severe performance degradation at extremely low bit-widths (e.g., 2 bits) because the limited discrete codebook of real numbers (e.g., binary quantization of {+1, 1} or ternary quantization of {+1, 0, 1}) cannot adequately approximate the heavy-tailed distribution of LLM weights without adapting the network parameters themselves. Extremely low-bit QAT and 1-bit architectures. To address the limitations of PTQ, QAT methods finetune the model parameters to adapt to the quantization noise. BitNet and its successor BitNet b1.58 introduced 1-bit (binary {+1, 1}) and 1.58-bit (ternary {+1, 0, 1}) architecture, respectively, demonstrating that LLMs can retain performance at extremely low precision if trained from scratch [30]. While effective, these methods often require training massive models from random initialization, incurring prohibitive computational costs. Although some works attempt to distill full-precision models into low-bit counterparts (e.g., LLM-QAT), the rigid topology of real-valued binary/ternary weights fundamentally limits the models capacity and optimization landscape, leading to instability or convergence issues when strictly enforced on pre-trained weights."
        },
        {
            "title": "2.2 Complex-Valued Neural Networks",
            "content": "The integration of complex numbers into neural networks boasts rich history, predominantly within domains where data possess intrinsic phase and magnitude, such as signal processing and imaging [31, 32, 33, 34, 35, 36]. By representing weights and activations in the complex domain, Complex-Valued Neural Networks (CVNNs) offer the theoretical potential to capture more intricate patterns and dependencies than their real-valued counterparts. However, despite this promise, the application of CVNNs to natural language processingand to Large Language Models (LLMs) in particularremains remarkably scarce. The iFairy approach. Recently, the iFairy [22] framework demonstrated the potential of complex-valued quantization for LLMs. By utilizing phase-aware codebook {1, ğ‘–} (the fourth roots of unity), iFairy showed that complex-valued Transformers could outperform real-valued binary networks at equivalent bit budgets. However, major bottleneck of iFairy and similar methods is the lack of compatibility with existing real-valued pre-trained models like LLaMA [2, 3, 4], OPT [37], and Qwen [5]. They typically require training 3 complex-valued model from scratch, which is impractical for most practitioners given the scale of modern LLMs. Our contribution. FAIRY2I bridges this gap by introducing widely-linear representation that mathematically equates real-valued layer to complex-valued one. This allows us to leverage the superior quantization topology of the complex domain, specifically the {1, ğ‘–} codebook, while initializing directly from powerful real-valued checkpoints, thereby combining the efficiency of low-bit quantization with the accessibility of pre-trained LLMs."
        },
        {
            "title": "3 Methods",
            "content": "In this section, we introduce the FAIRY2I framework comprehensively. Our goal is to start from pretrained real-valued model, keep its forward behavior intact before quantization, and then continue training at extremely low bit widths with simple and stable complex quantizer. The method proceeds in three steps that build upon each other. Step 1: Widely-linear transformation (Section 3.1). We first express each real linear layer in an equivalent widely-linear complex form. This transformation is exact and unique, and one can use the real and imaginary parts of complex-valued weights to recover the original real layer, and conversely. Before quantization, the network computes the same outputs as the original real model. Step 2: Phase-aware complex quantization (Section 3.2). We then quantize the complex parameters using phase-based scheme on the unit circle with the fixed codebook {1, ğ‘–}. Each weight is projected to the nearest codeword by angle, and two axis-wise scaling factors (real and imaginary, respectively) restore the magnitude. During QAT, we maintain full-precision master weights, use the quantized copies in the forward pass, and propagate gradients via straight-through estimator (STE). This step is simple to implement and applies identically to all widely-linear layers. Step 3: Recursive residual error quantization (Section 3.3). To further reduce error with minimal extra budget, we quantize the residual error after the first pass. At each stage, we project the current residual error to the same small codebook {1, ğ‘–}, subtract the quantized correction, calculate the new residual error, and repeat for small number of stages. The finally deployed weight is the sum of these few low-bit terms. In short, Section 3.1 provides the exact equivalence that allows us to reuse real checkpoints without altering pre-quantization behavior; Section 3.2 introduces stable, extremely low-bit quantizer in the complex plane; and Section 3.3 refines the approximation by recursively quantizing the residual error using the same mechanism."
        },
        {
            "title": "3.1 Widely-linear transformation from real domain to complex domain\nSetup and notation. We use column vectors. Let the real input and output be Ëœx âˆˆ R2ğ‘š and Ëœy âˆˆ R2ğ‘›. The\noriginal linear layer of real-valued models can be represented as:",
            "content": "y = x, R(2ğ‘›) (2ğ‘š) . We pair the last dimension as real/imaginary components and write: (cid:20) (cid:20) (cid:21) (cid:21) = Re Im , = Re Im , = Re + ğ‘– Im Cğ‘š, = Re + ğ‘– Im Cğ‘›. Our goal is to express the same linear map in widely-linear form: = + (U, Cğ‘›ğ‘š), and to provide an explicit, lossless correspondence between and (U, W). 4 (1) (2) Lossless real to complex reparameterization. Partition into ğ‘› ğ‘š real blocks: (cid:20) (cid:21) = R11 R12 R21 R22 , Rğ‘– ğ‘— Rğ‘›ğ‘š. Define: Re = 1 2 Re = 1 2 (R11 + R22), (R11 R22), Im = 1 2 Im = 1 2 (R21 R12), (R12 + R21), and set = Re + ğ‘– Im U, = Re + ğ‘– Im W. Theorem 1 (Exact equivalence). For every Cğ‘š, = (cid:20)"
        },
        {
            "title": "Re y\nIm y",
            "content": "(cid:21) (cid:20) = (cid:21)"
        },
        {
            "title": "Re x\nIm x",
            "content": "= Ux + Wx. Moreover, the pair (U, W) is unique. (3) (4) Proof. We utilize two block operators Rlin and Rconj that embed complex matrices into real 2 2 blocks: Rlin(U) = (cid:20) Re Im Re Im (cid:21) , Rconj(W) = (cid:20) Re Im Im Re (cid:21) . They satisfy, for all x: (cid:20) Re(Ux) Im(Ux) (cid:21) = Rlin(U) (cid:20) (cid:21) ,"
        },
        {
            "title": "Re x\nIm x",
            "content": "(cid:21) (cid:20) Re(Wx) Im(Wx) = Rconj(W) (cid:20) (cid:21) . Re Im (5) (6) Expanding the right-hand sides of Eq. (6) shows that Rlin(U) and Rconj(W) act on stacked real/imaginary parts exactly as the complex maps U() and W(). Solving = Rlin(U) + Rconj(W) blockwise yields Eq. (3). Substituting back U, defined in Eq. (3), we have: = Rlin(U) + Rconj(W), and consequently, for every x: = (cid:20) Re Im (cid:21) (cid:20) = (cid:21)"
        },
        {
            "title": "Re x\nIm x",
            "content": "= Ux + Wx. Uniqueness follows because Eq. (3) is the only blockwise solution. (7) (8) Self-attention in widely-linear form. For Transformer projections, we apply Eq. (3) independently to Rğ‘„, Rğ¾ , Rğ‘‰ , Rğ‘‚. Let q, k, be the complex-valued outputs of the Q/K/V projections. We use Hermitian scores: (cid:16) (cid:17) = 1 ğ‘‘ğ‘˜ Re qHk , (9) where ()H denotes the conjugate transpose. This formulation equals the original real dot-product scores when applied to stacked representations, i.e., Re(qHk) = Re(q) Re(k) +Im(q) Im(k) = k. The softmax and value aggregation act separately on the real and imaginary parts, as the attention score is real number; the output projection uses the same widely-linear mapping. Note that the self-attention in widely-linear form is compatible with highly-optimized FlashAttention kernels [38, 39, 40] since we can unstack the complexvalued q, k, into longer real-valued vector, respectively. 5 R2ğ‘š Real Linear Layer (cid:20) = (cid:21) R11 R12 R21 R22 = Rx R2ğ‘› Input Transformation (cid:20) (cid:21) = Re Im = Re + ğ‘– Im conj Parameter Transformation Re = 1 2 Im = 1 2 Re = 1 2 Im = 1 2 (R11 + R22) (R21 R12) (R11 R22) (R12 + R21) (Deriving U, from R) = Re + ğ‘– Im = Re + ğ‘– Im Widely Linear Complex Layer Output Transformation (cid:20) (cid:21) = Re Im = Re + ğ‘– Im + = Ux + Wx Cğ‘› Figure 1: Illustration of widely-linear transformation from the real domain to the complex domain. Remarks. Note that we assume the real linear matrix R(2ğ‘›) (2ğ‘š) has even dimensions. If the real dimension is odd, we can pad one zero channel to make it even, apply the mapping, and discard the padded output channel. Figure 1 illustrates the widely-linear transformation from real-valued linear layer to complex-valued linear layer."
        },
        {
            "title": "3.2 Complex-valued low-bit quantization",
            "content": "We quantize the complex weights that appear in the widely-linear form (both and W) using simple, phase-based rule called PhaseQuant on the unit circle, adapted from iFairy. The codebook is: S2-bit = {1, ğ‘–}. Deterministic projection by phase. For weight ğ‘¤ C, we choose the nearest codeword by angle: ğ‘(ğ‘¤) = arg max ğ‘  S2-bit Equivalently, with ğœƒ = Arg(ğ‘¤) (ğœ‹, ğœ‹] and ğ‘˜ = ğ‘re, ğ‘im {1, 0, +1}. (cid:1) (cid:5) (cid:0) ğ‘¤ ğ‘  Re (cid:4) 2ğœƒ ğœ‹ + 1 {1, ğ‘–}. (10) , ğ‘(ğ‘¤) = ğ‘– ğ‘˜. We write ğ‘(ğ‘¤) = ğ‘re + ğ‘– ğ‘im with Axis-wise scaling factors. To recover magnitude, we estimate two scales from the projected axes: ğ‘ re = ğ‘ im = (cid:12) (cid:12) 1 (cid:12) (cid:12){ğ‘¤ : ğ‘(ğ‘¤) {1}} 1 (cid:12) (cid:12){ğ‘¤ : ğ‘(ğ‘¤) {ğ‘–}} (cid:12) (cid:12) (cid:213) (cid:12) (cid:12) Re(ğ‘¤) (cid:12) (cid:12), ğ‘ (ğ‘¤) {1} (cid:213) (cid:12) (cid:12) Im(ğ‘¤) (cid:12) (cid:12). ğ‘ (ğ‘¤) {ğ‘– } (11) These are per-tensor averages1. 1In this paper, we only use per-tensor scaling factors and we leave the investigation of various group strategies as future work 6 Dequantization. Each weight is replaced in the forward pass by: Ë†ğ‘¤ = ğ‘ re ğ‘re + ğ‘– ğ‘ im ğ‘im. (12) QAT training. We maintain full-precision masters and use Eq. (10)(12) only in the forward pass. Gradients flow through the masters via straight-through estimator (STE), i.e., we treat ğ‘() as the identity function in the backward pass within small neighborhood. The scales in Eq. (11) are recomputed at each step and are differentiable w.r.t. the masters via their dependence on ğ‘¤. (i) The codebook {1, ğ‘–} utilizes the full 2-bit budget and is symmetric on the unit circle, Remarks. aligning well with complex geometry and attention scoring. (ii) The procedure applies elementwise to and W; it is independent of the widely-linear equivalence and can be applied to any complex layer. (iii) To alleviate the computational overhead during training, we adopt Gausss multiplication algorithm. Standard complex multiplication (ğ‘ + ğ‘–ğ‘)(ğ‘ + ğ‘–ğ‘‘) requires 4 real multiplications. Gauss optimization reformulates this as: Re = ğ‘ğ‘ ğ‘ğ‘‘, Im = (ğ‘ + ğ‘)(ğ‘ + ğ‘‘) ğ‘ğ‘ ğ‘ğ‘‘. This computation requires only 3 real multiplications and 5 additions. In the context of large-scale Transformer layers where matrix multiplication dominates, this reduction from 4 to 3 real multiplications per element provides theoretical 25% reduction in FLOPs for the GEMM operations in both forward and backward passes."
        },
        {
            "title": "3.3 Recursive residual quantization",
            "content": "We further reduce quantization error by repeatedly quantizing the residual left after the initial pass. The core concept is straightforward: represent each complex weight as short sum of very low-bit terms, where each new term corrects the approximation error of the previous ones. We apply this to the complex weights in the widely-linear form (both and W), using the same iFairy-style projection and axis-wise scaling as in Section 3.2. Intuitively, consider each complex weight as point in the complex plane. The first step projects it to one of four axis directions (1, ğ‘–) and applies scale for that axis, providing coarse approximation. The residual vector points from this approximation to the true weight. We then project the residual to the same small codebook and scale it. Adding this correction brings the approximation closer to the target. Each subsequent stage repeats this process on the remaining residual; accuracy improves, and the residual magnitude decreases. Formal definition of recursive residual quantization. Let ğ‘Š Cğ‘›ğ‘š be matrix of complex weights. Fix small codebook S, e.g., = {1, ğ‘–} for 2-bit quantization. Choose small number of stages ğ‘‡ {1, 2, . . .}. Initialize ğ‘… (0) := ğ‘Š and, for ğ‘¡ = 0, 1, . . . , ğ‘‡ 1, perform: Quantization: Apply PhaseQuant to ğ‘… (ğ‘¡ ) to obtain ğ‘(ğ‘¤) for each ğ‘¤ ğ‘… (ğ‘¡ ) according to Eq. (10). Let im . During this process, we also compute the scaling factors ğ‘  (ğ‘¡ ) re and ğ‘  (ğ‘¡ ) im . ğµ (ğ‘¡ ) = ğµ (ğ‘¡ ) re + ğ‘– ğµ (ğ‘¡ ) Dequantization and residual update: bğ‘Š (ğ‘¡ ) = ğ‘  (ğ‘¡ ) re ğµ (ğ‘¡ ) re + ğ‘– ğ‘  (ğ‘¡ ) im ğµ (ğ‘¡ ) im , ğ‘… (ğ‘¡+1) = ğ‘… (ğ‘¡ ) bğ‘Š (ğ‘¡ ) . 7 Full Precision R(0) = R(1) = R(0) cW(0) Phase Quant ğ‘  (0) re , ğ‘  (0) im cW(0) Stage 0 (ğ‘¡ = 0) Stage 1 (ğ‘¡ = 1) Input/Residual Weight Quantized code {1, ğ‘–} Phase Quant ğ‘  (1) re , ğ‘  (1) im cW(1) + Wğ‘ Deployed Weight Figure 2: Illustration of recursive residual quantization with ğ‘‡ = 2. After ğ‘‡ stages, the deployed approximation is the finite sum: ğ‘‡ 1(cid:213) bğ‘Š (ğ‘¡ ) . ğ‘Šğ‘ (13) ğ‘¡=0 When ğ‘‡ = 1, this reduces to the basic PhaseQuant quantization. For ğ‘‡ = 2, one extra low-bit term is added per weight, typically removing most of the remaining error. Figure 2 shows the workflow of recursive residual quantization with ğ‘‡ = 2. bğ‘Š (ğ‘¡ ) in the forward pass. Training with QAT. We maintain full-precision masters for ğ‘Š and compute The projection ğ‘() uses the hard nearest-codeword rule; in the backward pass, we apply straight-through estimator so that gradients flow to the masters. Scales are recomputed per step (or updated as moving averages) and are differentiable through their dependence on the residuals. We apply the same procedure independently to and in every widely-linear layer. ğ‘¡"
        },
        {
            "title": "3.4 Storage Eï¬€iciency and Parallel Multiplication-Free Inference",
            "content": "Storage eï¬€iciency. FAIRY2I achieves extreme storage compactness by utilizing the codebook = {1, ğ‘–}. Since = 4, each quantized complex weight can be exactly encoded using 2 bits. Consider the original real-valued weight matrix R(2ğ‘›) (2ğ‘š) , which contains total of 4ğ‘›ğ‘š real parameters. Through our widely-linear transformation, is reparameterized into two complex matrices U, Cğ‘›ğ‘š, resulting in total of 2ğ‘›ğ‘š complex weights. The total storage cost for the quantized model (at stage ğ‘‡ = 1) is therefore 2ğ‘›ğ‘š 2 bits = 4ğ‘›ğ‘š bits. Dividing this by the original parameter count (4ğ‘›ğ‘š), FAIRY2I effectively consumes exactly 1 bit per real parameter on average. When extending to recursive residual quantization with ğ‘‡ stages, the total memory footprint scales linearly as ğ‘‡ bits per real parameter. For example, our primary configuration with ğ‘‡ = 2 occupies 2 bits per real parameter. Parallel stagewise inference algorithm. The inference process for widely-linear layer with ğ‘‡ recursive stages is detailed in Algorithm 1. Since the discrete weight components ğµ (ğ‘¡ ) im only contain values from {1, 0, +1}, the matrix-vector products reduce to conditional accumulators. Specifically, multiplying by +1 is an addition, 1 is subtraction, and 0 is skip. Multiplying by ğ‘– simply involves swapping the real and imaginary components of the input activation with sign flip. Crucially, the loop over stages ğ‘¡ = 0 . . . ğ‘‡ 1 (lines 36) is data-independent and can be executed in parallel streams. re and ğµ (ğ‘¡ ) 8 }ğ‘‡ 1 ğ‘¡=0 where entries {1, 0, 1} Algorithm 1 Parallel Inference for FAIRY2I Layer Require: Input = xre + ğ‘–xim Cğ‘š Require: Quantized Codes {ğµ (ğ‘¡ ) re Require: Scales {ğ‘  (ğ‘¡ ) }ğ‘‡ 1 ğ‘¡=0 re Ensure: Output Cğ‘› 1: Initialize output accumulator 0 2: parallel for ğ‘¡ = 0 to ğ‘‡ 1 do 3: , ğµ (ğ‘¡ ) im , ğ‘  (ğ‘¡ ) im 4: 5: 6: 7: // Multiplication-free operations (Add/Sub only) re xre ğµ (ğ‘¡ ) vre ğµ (ğ‘¡ ) re xim + ğµ (ğ‘¡ ) vim ğµ (ğ‘¡ ) // Apply scalar scales (broadcast) re vre + ğ‘– ğ‘  (ğ‘¡ ) y(ğ‘¡ ) ğ‘  (ğ‘¡ ) im vim atomic add + y(ğ‘¡ ) im xim im xre 8: 9: end parallel for 10: return Latency and parallelism. While recursive residual quantization introduces summation over ğ‘‡ terms bğ‘Š (ğ‘¡ ) x), this does not imply linear increase in latency. Unlike autoregressive generation, where (y = steps must be sequential, the terms bğ‘Š (ğ‘¡ ) are independent with respect to the input x. Given sufficient parallel compute units (which GPUs provide in abundance), the critical path latency is: (cid:0) Ltotal = max ğ‘¡ (GEMM-free(ğ‘¡ ) ) (cid:1) + (ReduceSum), (14) which is effectively ğ‘‚ (1) with respect to ğ‘‡. This allows us to use ğ‘‡ = 2 or ğ‘‡ = 3 to drastically improve accuracy without slowing down the inference stream. LUT-Based optimization for CPU. Beyond multiplication-free arithmetic, the discreteness of our 2-bit weights enables highly efficient inference via Look-Up Tables (LUTs), particularly on CPUs [41]. Following the principles of optimized kernels like BitNet.cpp [42] and T-MAC [43], we can pack groups of four 2-bit complex weights into single 8-bit index. By pre-computing the partial outcomes of these weight combinations with INT8 activations, the inner loop of matrix multiplication reduces to simple table fetch and accumulation. This approach significantly accelerates execution by minimizing arithmetic instructions while retaining mathematical exactness."
        },
        {
            "title": "4 Evaluation",
            "content": "In this section, we empirically validate the effectiveness of the FAIRY2I framework. We primarily focus on the LLaMA-2 7B model to demonstrate that our method can successfully restore accuracy at extremely low bitwidths (1-bit and 2-bit effective) by reusing real-valued checkpoints, significantly outperforming real-valued quantization baselines."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Models and baselines. We conduct experiments on the LLaMA-2 7B model. We compare FAIRY2I against the following baselines: FP16: The original uncompressed LLaMA-2 7B model. 9 Real-valued QAT Baselines: To ensure fair comparison regarding the topological advantage of the complex domain, we implement two strong real-valued QAT baselines trained under the same settings: Real-Binary (1-bit): Weights are quantized to {+1, 1}, similar to BitNet [23]. Real-Ternary (1.58-bit): Weights are quantized to {+1, 0, 1}, similar to BitNet b1.58 [26]. PTQ Baselines: We also include results from GPTQ [11], QuIP# [15] and AQLM [44] for reference. Datasets and metrics. We evaluate the quantized models on both language modeling capabilities and downstream reasoning tasks using the lm-eval-harness framework [45]. Perplexity (PPL): We report perplexity on the validation set of C4 [46]. Zero-shot Tasks: We evaluate zero-shot accuracy on several common sense reasoning benchmarks: ARC-Easy [47], ARC-Challenge [47], HellaSwag [48], PIQA [49], and Winogrande [50]. Implementation details. We implement FAIRY2I using PyTorch. The real-valued pretrained weights are first transformed into the widely-linear complex form. We then continue pre-training with the QAT method on subset of the RedPajama dataset [51] for 30 billion tokens. We use the AdamW [52] optimizer with Warmup-Stable-Decay (WSD) learning rate schedule [53, 27]. The global batch size is set to 1 million tokens. Unless otherwise specified, we report results for two configurations of FAIRY2I: FAIRY2I-W1 (1-bit): Uses the codebook = {1, ğ‘–} for the base complex layer. As shown in Section 3.4, this method consumes 1 bit per parameter. FAIRY2I-W2 (2-bit): Applies one additional step of recursive residual quantization (ğ‘‡ = 2), adding another effective 1 bit per parameter."
        },
        {
            "title": "4.2 Main Results",
            "content": "Perplexity evaluation. Table 1 presents the perplexity results on C4. FAIRY2I significantly outperforms the real-valued baselines at the same or even higher bit budgets. Specifically, FAIRY2I-W1 achieves PPL of 11.03, surpassing the Real-Binary (1-bit) baseline (11.75), and even slightly outperforming the Real-Ternary (1.58 bit) baseline (11.03). Furthermore, with just one step of recursive residual quantization, FAIRY2I-W2 (2-bit) achieves remarkable PPL of 7.85, significantly closing the gap to the FP16 baseline (6.63). It is worth noting that FAIRY2I-W2 outperforms state-of-the-art 2-bit PTQ methods such as AQLM (8.54) and QuIP# (11.01), and even surpasses 3-bit GPTQ (10.61), demonstrating the efficiency of our residual correction mechanism. Zero-shot performance. Table 1 summarizes the accuracy on five downstream tasks. FAIRY2I demonstrates strong generalization capabilities. Notably, FAIRY2I-W1 achieves an average accuracy of 48.66%, outperforming the Real-Binary baseline (46.21%) at the same effective bit-width, while trailing the RealTernary (1.58-bit) model by only narrow margin (0.04%). FAIRY2I-W2 further elevates the performance to 62.00%, which is highly competitive with the FP16 baseline (64.72%) and significantly outperforms the leading 2-bit PTQ method AQLM (57.28%). This confirms that our phase-aware recursive quantization effectively preserves the models capabilities even under extreme compression. 10 Table 1: C4 Perplexity and Zero-shot Accuracy comparison on LLaMA-2 7B. Avg. denotes the average accuracy across the 5 tasks. Results for AQLM, QuIP#, and GPTQ are sourced from [44]. PPL Zero-shot Accuracy (%) Method Bits C4 ARC-e ARC-c HellaSwag PIQA Winogrande Avg. LLaMA-2 (FP16) 6.63 GPTQ [11] QuIP# [15] AQLM [44] Real-Binary (QAT) FAIRY2I-W1 (Ours) 3 2 2 1 1 Real-Ternary (QAT) FAIRY2I-W2 (Ours) 1.58 2 10.61 11.01 8.54 11.75 11.03 11.06 7.85 75.59 58.46 55.56 63. 53.32 56.56 55.93 72.73 43.17 31.06 28.84 32.76 22.70 24.82 24.15 39. 57.06 45.21 42.94 49.55 35.57 38.19 38.43 53.33 77.91 71.49 71.38 74. 66.81 70.08 69.80 76.17 69.85 59.19 62.43 65.67 52.64 53.67 55.17 68. 64.72 53.08 52.23 57.28 46.21 48.66 48.70 62."
        },
        {
            "title": "4.3 Ablation Studies",
            "content": "Effectiveness of recursive residual quantization. We analyze the impact of the recursive depth ğ‘‡ on model performance. Table 2 compares FAIRY2I-W1, FAIRY2I-W2, and further refined version FAIRY2I-W3 (ğ‘‡ = 3), which corresponds to an effective bit-width of 3 bits per parameter. As observed, increasing ğ‘‡ from 1 to 2 results in substantial reduction in C4 perplexity (20.76% improvement) and significant boost in zero-shot accuracy of 19.03%. However, upgrading from W2 to W3 yields diminishing returns, with only marginal improvements in PPL (4.11%) and average accuracy (0.88%). Given that W3 increases the memory footprint by 50% compared to W2, it suggests that ğ‘‡ = 2 is good tradeoff between model accuracy and storage efficiency for extreme quantization. Table 2: Ablation study on the number of recursive stages ğ‘‡ for FAIRY2I under the learning rate of 5e-4. FAIRY2I-W3 represents using 3 recursive stages with effective 3 bits. PPL Zero-shot Accuracy (%) Method Bits C4 Decrease ARC-e ARC-c HellaSwag PIQA Winogrande Avg. Increase FAIRY2I-W1 FAIRY2I-W2 FAIRY2I-W3 1 2 3 11.03 8.74 8. - 20.76% 4.11% 56.56 69.70 69.36 24.82 33.02 32.17 38.19 48.95 50.74 70.08 74.70 76.01 53.67 63.22 63. 48.66 57.92 58.43 - 19.03% 0.88% Sensitivity to learning rate. Training extremely low-bit networks is notoriously unstable and sensitive to hyperparameters. We evaluate the robustness of FAIRY2I-W2 by training with varying learning rate schedules of different decay times. We use the WSD scheduler. WSD consists of three phases: linear warmup from small initial value to peak, stable phase that holds the learning rate at the peak for period to stabilize training, and an optional cosine decay phase to reduce the learning rate later. To study sensitivity to the LR magnitude, we evaluate three WSD variants (denoted as LR1, LR2, and LR3). LR1: linear warmup to peak of 3 105 in 50 steps, then stable at the peak for the remainder of training with no decay applied. LR2: same as LR1 but decay to 5 106 from step 9000 within 2000 steps. LR3: same as LR2 but ecay to 1 106 from step 19000 within 1000 steps. Figure 3 illustrates the training dynamics. Figure 3(a) depicts the Warmup-Stable-Decay (WSD) learning rate schedules employed during training. Figure 3(b) plots the training loss curves corresponding to 11 different learning rate schedules. As observed, our method is robust across different learning rate schedules. Furthermore, the effectiveness of the WSD scheduler is clearly visible: as training progresses into the decay phase, the reduction in learning rate enables the model to converge to lower final loss, confirming that well-scheduled decay is crucial for optimizing quantized models. Table 3 shows the final PPL and task performance with different learning rate schedules. (a) Learning Rate Schedule (b) Training Loss Curves Figure 3: Sensitivity analysis of FAIRY2I to learning rate. (a) The WSD learning rate schedules. (b) Training loss curves under different learning rate schedules. Table 3: Ablation study on the learning rate schedule with FAIRY2I-W2. PPL Zero-shot Accuracy (%)"
        },
        {
            "title": "Bits",
            "content": "C4 ARC-e ARC-c HellaSwag PIQA Winogrande Avg. FAIRY2I-W2-LR1 FAIRY2I-W2-LR2 FAIRY2I-W3-LR3 2 2 2 8.42 7.98 7.85 69.49 72.39 72. 36.60 38.23 39.76 51.25 53.13 53.33 75.03 76.01 76.17 65.04 66.85 68.03 59.48 61.32 62."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose FAIRY2I, universal framework that bridges the representational capacity of complex-valued LLMs with the practical utility of pre-trained real-valued LLMs. By deriving an exact, widely-linear representation, we enable the seamless conversion of real-valued layers into the complex domain, allowing for the reuse of existing checkpoints without prohibitive retraining. Leveraging phase-aware codebook {1, ğ‘–} and recursive residual quantization mechanism, FAIRY2I maximizes the information density of extremely low-bit budgets. Experiments on LLaMA-2 7B demonstrate that our approach outperforms state-of-the-art real-valued binary and ternary methods, restoring performance to near-FP16 levels at an effective 2-bit precision. Future work. Several promising avenues remain for future exploration. First, we aim to develop specialized CUDA kernels and CPU optimization techniques to fully exploit the multiplication-free nature of our quantization scheme, accelerating both training and inference on commodity hardware. Second, we plan to scale FAIRY2I to larger foundation models (e.g., LLaMA-3 70B) and multimodal architectures to verify its 12 universality. Third, further theoretical investigation into the complex-valued loss landscape could provide deeper insights into the robustness observed in low-bit regimes. Finally, and perhaps most critically, our current implementation was limited to 30 billion tokens due to computational constraints. We hypothesize that the complex-valued representation possesses superior capacity that has yet to be fully exploited; with extended training on larger datasets, we believe FAIRY2I holds the potential to not merely match, but surpass the accuracy of the original full-precision baselines."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam et al. Gpt-4 technical report. In: arXiv preprint arXiv:2303.08774 (2023). [2] Hugo Touvron et al. Llama: Open and efficient foundation language models. In: arXiv preprint arXiv:2302.13971 (2023). [3] Hugo Touvron et al. Llama 2: Open foundation and fine-tuned chat models. In: arXiv preprint arXiv:2307.09288 (2023). [4] Abhimanyu Dubey et al. The llama 3 herd of models. In: arXiv e-prints (2024), arXiv2407. [5] An Yang et al. Qwen3 technical report. In: arXiv preprint arXiv:2505.09388 (2025). [6] Daya Guo et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. In: arXiv preprint arXiv:2501.12948 (2025). [7] Xupeng Miao et al. Towards efficient generative large language model serving: survey from algorithms to systems. In: arXiv preprint arXiv:2312.15234 (2023). [8] Zhongwei Wan et al. Efficient large language models: survey. In: arXiv preprint arXiv:2312.03863 (2023). [9] Woosuk Kwon et al. Efficient memory management for large language model serving with pagedattention. In: Proceedings of the 29th symposium on operating systems principles. 2023, pp. 611626. [10] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. survey on model compression for large language models. In: Transactions of the Association for Computational Linguistics 12 (2024), pp. 15561577. [11] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. In: arXiv preprint arXiv:2210.17323 (2022). [12] Ji Lin et al. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. In: arXiv preprint arXiv:2306.00978 (2023). [13] Guangxuan Xiao et al. Smoothquant: Accurate and efficient post-training quantization for large language models. In: International conference on machine learning. PMLR. 2023, pp. 3808738099. [14] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. Quip: 2-bit quantization of large language models with guarantees. In: Advances in Neural Information Processing Systems 36 (2023), pp. 43964429. [15] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks. In: arXiv preprint arXiv:2402.04396 (2024). [16] Zechun Liu et al. Spinquant: Llm quantization with learned rotations. In: arXiv preprint arXiv:2405.16406 (2024). [17] Zechun Liu et al. Llm-qat: Data-free quantization aware training for large language models. In: arXiv preprint arXiv:2305.17888 (2023). [18] Mengzhao Chen et al. Efficientqat: Efficient quantization-aware training for large language models. In: arXiv preprint arXiv:2407.11062 (2024). [19] Yelysei Bondarenko, Riccardo Del Chiaro, and Markus Nagel. Low-rank quantization-aware training for llms. In: arXiv preprint arXiv:2406.06385 (2024). [20] Zechun Liu et al. Paretoq: Scaling laws in extremely low-bit llm quantization. In: arXiv preprint arXiv:2502.02631 (2025). [21] Yoshua Bengio, Nicholas LÃ©onard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. In: arXiv preprint arXiv:1308.3432 (2013). [22] Feiyu Wang et al. iFairy: the First 2-bit Complex LLM with All Parameters in 1, i. In: arXiv preprint arXiv:2508.05571 (2025). [23] Hongyu Wang et al. Bitnet: Scaling 1-bit transformers for large language models. In: arXiv preprint arXiv:2310.11453 (2023). [24] Hongyu Wang, Shuming Ma, and Furu Wei. BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs. In: arXiv preprint arXiv:2504.18415 (2025). [25] Shuming Ma et al. BitNet b1. 58 2B4T Technical Report. In: arXiv preprint arXiv:2504.12285 (2025). [26] Shuming Ma et al. The era of 1-bit llms: All large language models are in 1.58 bits. In: arXiv preprint arXiv:2402.17764 1.4 (2024). [27] MiniCPM Team et al. MiniCPM4: Ultra-Efficient LLMs on End Devices. In: arXiv preprint arXiv:2506. (2025). [28] B. Picinbono and P. Chevalier. Widely linear estimation with complex data. In: IEEE Transactions on Signal Processing 43.8 (1995), pp. 20302033. DOI: 10.1109/78.403373. [29] Wenqi Shao et al. Omniquant: Omnidirectionally calibrated quantization for large language models. In: arXiv preprint arXiv:2308.13137 (2023). [30] Liangdong Liu, Zhitong Zheng, Cong Wang, Tianhuang Su, and Zhenyu Yang. Binary Neural Networks for Large Language Model: Survey. In: arXiv preprint arXiv:2502.19008 (2025). [31] Akira Hirose. Complex-valued neural networks. Springer, 2006. [32] Chiheb Trabelsi et al. Deep complex networks. In: arXiv preprint arXiv:1705.09792 (2017). [33] ChiYan Lee, Hideyuki Hasegawa, and Shangce Gao. Complex-valued neural networks: comprehensive survey. In: IEEE/CAA Journal of Automatica Sinica 9.8 (2022), pp. 14061426. [34] Joshua Bassey, Lijun Qian, and Xianfang Li. survey of complex-valued neural networks. In: arXiv preprint arXiv:2101.12249 (2021). [35] Muqiao Yang, Martin Ma, Dongyu Li, Yao-Hung Hubert Tsai, and Ruslan Salakhutdinov. Complex transformer: framework for modeling complex-valued sequence. In: ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. 2020, pp. 4232 4236. [36] Florian Eilers and Xiaoyi Jiang. Building blocks for complex-valued transformer architecture. In: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. 2023, pp. 15. [37] Susan Zhang et al. Opt: Open pre-trained transformer language models. In: arXiv preprint arXiv:2205.01068 (2022). [38] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. In: Advances in Neural Information Processing Systems (NeurIPS). 2022. [39] Tri Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. In: International Conference on Learning Representations (ICLR). 2024. [40] Jay Shah et al. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. In: Advances in Neural Information Processing Systems 37 (2024), pp. 6865868685. [41] Gunho Park et al. Lut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models. In: arXiv preprint arXiv:2206.09557 (2022). [42] [43] Jinheng Wang et al. Bitnet. cpp: Efficient edge inference for ternary llms. In: arXiv preprint arXiv:2502.11880 (2025). Jianyu Wei et al. T-mac: Cpu renaissance via table lookup for low-bit llm deployment on edge. In: Proceedings of the Twentieth European Conference on Computer Systems. 2025, pp. 278292. [44] Vage Egiazarian et al. Extreme compression of large language models via additive quantization. In: arXiv preprint arXiv:2401.06118 (2024). [45] Leo Gao et al. The Language Model Evaluation Harness. Version v0.4.3. July 2024. DOI: 10.5281/ zenodo.12608602. URL: https://zenodo.org/records/12608602. [46] Colin Raffel et al. Exploring the limits of transfer learning with unified text-to-text transformer. In: Journal of machine learning research 21.140 (2020), pp. 167. [47] Vikas Yadav, Steven Bethard, and Mihai Surdeanu. Quick and (not so) dirty: Unsupervised selection of justification sentences for multi-hop question answering. In: arXiv preprint arXiv:1911.07176 (2019). [48] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In: arXiv preprint arXiv:1905.07830 (2019). [49] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning about Physical Commonsense in Natural Language. In: AAAI Conference on Artificial Intelligence. 2019. URL: https://api.semanticscholar.org/CorpusID:208290939. [50] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In: Communications of the ACM 64.9 (2021), pp. 99 106. [51] Maurice Weber et al. Redpajama: an open dataset for training large language models. In: Advances in neural information processing systems 37 (2024), pp. 116462116492. [52] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In: arXiv preprint arXiv:1711.05101 (2017). [53] Xiao Bi et al. Deepseek llm: Scaling open-source language models with longtermism. In: arXiv preprint arXiv:2401.02954 (2024)."
        }
    ],
    "affiliations": [
        "Peking University"
    ]
}