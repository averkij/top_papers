{
    "paper_title": "Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought",
    "authors": [
        "Guijin Son",
        "Donghun Yang",
        "Hitesh Laxmichand Patel",
        "Amit Agarwal",
        "Hyunwoo Ko",
        "Chanuk Lim",
        "Srikant Panda",
        "Minhyuk Kim",
        "Nikunj Drolia",
        "Dasol Choi",
        "Kyong-Ha Lee",
        "Youngjae Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent frontier models employ long chain-of-thought reasoning to explore solution spaces in context and achieve stonger performance. While many works study distillation to build smaller yet capable models, most focus on English and little is known about language-specific reasoning. To bridge this gap, we first introduct **Language-Mixed CoT**, a reasoning schema that switches between English and a target language, using English as an anchor to excel in reasoning while minimizing translation artificats. As a Korean case study, we curate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and code; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k high-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5, Llama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves state-of-the-art performance, with the highest overall average score (64.0 \\pm 25), ranking first on 5/9 benchmarks and second on the remainder. Samller and mid-sized models also benefit substantially, with an average improvement of +18.6 points across teh evaluated nine benchmarks. Ablations show **Language-Mixed CoT** is more effective than monolingual CoT, also resulting in cross-lingual and mult-modal performance gains. We release our data-curation pipeline, evaluation system, datasets, and models to advance research on language-specific reasoning. Data and model collection: https://huggingface.co/KOREAson."
        },
        {
            "title": "Start",
            "content": "PUSHING ON MULTILINGUAL REASONING MODELS WITH LANGUAGE-MIXED CHAIN-OF-THOUGHT Guijin Son1,6 Donghun Yang2 Hitesh Laxmichand Patel3 Amit Agarwal3 Hyunwoo Ko1,6 Chanuk Lim2 Dasol Choi4,6 Kyong-Ha Lee2 Youngjae Yu5 Srikant Panda3 Minhyuk Kim4,6 Nikunj Drolia7 1OneLineAI 5Seoul National University 2KISTI 3Oracle AI 4Korea University 6Modulabs 7University College Dublin 5 2 0 2 5 ] . [ 1 0 3 2 4 0 . 0 1 5 2 : r spthsrbwls123@yonsei.ac.kr"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent frontier models employ long-chain-of-thought reasoning to explore solution spaces in context and achieve stronger performance. While many works study distillation to build smaller yet capable models, most focus on English and little is known about language-specific reasoning. To bridge this gap, we first introduce Language-Mixed CoT, reasoning schema that switches between English and target language, using English as an anchor to excel in reasoning while minimizing translation artifacts. As Korean case study, we curate YI-SANG: 5.79M nativeKorean prompts from web Q&A, exams, STEM, and code; 3.7M long reasoning traces generated from Qwen3-32B; and targeted 260k high-yield subset. We train nine models (4B35B) across six families (Qwen2.5, Llama-3.1, Gemma-3, etc). Our best model, KO-REAson-35B, achieves state-of-the-art performance, with the highest overall average score (64.02.5), ranking first on 5/9 benchmarks and second on the remainder. Smaller and mid-sized models also benefit substantially, with an average improvement of +18.6 points across the evaluated nine benchmarks. Ablations show Language-Mixed CoT is more effective than monolingual CoT, also resulting in cross-lingual and multi-modal performance gains. We release our data-curation pipeline, evaluation system, datasets, and models to advance research on language-specific reasoning."
        },
        {
            "title": "INTRODUCTION",
            "content": "Test-time scaling amplifies reasoning by allocating more samples or steps, enabling exploration, and self-correction (Jones, 2021). Recent advances show that large language models can internalize similar exploratory behavior (Gandhi et al., 2025) through longer chain-of-thought (CoT) acquired during training. Specifically, such behaviors stem during the post-training phase through reinforcement learning with verifiable rewards (RLVR) (OLMo et al., 2024; Guo et al., 2025). Unfortunately, such methodologies tend to be effective only for strong base models with large parameters (Yang et al., 2025; Rastogi et al., 2025). Therefore, open efforts have centered on distillation from frontier teacher models, combining systematic prompt collection with response generation and quality filtering (Muennighoff et al., 2025; Bercovich et al., 2025; Guha et al., 2025; Hugging Face, 2025). Such pipelines, however, overwhelmingly target English and, to lesser extent, Chinese (Liu et al., 2025a), leaving open how to achieve language-specific reasoning. To bridge this gap, we study how to construct reasoning model for mid-resource language through focused case study in Korean. We start from the empirical observation that pipelines relying heavily on translated corpora (lightblue, 2025; Lee et al., 2025a) exhibit degraded response quality from translation artifacts (Park et al., 2025; Li et al., 2025a) and poor robustness to everyday, colloquial expressions that rarely appear Corresponding Authors 1Data and Model Collection: https://huggingface.co/KOREAson"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: (Left) Thinking styles. Red: monolingual CoT carried out entirely in English. Blue: our proposed Language-Mixed CoT, which alternates between English (anchor) and Korean (target). (Right) Performance comparison of KO-REAson-35B (ours, solid line) with DeepSeek-R1-32B, Exaone-Deep-32B, GPT-OSS20B, and QwQ-32B. KO-REAson-35B achieves top-tier performance, ranking first or second on all tasks. in translated text. To address this, we propose two-step approach: (i) data curation, where we collect 5.79M Korean, user-authored Q&A prompts from the web to ensure broad coverage of natural, in-the-wild language; and (ii) reasoning supervision, where, when generating long reasoning traces with Qwen3-32B (Yang et al., 2025), we enforce Language-Mixed CoT, which allows the model to switch freely during the Think step between an anchor language (English) and the target language (Korean). This enables the model to leverage the anchor languages reasoning capabilities while preserving the semantics of the target language. In our experiments, Language-Mixed CoT consistently outperforms monolingual CoT, with larger gains on reasoning-heavy tasks relative to Korean-only, and on cultural understanding-heavy tasks relative to English-only. The collected dataset, YI-SANG, comprises 5.79M prompts paired with 3.7M long reasoning traces. To the best of our knowledge, this is the largest publicly documented post-training resource for the Korean language. To chart an affordable path to strong reasoning models, we conduct over 100 ablations (some scaling to thousands of H100 GPU-hours) covering teacher models, augmentation schemes, and seed sources, and we iteratively filter patterns that produce loss spikes. This process yields downsampled YI-SANG-HQ of 260k high-yield examples, on which we train the KOREAson series. As shown in Table 3, KO-REAson-35B outperforms state-of-the-art models trained on closed data (GPT-OSS-20B (Agarwal et al., 2025), R1-Distill-32B (Guo et al., 2025), QwQ32B (Team, 2025), EXAONE-Deep-32B (Research et al., 2025)) on average across nine tasks. We further demonstrate that these gains are consistent across model families and scales by training nine models (4B35B) spanning six families. Finally, we observe cross-lingual and multi-modal gains, despite training only on Korean text. Taken together, these results indicate that careful prompting and large-scale data collection can build open-recipes to rival closed systems. Our contributions are summarized as follows: We introduce YI-SANG, the largest publicly documented post-training dataset for Korean to date, comprising 5.79M prompts and 3.7M long reasoning traces, plus 260k high-yield subset (YI-SANG-HQ) distilled via extensive ablations. We propose Language-Mixed CoT, supervision scheme that lets models switch between an anchor language (English) and the target language (Korean) during the Think step, yielding significant gains over monolingual CoT baselines. We train and release the KO-REAson series (4B35B across five families) under the Apache-2.0 license, surpassing closed systems of comparable scale on nine benchmarks."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Recent work has pushed long reasoning into the mainstream. o1 (Jaech et al., 2024) showed that extending the thinking length of model improves performance, while R1 (Guo et al., 2025) revealed how long reasoning traces are structured and how to build models capable of such capability. DeepSeek also demonstrated that SFT-distilled (e.g., DeepSeek-Distill-R1) variants can inherit much of this ability from supervised fine-tuning alone. Subsequent efforts span online RL (Yu et al., 2025; Chen et al., 2025; Luo et al., 2025), offline RL (Research et al., 2025; Wen et al., 2025), and pure SFT (Muennighoff et al., 2025; Guha et al., 2025). consistent pattern emerges: successful online RL from cold start typically requires (i) strong base model (often 30B, with solid math/coding priors) (Yang et al., 2025; Rastogi et al., 2025), (ii) reliable process or reward model (Liu et al., 2025c; He et al., 2025), and (iii) large-scale, high-quality data (e.g., Numina-Math (LI et al., 2024)). These requirements increase cost and brittleness, concentrating progress in high-resource languages such as English and Chinese. Much less is known about bootstrapping reasoning models in mid-resource languages. Directly replicating high-resource pipelines is often infeasible due to weaker base models and limited highquality data. Prior work explores cross-lingual transfer from English training (Yong et al., 2025; Ranaldi & Pucci, 2025) or small-scale translated datasets (Son et al., 2025; Pipatanakul et al., 2025), relying heavily on transfer learning. However, these studies seldom yield well-calibrated, deployment-ready models with concentrated focus on math capabilities only. Our work differs from previous works by going beyond translation. We collect native prompts, systematically curate for quality, and introduce Language-Mixed CoT as more effective supervision signal. By varying only supervision format (long vs. short; language-mixed vs. monolingual), we isolate supervision effects from optimization confounds and provide mid-size models stable path to long-reasoning behavior without RL. We validate this methodology in Korean, an apt testbed: midresource language with an active LLM research ecosystem, scratch-trained base models (Bak et al., 2025; Lab, 2025; KISTI, 2024), dedicated general-knowledge (Son et al., 2024; Hong et al., 2025) and reasoning benchmarks (Ko et al., 2025), and sufficiently large web corpora for data construction. The proposed dataset, YI-SANG, is not only the largest Korean post-training corpus (Figure 2), but also methodological contribution: pipeline for converting noisy internet prompts into high-quality supervision. Our empirical results demonstrate its effectiveness and offer reproducible path for mid-resource communities to build competitive reasoning models."
        },
        {
            "title": "3.1 TRAINING DETAILS",
            "content": "Models To ensure robustness in our ablations, we run experiments on two base models: Gemma-3-4B (Team et al., 2025) and Kanana-1.5-8B (Bak et al., 2025). After determining the high-yield subset, we evaluate its efficacy by training across broader set of models, including Gemma-3-4B/12B, A.X-3.17B/35B (Lab, 2025), Kanana-1.5-8B, Llama-3.18B (Grattafiori et al., 2024), KONI-Llama-3.18B (KISTI, 2024), and Qwen2.5-7B/14B (Qwen et al., 2025). All experiments are conducted with the instruction-tuned versions of the models. See Appendix B.1 for further details on each model. Figure 2: An overview of publicly available Korean datasets. YI-SANG is larger than any fine-tuning dataset or pretraining corpus, with 6.77B tokens. Training Settings All training runs use minimum of 50,000 data points unless otherwise specified. Each experiment (including ablations) is trained for five epochs, except for A.X-3.1-35B, which we train for three epochs due to computational constraints. For further details on the hyperparameters used throughout training, see Appendix B."
        },
        {
            "title": "3.2 EVALUATION DETAILS",
            "content": "Benchmarks In this work, we divide our evaluation suite into two parts: held-in set, used for routine monitoring during training and ablation studies, and held-out set, evaluated once after all ablations and final training are complete. This is to support rapid iteration and prevent inadvertent overfitting to benchmarks during iterative training ablations. Held-in consists of four benchmarks. MCLM (Son et al., 2025) is translated collection of math problems from MATH500 and AIME2024, originally drawn from Olympiads, designed to test deep chain-of-thought reasoning rather than surface recall. KMMLU-Redux (Hong et al., 2025) is quality-controlled, down-sampled version of KMMLU (Son et al., 2024) that maintains correlations with the full suite while reducing evaluation cost; importantly, it spans both factual knowledge (e.g., history, law, medicine) and reasoning-intensive domains (e.g., mathematics, engineering, science). HAE-RAE Bench (Son et al., 2023) assesses Korean linguistic and cultural competence, covering vocabulary, reading comprehension, and historical content. For medical ablations, we also include ClinicalQA, Korean clinical QA benchmark derived from medical licensing examinations, consisting of problems based on chief complaints and medical specialties. Held-out covers broader set of benchmarks used only after all training ablation is done. KMMLUHard (Son et al., 2024) is adversarially filtered version of KMMLU for highest difficulty. KMMLUPro (Hong et al., 2025) contains expert-level professional licensure questions across 14 different categories, including Medicine, Finance, and Law. KSM (Ko et al., 2025) is set of competitionstyle mathematics problems from Korean contests. CLIcK (Kim et al., 2024) aggregates factual questions from Korean exams and textbooks across 11 categories, providing measure of Korean general world knowledge. Finally, KoBALT-700 (Shin et al., 2025) is linguistics-focused benchmark of 700 expert-written items that span syntax, semantics, morphology, phonology, and pragmatics, used to test fine-grained Korean linguistic competence. Evaluation Setup All evaluations are run with vLLM (Kwon et al., 2023) under the following configuration: temperature=0.7, top p=0.9, and max tokens=32,768. Models are instructed to present the final answer wrapped in boxed{...}, and we use math-verify 2 to validate the boxed value; outputs without valid answer are marked incorrect. All ablations use single evaluation; for the main experiments, we run three independent trials and report mean standard error."
        },
        {
            "title": "4 LANGUAGE-MIXED CHAIN-OF-THOUGHT",
            "content": "When constructing multilingual reasoning data in target language (other than English), central question is how to represent the reasoning process: should it be written in the target language or left in English? Prior work has typically chosen one of two monolingual setups, either entirely in English (Pipatanakul et al., 2025; Ha, 2025; Son et al., 2025) or entirely in the target language (lightblue, 2025; Lee et al., 2025a). Our initial exploration reveals critical shortcomings in both. Reasoning in English on Korean prompts introduces translation noise: prompts are often mistranslated, especially in culture-specific contexts, and over time, errors accumulate, leading the model to drift off topic once it forgets the original Korean wording. Conversely, reasoning in Korean produces notable drops in reasoning capability (Ko et al., 2025), and extended training in Korean induces distributional drift in English-pretrained bases (Hong et al., 2024), degrading their original strengths. To address both issues, we propose Language-Mixed CoT (See Figure 1 for example). During the Think phase, the model code-switches, performing most logical scaffolding in English while preserving key Korean terms and quotations. This keeps faithfulness to the prompt without sacrificing reasoning power. To generate Language-Mixed CoT, we prompt the teacher to preserve named entities, quoted spans, and key terms in Korean while generating the rest of the reasoning in English. After generation, we apply regex-based filter to discard samples whose Korean-character ratio lies outside 5% and 20%. In Table 1, we train three variants that think in English, in Korean, and Language-Mixed mode. Korean-only thinking shows clear drop in reasoning accuracy, though it remains more robust on culture-sensitive content, especially on Kanana. Notably however, LanguageMixed CoT outperforms both alternatives in most cases. 2https://github.com/huggingface/Math-Verify 3See Section C.2 for more details."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Language-Mixed CoT (ours) outperforms monolingual CoTs across models and sizes. Compared with Englishor Korean-only CoT, Language-Mixed CoT yields higher scores for both Gemma (4B) and Kanana (8B). Highest scores per column are highlighted in green . Abbreviations: HRB = HAE-RAE Bench; KMMLU-R = KMMLU-Redux. CoT Lang. Gemma-3-4B Kanana-1.5-8B HRB MCLM KMMLU-R HRB MCLM KMMLU-R English Korean Language-Mixed (Ours) 50.3 40.6 54.9 48.1 25.6 55.8 52.2 42.5 53.0 66.2 67.2 74.6 60.5 31.8 57.4 64.0 53.4 64."
        },
        {
            "title": "5 YI-SANG INSTRUCT",
            "content": "Despite many efforts to distill frontier models into smaller open models, only few manage to collect their own training corpus; most reuse or repackage existing datasets (Ye et al., 2025; Guan et al., 2025; Hugging Face, 2025; Hu et al., 2025). This pattern is also common in multilingual settings, and it materially affects outcomes: models trained through such pipelines lack robustness to everyday, colloquial expressions that rarely appear in translated text. To pursue truly multilingual reasoning, we therefore decide to construct our own dataset. This section describes our instruction collection (Section 5.1), response generation process (Section 5.2) for building YI-SANG, and presents ablations used to derive the high-yield subset YI-SANG-HQ (Section 5.3). Figure 3: Category distribution across different stages of the dataset collection. (a) Sources (N=54): counts of the public Q&A and community websites we compiled; categories were manually assigned by the authors based on contextual review. (b) Questions: after crawling, items inherit the category from their source. (c) Responses: after response generation, we added OpenThought (Guha et al., 2025) as an additional source. Colors are shared across panels; centers show total counts. 5."
        },
        {
            "title": "INSTRUCTION COLLECTION",
            "content": "Seed Instruction Collection. We curate native Korean prompts from public Q&A and community websites via two-step pipeline. (1) Source discovery. Using domain knowledge and targeted search, the authors compiled 54 candidate sites with user-posted questions and peer answers. Each site was assigned license category: (crawling and redistribution permitted), (crawling allowed but commercial use and redistribution prohibited), and (crawling prohibited). (2) Legal triage and crawling. We implement site-specific crawlers (one script per site) and exclude sites, low-volume sources, heavily obfuscated structures, and near-duplicates. In this stage, we remove 26 websites from the list. Data from sites is used for training and analysis but is not redistributed. Refinement and Filtering. It is common practice to refine web-collected seed instructions either with templates or LLM rewriting (Mishra et al., 2021; Xu et al., 2023) prior to training. However, we observe that such normalization removes user artifacts (typos, abbreviations, mixed script, and internet style) that harm robustness at deployment, so we keep prompts verbatim. We apply only light automatic filters: discard prompts with Korean-character ratio below 30%, and drop prompts that"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Average scores across HAE-RAE Bench, MCLM, and KMMLU-Redux for Gemma-3-4B and Kanana-1.5-8B under three settings. (a) Augmentation. Option and Style are comparable on Gemma-3-4B (49.3 vs 49.5), while Option has modest edge on Kanana-1.5-8B (58.8 vs 56.6); neither augmentation is uniformly superior. (b) Teacher (Long CoT). Qwen3-32B yields higher averages than Qwen3-4B (Gemma: 51.8 > 48.6; Kanana: 63.8 > 56.3). (c) Teacher (Short CoT). With shot CoT, Qwen3-32B tops Gemini-2.5-Pro (Gemma: 41.8> 39.7; Kanana: 48.5 > 45.9). Overall, Language-Mixed CoT and using Qwen3-32B as the teacher provide the strongest gains; both augmentation choices offer benefits. are too short or too long (length < 50 or > 8,192 characters). The Korean threshold was empirically chosen to exclude fully non-Korean items while retaining mixed-language coding prompts. Instruction Statistics. Figure 3 illustrates details on the collected sources and prompts. Initially, roughly 25.9% of our compiled sources were legal websites. However, they tend to be small in scale or legally restricted, so they contribute only minor share to the total number of crawled questions. In contrast, exam and daily communities host extensive, easily crawlable archives and are overrepresented. Given that long chain-of-thought training primarily improves reasoning capabilities rather than those tasks involving knowledge retrieval (Yeo et al., 2025; Sprague et al., 2024), we prioritize STEM/Code/Exam categories in subsequent collection and curation. Adding the OpenThought dataset. Finally, our web-sourced training mix lacks competition-level problems that are known to cultivate reasoning ability (Guan et al., 2025). We therefore add prompts from OpenThought (Guha et al., 2025) by translations through Gemini-2.5-Flash (Comanici et al., 2025). Earlier attempts with GPT-4o-mini (Hurst et al., 2024), Qwen2.5-72B-Instruct (Qwen et al., 2025), and Gemini-2.0-Flash (Deepmind, 2024) produced training instabilities."
        },
        {
            "title": "5.2 RESPONSE GENERATION",
            "content": "SFT over Reinforcement Learning. To build strong Korean reasoning model, we focus on SFT in this work. Although recent studies report sizable gains from RL-based preference optimization (e.g., GRPO (Shao et al., 2024)), particularly for sub-32B models (Rastogi et al., 2025; Guo et al., 2025), these methods presume access to strong base models (Wang et al., 2025). For Korean, such strong seeds are scarce, making RL vulnerable to the cold-start problem with unstable reward learning and poor exploration (Shao et al., 2025). Consequently, we prioritize SFT with curated data to build strong base model for subsequent RL efforts. Importantly, SFT has also been proven to be effective in training reasoning models (Hochlehnert et al., 2025; Ji et al., 2025), making it reliable first step. Response Generation Methodology. To build the SFT dataset, we initially consider two strategies: (a) agreement-sampling, where we sample multiple times from teacher model and accept the first that an LLM judge (Zheng et al., 2023) deems consistent with the web-crawled answer, and (b) hint-based refinement, where we prepend the crawled answer and ask the model to refine it. However, we find both concerning: (a) is prohibitively compute-intensive; (b) risks leakage, artifacts, and distribution shift that can hurt generalization. Moreover, web-scraped answers are unreliable, and the recent strong LLMs have chance to surpass crowd responses. It should also be noted that several works (Toshniwal et al., 2024), including OpenThought (Guha et al., 2025), and S1 (Muennighoff et al., 2025), have empirically shown that response filtering is not necessary, or hardly correlated with"
        },
        {
            "title": "Preprint",
            "content": "Table 2: Contribution of individual training categories. OpenThought and Exams provide the largest gains, followed by Code and Science. Medical boosts ClinicalQA but consistently harms performance on other benchmarks. Each run uses 50k examples from the target category, plus 3k EXAMS items for formatting. Since SCIENCE has only 37k examples, we use 37k+3k without up-sampling. The highest-scoring model is highlighted in green and the lowest-scoring model in red . Abbreviations: Clin. = ClinicalQA."
        },
        {
            "title": "OpenThought\nDaily\nMedical\nCode\nExams\nScience",
            "content": "Gemma-3-4B Kanana-1.5-8B HRB MCLM KMMLU-R Clin. HRB MCLM KMMLU-R Clin. 54.9 54.2 50.5 53.5 56.4 52.2 55.8 34.9 20.9 38.8 27.9 37.2 53.0 51.9 49.4 51.5 64.2 52.1 62.1 62.4 65.6 59.0 60.0 61.9 74.6 69.1 64.5 69.4 69.5 68.3 57.4 36.4 28.7 38.0 33.3 41. 64.4 58.7 57.0 59.1 67.0 58.8 73.97 70.0 70.3 64.4 69.9 67.5 the performance of the downstream model. We, therefore, choose to regenerate all targets from the prompt alone with strong teacher, without any web-collected oracle. Selecting Response Format and Teacher Model. To select the teacher model, we evaluate two candidates, Qwen3-32B and Qwen3-4B. We also test short-CoT setting, where the model is trained on plain instructional responses without explicit reasoning traces, similar to conventional instruction-tuning outputs. This variant is implemented with Qwen3-32B (reasoning disabled) and Gemini-2.5-Pro. Figure 4 reports the downstream results across teachers. As expected, Qwen3-32B with language-mixed CoT delivers the strongest performance. Notably, Qwen3-4B with reasoning surpasses both Gemini-2.5-Pro and Qwen3-32B without reasoning, highlighting the importance of explicit reasoning in unlocking LLM capabilities. Format Augmentation. The Exams category is highly standardized, typically question with four options. To improve robustness beyond the fixed template, we apply two augmentations: (a) Style augmentation. We keep the question unchanged and prepend or append short stylistic directives.4 (b) Option augmentation. We use BM25 retriever (Robertson et al., 1995) over the exam pool to find similar questions and merge their distractor options with the original item. We drop items containing negation cues to avoid semantic flips, remove near-duplicate items, cap the merged list at 10 options, and preserve the original correct option as the gold label. As shown in Figure 4, training with either augmentation alone yields comparable performance, so we adopt both."
        },
        {
            "title": "5.3 DATASET COMPOSITION",
            "content": "Building on these lessons, we use Qwen3-32B to generate language-mixed CoTs for the 5.79M prompts and augmentations. After filtering degenerations and enforcing Korean-ratio bounds, we obtain YI-SANG with 3.7M long-reasoning trajectories. However, while scaling data generally improves performance, multi-epoch training on our full 3.7M instances is impractical due to limited compute budget. We therefore run targeted ablations to identify smaller, high-yield mixture. What benefit does each category bring? We begin by training on each category at time. Each ablation run additionally includes 3,000 items from the Exams category to teach formatting. In Table 2, we observe that OpenThought delivers the largest gains on MCLM, followed by Science and Code. Exams are the most effective source for HAE-RAE Bench and KMMLU-Redux. Notably, the Medical category appears highly specialized: it boosts ClinicalQA but significantly hinders performance on all other benchmarks. These trends hold across both models, suggesting that the most effective mixture uses OpenThought and Exams as the foundation and adds Science/Code for additional math robustness. Additionally, we find that scaling the Daily and Medical subsets has adverse effects on overall performance, leading us to exclude them from the final training composition. 5 Finalizing the dataset. Finally, we decide to train only with: OpenThought, Code, Exams, and Science. This approximates about 1.8M instances. To surface data issues, we conduct one-epoch shakedown run with proxy model (Kanana-1.5-2.1B). We define loss spike as an abrupt rise in 4Examples include: return the final answer in boxed{} format, or output format: answer:<N>. 5See Section A.4 for details."
        },
        {
            "title": "Preprint",
            "content": "loss that does not recover immediately in the subsequent step. When such spikes occur, we locate the batch, manually inspect the items, implement rule-based filter to remove the failure pattern from the entire dataset, and restart the run. This process is repeated until the loss curve stabilizes. During this process, we identify three recurring triggers: degeneration cases where responses endlessly repeat identical phrases; samples that contain multiple <think> ... </think> blocks; and instances in which the final solution after the </think> tag is written in non-Korean language. We also find that small number of extremely long reasoning traces disproportionately slow training, leading us to discard any instance exceeding 16k tokens. Finally, we apply an n-gram similarity filter (n = 13) to decontaminate against both held-in and held-out benchmarks. After all filtering steps, the finalized YI-SANG-HQ corpus contains 260k instances, composed of 62k from OpenThought, 86k from Code, 37k from Science, and 66k from Exams. We also find that small number of extremely long reasoning traces disproportionately slow training, leading us to discard any instance exceeding 16k tokens. Finally, we apply an n-gram similarity filter (n = 13) to decontaminate against both held-in and held-out benchmarks. After all filtering steps, the finalized YI-SANG-HQ corpus contains 260k instances, composed of 62k from OpenThought, 86k from Code, 37k from Science, and 66k from Exams."
        },
        {
            "title": "6 RESULTS",
            "content": "Table 3: Comparison of 20B+ reasoning models. KO-REAson-35B (ours) matches state-of-the-art peers of similar scale while using only openly available data and code. Entries are reported as meanSE over = 3 independent runs. Bold marks the row-best; underline marks the second-best. When standard-error intervals overlap, ties are co-highlighted. Exact prompts are provided in the supplementary materials. Math (Ko) and AIME24 (Ko) are subsets of MCLM. Category Benchmark GPT-OSS-20B DS-R1-32B EXAONE-Deep-32B QwQ-32B KO-REAson-35B General Reasoning Ko-Specific KMMLU-Redux KMMLU-Pro KMMLU-Hard Math (Ko) AIME2024 (Ko) KSM HRB CLIcK KoBALT-700 Average 67.60.1 42.90.5 39.00.2 82.81.7 71.16.9 72.14.7 65.10.7 57.20.7 31.01. 58.81.2 70.01.6 45.70.3 43.31.0 85.42.1 51.77.1 62.85.1 70.80.4 66.60.6 33.30.1 56.44.5 68.22.2 43.51.8 43.51. 84.82.9 58.311.8 65.717.9 76.10.3 67.60.3 32.65.6 57.45.2 74.71.0 51.01.1 49.01.0 82.30.7 53.39.4 60.514.4 75.51.1 70.90.6 37.72. 59.63.1 76.00.4 47.40.6 51.40.5 87.50.6 66.711.5 65.78.6 78.90.7 70.90.3 34.91.0 64.02.5 YI-SANG-HQ Achieves State-Of-The-Art Performance. KO-REAson-35B, based on A.X-3.1 and trained on YI-SANG-HQ, outperforms state-of-the-art reasoning models of comparable scale, including GPT-OSS-20B (Agarwal et al., 2025), DeepSeek-R1-32B (Guo et al., 2025), EXAONEDeep-32B (Research et al., 2025), and QwQ-32B (Team, 2025). In Table 3, across nine benchmarks, KO-REAson-35B achieves the best performance on five tasks and ranks second on the remaining four, achieving the highest overall average. We note that performance on competition-level math datasets (AIME2024, KSM) trails GPT-OSS-20B, which we attribute to the relatively small amount of competition-style reasoning data in our mixture: only 60k translated OpenThought items after filtering, compared to nearly 1M competition-style problems in the original OpenThought project and 0.5M in Liu et al. (2025b). Nevertheless, KO-REAson-35B ranks second place on both benchmarks while relying primarily on web-collected data that constitutes the majority of training. This underscores the quality of the newly collected user prompts. We leave to future work on incorporating larger volumes of translated competition-style data to push performance further. YI-SANG-HQ Demonstrates Persistent Gains Across Model Size and Family. To further validate the efficacy of YI-SANG-HQ across diverse settings, we train nine models spanning 4B to 35B parameters from six different model families. Improvements are consistent across both scale and architecture, with especially pronounced gains on math-intensive benchmarks such as Math (Ko), AIME2024, and KSM, where models of all sizes benefit substantially. Korean-specific tasks (HRB1.0, CLIcK, and KoBALT-700) also show steady improvements, underscoring the value of YI-SANG-HQs"
        },
        {
            "title": "Preprint",
            "content": "Table 4: Performance of nine models (4B35B) trained on YI-SANG-HQ. The benefits of YI-SANG-HQ are consistent across model families and parameter scales. Results are meanSE over = 3 independent runs. Cases where performance drops after training (without overlap of standard errors) are highlighted . Abbreviations: K.M.-R = KMMLU-Redux; K.M.-P = KMMLU-Pro; K.M.-H = KMMLU-Hard. Model K.M.-R K.M.-P K.M.-H MATH AIME24 KSM HRB CLIcK KoBALT Gemma-3-4B + YI-SANG-HQ 40.71.7 65.53.5 26.71.3 35.33.6 19.40.2 41.63.2 Qwen-2.5-7B + YI-SANG-HQ A.X-3.1-7B + YI-SANG-HQ KONI-Llama-3.1-8B + YI-SANG-HQ Llama-3.1-8B + YI-SANG-HQ Kanana-1.5-8B + YI-SANG-HQ Gemma-3-12B + YI-SANG-HQ Qwen-2.5-14B + YI-SANG-HQ 52.60.3 72.00.4 62.40.5 70.00.9 20.70.4 69.60.1 40.01.2 68.90.2 53.74.9 70.70.5 59.10.6 72.70.9 24.41.3 77.10.7 34.00.1 44.60.5 38.80.3 39.00.7 16.00.6 39.60.5 23.81.3 38.60.4 37.70.2 39.90.7 39.90.3 43.21.2 22.70.6 50.00.2 20.70.2 46.70.1 36.32.0 45.70.5 9.70.4 44.70.6 19.50.2 45.30.0 27.20.1 44.80.5 29.80.0 47.10.1 14.01.0 51.50. <5B Models 41.925.0 69.71.4 <10B Models 58.16.4 77.30.7 48.218.9 82.82.9 18.72.1 71.71.4 29.37.1 72.20.7 54.50.0 67.71.4 <20B Models 73.22.1 75.30.7 64.62.9 81.81.4 <30B Models 1.72.4 15.02.4 12.86.7 38.813. 49.15.2 61.09.9 45.94.0 55.25.1 12.02.9 20.04.1 6.70.0 41.711.8 34.639.6 33.314.1 3.30.0 31.77.1 1.72.4 26.714.1 10.00.0 30.00.0 15.07.1 35.07.1 8.32.4 38.32.4 15.80.3 49.71.5 17.33.5 53.41.3 4.80.2 38.30.4 5.10.2 38.70.7 15.00.1 39.82. 28.10.3 46.16.7 20.70.8 55.68.4 60.41.1 65.00.8 71.30.7 72.50.9 21.71.8 58.30.8 43.70.4 58.30.4 70.38.2 72.90.9 69.80.2 68.80.2 20.79.3 74.50.2 56.90.6 61.00.4 64.80.4 62.00.9 21.90.4 56.51.0 41.50.6 54.90.4 63.50.3 64.00.4 62.20.5 64.60.2 31.50.7 67.50.5 19.30.8 24.11.4 25.02.3 23.90.7 0.50.2 21.40.4 8.10.6 18.90.1 20.60.5 28.60. 26.00.1 29.60.1 19.80.8 34.51.3 A.X-3.1-35B + YI-SANG-HQ 72.20.2 76.00.4 47.30.7 47.40.6 44.20.2 51.40.5 73.12.1 84.50. 16.73.3 66.711.5 26.80.8 65.78.6 84.00.5 78.90.7 76.60.6 70.90.3 35.50.2 34.91.0 curated multilingual and culturally grounded data. General knowledge evaluations (KMMLU-Redux, KMMLU-Pro, KMMLU-Hard) likewise improve, further demonstrating the broad coverage of the dataset. Performance degradation is observed in only two cases, both with marginal drops of less than two points. Overall, YI-SANG-HQ proves to be versatile and widely applicable training resource, capable of boosting models across families and scales, and offering substantial value for future research in multilingual and reasoning-focused LLMs. Cross-Lingual and Multi-Modal Free Lunch. To investigate whether post-training on YI-SANG-HQ yields broader generalization, we evaluate Gemma3-12B and its post-trained variant (KO-REAson-12B) on two English reasoning benchmarks (AIME-2025, short-form math; GPQA, STEM MCQA (Rein et al., 2024)) and two Korean vision-language benchmarks (KAIO-2, short-form STEM reasoning (Lee et al., 2025b); HAERAE-Vision, long-form commonsense reasoning). KO-REAson-12B outperforms the base model on all four, indicating both cross-lingual and multimodal gains. We attribute the English improvements to two factors: (i) the benchmarks emphasize largely universal math and science knowledge, which facilitates transfer across languages, and (ii) our Language-Mixed CoT includes English reasoning steps that push general reasoning capabilities. English performance gains are consistent over all trained models, see Table 11 for more details. Additionally, we also observe gains on visual reasoning despite no image data in posttraining, consistent with prior reports of multi-modal free lunch. (Choi et al., 2024; Rastogi et al., 2025) However, the transfer appears selective, with strong gains on 9 Figure 5: Performance of Gemma3-12B and its post-trained variant on English reasoning benchmarks and Korean multimodal benchmarks. KO-REASON-12B, trained only with text supervision, shows consistent gains across all tasks, indicating both cross-lingual and multimodal transfer."
        },
        {
            "title": "Preprint",
            "content": "reasoning-heavy tasks and limited benefit on shallow, factoid-style evaluations. See Appendix D.2 for complete results and experimental details."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this work, we present practical recipes for building reasoning models for mid-resource languages through Korean case study. We introduce Language-Mixed CoT and curate 5.9M native-authored Korean prompts, underscoring the value of better supervision signals and high-quality local data. Using Qwen3-32B as the teacher, we construct and release YI-SANG, the largest publicly available Korean training resource. Its high-yield subset, YI-SANG-HQ, delivers consistent gains in general knowledge and reasoning across six model families spanning 4B35B parameters, rivaling models trained on proprietary data. We hope our work benefits Korean practitioners and the broader multilingual community, offering guidance for training their own reasoning LLMs."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was supported in part by the Korea Institute of Science and Technology Information (KISTI) under Grant K25L1M1C1 for KONI (KISTI Open Neural Intelligence), large language model specialized in science and technology; and by the Brian Impact Foundation, nonprofit dedicated to advancing science and technology for all."
        },
        {
            "title": "REFERENCES",
            "content": "Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. Yunju Bak, Hojin Lee, Minho Ryu, Jiyeon Ham, Seungjae Jung, Daniel Wontae Nam, Taegyeong Eo, Donghun Lee, Doohae Jung, Boseop Kim, et al. Kanana: Compute-efficient bilingual language models. arXiv preprint arXiv:2502.18934, 2025. Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, et al. Llama-nemotron: Efficient reasoning models. arXiv preprint arXiv:2505.00949, 2025. Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acereason-nemotron: Advancing math and code reasoning through reinforcement learning. arXiv preprint arXiv:2505.16400, 2025. Dasol Choi, Guijin Son, Soo Yong Kim, Gio Paik, and Seunghyeok Hong. Improving fine-grained visual understanding in vlms through text-only training. arXiv preprint arXiv:2412.12940, 2024. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in neural information processing systems, 35: 1634416359, 2022. Google Deepmind. 2024. google-gemini-ai-update-december-2024/#project-astra. Introducing gemini 2.0: our new ai model the agentic era, https://blog.google/technology/google-deepmind/"
        },
        {
            "title": "URL",
            "content": "for Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, Wanjia Zhao, John Yang, Shreyas Pimpalgaonkar, Kartik Sharma, Charlie Cheng-Jie Ji, Yichuan Deng, Sarah Pratt, Vivek Ramanujan, Jon Saad-Falcon, Jeffrey Li, Achal Dave, Alon Albalak, Kushal Arora, Blake Wulfe, Chinmay Hegde, Greg Durrett, Sewoong Oh, Mohit Bansal, Saadia Gabriel, Aditya Grover, Kai-Wei Chang, Vaishaal Shankar, Aaron Gokaslan, Mike A. Merrill, Tatsunori Hashimoto, Yejin Choi, Jenia Jitsev, Reinhard Heckel, Maheswaran Sathiamoorthy, Alexandros G. Dimakis, and Ludwig Schmidt. Openthoughts: Data recipes for reasoning models, 2025. URL https://arxiv.org/abs/2506.04178. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Huy Hoang Ha. Pensez: Less data, better reasoningrethinking french llm. arXiv preprint arXiv:2503.13661, 2025."
        },
        {
            "title": "Preprint",
            "content": "Tao He, Rongchuan Mu, Lizi Liao, Yixin Cao, Ming Liu, and Bing Qin. Good learners think their thinking: Generative prm makes large reasoning model more efficient math learner. arXiv preprint arXiv:2507.23317, 2025. Andreas Hochlehnert, Hardik Bhatnagar, Vishaal Udandarao, Samuel Albanie, Ameya Prabhu, and Matthias Bethge. sober look at progress in language model reasoning: Pitfalls and paths to reproducibility. arXiv preprint arXiv:2504.07086, 2025. Jiwoo Hong, Noah Lee, Rodrigo Martınez-Castano, Cesar Rodrıguez, and James Thorne. Crosslingual transfer of reward models in multilingual alignment. arXiv preprint arXiv:2410.18027, 2024. Seokhee Hong, Sunkyoung Kim, Guijin Son, Soyeon Kim, Yeonjung Hong, and Jinsik Lee. From kmmlu-redux to kmmlu-pro: professional korean benchmark suite for llm evaluation. arXiv preprint arXiv:2507.08924, 2025. Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, and Yanning Chen. Liger kernel: Efficient triton kernels for llm training. arXiv preprint arXiv:2410.10989, 2024. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https: //github.com/huggingface/open-r1. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Yunjie Ji, Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Han Zhao, and Xiangang Li. Am-thinking-v1: Advancing the frontier of reasoning at 32b scale. arXiv preprint arXiv:2505.08311, 2025. Andy Jones. Scaling scaling laws with board games. arXiv preprint arXiv:2104.03113, 2021. Eunsu Kim, Juyoung Suk, Philhoon Oh, Haneul Yoo, James Thorne, and Alice Oh. Click: benchmark dataset of cultural and linguistic intelligence in korean. arXiv preprint arXiv:2403.06412, 2024. KISTI. Kisti-koni/koni-llama3.1-70b-instruct-preview. 2024. URL https://huggingface. co/KISTI-KONI/KONI-Llama3.1-70B-Instruct-preview. Hyunwoo Ko, Guijin Son, and Dasol Choi. Understand, solve and translate: Bridging the multilingual mathematical reasoning gap. arXiv preprint arXiv:2501.02448, 2025. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. SKT AI Model Lab. A.x 3.1. 2025. URL https://huggingface.co/skt/A.X-3.1. Jungyup Lee, Jemin Kim, Sang Park, and SeungJae Lee. Dna r1, 2025a. URL https: //huggingface.co/dnotitia/DNA-R1. Nahyun Lee, Guijin Son, Hyunwoo Ko, and Kyubeen Han. Kaio: collection of more challenging korean questions. arXiv preprint arXiv:2509.14752, 2025b."
        },
        {
            "title": "Preprint",
            "content": "Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, [https://huggingface. Guillaume Lample, and Stanislas Polu. co/AI-MO/NuminaMath-1.5](https://github.com/project-numina/ aimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024. Numinamath. Yafu Li, Ronghao Zhang, Zhilin Wang, Huajian Zhang, Leyang Cui, Yongjing Yin, Tong Xiao, and Yue Zhang. Lost in literalism: How supervised training shapes translationese in llms. arXiv preprint arXiv:2503.04369, 2025a. Yu Li, Zhuoshi Pan, Honglin Lin, Mengyuan Sun, Conghui He, and Lijun Wu. Can one domain help others? data-centric study on multi-domain reasoning via reinforcement learning. arXiv preprint arXiv:2507.17512, 2025b. lightblue. lightblue/DeepSeek-R1-Distill-Qwen-1.5B-Multilingual. https://huggingface. co/lightblue/DeepSeek-R1-Distill-Qwen-1.5B-Multilingual, 2025. Hugging Face model card, accessed 21 July 2025. Cong Liu, Zhong Wang, ShengYu Shen, Jialiang Peng, Xiaoli Zhang, ZhenDong Du, and YaFang Wang. The chinese dataset distilled from deepseek-r1-671b. https://huggingface.co/ datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k, 2025a. Yifei Liu, Li Lyna Zhang, Yi Zhu, Bingcheng Dong, Xudong Zhou, Ning Shang, Fan Yang, and Mao Yang. rstar-coder: Scaling competitive code reasoning with large-scale verified dataset. arXiv preprint arXiv:2505.21297, 2025b. Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time scaling for generalist reward modeling. arXiv preprint arXiv:2504.02495, 2025c. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1preview with 1.5b model by scaling rl. https://pretty-radio-b75.notion.site/ DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2, 2025. Notion Blog. Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773, 2021. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, et al. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656, 2024. ChaeHun Park, Yujin Baek, Jaeseok Kim, Yu-Jung Heo, Du-Seong Chang, and Jaegul Choo. Evaluating visual and cultural interpretation: The k-viscuit benchmark with human-vlm collaboration. arXiv preprint arXiv:2406.16469, 2024. Cheonbok Park, Jeonghoon Kim, Joosung Lee, Sanghwan Bae, Jaegul Choo, and Kang Min Yoo. Cross-lingual collapse: How language-centric foundation models shape reasoning in large language models. arXiv preprint arXiv:2506.05850, 2025. Kunat Pipatanakul, Pittawat Taveekitworachai, Potsawee Manakul, and Kasima Tharnpipitchai. Adapting language-specific llms to reasoning model in one day via model mergingan open recipe. arXiv preprint arXiv:2502.09056, 2025. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115."
        },
        {
            "title": "Preprint",
            "content": "Leonardo Ranaldi and Giulia Pucci. Multilingual reasoning via self-training. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 1156611582, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.577. URL https://aclanthology.org/2025.naacl-long.577/. Abhinav Rastogi, Albert Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, et al. Magistral. arXiv preprint arXiv:2506.10910, 2025. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. LG Research, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Yemuk Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Kijeong Jeon, et al. Exaone deep: Reasoning enhanced language models. arXiv preprint arXiv:2503.12524, 2025. Stephen Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, Mike Gatford, et al. Okapi at TREC-3. British Library Research and Development Department, 1995. Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, et al. Spurious rewards: Rethinking training signals in rlvr. arXiv preprint arXiv:2506.10947, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Hyopil Shin, Sangah Lee, Dongjun Jang, Wooseok Song, Jaeyoon Kim, Chaeyoung Oh, Hyemi Jo, Youngchae Ahn, Sihyun Oh, Hyohyeong Chang, et al. Kobalt: Korean benchmark for advanced linguistic tasks. arXiv preprint arXiv:2505.16125, 2025. Guijin Son, Hanwool Lee, Suwan Kim, Huiseo Kim, Jaecheol Lee, Je Won Yeom, Jihyu Jung, Jung Woo Kim, and Songseong Kim. Hae-rae bench: Evaluation of korean knowledge in language models. arXiv preprint arXiv:2309.02706, 2023. Guijin Son, Hanwool Lee, Sungdong Kim, Seungone Kim, Niklas Muennighoff, Taekyoon Choi, Cheonbok Park, Kang Min Yoo, and Stella Biderman. Kmmlu: Measuring massive multitask language understanding in korean. arXiv preprint arXiv:2402.11548, 2024. Guijin Son, Jiwoo Hong, Hyunwoo Ko, and James Thorne. Linguistic generalizability of test-time scaling in mathematical reasoning. arXiv preprint arXiv:2502.17407, 2025. Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Durrett. To cot or not to cot? chainof-thought helps mainly on math and symbolic reasoning. arXiv preprint arXiv:2409.12183, 2024. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https: //qwenlm.github.io/blog/qwq-32b/. Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data. arXiv preprint arXiv:2410.01560, 2024. Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. Octothinker: Mid-training incentivizes reinforcement learning scaling. arXiv preprint arXiv:2506.20512, 2025."
        },
        {
            "title": "Preprint",
            "content": "Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460, 2025. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chainof-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. Zheng-Xin Yong, Farid Adilazuarda, Jonibek Mansurov, Ruochen Zhang, Niklas Muennighoff, Carsten Eickhoff, Genta Indra Winata, Julia Kreutzer, Stephen Bach, and Alham Fikri Aji. Crosslingual reasoning through test-time scaling. arXiv preprint arXiv:2505.05408, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023."
        },
        {
            "title": "Preprint",
            "content": "A ADDITIONAL DETAILS ON YI-SANG. A.1 ORIGIN Our dataset takes its name from Yi Sang (1910-1937; pen name of Kim Hae-gyeong), Korean modernist and architect, known for his mathematically inflected literature. He employed geometric notation, numerical sequences, and experimental layouts into Korean literacy works. The name reflects our focus on formal Korean reasoning. Yi Sang also echoes Korean noun, meaning the most complete state, consistent with our goal to create the strongest reasoning dataset. A.2 PROMPTS Figure 6 presents the system prompt used throughout the paper to generate Language-Mixed CoT from teacher models. We notice that longer and more detailed instructions are likely to constrain stylistic diversity of responses. Therefore, we keep the prompt as simple as possible. Think carefully, do not translate the question while solving. Preserve the question in Korean so that you keep all details without adding noise. After you finish thinking, state your answer in fluent and coherent Korean. Figure 6: System prompt used for dataset generation. A.3 LICENSE In Table 5 we detail the license of our trained models. The models will be made available on HuggingFace. Both datasets YI-SANG and YI-SANG-HQ will be made available under the MIT License. Table 5: Summary of Base models, upstream licenses, our trained model names, and release licenses. We resort to the most open license possible. Base Model Upstream License Trained Model (ours) Release License Gemma3-4B Gemma3-12B Llama-3.1-8B KONI-Llama-3.1-8B A.X-3.1-Light A.X-3.1 Qwen2.5-7B Qwen2.5-14B Kanana1.5-8B Gemma License Gemma License Llama3 Community License Llama3 Community License Apache 2.0 Apache 2.0 Apache 2.0 Apache 2.0 Apache 2.0 KO-REAson-G3-4B-0831 KO-REAson-G3-12B-1002 KO-REAson-L3 1-8B-0831 KO-REAson-KL3 1-8B-0831 KO-REAson-AX3 1-8B-0831 KO-REAson-AX3 1-35B-1002 KO-REAson-Q2 5-7B-0831 KO-REAson-Q2 5-14B-1002 KO-REAson-K2505-8B-0831 Gemma License Gemma License Llama3 Community License Llama3 Community License Apache 2.0 Apache 2.0 Apache 2.0 Apache 2.0 Apache 2. A.4 ADDITIONAL ABLATIONS Scaling the Medical subset. To test for emergent gains, we double the Medical subset from 50k to 100k and retrain. Table 6 reports the performance gains relative to 50k. Gemma-3-4B decreases on all benchmarks, with the largest drop on ClinicalQA. Kanana-1.5-8B exhibits near-zero changes. Therefore, we exclude the Medical category from the final training mixture. Table 6: Size ablation on the Medical subset. Doubling the Medical subset from 50k to 100k leads to negative performance effects. Reported values show the change in accuracy; Avg (non-Clin.) is the unweighted mean of non-clinical benchmarks. Model Avg (non-Clin.) ClinicalQA Gemma-3-4B Kanana-1.5-8B 0.76 +0.09 2.30 +0.10 Scaling the Daily subset. Table 2 shows that Daily rarely leads any benchmark. We scale Daily by {20, 50, 100}k and mix 15k instances each from OpenThought and Exams. The two datasets are added to prevent downstream models from showing deflated scores on the academic benchmarks,"
        },
        {
            "title": "Preprint",
            "content": "since the Daily category is likely to lack academic value. As reported in Table 7, performance consistently drops as we scale. Therefore, we also exclude the Daily category. Table 7: Size ablation on the Daily subset. Overall performance declines as the subset increases in size. This may partly stem from limited benchmark coverage; nonetheless, the evidence is not enough to tolerate consistent drops across the remaining benchmarks. The highest-scoring model is highlighted in green . Data Mix Gemma-3-4B HRB MCLM KMMLU-R Kanana-1.5-8B HRB MCLM KMMLU-R 2:1.5:1.5 5:1.5:1.5 10:1.5:1. 56.2 55.5 55.8 48.8 48.1 40.3 54.0 53.0 51.6 73.1 68.9 69.9 48.8 45.7 43.4 60.8 59.7 58. A.5 ABLATION DETAILS Table 8 and 9 provide detailed results behind Figure 4. Table 8: Comparison of two augmentation strategies (style and option); no single method demonstrates clear advantage. The highest-scoring model is highlighted in green . Augmentation Gemma-3-4B HRB MCLM KMMLU-R Kanana-1.5-8B HRB MCLM KMMLU-R Style Option 56.4 55.8 27.9 30.2 64.2 61.9 69.5 72.8 33.3 37. 67.0 66.5 Table 9: Comparison of different teacher models and response formats. Training on long chain-of-thought reasoning generated by Qwen3-32B shows the best performance. Performance caps are most pronounced in the MCLM benchmark, implying its effectiveness in boosting reasoning performance. The highest-scoring model is highlighted in green . Teacher Model Gemma-3-4B HRB MCLM KMMLU-R Kanana-1.5-8B HRB MCLM KMMLU-R Language-Mixed CoT Qwen3-32B Qwen3-4B 54.4 48.6 48.1 45.0 53.0 52.3 73.1 67.8 Solution Only (Short CoT) Gemini-2.5-Pro Qwen3-32B 49.5 51.3 25.6 28.7 44.1 45.3 67.6 68.5 57.4 41. 24.0 23.3 60.8 59.1 46.2 53.7 ADDITIONAL DETAILS ON MODEL TRAINING. B.1 MODELS Gemma-3 (Team et al., 2025) is Googles third-generation open model family. We use 4B and 12B instruction-tuned variants. Gemma-3 is multimodal model (text and vision), though in this work we use it purely for text. The 4B version is pretrained on roughly 4T tokens, and the 12B on about 12T tokens. It is massively multilingual, covering more than 140 languages without special focus on any single one. Qwen-2.5 (Qwen et al., 2025) is built by Alibaba Cloud and trained on up to 18T tokens. It is grounded primarily in Chinese and English, but demonstrates solid multilingual capabilities with decent coverage of Korean (Hong et al., 2025). In our experiments, we use both the 7B and 14B instruction-tuned variants."
        },
        {
            "title": "Preprint",
            "content": "A.X-3.1 (Lab, 2025) is family of LLaMA-style models developed by SK Telecom with particular focus on Korean. It is trained on approximately 2.1 trillion tokens and achieves top-tier scores on Korean benchmarks, such as KMMLU, while still performing well in English. We employ both the 8B and 35B variants. Kanana-1.5-8B (Bak et al., 2025) is bilingual EnglishKorean model, trained by Kakao, with an 8B parameter LLaMA-style transformer. It is trained on about 3T tokens, with more than 10% Korean content, while the rest is primarily English. The training recipe includes staged pretraining and efficiency optimizations. Llama-3.1-8B-Instruct (Grattafiori et al., 2024) is trained on approximately 15T tokens and designed as multilingual model but with emphasis on eight major languages, including English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai. Although it is broadly multilingual, it remains relatively English-centric. KONI-Llama-3.1-8B (KISTI, 2024) is continual pretrained variant of Llama-3.1 developed by KISTI. It starts from the base Llama-3.1-8B architecture and undergoes continued pretraining on 0.5 trillion tokens of additional Korean text and domain-specific corpora in science and technology. B.2 HYPERPARAMETERS Training hardware spans from eight NVIDIA H100 to twenty-four NVIDIA H200 GPUs. Ablations use 5 epochs, global batch size of 128, bfloat16 precision, and AdamW (learning rate 2 105 with 10% warmup; weight decay 1 105). Loss is computed only on reasoning traces and solutions. We employ PyTorch FSDP, Liger kernels (Hsu et al., 2024), and FlashAttention-2 (Dao et al., 2022). For the final runs on YI-SANG-HQ we scale the global batch size to 512. B.3 PACKING )We train Gemma-3-4B and Kanana-1.5-8B on YI-SANG-HQ under two settings (with vs. without packing). Although packing provided substantial speedups, as shown in Table 10 we observe measurable drops on general-knowledge and reasoning benchmarks; accordingly, all reported models are trained without packing. Benchmarks Gemma-3-4B Kanana-1.5-8B packing wo packing packing wo packing KMMLU-Redux HAE-RAE Bench MCLM-Ko Training Time 62.87 59.62 55. 576 64.19 55.33 58.91 1728 70.06 75.88 62.02 1296 71.30 73.73 65. 3360 Table 10: Comparison of model performance with and without packing."
        },
        {
            "title": "C ADDITIONAL DETAILS ON EVALUATION",
            "content": "C.1 PROMPTS Figure 7 is the prompt used for evaluation. 문제 풀이를 마친 후, 최종 정답을 다음 형식으로 작성해 주세요: boxed{N}. Figure 7: System prompt used for evaluation on Korean benchmarks."
        },
        {
            "title": "Preprint",
            "content": "C.2 PROCESSING DETAILS We extract each models final answer from the first boxed{...}, that appears after the models hidden think (reasoning) content. Any boxed{...}, strings that occur inside the think section are ignored. If multiple boxed{...}, entries appear in the visible answer, we always take the first one and disregard the rest, even if they contradict one another. An answer is credited only if this first post-think boxed{...}, is parsable. If the model fails to produce parsable boxed{...},, the response is marked incorrect, even when the correct value appears elsewhere in plain text. If generation runs to the maximum token limit and no parsable boxed{...}, is produced (typically due to degeneration), the item is marked incorrect. By contrast, if generation is interrupted before reaching the max token limit due to hardware or runtime failure, we re-run the same prompt once with the same decoding settings; the score is based on the retry."
        },
        {
            "title": "D ADDITIONAL RESULTS",
            "content": "D.1 CROSS-LINGUAL GAINS ON ENGLISH BENCHMARKS Table 11: Performance of nine models (4B35B) trained on YI-SANG-HQ. Results are meanSE over n=3 runs on AIME24, AIME25, and GPQA. The benefits of YI-SANG-HQ are consistent across model families and scales. Model AIME24 AIME25 GPQA <5B Models Gemma-3-4B + YI-SANG-HQ 6.75.8 23.317.3 10.08.8 22.26.9 19.52.9 32.27.7 <10B Models Qwen-2.5-7B + YI-SANG-HQ A.X-3.1-7B + YI-SANG-HQ KONI-Llama-3.1-8B + YI-SANG-HQ Llama-3.1-8B + YI-SANG-HQ Kanana-1.5-8B + YI-SANG-HQ 6.70.0 41.11.9 13.30.0 46.75.8 0.00.0 21.11.9 0.00.0 28.93.8 5.61.9 25.67.7 7.81.9 34.43.8 13.35.8 31.11.9 0.00.0 32.21.9 0.00.0 21.11.9 12.21.9 27.81.9 27.13.5 43.11.3 25.63.7 37.72.9 14.11.5 39.20.8 19.20.5 40.20.8 31.12.5 38.90.5 <20B Models Gemma-3-12B + YI-SANG-HQ Qwen-2.5-14B + YI-SANG-HQ 13.30.0 42.27.7 7.85.1 41.115.0 15.61.9 30.05.8 13.33.3 42.210.2 32.01.2 45.16.7 26.31.3 51.76.6 <30B / 35B Models A.X-3.1-35B + YI-SANG-HQ 15.63.8 58.916. 15.61.9 53.312.0 37.00.8 47.85.3 Alongside the results in Table 4, we also observe consistent gains on English reasoning benchmarks such as AIME2024/2025 and GPQA (Rein et al., 2024). While the improvements are not yet sufficient to rival state-of-the-art systems of similar scale, it is notable that every model improves across all English benchmarks despite never seeing English prompts during training. We attribute this to two factors. First, the math and science benchmarks used here largely test universal knowledge, making them less dependent on the training language and enabling transfer from the Korean supervision. Second, the proposed Language-Mixed CoT likely helps models maintain alignment with their original English distribution, since they continue to practice reasoning partly in English. These findings highlight promising directions for further study on cross-lingual transfer in reasoning. D.2 CROSS-MODAL GAINS ON VISUAL LANGUAGE BENCHMARKS"
        },
        {
            "title": "Preprint",
            "content": "Earlier works have discovered the multi-modal free lunch, noting that Visual Language Models (VLMs) trained with text-only reasoning data often improve across wide range of vision benchmarks (Choi et al., 2024; Li et al., 2025b). We extend this line of inquiry by evaluating Gemma3-12B, model with visual encoder but trained solely on YI-SANG-HQ, across three multimodal benchmarks: K-Viscuit (knowledge-focused, MCQA) (Park et al., 2024), HAERAE-Vision (reasoning, long-form)6, and KAIO-2 (STEM/reasoning, short-form) (Lee et al., 2025b). As shown in Table 8, KO-REAson-12B achieves notable gains on reasoning-oriented tasks despite lacking vision training. Unlike prior reports of across-the-board improvements (Rastogi et al., 2025), however, we find that shallow factoid-style benchmarks such as K-Viscuit see little to no benefit. This suggests that the free lunch of text-based reasoning transfers selectively: boosting reasoning-heavy multimodal tasks, but not those requiring surface-level factual recall. Figure 8: Accuracy on K-Viscuit, KAIO-2, and HAERAE-Vision for Gemma3-12B, Gemma327B, and KO-REASON-12B. KO-REASON-12B is post-trained variant of Gemma3-12B on YISANG-HQ. D.3 IMPORTANCE OF HELD-IN BENCHMARKS AS PRACTICAL PROXIES. While we apply an n-gram filter for decontamination, our iterative process of retraining to refine subsets inevitably uses held-in benchmarks as proxy for progress. This raises the theoretical concern of gradually overfitting to held-in metrics. However, we view this practice as necessary and near-optimal compromise: without reliable proxy, it would not be possible to guide dataset construction effectively. Importantly, we do not advocate abandoning the distinction between held-in and held-out splits; both remain essential for fair evaluation. In practice (Figure 9), we find that performance gains are indeed larger on held-in benchmarks. Still, it should also be noted that gains are smaller at higher baselines overall, and part of the difference reflects the greater difficulty of held-out benchmarks. Crucially, Table 3 and Table 4 show that models trained on YI-SANG-HQ consistently improve across all benchmarks, including unseen held-out targets. This confirms that, despite mild contamination risk, our procedure achieves generalization while ensuring stable progress during training. 6currently under review, and therefore anonymized 20 Figure 9: Comparison of gains in Held-In/Out benchmark suites. Each point is (model, benchmark) pair; x-axis shows the baseline score (%), y-axis shows the improvement after training on the YI-SANG dataset. Green circles are Held-In benchmarks; red squares are Held-Out benchmarks (others). Solid/dashed lines are OLS fits."
        }
    ],
    "affiliations": [
        "KISTI",
        "Korea University",
        "Modulabs",
        "OneLineAI",
        "Oracle AI",
        "Seoul National University",
        "University College Dublin"
    ]
}