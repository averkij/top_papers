{
    "paper_title": "VMoBA: Mixture-of-Block Attention for Video Diffusion Models",
    "authors": [
        "Jianzong Wu",
        "Liang Hou",
        "Haotian Yang",
        "Xin Tao",
        "Ye Tian",
        "Pengfei Wan",
        "Di Zhang",
        "Yunhai Tong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The quadratic complexity of full attention mechanisms poses a significant bottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration, high-resolution videos. While various sparse attention methods have been proposed, many are designed as training-free inference accelerators or do not optimally capture the unique spatio-temporal characteristics inherent in video data when trained natively. This paper introduces Video Mixture of Block Attention (VMoBA), a novel sparse attention mechanism specifically adapted for VDMs. Motivated by an in-depth analysis of attention patterns within pre-trained video transformers, which revealed strong spatio-temporal locality, varying query importance, and head-specific concentration levels, VMoBA enhances the original MoBA framework with three key modifications: (1) a layer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to diverse spatio-temporal attention patterns and improve efficiency; (2) global block selection to prioritize the most salient query-key block interactions across an entire attention head; and (3) threshold-based block selection to dynamically determine the number of attended blocks based on their cumulative similarity. Extensive experiments demonstrate that VMoBA significantly accelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and 1.48x latency speedup, while attaining comparable or even superior generation quality to full attention. Furthermore, VMoBA exhibits competitive performance in training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for high-res video generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 8 5 8 3 2 . 6 0 5 2 : r VMoBA: Mixture-of-Block Attention for Video Diffusion Models Jianzong Wu1,2, Liang Hou2, Haotian Yang2, Xin Tao2, Ye Tian1 Pengfei Wan2, Di Zhang2, Yunhai Tong1 1Peking University 2Kling Team, Kuaishou Technology Code: https://github.com/KwaiVGI/VMoBA Email: jzwu@stu.pku.edu.cn Figure 1: (a) VMoBA performs better than Full Attention while reducing training time. (b) VMoBA is faster when the sequence length goes higher. The points represent tested latencies, and the lines are fitted with quadratic functions. (c) VMoBA is an efficient and effective adaptation of MoBA."
        },
        {
            "title": "Abstract",
            "content": "The quadratic complexity of full attention mechanisms poses significant bottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration, high-resolution videos. While various sparse attention methods have been proposed, many are designed as training-free inference accelerators or do not optimally capture the unique spatio-temporal characteristics inherent in video data when trained natively. This paper introduces Video Mixture of Block Attention (VMoBA), novel sparse attention mechanism specifically adapted for VDMs. Motivated by an in-depth analysis of attention patterns within pre-trained video transformers, which revealed strong spatio-temporal locality, varying query importance, and headspecific concentration levels, VMoBA enhances the original MoBA framework with three key modifications: (1) layer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to diverse spatio-temporal attention patterns and improve efficiency; (2) global block selection to prioritize the most salient querykey block interactions across an entire attention head; and (3) threshold-based The work is done during Jianzong Wus internship in Kling Team, Kuaishou Technology. Preprint. block selection to dynamically determine the number of attended blocks based on their cumulative similarity. Extensive experiments demonstrate that VMoBA significantly accelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and 1.48x latency speedup, while attaining comparable or even superior generation quality to full attention. Furthermore, VMoBA exhibits competitive performance in training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for high-res video generation."
        },
        {
            "title": "Introduction",
            "content": "Modeling long-sequence data is critical challenge in modern deep learning, particularly for Video Diffusion Models (VDMs) aiming to generate high-resolution, long-duration videos [27, 26, 10, 36, 34, 3, 4, 24, 2, 1, 12, 32]. The ubiquitous full attention mechanism, while powerful, exhibits computational complexity that scales quadratically with the sequence length [31]. This bottlenecks the ability to process longer sequences, especially for high-resolution video. For example, 720p video can exceed 76K tokens (Section 4). To mitigate this, various sparse attention mechanisms have been proposed [19, 41, 22, 42, 44, 37, 43]. The core idea is to restrict each query to interact with only subset of key-value pairs, thereby reducing computational complexity. Sparse attentions for VDMs are primarily designed as training-free methods [37, 42, 43]. However, these methods show suboptimal results without training (Section 4). Leveraging sparse attention to accelerate the training of VDMs on even longer sequences, while being realistic demand, remains relatively underexplored domain. Recently, Mixture of Block Attention (MoBA) [19] is introduced as sparse attention mechanism for training. It demonstrates effectiveness in Large Language Models (LLMs) training for long-context data. Unfortunately, our initial attempt to directly apply MoBA to the training of VDMs yields unsatisfactory results, as illustrated in Fig. 1a, where MoBA shows significant quality drop (VBench Score 68.25 -> 56.88). We attribute this to MoBAs design being primarily optimized for textual data, where the locality spans on the 1D space, leading to performance gap for the video generation task, where the locality lays on the 3D space. To bridge this gap, we conduct task-specific analysis of attention mechanisms within pre-trained Video Diffusion Transformers (DiTs). Our analysis (detailed in Section 3) reveals several key insights: Observation 1: Full attention map has 1-2-3D attention patterns. Video data exhibits strong spatio-temporal locality, with attention mechanisms showing focusing on tokens within local one, two, or three dimensional neighborhoods  (Fig. 3)  . MoBAs original key block partitioning flattening the latent space into one-dimensional sequence and then uniformly splitting into blocks fundamentally disrupts this locality. Spatially adjacent tokens may be assigned to different blocks, diluting the representational power of block means. Existing sparse attention methods, such as SparseVideoGen [37], categorizes attention heads into spatial and temporal. However, the role of any particular head can shift with changes in prompt, diffusion step, or layer. Lacking reliable prior on which heads should be spatial or temporal, SparseVideoGen computes small pilot attention before each layer to classify head types, incurring substantial overhead that, as sequence length grows, can render it even slower than full attention, as shown in Fig. 1b. Observation 2: Queries have various importance. Attention maps indicate that different query tokens receive varying degrees of attention, and their top similarity scores with key tokens can differ significantly  (Fig. 4)  . Selecting the same number of key blocks for each query might under-allocate resources to queries that inherently have stronger affinities with keys. Observation 3: Heads have different concentration levels. The concentration level of similarity scores varies across different attention heads  (Fig. 5)  . Some heads exhibit highly concentrated similarities, while others show more smoothed patterns. fixed top-k selection, as in MoBA, may not be optimal because it ignores that different heads have different similarity-score distributions. Motivated by these observations, we propose VMoBA, sparse attention specifically adapted for VDMs training. There are three key innovations corresponding to the observations: 2 Innovation 1: 1-2-3D block partition. Key blocks are partitioned using cyclical 1D-2D-3D scheme across layers. This allows the model to learn appropriate spatio-temporal attention patterns during training. It also improves efficiency compared to uniform 3D partitioning. Innovation 2: Query-global block selection. For each attention head, the top-scoring key blocks are selected from global pool aggregated across all query-key block products. This prioritizes blocks corresponding to the most important query-key interactions. Innovation 3: Threshold-based block selection. We dynamically determine the number of selected blocks based on their cumulative similarity score, rather than fixed top-k. This allows for more adaptive approximation of full attention by catering to the diverse concentration levels across heads. Through these targeted innovations, VMoBA enables efficient training of VDMs on longer sequences, significantly outperforming vanilla MoBA and achieving comparable or superior quality to full attention, as shown in Fig. 1a and Fig. 1c. Furthermore, VMoBA demonstrates competitive performance in training-free settings, offering substantial speedups for long sequences, as shown in Fig. 1b. Our contributions are summarized as follows: 1) We conduct an in-depth analysis of attention mechanisms in video DiTs, revealing key observations, including spatio-temporal characteristics, varying query importance, and diverse similarity distribution patterns. 2) We propose VMoBA, the first mixture-of-block sparse attention specifically for training VDMs on long sequences. VMoBA incorporates novel layer-wise recurrent block partition, global block selection, and threshold-based block selection strategies. 3) Extensive experiments demonstrate that VMoBA significantly accelerates training and inference for long video sequences while maintaining or improving generation quality compared to full attention and baseline sparse attention methods."
        },
        {
            "title": "2 Related Work",
            "content": "Diffusion model inference acceleration. The technologies of inference acceleration broadly fall into two categories. First, methods that reduce the number of denoising steps. These approaches modify only the inference procedure. Many edit the samplers or noise schedules to cut down diffusion timesteps [29, 15, 18, 16, 30, 20]. For example, high-order ODE solvers and multi-step methods can generate videos in tens of steps instead of hundreds [15, 18]. Other works exploit the redundancy between successive diffusion steps: by caching and reusing features (e.g., attention maps), they avoid repeated computation [23, 5, 45, 21, 14]. However, these training-free methods are highly sensitive to hyperparameters and can produce degraded fidelity [45, 5, 14]. Second, model distillation techniques train student network to mimic the original model with fewer steps [28, 39]. However, Distillation needs extra training on large-scale datasets, and the student model often generates on the same or smaller resolutions, struggling to scale the data to larger resolutions [28]. Efficient attention mechanisms. Linear attention methods, such as state-space models [9, 7, 17, 33] and RNN-derived architectures [25] achieve linear complexity by reformulating the attention operation. However, these schemes typically cannot be seamlessly interchanged with full attention, often requiring architectural changes. Moreover, they are observed falling short on some tasks [40] compared with full attention. Alternatively, sparse attention mechanisms limit each tokens attended set. Approaches like MoBA [19], NSA [41], and Block-Attention [22] allow Transformers to transition between full and sparse attention modes seamlessly. For video diffusion models, specialized sparse attention techniques have been introduced to accelerate generation at inference time, including Sliding Tile Attention [44], DiTFastAttn [42], SparseVideoGen [37], and SpargeAttn [43]. However, these techniques often serve training freely, offering limited benefit for speeding up model training. On the contrary, our proposed VMoBA is sparse attention mechanism for video diffusion models. It accelerates inference with lossless quality and speeds up model training towards longer sequences."
        },
        {
            "title": "3.1 Overview",
            "content": "VMoBA operates in three main steps, as illustrated in Fig. 2. Step 1: Partition and Mean Key Blocks. The input key tensor is first partitioned into non-overlapping blocks. Crucially, the partitioning strategy (1D, 2D, or 3D) is determined layer-wise, as detailed in Section 3.2. After 3 Figure 2: The overall pipeline of VMoBA. We first partition key blocks with Layer-wise Recurrent BLock Partition, then select the blocks using Global Block Selection and Threshold-based Block Selection. Finally, the attention is computed only with the selected blocks. partitioning, the mean of each key block is computed, resulting in set of key blocks B. Step 2: Select Key Blocks. For each attention head, similarity map is computed between all query tokens and all the key blocks derived in Step 1. Instead of selecting fixed number of key blocks for each query independently, VMOBA employs two essential modifications: 1) Global Block Selection (Section 3.3): The most significant query-key block interactions are identified from global pool across all queries for the current head. 2) Threshold-based Selection (Section 3.4): The number of selected key blocks is dynamically determined based on their cumulative similarity score relative to the total similarity, controlled by threshold τ . Step 3: Calculate Sparse Attention on Selected Blocks. Each query token performs attention only with the key-value pairs belonging to its selected blocks. The outputs from all attention heads are concatenated to form the final attention layer output. Note that while this describes the conceptual process, our implementation leverages FlashAttention [6] for efficient training, ensuring the output is equivalent to this theoretical procedure. Computational complexity analysis (FLOPs). The computational complexity of VMOBA primarily consists of two parts: 1) Block Selection: Calculating the similarity matrix between query tokens (each of dimension d) and Nb key blocks. This is approximately O(sdNb) = O(s2d/sb), where sb is block size. 2) Sparse Attention: Performing attention between query tokens and their selected kavg key-value pairs on average. The complexity is roughly O(skavgsbd). Note that here we omit the attention head, assuming is the whole hidden dimension with all the heads. The total FLOPs can be approximated as O(sd(s/sb + kavgsb)). This indicates that larger block sizes and fewer selected blocks per query contribute to lower FLOPs."
        },
        {
            "title": "3.2 Layer-wise Recurrent Block Partition",
            "content": "Video data inherently possesses strong spatio-temporal locality. Our analysis of attention maps from full attention DiT, Wan 2.1 1.3B [32], is illustrated in Fig. 3. We find several representative attention patterns across layers. Specifically, different layers might focus on temporal, spatial, or combined spatio-temporal neighborhoods. For example, layer 27 prioritizes interactions along the temporal axis (1D neighbor), layer 3 focuses on spatial relationships within frame (2D neighbor), and layer 20 attends to local 3D spatio-temporal volumes (3D neighbor). Specifically, for the 3D neighbor pattern, the queries 4 Figure 3: The query-key block attention map shows 1-2-3D distinct patterns. belonging to the first block attend primarily to the 1st, 2nd, 3rd, and 5th key blocks, which are accurate themselves and their neighbors in width, height, and time. To accommodate these varying patterns without costly dynamic selection [37], VMoBA employs Layer-wise Recurrent Block Partition strategy. We define three types of key block partitioning: 1) Temporal (1D) Partition. Blocks are formed along the time axis, grouping tokens from the near frames. 2) Spatial (2D) Partition. Blocks are formed along the height and width dimensions, grouping tokens from the local spatial position across all frames. 3) Spatio-temporal (3D) Partition. Blocks are formed as local 3D patches in the spatio-temporal latent space. The three partition functions are performed recursively across layers in our model, which can be formulated as: = rearrange(K (T )) (N b1 (N b2 (N b3 ) (sT b1 b2 b3 ), ) (T sH b2 ) (sT b3 b3 sW ), b2 sH if mod 3 = 0 if mod 3 = 1 if mod 3 = 2 sW b3 ), = mean(K) (N b1 (N b2 (N b3 ), b2 b3 ), b3 ), if mod 3 = 0 if mod 3 = 1 if mod 3 = 2 (1) (2) where is the key after rearrange, and is the key block tensor. is the layer number. , H, and represent the frame, height, and width of the video latent. are the number of bx blocks in time, height, and width dimensions for each partition pattern, respectively. sT , and sW bx bx are the corresponding block sizes. The subscript {1, 2, 3} indicates the partition pattern (1-2-3D). We omit batch size and hidden size here for simplicity. Thanks to the training nature of VMoBA, this cyclical 1D-2D-3D scheme allows the model to learn spatio-temporal attention patterns automatically. This approach improves efficiency by reducing the total number of blocks compared to uniform 3D partition across all layers, as VMoBAs speed is sensitive to the total block number. Furthermore, this structured partitioning helps ensure that semantically related tokens are more likely to be grouped within the same block, leading to more representative mean key blocks. , and bx , bx , sH bx"
        },
        {
            "title": "3.3 Global Block Selection",
            "content": "Figure 4: Summation of top 25% query-key similarities. Different queries have varying importance. Full attention maps indicate that different query tokens receive varying degrees of attention, and their top similarity scores with key tokens can differ significantly. As shown in Fig. 4, the sum of top 25% query-key similarities varies greatly across queries. For example, regions in astronaut and ground have high top similarity scores while regions in ball and sky have lower scores. This indicates that some queries inherently have stronger affinities or importance regarding key interactions. MoBAs original strategy selects fixed number of key blocks for each query independently. This can underallocate resources to queries that inherently have stronger affinities with keys and could benefit from interacting with more blocks, or over-allocate to less important queries. To address this, the proposed VMoBA implements Global Block Selection algorithm. For each attention head, instead of selecting blocks on per-query basis, we first compute all query-key block similarities. Then, the top-scoring key blocks are selected from this global pool of similarities, aggregated across all query-key block products. This prioritizes blocks corresponding to the most important overall query-key interactions, allowing sparsity to be allocated more effectively based on the collective signal strength of interactions. This can be formulated as: Mi = TopkMask(qibT , k), {1, 2, ..., h} (3) where the TopkMask(a, b) function selects the top values in and returns mask with selected positions set to 1. is the attention head index, from 1 to h, the number of attention heads. qi in shape [s, d] contains all the query tokens for head i, and bi in shape [Nb, d] is all the key blocks for head i. 5 qibT is matrix production to get their similarity matrix. Mi (Nb, s) is the selected query-key block pair mask. From this formula, VMoBA selects blocks globally, considering the different importance of queries. Here is the number of selected blocks, further defined in Section 3.4."
        },
        {
            "title": "3.4 Threshold-based Block Selection",
            "content": "The concentration level of similarity scores varies not only across queries but also across different attention heads. Some heads exhibit highly concentrated similarities towards few query-key block pairs, while others show more diffuse or smoothed patterns. This is illustrated in Fig. 5, which shows the distribution of sorted query-key block similarities for different heads. The varying slopes indicate different concentration levels. Specifically, head 4 rapidly increases the similarity on the most right part (the largest similarities), while head 1 requires more blocks to capture significant portion of the total similarity. For better understanding of the concentration level, we draw the 30% and 50% cutoff lines for each head, observing significant differences across the heads. The number of query-block pairs required to reach 50% cumulative similarity for heads 1 and 4 differs by nearly 25,000, which is highly significant. Under such an observation, fixed top-k selection of blocks, as in the original MoBA, which ignores these head-specific distribution differences, may not be optimal. It might select too few blocks for diffuse heads, thus losing information, or too many low-similarity blocks for concentrated heads, thus wasting computation. Therefore, VMoBA introduces Threshold-based Block Selection to determine the number of selected blocks dynamically. After obtaining the global query-key block similarity scores, we sort these similarity scores in descending order. We then select blocks starting from the highest similarity, accumulating their normalized similarity scores until this cumulative sum exceeds predefined threshold. The process to dynamically define can be written as: Figure 5: Sorted query-key block similarity and top 30%/50% cutoff lines. The right parts of the cutoff lines contain corresponding cumulative summations of query-key block similarity. = min (cid:110) Sorted( ˆSj) τ (cid:111) , (cid:88) j=1 (4) where ˆS = qibT is the computed query-key block similarity matrix. τ is hyperparameter to define the cumulative similarity threshold. This adaptive approach allows VMoBA to approximate full attention more effectively by catering to the diverse concentration levels across heads. It ensures that high-similarity pairs are captured while efficiently pruning low-similarity ones, acting as an effective compression of sparsity based on informational content, rather than fixed k."
        },
        {
            "title": "4.1 Setup",
            "content": "Datasets. For training VMoBA and the baseline models, we use the Koala-36M dataset [35]. We train at different resolutions to assess VMoBAs generalization capability. Importantly, in each experimental setting, all models are trained on the identical data to eliminate any confounding effects due to data variation. To evaluate the video generation ability, we use the VBench prompt list [11]. For training-free inference experiments, we use the original prompt list. For the training-based results, we use the prompts after optimization [38] because training prompts from Koala-36M [35] are long. Metrics. We adopt five assessment aspects of VBench [11] to evaluate the video generation ability. Specifically, we choose Overall Consistency (TextConsis), Dynamic Degree (Dynamic), Background Consistency (BGConsis), Imaging Quality (ImageQual), and Subject Consistency (SubConsist) as our metrics. Furthermore, we use PSNR [8] to evaluate the similarity of sparse and full attention 6 Table 1: Training-free inference comparison of VMoBA and baseline methods."
        },
        {
            "title": "Quality",
            "content": "PSNR TextConsis BGConsis ImageQual SubConsist 81480832 33K Tokens"
        },
        {
            "title": "FullAttn",
            "content": "DiTFastAttn [42] SVG [37] MoBA [19] VMoBA"
        },
        {
            "title": "FullAttn",
            "content": "81 720 1280 DiTFastAttn [42] 76K Tokens SVG [37] MoBA [19] VMoBA - 0.50 0.50 0.25 0.31 - 0.50 0.50 0.25 0.31 - 22.67 12.64 15.54 16.00 - 24.50 25.85 20.46 18.80 27.85% 27.34% 27.30% 28.47% 27.62% 27.99% 26.09% 27.64% 28.47% 28.06% 94.47% 91.68% 93.67% 94.84% 93.74% 93.74% 92.03% 92.11% 91.46% 92.85% 62.06% 60.27% 60.98% 60.06% 60.05% 63.87% 65.59% 63.56% 64.16% 64.39% 90.70% 90.57% 89.84% 89.90% 89.65% 92.56% 92.25% 93.98% 91.42% 92.08%"
        },
        {
            "title": "Efficiency",
            "content": "FLOPs 282.64T Latency 103s 182.73T (1.54x) 182.92T (1.54x) 133.50T (2.12x) 149.00T (1.90x) 89s (1.18x) 90s (1.17x) 126s (0.83x) 104s (1.01x) 1246.78T 406s 720.06T (1.73x) 720.50T (1.73x) 457.20T (2.73x) 519.75T (2.40x) 310s (1.31x) 314s (1.29x) 360s (1.13x) 300s (1.35x) Table 2: Training-based results. Training-free methods apply to tuned Full Attention models. Video Size Method Sparsity Quality TextConsis Dynamic BGConsis ImageQual SubConsist 93 576 1024 Pretrained Wan 2.1 FullAttn + DiTFastAttn [42] + SVG [37] 55K Tokens MoBA [19] VMoBA 141480 Pretrained Wan 2.1 FullAttn + DiTFastAttn [42] + SVG [37] 56K Tokens MoBA [19] VMoBA - - 0.50 0. 0.25 0.19 - - 0.50 0.50 0.25 0.18 28.22% 24.61% 22.60% 23.37% 23.06% 25.88% 27.11% 23.92% 21.39% 21.37% 23.22% 23.71% 62.97% 61.58% 61.57% 62.43% 5.80% 56.91% 56.58% 43.01% 44.35% 43.67% 11.97% 31.36% 93.94% 94.69% 88.59% 93.84% 97.60% 96.76% 92.32% 92.30% 89.47% 93.72% 95.39% 93.06% 63.81% 69.49% 67.36% 68.89% 63.73% 67.45% 63.49% 64.36% 63.64% 63.72% 65.07% 67.66% 92.40% 90.86% 83.33% 90.38% 94.30% 94.72% 92.66% 92.58% 86.92% 92.38% 93.40% 93.78% Efficiency FLOPs 705.02T Training Time - 705.02T (1.00x) 423.22T (1.66x) 423.55T (1.66x) 276 GPU Hours (1.00x) - - 282.69T (2.49x) 248.68T (2.83x) 226 GPU Hours (1.22x) 187 GPU Hours (1.48x) 724.97T - 724.97T (1.00x) 434.31T (1.67x) 434.64T (1.67x) 262 GPU Hours (1.00x) - - 289.16T (2.51x) 248.39T (2.92x) 209 GPU Hours (1.25x) 182 GPU Hours (1.44x) generated videos in the training-free inference setting. We also count the generation latency, training time, and the corresponding speedup to compare the efficiency of methods. Baselines. We compare VMoBA with its most relevant native training-targeted sparse attention method, MoBA [19]. We also compare with two training-free sparse attention methods designed for DiTs, DiTFastAttn [42] and SVG [37]. Full Attention [32] is considered ground-truth in the training-free setting. In the training-based experiments, it becomes comparable baseline method. For thorough insight into the results, we only compare the Attention mechanism with baselines, eliminating other tricks like cache or quantization. Implementation details. We use Wan 2.1 1.3B [32] as the base model for all the experiments. For VMoBA, we set the threshold τ to 0.25, while its token sparsity may vary across different settings (always smaller than τ because of the selection algorithm). The block size and block number differ in various resolutions. We primarily set the number of spatial-temporal blocks to 72, which is sufficient for most cases. Following previous works [37, 45, 13, 21, 14], we remain the first 25% of denoising steps with full attention. Training experiments are finished with 2000 steps. All experiments are on NVIDIA H800 GPUs. Additional implementation details can be found in the Appendix."
        },
        {
            "title": "4.2 Quantitative Results",
            "content": "Training-free inference. We conduct training-free inference experiments on two resolutions. One is the original resolution of Wan [32] (81x480x832, 33K tokens), and the other features an extended token length (81x720x1280, 76K tokens). The results are presented in Table 1. At the original resolution, VMoBA achieves 1.01x speedup over Full Attention, surpassing MoBAs 0.83x. While its speedup is slightly less than dedicated training-free methods, VMoBA demonstrates strong generation quality. It ranks second in TextConsis and BGConsis, with other scores being comparable (within 1% of the top baseline). Regarding similarity to Full Attention, as measured by PSNR, VMoBA is second only to DiTFastAttn. When extending to longer sequences, VMoBAs advantages in speed and quality become more pronounced. Specifically, VMoBA achieves 1.35x speedup, which is 19% higher than vanilla MoBA. This improvement is primarily because MoBAs 1D partitioning leads to excessive number of blocks. In terms of quality, VMoBA leads in BGConsis, is second in TextConsis and ImageQual, and is comparable in SubConsist. Its PSNR is lower than some methods, indicating that VMoBAs generated videos have less similarity to Full Attention outputs in this longer token setting. This might reflect characteristic of VMoBA: for long tokens, it can generate videos 7 that are not identical to Full Attention but still maintain high quality. In summary, despite being designed for training, VMoBA demonstrates acceptable performance in training-free settings. Training on longer sequences. To evaluate VMoBAs ability to fine-tune pre-trained models on longer sequences, we train Wan [32] on two extended resolutions: 1) Spatially extended video: 93x576x1024 (55K tokens). 2) Temporally extended video: 141x480x832 (56K tokens). Results are shown in Table 2. For spatially extended videos, VMoBA, with the lowest token sparsity (0.19) and training time, achieves quality comparable to or even exceeding Full Attention. The VBench mean score is 68.34 for VMoBA and 68.25 for Full Attention. Full Attention required 276 GPU Hours for training, while VMoBA only needed 187 GPU Hours, resulting in 1.48x speedup. The vanilla MoBAs score is particularly low on Dynamic Degree (5.80%), indicating its outputs have minimal dynamics and are almost static. This is likely because MoBAs 1D block partitioning struggles to capture spatio-temporal motion information effectively. DiTFastAttn and SVG, applied to the tuned Full Attention model, yield lower scores than Full Attention and do not offer training acceleration. For temporally extended videos, VMoBA achieves 1.44x speedup over Full Attention, again delivering comparable or superior performance. Notably, VMoBA outperforms Full Attention by 3.3% in Image Quality. In summary, the proposed VMoBA accelerates DiT training significantly while ensuring comparable or improved generation quality on longer sequences. Figure 6: Qualitative comparison of VMoBA and baseline methods."
        },
        {
            "title": "4.3 Qualitative Results",
            "content": "Fig. 6 showcases four examples, illustrating qualitative results under different settings. (a) Displays training-free inference results at the original resolution. Even with sparsity of 0.5, SVG [37] exhibits clear blurry strips at the top and bottom of the video, contributing to its very low PSNR (12.64). In contrast, VMoBA closely reproduces the video content. Interestingly, in the ground truth video generated by Full Attention, the seagulls head appears in an incorrect orientation, an artifact not present in VMoBAs generation. (b) Presents results for high-definition video (81x720x1280) with the prompt panda cooking in the kitchen. Full Attention generates panda and chopping board but misses the cooking aspect. VMoBA, however, generates dishes and condiments, aligning more closely with the prompt. This observation helps explain VMoBAs higher TextConsis score 28.06% vs. 27.99% for Full Attention. SVGs output is more similar to Full Attentions, resulting in higher PSNR but TextConsis score 0.42% lower than VMoBAs. (c) Illustrates results after training models on the spatially extended resolution. VMoBA generates an aesthetically pleasing scene of coastal waves in spring, consistent with the prompt. Full Attention, on the other hand, lacks clear depiction 8 of the coast. MoBAs output is notably static and less aesthetically appealing. This demonstrates that VMoBA, after training, can generate videos comparable to or better than Full Attention while achieving significant training speedup (1.48x). (d) shows results after training on temporally extended resolution. VMoBAs output exhibits better image quality and more realistic scene depiction than other methods. In summary, VMoBA offers significant speed improvements for long sequences. After training, it can achieve visual quality consistent with, and in some aspects, even better than, full attention mechanisms, particularly in terms of prompt alignment and imaging quality. Table 3: Ablation studies. DD, IQ, and SC represent Dynamic, ImageQual, and SubConsis. (a) Key block partition method ablation. (b) Block choice design ablation."
        },
        {
            "title": "Method",
            "content": "DD IQ SC Time"
        },
        {
            "title": "Method",
            "content": "DD IQ SC 1-2D 1-3D 2-3D 1-2-3D 176 55.49% 58.51% 86.12% 28.57% 66.71% 91.34% 187 57.01% 66.02% 94.75% 202 56.91% 67.45% 94.72% 187 (c) Ablation on threshold τ . τ 0.15 0.25 0.35 0."
        },
        {
            "title": "Sparsity",
            "content": "DD IQ SC Time 0.13 0.19 0.27 0.39 55.46% 66.82% 91.12% 162 56.91% 67.45% 94.72% 187 240 55.00% 67.63% 94.35% 56.74% 68.17% 94.42% topk + local threshold + local topk + global threshold + global 54.87% 65.59% 91.64% 55.19% 65.31% 92.43% 55.29% 64.58% 92.86% 56.91% 67.45% 94.72% (d) Ablation on the number of blocks. a-b-c is the number of 1-2-3D blocks, correspondingly. #Blocks DD IQ SC Time 8-24-36 8-48-72 24-48-144 57.25% 66.57% 93.59% 172 56.91% 67.45% 94.72% 187 57.70% 68.60% 94.42%"
        },
        {
            "title": "4.4 Ablation Study",
            "content": "Block partitioning strategy. We ablate the layer-wise recurrent block partitioning strategy by removing one of the 1D, 2D, or 3D partitioning patterns and show the results in Table 3a. Removing any single partitioning patter generally leads to drop in performance. For instance, The 2-3D strategy achieves slightly higher score but at the cost of increased training time. This suggests that the 1D partition contributes to efficiency. This highlights the benefit of incorporating diverse partitioning schemes to capture different aspects of spatio-temporal locality. Block choice design. We ablate the key components of our block selection mechanism: global block selection and threshold-based selection, detailed in Table 3b. The results show that our proposed \"threshold + global\" strategy achieves the best performance across all metrics. Removing either leads to performance drop. This indicates that both adaptively determining the number of blocks via threshold and selecting blocks from global pool are crucial for VMoBAs effectiveness. The influence of threshold τ . We investigate the impact of the threshold τ . Results are presented in Table 3c. We observe that larger threshold generally leads to better performance. However, this comes at the cost of increased training time. This is intuitive, as larger τ means selecting more blocks, thereby increasing computational load but also enhancing the models capacity. threshold of 0.25 offers good balance. The influence of block number. We evaluate the influence of the number of blocks, as shown in Table 3d. Generally, using more blocks tends to bring better performance. However, this also increases the training time. Our default setting of 8-48-72 achieves comparable scores with lower training cost, which provides good trade-off."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper introduces VMoBA, mixture-of-block sparse attention mechanism designed to address the computational challenges of training long-sequence data in VDMs. Our approach was motivated by task-specific analysis of attention mechanisms in pre-trained video DiTs. VMoBA incorporates three key innovations: layer-wise recurrent block partitioning, global block selection, and thresholdIt based block selection. Extensive experimental results demonstrate VMoBAs effectiveness. achieves speedups of up to 1.48x while maintaining or even improving generation quality compared to full attention. Moreover, VMoBA provides competitive speedups in training-free inference settings, showcasing its versatility. 9 Acknowledgement. This work is supported by the National Key Research and Development Program of China (No. 2023YFC3807603)."
        },
        {
            "title": "References",
            "content": "[1] Tim Brooks, Bill Peebles, Connor Holmes, Yufei Guo Will DePue, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Zeroscope, 2023. 2 [2] Tim Brooks, Bill Peebles, Connor Holmes, Yufei Guo Will DePue, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Sora: Creating video from text, 2024. 2 [3] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 2 [4] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. arXiv preprint arXiv:2401.09047, 2024. 2 [5] Pengtao Chen, Mingzhu Shen, Peng Ye, Jianjian Cao, Chongjun Tu, Christos-Savvas Bouganis, Yiren Zhao, and Tao Chen. δ-dit: training-free acceleration method tailored for diffusion transformers. CoRR, 2024. [6] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memoryefficient exact attention with io-awareness. NeurIPS, 2022. 4, 13 [7] Tri Dao and Albert Gu. Transformers are ssms: generalized models and efficient algorithms through structured state space duality. In ICML, 2024. 3 [8] Fernando Fardo, Victor Conforto, Francisco de Oliveira, and Paulo Rodrigues. formal evaluation of psnr as quality measurement parameter for image segmentation algorithms. arXiv preprint arXiv:1605.07116, 2016. 6 [9] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. 3 [10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In ICLR, 2024. 2 [11] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. 6 [12] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2 [13] Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Kai Li, and Song Han. Distrifusion: Distributed parallel inference for high-resolution diffusion models. In CVPR, 2024. 7 [14] Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. Timestep embedding tells: Its time to cache for video diffusion model. In CVPR, 2025. 3, 7 [15] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. In ICLR, 2022. 3 [16] Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. 3 [17] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, Jianbin Jiao, and Yunfan Liu. Vmamba: Visual state space model. NeurIPS, 2024. 3 [18] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-Solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. In NeurIPS, 2022. 3 [19] Enzhe Lu, Zhejun Jiang, Jingyuan Liu, et al. Moba: Mixture of block attention for long-context llms. arXiv preprint arXiv:2502.13189, 2025. 2, 3, 7, 12, 13 [20] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 3 [21] Zhengyao Lv, Chenyang Si, Junhao Song, Zhenyu Yang, Yu Qiao, Ziwei Liu, and Kwan-Yee Wong. Fastercache: Training-free video diffusion model acceleration with high quality. arXiv preprint arXiv:2410.19355, 2024. 3, [22] Dongyang Ma, Yan Wang, and Tian Lan. Block-attention for efficient prefilling. In ICLR, 2024. 2, 3 [23] Xinyin Ma, Gongfan Fang, and Xinchao Wang. DeepCache: Accelerating diffusion models for free. arXiv preprint arXiv:2312.00858, 2023. 3 [24] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 2 10 [25] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, et al. Rwkv: Reinventing rnns for the transformer era. In EMNLP, 2023. 3 [26] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2 [27] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. NeurIPS, 2022. 2 [28] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In ICLR, 2022. 3 [29] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2020. 3 [30] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In ICML, 2023. 3 [31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017. [32] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 4, 7, 8, 13 [33] Hongjie Wang, Chih-Yao Ma, Yen-Cheng Liu, Ji Hou, Tao Xu, Jialiang Wang, Felix Juefei-Xu, Yaqiao Luo, Peizhao Zhang, Tingbo Hou, et al. Lingen: Towards high-resolution minute-length text-to-video generation with linear computational complexity. arXiv preprint arXiv:2412.09856, 2024. 3 [34] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 2 [35] Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, et al. Koala-36m: large-scale video dataset improving consistency between fine-grained conditions and video content. arXiv preprint arXiv:2410.08260, 2024. 6 [36] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. [37] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. 2, 3, 5, 7, 8, 12 [38] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. CoRR, 2024. 6 [39] Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. In NeurIPS, 2024. 3 [40] Weihao Yu and Xinchao Wang. Mambaout: Do we really need mamba for vision? CoRR, 2024. 3 [41] Jingyang Yuan, Huazuo Gao, Damai Dai, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. arXiv preprint arXiv:2502.11089, 2025. 2, 3 [42] Zhihang Yuan, Hanling Zhang, Lu Pu, Xuefei Ning, Linfeng Zhang, Tianchen Zhao, Shengen Yan, Guohao Dai, and Yu Wang. Ditfastattn: Attention compression for diffusion transformer models. NeurIPS, 2024. 2, 3, 7, 12 [43] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference. arXiv preprint arXiv:2502.18137, 2025. 2, [44] Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhenghong Liu, and Hao Zhang. Fast video generation with sliding tile attention. arXiv preprint arXiv:2502.04507, 2025. 2, 3 [45] Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You. Real-time video generation with pyramid attention broadcast. CoRR, 2024. 3,"
        },
        {
            "title": "Appendix",
            "content": "Table of contents. The supplementary includes the following sections: Appendix A. Implementation details of the experiments. Appendix B. Additional experiments. Appendix C. Pre-training loss comparison between VMoBA and Full Attention. Appendix D. Limitations and future work."
        },
        {
            "title": "A Implementation Details",
            "content": "Table 4: Implementation detials of experiments in the main paper."
        },
        {
            "title": "Video Resolution",
            "content": "Training-free"
        },
        {
            "title": "Training",
            "content": "81 480 832 81 720 1280 93 576 1024 141 480 832 Latent Resolution Temporal Block Size Spatial Block Size Spatial-Temporal Block Size Temporal Blocks Spatial Blocks Spatial-Temporal Blocks Block Selection Type τ 21 30 52 3 5 13 7 5 13 7 24 72 Local + TopK 2 6 18 - 21 45 80 3 9 10 7 15 20 7 40 36 Local + TopK 2 10 9 - 24 36 64 3 6 8 8 12 8 8 48 72 36 30 52 3 5 13 12 10 13 7 24 36 Global + Threshold Global + Threshold - 0.25 - 0. The detailed hyperparameter settings of the main experiments are shown in Table 4. Note that apart from the 3D block partition algorithm, we use local and topk query-key block selection in the training-free experiments, which is the same option as the original MoBA [19]. We empirically find that adopting VMoBAs block selection strategy will cause vibration effects on the generated videos. On the contrary, the original local and topk options can better preserve the generation stability, probably due to their closer modeling nature to full attention. However, we use the full VMoBA setting in all training experiments, observing stronger performance and efficiency gain compared to MoBA, which demonstrates the effectiveness of VMoBA as sparse attention algorithm designed for training video diffusion models."
        },
        {
            "title": "B Additional Experiments",
            "content": "Table 5: Training results on original and shorter sequence lengths."
        },
        {
            "title": "Efficiency",
            "content": "TextConsis Dynamic BGConsis ImageQual SubConsist Origin 81 480 832 Pretrained Wan 2.1 FullAttn + DiTFastAttn [42] + SVG [37] 33K Tokens MoBA [19] VMoBA Shorter 81 320 512 Pretrained Wan 2.1 FullAttn + DiTFastAttn [42] + SVG [37] 13K Tokens MoBA [19] VMoBA - - 0.50 0.50 0.25 0.19 - - 0.50 0. 0.25 0.18 27.85% 20.32% 20.95% 19.11% 22.15% 21.72% 27.13% 22.13% 22.16% 20.99% 24.05% 24.29% 73.45% 98.43% 95.87% 97.96% 5.40% 69.35% 86.31% 49.10% 48.37% 48.70% 50.96% 38.40% 94.47% 92.39% 85.08% 93.06% 98.27% 97.16% 94.17% 98.15% 94.46% 97.84% 98.08% 98.09% 62.06% 61.18% 58.86% 60.13% 64.78% 65.44% 63.60% 62.84% 62.23% 62.03% 62.86% 62.92% 90.70% 85.09% 81.22% 84.12% 97.75% 97.92% 91.49% 96.42% 91.00% 95.70% 97.70% 98.78% FLOPs 282.64T Training Time - 282.64T (1.00x) 182.73T (1.55x) 182.92T (1.54x)"
        },
        {
            "title": "104 GPU Hours (1.00x)\n-\n-",
            "content": "133.50T (2.12x) 149.00T (1.90x)"
        },
        {
            "title": "126 GPU Hours (0.82x)\n104 GPU Hours (1.00x)",
            "content": "67.73T - 67.73T (1.00x) 51.09T (1.32x) 51.17T (1.32x)"
        },
        {
            "title": "88 GPU Hours (1.00x)\n-\n-",
            "content": "42.84T (1.58x) 40.48T (1.67x)"
        },
        {
            "title": "118 GPU Hours (0.75x)\n103 GPU Hours (0.86x)",
            "content": "We conduct additional training experiments on the original and shorter sequence lengths and show the results in Table 5. For the original sequence length (33K), VMoBAs speed is very close to the pre-trained full attention model, while for shorter sequence length (13K), VMoBA becomes even slower. This is reasonable because the acceleration of VMoBA is more significant when the sequence length goes longer, as shown in Fig. 1. Moreover, in these two experiments, VMoBA consistently outperforms MoBA in both performance and efficiency, further demonstrating its effectiveness. 12 Pre-training Loss Comparison Table 6: Model configuration of the pre-training experiments."
        },
        {
            "title": "FFN Dim",
            "content": "5.6M 60.4M 217M 526M 3 9 15 21 3 5 7 9 384 640 896 1152 896 2688 4480 6272 (a) 11k sequence length. (b) 46k sequence length. Figure 7: Validation loss comparison for pre-training between VMoBA and full attention. Blue and red lines are full attention and VMoBA, respectively. Deeper colors represent bigger model sizes. We conduct training-from-scratch experiments to evaluate the pre-training ability of VMoBA, compared with full attention. Following MoBA [19], we train on two sequence lengths, 11k (77 288 512) for shorter length, and 46k (77 576 1024) for longer length. The 1-2-3D block numbers of VMoBA are 10, 48, and 60. The threshold τ is set to 0.25. The model configuration are shown in Table 6. We use the same architecture as Wan 2.1 [32]. For VMoBA models, we replace all the self-attention layers with VMoBA, which is the only change. The results for 11K length are shown in Fig. 7a. Despite VMoBAs validation loss curves being consistently higher than those of full attention, the difference between them is shrinking as the model size increases. This indicates that VMoBA performs closely with full attention for larger model scales. Moreover, the results for 46K length are shown in Fig. 7b. We observe close overlapped validation loss curves between VMoBA and full attention pre-training, further demonstrating that VMoBA potentially performs better at longer sequences, which is also revealed by MoBA [19]. In conclusion, the pre-training experiments demonstrate that VMoBA is reliable alternative to full attention for the pre-training of video generation models."
        },
        {
            "title": "D Limitations and Future Work",
            "content": "As shown in Table 5, the speed up of VMoBA is sub-optimal when the sequence length is short, even though its FLOPs are lower. This may be due to the inconsistent memory distribution of selected blocks. The current FlashAttention [6]-based implementation may not be able to optimize the speed of the sparse attention mechanism. Future work may focus on more efficient and hardware-friendly implementations to make the practical speed of VMoBA closer to the theoretical FLOPs."
        }
    ],
    "affiliations": [
        "Kling Team, Kuaishou Technology",
        "Peking University"
    ]
}