{
    "paper_title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation",
    "authors": [
        "SII-OpenMOSS Team",
        ":",
        "Donghua Yu",
        "Mingshu Chen",
        "Qi Chen",
        "Qi Luo",
        "Qianyi Wu",
        "Qinyuan Cheng",
        "Ruixiao Li",
        "Tianyi Liang",
        "Wenbo Zhang",
        "Wenming Tu",
        "Xiangyu Peng",
        "Yang Gao",
        "Yanru Huo",
        "Ying Zhu",
        "Yinze Luo",
        "Yiyang Zhang",
        "Yuerong Song",
        "Zhe Xu",
        "Zhiyu Zhang",
        "Chenchen Yang",
        "Cheng Chang",
        "Chushu Zhou",
        "Hanfu Chen",
        "Hongnan Ma",
        "Jiaxi Li",
        "Jingqi Tong",
        "Junxi Liu",
        "Ke Chen",
        "Shimin Li",
        "Songlin Wang",
        "Wei Jiang",
        "Zhaoye Fei",
        "Zhiyuan Ning",
        "Chunguo Li",
        "Chenhui Li",
        "Ziwei He",
        "Zengfeng Huang",
        "Xie Chen",
        "Xipeng Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 ] . [ 1 4 9 7 8 0 . 2 0 6 2 : r MOVA: Towards Scalable and Synchronized VideoAudio Generation SII-OpenMOSS Team*"
        },
        {
            "title": "Abstract",
            "content": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs Mixture-of-Experts (MoE) architecture, with total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement. Blog: https://mosi.cn/models/mova Model: https://huggingface.co/collections/OpenMOSS-Team/mova Code: https://github.com/OpenMOSS/MOVA"
        },
        {
            "title": "Introduction",
            "content": "Video generation has long been pivotal domain in multimodal generative modeling. Historically, limitations in model capacity, data volume, and scalability hindered these models from achieving practically viable performance. The emergence of the scalable Diffusion Transformer architecture [1] has changed this landscape, giving rise to series of models-including Sora [2], OpenSora [3, 4], Wan [5], and LTX [6]that can generate high-fidelity, realistic videos. Beyond basic video synthesis, these models also demonstrate advanced capabilities such as long video generation [7], controllable synthesis [8], and even few-shot learning and visual reasoning [9, 10]. However, traditional video generation models often neglect the audio component, despite its critical role in multimedia content. Producing videos with synchronized sound typically requires cascaded pipeline[11], e.g., generating video first and then synthesizing audio based on the visuals, or vice versa. Such pipelines inherently limit generation quality, as the audio and video modalities do not interact during synthesis. Endto-end models like Veo3 [12] have demonstrated that high-fidelity audio-visual generation with precise synFull contributors can be found in the Contributors section. 1 Figure 1 Overview of MOVA capabilities. MOVA generates synchronized video and audio across diverse scenarios: multi-speaker speech with precise lip synchronization in both English and Chinese, physical sound effects aligned with visual events, and scene text generation. The model supports both 16:9 and 9:16 aspect ratios. chronization is possible, highlighting the importance of joint audio-video modeling. Similarly, Sora2 [13] exhibits impressive capabilities in generating synchronized audio and video. Yet, all these state-of-the-art systems are closed-source, and subsequent releases such as Wan2.6 [14], Kling Video 3,0 [15], and Seedance 2.0 [16] remain proprietary. As result, the development of high-quality video-audio generation models remains underexplored in the research community. Compared to video-only generation models, video-audio generation models introduce three major challenges: 1. Data Pipeline: Incorporating the audio modality requires fine-grained audio-video captioning pipeline. 2. Modality Fusion: To achieve harmonious and synchronized video-audio generation, these two modalities must mutually integrate information during the generation process. However, simultaneous bimodal generation faces challenges due to disparity in native information density between the two modalities, as well as challenges regarding fusion mechanisms and efficiency. 3. Scalability Verification: Most existing open-source models are evaluated on small-scale architectures and limited datasets [1720], often falling short in achieving high-quality results across both modalities. It remains an open question whether videoaudio models can sustain continuous performance improvements with larger datasets and model scales. In this work, we present MOVA (MOSS Video and Audio), video-audio generation foundation model capable of synthesizing multilingual speech with high-quality lip synchronization, as well as environmental sounds with precise audio-visual alignment. To achieve high-quality video-audio generation, we propose an asymmetric dual-tower architecture, combining pre-trained video tower with pre-trained audio tower. For modality fusion, we employ bidirectional cross-attention mechanism that enables rich interaction between the two modalities. To unleash the potential of this architecture, we construct fine-grained videoaudio annotation pipeline and scale up the training data, resulting in strong performance in synchronized video-audio generation. We observe consistent improvements in both lip synchronization and video-audio alignment metrics as training progresses. As illustrated in Figure 1, MOVA realizes scalable and synchronized video-audio generation. To support research and foster community creation, we release all model weights along with training, inference, and fine-tuning code. Our contributions can be summarized as follows: We develop MOVA, synchronized video-audio generation model with an asymmetric dual-tower 2 Figure 2 Model Structure Overview. MOVA couples an A14B video DiT backbone and 1.3B audio DiT backbone via 2.6B bidirectional Bridge module. architecture that leverages pre-trained video and audio generation models. We design fine-grained audio-video captioning pipeline to produce high-quality bimodal training data at scale. By scaling video-audio training, we achieve continuous improvements in synchronization performance across both modalities."
        },
        {
            "title": "2.1 Preliminaries",
            "content": "We consider joint generation of temporally aligned video-audio pairs from text prompt (optionally with first-frame image for I2VA). Let ğ‘‰ â„(1+ğ‘‡ğ‘£)ğ»ğ‘Š3 be video of ğ‘‡ğ‘£ + 1 frames and ğ´ â„ğ‘‡ğ‘1 be the corresponding 48 kHz mono audio waveform. We adopt pretrained VAEs to define compact latent spaces: the Wan2.1 video VAE [5] compresses ğ‘‰ into spatiotemporal latent ğ‘¥ğ‘£, and DAC-style audio VAE from HunyuanVideo-Foley [21] encodes ğ´ into an audio latent ğ‘¥ğ‘. All subsequent modules operate in these latent spaces. Given condition ğ‘, our goal is to generate synchronized (ğ‘¥ğ‘£, ğ‘¥ğ‘). Our training objective follows the flow matching method [22, 23]. At each timestep ğ‘¡, ğ‘¥ğ‘£ ğ‘¡ = (1 ğ‘¡)ğ‘¥ğ‘£ + ğ‘¡ğœ€ğ‘£, ğ‘¡ = (1 ğ‘¡)ğ‘¥ğ‘ + ğ‘¡ğœ€ğ‘, ğ‘¥ğ‘ (1) where ğœ€ğ‘£, ğœ€ğ‘ ğ’©(0, ğ¼) and ğ‘¡ ğ’°(0, 1). The target velocity is given by the derivative of the interpolation path: ğ‘£ğ‘£ ğ‘¡ = ğ‘£ğ‘ ğ‘¡ = dğ‘¥ğ‘£ ğ‘¡ dğ‘¡ dğ‘¥ğ‘ ğ‘¡ dğ‘¡ = ğœ€ğ‘£ ğ‘¥ğ‘£, = ğœ€ğ‘ ğ‘¥ğ‘. The training loss is defined as standard flow matching objective: â„’FM = ğ”¼ğ‘¡,ğ‘,ğ‘¥ğ‘£,ğ‘¥ğ‘,ğœ€[ğœ†ğ‘£ ğ‘£ğ‘£ ğœƒ(ğ‘¥ğ‘£ ğ‘¡ , ğ‘¥ğ‘ ğ‘¡ , ğ‘¡, ğ‘) ğ‘£ğ‘£ ğ‘¡ 2 + ğœ†ğ‘ ğ‘£ğ‘ ğœƒ(ğ‘¥ğ‘£ ğ‘¡ , ğ‘¥ğ‘ 2 ğ‘¡ , ğ‘¡, ğ‘) ğ‘£ğ‘ 2 ], ğ‘¡ (2) (3) where ğœƒ denotes the model parameters, ğ‘£ğœƒ denotes the predicted cross-modal velocity field, ğ‘ denotes the conditioning signal (text, optionally augmented with an input image), and ğœ†ğ‘£, ğœ†ğ‘ are loss weights that balance the videoand audio-velocity regression terms, respectively."
        },
        {
            "title": "2.2 Dual-Tower Diffusion Transformer",
            "content": "Our goal is to leverage powerful pretrained single-modality diffusion models and achieve synchronized videoaudio generation with minimal additional cost. Specifically, we adopt Wan2.2 I2V A14B [5] as the video backbone (for image+text conditioned I2VA) and 1.3B text-to-audio diffusion model with Wan2.1style architecture as the audio backbone. We couple these two backbones through lightweight dual-tower conditional Bridge, enabling bidirectional information exchange while keeping the core towers intact. Bridge Module. The bridge operates at the hidden-state level of the two DiT backbones. At each interaction layer, two cross-attention blocks are added: one injects video hidden states into the audio DiT. and one injects audio hidden states into the video DiT. Aligned RoPE. Video and audio latents live on different temporal grids: video latents are relatively coarse (due to temporal downsampling in the video VAE), while audio latents are much denser. If we apply crossattention with naive positional indices, tokens that represent the same physical time can be assigned to different positional slots (and conversely different physical times can share nearby indices), so queries and keys may correspond to mismatched physical times, causing audiovisual drift; this positional mismatch also breaks the translation invariance of the interaction process. Similarly to MMAudio [24] and OVI [18], we modify standard RoPE [25] to align the two modalities on the same time grid.: q(ğ‘š) = ğ‘…( ğ‘ ğœƒğ‘š ) q(ğ‘š), k(ğ‘š) = ğ‘…( ğ‘ ğœƒğ‘š ) k(ğ‘š), ğ‘…(ğœ™) = [ cos ğœ™ sin ğœ™ cos ğœ™ sin ğœ™ ] . Let ğ‘“ğ‘£ and ğ‘“ğ‘ denote the latent frame rates of video and audio after their respective VAEs. We map video indices into the audio time units by scaling the temporal position with the ratio ğ‘  = ğ‘“ğ‘/ğ‘“ğ‘£: This puts video and audio tokens on the same temporal scale. ğ‘ğ‘£(ğ‘–) = ğ‘  ğ‘–, ğ‘ğ‘(ğ‘—) = ğ‘—."
        },
        {
            "title": "3.1 Overview",
            "content": "To transform heterogeneous and noisy raw videos into reliable resources for video-audio generation, we develop systematic data curation pipeline. The pipeline first cleans and standardizes the inputs, followed by 4 Figure 3 Data curation overview. Our data pipeline consists of three stages. In the first stage, we preprocess the raw data into fixed-length clips with resolution of 720p, frame rate of 24fps, and duration of 8.05s. In the second stage, we filter these clips based on audio quality, video quality, and audio-visual alignment to obtain high-quality, synchronized clips. In the third stage, we utilize Qwen3-Omni and MiMo-VL to label the audio and visual information within the videos, respectively, and finally use GPT-OSS to merge these single-modality captions. Through our data pipeline, we have successfully curated high-quality audio-visual content along with corresponding, semantically rich captions. the annotation of high-quality, richly-detailed captions across multiple modalities. By carefully structuring the data pipeline into three stages, we progressively filter out low-quality samples to retain high-fidelity data characterized by strong audio-visual consistency and coherent semantic labels. We curate high-quality subset of audio-visual data from various public datasets (data collection, Section 3.2). Our collection spans multiple video formats (e.g., movies, vlogs, and animations) and diverse themes ranging from education and sports to animation and interviews. The three-stage pipeline then processes the raw video data. In the first stage (video preprocessing, Section 3.3), we normalize aspect ratios, resize videos, resample streams, and segment videos into fixed-length clips based on VAD and scene transition annotations. In the second stage (audio-visual quality assessment, Section 3.4), we evaluate audio quality, video quality, and audio-visual alignment, retaining only clips that are both high-fidelity and well-aligned. In the third stage (audio-visual captioning, Section 3.5), we design structured and instructive prompts, generate modality-specific captions with MLLMs, and integrate them into unified annotations using LLM. Our data pipeline is presented in Figure 3 and retention ratio is shown in Table 1. Table 1 Retention Ratio of Total Dataset Duration"
        },
        {
            "title": "Raw",
            "content": "Stage 1 (speech+nonspeech) Stage 1 (speech only) Stage 2 Retention Ratio (Relative to Raw Video) 100% 84.57% 58.75% 26.39%"
        },
        {
            "title": "3.2 Data Collection",
            "content": "We use filtered HQ subsets of the following public video datasets in our work: VGGSound [26], AutoReCap [27], ChronoMagic-Pro [28], ACAV-100M [29], OpenHumanVid [30], SpeakerVid-5M [31], and OpenVid1M [32]. In addition, we utilize large amount of in-house data. Our dataset encompasses broad spectrum of domains (education, sports, and beauty, news, etc.), providing the distributional diversity necessary to enhance the models generalization across complex real-world scenarios."
        },
        {
            "title": "3.3 Video Preprocessing",
            "content": "We design and implement scalable video preprocessing pipeline based on the Ray distributed framework, balancing data quality and processing efficiency. Initially, raw video data is filtered to remove samples with decoding failures or missing valid audio channels. Videos with unconventional container or encoding formats are remuxed or transcoded, respectively. Then, the pipeline generates segmentation metadata through three sequential steps: Core Content Normalization: The FFmpeg cropdetect filter is applied to detect blank areas and retain the core visual content. The main content is then centered, resized to 720p resolution, and symmetrically padded with black borders (pillarboxing or letterboxing) as necessary to conform to 9:16 or 16:9 aspect ratio. Voice Activity Detection: The audio track is extracted from each video and analyzed by the Silero Voice Activity Detection (VAD) [33] model to identify speech and non-speech intervals. Scene Transition Analysis: PySceneDetect is employed to detect and record the timestamps of scene change points throughout the video. By integrating the temporal information from VAD and scene transition analysis, we generate four types of fixed-duration (8.05-second) video segments: single-scene speech, single-scene non-speech, multi-scene speech, and multi-scene non-speech. This duration corresponds precisely to 193 video frames at 24 fps, calculated as the initial frame plus 8 seconds of video (1 + 8 24). For speech segments, the start time is adaptively adjusted to avoid truncating ongoing speech and ensure spoken-content continuity; the detailed algorithm is provided in Section A.3. Ultimately, only speech segments are selected for training, accounting for 69.47% of all preprocessed segments."
        },
        {
            "title": "3.4 Audio-Visual Quality Assessment",
            "content": "We conduct data quality assessment along three main dimensions: audio quality, video quality, and audiovisual alignment. Audio quality: We compute signal-level metrics such as silence ratio and bandwidth, and further evaluate both signal and aesthetic aspects using the Audiobox-aesthetics audio quality assessment tool [34]. Video quality: We apply the DOVER video quality assessment tool [35] to assess the video from both technical and aesthetic perspectives. Audio-visual alignment: We employ SynchFormer [36] to compute the temporal audio-visual synchronization of each video, and use ImageBind [37] to evaluate semantic audio-visual alignment. To determine the filtering thresholds, we manually inspect the videos retained under different metric cutoffs and set reasonable thresholds for each dimension accordingly. In addition, we apply an audio classification model [38] to categorize audio and construct speech/non-speech subsets depending on the target capability (e.g., lip synchronization vs. general foley/ambience modeling). We provide the detailed filtering thresholds in Section A.4."
        },
        {
            "title": "3.5 Audio-Visual Captioning",
            "content": "We employ open-source models for audio-visual captioning and subsequently use large language model (LLM) to merge the generated captions into coherent natural language descriptions, utilizing both NVIDIA GPUs and Ascend NPUs. Bimodality Model. To annotate the filtered audio-visual contents, we employ distinct pipelines for video and audio modalities. For video annotation, we utilize the MiMo-VL-7B-RL model [39] to generate video descriptions, with explicit instructions focusing on video scene transitions. For audio annotation, we implement dual-model strategy to separately handle speech and non-speech components. Specifically, Qwen3Omni-Instruct [40] was used for speech transcription, while Qwen3-Omni-Captioner [40] was applied to generate captions for non-speech sound and music. We then integrate these two subsets of annotations. This joint annotation strategy enables comprehensive coverage of both linguistic content and acoustic characteristics, reducing information loss and capturing multi-aspect audio semantics. Caption Merging. For annotations generated by the separate modality pipelines, our primary goal is to integrate the content from the visual and audio dimensions. We employ the GPT-OSS-120B model [41] to merge the video captions with the aggregated audio annotations (comprising both speech and non-speech elements) while performing consistency check to resolve potential cross-modal conflicts. Specifically, the model verifies the alignment between visual scenes and audio events to resolve potential conflicts. It then synthesizes these inputs into cohesive, natural language description that seamlessly blends visual information with audio details, ensuring the final output is contextually unified and suitable for the target application. All prompts of our caption process and detailed caption example are provided in Section A.5."
        },
        {
            "title": "4.1 Overview",
            "content": "Our training pipeline consists of two stages, as illustrated in Figure 4. We initialize the video tower and video VAE from Wan2.2 weights [5], and adopt the audio VAE from HunyuanVideo-Foley [21]; both VAEs remain frozen throughout training. In the first stage (audio tower pretraining, Section 4.2), we train standalone 1.3B text-to-audio DiT on diverse audio data covering music, general sounds, and speech. In the second stage (joint training, Section 4.4), the pretrained video tower (A14B), audio tower, and randomly initialized Bridge module are optimized together with heterogeneous learning rates. This joint training further proceeds through three phases with progressively refined data and resolution (Section 4.3): Phase 1 (360p, diverse data), Phase 2 (360p, quality-filtered data), and Phase 3 (720p, highest-quality data)."
        },
        {
            "title": "4.2 Audio Tower Training",
            "content": "Overview. To aligned with the video tower, we train an audio tower with the same architecture of Wan2.11.3B backbone. Relative to the original Wan2.1 setup, we replace Wan2.1s 3D positional encoding over (ğ‘“ , â„, ğ‘¤) with 1D positional encoding along the temporal axis. All other components and depth remain unchanged to maximally reuse engineering and training practices. Training Data. The text-to-audio model is trained on three domains: (i) general sounds drawn from WavCaps and VGGSound [26, 42]; (ii) music from JamendoMaxCaps [43]; and (iii) our in-house TTS data. We train on fixed-length clips. Each clip is paired with text prompt, which also carries an explicit duration token to control target length. Evaluation Metrics. We assess the capability of our audio tower through various robustness metrics and compare with other size-matched baselines. Specifically, we consider fidelity, diversity, semantic alignment, and perceptual quality, which provide an overall view of audio generation performance. For fidelity, we use the FrÃ©chet Distance (FD) with OpenL3 Embeddings [44], which measures how closely the generated 7 Figure 4 Training pipeline overview. (a) Audio tower pretraining: We train 1.3B text-to-audio model with Wan2.1style architecture on music, general sounds, and TTS data. The audio VAE remains frozen during this stage. (b) Synchronous joint training: The video tower (A14B, blue) and audio tower (1.3B, orange) are connected via bidirectional (c) Video and audio timesteps are sampled independently, allowing each modality Bridge cross-attention modules. to follow its own noise schedule. (d) Bridge modules use higher learning rate (ğœ‚br = 2 105) than backbone DiT blocks (ğœ‚b = 1 105) to accelerate cross-modal alignment while preserving pretrained priors. Both VAEs remain frozen throughout training. audio matches real audio in distribution. Diversity is quantified using the Inception Score (IS)[45] which evaluates whether the model produces both varied and high-quality samples. Furthermore, robustness is captured by KL divergence with PaSST[46] in order to assess the stability of generated audio across different conditions. To assess the semantic consistency, we compute the CLAP score[47] to measure alignment between the conditioning text prompt and the generated audio. Beyond four metrics mentioned above, we incorporate perceptual evaluation using the AudioBox Aesthetic Benchmark[48]. The benchmark reports four humanpreference-oriented indicators: aesthetics, naturalness, intelligibility and consistency. These dimensions capture overall appeal, absence of audible artifacts, clarity of linguistic or musical content, and the temporal coherence across the entire audio clip respectively. Results. As shown in Table 2, our model achieves competitive performance on the AudioCaps [49] benchmark. It attains strong IS of 10.54, clearly higher than AudioLDM2 (7.79), while maintaining competitive CLAP score of 0.463. In terms of FD openl3, our result (72.25) is nearly identical to AudioLDM2 [50, 51] (72.04) and much better than TangoFLUX [52] (80.47), indicating stronger semantic fidelity. The KL divergence (1.47) also improves upon AudioLDM2 (1.66), suggesting better distribution alignment. In Table 3, our method achieves the best results on Consistency (CU = 5.56) and Perceptual Quality (PQ = 6.20), surpassing advanced baselines such as AudioLDM2 and Tango2 [53]. Although it does not lead in CE or PC, the scores remain competitive. All these results demonstrate that our audio tower provides improvements in fidelity and coherence, while enhancing perceptual quality and semantic alignment compared to existing approaches. 8 Table 2 Text-to-audio performance benchmark with AudioCaps Dataset. We report parameter size, number of function evaluations (NFE), FrÃ©chet Distance with Openl3 embeddings (FD openl3), KL divergence with PaSST (KL passt), CLAP score, and Inception Score (IS). Model Params NFE FD openl3 KL passt CLAP score IS TangoFLUX [52] AudioLDM2 [51] Ours 516M 346M 1.3B 50 200 100 80.47 72.04 72.25 1.02 1.66 1.47 0.546 0.409 0.463 13.28 7.79 10.54 Table 3 AudioBox Benchmark results. CE, CU, PC, and PQ denote the four evaluation indicators from AudioBox. Best results are highlighted in bold. Model AudioLDM [50] AudioLDM2 [53] Make-An-Audio 2 [54] Tango 2 [53] TangoFLUX [52] Ours CE 3.27 3.48 3.23 3.47 3.54 3.41 CU 5.10 5.54 4.98 5.20 5.07 5. PC PQ 3.23 3.00 3.17 3.84 3.64 3.04 5.82 6.09 5.58 5.89 5.78 6."
        },
        {
            "title": "4.3 Progressive Joint Training",
            "content": "Our joint training follows three-phase data and resolution curriculum, progressively refining both data quality and output resolution. Phase 1 (360p baseline): We initialize from pretrained Wan2.2 A14B (video) and 1.3B audio tower, inserting the Bridge module with random initialization. Training proceeds at 360640 resolution for 193 frames (8 seconds at 24 fps) on 1024 GPUs. We train on approximately 61,500 hours of diverse video-audio data drawn from SpeakerVid5M, Chinese drama, cartoon, movies, YouTube, and OpenHumanVid. We use asymmetric sigma-shift values with shiftğ‘£ = 5.0 and shiftğ‘ = 1.0 to focus video learning on aggressive denoising while keeping audio transitions smoother, and employ aggressive text dropout (ğ‘text drop = 0.5) to force Bridge-based alignment learning. This phase runs for 1 epoch (15 days). Phase 2 (quality-filtered alignment): We refine the noise schedule by aligning audio to match video, setting shiftğ‘ = 5.0 to match the video schedule. While Phase 1 uses smoother audio schedule for stability, we find that high-fidelity timbre is sensitive to the noise schedule and denoising steps. Therefore, in Phase 2 we align the audio sigma shift to the video setting (shiftğ‘ = 5.0) to strengthen audio denoising and improve timbre fidelity. This alignment enables the audio tower to benefit from the same aggressive noise schedule that improves video quality, without requiring architectural changesonly the sigma-shift parameter is updated. We also reduce text dropout to ğ‘text drop = 0.2 to allow text-guided refinement while retaining learned crossmodal priors, and introduce LUFS normalization to mitigate CFG-induced loudness explosion. This phase trains on approximately 37,600 hours of quality-filtered data for 1 epoch (7 days). To improve training quality, we curate the Phase 2 dataset using three complementary filters. First, we use OCR to identify videos without burned-in subtitles, retaining 9.5M clips and appending This video has no subtitles. to their prompts to teach the model this distinction. Second, we retain videos with LSE-D 9.5 and LSE-C 4.5, yielding 2.5M clips with high-quality lip-audio correspondence. Third, we apply DOVER technical quality score > 0.15 to select videos with superior visual fidelity, yielding 2.4M clips. The resulting dataset contains 16.8M clips (37,600 hours), balancing scale and quality. Phase 3 (720p fine-tuning): We upscale to 7201280 resolution, training on approximately 11,000 hours of the 9 720p highest-quality subset (DOVER technical score > 0.14). The increased sequence length requires modified parallelism configuration: we increase context parallelism from CP=8 to CP=16. Checkpoint frequency increases to every 2000 steps to capture rapid convergence. This phase runs for 1 epoch (20 days). Computational Resources. All three phases run on 1024 GPUs (128 nodes, 8 GPUs per node). For 360p training (Phases 12), we use CP=8, yielding effective batch size 128. For 720p fine-tuning (Phase 3), increased sequence length requires CP=16, reducing effective batch size to 64. The complete training spans 42 days, totaling approximately 43,000 GPU-days. See Appendix A.1 for the complete hyperparameter configuration."
        },
        {
            "title": "4.4 Optimization Details",
            "content": "Throughout all three phases, we optimize the full model end-to-end: the Bridge module and both pretrained towers are updated jointly from the first step. This differs from two-stage warm-start that first trains the Bridge with frozen towers and then fine-tunes the full model. In our early experiments, the two-stage scheme reached an early performance plateau, which motivated end-to-end joint optimization. The main tension is that the Bridge must learn cross-modal correspondence quickly, while the large pretrained towers should remain stable and preserve their strong unimodal priors. We mitigate this tension with heterogeneous learning rates across module groups. Heterogeneous Learning Rates. To balance fast Bridge convergence with tower stability, we use higher learning rate for the Bridge (ğœ‚ğ‘ğ‘Ÿ = 2 105) than for the backbone towers (ğœ‚ğ‘ = 1 105). This factor-of-two difference accelerates Bridge learning while reducing forgetting in the pretrained towers. In our experiments, uniform learning rate either destabilizes the towers (if too high) or leaves the Bridge under-trained (if too low). Dual Sigma Shift. Conventional joint diffusion that forces video and audio to share the same timestep can be suboptimal. The two modalities have different effective complexities: audio uses fewer tokens per second, yet timbre fidelity is highly sensitive to noise schedules. single noise level may be too aggressive for one modality and too mild for the other, causing imbalanced gradients. To enable this, we decouple the timestep sampling for each modality. During training, we independently draw ğ‘¡ğ‘£ and ğ‘¡ğ‘ from ğ’°(0, 1): ğ‘§ğ‘¡ğ‘£ ğ‘£ = (1 ğœğ‘£(ğ‘¡ğ‘£)) ğ‘§0 ğ‘§ğ‘¡ğ‘ ğ‘ = (1 ğœğ‘(ğ‘¡ğ‘)) ğ‘§0 ğ‘£ + ğœğ‘£(ğ‘¡ğ‘£) ğœ–ğ‘£, ğ‘ + ğœğ‘(ğ‘¡ğ‘) ğœ–ğ‘, (4) where ğœğ‘š(ğ‘¡) = shiftğ‘šğ‘¡ shiftğ‘š+ğ‘¡(1shiftğ‘š) controls the noise schedule for modality ğ‘š {ğ‘£, ğ‘}. This decoupling provides two benefits. First, each modality follows its natural denoising trajectorywe set shiftğ‘£ = 5.0 (aggressive) and shiftğ‘ = 1.0 (gradual) in Phase 1, then align them to shiftğ‘ = 5.0 in Phase 2 to improve timbre fidelity. Second, noise schedules can be adjusted at inference time without retraining."
        },
        {
            "title": "4.5 Training Efficiency",
            "content": "For large-scale training, we shard model parameters with Fully Sharded Data Parallel (FSDP) [55] and adopt sequence parallelism via USP [56], achieving approximately 35% MFU. To eliminate redundant VAE computation introduced by sequence parallelism, we follow the approach in Wan [5]: for each CP group, the input preprocessing (primarily the VAE) is performed only once per CP step, and the preprocessed features are then broadcast from designated rank to all other ranks within the same CP group before being fed into the DiT backbone, thereby avoiding duplicated computation. We use manual memory management to avoid the same Pythons garbage collection (GC) overhead reported in OpenSora2 [4]. We also port our training stack to Ascend NPUs and apply operator fusion for attention kernels, tensor layout transforms, and rotary embedding computation to reduce framework overhead. Under an 8-device configuration (CP=4, DP-shard=2), we measure 34.1 s/step on 8 Ascend 910A2; benchmark details are provided in Appendix A.2. 10 Because standard FSDP requires consistent computation graph, for the A14B MoE video tower we adopt an alternating optimization strategy: on odd steps we sample high-noise timesteps for all samples and optimize the high-noise DiT, while on even steps we sample low-noise timesteps and optimize the low-noise DiT. In addition, the shared bridge and the audio tower are optimized at every step."
        },
        {
            "title": "5.1 Dual Classifier-Free Guidance",
            "content": "In the text-conditioned video-audio generation (T2VA) setting, we may view paired video-audio sample as single joint latent, where the only explicit condition is text. However, from single-modality perspective, the other modality provides additional conditional information: when predicting video, audio serves as condition, and vice versa. This perspective introduces extra controllability, because we can separately adjust the guidance strengths for text conditioning and cross-modal conditioning. Following the dual Classifier-Free Guidance (dual CFG) proposed in InstructPix2Pix [57], we adapt CFG to joint audio-video models with two conditioning sources: the textual prompt ğ‘ğ‘‡ and the cross-modal information induced by Bridge interactions ğ‘ğµ. We derive principled dual CFG formulation by decomposing the joint posterior via Bayes rule: ğ‘ƒ(ğ‘§ ğ‘ğ‘‡, ğ‘ğµ) = ğ‘ƒ(ğ‘ğ‘‡ ğ‘ğµ, ğ‘§)ğ‘ƒ(ğ‘ğµ ğ‘§)ğ‘ƒ(ğ‘§) ğ‘ƒ(ğ‘ğ‘‡, ğ‘ğµ) . Taking the score function (gradient of log-likelihood) and applying CFG scaling leads to: ğ‘£ğœƒ = ğ‘£ğœƒ(ğ‘§ğ‘¡, , ) + ğ‘ ğµ [ğ‘£ğœƒ(ğ‘§ğ‘¡, , ğ‘ğµ) ğ‘£ğœƒ(ğ‘§ğ‘¡, , )] + ğ‘ ğ‘‡ [ğ‘£ğœƒ(ğ‘§ğ‘¡, ğ‘ğ‘‡, ğ‘ğµ) ğ‘£ğœƒ(ğ‘§ğ‘¡, , ğ‘ğµ)] , (5) (6) where denotes null conditioning, ğ‘£ğœƒ(ğ‘§ğ‘¡, ğ‘ğ‘‡, ğ‘ğµ) is the models velocity prediction with both text and Bridge active, and ğ‘£ğœƒ(ğ‘§ğ‘¡, , ) disables both (the no-bridge mode disables cross-modal injection). By tuning ğ‘ ğµ and ğ‘ ğ‘‡, we control the trade-off between alignment and perceptual quality. In the most general setting, we use Dual CFG with three function evaluations per step (NFE= 3), and the two common special cases below reduce to NFE= 2: Dual CFG (general) (ğ‘ ğµ = ğ‘ ğ‘, ğ‘ ğ‘‡ = ğ‘ ğ‘¡): The full formulation that independently scales (i) modalityalignment guidance through the Bridge term and (ii) text guidance. This provides the most flexible control over the alignmentquality trade-off, at the cost of one additional model call (NFE= 3). Text-only CFG (ğ‘ ğµ = 1, ğ‘ ğ‘‡ = ğ‘ ): Standard formulation. Bridge remains active in both branches, so guidance does not explicitly amplify cross-modal alignment. Yields high semantic fidelity (e.g., ImageBind scores) but weaker temporal sync (higher DeSync). This is two-branch guidance scheme (NFE= 2). Text + modality CFG (ğ‘ ğµ = ğ‘ , ğ‘ ğ‘‡ = ğ‘ ): The unconditional branch disables Bridge injection, isolating the alignment signal. Produces stronger synchronization (lower DeSync, better lip-sync). This also uses two branches (NFE= 2)."
        },
        {
            "title": "5.2 Generation Workflow",
            "content": "To address diverse user input formats and maintain style consistency with training data, which, in our opinion, can incentivize models full potential. Given user-provided initial frame and text prompt, our workflow generates coherent video and audio, as illustrated in Figure 5. The primary objective of this pipeline is prompt enhancement: rather than directly using raw user inputs which often lack descriptive detail, we 11 Figure 5 The overall workflow of MOVA for text-image and text-only to video-audio generation. refine them to incentivize the models full generative potential. We observe that maintaining high performance is closely tied to the quality of the prompt; therefore, our multi-stage conditioning pipeline explicitly extracts visual grounding from the reference image and synthesizes an enriched narrative. This ensures that the final synthesis not only preserves the style, lighting, and cinematography established in the first frame but also aligns with the sophisticated data distribution the model was trained on. Our pipeline consists of three primary stages. First, we utilize Qwen3-VL [58], vision-language model, to extract structured visual description. This extraction is guided by curated prompt that constrains the model to four essential categories: (i) visual style, including color palette and lighting; (ii) cinematography, covering shot size, framing, and composition; (iii) visual elements, comprising subjects and their spatial relations; and (iv) OCR text, preserved exactly as it appears. This structured representation serves as an intermediate grounding that bridges the gap between the static image and the dynamic video. Second, we synthesize video generation prompt using LLMs (e.g. Gemini 2.5 Pro [59], conditioned on both the input text and the extracted visual description. The core design principle is to preserve the static attributes from the visual description while incorporating the temporal dynamics specified by the user text. We employ in-context learning [60] to ensure the generated prompt follows the narrative style and architecture of the training data. Finally, MOVA generates video content by leveraging both the synthesized prompt and the initial frame as dual conditioning. This mechanism integrates narrative descriptions with visual grounding to ensure temporal synchronization while adhering to established visual priors. Furthermore, the versatility of our workflow allows for text-to-video-audio generation using text prompt and an uninformative white image as input, thereby demonstrating MOVAs capacity for high-quality, zero-shot video synthesis."
        },
        {
            "title": "6.1 Experiment Setup",
            "content": "Benchmarks. In this work, we use two benchmarks to test the models video generation capabilities. We adopt Verse-Bench [17], which consists of 600 imagetext prompt pairs. To facilitate joint videoaudio generation, we employ GPT-5 [61] to unify visual and audio descriptions into single, cohesive prompt. While Verse-Bench [17] provides large-scale collection of imagetext prompts, it is not specifically designed for evaluating videoaudio generation in various scenes. To further evaluate joint videoaudio generation in 12 Figure 6 Dataset overview. (a) Category distribution of samples in the dataset, illustrating the relative proportions of different image categories. (b) Statistical distribution of prompt lengths. realistic and challenging settings, we construct dedicated evaluation benchmark covering diverse video generation scenarios. The benchmark is designed to assess capabilities that are critical for joint videoaudio modeling, including temporal coherence, audiovisual synchronization, multi-character interaction, and dynamic scene evolution. Unlike Verse-Bench, which provides broad visual coverage, our benchmark adopts finer-grained scenario taxonomy and explicitly categorizes samples into six representative video generation types, including multi-speaker interaction, movie-style narratives, sports competitions, game livestreams, camera motion sequences, and anime-style content. Detailed benchmark construction procedures and category descriptions are provided in Appendix A.6. Evaluation metrics. We evaluate our method through both objective benchmarks and subjective human evaluations. Objective evaluation: For objective evaluation, we report performance on Verse-Bench across several dimensions. We measure acoustic fidelity and diversity using the Inception Score (IS) computed with PANNs [62] classifier, and assess speech quality with DNSMOS [63]. Cross-modal semantic alignment is quantified using the ImageBind score (IB-Score) [37] computed on joint audio-video embeddings. Temporal consistency is measured by the DeSync score predicted by Synchformer [36]. Furthermore, to rigorously evaluate lip-sync precision, we include the Lip Sync Error - Confidence (LSE-C) and Distance (LSE-D) metrics derived from SyncNet [64]. Additionally, we evaluate the concatenated minimum permutation character error rate (cpCER) score on the multi-speaker subset of our constructed benchmark. This metric is designed to assess whether the speaker identities and speaking content are correctly reflected in the generated outputs. To evaluate speaker timbre and dialogue content, we employ the MOSS Transcribe Diarize [65]. This model performs speaker diarization using explicit speaker tags such as [S01] and [S02], followed by automatic speech recognition that transcribes the corresponding spoken content for each identified speaker. Subjective Evaluation: We conduct an arena-style preference study to evaluate human perception. The evaluation set comprises 732 samples, including 600 from Verse-Bench and 132 from our newly introduced benchmark, where half of the originally English-only Verse-Bench speech data was manually translated to construct bilingual mix. For each comparison, participants are tasked with selecting the superior video across five dimensions: (i) prompt adherence, (ii) visual-audio synchrony, (iii) lip-sync accuracy, (iv) video quality, and (v) audio-speech fidelity. standard ELO rating system is employed to compute model rankings based on pairwise human preference judgments. Following the official Chat13 Table 4 Quantitative comparison of audio-visual generation performance on Verse-Bench. IS and AV-Align metrics are evaluated on all Verse-Bench subsets; DNSMOS and Lip Sync metrics are evaluated on Verse-Bench set3; ASR Acc is evaluated on the multi-speaker subset. Bold and underlined values denote the best and second-best results, respectively. Model LTX-2 [66] Ovi [18] WAN2.1 + MMAudio MOVA-360p w/ dual CFG MOVA-720p w/ dual CFG ğ‘ ğµ - - - 1.0 3.5 1.0 3. Audio-Speech AV-Align Lip Sync ASR Acc IS DNSMOS DeSync IB-Score LSE-D LSE-C cpCER 3.066 3.680 4.036 4.269 4.169 3.936 3.814 3.635 3.516 3.797 3.674 3.671 3.751 0.451 0.515 0. 0.475 0.351 0.485 0.370 0.213 0.190 0.317 0.286 0.315 0.277 0.297 7.261 7.468 8.098 7.004 8.048 7.094 6.109 6.378 6.278 7.800 6.593 7.452 0.220 0.436 0.177 0.247 0.149 0.218 bot Arena implementation 2, we set the initial ELO rating to 1000 with K-factor of 4. The logistic scale and base are configured at 400 and 10, respectively. To ensure statistical robustness, we report the results using 1000 bootstrap iterations. Baselines. We compare MOVA with three baseline models under unified protocol. Specifically, the baselines span two paradigms: (i) synchronous audiovisual generators that produce video and speech jointly: LTX-2 [66] and Ovi [18]. (ii) cascaded pipeline formed by coupling WAN2.1 [5] for video with MMAudio [24] for audio. For fair comparison, we standardize the spatial resolution at 720p and adopt the recommended configurations for all baseline models. Ovi [18] is single-stage audiovideo generation model that jointly models both audio and video modalities within single process. By employing two architecturally DiTs for audio and video, Ovi achieves natural audiovisual synchronization without relying on separate pipelines or post hoc alignment. In the fusion blocks, OVI uses frozen T5 encoder to integrate the video model with pretrained audio model capable of generating both speech and environmental sounds. This shared semantic conditioning allows the audio and video branches to be guided by the same semantic context, strengthening cross-modal coherence and synchronization. LTX-2 [66] introduces an efficient audiovisual generation model that, like OVI, produces video and its synchronized audio within single diffusion process. Unlike OVI, LTX-2 adopts an asymmetric dualstream Transformer architecture, enabling temporal alignment through bidirectional attention across all layers. The model employs modality-specific VAEs and positional encodings for audio and video, which preserve generation quality while significantly improving computational efficiency. WAN2.1 + MMAudio is cascaded baseline that decouples video and audio generation. Specifically, we use WAN2.1 [5] as the video backbone, followed by MMAudio [24] for video-conditioned audio synthesis. Unlike joint or synchronous modeling, this pipeline allocates the primary modeling burden to strong video generator to capture spatiotemporal dynamics and kinematics, while the conditional audio generator focuses on acoustic consistency. This approach provides competitive system-level baseline for audio-visual pairing."
        },
        {
            "title": "6.2 Experimental Results",
            "content": "2https://colab.research.google.com/drive/1RAWb22-PFNI-X1gPVzc927SGUdfr6nsR 14 6.2.1 Comparison with Baseline Methods We evaluate the performance of MOVA against several competitive baselines, including LTX-2 [66], Ovi [18], and cascaded pipeline (WAN2.1 + MMAudio). Table 4 presents comprehensive quantitative comparison across four critical dimensions. Audio Fidelity and Speech Quality. MOVA demonstrates clear advantage in generating high-quality audio. Specifically, MOVA-360p achieves state-of-the-art IS of 4.269 and DNSMOS [63] of 3.797, significantly outperforming LTX-2 and Ovi. We observe that while the cascaded WAN2.1 + MMAudio baseline shows respectable IS (4.036), it lacks the capability to generate intelligible speech content. In contrast, MOVA maintains high speech naturalness and clarity even as we scale the resolution to 720p (DNSMOS of 3.671), suggesting that our unified modeling effectively captures the complex distribution of human speech and ambient sounds. Audio-Visual Alignment. We evaluate audio-visual alignment through two lenses: temporal synchronization (DeSync [36]) and cross-modal semantic alignment (IB-Score [37]). Compared to contemporary unified models like LTX-2 and Ovi, MOVA exhibits pronounced advantage. Specifically, MOVA-360p with dual CFG (ğ‘ ğµ = 3.5) achieves DeSync of 0.351 and an IB-Score of 0.315, significantly surpassing LTX-2 (0.451 / 0.213) and Ovi (0.515 / 0.190). This substantial margin, particularly the 50% improvement in IB-Score over Ovi, suggests that MOVA effectively binds auditory events to generated visual context. Notably, although the specialized cascaded pipeline (WAN2.1 + MMAudio) yields the best DeSync (0.260) due to its task-specific audio generator, MOVA virtually closes the gap in both temporal and semantic metrics. This demonstrates that our dual CFG strategy effectively amplifies the cross-modal alignment signal during inference, allowing unified architecture to match the precision of modularized pipelines without sacrificing structural simplicity or end-to-end coherence. Lip-Sync Precision. The accuracy of fine-grained lip synchronization is measured by LSE-D and LSE-C [64]. MOVA variants, particularly those equipped with dual CFG, exhibit dominant performance in this category. MOVA-360p w/ dual CFG achieves the best LSE-D (7.004) and LSE-C (7.800), representing substantial margin over LTX-2 (7.261/6.109) and Ovi (7.468/6.378). Interestingly, even without dual CFG, MOVA-720p maintains competitive LSE-C of 6.593. This superior lip-sync performance confirms that our model has internalized sophisticated mapping between phonetic features and labial dynamics, which is further refined by the explicit guidance of the Bridge interactions during inference. Multi-Speaker Attribution. As illustrated in Table 4, MOVA-720p achieves the cpCER of 0.149, lower than LTX-2 (0.220) and Ovi (0.436). The high error rate of Ovi in this metric suggests frequent voice-identity mismatch, where the model fails to associate the correct speech with the corresponding subject. The low cpCER of MOVA demonstrates its ability to maintain high-fidelity identity consistency and correct audiovisual attribution in crowded scenes, crucial requirement for realistic content generation. For both MOVA360p and MOVA-720p, the introduction of dual CFG leads to slight increase in cpCER. This is primarily because dual CFG redistributes the guidance strength among multiple conditional branches during sampling, thereby diluting the relative weight of the text condition and reducing the models ability to follow explicit speaker instructions, such as speaker tags [S01]/[S02] and their corresponding speech content. In multi-speaker scenarios, this weakened instruction-following behavior more easily results in mismatches between speaker identities and transcribed content, which is reflected in higher cpCER. In addition, compared to MOVA-360p, MOVA-720p consistently achieves lower cpCER under both settings. This advantage mainly stems from differences in training data scale and training stages. MOVA-720p is further fine-tuned on top of MOVA-360p with broader data coverage and more extensive exposure to multi-speaker samples, leading to improved stability in speaker identity and speech content association. 15 Figure 7 Ablation study on human preference. Table 5 Ablation study of ğ‘ ğµ. IS and AV-Align metrics are evaluated on all Verse-Bench subsets; DNSMOS and Lip Sync metrics are evaluated on Verse-Bench set3; ASR Acc is evaluated on the multi-speaker subset. Bold and underlined values denote the best and second-best results, respectively. Model ğ‘ ğµ Audio-Speech AV-Align Lip Sync ASR Acc IS DNSMOS DeSync IB-Score LSE-D LSE-C cpCER MOVA-360p 1.0 w/ dual CFG 2.0 3.0 3.5 4.0 4.269 4.222 4.319 4.169 4. 3.797 3.748 3.686 3.674 3.631 0.475 0.421 0.388 0.351 0.365 0.286 0.305 0.312 0.315 0.316 8.098 7.323 7.014 7.004 6.957 6.278 7.331 7.774 7.800 7.891 0.177 0.185 0.188 0.247 0. 6.2.2 Ablation Study We conduct series of ablation experiments of MOVA to evaluate the impact of our training strategy and inference configurations, which is shown in Table 4, Table 5 and Table 6. Scaling to High Resolution. We evaluate the empirical robustness of MOVA by scaling the generation resolution from 360p to 720p. As summarized in Table 4, the 720p variant demonstrates remarkawble consistency across diverse evaluation dimensions. Specifically, in terms of temporal and semantic alignment, MOVA-720p maintains DeSync of 0.485 and an IB-Score of 0.277, showing negligible degradation compared to the 360p base model (0.475 and 0.286, respectively). This stability is particularly noteworthy as increasing visual resolution often introduces challenges in maintaining cross-modal coherence. Additionally, MOVA-720p achieves Lip Sync LSE-C of 6.593, outperforming the 360p versions 6.278. Similarly, it reports the best-ever cpCER (0.149) on the multi-speaker subset. While there is marginal decrease in audio fidelity and speech quality metrics, which is common trade-off when the model capacity is further distributed to handle increased visual complexity, the overall performance profile remains highly competitive. These results collectively validate our staged training strategy, confirming that MOVA can effectively scale to high-resolution synthesis while preserving its foundational audio-visual generative priors. 16 Effect of Dual Classifier-Free Guidance. We evaluate the influence of the dual CFG scale ğ‘ ğµ in Table 5. Our results demonstrate synergistic improvement across all alignment-related metrics as ğ‘ ğµ increases from 1.0 to 4.0. Specifically, we observe consistent reduction in DeSync and LSE-D, alongside significant gain in IB-Score and LSE-C. For instance, as ğ‘ ğµ scales to 4.0, the LSE-C reaches peak of 7.891 and the DeSync score is minimized to 0.365. This uniform progression across multiple distinct metrics suggests that strengthening the modality-alignment guidance effectively improves the synchronization between generated video and audio. However, this heightened alignment precision comes at clear cost to speech quality and instruction following. As the guidance toward audio-visual alignment becomes more dominant, we observe concurrent degradation in DNSMOS and rise in cpCER (from 0.177 to 0.264). We interpret this phenomenon as form of conditional interference: in the multi-branch sampling process, excessively high ğ‘ ğµ prioritizes the geometric constraints of synchronization over the generative fidelity of the speech signal. This over-regularization potentially leads to diminished sensitivity to textual instructions (e.g., speaker-specific tags), causing the model to prioritize how the speech aligns with the video at the expense of what is being said and how natural it sounds. Emergent T2VA Capability. Interestingly, we find that MOVA exhibits strong emergent capability for the T2VA task, which is summarized in Table 6. By substituting the reference image with null placeholder (MOVA-360p-T2VA), we test whether the model can synthesize synchronized content driven solely by textual prompts. As summarized in Table 6, several intriguing observations emerge. Notably, the T2VA variant achieves superior IS of 4.370 and lower DeSync of 0.441 compared to the standard TI2VA baseline. This performance gain suggests that in the absence of explicit structural constraints from reference image, the model can more freely explore the joint audio-visual manifold, leading to higher audio fidelity and improved temporal synchronization. Predictably, identity-dependent metrics like LSE-C and LSE-D show marginal decline, as the null placeholder provides no lip geometry. However, the overall stability of the T2VA results is remarkable. This underscores that MOVA has internalized robust, decoupled yet highly coordinated prior for video and audio synthesis, allowing it to maintain temporal synchronization and semantic alignment even when the visual conditioning is entirely removed. 6.2.3 Arena-Based Human Evaluation Given the limitations of current objective evaluation frameworks for audio-visual generation models, MOVA introduces an Arena-based human preference evaluation paradigm that includes the latest open-source audio-visual generation models worldwide. The evaluation collected over 5,000 valid votes and systematically analyzed the results. To ensure fair comparison, all models utilize their respective official promptrefinement methods (e.g., our rewriter described in Section 5.2) to enhance the video generation prompt. Subjective Comparison with Baseline Methods. As shown in Figure 8, MOVA demonstrates clear superiority in human preference: it is more frequently selected by users in pairwise comparisons, achieving an ELO rating of 1113.8 (starting from an initial rating of 1000), significantly higher than all baseline models. MOVA consistently maintains win rate exceeding 50%, with win rates surpassing 70% against Ovi and the cascaded system (WAN + MMAudio). Subjective Ablation Study. As illustrated in Figure 7, we conduct an internal Arena to dissect the impact of prompt refinement, resolution scaling, and inference strategies on human preference. primary observation is the critical role of the prompt rewriter. Variants utilizing refined prompts consistently achieve superior ELO ratings, with MOVA-720p (w/ rewriter) reaching the peak score of 1025.3. Compared to the standard MOVA-720p (982.9), this substantial ELO gain validates our motivation: user-provided inputs often vary in format and level of detail, creating distribution gap with the models training data. By employing our multi-stage conditioning pipeline, which leverages LLMs to synthesize prompts that preserve visual grounding (e.g., style, cinematography) while incorporating temporal dynamics, we bridge this gap and effectively incentivize the models full generative potential. Regarding our inference strategy, we observe subtle trade-off; while dual CFG (ğ‘ ğµ = 3.5) significantly improves objective alignment metrics, it leads to 17 (a) Human preference ELO ranking. (b) Win rates of MOVA against baseline models. Figure 8 Arena evaluation results showing MOVAs performance in human preference studies. (a) ELO ratings demonstrate MOVAs superiority over baseline models. (b) Pairwise win rates show MOVA consistently outperforms all competitors, particularly against Ovi and the WAN + MMAudio cascade. Table 6 Evaluation of T2VA effectiveness. IS and AV-Align metrics are evaluated on all Verse-Bench subsets; DNSMOS and Lip Sync metrics are evaluated on Verse-Bench set3; ASR Acc is evaluated on the multi-speaker subset. Bold values denote the best results. Model Audio-Speech AV-Align Lip Sync ASR Acc IS DNSMOS DeSync IB-Score LSE-D LSE-C cpCER MOVA-360p 4.269 MOVA-360p-T2VA 4.370 3.797 3.767 0.475 0.441 0.286 0.281 8.098 8.362 6.278 5. 0.177 0.188 slight decrease in human preference scores, dropping from 1025.3 to 1014.5 in the rewriter-enhanced 720p models. We attribute this decrease to the formulation of dual CFG: by explicitly amplifying cross-modal alignment signals, the relative guidance scale for the primary text instruction is effectively diminished. This can occasionally result in reduced instruction-following."
        },
        {
            "title": "6.3 Scaling to Lip Synchronization",
            "content": "Lip synchronization is among the most demanding audiovideo generation tasks. Unlike discrete, onsetdriven events (e.g., chopping fruit or hitting drum) where alignment depends on few salient temporal onsets, speech requires continuous, fine-grained correspondence between mouth shapes and phonemes across long spans. We find that architectural mechanisms alone (e.g., Bridge modules for cross-modal attention) are insufficient to achieve high-quality lip synchronizationthe model must also learn phoneme-toviseme mappings from data, which requires larger capacity and more training examples. Figure 9 shows the progression of LSE-C (higher is better) and LSE-D (lower is better) across our three-stage training process. In Stage 1, we train at 360p with aggressive video denoising, mild audio denoising, and high text dropout to force the model to rely on cross-modal bridging for alignment. LSE-D drops rapidly and LSE-C rises, indicating the model quickly learns basic synchronization patterns. Stage 2 maintains 360p but aligns the noise schedules across modalities for more stable cross-modal attention, reduces text dropout to refine semantic details, and applies loudness normalization to avoid CFG-induced volume distortion. LSE-D continues to decrease while LSE-C shows notable jump, reflecting improved consistency and confidence in alignment. Finally, Stage 3 scales to 720p. With stable cross-modal alignment already established, the model can safely allocate capacity to higher resolution and finer spatial details without disrupting the learned synchronization structure. LSE-D further decreases and plateaus, while LSE-C stabilizes at high level, indicat18 Figure 9 Training progression across three stages. Stage 1 (360p, aggressive bridging) establishes basic alignment; Stage 2 (360p, aligned schedules) refines consistency; Stage 3 (720p) scales to high resolution while preserving synchronization. ing convergence to high-quality lip synchronization."
        },
        {
            "title": "7.1 Predefined sigma as an Implicit Synchronization Prior",
            "content": "Audio-visual synchronization inherently contains tension in conditioning direction. For many discrete, event-driven cases, the most natural formulation is VideoAudio: the visual stream deterministically anchors when and where events occur, and most sound events (impacts, collisions, cuts, percussive gestures) are visually driven with clear temporal onsets. In these settings, generating audio conditioned on video aligns with both human intuition and the causal structure of the scene. In contrast, other synchronization tasks are more realistically AudioVideo and better match practical applications. Speech-driven generation is the canonical example: given an audio track, the target is to produce temporally consistent facial motion and lip articulation (SpeechVideo), potentially with speaker-specific dynamics, language-dependent phonemeviseme mappings, and context-dependent expressiveness. This directionality is often required in downstream pipelines such as dubbing, avatar animation, and talking-head generation, where audio is fixed and visuals must adapt. This tension becomes more pronounced under the diffusion formulation. At each timestep, the noise levels for video and audio latents are governed by pre-defined schedules ğœğ‘£(ğ‘¡ğ‘£) and ğœğ‘(ğ‘¡ğ‘) (introduced in Dual Sigma Shift), which act as fixed priors and are not learnable during training. As result, the relative corruption of the audio stream is largely fixed by design, whereas the effective uncertainty in the visual stream can vary substantially with object scale and visual dominance in the frame (as discussed in Stable Diffusion 3 [67]). Consequently, the same global schedule can implicitly bias the conditional direction: for close-up shots of lip motion where the target region occupies large portion of the frame, the visual latent is relatively informative and the generation tends to behave like VideoAudio; conversely, when the relevant speaker occupies only small region, the visual evidence becomes comparatively uncertain, and the process may naturally shift toward AudioVideo, letting speech provide the more reliable temporal anchor."
        },
        {
            "title": "7.2 Classifier-Free Guidance: factorization order",
            "content": "Our dual-CFG derivation starts from the factorization ğ‘ƒ(ğ‘§ ğ‘ğ‘‡, ğ‘ğµ) ğ‘ƒ(ğ‘ğ‘‡ ğ‘ğµ, ğ‘§) ğ‘ƒ(ğ‘ğµ ğ‘§) ğ‘ƒ(ğ‘§), which induces natural nesting structure: we first turn on cross-modal information (via ğ‘ğµ) and then apply text guidance on top of it (via ğ‘ğ‘‡). This order is not the only valid choice. For example, one may alternatively 19 decompose the posterior as ğ‘ƒ(ğ‘§ ğ‘ğ‘‡, ğ‘ğµ) ğ‘ƒ(ğ‘ğµ ğ‘ğ‘‡, ğ‘§) ğ‘ƒ(ğ‘ğ‘‡ ğ‘§) ğ‘ƒ(ğ‘§), (7) and derive an analogous two-term guidance by swapping the roles of ğ‘ğ‘‡ and ğ‘ğµ. We adopt our order for practical reason: it admits clean reduction to the standard text-only CFG as special case. Concretely, setting ğ‘ ğµ = 1 makes the Bridge condition appear in both the conditional and unconditional text branches, so the guidance collapses to the familiar text-CFG form while keeping crossmodal injection fixed. In other words, by tuning (ğ‘ ğµ, ğ‘ ğ‘‡) we can continuously interpolate between text-only CFG and text + modality CFG without changing the sampling procedure. In contrast, under the alternative factorization, the branch structure couples ğ‘ğµ to the text-unconditional baseline in way that prevents an equivalent text-only CFG limit: there is no choice of guidance weights that keeps the Bridge behavior identical across the two text branches while still producing the standard textCFG difference term. As result, the swapped order does not provide an intuitive knob for only amplify text while leaving cross-modal interactions unchanged."
        },
        {
            "title": "7.3 Limitations",
            "content": "Audio Modeling Capacity and Coverage. Our audio tower follows the Wan2.1-1.3B backbone, which may limit the modeling capacity for acoustically rich or highly structured signals. In particular, we observe degraded performance on singing voice, complex sound textures, and music/instrumental content, where finegrained pitch/harmonic structure and long-range temporal dependencies are critical. More broadly, some audiovisual phenomena require stronger physical reasoning (e.g., correctly reflecting propagation delays such as the temporal offset between lightning and thunder), which is not explicitly enforced by our current objective and data. Multi-speaker Synchronization and Annotation Reliability. While our model can handle single-speaker lip-sync reliably in many cases, multi-speaker scenes remain challenging. Rapid speaker turn-taking, overlapping speech, and ambiguous on-screen attribution can lead to incorrect mouthaudio assignment and temporal drift. This issue is compounded by the data pipeline: diarization errors and imperfect activespeaker labels can propagate to training, making the model conflate speakers or learn inconsistent supervision. Improving multi-speaker supervision (e.g., stronger active-speaker detection, cross-modal speaker tracking, and better filtering of noisy segments) is necessary for robust deployment. Sequence Length and Computational Cost. Our current training and inference are constrained by sequence length. For example, 720p, 8 clip at 24 fps yields on the order of 1.6 105 tokens, resulting in high memory and compute costs. This limits throughput during training and increases latency at inference, especially when using the most general guidance setting (NFE= 3). Future work could address this bottleneck via more aggressive temporal/spatial compression, hierarchical or blockwise generation, and system-level optimizations tailored to long-context video tokens."
        },
        {
            "title": "8 Related Work",
            "content": "Video Generation. Diffusion transformers (DiTs) [1, 68] have enabled large-scale video synthesis. Open models such as Wan [5] and HunyuanVideo [69] achieve near-photorealistic generation through efficient attention [70, 71] and transformer scaling [72]. Recent work extends these models to long-horizon generation [73], controllable camera motion [74], and high-resolution outputs exceeding 1080p. However, most textto-video systems remain video-only, leaving audio generation as separate problem. Proprietary systems Veo3 [12] and Sora2 [13] demonstrate joint audio-video capabilities, but their closed-source nature limits reproducibility. 20 Audio Generation and Cascaded Pipelines. Latent diffusion enables scalable text-to-audio generation [50, 75]. Audio VAEs compress waveforms into compact latent representations: DAC [76] uses residual vector quantization for high-quality reconstruction, while Stable Audio [77] employs stereo variational autoencoder with spectral losses. Cascaded video-to-audio (V2A) pipelines [24, 78, 79] represent prevalent approach for audiovisual content creation. MMAudio [24] achieves temporal alignment by extracting features from video and using them as conditioning signals. While cascaded pipelines utilize strong singlemodality priors, the sequential factorization ignores bidirectional modality influence: audio cannot inform visual trajectory during sampling, and vice versa. Joint Audio-Video Generation. End-to-end joint generation has been explored to overcome cascaded limitations [1720, 8082]. MMDisCo [83] uses discriminator-guided cooperative diffusion to align pretrained models, though adversarial training introduces instability at scale. MM-Diffusion [82] and JavisDiT [81] propose dual-stream architectures with cross-modal attention but are restricted to ambient sounds. MTV [80] explicitly separates audio into speech, effects, and music tracks with disentangled control over lip motion, event timing, and visual mood, achieving precise audio-visual synchronization across diverse audio types. UniVerse-1 [17] integrates Wan2.1 and Ace through stitching-of-experts paradigm with independent noise sampling, but suffers from audio-video drift. Recent works such as Ovi [18], Harmony [19], and UniAVGen [20] adopt dual-tower architectures with RoPE-based positional encoding, achieving lip-synchronized video generation results without requiring prior separation of speech and environmental sounds. However, they have not scaled to general domains to demonstrate the full potential of the architecture. LTX-2 [66] successfully scales the dual-tower approach to cover both lip-synchronized speech and general domain sounds through large-scale data training, though the audio quality exhibits some electronic artifacts that require cleaner reconstruction. We address these limitations through capacity scaling with 29B dual-tower architecture, achieving high-fidelity audio and strong performance on both lip-synchronized speech and general domain sounds across bilingual settings."
        },
        {
            "title": "9 Conclusions",
            "content": "We presented MOVA, an open and scalable framework for joint videoaudio generation with 32B total parameters (18B active). MOVA uses an asymmetric dual-tower design (an A14B video backbone and 1.3B audio backbone) together with 2.6B bidirectional bridge and Aligned RoPE to support fine-grained temporal audiovisual interaction. Our work targets three key challenges in joint audiovideo generation: data, modeling, and scaling. We curate over 100,000 hours of fine-grained audiovisual data with sound, music, and speech annotations aligned to visual content. We also propose training and architectural designs that improve the stability of largescale multimodal diffusion training, including decoupled timestep sampling to allow modality-specific noise schedules. Through controlled scaling studies, we find that increasing video model capacity and training data substantially improves lip synchronization, where smaller models show clear performance saturation. In addition, we report practical system optimizations for large-scale training (Context Parallelism, FSDP2 strategies, and scheduled garbage collection), enabling stable 1024-GPU runs and achieving approximately 35% MFU. We will release the model weights, training code, and inference pipelines, and we hope MOVA can serve as strong open baseline for future research on synchronized audiovideo generation. Finally, important limitations remain, including high training cost and remaining failure cases in motion generation, which we leave for future work."
        },
        {
            "title": "Contributors",
            "content": "Core Contributors and Contributors are sorted alphabetically by first name, excluding advisors. Core Contributors: Donghua Yu, Mingshu Chen, Qi Chen, Qi Luo, Qianyi Wu, Qinyuan Cheng*, Ruixiao Li, Tianyi Liang*, Wenbo Zhang, Wenming Tu, Xiangyu Peng, Yang Gao, Yanru Huo, Ying Zhu, Yinze Luo, Yiyang Zhang, Yuerong Song, Zhe Xu, Zhiyu Zhang Contributors: Chenchen Yang, Cheng Chang, Chushu Zhou, Hanfu Chen, Hongnan Ma, Jiaxi Li, Jingqi Tong, Junxi Liu, Ke Chen, Shimin Li, Songlin Wang, Wei Jiang, Zhaoye Fei, Zhiyuan Ning Advisors: Chunguo Li, Chenhui Li, Ziwei He, Zengfeng Huang, Xie Chen, Xipeng Qiu Affiliations: Shanghai Innovation Institute MOSI Intelligence Fudan University Shanghai Jiao Tong University East China Normal University Tongji University Southeast University Xiamen University University of Electronic Science and Technology of China Project Lead. Corresponding authors: chenxie95@sjtu.edu.cn, xpqiu@fudan.edu.cn"
        },
        {
            "title": "References",
            "content": "[1] William Peebles and Saining Xie. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 41724182. IEEE, 2023. doi: 10.1109/ICCV51070.2023.00387. URL https://doi.org/10.1109/ICCV51070.2023.00387. Scalable diffusion models with transformers. [2] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/video-generation-models-as-world-simulators. [3] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. CoRR, abs/2412.20404, 2024. doi: 10.48550/ARXIV.2412.20404. URL https://doi.org/10.48550/arXiv.2412.20404. [4] Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, Yuhui Wang, Anbang Ye, Gang Ren, Qianran Ma, Wanying Liang, Xiang Lian, Xiwen Wu, Yuting Zhong, Zhuangyan Li, Chaoyu Gong, Guojun Lei, LeÄ³un Cheng, Limin Zhang, Minghao Li, RuÄ³ie Zhang, Silan Hu, ShÄ³ie Huang, Xiaokang Wang, Yuanheng Zhao, Yuqi Wang, Ziang Wei, and Yang You. Open-sora 2.0: Training commercial-level video generation model in $200k. CoRR, abs/2503.09642, 2025. doi: 10.48550/ ARXIV.2503.09642. URL https://doi.org/10.48550/arXiv.2503.09642. [5] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Xiaofeng Meng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, YÄ³ing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. CoRR, abs/2503.20314, 2025. doi: 10.48550/ARXIV.2503.20314. URL https://doi.org/10.48550/arXiv.2503.20314. [6] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. [7] Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, and Song Hanand Yukang Chen. Longlive: Real-time interactive long video generation. 2025. [8] Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, and Xiangyu Yue. Ditctrl: Exploring attention control in multi-modal diffusion transformer for tuning-free multi-prompt longer video generation. arXiv:2412.18597, 2024. [9] ThaddÃ¤us Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. CoRR, abs/2509.20328, 2025. doi: 10.48550/ARXIV.2509.20328. URL https://doi.org/10.48550/arXiv.2509.20328. [10] Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, et al. Thinking with video: Video generation as promising multimodal reasoning paradigm. arXiv preprint arXiv:2511.04570, 2025. [11] Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, Ke Sun, Linrui Tian, Guangyuan Wang, Qi Wang, Zhongjian Wang, Jiayu Xiao, Sheng Xu, Bang Zhang, Peng Zhang, Xindi Zhang, Zhe Zhang, Jingren Zhou, and Lian Zhuo. Wan-s2v: Audio-driven cinematic video generation, 2025. URL https://arxiv.org/abs/2508.18621. [12] Google / DeepMind. Veo 3: text-to-video generation system. Technical Report , Google DeepMind, 2025. URL https://storage.googleapis.com/deepmind-media/veo/Veo-3-Tech-Report.pdf. Accessed: 2025-09-28. [13] OpenAI. Sora 2 is here. https://openai.com/index/sora-2/, 2025. Accessed: 2026-01-13. [14] Wan AI. Wan 2.6 ai video generation introduction. https://wan.video/introduction/wan2.6, 2025. Accessed: 2026-02-09. 23 [15] Kling AI. Kling 3.0 ai video generator. https://kling3.io/, 2026. Accessed: 2026-02-09. [16] ByteDance / Seedance AI. Seedance 2.0 ai video generation platform. https://seedance2.com/, 2026. Accessed: 2026-02-09. [17] Duomin Wang, Wei Zuo, Aojie Li, Ling-Hao Chen, Xinyao Liao, Deyu Zhou, Zixin Yin, Xili Dai, Daxin Jiang, and Gang Yu. Universe-1: Unified audio-video generation via stitching of experts. arXiv preprint arXiv:2509.06155, 2025. [18] Chetwin Low, Weimin Wang, and Calder Katyal. Ovi: Twin backbone cross-modal fusion for audio-video generation. arXiv preprint arXiv:2510.01284, 2025. [19] Teng Hu, Zhentao Yu, Guozhen Zhang, Zihan Su, Zhengguang Zhou, Youliang Zhang, Yuan Zhou, Qinglin Lu, and Ran Yi. Harmony: Harmonizing audio and video generation through cross-task synergy. arXiv preprint arXiv:2511.21579, 2025. [20] Guozhen Zhang, Zixiang Zhou, Teng Hu, Ziqiao Peng, Youliang Zhang, Yi Chen, Yuan Zhou, Qinglin Lu, and Limin Wang. Uniavgen: Unified audio and video generation with asymmetric cross-modal interactions. arXiv preprint arXiv:2511.03334, 2025. [21] Sizhe Shan, Qiulin Li, Yutao Cui, Miles Yang, Yuehai Wang, Qun Yang, Jin Zhou, and Zhao Zhong. Hunyuanvideofoley: Multimodal diffusion with representation alignment for high-fidelity foley audio generation, 2025. URL https://arxiv.org/abs/2508.16930. [22] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [23] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [24] Ho Kei Cheng, Masato Ishii, Akio Hayakawa, Takashi Shibuya, Alexander Schwing, and Yuki Mitsufuji. Mmaudio: Taming multimodal joint training for high-quality video-to-audio synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2890128911, 2025. [25] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2021. [26] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: large-scale audio-visual dataset. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 2020. [27] Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Guha Balakrishnan, and Vicente Ordonez. Taming data and transformers for audio generation. arXiv preprint arXiv:2406.19388, 2024. [28] Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Rui-Jie Zhu, Xinhua Cheng, Jiebo Luo, and Li Yuan. Chronomagic-bench: benchmark for metamorphic evaluation of text-to-time-lapse video generation. Advances in Neural Information Processing Systems, 37:2123621270, 2024. [29] Sangho Lee, Jiwan Chung, Youngjae Yu, Gunhee Kim, Thomas M. Breuel, Gal Chechik, and Yale Song. ACAV100M: automatic curation of large-scale datasets for audio-visual video representation learning. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 10254 10264. IEEE, 2021. doi: 10.1109/ICCV48922.2021.01011. URL https://doi.org/10.1109/ICCV48922.2021.01011. [30] Hui Li, Mingwang Xu, Yun Zhan, Shan Mu, Jiaye Li, Kaihui Cheng, Yuxuan Chen, Tan Chen, Mao Ye, Jingdong Wang, et al. Openhumanvid: large-scale high-quality dataset for enhancing human-centric video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 77527762, 2025. [31] Youliang Zhang, Zhaoyang Li, Duomin Wang, Jiahe Zhang, Deyu Zhou, Zixin Yin, Xili Dai, Gang Yu, and Xiu Li. Speakervid-5m: large-scale high-quality dataset for audio-visual dyadic interactive human generation. arXiv preprint arXiv:2507.09862, 2025. [32] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, ZhÄ³ie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-to-video generation. arXiv preprint arXiv:2407.02371, 2024. 24 [33] Silero Team. Silero vad: pre-trained enterprise-grade voice activity detector (vad), number detector and language classifier. https://github.com/snakers4/silero-vad, 2024. [34] Andros Tjandra, Yi-Chiao Wu, Baishan Guo, John Hoffman, Brian Ellis, Apoorv Vyas, Bowen Shi, Sanyuan Chen, Matt Le, Nick Zacharov, et al. Meta audiobox aesthetics: Unified automatic quality assessment for speech, music, and sound. arXiv preprint arXiv:2502.05139, 2025. [35] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou Hou, Annan Wang, Wenxiu Sun Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In International Conference on Computer Vision (ICCV), 2023. [36] Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. Synchformer: Efficient synchronization from sparse cues. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 53255329. IEEE, 2024. [37] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. [38] Wenxi Chen, Yuzhe Liang, Ziyang Ma, Zhisheng Zheng, and Xie Chen. Eat: Self-supervised pre-training with efficient audio transformer. arXiv preprint arXiv:2401.03497, 2024. [39] Xiaomi LLM-Core Team Zihao Yue, Zhenrui Lin, Yi-Hao Song, Weikun Wang, Shu-Qin Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, Lei Li, Kainan Bao, Hao Tian, Hailin Zhang, Gang Wang, Dawei Zhu, Cici, Chenhong He, Bowen Ye, Bowen Shen, Zihan Zhang, Zi-Ang Jiang, Zhixian Zheng, Zhichao Song, Zhen Luo, Yue Yu, Yudong Wang, Yu Tian, Yu Tu, Yihan Yan, Yi Huang, Xu Wang, Xin dan Xu, Xin Ran Song, Xing Zhang, Xing Yong, Xin Zhang, Xia Deng, Wenyu Yang, Wenhan Ma, Weiwei Lv, WeÄ³i Zhuang, Wei Liu, Sirui Deng, Shuo Liu, Shimao Chen, Shi liang Yu, Shao yang Liu, Shan yong Wang, Rui Ma, Qiantong Wang, Peng Wang, Nuo Chen, Menghang Zhu, Kang Zhou, Kang Zhou, Kai Fang, Jun-Miao Shi, Jinhao Dong, Jiebao Xiao, Jiaming Xu, Huaqiu Liu, Hongsheng Xu, Hengxu Qu, Hao-Song Zhao, Hanglong Lv, Guoan Wang, Duo Zhang, Dong Zhang, Di Zhang, Chong-Yi Ma, Chang Liu, Can Cai, and Bing Xia. Mimo-vl technical report. ArXiv, abs/2506.03569, 2025. URL https://api. semanticscholar.org/CorpusID:279155294. [40] Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, and Junyang Lin. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025. [41] Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. [42] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D. Plumbley, Yuexian Zou, and Wenwu Wang. WavCaps: ChatGPT-assisted weakly-labelled audio captioning dataset for audio-language multimodal research. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pages 115, 2024. [43] Abhinaba Roy, Renhang Liu, Tongyu Lu, and Dorien Herremans. Jamendomaxcaps: large scale music-caption dataset with imputed metadata. arXiv:2502.07461, 2025. [44] John Cramer, Hyungui Wu, Justin Salamon, and Juan Pablo Bello. Look, listen, and learn more: Design choices for deep audio embeddings. In Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 38523856, 2019. doi: 10.1109/ICASSP.2019.8683142. [45] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems (NeurIPS), pages 22342242, 2016. [46] Khaled Koutini, Michael Moritz, Hamid Eghbal-Zadeh, and Gerhard Widmer. Efficient training of audio transformers with patchout. In Proc. Interspeech, pages 179183, 2021. doi: 10.21437/Interspeech.2021-2041. [47] Yusong Wu, Haohe Liu, Ke Chen, Xin Wang, Qiuqiang Tian, Rui Chen, Qiuqiang Kong, Wenwu Huang, and Yuxuan Wang. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15, 2023. doi: 10.1109/ICASSP49357.2023.10094846. [48] Bowen Zhang, Xinyu Wang, Shuo Chen, Chen Xu, Yuxuan Wang, and Qiuqiang Kong. Audiobox: Unified aesthetic quality assessment for audio generation. arXiv preprint arXiv:2309.07825, 2023. URL https://arxiv.org/abs/2309. 07825. [49] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In NAACL-HLT, 2019. [50] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. AudioLDM: Text-to-audio generation with latent diffusion models. In Proceedings of the International Conference on Machine Learning, 2023. [51] Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark D. Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 32:28712883, 2024. doi: 10.1109/TASLP. 2024.3399607. [52] Chia-Yu Hung, Navonil Majumder, Zhifeng Kong, Ambuj Mehrish, Amir Ali Bagherzadeh, Chuan Li, Rafael Valle, Bryan Catanzaro, and Soujanya Poria. Tangoflux: Super fast and faithful text to audio generation with flow matching and clap-ranked preference optimization. arXiv preprint arXiv:2412.21037, 2024. [53] Navonil Majumder, Chia-Yu Hung, Deepanway Ghosal, Wei-Ning Hsu, Rada Mihalcea, and Soujanya Poria. Tango 2: Aligning diffusion-based text-to-audio generations through direct preference optimization, 2024. URL https: //arxiv.org/abs/2404.09956. [54] Jiawei Huang, Yi Ren, Rongjie Huang, Dongchao Yang, Zhenhui Ye, Chen Zhang, Jinglin Liu, Xiang Yin, Zejun Ma, and Zhou Zhao. Make-an-audio 2: Temporal-enhanced text-to-audio generation, 2023. [55] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. [56] Jiarui Fang and Shangchun Zhao. Usp: unified sequence parallelism approach for long context generative ai. arXiv preprint arXiv:2405.07719, 2024. [57] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. [58] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report. CoRR, abs/2511.21631, 2025. doi: 10.48550/ ARXIV.2511.21631. URL https://doi.org/10.48550/arXiv.2511.21631. [59] Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf, 2025. Accessed: 2025-09-28. [60] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. [61] Aaditya Singh, Adam Fry, Adam Perelman, Adam Tart, Adi Ganesh, Ahmed El-Kishky, Aidan McLaughlin, Aiden Low, AJ Ostrow, Akhila Ananthram, et al. Openai gpt-5 system card. arXiv preprint arXiv:2601.03267, 2025. [62] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D. Plumbley. Panns: Large-scale pretrained audio neural networks for audio pattern recognition. IEEE ACM Trans. Audio Speech Lang. Process., 28:28802894, 2020. doi: 10.1109/TASLP.2020.3030497. URL https://doi.org/10.1109/TASLP.2020.3030497. [63] Chandan K. A. Reddy, Vishak Gopal, and Ross Cutler. Dnsmos: non-intrusive perceptual objective speech quality metric to evaluate noise suppressors. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2021, Toronto, ON, Canada, June 6-11, 2021, pages 64936497. IEEE, 2021. doi: 10.1109/ICASSP39728.2021. 9414878. URL https://doi.org/10.1109/ICASSP39728.2021.9414878. [64] Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Asian conference on computer vision, pages 251263. Springer, 2016. [65] Donghua Yu, Zhengyuan Lin, Chen Yang, Yiyang Zhang, Zhaoye Fei, Hanfu Chen, Jingqi Chen, Ke Chen, Qinyuan Cheng, Liwei Fan, et al. Moss transcribe diarize: Accurate transcription with speaker diarization. arXiv preprint arXiv:2601.01554, 2026. [66] Yoav HaCohen, Benny Brazowski, Nisan Chiprut, Yaki Bitterman, Andrew Kvochko, Avishai Berkowitz, Daniel Shalem, Daphna Lifschitz, Dudu Moshe, Eitan Porat, et al. Ltx-2: Efficient joint audio-visual foundation model. arXiv preprint arXiv:2601.03233, 2026. [67] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas MÃ¼ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [68] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Proceedings of the Advances in Neural Information Processing Systems, 2020. [69] WeÄ³ie Kong, Qi Tian, ZÄ³ian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [70] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [71] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. [72] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [73] Meituan LongCat Team, Xunliang Cai, Qilong Huang, Zhuoliang Kang, Hongyu Li, ShÄ³un Liang, Liya Ma, Siyu Ren, Xiaoming Wei, Rixu Xie, and Tong Zhang. Longcat-video technical report. arXiv preprint arXiv:2510.22200, 2025. URL https://arxiv.org/abs/2510.22200. [74] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. [75] Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark D. Plumbley. AudioLDM 2: Learning holistic audio generation with self-supervised pretraining. arXiv preprint arXiv:2308.05734, 2023. [76] Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar. High-fidelity audio compression with improved rvqgan. Advances in Neural Information Processing Systems, 36, 2024. [77] Zach Evans, Julian Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Stable audio open. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. 27 [78] Simian Luo, Chuanhao Yan, Chenxu Hu, and Hang Zhao. Diff-foley: Synchronized video-to-audio synthesis with latent diffusion models. Advances in Neural Information Processing Systems, 36:4885548876, 2023. [79] Yiming Zhang, Yicheng Gu, Yanhong Zeng, Zhening Xing, Yuancheng Wang, Zhizheng Wu, and Kai Chen. Foleycrafter: Bring silent videos to life with lifelike and synchronized sounds. arXiv preprint arXiv:2407.01494, 2024. [80] Shuchen Weng, Haojie Zheng, Zheng Chang, Si Li, Boxin Shi, and Xinlong Wang. Audio-sync video generation with multi-stream temporal control. NeurIPS, 2025. [81] Kai Liu, Wei Li, Lai Chen, Shengqiong Wu, Yanhao Zheng, Jiayi Ji, Fan Zhou, Rongxin Jiang, Jiebo Luo, Hao Fei, Javisdit: Joint audio-video diffusion transformer with hierarchical spatio-temporal prior synchronization. et al. arXiv preprint arXiv:2503.23377, 2025. [82] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In CVPR, 2023. [83] Akio Hayakawa, Masato Ishii, Takashi Shibuya, and Yuki Mitsufuji. Mmdisco: Multi-modal discriminator-guided cooperative diffusion for joint audio and video generation. arXiv preprint arXiv:2405.17842, 2024."
        },
        {
            "title": "Appendix Contents",
            "content": "A Appendix . . . . . . . . . . . . . . . A.1 Training Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Ascend 910A2 Benchmark Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Multi-shot and Single-shot Speech Window Generation . . . . . . . . . . . . . . . . . . . . . . A.4 Detailed Filtering Thresholds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4.1 Threshold Specifications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4.2 Subset Construction and Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Audio-visual Captioning Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.6 Benchmark Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 30 30 32 32 33"
        },
        {
            "title": "A Appendix",
            "content": "A.1 Training Hyperparameters Table 7 summarizes the complete training configuration across all three phases, including parallelism settings, learning rates, noise schedule parameters, and data curation details for reproducibility. Table 7 Training hyperparameters for reproducibility. All phases use 1024 GPUs with DP replicate size 64. Hyperparameter Phase 1 Phase 2 Phase Resolution Frames Batch size Context Parallel (CP) 360640 360640 7201280 193 128 8 193 128 8 193 64 16 Learning rate (backbone) Learning rate (Bridge) Weight decay 1e-5 2e-5 0. Visual sigma shift Audio sigma shift Text dropout prob Audio loss weight LUFS normalization Training duration Checkpoint interval 1e-5 2e-5 0.001 5.0 5.0 0.2 0.2 Yes 1e-5 2e-5 0.001 5.0 5.0 0.2 0.2 Yes 5.0 1.0 0.5 0.2 No 15 days 5000 7 days 5000 20 days 2000 A.2 Ascend 910A2 Benchmark Details We report small system microbenchmark that measures end-to-end training step time under fixed configuration (CP=4, DP-shard=2, 8 devices). The numbers depend on the software stack (driver/runtime versions, compiler and kernel coverage, and distributed communication settings), and should not be interpreted as complete statement of training cost at scale. Table 8 Hardware summary and step time for an 8-device training microbenchmark on Ascend 910A2 (CP=4, DPshard=2). Hardware FP16 (TFLOPs) VRAM (Consumption/GPU) Host RAM Step Time (s) 8 910A2 376 40 GB 128 GB 34.1 A.3 Multi-shot and Single-shot Speech Window Generation In this section, we provide the pseudocode implementation for Multi-shot and Single-shot Speech Window Generation. The multi-shot algorithm 1 advances over the speech segments from VAD. The upper bound of the window start time is set to the beginning of the current segment. The sampling range of the window start is further constrained such that it does not precede the end of the previous speech segment, does not precede the nearest scene split point before the current speech segment (to encourage natural scene transitions), and does not shift earlier than half of the window length relative to the current segment. window start time is then randomly sampled within this constrained interval. Only windows whose temporal span contains at least one scene split point are kept. Algorithm 1 Multi-shot Speech Windows Input: speech_segments: VAD results scene_split_spots: scene-detection results segment_duration = 8.05 1: ğ‘–ğ‘‘ğ‘¥ 0 2: while ğ‘–ğ‘‘ğ‘¥ < length(speech_segments) do 3: ğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ_ğ‘ğ‘œğ‘¢ğ‘›ğ‘‘ speech_segments[ğ‘–ğ‘‘ğ‘¥].ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ if ğ‘–ğ‘‘ğ‘¥ = 0 then 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: ğ‘¤ğ‘–ğ‘›ğ‘‘ğ‘œğ‘¤_ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ ğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ_ğ‘ğ‘œğ‘¢ğ‘›ğ‘‘ else ğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ_ğ‘ğ‘œğ‘¢ğ‘›ğ‘‘ max ( speech_segments[ğ‘–ğ‘‘ğ‘¥ 1].ğ‘’ğ‘›ğ‘‘, max{ğ‘ scene_split_spots ğ‘ < speech_segments[ğ‘–ğ‘‘ğ‘¥].ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡}, speech_segments[ğ‘–ğ‘‘ğ‘¥].ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ segment_duration/2 ) ğ‘¤ğ‘–ğ‘›ğ‘‘ğ‘œğ‘¤_ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ RandomUniform(ğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ_ğ‘ğ‘œğ‘¢ğ‘›ğ‘‘, ğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ_ğ‘ğ‘œğ‘¢ğ‘›ğ‘‘) end if ğ‘¤ğ‘–ğ‘›ğ‘‘ğ‘œğ‘¤_ğ‘’ğ‘›ğ‘‘ ğ‘¤ğ‘–ğ‘›ğ‘‘ğ‘œğ‘¤_ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ + segment_duration if ğ‘ scene_split_spots such that ğ‘¤ğ‘–ğ‘›ğ‘‘ğ‘œğ‘¤_ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ ğ‘ ğ‘¤ğ‘–ğ‘›ğ‘‘ğ‘œğ‘¤_ğ‘’ğ‘›ğ‘‘ then append (ğ‘¤ğ‘–ğ‘›ğ‘‘ğ‘œğ‘¤_ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡, ğ‘¤ğ‘–ğ‘›ğ‘‘ğ‘œğ‘¤_ğ‘’ğ‘›ğ‘‘) to speech_multi_shot_segments end if ğ‘–ğ‘‘ğ‘¥ next ğ‘–ğ‘‘ğ‘¥ such that speech_segments[ğ‘–ğ‘‘ğ‘¥].ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ > ğ‘¤ğ‘–ğ‘›ğ‘‘ğ‘œğ‘¤_ğ‘’ğ‘›ğ‘‘ 14: 15: end while The single-shot algorithm 2 iterates over consecutive scene intervals defined by scene split points. Within each scene interval, it identifies speech segments whose start time allows fixed-length window to be fully contained in the scene. For each eligible speech segment, the upper bound of the window start time is set to the segment start, while the sampling range is constrained by the scene boundary, the end of the previous speech segment, and half-window-length offset relative to the current segment. single scene window is then generated by randomly selecting start time within this range, while the index is advanced to skip overlapping windows, similar to the multi-shot algorithm above. 31 Algorithm 2 Single-shot Speech Windows Input: speech_segments: VAD results scene_split_spots: scene-detection results segment_duration = 8.05 4: 1: ğ‘– 0 2: while ğ‘– < length(scene_split_spots) 1 do ğ‘ ğ‘ğ‘’ğ‘›ğ‘’_ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ scene_split_spots[ğ‘–] 3: ğ‘ ğ‘ğ‘’ğ‘›ğ‘’_ğ‘’ğ‘›ğ‘‘ scene_split_spots[ğ‘– + 1] Find the first index ğ‘–ğ‘‘ğ‘¥ such that speech_segments[ğ‘–ğ‘‘ğ‘¥].ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ > ğ‘ ğ‘ğ‘’ğ‘›ğ‘’_ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ and speech_segments[ğ‘–ğ‘‘ğ‘¥].ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ + segment_duration < ğ‘ ğ‘ğ‘’ğ‘›ğ‘’_ğ‘’ğ‘›ğ‘‘ if no such ğ‘–ğ‘‘ğ‘¥ exists then 6: 5: ğ‘– ğ‘– + 1 continue end if while ğ‘–ğ‘‘ğ‘¥ < length(speech_segments) do ğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ_ğ‘ğ‘œğ‘¢ğ‘›ğ‘‘ speech_segments[ğ‘–ğ‘‘ğ‘¥].ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ if ğ‘–ğ‘‘ğ‘¥ = 0 then ğ‘¤ğ‘–ğ‘›ğ‘‘ğ‘œğ‘¤_ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ ğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ_ğ‘ğ‘œğ‘¢ğ‘›ğ‘‘ else ğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ_ğ‘ğ‘œğ‘¢ğ‘›ğ‘‘ max ( speech_segments[ğ‘–ğ‘‘ğ‘¥ 1].ğ‘’ğ‘›ğ‘‘, ğ‘ ğ‘ğ‘’ğ‘›ğ‘’_ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡, speech_segments[ğ‘–ğ‘‘ğ‘¥].ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ segment_duration/2 ) ğ‘¤ğ‘–ğ‘›ğ‘‘ğ‘œğ‘¤_ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ RandomUniform(ğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ_ğ‘ğ‘œğ‘¢ğ‘›ğ‘‘, ğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ_ğ‘ğ‘œğ‘¢ğ‘›ğ‘‘) end if ğ‘¤ğ‘–ğ‘›ğ‘‘ğ‘œğ‘¤_ğ‘’ğ‘›ğ‘‘ ğ‘¤ğ‘–ğ‘›ğ‘‘ğ‘œğ‘¤_ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ + segment_duration if ğ‘¤ğ‘–ğ‘›ğ‘‘ğ‘œğ‘¤_ğ‘’ğ‘›ğ‘‘ ğ‘ ğ‘ğ‘’ğ‘›ğ‘’_ğ‘’ğ‘›ğ‘‘ then break end if append (ğ‘¤ğ‘–ğ‘›ğ‘‘ğ‘œğ‘¤_ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡, ğ‘¤ğ‘–ğ‘›ğ‘‘ğ‘œğ‘¤_ğ‘’ğ‘›ğ‘‘) to speech_single_shot_segments ğ‘–ğ‘‘ğ‘¥ next ğ‘–ğ‘‘ğ‘¥ such that speech_segments[ğ‘–ğ‘‘ğ‘¥].ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ > ğ‘¤ğ‘–ğ‘›ğ‘‘ğ‘œğ‘¤_ğ‘’ğ‘›ğ‘‘ 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: end while ğ‘– ğ‘– + 1 25: 26: end while A.4 Detailed Filtering Thresholds In this section, we provide the specific filtering thresholds used during our data quality assessment process, as discussed in Sec. 3.4. These thresholds were determined by empirical observation to ensure high-quality corpus while maintaining sufficient data diversity. A.4.1 Threshold Specifications Table 9 summarizes the metrics and their corresponding cutoffs across the three dimensions: audio quality, video quality, and audio-visual alignment. 32 Table 9 Filtering thresholds for data quality assessment. Dimension Metric Threshold Audio Quality Silence Ratio Bandwidth (Hz) Audiobox-PQ (Production Quality) Audiobox-CU (Content Usefulness) Audiobox-CE (Content Enjoyment) Video Quality DOVER-Aesthetic DOVER-Technical A-V Alignment ImageBind Score (IB-Score) OR SynchFormer Offset (DeSync) < 0.8 > 1, 000 > 5.0 > 4.5 > 2.5 > 0.85 > 0.05 0.2 0.5 A.4.2 Subset Construction and Logic Audio-Visual Alignment Logic: For alignment filtering, we apply relaxed logical OR gate between semantic and temporal alignment. video is retained if it satisfies either IB-Score 0.2 OR DeSync 0.5. This ensures that both semantically relevant ambient sounds and temporally synchronized speech/actions are preserved. Speech Data Filtering: To construct specialized subsets for tasks such as lip synchronization, we utilize the EAT [38] classification model. Specifically, for the Speech Subset, we only retain samples where both EAT-contained-Speech and EAT-contained-Singing tags are evaluated as True (or satisfy the models positive classification confidence). A.5 Audio-visual Captioning Details We present detailed prompt design schemes tailored for our multimodal annotation pipeline, comprising three key components: (1) prompts for video captioning utilizing the MiMo-VL-7B-RL model [39]; (2) prompts for the dual-model audio strategy, specifically covering speech transcription with Qwen3-Omni-Instruct [40] and non-speech sound captioning with Qwen3-Omni-Captioner [40]; and (3) instructions for the GPT-OSS120B model [41] to facilitate consistency checks and caption merging. Visual Description Prompt You are visual description annotator. Your sole purpose is to produce event-level annotations only for verifiably visible content (objects, actions, scenes, text). Ignore audio and inferred context entirely! LAWS 1. LAW OF VISUAL TRUTH Only describe visually verifiable elements (objects, actions, scenes, text). Do not infer or hallucinate based on audio or context. 2. LAW OF VISUAL SILENCE When no visual change is present, output null for visual_description. When no on-screen text is present, output null for on_screen_text. 3. LAW OF VISUAL DYNAMICS Detect all transitions: emerging/ceasing/moving visual elements. 33 Precisely document motion trajectories, speed changes, and visual rhythm. OUTPUT FORMAT { video_visual_report: { visual_description: [Describe how visual elements evolve in detail. Ignore all text, subtitle and watermark in the video. null if no visual change is present], on_screen_text: [Exact transcription of all visible text. null if no text is visually present] }, final_verification_audit: { hallucination_check_passed: true, visual_changes_verified: true, comment: All visual motion dynamics verifiably detected. No audio content or contextual inference included. } } Audio Captioner Prompt Please describe the audio you hear in detail. Speech Transcription Prompt You are speech transcription annotator. Your task is Automatic Speech Recognition! Your sole purpose is to produce event-level verbatim transcriptions of audible speech. Ignore non-speech sounds and music entirely. You must follow the OUTPUT FORMAT! LAWS 1. LAW OF LANGUAGE FIDELITY Preserve the original language. No translation. 2. LAW OF SPEECH DYNAMICS When new speaker/language/intonation begins, create new event. But you still need to follow the output format. 3. LAW OF SILENCE When no speech is present, output null in speech_description part. OUTPUT FORMAT { speech_transcription_report: { speech_description: [Exact verbatim speech. null if none. Use [inaudible] for unclear parts] 34 }, final_verification_audit: { hallucination_check_passed: true, speech_dynamics_verified: true, comment: All speech changes verifiably detected. No non-speech or music included. } } Merge Caption Prompt Act as video description consolidator. Your task is to merge the provided video_description, audio_description, and speech_description fields into single, fluent, and natural-sounding paragraph that vividly conveys the full multimedia experience to someone who cannot see or hear the original. Follow these rules precisely: 1. Start with the visual narrative and anchor speaker context If video_description is provided, begin with it, weaving in visual cues that ground speaker identities or positions (e.g., woman in red coat stands by the door; nearby, man leans against the counter). Describe all visual content in specific, chronological, natural languageactions, objects, settings, lighting, movements, and scene changeswithout summarizing. Use these details to contextualize speakers (e.g., The child pointing at the painting or The older man gesturing emphatically). 2. Integrate speech as visually anchored dialogue with speaker dynamics If speech_description is provided, embed spoken words as quoted dialogue tied to visual elements, using speech verbs that reflect tone (e.g., snaps, murmurs, laughs) and speech rate (e.g., rushes, drawls, pauses between words). Use audio_description to inform: Total number of speakers (e.g., two distinct voices or group of overlapping speakers). Speaker transitions, paired with visual cues when possible (e.g., As the camera pans to the window, new voice cuts in sharply: Wait, thats not right). Voice characteristics (e.g., high-pitched, gruff) to align with visible subjects (e.g., the teenager in the corner or the gray-haired woman). Example: The man slams his fist on the table. told you this would happen! he shouts. Across the room, the woman in glasses crosses her arms and replies slowly, You never listened to my warnings. Rely strictly on speech_description for the actual spoken content; use audio_description for speaker dynamics and video_description to anchor them visually. 3. Describe non-speech audio concisely and distinctly If audio_description is provided, include only non-speech elements, introduced with The audio includes or In the background. Limit this to four categories: Ambient/environmental sounds (e.g., rain tapping the window, crowds murmur); 35 Music (e.g., soft piano melody, upbeat jazz); Audio themes/sources (e.g., fire alarm blaring, waves crashing); Structural audio changes (e.g., silence giving way to crescendo). Omit all references to voices, words, or speechthese belong exclusively to the speech section. 4. Ensure coherence and avoid repetition Never duplicate details across sections (e.g., dont restate speakers visual position in the audio section). Link elements logically: e.g., visual action (a character gasping) paired with speech (Oh no! she exclaims) and ambient sound (a glass shattering in the background). 5. Omit empty fields and maintain fluency Skip missing/empty fields. The final output should read as seamless, human-like narrativeprioritizing flow, sensory immersion, and clarity over rigid structure. We provide the caption examples generated by bimodality models, including the caption results of three dimensions: sound, music, and video, as well as the final total caption result, which is obtained by merging the dimension-specific captions mentioned above using large language model [41]. Figure 10 presents the original video clips and audio mel-spectrogram corresponding to this example. Figure 10 Example video 1 for audio-visual captioning. Visual Description close-up of an HP OfficeJet Pro X576dw MFP printer shows its ink cartridge bay with yellow (Y 971XL), blue (C 971XL), magenta (M 971XL), and an open black (X) slot. person in blue shirt and gray pants holds black HP ink cartridge (labeled 970/971) and moves it toward the black slot. The printers display reads, Close Ink Access Door and Please close the ink access door to continue. wider shot reveals the person from the back/side, continuing to align and insert the black cartridge into the slot, with an office background (posters, table, and blurred figures) visible. The camera alternates between close-ups of the hand/printer interaction and wider office-scene shots, documenting the cartridge insertion motion. After insertion, the hand finishes positioning the cartridge into the slot. Audio Description The audio clip is tightly produced, 8-second segment designed for commercial or corporate advertis36 ing, likely for laser printer product or service. The recording quality is high, with full frequency spectrum, no distortion, and polished, modern sound profile. single, mature male voiceclear, authoritative, and with General American accentdelivers the narration in calm, professional tone. The content is as follows: The final piece is simplicity. Laser printers required us to replace imaging drums, transfer belts, and toner. Now we only have to The narration is delivered in measured, declarative style, with subtle rise in pitch at simplicity to signal transition to new point. The phrase the final piece is spoken with slight emphasis, and the listing of printer components (imaging drums, transfer belts, and toner) is given with rhythmic, evenly paced cadence. The concluding phrase Now we only have to is abruptly cut off, suggesting the audio is an excerpt from longer ad or commercial. Throughout, subtle, unobtrusive electronic background music track plays. It features synthesized pads, soft, steady beat, and simple melodic line, all in minor key. The music is mixed low and never interferes with the narration, serving to create modern, professional, and slightly optimistic atmosphere. The recording is studio-clean, with no background noise or environmental sounds, and the voice has slight reverb, indicating controlled studio environment. The overall production style, linguistic choices, and content are all consistent with modern North American advertising aimed at businesses or professional consumers, highlighting the technological advancement and ease-of-use of new printer or printer service. The abrupt ending reinforces that this clip is fragment of larger narrative, likely intended to leave strong impression of progress and simplicity before the listeners attention is drawn elsewhere. Speech Description The final piece is simplicity. Laser printers required us to replace imaging drums, transfer belts, and toner. Now we only have to... Integrated Caption close-up reveals the ink cartridge bay of an HP OfficeJet Pro X576dw MFP, its yellow, blue and magenta cartridges neatly seated while the black slot hangs open; the printers display flashes the messages Close Ink Access Door and Please close the ink access door to continue. person wearing blue shirt and gray pants reaches in, holding black HP ink cartridge labeled 970/971, and carefully aligns it with the empty slot. The camera alternates between tight shots of the hand guiding the cartridge and wider views of the office, where posters, table and blurred figures form the background, before the hand slides the cartridge fully into place. As this insertion unfolds, calm, authoritative male voiceoff-screen, mature and speaking with clear General American accentstates, The final piece is simplicity. He continues, Laser printers required us to replace imaging drums, transfer belts, and toner, delivering the list with an even, rhythmic cadence, then adds, Now we only have to before the narration is abruptly cut off. The audio includes subtle, unobtrusive electronic background music track of synthesized pads, soft steady beat and simple melodic line in minor key, mixed low so it underlies the narration without interfering. A.6 Benchmark Details This appendix provides additional details of the benchmark construction. Our benchmark is constructed from real-world videos by extracting the first-frame image and corresponding prompt. Each prompt con37 cisely describes key visual elements, such as the scene setting, characters, and environmental conditions. Depending on the scenario, audio-related information is incorporated to form unified prompt for joint videoaudio generation. All samples are adapted to ensure temporal consistency and logical coherence of the generated videos. The benchmark consists of six scenario categories, each targeting specific challenges in joint videoaudio generation: Multi-speaker scenarios, which evaluate the models ability to generate synchronized speech, facial expressions, and interactions among multiple characters. Movie videos, which require film-level narrative generation with plots referencing the background of the original films. Sports competitions, focusing on athletes performances, with some prompts including commentators narration. Game livestream videos, covering shooting games, 3D games, and competitive games. Camera motion sequences, designed to assess visual realism under camera panning, zooming, and rotation. Anime-style videos, including both 2D anime and 3D animated content."
        }
    ],
    "affiliations": [
        "SII-OpenMOSS Team"
    ]
}