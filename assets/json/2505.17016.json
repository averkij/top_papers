{
    "paper_title": "Interactive Post-Training for Vision-Language-Action Models",
    "authors": [
        "Shuhan Tan",
        "Kairan Dou",
        "Yue Zhao",
        "Philipp Krähenbühl"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based interactive post-training paradigm that fine-tunes pretrained Vision-Language-Action (VLA) models using only sparse binary success rewards. Existing VLA training pipelines rely heavily on offline expert demonstration data and supervised imitation, limiting their ability to adapt to new tasks and environments under low-data regimes. RIPT-VLA addresses this by enabling interactive post-training with a stable policy optimization algorithm based on dynamic rollout sampling and leave-one-out advantage estimation. RIPT-VLA has the following characteristics. First, it applies to various VLA models, resulting in an improvement on the lightweight QueST model by 21.2%, and the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it is computationally efficient and data-efficient: with only one demonstration, RIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success rate within 15 iterations. Furthermore, we demonstrate that the policy learned by RIPT-VLA generalizes across different tasks and scenarios and is robust to the initial state context. These results highlight RIPT-VLA as a practical and effective paradigm for post-training VLA models through minimal supervision."
        },
        {
            "title": "Start",
            "content": "Interactive Post-Training for Vision-Language-Action Models Shuhan Tan1, Kairan Dou2, Yue Zhao1, Philipp Krähenbühl1 UT Austin1, Nankai University2 Code & Model: https://ariostgx.github.io/ript_vla/"
        },
        {
            "title": "Abstract",
            "content": "We introduce RIPT-VLA, simple and scalable reinforcement-learning-based interactive post-training paradigm that fine-tunes pretrained Vision-Language-Action (VLA) models using only sparse binary success rewards. Existing VLA training pipelines rely heavily on offline expert demonstration data and supervised imitation, limiting their ability to adapt to new tasks and environments under low-data regimes. RIPT-VLA addresses this by enabling interactive post-training with stable policy optimization algorithm based on dynamic rollout sampling and leaveon-out advantage estimation. RIPT-VLA has the following characteristics. First, RIPT-VLA applies to various VLA models, resulting in an improvement on the lightweight QueST model by 21.2%, and the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, RIPT-VLA is computationally efficient and data-efficient: With only one demonstration, RIPT-VLA enables an unworkable SFT model (4%) to succeed with 97% success rate within 15 iterations. Furthermore, we demonstrate that the policy learned by RIPT-VLA generalizes across different tasks and scenarios and is robust to the initial state context. These results highlight RIPT-VLA as practical and effective paradigm for post-training VLA models through minimal supervision. 5 2 0 2 2 2 ] . [ 1 6 1 0 7 1 . 5 0 5 2 : r Figure 1: Overview of RIPT-VLA. While VLA models are typically trained with two supervised stages, we propose third stage: Reinforcement Interactive Post-Training for VLA. RIPT-VLA sets state-of-the-art results across diverse benchmarks. It also presents remarkable improvement under the low-data regime, transforming an SFT model from near failure to 97% with one demonstration."
        },
        {
            "title": "Introduction",
            "content": "Vision-Language-Action (VLA) models [40] aim to enable agents to perceive, reason, and act in the physical world with unified interface. Current VLA models are trained with two supervised stages: large-scale pretraining on diverse human demonstrations, followed by supervised fine-tuning (SFT) on smaller-scale task-specific datasets. This paradigm has some distinct advantages: Pre-training enables the VLA model to build general visuomotor skills while SFT allows it to specialize in specific environments [15]. Supervised training allows VLA models to learn from large-scale pre-recorded vision-language-action datasets. However, this supervised approach also has two core limitations: First, data are collected offline. The VLA model learns to imitate interactions with the environment, but never sees the consequences of its own actions. As result, the learned policy often fails to handle the complexities of real-world scenarios, especially for long-horizon tasks. Second, task-specific SFT via imitation learning relies heavily on large-scale, high-quality human demonstrations. These data are expensive and time-consuming to collect, and performance degrades significantly when only small number of demonstrations are available. In this paper, we propose RIPT-VLA: third stage for VLA training paradigm with Reinforcement Interactive Post-Training. After pretraining and supervised fine-tuning, we allow the VLA model to interact with the multitask environment and receive binary success/failure rewards. We then optimize the VLA model to directly improve its success rate across multiple tasks through reinforcement learning. Inspired by prior RL frameworks for LLMs reasoning [9], we propose stable and efficient RL framework for VLA finetuning in Section 4. Specifically, we extend the LOOP framework [4] which combines REINFORCE leave-one-out (RLOO) advantage estimation [16] and proximal policy optimization (PPO) [28]. Unlike LOOP, we construct uniform batches of non-zero advantage samples, filtering out any group of trajectories with zero-advantage, and sampling rollouts until sufficient samples exist. This uniform batch construction leads to improved training stability, especially as training progresses and the VLA becomes more successful. RIPT-VLA allows efficient and stable VLA policy update without relying on shaped or learned rewards, or critic models. Using Reinforcement Learning in third training stage has few distinct advantages: It is more data efficient, yielding close to state-of-the-art performance with only single SFT demonstration. The resulting VLA model achieves much higher performance on the end-task, as it gets to see interactions with the environment during training. RIPT-VLA works with both tokenized [22] and continuous actions [14]. RIPT-VLA resonates with the recent trend of paradigm shift in LLM training [24, 9]. While pretraining on large-scale text corpora equips LLMs with broad knowledge and powerful skills, they often struggle with challenging tasks that require precise reasoning, multi-step planning, or tool using [34]. To address these limitations, reinforcement learning has emerged as critical post-training stage to reactivate and steer pretrained knowledge with only small amount of interactive feedback [24]. Similarly, we observe that pretrained VLA models also encode rich visuomotor skills, yet struggle to apply them effectively for new tasks and scenarios. RIPT-VLA bridges this gap by using only sparse binary rewards to unlock these latent skills with small number of optimization steps. Through comprehensive experiments in Section 5, we demonstrate that RIPT-VLA achieves state-ofthe-art results when combined with both large-scale and lightweight VLA models across diverse set of tasks. On the LIBERO benchmark [21], RIPT-VLA improves QueST [22], the best lightweight VLA model, on all four task suites by 10.9% absolute success rate (SR) on average  (Table 1)  . When evaluated on OpenVLA-OFT [14], the best-performing large VLA model with an already high success rate (96.7%), RIPT-VLA still helps by further reducing the failure rate from 3.3% to 2.5%. We also achieve top performance on many-task benchmarks LIBERO-90 (94.3%) and MetaWorld45 [36] (92.2%), showing the effectiveness of RIPT-VLA in improving multi-task (up to 90) performance with single model  (Table 2)  . Most notably, in the extreme low-data regime with only single training demo, RIPT-VLA adapts pretrained knowledge to new task goals or scenarios with remarkable efficiency, boosting success rate from below 4% to over 97% within only 15 RL iterations."
        },
        {
            "title": "2 Related Works",
            "content": "Vision-Language-Action Models. Vision-Language-Action (VLA) models empower embodied agents to interpret multimodal inputssuch as visual observations and natural-language instructionsand translate them into meaningful actions within the physical world [40]. Seminal works like RT-2 [40], RT-1 [2], PaLM-E [7], Octo [32], Dita [11], π0 [1], and π0.5 [13], together with 2 OpenVLA [15], showcase VLAs achieving emergent semantic reasoning and generalization to novel tasks and environments. These models are typically developed through two-stage supervisedlearning paradigm that begins with an initial pre-training phase on extensive, web-scale datasets [7, 3], which is crucial for acquiring generalizable visuomotor skills, grounding language in perception, and building robust internal representations. While this two-stage approach has advanced the field, its offline nature imposes key limitations. The supervised fine-tuning (SFT) stage typically requires vast expert demonstrations for new tasks or environments, thereby degrading few-shot performance. This highlights critical gap: the need for methods that adapt pretrained VLAs beyond static imitation by leveraging interactive experience and reducing reliance on extensive expert data. Reinforcement Learning for Large Language Models. Large Language Models (LLMs) offer precedent for enhancing pretrained models. While LLMs gain broad capabilities via pre-training and SFT, they often struggle with complex reasoning, planning, or constraint satisfaction [34]. To address this, Reinforcement Learning (RL) has emerged as transformative third stage in LLM trainingenabling learning from interactive feedback rather than static datasets [24]. Recent progress shows RL can unlock latent capabilities for math [19, 29], self-verifiable proofs [20], long-horizon planning through tree-of-thoughts [35], and preference-aligned generation with AI feedback [17]. This paradigm, in which pretrained knowledge is steered by targeted feedback, strongly motivates similar approach for Vision-Language-Action models: RL has the potential to adapt pretrained VLAs more effectively to the interactive and consequential nature of embodied tasks. Reinforcement Learning for VLA. Recent works have explored applying reinforcement learning to pretrained VLA models to overcome limitations of supervised fine-tuning and adapt to novel tasks without collecting new demonstrations. iRe-VLA [10] addresses optimization instability by alternating between PPO-based updates on frozen VLM backbone and supervised distillation stages. However, it still relies on learned value critic during PPO, and requires shaped reward functions or success weighting to guide policy learning. ConRFT [5] further combines offline Q-learning with online consistency-policy updates, but similarly depends on parameterized value function. Both methods require careful coordination between offline and online stages to stabilize critic learning. In contrast, RIPT-VLA introduces fully critic-free optimization framework with simpler training dynamics under sparse binary rewards."
        },
        {
            "title": "3 Preliminary",
            "content": "3.1 Vision-Language-Action Models Autoregressive VLA rollout. vision-language-action (VLA) model πθ maps sequence of observations and previous actions (o1:t, a1:t1), along with natural language goal g, to probability distribution over the next action at. These models operate autoregressively: at πθ( o1:t, g, a1:t1). Given an initial observation-goal pair context = (o1, g), the model generates sequence of actions conditioned on past information in an autoregressive way: πθ(a1:T o1:T , g) = (cid:89) t=1 πθ(at o1:t, g, a1:t1). (1) We denote this sampling process as = a1:T πθ( c), the observation sequence as = o1:T . Sequences terminate upon task success or reaching time limit. For each rollout sequence and task goal g, the environment returns binary reward = 1 when the task goal is successfully reached, and = 0 otherwise. The environment can be either simulator [36, 21] or the real world. There are two common ways of action prediction in VLA models. The tokenized action head represents actions as discrete tokens from fixed vocabulary and predicts actions via classification over the token set. In contrast, the regression action head directly predicts real-values action vectors via regression. Current VLA training paradigm. Current Vision-Language-Action (VLA) models are typically trained in two stages: Stage 1: Pretraining and Stage 2: Supervised Fine-tuning. In Stage 1, base policy πθ is pretrained on large-scale, diverse dataset of real-world demonstrations, denoted by Dpretrain = {(o, a, g)}N i=1. The policy is trained to imitate the ground-truth actions given offline data in Dpretrain. For VLA models with tokenized action head, the loss is: Lpre(θ) = E(o,a,g)Dpretrain (cid:34) (cid:88) log πθ(at o1:t, g, a1:t1) , (2) (cid:35) t=1 while for regression action head Lpre(θ) is implemented as an MSE or L1 loss. This stage enables VLA models to capture strong representations and learn general visuomotor and instruction-following capabilities. In Stage 2, the pretrained policy is supervised fine-tuned on smaller, multitask dataset to improve performance on small set of target tasks, denoted by Dsft = {(o, a, g)}N i=1. Typically, Dsft contains around 50 high-quality human demonstrations per task [22]. The VLA model is trained with the same objective function as in Stage 1. This stage enables the model to adapt its learned skills from Stage 1 to specialized set of skills for the target tasks. Although being the standard process of VLA training, this two-stage process has two significant issues. Firstly, it relies only on offline supervision and lacks interactive feedback from the environment. Therefore, the learned policy may often fail in real rollouts due to distribution shift and cascading errors, especially for long-term rollouts. Furthermore, the performance of VLA heavily relies on the high quality and quantity of the task-specific data in Dsft, which is often hard and costly to obtain. VLA as Markov decision processes. To better optimize VLA models, we define its task as Markov decision process (MDP). Each episode is initialized with context = (o1, g). The state is represented as [o1:t, g, a1:t1], which includes the language goal g, the sequence of past observations o1:t, and past actions a1:t1. At each timestep t, the VLA policy produces an action sampled from the policy distribution: at πθ( o1:t, g, a1:t1). The environment transitions to the next observation ot+1 based on hidden environment dynamics, producing new state [o1:t+1, g, a1:t]. After sequence of actions a1:T , the agent receives binary reward R(c, a) {0, 1} from the environment E, indicating task success or failure. The objective of VLA optimization is essentially learning policy πθ that maximizes expected task success reward: Rθ(c) = Eaπθ(c) [R(c, a)] . (3) 3.2 Reinforcement Policy Optimization We consider the reinforcement learning setting where an agent interacts with an environment to learn policy πθ(a c) that maximizes the expected return: EcDcontext, aπθ [R(c, a)], where is the context (e.g., goal and initial observation), is trajectory (e.g., sequence of actions), and R(c, a) {0, 1} is sparse binary reward returned by the environment. To optimize this objective, standard approach is policy gradient, which updates πθ with: θLθ(c) = Eaπθ [θ log πθ(a c) A(c, a)], (4) where A(c, a) is the advantage function indicating how much better the action is compared to baseline. In practice, it is hard to compute A(c, a), especially under sparse rewards. To address this issue, recent work proposed critic-free optimization framework called Leave-One-Out Proximal Policy Optimization (LOOP) [4]. Specifically, it combines the two methods below. Leave-One-Out Advantage Estimation (RLOO) [16]. For each sampled context c, we draw rollouts {ak πψ( c)}K k=1 under fixed sampling policy πψ. Each rollout receives binary reward Rk = R(c, ak). The leave-one-out baseline for rollout is computed by averaging the other rewards: bk = 1 (cid:88) j=k Rj, Ak = Rk bk. (5) This group-normalized advantage indicates how much better or worse rollout performance is relative to others given the same context. This allows us to efficiently compute stable advantage signal from sparse binary rewards, without requiring learning value functions. Proximal Policy Optimization (PPO) [28]. To update πθ using collected rollouts {(ck, ak, Ak)}, we compute the importance ratio rk = πθ(ak ck)/πψ(ak ck), where πθ is the current updating policy and πψ is the fixed sampling policy (normally set to the latest checkpoint of πθ). We then optimize πθ with the following clipped objective: LPPO = min (riAi, clip(ri, 1 ϵ, 1 + ϵ)Ai) , (6) 4 Algorithm 1 RIPT-VLA: Reinforcement Interactive Post-Training for VLA Model Update sampling VLA πψ πθ Initialize empty dataset Drollout while Drollout < do Sample context (g, o1) Dcontext Generate rollouts {ak πψ( c)}K Compute rewards {Rk R(c, ak)}K Compute baselines: bk 1 Compute advantages: Ak Rk bk for each if all = 0 then continue Input: Pretrained VLA πθ, reward function R(c, a), context dataset Dcontext 1: for step = 1 to do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end for end if Add (c, ak, Ak) for all to Drollout Update πθ with PPO loss (Equation 6) over Drollout end while for iteration = 1 to do k=1 j=k Rj end for K1 (cid:80) k=1 Rollout Collection Group Sampling Leave-One-Out Baseline Dynamic Rejection Policy Optimization where ϵ is small updating threshold (we use 0.2). This objective encourages rollouts with positive advantages while preventing unstable updates when πθ deviates too far from its previous version πψ. LOOP adopts PPO to optimize the advantage estimated by RLOO, which enables sample-efficient policy optimization in sparse reward settings without critics. It serves as an out-of-box working implementation for our interactive post-training framework in Section 4."
        },
        {
            "title": "4 RIPT-VLA",
            "content": "As mentioned above, there is gap between the current VLA training paradigm and our essential goal of making it work in our downstream tasks. On one hand, pure supervised training on offline data makes the policy fragile in real rollout due to compounding errors and the distribution gap between the offline dataset and online rollout. Furthermore, one has to collect sufficient number of high-quality demonstrations for offline datasets, especially Dsft, the model can easily overfit to the training distribution. In other words, optimizing VLA through Equation 2 does not necessarily improve the VLAs task execution success rate in Equation 3. To bridge this gap, we propose new VLA training paradigm that directly optimize pretrained VLA through interaction with the environment through Reinforcement Interactive Post-Training, or RIPT-VLA for short. 4.1 Reinforcement Interactive Post-Training for VLA Models The first two stages of our VLA training paradigm are the same as the standard setting. In Stage 1, we pretrain the VLA model on large, diverse dataset Dpretrain to learn visual-language representation and general visuomotor skills. Then, in Stage 2, we finetune VLA on small dataset Dsft to adapt it to follow instructions to solve small set of target tasks. These stages produce pretrained VLA policy πθ that can achieve non-zero success rate (can be very low) on the target tasks. In RIPT-VLA, we then conduct Stage 3: Reinforcement Interactive Post-Training. In this stage, we assume we can rollout πθ in an environment and receive binary reward R(c, a) {0, 1} given πθ( c), where is the initial context. In addition, we use an initial context dataset Dc = {(o1, g)} to set up task initializations for model rollouts. Typically, we obtain Dc by directly extracting the initial states from sequences in Dsft. For each optimization step, we iterate between two steps: rollout collection and policy optimization. During rollout collection, we randomly sample contexts ci Dc and let πθ interact with the environment to output sequence ai. For each rollout we collect its reward R(ci, ai) and compute its advantage Ai = A(ci, ai), which indicate how strong the model should be encouraged (A > 0) or 5 penalized (A < 0) for generating rollout a. We add all rollouts and rewards (ci, ai, Ai) to rollout dataset Drollout until we obtain rollouts: Drollout = {(ci, ai, Ai)}B During policy optimization, we optimize πθ with reinforcement learning algorithms on Drollout to maximize its expected task success rate in Equation 3 for iterations. After optimization, we use the updated VLA policy π θ to collect new rollouts and new step begins. This process repeats until we reach steps and outputs the final policy π θ , concluding the full VLA training paradigm. We then deploy π θ in the environment for testing. i=1 Although RIPT-VLA is simple in concept, it presents several challenges. First, we only have sparse binary rewards from each rollout sequence, no shaped reward is available. Training learned reward model to predict shaped reward values can easily lead to reward hacking [30], especially with limited rollout data. Second, as VLA models operate over long-horizon, multi-task environments, credit assignment becomes highly ambiguous. This causes the value target (e.g., from TD error) to be extremely noisy and uninformative. Third, training stable value function for VLA requires model of comparable capacity to the VLA itself, which significantly increases GPU memory usage and training cost for large VLA models [38]. Finally, in multitask environments, different task contexts can vary significantly in difficulty: some lead to trivial success while others consistently fail across all rollouts. This results in highly imbalanced success rates and unstable policy gradient updates. 4.2 Dynamic-Sampling Leave-One-Out Proximal Policy Optimization To implement RIPT-VLA in stable and sample-efficient way, we propose simple yet effective policy optimization framework in Algorithm 1. First, we adopt LOOP (Section 3.2) as the foundation of our implementation. LOOP is particularly well-suited for our VLA setting, where rollouts are long-horizon and efficient advantage estimation is required for its sparse reward signal. Furthermore, for VLA in multitask environments, we design dynamic rollout sampling mechanism to filter out uninformative contexts for more stable and efficient policy optimization. LOOP for RIPT-VLA. We apply LOOP [4] for both the rollout collection and policy optimization stage. During rollout collection, we conduct RLOO [16] advantage estimation. In this step, we use the most recent policy πθ as the sampling policy πψ. Given single context Dc, we collect trajectories by repeatingly sampling times from the policy given the same context: {ak πψ( c)}K k=1 from the environment E. For each rollout k, we compute the advantage Ak with Equation 5. For each epoch, we conduct group sampling on B/K contexts sampled from Dc, obtaining Drollout with rollouts. k=1. We obtain their corresponding rewards {Rk}K During policy optimization, we use PPO [28] to stabilize policy gradient updates. For each rollout sample (ci, ai, Ai) Drollout, we can compute its training objective LPPO with Equation 6. We perform this update over the collected rollout dataset Drollout using mini-batches for optimization steps each epoch. When = 1, the method corresponds to on-policy RLOO; when > 1, the same samples are reused for additional updates, resulting in partially off-policy optimization. Dynamic rollout sampling. VLA models often operate in multitask environments [15, 22, 31], where task difficulty varies widely across different contexts. Some contexts have already been well solved by the VLA model, leading to trivial success across K-group sampling, while others consistently fail due to inherent task complexity or distribution gap. Both cases result in rollout groups where all rollout samples receive identical rewards (all ones or all zeros), producing all-zero advantages in Equation 5. Therefore there is no gradient signal from Equation 6. Adding these samples to Drollout makes unstable gradient updates during batch optimization, as they contribute zero gradients that can dilute meaningful learning signals. To address this, we apply simple yet effective dynamic rejection strategy: we discard any sampled context for which all rollouts receive the same reward and resample new context from Dcontext for group sampling. As training progresses and the policy improves, an increasing number of task contexts yield uniformly successful rollouts. Dynamic rejection naturally filters out these solved contexts, allowing optimization to concentrate on the remaining harder contexts. Importantly, this method make the batch optimization of the PPO loss (Equation 6) to have the same effective batch size over all the minibatches across Drollout, which we empirically found to be important for stable policy optimization in RIPT-VLA. The full implementation of our optimization procedure is summarized in Algorithm 1. 4.3 Generalizing RIPT-VLA to Different VLA models RIPT-VLA is compatible with both discrete and continuous action representations commonly used in VLA models. RIPT-VLA requires the VLA model being able to output probability distribution over actions at each step, which is used in two key steps. First, during rollout collection, to support diverse rollouts in group sampling for the same initialization context, we need to randomly sample different actions from its output distribution: at πθ(at o1:t, g, a1:t1), (7) On the other hand, to perform stable policy optimization, we compute the trust region ri = πθ(aici) πψ(aici) in Equation 6 to constrain policy updates within small region of the original policy. key component in this formulation is computing the log-probability of the sampled action sequences under both policies. We compute the log-probability of sampled action sequence = (a1, . . . , aT ) as the sum of the per-step log-probabilities: log πθ(a c) = (cid:88) t=1 log πθ(at a<t, c). (8) In other words, we can apply RIPT-VLA to any VLA model πθ that we sample random action at from the per-step action distribution and compute log πθ(at a<t, c). Tokenized action head. For VLA models with discrete action outputs, e.g. QueST [22], actions are predicted as sequences of discrete tokens from fixed vocabulary, where the action header is classification head trained with NLL loss. Therefore, log πθ(at a<t, c) is directly obtained from applying softmax function to the models classification head output logits. We can also simply sample action tokens from the distribution after softmax. Regression action head. For continuous-action VLA models [14], actions are regressed using MSE or L1 loss, which do not produce log-probability. To enable policy gradient optimization, we extend the model with light-scale prediction head that estimates the scale σθ of the action value. Assuming the original output head provides the mean µθ, we treat the policy as factorized Gaussian (MSE) or Laplace (L1) distribution and train the scale head using the NLL loss in Equation 2 for few iterations on Dsft. After that, we can sample action at and compute log πθ(at a<t, c) with predicted µθ and σθ in closed form."
        },
        {
            "title": "5 Experiments",
            "content": "We evaluate RIPT-VLA on two widely used benchmarks for VLA learning: LIBERO [21] and MetaWorld [36]. We study several settings: (1) standard multitask (up to 90 tasks) setting in Sec. 5.2, (2) few-shot (1 5 demonstration) setting in Sec. 5.3, and (3) cross-task and cross-scenario setting in Secs. 5.4 and 5.5 to showcase the ability of fast generalization leveraging prior knowledge during pretraining. Additionally, we conducted studies to analyze the practical behavior of RIPT-VLA, including training curves, ablation studies as well as its sensitivity to the variance and diversity of the context dataset. 5.1 Setup Benchmark. LIBERO [21] is lifelong learning benchmark with 5 task suites. Each suite consists of set of language-guided manipulation tasks across multiple object types, task definitions and environment scenarios. Specifically, it includes 4 suites: Goal, Spatial, Object, and Long. Each suite is designed to evaluate specific aspect of object manipulation and containing 10 distinct tasks. In addition, it also includes LIBERO-90 suite that contains 90 different tasks to assess multitask performance at scale. MetaWorld [36] is manipulation task benchmark for few-shot learning models. We use Meta-Learning 45 (ML45) suite that contains 45 training tasks and 5 held-out tasks. For both benchmarks, each task comes with 50 expert demonstrations for training. At evaluation time, single VLA model is deployed across all tasks in suite and performs rollouts on 50 held-out test contexts per task. We measure performance with the average task success rate (SR). 7 Stage 1 + Stage 2 Models Method Goal Spatial Object Long Average Octo [32] OpenVLA [15] Dita [11] π0 + FAST [26] π0 [1] OpenVLA-OFT* [14] OpenVLA-OFT + RIPT 84.6 79.2 85.4 88.6 95.8 97.9 99.0 (+1.1) 78.9 84.7 84.2 96.4 96.8 97.6 98.6 (+1.0) 85.7 88.4 96.3 96.8 98.8 98.4 98.6 (+0.2) 51.1 53.7 63.8 60.2 85.2 92.9 93.8 (+0.9) 75.1 76.5 82.4 85.5 94.2 96.7 97.5 (+0.8) Stage-2 Models Method Goal Spatial Object Long Average Diffusion Policy [6] Seer [33] MDT [27] MDT+ [27] QueST [22] QueST + RIPT 68.3 73.5 80.8 92.7 (+11.9) 78.3 78.5 95.2 87.4 95.6 (+8.2) 92.5 87.5 97.8 93.6 98.4 (+4.8) 50.5 78.7 64.8 83.0 68.8 87.5 (+18.7) 72.4 76.1 82.7 93.6 (+10.9) Table 1: Multitask SR(%) on the four LIBERO suites. Bold indicates best result and underline marks the second-best. Improvements from RIPT-VLA are marked in red. *: OpenVLA-OFT results are obtained from running official checkpoints for each suite. Base models. We conduct RIPT-VLA on two pretrained VLA models with different design choices. OpenVLA-OFT [14] is an Optimized Fine-Tuned variant of the 7B OpenVLA model [15]. OpenVLA is initialized from multimodal backbone that combines Llama-2 7B language model with dual vision encoders [23, 37] and is pretrained on 970k robot-manipulation demonstrations. OFT replaces the original tokenized action decoder with continuous decoding head and trains with an L1 regression loss. This architecture represents the large-scale regression action VLA. QueST [22], on the other hand, is small-scale tokenized action VLA model with 20 million parameters. QueST first learns VQ-VAE that compresses short motion segments into discrete skill codebook; GPT-style transformer then autoregressively predicts these skill tokens conditioned on images and language, and small decoder turns tokens back into continuous joint commands. Implementation details. For OpenVLA-OFT, we fine-tune the model using the official checkpoints provided for each task suite. Training is conducted on 4 NVIDIA RTX A5000 GPUs, each with 24 GB memory. We use LoRA [12] with rank 32, and set = 8, = 192(8 24), = 1 and ϵ = 0.1, PPO mini-batch size of 4 per GPU. We set learning rate of 1e4 for the LoRA modules and 1e5 for the action head. Following Section 4.3, before applying RIPT-VLA, we first train small Laplace scale header from scratch (with the same architecture as the action header) with NLL loss on Dsft for 500 steps. For rollout collection during training, we randomly sample actions according to the Laplace distribution with scale predicted by this header. For evaluation, we directly use the mean value predicted by the original action header. For QueST, as official checkpoints are not provided, we first train the base model from scratch for each task suite following the official code and hyperparameters. In the multitask setting, we conduct RIPT-VLA on 3 GPUs with = 16, = 2880 (16 180). For the single-task setting, we use 1 GPU with = 16, = 160. For both settings, we set = 20, PPO mini-batch size of 8 per GPU, learning rate of 1e6, and the clipping parameter ϵ = 0.2. 5.2 Standard Multitask Training In this section, we evaluate RIPT-VLA under standard multitask benchmarks. For each suite, we use all 50 expert demonstrations per task as its SFT dataset Dsft. We conduct RIPT-VLA to finetune base model on the corresponding dataset for each task suite. Table 1 compares multitask performance on four LIBERO suites for different VLA models. We organize the results into two sets based on VLA training paradigm. In the Stage 1+ Stage 2 set, we include 5 state-of-the-art large VLA models: Octo [32], OpenVLA [15], Dita [11], π0 [1] and 8 Method ACT [8] PRISE [39] DP [6] VQ-BeT [18] ResNet-T [22] QueST [22] QueST + RIPT (improvement) Full Data LIBERO-90 ML45 5-shot Data LONG ML45 50.8 54.4 75.4 81.3 84.4 88.6 94.3 (+5.7) 90.8 80.4 90.3 87.6 88.4 91.0 92.2 (+1.2) 42.0 52.7 45.9 41.8 51.9 50.2 71.4 (+21.2) 70.8 66.8 65.0 65.6 54.0 63.6 76.0 (+12.4) Table 2: Mean Success Rate (SR%) across four evaluation settings: LIBERO-90 and ML45 (Full data), LONG and ML45 (5-shot). Figure 2: Few-shot curve on LIBERO-LONG. OpenVLA-OFT [14]. These models are typically larger than 500M parameters, pretrained (Stage-1) on large-scale general-purpose datasets, e.g., Open-X Embodiment [25], and then finetuned using 50 demonstrations per task for each LIBERO suite (Stage-2). In contrast, the Stage 2 set includes 4 representative small models: Diffusion Policy [6], Seer [33], MDT [27] and QueST [22]. These models are within 50M parameters and are directly trained on each LIBERO suite from scratch. We show that RIPT-VLA significantly improves the best-performing VLA model in both types, setting new state-of-the-art performance on the 4 LIBERO suites. Specifically, RIPT-VLA improves QueST on all four task suites by 10.9 absolute SR on average, and yields even larger gains of 18.7 for the challenging LONG suite. Notably, with RIPT-VLA, the small 20M QueST model achieves much better performance with large models like Dita (334M) and comparable with π0 (2B). When applying to OpenVLA-OFT, the best-performing large VLA model with already high SR, RIPT-VLA still further reduces the average failure rate from 3.3% to 2.4%. By applying RIPT-VLA, we set new state-of-the-art performance on 3 out of the 4 LIBERO suites (with only 0.2 gap on the Object suite), and achieve the highest average success rate across all tasks. These results show the RIPT-VLA is broadly effective: it can both unlock latent capabilities in small-scale models and further push the limits of the high-performing ones. In addition, in the left two columns of Table 2, we show the results on LIBERO-90 and ML45, which contain 90 and 45 diverse tasks respectively. These benchmarks assess the scalability and generalization of single VLA model across many skills. We apply RIPT-VLA to QueST and compare with representative imitation learning methods: ACT [8], PRISE [39], Diffusion Policy [6], VQ-BeT [18] and ResNet-T [22]. We show that RIPT-VLA improves performance of QueST by 5.7 and 1.2 absolute SR for LIBERO-90 and ML45, again setting new SOTA performance for both benchmarks. This confirms the utility of RIPT-VLA not only for improving performance on few related tasks, but also for scaling up to broader, more realistic scenarios where single model solves many different tasks. 5.3 Few-shot Multitask Training In this section we evaluate RIPT-VLA under few-shot multitask setting. For each suite, we uniformly sample 1 to 10 expert demonstrations from each task to constitute the few-shot SFT dataset Dsft. This setting reflects practical situation where large-scale data collection is not available. The right two columns of Table 2 show results under the 5-shot setting, where each task in the LIBERO-LONG and ML45 suites is trained with only 5 demonstrations. While baseline models struggle in this low-data regime, RIPT-VLA significantly improves QueST by 21.2 on LIBEROLONG and 12.4 on ML45. These results demonstrate that RIPT-VLA effectively addresses key limitation of standard VLA training with SFT: it enables strong performance even with minimal demonstrations, alleviating concerns about data scarcity in real-world multitask deployment. To further investigate the effect of the number of few-shot demonstrations, we conduct experiments under varying few-shot settings with QueST, ranging from 1 to 10 demonstrations per task on LIBERO-LONG. As shown in Figure 2, RIPT-VLA consistently largely improves the performance of the standard SFT model across all data scales. Note that even for the extremely low-data regime, where we only have 1 demonstration per task, RIPT-VLA can still achieve 20.8 absolute gain. As 9 Figure 3: Cross-scenario task generalization from Scenario to Scenario with the same goal. Figure 4: Cross-goal task generalization from Goal to Goal in the same scenario. the number of demonstrations increases, RIPT-VLA continues to yield performance improvements, indicating its strong sample efficiency and scalability. These results confirm that RIPT-VLA is robust across different levels of data scarcity and is applicable in both lowand high-data settings. 5.4 Cross-scenario Generalization Recent paradigm shift in LLM training demonstrates that reinforcement learning can reactivate and steer pretrained knowledge with only small amount of interactive feedback [24]. We adopt similar approach for VLA and ask: can RIPT-VLA enable sample-efficient pretrained visuomotor skill transfer across scenarios and goals? In this section, we experiment on the few-shot cross-scenario generalization setup. For each experiment, we consider pair of tasks that have the same task goal (e.g., turn on the stove and put the frying pan on it), but operate in different scenarios: Scenario and Scenario - with distanct background layouts and object configurations. In Stage 1, we pretrain QueST on Dpretrain = 50 demonstrations from Scenario to acquire general visuomotor skill for this task goal. In Stage 2, we conduct SFT on Dsft = {1, 2, 3, 4, 5} demonstrations from Scenario B. Then, in Stage 3, we apply RIPT-VLA to optimize the policy through interactive rollouts on contexts Dcontext extracted from Dsft. We then evaluate the model on the 50 testing contexts of Scenario B. We conduct experiments with 3 random seeds and plot the mean and variance across different sizes of Dsft. Figure 3 show results on 5 scenario pairs. We observe that standard SFT on VLA models clearly struggles in the 1-shot regime, achieving an average success rate of only around 5%, and in some cases dropping as low as 2%. Clearly, SFT fails to generalize the task knowledge from the pretraining stage to the new scenario. In contrast, RIPT-VLA dramatically improves performance, with absolute SR gain as high as 93.7% (from 3.5% SFT to 97.2%). As the size of Dsft increases, both SFT and RIPT-VLA performance improve, but RIPT-VLA consistently maintains strong improvement, often reaching near-100% performance with just 3-5 demonstrations. These results supports our core assumption: RIPT-VLA enables pretrained VLA models to rapidly activate and adapt learned skills with sparse binary reward feedback. 5.5 Cross-goal Generalization In this section, we investigate RIPT-VLA in cross-goal generalization setting. Here we focus on task pairs that operate in the same scenario but with different goals. Specifically, we select Task and Task such that they require the same visuomotor skills but have different goals. For example, Task 10 Figure 5: Training curve analysis of dynamic sampling. Figure 6: Analysis on context dataset size. Figure 7: Analysis on initial state std scale Method Goal Spatial Object Long 90 ML45 Average QueST + RIPT-VLA w/o Dynamic Sampling + RIPT-VLA (Ours) 80.8 90.6 92. 87.4 91.3 95.6 93.6 97.5 98.4 68.8 78.3 87.5 88.6 92.2 94.3 91.0 91.3 92.2 85.0 90.2 93. Table 3: Ablation on dynamic sampling. We compare full RIPT-VLA against variant without dynamic sampling and the QueST baseline across task types and multitask suites. is \"put the red mug on the right plate\" while Task is \"put the red mug on the left plate\". This setting tests whether pretrained visuomotor primitive skills (e.g., pick up and move) can be reused and recomposed to solve novel task goals (e.g., left vs. right). We again follow the 3-Stage paradigm: pretrain QueST on 50 demonstrations of Task A, SFT on 3-10 demonstrations on Task B, and then apply RIPT-VLA for Task B. Figure 4 presents results over 5 sets of tasks. We observe that cross-goal generalization is significantly more challenging. With 3 demonstrations, SFT models still struggle and reach only 0.7% success rate on average, almost not workable at all. With RIPT-VLA, we can improve model performance to 59.7% on average. Remarkably, for one task pair, RIPT-VLA improves the performance from near 0% success rate to 84.7%. As the number of demonstrations increases, RIPT-VLA consistently maintains large advantage across all data regions. At 10 demonstrations, the average success rate of RIPT-VLA reaches 79.7%, compared to only 29.4% for SFT. These results further show the limitation of SFT paradigm for VLA generalization under lowdata regime. In contrast, we show that RIPT-VLA not only helps adapt pre-trained skills to new environments, but also excels in fast generalization of task goal semantics. 5.6 Aditional Study Effect of dynamic rollout sampling. We ablate the impact of our dynamic rollout sampling strategy described in Section 4.2. We compare the full RIPT-VLA method with variant that disables dynamic rejection. As shown in Table 3, dynamic sampling significantly boosts performance across all task categories and suites. By filtering out uninformative rollout groups, dynamic sampling ensures stable and efficient learning with consistent gradient signal across batches. On average, we observe +3.3 absolute improvement in success rate compared to the non-dynamic variant, demonstrating its crucial role in stabilizing RIPT-VLA training. In Figure 5, we show the training curve (averaged over 3 seeds) of Column 2 of Figure 4. We see that dynamic rollout sampling accelerates convergence of RIPT-VLA, achieving consistently higher performance and more stable optimization. Effect of context dataset size. To study how the size of the context dataset Dc impacts performance, we fix the QueST model SFT-trained with only 1 demonstration for Column 2 of Figure 3 and vary the number of rollout contexts used in the RIPT-VLA stage. As shown in Figure 6, increasing the number of rollout contexts significantly improves performance. This is because more contexts provide greater diversity in initial states for the rollout interaction, allowing the model to better generalize across different setups in the testing environments. Notably, expanding Dc requires no additional human annotations: each context only consists of the initial observation state and no action is needed. This makes context dataset scaling cost-effective way to enhance generalization of RIPT-VLA. 11 Effect of context variance in RLOO group. In Equation 5, each batch of rollouts is grouped by shared initial state contexts. In realistic deployments, however, perfectly matching initial states is impractical due to inevitable setup noise. To simulate this, we compute the standard deviation of object initial positions across LIBERO-LONG, which is around 2.5 cm. Starting with QueST model SFT on 1 demo, we run RIPT-VLA while injecting Gaussian noise into the initial states with increasing scales of std. As shown in Figure 7, performance remains stable up to 1.0 (2.5 cm), and only begins to degrade beyond 2.0. Remarkably, even at 7.0 variance (17.5 cm), RIPT-VLA still outperforms the SFT baseline by significant margin."
        },
        {
            "title": "6 Conclusion",
            "content": "We presented RIPT-VLA, simple yet powerful reinforcement learning paradigm for post-training pretrained Vision-Language-Action (VLA) models using sparse binary task rewards. RIPT-VLA enables stable and data-efficient optimization without the need for shaped rewards, value functions, or reward modeling. Our method significantly improves performance across multiple VLA benchmarks and demonstrates remarkable adaptability even in extremely low-data settings. RIPT-VLA serves as scalable third-stage training paradigm that complements existing pretraining and supervised fine-tuning pipelines, unlocking the latent potential of large VLA models through direct environment interaction. An exciting future direction is to combine RIPT-VLA with reasoning and planning in VLA models to enable more sophisticated and generalizable behaviors in complex environments."
        },
        {
            "title": "References",
            "content": "[1] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. [2] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. [3] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-1: Robotics transformer for real-world control at scale. In arXiv preprint arXiv:2212.06817, 2022. [4] Kevin Chen, Marco Cusumano-Towner, Brody Huval, Aleksei Petrenko, Jackson Hamburger, Vladlen Koltun, and Philipp Krähenbühl. Reinforcement learning for long-horizon interactive llm agents, 2025. [5] Yuhui Chen, Shuai Tian, Shugao Liu, Yingting Zhou, Haoran Li, and Dongbin Zhao. Conrft: reinforced fine-tuning method for vla models via consistency policy, 2025. [6] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, 2023. [7] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. In ICML. PMLR, 2023. [8] Chen-Xiao Gao, Chenyang Wu, Mingjun Cao, Rui Kong, Zongzhang Zhang, and Yang Yu. Act: empowering decision transformer with dynamic programming via advantage conditioning. In AAAI, 2024. 12 [9] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [10] Yanjiang Guo, Jianke Zhang, Xiaoyu Chen, Xiang Ji, Yen-Jen Wang, Yucheng Hu, and Jianyu Chen. Improving vision-language-action model with online reinforcement learning, 2025. [11] Zhi Hou, Tianyi Zhang, Yuwen Xiong, Haonan Duan, Hengjun Pu, Ronglei Tong, Chengyang Zhao, Xizhou Zhu, Yu Qiao, Jifeng Dai, et al. Dita: Scaling diffusion transformer for generalist vision-language-action policy. arXiv preprint arXiv:2503.19757, 2025. [12] Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In ICLR, 2022. [13] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. Pi0.5: visionlanguage-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. [14] Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. In RSS, 2025. [15] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. In CoRL, 2024. [16] Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In ICLR, 2019. [17] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Ren Lu, Thomas Mesnard, Johan Ferret, Colton Bishop, Ethan Hall, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. In ICML, 2024. [18] Seungjae Lee, Yibin Wang, Haritheja Etukuru, Jin Kim, Nur Muhammad Mahi Shafiullah, and Lerrel Pinto. Behavior generation with latent actions. In ICML, 2024. [19] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations. [20] Aiwei Liu, Haoping Bai, Zhiyun Lu, Xiang Kong, Xiaoming Wang, Jiulong Shan, Meng Cao, and Lijie Wen. Direct large language model alignment through self-rewarding contrastive prompt distillation. In ACL, 2024. [21] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer in lifelong robot learning. In NeurIPS, 2023. [22] Atharva Mete, Haotian Xue, Albert Wilcox, Yongxin Chen, and Animesh Garg. Quest: Selfsupervised skill abstractions for learning continuous control. In NeurIPS, 2024. [23] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [24] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In NeurIPS, 2022. [25] Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open xembodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In ICRA, 2024. 13 [26] Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747, 2025. [27] Moritz Reuss, Ömer Erdinç Yagmurlu, Fabian Wenzel, and Rudolf Lioutikov. Multimodal diffusion transformer: Learning versatile behavior from multimodal goals. arXiv preprint arXiv:2407.05996, 2024. [28] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [29] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [30] Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. In NeurIPS, 2022. [31] Lingfeng Sun, Haichao Zhang, Wei Xu, and Masayoshi Tomizuka. Paco: Parametercompositional multi-task reinforcement learning. In NeurIPS, 2022. [32] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. [33] Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, and Jiangmiao Pang. Predictive inverse dynamics models are scalable learners for robotic manipulation. arXiv preprint arXiv:2412.15109, 2024. [34] Chaojie Wang, Yanchen Deng, Zhiyi Lyu, Liang Zeng, Jujie He, Shuicheng Yan, and Bo An. Q*: Improving multi-step reasoning for llms with deliberative planning. arXiv preprint arXiv:2406.14283, 2024. [35] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. NeurIPS, 2023. [36] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages 10941100. PMLR, 2020. [37] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, 2023. [38] Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, and Sergey Levine. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. In NeurIPS, 2024. [39] Ruijie Zheng, Ching-An Cheng, Hal Daumé Iii, Furong Huang, and Andrey Kolobov. Prise: Llm-style sequence compression for learning temporal action abstractions in control. In ICML, 2024. [40] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Pannag R. Sanketi, Grecia Salazar, Michael S. Ryoo, Krista Reymann, Kanishka Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey Levine, Lisa Lee, Tsang-Wei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry Kalashnikov, Ryan Julian, Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu, Alexander Herzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu, Pete Florence, Chelsea Finn, Kumar Avinava Dubey, Danny Driess, Tianli Ding, Krzysztof Marcin Choromanski, Xi Chen, Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez Arenas, and Kehang Han. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In CoRL, 2023."
        }
    ],
    "affiliations": [
        "Nankai University",
        "UT Austin"
    ]
}