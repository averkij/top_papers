{
    "paper_title": "AntiLeak-Bench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge",
    "authors": [
        "Xiaobao Wu",
        "Liangming Pan",
        "Yuxi Xie",
        "Ruiwen Zhou",
        "Shuai Zhao",
        "Yubo Ma",
        "Mingzhe Du",
        "Rui Mao",
        "Anh Tuan Luu",
        "William Yang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Data contamination hinders fair LLM evaluation by introducing test data into newer models' training sets. Existing studies solve this challenge by updating benchmarks with newly collected data. However, they fail to guarantee contamination-free evaluation as the newly collected data may contain pre-existing knowledge, and their benchmark updates rely on intensive human labor. To address these issues, we in this paper propose AntiLeak-Bench, an automated anti-leakage benchmarking framework. Instead of simply using newly collected data, we construct samples with explicitly new knowledge absent from LLMs' training sets, which thus ensures strictly contamination-free evaluation. We further design a fully automated workflow to build and update our benchmark without human labor. This significantly reduces the cost of benchmark maintenance to accommodate emerging LLMs. Through extensive experiments, we highlight that data contamination likely exists before LLMs' cutoff time and demonstrate AntiLeak-Bench effectively overcomes this challenge."
        },
        {
            "title": "Start",
            "content": "AntiLeak-Bench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge Xiaobao Wu1,2 Liangming Pan5 Yuxi Xie3,2 Ruiwen Zhou4,2 Shuai Zhao1 Yubo Ma1 Mingzhe Du1,3 Rui Mao1 Anh Tuan Luu1 William Yang Wang2 1Nanyang Technological University 2University of California, Santa Barbara 3National University of Singapore 4Shanghai Jiao Tong University 5 University of Arizona xiaobao002@e.ntu.edu.sg william@cs.ucsb.edu 4 2 0 2 8 1 ] . [ 1 0 7 6 3 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Data contamination hinders fair LLM evaluation by introducing test data into newer models training sets. Existing studies solve this challenge by updating benchmarks with newly collected data. However, they fail to guarantee contamination-free evaluation as the newly collected data may contain pre-existing knowledge, and their benchmark updates rely on intensive human labor. To address these issues, we in this paper propose AntiLeak-Bench, an automated anti-leakage benchmarking framework. Instead of simply using newly collected data, we construct samples with explicitly new knowledge absent from LLMs training sets, which thus ensures strictly contamination-free evaluation. We further design fully automated workflow to build and update our benchmark without human labor. This significantly reduces the cost of benchmark maintenance to accommodate emerging LLMs. Through extensive experiments, we highlight that data contamination likely exists before LLMs cutoff time and demonstrate AntiLeak-Bench effectively overcomes this challenge."
        },
        {
            "title": "Introduction",
            "content": "In recent years, Large Language Models (LLMs) have demonstrated prominent capabilities in multiple fields (Radford et al., 2019; Touvron et al., 2023). To thoroughly assess these capabilities, several benchmarks have been developed, such as MMLU (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021). They are typically static and publicly accessible, serving as standardized tools to evaluate LLMs performance. Unfortunately, the static nature of these benchmarks presents significant challenge: data contamination, where their test data may end up in newer LLMs training sets. This issue can inflate model performance and thus undermine the reliability and validity of 1Our code and data is available at https://github.com/ bobxwu/AntiLeak-Bench. 1 Figure 1: Illustration of AntiLeak-Bench. It constructs contamination-free samples with the knowledge updated after LLMs cutoff time, which thus are not in LLMs training sets. these benchmarks (Golchin and Surdeanu, 2023b; Roberts et al., 2023; Deng et al., 2024; Dong et al., 2024; Jiang et al., 2024). To avoid data contamination, recent studies dynamically update the benchmarks by collecting new data released after LLMs knowledge cutoff time (Kiela et al., 2021; Potts et al., 2021; Kasai et al., 2023; Jain et al., 2024). For instance, LiveBench (White et al., 2024) collects newly released questions from math exams and code platforms like LeetCode. However, despite their prevalence, we emphasize these benchmarks encounter two limitations: (i) Weak guarantee for contamination-free evaluation. They ignore to verify whether the newly collected data contain truly new knowledge (Roberts et al., 2023; White et al., 2024; Jain et al., 2024). For example, exam questions or coding problems from LeetCode may be later reused or referenced. As result, the knowledge of these data may overlap with LLMs training, which potentially leads to data contamination. (ii) High dependency on human labor for maintenance. Their benchmark updates often require intensive human labor, such as annotating the collected data (Gu et al., 2024; Xu et al., 2024). In consequence, this significantly hinders their frequent maintenance, especially in light of the rapid emergence of new LLMs. For instance, RealTimeQA (Kasai et al., 2023) and KoLA (Yu et al., 2023) have barely been updated recently. Together these limitations undermine the reliability and practicality of existing benchmarks for contamination-free evaluation. To address these limitations, we in this paper propose AntiLeak-Bench, an automated anti-leakage benchmarking framework to prevent data contamination. As illustrated in Figure 1, rather than directly collecting newly released data as previously, we identify new real-world knowledge updated after LLMs cutoff time. Then we construct questionanswering samples querying these updated knowledge, accompanied by their corresponding realworld supporting documents. This ensures the updated knowledge is absent from LLMs training sets, and thus the constructed samples with them are strictly contamination-free. Furthermore, we design fully automated workflow to build and update AntiLeak-Bench. It eliminates the need for human labor, enabling the benchmark to be seamlessly updated to accommodate emerging LLMs. As such, this significantly reduces the maintenance cost of our benchmark, enhancing its practicality and scalability. We evaluate series of LLMs on our AntiLeakBench with samples before and after the cutoff time. We observe that their performance commonly drops after the cutoff time. This trend highlights the likely data contamination in LLM evaluation. The experimental results additionally manifest the effectiveness of AntiLeak-Bench for contamination-free evaluation. The contributions of this paper can be concluded as follows: We propose AntiLeak-Bench, an automated anti-leakage benchmarking framework, ensuring contamination-free evaluation by constructing test samples with updated real-world knowledge. We propose an automated building workflow that automatically maintains and updates AntiLeakBench without human labor, enabling to easily accommodate emerging new LLMs. We conduct extensive experiments involving various LLMs on multiple tasks, and demonstrate the effectiveness of AntiLeak-Bench for contamination-free evaluation. ities of LLMs across various tasks, like questionanswering, reading comprehension, and math reasoning (Pan et al., 2023, 2024; Zhou et al., 2024). Notable examples include ARC (Clark et al., 2018), MMLU (Hendrycks et al., 2021), BIG-bench (Srivastava et al., 2022), and GSM8K (Cobbe et al., 2021). Although they have played significant role, their static nature may cause the data contamination issue (Magar and Schwartz, 2022; Yang et al., 2023; Golchin and Surdeanu, 2023a; Jacovi et al., 2023; Li, 2023; Oren et al., 2023; Sainz et al., 2023). Due to this, some work discloses several benchmarks are gradually less effective (Schaeffer, 2023; Zhou et al., 2023). For example, evidence shows several models have overfitted to the GSM8K benchmark, compromising its validity (Zhang et al., 2024). Contamination-Free Evaluation To achieve contamination-free evaluation, recent studies dynamically update benchmarks by collecting new data (Zhu et al., 2023; Thrush et al., 2022). For instance, RealTimeQA (Kasai et al., 2023) periodically collects multi-choice quizzes from newspapers. Similarly, LiveCodeBench (Jain et al., 2024) frequently crawls programming questions from code platforms like LeetCode. LiveBench (White et al., 2024) extends it by collecting data from more domains, such as math competitions, research papers, and news articles. These benchmarks cannot guarantee contamination-free evaluation since they rely solely on the newly released data and also need intensive human labor for construction (Liska et al., 2022; Mousavi et al., 2024). Recent ADU (Ying et al., 2024) updates existing benchmarks by paraphrasing data through LLMs, but it may risk introducing mistakes and biases into evaluation. Different from these studies, our AntiLeak-Bench constructs samples with newly updated real-world knowledge to ensure contamination-free evaluation. It also introduces fully automated building workflow without human labor. These differences make AntiLeak-Bench more reliable and practical benchmarking framework for consistent contamination-free evaluation."
        },
        {
            "title": "2 Related Work",
            "content": "Data Contamination Many benchmarks have been widely used to assess the impressive capabilIn this section, we present how to build our AntiLeak-Bench by automatically constructing contamination-free samples with updated knowledge. Figure 2 illustrates the building workflow. 2 Figure 2: Illustration of the automated benchmark building workflow without human labor. After data preparation, it includes three main steps: (1) Identify updated knowledge after the cutoff time; (2) Build supporting documents; (3) Construct contamination-free samples (Figure 3 exemplifies how to construct multi-hop samples)."
        },
        {
            "title": "3.1 Preparing Data",
            "content": "We begin by preparing the data to build the benchmarks. For the knowledge source, we leverage Wikidata (Vrandeˇcic and Krötzsch, 2014), widely used and frequently updated knowledge base. Wikidata provides an extensive repository of real-world factual claims involving numerous entities. Each claim is represented as triplet (subject, relation, object), such as (Lionel Andrés Messi, member of sports team, Paris Saint Germain). We sample subset of relations associated with physical entities, such as member of sports team; we exclude the less meaningful relations, for example, ones about virtual entities, like geometry coordinates, similar to Zhong et al. (2023); Wu et al. (2024d) (See details in Appendix A). For each claim, we extract two qualifiers from Wikidata: start time and end time, which specifies when the claim starts and ends, e.g., the timeline of football player in team. We use these prepared data to build and update our AntiLeak-Bench. 3."
        },
        {
            "title": "Identifying Updated Knowledge",
            "content": "We then identify updated knowledge based on the prepared data. Considering t1 as the knowledge cutoff time of an LLM and t2 as the current time, our goal is to find the updated knowledge that occurs after t1 and before t2. For this purpose, we figure out the history of claims. Specifically, we group all the claims by their subject and relation and sort the claims in each group chronologically based on their start time. If new claim emerges after the cutoff time t1 in the history, i.e., the object changes, we identify the new claim as the updated knowledge. For instance, in Figure 2 the object of (Lionel Andrés Messi; member of sports team) shifts after the cutoff time: Paris Saint Germain Inter Miami. From this shift, we extract updated knowledge with the new claim: (Lionel Andrés Messi; member of sports team; Inter Miami). We emphasize that the LLM is unaware of this knowledge because it occurs after its cutoff time. One may wonder what if the object changes twice and eventually reverts to its original one, like player returning to his previous team. To exclude this case, we additionally ensure that the new claim is different from the old one and consider it as updated knowledge."
        },
        {
            "title": "3.3 Building Supporting Documents",
            "content": "To provide context for the updated knowledge, we build supporting documents from real-world sources. While LLMs could generate such documents, this may introduce mistakes or biases as LLMs often hallucinate or misinterpret information (White et al., 2024). To ensure accuracy and reliability, we rely on the well-maintained and widely trusted Wikipedia as the source of supporting documents. This choice is further justified since the updates of Wikidata commonly follow the updates of Wikipedia (Vrandeˇcic and Krötzsch, 2014). In detail, we denote the identified updated knowledge as claim (s1, r1, o1) and retrieve the Wikipedia page revision history of either its subject s1 or object o1, determined by its relation, e.g., subject Lionel Andrés Messi for relation member of sports team in Figure 2. Then we find the revision made after the start time of the updated knowledge. 3 With this revision, we retrieve its corresponding article from Wikipedia and check if the summary of the article contains both the subject and object (or their aliases). If true, we consider this article as supporting document for the updated knowledge (We show their validity in Sec. 3.7). For example, Figure 2 illustrates Wikipedia article indicating Lionel Andrés Messi is member of Inter Miami. Here the supporting document is revised after LLMs cutoff time, so it is also nonexistent in their training sets."
        },
        {
            "title": "Samples",
            "content": "Now we construct test samples querying the above updated knowledge (s1, r1, o1), with its supporting document as context, denoted as D. Since the knowledge and the supporting document both are absent from LLMs training sets, the constructed samples are strictly contamination-free. This is different from previous studies: they overlook verifying that the inherent knowledge of their samples does not exist in LLMs training sets. Tasks We mainly focus on question-answering tasks, including both single-hop and multi-hop questions. Following the common practice (Yang et al., 2018; Koˇcisk`y et al., 2018; Bai et al., 2024), each sample is denoted as (Q, C, A) with question Q, context C, and expected answer A. Single-Hop. In this task, we directly ask what is the answer to the updated knowledge by providing the supporting document. We first consider Single-Hop Gold: Question is formulated with predefined templates based on the relation in the updated knowledge, Context only includes the supporting document D, and Answer is the object o1 and its aliases. For instance, in Figure 2 the question is What sports team is Lionel Andrés Messi member of?; the context is Lionel Andrés Messis Wikipedia article; the answer is his name and aliases. To enhance the difficulty, we further introduce new task: Single-Hop Nd: Question is the same as the above, but we augment Context with Nd distracting documents. Here these distracting documents are randomly sampled from other samples supporting documents which do not contain the subject s1 and object o1 of the updated knowledge. As such, this task further assesses the long-context capability of LLMs, Figure 3: Illustration of constructing multi-hop samples. Find the consequent relation of previous objects. specifically locating relevant information within distractors. Multi-Hop. Moreover, we construct multihop questions for harder reasoning. We first consider Multi-Hop Gold. Specifically, starting with the updated knowledge (s1, r1, o1), we build chain of connecting claims: ((s1, r1, o1), (s2, r2, o2), . . . , (sH , rH , oH )). Here the object of the i-th triplet serves as the subject of the (i+1)-th triplet, i.e., oi = si+1. As such, Question is formulated across this chain, Context includes the supporting document of each knowledge in this chain, and Answer is the last object oH . For instance, Figure 2 shows multi-hop question Who is the coach of the sports team that Lionel Andrés Messi is member of? and the context is Lionel Andrés Messis and Gerardo Martinos Wikipedia articles; the answer is the last object Gerardo Martino. Similarly, we also consider Multi-Hop Nd: the context additionally covers Nd randomly sampled supporting documents of other samples as the distracting documents. This task assesses the multi-hop reasoning ability of LLMs, requiring them to connect and integrate information across long context with distractors. Question Formats We consider two common question formats."
        },
        {
            "title": "Benchmark",
            "content": "Strictly Contamination-Free"
        },
        {
            "title": "Data Source",
            "content": "Realtime QA(Kasai et al., 2023) LiveBench(White et al., 2024) ADU(Ying et al., 2024) AntiLeak-Bench"
        },
        {
            "title": "Real world",
            "content": "Table 1: Comparisons between AntiLeak-Bench and other benchmarking frameworks. Generation. Question solely is the question"
        },
        {
            "title": "Examples",
            "content": "as aforementioned. Multi-Choice. In this format, we additionally prompt LLMs with 4 options and ask them to select one. We design the 4 options as follows: (a) Correct option, i.e., the correct answer to the question. (b) Unknown option, represented as string Unknown. (c) Outdated option. The old answer before the cutoff time, e.g., Paris Saint Germain) in Figure 2. (d) Noise option. randomly sampled, unrelated answer from other samples. Note that for multi-hop questions, we ignore finding outdated options and instead provide two noise options to accelerate the building workflow. The above introduces the automated workflow to build our AntiLeak-Bench. Table 2 shows an example of Single-Hop Gold, and more examples are in Appendix D."
        },
        {
            "title": "3.5 Benchmark Maintenance",
            "content": "Our AntiLeak-Bench supports easy maintenance. We only need to download the latest Wikidata dump and then execute our automated workflow to update benchmarks. The whole process requires no human labor, and hence we can effortlessly maintain the benchmarks for newer LLMs."
        },
        {
            "title": "3.6 Multilingual Benchmarks",
            "content": "Moreover, AntiLeak-Bench features multilingual evaluation. It can seamlessly produce samples in various languages through our automated workflow with the multilingual nature of both Wikidata and Wikipedia. This enables us to evaluate LLMs in various linguistic contexts. See more details in Appendix A."
        },
        {
            "title": "3.7 Human Verification on Data Quality",
            "content": "question (generation) answer (generation) question (multi-choice) answer (multi-choice) subject pid object object_old context What sports team is Lionel Andrés Messi member of? Inter Miami CF Inter Miami Club Internacional de Fútbol Miami What sports team is Lionel Andrés Messi member of? A. Inter Miami CF B. Paris Saint-Germain F.C. C. Prime Minister of Romania D. Unknown. Lionel Messi Lionel Andres Messi Lionel Andrés Messi P54 (member of sports team) Inter Miami CF Inter Miami Club Internacional de Fútbol Miami Paris Saint-Germain F.C. Paris Saint-Germain Football Club Paris Saint-Germain FC Lionel Andrés Messi (; born 24 June 1987), also known as Leo Messi, is an Argentine professional footballer who plays as forward for Major League Soccer club Inter Miami. . . Table 2: An example from AntiLeak-Bench."
        },
        {
            "title": "Quality Metrics",
            "content": "Single-Hop Gold Multi-Hop Gold"
        },
        {
            "title": "Context Accuracy\nAnswer Accuracy",
            "content": "97.3 96.7 98.7 97.3 Table 3: Data quality by human verification. Appendix for the detailed verification procedure. Table 3 shows that AntiLeak-Bench achieves high standard of data quality with question and context accuracy over 96%. To verify the data quality of produced samples in AntiLeak-Bench, we conduct human verifications. We ask human annotators to evaluate the accuracy of both answers and contexts in the samples. See"
        },
        {
            "title": "3.8 Comparisons with Existing Benchmarks",
            "content": "Table 1 compares our AntiLeak-Bench with other benchmarks, and we highlight its vital advantages: (i) Strictly contamination-free. AntiLeak-Bench 5 Figure 4: EM and F1 performance at each time interval. Figure 5: Correct and outdated option proportions at each time interval. ensures that the constructed samples must cover updated knowledge absent from LLMs training sets, which guarantees strictly contamination-free evaluation. (ii) Automated workflow. Our fully automated building workflow avoids human labor, which significantly reduces the cost of maintaining the benchmark for newer LLMs. In consequence, this ensures its adaptivity and long-term applicability. (iii) Multilingual. Our method supports to construction of multilingual samples. This enables us to extensively assess LLMs capabilities across different languages. (iv) Real-world data. We build test samples from real-world data sources like Wikipedia, rather than LLM-generated content. This grounds the benchmark in practical and authentic data. With these advantages, AntiLeakBench serves as reliable testbed for evaluating LLMs in strictly contamination-free environment."
        },
        {
            "title": "4 Experiment",
            "content": "In this section, we experiment with different LLMs on our AntiLeak-Bench to show its effectiveness and investigate data contamination."
        },
        {
            "title": "4.1 Experiment Setup",
            "content": "Large Language Models We experiment with the following 12 common open-source language models for experiments: Llama-2-7B (Touvron et al., 2023), Llama-2-13B (Touvron et al., 2023), Mistral-7B (Jiang et al., 2023), Vicuna-v1.5-7B (Chiang et al., 2023), LongChat-v1.5-7B (Li* et al., 2023), Llama-3.1-8B (Dubey et al., 2024), Phi3.5-mini (Abdin et al., 2024), Qwen-2-7B (Yang et al., 2024), Mistral-Nemo-12B (Jiang et al., 2023), Gemma-2-9B (Team, 2024). We also consider proprietary models: GPT-4o-mini and GPT-4o (Achiam et al., 2023). Table 8 summarizes their release and knowledge cutoff time. We use the prompts in appendix following Bai et al. (2024), which explicitly ask LLMs to use the provided context to answer questions. Constructing Test Samples To investigate the impact of data contamination, we on purpose construct two kinds of test samples: (i) Pre-cutoff samples, containing knowledge before the cutoff time. These samples may already exist in the LLMs training sets. (ii) Post-cutoff samples, containing knowledge updated after the cutoff time. These samples are absent from LLMs training sets. To construct these samples, we establish two time periods according to the LLMs cutoff time summarized in Table 8: (i) From 2022-01-01 to 2023-01-01, divided into 2-month intervals. This targets the first 5 models since their knowledge cutoff time mostly falls around Sep 2022, such as Vicuna-v1.5-7B. (ii) From 2023-05-01 to 2024-0801, divided into 3-month intervals. Similarly, this is tailored for the last 7 models whose cutoff time mostly falls around the end of 2023, e.g., Llama3.1-8B and GPT-4o. Here we divide these periods into shorter intervals because (i) this can ensure each interval contains sufficient number of samples, enabling statistically meaningful analysis, and (ii) this provides detailed granular insights into how 6 Language Models Single-Hop Multi-Hop Gold Nd=3 Nd=5 Nd= Gold Nd=3 Nd=5 Nd=7 Avg EM F1 EM F1 EM F1 EM F1 EM F1 EM F1 EM F1 EM F1 EM Llama-2-7B Llama-2-13B Mistral-7B Vicuna-v1.5-7B Longchat-v1.5-7B 40.6 42.7 65.4 66.8 75.5 19.2 Llama-3.1-8B 69.0 Phi-3.5-mini Qwen-2-7B 54.8 Mistral-Nemo-12B 82.7 85.0 Gemma-2-9B 78.5 GPT-4o-mini 81.2 GPT-4o 63.5 65.3 77.2 79.9 84.5 66.2 78.7 72.4 89.7 91.6 88.1 89.5 16.8 14.0 27.8 39.1 58. 21.4 34.0 15.5 75.6 80.2 80.3 84.1 41.2 40.6 41.3 60.4 72.8 59.4 40.5 38.5 83.8 86.2 89.2 90.8 11.6 9.4 16.7 25.8 47.6 18.1 26.5 9.8 66.3 68.8 79.1 83.5 30.9 30.6 27.3 48.3 65. 53.5 33.7 26.6 75.1 75.2 88.1 90.3 9.4 7.0 7.3 15.3 37.0 14.2 15.2 7.2 51.8 55.4 79.2 84.8 24.5 24.0 15.3 39.1 56.3 45.7 22.2 21.2 62.2 61.2 88.5 91.4 33.6 13.3 21.4 26.0 38. 24.4 45.4 35.9 57.7 82.7 68.8 71.5 50.2 34.6 27.9 43.5 51.4 50.2 59.7 48.3 67.3 86.4 83.1 85.9 19.4 4.1 11.5 11.1 17.6 11.7 20.8 23.7 39.1 63.0 60.5 71.9 32.2 21.5 17.2 22.9 30. 33.0 29.5 33.4 47.7 68.3 75.3 86.1 15.8 2.7 8.1 8.1 12.0 9.4 14.9 18.1 33.8 55.8 57.1 70.2 28.1 17.8 14.3 19.5 25.8 27.5 21.1 26.1 41.4 61.2 73.1 84.8 12.2 2.3 6.5 5.4 4. 6.8 9.8 13.6 24.0 49.0 54.2 70.2 22.7 15.2 11.1 15.7 3.9 21.9 14.4 20.1 29.0 53.5 70.6 84.8 19.9 11.9 20.6 24.7 36.4 15.6 29.4 22.3 53.9 67.5 69.7 77.2 36.7 31.2 29.0 41.2 48. 44.7 37.5 35.8 62.0 73.0 82.0 87.9 Table 4: EM (Exact Match) and F1 results in the generation format on AntiLeak-Bench. Gold means only gold documents; Nd is the number of distracting documents. The best is in bold. Language Models Single-Hop Multi-Hop Gold Nd=3 Nd=5 Nd=7 Gold Nd=3 Nd= Nd=7 Avg Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc Llama-2-7B Llama-2-13B Mistral-7B Vicuna-v1.5-7B Longchat-v1.5-7B 41.7 82.1 81.8 80.1 79."
        },
        {
            "title": "86.7\nLlama-3.1-8B\n87.4\nPhi-3.5-mini\nQwen-2-7B\n89.1\nMistral-Nemo-12B 88.5\n92.4\nGemma-2-9B\n93.2\nGPT-4o-mini\n92.8\nGPT-4o",
            "content": "30.7 82.2 81.8 80.0 79.7 90.4 87.5 39.7 71.1 92.4 93.2 92.8 3.7 73.7 65.9 75.6 68.5 62.2 85.6 83.0 88.8 86.7 93.8 93.5 5.6 73.6 65.8 75.4 68.8 74.0 85.8 27.9 71.8 86.5 93.8 93. 3.5 60.1 58.3 73.1 65.1 48.9 84.7 78.2 84.7 76.9 93.3 94.0 5.3 59.9 58.2 72.9 51.8 62.9 85.4 24.6 70.2 61.6 93.3 94.0 2.8 51.7 52.3 69.6 62.3 37.8 79.6 77.0 77.8 69.4 93.5 94. 5.4 51.3 52.3 69.4 61.2 52.9 82.5 78.5 83.8 69.3 93.5 94.0 18.7 97.5 88.7 96.8 93.2 70.5 96.5 97.6 91.1 97.1 98.5 97.9 30.9 97.5 88.6 96.9 93.4 81.4 97.0 98.3 94.6 97.1 98.5 97. 6.8 88.5 77.2 84.0 76.7 50.7 85.3 94.5 77.1 88.3 96.4 95.8 9.9 88.5 77.2 84.2 78.0 64.8 86.2 54.2 68.4 88.3 96.4 95.8 5.6 82.8 72.7 82.6 70.4 40.9 78.0 92.4 69.9 81.8 95.4 95. 8.1 83.1 72.8 83.0 71.5 56.2 80.3 46.4 64.0 65.4 95.4 95.4 3.6 75.2 67.7 77.0 66.6 30.8 68.6 91.5 43.1 77.4 93.5 93.9 6.9 75.2 67.2 77.2 68.0 44.9 72.3 91.7 58.7 77.4 93.5 93. 10.8 76.5 70.6 79.8 72.8 53.6 83.2 87.9 77.6 83.8 94.7 94.7 12.9 76.4 70.5 79.9 71.6 65.9 84.6 57.7 72.8 79.8 94.7 94.7 Table 5: Acc and F1 results in the multi-choice format on AntiLeak-Bench. Gold means only gold documents; Nd is the number of distracting documents. The best is in bold. contamination risks and model performance evolve over time. We report the statistics of the produced samples in our benchmark in Tables 6 and 7. See more building details in Appendix A. Evaluation Metrics For the generation format (Sec. 3.4), we use EM (Extract Matching) and token-based F1 scores, following the standard practice in question answering (Rajpurkar et al., 2016). For the multi-choice format, we use Acc (Accuracy) and F1 scores following Kasai et al. (2023). format (Llama-3.1-8B is excluded due to its low EM; See Sec. 4.3). We observe general performance decline for most LLMs after their knowledge cutoff time, although all samples are constructed in the same way. For instance, Vicunav1.5-7Bs EM and F1 start dropping near its cutoff time Sep 2022; Similarly, GPT-4o-mini remains stable until its cutoff time Oct 2023 and decreases thereafter. According to this decline, we conclude two key findings:"
        },
        {
            "title": "4.2 Data Contamination Analysis",
            "content": "We first analyze the impact of data contamination by looking into the performance trends of LLMs. Figure 4 presents the trends of EM and F1 scores under the Single-Hop Gold task in the generation (1) Pre-cutoff samples come with data contamination, which inflates LLMs performance. Precutoff samples may overlap with LLMs training sets. This allows LLMs to correctly handle these samples based on their prior knowledge rather than actually understanding the provided context. As 7 result, evaluating models solely on pre-cutoff samples can overestimate their capabilities. (2) Contamination-free post-cutoff samples are more challenging and can more accurately assess LLMs. Post-cutoff samples are free from contamination since they include knowledge updated after LLMs cutoff time. To deal with these samples, LLMs must demonstrate true comprehension and reasoning over the given context and questions, as they cannot rely on prior knowledge alone. As such, evaluating with post-cutoff samples more accurately reflects LLMs abilities. Interestingly, we notice that some LLMs experience performance drops even before the cutoff time. This is probably because their training data cover less knowledge close to the cutoff time. Knowledge sources such as news articles and Wikipedia pages usually become widely documented only after initial events occur. Besides, Figure 5 plots the proportions of correct and outdated options selected under the SingleHop Gold task in the multi-choice format (Llama2-7B is excluded due to its low performance; See Sec. 4.3). Notably LLMs increasingly favor outdated options over correct ones (See option types in Sec. 3.4). For example, Mistral-Nemo-7Bs proportion of selecting correct options decreases from 91.0% to 85.5%, while the proportion of outdated options rises from 6.4% to 13.3%. These results again validate that pre-cutoff samples, due to data contamination, could inflate the performance. In summary, the above results demonstrate the effectiveness of our AntiLeak-Bench, which effectively ensures contamination-free evaluation and serves as more reliable assessment of LLMs."
        },
        {
            "title": "4.3 Overall Performance Analysis",
            "content": "Next we analyze the overall performance of LLMs. Tables 4 and 5 summarize the average results over all samples across tasks in the generation and multichoice format respectively. According to these results, we have the following observations. (1) AntiLeak-Bench poses significant challenge for LLMs. Table 4 shows that most models score EM and F1 below 50, indicating substantial gap between their capabilities and the benchmarks requirements. Only two models, GPT-4o-mini and GPT-4o, reach EM and F1 scores around 70 and 80. These results highlight the challenging nature of our AntiLeak-Bench for evaluating LLMs. (2) Proprietary models lead in performance. Tables 4 and 5 reveal that proprietary models surpass open-source ones by large margin. For instance, GPT-4o achieves the highest average EM and F1 scores 77.2 and 87.9 respectively, while the runner-up open-source model only has 53.9 and 62.0. Additionally, GPT-4os performance remains relatively stable even with the increasing number of distracting documents. This substantial margin may be attributed to their longer max context length, superior model architectures, and larger parameter sizes."
        },
        {
            "title": "4.4 Difficulty Analysis",
            "content": "We further analyze the difficulty concerning tasks and question formats based on Tables 4 and 5. We summarize the key findings as follows. (1) Multi-choice format is significantly easier. LLMs generally perform better in the multi-choice format compared to the generation format. For example, GPT-4o-mini achieves Acc and F1 scores over 90, surpassing its performance in the generation format. This is because the multi-choice format simplifies the tasks by providing explicit answer options, enabling LLMs to identify correct answers with partial comprehension. (2) Longer contexts bring about higher difficulty. LLMs performance gradually decreases along with more distracting documents, such as from Nd=3 to 7 in Table 4. The reason is that long contexts distract LLMs from locating relevant information. This underscores the necessity of LLMs stronger ability to handle long contexts. (3) Multi-hop tasks are more challenging. Compared to single-hop ones, LLMs performance becomes lower on multi-hop tasks. For example, in Table 4 the EM score of Longchat-v1.5-7B decreases from 75.5 on the Single-Hop Gold to 38.8 on the Multi-Hop Gold; GPT-4o-mini decreases from 78.5 to 68.8. These multi-hop tasks require LLMs to decompose complex questions and reason across long and interconnected contexts."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose AntiLeak-Bench, an automated anti-leakage benchmarking framework. Unlike solely collecting newly released data as before, it constructs test samples based on identified updated real-world knowledge, ensuring strictly contamination-free evaluation. It also introduces fully automated building workflow without human 8 labor. This enables frequent and efficient benchmark updates to accommodate emerging LLMs, greatly simplifying maintenance. These advantages establish AntiLeak-Bench as an ideal testbed for contamination-free evaluation, providing accurate and fair assessments for LLMs. Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024. LongBench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31193137, Bangkok, Thailand. Association for Computational Linguistics."
        },
        {
            "title": "Limitations",
            "content": "We believe our work has the following limitations: To implement the automated building workflow, we mainly consider the single-hop and multi-hop question-answering tasks for evaluation. More diverse tasks can be explored in the future, which will more extensively evaluate LLMs in contamination-free manner. Our benchmarks use Wikidata and Wikipedia as data sources. While these sources are extensive and frequently updated, they may contain incorrect information. As discussed in Sec. 3.7, most produced samples are correct as verified by humans, but occasionally they are inaccurate. Incorporating additional or more rigorous data sources in the future may address this limitation."
        },
        {
            "title": "Ethics Statement",
            "content": "In this paper, we build and maintain our AntiLeakBench with Wikidata and Wikipedia as the data sources. We acknowledge that Wikidata and Wikipedia may contain inaccurate information in few cases as they are extremely abundant and rely on human labor for maintenance. Besides, in our work we focus on real-world commonsense knowledge by sampling relations with physical entities (See Appendix A). This can avoid harmful information in most situations."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. 2024. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, and Arman Cohan. 2024. Investigating data contamination in modern benchmarks for large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 86988711. Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Bin Gu, Mengfei Yang, and Ge Li. 2024. Generalization or memorization: Data contamination and trustworthy evaluation for large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1203912050, Bangkok, Thailand. Association for Computational Linguistics. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Shahriar Golchin and Mihai Surdeanu. 2023a. Data contamination quiz: tool to detect and estimate contamination in large language models. arXiv preprint arXiv:2311.06233. Shahriar Golchin and Mihai Surdeanu. 2023b. Time travel in llms: Tracing data contamination in large language models. arXiv preprint arXiv:2308.08493. Zhouhong Gu, Xiaoxuan Zhu, Haoning Ye, Lin Zhang, Jianchen Wang, Yixin Zhu, Sihang Jiang, Zhuozhi Xiong, Zihan Li, Weijie Wu, et al. 2024. Xiezhi: An ever-updating benchmark for holistic domain knowledge evaluation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1809918107. 9 Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In International Conference on Learning Representations. Alon Jacovi, Avi Caciularu, Omer Goldman, and Yoav Goldberg. 2023. Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 50755084. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Minhao Jiang, Ken Ziyu Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, and Sanmi Investigating data contamination Koyejo. 2024. for pre-training language models. arXiv preprint arXiv:2401.06059. Cyprien De Masson, Tim Scholtes, Manzil Zaheer, Susannah Young, et al. 2022. Streamingqa: benchmark for adaptation to new knowledge over time in question answering models. In International Conference on Machine Learning, pages 1360413622. PMLR. Inbal Magar and Roy Schwartz. 2022. Data contamination: From memorization to exploitation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 157165. Seyed Mahed Mousavi, Simone Alghisi, and Giuseppe Riccardi. 2024. Dyknow:dynamically verifying timesensitive factual knowledge in llms. arXiv preprint arXiv:2404.08700. Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori Hashimoto. 2023. Proving test set contamination in black-box language In The Twelfth International Conference models. on Learning Representations. Fengjun Pan, Xiaobao Wu, Zongrui Li, and Anh Tuan Luu. 2024. Are LLMs good zero-shot fallacy classifiers? In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1433814364, Miami, Florida, USA. Association for Computational Linguistics. Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Velocity Yu, Dragomir Radev, Noah Smith, Yejin Choi, and Kentaro Inui. 2023. Realtime qa: whats the answer right now? In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 4902549043. Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang Wang, Min-Yen Kan, and Preslav Nakov. 2023. Fact-checking complex claims with program-guided reasoning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 69817004. Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, et al. 2021. Dynabench: Rethinking benchmarking in nlp. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 41104124. Tomáš Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328. Dacheng Li*, Rulin Shao*, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023. How long can opensource llms truly promise on context length? Yucheng Li. 2023. An open source data contamination report for llama series models. arXiv preprint arXiv:2310.17589. Adam Liska, Tomas Kocisky, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal, DAutume Christopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela. 2021. Dynasent: dynamic benchmark for sentiment analysis. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 23882404. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392, Austin, Texas. Association for Computational Linguistics. Manley Roberts, Himanshu Thakur, Christine Herlihy, Colin White, and Samuel Dooley. 2023. To the cutoff... and beyond? longitudinal perspective on llm In The Twelfth International data contamination. Conference on Learning Representations. 10 Oscar Sainz, Jon Ander Campos, Iker García-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. 2023. Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark. In The 2023 Conference on Empirical Methods in Natural Language Processing. Rylan Schaeffer. 2023. Pretraining on the test set is all you need. arXiv preprint arXiv:2309.08632. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the arXiv preprint capabilities of language models. arXiv:2206.04615. Gemma Team. 2024. Gemma. blog. Tristan Thrush, Kushal Tirumala, Anmol Gupta, Max Bartolo, Pedro Rodriguez, Tariq Kane, William Gaviria Rojas, Peter Mattson, Adina Williams, and Douwe Kiela. 2022. Dynatask: framework for creating dynamic ai benchmark tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 174181. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Denny Vrandeˇcic and Markus Krötzsch. 2014. Wikidata: free collaborative knowledgebase. Commun. ACM, 57(10):7885. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, et al. 2024. Livebench: challenging, contamination-free llm benchmark. arXiv preprint arXiv:2406.19314. Xiaobao Wu, Chunping Li, Yan Zhu, and Yishu Miao. 2020. Short text topic modeling with topic distribution quantization and negative sampling decoder. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 17721782, Online. Xiaobao Wu, Anh Tuan Luu, and Xinshuai Dong. 2022. Mitigating data sparsity for short text topic modeling by topic-semantic contrastive learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 27482760, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Xiaobao Wu, Thong Nguyen, and Anh Tuan Luu. 2024a. survey on neural topic models: Methods, applications, and challenges. Artificial Intelligence Review. Xiaobao Wu, Thong Nguyen, Delvin Ce Zhang, William Yang Wang, and Anh Tuan Luu. 2024b. Fastopic: fast, adaptive, stable, and transferarXiv preprint able topic modeling paradigm. arXiv:2405.17978. Xiaobao Wu, Fengjun Pan, and Anh Tuan Luu. 2024c. Towards the TopMost: topic modeling system toolkit. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 3141, Bangkok, Thailand. Association for Computational Linguistics. Xiaobao Wu, Liangming Pan, William Yang Wang, and Anh Tuan Luu. 2024d. AKEW: Assessing knowlIn Proceedings of the edge editing in the wild. 2024 Conference on Empirical Methods in Natural Language Processing, pages 1511815133, Miami, Florida, USA. Association for Computational Linguistics. Zhikun Xu, Yinghui Li, Ruixue Ding, Xinyu Wang, Boli Chen, Yong Jiang, Xiaodong Deng, Jianxin Ma, Hai-Tao Zheng, Wenlian Lu, et al. 2024. Let llms take on the latest challenges! chinese dynamic arXiv preprint question answering benchmark. arXiv:2402.19248. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph Gonzalez, and Ion Stoica. 2023. Rethinking benchmark and contamination for language modarXiv preprint els with rephrased samples. arXiv:2311.04850. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. Jiahao Ying, Yixin Cao, Yushi Bai, Qianru Sun, Bo Wang, Wei Tang, Zhaojun Ding, Yizhe Yang, Xuanjing Huang, and Shuicheng Yan. 2024. Automating dataset updates towards reliable and timely evaluation of large language models. In The Thirtyeight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Conference on Neural Information Processing Systems, NeurIPS. Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, et al. 2023. Kola: Carefully benchmarking world knowledge of large language models. In The Twelfth International Conference on Learning Representations. Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, 11 Dylan Slack, Qin Lyu, et al. 2024. careful examination of large language model performance on grade school arithmetic. arXiv preprint arXiv:2405.00332. Zexuan Zhong, Zhengxuan Wu, Christopher Manning, Christopher Potts, and Danqi Chen. 2023. MQuAKE: Assessing knowledge editing in language models via In Proceedings of the 2023 multi-hop questions. Conference on Empirical Methods in Natural Language Processing, pages 1568615702, Singapore. Association for Computational Linguistics. Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. 2023. Dont make your llm an evaluation benchmark cheater. arXiv preprint arXiv:2311.01964. Ruiwen Zhou, Wenyue Hua, Liangming Pan, Sitao Cheng, Xiaobao Wu, En Yu, and William Yang Wang. 2024. Rulearena: benchmark for rule-guided reasoning with llms in real-world scenarios. arXiv preprint arXiv:2412.08972. Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, and Xing Xie. 2023. Dyval: Graph-informed dynamic evaluation of large language models. arXiv e-prints, pages arXiv2309."
        },
        {
            "title": "Time period",
            "content": "Gold Nd=3 Nd=5 Nd=7 Gold Nd=3 Nd=5 Nd=7 Single-Hop Multi-Hop 2022-01-01 to 2023-01-01 2023-05-01 to 2024-08-01 1090 1089 818 1088 818 1088 818 443 941 443 939 443 443 939 Table 6: Sample sizes in the constructed AntiLeak-Bench in the experiments. Single-Hop Multi-Hop"
        },
        {
            "title": "Time period",
            "content": "2022-01-01 to 2023-01-01 2023-05-01 to 2024-08-"
        },
        {
            "title": "Gold",
            "content": "5998 7210 Nd=3 Nd=5 Nd="
        },
        {
            "title": "Gold",
            "content": "Nd=3 Nd=5 Nd=7 23163 27501 33867 40800 46033 24646 25505 40611 43926 50846 53898 61761 66957 Table 7: Average word counts of samples in the constructed AntiLeak-Bench in the experiments."
        },
        {
            "title": "Release time Knowledge cutoff time",
            "content": "Llama-2-7B Llama-2-13B Mistral-7B Vicuna-v1.5-7B Longchat-v1.5-7B Llama-3.1-8B Phi-3.5-mini Qwen-2-7B Mistral-Nemo-12B Gemma-2-9B GPT-4o-mini GPT-4o 2023-07 2023-07 2023-09 2023-07 2023-07 2024-07 2024-08 2024-06 2024-07 2024-08 2024-07 2024-07 2022-09 2022-09 2022* 2022-09 2022-09 2023-12 2023-10 2023* 2024-04 2024-06* 2023-10 2023Table 8: Release dates and knowledge cutoff dates of LLMs. * means estimated time. The workflow automatically retrieves corresponding Wikipedia articles in that language. The statistics of produced benchmarks are reported in Tables 6 and 7."
        },
        {
            "title": "B Data Quality Verification",
            "content": "We recruit three annotators (graduate students) to verify the data quality of our benchmark. Specifically, we provide 100 samples of Single-Hop Gold and 100 samples of Multi-Hop Gold; then we ask annotators to determine (i) if the provided context can sufficiently answer the question; (ii) if the given answer is accurate and can appropriately address the question given the context. Table 3 summarizes the average accuracy scores concerning Single-Hop Gold and Multi-Hop Gold. We use the Wikidata dump released on 2024-0805 as our data source. To sample relations from Wikidata, we browse the relation list from Wikidata (Vrandeˇcic and Krötzsch, 2014) 2 and manually select common relations associated with physical entities and exclude the less meaningful ones, such as those related to virtual entries (e.g., geographic coordinates and IMDB ID). The sampled relations mainly focus on common topics, such as sports, politics, and entertainment (Wu et al., 2020, 2022, 2024c,a,b). We predefine the question templates for each sampled relation, for instance, What sports team is member of? for the relation member of sports team. For more details, see the configuration files in our code. We follow simplewikidata-db 3 and extract the claims of sampled relations, qualifiers, aliases, and Wikipedia titles from the dump. To find updated knowledge, we combine the claims and their start time and end time qualifiers and then sort them by start time. We check if the object changes after the preset cutoff time and identify updated knowledge if it changes, for instance, Messis sports team changed in Figure 2. Given an entity, we employ the MediaWiki APIs 4 and use its extracted Wikipedia title to retrieve its Wikipedia article and revision history. Note that our workflow supports building multilingual benchmarks. We only need to prepare the configuration files in another language and then execute the workflow by specifying that language. 2https://www.wikidata.org/wiki/Wikidata: Database_reports/List_of_properties/all 3https://github.com/neelguha/ simple-wikidata-db 4https://www.mediawiki.org/wiki/MediaWiki"
        },
        {
            "title": "C Prompts",
            "content": "Following Bai et al. (2024), we use the following prompts for generation and multi-choice question formats mentioned in Sec. 3.4. You are given an article and question . Answer the question based on the given article as concisely as you can , using single phrase or sentence if possible . Do not provide any explanation . Article: < context > Question: < question > Answer: You are given an article , question , and four options . Select one option to answer the question based on the given article . Only give the option (A , , , or D) , and do not output any other words . Article: < context > Question: < question > < options > Answer: 14 Examples of AntiLeak-Bench We list some examples in the AntiLeak-Bench. Examples of Single-Hop Gold [ { \" question \": \" What sports team is Duncan Cowan Ferguson coach of ? \", \" answer \": [ \" Inverness Caledonian Thistle F.C.\", \" Inverness Caledonian Thistle Football Club \", \" Inverness Caledonian Thistle FC \", \" Inverness Caledonian Thistle \", \" Inverness \", \" ICTFC \", \" Caley Thistle \" ], \" context \": \" Duncan Cowan Ferguson ( born 27 December 1971) is Scottish football coach and former player who is the manager of Scottish Championship club Inverness Caledonian Thistle ... \" }, { \" question \": \" What is the position held by Leo Docherty ? \", \" answer \": [ \" Minister of State for the Armed Forces \" ], \" context \": \" Leo Docherty ( born 4 October 1976) is British politician serving as Minister of State for the Armed Forces since 26 March 2024... \" }, { \" question \": \" What sports team is Keiren Westwood member of ? \", \" answer \": [ \" Queens Park Rangers F.C.\", \" Queens Park Rangers Football Club \", \" Queens Park Rangers FC \", \" Queens Park Rangers \", \" The Hoops \", \" The Rs \", \" QPRFC \" ], \" context \": \" Keiren Westwood ( born 23 October 1984) is professional footballer who plays as goalkeeper for Queens Park Rangers . Born in England , he plays international football for the Republic of Ireland ... \" } ] 15 Examples of Multi-Hop Gold [ { \" question \": \" What is the headquarter of the sports team that Kevin Luckassen is member of ?\", \" answer \": [ \" Arad \", \" Arad , Romania \" ], \" context \": [ \" Passage 1: Kevin Luckassen ( born 27 July 1993) is Dutch professional footballer who plays as forward for Romanian Liga club UTA Arad ... \", \" Passage 2: Asociatia Fotbal Club UTA Arad () , commonly known as UTA Arad or simply UTA ( Uzina Textila Arad (\" Textiles Factory of Arad \") ) , is Romanian professional football club based in the city of Arad , Romania , which competes in the Liga ... \" ] }, { \" question \": \" Who is the spouse of the officeholder of President of Finland ?\", \" answer \": [ \" Suzanne Innes - Stubb \", \" Suzanne Innes \", \" Suzanne Stubb \", \" Suzanne Elizabeth Innes - Stubb \", \" Suzanne Elizabeth Innes \" ], \" context \": [ \" Passage 1: Cai - Goran Alexander Stubb ( born 1 April 1968) is the 13 th and current President of Finland , having won the 2024 presidential election . He previously served as Prime Minister of Finland from 2014 to 2015... \", \" Passage 2: Suzanne Elizabeth Innes - Stubb ( born 25 January 1970) is British - Finnish attorney and the wife of Alexander Stubb , President of Finland . She was the first person of overseas origin to become the spouse of the President of Finland ... \" ] }, { \" question \": \" What is the country of citizenship of the head coach of the sports team that Marquez Reshard Valdes - Scantling is member of ?\", \" answer \": [ \" United States of America \", \" United States \", \" American \", \" Americans \" ], \" context \": [ \" Passage1 : Marquez Reshard Valdes - Scantling ( born October 10 , 1994) is an American football wide receiver for the Kansas City Chiefs of the National Football League ( NFL ). He played college football at NC State and South Florida , and was drafted by the Packers in the fifth round of the 2018 NFL Draft .\", \" Passage 2: Andrew Walter Reid ( born March 19 , 1958) is an American football coach who is the head coach for the Kansas City Chiefs of the National Football League ( NFL ). Reid was previously the head coach of the Philadelphia Eagles , position he held from 1999 to 2012. From 2001 to 2012 , he was also the Eagles ' executive vice president of football operations , making him the team 's general manager . He is the only NFL coach to win 100 games and appear in four consecutive conference championships with two different franchises .\" ] } ]"
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "National University of Singapore",
        "Shanghai Jiao Tong University",
        "University of Arizona",
        "University of California, Santa Barbara"
    ]
}