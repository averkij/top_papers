{
    "paper_title": "Change State Space Models for Remote Sensing Change Detection",
    "authors": [
        "Elman Ghazaei",
        "Erchan Aptoula"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite their frequent use for change detection, both ConvNets and Vision transformers (ViT) exhibit well-known limitations, namely the former struggle to model long-range dependencies while the latter are computationally inefficient, rendering them challenging to train on large-scale datasets. Vision Mamba, an architecture based on State Space Models has emerged as an alternative addressing the aforementioned deficiencies and has been already applied to remote sensing change detection, though mostly as a feature extracting backbone. In this article the Change State Space Model is introduced, that has been specifically designed for change detection by focusing on the relevant changes between bi-temporal images, effectively filtering out irrelevant information. By concentrating solely on the changed features, the number of network parameters is reduced, enhancing significantly computational efficiency while maintaining high detection performance and robustness against input degradation. The proposed model has been evaluated via three benchmark datasets, where it outperformed ConvNets, ViTs, and Mamba-based counterparts at a fraction of their computational complexity. The implementation will be made available at https://github.com/Elman295/CSSM upon acceptance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 0 8 0 1 1 . 4 0 5 2 : r Elman Ghazaei Faculty of Engineering and Natural Sciences (VPALab), Sabanci University, Türkiye elman.ghazaei@sabanciuniv.edu Erchan Aptoula Faculty of Engineering and Natural Sciences (VPALab), Sabanci University, Türkiye erchan.aptoula@sabanciuniv.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Despite their frequent use for change detection, both ConvNets and Vision transformers (ViT) exhibit well-known limitations, namely the former struggle to model long-range dependencies while the latter are computationally inefficient, rendering them challenging to train on large-scale datasets. Vision Mamba, an architecture based on State Space Models has emerged as an alternative addressing the aforementioned deficiencies and has been already applied to remote sensing change detection, though mostly as feature extracting backbone. In this article the Change State Space Model is introduced, that has been specifically designed for change detection by focusing on the relevant changes between bi-temporal images, effectively filtering out irrelevant information. By concentrating solely on the changed features, the number of network parameters is reduced, enhancing significantly computational efficiency while maintaining high detection performance and robustness against input degradation. The proposed model has been evaluated via three benchmark datasets, where it outperformed ConvNets, ViTs, and Mamba-based counterparts at fraction of their computational complexity. The implementation will be made available at https://github.com/Elman295/CSSM upon acceptance. Keywords Change Detection Mamba Optical remote sensing State Space Model"
        },
        {
            "title": "Introduction",
            "content": "Change detection (CD) refers to the study of changes on the Earths surface by analyzing remote sensing images captured from the same region at different time points. It has wide range of applications, including urban development planning, land cover change analysis, disaster impact assessment, and ecological monitoring [1, 2, 3, 4, 5]. The advent of deep learning and automated feature extraction has significantly enhanced the accuracy and performance of CD models. Convolutional Neural Networks (CNNs) have been widely applied to CD tasks in order to extract bi-temporal semantic features for identifying changed areas [6, 7, 8, 9]. However, they are inherently limited to capturing local features, and their inability to model long-range dependencies between pixels restricts their effectiveness in extracting complex features from multi-temporal images [10]. The advent of Vision Transformers (ViTs) [11] has offered solution to the limitations of CNNs via the multi-head attention mechanism, as it enabled ViTs to capture long-range dependencies and establish relationships between image patches [12, 13, 14, 15, 16]. However, ViTs exhibit quadratic complexity w.r.t. image size, leading to significant computational overhead, rendering them computationally inefficient for real-time and resource-constrained applications in CD. Structured State Space Models (S4) were introduced in order to address the aforementioned limitations, offering more computationally efficient alternative with linear complexity by leveraging state-space representations with specialized"
        },
        {
            "title": "Change State Space Models for Remote Sensing Change Detection",
            "content": "parameterization [17]. The Mamba architecture [18] in particular has further enhanced S4 by introducing selection mechanism that retains relevant information in an input-dependent manner, allowing for more efficient feature extraction, and leading to state-of-the-art performances, while maintaining acceptable computational efficiency levels. Although Mamba has been already applied to CD, [19, 20, 21], it has been merely utilized so far mostly as an independent feature extractor for each image separately, without accounting for the presence of irrelevant or redundant features. This lack of of explicit change-related representation modeling can potentially lead to the inclusion of redundant information. This limitation highlights the need for more targeted approach that directly captures and enhances change-specific features while suppressing irrelevant variations. In this article, the Change State Space Model (CSSM) is proposed as specialized architecture for remote sensing CD tasks. The CSSM enhances the Mamba structure by refining its feature extraction process to focus exclusively on relevant features associated with changed areas while eliminating irrelevant information. To achieve this, an L1 distance-based approach is employed, incorporating two distinct selection parameters to effectively isolate and extract features corresponding to regions where changes have occurred. By integrating this targeted selection mechanism, CSSM improves the accuracy and efficiency of change detection, addressing limitations in existing state-space models. Since CSSM focuses exclusively on the targeted regions where changes have occurred, the need for parameters is significantly reduced. As result, CSSM achieves state-of-the-art performance as well as robustness levels against input degradation, while utilizing only fraction of the parameters required by conventional approaches; specifically 5, 12.5, and 21.25 fewer parameters than the recently introduced MambaBCD-tiny, -small, and -base models [20]. The main contribution presented in this article is new Mamba based model, specifically designed for optical remote sensing change detection, by incorporating targeted feature extraction mechanism that focuses exclusively on relevant change-related features while discarding irrelevant information. Additionally, L1 distance-based selection mechanism with two distinct selection parameters is introduced to effectively extract features corresponding to regions where changes have occurred. The proposed model leads to significant reduction in terms of computational complexity, while achieving state-of-the-art performance and robustness against ConvNets, ViTs and alternative Mamba based counterparts across multiple benchmark datasets. The remainder of this article is organized as follows: Section 2 reviews related work, while Section 3 details the CSSM architecture. The experimental results and discussion are provided in Section 4 and Section 5 is dedicated to concluding remarks."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 ConvNet based CD Methods The advent of deep learning has had catalytic effect on CD performance, similarly to other remote sensing applications. In more detail, CNNs have been widely and rapidly adopted due to their ability to automatically learn features from data [22, 23]. major focus has been placed on bi-temporal image (preand post-event) feature fusion. Along this direction, U-Net-based architectures were proposed by Daudt et al. [6], where three fusion strategies were explored: early fusion before feature extraction, mid-fusion via feature concatenation, and mid-fusion via feature subtraction between preand post-event images. To better capture temporal dependencies, hybrid model combining CNNs and Long Short-Term Memory (LSTM) networks [24] was further proposed by Chen et al. [8], allowing both spatial and temporal patterns to be modeled. Feature learning was enhanced by Zhang et al. [9], where deep supervision was introduced into fusion network by applying loss functions to intermediate layers, encouraging better feature representation. Moreover, densely connected Siamese network, SNUNet, was proposed by Fang et al. [7], in which bi-temporal images were processed through parallel branches with shared weights and connected by dense skip connections for improved comparison. In more recent studies, attention mechanisms and prior knowledge have been incorporated to guide CD. Namely, Hierarchical Attention Network was proposed by Han et al. [25], where attention modules were used to highlight both local and global features. Building on this, Change Guiding Network was introduced [26], in which change priors were used to direct the learning process toward more relevant regions. Finally, Spatiotemporal Enhancement and Interlevel Fusion Network was proposed by Huang et al. [27], where multi-level spatial and temporal features were integrated to improve detection performance in complex environments. 2.2 Transformer-Based CD Methods The success of ViTs [28] in computer vision tasks [29, 30, 31], thanks mostly to their capacity to model long-range dependencies, has led to their rapid application to CD in remote sensing. In particular, Transformer-based Siamese network was proposed by Bandara et al. [12], where bi-temporal remote sensing images were compared through parallel branches using shared Transformer modules to highlight changes. In another method by Chen et al. [10], CNNs"
        },
        {
            "title": "Change State Space Models for Remote Sensing Change Detection",
            "content": "were first used to convert image patches into semantic tokens, which were then processed by Transformer encoders and decoders to generate change maps. To combine the strengths of CNN-based localization and Transformer-based global modeling, hybrid model called TransUNetCD was introduced by Li et al. [15], in which U-Net structure was enhanced with Transformer blocks to better capture both spatial and temporal features. different approach was adopted by Zhang et al. [13], where fully Transformer-based architecture was used. In this model, the Swin Transformer [32] was employed to extract both local and global representations, leading to improved performance in complex scenes. More recently, the Cross-Temporal Difference Transformer was proposed by Zhang et al. [14], where cross-temporal relations between image pairs were explicitly modeled using attention mechanisms to better capture subtle and meaningful differences. Through these studies, it has been demonstrated that Transformer-based architectures can provide powerful tools for remote sensing CD by effectively capturing contextual dependencies across time and space. 2.3 Mamba-Based CD Methods Several Mamba-based models have been proposed for remote sensing CD, leveraging its capacity to model longrange dependencies with linear computational complexity [18]. In particular, ChangeMamba [20] introduced three foundational architectures with Mamba blocks in both encoder and decoder stages, using shared weights and fusion module; however, it symmetrically processes both temporal inputs and lacks mechanisms to focus specifically on changed regions, resulting in redundant computation and weak change localization. Frequency-Enhanced Mamba[33] on the other hand, integrates frequency-domain information to better capture subtle variations, but it still models all spatial regions equally, without emphasizing areas of actual change. Moreover, CDMamba [19] adopts dual-stream architecture to highlight temporal differences, yet does not incorporate any region-aware or change-guided attention, limiting its effectiveness in terms of focusing on informative areas. ConMamba [21] blends CNN-based spatial feature extraction with Mamba-based temporal modeling for better efficiency, but similarly lacks an explicit change-detection mechanism, treating both changed and unchanged regions uniformly. RS-Mamba[34] introduces low-rank state-space model to reduce computational cost while maintaining long-range dependency capture, but it still lacks region-specific attention, leading to inefficient use of resources in detecting change. And finally CWmamba[35] was proposed as CNN and Mamba joint architecture to enhance spatial-temporal feature learning; however, while this hybrid model improves representation capacity, it still does not structurally differentiate changed from unchanged regions during modeling, leading to inefficient attention allocation and limited change localization precision. Overall, these methods utilize Mamba primarily as general-purpose feature extractor and fail to structurally prioritize changed regions, resulting in high parameter counts and inefficient use of modeling capacity. In contrast, the proposed CSSM directly modifies the state space model to focus on changed areas, enabling targeted representation learning and enhanced performance for resource-constrained CD scenarios."
        },
        {
            "title": "3 Proposed method",
            "content": "3.1 Architecture overview The proposed architecture  (Fig. 1)  admits two input images corresponding to preand post-event scenes, and consists of three core components: lightweight CNN encoder, CSSM blocks  (Fig. 2)  , and lightweight decoder. The encoder integrates convolutional layers with the ReLU activation function and max pooling for feature map downsampling. The CSSM blocks are composed of the proposed SSM-based block and the decoder incorporates convolutional layers, transposed convolutional layers, and unmax pooling for feature map upsampling, ultimately generating binary change prediction maps. Image pairs are initially concatenated and processed by the lightweight CNN-based encoder. The extracted features are then divided into two parts to separate the representations of the preand post-event images. For instance, if the feature maps have size of RCH , where denotes the number of channels (or feature maps), represents the height, and represents the width of each feature map, they are split into two halves along the channel dimension, resulting in F1 RCH such that + = C. Subsequently, the separated features are fed into the CSSM blocks, which extract only the relevant features while discarding redundant and irrelevant information. Finally, the lightweight decoder reconstructs the feature maps and generates accurate binary change masks. and F2 RCH 3.2 Light-weight CNN Encoder The light-weight convolutional encoder comprises four sequential convolutional blocks, each followed by max pooling operation. Each convolutional block consists of two convolutional layers, each of which employs kernel size of 3 3 with stride of 1 and padding of 1. These layers are followed by batch normalization [36] to stabilize training and"
        },
        {
            "title": "Change State Space Models for Remote Sensing Change Detection",
            "content": "Figure 1: Illustration of the proposed framework. The CSSM block is incorporated between the light-weight encoder and decoder to extract only the relevant and target features. Blue, purple, yellow, green, and red denote respectively the convolutional block, transposed convolutional block, max pooling, unmax pooling, and concatenation. The dotted arrows indicate skip connections. and stands for Separation and Concatenation, respectively. The numbers in the blocks indicate how the number of feature channels (i.e. filters) changes through the convolutional and transposed convolutional layers. ReLU activation function to introduce non-linearity. The transformation in each convolutional block can be expressed as: Fl = σ(BN (Wl Fl1 + bl)) (1) where Fl represents the feature maps at layer l, Wl and bl denote the learnable weights and biases of the convolutional filters, BN () represents batch normalization, σ() is the ReLU activation function, and denotes the convolution operation. Following each convolutional block, max pooling operation with kernel size of 2 2 and stride of 2 is applied, reducing the spatial dimensions of the feature maps while retaining essential information. 3.3 CSSM Block In the CSSM block  (Fig. 2)  , three projection parameters are utilized: one for extracting knowledge from pre-event features, one for extracting features from post-event features, and the last for preparing the output. The L1 distance between preand post-event projected features is used to extract only the changed features, and the model focuses solely on the changed regions in the bi-temporal images, without considering irrelevant features. Since the proposed CSSM focuses exclusively on changed areas, high accuracy can be achieved with reduced number of parameters. This is due to the fact that the method specifically targets the relevant features related to the changes, thereby eliminating the need for processing irrelevant features. Consequently, the model operates more efficiently, requiring fewer parameters while maintaining robust performance levels. 3.3.1 State Space Model (SSM) Mamba draws inspiration from linear time-invariant systems, which map 1-D function or sequence x(t) to an output y(t) via hidden state h(t) RN . These systems are formulated as linear ordinary differential equations: (cid:26)ht = Aht1 + Bxt yt = Cht 4 (2)"
        },
        {
            "title": "Change State Space Models for Remote Sensing Change Detection",
            "content": "Figure 2: Illustration of the proposed CSSM block. Input features from bi-temporal images (Fpre and Fpost) are first normalized using RMSNorm, followed by linear transformation and depthwise convolution (DWConv). The processed features are then passed through CSSM, where shared weights are utilized. The outputs undergo activation through the SiLU function before being combined via element-wise multiplication and summation, resulting in the final feature representations (F pre and post). where RN is the evolution parameter, and RN 1, R1N are the projection parameters. The Mamba model essentially serves as discrete version of this continuous system via the zero-order hold (ZOH) method: (cid:40) = exp(A) = (A)1(exp(A) I)B where denotes the discretization operator; after the ZOH discretization step, Mamba becomes: (cid:26)ht = Aht1 + Bxt yt = Cht (3) (4) 3.3.2 Change State Space Model (CSSM) The proposed CSSM is inspired by the Mamba architecture. However, CSSM differs in that it accepts two inputs, as opposed to the single input of the original Mamba architecture. The first input is designated for pre-event features (xt), while the second input is for post-event features (x t). The continuous form of the CSSM is presented in: ht = Aht1 + Bx yt = Cht + Dxt = Cht + Dx t Bxt (5) Bxt, enabling the detection of changed regions, since Bx where RN is the evolution parameter, and RN 1, RN 1, RN 1, RN 1, R1N , R1N are the projection parameters, and . stands for L1 distance. The input projection was modified by incorporating Bx extracts relevant features from post-event images, while Bxt extracts relevant features from pre-event images. It is assumed that subtracting the extracted post-event features from the pre-event features would yield the changed features. Moreover, the output projection was modified by incorporating Cht + Dxt and Cht + Dx to integrate relevant features from xt and into the outputs yt and t. This modification enables residual effects to directly propagate information from the input to the output, allowing for deeper model architecture and enhancing the overall representational capacity of the proposed approach. In addition, ZOH discretization (Eq. (3)) is also employed to transform the continuous system into its discrete counterpart. After applying ZOH discretization, the CSSM formula becomes: ht = Aht1 + Bx yt = Cht + Dxt = Cht + Dx Bxt (6)"
        },
        {
            "title": "Change State Space Models for Remote Sensing Change Detection",
            "content": "Table 1: Information about the datasets used in the experiments. Dataset SYSU-CD [40] LEVIR-CD + [41] WHU-CD [42] Data collection site Hong Kong Texas Christchurch Number of image pairs 20,000 985 1 Size 256 256 1024 1024 32, 508 15, 354 Resolution (m) 0.5 0.5 0. 3.3.3 CSSM Block overview Pre-event and post-event features are received as inputs, following which RMSNorm [37] is applied to normalize the features. Subsequently, shared-weight linear block is utilized, which performs linear transformation. This is followed by the application of Depthwise convolution [38] and the CSSM module, which is employed to extract the changed features. The output of the CSSM module is then multiplied by the SiLU activation function [39] applied to the linear outputs. Finally, residual connection is incorporated to sum the original images with the processed images, ensuring the preservation of essential information while enhancing feature representation. 3.4 Light-weight Decoder The light-weight CNN-based decoder consists of series of layers aimed at generating the final output, utilizing combination of max unpooling, convolution, and transposed convolution operations. The decoder employs four max unpooling blocks that upsample feature maps by reversing the pooling operations from the encoder stage, leveraging the stored indices. Following each max unpooling, convolutional layers are applied to refine the feature maps, utilizing kernel size of 3 3 and ReLU activation function to introduce non-linearity. Transposed convolution layers are then used for upsampling the feature maps, with the output of each block progressively merged with the corresponding encoder feature maps (via concatenation), ensuring spatial consistency across layers. Finally, convolutional layer is applied to generate the prediction mask. 3.5 Loss Function In terms of loss function, the cross-entropy function (LCE) was utilized to optimize the model parameters, while the Lovász-Softmax loss [43] (Llov) was employed in order to address the issue of imbalance in terms of changed and unchanged pixels counts as per [20]: LCE = (cid:88) yi log(ˆyi) (cid:88) Jk (mk(F )) Llov = 1 (7) (8) kK where denotes the set of classes (excluding background if applicable), and RHW represents the predicted logits before the softmax operation. For each class K, the error vector is defined as mk(F ) = 1 yk pk(F ), where yk is the ground truth binary mask for class k, and pk(F ) is the predicted probability for that class. The Lovász extension of the Jaccard loss for class is denoted by Jk . The final loss function Lseg of the proposed model is: Lseg = LCE + Llov (9)"
        },
        {
            "title": "4 Experiments and Discussion",
            "content": "4.1 Datasets 4.1.1 SYSU-CD [40] SYSU-CD is large-scale binary change detection dataset containing 20,000 pairs of high-resolution (0.5 m/pixel) aerial images collected from Hong Kong between 2007 and 2014. It captures diverse urban and coastal transformations, including high-rise construction, infrastructure development, suburban expansion, groundwork modifications, vegetation changes, road extensions, and sea-related construction. The dataset is specifically designed for deep learning applications and is partitioned into training, validation, and test sets following 6:2:2 ratio, with each sample consisting of 256 256 pixel image patches."
        },
        {
            "title": "Change State Space Models for Remote Sensing Change Detection",
            "content": "4.1.2 LEVIR-CD+ [41] LEVIR-CD+ is an extended version of the LEVIR-CD dataset, designed for binary change detection tasks with higher diversity and complexity. It consists of 985 pairs of high-resolution (0.5 m/pixel) remote sensing images, each measuring 1024 1024 pixels, covering various urban and suburban regions. The dataset captures wide range of land-use changes, including building construction and demolition, road expansion, vegetation changes, and other infrastructure developments. The data is partitioned into training, validation, and test sets following the split protocol outlined in [20]. 4.1.3 WHU-CD [42] WHU-CD is high-resolution remote sensing dataset designed for binary change detection tasks, focusing on urban development and land-use transformations. It consists of aerial image pairs with resolution of 0.3 m/pixel, covering various change scenarios such as building construction and demolition, road expansion, and vegetation changes. An area of 20.5 km2 was covered in the 2012 aerial images, capturing 12,796 buildings, whereas the 2016 images recorded an increase to 16,077 buildings within the same region, indicating notable urban expansion over the four-year period. The dataset provides accurately annotated change maps, making it valuable benchmark for evaluating deep learning-based change detection models. In accordance with the official protocol [42], the dataset is partitioned into training and testing areas for experimental evaluations. Datasets details are summarized in Table 1."
        },
        {
            "title": "4.2 Setup",
            "content": "4.2.1 Training details The Adam optimizer [44] was employed for model training. For all datasets, the initial learning rate was uniformly set to 0.001, and the StepLR learning scheduler was applied with step size of 10. In the experimental configuration of CSSM, the training epochs were set to 100. The same learning rate and number of training epochs were maintained for all other comparison methods. All experiments were conducted on NVIDIA GeForce RTX 4090 GPU. 4.2.2 Evaluation Metrics In order to address the extensive body of research in the field of CD, five key evaluation metrics were selected for performance assessment: Precision, Recall, F1-score, Mean Intersection over Union (mIoU), and Overall Accuracy (OA). The metrics were chosen for their ability to provide well-rounded analysis of model performance, covering aspects such as classification accuracy, the balance between false positives and false negatives, and the overall agreement between predicted and ground truth changes. Their definitions are provided below: Precision = P + Recall = P + Precision Recall Precision + Recall F1 = 2 IoU = P + + P + P + + + OA = (10) (11) (12) (13) (14) where TP, TN, FP and FN represent the number of true positives, the number of true negatives, the number of false positives and the number of false negatives respectively. 4.3 Comparison against existing approaches The proposed CSSM has been compared against wide range of recent CD methods based on CNNs, Transformers and of course Mamba architectures; namely: for CNNs, FC-EF [6], FC-Siam-Diff [6], FC-Siam-Conc [6], SiamCRNN [8] (utilizing ResNet architectures ranging from ResNet-18 to ResNet-101 [45]), SNUNet [7], DSIFN [9], HANet [25], CGNet [26], and SEIFNet [27], for transformers, ChangeFormer (versions 16) [12], BIT (with ResNet-18 to ResNet-101 [45] as the backbone for feature extraction) [10], TransUNetCD [15], SwinSUNet [13], and CTDFormer [14] and for Mamba, all three versions of ChangeMamba [20] (tiny, small, and base), CDMamba [19], RS-Mamba [34], FE-Mamba [33], ConMamba [21], and CWMamba [35]."
        },
        {
            "title": "Change State Space Models for Remote Sensing Change Detection",
            "content": "Table 2: Results of CD experiments with the highest values highlighted in red and the second highest in blue. LEVIR-CD + WHU-CD SYSU-CD ConvNet based approaches Method Rec Pre OA F1 IoU Rec Pre OA F1 IoU Rec Pre OA F1 IoU FC-EF [6] FC-Siam-Diff [6] FC-Siam-Conc [6] SiamCRNN-18 [8] SiamCRNN-34 [8] SiamCRNN-50 [8] SiamCRNN-101 [8] DSIFN [9] SNUNet [7] HANet [25] CGNet [26] SEIFNet [27] ChangeFormerV1 [12] ChangeFormerV2 [12] ChangeFormerV3 [12] ChangeFormerV4 [12] ChangeFormerV5 [12] ChangeFormerV6 [12] BIT-18 [45] BIT-34 [45] BIT-50 [45] BIT-101 [45] TransUNetCD [15] SwinSUNet [13] CTDFormer [14] MambaBCD-Tiny [20] MambaBCD-Small [20] MambaBCD-Base [20] ConMamba [21] CDMamba [19] RS-Mamba [34] CWMamba [35] FE-Mamba [33] CSSM (Ours) 71.77 74.02 78.49 84.25 83.88 81.61 80.96 84.36 78.73 75.53 86.02 81.86 77.00 81.32 79.97 76.68 77.96 78.57 80.86 80.96 81.84 81.20 84.18 85.85 80.03 87.26 86.49 87.57 - 81.00 82.19 85.51 - 91.74 69.12 81.49 78.39 81.22 82.28 85.39 85.56 83.78 71.07 79.70 81.46 84.83 82.18 79.10 81.34 75.07 78.50 67.66 83.76 85.87 85.02 83.91 83.08 85.34 80.58 88.82 89.17 89.24 - 85.11 79.67 88.97 - 93. 97.54 98.26 98.24 98.56 98.61 98.68 98.67 98.70 97.83 98.22 98.63 98.66 98.38 98.36 98.44 98.01 98.23 97.60 98.58 98.68 98.67 98.60 98.66 98.92 98.40 99.03 99.02 99.06 - 98.65 98.42 98.98 - 98.69 70.42 77.57 78.44 82.71 83.08 83.46 83.20 84.07 74.70 77.56 83.68 83.32 79.51 80.20 80.65 75.87 78.23 72.71 82.28 83.34 83.40 82.53 83.63 85.60 80.30 88.04 87.81 88.39 - 83.01 80.91 87.21 - 92. 54.34 63.36 64.53 70.52 71.05 71.61 71.23 72.52 59.62 63.34 71.94 71.41 86.33 84.69 87.72 90.48 89.10 91.45 90.45 83.45 87.36 88.30 90.79 90.66 83.50 90.86 84.02 91.56 93.88 86.70 87.79 97.46 88.04 88.01 94.47 91.93 98.87 99.13 98.94 99.34 99.39 99.30 99.19 99.31 99.10 99.16 99.48 99.36 Vision transformer based approaches 65.98 66.94 67.58 61.12 64.24 57.12 69.90 71.44 71.53 70.26 71.86 74.82 67. 84.30 83.41 85.55 84.85 84.87 81.90 90.36 90.10 90.33 90.24 90.50 92.03 85.37 90.80 88.77 88.25 90.09 90.20 85.49 90.30 89.14 89.70 89.83 85.48 94.08 92.23 99.11 99.00 99.05 99.10 99.11 98.83 99.29 99.23 99.26 99.27 99.09 99.50 99.20 Mamba based approaches 78.63 78.27 79.20 - 70.95 67.95 77.32 - 86.63 91.94 92.29 92.23 91.28 92.01 90.24 - 93.69 94. 94.76 95.90 96.18 94.79 95.58 95.50 - 96.74 95.75 99.52 99.57 99.58 99.39 99.51 99.44 - 99.62 99.12 84.89 87.67 85.83 91.02 91.42 90.57 89.10 89.91 87.70 88.16 92.59 91.29 87.43 86.00 86.88 87.39 87.45 83.66 90.33 89.62 90.01 90.04 87.79 93.04 88.67 93.33 94.06 94.19 93.01 93.76 92.79 - 95.19 94.98 73.74 78.04 75.18 83.51 84.20 82.76 80.34 81.67 78.09 78.82 86.21 83. 77.67 75.45 76.80 77.61 77.70 71.91 82.37 81.19 81.84 81.88 78.44 87.00 79.65 87.49 88.79 89.02 86.89 88.26 86.55 - - 90.80 75.17 75.30 76.75 76.83 76.85 78.40 80.48 82.02 72.21 76.14 74.37 78.29 75.82 75.62 75.24 77.90 74.00 72.38 76.42 74.63 77.90 75.58 77.73 79.75 75.53 79.59 78.25 80.31 - 77.06 76.68 80.27 79.96 82.46 76.47 76.28 73.67 84.80 85.13 83.41 80.40 75.83 74.09 78.71 86.37 78. 79.65 78.14 79.46 79.14 77.26 81.70 84.85 82.40 81.42 83.64 82.59 83.50 84.00 83.06 87.99 86.11 - 81.19 79.17 88.82 88.41 85.10 88.69 88.65 88.05 91.29 91.37 91.23 90.77 89.59 87.49 89.52 91.19 89.86 89.73 89.26 89.57 90.12 88.73 89.67 91.22 90.26 90.60 90.76 90.88 91.51 90.06 91.36 92.35 92.30 - 90.38 89.74 92.96 92.80 88.65 75.81 75.79 75.18 80.62 80.78 80.83 80.44 78.80 73.14 77.41 79.92 78. 77.69 76.86 77.29 78.81 75.60 76.76 80.41 78.32 79.62 79.41 80.09 81.58 78.08 81.29 82.83 83.11 - 79.07 77.91 84.33 83.97 83.66 61.04 61.01 60.23 67.54 67.75 67.82 67.28 65.02 57.66 63.14 66.55 64.54 63.52 62.42 62.99 65.03 60.77 62.29 67.24 64.37 66.14 65.84 66.79 68.89 64.04 68.48 70.70 71.10 - - - 72.90 - 72.95 For the methods whose implementation is unavailable, we directly use the accuracy reported in their original paper, and - is inserted in the table when the original paper does not provide metric value. 4.4 Results and Discussion The CD performance of the proposed CSSM architecture and of its counterparts against three benchmark datasets are shown in Table 2. In more detail, Table 2 contains the performance comparison of various CD methods on the LEVIR-CD+, WHU-CD and SYSU-CD datasets, categorized into CNN-based, Transformer-based, and Mamba-based approaches. Regarding the LEVIR-CD+ dataset, among CNN-based models, DSIFN [9] achieves the highest F1-score (84.07) and IoU (72.52), indicating competitive performance within this category. Transformer-based methods show further improvements, with SwinSUNet [13] attaining the highest F1-score (85.60) and IoU (74.82), surpassing most CNN-based approaches. The Mamba-based models exhibit superior results, with MambaBCD-base [20] reaching an F1-score of 88.39 and IoU of 79.20, outperforming other Mamba variants such as CDMamba. Notably, RS-Mamba, FE-Mamba, and CW-Mamba demonstrate comparable effectiveness, with CW-Mamba attaining an F1-score of 87.21 and an IoU of 77.32. The proposed CSSM model further advances the state of the art by attaining the best performance, with an F1-score of 92.39 and IoU of 86.63, underscoring the strength of state space models for remote sensing change detection. On the WHU-CD dataset, DSIFN records the highest precision (97.46) among CNN-based models, while CGNet obtains the highest recall (90.79). SwinSUNet again stands out among Transformer models, achieving the highest F1-score (93.04) and IoU (87.00). The Mamba-based models deliver consistently high results, among the Mamba-based methods, FE-Mamba is observed to achieve the highest F1-score and overall accuracy (OA), with values of 95.19 and 99.62, respectively. Although CDMamba and RS-Mamba attain F1-scores of 93.76 and 92.79, respectively, they do"
        },
        {
            "title": "Change State Space Models for Remote Sensing Change Detection",
            "content": "(a) Gaussian Blur (b) Gaussian Noise Figure 3: Robustness of different models against degraded data on the WHU-CD dataset. not surpass the performance of MambaBCD-base, which achieves an F1-score of 94.19. The proposed CSSM model demonstrates further advancements by exceeding the state-of-the-art methods in terms of Recall and IoU, achieving values of 94.23 and 90.80, respectively. For the SYSU-CD dataset, CNN-based methods demonstrate balanced performance, with SiamCRNN-50 achieving the best F1-score (80.83) and IoU (67.82) in its category. Transformer-based models, particularly SwinSUNet, yield higher accuracy, with an F1-score of 81.58 and IoU of 68.89. Mamba-based models outperform both CNN and Transformer approaches, with CDMamba, RS-Mamba, and CW-Mamba delivering consistent results. Among them, CWMamba obtains the highest F1-score (84.33), closely followed by FE-Mamba. The proposed CSSM again achieves the highest performance, reaching Recall of 82.46 and IoU of 72.95, reaffirming the advantages of state space models in modeling long-range dependencies and complex spatio-temporal patterns. 4.5 Computational cost analysis As the proposed method focuses solely on the changed regions, it is able to achieve high performance with fraction of the parameters of its counterparts. Specifically, network parameter counts and floating-point operations (FLOPs) are used to compare our model with CNN, Transformer, and Mamba-based methods in Table 3. CNNs are observed to exhibit wide range of complexity, with parameters varying from 1.35M (FC-EF) to 63.44M (SiamCRNN-101), while GFLOPs reach up to 329.58 (CGNet). Transformer-based architectures display significant variability, with ChangeFormer models showing substantial increase in computational cost, particularly in later versions, where GFLOPs exceed 800. Among transformer-based methods, BIT models are found to achieve moderate efficiency, with parameter counts ranging from 11.50M to 33.27M. Mamba-based methods also present diverse spectrum of designs. MambaBCD variants scale from Tiny (17.13M, 45.74 GFLOPs) to Base (84.70M, 179.32 GFLOPs). ConMamba maintains moderate profile (34.18M, 85.43 GFLOPs), while CDMamba shows low parameter count (10.99M) but high computational demand (259.49 GFLOPs), indicating compute-intensive structure. RSMamba achieves better balance with 27.90M parameters and 50.20 GFLOPs. FE-Mamba presents substantial architecture (51.33M, 136.90 GFLOPs), while CWMamba, despite having the highest parameter count (373.62M), surprisingly records very low FLOPs (37.32). The proposed method, CSSM, stands out by achieving the lowest computational complexity among all compared models, with only 4.34M parameters and 5.10 GFLOPs. This demonstrates significant improvement in efficiency, enabling competitive performance with minimal resource requirements. 4.6 Ablation Experiments 4.6.1 Layer Selection Table 4 presents the results of ablation experiments conducted on the WHU-CD dataset to assess the effect of the number of CSSM layers on model performance. It can be observed that performance improves with an increasing number of layers, reaching peak values with the 5-layer configuration. Specifically, the 5-layer CSSM achieved the highest recall (95.37%), precision (95.91%), F1 score (95.62%), and IoU (91.21%), while also maintaining strong overall accuracy of 99.21%. The 4-layer configuration also yielded competitive results, obtaining the best overall"
        },
        {
            "title": "Change State Space Models for Remote Sensing Change Detection",
            "content": "Table 3: Computational complexity comparison. Type Method Params (M) GFLOPs"
        },
        {
            "title": "CNN",
            "content": "Transformer Mamba FC-EF [6] FC-Siam-Diff [6] FC-Siam-Conc [6] SiamCRNN-18 [8] SiamCRNN-34 [8] SiamCRNN-50 [8] SiamCRNN-101 [8] DSIFN [9] SNU-Net [7] HANet [25] CGNet [26] SEIFNet [27] ChangeFormerV1 [12] ChangeFormerV2 [12] ChangeFormerV3 [12] ChangeFormerV4 [12] ChangeFormerV5 [12] ChangeFormerV6 [12] BIT-18 [45] BIT-34 [45] BIT-50 [45] BIT-101 [45] TransUNetCD [15] SwinSUNet [13] CTDFormer [14] MambaBCD-Tiny [20] MambaBCD-Small [20] MambaBCD-Base [20] ConMamba [21] CDMamba [19] RS-Mamba [34] CWMamba [35] FE-Mamba [33] CSSM 1.35 1.35 1.54 18.85 28.96 44.45 63.44 35.73 10.21 2.61 33.68 39. 29.84 24.30 30.40 33.61 55.27 41.03 11.50 21.61 24.28 33.27 28.37 39.28 3.85 17.13 49.94 84.70 34.18 10.99 27.9 373.62 51.33 4.34 14.13 18.66 21.07 86.64 113.28 185.30 224.30 329.03 176.36 70.68 329.58 167.75 46.62 44.54 43.68 852.53 841.08 811.15 106.14 190.83 218.04 380.62 244.54 43.50 303.77 45.74 114.82 179.32 85.43 259.49 50.20 37.32 136.90 5.10 accuracy (99.25%) and notable values across other metrics, including recall of 91.89%, precision of 93.01%, F1 score of 94.81%, and IoU of 90.23%. Although deeper configurations (68 layers) maintained relatively high performance, no further improvements were observed, suggesting that deeper models may introduce additional parameters without substantial gains. These findings indicate that increasing model depth enhances performance up to certain point, after which diminishing returns are encountered. 4.6.2 Robustness Against Degraded Input Data Furthermore, the robustness of the proposed method has been evaluated against degraded input data. During the inference phase, various levels of Gaussian blurring and Gaussian noise were introduced to the input data. The impact of these degradations on the F1 score across different methods is illustrated in Fig. 3, via the WHU-CD dataset."
        },
        {
            "title": "Change State Space Models for Remote Sensing Change Detection",
            "content": "Table 4: Results of Ablation Experiments on WHU-CD Method Rec Pre OA IoU CSSM 1-layer CSSM 2-layer CSSM 3-layer CSSM 4-layer CSSM 5-layer CSSM 6-layer CSSM 7-layer CSSM 8-layer 90.05 89.87 90.79 91.89 95.37 90.10 91.25 90.79 91.77 91.93 91.84 93.01 95.91 93.81 93.86 92.50 98.32 98.71 98.63 99.25 99.21 98.96 98.97 98.31 90.89 90.61 91.54 94.81 95.62 92.08 92.54 92. 84.36 84.56 84.96 90.23 91.21 85.61 86.34 85.52 (a) Pre-event (b) Post- (c) GT (d) FC-Conc (e) SNUNet (f) event Ch.Former (g) BIT (h) Ch.Mamba (i) CSSM Figure 4: Qualitative results on the WHU-CD dataset, with misdetected areas represented in red and missed areas shown in green. CNNs, including FC-Siam-Conc and SNUNet, exhibit notable sensitivity to both Gaussian blur and Gaussian noise. Under increasing blur levels, FC-Siam-Conc experiences sharp decline from 85.83 at σ = 0.0 to 75.6 at σ = 1.5, indicating strong dependence on fine spatial details. SNUNet also shows steady performance decrease, from 87.70 to 80.4, though less severe than FC-Siam-Conc. When subjected to Gaussian noise, similar trend is observed, with FC-Siam-Conc dropping to 77.6 at noise level 15, while SNUNet decreases to 81.1. These results suggest that CNN-based architectures struggle with robustness to image degradation, particularly for models that rely heavily on local feature extraction. Transformer-based models, such as ChangeFormer and BIT, demonstrate improved resilience compared to CNNs. When Gaussian blur is introduced, ChangeFormers F1-score drops from 87.45 at σ = 0.0 to 82.21 at σ = 1.5, while BIT, which starts at 90.33, maintains relatively stable performance of 84.57. Under Gaussian noise, ChangeFormers performance decreases to 80.21 at noise level 15, whereas BIT drops to 83.57, showing controlled degradation. These results highlight the ability of self-attention mechanisms to mitigate the impact of degradation by preserving long-range dependencies, making transformers more robust than CNNs in handling noisy and blurred images. Under increasing levels of Gaussian blur, CSSM demonstrated strong robustness, with the F1-score decreasing from 94.98 to 91.64 as σ increased to 1.5. In contrast, ChangeMamba exhibited slightly larger drop, from 94.19 to 90.36. similar trend was observed under Gaussian noise, where CSSM maintained higher stability, with the F1-score reducing from 94.98 to 90.74 at noise level 15, while ChangeMamba declined from 94.19 to 89.36. These results indicate that CSSM is more resilient to both blur and noise distortions, particularly at higher severity levels. Although ChangeMamba achieved competitive performance across all conditions, CSSM consistently outperformed it in terms of robustness. This suggests that CSSM offers improved reliability in scenarios involving degraded image quality, making it strong candidate for practical change detection applications."
        },
        {
            "title": "Change State Space Models for Remote Sensing Change Detection",
            "content": "(a) Pre-event (b) Post- (c) GT (d) FC-Conc (e) SNUNet (f) event Ch.Former (g) BIT (h) Ch.Mamba (i) CSSM Figure 5: Qualitative results on the SYSU-CD dataset, with misdetected areas represented in red and missed areas shown in green. (a) Pre-event (b) Post- (c) GT (d) FC-Conc (e) SNUNet (f) event Ch.Former (g) BIT (h) Ch.Mamba (i) CSSM Figure 6: Qualitative results on the LEVIR-CD+ dataset, with misdetected areas represented in red and missed areas shown in green. 4.7 Qualitative Results 4.7.1 Change maps The ground-truth (GT) data, as well as calculated change maps, with misdetected areas (False positives) represented in red and missed areas (False negatives) shown in green are provided in Figs. 4, 5 and 6 across the WHU-CD, SYSU-CD, and LEVIR-CD+ datasets, respectively. The results suggest that the proposed approach exhibits superior performance in capturing relevant features, leading to more accurate and robust results. The visual comparison highlights the effectiveness of our model, showcasing its ability to handle complex data more efficiently than the traditional CNN and transformer-based alternatives. Furthermore, the proposed method consistently achieves higher accuracy, indicating its potential for broader applications in the relevant field. These findings emphasize the advantages of our approach, demonstrating its capability to address the challenges posed by existing methods. 4.7.2 Class Activation Maps To illustrate that our model specifically focuses on the changed regions while discarding irrelevant parts, class activation maps (CAMs) are provided in Figs. 7, 8, and 9 across the three datasets. These maps highlight the areas that the model prioritizes, demonstrating its ability to concentrate on the regions of interest. By visualizing the CAMs, it becomes"
        },
        {
            "title": "Change State Space Models for Remote Sensing Change Detection",
            "content": "evident that our model effectively distinguishes between relevant and irrelevant areas, enhancing its performance in detecting changes while minimizing the impact of extraneous information. This selective focus on the changed regions underscores the efficiency of our approach in comparison to traditional models. (a) Pre-event (b) Post- (c) GT event Class (d) Activation Maps Figure 7: Class Activation Maps of WHU-CD (a) Pre-event (b) Post- (c) GT event Class (d) Activation Maps Figure 8: Class Activation Maps of SYSU-CD (a) Pre-event (b) Post- (c) GT event Class (d) Activation Maps Figure 9: Class Activation Maps of LEVIR-CD +"
        },
        {
            "title": "5 Conclusion",
            "content": "The Change State Space Model (CSSM) was introduced in this article as novel architecture for remote sensing change detection. By leveraging the efficiency of state space models, CSSM was designed to effectively capture relevant changes between bi-temporal images while filtering out irrelevant information. This formulation allowed for the elimination of"
        },
        {
            "title": "Change State Space Models for Remote Sensing Change Detection",
            "content": "redundant network parameters, thereby reducing computational complexity without compromising detection accuracy. Extensive experiments were conducted on three benchmark datasets, where CSSM exhibited superior performance compared to CNN-based, Transformer-based, and Mamba-based approaches. The results demonstrated that high accuracy could be achieved while ensuring scalability, making CSSM promising solution for large-scale remote sensing CD application. In future work, further optimizations may be explored, such as the integration of domain generalization techniques to enhance generalization across diverse remote sensing datasets."
        },
        {
            "title": "References",
            "content": "[1] Dengsheng Lu, Paul Mausel, Eduardo Brondizio, and Emilio Moran. Change detection techniques. International Journal of Remote Sensing, 25(12):23652401, 2004. [2] Hongruixuan Chen, Cuiling Lan, Jian Song, Clifford Broni-Bediako, Junshi Xia, and Naoto Yokoya. Objformer: Learning land-cover changes from paired osm data and optical high-resolution imagery via object-guided transformer. IEEE Transactions on Geoscience and Remote Sensing, 62, 2024. [3] Hongruixuan Chen, Naoto Yokoya, Chen Wu, and Bo Du. Unsupervised multimodal change detection based on structural relationship graph representation learning. IEEE Transactions on Geoscience and Remote Sensing, 60:118, 2022. [4] Haonan Guo, Qian Shi, Andrea Marinoni, Bo Du, and Liangpei Zhang. Deep building footprint update network: semi-supervised method for updating existing building footprint from bi-temporal remote sensing images. Remote Sensing of Environment, 264:112589, 2021. [5] Guangliang Cheng, Yunmeng Huang, Xiangtai Li, Shuchang Lyu, Zhaoyang Xu, Hongbo Zhao, Qi Zhao, and Shiming Xiang. Change detection methods for remote sensing in the last decade: comprehensive review. Remote Sensing, 16(13):2355, 2024. [6] Rodrigo Caye Daudt, Bertr Le Saux, and Alexandre Boulch. Fully convolutional siamese networks for change detection. In IEEE ICIP Proceedings, pages 40634067, Athens, Greece, 2018. [7] Sheng Fang, Kaiyu Li, Jinyuan Shao, and Zhe Li. SNUNet-CD: densely connected siamese network for change detection of VHR images. IEEE Geoscience and Remote Sensing Letters, 19:15, 2021. [8] Hongruixuan Chen, Chen Wu, Bo Du, Liangpei Zhang, and Le Wang. Change detection in multisource VHR images via deep siamese convolutional multiple-layers recurrent neural network. IEEE Transactions on Geoscience and Remote Sensing, 58(4):28482864, 2019. [9] Chenxiao Zhang, Peng Yue, Deodato Tapete, Liangcun Jiang, Boyi Shangguan, Li Huang, and Guangchao Liu. deeply supervised image fusion network for change detection in high resolution bi-temporal remote sensing images. ISPRS Journal of Photogrammetry and Remote Sensing, 166:183200, 2020. [10] Hao Chen, Zipeng Qi, and Zhenwei Shi. Remote sensing image change detection with transformers. IEEE Transactions on Geoscience and Remote Sensing, 60:114, 2021. [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [12] Wele Gedara Chaminda Bandara and Vishal Patel. transformer-based siamese network for change detection. In IEEE IGARSS proceedings, pages 207210, Kuala Lumpur, Malaysia, 2022. [13] Cui Zhang, Liejun Wang, Shuli Cheng, and Yongming Li. Swinsunet: Pure transformer network for remote sensing image change detection. IEEE Transactions on Geoscience and Remote Sensing, 60:113, 2022. [14] Kai Zhang, Xue Zhao, Feng Zhang, Lei Ding, Jiande Sun, and Lorenzo Bruzzone. Relation changes matter: Cross-temporal difference transformer for change detection in remote sensing images. IEEE Transactions on Geoscience and Remote Sensing, 61:115, 2023. [15] Qingyang Li, Ruofei Zhong, Xin Du, and Yu Du. Transunetcd: hybrid transformer network for change detection in optical remote-sensing images. IEEE Transactions on Geoscience and Remote Sensing, 60:119, 2022. [16] Hongruixuan Chen, Edoardo Nemni, Sofia Vallecorsa, Xi Li, Chen Wu, and Lars Bromley. Dual-tasks siamese transformer framework for building damage assessment. In IEEE IGARSS proceedings, pages 16001603, Kuala Lumpur, Malaysia, 2022. [17] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021."
        },
        {
            "title": "Change State Space Models for Remote Sensing Change Detection",
            "content": "[18] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [19] Haotian Zhang, Keyan Chen, Chenyang Liu, Hao Chen, Zhengxia Zou, and Zhenwei Shi. CDMamba: Remote sensing image change detection with mamba. arXiv preprint arXiv:2406.04207, 2024. [20] Hongruixuan Chen, Jian Song, Chengxi Han, Junshi Xia, and Naoto Yokoya. Changemamba: Remote sensing change detection with spatio-temporal state space model. IEEE Transactions on Geoscience and Remote Sensing, 62, 2024. [21] Zhiwei Dong, Genji Yuan, Zhen Hua, and Jinjiang Li. ConMamba: CNN and SSM high-performance hybrid network for remote sensing change detection. IEEE Transactions on Geoscience and Remote Sensing, 62, 2024. [22] Maoguo Gong, Tao Zhan, Puzhao Zhang, and Qiguang Miao. Superpixel-based difference representation learning for change detection in multispectral remote sensing images. IEEE Transactions on Geoscience and Remote sensing, 55(5):26582673, 2017. [23] Hongruixuan Chen, Chen Wu, Bo Du, and Liangpei Zhang. Deep siamese multi-scale convolutional network for change detection in multi-temporal VHR images. In IEEE MultiTemp proceedings, pages 14, 2019. [24] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):17351780, 1997. [25] Chengxi Han, Chen Wu, Haonan Guo, Meiqi Hu, and Hongruixuan Chen. Hanet: hierarchical attention network for change detection with bitemporal very-high-resolution remote sensing images. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 16:38673878, 2023. [26] Chengxi Han, Chen Wu, Haonan Guo, Meiqi Hu, Jiepan Li, and Hongruixuan Chen. Change guiding network: Incorporating change prior to guide change detection in remote sensing imagery. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 16:83958407, 2023. [27] Yanyuan Huang, Xinghua Li, Zhengshun Du, and Huanfeng Shen. Spatiotemporal enhancement and interlevel fusion network for remote sensing images change detection. IEEE Transactions on Geoscience and Remote Sensing, 62:114, 2024. [28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [29] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: survey. ACM computing surveys (CSUR), 54(10s):141, 2022. [30] Yakoub Bazi, Laila Bashmal, Mohamad Al Rahhal, Reham Al Dayil, and Naif Al Ajlan. Vision transformers for remote sensing image classification. Remote Sensing, 13(3):516, 2021. [31] Abdulaziz Amer Aleissaee, Amandeep Kumar, Rao Muhammad Anwer, Salman Khan, Hisham Cholakkal, GuiSong Xia, and Fahad Shahbaz Khan. Transformers in remote sensing: survey. Remote Sensing, 15(7):1860, 2023. [32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In IEEE CVPR proceedings, pages 10012 10022, Virtual, 2021. [33] Yan Xing, Yunan Jia, Sen Gao, Jiali Hu, and Rui Huang. Frequency-enhanced mamba for remote sensing change detection. IEEE Geoscience and Remote Sensing Letters, 22, 2025. [34] Sijie Zhao, Hao Chen, Xueliang Zhang, Pengfeng Xiao, Lei Bai, and Wanli Ouyang. Rs-mamba for large remote sensing image dense prediction. IEEE Transactions on Geoscience and Remote Sensing, 2024. [35] Yingchao Liu, Guangliang Cheng, Qihang Sun, Chunpeng Tian, and Lukun Wang. CWmamba: Leveraging CNN-Mamba fusion for enhanced change detection in remote sensing images. IEEE Geoscience and Remote Sensing Letters, 22, 2025. [36] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML proceedings, pages 448456, Lille,France, 2015. [37] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. [38] François Chollet. Xception: Deep learning with depthwise separable convolutions. In IEEE CVPR proceedings, pages 12511258, Hawaii, USA, 2017. [39] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural networks, 107:311, 2018."
        },
        {
            "title": "Change State Space Models for Remote Sensing Change Detection",
            "content": "[40] Qian Shi, Mengxi Liu, Shengchen Li, Xiaoping Liu, Fei Wang, and Liangpei Zhang. deeply supervised attention metric-based network and an open aerial image dataset for remote sensing change detection. IEEE Transactions on Geoscience and Remote Sensing, 60:116, 2021. [41] Hao Chen and Zhenwei Shi. spatial-temporal attention-based method and new dataset for remote sensing image change detection. Remote Sensing, 12(10):1662, 2020. [42] Shunping Ji, Shiqing Wei, and Meng Lu. Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set. IEEE Transactions on Geoscience and Remote Sensing, 57(1):574586, 2018. [43] Maxim Berman, Amal Rannen Triki, and Matthew Blaschko. The lovász-softmax loss: tractable surrogate for the optimization of the intersection-over-union measure in neural networks. In IEEE CVPR proceedings, pages 44134421, Salt Lake, USA, 2018. [44] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [45] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE CVPR proceedings, pages 770778, Las Vegas, USA, 2016."
        }
    ],
    "affiliations": [
        "Faculty of Engineering and Natural Sciences (VPALab), Sabanci University, Türkiye"
    ]
}