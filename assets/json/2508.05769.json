{
    "paper_title": "Improving Masked Style Transfer using Blended Partial Convolution",
    "authors": [
        "Seyed Hadi Seyed",
        "Ayberk Cansever",
        "David Hart"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Artistic style transfer has long been possible with the advancements of convolution- and transformer-based neural networks. Most algorithms apply the artistic style transfer to the whole image, but individual users may only need to apply a style transfer to a specific region in the image. The standard practice is to simply mask the image after the stylization. This work shows that this approach tends to improperly capture the style features in the region of interest. We propose a partial-convolution-based style transfer network that accurately applies the style features exclusively to the region of interest. Additionally, we present network-internal blending techniques that account for imperfections in the region selection. We show that this visually and quantitatively improves stylization using examples from the SA-1B dataset. Code is publicly available at https://github.com/davidmhart/StyleTransferMasked."
        },
        {
            "title": "Start",
            "content": "Ayberk Cansever East Carolina University 1000 East Fifth Street, Greenville, NC, USA *hartda23@ecu.edu *David Hart 5 2 0 2 7 ] . [ 1 9 6 7 5 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Artistic style transfer has long been possible with the advancements of convolutionand transformer-based neural networks. Most algorithms apply the artistic style transfer to the whole image, but individual users may only need to apply style transfer to speciﬁc region in the image. The standard practice is to simply mask the image after the stylization. This work shows that this approach tends to improperly capture the style features in the region of interest. We propose partial-convolution-based style transfer network that accurately applies the style features exclusively to the region of interest. Additionally, we present network-internal blending techniques that account for imperfections in the region selection. We show that this visually and quantitatively improves stylization using examples from the SA-1B dataset. Code is publicly available at https://github.com/davidmhart/StyleTransferMasked. 1. Introduction Image style transfer aims to reproduce the aesthetic characteristics of style image, such as painting, within the structural context of content image. This ﬁeld has been deeply explored over the last decade [24, 11, 12]. These works, however, have focused on stylizing whole content image based on single style image. Also in recent years, the Segment Anything Model [6] and follow-on work [16, 24] have revolutionized segmentation tasks by allowing for single click, non-class-based detector. Developments in this model are making segmentation of semantically meaningful regions more accessible than ever before. This work explores the use of style transfer when applied to segmented regions of content image. Common image processing software such as Photoshop allow users to style transfer images with masked content, but most programs simply style transfer the whole image and alpha blend the masked content back onto the original image. This naive approach can fail to capture the proper style statistics due a) Content b) Style and Mask c) Linear Style Transfer [11] d) Ours Figure 1. Comparison of masked linear style transfer to our proposed partial-convolution-based approach. When applying style transfer for given content image (a), style, and mask (b), simply masking the output of the style transfer (c) does not provide well-stylized features in the masked region. Our approach that incorporates the mask throughout the style transfer process (d) better matches original style features in the masked region. to the disparity between the distribution of colors in the whole image compared to the masked region. Very few other works have focused on the problem of style transfer within masked region [17] and none have presented feed-forward model that does so. We present feed-forward style transfer network that can style transfer any content and masked region given particular style. This is done using partial convolutions [14] within the style transfer framework presented by Li et al. [11] without any additional ﬁne-tuning. We will demonstrate that this approach better captures style properties within the masked region when compared to state-ofthe-art methods that are masked after processing. Qualitatively, we verify this with many example outputs, such as those shown in Fig. 1. Quantitatively, we show this by running metrics such as Earth Mover Distance and Perceptual Style Loss metrics against 500 images from SA-1B dataset. In addition to these analyses, we provide many practical improvements to the stylization. First, although partial convolution proves effective for style transfer in masked regions, visible seams often appear where stylized and nonstylized areas meet. To address these border artifacts, we explore various blending techniques that smooth the border and help account for inaccuracies in the segmentation, producing more coherent and visually appealing results. Second, the partial convolution network naturally lends itself to stylizing multiple regions in parallel, combining features at network level and creating smoothly transitioning results from one stylization to another. We describe how to setup such mutli-mask, multi-style network and showcase its results. In summary, our contributions include: new partial-convolution-based style transfer network that effectively stylizes masked content in an image without additional ﬁne-tuning. Qualitative and quantitative results of the approach on 500 images from the SA-1B dataset. Blending techniques that improve the appearance of the stylization relative to the background. multi-mask and multi-style conﬁguration that stylizes in single pass. 2. Related Works 2.1. Style Transfer Modern style transfer techniques begin with the groundbreaking work of Gatys et al. [3] who presented the ﬁrst CNN-based style transfer algorithm. This approach was optimization based and operated on single image and single style at time. Others works improved the speed and control of this approach [4, 7, 18]. Later, feed-forward method was presented by Johnson et al. [5] that trained network for single style image. After this training process, any content image could be fed into the network and stylized in quick feed-forward pass. Others also improved this approach [8, 19, 21]. In 2017, style transfer was reformulated as modiﬁed image reconstruction process [12]. After standard image reconstruction network is trained (usually an autoencoder), the content image is fed into the network and the intermediate representation is altered based on the style statistics. This approach is considered state-of-the-art and has CNN [11], vision transformer [2], and even diffusion network [1, 22] implementations. In this work, the linear transformer apporach by Li et al. [11] is used because it is fully convolutional neural network, making it easy to modify the convolution layers to be partial convolutions. 2.2. Segment Anything Models Segmentation is common computer vision task. Before the Segment Anything Model (SAM) [6], modern AIbased approaches were class-based, relying on class labels to complete the segmentation [15]. Popular approaches included models like Detectron [20]. The Segment Anything Model is unique because it was trained in an iterative fashion that does not require class labels, thus allowing it to segment objects with high accuracy, even if they have not been seen by the network before or have no semantic meaning. Many works have built on this approach [16, 24]. 2.3. Segmented Style Transfer Few works have yet to attempt to complete style transfer on segmented regions. Some approaches use segmentation to inﬂuence style transfer [13, 23] or use class-based labels [9, 10]. Recently, work titled SAMStyler [17] has combined the Segment-Anything Model with Style Transfer techniques, but relies on the slow optimization-based approach of the original Gatys implementation. In this work, we will present an approach that completes classless style transfer on segmented regions in fast, feed-forward way. 3. Flaws in Masking Standard Style Networks Throughout this work, we will refer to three stylization techniques which use the pretrained stylization network by Li et al. [11] as their base: Style-then-mask: stylize the image, then mask to include only the region of interest. Mask-then-style: mask the region of interest, applying black pixels elsewhere, then stylize the image. PartialConv: our approach that uses partial convolutions [14] in each layer of the stylization network to apply calculations only to the region of interest. We will describe the speciﬁcs of implementing our approach in Section 4, but ﬁrst, we provide intuition for why style-then-mask and mask-then-style have ﬂaws that can lead to poor stylizations. These ﬂaws generalize to any feed forward style network that applies masking as post process step. 3.1. Analyzing Color Distributions Stylization networks operate on the pixel colors for the content and style images, then perform some operation (usually in feature space) to give the content pixel colors the statistics of the style pixel colors [12]. Thus, the color distributions in an image for both the content and style images play vital role on the ﬁnal output of the stylization. Simply styling an image and then masking to region (style-then-mask) makes an important underlying assumption. It assumes that the color distributions for the whole image and the masked region are similar. This, however, is often not the case. The normalized histograms of the RGB colors for the content and its masked region of interest are not guaranteed to be the same, as shown in Fig. 2. Figure 2. The normalized histograms of pixel colors in red, green, and blue channels for the bird image. The statistics for original image (Top) and masked region (Bottom) are shown. The histograms show that the distribution of colors for the full image and masked region are disparate. This leads to the style-then-mask output misrepresenting aspects of the style in the masked region. Dissimilarities between the full image and masked region distributions cause issues in the stylization. Because the stylization networks apply style features across the whole image, this may under represent the distribution of style features and colors within the masked region. This can be visually seen in Fig. 3. Style-then-mask and mask-thenstyle, which only perform preor post-processing step, tend to have colors that are more washed out and overly dark or bright compared to the style. PartialConv, which only operates on pixels within the masked region, tends to much more closely match the true color distribution of the stylization image. As will be shown in Section 5, this can also be quantitatively veriﬁed with metrics such as Earth Movers Distance and Perceptual Style Loss. 3.2. How often is this an issue? While images will almost always have disparate color distributions from the masked region of interest, the question naturally arises of how frequently the distributions are dissimilar enough to cause noticeable change in the stylization. To answer this question, we conducted an experiment with 500 images from the SA-1B dataset [6] and 11 style images. For each content image and randomly selected style, random mask was selected (with at least 2% area of the image) and was stylized with the style-then-mask approach and our partial convolution approach. These two results and the original image, style, and mask were presented to user. The user then indicated which method more closely matched the style features. They could also indicate that the two methods appeared the same. Mask-then-style was excluded from the experiment for its tendency to always over brighten the region of interest. For these 500 images and masks, there were 212 times where PartialConv appeared to match the style features better than style-then-mask. In 283 cases, the two algorithms appeared to do the same and on rare occasion (5 times), Original and Style Masked Original Stylized Style-then-mask Stylized Masked Input Mask-then-style PartialConv Figure 3. An example comparing the three techniques: style-thenmask, mask-then-style, and partial convolution. content image, style image, and mask (Row 1) give the shown stylized output for the masked swan regions. Stylizing the whole image and then masking out the region (Row 2) or masking out the region and then stylizing (Row 3) is compared to our approach (Row 4). The resulting stylization from partial convolution tends to be closer to the input style features than the other two approaches. style-then-mask appeared to do better. For these 500 examples, the Earth Movers Distance (EMD) was processed between the whole image color distributions and the masked region. EMD assesses the alignment between the color distributions, with lower values indicating closer alignment. Rather than report red, green, and blue EMD, we summarize the EMD in two ways. First, we report EMD when computed on the grayscale version of the image. Second, we compute the sliced EMD that averages over projections of the 3D distribution in RGB. As expected, PartialConv tends to be better stylization when the EMD is large between the masked and whole image, Style Num Images Gray EMD Sliced EMD Num Images Gray EMD Sliced EMD Same Output Improved with PartialConv 23 30 20 23 28 33 26 35 19 288 0.167 0. 0.160 0.168 0.166 0.163 0.155 0. 0.176 0.198 0.130 0.176 0.224 0.311 0. 0.229 0.232 0.220 0.208 0.250 0. 0.250 0.175 0.232 25 25 12 19 17 25 5 21 25 212 0.198 0.190 0.274 0.206 0. 0.216 0.193 0.180 0.208 0.160 0.201 0. 0.275 0.253 0.326 0.283 0.290 0. 0.254 0.266 0.288 0.241 0.286 0.275 Total/Average Table 1. The results of the visual study. For each content/mask/style that was presented to the user, the output of Style-then-mask and PartialConv were shown. The user then selected whether the outputs appeared the same, or if one output appeared to match the style statistics better. The statistics of the selection for each style is shown (since style-then-mask was selected as superior only 5 times, it is grouped with the same category). The Earth Movers Distance (EMD) between the whole image and masked area was also computed and averages are shown in the table. As can be seen, larger EMD generally indicates that PartialConv will perform better, conﬁrming the intuition that regular style transfer is insufﬁcient the more dissimilar the whole-image and masked area color distributions become. The exceptions occur for styles that have narrow color distributions themselves. giving an average grayscale EMD of 0.203 and sliced EMD of 0.275. In comparison, the algorithms appeared similar when the EMD was lower, with an average grayscale EMD of 0.176 and sliced EMD of 0.232. The breakdown of EMD values for each style are shown in Tab. 1. These results showcase that EMD is an effective and intuitive metric for determining if masked style transfer will or will not beneﬁt from partial convolution. 4. Methodology We now describe how to implement our approach of the partial convolution style network. We also describe some practical improvements that allow for better blending and multi-mask/multi-style options. 4.1. Partial Convolution To set up the partial convolution style transfer network, the pretrained style transfer network presented by Li et al. is used [11]. Every convolution layer in the encoder, decoder, and transformation blocks is replaced with partial convolutions [14]. Partial convolution allows for internal masking of the convolution operator in any neural network, only performing the weighted average over pixels within the mask, as illustrated in Fig. 4. The same code and operator as presented in [14] is used, disabling the post convolution normalization that stabilizes training. Once each convolution in the style network is rewritten as partial convolution, the same pretrained weights can be used without any additional ﬁne-tuning. Our code for this modiﬁed style network is publicly available at https://github.com/davidmhart/StyleTransferMasked. Once the network is rewritten, the mask needs be properly accounted for at each stage of the style network. The input image and input mask are provided to the network for the computations. In the encoder, the mask goes through each padding and pooling layer that the image goes through to guarantee the correct size of the mask at each layer. The matrix multiplication that performs the style transformation is also masked at the feature level using the smallest output mask from the encoder. In the decoder, bilinear interpolation of the original mask is used to guarantee the correct size at each layer. Finally, the output is alpha blended with the original image and mask to place the stylized region back onto the background. All of these steps modify the style transfer network to only stylize the region inside the masked area without inﬂuencing the stylization with image values outside the masked area. 4.2. Blending This simple addition of partial convolution drastically improves stylization within the masked region, but masked style transfer also needs to be coherent with the background image it is placed on. Effective blending is crucial to address artifacts in style application, especially around mask Original Mask Feathering Figure 4. Visualization of the partial convolution operator. As the kernel for the CNN moves across the image, it is multiplied by the mask for the image, causing aggregation to only occur across pixels within the mask. This can effectively separate foreground information from background information. boundaries. These artifacts arise from the variation in semantic region types and quality of the segmentation mask provided by SAM [6]. Several blending techniques were considered that operate on the mask during various stages of the network. The three blending techniques employed in our approach include: Mask Feathering Before Encoding: Smoothes the edges of masks prior to encoding by gradually transitioning mask values from 1 to 0. Mask Expansion During Partial Convolution: Dynamically dilates masks within convolutional layers, enabling convolution ﬁlters to access additional context around mask edges. Content Feathering in Decoder: Includes the nonstylized content features from the standard autoencoder at each stage of the decoder, feathering the output mask of the style network to incorporate nearby content features. Visualizations of these different approaches are visualized in Fig. 5. Collectively, these techniques signiﬁcantly enhance the integration between stylized and non-stylized regions, improving overall visual coherence and reducing border artifacts. Additional implementation details for the blending techniques and are provided in the supplemental material. 4.3. Multi mask Style Transfer Due to the unique nature of the partial convolution network, it is possible to provide multiple masks and multiple styles to the network in one pass. Multi-mask style transfer enables the simultaneous application of distinct artistic styles to multiple, individually deﬁned regions of an image. By pairing each segmentation mask with speciﬁc style, the method processes multiple masked regions in parallel using partial convolution layers. Stylized features corresponding Mask Expansion Content Feathering Figure 5. Blending techniques integrated into the network architecture. For given mask (Top Left), mask feathering (Top Right) is applied before stylization occurs, while dynamic mask expansion (Bottom Left) dilates the mask during each partial convolution in the style network. Content feathering (Bottom Right) maps the unstylized content feature to the pixels around the mask during the decoder phase. Each of these approaches separately and collectively minimize boundary artifacts by giving the style network more context when assigning color values to the boundary pixels, enhancing output quality along the border. to each mask are merged at the feature level, with overlapping regions gracefully blended through weighted summation based on mask values, rather than being overwritten in sequential order. The ﬁnal image is reconstructed via single decoder conditioned on the merged feature representations, ensuring coherent integration of multiple styles within single output. This multi-mask network architecture is illustrated in the supplemental material. 5. Results 5.1. Qualitative Comparisons Many visual comparisons between masked style algorithms are provided for different content, style, and mask images in Fig. 6. In addition to style-then-mask and mask-then-style, we compare to masking two state-of-the-art style transfer methods: StyTr2 [2], vision-transformer-based approach, and StyleID[1] which uses diffusion network. We also compare to SAMstyler [17], recent method that is focused on masked stylization but relies on the slow optimization approach of Gatys et al. [3]. Additional qualitative examples are provided in the supplemental material. From these examples, several observations can be made. First, our partial convolution approach consistently matches style features and color distributions well. Others method Original Mask and Style StylethenMask [12] MaskthenStyle [12] SAM Styler [17] StyTR2 [2] StyleID [1] Partial Conv (Ours) Figure 6. Example outputs from different style techniques. Content images and style images give the following stylized output for the masked region. Note the ability of our approach to consistently match style colors and features in the masked region of interest. might match the style feature best in given example but then perform poorly in other scenarios. Second, as demonstrated in Section 3, the various methods perform similarly when the region of interest is large or contains color distribution similar to the background. 5.2. Quantitative Comparisons ized images match intended styles as perceived by neural networks, where lower values reﬂect better style coherence [5]. Earth Movers Distance (EMD) and sliced EMD can also be computed between the style image and the masked result, with lower EMD generally indicating better stylization. Fig. 7 illustrates example histograms of the stylized regions for comparing the distributions with EMD. Quantitative evaluations comparing Style-then-mask and PartialConv were conducted using Perceptual Style Loss metrics. Perceptual Style Loss measures how closely stylComparisons were made across 500 images from the SA-1B [6] dataset with randomly selected masks. The average grayscale EMD, sliced EMD, and Perceptual Style Table 3. Quantitative results for different feature combinations over the ﬁrst 500 images of the SA-1B dataset. Incorporating all three blending techniques provides the lowest scores on the two boundary metrics. Feature Feathering (Before) Expansion (During) Feathering (Decoder) 1 6 : : : 6 : 3 : : 6 4 6 6 : 6 : 6 6 : 6 6 7 6 6 6 : : : Grad. Magn. Color Cont. 142.9 135.3 90.38 129.9 88.03 84.61 82.65 157.1 28.84 28.02 33.50 27.01 30.93 27.98 26.23 31.19 large border region. Each blending technique was considered in isolation and in combination with the others, leading to eight possible conﬁgurations in all. Tab. 3 provides the quantitative evaluations for each of the conﬁgurations. Two metrics are used to evaluate the border consistency. The ﬁrst metric represents the average gradient magnitude across the RGB channels at the boundary between masked and unmasked regions. Lower values indicate smoother transitions in color and intensity along the border. The second metric is average color change in RGB along the border, with smaller values indicating smoother color transitions between the styled and unstyled areas. Based on the quantitative experiment, the content feathering in the decoder has the largest effect on decreasing border artifacts, while the combined approach of all three methods performed best overall. 5.4. Multi mask results As outlined in Section 4.3, our approach allows for stylizing two regions in parallel during the decoder stage, allowing for blendings between two styles with adjacent masked regions. In the multi-mask and multi-style scenario, each image region receives unique style simultaneously using partial convolution with mask feathering and expansion techniques. The example in Fig. 9 demonstrates seamless integration of multiple styles, with smooth transitions and minimal border artifacts, highlighting the practical effectiveness of this combined method in complex stylization tasks. 6. Conclusion In this work, we have presented new stylization method that is speciﬁcally designed for masked regions of an image. We have shown that it stylizes better than other approaches in the masked region by more closely matching style image statistics. We have demonstrated various blending techniques and other practical improvements. This can lead to improved user experiences and work ﬂows when working with style transfer algorithms in standard image applications. Future work could continue to develop maskfocused style transfer algorithms while incorporating modern network designs such as vision transformers and diffusion models. EMD 0.95 0.96 0. EMD Image 0.24 Red 0.31 Green 0.40 Blue Figure 7. The style image color distributions (top) compared to the masked region color distributions for style-then-mask (middle) and partial convolution (bottom). The EMD distance for each channel suggests that partial convolution aligns more closely with the style image statistics. This is consistent with perceptual style loss metrics: style-then-mask = 729, partialconv = 622. Table 2. Results on Masked Stylization averaged over 500 image/mask dataset. Our partial convolution based approach out performs the standard approach on each metric. Method Gray EMD Sliced EMD Style Loss Style-then-mask PartialConv 0.121 0.086 0.168 0.118 760 449 Loss are presented in Tab. 2. SamStyler [17] was excluded from the experiment because of its slow optimization time, as well as StyTr2 [2] and StyleID [1] due to the restriction of only processing 512 512 inputs. Results demonstrated partial convolution consistently achieved lower grayscale EMD, sliced EMD, and Perceptual Style Loss values, conﬁrming its effectiveness in producing visually coherent and structurally consistent stylizations with reduced artifacts compared to traditional methods. 5.3. Blending Results Fig. 8 provides various examples of apply blending techniques to the style transfer method. Overall, the combined application of mask feathering before encoding, mask expansion during partial convolution, and content feathering led to most natural coherent results at the border of the styled area and original background content. To quantitatively evaluate the different blending techniques, the same 500 images from the SA-1B dataset were used. Different masks were randomly selected to guarantee (a) Content Image (b) Stylized With Blending (c) Without Techniques (d) With Techniques Figure 8. Visual results of the combination of all three blending techniques. Notice that the edges are smoother and border artifacts are reduced dramatically with all techniques applied. Sequential Styling Parallel Styling Figure 9. Visual results of sequential styling compared to our parallel multi-mask/multi-style approach. Different stylized regions are blended around the touching borders of the masks which reduces stylization interruptions and avoids boundary artifacts."
        },
        {
            "title": "References",
            "content": "[1] Jiwoo Chung, Sangeek Hyun, and Jae-Pil Heo. Style injection in diffusion: training-free approach for adapting largescale diffusion models for style transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 87958805, 2024. 2, 5, 6, 7 [2] Yingying Deng, Fan Tang, Weiming Dong, Chongyang Ma, Xingjia Pan, Lei Wang, and Changsheng Xu. Stytr2: ImIn Proceedings of age style transfer with transformers. [15] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 34313440, 2015. 2 [16] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything in medical images. Nature Communications, 15(1):654, 2024. 1, 2 [17] Konstantinos Psychogyios, Helen C. Leligou, Filisia Melissari, Stavroula Bourou, Zacharias Anastasakis, and Theodore B. Zahariadis. Samstyler: Enhancing visual creativity with neural style transfer and segment anything model (sam). IEEE Access, 11:100256100267, 2023. 1, 2, 5, 6, 7 [18] Gilles Puy and Patrick Perez. ﬂexible convolutional solver for fast style transfers. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 2 [19] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Improved texture networks: Maximizing quality and diversity In The in feed-forward stylization and texture synthesis. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. [20] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github. com/facebookresearch/detectron2, 2019. 2 [21] Keiji Yanai and Ryosuke Tanno. Conditional fast style transfer network. In Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval, pages 434437, New York, NY, USA, 2017. ACM. 2 [22] Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong, and Changsheng Xu. Inversion-based style transfer with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1014610156, 2023. 2 [23] Hui-Huang Zhao, Paul Rosin, Yu-Kun Lai, and Yao-Nan Wang. Automatic semantic style transfer using deep convolutional neural networks and soft masks. The Visual Computer, 36(7):13071324, 2020. 2 [24] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once. Advances in Neural Information Processing Systems, 36, 2024. 1, 2 the IEEE/CVF conference on computer vision and pattern recognition, pages 1132611336, 2022. 1, 2, 5, 6, 7 [3] Leon Gatys, Alexander Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 24142423, 2016. 2, 5 [4] Leon Gatys, Alexander Ecker, Matthias Bethge, Aaron Hertzmann, and Eli Shechtman. Controlling perceptual factors in neural style transfer. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 39853993, 2017. 1, [5] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 694711. Springer, 2016. 2, 6 [6] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 1, 2, 3, 5, 6 [7] Nicholas Kolkin, Jason and Gregory Shakhnarovich. Style transfer by relaxed optimal transport In The IEEE Conference on Computer and self-similarity. Vision and Pattern Recognition (CVPR), 2019. 2 Salavon, [8] Dmytro Kotovenko, Artsiom Sanakoyeu, Pingchuan Ma, Sabine Lang, and Bjorn Ommer. content transformation block for image style transfer. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 2 [9] Harshmohan Kulkarni, Om Khare, Ninad Barve, and Sunil Mane. Improved object-based style transfer with single deep network, 2024. 2 [10] Lironne Kurzman, David Vazquez, and Issam Laradji. Classbased styling: Real-time localized style transfer with semantic segmentation. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 0 0, 2019. 2 [11] Xueting Li, Sifei Liu, Jan Kautz, and Ming-Hsuan Yang. Learning linear transformations for fast image and video style transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3809 3817, 2019. 1, 2, 4 [12] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Universal style transfer via feature transforms. Advances in neural information processing systems, 30, 2017. 1, 2, 6 [13] Zhijie Lin, Zhizhong Wang, Haibo Chen, Xiaolong Ma, Chuan Xie, Wei Xing, Lei Zhao, and Wei Song. Image style IEEE transfer algorithm based on semantic segmentation. Access, 9:5451854529, 2021. 2 [14] Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Image inpainting for irIn The European regular holes using partial convolutions. Conference on Computer Vision (ECCV), 2018. 1, 2, Improving Masked Style Transfer using Blended Partial Convolution - Supplemental Material 1. Implementing Blending Techniques While applying style transfer to segmented regions using partial convolution improves overall visual quality, artifacts can still emerge along the border of the regions. These artifacts are visual discontinuities that occur due to abrupt changes between the stylized (masked) and non-stylized (unmasked) areas. The blending techniques described in the paper mitigate these artifacts. Each blending technique involves feathering or expanding the mask in some way. These techniques, however, are applied at different stages of the convolution network. Mask Feathering is just applied to the initial mask before the style network is applied. Mask Expansion occurs at each layer of the network. Content Feathering occurs only in the decoder stage. This is illustrated in the Fig. 1. Mask Feathering is simple operation implemented using OpenCVs morphology package. We used feathering kernel with size of 5 pixels. Mask Expansion is implemented in PyTorch within the partial convolution operation. max pooling operation with kernel size of 3 and stride of 1 is used to expand the mask (similar to dilation in OpenCV). That mask is then used during the convolution, but not stored during the following layers, essentially updating the feature information 1 pixel around the original mask at each layer. Content Feathering is implemented in PyTorch within the decoder stage. Since the style network is autoencoder based, the content image can be fed in parallel to the autoencoder without performing the stylization process. These unstylized content features can be mapped around the stylized masked features using simple copy operations during the decoder stage. 2. Implementing Multi-mask Style Transfer Our multi-mask and multi-style approach is simple to implement given the existing pieces of our framework. Each mask and style is fed through separate encoder and stylizer in parallel. The simple modiﬁcation is to merge the separate masked features together before sending them through single decoder. For improved results, content features are also mapped into the decoder for the previously described blending technique. This whole process is visualized in Fig. 2. 3. Additional Style Transfer Results Figs. 3, 4, and 5 provide additional comparisons between our method and other techniques. 1 Figure 1. Blending techniques integrated into the network architecture. Mask feathering is applied before encoding, while dynamic mask expansion occurs during partial convolution. Content feathering occurs during the decoding stage to provide output context. Collectively these techniques minimize boundary artifacts and enhancing output quality. Figure 2. Multi-mask and Multi-style parallel processing framework. Different regions of the content image are associated with distinct style images, processed in parallel, and merged at the feature level before decoding into ﬁnal stylized image. Original Mask vs Style Style-then-mask Mask-then-style StyleID Ours Figure 3. Helicopter example outputs from different style techniques. Original Mask vs Style Style-then-mask Mask-then-style StyleID Ours Figure 4. Horse example outputs from different style techniques. Original Mask vs Style Style-then-mask Mask-then-style StyleID Ours Figure 5. Elephant example outputs from different style techniques."
        }
    ],
    "affiliations": [
        "East Carolina University"
    ]
}