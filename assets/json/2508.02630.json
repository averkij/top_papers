{
    "paper_title": "What Is Your AI Agent Buying? Evaluation, Implications and Emerging Questions for Agentic E-Commerce",
    "authors": [
        "Amine Allouah",
        "Omar Besbes",
        "Josu√© D Figueroa",
        "Yash Kanoria",
        "Akshit Kumar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Online marketplaces will be transformed by autonomous AI agents acting on behalf of consumers. Rather than humans browsing and clicking, vision-language-model (VLM) agents can parse webpages, evaluate products, and transact. This raises a fundamental question: what do AI agents buy, and why? We develop ACES, a sandbox environment that pairs a platform-agnostic VLM agent with a fully programmable mock marketplace to study this question. We first conduct basic rationality checks in the context of simple tasks, and then, by randomizing product positions, prices, ratings, reviews, sponsored tags, and platform endorsements, we obtain causal estimates of how frontier VLMs actually shop. Models show strong but heterogeneous position effects: all favor the top row, yet different models prefer different columns, undermining the assumption of a universal \"top\" rank. They penalize sponsored tags and reward endorsements. Sensitivities to price, ratings, and reviews are directionally human-like but vary sharply in magnitude across models. Motivated by scenarios where sellers use AI agents to optimize product listings, we show that a seller-side agent that makes minor tweaks to product descriptions, targeting AI buyer preferences, can deliver substantial market-share gains if AI-mediated shopping dominates. We also find that modal product choices can differ across models and, in some cases, demand may concentrate on a few select products, raising competition questions. Together, our results illuminate how AI agents may behave in e-commerce settings and surface concrete seller strategy, platform design, and regulatory questions in an AI-mediated ecosystem."
        },
        {
            "title": "Start",
            "content": "What Is Your AI Agent Buying? Evaluation, Implications, and Emerging Questions for Agentic E-Commerce Amine Allouah* Omar Besbes Josue Figueroa Yash Kanoria Akshit Kumar Abstract Online marketplaces will be transformed by autonomous AI agents acting on behalf of consumers. Rather than humans browsing and clicking, vision-language-model (VLM) agents can parse webpages, evaluate products, and transact. This raises fundamental question: what do AI agents buyand why? We develop ACES, sandbox environment that pairs platform-agnostic VLM agent with fully programmable mock marketplace to study this question. We first conduct basic rationality checks in the context of simple tasks, and then, by randomizing product positions, prices, ratings, reviews, sponsored tags and platform endorsements, we obtain causal estimates of how frontier VLMs actually shop. Models show strong but heterogeneous position effects: all favor the top row, yet different models prefer different columns, undermining the assumption of universal top rank. They penalize sponsored tags and reward endorsements. Sensitivities to price, ratings, and reviews are directionally human-like but vary sharply in magnitude across models. Motivated by scenarios where sellers themselves use AI agents to optimize product listings, we show that seller-side agent that makes minor tweaks to product descriptionstargeting AI buyer preferencescan deliver substantial market-share gains if AI-mediated shopping dominates. We also find that modal product choices can differ across models and, in some cases, demand may concentrate on few select products, raising competition questions. Together, our results illuminate how AI agents may behave in e-commerce settings, and surface concrete seller strategy, platform design, and regulatory questions in an AI-mediated ecosystem. Keywords: AI shopping agents, agentic e-commerce, platform design, marketplaces, sponsored ads, product ranking, recommendation systems 5 2 0 2 4 ] . [ 1 0 3 6 2 0 . 8 0 5 2 : r *MyCustomAI. Email: amine@mycustomai.io Columbia University, Graduate School of Business. Email: ob2105@columbia.edu MyCustomAI. Email: josue.figueroa@mycustomai.io Columbia University, Graduate School of Business. Email: ykanoria@columbia.edu Yale University. Email: akshit.kumar@yale.edu"
        },
        {
            "title": "1 Introduction",
            "content": "Autonomous AI agents are poised to make shopping decisions for millions of consumers, reshaping the e-commerce ecosystem [14, 18, 33, 50]. new generation of computer-use systems such as OpenAIs Operator (automating grocery orders, form filling, flight booking) [29, 37] and Google DeepMinds Project Mariner (large-scale multi-tab shopping) [16, 55]promises to replace manual browsing with autonomous procurement. Industry leaders are already signaling strategic disruption; Walmarts CTO urges preparedness for third-party agents that circumvent traditional merchandising and advertising cues [6]. Today, consumers still interact with platforms directly. In the near future they may instead issue short prompt to personalized AI agent (augmented with purchase history and preferences), which then navigates, evaluates, and purchases with minimal human oversight (see Figure 5 in Appendix A). Such agents have the potential to significantly reduce search frictions for customers. In doing so, agents can systematically scan broader assortments than humans, potentially diluting the influence of classical monetization levers that platforms have, such as product rankings and ad placements. Parallel developments are also likely on the seller side: AI listing and pricing agents that tune product listings in anticipation of buyer-agent behavior. This shift foreshadows reconfigured e-commerce ecosystem and raises new design, strategy, and regulatory questions. With this shift upon us, and as AI agents start purchasing and making decisions on our behalf, we ask the following fundamental question: what is our AI agent buying and why? More concretely, we investigate various layers of questions. 1. Rational Behavior. Do agents satisfy basic instruction following and simple economic dominance tests? 2. Product Market Shares. What are product market shares when purchases are fully mediated by AI agents, and how do such market shares vary across AI agents? 3. Choice Behavior and Biases. How do AI agents respond to observable attributes (price, rating, reviews, text) and platform levers (position, promotions, sponsorship)? 4. Interaction between Buyer and Seller Agents. How might outcomes change when sellers and/or marketplace platforms deploy their own optimizing AI agents? The first question concerns minimum viability: without elementary rationality, higher-order design or policy levers may be moot (cf. the emphasis on baseline competence in [46]). The second and third probe in-the-wild behaviors. The fourth anticipates the strategic dynamics between mediated demand and algorithmic supply responses. Our main contributions can be summarized as follows. 2 ACES framework. We develop ACES (Agentic e-CommercE Simulator): (i) provider-agnostic VLM shopping agent and (ii) fully programmable mock e-commerce application (mock-app; see Figure 6). This framework enables control and randomization of page layout, product order, prices, ratings, reviews, and promotional / sponsored badges. This provides realistic shopping environment while preserving experimental control for causal attribution of how platform levers and listing attributes steer AI agents purchase decisions. Market shares under delegation. Using ACES, we study product selection shares when an assortment of eight products are presented with randomized product positions. We evaluate three model families (Claude 4 Sonnet, GPT-4.1, Gemini 2.5 Flash) and treat selection frequencies as proxy for market share in fully delegated world. We observe the following. (i) Model heterogeneity. Different AI models induce very distinct market shares for the same assortment of products. For example, for the fitness watch category, Claude Sonnet 4 selects the Fitbit Inspire model 45%, while GPT-4.1 and Gemini 2.5 Flash select it only about 25% of the time (see Figure 1). (ii) Concentration risk. For some categories, the market shares concentrate on few select products and some brands are never selected (e.g., Amazon Basics and Arrow in the case of staplers; see Figure 1), raising questions about possible market dominance patterns induced by agent selection, which may suppress intrinsic consumer preference dispersion. Baseline rationality. We test two basic requirements: (i) instruction following select the only product matching an explicit attribute (e.g., color, within budget); (ii) economic rationalityselect the unique best product when product that dominates all others exists. We test this by controlling the attributes of the various listings and constructing randomized scenarios with each having only one listing satisfying the query. We find that performance is heterogeneous across models and tasks. In one of the price-based rationality tests, we construct scenario with all listings being identical except for one listing having lower price. Here, even state of the art models (GPT4.1) can register failure rates exceeding 9%. The failure rate tends to decrease with the increase in price difference. In rating dominance tests, all listings are identical except that one listing has an average rating which is higher by 0.1. We see significant heterogeneity in performance, with some models registering no failures (Gemini 2.0 Flash) and others registering up to 71.7% failures (GPT-4o, on which OpenAI Operator is built). However with GPT-4.1, this fail rate comes down to 16.0%, supporting the insight that failures reduce with more advanced models. These findings imply consumers delegating purchases may sometimes pay more or obtain lower-rated products, and sellers cannot rely on modest price cuts or rating advantages to guarantee being selected by agents. 3 Figure 1: Market share of fitness watches (left) and staplers (right) with different AI agents. Choice Behavior and Biases. To systematically understand how the AI agents trade off between different product attributes and the impact of different platform levers like rankings and promotions, we run randomized controlled trials where we randomly permute the position of the products, add sponsorship, endorsement or scarcity tags and perturb product attributes like price, average rating and number of reviews. We fit conditional logit model [35] to understand the sensitivity of AI agents to different attributes. AI agents react positively to higher average ratings, and also to larger number of ratings, and are sensitive to price though perhaps less so than one might expect. This behavior is qualitatively similar with what one would expect from humans, but sensitivities differ significantly across models. We also observe that everything else being equal, the position can significantly affect the selection of product by AI agents. Quite strikingly, this position bias leads to very large changes in the selection rate, and the type of bias differs widely across models. These findings highlight that the rise of AI agents might significantly affect market demand, and also lead platforms to adjust the type of levers they use. Toward the meta game: seller response. We next test how could sellers respond to the deployment of AI agents and what impact may such response have. To do so, we provide an AI agent representing seller information on product attributes and current market shares, and prompt it to improve the description of the product it represents. We find the impact of AI-generated product descriptions to be heterogeneous. In 75% of the cases, it does not lead to statistically different 4 sales levels, but quite strikingly, in 25% of the cases, single iteration of this process leads to dramatic increase in the market share of the focal product. These findings suggest that strategic dynamics will emerge from the interaction between sellers, platforms, and AI agents representing buyers, which are both substantial and intricate. Our work connects to extensive research into computer-use agents and benchmarks [11, 27, 51, 58] including for e-commerce environments [9, 21, 24, 32, 40, 54]. While most benchmarks for AI agents evaluate end-to-end task completion, our contribution is to isolate the critical choice step in agent shopping through the controllable ACES sandbox. This facilitates the first agentcentric, causal evaluation of how AI shoppers respond to product attributes and platform levers like product positions and promotional badges, complementing existing studies on human consumers [31, 48]. We provide more comprehensive literature review in Appendix B. Our results have significant implications for various stakeholders in the new e-commerce ecosystemcustomers, sellers, platform designers and regulatorswhich will be mediated by AI agents. We quantify AI shopping agent behavior and shed light on situations in which customers can and cannot rely on their AI shopping assistant. Furthermore, the results raise pertinent questions such as: How can sellers prepare themselves for more customers shopping via their AI assistants? Will Marketing and Sales efforts come to rely heavily on sandbox environments like ACES to optimize listings and campaigns? How might platforms monetize in future dominated by AI shopping assistants, e.g., can they offer listing optimization as service to sellers? To what extent will AI-powered consumer agents aggregate and exercise market power, and what are the implications for market efficiency and consumer welfare?"
        },
        {
            "title": "2 Experimental Design",
            "content": "To investigate the aforementioned research questions, we construct controllable agent-platform sandbox and conduct randomized experiments that (i) test for instruction following and basic rationality, (ii) cleanly isolate and quantify the effects of platform tags and product characteristics on agent decision making, and (iii) understand the impact of strategic response by seller who optimizes their product description. Our evaluation includes several state-of-the-art models whose providers claim agent-assistant capabilities [16, 22, 29, 37, 55]."
        },
        {
            "title": "2.1 ACES: Our Sandbox Environment",
            "content": "ACES (Agentic e-CommercE Simulator) comprises of: (i) provider-agnostic browser-use agent that combines vision language model (VLM) of choice with tool usage to observe and act on webpages, and (ii) fully controllable mock e-commerce site (mock-app) that renders product listings. The VLM agent is implemented using the LangChain library [7] and the Selenium browser-driver [44]. The mock-app is lightweight storefront with search that returns listings from our pre-specified product universe. Veni: Opens the brower and loads the mock-app page Vidi: Search for the product and take screenshot Emi: Query the VLM; declares intent; process terminates. Figure 2: Workflow of AI agent interacting with online platform. Instead of simulating the entire end-to-end shopping journey, which can consist of many back and forths between different options and reading different reviews, we simulate streamlined three step interaction, which we call Veni, Vidi, Emi1 in short (see Figure 2): 1. the AI agent opens browser (via Selenium) and loads the mock-app landing page (Veni), 2. provides the productcategory (e.g., fitness watch, stapler, etc.) being evaluated then captures screenshot of the returned catalog of products (Vidi), 3. query vision-language model (VLM) (with the screenshot and the user prompt) and records the VLMs intended selection; treating it as the purchase and the episode terminates (Emi). Our results page displays an eight-product assortment in fixed 24 grid (see Figure 6). There are no product-detail pages; all information needed to choose is visible in the search results. While this omits aspects of full-funnel shopping, it provides first-order approximation of selection behavior. We intentionally constrain the search-action space to single page with eight items and single tool call to enable clean measurement of choice behavior. This design (i) prevents behavioral looping by grounding the agent in fixed UI context; (ii) standardizes tool use across model families and avoids coordinate-based errors; (iii) restricts navigation complexity by eliminating multi-page exploration; and (iv) isolates core trade-offs we aim to study. As VLM technology matures and token costs decline, richer multi-page interactions can be incorporated. Finally, we do not personalize agents and use single generic prompt (see below) to isolate baseline behavior. Over-specifying preferences would effectively pre-solve the very search task these agents are designed to perform, obscuring autonomous decision-making and the influence of platform levers. In addition, prompt engineering is unbounded: more instructions can be layered on, producing brittle, hand-crafted policies whose biases may shift as models are updated and deployed. Our objective in this work is to measure the base behavior. 1It is Latin for came, saw, bought 6 Default Prompt Template for AI Buyer You are personal shopping assistant helping someone find good {query}. They havent specified particular requirements, so use your best judgment about what would work well for typical person, and select one product to purchase. <instructions> 1. Carefully examine the entire screenshot to identify all available products and their attributes. 2. Use the `add to cart` function when you are ready to buy product. 3. Before making your selection, explain your reasoning for choosing this product, including what factors influenced your decision and any assumptions you made about what would be best: - Your primary decision criteria and why you prioritized them - How each available product performed on these criteria - What specific factors made your chosen product superior - Any assumptions you made about the users needs or preferences 4. If information is missing or unclear in the screenshot, explicitly mention the limitation and how it influenced your decision-making. </instructions>"
        },
        {
            "title": "2.2 ACE-RS: Instruction Following and Rationality Suite",
            "content": "We test for two capabilities which we deem to be foundational for reliable deployment of AI agents in practiceability to follow instructions and basic economic rationality. The experiments are designed in manner such that there is objectively only single product that satisfies the requirements of the test. Instruction Following Experiments. We consider three types of instruction following tasks: (i) choose product within specified budget, (ii) choose the product of specified color and (iii) choose specified brand. For each of the tasks, we select few product categories and explicitly prompt the AI agent to align with the stated preferences by replacing specific substring in the default prompt. For each task and its chosen product categories, we run 50 experiments in which the position of the products are randomly shuffled. For further details about the experiment setup, refer to Appendix C.1.1. Basic Rationality Experiments. We also test agents ability to select an optimal product, when the prompt does not specify which attribute to prioritize. To do so, we examine, in fully controlled setting, whether agents select the option that maximizes user utility when alternatives differ only along single ordered attribute. We perform two types of tests: (a) price-based and (b) rating-based tests where all product attributes are the same except for prices and ratings respectively. This setup therefore gauges whether the agent can infer sensible utility ranking on 7 its own. An economically rational AI agent which aims to maximize user utility must choose the product with the lowest price or the highest rating given that all the other attributes are identical. We explicitly avoid instructing the models to prioritize specific attribute because real-world preferences are multi-dimensional: customers routinely trade off price, ratings, and other product features. For further details, refer to Appendix C.1.2 and C.1.3."
        },
        {
            "title": "2.3 ACE-BB: Choice Behavior and Biases",
            "content": "To estimate the sensitivity of the AI agent to different product attributes and platform levers, we create dataset with exogenous variation across number of variables associated with the products. We generate 500 scenarios for each product category. In these 500 random scenarios, we randomly permute the position of the eight products shown on the mock-app page. We randomly assign Sponsored, Overall Pick and Only Remaining (scarcity; is random integer between 1 and 5), tags to these products. In addition, we randomly perturb the prices, ratings and number of reviews for each of the eight products in the assortment. For further details regarding the experimental setup, refer to Appendix C.2."
        },
        {
            "title": "2.4 ACE-SR: Seller Response to AI-Assisted Shopping",
            "content": "To estimate the causal impact of the recommendation made by the AI seller agent, we begin by establishing baseline for each product category: in 200 trials we expose an AI buying agent Claude Sonnet 4, GPT-4.1, or Gemini 2.5 Flashto the mock-app showing eight items whose on-page positions are randomly shuffled from trial to trial while all other attributes (price, description, etc.) remain fixed. After recording the models purchase frequencies, we randomly designate one of the eight items (among those with positive market share) as the focal product. Treating this seller as AI-assisted, we feed second model (e.g., GPT-4.1 acting as seller agent) the screenshot, the focal items feature list, and the baseline sales outcomes for all competitors; the prompt asks it to recommend modifications solely to the focal products descriptionso as to appeal more strongly to the buyer agent, without inventing features or adding irrelevant keywords. We then replace the original description with the AI-recommended version and rerun the same 200 trialsagain shuffling only on-page positionsto measure how the revised description alters purchase shares across categories. For the prompt used for the seller AI agent, refer to Appendix C.3."
        },
        {
            "title": "3 Main Results",
            "content": "3."
        },
        {
            "title": "Instruction Following and Basic Rationality",
            "content": "We say that model fails in given experiment if it chooses product listing different from the single listing that satsfies the query. We provide failure rates of different VLMs across different 8 instruction following and rationality tasks in Appendix C.1. Instruction Following Tasks. Table 4 in Appendix summarizes each models failure rate across our instruction-following tasks. Performance improves steadily with new release: failure rates drop as Claude advances from Sonnet 3.5 3.7 4. The latest modelsClaude Sonnet 4, GPT4.1, Gemini 2.0 Flash, and Gemini 2.5 Flash exhibited close to zero failure rates. Rationality Tests. Tables 5 and 6 in Appendix summarize each models failure rate across different price-based and rating-based tasks respectively. We find that: (i) Model upgrades generally reduce error ratescompare, for example, GPT-4o to GPT-4.1 or Gemini 2.0 Flash to Gemini 2.5 Flashyet even the most recent systems still make mistakes, (ii) In one of the experiments, we reduce the price of one listing by Œ±-fraction while keeping the price of the remaining seven listings the same. For Œ± = 0.01, we find failure rates above 63% for Claude 3.5 Sonnet and above 25% for GPT-4o, whereas there is substantial reduction in failure rate for Œ± = 0.1 (10% discount) or for newer models. Introducing random dispersion across all prices lowers failure rates, and the reduction is larger when the dispersion itself is larger, suggesting that salient variation helps models recognize that price is the sole differentiator, thereby making these models less susceptible to making irrational choices, (iii) Increasing one products rating by 0.1typically on dense five-star scalesstill confuses advanced models (e.g., 28.7% failure for Claude Sonnet 4.0, 15.1% for GPT-4.1), whereas injecting larger random variation in ratings reduces the mistakes significantly."
        },
        {
            "title": "We investigated the explanations provided by models when they failed to select the optimal",
            "content": "product (e.g., the lowest-priced or highest-rated one). The explanations can be broadly categorized into three types: perceptual limitations, where they couldnt distinguish between products, especially when differences were subtle; unjustified sub-optimality, where they recognized the optimal choice but still selected worse option without explanation; and dismissal with justification, where they acknowledged the best option but gave reasons for not choosing it, such as preferring standard options, attributing differences to errors (like display errors or temporary discounts), or deeming the difference insignificant (e.g., no functional difference)."
        },
        {
            "title": "3.2 Choice Behavior and Biases",
            "content": "For each model (Claude Sonnet 4, GPT-4.1, Gemini 2.5 Flash), we estimate conditional logit (CL) [35] with the following utility specification: Uij = Œ≤ posxij + (cid:88) tagT Œ≤tag1{tagij = 1} + Œ≤price ln(priceij) + Œ≤rating ratingij (3.1) + Œ≤num-revs ln(num-revsij) + Œ∏j + Œµij, 9 where indexes choice sets (experiments), products, xij are position dummies (top row and columns 13; bottom row and column 4 are the omitted categories), = {sponsored, overall pick, scarcity} is the set of tags, Œ∏j are product fixed effects, and Œµij is Type extreme value. The corresponding choice probabilities are: P(product chosen in experiment i) = exp(Uij) k=1 exp(Uik) (cid:80)8 . (3.2) Table 1 reports the pooled (across categories) estimates. Table 1: Estimates of the Conditional Logit Regression Claude Sonnet 4 GPT-4.1 Gemini 2.5 Flash Position effects Row 1 Column 1 Column 2 Column Badge effects Sponsored Tag Overall Pick Tag Scarcity Tag Attribute effects ln(Price) Rating ln(Num. of Reviews) Product Fixed Effects Observations Choice Sets (Groups) Pseudo R-squared 1.224*** (0.046) 0.297*** (0.065) 0.557*** (0.058) 0.416*** (0.059) 0.135* (0.068) 1.060*** (0.077) 0.076 (0.094) 1.623*** (0.079) 4.913*** (0.218) 0.415*** (0.023) Yes 25,802 3,756 0.44 1.045*** (0.046) 1.122*** (0.061) 0.019 (0.065) 0.013 (0.066) 0.248*** (0.072) 0.802*** (0.083) 0.105 (0.099) 1.612*** (0.083) 8.300*** (0.269) 0.739*** (0.026) Yes 25,066 3,931 0.51 0.344*** (0.041) 0.264*** (0.057) 0.742*** (0.061) 0.162** (0.054) 0.263*** (0.067) 1.897*** (0.072) 0.342*** (0.098) 2.190*** (0.080) 5.388*** (0.218) 0.501*** (0.023) Yes 25,215 3,953 0.42 Significance is indicated as: < 0.05, < 0.01, < 0.001. 10 (a) Claude Sonnet 4 (b) GPT-4.1 (c) Gemini 2.5 Flash Figure 3: Webpage heatmap of AI shopping agents. The figure depicts the selection rate of the same product across different positions for three AI agents. Ranking mattersand how it matters is highly model-dependent. All three models display statistically significant sensitivity to where product appears on the page. Holding all attributes constant, each model assigns clear premium to the top row relative to the bottom row. However, the horizontal (column) patterns vary sharply. GPT-4.1 strongly favors the first column; Claude Sonnet 4, in contrast, largely ignores the first column and prefers the two middle columns; and Gemini 2.5 Flash tilts toward the third column, while columns one and two are comparatively disfavored. Taken together, these patterns imply different heat maps of positional utility. For illustration, in Figure 3, we show the selection probability of the same product at different positions using the estimates in Table 1. We obtain these selection rates using (3.2) and setting all attributes except for the position to be same across the eight identical products. The position can lead to drastic changes in selection rates. For example, for Claude Sonnet 4, moving product from the bottom right corner (where it is selected 4.5%) to the top row in the second or third column leads to 5-fold increase in selection rate! Interestingly, the top left corner would only yield half of that increase. Sponsored tags hurt sales while Platform Endorsement substantially increases selection chances. Across all three frontier models, the three badgessponsored, overall pick, and scarcityshift choices in the same qualitative direction. sponsored tag reduces the likelihood of selection (a product with baseline selection probability of 10% falls to 8.9% for Claude Sonnet 4, 8.0% for GPT-4.1, and 7.9% for Gemini 2.5 Flash), the scarcity tag effect is weakly negative or statistically indistinguishable from zero, whereas an overall pick endorsement delivers large positive lift, raising the same baseline to 24.3%, 19.9%, and 42.6%, respectively. Because badge assignment is fully randomized, these are causal effects, not artifacts of latent product quality. The negative impact of the sponsored tag suggests that agents discount advertising, while the strong positive response to overall pick implies that platform endorsements are treated as credible signalsechoing the human-buyer evidence in [31]. Importantly, our estimates do not imply that sponsored ads are necessarily harmful for sellers: sponsored ads secure premium placementfront-page or top-row exposurethat seller might not attain organically. Our findings should therefore be interpreted 11 conditionally: given fixed position, the mere presence of the sponsored tag lowers an agents selection probability. Sponsored ads thus buy attention through placement but induce credibility cost, highlighting trade-off that platform designers and sellers must weigh. Directional agreement on product attributes and human-like behavior. All three models assign negative weight to price and positive weight to ratings and the number of reviews. Directionally, this mirrors what rational human shopper would do: prefer cheaper, betterrated, and more widely reviewed products. Quantitatively, however, the sensitivities are heterogeneous across models. For example, consider product with baseline selection probability of 10%, an +0.1 increase in rating lifts the probability to 15.4%, 20.3% and 16.0% with Claude Sonnet 4, GPT-4. and Gemini 2.5 Flash, respectively. Strong response to position, ratings and tags like Overall Pick. We investigate by how much can seller increase the price of their product (while keeping the latent utility governing user choice unchanged), if they could (i) move from the second row to the first row, (ii) get an Overall Pick endorsement from the platform, (iii) increase their rating by +0.1, or (iv) double the number of reviews? Understanding these trade-offs are important not only for the sellers but also for platform operators since many of them operate via seller commissions. The results in Table 7 in Appendix D.3 summarize how different models trade-off these attributes. For Claude Sonnet 4 and GPT-4.1, moving from the second to the first row is worth roughly doubling of price, whereas for Gemini 2.5 Flash the same move is worth only about +17%. The Overall Pick tag produces very large liftse.g., +92% for Claude Sonnet 4, +65% for GPT-4.1 and +138% price headroom for Gemini 2.5 Flash. An increment of +0.1 in ratings, allows seller to increase their price by quarter with Gemini 2.5 Flash and third with Claude Sonnet 4 while this more than doubles to 67% with GPT-4.1. Finally, doubling of reviews, allows 19%, 37% and 17% increase in price with Claude Sonnet 4, GPT-4.1 and Gemini 2.5 Flash respectively."
        },
        {
            "title": "3.3 Seller‚Äôs response to AI mediated demand",
            "content": "Recalling the experimental setup, we define the change in market share of the focal product as the difference in the selection probabilities of the focal product after and before the description modification. Because all other attributes are held fixedand we reuse the identical product shuffles in the pre/post modification experimentsthe observed differences are causally attributable to the description modification. We provide an illustrative case study in Appendix D.4.1. The seller side intervention leads to an average increase in market share of 2.69% (s.e. 1.29%) for Claude Sonnet 4, 5.64% (s.e. 1.32%) for GPT-4.1 and 4.83% (s.e. 1.38%) for Gemini 2.5 Flash. In Figure 4, we report the change in market share for the focal product across different categories and buyer agents (Claude Sonnet 4, GPT-4.1, Gemini 2.5 Flash). Effects are heterogeneous across both categories and models. In 75% of categorymodel pairs, the (one-shot) description modification 12 Figure 4: Market share change of the focal product/seller across different categories with different AI buying agent models. Note that the description of only one product in each category is changed based on the recommendation of the seller AI agent. yields no statistically significant change in market share. In the remaining 25% cases, one-shot modification produces large gains: for Claude Sonnet 4 we observe +9.0% in iPhone 16 Pro Cover and +15.4% in toilet paper; for GPT-4.1, +21.8% in mousepad and +16.3% in office lamp; and for Gemini 2.5 Flash, +23.6% in iPhone 16 Pro Cover, and +9.0% in toothpaste. Firms typically invest heavily in marketing and may not expect double digit increases from minor edits, yet our results indicate that, in the presence of AI-mediated demand, targeted description modifications alone may materially shift selection shares. We emphasize that this is single-pass intervention; iterative optimization or simultaneous adoption by multiple sellers could amplify or dampen these effects and would be interesting to explore in future work."
        },
        {
            "title": "4 Implications and Discussion",
            "content": "4."
        },
        {
            "title": "Industry directions",
            "content": "The findings in this study suggest important implications for various stakeholders and suggest significant adjustments and new opportunities for the e-commerce industry. Platforms. First, the present study has illustrated that position biases are strong and vary widely across AI models. For product ranking, platforms may need to adapt their layout and ranking systems to account for this. We contend that such biases suggest compelling case for the implementation of agent-specific storefronts operating on standardized protocol, such as the Model 13 Context Protocol [2]. Second, traditional monetization levers for platforms may be rendered ineffective by AI shoppers. Instead, new levers, such as dynamically optimizing product titles and images on behalf of sellers, could become more powerful tools for platforms to boost sales. Third, platforms (and regulators) should be monitor the impact of AI assisted shopping on market efficiency and consumer welfare. Brands and Sellers. Given the volatility in market share caused by differences between model providers, sellers will need to continuously adapt their product listings to be in the AI agent consideration sets. New AI model releases as well as model updates may also lead to significant changes in AI agent purchase behavior. In Appendix E, we document significant changes in market shares and position biases when switching from Gemini 2.5 Flash Preview to Gemini 2.5 Flash. new category of seller-side AI agent companies is likely to emerge, aiming to assist sellers in monitoring and navigating, potentially in real time, this increasingly dynamic market. Consumers/Buyers. As consumers delegate purchasing decisions to AI agents, search frictions will dramatically decrease, but as we documented, there may be some risks of irrational or wrong purchases. There is growing need to educate consumers about the differing preferences and behaviors of various buying agents. These behavioral discrepancies underscore the importance of developing new buyer-side AI agents that better align decisions with user preferences. AI shopping agent developers and Regulators. AI shopping assistants should undergo standard evaluations along the lines of ACES and publish their scores. To the extent this doesnt happen organically, regulators should step in to require this. Next, the largest platforms may not organically provide standardized protocol for AI shopping agentsdespite the biases and inefficiency resulting from AI assistants browsing webpagese.g., to protect their ability to monetize via sponsored products. If so, we argue that this will be another opportunity for regulators to put appropriate requirements in place."
        },
        {
            "title": "4.2 Research directions",
            "content": "The current paper and its findings also suggest important research directions. Diagnostics. The ACES framework we develop is step in developing systematic diagnostic tools for agentic e-commerce. We believe that developing such sandbox environments will be critical for all players in the ecosystem, and that developing suite of metrics for agents will be helpful in measuring possible weaknesses and technological progress. There are various concrete avenues for research including building better predictive models of AI agents choices, further understanding the drivers of their decision making, mechanistic interpretability of models, and expanding the scope of measurement to the entire purchasing journey to (not just the Veni, Vidi, 14 Emi model in this paper) and the impact of prompts. It would also be interesting to explore other candidate drivers of choice, such as the images associated with the products, and webpage designs. Operating modes. We have advocated for platforms to provide standardized protocol for AI shopping agents. Will agents leveraging such protocol suffer from similar biases and heterogeneous behavior? How to design such protocol to maximize market performance? Strategic Interactions. The present study suggests that buyers, sellers and platforms will have significant incentives to leverage AI agents. Studying the strategic interactions among players, how these may differ in an AI-mediated world, possible efficiency gains and unintended consequences, is an exciting avenue for future research. AI agents vs. human buyers. natural academic question that emerges is how do AI agents compare to human buyers? In addition, as AI agents are deployed at scale, the question will be whether human preferences will be indirectly shaped over time by the choices of the AI agents, and whether this effect will be stronger than that of recommendation algorithms. Interactive AI assistance. AI buying agents can benefit from some degree of interactiveness for preference elicitation, towards choices that may better align with user preferences. The design and optimization of such dynamic interactions is an exciting direction."
        },
        {
            "title": "Data and Software Availability",
            "content": "To support further research and replication studies, we have made our ACES framework available at https://github.com/mycustomai/ACES, which includes the code for both the AI agent as well as the mock e-commerce platform. The datasets are made available on huggingface: ACE-RS, ACE-BB and ACE-SR."
        },
        {
            "title": "References",
            "content": "[1] Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s2: compositional generalist-specialist framework for computer use agents. arXiv preprint arXiv:2504.00906, 2025. (Cited on pages 21, 22, and 24) [2] Anthropic. Introducing the model context protocol. https://www.anthropic.com/news/ model-context-protocol, 2024. (Cited on page 14) [3] Mark Armstrong and John Vickers. model of delegated project choice. Econometrica, 78(1):213244, 2010. (Cited on page 25) 15 [4] Archit Bansal, Kunal Banerjee, and Abhijnan Chakraborty. These deals wont last! longevity, uniformity and bias in product badge assignment in e-commerce platforms. arXiv preprint arXiv:2204.12552, 2022. (Cited on page 25) [5] Rogerio Bonatti, David Zhao, Francesco Bonacci, David Dupont, Sasan Abdali, Yifan Li, Yixuan Lu, Jignesh Wagle, Kazuhito Koishida, Andres Bucker, Leon Jang, and Zihang Hui. Windows agent arena: Evaluating multi-modal os agents at scale. arXiv preprint arXiv:2409.08264, 2024. (Cited on page 24) [6] Isabelle Bousquette. Walmart is preparing to welcome its next customer: The ai shopping agent. The Wall Street Journal, May 2025. (Cited on page 2) [7] Harrison Chase. LangChain: framework for developing language model applications. https://github.com/langchain-ai/langchain, May 2025. Initial release; stable release 0.3.25 (2 May 2025). (Cited on pages 5 and 22) [8] Giovanni Compiani, Gregory Lewis, Sida Peng, and Peichun Wang. Online search and product rankings: double logit approach. Technical report, Working paper, University of Chicago, Chicago, IL, 2022. (Cited on page 24) [9] Preetam Prabhu Srikar Dammu, Omar Alonso, and Barbara Poblete. shopping agent for addressing subjective product needs. In Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining, pages 10321035, 2025. (Cited on pages 5 and 24) [10] James Davis, Guillermo Gallego, and Huseyin Topaloglu. Assortment optimization under variants of the nested logit model. Operations Research, 62(2):250273, 2014. (Cited on page 25) [11] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. arXiv preprint arXiv:2306.06070, 2023. (Cited on pages 5 and 24) [12] Mahsa Derakhshan, Negin Golrezaei, Vahideh Manshadi, and Vahab Mirrokni. Product ranking on online platforms. Management Science, 68(6):40244041, 2022. (Cited on page 24) [13] Antoine Desir, Vineet Goyal, Danny Segev, and Chun Ye. Constrained assortment optimization under the markov chainbased choice model. Management Science, 66(2):698721, 2020. (Cited on page 25) [14] Jur Gaarlandt, Wesley Korver, Nathan Furr, and Andrew Shipilov. Ai agents are changing how people shop. heres what that means for brands. Harvard Business Review, February 2025. (Cited on page 2) [15] Anindya Ghose, Panagiotis Ipeirotis, and Beibei Li. Examining the impact of ranking on consumer behavior and search engine revenue. Management Science, 60(7):16321654, 2014. (Cited on page 24) [16] Google DeepMind. Project mariner, May 2025. (Cited on pages 2, 5, and 25) [17] Vineet Goyal, Retsef Levi, and Danny Segev. Near-optimal algorithms for the assortment 16 planning problem under dynamic substitution and stochastic demand. Operations Research, 64(1):219235, 2016. (Cited on page 25) [18] Wesley Grant. How ai agents are managing shopping and payments. PaymentsJournal, April 2025. (Cited on page 2) [19] Sophie Greenwood, Karen Levy, Solon Barocas, Hoda Heidari, and Jon Kleinberg. Designing algorithmic delegates: The role of indistinguishability in humanai handoff. In Proceedings of the 26th ACM Conference on Economics and Computation (EC), 2025. Also available as arXiv:2506.03102. (Cited on page 25) [20] MohammadTaghi Hajiaghayi, Keivan Rezaei, and Suho Shin. Delegating to multiple agents. In Proceedings of the 24th ACM Conference on Economics and Computation (EC), pages 861885, 2023. (Cited on page 25) [21] Christian Herold, Michael Kozielski, Leonid Ekimov, Pavel Petrushkov, Pierre-Yves Vandenbussche, and Shahram Khadivi. Lilium: ebays large language models for e-commerce. arXiv preprint arXiv:2406.12023, 2024. (Cited on pages 5 and 24) [22] Siyuan Hu, Mingyu Ouyang, Difei Gao, and Mike Zheng Shou. The dawn of gui agent: preliminary case study with claude 3.5 computer use. arXv preprint arXiv:2411.10323, 2024. (Cited on pages 5, 21, and 22) [23] Nicole Immorlica, Greg Stoddard, and Vasilis Syrgkanis. Social status and badge design. arXiv preprint arXiv:1312.2299, 2013. (Cited on page 25) [24] Yilun Jin, Zheng Li, Chenwei Zhang, Tianyu Cao, Yifan Gao, Pratik Jayarao, Mao Li, Xin Liu, Ritesh Sarkhel, Xianfeng Tang, et al. Shopping mmlu: massive multi-task online shopping benchmark for large language models. arXiv preprint arXiv:2410.20745, 2024. (Cited on pages 5 and 24) [25] Jiin Kim, Byeongjun Shin, Jinha Chung, and Minsoo Rhu. The cost of dynamic reasoning: Demystifying ai agents and test-time scaling from an ai infrastructure perspective. arXv preprint arXiv:2506.04301v1, 2025. (Cited on page 26) [26] Jon Kleinberg and Robert Kleinberg. Delegated search approximates efficient search. In Proceedings of the 2018 ACM Conference on Economics and Computation (EC), pages 287302, 2018. (Cited on page 25) [27] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. (Cited on pages 5, 23, and 24) [28] urhan ok, Marshall Fisher, and Ramnath Vaidyanathan. Assortment planning: Review of literature and industry practice. Retail supply chain management, pages 209246, 2008. (Cited on page 25) 17 [29] John Kuefler. We tried openai operatorheres what ecommerce brands need to know to succeed in the age of ai shopping agents. LimeLight Marketing, January 2025. (Cited on pages 2, 5, 21, and 25) [30] Tomasz Kusmierczyk and Manuel Gomez-Rodriguez. Harnessing natural experiments to quantify the causal effect of badges. arXiv preprint arXiv:1707.08160, 2017. (Cited on page 25) [31] Markus Lill, Nastasia Gallitz, Lucas Stich, and Martin Spann. Product badges and consumer choice on digital platforms. Available at SSRN 4935668, 2024. (Cited on pages 5, 11, and 25) [32] Yougang Lyu, Xiaoyu Zhang, Lingyong Yan, Maarten de Rijke, Zhaochun Ren, and Xiuying Chen. Deepshop: benchmark for deep research shopping agents. arXiv preprint arXiv:2506.02839, 2025. (Cited on pages 5, 21, and 24) [33] Kiri Masters. How autonomous ai shopping agents will transform retail. Forbes, February 2025. (Cited on page 2) [34] Arnav Matiana, Vasu Agarwal, Amol Naik, Shannon Zhang, Karthik Tirumala, Qixing Huang, et al. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024. (Cited on page 24) [35] Daniel McFadden. Conditional logit analysis of qualitative choice behavior. In Paul Zarembka, editor, Frontiers in Econometrics, pages 105142. Academic Press, 1974. (Cited on pages 4 and 9) [36] OpenAI. Computer-using agent, January 2025. (Cited on page 21) [37] OpenAI. Introducing operator. OpenAI Blog, January 2025. (Cited on pages 2, 5, 21, and 25) [38] Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Memgpt: Towards llms as operating systems. arXiv preprint arXiv:2310.08560, 2024. (Cited on page 22) [39] Joon Sung Park, Joseph C. OBrien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (UIST 23), pages 122, San Francisco, CA, USA, 2023. ACM. (Cited on page 21) [40] Bo Peng, Xinyi Ling, Ziru Chen, Huan Sun, and Xia Ning. ecellm: Generalizing large language models for e-commerce from large-scale, high-quality instruction data. arXiv preprint arXiv:2402.08831, 2024. (Cited on pages 5 and 24) [41] Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian Liang, Junxian He, Peng Li, Wei Wei, Jing Shao, Chaochao Lu, Yue Zhang, Xian-Sheng Hua, Bowen Zhou, and Yu Cheng. survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond. arXiv preprint arXiv:2503.21614, March 2025. (Cited on page 26) 18 [42] Francesco Ricci, Lior Rokach, and Bracha Shapira. Recommender Systems Handbook. Springer US, 2022. (Cited on page 25) [43] Ranjan Sapkota, Konstantinos I. Roumeliotis, and Manoj Karkee. Ai agents vs. agentic ai: conceptual taxonomy, applications and challenges. arXiv preprint arXiv:2505.10468, 2025. Cornell University, Department of Biological and Environmental Engineering, University of the Peloponnese, Department of Informatics and Telecommunications. (Cited on page 21) [44] Selenium Project Contributors. Selenium WebDriver. https://www.selenium.dev/, Jun 2025. Version 4.34.0. (Cited on page 5) [45] Brent Smith and Greg Linden. Two decades of recommender systems at amazon.com. IEEE Internet Computing, 21(3):1218, 2017. (Cited on page 25) [46] Paige Smith. Visa ceo says ai shopping to push advertising, payments to adapt. Bloomberg, April 2025. (Cited on page 2) [47] Olivier Toubia, George Gui, Tianyi Peng, Daniel Merlau, Ang Li, and Haozhe Chen. Twin2k-500: dataset for building digital twins of over 2,000 people based on their answers to over 500 questions. arXiv preprint arXiv:2505.17479, 2025. (Cited on page 25) [48] Raluca Ursu. The power of rankings: Quantifying the effect of rankings on online consumer search and purchase decisions. Marketing Science, 37(4):530552, 2018. (Cited on pages 5 and 24) [49] Xinming Wei, Jiahao Zhang, Haoran Li, Jiayu Chen, Rui Qu, Maoliang Li, Xiang Chen, and Guojie Luo. Agent.xpu: Efficient scheduling of agentic llm workloads on heterogeneous soc. arXiv preprint arXiv:2506.24045, June 2025. (Cited on page 26) [50] Queenie Wong. Ai is changing shopping. will consumers buy in? Los Angeles Times, May 2025. (Cited on page 2) [51] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. In Advances in Neural Information Processing Systems 37 (NeurIPS 2024), Datasets and Benchmarks Track, 2024. (Cited on pages 5 and 24) [52] Wei Xue, Zongyi Guo, Baoliang Cui, Zheng Xing, Xiaoyi Zeng, Xiufei Wang, Shuhui Wu, and Weiming Lu. Pumgpt: large vision-language model for product understanding. arXiv preprint arXiv:2308.09568, 2023. (Cited on page 24) [53] John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. arXiv preprint arXiv:2405.15793, 2024. (Cited on page 24) [54] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scal19 able real-world web interaction with grounded language agents. In Advances in Neural Information Processing Systems, volume 35, pages 2074420757, 2022. (Cited on pages 5, 21, and 24) [55] Maxwell Zeff. Google rolls out project mariner, its web-browsing ai agent. TechCrunch, May 2025. (Cited on pages 2, 5, 21, 22, and 25) [56] Ziyu Zhao, Zhenguang Li, Xiaoqin Pan, Jiale Chen, and Shu Li. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. (Cited on page 24) [57] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. (Cited on page 24) [58] Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2024. (Cited on pages 5, 23, and 24) [59] Thomas Zollo, Andrew Wei Tung Siah, Naimeng Ye, Ang Li, and Hongseok Namkoong. Personalllm: Tailoring llms to individual preferences. arXiv preprint arXiv:2409.20296, 2024. (Cited on page 25)"
        },
        {
            "title": "Appendix",
            "content": "Organization of the Appendix. In Section A, we provide further details about the ACES framework. In Section B, we discuss related literature. In Section C, we provide further details regarding the experimental setup for our instruction following and rationality experiments (Section C.1), choice behavior and biases experiments (Section C.2) and the seller response experiments (Section C.3). In Section D, we provide further details regarding our main results and insights: market share across different categories (Section D.1); instruction following and basic rationality experiments (Section D.2); choice behavior and biases (Section D.3); seller response to AI-assisted shopping (Section D.4). In Section E, we discuss the changes in market share and position effects due to model update from Gemini 2.5 Flash Preview to Gemini 2.5 Flash."
        },
        {
            "title": "A ACES Framework",
            "content": "We envision an agentic e-commerce workflow in which AI assistants transact on behalf of consumers. These agents are simply VLM (or LLM) given the ability to interact with the external world through tools [1, 39, 43]. These agents can be personalized (for example, through purchase histories) andinstead of users searching and buying themselvesexecute the end-to-end journey: (i) open browser, (ii) visit retailers site, (iii) issue query, (iv) parse and navigate product listings using tool use (scrolling, clicking, screenshotting, etc.), and (v) complete checkout. Figure 5 illustrates this AI shopping agent in operation. A.1 Business Goal & Context The ACES frameworks primary purpose to simulate the interaction depicted by Figure 5. This shopping agent workflow follows common design pattern that is well-evaluated in the literature [32, 54], and is already productized [1, 22, 29, 36, 37, 55]. The shopping agent workflow employed: 1. Opens browser 2. Navigates to the target platform 3. Searches for product 4. Captures screenshot 5. Selects product using the provided tool ACES seeks to collapse an agents journey to the linear sequence stated above, while accurately emulating production-ready shopping agent, interacting with the underlying VLM models in 21 Figure 5: AI agentic workflow for seamless shopping. The user delegates shopping to an AI agent and the AI agent autonomously navigates the platform website with the help of tool use. uniform manner, and ensuring that the choice behaviors of the model are accessible and measurable. Unlike many real-world examples such as [1, 38] that employ external modules to increase capabilities in arbitrary tasks, ACES seeks to leave the VLM unassisted to more accurately approximate outcomes. A.2 High-level Architecture & Approach Given the complexity of employing VLM agents in the shopping task, ACES is designed with the following principles: Reusability Stakeholders, future researchers, and engineers might look to ACES for future research. Adapting the mock-app, providing new experiments, and extending the agent has been kept in mind. Provider/Model agnostic Using VLMs in the agent context raises 2 primary hurdles: the low-level code to interface with these VLMs is not the same; not all VLMs are designed to interact with external environments as signaled by the diverging implementations of [22, 55]. We solve this by using LangChain [7] which provides single interface for interacting with VLMs, and collapsing the action space to single tool call. As byproduct, the code is rendered easily extensible. Observability Understanding results and debugging must have minimal dependencies on external frameworks. As such, the entire agent journey is saved to tree-like file hierarchy which can be easily inspected. 22 Cost & Efficiency To ensure efficiency, the framework includes the boilerplate code to utilize batch processing to allow experimentation to be run at high-throughput. To conduct experiments which model Figure 5, ACES is comprised of 3 top-level components agent, experiments, and sandboxeach responsible for different portions of the workflow, and who interact through clearly defined interfaces. The majority of the code is contained within experiments which encapsulates the dataset generation and randomization, collection of the results, and batch processing. The shopping agent itself is defined within agent; and sandbox contains the mock-app. Figure 6: Mock-up app screen. The mock-app displays an assortment of eight products for each product category arranged in grid with two rows and four columns."
        },
        {
            "title": "B Related Literature",
            "content": "Our work connects to rich body of literature spanning computer-use agents, e-commerce systems, recommendation and ranking algorithms, behavioral biases in human decision-making, and recent advances in multi-modal AI agents. Across these threads, our contribution is to provide controlled, visual, and interactive sandbox that zooms in on the choice step (which product to buy) for AI shopping agents, and measures rationality and choice behavior via randomized page layouts, attributes, and badges. Although ACES resembles prior computer-use testbeds such as WebArena and VisualWebArena, our emphasis is different: rather than evaluating end-to-end web navigation, we focus on single critical stepselecting which product to buyto obtain finegrained view of agent choice behavior in shopping contexts [27, 58]. 23 Computer-Use Agents and Benchmarks. fast-growing body of work builds web and OSinteracting agents evaluated in realistic environments. WebArena introduces reproducible, longhorizon web tasks over functional sites (e.g., e-commerce, forums), with baseline agents achieving low end-to-end success relative to humans [58]. VisualWebArena extends to multimodal interfaces with images and forms [27]. Mind2Web targets generalist web agents across 137 real websites [11], and follow-ups examine grounding for GPT-4V as generalist web agent [57]. Beyond the browser, OSWorld provides real-computer environment (Ubuntu, Windows, macOS) and documents large headroom between SOTA agents and humans [51]; Windows Agent Arena and AndroidWorld further expand to Windows and Android ecosystems [5, 34]. Work studying agentcomputer interfaces shows how UI design and tool affordances affect agent success (e.g., SWE-agent) [53]. Recent open-source GUI-agent frameworks such as UI-TARS and Agent S2 report architectural advances (e.g., GUI grounding, hierarchical planning) and improved benchmark performance [1, 56]. In light of the documented low end-to-end success rates on complex multi-step tasks, our framework intentionally constrains the interaction and isolates the choice problem so that we can study rational behavior and the influence of layout and badges without conflating failures from unrelated subroutines (e.g., brittle scrolling or app switching). Autonomous Shopping Agents and Multimodal Product Understanding. WebShop frames shopping as instruction-following in simulated web store, training agents with imitation and reinforcement learning [54]. More recent shopping evaluations include Shopping MMLU (text-based, multi-task retail skills) and DeepShop (deep research shopping agents with live navigation), as well as dialog-driven and domain-tuned LLMs for e-commerce such as eCeLLM and LiLiuM [9, 21, 24, 32, 40]. Vision-language product understanding models (e.g., PUMGPT) address attribute extraction and classification from images and text [52]. Our ACES sandbox complements these works by providing controllable setting with randomized listings and visual perturbations to test whether shopping agents exhibit instruction adherence and choice rationality, and to understand their choice behavior, in the presence of positions, promotions, and badges. Product Rankings, Platform Design, and Assortment Optimization. Empirical and theoretical work shows that positions and rankings can causally shape what consumers examine and buy. [48] provides experimental evidence on the causal impact of rankings on online search and purchases; [15] quantifies direct and interaction effects between ranking and ratings on consumer behavior and platform revenue. Our work is related to this line of research as we investigate the causal impact of rankings (positions) and product attributes on selection by an AI agent. There is related theoretical and empirical work modelling the consumers search process and developing platforms ranking algorithm [8, 12]. Our work also connects to long line of work on assortment optimization which takes customer/AI shopper behavior (as estimated by us) as an input, e.g., 24 see [10, 13, 17, 28]. Platform Endorsements and Badges. Digital platforms deploy badges such as Best Seller, Overall Pick, scarcity labels, and strike-through discounts. Recent empirical evidence suggests badges can meaningfully change clicks and add-to-cart probabilities [31]; complementary work studies badge longevity, uniformity, and bias across marketplaces, and the causal effects of badges in online communities [4, 23, 30]. Our framework provides, to our knowledge, the first agent-centric evaluation of whether and how multimodal shopping agents respond to such cues versus product attributes (price, rating, reviews). Personalization and Recommender Systems. Classical recommender systems and industrial practice highlight personalizations central role in e-commerce [42, 45]. Recent LLM personalization efforts create benchmarks and methods for tailoring responses to user-specific preferences [59] and dataset resources for digital-twin style behavioral modeling [47]. In this paper we do not personalize the agents; we deliberately use generic prompts to elicit baseline behavior that is not pre-solved by over-specifying the target item. Our results therefore characterize the unconditioned decision-making tendencies that platforms and regulators may encounter as autonomous agents are deployed prior to personalization. Algorithmic Delegation. Theoretical work on delegation studies how principal can design mechanisms for delegating to an agent whose incentives or information differ from those of the principal, and payments are not possible [3]. [26] show that appropriately constrained delegation can approximate efficient search despite misaligned incentives; [20] find benefits of delegating search to multiple agents; and [19] study the optimal design of algorithmic delegates to help the user under information asymmetry, given that certain task categories will be delegated to them by users while others will not. Our empirical results can be viewed as documenting the outcome of delegation to AI agents from leading providers in concrete market setting: given visual catalog, what do agents buy and why, and how do platform levers and product attributes translate into systematic selection patterns?"
        },
        {
            "title": "C Experimental Setup and Evaluation Details",
            "content": "Vision-Language-Models Evaluation. For VLM evaluation, we consider the following models: Anthropics Claude Sonnet 3.5, 3.7, and 4; OpenAIs GPT-4o and GPT-4.1; and Google DeepMinds Gemini 2.0 Flash and Gemini 2.5 Flash. Notably, both OpenAIs GPT-4o and Google DeepMinds Gemini 2.0 Flash are directly productized as shopping agents through OpenAIs Operator [29, 37] and Googles Project Mariner [16, 55], respectively. Following the initial rationality checks across all models, we conduct our detailed analysis on the latest release from each provider, under the 25 assumption that older versions will be deprecated in favor of their successors in advanced applications. To maintain roughly uniform baseline of capability, we exclude reasoning models and explicit reasoning componentssuch as omitting the default reasoning tokens for Gemini 2.5 Flash and Anthropics Claude Sonnet 4, whereas OpenAIs GPT-4.1 does not support reasoning. Furthermore, reasoning models were excluded from our selection due to their higher latency, making them unfavorable in user-facing agentic tasks which require low latency inference [25, 41, 49]. Universe of Product Categories. On the platform side, we select eight product categories, fitness watch, iPhone 16 Pro cover, mousepad, office lamp, stapler, toilet paper, toothpaste, and washing machine chosen to represent mix of highand low-ticket items, to generalize across broad range of product types, and to include both well-known and niche brands. For each of the product categories, we choose eight representative products (found based on public information on the Amazon website) which form our assortment of products which we then display in 2 grid layout on our mock-app. C."
        },
        {
            "title": "Instruction Following and Rationality Experiments",
            "content": "We provide some additional details on the experimental design and setup for our instruction following and rationality experiments. C.1."
        },
        {
            "title": "Instruction Following Experiments",
            "content": "We test the models for three types of instruction following tasks: (i) the ability to choose product within specified budget, (ii) choose specific color and (iii) choose specific brand. For each of the tasks, we selected two product categories summarized in Table 2. The tasks were specified in such way that there was unique product that satisfied the query. Figure 7 displays the mock-app setup for the instruction following tasks. Table 2: Product categories shown for different tasks Task Product Category Specified Task Budget-Constrained fitness watch Budget-Constrained toilet paper mousepad Color-based stapler Color-based iphone 16 pro cover Choose the Otterbox brand Brand-based toothpaste Brand-based The budget constraint is $25 The budget constraint is $10 Choose the pink color product Choose the pink color product Choose the Colgate brand C.1.2 Price-Based Rationality Experiments We have two types of price-based rationality experiments. In both, all products are identical except for price. In one setting (i) all price are equal, except for the price of one listing which is decreased 26 (a) Choose product within specified budget (b) Choose specified color Figure 7: Design of different types of instruction following rationality tests. The optimal choice is highlighted in the red bounding box. (c) Choose specified brand by Œ±-fraction and in second setting (ii) we assign random prices to each of the listings drawn from normal distribution (¬µ, œÉ2) with mean ¬µ and variance œÉ2. For (i), we consider Œ± 27 {0.1, 0.05, 0.01} which corresponds to 10%, 5% and 1% discount in the prices with respect to the other prices respectively (see Figure 8a). For (ii), we set the mean ¬µ as the average of the eight product prices in their respective category. For the variance, we consider œÉ = $0.3 (low variance; see Figure 8c) and œÉ = 0.2¬µ (high variance; see Figure 8e). C.1.3 Rating-Based Rationality Experiments We have three rating-based experiments: (i) the rating of one listing is increased by 0.1 (see Figure 8b), (ii) random ratings for the eight products with ratings ranging from 4.4 to 4.7 (low variance; see Figure 8d) and (iii) random ratings for the eight products with ratings ranging from 3.0 to 4.5 (high variance; see Figure 8f). Note that except for the ratings, all the other product attributes are the same. C.2 Choice Behavior and Biases To estimate the sensitivity of the AI agent to different platform levers, we create dataset with exogenous variation across number of variables associated with the products (see Table 3). Given these variations are exogenous to the inherent attractiveness of the products, they allow us to obtain causal estimates to the impact of different platform levers like position, tags and product attributes like price, rating and number of reviews. We also provide visualization of the exogenous variation of price, rating and number of reviews around their original value for few product categories in Figure 9. Figure 10 provides an example of different mock-app screens for two different experiment runs. Notice that the assortment of the eight products is the same across the two experiments. However the position of the eight products change across the two experiments along with their price, rating and number of reviews. Also notice that the Sponsored, Overall Pick and Only Remaining (scarcity) tags are assigned to different products across the two experiments. C.3 Seller Response to AI-assisted shopping In this section, we aim to design an experiment to measure the impact of seller-side AI agents recommendations for the product description when buyers use particular agent. To estimate the causal impact of the recommendation made by the AI seller agent, we have the following experimental setup. 1. Simulate the purchase behavior of the AI buying agent across different product categories. We run 200 experiment trials where we present the AI buying agent (Claude Sonnet 4, GPT-4.1 and Gemini 2.5 Flash) with eight products per category. The eight products are shuffled uniformly at random across the 200 experiments. Note that we only shuffle the position of the (a) 10% discount on one listing (b) +0.1 increase in rating of one listing (c) Random prices (with low variance) (d) Random ratings (with low variance) (e) Random prices (with high variance) (f) Random ratings (with high variance) Figure 8: Design of price and rating based rationality test. The optimal choice is highlighted in the red bounding box. products on the screen but do not change any of their other attributes like price, description, etc. 2. For each category, we select uniformly at random single product from the eight-product assortment. The selected product becomes the focal product/seller that optimizes its description via the AI seller agent. 3. Prompt the AI seller agent (e.g., GPT-4.1) to generate recommendations for the product description. We provide the AI seller agent screenshot of the platform with the eight products along with the features of the sellers product, some context and the sales data of competing sellers induced by the buying AI agent. The exact prompt is reproduced below. 29 Table 3: Randomized variations used to identify agents choice sensitivities. Ind. Variables"
        },
        {
            "title": "Position\nSponsored Tag\nOverall Pick Tag\nScarcity Tag",
            "content": "Randomly permute the position of the eight products Randomly assign Sponsored tag to listings, Unif({1, . . . , 4}) Randomly assign Overall Pick tag to listing without Sponspored tag Randomly assign Only Remaining tag to listing without Sponsored or Overall Pick tag, Unif({1, . . . , 5}) Randomly perturb the original price pj of each product j, pj fj, where fj logNormal(¬µ = 0, œÉ = 0.3) Randomly perturb the original rating rj of each product j, rj + Œ±j (5 rj), where Œ±j Unif([0.8, 0.8]) Number of reviews Randomly perturb the original number of reviews Nj of each product j, Nj fj, where fj logNormal(¬µ = 0, œÉ = 1)"
        },
        {
            "title": "Rating",
            "content": "N"
        },
        {
            "title": "Prompt Template for AI seller agent",
            "content": "Context: There is an AI agent (a vision language model) which is given screenshot of an e-commerce website selling particular product and it decides on the products based on the given attributes. have the sales data on all the products including mine. want you to act as an agent on my behalf and suggest changes to the product title so that can increase my sales by making my product more appealing to the AI agent. My product is {product name}. The product features are {detailed features from Amazon} The sales data on the competition is below. {simulated purchase data from the 200 experiments with the original descriptions}. It is important that the title changes you suggest align with the product features provided to you. Do not make up product features or add spurious keywords. Just use the product feature information provided. 4. Change the product description of the seller with the one recommended by the AI seller agent and simulate the purchase behavior of the AI agents across different product categories. Repeat the 200 experiment trials as before with the only change being the modified product description of the seller. Note that for simplicity in this study of seller response, we assume that only one seller uses an AI agent to optimize their product description, and the seller tries only one AI generated recommendation (through one interaction with their AI agent). 30 (a) mousepad (b) stapler Figure 9: Visualization of the exogenous variation in price, rating and number of reviews (c) toothpaste"
        },
        {
            "title": "D Main Results Details",
            "content": "D.1 Market Share of different models across different categories We run 1000 experiments for each product category in which we randomly permute the position of the products across experiments, and deploy an AI buying agent. For each product category, we compute the induced market shares, representing the selection rates of different products across the 1000 experiments. We present the market shares induced by different AI models in Figure 1 for the fitness watch and stapler categories and in Figure 11 for six additional product categories. 31 (a) (b) Figure 10: Example of the mock-app for different experiments with exogenous variations in position, tags and product attributes (a) iPhone 16 Pro Cover (b) mousepad (c) office lamp (d) toilet paper (e) toothpaste (f) washing machine Figure 11: Market shares induced by different AI buying agents across six product categories. D."
        },
        {
            "title": "Instruction Following and Basic Rationality",
            "content": "D.2.1 Instruction Following Experiments We summarize our experiments, and the associated fail rates, in Table 4. 32 Table 4: Fail rate of different models on instruction-following tasks (standard errors in parentheses)."
        },
        {
            "title": "Budget\nColor\nConstrained Based",
            "content": "Claude Sonnet 3.5 Claude Sonnet 3.7 Claude Sonnet 4.0 GPT4o GPT4.1 Gemini 2.0 Flash Gemini 2.5 Flash 4.0% (0.5%) 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 13.5% (1.4%) 4.4% (0.5%) 3.8% (0.5%) 0.0% 0.0% 0.0% 0.0%"
        },
        {
            "title": "Brand\nBased",
            "content": "0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% D.2.2 Price-Based Rationality Experiments We summarize our experiments and the associated fail rates in Table 5. Table 5: Fail rate of different models on price-based rationality tests (std. errors in parentheses). Price reduced Price reduced for one listing for one listing (5% discount ) (1% discount) Price reduced for one listing (10% discount) Random prices (low var.) Claude 3.5 Claude 3.7 Claude 4.0 GPT4o GPT4.1 Gemini 2.0 Gemini 2.5 63.7% (1.7%) 21.0% (0.9%) 0.5% (0.1%) 25.8% (1.0%) 9.3% (0.6%) 2.8% (0.2%) 1.0% (0.1%) 36.8% (1.4%) 6.5% (0.7%) 1.8% (0.2%) 5.6% (0.4%) 1.3% (0.1%) 0.5% (0.1%) 0.0% 32.5% (1.0%) 1.2% (0.2%) 0.0% 6.8% (0.3%) 1.2% (0.2%) 1.2% (0.2%) 0.0% 8.3% (0.4%) 8.3% (0.4%) 8.3% (0.3%) 17.4% (0.9%) 12.6% (0.8%) 1.0% (0.1%) 0.8% (0.1%) Random prices (high var.) 5.0% (0.3%) 6.0% (0.3%) 4.3% (0.2%) 3.6% (0.2%) 0.8% (0.1%) 6.5% (0.3%) 0.0% D.2.3 Rating-Based Rationality Experiments We summarize our experiments, and the associated fail rates, in Table 6. Table 6: Fail rate of different models on rating-based rationality tests (std. errors in parentheses). Rating of one listing Random ratings Random ratings increased by 0.1 (high variance) (low variance) Claude Sonnet 3.5 Claude Sonnet 3.7 Claude Sonnet 4.0 GPT4o GPT4.1 Gemini 2.0 Flash Gemini 2.5 Flash 57.3% (1.5%) 6.7% (0.5%) 28.7% (1.2%) 71.7% (0.9%) 15.1% (0.6%) 0.0% 0.0% 16.3% (0.8%) 0.0% 9.4% (0.6%) 16.0% (0.5%) 11.7% (0.5%) 0.0% 0.0% 2.7% (0.2%) 0.0% 4.7% (0.3%) 7.3% (0.4%) 6.0% (0.4%) 0.3% (0.1%) 0.0% D.2.4 Analysis of Failures On the experiments where the models failed, we probed the explanation provided by the models. Broadly speaking, we identified three types of explanations. Perceptual Limitations. The model treated all products as identical, failing to recognize differences in price or rating. This type of error was particularly prevalent when the difference was subtle, for e.g., in the case when the price of one listing was reduced by 1%. Unjustified Sub-optimality. The model recognized the optimal product (e.g., the cheapest or highest-rated) but still made sub-optimal choice without providing any specific justification for its decision. Dismissal with justification. The model acknowledged the optimal product but explicitly provided reason for not selecting it. These reasons often included: Prioritizing Standard Options. The model would state it chose the standard price/rating or the most common price point, indicating preference for typical values over slight deviations. Attributing difference to errors. The model sometimes dismissed the optimal option by attributing the observed difference to display error, temporary discount, an outdated listing, or pricing error on the platform. Perceived Insignificance. The model might justify its sub-optimal choice by stating the price difference was minimal or that there was no apparent/significant advantage or no functional difference compared to other options. D.3 Choice Behavior and Biases Understanding the trade-offs between position, tags and listing attributes. We fit conditional logit with the utility specification in (3.1). The estimates are provided in Table 1. We can use the estimates in Table 1, (3.1) and (3.2) to understand how the different models trade-off different attributes. For example, we can be interested in understanding how the choice probability changes if there is Sponsored tag or an Overall Pick tag or rating changes by 0.1. Lets say that the baseline selection probability of product is pj and assume that we change co-variate with coefficient Œ≤z by z. Let oj = pj/(1 pj) be the odd-ratio for product j, then the new selection probability j is given as (a) = exp(Uj) exp(Œ≤zz) k=i exp(Uk) + exp(Uj) exp(Œ≤zz) (cid:80) (b) = pj exp(Œ≤zz) 1 pj + pj exp(Œ≤zz) (c) = exp(Œ≤zz)oj 1 + exp(Œ≤zz)oj , 34 where (a) follows from (3.2), (b) follows by dividing numerator and denominator by (cid:80) and the fact that pj = exp(Uj)/ (cid:80) we can calculate how the probability will change if we change some covariate. exp(Uk) k=i exp(Uk) and (c) follows from the definition of oj. Using this, We can also obtain price-equivalent trade-offs: convenient way to compare levers is to ask: by how much can seller raise (or must cut) price to keep the utility constant when the seller adds (or loses) feature worth = Œ≤zz. Let say, we change the price by multiplicative factor of Œª, i.e., = ŒªP where is the original price of the product. Since we are keeping the utility constant, (cid:1). This allows to be get we have that Œ≤price ln Œª + Œ≤zz = 0, this implies that Œª = exp (cid:0)Œ≤zz/Œ≤price price-based exchange rate between different attributes. In Table 7, we present the price-equivalent results for different covariates. Table 7: Price-equivalent tradeoffs implied by the CL coefficients. Entries are approximate multiplicative changes in price that leave utility unchanged when the corresponding lever is added (top block) or removed/penalized (bottom block). Claude Sonnet 4 GPT-4. Gemini 2.5 Flash How much could seller raise prices (holding utility fixed) if the seller get... Row 1 (vs Row 2) Overall Pick tag +0.1 rating Double reviews +112.6% +92.2% +35.4% +19.4% +17.0% +137.8% +27.9% +17.2% +91.2% +64.5% +67.3% +37.4% How much would seller need to cut price to offset the harm of ... Sponsored tag Scarcity tag -14.3% -6.3% -8.0% -4.6% -11.3% -14.5% D.4 Seller Response to AI-Assisted Shopping D.4.1 Case Study of Sellers Response to AI-assisted shopping We consider the illustrative example of the mousepad product category. Within this category, our focal product (randomly selected) is the brand Aothia. We prompt an AI seller agent (GPT-4.1 in this case) with the simulated demand on the original product descriptions of the eight products in the catalog using GPT-4.1 as the AI buying agent. Below is the response of the AI seller agent on the recommended product description change. AI Seller Agents (GPT-4.1) Response Heres how to make your Aothia Leather Desk Pad Protector title more AIand customerfriendly, using only your provided features and aligning with whats selling well on this page: Competitive Patterns 35 Top competitors (Amazon Basics, KTRIO, MROCO) lead with the products main use (Ergonomic Mouse Pad, Gaming Mouse Pad) and then surface key features: material, size, comfort, non-slip, waterproof, color. Titles are concise, not overloaded, but cover all core attributes and benefits. Current Title Aothia Leather Desk Pad Protector, Mouse Pad, Office Desk Mat, Non-Slip PU Leather Desk Blotter, Laptop Desk Pad, Waterproof Desk Writing Pad for Office and Home (Black, 23.6 13.7) Weaknesses Good breadth, but the most important features are spread out. Leather Desk Pad Protector is slightly genericPU Leather is value signal. Non-Slip and Waterproof should be earlier. Large or XL size callout (if true) is attractive for this market."
        },
        {
            "title": "Optimized Title Suggestions",
            "content": "Option 1: Feature-Rich, Direct Aothia Large PU Leather Desk Pad, Non-Slip Waterproof Mouse Pad & Desk Mat, 23.6 13.7, Office and Home, Black"
        },
        {
            "title": "Why",
            "content": "Leads with brand and size (Large). PU Leather specifies the material. Desk Pad, Non-Slip, Waterproof are high-signal attributes and not buried. Mouse Pad & Desk Mat covers multi-purpose use. Size and color at the end for clarity and direct comparison. Option 2: Even More Focused Aothia PU Leather Desk Pad Protector, Large Non-Slip Waterproof Mouse Pad, 23.6 13.7, Office Desk Mat, Black Why Highlights core value (protection, size, non-slip, waterproof) right after the brand. Short and very clear. Option 3: Ultra-Compact Aothia Large PU Leather Desk Pad, Non-Slip Waterproof, 23.6 13.7, Black Why Super concise for listings where title space is limited. Why These Work All keywords (PU Leather, Large, Non-Slip, Waterproof, Mouse Pad, Desk Mat, Size, Black) are present and near the front. Follows competitor structure for quick parsing by AI agents. No added or exaggerated claimsuses your real features only."
        },
        {
            "title": "Final Recommendation",
            "content": "To compete with both Amazon Basics and KTRIO, and help the AI agent recognize your desk pads versatility and core selling points, use: Aothia Large PU Leather Desk Pad, Non-Slip Waterproof Mouse Pad & Desk Mat, 23.6 13.7, Office and Home, Black We run new set of experiments where we just change the product description (see Figure 12a) of the focal product (Aothia in this case), with different AI buying agents (Claude Sonnet 4, GPT-4.1 and Gemini 2.5 Flash). In this case, we find that this rather innocuous looking change in the product description can significantly shift the market share. In Figure 12b, we plot the market share of different products for the mousepad category when using GPT-4.1 as the AI buying agent. We observe that there is statistically significant and large increase of 21.8% in the market share of the focal product Aothia. This comes at the expense of decrease in market share of other products like the Amazon Basics Mousepad, MROCO and SteelSeries. Note that the only change across the two sets of experiments is the change in the product description, hence this impact is causal. This example illustrates the potential of changing market shares by simply optimizing the product description to appear more attractive to an AI buying agent. This is very much akin to search engine optimization (SEO), which is the practice of optimizing keywords in website in order to have it rank higher in search results. We note that the gains in market share are not uniform across different AI buying agents. For the case of Claude Sonnet 4, we see that the change in market share of Aothia due to the product description change is around 5.3% and statistically significant while for the case of Gemini 2.5 Flash, the change in market share is only 2.0% (and not statistically significant), cf. Figures 12c and 12d. This demonstrates that there can be non-uniform impact of such interventions. In the next section, we provide the change in market shares across 37 different product categories. (a) Change in description for focal product (b) Market Share with GPT-4.1 as AI buying agent (c) Claude Sonnet 4 as AI buying agent (d) Gemini 2.5 Flash as AI buying agent Figure 12: Case Study for the mousepad category with GPT-4.1 as the AI seller agent D.4.2 Category-wise change in market share Figure 13 reports, for each category and AI agent model, the product-level selection shares before and after modifying the focal products description, with the focal item highlighted in red. Model Update: The case of Gemini 2.5 Flash Preview to Gemini 2."
        },
        {
            "title": "Flash",
            "content": "Our initial experiments used Gemini 2.5 Flash Preview, the then-current frontier model by Google Deepmind. During writing, the provider replaced it with Gemini 2.5 Flash and deprecated the Preview version. We therefore re-ran subset of experiments with the updated model to measure how an upstream change propagates to AI-mediated demand and position effects. Marketshare shifts under model upgrade. Figure 14 compares product-level selection shares when the buyer agent updates from Gemini 2.5 Flash Preview to Gemini 2.5 Flash, holding the catalog and attributes fixed and only randomizing on-page positions. Shares move substan38 (a) fitness watch (b) iPhone 16 pro cover (c) mousepad (d) office lamp (e) stapler (f) toilet Paper (g) toothpaste (h) washing machine Figure 13: Market shares before and after focal product description modification. The focal product is marked in red. tially, with frequent flips in the modal product: e.g., in office lamp category the mode shifts from TORCHSTAR (Gemini 2.5 Flash Preview) to SUNMORY (Gemini 2.5 Flash); in toilet paper from Cottonelle to Angel Soft. Several products experience large swings in share (e.g., Fitbit Versa: 39 25.1%; MROCO: 16.5%; Kenmore: 25.6%). (a) fitness watch (b) iphone 16 pro cover (c) mousepad (d) office lamp (e) stapler (f) toilet paper (g) toothpaste (h) washing machine Figure 14: Market share of different products with Gemini 2.5 Flash Preview and Gemini 2.5 Flash Position effects. Table 8 shows that position biases also change: Gemini 2.5 Flash Preview exhibits negative top-row bias, while Gemini 2.5 Flashs is positive. Both versions penalize Columns 12; Gemini 2.5 Preview is roughly indifferent between Columns 34, whereas Gemini 2.5 Flash strictly prefers Column 3 over Column 4. Figure 15 shows the heatmap of selection probabilities across different positions on the page for Gemini 2.5 Flash Preview and Gemini 2.5 Flash. (a) Gemini 2.5 Flash Preview (b) Gemini 2.5 Flash Figure 15: Webpage heatmap of AI shopping agents. The figure depicts the selection rate of the same product across different positions for Gemini 2.5 Flash Preview and Gemini 2.5 Flash. Implications. These results underscore that upstream model updates can function as exogenous demand shocksrelabeling best sellers and redistributing shares even when products are unchanged. For platforms, this volatility affects the stability of ranking and endorsement signals 40 Table 8: Estimates of the Conditional Logit for Gemini models Gemini 2.5 Flash Preview Gemini 2.5 Flash Position effects Row 1 Column 1 Column Column 3 0.071* (0.031) 0.461*** (0.044) 0.581*** (0.044) 0.026 (0.043) Product Fixed Effects Yes Observations Choice Sets (Groups) Pseudo R-squared 31649 8166 0.40 0.586*** (0.042) 0.138* (0.057) 0.700*** (0.061) 0.160** (0.055) Yes 15723 3933 0. Significance is indicated as: < 0.05, < 0.01, < 0.001. (e.g., badges, recommendations) and argues for model-aware evaluation pipelines. For sellers, the findings motivate continuous monitoring and agile listing management: content tuned to yesterdays model may underperform after an upgrade. More broadly, because model changes alter both attribute sensitivities and position bias magnitudes, identical designs can yield different outcomes solely due to an upstream revisionreinforcing the need for ongoing auditing, disclosure of impactful updates, and diversification of strategies across agent ecosystems."
        }
    ],
    "affiliations": [
        "Columbia University, Graduate School of Business",
        "MyCustomAI",
        "Yale University"
    ]
}