{
    "paper_title": "DiffCLIP: Differential Attention Meets CLIP",
    "authors": [
        "Hasan Abed Al Kader Hammoud",
        "Bernard Ghanem"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose DiffCLIP, a novel vision-language model that extends the differential attention mechanism to CLIP architectures. Differential attention was originally developed for large language models to amplify relevant context while canceling out noisy information. In this work, we integrate this mechanism into CLIP's dual encoder (image and text) framework. With minimal additional parameters, DiffCLIP achieves superior performance on image-text understanding tasks. Across zero-shot classification, retrieval, and robustness benchmarks, DiffCLIP consistently outperforms baseline CLIP models. Notably, these gains come with negligible computational overhead, demonstrating that differential attention can significantly enhance multi-modal representations without sacrificing efficiency. Code can be found at https://github.com/hammoudhasan/DiffCLIP."
        },
        {
            "title": "Start",
            "content": "DiffCLIP: Differential Attention Meets CLIP Hasan Abed Al Kader Hammoud* KAUST"
        },
        {
            "title": "Bernard Ghanem\nKAUST",
            "content": "5 2 0 2 9 ] . [ 1 6 2 6 6 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We propose DiffCLIP, novel vision-language model that extends the differential attention mechanism to CLIP architectures. Differential attention was originally developed for large language models to amplify relevant context while canceling out noisy information. In this work, we integrate this mechanism into CLIPs dual encoder (image and text) framework. With minimal additional parameters, DiffCLIP achieves superior performance on image-text understanding tasks. Across zero-shot classification, retrieval, and robustness benchmarks, DiffCLIP consistently outperforms baseline CLIP models. Notably, these gains come with negligible computational overhead, demonstrating that differential attention can significantly enhance multi-modal representations without sacrificing efficiency. Code can be found at https://github.com/hammoudhasan/ DiffCLIP. 1. Introduction Vision-language models (VLMs) have made remarkable progress in bridging the gap between textual and visual modalities, enabling powerful capabilities such as zeroshot image classification, image-text retrieval, and descriptive captioning [17, 33]. By aligning images and text in joint embedding space, these models capture broad semantic relationships across modalities and often excel at out-of-distribution generalization. Among VLMs, Contrastive Language-Image Pre-training (CLIP) [33] stands out as foundational approach, demonstrating strong zeroshot performance on numerous benchmarks with minimal fine-tuning. While CLIPs contrastive training regime has been widely adopted, its attention mechanism can sometimes focus on irrelevant or spurious features in both the image and text encoders. This attention noise can hamper fine-grained understanding, particularly when precise localization or exInterestingly, replicit contextual knowledge is required. Figure 1. CC3M Pretraining: CLIP vs. DiffCLIP Across Six Tasks. We compare standard CLIP (blue) and our DiffCLIP variant (pink) on linear probing, few-shot classification, image/text retrieval, zero-shot ImageNet, and zero-shot OOD. In each case, DiffCLIP consistently outperforms CLIP, highlighting the effectiveness of differential attention with only 0.003% extra parameters. cent language modeling research has proposed differential attention mechanism [48], which subtracts complementary attention distributions to suppress noise and highlight salient tokens. However, whether similar strategy would be effective for multimodal tasks has remained an open question. Can differential attention be adapted to vision-language models in way that meaningfully improves their ability to focus on relevant features across modalities? *Correspondence at hasanabedalkader.hammoud@kaust.edu.sa Motivated by this question, we introduce DiffCLIP, an extension of CLIP that integrates differential attention into both the vision and text encoders. By learning two attention maps and subtracting one from the other, DiffCLIP effectively cancels out misaligned or noisy signals, enabling more precise alignment of images and text. Crucially, this enhancement introduces only negligible overhead in model parameters and computational cost. Our results show that DiffCLIP consistently outperforms standard CLIP on wide range of tasksincluding linear probing, few-shot classification, image-text retrieval, out-of-domain robustness, and fine-grained visual understandinghighlighting the efficacy of differential attention in multimodal setting. As shown in Figure 1, DiffCLIP is capable of improving performance across various benchmarks with only 0.003% extra parameters. Figure 2 also shows how DiffCLIP is capable of suppressing attention noise compared to CLIP models with vanilla non-differential attention. Our contributions are threefold: We propose DiffCLIP, the first integration of differential attention into CLIP-based VLMs, yielding simple yet effective approach to reducing attention noise in both vision and text streams. Through extensive experiments on Conceptual Captions 3M/12M pretraining, we demonstrate consistent gains over baseline CLIP across diverse suite of tasks, with minimal parameter overhead of roughly 0.003%. We perform detailed ablations, showing that (i) dynamic initialization can boost zero-shot performance, and (ii) applying differential attention solely in the vision encoder already captures most of the benefits, suggesting flexible, cost-effective path to improved multimodal learning. The remainder of this paper is organized as follows. Section 2 surveys previous work on training-centric, modelcentric, and data-centric strategies for enhancing CLIP. Section 3 provides an overview of the standard Transformer attention mechanism, the differential attention concept, and the CLIP framework. Section 4 details our experimental setup, empirical results, and ablation studies, while Sections 5 and 6 concludes with discussion of future research directions and wrapping up of the paper. 2. Related Work Vision-language pre-training (VLP) has advanced our ability to learn joint representations of images and text, leading to improvements in tasks such as image retrieval, visual question answering, and zero-shot classification [10, 51]. CLIP [33] has been central to this progress by using contrastive loss to align image and text embeddings from largescale image-caption data. Despite CLIPs strong zero-shot performance, researchers continue to explore improvements in its training, architecture, and data collection strategies. These efforts generally fall into three categories: trainingcentric, model-centric, and data-centric approaches. Training-Centric Approaches common strategy is to enrich CLIPs contrastive framework with additional objectives. For example, SLIP [31] adds masked image modeling to boost downstream results, while DeCLIP [22] uses nearest-neighbor supervision to enhance data efficiency. SigLIP [50] replaces the standard softmax temperature with sigmoid loss, allowing larger batch training and improving generalization and robustness to noisy labels. RetrievalEnhanced CLIP [16] leverages external memory of imagetext pairs at inference, achieving significant gains on finegrained zero-shot tasks. Further, novel training objectives, such as those proposed by Yang et al. [47] and PyramidCLIP [11], aggregate information across multiple semantic levels, highlighting the benefit of diversified training signals for improved CLIP performance. Model-Centric Approaches Another line of work modifies CLIPs architecture for greater efficiency or accuracy. The original CLIP [33] employs Transformer [41] for text and either ResNet [13] or Vision Transformer (ViT) [7] for images. Subsequent studies incorporate ideas from object detection and segmentation to capture finer visual details, such as region-level representations [45, 53]. Recently, ViTamin [5] proposed specialized vision transformer architecture tailored specifically for multimodal models, demonstrating improved zero-shot results compared to standard ViTs under similar training setups. Other researchers attempt to unify image and text encoders into single Transformer [40], although this approach is less common. Notably, few methods have altered the core attention mechanism within CLIP. Our work addresses this gap by adapting Differential Attention [48], originally proposed for language models, to CLIPs multimodal setting. This adaptation aims to reduce attention noise and enhance representation quality. Data-Centric Approaches Data-centric methods emphasize improving the size, diversity, and quality of pre-training datasets. Initial efforts focused on scaling datasets [17, 33], while more recent approaches prioritize richer and cleaner supervision. VeCLIP [20] uses large language models (LLMs) to generate detailed and enriched captions, enhancing textual supervision. Similarly, CLIPS [27] utilizes truncated synthetic captions to improve visual grounding and retrieval performance, showing that carefully controlled synthetic textual inputs can surpass standard image-caption pairs. SynthCLIP [12] explores training entirely on synthetic image-text pairs. Further methods employ filtering techniques to eliminate noisy or irrelevant samples [1, 9], while Cluster Masking [43] proposes masking clusters of similar image patches, leading to faster training and improved representation quality. These efforts underline the potential of data curation Figure 2. Comparing CLIP vs. DiffCLIP Attention Maps. For two images (rows), we visualize where CLIP and DiffCLIP attend when matching each image against two different textual queries. While CLIP allocates attention to irrelevant background regions, DiffCLIP more effectively centers on query-relevant objects, highlighting how differential attention can reduce noise and improve focus. Queries: First Row: Mug, Lamp; Second Row: Flower, Dog. and augmentation strategies in bolstering the efficacy of CLIP-based models. Beyond performance, fairness and compositionality have also received increased attention. FairCLIP [29] addresses demographic biases found in models like CLIP by using optimal-transport-based feature alignment across demographic groups. Meanwhile, iterated learning approaches [52] tackle the compositional limitations of large visionlanguage models, promoting representations that generalize more reliably to complex and compositional visuallinguistic scenarios. In this paper, we contribute to the model-centric direction by adapting Differential Attention [48] to CLIPs dual-encoder architecture. Through this adaptation, we aim to reduce attention noise and enhance performance across various image-text understanding tasks. to align images and text in shared representation space. These components form the basis for our model and experiments. 3.1. Transformer Attention Transformer networks [41] capture relationships among elements in sequence through self-attention operation. Let RN be an input sequence of tokens (or image patches), each embedded in d-dimensional space. The Transformer maps to queries (Q), keys (K), and values (V) using learned weight matrices: = Q, = K, = , where Q, K, Rdd. Self-attention scores are then computed via scaled dot-products: = softmax (cid:16) QK (cid:17) , 3. Preliminaries and these scores are used to weight V: In this section, we outline the fundamental concepts that are essential for our approach. We begin by reviewing the Transformer self-attention mechanism [41], which is widely used in modern sequence modeling. Next, we introduce differential attention [48], technique designed to reduce attention noise by leveraging complementary attention distributions. Finally, we summarize the Contrastive LanguageImage Pre-training (CLIP) framework [33], which learns Attn(X) = V. To capture different types of relationships, Transformers use multi-head attention (MHA). An MHA module with heads splits each projection into lower-dimensional parts of size dh = d/h. In each head i, Attni(X) = softmax (cid:16) QiK dh (cid:17) Vi, where Qi, Ki, Vi RN dh. The head outputs are concatenated and projected back: MHA(X) = (cid:2)Attn1(X) . . . Attnh(X)(cid:3) O, with R(h dh)d. Despite remarkable success in many areas, standard attention can assign non-negligible weights to irrelevant tokens (often called attention noise) [18, 26], which can degrade performance in settings requiring precise focus. 3.2. Differential Attention Differential attention [48] addresses attention noise by learning two separate attention distributions and subtracting one from the other, effectively canceling out spurious alignments. Single-Head Differential Attention. Let RN be the input to single attention head. We split and into two halves, denoted by subscripts 1 and 2: [Q1; Q2] = Q, [K1; K2] = K, = , 2 . Each half computes its where Q1, Q2, K1, K2 RN own attention distribution: (cid:16) Q1K (cid:17) 1 d/2 A1 = softmax By learning complementary attention maps in each head and subtracting them, Diff MHA aims to amplify relevant patterns while reducing noise. 3.3. CLIP Training Contrastive Language-Image Pre-training (CLIP) [33] learns image and text embeddings in shared space using large collection of paired image-text examples {(Ik, Tk)}M k=1. It consists of two encoders: one for images (cid:0)fθ (cid:1). Their outputs are normalized to unit length: (cid:1) and one for text (cid:0)gϕ ui = fθ(Ii) fθ(Ii) , vi = gϕ(Ti) gϕ(Ti)2 . For batch of pairs, CLIP forms similarity matrix Sij = vj τ , where τ is (learned or fixed) temperature parameter. The text-to-image contrastive loss is , A2 = softmax (cid:16) Q2K 2 d/2 (cid:17) . Lti = 1 (cid:88) i=1 log exp(Sii) j=1 exp(Sij) (cid:80)N , The output is formed by subtracting the second distribution (scaled by learnable parameter λ) from the first: and the image-to-text counterpart is DiffAttn(X) = (cid:0)A1 λ A2 (cid:1) V. The parameter λ is trained to control how strongly the second distribution is subtracted: λ = exp(cid:0)λq1 λk1 (cid:1) exp(cid:0)λq2 λk2 (cid:1) + λinit, where λq1 , λk1 , λq2, λk2 are learnable weights and λinit is hyperparameter. This subtraction often yields sparser, more focused attention map, which can improve results in scenarios sensitive to background or redundant signals [48]. Multi-Head Extension. Like standard attention, differential attention can be extended to multiple heads. In Differential Multi-Head Attention (Diff MHA), each head applies the differential step independently: (cid:32) DiffAttni(X) = softmax (cid:16) Q1,iK 1,i (cid:112)dh/2 (cid:17) λ softmax (cid:16) Q2,iK 2,i (cid:112)dh/ (cid:33) (cid:17) Vi, where Q1,i, Q2,i, K1,i, K2,i RN (dh/2). The final output is then DiffMHA(X) = (cid:2)DiffAttn1(X) . . . DiffAttnh(X)(cid:3) O. Lit = 1 N (cid:88) i=1 log (cid:80)N exp(Sii) j=1 exp(Sji) . The overall objective is LCLIP = 1 2 (cid:16) Lti + Lit (cid:17) . By encouraging matching image-text pairs to have high similarity (and non-matching pairs to have low similarity), CLIP learns robust features that often transfer well to downstream tasks like zero-shot classification and retrieval. 4. Experiments We present an extensive empirical study to investigate whether differential attention can benefit CLIP-style visionlanguage models. We first describe our dataset sources and training configurations, then evaluate both standard CLIP and our DiffCLIP variant under linear probing, few-shot classification, and image-text retrieval. We also test robustness to distribution shifts (via OOD ImageNet) and finegrained features (via MMVP), and conclude with ablation studies on the initialization of the differential attention parameter λinit and on applying differential attention to only the vision encoder. Table 1. Classification Performance (Linear Probing and Few-Shot). We compare CLIP and DiffCLIP on nine classification tasks with two pretraining sets (CC3M and CC12M). The top block reports linear probing accuracy, while the bottom block shows few-shot results. Numbers in parentheses indicate absolute gains or drops for DiffCLIP relative to CLIP. Pretraining Model CaltechDTD Pets Flowers SUN397 Aircraft CIFAR10 CIFAR100 Food-101 Avg. Linear Probing CC3M CC3M CC12M CC12M Few-Shot CC3M CC3M CC12M CC12M CLIP DiffCLIP CLIP DiffCLIP CLIP DiffCLIP CLIP DiffCLIP 72.5 76.2 (+3.7) 88.3 89.5 (+1.2) 90.4 91.6 (+1.2) 97.4 97.6 (+0.2) 58.7 60.2 (+1.5) 61.0 62.2 (+1.2) 85.8 86.6 (+0.8) 54.1 56.2 (+2.1) 35.7 34.6 (-1.1) 83.5 83.9 (+0.4) 71.2 71.8 (+0.6) 79.5 83.0 (+3.5) 92.6 93.5 (+0.9) 68.3 69.4 (+1.1) 48.8 46.4 (-2.4) 92.0 90.7 (-1.3) 72.9 73.2 (+0.3) 69.6 71.6 (+2.0) 92.5 92.9 (+0.4) 91.8 92.8 (+1.0) 44.6 45.4 (+0.8) 81.9 82.2 (+0.3) 86.3 88.2 (+1.9) 96.9 97.3 (+0.4) 96.5 96.8 (+0.3) 56.1 55.2 (-0.9) 63.4 62.4 (-1.0) 81.3 80.3 (-1.0) 63.4 63.7 (+0.3) 74.7 73.3 (-1.4) 72.8 73.5 (+0.7) 85.1 83.3 (-1.8) 59.1 59.4 (+0.3) 77.5 77.7 (+0.2) 63.8 64.8 (+1.0) 77.0 77.3 (+0.3) 67.0 68.3 (+1.3) 86.0 87.5 (+1.5) 73.9 74.6 (+0.7) 85.3 85.4 (+0.1) Table 2. Zero-Shot Retrieval and ImageNet Zero-shot Accuracy. We report image and text retrieval (Recall@5, %) and zero-shot ImageNet accuracy (%) for CLIP vs. DiffCLIP, using CC3M or CC12M as pretraining data. Values in parentheses reflect absolute gains or drops for DiffCLIP relative to CLIP. Pretraining Model Image Retrieval (R@5) Text Retrieval (R@5) Zero-Shot Flickr30k Flickr8k MSCOCO Avg. Flickr30k Flickr8k MSCOCO Avg. ImageNet CC3M CC3M CC12M CC12M CLIP DiffCLIP CLIP DiffCLIP 31.8 32.9 (+1.1) 35.4 36.5 (+1.1) 62.5 62.2 (-0.3) 62.1 61.5 (-0.6) 19.4 20.9 (+1.5) 41.3 42.3 (+1.0) 28.9 30.1 (+1.2) 55.3 55.3 (+0.0) 43.4 44.7 (+1.3) 76.8 77.4 (+0.6) 46.2 47.8 (+1.6) 77.7 77.4 (-0.3) 25.4 27.6 (+2.2) 53.8 55.5 (+1.7) 38.3 40.1 (+1.8) 69.4 70.1 (+0.7) 13.6 14.4 (+0.8) 31.8 33.8 (+2.0) 4.1. Experimental Setup Datasets. We pretrain on Conceptual Captions 3M (CC3M) [38] and Conceptual Captions 12M (CC12M) [4]. After downloading using img2dataset [2] (with shorter edge resized to 224), we end up with about 2.3M image-text pairs for CC3M and 7.9M for CC12M. For CC3M, we train on four A100 GPUs, while CC12M uses eight A100 GPUs to reduce training time. Text data is minimally processed, limited to basic tokenization. Training Parameters. All models train for 40 epochs, using one epoch of linear warmup, global batch size of 4096, and AdamW optimizer [28]. We set the base learning rate to 5 104 with weight decay of 0.5. For DiffCLIP, every attention layer in both the vision and text encoders is replaced with differential attention. We initialize each layers λ at 0.8 unless stated otherwise. This setup introduces only minor parameter overhead: roughly 0.003% additional parameters relative to standard CLIP-B/16. Training parameters are chosen similar to SynthCLIP [12] and training code is adopted from SLIP [31]. Evaluation Protocol. We follow established practices for linear probing and few-shot evaluation [8] on nine image-classification datasets: DTD [6], Flowers [32], Pets, Caltech-101 [21], Aircraft [30], CIFAR-10 [19], SUN397 [44], CIFAR-100 [19], and Food-101 [3]. For retrieval (image-to-text and text-to-image) on Flickr8k [34], Flickr30k [49], and MSCOCO [24], we use the LAION CLIP Benchmark framework [37]. We measure zero-shot robustness on ImageNet [36] and its variants (ImageNet-V2 [35], ImageNet-A [15], ImageNet-R [14], and ImageNet-Sketch [42]). Finally, we use the MMVPVLM benchmark [39] to check how well each model focuses on fine-grained visual details. 4.2. Do CLIP Models Benefit from Differential Attention? Motivation. To evaluate the effectiveness of our proposed DiffCLIP, we test its performance across tasks involving image classification, image-text retrieval, and zero-shot generalization, following common benchmarks established in prior literature [12]. Figure 3. OOD Zero-Shot ImageNet Performance. Comparison of zero-shot accuracy (%) on ImageNet, ImageNet-V2, ImageNet-A, ImageNet-R, and ImageNet-Sketch, plus the average. Bars show performance of CLIP (blue) versus DiffCLIP (pink), trained on CC3M (left) or CC12M (right). Numerical deltas above the bars indicate the absolute improvement or drop for DiffCLIP relative to CLIP. DiffCLIP improves on average the zero-shot performance on OOD ImageNet datasets as compared to CLIP. Results. We compare baseline CLIP-B/16 to our DiffCLIP-B/16 (with differential attention in both vision and text encoders). Table 1 shows linear probing and fewshot classification results for models pretrained on both CC3M and CC12M. DiffCLIP outperforms standard CLIP on almost every dataset. For example, with CC3M pretraining, DiffCLIP achieves about +1% gain in linear probing and +0.7% in few-shot accuracy. Table 2 presents retrieval metrics and zero-shot ImageNet. DiffCLIP again surpasses CLIP on image and text retrieval: for CC3M, we see an average improvement of about 1.2% (image retrieval) and 1.8% (text retrieval). On zero-shot ImageNet, DiffCLIP-CC3M increases accuracy by 0.8%, with even larger gains of +2.0% when using CC12M. Conclusion. Even though DiffCLIP only adds tiny fraction of extra parameters, it consistently outperforms standard CLIP on classification and retrieval benchmarks. This suggests that differential attention is lightweight yet effective way to enhance visionlanguage representation. 4.3. Does Differential Attention Improve Out-ofDomain Robustness? Motivation. Having observed improvements on indistribution ImageNet, we ask if these gains carry over to more challenging out-of-domain variants. Real-world applications often involve domain shifts, and CLIPs zero-shot adaptability has been tested on ImageNet-V2, ImageNet-A, ImageNet-R, and ImageNet-Sketchbenchmarks known to stress model robustness beyond standard ImageNet. Understanding how differential attention influences robustness in such scenarios is crucial for assessing its practical utility in deployment settings. We aim to see if differential attention helps maintain or improve performance under such shifts. ImageNet-A, ImageNet-V2, Results. Figure 3 summarizes zero-shot performance across ImageNet-R, and ImageNet-Sketch. Models with differential attention outperform standard CLIP by an average of 2.1%, suggesting that subtracting noisy attention patterns yields features that generalize more robustly, even under significant distribution shifts. Conclusion. DiffCLIP not only enhances indistribution performance but also strengthens zeroshot robustness against substantial domain shifts, further demonstrating the benefits of differential attention. 4.4. Does DiffCLIP Improve Fine-Grained Visual Understanding? MMVP-VLM Benchmark. To test fine-grained visual understanding, we employ the MMVP-VLM benchmark [39]. This benchmark measures how well vision-language models capture nuanced visual properties, such as object orientation, presence, and relational context, beyond straightforward recognition. Both CLIP and DiffCLIP are pretrained on CC12M under identical settings. Results. On average, DiffCLIP improves MMVP-VLM accuracy by 5.7% relative to baseline CLIP. radar plot (Figure 4) shows DiffCLIP surpassing or matching CLIP on nearly all categories except one (state and condition). This suggests that subtracting noisy attention patterns (via differential attention) helps the model attend to more subtle details in images. Conclusion. By mitigating extraneous context through differential attention, DiffCLIP achieves stronger fine-grained visual understanding. These gains highlight the effectiveness of explicitly canceling irrelevant attention weights in multimodal settings. 4.5. Dynamic or Static λinit? Motivation. All previous experiments used fixed initialization λinit = 0.8 for differential attention. However, [48] proposes dynamic schedule: λinit(l) = 0.8 0.6 exp(0.3 l), where is the layer index. We denote the model using this schedule as DiffCLIP. Results. Figure 5 summarizes six tasks: linear probing, few-shot classification, image retrieval, text retrieval, zeroshot ImageNet, and zero-shot OOD. Compared to the baseline CC12M CLIP, DiffCLIP improves zero-shot ImageNet by +2.8% and text retrieval by +1.5%. It also raises zero-shot OOD accuracy by +1.3%. However, relative to standard DiffCLIP (with fixed λinit = 0.8), DiffCLIP is +0.8% better on zero-shot ImageNet and +0.8% on text retrieval, but it underperforms or only slightly improves on other tasks. For instance, in zero-shot OOD, DiffCLIP is -0.8% behind standard DiffCLIP. Conclusion. dynamic λ schedule yields notable gains on zero-shot ImageNet and text retrieval, though it lags behind the simpler constant initialization on several other benchmarks. Future work might explore how best to tune or combine these schedules to achieve consistent improvements. 4.6. Does Applying Differential Attention to Vision Only Suffice? Motivation. Because the vision encoder often plays dominant role in CLIP models, one might ask whether differential attention is necessary in both encoders. We define DiffCLIP as variant that integrates differential attention only in the vision encoder, leaving the text encoder with regular attention. Results. Figure 5 compares CLIP, DiffCLIP, and DiffCLIP across six tasks: linear probing, few-shot classification, image retrieval, text retrieval, zero-shot ImageNet, and zero-shot OOD. DiffCLIP improves upon baseline Figure 4. MMVP-VLM Benchmarking. Radar plot illustrating performance on different fine-grained visual categories. Both models (CLIP in blue, DiffCLIP in pink) are evaluated on properties like orientation, positional context, and color appearance. DiffCLIP (average 27.6%) consistently outperforms CLIP (average 21.9%), demonstrating more focused attention on subtle visual details. CLIP by +0.1% in linear probing, +0.3% in few-shot, +0.4% in image retrieval, +1.2% in text retrieval, +1.9% on zero-shot ImageNet, and +2.3% on zero-shot OOD. Compared to DiffCLIP, DiffCLIP surpasses or matches performance on few-shot, image retrieval, text retrieval, and zero-shot OOD, but is slightly behind on linear probing and standard zero-shot ImageNet. Conclusion. Applying differential attention solely to the vision encoder already brings sizable gains. Interestingly, DiffCLIP can even match or exceed full DiffCLIP on certain tasks, suggesting that most of the performance boost may come from more robust visual feature extraction. 5. Future Directions & Limitations 5.1. Beyond CLIP An intriguing question for future research is how vision encoder trained with differential attention within the CLIP framework would perform when integrated into larger, more sophisticated vision-language models such as LLaVA [25] or TinyLLaVA [54]. To provide initial insights into this possibility, we conducted preliminary experiments by combining our DiffCLIP-CC12M vision encoder with the QwenFigure 5. Comparing Different DiffCLIP Variants. We evaluate four models on six tasks (linear probing, few-shot, image retrieval, text retrieval, ImageNet zero-shot, and zero-shot OOD), all pretrained on CC12M. CLIP (blue) is the baseline, DiffCLIP (pink) uses fixed differential attention parameter, DiffCLIP (purple) employs dynamic schedule for differential attention, and DiffCLIP (yellow) applies differential attention only to the vision encoder. Table 3. CLIP vs. DiffCLIP on POPE Hallucination Benchmark. We compare models across three POPE categories, showing accuracy, precision, and recall. Absolute improvements of DiffCLIP over CLIP are highlighted in parentheses. CLIP DiffCLIP POPE Accuracy Precision Recall Accuracy Precision Recall Random Popular Adversarial 50.14 50.27 50. 50.07 50.13 50.08 98.21 99.33 99.33 50.41 (+0.27) 50.27 (+0.00) 50.20 (+0.03) 50.21 (+0.14) 50.13 (+0.00) 50.10 (+0.02) 98.56 (+0.35) 99.47 (+0.14) 99.47 (+0.14) 2.5-Instruct-0.5B [46] language encoder. We followed typical two-stage training procedure: first, linear projector was trained to align visual tokens with the language embedding space, freezing all other components; second, both the projector and the language encoder underwent instruction fine-tuning. For the projection training, we utilized the LAION-CCSBU dataset (558K image-text pairs) used in the LLaVA training setup. For instruction fine-tuning, we adopted the COCO [24] subset (approximately 350K pairs) also used by LLaVA. All experiments were conducted using the TinyLLaVA repository on 4 A100-80GB GPUs. The hyperparameters for fine-tuning included batch size of 48 samples per GPU, learning rate of 2 105, zero weight decay, warm-up ratio of 0.03, and cosine decay scheduling. The projection pretraining similarly employed 48 samples per GPU, learning rate of 1 103, no weight decay, warm-up ratio of 0.03, and cosine decay scheduling. We evaluated the resulting models on the POPE [23] hallucination dataset, which assesses models susceptibility to visual hallucinations. Despite the modest size of observed improvements, DiffCLIP-CC12M consistently outperformed the CLIP-CC12M baseline across all metrics. These initial findings suggest that differential attentiontrained vision encoders could enhance performance when integrated into broader vision-language architectures, making it promising direction for further exploration. 5.2. Scaling Data and Architecture Training CLIP models with ViT-B/16 backbone on the CC12M dataset (7.9M samples) currently requires approximately 10 GPU-days on A100 GPUs, translating to roughly $600 using Google Cloud Platform (GCP). natural future direction would involve exploring how differential attention performs when scaling to larger architectures (e.g., ViT-L or ViT-H) and substantially bigger datasets (e.g., LAION400M). Investigating such scaling could reveal whether the performance gains observed with DiffCLIP persist or even amplify as model size and dataset scale increase, offering insights into the broader applicability and benefits of differential attention in vision-language pretraining. 6. Conclusion We introduced DiffCLIP, which integrates differential attention into CLIP-based vision-language models to better filter out noisy alignments. Through extensive experiments on classification, retrieval, robustness, and fine-grained benchmarks, DiffCLIP consistently improves over standard CLIP with minimal overhead. Further ablations highlight the flexibility of dynamic attention schedules and vision-only setups. We hope these findings inspire future research on more efficient, robust attention mechanisms in large multimodal learning. 7. Acknowledgements The research reported in this publication was supported by funding from King Abdullah University of Science and Technology (KAUST) - Center of Excellence for Generative AI, under award number 5940."
        },
        {
            "title": "References",
            "content": "[1] Amro Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S. Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication, 2023. 2 [2] Romain Beaumont. img2dataset: Easily turn large sets of image urls to an image dataset. https://github.com/ rom1504/img2dataset, 2021. 5 [3] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101mining discriminative components with random forests. In ECCV, pages 446461. Springer, 2014. 5 [4] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021. 5 [5] Jieneng Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Vitamin: Designing scalable vision modIn CVPR, pages 12954 els in the vision-language era. 12966, 2024. [6] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. In CVPR, 2014. Vedaldi. Describing textures in the wild. 5 [7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: TransarXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 2 [8] Mohamed El Banani, Karan Desai, and Justin Johnson. Learning Visual Representations via Language-Guided Sampling. In CVPR, 2023. 5 [9] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets, 2023. 2 [10] Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et al. Vision-language pre-training: Basics, recent advances, and future trends. Foundations and Trends in Computer Graphics and Vision, 14(34):163352, 2022. cal feature alignment for vision-language model pretraining. NeurIPS, 35:3595935970, 2022. 2 [12] Hasan Abed Al Kader Hammoud, Hani Itani, Fabio Pizzati, Philip Torr, Adel Bibi, and Bernard Ghanem. Synthclip: Are we ready for fully synthetic clip training? arXiv preprint arXiv:2402.01832, 2024. 2, 5 [13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. In CVPR, Deep residual learning for image recognition. pages 770778, 2016. 2 [14] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: critical analysis of out-of-distribution generalization. In ICCV, pages 83408349, 2021. 5 [15] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In CVPR, pages 1526215271, 2021. [16] Ahmet Iscen, Mathilde Caron, Alireza Fathi, and Cordelia Schmid. Retrieval-enhanced contrastive vision-text models. arXiv preprint arXiv:2306.07196, 2023. 2 [17] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, pages 4904 4916. PMLR, 2021. 1, 2 [18] Greg Kamradt. Needle in Haystack - pressure testhttps : / / github . com / gkamradt / ing LLMs. LLMTest_NeedleInAHaystack/tree/main, 2023. 4 [19] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 5 [20] Zhengfeng Lai, Haotian Zhang, Bowen Zhang, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, et al. Veclip: Improving clip training via visual-enriched captions. In ECCV, pages 111 127. Springer, 2024. [21] Fei-Fei Li, Marco Andreeto, MarcAurelio Ranzato, and Pietro Perona. Caltech 101, 2022. 5 [22] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Supervision exists everywhere: data efficient contrastive arXiv preprint language-image pre-training paradigm. arXiv:2110.05208, 2021. 2 [23] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large visionlanguage models. In ACL, pages 292305, Singapore, 2023. Association for Computational Linguistics. 8 [24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740755. Springer, 2014. 5, 8 [25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36:3489234916, 2023. [11] Yuting Gao, Jinfeng Liu, Zihan Xu, Jun Zhang, Ke Li, Rongrong Ji, and Chunhua Shen. Pyramidclip: Hierarchi- [26] Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12: 157173, 2024. 4 [27] Yanqing Liu, Xianhang Li, Zeyu Wang, Bingchen Zhao, and Cihang Xie. Clips: An enhanced clip framework for learning with synthetic captions. arXiv preprint arXiv:2411.16828, 2024. 2 [28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [29] Yan Luo, Min Shi, Muhammad Osama Khan, Muhammad Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu Tian, Luo Song, Ava Kouhana, Tobias Elze, et al. Fairclip: Harnessing fairness in vision-language learning. In CVPR, pages 1228912301, 2024. 3 [30] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. Technical report, 2013. 5 [31] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language-image pretraining. In ECCV, pages 529544. Springer, 2022. 2, 5 [32] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over large number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing, pages 722729. IEEE, 2008. 5 [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763. PmLR, 2021. 1, 2, 3, 4 [34] Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia Hockenmaier. Collecting image annotations using amazons mechanical turk. In Proceedings of the NAACL HLT 2010 workshop on creating speech and language data with Amazons Mechanical Turk, pages 139147, 2010. 5 [35] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In ICML, pages 53895400. PMLR, 2019. 5 [36] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej KarpaImagenet thy, Aditya Khosla, Michael Bernstein, et al. large scale visual recognition challenge. IJCV, 115:211252, 2015. [37] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 35:25278 25294, 2022. 5 [38] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018. 5 [39] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the In CVPR, pages visual shortcomings of multimodal llms. 95689578, 2024. 5, 6 [40] Michael Tschannen, Basil Mustafa, and Neil Houlsby. Image-and-language understanding from pixels only. arXiv preprint arXiv:2212.08045, 2022. 2 [41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 30, 2017. 2, 3 [42] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric Xing. Learning robust global representations by penalizing local predictive power. NeurIPS, 32, 2019. [43] Zihao Wei, Zixuan Pan, and Andrew Owens. Efficient vision-language pre-training by cluster masking. In CVPR, pages 2681526825, 2024. 2 [44] Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR, pages 34853492. IEEE, 2010. 5 [45] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision. arXiv preprint arXiv:2202.11094, 2022. 2 [46] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 8 [47] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, and Jianfeng Gao. Unified contrastive learning In CVPR, pages 1916319173, in image-text-label space. 2022. 2 [48] Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, and Furu Wei. Differential transformer. arXiv preprint arXiv:2410.05258, 2024. 1, 2, 3, 4, [49] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:6778, 2014. 5 [50] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, pages 1197511986, 2023. 2 [51] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. IEEE Vision-language models for vision tasks: survey. TPAMI, 2024. 2 [52] Chenhao Zheng, Jieyu Zhang, Aniruddha Kembhavi, and Ranjay Krishna. Iterated learning improves compositionality in large vision-language models. In CVPR, pages 13785 13795, 2024. 3 [53] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image pretraining. In CVPR, pages 1679316803, 2022. 2 [54] Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, and Lei Huang. Tinyllava: framework of small-scale large multimodal models. arXiv preprint arXiv:2402.14289, 2024."
        }
    ],
    "affiliations": [
        "KAUST"
    ]
}