{
    "paper_title": "AI-University: An LLM-based platform for instructional alignment to scientific classrooms",
    "authors": [
        "Mostafa Faghih Shojaei",
        "Rahul Gulati",
        "Benjamin A. Jasperson",
        "Shangshang Wang",
        "Simone Cimolato",
        "Dangli Cao",
        "Willie Neiswanger",
        "Krishna Garikipati"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce AI University (AI-U), a flexible framework for AI-driven course content delivery that adapts to instructors' teaching styles. At its core, AI-U fine-tunes a large language model (LLM) with retrieval-augmented generation (RAG) to generate instructor-aligned responses from lecture videos, notes, and textbooks. Using a graduate-level finite-element-method (FEM) course as a case study, we present a scalable pipeline to systematically construct training data, fine-tune an open-source LLM with Low-Rank Adaptation (LoRA), and optimize its responses through RAG-based synthesis. Our evaluation - combining cosine similarity, LLM-based assessment, and expert review - demonstrates strong alignment with course materials. We also have developed a prototype web application, available at https://my-ai-university.com, that enhances traceability by linking AI-generated responses to specific sections of the relevant course material and time-stamped instances of the open-access video lectures. Our expert model is found to have greater cosine similarity with a reference on 86% of test cases. An LLM judge also found our expert model to outperform the base Llama 3.2 model approximately four times out of five. AI-U offers a scalable approach to AI-assisted education, paving the way for broader adoption in higher education. Here, our framework has been presented in the setting of a class on FEM - a subject that is central to training PhD and Master students in engineering science. However, this setting is a particular instance of a broader context: fine-tuning LLMs to research content in science."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 6 4 8 8 0 . 4 0 5 2 : r Preprint. Under review. AI-University: An LLM-based platform for instructional alignment to scientific classrooms Mostafa Faghih Shojaei1,*, Rahul Gulati1,*, Benjamin A. Jasperson1,*, Shangshang Wang2,*, Simone Cimolato3, Dangli Cao4, Willie Neiswanger2, and Krishna Garikipati1,+ 1Department of Aerospace and Mechanical Engineering, University of Southern California 2Department of Computer Science, University of Southern California 3Department of Astronautical Engineering, University of Southern California 4Department of Electrical and Computer Engineering, University of Southern California *Equal contribution. Listing order is alphabetical. +Corresponding author: garikipa@usc.edu"
        },
        {
            "title": "Abstract",
            "content": "We introduce AI University (AI-U), flexible framework for AI-driven course content delivery that adapts to instructors teaching styles. At its core, AI-U fine-tunes large language model (LLM) with retrievalaugmented generation (RAG) to generate instructor-aligned responses from lecture videos, notes, and textbooks. Using graduate-level finiteelement-method (FEM) course as case study, we present scalable pipeline to systematically construct training data, fine-tune an open-source LLM with Low-Rank Adaptation (LoRA), and optimize its responses through RAG-based synthesis. Our evaluationcombining cosine similarity, LLMbased assessment, and expert reviewdemonstrates strong alignment with course materials. We also have developed prototype web application, available at https://my-ai-university.com, that enhances traceability by linking AI-generated responses to specific sections of the relevant course material and time-stamped instances of the open-access video lectures. Our expert model is found to have greater cosine similarity with reference on 86% of test cases. An LLM judge also found our expert model to outperform the base Llama 3.2 model approximately four times out of five. AI-U offers scalable approach to AI-assisted education, paving the way for broader adoption in higher education. Here, our framework has been presented in the setting of class on FEMa subject that is central to training PhD and Master students in engineering science. However, this setting is particular instance of broader context: fine-tuning LLMs to research content in science."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) are rapidly rising to positions of dominance across wide range of applications, shaping the future of everyday interactions with artificial intelligence. Most widely available LLMs are trained on an extensive corpus of internet-sourced data, enabling them to achieve remarkable accuracy in general tasks. However, there is growing need for domain-specific LLMs tailored to specialized knowledge areas. As LLMs continue to evolve, the next frontier lies in developing models optimized for task-specific roles. This demand has been further amplified by the rise of agentic workflows, where each agent is specialized model designed for distinct function, driving the need for more precise and efficient task-specific LLMs. Fine-tuning LLMs for domain-specific applications offers several key advantages. Industries can train these models on proprietary data, ensuring they align with specialized knowledge 1 Preprint. Under review. and business needs while also adopting any desired style and behavior. Additionally, finetuning allows for the incorporation of new information beyond the original training cutoff, which is particularly crucial as LLMs have now reached stage where they encompass vast internet-based knowledge (Ding et al., 2023). Rather than training models from scratch, fine-tuning provides more efficient way to update and refine them with the latest data. Furthermore, this approach enables the creation of highly personalized AI assistants tailored to the style and behavior of individual users, enhancing adaptability and user experience. These benefits extend to the classroom, where there is growing demand for scalable, accurate, and interactive teaching aids that support educators and enhance student learning while taking into account student privacy and equitable access. Here, we propose structured framework, the AI-University, designed for university courses and adaptable to instructors needs. We apply this framework to graduate-level course on the Finite Element Method, numerical technique for solving partial differential equations (PDEs) that is by far the most widely used computational framework across all engineering simulations (see 2.4 for background). In particular, we aim to mirror the style of course taught over multiple semesters, with recorded lectures available online (Garikipati, 2024; 2025). While existing LLMs have broad understanding of the subject matter based on publicly available knowledge, they lack the distinct instructional style of this course. This includes the instructors approach to introducing new concepts, the preferred use of terminology and symbols, and their unique conversational tone. To address this gap, we propose platform that integrates LLMs with retrieval-augmented generation (RAG) to create customized AI assistant tailored to the course. We use the evaluation metrics of cosine similarity and LLM-as-a-judge in conjunction with domain-specific expert reviews to demonstrate the effectiveness of AI-U. The current experiment follows static approach, where all course materials are available before fine-tuning the LLM. However, the workflow is designed to be dynamic, allowing instructors to fine-tune the model with initial course materials at the start of the semester and then continuously update it with new lecture notes, or other content, through RAG-based synthesis model. The fine-tuning data is generated through data generation pipeline that takes course materials and produces question-answer pairs used to fine-tune domainexpert LLM, which we call LLaMA-TOMMI-1.0 (Trained On Mechanics Materials Instructor). By combining responses from LLaMA-TOMMI-1.0 with real-time retrieval of course-specific information, this approach ensures that the assistant remains up-to-date and aligned with the evolving content of the course, ultimately creating more adaptive and personalized learning experience. We note that, while proprietary software was used for some portions of this work, the workflow has been prepared in such way that it can be performed using entirely open-source resources, supporting local hosting for data privacy (Dorca Josa & Bleda-Bejar, 2024) as well as equitable access to all learners. Overall, AI-U represents an advancement in integrating AI into education, enhancing both instructional efficiency and, potentially, student engagement. Its main contributions include: scalable AI-driven question-answer data generation pipeline to produce the domain-specific fine-tuning dataset, with outputs verified by domain experts. workflow in which fine-tuned expert model, LLaMA-TOMMI-1.0, feeds into RAG-based synthesis model, enabling adaptable data updates and the generation of responses in the style of the course with course-specific references. prototype web application that integrates AI-generated responses with relevant course materials and open-access video lectures playable at the related timestamps. pipeline demonstration using fine-tuned open-source model; additionally, the entire system can easily be built with open-source tools, enabling local deployment and reducing privacy risks from external data sharing. Our dataset and code are available on Huggingface1 and GitHub2. 1https://huggingface.co/my-ai-university 2https://github.com/my-ai-university/finite-element-method 2 Preprint. Under review."
        },
        {
            "title": "2.1 Large language models",
            "content": "Modern LLMs can be traced to the seminal work by Vaswani et al. (2017) and its focus on the attention mechanism and the ability to parallelize training. This has led to the general trend of bigger-is-better, with an emphasis on more training data and larger models. While proprietary models such as ChatGPT have until recently topped benchmark tests, the performance of open source models such as BERT, LLaMA (Grattafiori et al., 2024), and DeepSeek (Bi et al., 2024; Liu et al., 2024) has steadily improved. It has become common place to fine-tune pre-trained base LLM for applications requiring domain-specific information. The understanding that over-parameterized models reside on low intrinsic dimension (Aghajanyan et al., 2020; Li et al., 2018) led to Low-Rank Adaptation (LoRA) (Hu et al., 2021). LoRA is parameter-efficient fine-tuning method that adds low-rank matrices into the frozen layers of pre-trained model. Additional references or proprietary information can be provided to the base LLM through methods such as RAG (Lewis et al., 2021). At basic level, RAG pipeline will take an input sequence, embed it using an embedding model, and use pre-trained retriever to find the top-k most relevant documents. These documents are typically embedded offline using the same embedding model and stored for later use. The original query is then augmented by the retrieved documents and used by generator, typically an LLM, to produce the final response."
        },
        {
            "title": "2.2 Applications in education",
            "content": "As generative AI technology has evolved, so too have examples of their use in higher education settings (Xu et al., 2024; Wang et al., 2024). These include their use as debugging tools for computer science students (Yang et al., 2024), simulating classroom environment for users (Zhang et al., 2024), and serving as teaching assistant (Hicke et al., 2023; Anishka et al., 2024). We especially highlight the work by Hicke et al. (2023), combining LLaMA-2 base model with supervised fine-tuning, RAG and Direct Preference Optimization (DPO) to create an AI-TA for an introductory computer science course. Notably, their training data source consists of available question-answer pairs from eight previous course semesters, which is not available for our course. Instead, our data workflow will enable instructors to fine-tune course assistant when historical data is not available, while also delivering platform whose functioning mirrors the course instructor."
        },
        {
            "title": "2.3 Shortcomings and concerns",
            "content": "The advances notwithstanding, there remain concerns preventing the rapid adoption of LLMs in the classroom. Commercial and third-party software bring concerns about data privacy and security, both for student data as well as proprietary teaching material (Chan, 2023). Equitable access is another concern, with students of higher socioeconomic status or AI literacy appearing to benefit the most (Yu et al., 2024). To encourage students to use them over commercially available options, course-provided assistants will need to be tailored for course-specific terminology, materials, and teaching styles. RAG-based approaches, while relatively simple to implement, are limited by their context windows. While this has improved greatly with recent LLMs, they still lack the ability to tailor response based on large corpus of reference data. Xing et al. (2024) showed that knowledge graphs are one effective approach to improving scalability and performance of RAG-based systems."
        },
        {
            "title": "2.4 The Finite Element Method",
            "content": "The finite element method is numerical technique to solve partial differential equations (PDEs) (Hughes, 1987; Zienkiewicz et al., 2013). It leverages variational methods to convert PDE from the strong form to the weak form. Specifically, we relax the smoothness and differentiability requirements on the solution by multiplying the PDE with test functions and redistributing derivatives to the test function, enabling numerical solutions using piecewise polynomial basis functions. 3 Preprint. Under review. Figure 1: Overview of the AI-University framework. Sections marked with dashed line are performed once, at the beginning. Figure 2: Overview of the data generation framework. Sections marked with dashed line are performed once, at the beginning."
        },
        {
            "title": "3 Methods",
            "content": "3."
        },
        {
            "title": "Inquiry pipeline",
            "content": "The inquiry pipeline is shown in Figure 1. high-level summary is provided here, with additional details in the sections below. The users query is first answered by an expert model, course-specific fine-tuned LLM trained to respond in the instructors style. This expert-generated response, along with the original user query, is then passed to synthesis model via carefully constructed synthesis prompt. Within the synthesis pipeline, relevant context is first retrieved using the query through RAG-based pipeline from database of embedded course materials. Next, leveraging the synthesis prompt, an LLM integrates the expert models response with the retrieved context to produce an enhanced, unified answer. This synthesized response includes relevant reference links pointing directly to specific sections of the course materials."
        },
        {
            "title": "3.2 Training data",
            "content": "The training data generation pipeline is shown in Figure 2. We approach the generation of training data in manner similar to an instructor preparing course materials, by starting with the reference textbook for the course. In this case, we choose canonical work in the 4 Preprint. Under review. field by Hughes (1987). With the authors permission, we convert textbook PDF files to latex using commercially-available document conversion software3. Subject-matter-expert audits of the output and restructuring of the latex sections ensure data integrity. In addition to the textbook, we leverage course content available at Garikipati (2025) and Garikipati (2024), also with permission of the content creators. We obtain transcripts of the course lectures from the online playlist, from which we generate series of questions and answers ChatGPT 4o with the same workflow and prompts as the textbook material. We meticulously generate question-answer pairs for each reference section using an LLM (ChatGPT 4o, although any LLM can be used) and carefully architected workflow. Using prompt engineering to achieve the desired format and focus, we generate questions for each section of reference material (see A.1 for prompt). For each question, we retrieve reference material chunks from the available course material using cosine similarity between the embedded prompt and reference material. Through prompt engineering, we ensure that we only use the supplied reference material during answer generation and not the LLMs background knowledge of FEM (see A.2 for prompt). We incorporate additional course-related coding assignments as training data to enhance the dataset. We systematically generate question-answer pairs from existing course assignments (Garikipati, 2025) using ChatGPT 4o. To ensure sufficient data coverage, three distinct prompting strategies are employed. The first prompt aims to generate comprehensive set of question-answer pairs with detailed coverage of key concepts. These pairs are derived from coding assignments that provided code template requiring modification (see A.4 for prompt 1). The second prompt focuses on creating question-answer pairs that mirror the conceptual depth of the original coding assignments, ensuring that the questions accurately assess the same knowledge areas (see A.5 for prompt 2). Since students have the flexibility to code in both C++ and Python, we introduce third prompt to generate question-answer pairs specifically tailored for Python, aligning with the structure and intent of the C++ assignments (see A.6 for prompt 3). In total, 4648 question-answer pairs are generated from the data sources (textbook, 1053; coding examples, 286; online course transcripts, 3309). We reserve 10% of this data for testing, using the rest for training and hyperparameter optimization."
        },
        {
            "title": "3.3 Fine-tuning Llama-3.2-11B",
            "content": "We select Llama-3.2-11B-Vision-Instruct (Grattafiori et al., 2024) as our base model due to its balance between performance and computational efficiency. While larger models may offer superior general knowledge, medium-sized model like Llama-3.2-11B provides strong domain-specific capabilities with reduced resource overhead. We perform fine-tuning within the Hugging Face ecosystem using the Transformers library (Wolf et al., 2020). This leverages PEFT (Parameter-Efficient Fine-Tuning) (Mangrulkar et al., 2022) to implement LoRA, enabling efficient adaptation of the model without modifying all parameters. Chat templating is handled using the PreTrainedTokenizerFast.apply_chat_template method. For domain adaptation, we employ the system prompt outlined in A.3. Model weights are loaded in bfloat16 for halfprecision computation. We utilize the Accelerate library Gugger et al. (2022) to distribute fine-tuning across two A40 GPUs, optimizing memory usage and training speed."
        },
        {
            "title": "3.3.1 Hyperparameter optimization",
            "content": "We employ Optuna for hyperparameter optimization, systematically searching for optimal configurations to enhance model performance (Akiba et al., 2019). The following hyperparameter space is explored: Learning rate: varied in the range [1e5, 1e3] Gradient accumulation steps: 2 Epochs: 5 3https://mathpix.com/ 5 Preprint. Under review. LoRA parameters: Rank: varied in the range [8, 64] Alpha: varied in the range [32, 128] Dropout: chosen from [0.05, 0.1] Target modules: {q, k, v, o, gate, up, down} Warmup steps: 100 Max token length: 700 Optimization is conducted on two A40 GPUs, utilizing the full training dataset with crossentropy loss. After hyperparameter tuning, the optimal hyperparameters obtained are: LoRA Rank = 45, LoRA Alpha = 65, LoRA Dropout = 0.05, and Learning rate = 5e5. Training logs and experiment tracking are managed via Weights and Biases (WANDB) to ensure reproducibility and analysis of model performance across trials4."
        },
        {
            "title": "3.4 RAG + Synthesis agent",
            "content": "We use RAG-based pipeline to retrieve course-specific material. user query is embedded into the same space as the course material, and relevant chunks are identified by cosine similarity. The top-k reference chunks are then returned to the synthesis agent along with the LLaMA-TOMMI-1.0 response (Figure 1). The synthesis agent system prompt (A.7) ensures that the response maintains the course style while supplementing with reference material. Reference links are provided to the user along with the synthesis agent response (Figure 1), building confidence in the response and allowing the user to more extensively explore the source material. Note that the user has the option to not use fine-tuned model in the current interface, in which case the workflow mimics traditional RAG pipeline."
        },
        {
            "title": "3.5 Evaluation",
            "content": "We hold out 10% of our data from the training data set and reserve it for testing. The base model and LLaMA-TOMMI-1.0 are queried with the test questions, and their responses are recorded. Two approaches are presented here to evaluate the effectiveness of our fine-tuning. First, we embed the model responses and use cosine similarity to evaluate how semantically similar they are against ground truth embedded answers: cosine similarity = remb ˆremb rembˆremb (1) where remb is the embedded ground truth answer and ˆremb is the embedded model response. We report both the average cosine similarity and win rate across all test data, where win rate is defined as the number of times given model has the higher cosine similarity when answering test question. Next, we use an independent LLM-as-a-Judge (judge) to evaluate the effectiveness of fine-tuning for adopting the course style. The ground truth reference answer, base model response, and LLaMA-TOMMI-1.0 response are provided to the judge with one of two system prompts. The first prompt focuses on lexical matching, structural similarity, and example consistency. The second prompt refers to alignment to the reference and completeness. The full prompts are provided in Appendix A.8. In both cases, the judge is instructed to return either the base model as winner, LLaMA-TOMMI-1.0 as winner, both models if equally aligned, or neither model as being aligned with the ground truth response."
        },
        {
            "title": "3.6 Web application",
            "content": "Figure 3 presents the user interface demonstration, available at https://my-ai-university. com. The current implementation uses the Streamlit app5 and is hosted on the HuggingFace spaces platform6. It allows the user to select the number of relevant video lectures and 4https://wandb.ai/my-ai-university/finite-element-method 5https://streamlit.io/ 6https://huggingface.co/spaces 6 Preprint. Under review. Figure 3: Demonstration of the web application, available at https://my-ai-university.com Table 1: Fine-tuning effectiveness, as evaluated using cosine similarity and LLM-as-aJudge. For cosine similarity, both the ground truth label and the model response are embedded using OpenAIs best vector embedding model (text-embedding-3-large) and used to calculate the average cosine similarity across all results (Avg. Cos. Sim.). When choosing which answer is best aligned using cosine similarity (Winner Cos. Sim.), the results show an overwhelming preference for the LLaMA-TOMMI-1.0 model. Two prompts were provided for LLM-as-a-Judge rating, see A.8."
        },
        {
            "title": "Model",
            "content": "LLaMA 3.2 base model LLaMA-TOMMI-1.0 Both models Neither model Avg. Cos. Sim. 0.818 0.879 - - Winner Cos. Sim. 13.97% 86.02% - - Judge Judge #2 #1 8.39% 26.88% 43.44% 43.23% 2.80% 9.03% 45.38% 20.86% textbook sections to retrieve, along with the maximum number of context tokens allowed per content. By default, the expert model is the fine-tuned LlaMA-TOMMI-1.0, but users may switch to an open-source (LLaMA-3.2-11B) or commercial (GPT-4o-mini) alternative, both adapted solely through prompt engineering. The synthesis LLM can be either GPT-4omini (via API) or LLaMA-3.2-11B (run locally). The expert model, if used, includes beam search functionality along with the ability to specify limit on the maximum number of new tokens. The synthesis model temperature and Top can also be specified."
        },
        {
            "title": "4.1 Fine-tuning effectiveness",
            "content": "We now assess the effectiveness of fine-tuning the base model. Table 1 presents the observed alignment between ground-truth labels and inferred responses across all test datapoints. The test set comprises 465 question-answer pairs withheld from the training data (Section 3.2). Results indicate that fine-tuning significantly enhances alignment with the course material, as evidenced by an increase in cosine similarity between inferred responses and groundtruth labels. On average, LLaMA-TOMMI-1.0 achieves 6.10% higher cosine similarity than the base model. Moreover, when selecting the model with the higher cosine similarity as the winner for each test case, LLaMA-TOMMI-1.0 prevails 86% of the time, demonstrating substantial improvement. We expand on these results by prompting an LLM to evaluate the responses. Specifically, we use ChatGPT-4o as our LLM judge. We design two sets of evaluation prompts (detailed 7 Preprint. Under review. in 3.5 and A.8) and conduct two separate evaluation rounds. As shown in Table 1, the results highlight clear advantage for LLaMA-TOMMI-1.0. Under the first evaluation, LLaMA-TOMMI-1.0 is selected 43.44% of the time, compared to 8.39% for the base model. The second evaluation further reinforces this trend, with LLaMA-TOMMI-1.0 winning 43.23% of the time, versus 26.88% for the base model."
        },
        {
            "title": "4.2 Learning environment evaluation",
            "content": "We present as demonstration one sample test case, highlighting both the LLaMA-TOMMI1.0 and platform responses in comparison with the default label answer. Note how in this example, context specific variables and specified by the user are interpreted incorrectly by the base model but correctly by LLaMA-TOMMI-1.0, when compared with the Label/Default answer. Additional examples are shown in Appendix B. [User Query]: In the context of 3D elasticity, how are the degrees of freedom for each node represented in the global 'c' and 'd bar' vectors? ============================================= [Base Model Response (truncated for brevity]: In the context of 3D elasticity, the degrees of freedom (DOFs) for each node are typically represented in the global 'c' and 'd' vectors as follows: Let's consider 3D elasticity problem with mesh consisting of 'n' nodes, each with DOFs (displacements in the x, y, and directions). The global 'c' vector represents the coefficients of the stiffness matrix, and the global 'd' vector represents the forces applied to the system. The global 'c' vector can be partitioned into three sub-vectors, each representing the coefficients of the stiffness matrix for particular DOF: $$c=[c_x c_y c_z]$$ where $c_x$, $c_y$, and $c_z$ are sub-vectors of size 3nx3n representing the coefficients of the stiffness matrix for the x, y, and DOFs, respectively. Similarly, the global 'd' vector can be partitioned into three sub-vectors, each representing the forces applied to the system for particular DOF: $$d=[d_x d_y d_z]$$ where $d_x$, $d_y$, and $d_z$ are sub-vectors of size $3n$ representing the forces applied to the system for the x, y, and DOFs, respectively. In the global stiffness matrix, the coefficients can be represented as... ============================================= [LLaMA-TOMMI-1.0 Response]: In the context of 3D elasticity, the degrees of freedom for each node are represented in the global 'c' and 'd bar' vectors as follows: - The global 'c' vector, which is row vector, contains the degrees of freedom corresponding to the spatial dimensions (1, 2, and 3) for each node. For node with total of $N_{sd}$ spatial dimensions, the 'c' vector will have $N_{sd}$ entries for that node. - The global 'd bar' vector, which is column vector, also contains the degrees of freedom corresponding to the spatial dimensions (1, 2, and 3) for each node. For node with total of $N_{sd}$ spatial dimensions, the 'd bar' vector will have $N_{ sd}$ entries for that node. These vectors are constructed by considering the spatial dimensions for each node and are used to represent the degrees of freedom in the global system of equations. ============================================= [Platform Response]: In the context of 3D elasticity, the degrees of freedom for each node are represented in the global 'c' and 'd bar' vectors as follows: - The **global 'c' vector** is row vector that contains the degrees of freedom corresponding to the spatial dimensions (1, 2, and 3) for each node. For node with total of $N_{sd}$ spatial dimensions, the 'c' vector will have $N_{sd}$ entries for that node. This vector is constructed to represent the various degrees of freedom associated with each node in the global system. 8 Preprint. Under review. - The **global 'd bar' vector** is column vector that also contains the degrees of freedom corresponding to the spatial dimensions (1, 2, and 3) for each node. Similar to the 'c' vector, for node with total of $N_{sd}$ spatial dimensions, the 'd bar' vector will have $N_{sd}$ entries for that node. This vector serves to represent the same degrees of freedom but in different format suitable for the system of equations. These vectors are essential in formulating the global system of equations in finite element analysis, ensuring that all degrees of freedom are accounted for in the analysis process [Video 1, time 04:30]. ============================================= [Label/Default Answer]: In the context of 3D elasticity, the degrees of freedom for each node are represented in the global 'c' and 'd bar' vectors as follows: - **Global 'c' Vector**: This vector is defined by following the global nodes. For each global node, there are three degrees of freedom corresponding to the three spatial dimensions. For example, for global node 1, the degrees of freedom would be represented as $c_{11}$, $c_{12}$, and $c_{13}$, corresponding to spatial dimensions one, two, and three, respectively. This pattern continues for each node, such that for general node A, the degrees of freedom are $c_{A1}$, $c_{A2}$, and $c_{A3}$. - **Global 'd bar' Vector**: Similarly, the 'd bar' vector is structured to represent the displacement degrees of freedom for each node in the respective coordinate directions. For example, for node 1, the degrees of freedom are represented as $d_ {11}$, $d_{12}$, and $d_{13}$. For general node A, the degrees of freedom are $d_{ A1}$, $d_{A2}$, and $d_{A3}$, representing displacements in the directions one, two, and three, respectively. The 'd bar' vector is typically larger than the 'c' vector because it includes all degrees of freedom before accounting for Dirichlet boundary conditions, which may reduce the number of active degrees of freedom in the 'c' vector."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we present the AI University (AI-U), versatile and flexible framework designed to deliver bespoke science course content. By fine-tuning Large Language Model (LLM) and incorporating Retrieval Augmented Generation (RAG) system, AI-U creates an interactive learning environment that mirrors an instructors teaching style. By framing our work in the setting of graduate-level Finite Element Method class (FEM) as an example, we demonstrate the frameworks ability to generate accurate and highly relevant responses to course content by learning from diverse set of course materials such as lecture video transcripts, notes, assignments and textbooks. The LLaMA 3.2 11 billion parameter model optimized with LoRA performs well after hyper-parameter tuning, and we validate its performance by cosine similarity and LLM-as-a-judge evaluations, with further confirmation of its scientific accuracy by human experts. key feature of AI-U is its web application prototype, which not only provides comprehensive responses, but also enhances response credibility, traceability, and student understanding by linking to relevant course material. The framework is designed to be dynamic, supporting the continuous updating of new lecture content through the RAG, ensuring that it remains consistent with the evolution of the course throughout the semester. This assistant extends learning beyond the classroom and can support discussions on platforms like Canvas or Piazza, where students often seek assistance outside of scheduled class hours. When instructors or TAs are unavailable, the AI assistant can provide timely and contextually relevant responses to student queries. This research marks an important advancement in embedding AI into higher education, providing scalable solution to enhance teaching efficiency and student engagement. We envision AI-U as foundational tool that can be widely applied across academic fields, ultimately contributing to the construction of an integrated AI-enhanced university education system. 9 Preprint. Under review. Finally, we note that our framework has been presented in the setting of class on FEMa subject that is central to training PhD and Master students in engineering science. More importantly however, we emphasize that this setting is particular instance of broader context: fine-tuning LLMs to research content in science. In this regard, our use of textbook, class notes and video lecture content would be supplemented by the broader technical literature, recorded research talks and simulations in multi-modal learning environment. RAG, reasoning and multi-agentic inferencing would play important roles."
        },
        {
            "title": "Acknowledgments",
            "content": "The authors gratefully acknowledge the funding support from the U.S. National Science Foundation (NSF) through the EAGER program (Award No. 2427856). Additionally, we appreciate the computational resources provided by NSF under the ACCESS program (Grant No. MCH240079) and the USC Center for Advanced Research Computing (CARC)."
        },
        {
            "title": "Ethics Statement",
            "content": "AI-Generated Responses: AI-U should be used responsibly. Answers are generated using AI and, while thorough, may not always be 100% accurate. Please verify the information independently. Content Ownership: All video content and lecture material referenced belong to their original creators. The textbook The Finite Element Method: Linear Static and Dynamic Finite Element Analysis was used with permission of the author. All other course material was used with permission of the content creators. We encourage users to view the original material on verified platforms to ensure authenticity and accuracy. To ensure privacy, no student data was processed as part of the training data generation pipeline. Educational Fair Use: This tool is intended solely for educational purposes and operates under the principles of fair use. It is not authorized for commercial applications"
        },
        {
            "title": "References",
            "content": "Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning, December 2020. Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: Next-generation Hyperparameter Optimization Framework. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, ISBN 978-1-4503-6201-6. doi: pp. 26232631, Anchorage AK USA, July 2019. ACM. 10.1145/3292500.3330701. Anishka, Atharva Mehta, Nipun Gupta, Aarav Balachandran, Dhruv Kumar, and Pankaj Jalote. Can ChatGPT Play the Role of Teaching Assistant in an Introductory Programming Course?, January 2024. Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. Cecilia Ka Yuk Chan. comprehensive AI policy education framework for university teaching and learning. International Journal of Educational Technology in Higher Education, 20(1):38, July 2023. ISSN 2365-9440. doi: 10.1186/s41239-023-00408-3. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong Sun. Parameterefficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence, 5(3):220235, March 2023. ISSN 2522-5839. doi: 10.1038/s42256-023-00626-4. 10 Preprint. Under review. Aleix Dorca Josa and Marc Bleda-Bejar. LOCAL LLMS: SAFEGUARDING DATA PRIVACY IN THE AGE OF GENERATIVE AI. CASE STUDY AT THE UNIVERSITY OF ANDORRA. In 17th Annual International Conference of Education, Research and Innovation, pp. 78797888, Seville, Spain, November 2024. doi: 10.21125/iceri.2024.1931. Krishna Garikipati. Introduction to finite element methods. YouTube playlist, 2024. URL https://www.youtube.com/playlist?list=PLJhG d-Sp JHKVRhfTgDqbic 4MHpltXZ. Accessed: 2025-03-18. Krishna Garikipati. The finite element method for problems in physics. Coursera, Online Course, 2025. Available at https://www.coursera.org/learn/finite-element-method. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/huggingface/accelerate, 2022. Yann Hicke, Anmol Agarwal, Qianou Ma, and Paul Denny. AI-TA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs, December 2023. URL http://arxiv.org/abs/2311.02775. arXiv:2311.02775 [cs]. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models, October 2021. Thomas J. R. Hughes. The Finite Element Method: Linear Static and Dynamic Finite Element Analysis. Prentice Hall, Englewood Cliffs, NJ, 1987. ISBN 0-13-317025-X. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich uttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, Sebastian Riedel, and Douwe Kiela. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, April 2021. Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Dimension of Objective Landscapes, April 2018. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. Peft: State-of-the-art parameter-efficient fine-tuning methods. https: //github.com/huggingface/peft, 2022. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang, Jiliang Tang, Philip S. Yu, and Qingsong Wen. Large Language Models for Education: Survey and Outlook, April 2024. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. Transformers: Stateof-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pp. 3845, 2020. Wanli Xing, Chenglu Li, Hai Li, Wangda Zhu, Bailing Lyu, and Zeyu Yan. Is RetrievalAugmented Generation All You Need? Investigating Structured External Memory to Enhance Large Language Models Generation for Math Learning, September 2024. 11 Preprint. Under review. Qiang Xu, Jiacheng Gu, and Joan Lu. Leveraging Artificial Intelligence and Large Language Models for Enhanced Teaching and Learning: Systematic Literature Review. In 2024 13th International Conference on Computer Technologies and Development (TechDev), pp. 73 77, Huddersfield, United Kingdom, October 2024. IEEE. ISBN 9798331539771. doi: 10.1109/TechDev64369.2024.00021. Stephanie Yang, Hanzhang Zhao, Yudian Xu, Karen Brennan, and Bertrand Schneider. Debugging with an AI Tutor: Investigating Novice Help-seeking Behaviors and Perceived Learning. In Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1, pp. 8494, Melbourne VIC Australia, August 2024. ACM. ISBN 9798400704758. doi: 10.1145/3632620.3671092. Renzhe Yu, Zhen Xu, Sky CH-Wang, and Richard Arum. Whose ChatGPT? Unveiling Real-World Educational Inequalities Introduced by Large Language Models, November 2024. Zheyuan Zhang, Daniel Zhang-Li, Jifan Yu, Linlu Gong, Jinchang Zhou, Zhanxin Hao, Jianxiao Jiang, Jie Cao, Huiqin Liu, Zhiyuan Liu, Lei Hou, and Juanzi Li. Simulating Classroom Education with LLM-Empowered Agents, November 2024. O. C. Zienkiewicz, Robert L. Taylor, and J. Z. Zhu. The Finite Element Method: Its Basis and Fundamentals. Elsevier, Butterworth-Heinemann, Amsterdam, seventh edition edition, 2013. ISBN 978-1-85617-633-0."
        },
        {
            "title": "A Prompts",
            "content": "A.1 Textbook question generation The following prompt is used to generate questions from the textbook and course transcripts. You are an AI assistant specialized in creating educational content for Finite Element Method (FEM). Generate comprehensive set of questions on topics related to FEM from the input text. **Only questions, no answer is needed.** Follow these guidelines: 1. Questions: - Focus on fundamental concepts, theories, and general applications of FEM. - Ensure that the questions are relevant to the input text, and can be at least partially answered using the provided text. - Emphasize broad understanding rather than niche knowledge. - Questions can be of any length needed to fully express the concept being tested. - Complex questions involving multiple parts or mathematical derivations are encouraged. - Each question should have all the information needed such that it makes sense without referencing the input text. - Any variables that are used in the question must be defined in the question. - Provide enough information such that the question makes sense without referencing specific chapter or section. - Do not refer to the proof number in the question text when generating questions about proof. - Add description of any proofs used when generating questions about proofs. 2. Coverage: - For each question, include \"coverage\" field. - In this field, estimate the percentage of the possible answer that is covered by the input text. - Use your judgment to assign realistic percentage in integer form, considering the depth and specificity of the input text. Note: Mathematical Notation: - Use LaTeX formatting for mathematical expressions - For inline equations, use single $ wrapper (e.g., \"Calculate the strain energy $U = frac{1}{2}int_V sigmaepsilon dV$\") Preprint. Under review. - For display equations, use double $$ wrapper, e.g.: \"Derive the stiffness matrix given the following stress-strain relationship: $$ begin{{bmatrix}} sigma_{{xx}} sigma_{{yy}} tau_{{xy}} end{{bmatrix}} = begin{{bmatrix}} D_{{11}} & D_{{12}} & 0 D_{{21}} & D_{{22}} & 0 0 & 0 & D_{{33}} end{{bmatrix}} begin{{bmatrix}} epsilon_{{xx}} epsilon_{{yy}} gamma_{{xy}} end{{bmatrix}} $$\" Note: Your response format as JSON must adhere to the following structure: [ {{ \"question\": \"What are the shape functions and their role in accuracy of approximations?\", \"coverage\": 95 }}, {{ \"question\": \"How are boundary conditions imposed? Explain elimination approach.\", \"coverage\": 70 }} ] Do not include the word JSON at the start of the response. Generate as many questions as needed to cover the input text, up to {k} diverse questions, with Coverage 30-100 percentage. A.2 Textbook answer generation The following prompt is used to generate answers for the textbook and course transcript questions. You are an AI teaching assistant for Finite Element Method (FEM) course. Answer questions based EXCLUSIVELY on the provided context. If context is insufficient for very accurate answer, respond with: Answer: \"NOT ENOUGH INFO.\" If context is sufficient: 1. Answer Guidelines: - Use only information from the context - Restrict your use of finite element method knowledge to what is provided in the context provided. Do not use additional background finite element method knowledge in generating the answer (you may use background knowledge from other areas). - Show step-by-step work for calculations - For multiple valid interpretations, provide separate answers 2. Mathematical Notation: - Use $ for inline equations (e.g., $U = frac{{1}}{{2}} int_V sigma epsilon dV$) - Use $$ for display equations, especially matrices: $$ begin{{bmatrix}} sigma_{{xx}} & sigma_{{xy}} sigma_{{yx}} & sigma_{{yy}} end{{bmatrix}} $$ 13 Preprint. Under review. Note: Focus on FEM fundamentals, theories, and applications as presented in the context. \"\"\" user_prompt = f\"\"\" Context: {context} Question: {question} Answer (based EXCLUSIVELY on the above context): A.3 Fine-tuning system prompt The following system prompt is provided to the LLM during fine-tuning of LLaMA-TOMMI1.0. You are an AI professor for Finite Element Method (FEM) course. You are asked question by student and return an appropriate answer based on course material. Your response focuses on FEM fundamentals, theories, and applications as presented in the course. Use standard latex notation when replying with mathematical notation. A.4 Coding question-answer generation prompt 1 The following prompt is one of three that was used to generate questions and answers from previous course coding assignments. You are an expert in finite element methods (FEM), the deal.II library, and C++. You are tasked with creating detailed question-answer pairs for coding assignment. The assignment description, along with the solution files (`main.cc` and `fem.h`), is provided. Follow these detailed instructions to generate the Q&A pairs: 1. **Functions as Answers:** Each answer must include the implementation of individual functions or classes from the code files. 2. **Cover All Code Components:** Generate questions for every function, constructor, destructor, and class definition in both `main.cc` and `fem.h`. Ensure that no code component is left out. 3. **Detailed Question Context:** Each question must: - Include **general problem statement** derived from the assignment description to provide clear context. - Stand alone, without referencing the assignment, other questions, or answers, so that it makes sense independently. - Clearly ask for the specific function, constructor, destructor, or class related to the problem context. - Mention that the answer can use the open source library dealii 4. **Variety in Questions:** In addition to asking for individual functions: - Include questions that require the entire class implementation as an answer (e.g ., the `FEM` class). - Include question asking for the names of all functions required to solve the assignment. 5. **Formatting:** Use the following format for the Q&A pairs. Make sure not to number them: ``` Q: <Insert detailed question here> A: <Insert complete function/class implementation here> ``` 14 Preprint. Under review. 6. **Descriptive Questions:** The questions should be long enough and verbose so that they are standalone and cover all the descriptive background from the original assignment without refering to the assignment. 7. **Example Question for Context:** Use the style below as reference for detailing each question: - Example Q: Consider the following differential equation of elastostatics, in strong form: Find $u$ satisfying begin{displaymath} (E,A, u_{,x})_{,x} + f,A = 0,quad mbox{in}; (0,L), end{displaymath} noindent for the following sets of boundary conditions and forcing function ($bar{f}$ and $hat{f}$ are constants): begin{itemize} setlength{itemsep}{0pt} item[(romannumeral 1)]$u(0) = g_1$, $u(L) = g_2$, $f = bar{f} x$, item[(romannumeral 2)]$u(0) = g_1$, $EAu_{,x} = h$ at $x = L$, $f = bar{f} x$, end{itemize} When writing one-dimensional finite element code in C++ using the deal.II FEM library framework to solve the given problem, what will the class constructor look like? - Example A: Here is the class constructor to solve this problem: ``` template <int dim> FEM<dim>::FEM (unsigned int order,unsigned int problem) : fe(FE_Q<dim>(QIterated<1>(QTrapez<1>(),order)), dim), dof_handler (triangulation) { basisFunctionOrder = order; prob = problem; for (unsigned int i=0; i<dim; ++i){ nodal_solution_names.push_back(\"u\"); nodal_data_component_interpretation.push_back(DataComponentInterpretation:: component_is_part_of_vector); } } Here are the files related to the coding assignment: 1. Assignment Description: {assignment_description} 2. Contents of main.cc: {main_code} 3. Contents of fem.h: {fem_code} A.5 Coding question-answer generation prompt 2 The following prompt is one of three that was used to generate questions and answers from previous course coding assignments. You are an expert in finite element methods (FEM), the deal.II library, and C++. You are tasked with creating detailed question-answer pairs for coding assignment. The assignment description and the coding template file, along with the solution files (`main.cc` and `fem.h`), are provided. Follow these detailed instructions to generate the Q&A pairs: 15 Preprint. Under review. 1. **Test on identical material/information as the provided assignment template:** Question Answer pairs must be based on what the coding assignment is targeting the student to understand. The student is expected to use the template coding files and fill them to get the solution coding files. Match the differences between the coding template files and the coding solution and base your question -answers on this. Essentially the QA pairs generated should quiz the student on the identical material tested by the coding assignment and the provide coding template. 2. **Detailed Question Context:** Each question must: - Include **general problem statement** derived from the assignment description to provide clear context. - Stand alone, without referencing the assignment, other questions, or answers, so that it makes sense independently. - Clearly ask for the specific function, constructor, destructor, or class related to the problem context as in the previous point. - Mention that the answer can use the open source library dealii - The questions should be long enough and verbose so that they are standalone and cover all the descriptive background from the original assignment without refering to the assignment. - If the assignment asks for something particular to be implemented such as the boundary condition (pde variables, mesh variables etc), the question should list the boundary conditions to be implemented. 3. **Generate as many questions:** Cover all the assignment problem specific implementations in the code even if they are already provided in the template files. 4. **Formatting:** Use the following format for the Q&A pairs. Make sure not to number them: ``` Q: <Insert detailed question here> A: <Insert function/class implementation here> Here are the files related to the coding assignment: 1. Assignment Description: {assignment_description} 2. Contents of Template main.cc: {templateMain} 3. Contents of template fem.h: {templateFEM} 4. Contents of solution main.cc: {main_code} 5. Contents of solution fem.h: {fem_code} A.6 Coding question-answer generation prompt 3 The following prompt is one of three that was used to generate questions and answers from previous course coding assignments. You are an expert in finite element methods (FEM), the fenics library, and python. You are tasked with creating detailed question-answer pairs for coding assignment. The assignment description, along with the solution file (`fem.h`), is provided. Follow these detailed instructions to generate the Q&A pairs: 1. **Answers Based On Code:** Answers should be based on code implementation. Preprint. Under review. 2. **Cover All Code Components:** Generate as many questions using `fem.h` ensuring no code component is left out. More the questions, the better. It is ok if some questions are repeated/have some overlap. 3. **Detailed Question Context:** Each question must: - Include **general problem statement** derived from the assignment description to provide clear context. - Stand alone, without referencing the assignment, other questions, or answers, so that it makes sense independently. - Clearly ask for the specific code implementation related to the problem context. - Mention that the answer should be based on open source finite element library fenics - The questions should be long enough and verbose so that they are standalone and cover all the descriptive background from the original assignment without refering to the assignment. - If the assignment asks for something particular to be implemented such as the boundary condition (pde variables, mesh variables etc), the question should list the boundary conditions to be implemented. - Make sure to not refer to the assignment. 4. **Formatting:** Use the following format for the Q&A pairs. Make sure not to number them: ``` Q: <Insert detailed question here> A: <Insert complete function/class implementation here> ``` Here are the files related to the coding assignment: 1. Assignment Description: {assignmentDescription} 2. Contents of fem.h: {femCode} A.7 Synthesis agent The following is the system prompt used by the synthesis agent. You are an AI teaching assistant for {subject_matter} course. Your task is to answer questions based EXCLUSIVELY on the content provided from the professor's teaching materials. Do NOT use any external knowledge or information not present in the given context. IMPORTANT: Before proceeding, carefully analyze the provided context and the question. If the context lacks sufficient information to answer the question adequately, respond EXACTLY as follows and then STOP: \"NOT_ENOUGH_INFO The provided context doesn't contain enough information to fully answer this question. You may want to increase the number of relevant context passages or adjust the options and try again.\" If the context is sufficient, continue with the remaining guidelines. Guidelines: 1. Strictly adhere to the information in the context. Do not make assumptions or use general knowledge outside of the provided materials. 2. For partial answers: a) Provide the information you can based on the context. b) Clearly identify which aspects of the question you cannot address due to limited context. 3. Referencing: a) Always cite your sources by referencing the video number and the given time in brackets and **bold** (e.g., [**Video 3, time 03:14**]) after each piece of information you use in your answer. b) You may cite multiple references if they discuss the same content (e.g., [** Video 3, time 03:14; Video 1, time 12:04**]). However, try to reference them separately if they cover different aspects of the answer. 4. Length of response: 17 Preprint. Under review. a) Use approximately 120-200 tokens for each video referenced. b) If referencing multiple videos that discuss the same content, you can use combined total of 120-200 tokens for all refrences. 5. Style and Formatting: a) Provide the answer in markdown format. b) Do not use any titles, sections, or subsections. Use mainly paragraphs. Bold text, items, and bullet points if it helps. c) Symbols and equations within the text MUST be placed between $ and $, e.g., $x=0$ is the min of $sigma(x)=xˆ2$. d) For equations between paragraphs, use nn$ and $nn. For example, in the following equation: nn$ = mcˆ2 $nn, note $c$ as the speed of light . 6. If multiple interpretations of the question are possible based on the context, acknowledge this and provide answers for each interpretation. 7. Use technical language appropriate for {subject_matter} course, but be prepared to explain complex terms if asked. 8. If the question involves calculations, show your work step-by-step, citing the relevant formulas or methods from the context. A.8 LLM-as-a-judge The following prompts are used as part of the LLM-as-a-Judge evaluation. Judge 1: Evaluate which model response better aligns with the professor's reference answer to question ONLY based on the following three key dimensions: 1. Lexical matching: Does the response use the same key terms, phrases, and specific wording as the professor's answer? 2. Structural similarity: Does the response follow the same order and organization of ideas as the professor's answer? 3. Example consistency: Does the response use the same specific examples as the professor's answer? Return your decision as JSON object: - \"winner\": \"model 1\", \"model 2\", \"neither\" (if both significantly diverge), or \" both\" (if equally aligned) - \"justification\": brief explanation of your choice based on the three dimensions above Question: {question} Professor's Answer (Reference): {prof_ans} Model 1 Response: {base_model} Model 2 Response: {fine_tuned} Output only the JSON. Judge 2: Evaluate which model response better aligns with the professor's reference answer to question ONLY based on the following three key dimensions: 1. **Content Accuracy**: Does the response convey correct information without factual errors or misconceptions? 2. **Conceptual Alignment**: Does the response reflect the professor's key ideas and reasoning, even if phrased differently or in different order? 3. **Completeness**: Does the response fully address all parts of the question that the professor addressed? Return your decision as JSON object: - \"winner\": \"model 1\", \"model 2\", \"neither\" (if both responses contain major issues), or \"both\" (if equally strong) - \"justification\": brief explanation of your choice based on the three dimensions above 18 Preprint. Under review. Question: {question} Professor's Answer (Reference): {prof_ans} Model 1 Response: {base_model} Model 2 Response: {fine_tuned} Output only the JSON."
        },
        {
            "title": "B Platform evaluation",
            "content": "Additional examples of full platform response. B.1 Query 1 ================= [User Query]: What is the significance of the term $c_eˆT K_e d_e$ in the finite element method, and how is it used in the assembly process? ================= [Base Model Response]: In the finite element method (FEM), the term $c_eˆT K_e d_e$ is crucial component of the assembly process. Here's its significance and how it's used: Significance: In the FEM, the global stiffness matrix KK is assembled from the element stiffness matrices $K_e$ of individual elements. The term $c_eˆT K_e d_e$ represents the contribution of single element to the global stiffness matrix. Here, $c_e$ is the column vector of degrees of freedom (DOFs) of the element, and $K_e$ is the element stiffness matrix. Assembly process: When assembling the global stiffness matrix KK, we need to consider the contributions of all elements to the global system. The term $c_eˆT K_e d_e$ represents the contribution of single element ee to the global stiffness matrix. The assembly process involves summing up the contributions of all elements to obtain the global stiffness matrix. Mathematically, the assembly process can be represented as: $K=sum e=1NeceTKedeK= sum_{e=1}ˆ{N_e} c_eˆT K_e d_e$$ where $N_e$ is the total number of elements, and $c_e$ and $d_e$ are the column vectors of DOFs for element $e_e$ [LLama-TOMMI-1.0 Response]: The term $c_eˆT K_e d_e$ represents the contribution of an element to the global stiffness matrix in the finite element method. It is used in the assembly process to construct the global stiffness matrix KK by summing the contributions from all elements. Specifically, $c_eˆT K_e d_e$ is the product of the transpose of the element's degrees of freedom vector $c_e$, the element stiffness matrix $K_e$, and the element's displacement vector $d_e$. This term is crucial as it accounts for the local stiffness and displacement effects of the element within the global system. During assembly, this term is added to the appropriate locations in the global stiffness matrix KK and force vector FF, ensuring that the local element behavior is accurately represented in the global system. ================= [Platform Response]: The term $c_eˆT K_e d_e$ is significant in the finite element method as it encapsulates the contribution of an individual element to the overall system's behavior. Here, 19 Preprint. Under review. $c_e$ represents the vector of the element's degrees of freedom, $K_e$ is the element stiffness matrix, and $d_e$ is the displacement vector for that element. This term is crucial during the assembly process, where contributions from all elements are summed to form the global stiffness matrix KK and the global force vector FF. In the assembly process, each element's contribution, represented by $c_eˆT K_e d_e$, is added to the appropriate locations in the global matrices. This ensures that the local stiffness and displacement effects of each element are accurately integrated into the global system, allowing for comprehensive analysis of the entire structure or system being modeled. The assembly algorithm systematically combines these contributions to reflect the interactions and constraints present in the physical system being analyzed [Video 1, time 02:04; Section 1 (2.9 ELASTOSTATICS: ELEMENT STIFFNESS MATRIX AND FORCE VECTOR)**]. ================= [Label/Default Answer]: The term $c_eˆT K_e d_e$ in the finite element method represents the contribution of an individual element to the global system of equations. Here, $c_e$ is vector of degrees of freedom associated with the weighting function for the element, $K_e$ is the element stiffness matrix, and $d_e$ is the vector of degrees of freedom associated with the solution for the element. In the assembly process, this term is used to construct the global stiffness matrix and the global force vector. The assembly involves summing the contributions from all elements to form the global system. Specifically, the contributions from each element's stiffness matrix $K_e$ and force vector are added to the appropriate locations in the global stiffness matrix $boldsymbol{K}$ and global force vector $ boldsymbol{F}$, respectively. This is done by mapping the local degrees of freedom to the global degrees of freedom using location matrix or similar mapping strategy. The assembly process ensures that the global system accurately represents the behavior of the entire domain by incorporating the contributions from all elements. B.2 Query 2 [User Query]: In the context of the given recursive formula, what does the term $Aˆi Delta tau(t_{ n-i})$ represent? ================= [Base Model Response]: In the context of the Finite Element Method (FEM), the term $Aˆi Delta tau(t_{n-i})$ is related to the Newmark-beta method, which is numerical integration scheme used to solve the dynamic equations of motion. Here, $A_i$ is the ith diagonal element of the mass matrix $A$, $Delta t$ is the time step size, and $tau (t_{n-i}) $ is the value of the time-dependent function $tau (t)$ at time $t_{n-1}$. In the Newmark-beta method, the solution at time $t_n+1$ is obtained by extrapolating the solution at time $t_n$ using the following recursive formula: $mathbf{u}_{n+1} = mathbf{u}_n + Delta left( (1 - 2beta) mathbf{M}ˆ{-1} mathbf{ f}n + 2beta mathbf{M}ˆ{-1} mathbf{f}{n+1} right) + Delta tˆ2 left( beta mathbf{A} mathbf{u}_n ================= [LLaMA-TOMMI-1.0 Response]: In the context of the given recursive formula, the term $Aˆi Delta tau(t_{n-i})$ represents the contribution from the ii-th past time step to the current time step $t_n$. Here, $A_i$ is coefficient that arises from the recursive formula, $Delta t$ is the time step size, and $tau(t_{n-i})$ is the value of the function $tau$ at the time step $t_{n-1}$. This term is part of the recursive expression that 20 Preprint. Under review. updates the value of $tau$ at the current time step $t_n$ by incorporating the effects of the previous time steps. ================= [Platform Response]: In the context of the given recursive formula, the term $Aˆi Delta tau(t_{n-i})$ represents the contribution from the ii-th past time step to the current time step $t_n$. Here, $A_i$ is coefficient that arises from the recursive formula, $Delta t$ is the time step size, and $tau(t_{n-i})$ is the value of the function $tau$ at the time step $t_{n-1}$. This term is part of the recursive expression that updates the value of $tau$ at the current time step $t_n$ by incorporating the effects of the previous time steps. This explanation aligns with the recursive relationship discussed in the context of error propagation in the finite element method [Video 1, time 14:52]. ================= [Label/Default Answer]: The term $Aˆi Delta tau(t_{n-i})$ in the given recursive formula represents component of the accumulated error at time step $n+1$. Specifically, it is part of the summation that accounts for the influence of the local truncation error $tau( t_{n-i})$ at each previous time step $t_{n-i}$, scaled by the time step size $ Delta t$ and the amplification factor $Aˆi$. This term reflects how the local truncation errors from previous time steps contribute to the overall error at the current time step $n+1$. B.3 Query 3 [User Query]: What is the importance of ensuring that the stiffness matrix $K$ is square in the presence of Dirichlet boundary conditions? ================= [Base Model Response]: In the context of the Finite Element Method (FEM), the stiffness matrix $mathbf{K}$ is square matrix that represents the relationship between the displacements and forces in system. When Dirichlet boundary conditions are applied, which specify the displacement of certain nodes on the boundary, the system becomes constrained. To understand the importance of ensuring that the stiffness matrix $mathbf{K}$ is square, let's consider the general form of the FEM equations: $begin{bmatrix} mathbf{K} & mathbf{F}ˆT mathbf{F} & mathbf{0} end{bmatrix} begin{Bmatrix} mathbf{u} mathbf{p} end{Bmatrix} = begin{Bmatrix} mathbf{f} mathbf{0} end{Bmatrix}$ where $mathbf{K}$ is the stiffness matrix, $mathbf{F}$ is the force matrix, $mathbf{u }$ is the displacement vector, $mathbf{p}$ is the pressure vector (if applicable), and $mathbf{f}$ is the external force vector. When Dirichlet boundary conditions are applied, the displacement of certain nodes is fixed, which means that the corresponding rows of the displacement vector $mathbf{ u}$ are known. This results in reduced system of equations, where the number of unknowns is less than the number of equations. To solve this reduced system, we need to eliminate the known displacement components from the system. This can be achieved by partitioning the stiffness matrix $mathbf{ K}$ and the displacement vector $mathbf{u}$ into two parts: $mathbf{K} = begin{bmatrix} mathbf{K}_{uu} & mathbf{K}_{up} mathbf{K}_{pu} & mathbf{K}_{pp} end{bmatrix}$ $mathbf{u} = begin{Bmatrix} mathbf{u}_u mathbf{u}_p end{Bmatrix}$ Preprint. Under review. where $mathbf{K}_{uu}$ is the submatrix corresponding to the unknown displacements, $ mathbf{K}_{up}$ is the submatrix corresponding to the known displacements, $mathbf{ u}_u$ is the subvector of unknown displacements, and $mathbf{u}_p$ is the subvector of known displacements. The reduced system of equations can be written as: $begin{bmatrix} mathbf{K}_{uu} & mathbf{K}_{up} mathbf{K}_{pu} & mathbf{K}_{pp} end{bmatrix} begin{Bmatrix} mathbf{u}_u mathbf{u}_p end{Bmatrix} = begin{ Bmatrix} mathbf{f}_u mathbf{0} end{Bmatrix}$ where $mathbf{f}_u$ is the subvector of external forces corresponding to the unknown displacements. To eliminate the known displacement components, we can use the fact that $mathbf{u}_p$ is known. Multiplying the second row of the matrix equation by $mathbf{u}_p$ and using the fact that $mathbf{K}_{pu}mathbf{u}_p = mathbf{0}$ (since the known displacements do not contribute to the internal forces), we get: $mathbf{K}_{pu}mathbf{u}_u = -mathbf{K}_{pp}mathbf{u}_p$ Substituting this expression into the reduced system of equations, we get: $begin{bmatrix} mathbf{K}_{uu} & mathbf{K}_{up} mathbf{0} & mathbf{K}_{pp} end{ bmatrix} begin{Bmatrix} mathbf{u}_u mathbf{u}_p end{Bmatrix} = begin{ Bmatrix} mathbf{f}_u mathbf{0} end{Bmatrix}$ Now, we can see that the stiffness matrix $mathbf{K}$ is no longer square, but the submatrix $begin{bmatrix} mathbf{K}_{uu} & mathbf{K}_{up} mathbf{0} & mathbf{K}_{pp} end{bmatrix}$ is square. If the stiffness matrix $mathbf{K}$ is not square, the system of equations would be inconsistent, and we would not be able to solve for the unknown displacements. Therefore, it is essential to ensure that the stiffness matrix $mathbf{K}$ is square when applying Dirichlet boundary conditions. In practice, this can be achieved by: 1. Removing the rows and columns corresponding to the known displacements from the stiffness matrix $mathbf{K}$. 2. Modifying the force vector $mathbf{f}$ to account for the known displacements. 3. Using solver that can handle the reduced system of equations. By ensuring that the stiffness matrix $mathbf{K}$ is square, we can solve the system of equations accurately and efficiently, and obtain the correct displacements and stresses in the structure. ================= [LLaMA-TOMMI-1.0 Response]: The importance of ensuring that the stiffness matrix $K$ is square in the presence of Dirichlet boundary conditions lies in the fact that square matrix allows for the direct inversion and solution of the system of equations $K = F$. When Dirichlet boundary conditions are applied, certain degrees of freedom in the global displacement vector $d$ are known and fixed, which reduces the size of the system of equations. This reduction results in smaller, square stiffness matrix $K$ that can be inverted to solve for the unknown degrees of freedom. The square nature of the matrix ensures that the system of equations is well-posed and can be solved using standard linear algebra techniques, such as Gaussian elimination or Cholesky decomposition, which require the matrix to be square. ================= [Platform Response]: The importance of ensuring that the stiffness matrix $K$ is square in the presence of Dirichlet boundary conditions is primarily related to the need for the matrix to be invertible. When Dirichlet boundary conditions are applied, specific degrees of 22 Preprint. Under review. freedom are fixed, which modifies the global stiffness matrix $K$ and the global force vector $F$. This modification involves moving the known degrees of freedom to the right-hand side of the equation, resulting in reduced system where the number of equations matches the number of unknowns. This ensures that the matrix $K$ remains square, which is crucial for solving the system of equations represented as $Kd=F$, as square matrix guarantees unique solution. If $K$ were not square, it could lead to an overdetermined or underdetermined system due to an excess or deficiency of Dirichlet boundary conditions, respectively. Such scenarios could result in inconsistencies or unsolvable equations, undermining the integrity of the finite element analysis. The context emphasizes that the presence of Dirichlet boundary conditions necessitates that $K$ is square to maintain its properties and ensure valid solution can be obtained from the system [Video 2, time 04:57; Video 1, time 02:10]. ================= [Label/Default Answer]: The importance of ensuring that the stiffness matrix $K$ is square in the presence of Dirichlet boundary conditions is related to the solvability of the finite element equations. When Dirichlet boundary conditions are applied, certain degrees of freedom are known and can be moved to the right-hand side of the equation, effectively reducing the number of unknowns in the system. This process results in reduced system where the stiffness matrix $K$ becomes square, meaning it has the same number of rows and columns. square matrix is necessary for the system of equations $Kd = F$ to be solvable, as it allows for the possibility of inverting the matrix $K$ to find the displacement vector $d$. Additionally, ensuring that $K$ is square helps maintain the positive definiteness of the matrix, which is crucial for the stability and uniqueness of the solution."
        }
    ],
    "affiliations": [
        "Department of Aerospace and Mechanical Engineering, University of Southern California",
        "Department of Astronautical Engineering, University of Southern California",
        "Department of Computer Science, University of Southern California",
        "Department of Electrical and Computer Engineering, University of Southern California"
    ]
}